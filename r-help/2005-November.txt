From Ted.Harding at nessie.mcc.ac.uk  Tue Nov  1 00:00:41 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 31 Oct 2005 23:00:41 -0000 (GMT)
Subject: [R] why does glm.predict give values over 1 ?
In-Reply-To: <Pine.GSO.4.58L.0510311737430.10652@mass-toolpike.mit.edu>
Message-ID: <XFMail.051031230041.Ted.Harding@nessie.mcc.ac.uk>

On 31-Oct-05 Rohit Singh wrote:
> Hi,
> 
>  This is a newbie question. I have been using glm to perform some
> logistic regression. However, if I take the fitted parameters (as
> part of the glm object) and pass them on the glm.predict function,
> for some test cases I am getting predicted values that are a little
> over 1.  This is a bit puzzling for me, because my understanding
> was that these numbers are probabilities and so should be between
> 0 and 1.
> 
> Thanks a lot! I'd appreciate any help you could provide.
> 
> -rohit

Indeed this should not happen, and probably there is some mistake
in the way you use the predict function (which requires a little
care).

However, it's not possible to point-point what is happening
without seeing a specific case. Can you post an example of the
code you use when this happens? And, if feasible, also an example
of data.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 31-Oct-05                                       Time: 23:00:39
------------------------------ XFMail ------------------------------



From redbeard at arrr.net  Tue Nov  1 00:14:06 2005
From: redbeard at arrr.net (Jarrett Byrnes)
Date: Mon, 31 Oct 2005 15:14:06 -0800
Subject: [R] R Graphs in Powerpoint
In-Reply-To: <XFMail.051031230041.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051031230041.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1021e23a2ea288a066f22699f827cf73@arrr.net>

Hey, all.  Quick question.  I'm attempting to use some of the great 
graphs generated in R for an upcoming talk that I'm writing in 
Powerpoint.  Copying and pasting (I'm using OSX) yields graphs that 
look great in Powerpoint - until I resize them.  Then fonts, points, 
and lines all become quite pixelated and blurry.  Even if I size the 
window properly first, and then copy and paste in the graph, when I 
then view the slideshow, the graphs come out pixelated and blurry.

Is there any good solution to this, or is this some fundamental 
incompatibility that I can't get around?

-Jarrett



From rsingh at MIT.EDU  Tue Nov  1 00:14:17 2005
From: rsingh at MIT.EDU (Rohit Singh)
Date: Mon, 31 Oct 2005 18:14:17 -0500 (EST)
Subject: [R] why does glm.predict give values over 1 ?
In-Reply-To: <XFMail.051031230041.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051031230041.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.GSO.4.58L.0510311807020.10652@mass-toolpike.mit.edu>

Hi Ted,
 So here's what I'm doing:

This is my call to predict.glm:

> pY <- predict.glm(from69.fin.glm, newdata=d.tab, type="response")

This is what the fitted glm object looks like:

> from69.fin.glm

Call:  glm(formula = TR ~ z1 + e12_div_p_n + z2 + p_n, data = j2.tab)

Coefficients:
(Intercept)           z1  e12_div_p_n           z2          p_n
  0.0462932    0.0063221   -0.0202138    0.0063221    0.0004168

Degrees of Freedom: 137 Total (i.e. Null);  133 Residual
Null Deviance:      34.32
Residual Deviance: 21.93        AIC: 149.8


This is an example of what the data file looks like

TR  s_n p_n z1 z2 z1_div_s_n z2_div_s_n z1_div_p_n z2_div_p_n e1 e2 e1_div_s_n e2_div_s_n e1_div_p_n e2_div_p_n e12 e12_div_s_n e12_div_p_n
0 169.000 167.141 8.800 3.800 0.052 0.022 0.053 0.023 -2295.000 -4007.000 -13.580 -23.710 -13.731 -23.974 0.000 0.000 0.000
1 615.500 615.352 29.700 21.800 0.048 0.035 0.048 0.035 -5344.000 -4248.000 -8.682 -6.902 -8.684 -6.903 141.740 0.230 0.230
0 409.500 388.149 5.400 19.000 0.013 0.046 0.014 0.049 -6328.000 -4597.000 -15.453 -11.226 -16.303 -11.843 1069.890 2.613 2.756
0 782.500 776.276 26.100 28.800 0.033 0.037 0.034 0.037 -1279.000 1260.000 -1.635 1.610 -1.648 1.623 67.500 0.086 0.087
1 355.500 355.117 28.800 32.400 0.081 0.091 0.081 0.091 -10600.000 -9670.000 -29.817 -27.201 -29.849 -27.230 418.560 1.177 1.179
0 184.500 164.012 4.900 9.500 0.027 0.051 0.030 0.058 -4519.000 -1901.000 -24.493 -10.304 -27.553 -11.591 -963.600 -5.223 -5.875



Thanks,
rohit

On Mon, 31 Oct 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 31-Oct-05 Rohit Singh wrote:
> > Hi,
> >
> >  This is a newbie question. I have been using glm to perform some
> > logistic regression. However, if I take the fitted parameters (as
> > part of the glm object) and pass them on the glm.predict function,
> > for some test cases I am getting predicted values that are a little
> > over 1.  This is a bit puzzling for me, because my understanding
> > was that these numbers are probabilities and so should be between
> > 0 and 1.
> >
> > Thanks a lot! I'd appreciate any help you could provide.
> >
> > -rohit
>
> Indeed this should not happen, and probably there is some mistake
> in the way you use the predict function (which requires a little
> care).
>
> However, it's not possible to point-point what is happening
> without seeing a specific case. Can you post an example of the
> code you use when this happens? And, if feasible, also an example
> of data.
>
> Best wishes,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 31-Oct-05                                       Time: 23:00:39
> ------------------------------ XFMail ------------------------------
>



From Ted.Harding at nessie.mcc.ac.uk  Tue Nov  1 00:32:49 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 31 Oct 2005 23:32:49 -0000 (GMT)
Subject: [R] why does glm.predict give values over 1 ?
In-Reply-To: <Pine.GSO.4.58L.0510311807020.10652@mass-toolpike.mit.edu>
Message-ID: <XFMail.051031233249.Ted.Harding@nessie.mcc.ac.uk>

Hi Rohit:

On 31-Oct-05 Rohit Singh wrote:
> Hi Ted,
>  So here's what I'm doing:
> 
> This is my call to predict.glm:
> 
>> pY <- predict.glm(from69.fin.glm, newdata=d.tab, type="response")
> 
> This is what the fitted glm object looks like:
> 
>> from69.fin.glm
> 
> Call:  glm(formula = TR ~ z1 + e12_div_p_n + z2 + p_n, data = j2.tab)

***>>> It looks as though you have omitted the "family" parameter.
The default is "gaussian" (see "?glm") but for logistic regression
you need 'family = "binomial"',the default link for "binomial"
being "logit" which is correct for logistic regression. If you
were doing probit analysis, for example then you would need
to specify 'family=binomial(link="probit")'. So your call to glm
should look like

from69.fin.glm <- glm(TR ~ z1 + e12_div_p_n + z2 + p_n,
                      data = j2.tab,
                      family=binomial)

Try that -- it should be OK this time! (I think your call to
predict.glm looks all right, provided the datafram d.tab is
of the correct structure for your data).

Best wishes,
Ted.


> Coefficients:
> (Intercept)           z1  e12_div_p_n           z2          p_n
>   0.0462932    0.0063221   -0.0202138    0.0063221    0.0004168
> 
> Degrees of Freedom: 137 Total (i.e. Null);  133 Residual
> Null Deviance:      34.32
> Residual Deviance: 21.93        AIC: 149.8
> 
> 
> This is an example of what the data file looks like
> 
> TR  s_n p_n z1 z2 z1_div_s_n z2_div_s_n z1_div_p_n z2_div_p_n e1 e2
> e1_div_s_n e2_div_s_n e1_div_p_n e2_div_p_n e12 e12_div_s_n e12_div_p_n
> 0 169.000 167.141 8.800 3.800 0.052 0.022 0.053 0.023 -2295.000
> -4007.000 -13.580 -23.710 -13.731 -23.974 0.000 0.000 0.000
> 1 615.500 615.352 29.700 21.800 0.048 0.035 0.048 0.035 -5344.000
> -4248.000 -8.682 -6.902 -8.684 -6.903 141.740 0.230 0.230
> 0 409.500 388.149 5.400 19.000 0.013 0.046 0.014 0.049 -6328.000
> -4597.000 -15.453 -11.226 -16.303 -11.843 1069.890 2.613 2.756
> 0 782.500 776.276 26.100 28.800 0.033 0.037 0.034 0.037 -1279.000
> 1260.000 -1.635 1.610 -1.648 1.623 67.500 0.086 0.087
> 1 355.500 355.117 28.800 32.400 0.081 0.091 0.081 0.091 -10600.000
> -9670.000 -29.817 -27.201 -29.849 -27.230 418.560 1.177 1.179
> 0 184.500 164.012 4.900 9.500 0.027 0.051 0.030 0.058 -4519.000
> -1901.000 -24.493 -10.304 -27.553 -11.591 -963.600 -5.223 -5.875
> 
> 
> 
> Thanks,
> rohit
> 
> On Mon, 31 Oct 2005 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> On 31-Oct-05 Rohit Singh wrote:
>> > Hi,
>> >
>> >  This is a newbie question. I have been using glm to perform some
>> > logistic regression. However, if I take the fitted parameters (as
>> > part of the glm object) and pass them on the glm.predict function,
>> > for some test cases I am getting predicted values that are a little
>> > over 1.  This is a bit puzzling for me, because my understanding
>> > was that these numbers are probabilities and so should be between
>> > 0 and 1.
>> >
>> > Thanks a lot! I'd appreciate any help you could provide.
>> >
>> > -rohit
>>
>> Indeed this should not happen, and probably there is some mistake
>> in the way you use the predict function (which requires a little
>> care).
>>
>> However, it's not possible to point-point what is happening
>> without seeing a specific case. Can you post an example of the
>> code you use when this happens? And, if feasible, also an example
>> of data.
>>
>> Best wishes,
>> Ted.
>>
>>
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861
>> Date: 31-Oct-05                                       Time: 23:00:39
>> ------------------------------ XFMail ------------------------------
>>

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 31-Oct-05                                       Time: 23:32:45
------------------------------ XFMail ------------------------------



From tlumley at u.washington.edu  Tue Nov  1 00:44:52 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 31 Oct 2005 15:44:52 -0800 (PST)
Subject: [R] Re  R Graphs in Powerpoint
In-Reply-To: <1021e23a2ea288a066f22699f827cf73@arrr.net>
References: <XFMail.051031230041.Ted.Harding@nessie.mcc.ac.uk>
	<1021e23a2ea288a066f22699f827cf73@arrr.net>
Message-ID: <Pine.LNX.4.63a.0510311542130.24405@homer22.u.washington.edu>

On Mon, 31 Oct 2005, Jarrett Byrnes wrote:

> Hey, all.  Quick question.  I'm attempting to use some of the great
> graphs generated in R for an upcoming talk that I'm writing in
> Powerpoint.  Copying and pasting (I'm using OSX) yields graphs that
> look great in Powerpoint - until I resize them.  Then fonts, points,
> and lines all become quite pixelated and blurry.  Even if I size the
> window properly first, and then copy and paste in the graph, when I
> then view the slideshow, the graphs come out pixelated and blurry.
>
> Is there any good solution to this, or is this some fundamental
> incompatibility that I can't get around?

This is an OS X problem -- Powerpoint on OS X doesn't use the native (PDF) 
graphics format, so copy and paste doesn't work well.

People have posted solutions in the past, ranging from going via Photoshop 
to creating png() graphics files in R and importing them into Powerpoint.

 	-thomas



From ggrothendieck at gmail.com  Tue Nov  1 04:58:33 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 31 Oct 2005 22:58:33 -0500
Subject: [R] nls() fit to Kahnemann/ Tversky function
In-Reply-To: <4366972F.1090909@t-online.de>
References: <4366972F.1090909@t-online.de>
Message-ID: <971536df0510311958i1266f886r71ae7a719b19e512@mail.gmail.com>

Note that a simple logistic with a saturation level of 1 seems
to do quite well.  Below we have removed the last point in order
to avoid the singularity:

x <- p.kum[-10]
y <- felt.prob.kum[-10]
plot(log(y/(1-y)) ~ x)
abline(lm(log(y/(1-y)) ~ x), col = "red")

On 10/31/05, Mark Hempelmann <neo27 at t-online.de> wrote:
> Dear WizaRds,
>
>     I would like to fit a curve to ten points with nls() for one
> unknown parameter gamma in the Kahnemann/ Tversky function, but somehow
> it won't work and I am unable to locate my mistake.
>
> p.kum <- seq(0.1,1, by=0.1)
> felt.prob.kum <- c(0.16, 0.23, 0.36, 0.49, 0.61, 0.71, 0.85, 0.89, 0.95,
> 1) ## how to find a function that fits these points nicely?
> plot(p.kum, felt.prob.kum) ## looks a little like an "S"
>
> gamma <- rep(0.5, 10)
> nls.dataframe <- data.frame(p.kum,felt.prob.kum, gamma)
>
> nls.kurve <- nls( formula = felt.prob.kum ~
> p.kum^gamma/(p.kum^gamma+(1-p.kum)^gamma)^(1/gamma), data=nls.dataframe,
> start=c(gamma=gamma), algorithm="plinear" )
>
> summary(nls.kurve)
>
> gives: Error in La.chol2inv(x, size) : 'size' cannot exceed nrow(x) = 10
>
>     If I go with the Gauss-Newton algorithm I get an singular gradient
> matrix error, so I tried the Golub-Pereyra algorithm for partially
> linear least-squares.
>
>     It also seems the nls model tries to find ten different gammas, but
> I want only one single gamma parameter for the function. I appreciate
> your help and support. Thank you.
>
> sol lucet omnibus
> Mark Hempelmann
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mcclatchie.sam at saugov.sa.gov.au  Tue Nov  1 07:30:30 2005
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Tue, 1 Nov 2005 17:00:30 +1030 
Subject: [R] balloonplot/ package gplots/ getting rid of the grid
Message-ID: <BEA6A7E18959A04385DC14D24619F89F01D73E21@sagemsg0008.sagemsmrd01.sa.gov.au>

Background:
OS: Linux Mandrake 10.1
release: R 2.1.1
editor: GNU Emacs 21.3.2
front-end: ESS 5.2.3
---------------------------------

Colleagues

Does anyone know how to get rid of the grid in balloonplot? I have read the
help file.

tt <- seq(1,10)
tt2 <- tt+20
tt3 <-tt*1.5
balloonplot(tt, tt2, tt3)

I have a matrix of 278 * 278 and the grid makes the plot opaque.

Thanks, 

Sam
----
Sam McClatchie,
Biological oceanography 
South Australian Aquatic Sciences Centre
PO Box 120, Henley Beach 5022
Adelaide, South Australia
email <mcclatchie.sam at saugov.sa.gov.au>
Cellular: 0431 304 497 
Telephone: (61-8) 8207 5448
FAX: (61-8) 8207 5481
Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
  
                   /\
      ...>><xX(??> 
                //// \\\\
                   <??)Xx><<
              /////  \\\\\\
                        ><(((??> 
  >><(((??>   ...>><xX(??>O<??)Xx><<



From ggrothendieck at gmail.com  Tue Nov  1 08:02:25 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 1 Nov 2005 02:02:25 -0500
Subject: [R] balloonplot/ package gplots/ getting rid of the grid
In-Reply-To: <BEA6A7E18959A04385DC14D24619F89F01D73E21@sagemsg0008.sagemsmrd01.sa.gov.au>
References: <BEA6A7E18959A04385DC14D24619F89F01D73E21@sagemsg0008.sagemsmrd01.sa.gov.au>
Message-ID: <971536df0510312302m391b9a50o44d0c4abe1f03820@mail.gmail.com>

Get the gplots:::balloonplot.default source and remove
the two abline lines.

On 11/1/05, McClatchie, Sam (PIRSA-SARDI)
<mcclatchie.sam at saugov.sa.gov.au> wrote:
> Background:
> OS: Linux Mandrake 10.1
> release: R 2.1.1
> editor: GNU Emacs 21.3.2
> front-end: ESS 5.2.3
> ---------------------------------
>
> Colleagues
>
> Does anyone know how to get rid of the grid in balloonplot? I have read the
> help file.
>
> tt <- seq(1,10)
> tt2 <- tt+20
> tt3 <-tt*1.5
> balloonplot(tt, tt2, tt3)
>
> I have a matrix of 278 * 278 and the grid makes the plot opaque.
>
> Thanks,
>
> Sam
> ----
> Sam McClatchie,
> Biological oceanography
> South Australian Aquatic Sciences Centre
> PO Box 120, Henley Beach 5022
> Adelaide, South Australia
> email <mcclatchie.sam at saugov.sa.gov.au>
> Cellular: 0431 304 497
> Telephone: (61-8) 8207 5448
> FAX: (61-8) 8207 5481
> Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
>
>                   /\
>      ...>><xX(??>
>                //// \\\\
>                   <??)Xx><<
>              /////  \\\\\\
>                        ><(((??>
>  >><(((??>   ...>><xX(??>O<??)Xx><<
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Bernhard_Pfaff at fra.invesco.com  Tue Nov  1 09:36:53 2005
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 1 Nov 2005 08:36:53 -0000 
Subject: [R] How can I test temporal autocorrelation of binary data?
Message-ID: <25D1C2585277D311B9A20000F6CCC71B0735BB10@DEFRAEX02>


Depending on what you are really want to infer from the autocorrelations,
you want to consider the runs-test, too.

HTH,
Bernhard



If you mean you want to test that there is no autocorrelation,
then there is some information on using the Ljung-Box test on
such data in the working paper 'Robustness of the Ljung-Box
test and its rank equivalent' on the Burns Statistics website.

The executive summary is that the test seems to do okay as
long as one of the values is not too dominant.  What 'dominant'
means depends on the length of the series.

If some one has a better answer, I'm keen to hear it.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

dschad at umich.edu wrote:

>Hi,
>
>I have a binary (o/1 - coded) data set and want to test it's
autocorrelation
>structure. Is that function implemented in R?
>Can I use the ACF - funtion with binary data?
>
>Thanks for your help,
>Daniel
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>
>
>  
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
-------------- next part --------------
*****************************************************************
Confidentiality Note: The information contained in this message,
and any attachments, may contain confidential and/or privileged
material. It is intended solely for the person(s) or entity to
which it is addressed. Any review, retransmission, dissemination,
or taking of any action in reliance upon this information by
persons or entities other than the intended recipient(s) is
prohibited. If you received this in error, please contact the
sender and delete the material from any computer.
*****************************************************************

From d.scott at auckland.ac.nz  Tue Nov  1 10:43:10 2005
From: d.scott at auckland.ac.nz (David Scott)
Date: Tue, 1 Nov 2005 22:43:10 +1300 (NZDT)
Subject: [R] Help with try or tryCatch
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020C9466FA@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB548020C9466FA@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <Pine.LNX.4.61.0511011138360.28280@stat12.stat.auckland.ac.nz>

On Mon, 31 Oct 2005, McGehee, Robert wrote:

> It sounds like you want `try` with the argument `silent = TRUE`. This
> will allow you to keep running your program without errors. If you want
> to check if the line had an error, you can error control by seeing if
> the class of the resulting object is "try-error". For example, let's say
> I wanted to make an error-proof `plus` function, such that trying "a" +
> 2 would result in NA instead of an error.
>
> newPlus <- function(x, y) {
> 	answer <- try(x + y, silent = TRUE)
> 	if (class(answer) == "try-error") return(NA) else return(answer)
> }
>

This approach worked. I had to define a test function of this sort outside 
of the loops and then call it within the loops with appropriate parameter 
values. Thanks for the assistance.

David Scott


-------------------------------------------------------
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From h.brunschwig at utoronto.ca  Tue Nov  1 11:03:31 2005
From: h.brunschwig at utoronto.ca (Hadassa Brunschwig)
Date: Tue,  1 Nov 2005 08:03:31 -0200
Subject: [R] R2WinBUGS: Comparison to WinBUGS
Message-ID: <1130839411.43673d73a60ba@webmail.utoronto.ca>

Hi R-Users!

I know I posted the question before (see archives) but I have not been able to
find the mistake. Again using R2WinBUGS and WinBUGS does not yield the same
result (although to my opinion the commands are the same). The variance of the
parameters is much bigger and the parameter estimates are a bit different, too.
If anybody has the time and interest to get the files and data and see if he/she
has the same problems, I would be happy to provide these (and to discuss it
further). It seems though that it must be my mistake SOMEWHERE because nobody
else reported any problems yet... 

Thanks.

Hadassa
-- 

Hadassa Brunschwig
Birmannsgasse 10A
CH-4055 Basel
Switzerland
Phone: +41 78 797 6065
Email: h.brunschwig at utoronto.ca



From bitwrit at ozemail.com.au  Tue Nov  1 22:20:22 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Tue, 01 Nov 2005 21:20:22 +0000
Subject: [R] line vector plots
Message-ID: <4367DC16.4060600@ozemail.com.au>

Eduardo Klein wrote:

 > Hi Jim,
 >
 > Thanks for the tip, but I really need the same polar chart but over a 
line no in a circle. I didn't find it on the plotrix package.
 >
Hmmm, are you looking for something like this?

feather.plot<-function(x,y,xpos,yref=0,use.arrows=FALSE,...) {
  if(missing(xpos)) xpos<-1:length(x)
  xlim<-range(x+xpos)
  ylim<-range(y)
  plot(0,xlim=xlim,ylim=ylim,type="n")
  abline(h=yref)
  if(use.arrows) arrows(xpos,yref,xpos+x,y,length=0.1,...)
  else segments(xpos,yref,xpos+x,y,...)
}

where x and y are the components of the vectors and xpos is the 
positions on the time line? If so, I'll add that to plotrix - looks useful.

Jim



From Christoph.Scherber at uni-jena.de  Tue Nov  1 12:32:20 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Tue, 01 Nov 2005 12:32:20 +0100
Subject: [R] standard errors in summary.lm
Message-ID: <43675244.8060906@uni-jena.de>

Dear R users,

If I have an aov object, how are the standard errors of the estimates in 
summary.lm calculated?

Using treatment contrasts, I would like to use the estimated differences 
in mean values ("intercepts") to calculate the mean values per factor 
level, and for these mean values I??f like to use the model output to 
calculate the standard errors.

Many thanks for your help!
Regards,
Christoph.



From ftm20 at cam.ac.uk  Tue Nov  1 12:37:25 2005
From: ftm20 at cam.ac.uk (Fiona Mc Cahey)
Date: Tue, 1 Nov 2005 11:37:25 -0000
Subject: [R] How to change label size in a plot
Message-ID: <000901c5ded8$a1b6c860$0b796f83@ch.cam.ac.uk>

Dear all,

I have created a plot like this:

matrix<-read.table("G:\\my documents\\names.txt",header=T)
hc<-hclust(dist(matrix)^2,"cen")
memb<-cutree(hc,k=10)
plot(hc,col="black")

This prints out a dendrogram with labels, but the text of the labels is
large, so they overlap and are unreadable. How can I decrease the size of
the lable text on a dendrogram so that it does not overlap and is therefore
readable?

I know the question is similar to that posted by Michael Schmitt on 19 March
2005, but as far as I can see, the question was not answered.

Many Thanks,

Fiona
____________________________________________
Fíona Mc Cahey

Department of Chemistry
University of Cambridge
Lensfield Road
Cambridge, CB2 1EW



From Wolfgang.Viechtbauer at STAT.unimaas.nl  Tue Nov  1 13:18:49 2005
From: Wolfgang.Viechtbauer at STAT.unimaas.nl (Viechtbauer Wolfgang (STAT))
Date: Tue, 1 Nov 2005 13:18:49 +0100
Subject: [R] Doubly Non-Central F-Distribution
Message-ID: <329A68716B57D54E8D39FD3F8A4A84DF01F8AE2B@um-mail0136.unimaas.nl>

Hello All,

Has anyone written a function for the distribution function of a
*doubly* non-central F-distribution? I looked through the archives, but
didn't find anything. Thanks!

Wolfgang



From rjohnson at ncifcrf.gov  Tue Nov  1 14:22:21 2005
From: rjohnson at ncifcrf.gov (Randall C Johnson {Contr.])
Date: Tue, 01 Nov 2005 08:22:21 -0500
Subject: [R] R Graphs in Powerpoint
In-Reply-To: <1021e23a2ea288a066f22699f827cf73@arrr.net>
Message-ID: <BF8CD63D.5191%rjohnson@ncifcrf.gov>

Have you tried using Keynote? It is a part of iWork and handles pdf files
properly (you can resize as many times as you want without loosing image
quality). It also will convert to and from Power Point. I would highly
recommend it to anyone using OS X.

Randy


On 10/31/05 6:14 PM, "Jarrett Byrnes" <redbeard at arrr.net> wrote:

> Hey, all.  Quick question.  I'm attempting to use some of the great
> graphs generated in R for an upcoming talk that I'm writing in
> Powerpoint.  Copying and pasting (I'm using OSX) yields graphs that
> look great in Powerpoint - until I resize them.  Then fonts, points,
> and lines all become quite pixelated and blurry.  Even if I size the
> window properly first, and then copy and paste in the graph, when I
> then view the slideshow, the graphs come out pixelated and blurry.
> 
> Is there any good solution to this, or is this some fundamental
> incompatibility that I can't get around?
> 
> -Jarrett
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Randall C Johnson
Bioinformatics Analyst
SAIC-Frederick, Inc (Contractor)
Laboratory of Genomic Diversity
NCI-Frederick
P.O. Box B
1050 Boyles Street
Bldg 560, Rm 11-85
Frederick, MD 21702
Phone: (301) 846-1304
Fax: (301) 846-1686
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From andy_liaw at merck.com  Tue Nov  1 14:24:32 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 1 Nov 2005 08:24:32 -0500
Subject: [R] Help with try or tryCatch
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED547@usctmx1106.merck.com>

Just a bit of nitpicking:  I believe the preferred way is to see if the
result of try() _inherits_ the try-error class.  This applies to all S3
classes.  I.e., the relevant line should be something like:

    if (inherits(answer, "try-error")) ...

Andy

> From: David Scott
> 
> On Mon, 31 Oct 2005, McGehee, Robert wrote:
> 
> > It sounds like you want `try` with the argument `silent = 
> TRUE`. This
> > will allow you to keep running your program without errors. 
> If you want
> > to check if the line had an error, you can error control by 
> seeing if
> > the class of the resulting object is "try-error". For 
> example, let's say
> > I wanted to make an error-proof `plus` function, such that 
> trying "a" +
> > 2 would result in NA instead of an error.
> >
> > newPlus <- function(x, y) {
> > 	answer <- try(x + y, silent = TRUE)
> > 	if (class(answer) == "try-error") return(NA) else return(answer)
> > }
> >
> 
> This approach worked. I had to define a test function of this 
> sort outside 
> of the loops and then call it within the loops with 
> appropriate parameter 
> values. Thanks for the assistance.
> 
> David Scott
> 
> 
> -------------------------------------------------------
> David Scott	Department of Statistics, Tamaki Campus
>  		The University of Auckland, PB 92019
>  		Auckland	NEW ZEALAND
> Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
> Email:	d.scott at auckland.ac.nz
> 
> 
> Graduate Officer, Department of Statistics
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Jo.Hardin at pomona.edu  Tue Nov  1 14:20:52 2005
From: Jo.Hardin at pomona.edu (Johanna Hardin)
Date: Tue, 1 Nov 2005 05:20:52 -0800
Subject: [R] Greek letters in plots
Message-ID: <658E70C89419784380590FDD04A1D918034B7EAF@EVSFACULTYSTAFF.campus.pomona.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051101/bed3cafa/attachment.pl

From andy_liaw at merck.com  Tue Nov  1 14:35:30 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 1 Nov 2005 08:35:30 -0500
Subject: [R] write.table call
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED548@usctmx1106.merck.com>

RTFM, in particular the "CSV Files" section of ?write.table.

BTW, R itself does not write "xls" files.

Andy

> From: Li,Qinghong,ST.LOUIS,Molecular Biology
> 
> Hi,
> 
> I use write.table() to write a file to an external xls file. 
> the column names left-shift one position in output file. I 
> check with col.names() row.names(), the file is fine. How to 
> prevent the shifting? 
> 
> I71	I111	I304	I307	I305	I306	I114	I72		
> AFFX-BioB-5_at	6.66435	6.787807	5.335962	
> 5.250163	6.47423	5.882104	5.965109	6.591687195	
> AFFX-BioB-M_at	6.163227	5.965427	
> 4.665569	2.743531	6.097244	5.77137	
> 5.113683	6.314003982	
> 
> Thanks,
> Johnny
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From B.Rowlingson at lancaster.ac.uk  Tue Nov  1 14:49:44 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 01 Nov 2005 13:49:44 +0000
Subject: [R] Greek letters in plots
In-Reply-To: <658E70C89419784380590FDD04A1D918034B7EAF@EVSFACULTYSTAFF.campus.pomona.edu>
References: <658E70C89419784380590FDD04A1D918034B7EAF@EVSFACULTYSTAFF.campus.pomona.edu>
Message-ID: <43677278.1050807@lancaster.ac.uk>

Johanna Hardin wrote:

> 
> *      paste("rho=", cor2[i])

> will produce a label of "rho=0.74", or whatever.  But if I use
> 'substitute' or 'evaluate' commands in order to get a real Greek letter,
> I lose the ability to paste it with a data value.

  Use parse() to turn that text into an expression. And use '==':

  > rho=0.78
  > plot(1:10,main=parse(text=paste("rho == ",rho)))

Not sure where your label is going, but the same thing should work for 
axes and text labels.

Baz



From MSchwartz at mn.rr.com  Tue Nov  1 14:54:20 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 01 Nov 2005 07:54:20 -0600
Subject: [R] Greek letters in plots
In-Reply-To: <658E70C89419784380590FDD04A1D918034B7EAF@EVSFACULTYSTAFF.campus.pomona.edu>
References: <658E70C89419784380590FDD04A1D918034B7EAF@EVSFACULTYSTAFF.campus.pomona.edu>
Message-ID: <1130853260.27481.0.camel@localhost.localdomain>

On Tue, 2005-11-01 at 05:20 -0800, Johanna Hardin wrote:
> Hi, all.  I know that this is probably something that others have asked,
> but I can't find a reference in either the FAQ or the help pages.
> 
> I'm trying to find a way to put Greek letters as a label of the plot
> *with* a value from the data.  Previously I've used pasted and the word
> "rho".
>
> *      paste("rho=", cor2[i])
> 
> will produce a label of "rho=0.74", or whatever.  But if I use
> 'substitute' or 'evaluate' commands in order to get a real Greek letter,
> I lose the ability to paste it with a data value.

> Any ideas?

> Thanks, Jo

Try this:

  cor2 <- 0.74
  plot(1:5)
  title(bquote(rho == .(cor2)))

See ?plotmath and ?bquote for more information, noting the use of
the .(Variable) syntax for variable substitution.

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Tue Nov  1 15:08:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Nov 2005 15:08:03 +0100
Subject: [R] R2WinBUGS: Comparison to WinBUGS
In-Reply-To: <1130839411.43673d73a60ba@webmail.utoronto.ca>
References: <1130839411.43673d73a60ba@webmail.utoronto.ca>
Message-ID: <436776C3.9030400@statistik.uni-dortmund.de>

Hadassa Brunschwig wrote:
> Hi R-Users!
> 
> I know I posted the question before (see archives) but I have not been able to
> find the mistake. Again using R2WinBUGS and WinBUGS does not yield the same
> result (although to my opinion the commands are the same). The variance of the
> parameters is much bigger and the parameter estimates are a bit different, too.
> If anybody has the time and interest to get the files and data and see if he/she
> has the same problems, I would be happy to provide these (and to discuss it
> further). It seems though that it must be my mistake SOMEWHERE because nobody
> else reported any problems yet... 
> 
> Thanks.
> 
> Hadassa

Please send me your data in a private message. Perhaps we (let me 
include Sibylle Sturtz here, since she certainly knows better) find some 
time during the next week to look into it.

Uwe Ligges



From tlumley at u.washington.edu  Tue Nov  1 16:13:06 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 1 Nov 2005 07:13:06 -0800 (PST)
Subject: [R] Greek letters in plots
In-Reply-To: <1130853260.27481.0.camel@localhost.localdomain>
References: <658E70C89419784380590FDD04A1D918034B7EAF@EVSFACULTYSTAFF.campus.pomona.edu>
	<1130853260.27481.0.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.63a.0511010711300.30147@homer21.u.washington.edu>

On Tue, 1 Nov 2005, Marc Schwartz wrote:

> On Tue, 2005-11-01 at 05:20 -0800, Johanna Hardin wrote:
>> Hi, all.  I know that this is probably something that others have asked,
>> but I can't find a reference in either the FAQ or the help pages.

FAQ 7.13

and the example in ?plotmath that starts
      ## How to combine "math" and numeric variables :


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From p.dalgaard at biostat.ku.dk  Tue Nov  1 16:16:29 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Nov 2005 16:16:29 +0100
Subject: [R] Greek letters in plots
In-Reply-To: <1130853260.27481.0.camel@localhost.localdomain>
References: <658E70C89419784380590FDD04A1D918034B7EAF@EVSFACULTYSTAFF.campus.pomona.edu>
	<1130853260.27481.0.camel@localhost.localdomain>
Message-ID: <x2acgos8j6.fsf@viggo.kubism.ku.dk>

Marc Schwartz <MSchwartz at mn.rr.com> writes:

> Try this:
> 
>   cor2 <- 0.74
>   plot(1:5)
>   title(bquote(rho == .(cor2)))
> 
> See ?plotmath and ?bquote for more information, noting the use of
> the .(Variable) syntax for variable substitution.

Yep, bquote() is nice. Other solutions include

  substitute(rho == . , list( . = cor2))

which I suppose is pretty much what bquote does internally
-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From chris.bodily at autolivasp.com  Tue Nov  1 17:15:50 2005
From: chris.bodily at autolivasp.com (chris.bodily@autolivasp.com)
Date: Tue, 1 Nov 2005 09:15:50 -0700
Subject: [R] Unexpected result from binary greater than operator
Message-ID: <OF4164ECA1.669FA2D9-ON872570AC.00574D6A-872570AC.005956DB@LocalDomain>

Hi All,

I recently encountered results that I did not expect, exhibited by the
following code snippet:

test <- function() {
    minX <- 4.2
    min0 <- 4.1
    sigmaG <- 0.1
    Diff <- minX-min0
    print(c(Diff=Diff,sigmaG=sigmaG))
    cat("is Diff > sigmaG?:", Diff > sigmaG,"\n")
    cat("is (4.2 - 4.1) > 0.1?:",(4.2 - 4.1) > 0.1,"\n")
    cat("is 0.1 > 0.1?:", 0.1>0.1,"\n")
}

When I execute the above function I get the following:
> test()
  Diff sigmaG
   0.1    0.1
is Diff > sigmaG?: TRUE
is (4.2 - 4.1) > 0.1?: TRUE
is 0.1 > 0.1?: FALSE

Can someone please help me understand why R returns TRUE for (4.2 - 4.1) >
0.1 ?

Thanks so much,
Chris


I'm running the precompiled R-2.2.0 binary for Windows on WinXP Pro SP1.

>Sys.getlocale()
"LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252"

>version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    2.0
year     2005
month    10
day      06
svn rev  35749
language R



From murdoch at stats.uwo.ca  Tue Nov  1 17:22:00 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 01 Nov 2005 11:22:00 -0500
Subject: [R] Unexpected result from binary greater than operator
In-Reply-To: <OF4164ECA1.669FA2D9-ON872570AC.00574D6A-872570AC.005956DB@LocalDomain>
References: <OF4164ECA1.669FA2D9-ON872570AC.00574D6A-872570AC.005956DB@LocalDomain>
Message-ID: <43679628.2010207@stats.uwo.ca>

On 11/1/2005 11:15 AM, chris.bodily at autolivasp.com wrote:

> Can someone please help me understand why R returns TRUE for (4.2 - 4.1) >
> 0.1 ?

Rounding error.  See the FAQ item

7.31 Why doesn't R think these numbers are equal?

The only numbers that can be represented exactly in R's numeric type are 
integers and fractions whose denominator is a power of 2. Other numbers 
have to be rounded to (typically) 53 binary digits accuracy. As a 
result, two floating point numbers will not reliably be equal unless 
they have been computed by the same algorithm, and not always even then. 
For example

      R> a <- sqrt(2)
      R> a * a == 2
      [1] FALSE
      R> a * a - 2
      [1] 4.440892e-16

The function all.equal() compares two objects using a numeric tolerance 
of .Machine$double.eps ^ 0.5. If you want much greater accuracy than 
this you will need to consider error propagation carefully.

For more information, see e.g. David Goldberg (1991), ?What Every 
Computer Scientist Should Know About Floating-Point Arithmetic?, ACM 
Computing Surveys, 23/1, 5?48, also available via 
http://docs.sun.com/source/806-3568/ncg_goldberg.html

Duncan Murdoch



From sundar.dorai-raj at pdf.com  Tue Nov  1 17:26:13 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 01 Nov 2005 10:26:13 -0600
Subject: [R] Unexpected result from binary greater than operator
In-Reply-To: <OF4164ECA1.669FA2D9-ON872570AC.00574D6A-872570AC.005956DB@LocalDomain>
References: <OF4164ECA1.669FA2D9-ON872570AC.00574D6A-872570AC.005956DB@LocalDomain>
Message-ID: <43679725.1020708@pdf.com>



chris.bodily at autolivasp.com wrote:
> Hi All,
> 
> I recently encountered results that I did not expect, exhibited by the
> following code snippet:
> 
> test <- function() {
>     minX <- 4.2
>     min0 <- 4.1
>     sigmaG <- 0.1
>     Diff <- minX-min0
>     print(c(Diff=Diff,sigmaG=sigmaG))
>     cat("is Diff > sigmaG?:", Diff > sigmaG,"\n")
>     cat("is (4.2 - 4.1) > 0.1?:",(4.2 - 4.1) > 0.1,"\n")
>     cat("is 0.1 > 0.1?:", 0.1>0.1,"\n")
> }
> 
> When I execute the above function I get the following:
> 
>>test()
> 
>   Diff sigmaG
>    0.1    0.1
> is Diff > sigmaG?: TRUE
> is (4.2 - 4.1) > 0.1?: TRUE
> is 0.1 > 0.1?: FALSE
> 
> Can someone please help me understand why R returns TRUE for (4.2 - 4.1) >
> 0.1 ?
> 
> Thanks so much,
> Chris
> 
> 
> I'm running the precompiled R-2.2.0 binary for Windows on WinXP Pro SP1.
> 
> 
>>Sys.getlocale()
> 
> "LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252"
> 
> 
>>version
> 
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.0
> year     2005
> month    10
> day      06
> svn rev  35749
> language R
> 

Seems like this is coming up about once a week. See FAQ 7.31.

http://cran.r-project.org/doc/FAQ/R-FAQ.html

Hint:

print(4.2 - 4.1, digits = 16)

--sundar



From p.dalgaard at biostat.ku.dk  Tue Nov  1 17:28:35 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Nov 2005 17:28:35 +0100
Subject: [R] Unexpected result from binary greater than operator
In-Reply-To: <OF4164ECA1.669FA2D9-ON872570AC.00574D6A-872570AC.005956DB@LocalDomain>
References: <OF4164ECA1.669FA2D9-ON872570AC.00574D6A-872570AC.005956DB@LocalDomain>
Message-ID: <x2wtjsqqmk.fsf@viggo.kubism.ku.dk>

chris.bodily at autolivasp.com writes:

> Hi All,
> 
> I recently encountered results that I did not expect, exhibited by the
> following code snippet:
> 
> test <- function() {
>     minX <- 4.2
>     min0 <- 4.1
>     sigmaG <- 0.1
>     Diff <- minX-min0
>     print(c(Diff=Diff,sigmaG=sigmaG))
>     cat("is Diff > sigmaG?:", Diff > sigmaG,"\n")
>     cat("is (4.2 - 4.1) > 0.1?:",(4.2 - 4.1) > 0.1,"\n")
>     cat("is 0.1 > 0.1?:", 0.1>0.1,"\n")
> }
> 
> When I execute the above function I get the following:
> > test()
>   Diff sigmaG
>    0.1    0.1
> is Diff > sigmaG?: TRUE
> is (4.2 - 4.1) > 0.1?: TRUE
> is 0.1 > 0.1?: FALSE
> 
> Can someone please help me understand why R returns TRUE for (4.2 - 4.1) >
> 0.1 ?

Section 7.31 in the FAQ should help:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f
 


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From petr.pikal at precheza.cz  Tue Nov  1 17:29:07 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 01 Nov 2005 17:29:07 +0100
Subject: [R] Unexpected result from binary greater than operator
In-Reply-To: <OF4164ECA1.669FA2D9-ON872570AC.00574D6A-872570AC.005956DB@precheza.cz>
Message-ID: <4367A5E3.30369.23A58D9@localhost>

Hi.
Easy. Regardless of OS and R version answer is

binary representation of fractions.

>    (4.2-4.1)>.1
[1] TRUE

but

>    (4.3-4.2)>.1
[1] FALSE

> print(4.3-4.2, digits=20)
[1] 0.099999999999999645

You can not expect precise results using not precise computers.

Use e.g. round

>    round(4.3-4.2)>.1
[1] FALSE

HTH
Petr


On 1 Nov 2005 at 9:15, chris.bodily at autolivasp.com wrote:

To:             	r-help at stat.math.ethz.ch
From:           	chris.bodily at autolivasp.com
Date sent:      	Tue, 1 Nov 2005 09:15:50 -0700
Subject:        	[R] Unexpected result from binary greater than operator

> Hi All,
> 
> I recently encountered results that I did not expect, exhibited by the
> following code snippet:
> 
> test <- function() {
>     minX <- 4.2
>     min0 <- 4.1
>     sigmaG <- 0.1
>     Diff <- minX-min0
>     print(c(Diff=Diff,sigmaG=sigmaG))
>     cat("is Diff > sigmaG?:", Diff > sigmaG,"\n")
>     cat("is (4.2 - 4.1) > 0.1?:",(4.2 - 4.1) > 0.1,"\n")
>     cat("is 0.1 > 0.1?:", 0.1>0.1,"\n")
> }
> 
> When I execute the above function I get the following:
> > test()
>   Diff sigmaG
>    0.1    0.1
> is Diff > sigmaG?: TRUE
> is (4.2 - 4.1) > 0.1?: TRUE
> is 0.1 > 0.1?: FALSE
> 
> Can someone please help me understand why R returns TRUE for (4.2 -
> 4.1) > 0.1 ?
> 
> Thanks so much,
> Chris
> 
> 
> I'm running the precompiled R-2.2.0 binary for Windows on WinXP Pro
> SP1.
> 
> >Sys.getlocale()
> "LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252"
> 
> >version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.0
> year     2005
> month    10
> day      06
> svn rev  35749
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From abunn at whrc.org  Tue Nov  1 17:34:40 2005
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 1 Nov 2005 11:34:40 -0500
Subject: [R] Unexpected result from binary greater than operator
In-Reply-To: <OF4164ECA1.669FA2D9-ON872570AC.00574D6A-872570AC.005956DB@LocalDomain>
Message-ID: <NEBBIPHDAMMOKDKPOFFIEECBDMAA.abunn@whrc.org>

Look at ?all.equal and ?identical as well as searching the archives for
those terms. You'll find many an illuminating thread on precision, floating
point arithmetic and other wonders.

minX <- 4.2
min0 <- 4.1
sigmaG <- 0.1
Diff <- minX-min0
all.equal(Diff, sigmaG)
identical(Diff, sigmaG)

HTH, Andy



From pieterprovoost at gmail.com  Tue Nov  1 17:38:45 2005
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Tue, 01 Nov 2005 17:38:45 +0100
Subject: [R] dataframe conversion
Message-ID: <43679A15.7060801@gmail.com>

The data structures in R are still very puzzling to me. Can anyone tell 
me how I can easily convert these two dataframes to one single dataframe 
with two columns (mean and sd) with 7 rows?

 > meanprofile
       V1       V2       V3       V4       V5       V6       V7
2292.001 2178.620 1654.310 1784.004 1160.052 1142.061 1046.675
 > sdprofile
       V1       V2       V3       V4       V5       V6       V7
310.6714 347.2072 197.2464 532.3916 161.2955 227.3634 108.5017

Thanks!
Pieter



From abunn at whrc.org  Tue Nov  1 17:50:56 2005
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 1 Nov 2005 11:50:56 -0500
Subject: [R] dataframe conversion
In-Reply-To: <43679A15.7060801@gmail.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIEECCDMAA.abunn@whrc.org>

> The data structures in R are still very puzzling to me. Can anyone tell 
> me how I can easily convert these two dataframes to one single dataframe 
> with two columns (mean and sd) with 7 rows?
> 
>  > meanprofile
>        V1       V2       V3       V4       V5       V6       V7
> 2292.001 2178.620 1654.310 1784.004 1160.052 1142.061 1046.675
>  > sdprofile
>        V1       V2       V3       V4       V5       V6       V7
> 310.6714 347.2072 197.2464 532.3916 161.2955 227.3634 108.5017

There are several ways to go about this. Here's one:
t(rbind(meanprofile, sdprofile))

?rbind and ?t
HTH, Andy



From ligges at statistik.uni-dortmund.de  Tue Nov  1 17:53:57 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Nov 2005 17:53:57 +0100
Subject: [R] dataframe conversion
In-Reply-To: <43679A15.7060801@gmail.com>
References: <43679A15.7060801@gmail.com>
Message-ID: <43679DA5.6030202@statistik.uni-dortmund.de>

Pieter Provoost wrote:
> The data structures in R are still very puzzling to me. Can anyone tell 
> me how I can easily convert these two dataframes to one single dataframe 
> with two columns (mean and sd) with 7 rows?
> 
>  > meanprofile
>        V1       V2       V3       V4       V5       V6       V7
> 2292.001 2178.620 1654.310 1784.004 1160.052 1142.061 1046.675
>  > sdprofile
>        V1       V2       V3       V4       V5       V6       V7
> 310.6714 347.2072 197.2464 532.3916 161.2955 227.3634 108.5017


This is quite an unusual task, because you won't have data structure in 
a data.frame most of the times. In particular, you cannot make a row 
from a data.frame to a column generally.

In this case, we just convert the data.frame to matrix. I think most 
easily you can write:

   dat <- t(rbind(meanprofile, sdprofile))
   colnames(dat) <- c("meanprofile", "sdprofile")

Uwe Ligges



> Thanks!
> Pieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Nov  1 17:55:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Nov 2005 17:55:17 +0100
Subject: [R] How to change label size in a plot
In-Reply-To: <000901c5ded8$a1b6c860$0b796f83@ch.cam.ac.uk>
References: <000901c5ded8$a1b6c860$0b796f83@ch.cam.ac.uk>
Message-ID: <43679DF5.7020708@statistik.uni-dortmund.de>

Fiona Mc Cahey wrote:

> Dear all,
> 
> I have created a plot like this:
> 
> matrix<-read.table("G:\\my documents\\names.txt",header=T)
> hc<-hclust(dist(matrix)^2,"cen")
> memb<-cutree(hc,k=10)
> plot(hc,col="black")

What about specifying argument "cex"?

Uwe Ligges


> This prints out a dendrogram with labels, but the text of the labels is
> large, so they overlap and are unreadable. How can I decrease the size of
> the lable text on a dendrogram so that it does not overlap and is therefore
> readable?
> 
> I know the question is similar to that posted by Michael Schmitt on 19 March
> 2005, but as far as I can see, the question was not answered.
> 
> Many Thanks,
> 
> Fiona
> ____________________________________________
> Fíona Mc Cahey
> 
> Department of Chemistry
> University of Cambridge
> Lensfield Road
> Cambridge, CB2 1EW
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Nov  1 17:58:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Nov 2005 17:58:17 +0100
Subject: [R] Build R package with shared library
In-Reply-To: <a55593730510311332k53e881e7n93861f3ed228bd34@mail.gmail.com>
References: <a55593730510311332k53e881e7n93861f3ed228bd34@mail.gmail.com>
Message-ID: <43679EA9.1060003@statistik.uni-dortmund.de>

Marcelo Damasceno wrote:

> Hello to all,
> 
> I am try to build a package, I do the follow commands: "R CMD check pack",
> "R CMD pack build" and run OK, no errors. I put my shared library in package
> subdirectory R, src, but it is not put a shared library ".so" in directory
> "/usr/lib/R/library/pack/lib". Below is my code :


Hmmm. You have to INSTALL a package. Shared libraries will be build on 
the fly while installing. Hence I think you have not read the mnauals 
carefully enough.

You can put C and Fortran files in ./src, the shared library will be 
installed during
   R CMD INSTALL pack
into the ./lib directory.


> dyn.load("pack.so",PACKAGE="pack")
> argc<-2
> argv<-c("./test",file1)
> .C("main",as.integer(argc),as.vector(argv),PACKAGE="pack")

You really want to pass a directory and a filename to your shared 
library? So, why are you using R in between?

Uwe Ligges



> What am I doing wrong?
> 
> --
> Marcelo Damasceno de Melo
> Graduando em Ci??ncia da Computa????o
> Departamento de Tecnologia da Informa????o - TCI
> Universidade Federal de Alagoas - UFAL
> Macei?? - Alagoas - Brasil
> Projeto CoCADa - Constru????o do Conhecimento por Agrupamento de dados
> +55 82 8801-2119
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From cznm4 at mizzou.edu  Tue Nov  1 17:57:02 2005
From: cznm4 at mizzou.edu (Zhu, Chao (UMC-Student))
Date: Tue, 1 Nov 2005 10:57:02 -0600
Subject: [R] kernel smoothing
Message-ID: <C4A33886378A9447B5C05B3010868146FFA40A@UM-EMAIL10.um.umsystem.edu>

Dear all,
 
I want to use kernel estimator to smooth some step funtions such as the empirical distribution and hazard function in survival analysis. Is there a function or package to deal with it in R? 
How do I use it?
 
Thanks for your help.
 
Jimmy



From dejongroel at gmail.com  Tue Nov  1 18:00:42 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Tue, 01 Nov 2005 18:00:42 +0100
Subject: [R] glmmpql and lmer keep failing
Message-ID: <43679F3A.10000@gmail.com>

Hello,

I'm running a simulation study of a multilevel model with binary 
response using the binomial probit link. It is a random intercept and 
random slope model.  GLMMPQL and lmer fail to converge on a 
*significant* portion of the *generated* datasets, while MlWin gives 
reasonable estimates on those datasets. This is unacceptable. Does 
anyone has similar experiences?

Regards,
	Roel de Jong



From ggrothendieck at gmail.com  Tue Nov  1 18:06:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 1 Nov 2005 12:06:22 -0500
Subject: [R] dataframe conversion
In-Reply-To: <43679DA5.6030202@statistik.uni-dortmund.de>
References: <43679A15.7060801@gmail.com>
	<43679DA5.6030202@statistik.uni-dortmund.de>
Message-ID: <971536df0511010906o33c4b932n76d50f04f04ffe81@mail.gmail.com>

On 11/1/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Pieter Provoost wrote:
> > The data structures in R are still very puzzling to me. Can anyone tell
> > me how I can easily convert these two dataframes to one single dataframe
> > with two columns (mean and sd) with 7 rows?
> >
> >  > meanprofile
> >        V1       V2       V3       V4       V5       V6       V7
> > 2292.001 2178.620 1654.310 1784.004 1160.052 1142.061 1046.675
> >  > sdprofile
> >        V1       V2       V3       V4       V5       V6       V7
> > 310.6714 347.2072 197.2464 532.3916 161.2955 227.3634 108.5017
>
>
> This is quite an unusual task, because you won't have data structure in
> a data.frame most of the times. In particular, you cannot make a row
> from a data.frame to a column generally.
>
> In this case, we just convert the data.frame to matrix. I think most
> easily you can write:
>
>   dat <- t(rbind(meanprofile, sdprofile))
>   colnames(dat) <- c("meanprofile", "sdprofile")
>

Or perhaps:

data.frame(meanprofile = unlist(meanprofile), sdprofile = unlist(sdprofile))



From quantpm at yahoo.com  Tue Nov  1 18:09:08 2005
From: quantpm at yahoo.com (t c)
Date: Tue, 1 Nov 2005 09:09:08 -0800 (PST)
Subject: [R] percent rank by an index key?
Message-ID: <20051101170909.57746.qmail@web35001.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051101/9f7bd0bf/attachment.pl

From gunter.berton at gene.com  Tue Nov  1 18:13:43 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 1 Nov 2005 09:13:43 -0800
Subject: [R] kernel smoothing
In-Reply-To: <C4A33886378A9447B5C05B3010868146FFA40A@UM-EMAIL10.um.umsystem.edu>
Message-ID: <200511011713.jA1HDhLA013654@ohm.gene.com>

Please, please read the docs! That's what they're for.

RsiteSearch("smoothing")
help.search("smoothing")

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Zhu, 
> Chao (UMC-Student)
> Sent: Tuesday, November 01, 2005 8:57 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] kernel smoothing
> 
> Dear all,
>  
> I want to use kernel estimator to smooth some step funtions 
> such as the empirical distribution and hazard function in 
> survival analysis. Is there a function or package to deal 
> with it in R? 
> How do I use it?
>  
> Thanks for your help.
>  
> Jimmy
> 
>



From sundar.dorai-raj at pdf.com  Tue Nov  1 18:25:57 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 01 Nov 2005 11:25:57 -0600
Subject: [R] percent rank by an index key?
In-Reply-To: <20051101170909.57746.qmail@web35001.mail.mud.yahoo.com>
References: <20051101170909.57746.qmail@web35001.mail.mud.yahoo.com>
Message-ID: <4367A525.6080804@pdf.com>



t c wrote:
> What is the easiest way to calculate a percent rank ?by? an index key?
> 
>  
> 
> Foe example, I have a dataset with 3 fields:
> 
>  
> 
> Year,    State,   Income ,
> 
>  
> 
> I wish to calculate the rank, by year, by state.
> 
> I also wish to calculate the ?percent rank?, where I define percent rank as rank/n.
> 
>  
> 
> (n is the number of numeric data points within each date-state grouping.)
> 
>  
> 
>  
> 
> This is what I am currently doing:
> 
>  
> 
> 1.  I create a ?group by? field by using the paste function to combine date and state into a field called date_state.   I then use the rank function to calculate the rank by date, by state. 
> 
>  
> 
> 2. I then add a field called ?one? that I set to 1 if the value in income is numeric and to 0 if it is not.
> 
>  
> 
> 3. I then take an aggregate sum of ?one?.  This gives me a count (n) for each date-state grouping.
> 
>  
> 
>  
> 
> 4. I next use merge to add this count to the table.
> 
>  
> 
> 5. Finally, I calculate the percent rank.
> 
>  
> 
> Pr<-rank/n
> 
>  
> 
> The merge takes quite a bit of time to process. 
> 
>  
> 
> Is there an easier/more efficient way to calculate the percent rank?
> 

How about using ?by:

set.seed(100)
# fake data set, replace with your own
# "Subject" is just a dummy to produce replicates
x <- expand.grid(Year = 2000:2005,
                  State = c("TX", "AL"),
                  Subject = 1:10)
x$Income <- floor(runif(NROW(x)) * 100000)

r <- by(x$Income, x[c("Year", "State")],
         function(x) {
           r <- rank(x)
           n <- length(x)
           cbind(Rank = r, PRank = r/n)
         })
x <- cbind(x, do.call("rbind", r))

HTH,

--sundar



From andy_liaw at merck.com  Tue Nov  1 18:50:30 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 1 Nov 2005 12:50:30 -0500
Subject: [R] kernel smoothing
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED551@usctmx1106.merck.com>

Smoothing CDF or survival functions are not quite the same as usual
scatterplot smoothing, at least because of the monotonicity contraint.  I
don't know of kernel methods for smoothing such functions, but I believe
people have used splines.  I'm no expert though.  It's best for you to
consult an expert or search the literature.

Andy

> From: Zhu, Chao (UMC-Student)
> 
> Dear all,
>  
> I want to use kernel estimator to smooth some step funtions 
> such as the empirical distribution and hazard function in 
> survival analysis. Is there a function or package to deal 
> with it in R? 
> How do I use it?
>  
> Thanks for your help.
>  
> Jimmy
> 
>



From emilie.berthiaume at usherbrooke.ca  Tue Nov  1 18:55:17 2005
From: emilie.berthiaume at usherbrooke.ca (Emilie Berthiaume)
Date: Tue, 1 Nov 2005 12:55:17 -0500
Subject: [R] function effect and standard error
Message-ID: <00a701c5df0d$6bfb08f0$731ad284@eberthiaume>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051101/91c844f6/attachment.pl

From KKIII at Indiana.Edu  Tue Nov  1 19:57:17 2005
From: KKIII at Indiana.Edu (Ken Kelley)
Date: Tue, 01 Nov 2005 13:57:17 -0500
Subject: [R] Doubly Non-Central F-Distribution
In-Reply-To: <329A68716B57D54E8D39FD3F8A4A84DF01F8AE2B@um-mail0136.unimaas.nl>
References: <329A68716B57D54E8D39FD3F8A4A84DF01F8AE2B@um-mail0136.unimaas.nl>
Message-ID: <4367BA8D.6030703@Indiana.Edu>

Hi Wolfgang.

I would be surprised if a doubly noncentral F-distribution function is 
available (and encourage you to check its accuracy if it is). Although R 
allows pf() to have a noncentral value specified, the results are not 
always accurate. I posted about this about a week ago (10/24/05) and 
didn't hear anything back regarding the accuracy issue. The bug I 
reported is here (# 8251): 
<http://viggo.kubism.ku.dk/cgi-bin/R/Accuracy?id=8251;user=guest>

I am very interested in (singularly) noncentral F-distribution 
functions, so I too would like to learn of a doubly noncentral 
F-distribution function (since the singularly noncentral F is a special 
case of the doubly noncentral F; or learn of an alternative or fix to 
pf() as it now stands).

Have a good day,
Ken

Viechtbauer Wolfgang (STAT) wrote:
> Hello All,
> 
> Has anyone written a function for the distribution function of a
> *doubly* non-central F-distribution? I looked through the archives, but
> didn't find anything. Thanks!
> 
> Wolfgang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Ken Kelley, Ph.D.
Inquiry Methodology Program
Indiana University
201 North Rose Avenue, Room 4004
Bloomington, Indiana 47405
http://www.indiana.edu/~kenkel



From lars.johnsen at lili.uib.no  Tue Nov  1 20:36:19 2005
From: lars.johnsen at lili.uib.no (Lars G. Johnsen)
Date: Tue, 01 Nov 2005 20:36:19 +0100
Subject: [R] Error on read.table
Message-ID: <op.szkt2tbu0kw1f2@larsdell.uib.no>


Hello

When trying to read a table from excel, the generated error message is not  
documented in the "R data import" document:

    > v2 <- read.table("v2-101-405-excel.txt", header=T,row.names=0)
    Error in read.table("v2-101-405-excel.txt", header = T, row.names = 0) :
            attempt to select less than one element

How do I proceed to resolve this problem?

thanks
Lars

Here are the first three out of around 500 rows in the file (missing data  
in Target.RESP col is marked as NA):

Subject	Sex	Age	Gruppe	Type	Target.CRESP	Target.RESP	Target.RT	Prime.OnsetTime	Target.OnsetTime	Ordning	Ident
101	female	22	7	t	0	0	1520	155074	158497	101	145
101	female	22	9	o	0	1	3020	161678	166005	102	160



From jfox at mcmaster.ca  Tue Nov  1 20:44:37 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 01 Nov 2005 14:44:37 -0500
Subject: [R] function effect and standard error
In-Reply-To: <00a701c5df0d$6bfb08f0$731ad284@eberthiaume>
Message-ID: <web-108304903@cgpsrv2.cis.mcmaster.ca>

Dear Emilie,

This is, I think, the effect() function in the effects() package.

By the way, the model that you've fit with glm() is just a linear
model, and could also have been fit with lm().

For a model with this simple structure, effect() computes the "adjusted
means" for the factor sp, holding other predictors to their average
values. These effects are just fitted values under the model, and the
standard errors reported are for the fitted values.

For details, see the paper at
<http://www.jstatsoft.org/counter.php?id=75&url=v08/i15/effect-displays-revised.pdf>.

I hope this helps.

John

On Tue, 1 Nov 2005 12:55:17 -0500
 "Emilie Berthiaume" <emilie.berthiaume at usherbrooke.ca> wrote:
> Hi list!
> 
> I did the following regression:
> reg1 <-  glm(alti~sp + ovent + vivent + nuage, family=gaussian,
> data=meteo1)
> 
> I was interested in knowing the effect of the species (sp) in reg1
> and so I used the function ??effect??:
> 
> effect.sp <- effect ("sp", reg1, se=TRUE)
> 
> with this output:
> sp
>       AK       BW       NH       OS       RT       SS 
> 2.730101 2.885363 2.753774 2.750311 3.084606 2.834390 
> 
> If I enter the following command:
>     effect.sp$se
> I get this output:
>             1        253       3100        488        514       6100 
>     0.07924610 0.06713200 0.11493178 0.13106639 0.05252749 0.04208334
> 
> 
> My question is:  Do the numbers on the second line of this output
> represent the standard error?  What do the numbers on the top line
> represent?
> 
> Thank you,
> 
> Emilie Berthiaume
> graduate student
> Biology Departement
> Universit?? de Sherbrooke
> 2500 boul. de l'Universit??
> Sherbrooke, Qu??bec
> J1K 2R1 CANADA
> 
> T??l: 1-819-821-8000 poste 2059
> Fax: 1-819-821-8049
> emilie.berthiaume at USherbrooke.ca
> 	[[alternative HTML version deleted]]
> 

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From marc.kirchner at iwr.uni-heidelberg.de  Tue Nov  1 20:50:13 2005
From: marc.kirchner at iwr.uni-heidelberg.de (Marc Kirchner)
Date: Tue, 1 Nov 2005 19:50:13 +0000
Subject: [R] Error on read.table
In-Reply-To: <op.szkt2tbu0kw1f2@larsdell.uib.no>
References: <op.szkt2tbu0kw1f2@larsdell.uib.no>
Message-ID: <20051101195008.GA4694@iwr.uni-heidelberg.de>

Hello Lars,

from the read.table documentation:

row.names: a vector of row names.  This can be a vector giving the
          actual row names, or a single number giving the column of the
          table which contains the row names, or character string
          giving the name of the table column containing the row names.

Column numbering in R starts at 1 but you tell read.table to use the 0th column
- this is consitent with your error message.

The folowing works for me:

> x <- read.table("bla", header=T)
> x
  Subject    Sex Age Gruppe Type Target.CRESP Target.RESP Target.RT
1     101 female  22      7    t            0           0      1520
2     101 female  22      9    o            0           1      3020
  Prime.OnsetTime Target.OnsetTime Ordning Ident
1          155074           158497     101   145
2          161678           166005     102   160

-Marc

-- 
========================================================
Dipl. Inform. Med. Marc Kirchner
Interdisciplinary Centre for Scientific Computing (IWR)
Multidimensional Image Processing
INF 368
University of Heidelberg
D-69120 Heidelberg
Tel: ++49-6221-54 87 97
Fax: ++49-6221-54 88 50
marc.kirchner at iwr.uni-heidelberg.de

-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051101/7d8e40a2/attachment.bin

From marcelodamasceno at gmail.com  Tue Nov  1 22:07:31 2005
From: marcelodamasceno at gmail.com (Marcelo Damasceno)
Date: Tue, 1 Nov 2005 19:07:31 -0200
Subject: [R] Problems with dyn.load()
Message-ID: <a55593730511011307k6ba5b4b6i2ce996b32336a854@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051101/99bccd05/attachment.pl

From stratja at auburn.edu  Tue Nov  1 22:13:45 2005
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Tue, 01 Nov 2005 15:13:45 -0600
Subject: [R] index question
Message-ID: <s367863e.093@TMIA1.AUBURN.EDU>

Thanks for those on the list that answered my previous question.  I'm
just about where I need to be (looking at output).  

In the hier.part documentation there is a line env <- urbanwq[,2:8].  

This means use rows 2 through 8 in the data frame "urbanwq", right? 
What does the comma represent?  If one wasn't using column headers would
this be necessary?

Thanks,

Jeff



****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja



From roebuck at mdanderson.org  Tue Nov  1 22:14:33 2005
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Tue, 1 Nov 2005 15:14:33 -0600 (CST)
Subject: [R] Problems with dyn.load()
In-Reply-To: <a55593730511011307k6ba5b4b6i2ce996b32336a854@mail.gmail.com>
References: <a55593730511011307k6ba5b4b6i2ce996b32336a854@mail.gmail.com>
Message-ID: <Pine.OSF.4.58.0511011513160.110470@wotan.mdacc.tmc.edu>

On Tue, 1 Nov 2005, Marcelo Damasceno wrote:

> Is there some function that returns the directory where
> the libraries of the instaled package are? Because in
> Mandrake, R installs in /usr/lib/R/library/pack/libs, and in
> Debian it installs in /usr/local/lib/R/library/pack/libs.

?system.file

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From sundar.dorai-raj at pdf.com  Tue Nov  1 22:25:06 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 01 Nov 2005 15:25:06 -0600
Subject: [R] index question
In-Reply-To: <s367863e.093@TMIA1.AUBURN.EDU>
References: <s367863e.093@TMIA1.AUBURN.EDU>
Message-ID: <4367DD32.5010606@pdf.com>



Jeffrey Stratford wrote:
> Thanks for those on the list that answered my previous question.  I'm
> just about where I need to be (looking at output).  
> 
> In the hier.part documentation there is a line env <- urbanwq[,2:8].  
> 
> This means use rows 2 through 8 in the data frame "urbanwq", right? 
> What does the comma represent?  If one wasn't using column headers would
> this be necessary?
> 
> Thanks,
> 
> Jeff
> 
> 

Hi, Jeff,

Actually it means *columns* 2:8. See the documentation and examples in 
help("[").

--sundar

/war eagle



From DSmith2 at dhs.ca.gov  Tue Nov  1 22:43:42 2005
From: DSmith2 at dhs.ca.gov (Smith, Daniel (DHS-DEODC-EHIB))
Date: Tue, 1 Nov 2005 13:43:42 -0800
Subject: [R] R Graphs in Powerpoint
Message-ID: <3144C4F831806648B4FD0BA61219385901074E13@dhsexcmsg05.intra.dhs.ca.gov>

I've tried several methods in OS X, and here's what works best for me.  Save the R graphic as a PDF file.  Open it with Apple's "Preview" application, and save it as a PNG file.  The resulting .png file can be inserted into MS Word or PowerPoint, can be resized, and looks good on either OS X or Windows.  There are other programs available for translating the pdf file to png (like the shareware application Graphic Converter), but I've found that Preview produces the best results.    

Daniel Smith
Environmental Health Investigations Branch
California Dept of Health Services


-----Original Message-----
Date: Mon, 31 Oct 2005 15:14:06 -0800
From: Jarrett Byrnes <redbeard at arrr.net>
Subject: [R] R Graphs in Powerpoint
To: r-help at stat.math.ethz.ch
Message-ID: <1021e23a2ea288a066f22699f827cf73 at arrr.net>
Content-Type: text/plain; charset=US-ASCII; format=flowed

Hey, all.  Quick question.  I'm attempting to use some of the great 
graphs generated in R for an upcoming talk that I'm writing in 
Powerpoint.  Copying and pasting (I'm using OSX) yields graphs that 
look great in Powerpoint - until I resize them.  Then fonts, points, 
and lines all become quite pixelated and blurry.  Even if I size the 
window properly first, and then copy and paste in the graph, when I 
then view the slideshow, the graphs come out pixelated and blurry.

Is there any good solution to this, or is this some fundamental 
incompatibility that I can't get around?

-Jarrett



From evan at ausvet.com.au  Tue Nov  1 22:55:20 2005
From: evan at ausvet.com.au (Evan Sergeant)
Date: Wed, 2 Nov 2005 07:55:20 +1000 (EST)
Subject: [R] RODBC error
Message-ID: <200511012155.jA1LtK7v055916@server.ausvet.com.au>

Hi,

I hope that someone can help me with the following problem with RODBC 
connection to a MySQL database

I am running R version 2.2.0 on windows XP, and have the MySQL database 
registered in Windows ODBC.

I have set up a web interface on my personal ISS web server using PhP 
to accept input values and then call Rterm with an R script to access 
the database and return a summary of the analysed data.

This works perfectly if I run it from Rgui, or if I run Rterm from the 
dos prompt using the same command line arguments, but returns an RODBC 
error (below) when calling Rterm from the web interface

1: [RODBC] ERROR: state IM002, code 0, message [Microsoft][ODBC Driver 
Manager] Data source name not found and no default driver specified 

There appears to be some problem with recognising the database when 
called from the web page, even though it is not a problem at any other 
times.

Does anyone have any suggestions as to what I can do to overcome this 
error

Thanks for your help

cheers

Evan Sergeant

AusVet Animal Health Services
69 Turner Cr, 
Orange NSW 2800 
Australia

Phone +61 2 6362 1598
Fax      +61 2 6369 1473
Email:  evan at ausvet.com.au
Web site: http://www.ausvet.com.au
MLA's Q Fever Register: www.qfever.org

This transmission is for the intended addressee only and is 
confidential 
information. If you have received this transmission in error, please 
delete 
it and notify the sender. The contents of this email are the opinion of 
the 
writer only a



From bill.shipley at usherbrooke.ca  Tue Nov  1 23:00:26 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Tue, 1 Nov 2005 17:00:26 -0500
Subject: [R] coding nesting in data for nlme  example of Wafer data set.
Message-ID: <004501c5df2f$ab2ff130$9a1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051101/8d5f47f3/attachment.pl

From ayost at ODF.STATE.OR.US  Tue Nov  1 23:01:31 2005
From: ayost at ODF.STATE.OR.US (YOST Andrew)
Date: Tue, 1 Nov 2005 14:01:31 -0800
Subject: [R] Area under standard normal density
Message-ID: <69046F66C979654B9E93C39017FB46C3068423@salemmail.odf.state.or.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051101/3d72cdd9/attachment.pl

From mschwartz at mn.rr.com  Tue Nov  1 23:03:48 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 01 Nov 2005 16:03:48 -0600
Subject: [R] R Graphs in Powerpoint
In-Reply-To: <3144C4F831806648B4FD0BA61219385901074E13@dhsexcmsg05.intra.dhs.ca.gov>
References: <3144C4F831806648B4FD0BA61219385901074E13@dhsexcmsg05.intra.dhs.ca.gov>
Message-ID: <1130882628.19882.1.camel@localhost.localdomain>

One other option, just to throw it out there, though it involves a few
more steps.

1. Generate the R plots as EPS files.

2. Import them into Powerpoint onto the required slides. Resize and/or
place as required. Recent versions of Powerpoint will auto-generate a
bitmapped preview image upon import.

3. Print the full Powerpoint presentation to a PS file, using a PS
printer driver. This will result in high quality images.

4. Convert the PS file to PDF, using Ghostscript (ps2pdf) or similar.

5. Display the presentation using Acrobat Reader in full screen mode to
your audience.


This works well, as long as you are not using complex object/slide
transitions, animations and the like in Powerpoint and takes advantage
of the higher quality vector format of EPS graphics as opposed to the
bitmapped graphic formats.

HTH,

Marc Schwartz


On Tue, 2005-11-01 at 13:43 -0800, Smith, Daniel (DHS-DEODC-EHIB) wrote:
> I've tried several methods in OS X, and here's what works best for me.
> Save the R graphic as a PDF file.  Open it with Apple's "Preview"
> application, and save it as a PNG file.  The resulting .png file can
> be inserted into MS Word or PowerPoint, can be resized, and looks good
> on either OS X or Windows.  There are other programs available for
> translating the pdf file to png (like the shareware application
> Graphic Converter), but I've found that Preview produces the best
> results.    
> 
> Daniel Smith
> Environmental Health Investigations Branch
> California Dept of Health Services
> 
> 
> -----Original Message-----
> Date: Mon, 31 Oct 2005 15:14:06 -0800
> From: Jarrett Byrnes <redbeard at arrr.net>
> Subject: [R] R Graphs in Powerpoint
> To: r-help at stat.math.ethz.ch
> Message-ID: <1021e23a2ea288a066f22699f827cf73 at arrr.net>
> Content-Type: text/plain; charset=US-ASCII; format=flowed
> 
> Hey, all.  Quick question.  I'm attempting to use some of the great 
> graphs generated in R for an upcoming talk that I'm writing in 
> Powerpoint.  Copying and pasting (I'm using OSX) yields graphs that 
> look great in Powerpoint - until I resize them.  Then fonts, points, 
> and lines all become quite pixelated and blurry.  Even if I size the 
> window properly first, and then copy and paste in the graph, when I 
> then view the slideshow, the graphs come out pixelated and blurry.
> 
> Is there any good solution to this, or is this some fundamental 
> incompatibility that I can't get around?
> 
> -Jarrett



From stratja at auburn.edu  Tue Nov  1 23:12:03 2005
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Tue, 01 Nov 2005 16:12:03 -0600
Subject: [R] help with hier.part
Message-ID: <s36793ea.021@TMIA1.AUBURN.EDU>

R-users,

Attached is the file  (SR_use2.txt) I'd like to include and includes
column headers.  nat_est is the response variable and is the number of
species at a particular point.  The other variables are the explanatory
vars (vark, var2, var1, UK, U2, U1, GK, G2, G1, PK, P2, P1).  

Here is Walsh's sample code for hier.part:

data(urbanwq)
env <- urbanwq[,2,8]
hier.part(urbanwq$lec, env, fam="gaussian", gof="Rssqu")

The code I wrote is 

library(hier.part)
SRUSE<- read.table("F:\\GEORGIA\\species_richness\\SR_use2.txt", sep="
", header = TRUE, row.names = 1)
TEMP<- SRUSE[2:13]
hier.part(SRUSE$nat_est,TEMP, family="NegBin", gof="logLik", barplot=
TRUE)

So far this doesn't work and I'd really appreciate some help.

While I have your ears, what books would one make for the clueless?  

Many thanks,

Jeff

PS. nat_est is the estimated number of species (species richness). 
Around each of the sampling points I calculated the % of different types
of cover (pine, hardwoods, number of different covers) in three scales
around the sampling points (1000, 200, and 100 m).  What I'm hoping to
do with the analysis is to find the best scales and parameters that best
predicts species richness. 
.  

****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja
****************************************

From sundar.dorai-raj at pdf.com  Tue Nov  1 23:21:37 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 01 Nov 2005 16:21:37 -0600
Subject: [R] Area under standard normal density
In-Reply-To: <69046F66C979654B9E93C39017FB46C3068423@salemmail.odf.state.or.us>
References: <69046F66C979654B9E93C39017FB46C3068423@salemmail.odf.state.or.us>
Message-ID: <4367EA71.60608@pdf.com>



YOST Andrew wrote:
> What is the correct syntax for finding the area under the standard
> normal density for a particular value of z?
>  
> Thanks
> 

?qnorm



From sundar.dorai-raj at pdf.com  Tue Nov  1 23:30:28 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 01 Nov 2005 16:30:28 -0600
Subject: [R] Area under standard normal density
In-Reply-To: <4367EA71.60608@pdf.com>
References: <69046F66C979654B9E93C39017FB46C3068423@salemmail.odf.state.or.us>
	<4367EA71.60608@pdf.com>
Message-ID: <4367EC84.609@pdf.com>

Sorry, ?pnorm.

--sundar

P.S. Thanks to the anonymous correction.

Sundar Dorai-Raj wrote:
> 
> YOST Andrew wrote:
> 
>>What is the correct syntax for finding the area under the standard
>>normal density for a particular value of z?
>> 
>>Thanks
>>
> 
> 
> ?qnorm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From fuyaonv at hotmail.com  Tue Nov  1 23:30:36 2005
From: fuyaonv at hotmail.com (Lynette Sun)
Date: Tue, 01 Nov 2005 22:30:36 +0000
Subject: [R] multidimensional integration not over a multidimensional
	rectangle
In-Reply-To: <4367EA71.60608@pdf.com>
Message-ID: <BAY102-F21F8CD6F04A53A57F1CAE6B36F0@phx.gbl>

Hi,

anyone knows about any functions in R can get multidimensional integration 
not over a multidimensional rectangle (not adapt).

For example, I tried the following function f(x,n)=x^n/n!

phi.fun<-function(x,n)
{ if (n==1) {
	x
	}else{
		integrate(phi.fun, lower=0, upper=x, n=n-1)$value
		}
}

I could get f(4,2)=4^2/2!=8, but failed in f(4,3)=4^3/3! Thanks

Best,
Lynette



From dejongroel at gmail.com  Wed Nov  2 00:05:03 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Wed, 02 Nov 2005 00:05:03 +0100
Subject: [R] glmmpql and lmer keep failing
In-Reply-To: <43679F3A.10000@gmail.com>
References: <43679F3A.10000@gmail.com>
Message-ID: <4367F49F.30106@gmail.com>

Formulated more directly, are there plans for the implementation of the 
crude but more robust Marginal Quasi Likelihood estimation in for 
example LME?

Regards,
	Roel de Jong

Roel de Jong wrote:
> Hello,
> 
> I'm running a simulation study of a multilevel model with binary 
> response using the binomial probit link. It is a random intercept and 
> random slope model.  GLMMPQL and lmer fail to converge on a 
> *significant* portion of the *generated* datasets, while MlWin gives 
> reasonable estimates on those datasets. This is unacceptable. Does 
> anyone has similar experiences?
> 
> Regards,
>     Roel de Jong
>



From wcai11 at hotmail.com  Wed Nov  2 00:35:03 2005
From: wcai11 at hotmail.com (Weijie Cai)
Date: Tue, 01 Nov 2005 18:35:03 -0500
Subject: [R] two sample Cramer-von Mises test
Message-ID: <BAY103-F15B234FF43FA4DD376B1EAD36F0@phx.gbl>

Hello list,

Is there any function in some package can calculate two sample Cramer-von 
Mises test statistic? I searched around and only find one sample version 
cvm.test() in nortest package.

thanks,
WC

_________________________________________________________________
Don’t just search. Find. Check out the new MSN Search!



From HDoran at air.org  Wed Nov  2 00:39:56 2005
From: HDoran at air.org (Doran, Harold)
Date: Tue, 1 Nov 2005 18:39:56 -0500
Subject: [R] glmmpql and lmer keep failing
Message-ID: <F5ED48890E2ACB468D0F3A64989D335AC990C6@dc1ex3.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051101/e029b5a2/attachment.pl

From otter at otter-rsch.com  Wed Nov  2 01:04:07 2005
From: otter at otter-rsch.com (dave fournier)
Date: Tue, 01 Nov 2005 16:04:07 -0800
Subject: [R]  glmmpql and lmer keep failing
Message-ID: <43680277.3010205@otter-rsch.com>

Hi,

I believe we could extend our upcoming release of our
freely available AD Model Builder negative binomial mixed model

           http://otter-rsch.com/admbre/admbre.html

packaqge for R to include your model.  Writing the model is
simple, it is the interface with R that is a bit more difficult.
If you contact me privately I will take a look at it.

   Cheers,

   Dave

-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3s3
Canada
http://otter-rsch.com


--



From dejongroel at gmail.com  Wed Nov  2 01:51:27 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Wed, 02 Nov 2005 01:51:27 +0100
Subject: [R] glmmpql and lmer keep failing
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335AC990C6@dc1ex3.air.org>
References: <F5ED48890E2ACB468D0F3A64989D335AC990C6@dc1ex3.air.org>
Message-ID: <43680D8F.9010506@gmail.com>

The word unacceptable was maybe poorly chosen, but I meant it was 
unacceptable for my purposes that a lot of samples needed to be 
discarded. Further, I don't need to give code, data, or other reasons 
why the model may fail because it is well known that PQL can suffer from 
convergence problems, and this issue is already well documented in the R 
help list and articles I skimmed through before posting.  So all I 
wanted to know if how someone else with similar experiences tackled this 
problem, and if there are plans for the implementation of MQL in lme or 
another package.

As of my own contribution, I will soon release a Gibbs sampler for 
multilevel models under the binomial probit link that also can handle 
random slopes.

But I'm sorry I bothered you, guardian of the shining multilevel star.

Roel.





Doran, Harold wrote:
> Sorry to be blunt, but to make the statement that it is unacceptable 
> without providing any reason why the model may fail to converge seems a 
> little presumptious. Your statement really bothers me, especially 
> knowing how hard the developer works on keeping this function at a 
> premier level. Ask anyone working with mixed models and they will all 
> agree that Doug Bates and this function are shining stars in the world 
> of computational statistics.
> 
> R is open source, and therefore you are certainly welcome and encouraged 
> to add to its functionality if you wish. Basically, one may argue that 
> you are in a "put up, or shut up" position. Feel free to write a nice 
> piece of code based on MQL that we can use (and critique).
> 
> But even more importantly, in order to provide any help to you at all, 
> you should provide sample data and examples of your code. We may find 
> that the problem is the user and not the function.
> 
> I generally disagree with rudeness on this list, and agree that my tone 
> is not very agreeable, but please read the posting guide and follow 
> basic protocol by describing your problem, create sample data, 
> illustrate your code, and then ask for help.
> 
> Harold
> 
> 
> 
> -----Original Message-----
> From:   r-help-bounces at stat.math.ethz.ch on behalf of Roel de Jong
> Sent:   Tue 11/1/2005 6:05 PM
> To:     r-help
> Cc:    
> Subject:        Re: [R] glmmpql and lmer keep failing
> 
> Formulated more directly, are there plans for the implementation of the
> crude but more robust Marginal Quasi Likelihood estimation in for
> example LME?
> 
> Regards,
>         Roel de Jong
> 
> Roel de Jong wrote:
>  > Hello,
>  >
>  > I'm running a simulation study of a multilevel model with binary
>  > response using the binomial probit link. It is a random intercept and
>  > random slope model.  GLMMPQL and lmer fail to converge on a
>  > *significant* portion of the *generated* datasets, while MlWin gives
>  > reasonable estimates on those datasets. This is unacceptable. Does
>  > anyone has similar experiences?
>  >
>  > Regards,
>  >     Roel de Jong
>  >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From klebyn at yahoo.com.br  Wed Nov  2 02:24:53 2005
From: klebyn at yahoo.com.br (klebyn)
Date: Tue, 01 Nov 2005 23:24:53 -0200
Subject: [R] help with the coordinates of the ECDF object
Message-ID: <43681565.4090708@yahoo.com.br>



Hi all R users


I would like to know how acess the coordinates
of the ECDF object.

I look for the example,
in this part:
######################
     print(ls.Fn12 <- ls(env= environment(Fn12)))
######################

but I do not know to extract
the Y coordinate and put it in other variable.

My objective is to make a plot
and identify the points with labels.


############# Example by  ?ecdf

 y <- round(rnorm(12),1); y[3] <- y[1]
     Fn12 <- ecdf(y)
     Fn12
     print(knots(Fn12), dig=2)
     12*Fn12(knots(Fn12)) ## ~= 1:12  if there were no ties

     summary(Fn12)
     summary.stepfun(Fn12)

     print(ls.Fn12 <- ls(env= environment(Fn12)))
     ##[1] "f"  "method"  "n"  "x"  "y"  "yleft"  "yright"

############# Example by  ?ecdf


My objetive seems to this:

plot(Fn12)
identify( x = knots(Fn12),  y = ??????????, labels="my string set")

or text...


thanks in advanced

klebyn



From nlemeur at fhcrc.org  Wed Nov  2 02:26:20 2005
From: nlemeur at fhcrc.org (Nolwenn LeMeur)
Date: Tue, 1 Nov 2005 17:26:20 -0800 (PST)
Subject: [R] help with the coordinates of the ECDF object
In-Reply-To: <43681565.4090708@yahoo.com.br>
References: <43681565.4090708@yahoo.com.br>
Message-ID: <Pine.LNX.4.61.0511011725080.29614@vole.fhcrc.org>

Hi,

try environment(Fn12)$y
    environment(Fn12)$x

Nolwenn

**************************************
Nolwenn Le Meur, PhD
Fred Hutchinson Cancer Research Center
Computational Biology
1100 Fairview Ave. N., M2-B876
P.O. Box 19024
Seattle, WA 98109-1024

On Tue, 1 Nov 2005, klebyn wrote:

> 
> 
> Hi all R users
> 
> 
> I would like to know how acess the coordinates
> of the ECDF object.
> 
> I look for the example,
> in this part:
> ######################
>      print(ls.Fn12 <- ls(env= environment(Fn12)))
> ######################
> 
> but I do not know to extract
> the Y coordinate and put it in other variable.
> 
> My objective is to make a plot
> and identify the points with labels.
> 
> 
> ############# Example by  ?ecdf
> 
>  y <- round(rnorm(12),1); y[3] <- y[1]
>      Fn12 <- ecdf(y)
>      Fn12
>      print(knots(Fn12), dig=2)
>      12*Fn12(knots(Fn12)) ## ~= 1:12  if there were no ties
> 
>      summary(Fn12)
>      summary.stepfun(Fn12)
> 
>      print(ls.Fn12 <- ls(env= environment(Fn12)))
>      ##[1] "f"  "method"  "n"  "x"  "y"  "yleft"  "yright"
> 
> ############# Example by  ?ecdf
> 
> 
> My objetive seems to this:
> 
> plot(Fn12)
> identify( x = knots(Fn12),  y = ??????????, labels="my string set")
> 
> or text...
> 
> 
> thanks in advanced
> 
> klebyn
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Murraypu at aimnsw.com.au  Wed Nov  2 02:34:36 2005
From: Murraypu at aimnsw.com.au (Murray Pung)
Date: Wed, 2 Nov 2005 12:34:36 +1100
Subject: [R] help with the coordinates of the ECDF object
Message-ID: <3028F4C4647C9043B870276E28C69FD6013437ED@syd05.aimnsw.com.au>

If you wish to apply labels to identified points: I am sure there is a better way, but what I have done is use

locator(n)  #n is how many points you wish to identify. Then click on the points. The coordinates of each point are returned.

Then I use

identify(x, y, labels = My Label)

...for each point identified. It would be great if you could click on the points and have them identified & labelled, but I'm not sure how to do it.

-----Original Message-----
From: klebyn [mailto:klebyn at yahoo.com.br]
Sent: Wednesday, 2 November 2005 12:25 PM
To: r-help at stat.math.ethz.ch
Subject: [R] help with the coordinates of the ECDF object




Hi all R users


I would like to know how acess the coordinates
of the ECDF object.

I look for the example,
in this part:
######################
     print(ls.Fn12 <- ls(env= environment(Fn12)))
######################

but I do not know to extract
the Y coordinate and put it in other variable.

My objective is to make a plot
and identify the points with labels.


############# Example by  ?ecdf

 y <- round(rnorm(12),1); y[3] <- y[1]
     Fn12 <- ecdf(y)
     Fn12
     print(knots(Fn12), dig=2)
     12*Fn12(knots(Fn12)) ## ~= 1:12  if there were no ties

     summary(Fn12)
     summary.stepfun(Fn12)

     print(ls.Fn12 <- ls(env= environment(Fn12)))
     ##[1] "f"  "method"  "n"  "x"  "y"  "yleft"  "yright"

############# Example by  ?ecdf


My objetive seems to this:

plot(Fn12)
identify( x = knots(Fn12),  y = ??????????, labels="my string set")

or text...


thanks in advanced

klebyn

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From alexfang at stat.rutgers.edu  Wed Nov  2 02:44:34 2005
From: alexfang at stat.rutgers.edu (Jiangang Fang (Alex))
Date: Tue, 01 Nov 2005 20:44:34 -0500
Subject: [R] Trellis Plot Group Variable Font size
Message-ID: <43681A02.2030407@stat.rutgers.edu>

I plot a trellis plot in R using the lattice package.  The size of the 
group variable is pretty big (over 100 groups).  I used to do it in 
Splus and the pic there is pretty good.  Now I transfer to R but the 
plot is really not accetable. The group variable font in the R produced 
trellis is so huge that it takes over the major area of the plot.  Any 
one knows how to make the font size smaller?  thanks very much.  best, Alex



From 042045003 at fudan.edu.cn  Wed Nov  2 02:53:01 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Wed, 02 Nov 2005 09:53:01 +0800
Subject: [R] index question
Message-ID: <0IPB00EPG0R3VI@mail.fudan.edu.cn>

	

======= 2005-11-02 05:13:45 ÄúÔÚÀ´ÐÅÖÐÐ´µÀ£º=======

>Thanks for those on the list that answered my previous question.  I'm
>just about where I need to be (looking at output).  
>
>In the hier.part documentation there is a line env <- urbanwq[,2:8].  
>
>This means use rows 2 through 8 in the data frame "urbanwq", right? 
his means use columns 2 through 8 in the data frame "urbanwq"

>What does the comma represent? 
It means the first argument to function "[" is left blank,so makes R knows 2:8 is the second argument to "[".

> If one wasn't using column headers would
>this be necessary?
It still is necessary.

>
>Thanks,
>
>Jeff
>
>
>
>****************************************
>Jeffrey A. Stratford, Ph.D.
>Postdoctoral Associate
>331 Funchess Hall
>Department of Biological Sciences
>Auburn University
>Auburn, AL 36849
>334-329-9198
>FAX 334-844-9234
>http://www.auburn.edu/~stratja
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-11-02

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From sokol.haxhinasto at gmail.com  Wed Nov  2 03:49:14 2005
From: sokol.haxhinasto at gmail.com (Sokol Haxhinasto)
Date: Tue, 01 Nov 2005 21:49:14 -0500
Subject: [R] Tcl/tk
Message-ID: <BF8D935A.4DBD%sokol.haxhinasto@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051101/9c1f92e8/attachment.pl

From ForresterG at landcareresearch.co.nz  Wed Nov  2 05:29:28 2005
From: ForresterG at landcareresearch.co.nz (Guy Forrester)
Date: Wed, 02 Nov 2005 17:29:28 +1300
Subject: [R] NLME
Message-ID: <s368f781.094@smtp.landcareresearch.co.nz>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051102/16aee981/attachment.pl

From petr.pikal at precheza.cz  Wed Nov  2 09:44:22 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 02 Nov 2005 09:44:22 +0100
Subject: [R] help with hier.part
In-Reply-To: <s36793ea.021@TMIA1.AUBURN.EDU>
Message-ID: <43688A76.2977.69FFCD@localhost>

Hi

although I do not know anything about hier.part package I try few 
comments

- see posting guide as it suggest to try to present a toy example 
which shows your problem

- are there some error messages or the result is not as you expect?

- what is TEMP - seems to me that you need to define it before 
hier.part() function and you did it but I am not sure if it contained 
what it should contain

- usually when something "does not work" means I made a mistake 
and I have to bore deeper to to syntax and man pages of a 
function. I swear that reading all posibble sources of information is 
worth the time if you really want to use R.

HTH
Petr


On 1 Nov 2005 at 16:12, Jeffrey Stratford wrote:

Date sent:      	Tue, 01 Nov 2005 16:12:03 -0600
From:           	"Jeffrey Stratford" <stratja at auburn.edu>
To:             	<r-help at stat.math.ethz.ch>
Subject:        	[R] help with hier.part

> R-users,
> 
> Attached is the file  (SR_use2.txt) I'd like to include and includes
> column headers.  nat_est is the response variable and is the number of
> species at a particular point.  The other variables are the
> explanatory vars (vark, var2, var1, UK, U2, U1, GK, G2, G1, PK, P2,
> P1).  
> 
> Here is Walsh's sample code for hier.part:
> 
> data(urbanwq)
> env <- urbanwq[,2,8]
> hier.part(urbanwq$lec, env, fam="gaussian", gof="Rssqu")
> 
> The code I wrote is 
> 
> library(hier.part)
> SRUSE<- read.table("F:\\GEORGIA\\species_richness\\SR_use2.txt", sep="
> ", header = TRUE, row.names = 1) TEMP<- SRUSE[2:13]
> hier.part(SRUSE$nat_est,TEMP, family="NegBin", gof="logLik", barplot=
> TRUE)
> 
> So far this doesn't work and I'd really appreciate some help.
> 
> While I have your ears, what books would one make for the clueless?  
> 
> Many thanks,
> 
> Jeff
> 
> PS. nat_est is the estimated number of species (species richness).
> Around each of the sampling points I calculated the % of different
> types of cover (pine, hardwoods, number of different covers) in three
> scales around the sampling points (1000, 200, and 100 m).  What I'm
> hoping to do with the analysis is to find the best scales and
> parameters that best predicts species richness. .  
> 
> ****************************************
> Jeffrey A. Stratford, Ph.D.
> Postdoctoral Associate
> 331 Funchess Hall
> Department of Biological Sciences
> Auburn University
> Auburn, AL 36849
> 334-329-9198
> FAX 334-844-9234
> http://www.auburn.edu/~stratja
> ****************************************
> 

Petr Pikal
petr.pikal at precheza.cz



From dieter.menne at menne-biomed.de  Wed Nov  2 11:15:34 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 2 Nov 2005 10:15:34 +0000 (UTC)
Subject: [R] NLME
References: <s368f781.094@smtp.landcareresearch.co.nz>
Message-ID: <loom.20051102T111049-798@post.gmane.org>

Guy Forrester <ForresterG <at> landcareresearch.co.nz> writes:

> R : Copyright 2005, The R Foundation for Statistical Computing
> Version 2.1.1  (2005-06-20), ISBN 3-900051-07-0

>  Jose Pinheiro, Douglas Bates, Saikat DebRoy and Deepayan Sarkar (2005). 
...
> I am trying to run the scripts from the Mixed Models book and am running into 
some difficulty - could any one
> tell me why this doesn't work (from pages 369 - 370) 
> 
>  
> CO2  #FINE
> plot(CO2,outer= ~Treatment*Type,layout=c(4,1)) #FINE

These examples were written for S; until version 2.1.1 (the version you are 
using) the optimizing engine for R was different from S, and some of the 
examples did not work. Consult the nlme/scripts, were the non-converging parts 
are commented out.

>From 2.2.0, the R/s engines should be similar, so if you update the examples 
from the book should run. My mileage varied, however:

After installing R 2.2.0 (Windows), I immediately tried to run the ping-pong 
examples in ch06.R and ch08.R, because I had encountered quite few of these in 
my own stuff.

fm1Quin.nlme works now.

Strangely, fm2Pheno.nlme still plays merry-go-round. However, when I changed 
pnlsTol to 0.1, it worked. The results were close to those published, but not 
exactly the same. I did not try this in previous versions, maybe it would have 
worked there too with this modification.

Dieter



From ajayshah at lubyanka.local  Wed Nov  2 12:08:54 2005
From: ajayshah at lubyanka.local (Ajay Shah)
Date: Wed,  2 Nov 2005 16:38:54 +0530 (IST)
Subject: [R] Bug report on get.hist.quote
Message-ID: <20051102110854.AFD521DC5EF@lubyanka.local>


> get.hist.quote(instrument="INR/USD", provider="oanda", start="2005-10-20")
trying URL 'http://www.oanda.com/convert/fxhistory?lang=en&date1=10%2F20%2F2005&date=11%2F01%2F2005&date_fmt=us&exch=INR&exch2=&expr=USD&expr2=&margin_fixed=0&&SUBMIT=Get+Table&format=ASCII&redirected=1'
Content type 'text/html' length unknown
opened URL
.......... ...
downloaded 13Kb

2005-10-20 2005-10-21 2005-10-22 2005-10-23 2005-10-24 2005-10-25 2005-10-26 
   0.02220    0.02218    0.02224    0.02224    0.02224    0.02219    0.02226 
2005-10-27 2005-10-28 2005-10-29 2005-10-30 2005-10-31 2005-11-01 
   0.02224    0.02225    0.02224    0.02224    0.02224    0.02221 


The answer is wrong. What is shown here is USD/INR, not INR/USD.



From ktiwari at bgc-jena.mpg.de  Wed Nov  2 12:17:05 2005
From: ktiwari at bgc-jena.mpg.de (Yogesh K. Tiwari)
Date: Wed, 02 Nov 2005 12:17:05 +0100
Subject: [R] how to overlay many plot windows in a sigle frame
Message-ID: <4368A031.5080507@bgc-jena.mpg.de>

Hello R Users,

I want to plot many windows in a single frame.

For example, suppose I have to plot the
vertical profile of the aircraft co2
measurement in different months in a year,
and I want to plot these different months in
separate windows but the final 12 plots I
want to keep in single frame. So how I can do
this in R.

There would be any overlay command after
ploting the first winodw so I can send the
second plot window in the same frame, side by
side with the old window.

How I can do this ?

Many thanks for help,

Regards,
Yogesh
-- 

===========================================
Yogesh Tiwari,
Max-Planck Institute for Biogeochemistry,
Hans-Knoell Strasse 10,
D-07745 Jena,
Germany

Office   : 0049 3641 576 376
Home     : 0049 3641 223 163
Fax      : 0049 3641 577 300
Cell     : 0049 1520 4591 008
e-mail   : yogesh.tiwari at bgc-jena.mpg.de



From Roger.Bivand at nhh.no  Wed Nov  2 12:26:25 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 2 Nov 2005 12:26:25 +0100 (CET)
Subject: [R] how to overlay many plot windows in a sigle frame
In-Reply-To: <4368A031.5080507@bgc-jena.mpg.de>
Message-ID: <Pine.LNX.4.44.0511021224300.19394-100000@reclus.nhh.no>

On Wed, 2 Nov 2005, Yogesh K. Tiwari wrote:

> Hello R Users,
> 
> I want to plot many windows in a single frame.

?par

opar <- par(mfrow=c(3,4))
for (i in 1:12) plot(runif(i*10))
par(opar)



> 
> For example, suppose I have to plot the
> vertical profile of the aircraft co2
> measurement in different months in a year,
> and I want to plot these different months in
> separate windows but the final 12 plots I
> want to keep in single frame. So how I can do
> this in R.
> 
> There would be any overlay command after
> ploting the first winodw so I can send the
> second plot window in the same frame, side by
> side with the old window.
> 
> How I can do this ?
> 
> Many thanks for help,
> 
> Regards,
> Yogesh
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ligges at statistik.uni-dortmund.de  Wed Nov  2 12:53:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 02 Nov 2005 12:53:27 +0100
Subject: [R] how to overlay many plot windows in a sigle frame
In-Reply-To: <Pine.LNX.4.44.0511021224300.19394-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0511021224300.19394-100000@reclus.nhh.no>
Message-ID: <4368A8B7.2060102@statistik.uni-dortmund.de>

Roger Bivand wrote:

> On Wed, 2 Nov 2005, Yogesh K. Tiwari wrote:
> 
> 
>>Hello R Users,
>>
>>I want to plot many windows in a single frame.
> 
> 
> ?par
> 
> opar <- par(mfrow=c(3,4))
> for (i in 1:12) plot(runif(i*10))
> par(opar)


Alternatively, maybe some lattice graphics for the perticular case 
mentioned below where the arrangement takes place automatically?

Uwe Ligges


> 
> 
> 
>>For example, suppose I have to plot the
>>vertical profile of the aircraft co2
>>measurement in different months in a year,
>>and I want to plot these different months in
>>separate windows but the final 12 plots I
>>want to keep in single frame. So how I can do
>>this in R.
>>
>>There would be any overlay command after
>>ploting the first winodw so I can send the
>>second plot window in the same frame, side by
>>side with the old window.
>>
>>How I can do this ?
>>
>>Many thanks for help,
>>
>>Regards,
>>Yogesh
>>
> 
>



From illyese at freemail.hu  Wed Nov  2 13:10:38 2005
From: illyese at freemail.hu (Illyes Eszter)
Date: Wed, 2 Nov 2005 13:10:38 +0100 (CET)
Subject: [R] (no subject)
Message-ID: <freemail.20051002131038.72523@fm05.freemail.hu>

Dear Colleagues,

It is Eszter Illy??s from Hungary. I just have started to use R yesterday, 
so I am a real beginner (but I could run some PCOA and managed to 
transpose a dataframe). 

No I have a problem that I would like to standardize my datafile with 
the row totals and than make an arcsin squareroot transformation. 
How can I do that? In which package shall I look for and for what 
option? 

Thank you very much, have a nice day

Eszter

_______________________________________________________________________
K??ptipp! J??tssz a T-Online k??pkeres??j??vel! Ha el??g ??gyes ??s gyors vagy,
felker??lhetsz a dics??s??gt??bl??ra...  www.t-online.hu



From ligges at statistik.uni-dortmund.de  Wed Nov  2 13:17:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 02 Nov 2005 13:17:39 +0100
Subject: [R] forgot to read the posting guide, was: Re:  (no subject)
In-Reply-To: <freemail.20051002131038.72523@fm05.freemail.hu>
References: <freemail.20051002131038.72523@fm05.freemail.hu>
Message-ID: <4368AE63.8070905@statistik.uni-dortmund.de>

Illyes Eszter wrote:
> Dear Colleagues,
> 
> It is Eszter Illy??s from Hungary. I just have started to use R yesterday, 
> so I am a real beginner (but I could run some PCOA and managed to 
> transpose a dataframe). 
> 
> No I have a problem that I would like to standardize my datafile with 
> the row totals and than make an arcsin squareroot transformation. 
> How can I do that? In which package shall I look for and for what 
> option? 
> 
> Thank you very much, have a nice day
> 
> Eszter
> 
> _______________________________________________________________________
> K??ptipp! J??tssz a T-Online k??pkeres??j??vel! Ha el??g ??gyes ??s gyors vagy,
> felker??lhetsz a dics??s??gt??bl??ra...  www.t-online.hu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

If you have started yesterday you may have found R useful, but you for 
sure forgot to read the manuals and the posting guide (cited at the end 
of each R-help message, see above) before asking your questions. One 
point is, e.g., to use a sensible subject line in your message.
After havng read the relevant documents, you might want to ask again...

Best,
Uwe Ligges



From gavin.simpson at ucl.ac.uk  Wed Nov  2 13:27:12 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 02 Nov 2005 12:27:12 +0000
Subject: [R] (no subject)
In-Reply-To: <freemail.20051002131038.72523@fm05.freemail.hu>
References: <freemail.20051002131038.72523@fm05.freemail.hu>
Message-ID: <1130934432.22231.52.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2005-11-02 at 13:10 +0100, Illyes Eszter wrote:
> Dear Colleagues,
> 
> It is Eszter IllyÃ©s from Hungary. I just have started to use R yesterday, 
> so I am a real beginner (but I could run some PCOA and managed to 
> transpose a dataframe). 
> 
> No I have a problem that I would like to standardize my datafile with 
> the row totals and than make an arcsin squareroot transformation. 
> How can I do that? In which package shall I look for and for what 
> option? 

For standardising by row total look at decostand() with method = "total"
in package vegan.

(360 / (2 * pi)) * asin(sqrt(x)) where x is vector of data-points will
apply that transformation. If you want to apply it to all columns in
your dataframe then look at ?sapply and modify the above accordingly.

HTH

G

> Thank you very much, have a nice day
> 
> Eszter
> 
> _______________________________________________________________________
> KÃ©ptipp! JÃ¡tssz a T-Online kÃ©pkeresÅ‘jÃ©vel! Ha elÃ©g Ã¼gyes Ã©s gyors vagy,
> felkerÃ¼lhetsz a dicsÅ‘sÃ©gtÃ¡blÃ¡ra...  www.t-online.hu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From mmiller at nassp.uct.ac.za  Wed Nov  2 13:32:52 2005
From: mmiller at nassp.uct.ac.za (Mark Miller)
Date: Wed, 2 Nov 2005 14:32:52 +0200
Subject: [R] Distribution fitting problem
Message-ID: <200511021432.52723.mmiller@nassp.uct.ac.za>

I am using the MASS library function 

fitdistr(x, dpois, list(lambda=2))

but I get 

Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
        Function cannot be evaluated at initial parameters
In addition: There were 50 or more warnings (use warnings() to see the first 
50)

and all the first 50 warnings say 

1:  non-integer x = 1.452222
etc

Can anyone tell me what I am doing wrong. p.s. the data was read in from 
a .csv file that I wrote using octave



From Pascal.Niklaus at unibas.ch  Wed Nov  2 13:59:55 2005
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Wed, 2 Nov 2005 13:59:55 +0100
Subject: [R] aggregating data and missing values
Message-ID: <200511021359.55327.Pascal.Niklaus@unibas.ch>

Hi all,

I would like to aggregate a large data file that is defined by a number of 
factors and associated values. The point is that not all factor level 
combinations are present in the data file  -- these "missing" values are in 
fact to be treated as zeroes.

Is there a straightforward way to 
a) either expand the existing data set so that the missing factor combinations 
can be added, or 
b) an "aggregate" function that generates a row of data for all given factor 
combinations.

Here is an example:

a) "complete" data set:

> example <- 
data.frame(f1=factor(rep(LETTERS[1:3],each=4)),f2=factor(letters[1:2]),d=1:12)
> aggregate(cbind(d=example$d),by=list(f1=example$f1,f2=example$f2),sum)
  f1 f2  d
1  A  a  4
2  B  a 12
3  C  a 20
4  A  b  6
5  B  b 14
6  C  b 22

b) data set with "missing combinations":

> example2 <- example[c(-10,-12),]
> aggregate(cbind(d=example2$d),by=list(f1=example2$f1,f2=example2$f2),sum)
  f1 f2  d
1  A  a  4
2  B  a 12
3  C  a 20
4  A  b  6
5  B  b 14

Here, I would like to have the missing row width f1=C, f2=b, d=NA.

The solution I have come up with is very slow and cumbersome (because there a 
re many factors) and I am convinced that there is a better way to do this (I 
create a new data frame with all factor combinations present and then copy 
the results from the call to aggregate line by line into the new data frame).

Thanks for your help

Pascal



From kynn at panix.com  Wed Nov  2 14:24:14 2005
From: kynn at panix.com (kynn@panix.com)
Date: Wed, 2 Nov 2005 08:24:14 -0500 (EST)
Subject: [R] Anything like associative arrays in R?
Message-ID: <200511021324.jA2DOEe15697@panix3.panix.com>





Let me preface my question by stressing that I am much less interested
in the answer than in learning a way I could have *found the answer
myself*.  (As helpful as the participants in this list are, I have far
too many R-related questions to resolve by posting here, and as I've
written before, in my experience the R documentation has not been very
helpful, but I remain hopeful that I may have managed to miss some
crucial document.)

The task I want to accomplish is very simple: to define and
sequentially initialize M x N variables *programmatically*, according
to two different categories, containing N and M values, respectively.

In languages with associative arrays, the typical way to do this is to
define a 2-d associative array; e.g. in Perl one could do

  for $i ( 'foo', 'bar', 'baz' ) {

    for $j ( 'eenie', 'meenie', 'minie', 'moe' ) {

      $table{ $i }{ $j } = read_table( "path/to/data/${i}_${j}.dat" );
    }

  }

How does one do this in R?  In particular, what's the equivalent of
the above in R?

Most importantly, how could I have found out this answer from the
R docs?

Many thanks in advance,

kj



From dieter.menne at menne-biomed.de  Wed Nov  2 14:23:13 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 2 Nov 2005 13:23:13 +0000 (UTC)
Subject: [R] Distribution fitting problem
References: <200511021432.52723.mmiller@nassp.uct.ac.za>
Message-ID: <loom.20051102T142205-779@post.gmane.org>

Mark Miller <mmiller <at> nassp.uct.ac.za> writes:

> 
> I am using the MASS library function 
> 
> fitdistr(x, dpois, list(lambda=2))
> 
> but I get 
> 
> Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
>         Function cannot be evaluated at initial parameters
> In addition: There were 50 or more warnings (use warnings() to see the first 
> 50)
> 

The docs say:

For the following named distributions, reasonable starting values will be 
computed if start is omitted or only partially specified: cauchy, gamma, 
logistic, negative binomial (parametrized by mu and size), t and weibull. 

dpois is not among them, so you probably have to provide reasonable starting 
values for the parameters.

Dieter



From jholtman at gmail.com  Wed Nov  2 14:30:23 2005
From: jholtman at gmail.com (jim holtman)
Date: Wed, 2 Nov 2005 08:30:23 -0500
Subject: [R] Anything like associative arrays in R?
In-Reply-To: <200511021324.jA2DOEe15697@panix3.panix.com>
References: <200511021324.jA2DOEe15697@panix3.panix.com>
Message-ID: <644e1f320511020530h621b0984u664cbb62a0d4ce2b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051102/ce5dffdb/attachment.pl

From mmiller at nassp.uct.ac.za  Wed Nov  2 14:31:19 2005
From: mmiller at nassp.uct.ac.za (Mark Miller)
Date: Wed, 2 Nov 2005 15:31:19 +0200
Subject: [R] Distribution fitting problem
In-Reply-To: <4368B4B9.3060405@pdf.com>
References: <200511021432.52723.mmiller@nassp.uct.ac.za>
	<4368B4B9.3060405@pdf.com>
Message-ID: <200511021531.20085.mmiller@nassp.uct.ac.za>

Can you advise another distribution, was thinking of exponential, but was 
advised poisson since independent, forgot about requiring integers


On Wednesday 02 November 2005 14:44, you wrote:
> Mark Miller wrote:
> > I am using the MASS library function
> >
> > fitdistr(x, dpois, list(lambda=2))
> >
> > but I get
> >
> > Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
> >         Function cannot be evaluated at initial parameters
> > In addition: There were 50 or more warnings (use warnings() to see the
> > first 50)
> >
> > and all the first 50 warnings say
> >
> > 1:  non-integer x = 1.452222
> > etc
> >
> > Can anyone tell me what I am doing wrong. p.s. the data was read in from
> > a .csv file that I wrote using octave
>
> Hi, Mark,
>
> If you think the data are poisson, the observations should be integers.
>
> --sundar



From ggrothendieck at gmail.com  Wed Nov  2 14:34:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 2 Nov 2005 08:34:01 -0500
Subject: [R] aggregating data and missing values
In-Reply-To: <200511021359.55327.Pascal.Niklaus@unibas.ch>
References: <200511021359.55327.Pascal.Niklaus@unibas.ch>
Message-ID: <971536df0511020534q372a90b5n9c5941a2b3444fdf@mail.gmail.com>

On 11/2/05, Pascal A. Niklaus <Pascal.Niklaus at unibas.ch> wrote:
> Hi all,
>
> I would like to aggregate a large data file that is defined by a number of
> factors and associated values. The point is that not all factor level
> combinations are present in the data file  -- these "missing" values are in
> fact to be treated as zeroes.
>
> Is there a straightforward way to
> a) either expand the existing data set so that the missing factor combinations
> can be added, or
> b) an "aggregate" function that generates a row of data for all given factor
> combinations.
>
> Here is an example:
>
> a) "complete" data set:
>
> > example <-
> data.frame(f1=factor(rep(LETTERS[1:3],each=4)),f2=factor(letters[1:2]),d=1:12)
> > aggregate(cbind(d=example$d),by=list(f1=example$f1,f2=example$f2),sum)
>  f1 f2  d
> 1  A  a  4
> 2  B  a 12
> 3  C  a 20
> 4  A  b  6
> 5  B  b 14
> 6  C  b 22
>
> b) data set with "missing combinations":
>
> > example2 <- example[c(-10,-12),]
> > aggregate(cbind(d=example2$d),by=list(f1=example2$f1,f2=example2$f2),sum)
>  f1 f2  d
> 1  A  a  4
> 2  B  a 12
> 3  C  a 20
> 4  A  b  6
> 5  B  b 14
>
> Here, I would like to have the missing row width f1=C, f2=b, d=NA.

Suppose the result of the aggregate just shown is example2.ag .  Then

merge(example2.ag, expand.grid(f1 = LETTERS[1:3], f2 = letters[1:2]),
all = TRUE)



From azzalini at stat.unipd.it  Wed Nov  2 14:35:37 2005
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Wed, 2 Nov 2005 14:35:37 +0100
Subject: [R] Distribution fitting problem
In-Reply-To: <200511021432.52723.mmiller@nassp.uct.ac.za>
References: <200511021432.52723.mmiller@nassp.uct.ac.za>
Message-ID: <20051102143537.7831bea0.azzalini@stat.unipd.it>

On Wed, 2 Nov 2005 14:32:52 +0200, Mark Miller wrote:

MM> I am using the MASS library function 
MM> 
MM> fitdistr(x, dpois, list(lambda=2))
MM> 
MM> but I get 
MM> 
MM> Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
MM>         Function cannot be evaluated at initial parameters
MM> In addition: There were 50 or more warnings (use warnings() to see
MM> the first  50)
MM> 
MM> and all the first 50 warnings say 
MM> 
MM> 1:  non-integer x = 1.452222
MM> etc
MM> 

are the data integers (as implicit in the assumption of Poisson dist'n)?
the above message seems to say that they are not 

Adelchi Azzalini

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit?? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/



From sdavis2 at mail.nih.gov  Wed Nov  2 14:36:15 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 02 Nov 2005 08:36:15 -0500
Subject: [R] Anything like associative arrays in R?
In-Reply-To: <200511021324.jA2DOEe15697@panix3.panix.com>
Message-ID: <BF8E2AFF.122AD%sdavis2@mail.nih.gov>

On 11/2/05 8:24 AM, "kynn at panix.com" <kynn at panix.com> wrote:

> 
> 
> 
> 
> Let me preface my question by stressing that I am much less interested
> in the answer than in learning a way I could have *found the answer
> myself*.  (As helpful as the participants in this list are, I have far
> too many R-related questions to resolve by posting here, and as I've
> written before, in my experience the R documentation has not been very
> helpful, but I remain hopeful that I may have managed to miss some
> crucial document.)
> 
> The task I want to accomplish is very simple: to define and
> sequentially initialize M x N variables *programmatically*, according
> to two different categories, containing N and M values, respectively.
> 
> In languages with associative arrays, the typical way to do this is to
> define a 2-d associative array; e.g. in Perl one could do
> 
> for $i ( 'foo', 'bar', 'baz' ) {
> 
>   for $j ( 'eenie', 'meenie', 'minie', 'moe' ) {
> 
>     $table{ $i }{ $j } = read_table( "path/to/data/${i}_${j}.dat" );
>   }
> 
> }
> 
> How does one do this in R?  In particular, what's the equivalent of
> the above in R?
> 
> Most importantly, how could I have found out this answer from the
> R docs?

I'm sure that you will get several answers about the specifics of how to do
this.  As for where to look, in the "Introduction to R" manual, you will
find this section on lists:

http://cran.r-project.org/doc/manuals/R-intro.html#Lists-and-data-frames

It really helps to sit down with these documents and work your way through
them, as you have probably already started to do.  Also, read the emails
from r-help for a while to learn what is possible to do with R.

Sean



From timo.becker at oeaw.ac.at  Wed Nov  2 14:51:02 2005
From: timo.becker at oeaw.ac.at (Timo Becker)
Date: Wed, 02 Nov 2005 14:51:02 +0100
Subject: [R] x/y coordinates of dendrogram branches
In-Reply-To: <mailman.8.1130842801.9167.r-help@stat.math.ethz.ch>
References: <mailman.8.1130842801.9167.r-help@stat.math.ethz.ch>
Message-ID: <4368C446.1060204@oeaw.ac.at>

Dear R-users,

I need some help concerning the plotting of dendrograms for hierarchical 
agglomerative clustering.
The agglomeration niveau of each step should be displayed at the 
branches of the dendrogram.
For this I need the x/y coordinates of the branch-agglomerations of the 
dendrogram.
The y-values are known (the heights of the agglomeration), but how can I 
get the x-values?

 > mydata <- c(1,2,3,4,5)
 > hc <- hclust(dist(mydata), method="average")
 > hc$height # these are the y-coordinates
[1] 1.0 1.0 1.5 2.5

I experimented with the dendrogram object because it gives the midpoints:

 > dend <- as.dendrogram(hc)
 > attributes(dend[[1]])$midpoint
[1] 0.5

Perhaps I could loop over all possible branches and get the midpoints if 
I knew how to convert them to x-values.
The manual says that midpoint is the "numeric horizontal distance of the 
node from the left border (the leftmost leaf) of the
branch (unit 1 between all leaves)."
But how do I know which one is the leftmost leaf and what is its 
horizontal value?

Thanks in advance,
Timo



From sundar.dorai-raj at pdf.com  Wed Nov  2 14:58:58 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 02 Nov 2005 07:58:58 -0600
Subject: [R] Distribution fitting problem
In-Reply-To: <200511021531.20085.mmiller@nassp.uct.ac.za>
References: <200511021432.52723.mmiller@nassp.uct.ac.za>
	<4368B4B9.3060405@pdf.com>
	<200511021531.20085.mmiller@nassp.uct.ac.za>
Message-ID: <4368C622.90800@pdf.com>

Hi, Mark,

Not without seeing you data. You only provide the first value is a 
warning message below.

--sundar

Mark Miller wrote:
> Can you advise another distribution, was thinking of exponential, but was 
> advised poisson since independent, forgot about requiring integers
> 
> 
> On Wednesday 02 November 2005 14:44, you wrote:
> 
>>Mark Miller wrote:
>>
>>>I am using the MASS library function
>>>
>>>fitdistr(x, dpois, list(lambda=2))
>>>
>>>but I get
>>>
>>>Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
>>>        Function cannot be evaluated at initial parameters
>>>In addition: There were 50 or more warnings (use warnings() to see the
>>>first 50)
>>>
>>>and all the first 50 warnings say
>>>
>>>1:  non-integer x = 1.452222
>>>etc
>>>
>>>Can anyone tell me what I am doing wrong. p.s. the data was read in from
>>>a .csv file that I wrote using octave
>>
>>Hi, Mark,
>>
>>If you think the data are poisson, the observations should be integers.
>>
>>--sundar



From je_lemaitre at hotmail.com  Wed Nov  2 15:13:09 2005
From: je_lemaitre at hotmail.com (=?iso-8859-1?B?Suly9G1lIExlbWHudHJl?=)
Date: Wed, 2 Nov 2005 09:13:09 -0500
Subject: [R] nlminb failed to converge with lmer
Message-ID: <BAY103-DAV730DB923CEA2BE78233E5906E0@phx.gbl>

Dear all,

I'm building binomial mixed-model using lme4 package. 
I'm able to obtain outputs properly except when I include two particular
variables: date (from 23 to 34; 1 being to first sampling day) and Latitude
(UTM/100 000, from 55.42 to 56.53). No "NA" is any of those variables.
In those cases, I get the warning message: "nlminb failed to converge"
I tried to modify lmer controls as : msMaxIter; maxIter... but I still get
the message.
Should I bother about it?
If yes, what should I do to not get the message?

Thank you all in advance for you answers.

PS: My model writing is for example

Fm<-lmer(alive~factor(sex)+mass+parasite+Latitude+(1|ID), family=binomial,
method="AGQ", data=donnee).

Where "parasite" is presence/absence (0 or 1) and ID is the station identity
where I captured 0 to 20 specimens, each being alive or dead, having a sex,
a mass, a presence/absence of parasite. Latitude is given at the station
level. Date is given at the specimen level because I sampled for 4 days in
each station.


J??r??me Lema??tre
Ph.D. student
D??partment of biology,
University Laval
Quebec, Canada



From reilly at stat.auckland.ac.nz  Wed Nov  2 15:17:57 2005
From: reilly at stat.auckland.ac.nz (James Reilly)
Date: Thu, 03 Nov 2005 03:17:57 +1300
Subject: [R] Anything like associative arrays in R?
In-Reply-To: <644e1f320511020530h621b0984u664cbb62a0d4ce2b@mail.gmail.com>
References: <200511021324.jA2DOEe15697@panix3.panix.com>
	<644e1f320511020530h621b0984u664cbb62a0d4ce2b@mail.gmail.com>
Message-ID: <4368CA95.2060407@stat.auckland.ac.nz>


Also, if you want to read in various data files as in your Perl example,
you could replace the line inside both loops with something like this:
x[[i]][[j]] <- read.table(paste('/path/to/data/',i,'_',j,'.dat',sep=''))

See http://cran.r-project.org/doc/manuals/R-intro.html#Character-vectors
for another paste() function example.


On 3/11/2005 2:30 a.m., jim holtman wrote:
> Is this what you want?
>  x <- list()
> for (i in c('test', 'some', 'more')){
> for(j in c('lv1', 'lv2', 'lv3')){
> x[[i]][[j]] <- runif(10)
> }
> }
> x
> x[['some']][['lv2']]
> 
> 
>  On 11/2/05, kynn at panix.com <kynn at panix.com> wrote:
> 
>>
>>
>>
>>
>>Let me preface my question by stressing that I am much less interested
>>in the answer than in learning a way I could have *found the answer
>>myself*. (As helpful as the participants in this list are, I have far
>>too many R-related questions to resolve by posting here, and as I've
>>written before, in my experience the R documentation has not been very
>>helpful, but I remain hopeful that I may have managed to miss some
>>crucial document.)
>>
>>The task I want to accomplish is very simple: to define and
>>sequentially initialize M x N variables *programmatically*, according
>>to two different categories, containing N and M values, respectively.
>>
>>In languages with associative arrays, the typical way to do this is to
>>define a 2-d associative array; e.g. in Perl one could do
>>
>>for $i ( 'foo', 'bar', 'baz' ) {
>>
>>for $j ( 'eenie', 'meenie', 'minie', 'moe' ) {
>>
>>$table{ $i }{ $j } = read_table( "path/to/data/${i}_${j}.dat" );
>>}
>>
>>}
>>
>>How does one do this in R? In particular, what's the equivalent of
>>the above in R?
>>
>>Most importantly, how could I have found out this answer from the
>>R docs?
>>
>>Many thanks in advance,
>>
>>kj
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> 
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
> 
> What the problem you are trying to solve?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
James Reilly
Department of Statistics, University of Auckland
Private Bag 92019, Auckland, New Zealand



From ManuelPerera-Chang at fmc-ag.com  Wed Nov  2 15:34:07 2005
From: ManuelPerera-Chang at fmc-ag.com (ManuelPerera-Chang@fmc-ag.com)
Date: Wed, 2 Nov 2005 15:34:07 +0100
Subject: [R] (no subject)
Message-ID: <OFEF49542F.351D4485-ONC12570AD.004F72F8-C12570AD.00500781@notes.fresenius.de>





Hi Eszter,
These are all in the "base" package ..
Please try ...

?sapply
?diag
?sqrt

for the standardization ...

invTotal<-t(1/sapply(mydataset,sum,na.rm=T))
szorzo<-diag(as.vector(invTotal),length(invTotal))
szorzo%*%t(mydataset)

Szia,

Manuel



                                                                                                                                       
                                                                                                                                       
                                                                                                                                       




Dear Colleagues,

It is Eszter Illy??s from Hungary. I just have started to use R yesterday,
so I am a real beginner (but I could run some PCOA and managed to
transpose a dataframe).

No I have a problem that I would like to standardize my datafile with
the row totals and than make an arcsin squareroot transformation.
How can I do that? In which package shall I look for and for what
option?

Thank you very much, have a nice day

Eszter

_______________________________________________________________________
K??ptipp! J??tssz a T-Online k??pkeres??j??vel! Ha el??g ??gyes ??s gyors vagy,
felker??lhetsz a dics??s??gt??bl??ra...  www.t-online.hu

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From francoisromain at free.fr  Wed Nov  2 16:05:05 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 02 Nov 2005 16:05:05 +0100
Subject: [R] x/y coordinates of dendrogram branches
In-Reply-To: <4368C446.1060204@oeaw.ac.at>
References: <mailman.8.1130842801.9167.r-help@stat.math.ethz.ch>
	<4368C446.1060204@oeaw.ac.at>
Message-ID: <4368D5A1.4020608@free.fr>

Hi Timo,

Here is a quick and dirty recursive function.
##################
absi <- function(hc, level=length(hc$height), init=TRUE){
  if(init){
    .count <<- 0
    .topAbsis <<- NULL
    .heights <<- NULL
  }
  if(level<0) {
    .count <<- .count + 1
    return(.count)
  }
  node <- hc$merge[level,]
  le <- absi(hc, node[1], init=FALSE)
  ri <- absi(hc, node[2], init=FALSE)
 
  mid <- (le+ri)/2
 
  .topAbsis <<-  c(.topAbsis, mid)
  .heights <<- c(.heights, hc$height[level])
 
  invisible(mid)
}
#################

R> mydata <- c(1,2,3,4,5)
R> hc <- hclust(dist(mydata), method="average")
R> absi(hc)
R> .topAbsis # those are x-coordinates
[1] 1.500 4.500 3.750 2.625
R> .heights  # and y-coordinates
[1] 1.0 1.0 1.5 2.5
R> plot(hc)
R> points(.topAbsis, .heights, pch=21, bg=2, cex=5)


Is this close to what tou need ?


Romain




Le 02.11.2005 14:51, Timo Becker a ??crit :

>Dear R-users,
>
>I need some help concerning the plotting of dendrograms for hierarchical 
>agglomerative clustering.
>The agglomeration niveau of each step should be displayed at the 
>branches of the dendrogram.
>For this I need the x/y coordinates of the branch-agglomerations of the 
>dendrogram.
>The y-values are known (the heights of the agglomeration), but how can I 
>get the x-values?
>
> > mydata <- c(1,2,3,4,5)
> > hc <- hclust(dist(mydata), method="average")
> > hc$height # these are the y-coordinates
>[1] 1.0 1.0 1.5 2.5
>
>I experimented with the dendrogram object because it gives the midpoints:
>
> > dend <- as.dendrogram(hc)
> > attributes(dend[[1]])$midpoint
>[1] 0.5
>
>Perhaps I could loop over all possible branches and get the midpoints if 
>I knew how to convert them to x-values.
>The manual says that midpoint is the "numeric horizontal distance of the 
>node from the left border (the leftmost leaf) of the
>branch (unit 1 between all leaves)."
>But how do I know which one is the leftmost leaf and what is its 
>horizontal value?
>
>Thanks in advance,
>Timo
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From sara at gmesintra.com  Wed Nov  2 16:15:21 2005
From: sara at gmesintra.com (Sara Mouro)
Date: Wed, 2 Nov 2005 15:15:21 -0000
Subject: [R] margins too large
Message-ID: <200511021515.jA2FFQSY014706@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051102/d60ff727/attachment.pl

From alexfang at stat.rutgers.edu  Wed Nov  2 16:07:21 2005
From: alexfang at stat.rutgers.edu (alexfang@stat.rutgers.edu)
Date: Wed,  2 Nov 2005 15:07:21 +0000
Subject: [R] For multiple trellis plot, how to add a global title
Message-ID: <1130944041.4368d62923348@outlier.rutgers.edu>



I use print.trellis() to draw multilple plots in a single frame, but than how 
could add a global title and subtitle to the final plot.  Thank you!

eg code:
print(position=(0,0,1,0,5), more=T, xyplot(...))
print(position=(0,0.5,1,0,1), more=F, xyplot(...))

##### Will produce error if I put the following:
# title(main="global main title")
# title(sub="global sub title")



What should I do?  thank you!

best, Alex



From Roger.Bivand at nhh.no  Wed Nov  2 16:34:07 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 2 Nov 2005 16:34:07 +0100 (CET)
Subject: [R] margins too large
In-Reply-To: <200511021515.jA2FFQSY014706@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0511021623530.19755-100000@reclus.nhh.no>

On Wed, 2 Nov 2005, Sara Mouro wrote:

> Dear all,
> 
> How can I explian and solve the error message:
>  	"margins too large"
> 
> which appears when I do something like: 
> KK <- alltypes(SpatData, "K")
> plot.fasp(KK)
> 
> Hope someone can please help me on this.

First, you need to state which contributed package alltypes() belongs to
(spatstat). Also, note that questions about packages should be directed to
their maintainers, rather than the list; always say which version of R 
and which version of the package you are using (package version shown 
i.a. by help(package=<package_name>)).

If you like, try running plot.fasp() under debug() - debug(plot.fasp), to
see precisely what in your data is causing it to fall over, and/or running
traceback() immediately after plot.fasp() throws the error. It seems to be
your data, because the examples for these functions in spatstat 1.7-12 on
R 2.2.0 run without problems for me.

> 
> Regards,
> Sara Mouro
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From tom at maladmin.com  Wed Nov  2 12:00:56 2005
From: tom at maladmin.com (tom wright)
Date: Wed, 02 Nov 2005 06:00:56 -0500
Subject: [R] RODBC error
In-Reply-To: <200511012155.jA1LtK7v055916@server.ausvet.com.au>
References: <200511012155.jA1LtK7v055916@server.ausvet.com.au>
Message-ID: <1130929257.4362.6.camel@localhost.localdomain>


On Wed, 2005-02-11 at 07:55 +1000, Evan Sergeant wrote:
> Hi,
> 
> I hope that someone can help me with the following problem with RODBC 
> connection to a MySQL database
> 
> I am running R version 2.2.0 on windows XP, and have the MySQL database 
> registered in Windows ODBC.
> 
> I have set up a web interface on my personal ISS web server using PhP 
> to accept input values and then call Rterm with an R script to access 
> the database and return a summary of the analysed data.
> 
> This works perfectly if I run it from Rgui, or if I run Rterm from the 
> dos prompt using the same command line arguments, but returns an RODBC 
> error (below) when calling Rterm from the web interface
> 
> 1: [RODBC] ERROR: state IM002, code 0, message [Microsoft][ODBC Driver 
> Manager] Data source name not found and no default driver specified 
> 
> There appears to be some problem with recognising the database when 
> called from the web page, even though it is not a problem at any other 
> times.
> 
> Does anyone have any suggestions as to what I can do to overcome this 
> error
> 
> Thanks for your help
> 
> cheers
> 
> Evan Sergeant
Evan,
It sounds like your problem is occuring when you try to access the
database as a different user than yourself. Its been a while since I've
been familiar with ms systems but IRC pws runs as its own user
(ISUSR_...???). At the very least you will have to ensure that the
datasource is registered as a system DSN not a user DSN. Appart from
that you may run into other issues with access privileges.

good luck
tom



From roebuck at mdanderson.org  Wed Nov  2 17:06:09 2005
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Wed, 2 Nov 2005 10:06:09 -0600 (CST)
Subject: [R] Tcl/tk [for Mac OS X]
In-Reply-To: <BF8D935A.4DBD%sokol.haxhinasto@gmail.com>
References: <BF8D935A.4DBD%sokol.haxhinasto@gmail.com>
Message-ID: <Pine.OSF.4.58.0511021004200.165187@wotan.mdacc.tmc.edu>

On Tue, 1 Nov 2005, Sokol Haxhinasto wrote:

> I recently installed the R (the most recent) and Bioconductor on my
> computer.
> I installed the packages in "affy", the affyGui and the limmaGui on my
> computer and they all appear in the Load Package window.
> After trying to load the affyGui or the limmaGui packages I receive the
> following message entitled: "Tcl/Tk Extension(s) Not Found"-
>
> limmaGUI requires the Tcl/Tk extensions, BWidget and Tktable.
> You must have Tcl/Tk installed on your computer, not just the minimal
> Tcl/Tk installation which comes with R (for Windows).  If you do have
> Tcl/Tk installed, including the extensions (e.g. using the ActiveTcl
> distribution in Windows), make sure that R can find the path to the Tcl
> library, e.g. C:\Tcl\lib (on Windows) or /usr/lib (on Linux/Unix) or
> /sw/lib on Mac OSX.
>
> Any suggestions.
> Also I am trying to get the R.app installed and run on my mac, and am not
> successful.

<http://cran.hostingzero.com/bin/macosx/RMacOSX-FAQ.html#TclTk-issues>

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From nali at umn.edu  Wed Nov  2 17:03:06 2005
From: nali at umn.edu (Na Li)
Date: Wed, 02 Nov 2005 10:03:06 -0600
Subject: [R] RSQLite problems
References: <29k6fy7v11.fsf@bass.biostat.umn.edu>
	<20051028021259.GB24496@jessie.research.bell-labs.com>
Message-ID: <yxwtjrjav9.fsf@bass.biostat.umn.edu>

On 27 Oct 2005, David James wrote:

> Thanks for reporting the two problems. I'm attaching a simple update
> to two functions that will allow you to specify a different separator, 
> e.g., using your example:
> 
> dbWriteTable(con, "barley", barley, overwrite = TRUE, sep = ";")
> 
> This workaround still relies in dumping the data.frame into a temporary
> file and then importing into SQLite, but using prepared statements (which
> SQLite 3 supports) will require some more work.

Thanks.  This worked fine.

Perhaps the only SQL statement that I need is 'SELECT'.  I'm by no means a SQL
expert, but from the limited experiment that I've done, to select and merge
some variables from three tables.  The performance of 'SELECT' seemed to be
pretty bad (with SQLite backend) compared to simply 'merge' the R data frames
(although that has to be done two tables a time). In addition, somehow, only
500 rows were returned (should be about 4500-5000).

Do you have any experience about the performance of using R and DBI for data
management?

Michael



From ccatj at web.de  Wed Nov  2 17:22:31 2005
From: ccatj at web.de (Christian Jones)
Date: Wed, 02 Nov 2005 17:22:31 +0100
Subject: [R] extracting values with $
Message-ID: <387918775@web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051102/f1def3e7/attachment.pl

From mi2kelgrum at yahoo.com  Wed Nov  2 18:03:57 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Wed, 2 Nov 2005 09:03:57 -0800 (PST)
Subject: [R] readline() and Rterm in Windows
Message-ID: <20051102170357.44328.qmail@web60212.mail.yahoo.com>

I'm running an R script in Rterm and would like the
user to be prompted for input as in:

id <- readline("Please enter ID: ")
myfunction(id)

. . . etc.

This works when I run one line at a time in RGui, but
not when I try to run the script in Rterm (I'm working
with R 2.2.0 in Windows Server 2003).

Is there any way to do this?

Mikkel



From murdoch at stats.uwo.ca  Wed Nov  2 18:17:18 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 02 Nov 2005 12:17:18 -0500
Subject: [R] readline() and Rterm in Windows
In-Reply-To: <20051102170357.44328.qmail@web60212.mail.yahoo.com>
References: <20051102170357.44328.qmail@web60212.mail.yahoo.com>
Message-ID: <4368F49E.9060705@stats.uwo.ca>

Mikkel Grum wrote:
> I'm running an R script in Rterm and would like the
> user to be prompted for input as in:
> 
> id <- readline("Please enter ID: ")
> myfunction(id)
> 
> . . . etc.
> 
> This works when I run one line at a time in RGui, but
> not when I try to run the script in Rterm (I'm working
> with R 2.2.0 in Windows Server 2003).
> 
> Is there any way to do this?

It works for me.  What goes wrong when you try it?  (And how exactly are 
you executing the script?)  If you're trying something like

Rterm <script.R

it will fail, because readline will read from the file.  But if you use 
source() in Rterm, it should do what you want.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Wed Nov  2 18:23:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Nov 2005 17:23:43 +0000 (GMT)
Subject: [R] readline() and Rterm in Windows
In-Reply-To: <20051102170357.44328.qmail@web60212.mail.yahoo.com>
References: <20051102170357.44328.qmail@web60212.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0511021717330.32249@gannet.stats>

What does `not work' mean here?  In particular, what does `running an R 
script in' mean?

It works as I would expect: if foo.R contains

 	id <- readline("Please enter ID: ")

then

> source("foo.R")
Please enter ID: 5
> id
[1] "5"

Please do read the posting guide and explain what you are doing it enough 
detail for people to reproduce it.


On Wed, 2 Nov 2005, Mikkel Grum wrote:

> I'm running an R script in Rterm and would like the
> user to be prompted for input as in:
>
> id <- readline("Please enter ID: ")
> myfunction(id)
>
> . . . etc.
>
> This works when I run one line at a time in RGui, but
> not when I try to run the script in Rterm (I'm working
> with R 2.2.0 in Windows Server 2003).
>
> Is there any way to do this?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From leaflovesun at yahoo.ca  Wed Nov  2 18:48:45 2005
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Wed, 2 Nov 2005 10:48:45 -0700
Subject: [R] breaks in hist()
Message-ID: <200511021752.jA2Hqlh7019254@hypatia.math.ethz.ch>

Dear listers,

A quick question about breaks in hist().

The histogram is highly screwed to the right, say, the range of the vector is [0, 2], but 95% of the value is squeezed in the interval (0.01, 0.2). My question is : how to set the breaks then make the histogram look even?

Thanks in advance,

Leaf



From gunter.berton at gene.com  Wed Nov  2 19:09:47 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 2 Nov 2005 10:09:47 -0800
Subject: [R] Visualizing a Data Distribution -- Was: breaks in hist()
In-Reply-To: <200511021752.jA2Hqlh7019254@hypatia.math.ethz.ch>
Message-ID: <200511021809.jA2I9lKj029192@faraday.gene.com>

Leaf:

An interesting question concerning graphical perception. As you have noted,
choice of bin boundaries in a histogram can have a big effect on how a
distribution is perceived. My $.02 (U.S.):

Histograms are a relic of manual data plotting. We have much better
alternatives these days that should be used instead. e.g.

1. (my preference, but properly not consumer-friendly). Plot the cdf instead
(?ecdf) .

2. Plot a density estimator (?density ; ?densityplot)

3. See David Scott's ash package, perhaps the KernSmooth package also
(though density() probably already has anything that you'd need from it). 

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leaf Sun
> Sent: Wednesday, November 02, 2005 9:49 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] breaks in hist()
> 
> Dear listers,
> 
> A quick question about breaks in hist().
> 
> The histogram is highly screwed to the right, say, the range 
> of the vector is [0, 2], but 95% of the value is squeezed in 
> the interval (0.01, 0.2). My question is : how to set the 
> breaks then make the histogram look even?
> 
> Thanks in advance,
> 
> Leaf
> 
>



From davidr at rhotrading.com  Wed Nov  2 19:43:28 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 2 Nov 2005 12:43:28 -0600
Subject: [R] Bug report on get.hist.quote
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A6E16FD@rhosvr02.rhotrading.com>

Perhaps you forgot that currency pairs are named upsidedown; e.g.,
USD/JPY is yen per dollar? (This is sometimes written USD-JPY to try to
avoid the confusion. It is read "dollar-yen".)

David L. Reiner
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Ajay Shah
> Sent: Wednesday, November 02, 2005 5:09 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Bug report on get.hist.quote
> 
> 
> > get.hist.quote(instrument="INR/USD", provider="oanda",
start="2005-10-
> 20")
> trying URL
>
'http://www.oanda.com/convert/fxhistory?lang=en&date1=10%2F20%2F2005&dat
e=
>
11%2F01%2F2005&date_fmt=us&exch=INR&exch2=&expr=USD&expr2=&margin_fixed=
0&
> &SUBMIT=Get+Table&format=ASCII&redirected=1'
> Content type 'text/html' length unknown
> opened URL
> .......... ...
> downloaded 13Kb
> 
> 2005-10-20 2005-10-21 2005-10-22 2005-10-23 2005-10-24 2005-10-25
2005-10-
> 26
>    0.02220    0.02218    0.02224    0.02224    0.02224    0.02219
> 0.02226
> 2005-10-27 2005-10-28 2005-10-29 2005-10-30 2005-10-31 2005-11-01
>    0.02224    0.02225    0.02224    0.02224    0.02224    0.02221
> 
> 
> The answer is wrong. What is shown here is USD/INR, not INR/USD.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From gerifalte28 at hotmail.com  Wed Nov  2 19:49:24 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 02 Nov 2005 18:49:24 +0000
Subject: [R] breaks in hist()
In-Reply-To: <200511021752.jA2Hqlh7019254@hypatia.math.ethz.ch>
Message-ID: <BAY103-F8AC8B243C9167451CE842A66E0@phx.gbl>

Hi Leaf

The word "even" can be interpreted in several ways but I will give it a 
shot.
If you want to specify the breakpoints to represent the aggregation in your 
data you can use the argument breaks within histogram i.e.

x=c(runif(95,0,0.2),runif(5,.21,2))
hist(x, breaks=seq(0,2,.1), freq=F )#It will use breakpoints at 
0,0.1,0.2,...2

or you can also suggest a pre-defined number of cells i.e.
hist(x, breaks=7, freq=F )


You can also add a density line over the histogram using lines
lines(density(x, bw=.1))

Alternativelly, you can use some more basic visualization like a 
stem-and-leaf plot
stem(x)

I hope this helps

Francisco


>From: "Leaf Sun" <leaflovesun at yahoo.ca>
>To: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
>Subject: [R] breaks in hist()
>Date: Wed, 2 Nov 2005 10:48:45 -0700
>
>Dear listers,
>
>A quick question about breaks in hist().
>
>The histogram is highly screwed to the right, say, the range of the vector 
>is [0, 2], but 95% of the value is squeezed in the interval (0.01, 0.2). My 
>question is : how to set the breaks then make the histogram look even?
>
>Thanks in advance,
>
>Leaf
>


>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From efg at stowers-institute.org  Wed Nov  2 19:45:53 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Wed, 2 Nov 2005 12:45:53 -0600
Subject: [R] RODBC and Excel:  Wrong Data Type Assumed on Import
Message-ID: <dkb1h2$620$1@sea.gmane.org>

The first column in my Excel sheet has mostly numbers but I need to treat it
as character data:

> library(RODBC)
> channel <- odbcConnectExcel("U:/efg/lab/R/Plasmid/construct list.xls")
> plasmid <- sqlFetch(channel,"Sheet1", as.is=TRUE)
> odbcClose(channel)

> names(plasmid)
[1] "Plasmid Number" "Plasmid"        "Concentration"  "Comments"
"Lost"

# How is the type decided?  I need a character type.
> class(plasmid$"Plasmid Number")
[1] "numeric"
> typeof(plasmid$"Plasmid Number")
[1] "double"

> plasmid$"Plasmid Number"[273:276]
[1] 274  NA  NA 276

The two NAs are supposed to be 275a and 275b.  I tried the "as.is=TRUE" but
that didn't help.

I consulted Section 4, Relational databases, in the R Data Import/Export
document (for Version 2.2.0).

Section 4.2.2, Data types, was not helpful.  In particular, this did not
seem helpful:  "The more comprehensive of the R interface packages hide the
type conversion issues from the user."

Section 4.3.2, Package RODBC, provided a "simple example of using ODBC ..
with a(sic) Excel spreadsheet" but is silent on how to control the data type
on import.  Could the documentation be expanded to address this issue?

I really need to show "Plasmid 275a" and "Plasmid 275b" instead of "Plasmid
NA".

Thanks for any help with this.

efg
--
Earl F. Glynn
Scientific Programmer
Bioinformatics Department
Stowers Institute for Medical Research



From murdoch at stats.uwo.ca  Wed Nov  2 19:55:22 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 02 Nov 2005 13:55:22 -0500
Subject: [R] [Rd] unvectorized option for outer()
In-Reply-To: <43629490.9060204@acm.org>
References: <8B08A3A1EA7AAC41BE24C750338754E69FDF14@HERMES.demogr.mpg.de>
	<43629490.9060204@acm.org>
Message-ID: <43690B9A.4040102@stats.uwo.ca>

I've now added a version of Vectorize to R-devel.

Duncan



From br44114 at gmail.com  Wed Nov  2 20:07:12 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 2 Nov 2005 14:07:12 -0500
Subject: [R] Visualizing a Data Distribution -- Was: breaks in hist()
Message-ID: <8d5a36350511021107s1cf0bba9n8e3c3052a04f3f73@mail.gmail.com>

> > Leaf Sun wrote:
> > The histogram is highly screwed to the right, say, the range
> > of the vector is [0, 2], but 95% of the value is squeezed in
> > the interval (0.01, 0.2).

I guess the histogram is as you wrote. See
http://web.maths.unsw.edu.au/~tduong/seminars/intro2kde/
for a short explanation.


> -----Original Message-----
> From: Berton Gunter [mailto:gunter.berton at gene.com]
> Sent: Wednesday, November 02, 2005 1:10 PM
> To: 'Leaf Sun'; r-help at stat.math.ethz.ch
> Subject: [R] Visualizing a Data Distribution -- Was: breaks in hist()
>
>
> Leaf:
>
> An interesting question concerning graphical perception. As
> you have noted,
> choice of bin boundaries in a histogram can have a big effect on how a
> distribution is perceived. My $.02 (U.S.):
>
> Histograms are a relic of manual data plotting. We have much better
> alternatives these days that should be used instead. e.g.
>
> 1. (my preference, but properly not consumer-friendly). Plot
> the cdf instead
> (?ecdf) .
>
> 2. Plot a density estimator (?density ; ?densityplot)
>
> 3. See David Scott's ash package, perhaps the KernSmooth package also
> (though density() probably already has anything that you'd
> need from it).
>
> Cheers,
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the
> scientific learning
> process."  - George E. P. Box
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leaf Sun
> > Sent: Wednesday, November 02, 2005 9:49 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] breaks in hist()
> >
> > Dear listers,
> >
> > A quick question about breaks in hist().
> >
> > The histogram is highly screwed to the right, say, the range
> > of the vector is [0, 2], but 95% of the value is squeezed in
> > the interval (0.01, 0.2). My question is : how to set the
> > breaks then make the histogram look even?
> >
> > Thanks in advance,
> >
> > Leaf
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From deepayan.sarkar at gmail.com  Wed Nov  2 20:07:32 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 2 Nov 2005 13:07:32 -0600
Subject: [R] For multiple trellis plot, how to add a global title
In-Reply-To: <1130944041.4368d62923348@outlier.rutgers.edu>
References: <1130944041.4368d62923348@outlier.rutgers.edu>
Message-ID: <eb555e660511021107t742342d1m912642ea305041c5@mail.gmail.com>

On 11/2/05, alexfang at stat.rutgers.edu <alexfang at stat.rutgers.edu> wrote:
>
>
> I use print.trellis() to draw multilple plots in a single frame, but than
> how
> could add a global title and subtitle to the final plot.  Thank you!
>
> eg code:
> print(position=(0,0,1,0,5), more=T, xyplot(...))
> print(position=(0,0.5,1,0,1), more=F, xyplot(...))
>
> ##### Will produce error if I put the following:
> # title(main="global main title")
> # title(sub="global sub title")
>
> What should I do?  thank you!

Use grid.text, e.g.

library(grid)
grid.text("global main title", x = 0.5, y = 0.99, just = c("centre", "top"))

-Deepayan



From deepayan.sarkar at gmail.com  Wed Nov  2 20:12:22 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 2 Nov 2005 13:12:22 -0600
Subject: [R] Trellis Plot Group Variable Font size
In-Reply-To: <43681A02.2030407@stat.rutgers.edu>
References: <43681A02.2030407@stat.rutgers.edu>
Message-ID: <eb555e660511021112i6c87fde1r115bdc8137909f1@mail.gmail.com>

On 11/1/05, Jiangang Fang (Alex) <alexfang at stat.rutgers.edu> wrote:
> I plot a trellis plot in R using the lattice package.  The size of the
> group variable is pretty big (over 100 groups).  I used to do it in
> Splus and the pic there is pretty good.  Now I transfer to R but the
> plot is really not accetable. The group variable font in the R produced
> trellis is so huge that it takes over the major area of the plot.  Any
> one knows how to make the font size smaller?  thanks very much.  best, Alex
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Please follow the suggestion above. We are not mind readers and your
mail has no information that allows us to reproduce your problem.

Deepayan



From german.lopez at ua.es  Wed Nov  2 21:05:00 2005
From: german.lopez at ua.es (german.lopez@ua.es)
Date: Wed, 2 Nov 2005 21:05:00 +0100
Subject: [R] model selection based on AICc
Message-ID: <200511022005.jA2K50gu008411@aitana.cpd.ua.es>

Dear members of the list,
  I'm fitting poisson regression models using stepAIC that appear to 
be overparametrized. I would like to know if there is the 
possibility of fitting models by steps but using the AICc instead of 
AIC.
  Best wishes
  German Lopez



From mi2kelgrum at yahoo.com  Wed Nov  2 21:09:02 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Wed, 2 Nov 2005 12:09:02 -0800 (PST)
Subject: [R] readline() and Rterm in Windows
In-Reply-To: <Pine.LNX.4.61.0511021717330.32249@gannet.stats>
Message-ID: <20051102200902.83506.qmail@web60217.mail.yahoo.com>

Duncan and Prof, thanks for your comments and
apologies for not being more specific. I'm not getting
the same results you get from the steps you propose.

If I write a script foo.R with two lines

	id <- readline("Please enter an ID: ")
	id

and then use source("foo.R") (either at the Rterm
prompt, or in RGui) it is true that get prompted, but
the second line does not visibly run, i.e. I get
> source("id.r")
Please enter an ID: 5
>

and if I then type id, I get
> id
[1] "id"

If I cut and paste the two lines in RGui (in one go),
I get
> id <- readline("Please enter an ID: ")
Please enter an ID: id
>

What I really want is a batch file on the desktop with
the following commands:

   c:\r\R-2.2.0\bin\Rterm.exe --no-save --no-restore
<script.R> script.out 2>&1
   c:\texmf\miktex\bin\latex
\nonstopmode\input{blue.tex}


and script.R reads something like:

   id <- readline("Please enter an ID: ")
   id
   Sweave("blue.Rnw")

I said that script.R didn't run, which was an
incorrect description. It runs without prompting for
the ID, and gives error messages all through because
blue.Rnw needs the id.

This is a very simplified version of what I'm doing,
but if I use only the first line of the batch file and
the first two lines of the script and could get that
to work, I could figure out the rest.

Mikkel



--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> What does `not work' mean here?  In particular, what
> does `running an R 
> script in' mean?
> 
> It works as I would expect: if foo.R contains
> 
>  	id <- readline("Please enter ID: ")
> 
> then
> 
> > source("foo.R")
> Please enter ID: 5
> > id
> [1] "5"
> 
> Please do read the posting guide and explain what
> you are doing it enough 
> detail for people to reproduce it.
> 
> 
> On Wed, 2 Nov 2005, Mikkel Grum wrote:
> 
> > I'm running an R script in Rterm and would like
> the
> > user to be prompted for input as in:
> >
> > id <- readline("Please enter ID: ")
> > myfunction(id)
> >
> > . . . etc.
> >
> > This works when I run one line at a time in RGui,
> but
> > not when I try to run the script in Rterm (I'm
> working
> > with R 2.2.0 in Windows Server 2003).
> >
> > Is there any way to do this?
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
>



From h.wickham at gmail.com  Wed Nov  2 21:09:40 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 2 Nov 2005 14:09:40 -0600
Subject: [R] Placing a grob in multiple viewports
Message-ID: <f8e6ff050511021209l7b368ea4o18a071727055cce@mail.gmail.com>

Hi all,

What's the best way of placing a grob into muliple viewports?  I've
been using code like:

plot_grob_matrix <- function(gm, type=deparse(substitute(gm))) {	
	grid <- expand.grid(x=1:nrow(gm), y=1:ncol(gm))
	do.call(gList,mapply(function(x,y) editGrob(gm[[x,y]], vp=vp_path(x,
y, type)), grid$x, grid$y, SIMPLIFY=FALSE))
}

vp_path <- function(row, col, type) {
	vpPath("layout", paste(type, row, col, sep="_"))
}

to place a matrix of grobs into a similar looking matrix of viewports
(constructed by another function).  This works fine if the matrix
contains different grobs, but only draws one if they are all the same 
(obviously due to the copy by reference nature of grobs). What's a
better way of doing this? Most of the time the grobs are different
(ie. components of a plot), but sometimes are shared (eg. common
axes).

(any hints on a nicer way to construct the gList would also be appreciated)

Thanks,

Hadley



From german.lopez at ua.es  Wed Nov  2 21:10:42 2005
From: german.lopez at ua.es (german.lopez@ua.es)
Date: Wed, 2 Nov 2005 21:10:42 +0100
Subject: [R] model selection based on AICc
Message-ID: <200511022010.jA2KAga2010594@aitana.cpd.ua.es>

Dear members of the list,
  I'm fitting poisson regression models using stepAIC that appear to 
be overparametrized. I would like to know if there is the 
possibility of fitting models by steps but using the AICc instead of 
AIC.
  Best wishes
  German Lopez



From booopi at yahoo.com  Wed Nov  2 22:22:55 2005
From: booopi at yahoo.com (booop booop)
Date: Wed, 2 Nov 2005 13:22:55 -0800 (PST)
Subject: [R] help : matrix row/column random mixing
Message-ID: <20051102212255.28247.qmail@web61225.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051102/1687c9f3/attachment.pl

From maechler at stat.math.ethz.ch  Wed Nov  2 23:03:21 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 2 Nov 2005 23:03:21 +0100
Subject: [R] help : matrix row/column random mixing
In-Reply-To: <20051102212255.28247.qmail@web61225.mail.yahoo.com>
References: <20051102212255.28247.qmail@web61225.mail.yahoo.com>
Message-ID: <17257.14249.799020.755155@stat.math.ethz.ch>

>>>>> "booop" == booop booop <booopi at yahoo.com>
>>>>>     on Wed, 2 Nov 2005 13:22:55 -0800 (PST) writes:

    booop> Could anybody help me in mixing the matrix values
    booop> randomly (first rows and then columns)...  for eg
    booop> putting the 1 st row in the place of 3 rd and 3rd in
    booop> place of 4 th and 4th in place of 1st row
    booop> ...something like this in columns also..
 
    booop> Please suggest me some ways..
 
    booop> thanks a lot..
 
    booop> with regards, boopathy.

Quite a few of us don't like such completely anonymous postings.
At least tell us a "who you are" in your signature....

... and 
		
    booop> ---------------------------------

    booop> 	[[alternative HTML version deleted]]

    booop> ______________________________________________
    booop> R-help at stat.math.ethz.ch mailing list
    booop> https://stat.ethz.ch/mailman/listinfo/r-help
    booop> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Indeed, please do read it!



From michal at cvu.wustl.edu  Wed Nov  2 23:06:58 2005
From: michal at cvu.wustl.edu (Michal Lijowski)
Date: Wed, 02 Nov 2005 16:06:58 -0600
Subject: [R] Orientation of tickmarks labels in boxplot/plot
Message-ID: <1130969219.22750.27.camel@ml-cvu.wustl.edu>

Hi,

I have been trying draw tickmark labels along
the y - axis perpendicular to the y axis while
labels along the x - axis parallel to x axis
while making box plot.

Here is my test dataset.

 TData
   ID Ratio
1   0 7.075
2   0 7.414
3   0 7.403
4   0 7.168
5   0 6.820
6   0 7.294
7   0 7.238
8   0 7.938
9   1 7.708
10  1 8.691
11  1 8.714
12  1 8.066
13  1 8.949
14  1 8.590
15  1 8.714
16  1 8.601

 boxplot(Ratio ~ ID, data=TData)

makes box plot with tickmark labels parallel to the y - axis.
So I try 

boxplot(Ratio ~ ID, data=TData, axes=FALSE)
par(las=0)
axis(1)
and I get x - axis ranging from 0.5 to 2.5 (why?) and 
boxes at 1 and 2.
par(las=2)
axis(2)
box()
So, if I set tickmark labels parallel to y - axis
somehow the x - axis range is not what I expect even
if I use xlim = c(0.0, 3.0)
 in boxplot(Ratio ~ Id, data=TData, axes=FALSE, xlim=c(0.0, 3.0))
 par(las=0)
 axis(1)

Plots are in the attachments in pdf format.

I appreciate any tips.

I am using R 2.2.0 (2005-10-06) on FC4.

Michal







-------------- next part --------------
A non-text attachment was scrubbed...
Name: RPlot1.pdf
Type: application/pdf
Size: 3578 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051102/db2c5449/RPlot1.pdf
-------------- next part --------------
A non-text attachment was scrubbed...
Name: RPlot2.pdf
Type: application/pdf
Size: 2731 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051102/db2c5449/RPlot2.pdf

From murdoch at stats.uwo.ca  Wed Nov  2 23:08:55 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 02 Nov 2005 17:08:55 -0500
Subject: [R] readline() and Rterm in Windows
In-Reply-To: <20051102200902.83506.qmail@web60217.mail.yahoo.com>
References: <20051102200902.83506.qmail@web60217.mail.yahoo.com>
Message-ID: <436938F7.3020007@stats.uwo.ca>

Mikkel Grum wrote:
> Duncan and Prof, thanks for your comments and
> apologies for not being more specific. I'm not getting
> the same results you get from the steps you propose.
> 
> If I write a script foo.R with two lines
> 
> 	id <- readline("Please enter an ID: ")
> 	id
> 
> and then use source("foo.R") (either at the Rterm
> prompt, or in RGui) it is true that get prompted, but
> the second line does not visibly run, i.e. I get
> 
>>source("id.r")
> 
> Please enter an ID: 5
> 
> 
> and if I then type id, I get
> 
>>id
> 
> [1] "id"
> 
> If I cut and paste the two lines in RGui (in one go),
> I get
> 
>>id <- readline("Please enter an ID: ")
> 
> Please enter an ID: id
> 
> 
> What I really want is a batch file on the desktop with
> the following commands:
> 
>    c:\r\R-2.2.0\bin\Rterm.exe --no-save --no-restore
> <script.R> script.out 2>&1
>    c:\texmf\miktex\bin\latex
> \nonstopmode\input{blue.tex}
> 
> 
> and script.R reads something like:
> 
>    id <- readline("Please enter an ID: ")
>    id
>    Sweave("blue.Rnw")
> 
> I said that script.R didn't run, which was an
> incorrect description. It runs without prompting for
> the ID, and gives error messages all through because
> blue.Rnw needs the id.
> 
> This is a very simplified version of what I'm doing,
> but if I use only the first line of the batch file and
> the first two lines of the script and could get that
> to work, I could figure out the rest.

It won't work so simply.  You're redirecting stdin, so user input would 
be taken from there; you're redirecting stdout and stderr, so the prompt 
won't be visible to the user.

You need to open new handles to the console.  The code below will do it 
in Windows; the syntax to specify the console in Unix-alikes will be 
different (but I don't know what it is).

conout <- file('CONOUT$','w')
conin <- file('CONIN$', 'r')
cat('Please enter an ID:', file=conout)
flush(conout)
id <- readLines(conin, 1)
print(id)

Duncan Murdoch



From droberts at montana.edu  Wed Nov  2 23:13:19 2005
From: droberts at montana.edu (Dave Roberts)
Date: Wed, 02 Nov 2005 15:13:19 -0700
Subject: [R] help : matrix row/column random mixing
In-Reply-To: <20051102212255.28247.qmail@web61225.mail.yahoo.com>
References: <20051102212255.28247.qmail@web61225.mail.yahoo.com>
Message-ID: <436939FF.5030205@montana.edu>

There are several bootstrap packages available at CRAN that probably 
provide an elegant solution, but simply permuting the matrix is pretty easy

 > data <- matrix(1:100,nrow=5) # matrix of 5 rows and 20 columns
 > x <- data[sample(1:5),] # permute the rows
 > y <- x[,sample(1:20)] # permute the columns

to really scamble the matrix

 > z <- matrix(sample(data),nrow=5)

Dave R.

booop booop wrote:
> Could anybody help me in mixing the matrix values randomly (first rows and then columns)...
>  
> for eg  putting the 1 st row in the place of 3 rd and 3rd in place of 4 th and 4th in place of 1st row ...something like this in columns also..
>  
> Please suggest me some ways..
>  
> thanks a lot..
>  
> with regards,
> boopathy.
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From gunter.berton at gene.com  Wed Nov  2 23:30:41 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 2 Nov 2005 14:30:41 -0800
Subject: [R] Orientation of tickmarks labels in boxplot/plot
In-Reply-To: <1130969219.22750.27.camel@ml-cvu.wustl.edu>
Message-ID: <200511022230.jA2MUf2D026698@ohm.gene.com>

Michal:

> and I get x - axis ranging from 0.5 to 2.5 (why?) and 
> boxes at 1 and 2.

See the "at" argument of ?bxp and note what it says there regarding the
default. Read the docs more carefully -- ?boxplot has the link to bxp().

Bert Gunter
Genentech



From mschwartz at mn.rr.com  Wed Nov  2 23:47:50 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 02 Nov 2005 16:47:50 -0600
Subject: [R] Orientation of tickmarks labels in boxplot/plot
In-Reply-To: <1130969219.22750.27.camel@ml-cvu.wustl.edu>
References: <1130969219.22750.27.camel@ml-cvu.wustl.edu>
Message-ID: <1130971671.6186.13.camel@localhost.localdomain>

On Wed, 2005-11-02 at 16:06 -0600, Michal Lijowski wrote:
> Hi,
> 
> I have been trying draw tickmark labels along
> the y - axis perpendicular to the y axis while
> labels along the x - axis parallel to x axis
> while making box plot.
> 
> Here is my test dataset.
> 
>  TData
>    ID Ratio
> 1   0 7.075
> 2   0 7.414
> 3   0 7.403
> 4   0 7.168
> 5   0 6.820
> 6   0 7.294
> 7   0 7.238
> 8   0 7.938
> 9   1 7.708
> 10  1 8.691
> 11  1 8.714
> 12  1 8.066
> 13  1 8.949
> 14  1 8.590
> 15  1 8.714
> 16  1 8.601
> 
>  boxplot(Ratio ~ ID, data=TData)
> 
> makes box plot with tickmark labels parallel to the y - axis.
> So I try 
> 
> boxplot(Ratio ~ ID, data=TData, axes=FALSE)
> par(las=0)
> axis(1)
> and I get x - axis ranging from 0.5 to 2.5 (why?) and 
> boxes at 1 and 2.
> par(las=2)
> axis(2)
> box()
> So, if I set tickmark labels parallel to y - axis
> somehow the x - axis range is not what I expect even
> if I use xlim = c(0.0, 3.0)
>  in boxplot(Ratio ~ Id, data=TData, axes=FALSE, xlim=c(0.0, 3.0))
>  par(las=0)
>  axis(1)
> 
> Plots are in the attachments in pdf format.
> 
> I appreciate any tips.
> 
> I am using R 2.2.0 (2005-10-06) on FC4.
> 
> Michal

I suspect that you want this:

  boxplot(Ratio ~ ID, data=TData, las = 1)

so that the x and y axis labels are horizontal. See ?par and review the
options for 'las'. '1' is for axis tick mark labels to be horizontal.

The x axis (horizontal axis if the boxes are vertical, the vertical axis
if the boxes are horizontal) is set by default to the number of boxes
(groups) +/- 0.5.  'xlim' is ignored in both cases.

The boxes themselves are placed at integer values from 1:N, where N is
the number of groups.

There is the 'at' argument and there is an example of its use
in ?boxplot. You can also search the archives for posts where variations
on the use of 'at' have been posted (recently, in fact...)

The actual plotting of the boxplots is done by bxp(), so review the help
for that function as well.

HTH,

Marc Schwartz



From maj at stats.waikato.ac.nz  Thu Nov  3 01:02:18 2005
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 03 Nov 2005 13:02:18 +1300
Subject: [R] Extracting variance components in lme
Message-ID: <4369538A.9030205@stats.waikato.ac.nz>

Consider the output for the inroductory "Rail" example in "Mixed Effects 
Models in S and S-PLUS" by Pinheiro and Bates:

 > summary(fm1Rail.lme)
Linear mixed-effects model fit by REML
  Data: Rail
       AIC      BIC   logLik
   128.177 130.6766 -61.0885

Random effects:
  Formula: ~1 | Rail
         (Intercept) Residual
StdDev:    24.80547 4.020779

Fixed effects: travel ~ 1
             Value Std.Error DF  t-value p-value
(Intercept)  66.5  10.17104 12 6.538173       0

Standardized Within-Group Residuals:
         Min          Q1         Med          Q3         Max
-1.61882658 -0.28217671  0.03569328  0.21955784  1.61437744

Number of Observations: 18
Number of Groups: 6


I want to extract the variance components  sigma = 4.020779 (the within 
component) and sigma_b = 24.80547 (the between component).

I can get sigma easily:
sigma <-  fm1Rail.lme$sigma

but how can I get sigma_b ?

Cheers,  Murray Jorgensen

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862



From maj at stats.waikato.ac.nz  Thu Nov  3 01:47:24 2005
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 03 Nov 2005 13:47:24 +1300
Subject: [R] Extracting Variance Components fro lme
Message-ID: <43695E1C.4030706@stats.waikato.ac.nz>

It seems that what I need to get the within group component is

  as.numeric(VarCorr(fm1Rail.lme)[2,2])

thanks to Bert Gunter and Peter Alspach.

Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862



From blomsp at ozemail.com.au  Thu Nov  3 01:52:51 2005
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Thu, 03 Nov 2005 11:52:51 +1100
Subject: [R] Extracting variance components in lme
In-Reply-To: <4369538A.9030205@stats.waikato.ac.nz>
References: <4369538A.9030205@stats.waikato.ac.nz>
Message-ID: <7.0.0.10.0.20051103114652.019eb8e0@ozemail.com.au>

v <- VarCorr(fm1Rail.lme)

str(v) # get an idea of how v is structured. This suggests:

 > as.numeric(v[1, 2])
[1] 24.80547

There may be easier and better ways....

HTH,

Simon.


At 11:02 AM 3/11/2005, you wrote:
>Consider the output for the inroductory "Rail" example in "Mixed Effects
>Models in S and S-PLUS" by Pinheiro and Bates:
>
>  > summary(fm1Rail.lme)
>Linear mixed-effects model fit by REML
>   Data: Rail
>        AIC      BIC   logLik
>    128.177 130.6766 -61.0885
>
>Random effects:
>   Formula: ~1 | Rail
>          (Intercept) Residual
>StdDev:    24.80547 4.020779
>
>Fixed effects: travel ~ 1
>              Value Std.Error DF  t-value p-value
>(Intercept)  66.5  10.17104 12 6.538173       0
>
>Standardized Within-Group Residuals:
>          Min          Q1         Med          Q3         Max
>-1.61882658 -0.28217671  0.03569328  0.21955784  1.61437744
>
>Number of Observations: 18
>Number of Groups: 6
>
>
>I want to extract the variance components  sigma = 4.020779 (the within
>component) and sigma_b = 24.80547 (the between component).
>
>I can get sigma easily:
>sigma <-  fm1Rail.lme$sigma
>
>but how can I get sigma_b ?
>
>Cheers,  Murray Jorgensen
>
>--
>Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
>Department of Statistics, University of Waikato, Hamilton, New Zealand
>Email: maj at waikato.ac.nz                                Fax 7 838 4155
>Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maj at stats.waikato.ac.nz  Thu Nov  3 02:00:27 2005
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 03 Nov 2005 14:00:27 +1300
Subject: [R] Extracting variance components in lme
In-Reply-To: <7.0.0.10.0.20051103114652.019eb8e0@ozemail.com.au>
References: <4369538A.9030205@stats.waikato.ac.nz>
	<7.0.0.10.0.20051103114652.019eb8e0@ozemail.com.au>
Message-ID: <4369612B.7050500@stats.waikato.ac.nz>

Woops! I should have written:


as.numeric(VarCorr(fm1Rail.lme)[1,2])

for the "within" component.

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862



From spencer.graves at pdf.com  Thu Nov  3 03:01:03 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Nov 2005 18:01:03 -0800
Subject: [R] ML optimization question--unidimensional unfolding scaling
In-Reply-To: <BF743C38.11F1D%pmuhl830@gmail.com>
References: <BF743C38.11F1D%pmuhl830@gmail.com>
Message-ID: <43696F5F.1000703@pdf.com>

	  Have you looked at the code for "optim"?  If you execute "optim", it 
will list the code.  You can copy that into a script file and walk 
through it line by line to figure out what it does.  By doing this, you 
should be able to find a place in the iteration where you can test both 
branches of each bifurcation and pick one -- or keep a list of however 
many you want and follow them all more or less simultaneously, pruning 
the ones that seem too implausible.  Then you can alternate between a 
piece of the "optim" code, bifurcating and pruning, adjusting each and 
printing intermediate progress reports to help you understand what it's 
doing and how you might want to modify it.

	  With a bit more effort, you can get the official source code with 
comments.  To do that, I think you go to "www.r-project.org" -> CRAN -> 
(select a local mirror) -> "Software:  R sources".  From there, just 
download "The latest release:  R-2.2.0.tar.gz".

	  For more detailed help, I suggest you try to think of the simplest 
possible toy problem that still contains one of the issues you find most 
difficult.  Then send that to this list.  If readers can copy a few 
lines of R code from your email into R and try a couple of things in 
less than a minute, I think you might get more useful replies quicker.

	  Best Wishes,
	  Spencer Graves

Peter Muhlberger wrote:

> Hi Spencer:  Thanks for your interest!  Also, the posting guide was helpful.
> 
> I think my problem might be solved if I could find a way to terminate nlm or
> optim runs from within the user-given minimization function they call.
> Optimization is unconstrained.
> 
> I'm essentially using normal like curves that translate observed values on a
> set of variables (one curve per variable) into latent unfolded values.  The
> observed values are on the Y-axis & the latent (hence parameters to be
> estimated) are on the X-axis.  The problem is that there are two points into
> which an observed value can map on a curve--one on either side of the curve
> mean.  Only one of these values actually will be optimal for all observed
> variables, but it's easy to show that most estimation methods will get stuck
> on the non-optimal value if they find that one first.  Moving away from that
> point, the likelihood gets a whole lot worse before the routine will 'see'
> the optimal point on the other side of the normal curve.
> 
> SANN might work, but I kind of wonder how useful it'd be in estimating
> hundreds of parameters--thanks to that latent scale.
> 
> My (possibly harebrained) thought for how to estimate this unfolding using
> some gradient-based method would be to run through some iterations and then
> check to see whether a better solution exists on the 'other side' of the
> normal curves.  If it does, replace those parameters with the better ones.
> Because this causes the likelihood to jump, I'd probably have to start the
> estimation process over again (maybe).  But, I see no way from within the
> minimization function called by NLM or optim to tell NLM or optim to
> terminate its current run.  I could make the algorithm recursive, but that
> eats up resources & will probably have to be terminated w/ an error.
> 
> Peter
> 
> 
> On 10/11/05 11:11 PM, "Spencer Graves" <spencer.graves at pdf.com> wrote:
> 
> 
>> There may be a few problems where ML (or more generally Bayes) fails
>>to give sensible answers, but they are relatively rare.
>>
>> What is your likelihood?  How many parameters are you trying to
>>estimate?
>>
>> Are you using constrained or unconstrained optimization?  If
>>constrained, I suggest you remove the constraints by appropriate
>>transformation.  When considering alternative transformations, I
>>consider (a) what makes physical sense, and (b) which transformation
>>produces a log likelihood that is more close to being parabolic.
>>
>> Hou are you calling "optim"?  Have you tried all "SANN" as well as
>>"Nelder-Mead", "BFGS", and "CG"?  If you are using constrained
>>optimization, I suggest you move the constraints to Inf by appropriate
>>transformation and use the other methods, as I just suggested.
>>
>> If you would still like more suggestions from this group, please
>>provide more detail -- but as tersely as possible.  The posting guide
>>is, I believe, quite useful (www.R-project.org/posting-guide.html).
>>
>> spencer graves
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From mi2kelgrum at yahoo.com  Thu Nov  3 09:35:09 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Thu, 3 Nov 2005 00:35:09 -0800 (PST)
Subject: [R] readline() and Rterm in Windows
In-Reply-To: <436938F7.3020007@stats.uwo.ca>
Message-ID: <20051103083509.85187.qmail@web60221.mail.yahoo.com>

I've tried your proposal in a number of ways, and
there must be something I'm not understanding. If I
run your script (using source() in RGui, or ctrl-R
from the R Editor, I get:

> conout <- file('CONOUT$','w')
Error in file("CONOUT$", "w") : unable to open
connection
In addition: Warning message:
cannot open file 'CONOUT$', reason 'Permission denied'

>

so I added the path as in:

conout <- file('C:\\R\\R-2.2.0\\CONOUT$','w')
conin <- file('C:\\R\\R-2.2.0\\CONIN$', 'r')
cat('Please enter an ID:', file=conout)
flush(conout)
id <- readLines(conin, 1)
print(id)


Using RGui and ctrl-R from the R Editor, I get

> conout <- file('C:\\R\\R-2.2.0\\CONOUT$','w')
> conin <- file('C:\\R\\R-2.2.0\\CONIN$', 'r')
Error in file("C:\\R\\R-2.2.0\\CONIN$", "r") : 
        unable to open connection
In addition: Warning message:
cannot open file 'C:\R\R-2.2.0\CONIN$', reason 'No
such file or directory' 
> cat('Please enter an ID:', file=conout)
> flush(conout)
> id <- readLines(conin, 1)
Error in readLines(conin, 1) : object "conin" not
found

and with
> source("foo.R")
Error in file("C:\\R\\R-2.2.0\\CONIN$", "r") : 
        unable to open connection
In addition: Warning message:
cannot open file 'C:\R\R-2.2.0\CONIN$', reason 'No
such file or directory' 
> 

When I create a batch file with the following command
:
C:\R\R-2.2.0\bin\Rterm.exe --vanilla
<C:\R\R-2.2.0\foo.R> C:\R\R-2.2.0\foo.out

and double click on the batch file, the out file gives
me:

R : Copyright 2005, The R Foundation for Statistical
Computing
Version 2.2.0  (2005-10-06 r35749)
ISBN 3-900051-07-0
. . .
Type 'q()' to quit R.

> conout <- file('C:\\R\\R-2.2.0\\CONOUT$','w')
> conin <- file('C:\\R\\R-2.2.0\\CONIN$', 'r')

and nothing else. In none of the situations do I get
prompted for input. What am I doing hopelessly wrong?

Mikkel

--- Duncan Murdoch <murdoch at stats.uwo.ca> wrote:

> Mikkel Grum wrote:
> > Duncan and Prof, thanks for your comments and
> > apologies for not being more specific. I'm not
> getting
> > the same results you get from the steps you
> propose.
> > 
> > If I write a script foo.R with two lines
> > 
> > 	id <- readline("Please enter an ID: ")
> > 	id
> > 
> > and then use source("foo.R") (either at the Rterm
> > prompt, or in RGui) it is true that get prompted,
> but
> > the second line does not visibly run, i.e. I get
> > 
> >>source("id.r")
> > 
> > Please enter an ID: 5
> > 
> > 
> > and if I then type id, I get
> > 
> >>id
> > 
> > [1] "id"
> > 
> > If I cut and paste the two lines in RGui (in one
> go),
> > I get
> > 
> >>id <- readline("Please enter an ID: ")
> > 
> > Please enter an ID: id
> > 
> > 
> > What I really want is a batch file on the desktop
> with
> > the following commands:
> > 
> >    c:\r\R-2.2.0\bin\Rterm.exe --no-save
> --no-restore
> > <script.R> script.out 2>&1
> >    c:\texmf\miktex\bin\latex
> > \nonstopmode\input{blue.tex}
> > 
> > 
> > and script.R reads something like:
> > 
> >    id <- readline("Please enter an ID: ")
> >    id
> >    Sweave("blue.Rnw")
> > 
> > I said that script.R didn't run, which was an
> > incorrect description. It runs without prompting
> for
> > the ID, and gives error messages all through
> because
> > blue.Rnw needs the id.
> > 
> > This is a very simplified version of what I'm
> doing,
> > but if I use only the first line of the batch file
> and
> > the first two lines of the script and could get
> that
> > to work, I could figure out the rest.
> 
> It won't work so simply.  You're redirecting stdin,
> so user input would 
> be taken from there; you're redirecting stdout and
> stderr, so the prompt 
> won't be visible to the user.
> 
> You need to open new handles to the console.  The
> code below will do it 
> in Windows; the syntax to specify the console in
> Unix-alikes will be 
> different (but I don't know what it is).
> 
> conout <- file('CONOUT$','w')
> conin <- file('CONIN$', 'r')
> cat('Please enter an ID:', file=conout)
> flush(conout)
> id <- readLines(conin, 1)
> print(id)
> 
> Duncan Murdoch
>



From moritz.marienfeld at arcor.de  Thu Nov  3 11:46:28 2005
From: moritz.marienfeld at arcor.de (moritz.marienfeld@arcor.de)
Date: Thu, 3 Nov 2005 11:46:28 +0100 (CET)
Subject: [R] R save very huge matrices in files
Message-ID: <12070043.1131014788452.JavaMail.ngmail@webmail-04.arcor-online.net>


I have to work with really huge matrices (about 1000*1000 or more). And I want to save those matrices in some file on my computer.
I tried to do so by using the command

write.tabe(SMatrix,file="C:/Programme/rw1061/SMatrix.txt",sep=" ",quote=FALSE,row.names=FALSE,col.names=FALSE)

SMatrix is the matrix I want as a file.

Unfortunately this does not work. Error message:

Error: cannot allocate vector of size 32665 Kb
In addition: Warning message: 
Reached total allocation of 255Mb: see help(memory.size) 

Is there any command, which could help? What can I to in order to save SMatrix?

Moritz Marienfeld



From murdoch at stats.uwo.ca  Thu Nov  3 12:37:09 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 03 Nov 2005 06:37:09 -0500
Subject: [R] readline() and Rterm in Windows
In-Reply-To: <20051103083509.85187.qmail@web60221.mail.yahoo.com>
References: <20051103083509.85187.qmail@web60221.mail.yahoo.com>
Message-ID: <4369F665.5070800@stats.uwo.ca>

Mikkel Grum wrote:
> I've tried your proposal in a number of ways, and
> there must be something I'm not understanding. If I
> run your script (using source() in RGui, or ctrl-R
> from the R Editor, I get:

It requires a command line console, i.e. it will only work in Rterm, not 
Rgui.  I was assuming you'd run it using the style of your batch file 
down below, but without changing the paths.

Duncan Murdoch
> 
> 
>>conout <- file('CONOUT$','w')
> 
> Error in file("CONOUT$", "w") : unable to open
> connection
> In addition: Warning message:
> cannot open file 'CONOUT$', reason 'Permission denied'
> 
> 
> 
> so I added the path as in:
> 
> conout <- file('C:\\R\\R-2.2.0\\CONOUT$','w')
> conin <- file('C:\\R\\R-2.2.0\\CONIN$', 'r')
> cat('Please enter an ID:', file=conout)
> flush(conout)
> id <- readLines(conin, 1)
> print(id)
> 
> 
> Using RGui and ctrl-R from the R Editor, I get
> 
> 
>>conout <- file('C:\\R\\R-2.2.0\\CONOUT$','w')
>>conin <- file('C:\\R\\R-2.2.0\\CONIN$', 'r')
> 
> Error in file("C:\\R\\R-2.2.0\\CONIN$", "r") : 
>         unable to open connection
> In addition: Warning message:
> cannot open file 'C:\R\R-2.2.0\CONIN$', reason 'No
> such file or directory' 
> 
>>cat('Please enter an ID:', file=conout)
>>flush(conout)
>>id <- readLines(conin, 1)
> 
> Error in readLines(conin, 1) : object "conin" not
> found
> 
> and with
> 
>>source("foo.R")
> 
> Error in file("C:\\R\\R-2.2.0\\CONIN$", "r") : 
>         unable to open connection
> In addition: Warning message:
> cannot open file 'C:\R\R-2.2.0\CONIN$', reason 'No
> such file or directory' 
> 
> 
> When I create a batch file with the following command
> :
> C:\R\R-2.2.0\bin\Rterm.exe --vanilla
> <C:\R\R-2.2.0\foo.R> C:\R\R-2.2.0\foo.out
> 
> and double click on the batch file, the out file gives
> me:
> 
> R : Copyright 2005, The R Foundation for Statistical
> Computing
> Version 2.2.0  (2005-10-06 r35749)
> ISBN 3-900051-07-0
> . . .
> Type 'q()' to quit R.
> 
> 
>>conout <- file('C:\\R\\R-2.2.0\\CONOUT$','w')
>>conin <- file('C:\\R\\R-2.2.0\\CONIN$', 'r')
> 
> 
> and nothing else. In none of the situations do I get
> prompted for input. What am I doing hopelessly wrong?
> 
> Mikkel
> 
> --- Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> 
> 
>>Mikkel Grum wrote:
>>
>>>Duncan and Prof, thanks for your comments and
>>>apologies for not being more specific. I'm not
>>
>>getting
>>
>>>the same results you get from the steps you
>>
>>propose.
>>
>>>If I write a script foo.R with two lines
>>>
>>>	id <- readline("Please enter an ID: ")
>>>	id
>>>
>>>and then use source("foo.R") (either at the Rterm
>>>prompt, or in RGui) it is true that get prompted,
>>
>>but
>>
>>>the second line does not visibly run, i.e. I get
>>>
>>>
>>>>source("id.r")
>>>
>>>Please enter an ID: 5
>>>
>>>
>>>and if I then type id, I get
>>>
>>>
>>>>id
>>>
>>>[1] "id"
>>>
>>>If I cut and paste the two lines in RGui (in one
>>
>>go),
>>
>>>I get
>>>
>>>
>>>>id <- readline("Please enter an ID: ")
>>>
>>>Please enter an ID: id
>>>
>>>
>>>What I really want is a batch file on the desktop
>>
>>with
>>
>>>the following commands:
>>>
>>>   c:\r\R-2.2.0\bin\Rterm.exe --no-save
>>
>>--no-restore
>>
>>><script.R> script.out 2>&1
>>>   c:\texmf\miktex\bin\latex
>>>\nonstopmode\input{blue.tex}
>>>
>>>
>>>and script.R reads something like:
>>>
>>>   id <- readline("Please enter an ID: ")
>>>   id
>>>   Sweave("blue.Rnw")
>>>
>>>I said that script.R didn't run, which was an
>>>incorrect description. It runs without prompting
>>
>>for
>>
>>>the ID, and gives error messages all through
>>
>>because
>>
>>>blue.Rnw needs the id.
>>>
>>>This is a very simplified version of what I'm
>>
>>doing,
>>
>>>but if I use only the first line of the batch file
>>
>>and
>>
>>>the first two lines of the script and could get
>>
>>that
>>
>>>to work, I could figure out the rest.
>>
>>It won't work so simply.  You're redirecting stdin,
>>so user input would 
>>be taken from there; you're redirecting stdout and
>>stderr, so the prompt 
>>won't be visible to the user.
>>
>>You need to open new handles to the console.  The
>>code below will do it 
>>in Windows; the syntax to specify the console in
>>Unix-alikes will be 
>>different (but I don't know what it is).
>>
>>conout <- file('CONOUT$','w')
>>conin <- file('CONIN$', 'r')
>>cat('Please enter an ID:', file=conout)
>>flush(conout)
>>id <- readLines(conin, 1)
>>print(id)
>>
>>Duncan Murdoch
>>
> 
> 
> 
> 
> 	
> 		
> __________________________________ 
> Yahoo! Mail - PC Magazine Editors' Choice 2005 
> http://mail.yahoo.com



From petr.pikal at precheza.cz  Thu Nov  3 13:17:58 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 03 Nov 2005 13:17:58 +0100
Subject: [R] RODBC and Excel:  Wrong Data Type Assumed on Import
In-Reply-To: <dkb1h2$620$1@sea.gmane.org>
Message-ID: <436A0E06.18872.14BA52F@localhost>

Hi

As I now exclusively use copy paste method to transfer data from 
Excel to R I tried it and I got correctly a factor column when there 
were some non numeric data in Excel.

Ctrl-C in Excel
mydf<-read.delim("clipboard") in R


Are you sure that a respective column in Excel has values 275a and 
275b in it? 

If yes I had tried to define colClasses vector for your columns.

HTH
Petr


On 2 Nov 2005 at 12:45, Earl F. Glynn wrote:

To:             	r-help at stat.math.ethz.ch
From:           	"Earl F. Glynn" <efg at stowers-institute.org>
Date sent:      	Wed, 2 Nov 2005 12:45:53 -0600
Subject:        	[R] RODBC and Excel:  Wrong Data Type Assumed on Import

> The first column in my Excel sheet has mostly numbers but I need to
> treat it as character data:
> 
> > library(RODBC)
> > channel <- odbcConnectExcel("U:/efg/lab/R/Plasmid/construct
> > list.xls") plasmid <- sqlFetch(channel,"Sheet1", as.is=TRUE)
> > odbcClose(channel)
> 
> > names(plasmid)
> [1] "Plasmid Number" "Plasmid"        "Concentration"  "Comments"
> "Lost"
> 
> # How is the type decided?  I need a character type.
> > class(plasmid$"Plasmid Number")
> [1] "numeric"
> > typeof(plasmid$"Plasmid Number")
> [1] "double"
> 
> > plasmid$"Plasmid Number"[273:276]
> [1] 274  NA  NA 276
> 
> The two NAs are supposed to be 275a and 275b.  I tried the
> "as.is=TRUE" but that didn't help.
> 
> I consulted Section 4, Relational databases, in the R Data
> Import/Export document (for Version 2.2.0).
> 
> Section 4.2.2, Data types, was not helpful.  In particular, this did
> not seem helpful:  "The more comprehensive of the R interface packages
> hide the type conversion issues from the user."
> 
> Section 4.3.2, Package RODBC, provided a "simple example of using ODBC
> .. with a(sic) Excel spreadsheet" but is silent on how to control the
> data type on import.  Could the documentation be expanded to address
> this issue?
> 
> I really need to show "Plasmid 275a" and "Plasmid 275b" instead of
> "Plasmid NA".
> 
> Thanks for any help with this.
> 
> efg
> --
> Earl F. Glynn
> Scientific Programmer
> Bioinformatics Department
> Stowers Institute for Medical Research
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Thu Nov  3 13:25:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Nov 2005 12:25:54 +0000 (GMT)
Subject: [R] R save very huge matrices in files
In-Reply-To: <12070043.1131014788452.JavaMail.ngmail@webmail-04.arcor-online.net>
References: <12070043.1131014788452.JavaMail.ngmail@webmail-04.arcor-online.net>
Message-ID: <Pine.LNX.4.61.0511031223470.447@gannet.stats>

Please read the `R Data Import/Export Manual' and the help page for 
write.table.

Is this really rw1061, that is R 1.6.1?  If so you need to upgrade, as 
write.table is _very_ much more efficient in recent versions.

On Thu, 3 Nov 2005 moritz.marienfeld at arcor.de wrote:

>
> I have to work with really huge matrices (about 1000*1000 or more). And I want to save those matrices in some file on my computer.
> I tried to do so by using the command
>
> write.tabe(SMatrix,file="C:/Programme/rw1061/SMatrix.txt",sep=" ",quote=FALSE,row.names=FALSE,col.names=FALSE)
>
> SMatrix is the matrix I want as a file.
>
> Unfortunately this does not work. Error message:
>
> Error: cannot allocate vector of size 32665 Kb
> In addition: Warning message:
> Reached total allocation of 255Mb: see help(memory.size)
>
> Is there any command, which could help? What can I to in order to save SMatrix?
>
> Moritz Marienfeld
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From a.trapletti at swissonline.ch  Thu Nov  3 13:44:46 2005
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Thu, 03 Nov 2005 13:44:46 +0100
Subject: [R] Bug report on get.hist.quote
Message-ID: <436A063E.8000701@swissonline.ch>

>
>
>get.hist.quote(instrument="INR/USD", provider="oanda", start="2005-10-20")
>  
>
>trying URL 'http://www.oanda.com/convert/fxhistory?lang=en&date1=10%2F20%2F2005&date=11%2F01%2F2005&date_fmt=us&exch=INR&exch2=&expr=USD&expr2=&margin_fixed=0&&SUBMIT=Get+Table&format=ASCII&redirected=1'
>Content type 'text/html' length unknown
>opened URL
>.......... ...
>downloaded 13Kb
>
>2005-10-20 2005-10-21 2005-10-22 2005-10-23 2005-10-24 2005-10-25 2005-10-26 
>   0.02220    0.02218    0.02224    0.02224    0.02224    0.02219    0.02226 
>2005-10-27 2005-10-28 2005-10-29 2005-10-30 2005-10-31 2005-11-01 
>   0.02224    0.02225    0.02224    0.02224    0.02224    0.02221 
>
>
>The answer is wrong. What is shown here is USD/INR, not INR/USD.
>
>
>  
>
Ajay,

This is not a bug! The answer is correct. *1 Indian Rupee = 0.02220 US 
Dollar on *2005-10-20 and this is also the standard way currencies are 
quoted. Pls read, e.g.

http://fxtrade.oanda.com/currency_trading/fxbasics.shtml

Best regards
Adrian



From vdemart1 at tin.it  Thu Nov  3 13:56:51 2005
From: vdemart1 at tin.it (Vittorio)
Date: Thu, 3 Nov 2005 13:56:51 +0100 (GMT+01:00)
Subject: [R] Problem installing ROracle package under R
Message-ID: <6140980.1131022611391.JavaMail.root@pswm18.cp.tin.it>

Context: Pentium 4 with FreeBSD 5.4 and R 2.2.0

I'm trying to install 
the package ROracle under R.
To start with I installed the oracle8-
client from the ports and referred to it via the variable $HOME_ORACLE 
as /usr/local/oracle8-client. Then I started R. After issuing install.
packages("ROracle") R downloaded the needed package and started to 
compile it but complained:
...............................
downloaded 
138Kb

* Installing *source* package 'ROracle' ...
creating cache .
/config.cache
checking how to run the C preprocessor... cc -E
ROracle 
configuration warning:
Oracle pre-compiler proc not in 
/usr/local/oracle8-client/bin/proc
you may not be able to compile 
ROracle

"/tmp/oraLibs2442.mk", line 2: Could not find 
/usr/local/oracle8-client/precomp/lib/env_precomp.mk
make: fatal errors 
encountered -- cannot continue
updating cache ./config.cache
creating .
/config.status
creating src/Makevars
creating src/Makefile
** libs
R 
CMD COMPILE RS-DBI.c
cc -I/usr/local/lib/R/include  -
I/usr/local/include -D__NO_MATH_INLINES  -fPIC  -O -pipe -
march=pentium4 -c RS-DBI.c -o RS-DBI.o
proc CODE=ANSI_C MODE=ORACLE 
INCLUDE=/usr/local/lib/R/include  PARSE=NONE LINES=false RS-Oracle.pc
proc: not found
*** Error code 127

Stop in /tmp/R.INSTALL.
C0fZy6/ROracle/src.
ERROR: compilation failed for package 'ROracle'
** 
Removing '/usr/local/lib/R/library/ROracle'

The downloaded packages 
are in
        /tmp/RtmpRLVjrq/downloaded_packages
Warning message:
installation of package 'ROracle' had non-zero exit status in: install.
packages("ROracle")
...........................................

So 
various directories and relating programs are missing! What else should 
I install to make ROracle work?

Ciao
Vittorio



From palvarez7777 at yahoo.es  Thu Nov  3 14:01:49 2005
From: palvarez7777 at yahoo.es (Alvarez Pedro)
Date: Thu, 3 Nov 2005 14:01:49 +0100 (CET)
Subject: [R] quadratic form
Message-ID: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>

On page 22 of the R-introduction guide it's written:

the quadratic form x^{'} A^{-1} x which is used in
multivariate computations, should be computed by
something like x%*%solve(A,x), rather than computing
the inverse of A.

Why isn't it good to compute t(x) %*% solve(A) %*% x?

Thanks a lot for help!



From gavin.simpson at ucl.ac.uk  Thu Nov  3 14:08:18 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 03 Nov 2005 13:08:18 +0000
Subject: [R] merging dataframes
Message-ID: <1131023298.25175.18.camel@gsimpson.geog.ucl.ac.uk>

Dear List,

I often have to merge two or more data frames containing unique row
names but with some columns (names) common to the two data frames and
some columns not common. This toy example will explain the kind of setup
I am talking about:

mat1 <- as.data.frame(matrix(rnorm(20), nrow = 5))
mat2 <- as.data.frame(matrix(rnorm(20), nrow = 4))
rownames(mat1) <- paste("site", 1:5, sep="")
rownames(mat2) <- paste("site", 6:9, sep="")
names(mat1) <- paste("species", c(1,3,5,7), sep="")
names(mat2) <- paste("species", c(2,3,4,7,9), sep="")
mat1
mat2

So sites (rows) are unique across both data frames, but there are only 7
unique species (columns):

unique(c(names(mat1), names(mat2)))

merge(mat1, mat2, all = TRUE)

gives almost what I want, but it drops or looses the rownames()
information from the two merged data frames, and it re-orders the rows
so that one simply cannot write back the correct row names.

How might I go about merging two data frames as I have described, but
preserve the row names and more importantly, keep the order of rows the
same, so that rows from mat1 come before the rows of mat2?

Many thanks,

Gavin
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From andy_liaw at merck.com  Thu Nov  3 14:08:15 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 3 Nov 2005 08:08:15 -0500
Subject: [R] ML optimization question--unidimensional unfolding scalin g
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED55D@usctmx1106.merck.com>

Alternatively, just type debug(optim) before using it, then step through it
by hitting enter repeatedly...

When you're done, do undebug(optim).

Andy

> From: Spencer Graves
> 
> 	  Have you looked at the code for "optim"?  If you 
> execute "optim", it 
> will list the code.  You can copy that into a script file and walk 
> through it line by line to figure out what it does.  By doing 
> this, you 
> should be able to find a place in the iteration where you can 
> test both 
> branches of each bifurcation and pick one -- or keep a list 
> of however 
> many you want and follow them all more or less 
> simultaneously, pruning 
> the ones that seem too implausible.  Then you can alternate between a 
> piece of the "optim" code, bifurcating and pruning, adjusting 
> each and 
> printing intermediate progress reports to help you understand 
> what it's 
> doing and how you might want to modify it.
> 
> 	  With a bit more effort, you can get the official 
> source code with 
> comments.  To do that, I think you go to "www.r-project.org" 
> -> CRAN -> 
> (select a local mirror) -> "Software:  R sources".  From there, just 
> download "The latest release:  R-2.2.0.tar.gz".
> 
> 	  For more detailed help, I suggest you try to think of 
> the simplest 
> possible toy problem that still contains one of the issues 
> you find most 
> difficult.  Then send that to this list.  If readers can copy a few 
> lines of R code from your email into R and try a couple of things in 
> less than a minute, I think you might get more useful replies quicker.
> 
> 	  Best Wishes,
> 	  Spencer Graves
> 
> Peter Muhlberger wrote:
> 
> > Hi Spencer:  Thanks for your interest!  Also, the posting 
> guide was helpful.
> > 
> > I think my problem might be solved if I could find a way to 
> terminate nlm or
> > optim runs from within the user-given minimization function 
> they call.
> > Optimization is unconstrained.
> > 
> > I'm essentially using normal like curves that translate 
> observed values on a
> > set of variables (one curve per variable) into latent 
> unfolded values.  The
> > observed values are on the Y-axis & the latent (hence 
> parameters to be
> > estimated) are on the X-axis.  The problem is that there 
> are two points into
> > which an observed value can map on a curve--one on either 
> side of the curve
> > mean.  Only one of these values actually will be optimal 
> for all observed
> > variables, but it's easy to show that most estimation 
> methods will get stuck
> > on the non-optimal value if they find that one first.  
> Moving away from that
> > point, the likelihood gets a whole lot worse before the 
> routine will 'see'
> > the optimal point on the other side of the normal curve.
> > 
> > SANN might work, but I kind of wonder how useful it'd be in 
> estimating
> > hundreds of parameters--thanks to that latent scale.
> > 
> > My (possibly harebrained) thought for how to estimate this 
> unfolding using
> > some gradient-based method would be to run through some 
> iterations and then
> > check to see whether a better solution exists on the 'other 
> side' of the
> > normal curves.  If it does, replace those parameters with 
> the better ones.
> > Because this causes the likelihood to jump, I'd probably 
> have to start the
> > estimation process over again (maybe).  But, I see no way 
> from within the
> > minimization function called by NLM or optim to tell NLM or optim to
> > terminate its current run.  I could make the algorithm 
> recursive, but that
> > eats up resources & will probably have to be terminated w/ an error.
> > 
> > Peter
> > 
> > 
> > On 10/11/05 11:11 PM, "Spencer Graves" 
> <spencer.graves at pdf.com> wrote:
> > 
> > 
> >> There may be a few problems where ML (or more generally 
> Bayes) fails
> >>to give sensible answers, but they are relatively rare.
> >>
> >> What is your likelihood?  How many parameters are you trying to
> >>estimate?
> >>
> >> Are you using constrained or unconstrained optimization?  If
> >>constrained, I suggest you remove the constraints by appropriate
> >>transformation.  When considering alternative transformations, I
> >>consider (a) what makes physical sense, and (b) which transformation
> >>produces a log likelihood that is more close to being parabolic.
> >>
> >> Hou are you calling "optim"?  Have you tried all "SANN" as well as
> >>"Nelder-Mead", "BFGS", and "CG"?  If you are using constrained
> >>optimization, I suggest you move the constraints to Inf by 
> appropriate
> >>transformation and use the other methods, as I just suggested.
> >>
> >> If you would still like more suggestions from this group, please
> >>provide more detail -- but as tersely as possible.  The 
> posting guide
> >>is, I believe, quite useful (www.R-project.org/posting-guide.html).
> >>
> >> spencer graves
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> -- 
> Spencer 
> Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From HDoran at air.org  Thu Nov  3 14:12:58 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 3 Nov 2005 08:12:58 -0500
Subject: [R] quadratic form
Message-ID: <F5ED48890E2ACB468D0F3A64989D335ACDC975@dc1ex3.air.org>

Pedro:

Solving the equation in this fashion can be computationally slow and
unstable. There is R an article in R News that answers this question
directly and is a pretty easy read with good examples. Check it out at
the following link:

http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alvarez Pedro
Sent: Thursday, November 03, 2005 8:02 AM
To: r-help at stat.math.ethz.ch
Subject: [R] quadratic form

On page 22 of the R-introduction guide it's written:

the quadratic form x^{'} A^{-1} x which is used in multivariate
computations, should be computed by something like x%*%solve(A,x),
rather than computing the inverse of A.

Why isn't it good to compute t(x) %*% solve(A) %*% x?

Thanks a lot for help!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From hagemann1 at egs.uct.ac.za  Thu Nov  3 14:15:31 2005
From: hagemann1 at egs.uct.ac.za (Kilian Hagemann)
Date: Thu, 3 Nov 2005 15:15:31 +0200
Subject: [R] How to calculate errors in histogram values
Message-ID: <200511031515.31353.hagemann1@egs.uct.ac.za>

Hi there,

I'm new to R but I thought this is the most likely place I could get advice or 
hints w.r.t the following problem:

I have a series of measurements xi with associated uncertainties dxi. I would 
like to construct the probability density histogram of this data where each 
density estimate has an associated error that is derived from the dxi. In 
other words, for large dxi the histogram should also display large 
uncertainties and vice versa. I need this for a curve fitting algorithm.

I have seen many crude ways of working out the error in each bin based on the 
bin count alone, but that's obviously independent of the dxi and thus not 
what I'm after.

So,

1) Is there an R package that can do this (there's nothing in the refence of 
2.1.1)? If so, what algorithm does it use?

2) Could anybody please point me in the right direction (papers, books, 
websites etc.)

Thanks,

-- 
Kilian Hagemann

Climate Systems Analysis Group
University of Cape Town
Republic of South Africa
Tel(w): ++27 21 650 2748



From ligges at statistik.uni-dortmund.de  Thu Nov  3 14:29:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 03 Nov 2005 14:29:37 +0100
Subject: [R] quadratic form
In-Reply-To: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>
References: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>
Message-ID: <436A10C1.4040005@statistik.uni-dortmund.de>

Alvarez Pedro wrote:

> On page 22 of the R-introduction guide it's written:
> 
> the quadratic form x^{'} A^{-1} x which is used in
> multivariate computations, should be computed by
> something like x%*%solve(A,x), rather than computing
> the inverse of A.
> 
> Why isn't it good to compute t(x) %*% solve(A) %*% x?
> 
> Thanks a lot for help!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

See

Bates, D. (2004): Least Squares Calculations in R. R News 4(1), 17-20, 
http://CRAN.R-project.org/doc/Rnews/

Uwe Ligges



From Roger.Bivand at nhh.no  Thu Nov  3 14:28:36 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 3 Nov 2005 14:28:36 +0100 (CET)
Subject: [R] merging dataframes
In-Reply-To: <1131023298.25175.18.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0511031423520.21407-100000@reclus.nhh.no>

On Thu, 3 Nov 2005, Gavin Simpson wrote:

> Dear List,
> 
> I often have to merge two or more data frames containing unique row
> names but with some columns (names) common to the two data frames and
> some columns not common. This toy example will explain the kind of setup
> I am talking about:
> 
> mat1 <- as.data.frame(matrix(rnorm(20), nrow = 5))
> mat2 <- as.data.frame(matrix(rnorm(20), nrow = 4))
> rownames(mat1) <- paste("site", 1:5, sep="")
> rownames(mat2) <- paste("site", 6:9, sep="")
> names(mat1) <- paste("species", c(1,3,5,7), sep="")
> names(mat2) <- paste("species", c(2,3,4,7,9), sep="")
> mat1
> mat2
> 
> So sites (rows) are unique across both data frames, but there are only 7
> unique species (columns):
> 
> unique(c(names(mat1), names(mat2)))
> 
> merge(mat1, mat2, all = TRUE)
> 
> gives almost what I want, but it drops or looses the rownames()
> information from the two merged data frames, and it re-orders the rows
> so that one simply cannot write back the correct row names.
> 
> How might I go about merging two data frames as I have described, but
> preserve the row names and more importantly, keep the order of rows the
> same, so that rows from mat1 come before the rows of mat2?

merge(mat1, mat2, all = TRUE, sort=FALSE)

seems to fix the second question. The first is mentioned tangentially in 
the help page details if you are merging on row names, which you are not - 
maybe prepend to both a column called sites:

mat1a <- data.frame(sites=row.names(mat1), mat1)
mat2a <- data.frame(sites=row.names(mat2), mat2)
data.frame(merge(mat1a, mat2a, all = TRUE, sort=FALSE), row.names="sites")

is a bit long-winded, but gets you there.

Roger

> 
> Many thanks,
> 
> Gavin
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From dimitris.rizopoulos at med.kuleuven.be  Thu Nov  3 14:33:36 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 3 Nov 2005 14:33:36 +0100
Subject: [R] merging dataframes
References: <1131023298.25175.18.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <003101c5e07b$31889100$0540210a@www.domain>

you could use something like:

mat1$id1 <- 1:nrow(mat1)
mat2$id2 <- 1:nrow(mat2)

out <- merge(mat1, mat2, all = TRUE)
out[order(out$id1, out$id2), ]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Gavin Simpson" <gavin.simpson at ucl.ac.uk>
To: "R-help" <R-help at stat.math.ethz.ch>
Sent: Thursday, November 03, 2005 2:08 PM
Subject: [R] merging dataframes


> Dear List,
>
> I often have to merge two or more data frames containing unique row
> names but with some columns (names) common to the two data frames 
> and
> some columns not common. This toy example will explain the kind of 
> setup
> I am talking about:
>
> mat1 <- as.data.frame(matrix(rnorm(20), nrow = 5))
> mat2 <- as.data.frame(matrix(rnorm(20), nrow = 4))
> rownames(mat1) <- paste("site", 1:5, sep="")
> rownames(mat2) <- paste("site", 6:9, sep="")
> names(mat1) <- paste("species", c(1,3,5,7), sep="")
> names(mat2) <- paste("species", c(2,3,4,7,9), sep="")
> mat1
> mat2
>
> So sites (rows) are unique across both data frames, but there are 
> only 7
> unique species (columns):
>
> unique(c(names(mat1), names(mat2)))
>
> merge(mat1, mat2, all = TRUE)
>
> gives almost what I want, but it drops or looses the rownames()
> information from the two merged data frames, and it re-orders the 
> rows
> so that one simply cannot write back the correct row names.
>
> How might I go about merging two data frames as I have described, 
> but
> preserve the row names and more importantly, keep the order of rows 
> the
> same, so that rows from mat1 come before the rows of mat2?
>
> Many thanks,
>
> Gavin
> -- 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> Gavin Simpson                     [T] +44 (0)20 7679 5522
> ENSIS Research Fellow             [F] +44 (0)20 7679 7565
> ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
> UCL Department of Geography       [W] 
> http://www.ucl.ac.uk/~ucfagls/cv/
> 26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
> London.  WC1H 0AP.
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From andy_liaw at merck.com  Thu Nov  3 14:33:46 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 3 Nov 2005 08:33:46 -0500
Subject: [R] merging dataframes
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED560@usctmx1106.merck.com>

The `Value' section of ?merge does say that `... in all cases the result has
no special row names', so you're left to handle that on your own.  One
possibility is to use

  result <- merge(mat1, mat2, all=TRUE, sort=FALSE)

so that the sorting is not done, then you can just do

  rownames(result) <- c(rownames(mat1), rownames(mat2))

Cheers,
Andy


> From: Gavin Simpson
> 
> Dear List,
> 
> I often have to merge two or more data frames containing unique row
> names but with some columns (names) common to the two data frames and
> some columns not common. This toy example will explain the 
> kind of setup
> I am talking about:
> 
> mat1 <- as.data.frame(matrix(rnorm(20), nrow = 5))
> mat2 <- as.data.frame(matrix(rnorm(20), nrow = 4))
> rownames(mat1) <- paste("site", 1:5, sep="")
> rownames(mat2) <- paste("site", 6:9, sep="")
> names(mat1) <- paste("species", c(1,3,5,7), sep="")
> names(mat2) <- paste("species", c(2,3,4,7,9), sep="")
> mat1
> mat2
> 
> So sites (rows) are unique across both data frames, but there 
> are only 7
> unique species (columns):
> 
> unique(c(names(mat1), names(mat2)))
> 
> merge(mat1, mat2, all = TRUE)
> 
> gives almost what I want, but it drops or looses the rownames()
> information from the two merged data frames, and it re-orders the rows
> so that one simply cannot write back the correct row names.
> 
> How might I go about merging two data frames as I have described, but
> preserve the row names and more importantly, keep the order 
> of rows the
> same, so that rows from mat1 come before the rows of mat2?
> 
> Many thanks,
> 
> Gavin
> -- 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~
> %~%~%~%~%
> Gavin Simpson                     [T] +44 (0)20 7679 5522
> ENSIS Research Fellow             [F] +44 (0)20 7679 7565
> ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
> UCL Department of Geography       [W] 
> http://www.ucl.ac.uk/~ucfagls/cv/
> 26 Bedford Way              
>       [W] http://www.ucl.ac.uk/~ucfagls/
> London.  WC1H 0AP.
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~
> %~%~%~%~%
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Thu Nov  3 14:35:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Nov 2005 13:35:03 +0000 (GMT)
Subject: [R] quadratic form
In-Reply-To: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>
References: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0511031330230.9940@gannet.stats>

On Thu, 3 Nov 2005, Alvarez Pedro wrote:

> On page 22 of the R-introduction guide it's written:
>
> the quadratic form x^{'} A^{-1} x which is used in
> multivariate computations, should be computed by
> something like x%*%solve(A,x), rather than computing
> the inverse of A.
>
> Why isn't it good to compute t(x) %*% solve(A) %*% x?

The answer is only two lines above;

   Numerically, it is both inefficient and potentially unstable to compute
   @code{x <- solve(A) %*% b} instead of @code{solve(A,b)}.

See also the footnote.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mi2kelgrum at yahoo.com  Thu Nov  3 14:35:09 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Thu, 3 Nov 2005 05:35:09 -0800 (PST)
Subject: [R] readline() and Rterm in Windows
In-Reply-To: <4369F665.5070800@stats.uwo.ca>
Message-ID: <20051103133509.53118.qmail@web60216.mail.yahoo.com>

And that was the only combination I didn't try, duhh.
As you say, it works. Excellent! TX! They'll be a
number of pleased usres too.

--- Duncan Murdoch <murdoch at stats.uwo.ca> wrote:

> Mikkel Grum wrote:
> > I've tried your proposal in a number of ways, and
> > there must be something I'm not understanding. If
> I
> > run your script (using source() in RGui, or ctrl-R
> > from the R Editor, I get:
> 
> It requires a command line console, i.e. it will
> only work in Rterm, not 
> Rgui.  I was assuming you'd run it using the style
> of your batch file 
> down below, but without changing the paths.
> 
> Duncan Murdoch
> > 
> > 
> >>conout <- file('CONOUT$','w')
> > 
> > Error in file("CONOUT$", "w") : unable to open
> > connection
> > In addition: Warning message:
> > cannot open file 'CONOUT$', reason 'Permission
> denied'
> > 
> > 
> > 
> > so I added the path as in:
> > 
> > conout <- file('C:\\R\\R-2.2.0\\CONOUT$','w')
> > conin <- file('C:\\R\\R-2.2.0\\CONIN$', 'r')
> > cat('Please enter an ID:', file=conout)
> > flush(conout)
> > id <- readLines(conin, 1)
> > print(id)
> > 
> > 
> > Using RGui and ctrl-R from the R Editor, I get
> > 
> > 
> >>conout <- file('C:\\R\\R-2.2.0\\CONOUT$','w')
> >>conin <- file('C:\\R\\R-2.2.0\\CONIN$', 'r')
> > 
> > Error in file("C:\\R\\R-2.2.0\\CONIN$", "r") : 
> >         unable to open connection
> > In addition: Warning message:
> > cannot open file 'C:\R\R-2.2.0\CONIN$', reason 'No
> > such file or directory' 
> > 
> >>cat('Please enter an ID:', file=conout)
> >>flush(conout)
> >>id <- readLines(conin, 1)
> > 
> > Error in readLines(conin, 1) : object "conin" not
> > found
> > 
> > and with
> > 
> >>source("foo.R")
> > 
> > Error in file("C:\\R\\R-2.2.0\\CONIN$", "r") : 
> >         unable to open connection
> > In addition: Warning message:
> > cannot open file 'C:\R\R-2.2.0\CONIN$', reason 'No
> > such file or directory' 
> > 
> > 
> > When I create a batch file with the following
> command
> > :
> > C:\R\R-2.2.0\bin\Rterm.exe --vanilla
> > <C:\R\R-2.2.0\foo.R> C:\R\R-2.2.0\foo.out
> > 
> > and double click on the batch file, the out file
> gives
> > me:
> > 
> > R : Copyright 2005, The R Foundation for
> Statistical
> > Computing
> > Version 2.2.0  (2005-10-06 r35749)
> > ISBN 3-900051-07-0
> > . . .
> > Type 'q()' to quit R.
> > 
> > 
> >>conout <- file('C:\\R\\R-2.2.0\\CONOUT$','w')
> >>conin <- file('C:\\R\\R-2.2.0\\CONIN$', 'r')
> > 
> > 
> > and nothing else. In none of the situations do I
> get
> > prompted for input. What am I doing hopelessly
> wrong?
> > 
> > Mikkel
> > 
> > --- Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > 
> > 
> >>Mikkel Grum wrote:
> >>
> >>>Duncan and Prof, thanks for your comments and
> >>>apologies for not being more specific. I'm not
> >>
> >>getting
> >>
> >>>the same results you get from the steps you
> >>
> >>propose.
> >>
> >>>If I write a script foo.R with two lines
> >>>
> >>>	id <- readline("Please enter an ID: ")
> >>>	id
> >>>
> >>>and then use source("foo.R") (either at the Rterm
> >>>prompt, or in RGui) it is true that get prompted,
> >>
> >>but
> >>
> >>>the second line does not visibly run, i.e. I get
> >>>
> >>>
> >>>>source("id.r")
> >>>
> >>>Please enter an ID: 5
> >>>
> >>>
> >>>and if I then type id, I get
> >>>
> >>>
> >>>>id
> >>>
> >>>[1] "id"
> >>>
> >>>If I cut and paste the two lines in RGui (in one
> >>
> >>go),
> >>
> >>>I get
> >>>
> >>>
> >>>>id <- readline("Please enter an ID: ")
> >>>
> >>>Please enter an ID: id
> >>>
> >>>
> >>>What I really want is a batch file on the desktop
> >>
> >>with
> >>
> >>>the following commands:
> >>>
> >>>   c:\r\R-2.2.0\bin\Rterm.exe --no-save
> >>
> >>--no-restore
> >>
> >>><script.R> script.out 2>&1
> >>>   c:\texmf\miktex\bin\latex
> >>>\nonstopmode\input{blue.tex}
> >>>
> >>>
> >>>and script.R reads something like:
> >>>
> >>>   id <- readline("Please enter an ID: ")
> >>>   id
> >>>   Sweave("blue.Rnw")
> >>>
> >>>I said that script.R didn't run, which was an
> >>>incorrect description. It runs without prompting
> >>
> >>for
> >>
> >>>the ID, and gives error messages all through
> >>
> >>because
> >>
> >>>blue.Rnw needs the id.
> >>>
> >>>This is a very simplified version of what I'm
> >>
> >>doing,
> >>
> >>>but if I use only the first line of the batch
> file
> >>
> >>and
> >>
> >>>the first two lines of the script and could get
> >>
> >>that
> >>
> >>>to work, I could figure out the rest.
> >>
> >>It won't work so simply.  You're redirecting
> stdin,
> >>so user input would 
> >>be taken from there; you're redirecting stdout and
> >>stderr, so the prompt 
> >>won't be visible to the user.
> >>
> 
=== message truncated ===



From ManuelPerera-Chang at fmc-ag.com  Thu Nov  3 14:44:19 2005
From: ManuelPerera-Chang at fmc-ag.com (ManuelPerera-Chang@fmc-ag.com)
Date: Thu, 3 Nov 2005 14:44:19 +0100
Subject: [R]  merging dataframes
Message-ID: <OFB51033D2.24DB71DF-ONC12570AE.004A88E8-C12570AE.004B7857@notes.fresenius.de>





Hi,

what about padding both datasets with dummy missing records ... and then
play with cbind and rbind

... like e.g.

> species5<-c(NA,NA,NA,NA)

> modmat2<-cbind(mat2,species1,species5)

and then similarly with mat1 ...
e.g.
species2<-c(NA,NA,NA,NA,NA)

> modmad1<-cbind(mat1,species2,species4,species9)

> rbind(modmad1,modmat2)
        species1    species3   species5   species7   species2   species4
species9
site1 -0.7190044 -0.52482580 -1.1813567 -1.5584831         NA         NA
NA
site2 -1.1782180  1.72337964  0.1652343 -0.9026087         NA         NA
NA
site3  0.3823015 -0.07226644 -1.2907470 -0.3692091         NA         NA
NA
site4 -1.3051131 -0.61107947  0.6264416  1.5259373         NA         NA
NA
site5  0.2028565 -1.28374638  1.6284780 -1.2975163         NA         NA
NA
site6         NA  1.19088414         NA  0.3159949 -0.1624538  0.5987733
0.2205512
site7         NA  0.75292176         NA  1.7524988  0.8335334 -0.7998774
-0.9788762
site8         NA -0.47803396         NA -1.3041628  1.7925165 -0.4153879
-0.4708165
site9         NA -0.20063523         NA  1.8119115  1.5351801 -1.3334419
0.5812675

it will need modifications of course if you are working with several
datasets

Saludos,

Manuel



From gavin.simpson at ucl.ac.uk  Thu Nov  3 14:46:46 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 03 Nov 2005 13:46:46 +0000
Subject: [R] merging dataframes
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED560@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED560@usctmx1106.merck.com>
Message-ID: <1131025606.25175.22.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2005-11-03 at 08:33 -0500, Liaw, Andy wrote:
> The `Value' section of ?merge does say that `... in all cases the result has
> no special row names', so you're left to handle that on your own.  One
> possibility is to use
> 
>   result <- merge(mat1, mat2, all=TRUE, sort=FALSE)
> 
> so that the sorting is not done, then you can just do
> 
>   rownames(result) <- c(rownames(mat1), rownames(mat2))
> 
> Cheers,
> Andy

Thanks Roger, Andy and Dimitris for your solutions.

I'd missed the default for sort being set to TRUE - must pay more
attention in class. Roger's and Andy's approaches seem like the most
fool-proof way of canning this into a function that does all the I asked
and a bit of tidying up of NA's etc.

Cheers,

G

> 
> > From: Gavin Simpson
> > 
> > Dear List,
> > 
> > I often have to merge two or more data frames containing unique row
> > names but with some columns (names) common to the two data frames and
> > some columns not common. This toy example will explain the 
> > kind of setup
> > I am talking about:
> > 
> > mat1 <- as.data.frame(matrix(rnorm(20), nrow = 5))
> > mat2 <- as.data.frame(matrix(rnorm(20), nrow = 4))
> > rownames(mat1) <- paste("site", 1:5, sep="")
> > rownames(mat2) <- paste("site", 6:9, sep="")
> > names(mat1) <- paste("species", c(1,3,5,7), sep="")
> > names(mat2) <- paste("species", c(2,3,4,7,9), sep="")
> > mat1
> > mat2
> > 
> > So sites (rows) are unique across both data frames, but there 
> > are only 7
> > unique species (columns):
> > 
> > unique(c(names(mat1), names(mat2)))
> > 
> > merge(mat1, mat2, all = TRUE)
> > 
> > gives almost what I want, but it drops or looses the rownames()
> > information from the two merged data frames, and it re-orders the rows
> > so that one simply cannot write back the correct row names.
> > 
> > How might I go about merging two data frames as I have described, but
> > preserve the row names and more importantly, keep the order 
> > of rows the
> > same, so that rows from mat1 come before the rows of mat2?
> > 
> > Many thanks,
> > 
> > Gavin
> > -- 
> > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~
> > %~%~%~%~%
> > Gavin Simpson                     [T] +44 (0)20 7679 5522
> > ENSIS Research Fellow             [F] +44 (0)20 7679 7565
> > ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
> > UCL Department of Geography       [W] 
> > http://www.ucl.ac.uk/~ucfagls/cv/
> > 26 Bedford Way              
> >       [W] http://www.ucl.ac.uk/~ucfagls/
> > London.  WC1H 0AP.
> > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~
> > %~%~%~%~%
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From SoukupM at cder.fda.gov  Thu Nov  3 15:11:40 2005
From: SoukupM at cder.fda.gov (Soukup, Mat)
Date: Thu, 3 Nov 2005 09:11:40 -0500
Subject: [R] Potential for R to conflict with other softwares
Message-ID: <9AB9FB1124563249B0B39A1FEF1E94B37E1B85@cdsx06.cder.fda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051103/53ae1770/attachment.pl

From leog at anicca-vijja.de  Thu Nov  3 15:13:02 2005
From: leog at anicca-vijja.de (=?ISO-8859-1?Q?Leo_G=FCrtler?=)
Date: Thu, 03 Nov 2005 15:13:02 +0100
Subject: [R] problems with pan(): Indizierung ausserhalb der Grenzen =
 subscript out of bounds
Message-ID: <436A1AEE.3000609@anicca-vijja.de>

Dear alltogether,

I tried pan() to impute NAs for longitudinal data.
The terminology in the following output follows the pan manpage. No data 
are attached to this script as this may be too huge.


y = 15 responses
pred = at first just intercept was tried (later on covariates should follow)
subj = 168 different subjects with 4 to 6 observations for each subject 
at time points t1, t2, ..., t6

# extract of y
 > y[1:4,]
    anpr impr kepr   lernpr  lstofpr  nachwela   nachfak    nachw     
sbpr widapr zdompr   zerfpr zgleipr  zstimpr  zugrupr
2   3.50 2.75  3.4 2.222222 2.666667  3.333333 15.000000 5.909091 
2.333333      2    1.5 3.666667       4 3.000000 3.555556
202 2.25 2.50  3.6 2.222222 3.666667 12.000000 13.750000 7.777778 
1.666667      2    2.0 3.333333       4 3.333333 3.555556
402   NA   NA   NA       NA       NA        NA        NA       NA       
NA     NA     NA       NA      NA       NA       NA
602 1.75 2.75  3.4 1.555556 3.333333  2.666667  6.666667 5.000000 
2.000000      2    1.0 3.333333       4 2.666667 3.333333
 > dim(y)
[1] 940  15 # matrix y with 15 responses and 940 obs
# y is ordered according to subj
 > length(subj)
[1] 940     #940 obs of 168 different subjects (persons)
# extract of subj
 > subj[1:30]
 [1] 2 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 5 6 6 6 6 6 6 7 7
# how many observations for each subject
 > table(subj)
subj
  2   3   4   5   6   7   8   9  12  13  14  15  16  17  18  19  20  21  
23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  41  
42  43  44  46  47  48  49
  6   5   5   6   6   6   5   6   6   6   6   6   5   6   6   5   6   
6   6   6   5   6   6   5   6   6   4   6   5   5   6   6   6   5   6   
6   4   6   5   6   5   6   6
 50  52  53  55  57  58  59  60  61  62  64  66  67  68  69  70  72  73  
75  76  77  78  79  81  82  83  84  85  86  87  88  89  90  91  92  93  
94  95  97  98  99 100 101
  6   5   5   5   6   6   5   5   6   6   6   6   4   6   6   5   5   
6   6   5   6   4   6   5   5   4   6   5   6   6   4   5   5   6   6   
6   6   6   6   6   6   6   6
103 104 105 106 107 108 109 110 112 113 114 115 116 117 118 121 122 123 
124 125 126 127 128 129 130 131 132 133 134 135 137 138 139 140 141 142 
144 146 147 148 149 150 151
  5   6   5   6   6   6   5   5   5   6   6   5   5   6   5   6   6   
6   6   6   5   4   6   6   6   6   6   4   4   6   5   4   6   5   4   
6   6   6   6   6   6   5   5
152 153 154 155 156 157 158 159 160 162 164 165 166 167 170 171 173 174 
175 176 178 179 181 182 185 186 187 188 189 190 191 192 193 195 196 197 
198 199 200
  5   6   6   6   5   6   6   6   6   6   6   6   6   4   5   6   6   
6   6   5   6   6   6   6   6   6   6   5   6   6   6   5   6   6   6   
6   6   6   6
 > pred <- cbind(interc=rep(1,dim(y)[1]))     # just intercept (at first)
 > dim(pred)
[1] 940  1
xcol <- 1:dim(pred)[2]
 > xcol
[1] 1
#xcol = 1 , using all number of cols of pred[]
 > zcol <- c(1)    # = 1 , number of cols to use
 > y.ncol <- dim(y)[2]
 > n.zcol <- length(zcol)
 > prior <- list(a=y.ncol,
+              Binv=diag(y.ncol),
+              c=n.zcol,
+              Dinv=diag(n.zcol))
 > prior
$a
[1] 15

$Binv
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] 
[,13] [,14] [,15]
 [1,]    1    0    0    0    0    0    0    0    0     0     0     0     
0     0     0
 [2,]    0    1    0    0    0    0    0    0    0     0     0     0     
0     0     0
 [3,]    0    0    1    0    0    0    0    0    0     0     0     0     
0     0     0
 [4,]    0    0    0    1    0    0    0    0    0     0     0     0     
0     0     0
 [5,]    0    0    0    0    1    0    0    0    0     0     0     0     
0     0     0
 [6,]    0    0    0    0    0    1    0    0    0     0     0     0     
0     0     0
 [7,]    0    0    0    0    0    0    1    0    0     0     0     0     
0     0     0
 [8,]    0    0    0    0    0    0    0    1    0     0     0     0     
0     0     0
 [9,]    0    0    0    0    0    0    0    0    1     0     0     0     
0     0     0
[10,]    0    0    0    0    0    0    0    0    0     1     0     0     
0     0     0
[11,]    0    0    0    0    0    0    0    0    0     0     1     0     
0     0     0
[12,]    0    0    0    0    0    0    0    0    0     0     0     1     
0     0     0
[13,]    0    0    0    0    0    0    0    0    0     0     0     0     
1     0     0
[14,]    0    0    0    0    0    0    0    0    0     0     0     0     
0     1     0
[15,]    0    0    0    0    0    0    0    0    0     0     0     0     
0     0     1

$c
[1] 1

$Dinv
     [,1]
[1,]    1

#prior         a = number of cols in y
#              Binv = identity matrix (ncols = nrows = y)
#              c = length of zcol[]
#              Dinv = identity matrix (ncols = length(nzcol))

Now the error message:

 > pan(y, subj, pred, xcol, zcol, prior, seed=1234,iter=1000)
Fehler: Indizierung au??erhalb der Grenzen"
# error massage = subscript out of bounds

I do not understand that. Thank you very much for every suggestion.

best,

leo



From p.dalgaard at biostat.ku.dk  Thu Nov  3 15:17:15 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Nov 2005 15:17:15 +0100
Subject: [R] quadratic form
In-Reply-To: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>
References: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>
Message-ID: <x2ll05vms4.fsf@viggo.kubism.ku.dk>

Alvarez Pedro <palvarez7777 at yahoo.es> writes:

> On page 22 of the R-introduction guide it's written:
> 
> the quadratic form x^{'} A^{-1} x which is used in
> multivariate computations, should be computed by
> something like x%*%solve(A,x), rather than computing
> the inverse of A.
> 
> Why isn't it good to compute t(x) %*% solve(A) %*% x?

It's just a waste of CPU time. For k x k matrices, solution of Ax=b is
of computational complexity O(k^2) whereas inversion of A is O(k^3).
This is obviously more important for k=1000 than for k=5.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Thu Nov  3 15:30:15 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 03 Nov 2005 09:30:15 -0500
Subject: [R] Potential for R to conflict with other softwares
In-Reply-To: <9AB9FB1124563249B0B39A1FEF1E94B37E1B85@cdsx06.cder.fda.gov>
References: <9AB9FB1124563249B0B39A1FEF1E94B37E1B85@cdsx06.cder.fda.gov>
Message-ID: <436A1EF7.402@stats.uwo.ca>

On 11/3/2005 9:11 AM, Soukup, Mat wrote:
> Hi.
> 
> After some time, my collegues at the Food and Drug Adminstration have
> finally acknowledged R as a powerful statistical computing environment.
> However, in order to comply with the Office of Information and Technology
> standards there are a couple of questions about whether R could interfere
> with other software. As I'm more of a driver of the R software and not a
> mechanic, I was hoping for the insight of the many great useRs. Below is a
> list of 5 proposed questions to which I value any comment.
> 
> Thank you for your time,
> 
> -Mat
> 
> 

These answers are about the Windows version only, but from the 
questions, I think that's what you were looking for.  They apply to all 
versions since 1.6.x at least (though the earlier ones would have put 
fewer entries into the registry, they put them in the same places).

> 1. Does R have high resolution graphics?

Yes, but I don't think I get the point of this question.  How would that 
interfere with other software?
> 
> 2. Does R have .dll files, or other executables which are not located in the
> R software directory tree?

No, it installs everything below R_HOME.
> 
> 3. Does R modify the Windows registry in a non-obvious way, i.e. other than
> defining itself and what extensions to associate with R, and what are those
> extensions?

I think all of its modifications would count as obvious.  They are 
mainly below HKLM/Software/R-core or HKCU/Software/R-core (where the 
file locations are recorded); additionally file associations are set up 
for .Rdata files (which are called RWorkspace files there), and an 
uninstall entry is made.
> 
> 4. Does R add macros to any part of MS Office?

No.
> 
> 5. Can you anticipate any other way in which installing and using R could
> disrupt the operation of another software?

No, not really.  Maybe users will become addicted to it?  ;-)

Duncan Murdoch
>  
> 
> ***********************************************************************
> Mat Soukup, Ph.D.
> Food and Drug Administration
> 10903 New Hampshire Ave. 
> BLDG 22 RM 5329
> Silver Spring, MD 20993-0002
> Phone: 301.796.1005
> ***********************************************************************
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From cg.pettersson at evp.slu.se  Thu Nov  3 16:03:05 2005
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Thu, 03 Nov 2005 16:03:05 +0100
Subject: [R] Problems with abline adding regression line to a graph
Message-ID: <436A26A9.7050603@evp.slu.se>

Hello all,

R2.1.1, W2k

I try to make a plot of a simple regression model in this way:

 > with(njfA_bcd, {
+ plot(TC_OS.G31,Prot,cex = 2, col = "red", xlab= "TC/OS at GS32",
+ ylab="Grain crude protein (CP)")
+ })

This part works well and produces the datapoints as red circles.
When I try to add a line, using a fitted linear model in a way
that works perfect with other variables in the same dateset the
following happens:

 > with(njfA_bcd, {
+    abline(lm(predict(m1tc) ~ TC_OS.G31), lty = 1, col = "red")
+ })
Error in model.frame(formula, rownames, variables, varnames, extras, 
extranames,  :
        variable lengths differ

And this means?

There exists missing values for TC_OS.G31 in the dataset. From the 
beginning
m1tc was a lm() object, which gave the same Error message. To try to fix the
problem I changed to lme() and used na.action=na.omit explicitely, but this
didn??t help.

Here is the summary of m1tc:

 > summary(m1tc)
Linear mixed-effects model fit by REML
 Data: njfA_bcd
       AIC      BIC    logLik
  209.4914 219.0692 -100.7457

Random effects:
 Formula: ~1 | Trial
        (Intercept)  Residual
StdDev:    1.242184 0.6520464

Fixed effects: Prot ~ TC_OS.G31
                Value Std.Error DF   t-value p-value
(Intercept)  14.86209  0.957630 68 15.519662       0
TC_OS.G31   -24.22286  4.792801 68 -5.054008       0
 Correlation:
          (Intr)
TC_OS.G31 -0.935

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max
-1.68329774 -0.73751040 -0.05600477  0.68301243  2.21693174

Number of Observations: 83
Number of Groups: 14
 >

What is happening and what shall I do about it?

Cheers
/CG

-- 
CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences (SLU)
Dept. of Crop Production Ecology. Box 7043.
SE-750 07 UPPSALA, Sweden.
+46 18 671428, +46 70 3306685
cg.pettersson at evp.slu.se



From B.Rowlingson at lancaster.ac.uk  Thu Nov  3 15:47:32 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 03 Nov 2005 14:47:32 +0000
Subject: [R] Rserve/Python
Message-ID: <436A2304.2040306@lancaster.ac.uk>

Has anyone done anything on a Python client for Rserve?

Simon Urbanek (Rserve dev) tells me he heard of some people working on 
it a couple of years ago but nothing came of it. If anyone has done 
anything, or might find it interesting, please get in touch with me.

I know there's also the RSPython package but I'm tied to a particular 
python version and I can't be sure I'll get RSPython working (in Windows 
as well as Linux). At least if I write a Python-Rserve client its my job 
to make it work!

Thanks,

Barry Rowlingson
Maths and Stats
Lancaster University



From andy_liaw at merck.com  Thu Nov  3 15:54:59 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 3 Nov 2005 09:54:59 -0500
Subject: [R] locfit: simultaneous confidence band
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED561@usctmx1106.merck.com>

Apologies for coming to this so late...

Variance is rarely known in real life data.  You should really consult the
book `Local Regression and Likelihood' by Prof. Loader for the details on
simultaneous confidence bands.  `Locfit' is the support software for that
book.

Andy

> From: Michael G??lger
> 
> I'm using the package 'locfit' for nonparametric regression. 
> This package
> contains the function 'scb' to compute simultaneous confidence bands. 
> The variance of the data is unknown. Up to now I compute a fit with
> 'locfit'. Afterwards an estimate of the residual variance is 
> computed by the
> function 'rv'. The weights in the 'scb'-function are set 1/sigma^2 to
> compute the confidence band.
> Is this procedure correct or is there any other way to 
> compute confidence
> bands with unknown variance?
> 
> Thanks very much for any help you can offer. 
> 
> Michael G??lger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Nov  3 16:00:37 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 3 Nov 2005 10:00:37 -0500 
Subject: [R] Search within a file
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F414B@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051103/59cc017e/attachment.pl

From ggrothendieck at gmail.com  Thu Nov  3 16:15:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 3 Nov 2005 10:15:47 -0500
Subject: [R] Search within a file
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F414B@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F414B@us-arlington-0668.mail.saic.com>
Message-ID: <971536df0511030715hec47b45m94121cb1bb077610@mail.gmail.com>

Would this be ok (on Windows, use grep on UNIX):

# line numbers of all lines conitaining R in the R README file
setwd(R.home())
as.numeric(sub(":.*", "", system("findstr /n R README", intern = TRUE)))


On 11/3/05, Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI at saic.com> wrote:
> Hi,
>
> I am looking for a way to search a file for position of some expression,
> from within R. My current code:
>
>  sha1Pos = gregexpr("<sha1>", readChar(filename,
> file.info(filename)$size))[[1]]
>
> Works fine for small files, but text files I will be working with might get
> up to Gb range, so I was trying to accomplish the same without loading the
> whole file into R.
>
> I realize this is not what R is designed to do, but maybe there is some way
> I am missing.
>
> Jarek Tuszynski
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Thu Nov  3 16:23:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 3 Nov 2005 10:23:03 -0500
Subject: [R] npmc package
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED562@usctmx1106.merck.com>

I just downloaded the file from that location, and took a quick look.  At
least the only thing that R CMD check under 2.2.0 only complained about the
file report.Rd.  The problem seems to be 

  \keyword { print }

instead of 

  \keyword{print}

If no one else is aware of any other problems with the package (e.g., ones
that R CMD check can not test for), please let me know.  Otherwise I'll take
over the maintanence and submit it to CRAN.

Andy



> From: Prof Brian Ripley
> 
> On Fri, 21 Oct 2005, Kjetil Holuerson wrote:
> 
> > Martin Maechler wrote:
> >>>>>>> "Carlos" == Carlos Mauricio Cardeal Mendes <mcardeal at ufba.br>
> >>>>>>>     on Wed, 19 Oct 2005 15:11:32 -0300 writes:
> >>
> >>     Carlos> So, is there another package to substitute those
> >>     Carlos> functions described on "ORPHANED" npmc package ?
> >>
> >> May be not.
> >> But nobody stops you from becoming the new maintainer of the
> >
> > Just checked. This package is not  now in the ORPHANES
> > subdirectory, neither in the main CRAN  listing.
> 
> See http://cran.r-project.org/src/contrib/Archive/N/
> 
> The Orphaned (sic) subdirectory applies to packages dropped 
> by the former 
> maintainer which no longer pass R CMD check, and also to 
> those where the 
> CRAN maintainers are able to deduce it has been dropped.  
> Others which 
> just fail without a positive indication of no active 
> maintainer may end in 
> the archive.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From r.hankin at noc.soton.ac.uk  Thu Nov  3 16:25:49 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 3 Nov 2005 15:25:49 +0000
Subject: [R] quadratic form
In-Reply-To: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>
References: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>
Message-ID: <3EBB05D6-62BE-4F08-AD7C-8672F1BAAD93@soc.soton.ac.uk>

Hi Alvarez


If you define

quad.form.inv <-  function (M, x)
{
     drop(crossprod(x, solve(M, x)))
}

then you will avoid an expensive call to %*% as well.


HTH


Robin

On 3 Nov 2005, at 13:01, Alvarez Pedro wrote:

> On page 22 of the R-introduction guide it's written:
>
> the quadratic form x^{'} A^{-1} x which is used in
> multivariate computations, should be computed by
> something like x%*%solve(A,x), rather than computing
> the inverse of A.
>
> Why isn't it good to compute t(x) %*% solve(A) %*% x?
>
> Thanks a lot for help!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From kwright68 at gmail.com  Thu Nov  3 16:26:47 2005
From: kwright68 at gmail.com (Kevin Wright)
Date: Thu, 3 Nov 2005 09:26:47 -0600
Subject: [R] RODBC and Excel: Wrong Data Type Assumed on Import
Message-ID: <adf71a630511030726v10f1dc77sa7c760b96a27480f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051103/f35cfe52/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Thu Nov  3 16:45:38 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 03 Nov 2005 15:45:38 -0000 (GMT)
Subject: [R] How to calculate errors in histogram values
In-Reply-To: <200511031515.31353.hagemann1@egs.uct.ac.za>
Message-ID: <XFMail.051103154538.Ted.Harding@nessie.mcc.ac.uk>

On 03-Nov-05 Kilian Hagemann wrote:
> Hi there,
> 
> I'm new to R but I thought this is the most likely place I
> could get advice or hints w.r.t the following problem:
> 
> I have a series of measurements xi with associated uncertainties dxi.
> I would like to construct the probability density histogram of this
> data where each density estimate has an associated error that is
> derived from the dxi.
> In other words, for large dxi the histogram should also display
> large uncertainties and vice versa. I need this for a curve fitting
> algorithm.
> 
> I have seen many crude ways of working out the error in each bin based
> on the bin count alone, but that's obviously independent of the dxi
> and thus not what I'm after.
> 
> So,
> 
> 1) Is there an R package that can do this (there's nothing in the
> refence of 
> 2.1.1)? If so, what algorithm does it use?
> 
> 2) Could anybody please point me in the right direction (papers, books,
> websites etc.)
> 
> Thanks,
> 
> -- 
> Kilian Hagemann

I don't know about an R package that would deal with this directly,
but I can think of an aproach, not difficult to implement in R,
which may be helpful.

I'm going to assume (at any rate for the time being), that you
are interested in a "per-bin" uncertainty, i.e. that you want
to be able to to answer "For each bin, what is the uncertainty
in the count for this bin, regardless of any other bins?"
I.e. you are ignoring the fact that there is correlation between
bins (what goes into one bin can not go into another).

1. Say you have N observations (i = 1:N). Draw a preliminary
   histogram, and from this decide on a good set of fixed breaks.

2. Extend this list by a few bins on either side (you may have
   to return to this point, depending on the outcome of later
   stages). Say this gives you K bins (j = 1:K).

3. For each data-point xi, with associated dxi, and for each bin j,
   use this to compute the "probability" pij that a point with
   mean xi and "error" dxi should fall into bin j. This might be
   based on something as naive as integrating a Normal distribution
   with mean xi and SD dxi over the range of the bin j.

4. You then have an array P = p[i,j], say, where in row i you
   have the computed probabilities for bins 1:K

5. Now: for bin j, you have an N-column of pij values.

   The expected number of the N which might "really" be in bin j
   is then

     Ej = sum(P[,j])

   and its variance (assuming that the "errors" in the xi are
   independent of each other) is

     Vj = sum(P[,j]*(1 - P[,j]))

6. So now you have the "bin that might have been", with expected
   value Ej and standard deviation Sj = sqrt(Vj). Now draw a
   "histogram" (you can use 'lines()' for this in R) with "bin
   heights" Ej, and "errors bars" +/- Sj.

   It is at this stage that you may have to go back to stage 2.
   In order to be sure that you will not overlook xi values that
   spill outside the range of the bins you chose in stage 2,
   you need to verify that the bins you are using extend beyond
   the range of the original data, and that the two end-bins
   have negligible E1 and EK.

7. NOTE that the Ej will in general be different from the counts
   in bin j from the original data. This is due to "overspill":
   if you have an original bin with a small count, which has next
   to it a bin with a large count, the uncertainty about whether
   some of the latter should really be in the former will contribute
   positively to the bin with the small count, by a larger amount
   than the bin with the small count will contribute to the bin
   with the large count.

   As you can see, this will have an effect of somewhat "flattening"
   the histogram, and of smoothing irregular variation from bin to
   bin.

This is just an outline of a possible approach, which you may be
able to develop to better suit your purposes if they are different
from what I've been assuming.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 03-Nov-05                                       Time: 15:45:35
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Thu Nov  3 16:46:16 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Nov 2005 16:46:16 +0100
Subject: [R] Potential for R to conflict with other softwares
In-Reply-To: <436A1EF7.402@stats.uwo.ca>
References: <9AB9FB1124563249B0B39A1FEF1E94B37E1B85@cdsx06.cder.fda.gov>
	<436A1EF7.402@stats.uwo.ca>
Message-ID: <x2d5lhvinr.fsf@viggo.kubism.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> On 11/3/2005 9:11 AM, Soukup, Mat wrote:
> > Hi.
> > 
> > After some time, my collegues at the Food and Drug Adminstration have
> > finally acknowledged R as a powerful statistical computing environment.
> > However, in order to comply with the Office of Information and Technology
> > standards there are a couple of questions about whether R could interfere
> > with other software. As I'm more of a driver of the R software and not a
> > mechanic, I was hoping for the insight of the many great useRs. Below is a
> > list of 5 proposed questions to which I value any comment.
> > 
> > Thank you for your time,
> > 
> > -Mat
> > 
> > 
> 
> These answers are about the Windows version only, but from the 
> questions, I think that's what you were looking for.  They apply to all 
> versions since 1.6.x at least (though the earlier ones would have put 
> fewer entries into the registry, they put them in the same places).
> 
> > 1. Does R have high resolution graphics?
> 
> Yes, but I don't think I get the point of this question.  How would that 
> interfere with other software?

Device drivers! We have seen cases where the FPU control word had to
be reset (DM will know this more precisely than me, I think). We do
have code in place to catch that particular issue though. 

If you want to be really sure, I believe that batch runs using the
postscript() or pdf() drivers would be immune to such issues (right?).

(BTW, does the FDA trust SAS for Windows? I've seen a weird thing or
two happening to the display there...)

> > 
> > 2. Does R have .dll files, or other executables which are not located in the
> > R software directory tree?
> 
> No, it installs everything below R_HOME.

We do rely on MCVCRT.DLL and some other system DLLs though, and I
think this can get modified by other software.

> > 
> > 3. Does R modify the Windows registry in a non-obvious way, i.e. other than
> > defining itself and what extensions to associate with R, and what are those
> > extensions?
> 
> I think all of its modifications would count as obvious.  They are 
> mainly below HKLM/Software/R-core or HKCU/Software/R-core (where the 
> file locations are recorded); additionally file associations are set up 
> for .Rdata files (which are called RWorkspace files there), and an 
> uninstall entry is made.
> > 
> > 4. Does R add macros to any part of MS Office?
> 
> No.
> > 
> > 5. Can you anticipate any other way in which installing and using R could
> > disrupt the operation of another software?
> 
> No, not really.  Maybe users will become addicted to it?  ;-)
> 
> Duncan Murdoch
> >  
> > 
> > ***********************************************************************
> > Mat Soukup, Ph.D.
> > Food and Drug Administration
> > 10903 New Hampshire Ave. 
> > BLDG 22 RM 5329
> > Silver Spring, MD 20993-0002
> > Phone: 301.796.1005
> > ***********************************************************************

(& I think Mat is owed a big "Thank you" for his efforts).

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ezhil02 at yahoo.com  Thu Nov  3 17:02:25 2005
From: ezhil02 at yahoo.com (A Ezhil)
Date: Thu, 3 Nov 2005 08:02:25 -0800 (PST)
Subject: [R] MDS: Sample in one group appears twice. Why?
Message-ID: <20051103160225.91037.qmail@web32107.mail.mud.yahoo.com>

Hi All,

I am trying to apply MDS for 4 groups in my data. The
groups are:  

groups = list( Day1C=c(9), Day1T=c(7,8,10),
Day2C=c(1,2,3,6,11,13,14,15), Day2T=c(4,5,12,16,17,18)
)

When I do the MDS plot the group1 member appears twice
instead of one time in the plot. I don't know why this
is happening.

I would greatly appreciate your help in fixing this
problem.

Thanks in Advance.
Ezhil



From german.lopez at ua.es  Thu Nov  3 17:02:49 2005
From: german.lopez at ua.es (german.lopez@ua.es)
Date: Thu, 3 Nov 2005 17:02:49 +0100
Subject: [R] Help on model selection using AICc
Message-ID: <200511031602.jA3G2nuX018575@aitana.cpd.ua.es>

Hi,
   I'm fitting poisson regression models to counts of birds in 
1x1 km squares using several environmental variables as predictors. 
I do this in a stepwise way, using the stepAIC function. However the 
resulting models appear to be overparametrized, since too much 
variables were included. 
  I would like to know if there is the possibility of fitting models 
by steps but using the AICc instead of AIC. Or at least I wonder if it 
would be possible to save the AIC value and number of parameters of 
models fitted in each step and to calculate AICc afterward.
   Help on this will be very much appreciated
   German Lopez



From spencer.graves at pdf.com  Thu Nov  3 17:03:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 03 Nov 2005 08:03:01 -0800
Subject: [R] ML optimization question--unidimensional unfolding scalin g
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED55D@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED55D@usctmx1106.merck.com>
Message-ID: <436A34B5.5000901@pdf.com>

Hi, Andy and Peter:

	  That's interesting.  I still like the idea of making my own local 
copy, because I can more easily add comments and test ideas while 
working through the code.  I haven't used "debug", but I think I should 
try it, because some things occur when running a function that don't 
occur when I walk through it line by line, e.g., parsing the "call" and 
"..." arguments.

	  Two more comments on the original question:

	  1.  What is the structure of your data?  Have you considered 
techniques for Multidimensional Scaling (MDS)?  It seems that your 
problem is just a univariate analogue of the MDS problem.  For metric 
MDS from a complete distance matrix, the solution is relatively 
straightforward computation of eigenvalues and vectors from a matrix 
computed from the distance matrix, and there is software widely 
available for the nonmetric MDS problem.  For a terse introduction to 
that literature, see Venables and Ripley (2002) Modern Applied 
Statistics with S, 4th ed. (Springer, "distance methods" in sec. 11.1, 
pp. 306-308).

	  2.  If you don't have a complete distance matrix, might it be 
feasible to approach the problem starting small and building larger, 
i.e., start with 3 nodes, then add a fourth, etc.?

	  spencer graves

Liaw, Andy wrote:

> Alternatively, just type debug(optim) before using it, then step through it
> by hitting enter repeatedly...
> 
> When you're done, do undebug(optim).
> 
> Andy
> 
> 
>>From: Spencer Graves
>>
>>	  Have you looked at the code for "optim"?  If you 
>>execute "optim", it 
>>will list the code.  You can copy that into a script file and walk 
>>through it line by line to figure out what it does.  By doing 
>>this, you 
>>should be able to find a place in the iteration where you can 
>>test both 
>>branches of each bifurcation and pick one -- or keep a list 
>>of however 
>>many you want and follow them all more or less 
>>simultaneously, pruning 
>>the ones that seem too implausible.  Then you can alternate between a 
>>piece of the "optim" code, bifurcating and pruning, adjusting 
>>each and 
>>printing intermediate progress reports to help you understand 
>>what it's 
>>doing and how you might want to modify it.
>>
>>	  With a bit more effort, you can get the official 
>>source code with 
>>comments.  To do that, I think you go to "www.r-project.org" 
>>-> CRAN -> 
>>(select a local mirror) -> "Software:  R sources".  From there, just 
>>download "The latest release:  R-2.2.0.tar.gz".
>>
>>	  For more detailed help, I suggest you try to think of 
>>the simplest 
>>possible toy problem that still contains one of the issues 
>>you find most 
>>difficult.  Then send that to this list.  If readers can copy a few 
>>lines of R code from your email into R and try a couple of things in 
>>less than a minute, I think you might get more useful replies quicker.
>>
>>	  Best Wishes,
>>	  Spencer Graves
>>
>>Peter Muhlberger wrote:
>>
>>
>>>Hi Spencer:  Thanks for your interest!  Also, the posting 
>>
>>guide was helpful.
>>
>>>I think my problem might be solved if I could find a way to 
>>
>>terminate nlm or
>>
>>>optim runs from within the user-given minimization function 
>>
>>they call.
>>
>>>Optimization is unconstrained.
>>>
>>>I'm essentially using normal like curves that translate 
>>
>>observed values on a
>>
>>>set of variables (one curve per variable) into latent 
>>
>>unfolded values.  The
>>
>>>observed values are on the Y-axis & the latent (hence 
>>
>>parameters to be
>>
>>>estimated) are on the X-axis.  The problem is that there 
>>
>>are two points into
>>
>>>which an observed value can map on a curve--one on either 
>>
>>side of the curve
>>
>>>mean.  Only one of these values actually will be optimal 
>>
>>for all observed
>>
>>>variables, but it's easy to show that most estimation 
>>
>>methods will get stuck
>>
>>>on the non-optimal value if they find that one first.  
>>
>>Moving away from that
>>
>>>point, the likelihood gets a whole lot worse before the 
>>
>>routine will 'see'
>>
>>>the optimal point on the other side of the normal curve.
>>>
>>>SANN might work, but I kind of wonder how useful it'd be in 
>>
>>estimating
>>
>>>hundreds of parameters--thanks to that latent scale.
>>>
>>>My (possibly harebrained) thought for how to estimate this 
>>
>>unfolding using
>>
>>>some gradient-based method would be to run through some 
>>
>>iterations and then
>>
>>>check to see whether a better solution exists on the 'other 
>>
>>side' of the
>>
>>>normal curves.  If it does, replace those parameters with 
>>
>>the better ones.
>>
>>>Because this causes the likelihood to jump, I'd probably 
>>
>>have to start the
>>
>>>estimation process over again (maybe).  But, I see no way 
>>
>>from within the
>>
>>>minimization function called by NLM or optim to tell NLM or optim to
>>>terminate its current run.  I could make the algorithm 
>>
>>recursive, but that
>>
>>>eats up resources & will probably have to be terminated w/ an error.
>>>
>>>Peter
>>>
>>>
>>>On 10/11/05 11:11 PM, "Spencer Graves" 
>>
>><spencer.graves at pdf.com> wrote:
>>
>>>
>>>>There may be a few problems where ML (or more generally 
>>
>>Bayes) fails
>>
>>>>to give sensible answers, but they are relatively rare.
>>>>
>>>>What is your likelihood?  How many parameters are you trying to
>>>>estimate?
>>>>
>>>>Are you using constrained or unconstrained optimization?  If
>>>>constrained, I suggest you remove the constraints by appropriate
>>>>transformation.  When considering alternative transformations, I
>>>>consider (a) what makes physical sense, and (b) which transformation
>>>>produces a log likelihood that is more close to being parabolic.
>>>>
>>>>Hou are you calling "optim"?  Have you tried all "SANN" as well as
>>>>"Nelder-Mead", "BFGS", and "CG"?  If you are using constrained
>>>>optimization, I suggest you move the constraints to Inf by 
>>
>>appropriate
>>
>>>>transformation and use the other methods, as I just suggested.
>>>>
>>>>If you would still like more suggestions from this group, please
>>>>provide more detail -- but as tersely as possible.  The 
>>
>>posting guide
>>
>>>>is, I believe, quite useful (www.R-project.org/posting-guide.html).
>>>>
>>>>spencer graves
>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>
>>http://www.R-project.org/posting-guide.html
>>
>>-- 
>>Spencer 
>>Graves, PhD
>>Senior Development Engineer
>>PDF Solutions, Inc.
>>333 West San Carlos Street Suite 700
>>San Jose, CA 95110, USA
>>
>>spencer.graves at pdf.com
>>www.pdf.com <http://www.pdf.com>
>>Tel:  408-938-4420
>>Fax: 408-280-7915
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Sandrine.Coelho at univ-tlse1.fr  Thu Nov  3 17:12:23 2005
From: Sandrine.Coelho at univ-tlse1.fr (SANDRINE COELHO)
Date: Thu,  3 Nov 2005 17:12:23 +0100
Subject: [R] fatal error unused tempdir
Message-ID: <1131034343.436a36e7696a8@webmail.univ-tlse1.fr>


Hello,

I am running R on XP Windows machine, and frequently I have enable to start R. I
have this message: "Fatal Error: cannot find unused tempdir name". I don't know
why.

Thanks, in advance, for your help

Sandrine Coelho



From ggrothendieck at gmail.com  Thu Nov  3 17:13:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 3 Nov 2005 11:13:15 -0500
Subject: [R] PL2006 Program Ballot
Message-ID: <971536df0511030813n43c1ad0br518c3eb14d5a3729@mail.gmail.com>

Anyone interested in voting for R to be included in the annual
Pricelessware list (Windows freeware list voted on by the readers
of the alt.comp.freeware newsgroup, see www.pricelesswarehome.org)
can reply to this message:

http://groups.google.com/group/alt.comp.freeware/msg/d9e52e406d9e2ceb

simply deleting all programs you don't want to vote for and leaving
R (and any other program) you wish to vote for.  Deadline for voting
is November 7.



From murdoch at stats.uwo.ca  Thu Nov  3 17:12:37 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 03 Nov 2005 11:12:37 -0500
Subject: [R] Potential for R to conflict with other softwares
In-Reply-To: <x2d5lhvinr.fsf@viggo.kubism.ku.dk>
References: <9AB9FB1124563249B0B39A1FEF1E94B37E1B85@cdsx06.cder.fda.gov>	<436A1EF7.402@stats.uwo.ca>
	<x2d5lhvinr.fsf@viggo.kubism.ku.dk>
Message-ID: <436A36F5.8020005@stats.uwo.ca>

On 11/3/2005 10:46 AM, Peter Dalgaard wrote:
> Duncan Murdoch <murdoch at stats.uwo.ca> writes:
> 
>> On 11/3/2005 9:11 AM, Soukup, Mat wrote:
>> > Hi.
>> > 
>> > After some time, my collegues at the Food and Drug Adminstration have
>> > finally acknowledged R as a powerful statistical computing environment.
>> > However, in order to comply with the Office of Information and Technology
>> > standards there are a couple of questions about whether R could interfere
>> > with other software. As I'm more of a driver of the R software and not a
>> > mechanic, I was hoping for the insight of the many great useRs. Below is a
>> > list of 5 proposed questions to which I value any comment.
>> > 
>> > Thank you for your time,
>> > 
>> > -Mat
>> > 
>> > 
>> 
>> These answers are about the Windows version only, but from the 
>> questions, I think that's what you were looking for.  They apply to all 
>> versions since 1.6.x at least (though the earlier ones would have put 
>> fewer entries into the registry, they put them in the same places).
>> 
>> > 1. Does R have high resolution graphics?
>> 
>> Yes, but I don't think I get the point of this question.  How would that 
>> interfere with other software?
> 
> Device drivers! We have seen cases where the FPU control word had to
> be reset (DM will know this more precisely than me, I think). We do
> have code in place to catch that particular issue though. 
> If you want to be really sure, I believe that batch runs using the
> postscript() or pdf() drivers would be immune to such issues (right?).
> 
> (BTW, does the FDA trust SAS for Windows? I've seen a weird thing or
> two happening to the display there...)
> 
>> > 
>> > 2. Does R have .dll files, or other executables which are not located in the
>> > R software directory tree?
>> 
>> No, it installs everything below R_HOME.
> 
> We do rely on MCVCRT.DLL and some other system DLLs though, and I
> think this can get modified by other software.

That's MSVCRT.DLL, the run-time library for MS Visual C++.  It has been 
distributed with the OS since Win98 or so, and we don't touch it, so 
this is more of a possibility of other software interfering with us by 
replacing it with a bad version.

> 
>> > 
>> > 3. Does R modify the Windows registry in a non-obvious way, i.e. other than
>> > defining itself and what extensions to associate with R, and what are those
>> > extensions?
>> 
>> I think all of its modifications would count as obvious.  They are 
>> mainly below HKLM/Software/R-core or HKCU/Software/R-core (where the 
>> file locations are recorded); additionally file associations are set up 
>> for .Rdata files (which are called RWorkspace files there), and an 
>> uninstall entry is made.
>> > 
>> > 4. Does R add macros to any part of MS Office?
>> 
>> No.
>> > 
>> > 5. Can you anticipate any other way in which installing and using R could
>> > disrupt the operation of another software?
>> 
>> No, not really.  Maybe users will become addicted to it?  ;-)
>> 
>> Duncan Murdoch
>> >  
>> > 
>> > ***********************************************************************
>> > Mat Soukup, Ph.D.
>> > Food and Drug Administration
>> > 10903 New Hampshire Ave. 
>> > BLDG 22 RM 5329
>> > Silver Spring, MD 20993-0002
>> > Phone: 301.796.1005
>> > ***********************************************************************
> 
> (& I think Mat is owed a big "Thank you" for his efforts).

Yes, indeed!

Duncan Murdoch



From jerk_alert at hotmail.com  Thu Nov  3 17:22:49 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Thu, 03 Nov 2005 16:22:49 +0000
Subject: [R] text mining with R
Message-ID: <BAY101-F25AFBF337FD32F68BB7AFBE8610@phx.gbl>

Hi all,

Just wondering if anyone knows of any text mining projects in R...I googled 
a bit but didn't get anything...

TIA,
ken



From jari.oksanen at oulu.fi  Thu Nov  3 17:26:41 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Thu, 03 Nov 2005 18:26:41 +0200
Subject: [R] quadratic form
In-Reply-To: <3EBB05D6-62BE-4F08-AD7C-8672F1BAAD93@soc.soton.ac.uk>
References: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>
	<3EBB05D6-62BE-4F08-AD7C-8672F1BAAD93@soc.soton.ac.uk>
Message-ID: <df3ecf211c58a210ff36157dca3a86f2@oulu.fi>


On 3 Nov 2005, at 17:25, Robin Hankin wrote:

> Hi Alvarez
>
>
> If you define
>
> quad.form.inv <-  function (M, x)
> {
>      drop(crossprod(x, solve(M, x)))
> }
>
> then you will avoid an expensive call to %*% as well.
>
Is %*% really expensive in all platforms? I had a function that used QR 
decomposition instead of quadratic forms, but then I got a message from 
Canada suggesting that %*% would be faster. Indeed, it was in 
not-too-large data sets and in Mac (or powerpc). I run some tests with 
real applications, and found that my 800MHz iBook G4 run like a 2.5GHz 
Intel machine when %*% was used. This really was architecture 
dependent, since the performance boost was similar under OS X and Linux 
in the very same PowerPC. So it seems that %*% is very cheap if you 
have PowerPC, but it may be expensive in Intel. (I also run a test in 
Sun, and it was somewhere between Intel and PowerPC.)

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From p.dalgaard at biostat.ku.dk  Thu Nov  3 17:30:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Nov 2005 17:30:08 +0100
Subject: [R] quadratic form
In-Reply-To: <x2ll05vms4.fsf@viggo.kubism.ku.dk>
References: <20051103130149.22706.qmail@web25203.mail.ukl.yahoo.com>
	<x2ll05vms4.fsf@viggo.kubism.ku.dk>
Message-ID: <x28xw5vgmn.fsf@viggo.kubism.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Alvarez Pedro <palvarez7777 at yahoo.es> writes:
> 
> > On page 22 of the R-introduction guide it's written:
> > 
> > the quadratic form x^{'} A^{-1} x which is used in
> > multivariate computations, should be computed by
> > something like x%*%solve(A,x), rather than computing
> > the inverse of A.
> > 
> > Why isn't it good to compute t(x) %*% solve(A) %*% x?
> 
> It's just a waste of CPU time. For k x k matrices, solution of Ax=b is
> of computational complexity O(k^2) whereas inversion of A is O(k^3).
> This is obviously more important for k=1000 than for k=5.

Erm, Thomas Lumley points out that solution of Ax=b is also of order
k^3 ingeneral (R uses a QR decomposition). So it's just the multiplier
that differs. The savings should still be on the order of k^3 though.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From abunn at whrc.org  Thu Nov  3 17:33:52 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 3 Nov 2005 11:33:52 -0500
Subject: [R] text mining with R
In-Reply-To: <BAY101-F25AFBF337FD32F68BB7AFBE8610@phx.gbl>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEEODMAA.abunn@whrc.org>

> Just wondering if anyone knows of any text mining projects in 
> R...I googled 
> a bit but didn't get anything...

RSiteSearch("text mining") turns up 85 hits...



From murdoch at stats.uwo.ca  Thu Nov  3 17:37:33 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 03 Nov 2005 11:37:33 -0500
Subject: [R] fatal error unused tempdir
In-Reply-To: <1131034343.436a36e7696a8@webmail.univ-tlse1.fr>
References: <1131034343.436a36e7696a8@webmail.univ-tlse1.fr>
Message-ID: <436A3CCD.9000401@stats.uwo.ca>

On 11/3/2005 11:12 AM, SANDRINE COELHO wrote:
> Hello,
> 
> I am running R on XP Windows machine, and frequently I have enable to start R. I
> have this message: "Fatal Error: cannot find unused tempdir name". I don't know
> why.

This has come up before.  When R starts up, it tries to create a folder 
for temporary files, using the TMP, TEMP or R_USER environment variables 
(in that order) to find a base path.  The tempdir() function will tell 
you the folder name.

It chooses the name by appending a random number between 0 and 65535 to 
a base name.  It makes 100 attempts at this, and if it can't find a name 
that works, it bails out.  That's what you're seeing.  Sometimes it gets 
lucky and finds a name in the first 100 tries; then you don't get the 
error.

R tries to delete the directory when it quits, but if there are files 
there, deletion will fail.  So you should probably try to track down 
what is producing files and not getting rid of them.

What you need to do is to find the directory containing all these 
temporary files (print tempdir() on one of your successful attempts), 
and delete all the old ones.

Duncan Murdoch



From William.Alpert at barrons.com  Thu Nov  3 17:45:21 2005
From: William.Alpert at barrons.com (Alpert, William)
Date: Thu, 3 Nov 2005 11:45:21 -0500
Subject: [R] newbie graphics question: Two density plots in same frame ?
Message-ID: <1CDEA031FEA76E4F9A41822D63A2D2BC09862628@SBKE2KMB04.win.dowjones.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051103/f593e227/attachment.pl

From br44114 at gmail.com  Thu Nov  3 18:00:33 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 3 Nov 2005 12:00:33 -0500
Subject: [R] newbie graphics question: Two density plots in same frame ?
Message-ID: <8d5a36350511030900j19980db6v3aa8b0b10697a2cc@mail.gmail.com>

Here's a function that you can customize to fit your needs. lst is a named list.

multicomp <- function(lst)
{
clr <- c("darkgreen","red","blue","brown","magenta")
alldens <- lapply(lst,function(x) {density(x,from=min(x),to=max(x))})
allx <- sapply(alldens,function(d) {d$x})
ally <- sapply(alldens,function(d) {d$y})
plot(allx,ally,type="n")
for (i in 1:length(lst)) {
	lines(alldens[[i]]$x,alldens[[i]]$y,lty=i,col=clr[i],lwd=3)
	}
legend("topright",xjust=1,legend=names(lst),lwd=3,lty=1:length(lst),col=head(clr,length(lst)))
}
#---------------
toplot <- list(var1=dfr1$var,var2=dfr2$var)
multicomp(toplot)


> -----Original Message-----
> From: Alpert, William [mailto:William.Alpert at barrons.com]
> Sent: Thursday, November 03, 2005 11:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] newbie graphics question: Two density plots in
> same frame ?
>
>
> I swear I've scoured the help files and several texts before posting
> what feels like a dumb newbie question.
>
> How can I draw two kernel density plots in the same frame ? I have
> similar variables in two separate data frames, and I would
> like to show
> their two histograms/densities in a single picture.  Same
> units, scale,
> range for both, so I'm simply trying to draw one and then add
> the other
> to the picture.  Nothin' fancy.
>
>
> Bill Alpert
>
> Sr. Editor
>
> Barron's
>
> 212.416.2742
>
> william.alpert at barrons.com
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From abunn at whrc.org  Thu Nov  3 18:14:12 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 3 Nov 2005 12:14:12 -0500
Subject: [R] Add dots at the mean of a bwplot using panel.points
In-Reply-To: <8d5a36350511030900j19980db6v3aa8b0b10697a2cc@mail.gmail.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIAEFADMAA.abunn@whrc.org>

How can I modify the example below to put a dot at the mean of each violin
plot? I assume I use panel.points but that's as far as I can go.

     bwplot(voice.part ~ height, singer,
            panel = function(..., box.ratio) {
                panel.violin(..., col = "transparent",
                             varwidth = FALSE, box.ratio = box.ratio)
                #panel.points(mean(x.....))
            } )

TIA, Andy



From gerifalte28 at hotmail.com  Thu Nov  3 18:15:46 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 03 Nov 2005 17:15:46 +0000
Subject: [R] newbie graphics question: Two density plots in same frame ?
In-Reply-To: <1CDEA031FEA76E4F9A41822D63A2D2BC09862628@SBKE2KMB04.win.dowjones.net>
Message-ID: <BAY103-F16C943B336D41359AFDF59A6610@phx.gbl>

To plot two Kernel densities you can use matplot:

x1<-density(rnorm(100))
x2<-density(rnorm(100))
matplot(cbind(x1$y,x2$y), type="l")


Or if both distributions are really very similar and you don't have to 
adjust the axes you can simply use
plot(x1)
lines(x2, col="red")


Finally if you want to have two histograms in the same picture (I would not 
recomend it tough since the distributions are similar so the overlapping 
will make it very messy) you can use the argument "add" within hist

hist(rnorm(100), col="red")
hist(rnorm(100), col="blue", add=T)

I hope this helps

Francisco

>From: "Alpert, William" <William.Alpert at barrons.com>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] newbie graphics question: Two density plots in same frame ?
>Date: Thu, 3 Nov 2005 11:45:21 -0500
>
>I swear I've scoured the help files and several texts before posting
>what feels like a dumb newbie question.
>
>How can I draw two kernel density plots in the same frame ? I have
>similar variables in two separate data frames, and I would like to show
>their two histograms/densities in a single picture.  Same units, scale,
>range for both, so I'm simply trying to draw one and then add the other
>to the picture.  Nothin' fancy.
>
>
>Bill Alpert
>
>Sr. Editor
>
>Barron's
>
>212.416.2742
>
>william.alpert at barrons.com
>
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From leaflovesun at yahoo.ca  Thu Nov  3 18:21:22 2005
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Thu, 3 Nov 2005 10:21:22 -0700
Subject: [R] Visualizing a Data Distribution -- Was: breaks in hist()
Message-ID: <200511031721.jA3HLP6H019631@hypatia.math.ethz.ch>

 Thanks for all the response. I think plotting a cdf or taking transformation could make the plot look better.

 But my further question is how to set the breaks to make the histogram concentrate in the interval of (0.01,0.2). I can even ignore the other parts of the values. 

Thanks!

Leaf



======= At 2005-11-02, 12:07:12 you wrote: =======

>> > Leaf Sun wrote:
>> > The histogram is highly screwed to the right, say, the range
>> > of the vector is [0, 2], but 95% of the value is squeezed in
>> > the interval (0.01, 0.2).
>
>I guess the histogram is as you wrote. See
>http://web.maths.unsw.edu.au/~tduong/seminars/intro2kde/
>for a short explanation.
>
>
>> -----Original Message-----
>> From: Berton Gunter [mailto:gunter.berton at gene.com]
>> Sent: Wednesday, November 02, 2005 1:10 PM
>> To: 'Leaf Sun'; r-help at stat.math.ethz.ch
>> Subject: [R] Visualizing a Data Distribution -- Was: breaks in hist()
>>
>>
>> Leaf:
>>
>> An interesting question concerning graphical perception. As
>> you have noted,
>> choice of bin boundaries in a histogram can have a big effect on how a
>> distribution is perceived. My $.02 (U.S.):
>>
>> Histograms are a relic of manual data plotting. We have much better
>> alternatives these days that should be used instead. e.g.
>>
>> 1. (my preference, but properly not consumer-friendly). Plot
>> the cdf instead
>> (?ecdf) .
>>
>> 2. Plot a density estimator (?density ; ?densityplot)
>>
>> 3. See David Scott's ash package, perhaps the KernSmooth package also
>> (though density() probably already has anything that you'd
>> need from it).
>>
>> Cheers,
>>
>> -- Bert Gunter
>> Genentech Non-Clinical Statistics
>> South San Francisco, CA
>>
>> "The business of the statistician is to catalyze the
>> scientific learning
>> process."  - George E. P. Box
>>
>>
>>
>> > -----Original Message-----
>> > From: r-help-bounces at stat.math.ethz.ch
>> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leaf Sun
>> > Sent: Wednesday, November 02, 2005 9:49 AM
>> > To: r-help at stat.math.ethz.ch
>> > Subject: [R] breaks in hist()
>> >
>> > Dear listers,
>> >
>> > A quick question about breaks in hist().
>> >
>> > The histogram is highly screwed to the right, say, the range
>> > of the vector is [0, 2], but 95% of the value is squeezed in
>> > the interval (0.01, 0.2). My question is : how to set the
>> > breaks then make the histogram look even?
>> >
>> > Thanks in advance,
>> >
>> > Leaf
>> >
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>

= = = = = = = = = = = = = = = = = = = =



From andy_liaw at merck.com  Thu Nov  3 18:27:46 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 3 Nov 2005 12:27:46 -0500
Subject: [R] quadratic form
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED566@usctmx1106.merck.com>

If you meant QR vs. inverting X'X for linear regression, the motivation for
using QR is not speed, but numerical stability.  There's no univerally good
least squares algorithm that would be uniformly better than anything else
for any kind of data.

Andy

> From: Jari Oksanen
> 
> 
> On 3 Nov 2005, at 17:25, Robin Hankin wrote:
> 
> > Hi Alvarez
> >
> >
> > If you define
> >
> > quad.form.inv <-  function (M, x)
> > {
> >      drop(crossprod(x, solve(M, x)))
> > }
> >
> > then you will avoid an expensive call to %*% as well.
> >
> Is %*% really expensive in all platforms? I had a function 
> that used QR 
> decomposition instead of quadratic forms, but then I got a 
> message from 
> Canada suggesting that %*% would be faster. Indeed, it was in 
> not-too-large data sets and in Mac (or powerpc). I run some 
> tests with 
> real applications, and found that my 800MHz iBook G4 run like 
> a 2.5GHz 
> Intel machine when %*% was used. This really was architecture 
> dependent, since the performance boost was similar under OS X 
> and Linux 
> in the very same PowerPC. So it seems that %*% is very cheap if you 
> have PowerPC, but it may be expensive in Intel. (I also run a test in 
> Sun, and it was somewhere between Intel and PowerPC.)
> 
> cheers, jari oksanen
> --
> Jari Oksanen, Oulu, Finland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From christian_mora at arauco.cl  Thu Nov  3 18:21:46 2005
From: christian_mora at arauco.cl (Christian Mora)
Date: Thu, 3 Nov 2005 13:21:46 -0400
Subject: [R] nlme questions
Message-ID: <OF31D562AB.1B445541-ON842570AE.005E12E8@arauco.cl>





Dear R users;

Ive got two questions concerning nlme library 3.1-65 (running on R 2.2.0 /
Win XP Pro). The first one is related to augPred function. Ive been working
with a nonlinear mixed model with no problems so far. However, when the
parameters of the model are specified in terms of some other covariates,
say treatment (i.e. phi1~trt1+trt2, etc) the augPred function give me the
following error: "Error in predict.nlme(object,
value[1:(nrow(value)/nL),,drop=FALSE], : Levels 0,1 not allowed for trt1,
trt2". The same model specification as well as the augPred function under
SPlus 2000 run without problems. The second question has to deal with the
time needed for the model to converge. It really takes a lot of time to fit
the model on R in relation to the time required to fit the same model on
SPlus. I can imagine this is related to the optimization algorithm or
something like that, but I would like to have a different opinion on these
two issues.

Thanks in advance

Christian Mora



From peterm at andrew.cmu.edu  Thu Nov  3 18:42:53 2005
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Thu, 03 Nov 2005 12:42:53 -0500
Subject: [R] ML optimization question--unidimensional unfolding scaling
In-Reply-To: <43696F5F.1000703@pdf.com>
Message-ID: <BF8FB64D.1214A%peterm@andrew.cmu.edu>

Hi Spencer & Andy:  Thanks for your thoughtful input!  I did at one point
look at the optim() function & run debug on it (wasn't aware of
browser--that's helpful!).  My impression is that optim() simply calls a C
function that handles the maximization.  So if I want to break out of my
likelihood function to restart optim() w/ new values, it seems I'd have to
somehow communicate to C that it's time to stop.  May need to rewrite the C,
with which I'm not familiar--Java yes, so maybe when I have some real free
time....

Another possibility might be finding some jerry-rigged way to break out of
optim.  Maybe if I tell the likelihood function to freeze its returned value
at some point, optim will conclude it's done and stop.  Probably inefficient
& I will have the problem of telling when the break point ought to occur.
Just wish there were some programmatic way to say 'stop this and return
control to the higher-level calling function 'blah''.

A third possibility is one suggested by Spencer who seems to think it's ok
for the routine to pursue multiple branches w/o restarting, hence no restart
problem.  But w/ Newtonian-style convergence the latent scale values (which
are parameters to be estimated) have current positions & are supposed to
smoothly move toward lower likelihood values.  What will happen in branched
convergence, however, is that some of the latent values will prove to have
better values on the other side of a normal curve from their current
position.  My guess is that this will cause the likelihood function to make
a sudden, non-continuous jump not predictable by derivatives, which may mean
it can't converge properly.

Spencer's MDS alternative is intriguing & I'll need to think more about it.
Maybe I should also consider full-out Bayesian Monte Carlo methods (if I
have time), which would simultaneously explore the whole solution space.

Thanks,
Peter

On 11/2/05 9:01 PM, "Spencer Graves" <spencer.graves at pdf.com> wrote:

>  Have you looked at the code for "optim"?  If you execute "optim", it
> will list the code.  You can copy that into a script file and walk
> through it line by line to figure out what it does.  By doing this, you
> should be able to find a place in the iteration where you can test both
> branches of each bifurcation and pick one -- or keep a list of however
> many you want and follow them all more or less simultaneously, pruning
> the ones that seem too implausible.  Then you can alternate between a
> piece of the "optim" code, bifurcating and pruning, adjusting each and
> printing intermediate progress reports to help you understand what it's
> doing and how you might want to modify it.
> 
>  With a bit more effort, you can get the official source code with
> comments.  To do that, I think you go to "www.r-project.org" -> CRAN ->
> (select a local mirror) -> "Software:  R sources".  From there, just
> download "The latest release:  R-2.2.0.tar.gz".
> 
>  For more detailed help, I suggest you try to think of the simplest
> possible toy problem that still contains one of the issues you find most
> difficult.  Then send that to this list.  If readers can copy a few
> lines of R code from your email into R and try a couple of things in
> less than a minute, I think you might get more useful replies quicker.

On 11/3/05 8:08 AM, "Liaw, Andy" <andy_liaw at merck.com> wrote:

> Alternatively, just type debug(optim) before using it, then step through it
> by hitting enter repeatedly...
> 
> When you're done, do undebug(optim).

On 11/3/05 11:06 AM, "Liaw, Andy" <andy_liaw at merck.com> wrote:

> Essentially all that debug() does is like inserting browser() as the first
> line of the function being debug()ed.  You can type just about any command
> at the browser> prompt, e.g., for checking data, etc.  ?browser has list of
> special commands for the browser> prompt.
> 
> Andy



From andreas.cordes at stud.uni-goettingen.de  Thu Nov  3 18:43:33 2005
From: andreas.cordes at stud.uni-goettingen.de (Andreas Cordes)
Date: Thu, 03 Nov 2005 18:43:33 +0100
Subject: [R] Fitting heteroscedastic linear models/ problems with varIdent
 of nlme
Message-ID: <436A4C45.9070109@stud.uni-goettingen.de>

Hi,
I would like to fit a model for a factorial design that allows for 
unequal variances in all groups. If I am not mistaken, this can be done 
in lm by specifying weights.
A function intended to specify weights for unequal variance structures 
is provided in the nlme library with the varIdent function. Is it 
apropriate to use these weights with lm? If not, is there another 
possibility to do factorial designs with heteroscedasticity?

When trying to use varIdent I get an errormessage that says that 
varIndent is not a defined class. The function calls are written in the 
same way as in Pinheiro & Bates Book. Their example works. With my data 
it doesnt. I somehow fail to figure out the difference between them.
In the remainder is a subset of my Dataset in which the problem also 
occours. A,B,C are metric variables, D is a factor.

I wold be thankful if any Ideas you might have.

thank you for your attention
Andreas

 > dat2
        A  B  C D
1   990.5 20 46 1
2   990.5 20 44 1
3   704.5 19 35 1
4   990.5 20 39 1
5  2240.5 25 79 2
6  2240.5 25 43 2
7  2240.5 25 44 2
8  2240.5 25 50 2
9  2240.5 25 56 2
10  470.0 17 51 2

 > vi<-varIdent(form=~1|D)
 > vi<-initialize(vi,dat2)
Error in getClass(Class) : c("\"varIdent\" is not a defined class", 
"\"varFunc\" is not a defined class")
In addition: Warning message:
the condition has length > 1 and only the first element will be used in: 
if (!is.na(match(Class, .BasicClasses))) return(newBasic(Class, 
 >



From peterm at andrew.cmu.edu  Thu Nov  3 19:02:16 2005
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Thu, 03 Nov 2005 13:02:16 -0500
Subject: [R] ML optimization question--unidimensional unfolding scalin	g
In-Reply-To: <436A34B5.5000901@pdf.com>
Message-ID: <BF8FBAD8.1214F%peterm@andrew.cmu.edu>

Hi Spencer:  Just realized I may have misunderstood your comments about
branching--you may have been thinking about a restart.  Sorry if I
misrepresented them.

See below:


On 11/3/05 11:03 AM, "Spencer Graves" <spencer.graves at pdf.com> wrote:

> Hi, Andy and Peter:
> 
>  That's interesting.  I still like the idea of making my own local
> copy, because I can more easily add comments and test ideas while
> working through the code.  I haven't used "debug", but I think I should
> try it, because some things occur when running a function that don't
> occur when I walk through it line by line, e.g., parsing the "call" and
> "..." arguments.
> 

Debug's handy tho I think it is line by line.

>  Two more comments on the original question:
> 
>  1.  What is the structure of your data?  Have you considered
> techniques for Multidimensional Scaling (MDS)?  It seems that your
> problem is just a univariate analogue of the MDS problem.  For metric
> MDS from a complete distance matrix, the solution is relatively
> straightforward computation of eigenvalues and vectors from a matrix
> computed from the distance matrix, and there is software widely
> available for the nonmetric MDS problem.  For a terse introduction to
> that literature, see Venables and Ripley (2002) Modern Applied
> Statistics with S, 4th ed. (Springer, "distance methods" in sec. 11.1,
> pp. 306-308).
> 

I was looking for something on MDS in R, that'll be handy!

The data structure is a set of variables (say about 6) that I have reason to
believe measure an underlying dimension.  I suspect that several of the
variables are unfolding--that is, they have their highest value for some
point on the scale and fall off w/ distance from that point in either
direction.  The degree of fall-off may vary depending on the variable.  Some
seem to fall off very rapidly, others not.  A couple variables probably
monotonically increase w/ the underlying scale, so they don't unfold.  I can
construct a distance matrix consisting of distances between these variables.

Do you think MDS might be able to handle an arrangement like this, w/ some
values folded about a scale point and with drop-off varying between
variables?  The distances between the variables do not map in any
straightforward way into distances on the underlying scale because of
folding and non-linearity.

>  2.  If you don't have a complete distance matrix, might it be
> feasible to approach the problem starting small and building larger,
> i.e., start with 3 nodes, then add a fourth, etc.?
> 

Not sure I follow, but I do have a complete distance matrix of distances
between the variables.

>  spencer graves

Thanks,

Peter



From mlyman at byu.edu  Thu Nov  3 19:05:03 2005
From: mlyman at byu.edu (Mark Lyman)
Date: Thu, 03 Nov 2005 11:05:03 -0700
Subject: [R] Specify Z matrix with lmer function
Message-ID: <436A514F.8080202@byu.edu>

Is there a way to specify a Z matrix using the lmer function, where the 
model is written as y = X*Beta + Z*u + e?

I am trying to reproduce smoothing methods illustrated in the paper 
"Smoothing with Mixed Model Software" my Long Ngo and M.P. Wand. 
published in the /Journal of Statistical Software/ in 2004 using the 
lme4 and Matrix packages. The code and data sets used can be found at 
http://www.jstatsoft.org/v09/i01/.

There original code did  not work for me without slight modifications 
here is the code that I used with my modifications noted.

x <- fossil$age
y <- 100000*fossil$strontium.ratio
knots <- seq(94,121,length=25)
n <- length(x)
X <- cbind(rep(1,n),x)
Z <- outer(x,knots,"-")
Z <- Z*(Z>0)
# I had to create the groupedData object with one group to fit the model 
I wanted
grp <- rep(1,n)
grp.dat<-groupedData(y~Z|grp)
fit <- lme(y~-1+X,random=pdIdent(~-1+Z),data=grp.dat)

I would like to know how I could fit this same model using the lmer 
function. Specifically can I specify a Z matrix in the same way as I do 
above in lme?

Thanks,
Mark



From fuyaonv at hotmail.com  Thu Nov  3 19:28:42 2005
From: fuyaonv at hotmail.com (Lynette Sun)
Date: Thu, 03 Nov 2005 18:28:42 +0000
Subject: [R] multidimensional integration not over a
	multidimensionalrectangle
Message-ID: <BAY102-F2C6A03021F1D099B0C406B3610@phx.gbl>

Hi,

anyone knows about any functions in R can get multidimensional integration
not over a multidimensional rectangle (not adapt).

For example, I tried the following function f(x,n)=x^n/n!

phi.fun<-function(x,n)
{ if (n==1) {
	x
	}else{
		integrate(phi.fun, lower=0, upper=x, n=n-1)$value
		}
}

I could get f(4,2)=4^2/2!=8, but failed in f(4,3)=4^3/3! Thanks

Best,
Lynette



From dieter.menne at menne-biomed.de  Thu Nov  3 19:30:03 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 3 Nov 2005 18:30:03 +0000 (UTC)
Subject: [R] Fitting heteroscedastic linear models/ problems with
	varIdent of nlme
References: <436A4C45.9070109@stud.uni-goettingen.de>
Message-ID: <loom.20051103T192411-989@post.gmane.org>

Andreas Cordes <andreas.cordes <at> stud.uni-goettingen.de> writes:

> 
> Hi,
> I would like to fit a model for a factorial design that allows for 
> unequal variances in all groups. If I am not mistaken, this can be done 
> in lm by specifying weights.

> A function intended to specify weights for unequal variance structures 
> is provided in the nlme library with the varIdent function. Is it 
> apropriate to use these weights with lm? If not, is there another 
> possibility to do factorial designs with heteroscedasticity?

No, varIdent and friends is made for use with mixed effect models in package 
nlme.

Dieter



From mac at dgp.toronto.edu  Thu Nov  3 20:22:51 2005
From: mac at dgp.toronto.edu (Maciej Kalisiak)
Date: Thu, 3 Nov 2005 14:22:51 -0500
Subject: [R] multivariate nonparametric regression with e >= 0
Message-ID: <20051103192250.GC4448@shodan>

Hello all,

I'm a relatively new user of R, having mostly used it only for plotting so
far.  I'm also not very familiar with regression methods, hence forgive my
greenness on the topic.

What I want to do in R is multivariate nonparametric regression, with a slight
hitch.  From my experimental data I have a multitude of samples whose values
approximate a function `f' that is defined over a 5D space (i.e., f: R^5->R).
The values of the collected samples, call these `y', approximate `f', but due
to the process by which they are collected, they always over-estimate (i.e., y
= f + e, e >= 0).  The distribution of the error `e' can likely be modelled
using the positive half of the normal distribution.

Naturally I'm trying to obtain a smooth and relatively faithful approximation
of `f' using the collected samples `y'.  What would be the most fruitful
approach in R to doing this?  Even suggestions on which package/function to
use would be tremendously helpful, as I don't yet know what their
strengths/weaknesses are.

Also, I would consider parametric regression as well, but in the general case
I don't think I can assume/guess for my data at what the appropriate
parametric basis functions should be...

-- 
Maciej Kalisiak <mac at dgp.toronto.edu>
http://www.dgp.toronto.edu/~mac/



From deepayan.sarkar at gmail.com  Thu Nov  3 20:28:57 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 3 Nov 2005 13:28:57 -0600
Subject: [R] newbie graphics question: Two density plots in same frame ?
In-Reply-To: <1CDEA031FEA76E4F9A41822D63A2D2BC09862628@SBKE2KMB04.win.dowjones.net>
References: <1CDEA031FEA76E4F9A41822D63A2D2BC09862628@SBKE2KMB04.win.dowjones.net>
Message-ID: <eb555e660511031128v52f762b1p2147dbcf5117cc07@mail.gmail.com>

On 11/3/05, Alpert, William <William.Alpert at barrons.com> wrote:
> I swear I've scoured the help files and several texts before posting
> what feels like a dumb newbie question.
>
> How can I draw two kernel density plots in the same frame ? I have
> similar variables in two separate data frames, and I would like to show
> their two histograms/densities in a single picture.  Same units, scale,
> range for both, so I'm simply trying to draw one and then add the other
> to the picture.  Nothin' fancy.

Using densityplot from lattice:

library(lattice)
d1 = data.frame(x = rnorm(100))
d2 = data.frame(x = rnorm(100, mean = 0.5))
densityplot(~d1$x + d2$x, plot.points = FALSE)

-Deepayan



From tlumley at u.washington.edu  Thu Nov  3 20:37:25 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 3 Nov 2005 11:37:25 -0800 (PST)
Subject: [R] Problems with pf() with certain noncentral values/degrees
 of freedom combinations
In-Reply-To: <435D1356.5010600@Indiana.Edu>
References: <435D1356.5010600@Indiana.Edu>
Message-ID: <Pine.LNX.4.63a.0511031129430.3107@homer24.u.washington.edu>


The problem is in src/nmath/pnbeta.c, which has an iteration limit of 
100, not enough for these problems.  Increasing the iteration limit to 
1000 seems to work.

 	-thomas


On Mon, 24 Oct 2005, Ken Kelley wrote:

> Hello all.
>
> It seems that the pf() function when used with noncentral parameters can
> behave badly at times. I've included some examples below, but what is
> happening is that with some combinations of df and ncp parameters,
> regardless of how large the quantile gets, the same probability value is
> returned. Upon first glance noncentral values greater than 200 may seem
> large, but they are in some contexts not large at all. The problems with
> pf() can thus have serious implications (for example, in the context of
> sample size planning).
>
> I noticed that in in 1999 and 2000 issues with large degrees of freedom
> came about (PR#138), but I couldn't find the present issue reported
> anywhere.
>
> Might there be a way to make the algorithm more stable? I'm not sure how
> difficult this issue might be to fix, but hopefully it won't be too bad
> and can be easily done. Any thoughts on a workaround until then?
>
> Thanks,
> Ken Kelley
>
> # <Begin example code>
> X <- seq(10, 600, 10)
>
> # Gets stuck at .99135
> ############################
> round(pf(X, 10, 1000, 225), 5)
> round(pf(X, 10, 200, 225), 5)
>
> round(pf(X, 5, 1000, 225), 5)
> round(pf(X, 5, 200, 225), 5)
>
> round(pf(X, 1, 1000, 225), 5)
> round(pf(X, 1, 200, 225), 5)
>
> # Gets stuck at .97035
> ############################
> round(pf(X, 10, 1000, 250), 5)
> round(pf(X, 10, 200, 250), 5)
>
> round(pf(X, 5, 1000, 250), 5)
> round(pf(X, 5, 200, 250), 5)
>
> round(pf(X, 1, 1000, 250), 5)
> round(pf(X, 1, 200, 250), 5)
>
> # Gets stuck at .93539
> ############################
> round(pf(X, 10, 1000, 275), 5)
> round(pf(X, 10, 200, 275), 5)
>
> round(pf(X, 5, 1000, 275), 5)
> round(pf(X, 5, 200, 275), 5)
>
> round(pf(X, 1, 1000, 275), 5)
> round(pf(X, 1, 200, 275), 5)
> # <end example code>
>
> > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.0
> year     2005
> month    10
> day      06
> svn rev  35749
> language R
>
> -- 
> Ken Kelley, Ph.D.
> Inquiry Methodology Program
> Indiana University
> 201 North Rose Avenue, Room 4004
> Bloomington, Indiana 47405
> http://www.indiana.edu/~kenkel
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From p.murrell at auckland.ac.nz  Thu Nov  3 20:41:08 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 04 Nov 2005 08:41:08 +1300
Subject: [R] margins too large
References: <200511021515.jA2FFQSY014706@hypatia.math.ethz.ch>
Message-ID: <436A67D4.1050502@stat.auckland.ac.nz>

Hi


Sara Mouro wrote:
> Dear all,
> 
> How can I explian and solve the error message:
>  	"margins too large"


Do you mean "figure margins too large"?  If so, it means that there is 
not enough room for your plot;  try making the graphics window (or page 
size) bigger.  Depending on how "standard" plot.fasp() is, you could 
also try reducing the plot margins by something like ...

par(mar=rep(1, 4))

Paul


> which appears when I do something like: 
> KK <- alltypes(SpatData, "K")
> plot.fasp(KK)
> 
> Hope someone can please help me on this.
> 
> Regards,
> Sara Mouro
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From prasannaprakash at gmail.com  Thu Nov  3 21:07:12 2005
From: prasannaprakash at gmail.com (Prasanna)
Date: Thu, 3 Nov 2005 21:07:12 +0100
Subject: [R] Help in expand.grid() (Restricted combination)
Message-ID: <fc5b8ae70511031207r5ef61ac8q81d43371c8086dba@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051103/224bb636/attachment.pl

From efg at stowers-institute.org  Thu Nov  3 22:04:41 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 3 Nov 2005 15:04:41 -0600
Subject: [R] RODBC and Excel: Wrong Data Type Assumed on Import
References: <adf71a630511030726v10f1dc77sa7c760b96a27480f@mail.gmail.com>
Message-ID: <dkdu1a$jhc$1@sea.gmane.org>

"Kevin Wright" <kwright68 at gmail.com> wrote in message
news:adf71a630511030726v10f1dc77sa7c760b96a27480f at mail.gmail.com...
> >From my experience (somewhat of a guess):

> Excel uses the first 16 rows of data to determine if a column is numeric
or
> character. The data type which is most common in the first 16 rows will
then
> be used for the whole column.

I ran some experiments trying to force RODBC to read column 1 of my
worksheet as character data (the data are mostly numbers with two
exceptions, 275a and 275b, as mentioned earlier).



Here's the base code:



> library(RODBC)

> channel <- odbcConnectExcel("U:/efg/lab/R/Krumlauf-Plasmid/construct
list.xls")

> plasmid <- sqlFetch(channel,"Sheet1", as.is=TRUE)

> odbcClose(channel)

> names(plasmid)

[1] "Plasmid Number" "Plasmid"        "Concentration"  "Comments"
"Lost"



When Excel Sheet1 has rows 2:13 as an "X" to attempt to force treatment of
column 1 as character data:



> class(plasmid$"Plasmid Number")

[1] "numeric"

> typeof(plasmid$"Plasmid Number")

[1] "double"

> plasmid$"Plasmid Number"[1:20]

 [1] NA NA NA NA NA NA NA NA NA NA NA NA  2  3  4  5  6  7  8  9



Why would any software with 12 consecutive "X" character strings "assume"
the data are purely numeric?



Add one more "X" so rows 2:14 have an "X" to attempt to force treatment of
column 1 as character data:



> class(plasmid$"Plasmid Number")

[1] "character"

> typeof(plasmid$"Plasmid Number")

[1] "character"

> plasmid$"Plasmid Number"[1:20]

 [1] "X" "X" "X" "X" "X" "X" "X" "X" "X" "X" "X" "X" "X" NA  NA  NA  NA  NA
NA  NA



So RODBC now recognizes "character" Xs in column 1 and then declares all
numbers as invalid?  These are incredibly (bad) assumptions.



I say this is a "bug", but it may be an ODBC problem and not one with "R.
And if this is not an official "bug", then it's a serious design problem.
Minimally, this issue should be described in the R Data Import/Export
document, which everyone is told to read before asking a question.



It's frustrating when packages like this work for "toy" problems, and the
documentation never mentions the pitfalls of real data.


> The gregmisc bundle has a different read.xls function that uses a Perl
> script (xls2csv) and seems to be safer with mixed-type columns.
> Requires a working version of Perl.

Thanks for this suggestion, but I think I'll just convert the Excel
spreadsheet to a .csv and maintain it in that format.

efg



From p.dalgaard at biostat.ku.dk  Thu Nov  3 22:16:05 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Nov 2005 22:16:05 +0100
Subject: [R] Fitting heteroscedastic linear models/ problems with
	varIdent of nlme
In-Reply-To: <loom.20051103T192411-989@post.gmane.org>
References: <436A4C45.9070109@stud.uni-goettingen.de>
	<loom.20051103T192411-989@post.gmane.org>
Message-ID: <x2psphh1pm.fsf@turmalin.kubism.ku.dk>

Dieter Menne <dieter.menne at menne-biomed.de> writes:

> Andreas Cordes <andreas.cordes <at> stud.uni-goettingen.de> writes:
> 
> > 
> > Hi,
> > I would like to fit a model for a factorial design that allows for 
> > unequal variances in all groups. If I am not mistaken, this can be done 
> > in lm by specifying weights.
> 
> > A function intended to specify weights for unequal variance structures 
> > is provided in the nlme library with the varIdent function. Is it 
> > apropriate to use these weights with lm? If not, is there another 
> > possibility to do factorial designs with heteroscedasticity?
> 
> No, varIdent and friends is made for use with mixed effect models in package 
> nlme.

- or with generalized least squares, using the gls() function in the same
package. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From abunn at whrc.org  Thu Nov  3 22:25:17 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 3 Nov 2005 16:25:17 -0500
Subject: [R] Add dots at the mean of a bwplot using panel.points
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIAEFADMAA.abunn@whrc.org>
Message-ID: <NEBBIPHDAMMOKDKPOFFIMEFGDMAA.abunn@whrc.org>

> How can I modify the example below to put a dot at the mean of each violin
> plot? I assume I use panel.points but that's as far as I can go.
>
>      bwplot(voice.part ~ height, singer,
>             panel = function(..., box.ratio) {
>                 panel.violin(..., col = "transparent",
>                              varwidth = FALSE, box.ratio = box.ratio)
>                 #panel.points(mean(x.....))
>             } )

Well, I answered my own question and learned something of the dark art of
lattice plots in the process. If this is an inane way to go about this, then
somebody please say so.

panel.mean <- function(x,y,...){
  y <- as.numeric(y)
  y.unique <- sort(unique(y))
  for(Y in y.unique) {
    X <- x[y == Y]
    if (!length(X)) next
    mean.value <- list(x = mean(X), y = Y)
    do.call("lpoints", c(mean.value, pch = 20))
  }
}

bwplot(voice.part ~ height, singer,
            panel = function(...) {
              panel.violin(...,col = "transparent")
              panel.mean(...)
              })



From worknit at gmail.com  Thu Nov  3 22:52:15 2005
From: worknit at gmail.com (Jon Savian)
Date: Thu, 3 Nov 2005 13:52:15 -0800
Subject: [R] Using R with SGE
Message-ID: <8d9f49590511031352s5e328380mf45ae54baab9125f@mail.gmail.com>

Hello,

I was wondering if there is a way to use R with the Sun Grid Engine,
as opposed to using Rmpi.  Our sge has a lammpi parrallel environment
as well.  Does anyone have a script to submit a batch R job using the
SGE scheduler?  Or can point me to a good tutorial?

Thanks



From nicholas-ettinger at uiowa.edu  Thu Nov  3 22:53:05 2005
From: nicholas-ettinger at uiowa.edu (Ettinger, Nicholas)
Date: Thu, 3 Nov 2005 15:53:05 -0600
Subject: [R] Error message: " The following object(s) are masked"
Message-ID: <A4AA05CE92DACC43886D133DB94A926E205EA993@medicine-exch1.medicine.uiowa.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051103/9ff1e7aa/attachment.pl

From gunter.berton at gene.com  Thu Nov  3 23:39:44 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 3 Nov 2005 14:39:44 -0800
Subject: [R] Help in expand.grid() (Restricted combination)
In-Reply-To: <fc5b8ae70511031207r5ef61ac8q81d43371c8086dba@mail.gmail.com>
Message-ID: <200511032239.jA3Mdk5s004648@ohm.gene.com>

If I understand you correctly, you cannot do this with a data.frame, which
must be rectangular with equal numbers of entries (columns) in each row. See
?.data.frame and read "An Introduction to R" for these basics.  You could
make the 3rd column for exhaustive = NA (or maybe an empty string, "") I
suppose, and rbind the results you want:

part1<-expand.grid(e='nearest-neighbor',d=c(70,75,80,85,90,92,94,96,98,99),
	n=c(20,25,30,35,40))

part2<-data.frame(e='exhaustive',d=c(70,75,80,85,90,92,94,96,98,99),n=NA)

result<-rbind(part1,part2)


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prasanna
> Sent: Thursday, November 03, 2005 12:07 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help in expand.grid() (Restricted combination)
> 
> Dear Rs:
> 
> BY having the following code:
> 
> candidates<-expand.grid(e=c("nearest-neighbor","exaustive"),
> d=c(70,75,80,85,90,92,94,96,98,99),
> n=c(20,25,30,35,40))
> 
> results in :
> 
> e d n
> 1 nearest-neighbor 70 20
> 2 exaustive 70 20
> 3 nearest-neighbor 75 20
> 4 exaustive 75 20
> ................
> 90 exaustive 90 40
> 91 nearest-neighbor 92 40
> 92 exaustive 92 40
> 93 nearest-neighbor 94 40
> 94 exaustive 94 40
> 95 nearest-neighbor 96 40
> 96 exaustive 96 40
> 97 nearest-neighbor 98 40
> 98 exaustive 98 40
> 99 nearest-neighbor 99 40
> 100 exaustive 99 40
> 
> 
> I need to associate "nearest-neighbor" with
> d=c(70,75,80,85,90,92,94,96,98,99), n=c(20,25,30,35,40)
> but "exaustive" only with d=c(70,75,80,85,90,92,94,96,98,99). 
> Therefore I
> will have only 50+10 combinations not 100.
> I need to have combination as shown below
> 
> 1 nearest-neighbor 70 20
> 2 exaustive 70
> 3 nearest-neighbor 75 20
> 4 exaustive 75
> ................
> 60
> 
> Thanks
> Prasanna
> 
> 
> --
> Prasanna BALAPRAKASH
> IRIDIA, Universiti Libre de Bruxelles
> 50, Av. F. Roosevelt, CP 194/6
> 1050 Brussels
> Belgium.
> 
> 	[[alternative HTML version deleted]]
> 
>



From SoukupM at cder.fda.gov  Thu Nov  3 23:40:34 2005
From: SoukupM at cder.fda.gov (Soukup, Mat)
Date: Thu, 3 Nov 2005 17:40:34 -0500
Subject: [R] Potential for R to conflict with other softwares
Message-ID: <9AB9FB1124563249B0B39A1FEF1E94B37E1B9E@cdsx06.cder.fda.gov>

I just wanted to make one clarification about my statement: "After some
time, my collegues at the Food and Drug Adminstration have finally
acknowledged R as a powerful statistical computing environment." I did not
intend for this to read that R has been acknowledged as being  21 CFR Part
11 compliant. This is a whole other ball game. What I meant to say is that,
within the Center of Drug Evaluation and Research (CDER) the Office of
Biostatistics is willing to look into whether or not reviewers can download
R onto their government issued PC's. And this must all be approved by the
Office of Information and Technology. So admittedly, this is only a small
step, but nonetheless it is a step in the right direction.

I apologize for any confusion. Cheers,

Mat

Disclaimer which I also forgot in the original post: The following views are
those of the writer and do not necessarily reflect those of the FDA.



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
> Sent: Thursday, November 03, 2005 6:30 AM
> To: Soukup, Mat
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] Potential for R to conflict with other softwares
> 
> On 11/3/2005 9:11 AM, Soukup, Mat wrote:
> > Hi.
> > 
> > After some time, my collegues at the Food and Drug 
> Adminstration have
> > finally acknowledged R as a powerful statistical computing 
> environment.
> > However, in order to comply with the Office of Information 
> and Technology
> > standards there are a couple of questions about whether R 
> could interfere
> > with other software. As I'm more of a driver of the R 
> software and not a
> > mechanic, I was hoping for the insight of the many great 
> useRs. Below is a
> > list of 5 proposed questions to which I value any comment.
> > 
> > Thank you for your time,
> > 
> > -Mat
> > 
> > 
> 
> These answers are about the Windows version only, but from the 
> questions, I think that's what you were looking for.  They 
> apply to all 
> versions since 1.6.x at least (though the earlier ones would have put 
> fewer entries into the registry, they put them in the same places).
> 
> > 1. Does R have high resolution graphics?
> 
> Yes, but I don't think I get the point of this question.  How 
> would that 
> interfere with other software?
> > 
> > 2. Does R have .dll files, or other executables which are 
> not located in the
> > R software directory tree?
> 
> No, it installs everything below R_HOME.
> > 
> > 3. Does R modify the Windows registry in a non-obvious way, 
> i.e. other than
> > defining itself and what extensions to associate with R, 
> and what are those
> > extensions?
> 
> I think all of its modifications would count as obvious.  They are 
> mainly below HKLM/Software/R-core or HKCU/Software/R-core (where the 
> file locations are recorded); additionally file associations 
> are set up 
> for .Rdata files (which are called RWorkspace files there), and an 
> uninstall entry is made.
> > 
> > 4. Does R add macros to any part of MS Office?
> 
> No.
> > 
> > 5. Can you anticipate any other way in which installing and 
> using R could
> > disrupt the operation of another software?
> 
> No, not really.  Maybe users will become addicted to it?  ;-)
> 
> Duncan Murdoch
> >  
> > 
> > 
> **************************************************************
> *********
> > Mat Soukup, Ph.D.
> > Food and Drug Administration
> > 10903 New Hampshire Ave. 
> > BLDG 22 RM 5329
> > Silver Spring, MD 20993-0002
> > Phone: 301.796.1005
> > 
> **************************************************************
> *********
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From deepayan.sarkar at gmail.com  Fri Nov  4 00:10:24 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 3 Nov 2005 17:10:24 -0600
Subject: [R] Add dots at the mean of a bwplot using panel.points
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIMEFGDMAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIAEFADMAA.abunn@whrc.org>
	<NEBBIPHDAMMOKDKPOFFIMEFGDMAA.abunn@whrc.org>
Message-ID: <eb555e660511031510v420b7242x100b82e7913012b4@mail.gmail.com>

On 11/3/05, Andy Bunn <abunn at whrc.org> wrote:
> > How can I modify the example below to put a dot at the mean of each
> violin
> > plot? I assume I use panel.points but that's as far as I can go.
> >
> >      bwplot(voice.part ~ height, singer,
> >             panel = function(..., box.ratio) {
> >                 panel.violin(..., col = "transparent",
> >                              varwidth = FALSE, box.ratio = box.ratio)
> >                 #panel.points(mean(x.....))
> >             } )
>
> Well, I answered my own question and learned something of the dark art of
> lattice plots in the process. If this is an inane way to go about this,
> then
> somebody please say so.

panel.mean <- function(x, y, ...) {
    tmp <- tapply(x, y, FUN = mean)
    panel.points(tmp, seq(tmp), pch = 20, ...)
}

is more direct, but otherwise your solution is fine.

-Deepayan

> panel.mean <- function(x,y,...){
>   y <- as.numeric(y)
>   y.unique <- sort(unique(y))
>   for(Y in y.unique) {
>     X <- x[y == Y]
>     if (!length(X)) next
>     mean.value <- list(x = mean(X), y = Y)
>     do.call("lpoints", c(mean.value, pch = 20))
>   }
> }
>
> bwplot(voice.part ~ height, singer,
>             panel = function(...) {
>               panel.violin(...,col = "transparent")
>               panel.mean(...)
>               })



From ggrothendieck at gmail.com  Fri Nov  4 02:53:04 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 3 Nov 2005 20:53:04 -0500
Subject: [R] RODBC and Excel: Wrong Data Type Assumed on Import
In-Reply-To: <adf71a630511030726v10f1dc77sa7c760b96a27480f@mail.gmail.com>
References: <adf71a630511030726v10f1dc77sa7c760b96a27480f@mail.gmail.com>
Message-ID: <971536df0511031753y6eb4e271o797e862bc2740e1a@mail.gmail.com>

You could try using the COM interface rather than the ODBC
interface.  Try code such as this:

library(RDCOMClient)
xls <- COMCreate("Excel.Application")
xls[["Workbooks"]]$Open("MySpreadsheet.xls")
sheet <- xls[["ActiveSheet"]]
mydata <- sheet[["UsedRange"]][["value"]]
xls$Quit()

# convert mydata to a character matrix
mydata.char <- matrix(unlist(mydata), nc = length(xx))



On 11/3/05, Kevin Wright <kwright68 at gmail.com> wrote:
> >From my experience (somewhat of a guess):
>
> 1.
>
> Excel uses the first 16 rows of data to determine if a column is numeric or
> character. The data type which is most common in the first 16 rows will then
> be used for the whole column. If you sort the data so that at least the
> first 9 rows have character data, you may find this allows the data to be
> interpreted as character. There is supposedly a registy setting that can
> control how many lines to use (instead of 16), but I have not had success
> with the setting. I suspect that ODBC uses JET4, which may be the real
> source of the problem. See more here:
> http://www.dicks-blog.com/archives/2004/06/03/external-data-mixed-data-types/
>
> 2.
>
> The gregmisc bundle has a different read.xls function that uses a Perl
> script (xls2csv) and seems to be safer with mixed-type columns.
> Requires a working version of Perl.
>
> Best,
>
> Kevin Wright
>
>
>
> The first column in my Excel sheet has mostly numbers but I need to treat it
> as character data:
>
> > library(RODBC)
> <http://tolstoy.newcastle.edu.au/R/help/05/09/11324.html#14938qlink1>
> *> channel <- odbcConnectExcel("U:/efg/lab/R/Plasmid/construct list.xls") *
> *> plasmid <- sqlFetch(channel,"Sheet1", as.is=TRUE) *
> *> odbcClose(channel) *
>
> > names(plasmid)
>
> [1] "Plasmid Number" "Plasmid" "Concentration" "Comments" "Lost"
>
> # How is the type decided? I need a character type.
> > class(plasmid$"Plasmid Number")
>
> [1] "numeric"
> > typeof(plasmid$"Plasmid Number")
>
> [1] "double"
>
> > plasmid$"Plasmid Number"[273:276]
>
> [1] 274 NA NA 276
>
> The two NAs are supposed to be 275a and 275b. I tried the "as.is=TRUE" but
> that didn't help.
>
> I consulted Section 4, Relational databases, in the R Data Import/Export
> document (for Version 2.2.0).
>
> Section 4.2.2, Data types, was not helpful. In particular, this did not seem
> helpful: "The more comprehensive of the R interface packages hide the type
> conversion issues from the user."
>
> Section 4.3.2, Package RODBC, provided a "simple example of using ODBC ..
> with a(sic) Excel spreadsheet" but is silent on how to control the data type
> on import. Could the documentation be expanded to address this issue?
>
> I really need to show "Plasmid 275a" and "Plasmid 275b" instead of "Plasmid
> NA".
>
> Thanks for any help with this.
>
> efg
>
> --
> Earl F. Glynn
> Scientific Programmer
> Bioinformatics Department
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Fri Nov  4 03:47:40 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 03 Nov 2005 18:47:40 -0800
Subject: [R] FIGARCH
In-Reply-To: <14850601FF012647A90A5DB31F96DB371D1703@INBLRDC01.BANG.irpvl.com>
References: <14850601FF012647A90A5DB31F96DB371D1703@INBLRDC01.BANG.irpvl.com>
Message-ID: <436ACBCC.6070402@pdf.com>

	  RSiteSearch("FIGARCH") revealed that Diethelm Wuertz prepared an R 
interface to Ox Garch.  However, "Ox and all its components are 
copyright of Jurgen A. Doornik. The Console (command line) versions may 
be used freely for academic research and teaching purposes only. 
Commercial users and others who do not qualify for the free version must 
purchase the Windows version of Ox ... ."  See 
"http://finzi.psych.upenn.edu/R/library/fSeries/html/A3-GarchOxModelling.html".

	  I don't know if this will help you.
	  Best Wishes,
	  Spencer Graves

Sumanta Basak wrote:
> Hi All,
> 
>  
> 
> Currently I'm working in FIGARCH process [Fractionally Integrated
> Generalized Autoregressive Conditional Heteroscedasticity]. I've already
> got the codes to do the process in S-Plus. Can anyone help me to do it
> in R?
> 
>  
> 
>  
> 
> Thanks,
> 
> SUMANTA BASAK.
> 
> 
> -------------------------------------------------------------------------------------------------------------------
> This e-mail may contain confidential and/or privileged infor...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From redbeard at arrr.net  Fri Nov  4 07:31:55 2005
From: redbeard at arrr.net (Jarrett Byrnes)
Date: Thu, 3 Nov 2005 22:31:55 -0800
Subject: [R] Plotting Factorial GLMs
Message-ID: <c46712003b9b3318f93dc83223406033@arrr.net>

Hello all,
	I'm attempting to plot the functions from a generalized linear model 
while iterating over multiple levels of a factor in the model.  In 
other words, I have a data set

Block, Treatment.Level, Response.Level

So, the glm and code to plot should be

logit.reg<-glm(formula = Response.Level ~ Treatment.Level + Block,
			family=quasibinomial(link="logit")))

plot( Response.Level ~ Treatment.Level)

logit.reg.function <- function (trt, blk) predict(logit.reg, 
data.frame(Treatment.Level=trt, Block=blk)

curve(logit.reg.function(x, "A"), add=TRUE)


But I get the error:
Error in xy.coords(x, y) : 'x' and 'y' lengths differ

Now, if I set Block="A" in the function, and take blk out, as well as 
taking the "A" out of the curve statement, it plots just fine.  What am 
I doing wrong, as this would be a nice, quick, and easy way to whip up 
multiple curves from a factorial dataset!



From sfalcon at fhcrc.org  Fri Nov  4 07:43:40 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 03 Nov 2005 22:43:40 -0800
Subject: [R] Search within a file
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F414B@us-arlington-0668.mail.saic.com>
	(Jaroslaw W. Tuszynski's message of "Thu, 3 Nov 2005 10:00:37 -0500")
References: <CA0BCF3BED56294AB91E3AD74B849FD57F414B@us-arlington-0668.mail.saic.com>
Message-ID: <m2r79wud43.fsf@macaroni.local>

On  3 Nov 2005, JAROSLAW.W.TUSZYNSKI at saic.com wrote:
> I am looking for a way to search a file for position of some
> expression, from within R. My current code:
>
> sha1Pos = gregexpr("<sha1>", readChar(filename,
> file.info(filename)$size))[[1]]
>
> Works fine for small files, but text files I will be working with
> might get up to Gb range, so I was trying to accomplish the same
> without loading the whole file into R.

I would think you could use readLines to read in a batch of lines, run
(g)regexpr, and keep track of matches and position.

Create a connection to the file using file() first, and then
subsequent calls to readLines will start where you left off.

But you will need to adjust the position indices returned by gregexpr
by how far into the file you are.  Seems very doable.

+ seth



From petr.pikal at precheza.cz  Fri Nov  4 08:50:56 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 04 Nov 2005 08:50:56 +0100
Subject: [R] Problems with abline adding regression line to a graph
In-Reply-To: <436A26A9.7050603@evp.slu.se>
Message-ID: <436B20F0.16047.171107@localhost>

Hi

On 3 Nov 2005 at 16:03, CG Pettersson wrote:

Date sent:      	Thu, 03 Nov 2005 16:03:05 +0100
From:           	CG Pettersson <cg.pettersson at evp.slu.se>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Problems with abline adding regression line to a graph

> Hello all,
> 
> R2.1.1, W2k
> 
> I try to make a plot of a simple regression model in this way:
> 
>  > with(njfA_bcd, {
> + plot(TC_OS.G31,Prot,cex = 2, col = "red", xlab= "TC/OS at GS32",
> + ylab="Grain crude protein (CP)")
> + })
> 
> This part works well and produces the datapoints as red circles.
> When I try to add a line, using a fitted linear model in a way
> that works perfect with other variables in the same dateset the
> following happens:
> 
>  > with(njfA_bcd, {
> +    abline(lm(predict(m1tc) ~ TC_OS.G31), lty = 1, col = "red")
> + })
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>         variable lengths differ
> 
> And this means?
> 
> There exists missing values for TC_OS.G31 in the dataset. From the
> beginning m1tc was a lm() object, which gave the same Error message.
> To try to fix the problem I changed to lme() and used
> na.action=na.omit explicitely, but this didn??t help.

na.action=na.exclude

should help.

It should be probably mentioned in lm help page, at least as a 
hyperlink to na.* parametrs.

HTH
Petr



> 
> Here is the summary of m1tc:
> 
>  > summary(m1tc)
> Linear mixed-effects model fit by REML
>  Data: njfA_bcd
>        AIC      BIC    logLik
>   209.4914 219.0692 -100.7457
> 
> Random effects:
>  Formula: ~1 | Trial
>         (Intercept)  Residual
> StdDev:    1.242184 0.6520464
> 
> Fixed effects: Prot ~ TC_OS.G31
>                 Value Std.Error DF   t-value p-value
> (Intercept)  14.86209  0.957630 68 15.519662       0
> TC_OS.G31   -24.22286  4.792801 68 -5.054008       0
>  Correlation:
>           (Intr)
> TC_OS.G31 -0.935
> 
> Standardized Within-Group Residuals:
>         Min          Q1         Med          Q3         Max
> -1.68329774 -0.73751040 -0.05600477  0.68301243  2.21693174
> 
> Number of Observations: 83
> Number of Groups: 14
>  >
> 
> What is happening and what shall I do about it?
> 
> Cheers
> /CG
> 
> -- 
> CG Pettersson, MSci, PhD Stud.
> Swedish University of Agricultural Sciences (SLU)
> Dept. of Crop Production Ecology. Box 7043.
> SE-750 07 UPPSALA, Sweden.
> +46 18 671428, +46 70 3306685
> cg.pettersson at evp.slu.se
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ligges at statistik.uni-dortmund.de  Fri Nov  4 09:00:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Nov 2005 09:00:15 +0100
Subject: [R] Error message: " The following object(s) are masked"
In-Reply-To: <A4AA05CE92DACC43886D133DB94A926E205EA993@medicine-exch1.medicine.uiowa.edu>
References: <A4AA05CE92DACC43886D133DB94A926E205EA993@medicine-exch1.medicine.uiowa.edu>
Message-ID: <436B150F.1060005@statistik.uni-dortmund.de>

Ettinger, Nicholas wrote:

> Hello!
> 
> First time posting here:
> 
> Here is my code:
> 
> x <- c(1:22)
> finaloutput=cidrm=NULL
> finaldiversityoutput=diversitym=NULL
> 
> diversityinfo=read.table("Diversity_info.txt", header=T, sep="\t",
> row.names=NULL)
> attach(diversityinfo)
> diversitynr=nrow(diversityinfo)
> diversitytemp <- matrix(0,nrow=diversitynr,ncol=1)
> 
> for(j in 1:length(x))
> {
> diversitym=read.table(paste(paste("Div-Chr",x[j],
> sep=""),"_corr.txt",sep=""), header=T, sep="\t", row.names=NULL)
> attach(diversitym)
> diversitytemp <- cbind(diversitytemp,diversitym)
> print(paste(paste("Diversity-Chrom",x[j], sep=""),".txt",sep=""))
> print(dim(diversitytemp))
> }
> 
> I am essentially trying to combine several tab-delimited text data files
> together into one big file.
> 
> I recently upgraded to R 2.2.0.  I now get multiple error messages of
> the form:
> The following object(s) are masked from diversitym ( position 4 ) :
> 
>          X X.1 X.2


These are *Warning* messges, not errors.

You are using attach but never detach in your loop. Each attach masks 
the objects you have attached formerly, because there are the same names 
in it, obviously.

Your code works as before, but you should not use attach without 
detaching. I'd suggest not to use attach() at all unless you know what 
you are doing and it really improved convenience in interactive analyses.

Uwe Ligges



> I searched on "masked" and read the manual about "conflicts" but I don't
> really understand what the issue is.  I didn't get this error message,
> using the same code with R 2.1.0.  Can somebody help (I'm not a
> programmer).
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bartjoosen at hotmail.com  Fri Nov  4 10:09:41 2005
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Fri, 04 Nov 2005 09:09:41 +0000
Subject: [R] t test on data frame
Message-ID: <BAY111-F16F5DFFAED32EF6DDA5BD7D8600@phx.gbl>



Hi,

I have constructed a dataframe as follows:

Oil <- rep(c("Oil1","Oil2","Oil3"),8)
Comp <- rep(rep(c("C1","C2"),c(4,4)),3)
Mth <- rep(c("M1","M1","M2","M2"),6)
Meas <- rep(c(1,2),12)
Result <- rnorm(24,mean=100, sd=5)
df <- data.frame(Oil, Comp, Mth, Meas, Result)

The same compound (Comp) is found in different oils.
Now I want to see if there are differences in method per compound.
I tried this one:
df.t <- t.test(Result~Mth | Comp, df)

But I get:
Error in t.test.formula(Result ~ Mth | Comp, df) :
         grouping factor must have exactly 2 levels
In addition: Warning message:
| not meaningful for factors in: Ops.factor(Mth, Comp)

How can I get my results like:
Comp1: Mean Mth 1: ....
           Mean Mth 2: ...
           t = ....
Comp 2: ....
....

Is this possible?
And please: How can I do it?

Kind regards

Bart



From stefaan.lhermitte at biw.kuleuven.be  Fri Nov  4 10:28:23 2005
From: stefaan.lhermitte at biw.kuleuven.be (Stefaan Lhermitte)
Date: Fri, 04 Nov 2005 10:28:23 +0100
Subject: [R] Simplify iterative programming
Message-ID: <436B29B7.5070202@biw.kuleuven.be>

Dear,

I am looking for the simplification of a formula to improve the
calculation speed of my program. Therefore I want to simplify the
following formula:

H = sum{i=0..n-1 ,  [ sum {j=0..m-1 ,  sqrt ( (Ai - Bj)^2 + (Ci -
Dj)^2) }  ]  }

where:
A, C = two vectors (with numerical data) of length n
B, D = two vectors (with numerical data) of length m
sqrt = square root
Ai = element of A with index i
Bj = element of B with index j
Ci = element of C with index i
Dj = element of C with index j

I am calculating H in a merging process so A, B will merge in step 2
into A' and C, D into C':
A' = {A,B} : vector of length (n+m)
C' = {C,D} : vector of length (n+m)

Then again I will calculate H with two new vectors X and Y (of length
p):
H = sum{i=0..n+m-1 ,  [ sum {j=0..p-1 ,  sqrt ( (A'i - Xj)^2 + (C'i -
Yj)^2) }  ]  }

These steps are iterated in a loop with always new vectors (e.g. X and
Y)

Now I'am looking for a simplication of H in order to avoid long
calculation time.
I know a computional simplified formula exists for the standard
deviation (sd) that is much easier in iterative programming. Therefore
I wondered I anybody knew about analog simplifications to simplify H:

sd = sqrt [ sum{i=0..n-1, (Xi - mean(X) )^2 ) /n } ]  -> simplified
computation
-> sqrt [  (n *  sum{i=0..n-1,  X^2 } ) - ( sum{i=0..n-1,  X } ^2 ) /
n^2 ]

This simplied formula is much easier in iterative programming, since I
don't have to keep every element of X .

For example if we want to calculate sd of A' with the vectors A and C:
sd(A')
= sqrt [  ((n+m) *  sum{i=0..n+m-1,  A'^2 } ) - ( sum{i=0..n+m-1,  A' }
^2 ) /  (n+m)^2 ]
= sqrt [  ((n+m)*  (sum{i=0..n,  A^2 } + sum{i=0..m,  C^2 } ) )
     - ( ( sum{i=0..n-1,  A } + sum{i=0..m-1,  C } )^2 ) /  (n+m)^2 ]

The advantage of this formula is, that I don't have to keep every value
of A and C to calculate sd(A'). I can do the following replacements:
sum{i=0..n,  A^2 } = A2
sum{i=0..m,  C^2 } = C2
sum{i=0..n-1,  A } = A3
sum{i=0..m-1,  C } = C3

So sd(A')=
  sqrt [  ( (n+m)*(A2+ C2) )  - ( (A3 + C3)^2 ) /  (n+m)^2 ]

In this way my computation intensive calculation is replaced by a
calculation of simple numbers.

Can anybody help me to do something comparable for H? Any other help to
calculate H easily in an iterative process is also welcome!

Kind regards,
Stef

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From kristel.joossens at econ.kuleuven.be  Fri Nov  4 11:03:31 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Fri, 04 Nov 2005 11:03:31 +0100
Subject: [R] Simplify iterative programming
In-Reply-To: <436B29B7.5070202@biw.kuleuven.be>
References: <436B29B7.5070202@biw.kuleuven.be>
Message-ID: <436B31F3.6000600@econ.kuleuven.be>

Hello Stefaan,

I'm not an expert, but maybe something like this is quite 
straightforward withourtusing for-loops? (It is an idea I wrote down, 
but check of course if this is correct!)

term1 =(matrix(A,ncol=m,nrow=n)-matrix(B,ncol=m,nrow=n,byrow=TRUE))^2
term2 =(matrix(C,ncol=m,nrow=n)-matrix(D,ncol=m,nrow=n,byrow=TRUE))^2
H = sum(sqrt(term1+term2))

Kind regards,
Kristel

Stefaan Lhermitte wrote:
> Dear,
> 
> I am looking for the simplification of a formula to improve the
> calculation speed of my program. Therefore I want to simplify the
> following formula:
> 
> H = sum{i=0..n-1 ,  [ sum {j=0..m-1 ,  sqrt ( (Ai - Bj)^2 + (Ci -
> Dj)^2) }  ]  }
> 
> where:
> A, C = two vectors (with numerical data) of length n
> B, D = two vectors (with numerical data) of length m
> sqrt = square root
> Ai = element of A with index i
> Bj = element of B with index j
> Ci = element of C with index i
> Dj = element of C with index j
> 
> I am calculating H in a merging process so A, B will merge in step 2
> into A' and C, D into C':
> A' = {A,B} : vector of length (n+m)
> C' = {C,D} : vector of length (n+m)
> 
> Then again I will calculate H with two new vectors X and Y (of length
> p):
> H = sum{i=0..n+m-1 ,  [ sum {j=0..p-1 ,  sqrt ( (A'i - Xj)^2 + (C'i -
> Yj)^2) }  ]  }
> 
> These steps are iterated in a loop with always new vectors (e.g. X and
> Y)
> 
> Now I'am looking for a simplication of H in order to avoid long
> calculation time.
> I know a computional simplified formula exists for the standard
> deviation (sd) that is much easier in iterative programming. Therefore
> I wondered I anybody knew about analog simplifications to simplify H:
> 
> sd = sqrt [ sum{i=0..n-1, (Xi - mean(X) )^2 ) /n } ]  -> simplified
> computation
> -> sqrt [  (n *  sum{i=0..n-1,  X^2 } ) - ( sum{i=0..n-1,  X } ^2 ) /
> n^2 ]
> 
> This simplied formula is much easier in iterative programming, since I
> don't have to keep every element of X .
> 
> For example if we want to calculate sd of A' with the vectors A and C:
> sd(A')
> = sqrt [  ((n+m) *  sum{i=0..n+m-1,  A'^2 } ) - ( sum{i=0..n+m-1,  A' }
> ^2 ) /  (n+m)^2 ]
> = sqrt [  ((n+m)*  (sum{i=0..n,  A^2 } + sum{i=0..m,  C^2 } ) )
>      - ( ( sum{i=0..n-1,  A } + sum{i=0..m-1,  C } )^2 ) /  (n+m)^2 ]
> 
> The advantage of this formula is, that I don't have to keep every value
> of A and C to calculate sd(A'). I can do the following replacements:
> sum{i=0..n,  A^2 } = A2
> sum{i=0..m,  C^2 } = C2
> sum{i=0..n-1,  A } = A3
> sum{i=0..m-1,  C } = C3
> 
> So sd(A')=
>   sqrt [  ( (n+m)*(A2+ C2) )  - ( (A3 + C3)^2 ) /  (n+m)^2 ]
> 
> In this way my computation intensive calculation is replaced by a
> calculation of simple numbers.
> 
> Can anybody help me to do something comparable for H? Any other help to
> calculate H easily in an iterative process is also welcome!
> 
> Kind regards,
> Stef
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 

_________________________________________________________________
Kristel Joossens
Ph.D. Student
Faculty of Economics and Applied Economics
Research center ORSTAT
K.U. Leuven
Naamsestraat 69
B-3000 Leuven
Belgium
Tel: +32 16 326929
Fax: +32 16 326732
E-mail: Kristel.Joossens at econ.kuleuven.be
Url: http://www.econ.kuleuven.ac.be/Kristel.Joossens/public/

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From vdooren at rulsfb.leidenuniv.nl  Fri Nov  4 11:43:47 2005
From: vdooren at rulsfb.leidenuniv.nl (Tom Van Dooren)
Date: Fri, 04 Nov 2005 11:43:47 +0100
Subject: [R] residual variance in lmer
Message-ID: <436B3B63.3000205@rulsfb.leidenuniv.nl>

Hi,
when running lmer with family set to quasibinomial,
is the "residual" variance reported among the random effects, the 
dispersion parameter as in glm, or something else?
Thanks,
Tom



From jens at binf.ku.dk  Fri Nov  4 11:52:48 2005
From: jens at binf.ku.dk (Jens Vilstrup Johansen)
Date: Fri, 04 Nov 2005 11:52:48 +0100
Subject: [R] How to load a workspace, which was created in Windows, in Unix
Message-ID: <436B3D80.8050704@binf.ku.dk>

Hi all

I can't seem to load a workspace created in Windows, when I try to load
it in R on a Unix machine.
How can I do it?

Thanks!

regards,
 Jens Johansen



From dieter.menne at menne-biomed.de  Fri Nov  4 11:57:00 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 4 Nov 2005 10:57:00 +0000 (UTC)
Subject: [R] t test on data frame
References: <BAY111-F16F5DFFAED32EF6DDA5BD7D8600@phx.gbl>
Message-ID: <loom.20051104T115404-940@post.gmane.org>

Bart Joosen <bartjoosen <at> hotmail.com> writes:
> I have constructed a dataframe as follows:
> 
> Oil <- rep(c("Oil1","Oil2","Oil3"),8)
> Comp <- rep(rep(c("C1","C2"),c(4,4)),3)
> Mth <- rep(c("M1","M1","M2","M2"),6)
> Meas <- rep(c(1,2),12)
> Result <- rnorm(24,mean=100, sd=5)
> df <- data.frame(Oil, Comp, Mth, Meas, Result)
> 
> The same compound (Comp) is found in different oils.
> Now I want to see if there are differences in method per compound.
> I tried this one:
> df.t <- t.test(Result~Mth | Comp, df)
> 
> But I get:
> Error in t.test.formula(Result ~ Mth | Comp, df) :
>          grouping factor must have exactly 2 levels
> In addition: Warning message:
> | not meaningful for factors in: Ops.factor(Mth, Comp)
> 

The Docs say:

>The formula interface is only applicable for the 2-sample tests. 

So the grouping cannot by used like one could expect to say "do it by groups". 
You must do it manually, for example:

by(df, df$Comp, function(x) t.test(Result~Mth,x))

See docs of "by" how to extract parameters from the results instead of 
printing. And don't forget multtest, if you superiors ask for all the 300 
compounds your company produces against each other.

Dieter



From p.dalgaard at biostat.ku.dk  Fri Nov  4 12:24:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Nov 2005 12:24:39 +0100
Subject: [R] How to load a workspace, which was created in Windows,
	in Unix
In-Reply-To: <436B3D80.8050704@binf.ku.dk>
References: <436B3D80.8050704@binf.ku.dk>
Message-ID: <x2wtjoy7t4.fsf@viggo.kubism.ku.dk>

Jens Vilstrup Johansen <jens at binf.ku.dk> writes:

> Hi all
> 
> I can't seem to load a workspace created in Windows, when I try to load
> it in R on a Unix machine.
> How can I do it?

load("myfile") should work...

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From leog at anicca-vijja.de  Fri Nov  4 12:41:35 2005
From: leog at anicca-vijja.de (=?ISO-8859-1?Q?Leo_G=FCrtler?=)
Date: Fri, 04 Nov 2005 12:41:35 +0100
Subject: [R] plot of mice.mids objects
Message-ID: <436B48EF.6000701@anicca-vijja.de>

Hello...

how can I plot mice.mids objects as described by Buuren (2000)
http://web.inter.nl.net/users/S.van.Buuren/mi/docs/Manual.pdf
page 17, if there are many variables (~80) with NAs included?
mice runs well, but the plot is not possible because it seems that there 
are too many variables.

 > imp.temp <- mice(data.withnas, m=5)
 > plot(imp.temp)
Fehler in plot.new() : Grafikr??nder zu gro??
= error in plot.new(): boundaries of graphics too big/huge

Is it possible to run the plot() sequentially, so that only a selected 
part of the variables are plotted?
Of course I can run mice() on a reduced dataset, but then the 
estimations are different what is not useful here.

thanks + best,

leo


-- 

email: leog at anicca-vijja.de
www: http://www.anicca-vijja.de/



From dejongroel at gmail.com  Fri Nov  4 12:57:05 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Fri, 04 Nov 2005 12:57:05 +0100
Subject: [R] plot of mice.mids objects
In-Reply-To: <436B48EF.6000701@anicca-vijja.de>
References: <436B48EF.6000701@anicca-vijja.de>
Message-ID: <436B4C91.5050309@gmail.com>

Hi,

it is impossible to plot a subset of the variables at the moment, but we 
could add it.

p.s. The latest version of mice is v1.14, which can be downloaded from 
CRAN.

Roel de Jong.

Leo G??rtler wrote:
> Hello...
> 
> how can I plot mice.mids objects as described by Buuren (2000)
> http://web.inter.nl.net/users/S.van.Buuren/mi/docs/Manual.pdf
> page 17, if there are many variables (~80) with NAs included?
> mice runs well, but the plot is not possible because it seems that there 
> are too many variables.
> 
>  > imp.temp <- mice(data.withnas, m=5)
>  > plot(imp.temp)
> Fehler in plot.new() : Grafikr??nder zu gro??
> = error in plot.new(): boundaries of graphics too big/huge
> 
> Is it possible to run the plot() sequentially, so that only a selected 
> part of the variables are plotted?
> Of course I can run mice() on a reduced dataset, but then the 
> estimations are different what is not useful here.
> 
> thanks + best,
> 
> leo
> 
>



From ripley at stats.ox.ac.uk  Fri Nov  4 13:00:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Nov 2005 12:00:37 +0000 (GMT)
Subject: [R] How to load a workspace, which was created in Windows,
 in Unix
In-Reply-To: <436B3D80.8050704@binf.ku.dk>
References: <436B3D80.8050704@binf.ku.dk>
Message-ID: <Pine.LNX.4.61.0511041158520.24909@gannet.stats>

On Fri, 4 Nov 2005, Jens Vilstrup Johansen wrote:

> Hi all
>
> I can't seem to load a workspace created in Windows, when I try to load
> it in R on a Unix machine.
> How can I do it?

What error message do you get?

It normally works (and is widely used in R packages for the .rda format), 
so did you transfer the .RData in binary mode?  For completeness, do an 
MD5 check on the file before and after transfer (via package tools).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.be  Fri Nov  4 13:22:27 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 4 Nov 2005 13:22:27 +0100
Subject: [R] Simplify iterative programming
References: <436B29B7.5070202@biw.kuleuven.be>
Message-ID: <00f801c5e13a$6b4676e0$0540210a@www.domain>

you could consider something like:

H <- sum(sqrt(outer(A, B, "-")^2 + outer(C, D, "-")^2))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Stefaan Lhermitte" <stefaan.lhermitte at biw.kuleuven.be>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, November 04, 2005 10:28 AM
Subject: [R] Simplify iterative programming


> Dear,
>
> I am looking for the simplification of a formula to improve the
> calculation speed of my program. Therefore I want to simplify the
> following formula:
>
> H = sum{i=0..n-1 ,  [ sum {j=0..m-1 ,  sqrt ( (Ai - Bj)^2 + (Ci -
> Dj)^2) }  ]  }
>
> where:
> A, C = two vectors (with numerical data) of length n
> B, D = two vectors (with numerical data) of length m
> sqrt = square root
> Ai = element of A with index i
> Bj = element of B with index j
> Ci = element of C with index i
> Dj = element of C with index j
>
> I am calculating H in a merging process so A, B will merge in step 2
> into A' and C, D into C':
> A' = {A,B} : vector of length (n+m)
> C' = {C,D} : vector of length (n+m)
>
> Then again I will calculate H with two new vectors X and Y (of 
> length
> p):
> H = sum{i=0..n+m-1 ,  [ sum {j=0..p-1 ,  sqrt ( (A'i - Xj)^2 + 
> (C'i -
> Yj)^2) }  ]  }
>
> These steps are iterated in a loop with always new vectors (e.g. X 
> and
> Y)
>
> Now I'am looking for a simplication of H in order to avoid long
> calculation time.
> I know a computional simplified formula exists for the standard
> deviation (sd) that is much easier in iterative programming. 
> Therefore
> I wondered I anybody knew about analog simplifications to simplify 
> H:
>
> sd = sqrt [ sum{i=0..n-1, (Xi - mean(X) )^2 ) /n } ]  -> simplified
> computation
> -> sqrt [  (n *  sum{i=0..n-1,  X^2 } ) - ( sum{i=0..n-1,  X } ^2 ) 
> /
> n^2 ]
>
> This simplied formula is much easier in iterative programming, since 
> I
> don't have to keep every element of X .
>
> For example if we want to calculate sd of A' with the vectors A and 
> C:
> sd(A')
> = sqrt [  ((n+m) *  sum{i=0..n+m-1,  A'^2 } ) - ( sum{i=0..n+m-1, 
> A' }
> ^2 ) /  (n+m)^2 ]
> = sqrt [  ((n+m)*  (sum{i=0..n,  A^2 } + sum{i=0..m,  C^2 } ) )
>     - ( ( sum{i=0..n-1,  A } + sum{i=0..m-1,  C } )^2 ) /  (n+m)^2 ]
>
> The advantage of this formula is, that I don't have to keep every 
> value
> of A and C to calculate sd(A'). I can do the following replacements:
> sum{i=0..n,  A^2 } = A2
> sum{i=0..m,  C^2 } = C2
> sum{i=0..n-1,  A } = A3
> sum{i=0..m-1,  C } = C3
>
> So sd(A')=
>  sqrt [  ( (n+m)*(A2+ C2) )  - ( (A3 + C3)^2 ) /  (n+m)^2 ]
>
> In this way my computation intensive calculation is replaced by a
> calculation of simple numbers.
>
> Can anybody help me to do something comparable for H? Any other help 
> to
> calculate H easily in an iterative process is also welcome!
>
> Kind regards,
> Stef
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From veand at frisurf.no  Fri Nov  4 14:05:05 2005
From: veand at frisurf.no (Vegard Andersen)
Date: Fri, 04 Nov 2005 14:05:05 +0100
Subject: [R] Background only white when using rgl.postscript()
Message-ID: <op.szpvyr0h5ukqp9@ismlap41.ad.uit.no>

Hello!

I use scatter3D() to produce my graphics. This works as expected, and I am  
able to set my background color to "black". But when I write to file using  
rgl.postscript(), the background color changes to white.

Can anyone tell me how I can avoid this problem?

Example code:

# Produces 3D graph with black background:
scatter3d(X, Y, Z, bg="black")

# Writes a file which has white background:
rgl.postscript("c://test//test3D.eps", fmt="eps")


Thanks in advance!

Best regards,
Vegard Andersen
Institute of Community Medicine
University of Tromso, Norway.



From tts_boopathy at yahoo.com  Fri Nov  4 14:32:38 2005
From: tts_boopathy at yahoo.com (shanmuha boopathy)
Date: Fri, 4 Nov 2005 05:32:38 -0800 (PST)
Subject: [R] R graphics help
Message-ID: <20051104133238.27246.qmail@web33811.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051104/8b2bcfa2/attachment.pl

From Matthias.Templ at statistik.gv.at  Fri Nov  4 14:42:58 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 4 Nov 2005 14:42:58 +0100
Subject: [R] R graphics help
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BACC5@xchg1.statistik.local>

Probably you can get some ideas from the balloonplot function in package gplots.

Best,
Matthias

> 
> Halo friends,
>  
> I have a problem to solve in R graphics..
> I have a matrix like 
>  
> A=   2 3 4
>        5 6 7
>        8 9 4
>  
> and I like to generate the same matrix in terms of shaded 
> circles in which the density of shading depends on the value...
>  
>  for eg in the above matrix A the value 2 is a circle shaded 
> very lightly
>                                                        9 is 
> of full dark circle(almost black)
>  
> so when the value increses then the darkness also increases..
>  
> Please help me in giving your suggestions..
>  
> with regards,
> boopathy.
> 
> 
> Thirumalai Shanmuha Boopathy, 
> Zimmer no : 07-15,
> R??tscher strasse 165, 
> 52072  Aachen . 
> Germany.
>  
> Home zone   :  0049 - 241 - 9813409
> Mobile zone :  0049 - 176 - 23567867
> 
> 
> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
>



From flom at ndri.org  Fri Nov  4 14:46:47 2005
From: flom at ndri.org (Peter Flom)
Date: Fri, 04 Nov 2005 08:46:47 -0500
Subject: [R] Graphics question on putting axes in the margins
Message-ID: <s36b2016.019@MAIL.NDRI.ORG>

Hello

I am writing an article demonstrating how quantile plots work.  In one
figure, I generate 3 sets of normal data, and add different sorts of
outliers to each.  I then make 9 qqnorm plots, in a 3x3 array:

                   Outlier1   Outler2     Outlier3
Dataset1
Dataset2 
Dataset3


which I did using mfrow = c(3,3).

I'd like to use the space on the page as efficiently as possible,so I
made the margins small, but that leaves no space for axes.  Is there a
way to put axes only along the bottom and left side, so that a) The
individual qqplots maintain their shape and b) less space on the page is
taken up by white space?

I tried doing this with layout, but couldn't figure out how to do it
(although I looked extensively at Paul Murrell's excellent book on R
graphics)

TIA

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
http://cduhr.ndri.org
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From ezhil02 at yahoo.com  Wed Nov  2 12:42:39 2005
From: ezhil02 at yahoo.com (A Ezhil)
Date: Wed, 2 Nov 2005 03:42:39 -0800 (PST)
Subject: [R] Help for MDS !!
Message-ID: <20051102114239.70097.qmail@web32102.mail.mud.yahoo.com>

Hi All,

I am trying to apply MDS for 4 groups in my data. The
groups are:  

groups = list( Day1C=c(9), Day1T=c(7,8,10),
Day2C=c(1,2,3,6,11,13,14,15), Day2T=c(4,5,12,16,17,18)
)

When I do the MDS plot the group1 appears twice
instead of one time in the plot. I don't know why this
is happening.

I would greatly appreciate your help in fixing this
problem.

Thanks in Advance.
Ezhil



From ggmeyers at optonline.net  Thu Nov  3 03:49:52 2005
From: ggmeyers at optonline.net (Glenn Meyers)
Date: Wed, 02 Nov 2005 21:49:52 -0500
Subject: [R] Real FFT
Message-ID: <000601c5e021$43e8f3d0$6801a8c0@Glenn>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051102/a5c39b0d/attachment.pl

From dmbates at gmail.com  Fri Nov  4 14:50:36 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 4 Nov 2005 07:50:36 -0600
Subject: [R] residual variance in lmer
In-Reply-To: <436B3B63.3000205@rulsfb.leidenuniv.nl>
References: <436B3B63.3000205@rulsfb.leidenuniv.nl>
Message-ID: <40e66e0b0511040550o15c8acaelbe7567d0925c5990@mail.gmail.com>

On 11/4/05, Tom Van Dooren <vdooren at rulsfb.leidenuniv.nl> wrote:

> when running lmer with family set to quasibinomial,
> is the "residual" variance reported among the random effects, the
> dispersion parameter as in glm, or something else?

It should be the dispersion parameter.  I say "should be" because I am
not that familiar with the quasibinomial family and I don't know
exactly how the dispersion parameter is defined.  I can tell you that
the 'residual' variance is evaluated as the ML estimate of the
variance from the last weighted penalized least squares problem used
in the iterative estimation.  Does that help?



From wwguocn at hotmail.com  Thu Nov  3 14:28:33 2005
From: wwguocn at hotmail.com (=?gb2312?B?ufkgzqzOrA==?=)
Date: Thu, 03 Nov 2005 21:28:33 +0800
Subject: [R] Can I do content analysis by R?
Message-ID: <BAY113-F173074699EEB0893093556BF610@phx.gbl>

Can R conduct content analysis? I search "content analysis" in mail 
archives and only find the concord package can compute Krippendorff's 
alpha. If R cannot cope with content analysis, does anyone can tell me some 
other softwares? Thank you very much!



From meier at stat.math.ethz.ch  Fri Nov  4 15:09:36 2005
From: meier at stat.math.ethz.ch (Lukas Meier)
Date: Fri, 4 Nov 2005 15:09:36 +0100
Subject: [R] model.matrix for non-hierarchical models
Message-ID: <17259.27552.773095.502243@stat.math.ethz.ch>

Dear R-users,

Using the function model.matrix I noticed the following
behaviour (example below): Using the formula "~ a + a:b" will
give a matrix of the same dimension as using "~ a * b". In the
first case there are additional columns for the interaction (to
compensate for the missing main-effect).

dd <- data.frame(a = gl(3,4), b = gl(4,1,12))
model.matrix(~ a*b, dd, contrasts = list(a="contr.sum", 
             b="contr.sum"))
model.matrix(~ a + a:b, dd, contrasts = list(a="contr.sum", 
             b="contr.sum"))

Is there any way to get the design matrix corresponding to "~ a
+ a:b" or do I have to do this manually?

Thanks for your help,

Lukas Meier



From ligges at statistik.uni-dortmund.de  Fri Nov  4 15:12:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Nov 2005 15:12:49 +0100
Subject: [R] question about precision, floor, and powers of two.
In-Reply-To: <e89bb7ac0510310903m77d159e6nb9f2e7188ff82d90@mail.gmail.com>
References: <e89bb7ac0510310903m77d159e6nb9f2e7188ff82d90@mail.gmail.com>
Message-ID: <436B6C61.7070606@statistik.uni-dortmund.de>

Dr Carbon wrote:

> At the risk of being beaten about the face and body, can somebody explain
> why the middle example: log2(2^3); floor(log2(2^3)) is different than
> examples 1 and 3?


Because

 > log2(2^3) - 3
[1] -4.440892e-16

see the R FAQ "Why doesn't R think these numbers are equal?".

Uwe Ligges

> 
>>log2(2^2); floor(log2(2^2))
> 
> [1] 2
> [1] 2
> 
>>log2(2^3); floor(log2(2^3))
> 
> [1] 3
> [1] 2
> 
>>log2(2^4); floor(log2(2^4))
> 
> [1] 4
> [1] 4
> 
> 
> DrC
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rechung at gmail.com  Fri Nov  4 15:13:03 2005
From: rechung at gmail.com (Robert Chung)
Date: Fri, 4 Nov 2005 15:13:03 +0100
Subject: [R] Graphics question on putting axes in the margins
References: <s36b2016.019@MAIL.NDRI.ORG>
Message-ID: <dkfq9j$r9e$1@sea.gmane.org>

Peter Flom wrote:

> which I did using mfrow = c(3,3).
>
> I'd like to use the space on the page as efficiently as possible,so I
> made the margins small, but that leaves no space for axes.  Is there a
> way to put axes only along the bottom and left side, so that a) The
> individual qqplots maintain their shape and b) less space on the page is
> taken up by white space?

Take a look at the code for plot.ts(..., plot.type="multiple").
It uses plot(..., axes=F), followed by calls to box() and axis().



From sdavis2 at mail.nih.gov  Fri Nov  4 15:26:04 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 04 Nov 2005 09:26:04 -0500
Subject: [R] Help for MDS !!
In-Reply-To: <20051102114239.70097.qmail@web32102.mail.mud.yahoo.com>
Message-ID: <BF90D9AC.126DA%sdavis2@mail.nih.gov>

On 11/2/05 6:42 AM, "A Ezhil" <ezhil02 at yahoo.com> wrote:

> Hi All,
> 
> I am trying to apply MDS for 4 groups in my data. The
> groups are:  
> 
> groups = list( Day1C=c(9), Day1T=c(7,8,10),
> Day2C=c(1,2,3,6,11,13,14,15), Day2T=c(4,5,12,16,17,18)
> )
> 
> When I do the MDS plot the group1 appears twice
> instead of one time in the plot. I don't know why this
> is happening.
> 
> I would greatly appreciate your help in fixing this
> problem.

You will need to supply the actual code that you use to do the MDS plot.
Showing a list of groups doesn't really help us troubleshoot.

Sean



From Rau at demogr.mpg.de  Fri Nov  4 15:59:57 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Fri, 4 Nov 2005 15:59:57 +0100
Subject: [R] Optim function
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E69FDF6D@HERMES.demogr.mpg.de>

Hi,

is this what you meant?

myfun <- function(x) {
  return(x^2)
}

optim(par=0.5, fn=myfun, method="BFGS")$par 


Best,
Roland


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> jmbucci at stat.ucla.edu
> Sent: Friday, October 28, 2005 1:23 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Optim function
> 
> Hello,
> 
> Is there a general procedure when using the the optim 
> function("BFGS") to
> only output the converging value (i.e. final par value) 
> rather than all
> the other output?
> 
> Thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From mineoeli at unipa.it  Fri Nov  4 16:02:35 2005
From: mineoeli at unipa.it (Elio Mineo)
Date: Fri, 04 Nov 2005 16:02:35 +0100
Subject: [R] Optim function
In-Reply-To: <1491.164.67.201.106.1130455399.squirrel@164.67.201.106>
References: <1491.164.67.201.106.1130455399.squirrel@164.67.201.106>
Message-ID: <436B780B.1060803@unipa.it>

Try:

optim(.....)$par

Bye
Elio

jmbucci at stat.ucla.edu wrote:

>Hello,
>
>Is there a general procedure when using the the optim function("BFGS") to
>only output the converging value (i.e. final par value) rather than all
>the other output?
>
>Thank you.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>

-- 
----------------------------------------------------------------------------------
Prof. Angelo M. Mineo
Dipartimento di Scienze Statistiche e Matematiche "S. Vianelli"
Universit?? degli Studi di Palermo
Viale delle Scienze
90128 Palermo
url: http://dssm.unipa.it/elio



From clists at perrin.socsci.unc.edu  Fri Nov  4 16:05:50 2005
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Fri, 4 Nov 2005 10:05:50 -0500 (EST)
Subject: [R] Can I do content analysis by R?
In-Reply-To: <BAY113-F173074699EEB0893093556BF610@phx.gbl>
References: <BAY113-F173074699EEB0893093556BF610@phx.gbl>
Message-ID: <Pine.LNX.4.53.0511041005170.24047@perrin.socsci.unc.edu>

What, in particular, do you want to do? Content analysis is a mode of
research, not a statistical technique.

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu


On Thu, 3 Nov 2005, [gb2312] ¹ù Î¬Î¬ wrote:

> Can R conduct content analysis? I search "content analysis" in mail
> archives and only find the concord package can compute Krippendorff's
> alpha. If R cannot cope with content analysis, does anyone can tell me some
> other softwares? Thank you very much!
>
>



From ezhil02 at yahoo.com  Fri Nov  4 16:10:11 2005
From: ezhil02 at yahoo.com (A Ezhil)
Date: Fri, 4 Nov 2005 07:10:11 -0800 (PST)
Subject: [R] Help for MDS !!
In-Reply-To: <BF90D9AC.126DA%sdavis2@mail.nih.gov>
Message-ID: <20051104151011.67454.qmail@web32102.mail.mud.yahoo.com>

Hi Sean,

I am sorry for that. I am using the following code for
doing MDS:

groups = list( Day1C=c(1), Day1T=c(2,3,4),
Day2C=c(5,6,7,8,9,10,11,12), Day2T=c(14,15,16,17,18))
len = length(groups)
dist = dist(data, method="euclidean") 
data.mds=isoMDS(dist)
plot(data.mds$points,type = "n",main="MDS plot")
for ( i in 1:len ) 
	text(data.mds$points[groups[[i]] ,], labels =
colnames(data)[groups[[i]]],col=i )

Thanks in Advance.

Regards,
Ezhil

--- Sean Davis <sdavis2 at mail.nih.gov> wrote:

> On 11/2/05 6:42 AM, "A Ezhil" <ezhil02 at yahoo.com>
> wrote:
> 
> > Hi All,
> > 
> > I am trying to apply MDS for 4 groups in my data.
> The
> > groups are:  
> > 
> > groups = list( Day1C=c(9), Day1T=c(7,8,10),
> > Day2C=c(1,2,3,6,11,13,14,15),
> Day2T=c(4,5,12,16,17,18)
> > )
> > 
> > When I do the MDS plot the group1 appears twice
> > instead of one time in the plot. I don't know why
> this
> > is happening.
> > 
> > I would greatly appreciate your help in fixing
> this
> > problem.
> 
> You will need to supply the actual code that you use
> to do the MDS plot.
> Showing a list of groups doesn't really help us
> troubleshoot.
> 
> Sean
> 
>



From cepardot at cable.net.co  Fri Nov  4 16:13:13 2005
From: cepardot at cable.net.co (=?gb2312?B?Q2FtcG8gRWyoqmFzIFBBUkRP?=)
Date: Fri, 04 Nov 2005 10:13:13 -0500
Subject: [R] Can I do content analysis by R?
References: <BAY113-F173074699EEB0893093556BF610@phx.gbl>
Message-ID: <001901c5e152$4833a840$3fc24f45@PARDOBERNAL>

Look for in  http://egsh.enst.fr/lebart/


Campo El¨ªas  PARDO
Profesor Asociado
Universidad Nacional de Colombia
Facultad de Ciencias
Departamento de Estad¨ªstica
Emails:
cepardot at cable.net.co
cepardot at unal.edu.co



From ripley at stats.ox.ac.uk  Fri Nov  4 16:16:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Nov 2005 15:16:58 +0000 (GMT)
Subject: [R] residual variance in lmer
In-Reply-To: <40e66e0b0511040550o15c8acaelbe7567d0925c5990@mail.gmail.com>
References: <436B3B63.3000205@rulsfb.leidenuniv.nl>
	<40e66e0b0511040550o15c8acaelbe7567d0925c5990@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0511041512040.30492@gannet.stats>

On Fri, 4 Nov 2005, Douglas Bates wrote:

> On 11/4/05, Tom Van Dooren <vdooren at rulsfb.leidenuniv.nl> wrote:
>
>> when running lmer with family set to quasibinomial,
>> is the "residual" variance reported among the random effects, the
>> dispersion parameter as in glm, or something else?
>
> It should be the dispersion parameter.  I say "should be" because I am
> not that familiar with the quasibinomial family and I don't know
> exactly how the dispersion parameter is defined.  I can tell you that
> the 'residual' variance is evaluated as the ML estimate of the
> variance from the last weighted penalized least squares problem used
> in the iterative estimation.  Does that help?

I would say that is an estimate of the dispersion parameter, different 
from those used in glm() as indeed it should be as this is a GLM.

Estimation of the dispersion in a quasibinomial family is pretty tricky, 
even for a glm.  Be prepared for estimates to be biased.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Nov  4 16:48:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Nov 2005 15:48:34 +0000 (GMT)
Subject: [R] residual variance in lmer
In-Reply-To: <Pine.LNX.4.61.0511041512040.30492@gannet.stats>
References: <436B3B63.3000205@rulsfb.leidenuniv.nl>
	<40e66e0b0511040550o15c8acaelbe7567d0925c5990@mail.gmail.com>
	<Pine.LNX.4.61.0511041512040.30492@gannet.stats>
Message-ID: <Pine.LNX.4.61.0511041547520.2749@gannet.stats>

On Fri, 4 Nov 2005, Prof Brian Ripley wrote:

> On Fri, 4 Nov 2005, Douglas Bates wrote:
>
>> On 11/4/05, Tom Van Dooren <vdooren at rulsfb.leidenuniv.nl> wrote:
>>
>>> when running lmer with family set to quasibinomial,
>>> is the "residual" variance reported among the random effects, the
>>> dispersion parameter as in glm, or something else?
>>
>> It should be the dispersion parameter.  I say "should be" because I am
>> not that familiar with the quasibinomial family and I don't know
>> exactly how the dispersion parameter is defined.  I can tell you that
>> the 'residual' variance is evaluated as the ML estimate of the
>> variance from the last weighted penalized least squares problem used
>> in the iterative estimation.  Does that help?
>
> I would say that is an estimate of the dispersion parameter, different
> from those used in glm() as indeed it should be as this is a GLM.
                                                             ^not
Oops, sorry.

> Estimation of the dispersion in a quasibinomial family is pretty tricky,
> even for a glm.  Be prepared for estimates to be biased.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From JAROSLAW.W.TUSZYNSKI at saic.com  Fri Nov  4 16:53:40 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 4 Nov 2005 10:53:40 -0500 
Subject: [R] Search within a file
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F414E@us-arlington-0668.mail.saic.com>

Thanks for a great suggestions. I guess the code you suggested would look
something like this:

fregexpr = function(pattern, filename) 
{ # same as gregexpr but operating on files not strings
  # Only single string 'pattern's allowed 
	buf.size=1024
	n  = file.info(filename)$size
	pos = NULL
	fp = file(filename, "rb")
	for (d in seq(1,n,by=buf.size)) {
		m = if (n-d>buf.size) buf.size else n-d
		p = gregexpr(pattern, readChar(fp, m))[[1]]
		if(p[1]>0) pos=c(pos, p+d-1)
	}
	close(fp)
	if (is.null(pos)) pos=-1
	return (pos)
}


> fname = file.path(R.home(),"COPYING")
> fregexpr("right", fname)
 [1]    73  1347  1422  1460  1727  1879  1908  1939  3106  3350  4240  5530
[13]  6637  6661  6740  9460  9534 10503 11756 12528 12566 13805 15907 16056
[25] 17053 17681 17813
> gregexpr("right", readChar(fname,file.info(fname)$size))[[1]]
 [1]    73  1347  1422  1460  1727  1879  1908  1939  3106  3350  4240  5530
[13]  6637  6661  6740  9460  9534 10503 11756 12528 12566 13805 15907 16056
[25] 17053 17681 17813
attr(,"match.length")
 [1] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5


The function above does what I need, if someone needs a function that
parallels gregexpr but operates on files not strings, than most of the work
would be in modifying line "if(p[1]>0) pos=c(pos, p+d-1)" to do
concatination and addition on lists.

Thanks 

Jarek Tuszynski


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Seth Falcon
Sent: Friday, November 04, 2005 1:44 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Search within a file


On  3 Nov 2005, JAROSLAW.W.TUSZYNSKI at saic.com wrote:
> I am looking for a way to search a file for position of some 
> expression, from within R. My current code:
>
> sha1Pos = gregexpr("<sha1>", readChar(filename, 
> file.info(filename)$size))[[1]]
>
> Works fine for small files, but text files I will be working with 
> might get up to Gb range, so I was trying to accomplish the same 
> without loading the whole file into R.

I would think you could use readLines to read in a batch of lines, run
(g)regexpr, and keep track of matches and position.

Create a connection to the file using file() first, and then subsequent
calls to readLines will start where you left off.

But you will need to adjust the position indices returned by gregexpr by how
far into the file you are.  Seems very doable.

+ seth

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Nov  4 16:58:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Nov 2005 16:58:22 +0100
Subject: [R] question about precision, floor, and powers of two.
In-Reply-To: <436B6C61.7070606@statistik.uni-dortmund.de>
References: <e89bb7ac0510310903m77d159e6nb9f2e7188ff82d90@mail.gmail.com>
	<436B6C61.7070606@statistik.uni-dortmund.de>
Message-ID: <x2fyqcxv4x.fsf@viggo.kubism.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Dr Carbon wrote:
> 
> > At the risk of being beaten about the face and body, can somebody explain
> > why the middle example: log2(2^3); floor(log2(2^3)) is different than
> > examples 1 and 3?
> 
> 
> Because
> 
>  > log2(2^3) - 3
> [1] -4.440892e-16
> 
> see the R FAQ "Why doesn't R think these numbers are equal?".
> 
> Uwe Ligges

In this particular case, it is slightly odd that we can't get an exact
answer for operations that could in principle be carried out using
integer arithmetic, but we're actually calculating log(8)/log(2).

(Curiously, the same effect is not seen on Linux or Solaris until 

 > log2(2^29)-29
[1] 3.552714e-15

)
 
> > 
> >>log2(2^2); floor(log2(2^2))
> > 
> > [1] 2
> > [1] 2
> > 
> >>log2(2^3); floor(log2(2^3))
> > 
> > [1] 3
> > [1] 2
> > 
> >>log2(2^4); floor(log2(2^4))
> > 
> > [1] 4
> > [1] 4
> > 
> > 
> > DrC
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From JAROSLAW.W.TUSZYNSKI at saic.com  Fri Nov  4 17:03:25 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 4 Nov 2005 11:03:25 -0500 
Subject: [R] Real FFT
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F414F@us-arlington-0668.mail.saic.com>

How about 'fft' function. It is a little higher level than Fortran function
you mention, since array can be any length and you do not need to pack
complex numbers into a vector of floats.

Jarek Tuszynski

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Glenn Meyers
Sent: Wednesday, November 02, 2005 9:50 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Real FFT


The book "Numerical Recipes in Fortran77" by Press, Teukolsky, Vetterling
and Flannery describes a way to "pack" the even and odd coordinates of a
real vector "R" into a complex vector "C" of half the length.  Then using
various FFT symmetries, they extract the FFT of "R" from the FFT of "C"
which is half the length of "R",

There is also an inverse process.  

Is there an R function that does this?

Glenn Meyers
ggmeyers at optonline.net

gmeyers at iso.com


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Nov  4 17:04:57 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Nov 2005 17:04:57 +0100
Subject: [R] model.matrix for non-hierarchical models
In-Reply-To: <17259.27552.773095.502243@stat.math.ethz.ch>
References: <17259.27552.773095.502243@stat.math.ethz.ch>
Message-ID: <x2br10xuty.fsf@viggo.kubism.ku.dk>

Lukas Meier <meier at stat.math.ethz.ch> writes:

> Dear R-users,
> 
> Using the function model.matrix I noticed the following
> behaviour (example below): Using the formula "~ a + a:b" will
> give a matrix of the same dimension as using "~ a * b". In the
> first case there are additional columns for the interaction (to
> compensate for the missing main-effect).
> 
> dd <- data.frame(a = gl(3,4), b = gl(4,1,12))
> model.matrix(~ a*b, dd, contrasts = list(a="contr.sum", 
>              b="contr.sum"))
> model.matrix(~ a + a:b, dd, contrasts = list(a="contr.sum", 
>              b="contr.sum"))
> 
> Is there any way to get the design matrix corresponding to "~ a
> + a:b" or do I have to do this manually?

The design matrix corresponding to ~ a + a:b is what you go above. If
that wasn't what you wanted, perhaps you should tell us what you did
want.
 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri Nov  4 17:15:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Nov 2005 16:15:04 +0000 (GMT)
Subject: [R] question about precision, floor, and powers of two.
In-Reply-To: <x2fyqcxv4x.fsf@viggo.kubism.ku.dk>
References: <e89bb7ac0510310903m77d159e6nb9f2e7188ff82d90@mail.gmail.com>
	<436B6C61.7070606@statistik.uni-dortmund.de>
	<x2fyqcxv4x.fsf@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0511041609200.9525@gannet.stats>

On Fri, 4 Nov 2005, Peter Dalgaard wrote:

> Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:
>
>> Dr Carbon wrote:
>>
>>> At the risk of being beaten about the face and body, can somebody explain
>>> why the middle example: log2(2^3); floor(log2(2^3)) is different than
>>> examples 1 and 3?
>>
>>
>> Because
>>
>> > log2(2^3) - 3
>> [1] -4.440892e-16
>>
>> see the R FAQ "Why doesn't R think these numbers are equal?".
>>
>> Uwe Ligges
>
> In this particular case, it is slightly odd that we can't get an exact
> answer for operations that could in principle be carried out using
> integer arithmetic, but we're actually calculating log(8)/log(2).
>
> (Curiously, the same effect is not seen on Linux or Solaris until
>
> > log2(2^29)-29
> [1] 3.552714e-15
>
> )

Yes, since there are C99/POSIX functions log2 and log10.  We ought to make 
use of them if available (they are currently on Windows).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From meier at stat.math.ethz.ch  Fri Nov  4 17:17:27 2005
From: meier at stat.math.ethz.ch (Lukas Meier)
Date: Fri, 4 Nov 2005 17:17:27 +0100
Subject: [R] model.matrix for non-hierarchical models
In-Reply-To: <x2br10xuty.fsf@viggo.kubism.ku.dk>
References: <17259.27552.773095.502243@stat.math.ethz.ch>
	<x2br10xuty.fsf@viggo.kubism.ku.dk>
Message-ID: <17259.35223.325381.228131@stat.math.ethz.ch>

Peter Dalgaard writes:
 > Lukas Meier <meier at stat.math.ethz.ch> writes:
 > 
 > > Dear R-users,
 > > 
 > > Using the function model.matrix I noticed the following
 > > behaviour (example below): Using the formula "~ a + a:b" will
 > > give a matrix of the same dimension as using "~ a * b". In the
 > > first case there are additional columns for the interaction (to
 > > compensate for the missing main-effect).
 > > 
 > > dd <- data.frame(a = gl(3,4), b = gl(4,1,12))
 > > model.matrix(~ a*b, dd, contrasts = list(a="contr.sum", 
 > >              b="contr.sum"))
 > > model.matrix(~ a + a:b, dd, contrasts = list(a="contr.sum", 
 > >              b="contr.sum"))
 > > 
 > > Is there any way to get the design matrix corresponding to "~ a
 > > + a:b" or do I have to do this manually?
 > 
 > The design matrix corresponding to ~ a + a:b is what you go above. If
 > that wasn't what you wanted, perhaps you should tell us what you did
 > want.

The problem is that when using  "~ a + a:b" a model matrix with
the same number of columns like the matrix for the full model
(~a*b) is created. I'd like that when using "~ a + a:b" I get
the same matrix as the full model up to the deletion of the
columns corresponding to the main-effect of b. 

Regards,

Lukas Meier



From gunter.berton at gene.com  Fri Nov  4 17:22:15 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 4 Nov 2005 08:22:15 -0800
Subject: [R] question about precision, floor, and powers of two.
In-Reply-To: <x2fyqcxv4x.fsf@viggo.kubism.ku.dk>
Message-ID: <200511041622.jA4GMF8e019767@volta.gene.com>


> In this particular case, it is slightly odd that we can't get an exact
> answer for operations that could in principle be carried out using
> integer arithmetic, but we're actually calculating log(8)/log(2).
> 
> (Curiously, the same effect is not seen on Linux or Solaris until 
> 
>  > log2(2^29)-29
> [1] 3.552714e-15
> 
>

But it's a nice example of the risks of using tests for exact equality (via
==, say)with floating point arithmetic -- even when it **seems** that we
have integers. I'll bet a bunch of my code would flop because of this.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From murdoch at stats.uwo.ca  Fri Nov  4 17:21:13 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 04 Nov 2005 11:21:13 -0500
Subject: [R] question about precision, floor, and powers of two.
In-Reply-To: <x2fyqcxv4x.fsf@viggo.kubism.ku.dk>
References: <e89bb7ac0510310903m77d159e6nb9f2e7188ff82d90@mail.gmail.com>	<436B6C61.7070606@statistik.uni-dortmund.de>
	<x2fyqcxv4x.fsf@viggo.kubism.ku.dk>
Message-ID: <436B8A79.5080807@stats.uwo.ca>

On 11/4/2005 10:58 AM, Peter Dalgaard wrote:
> Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:
> 
>> Dr Carbon wrote:
>> 
>> > At the risk of being beaten about the face and body, can somebody explain
>> > why the middle example: log2(2^3); floor(log2(2^3)) is different than
>> > examples 1 and 3?
>> 
>> 
>> Because
>> 
>>  > log2(2^3) - 3
>> [1] -4.440892e-16
>> 
>> see the R FAQ "Why doesn't R think these numbers are equal?".
>> 
>> Uwe Ligges
> 
> In this particular case, it is slightly odd that we can't get an exact
> answer for operations that could in principle be carried out using
> integer arithmetic, but we're actually calculating log(8)/log(2).
> 
> (Curiously, the same effect is not seen on Linux or Solaris until 
> 
>  > log2(2^29)-29
> [1] 3.552714e-15

In Windows 2.2.0 and R-devel, I get

 > log2(2^3) - 3
[1] 0
 > log2(2^28) - 28
[1] 0
 > log2(2^29) - 29
[1] 3.552714e-15

I suspect the difference is that the machines with the inaccurate answer 
have had some DLL or video driver set their precision to 53 bits instead 
of the default 64.

Maybe we need to be even more defensive against such changes.

Duncan Murdoch
> 
> )
>  
>> > 
>> >>log2(2^2); floor(log2(2^2))
>> > 
>> > [1] 2
>> > [1] 2
>> > 
>> >>log2(2^3); floor(log2(2^3))
>> > 
>> > [1] 3
>> > [1] 2
>> > 
>> >>log2(2^4); floor(log2(2^4))
>> > 
>> > [1] 4
>> > [1] 4
>> > 
>> > 
>> > DrC
>> > 
>> > 	[[alternative HTML version deleted]]
>> > 
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>



From kim at germinal.toulouse.inra.fr  Fri Nov  4 17:25:55 2005
From: kim at germinal.toulouse.inra.fr (Kim Anh =?iso-8859-1?Q?L=EA?= Cao)
Date: Fri, 04 Nov 2005 17:25:55 +0100
Subject: [R] random forests and memory allocation
Message-ID: <5.0.2.1.2.20051104171020.00bdf690@germinal.toulouse.inra.fr>

Dear all,

I am trying to run a random forest on a 4-class dataset with 10 215 variables.
Whatever the number of trees (I was thinking of combining the trees), I 
first got the error on Windows:

Error: cannot allocate vector of size 407562 Kb
In addition: Warning messages:
1: Reached total allocation of 511Mb: see help(memory.size)
2: Reached total allocation of 511Mb: see help(memory.size)

We did then try on linux on a super calculator with 130 Go of RAM and 68 
processors of 1.5Ghz each.

We then got the following error:
Error: protect(): protection stack overflow
Execution halted

Is there anything that can be done to run this random forest ? In the 
?Startup I did not understand how to increase the stack size...

Regards,

Kim-Anh



From redbeard at arrr.net  Fri Nov  4 17:53:34 2005
From: redbeard at arrr.net (Jarrett Byrnes)
Date: Fri, 4 Nov 2005 08:53:34 -0800
Subject: [R] Plotting Factorial GLMs
In-Reply-To: <c46712003b9b3318f93dc83223406033@arrr.net>
References: <c46712003b9b3318f93dc83223406033@arrr.net>
Message-ID: <6a572b07ae4cb4d58e6df4d1ff86871e@arrr.net>

Thanks to all who replied (although next time, reply to the list!)

The simplest answer is to replace "A" in the curve statement with the 
following:

rep("A", times=length(x))

Now if I could just figure out the workaround to plotting multiple sets 
of points onto my graph in the first place (i.e. break up the x,y 
values by block, then color them differently, and plot them on top of 
each other as add=TRUE doesn't seem to work for the plot statement - 
any pointers?)


On Nov 3, 2005, at 10:31 PM, Jarrett Byrnes wrote:

> Hello all,
> 	I'm attempting to plot the functions from a generalized linear model
> while iterating over multiple levels of a factor in the model.  In
> other words, I have a data set
>
> Block, Treatment.Level, Response.Level
>
> So, the glm and code to plot should be
>
> logit.reg<-glm(formula = Response.Level ~ Treatment.Level + Block,
> 			family=quasibinomial(link="logit")))
>
> plot( Response.Level ~ Treatment.Level)
>
> logit.reg.function <- function (trt, blk) predict(logit.reg,
> data.frame(Treatment.Level=trt, Block=blk)
>
> curve(logit.reg.function(x, "A"), add=TRUE)
>
>
> But I get the error:
> Error in xy.coords(x, y) : 'x' and 'y' lengths differ
>
> Now, if I set Block="A" in the function, and take blk out, as well as
> taking the "A" out of the curve statement, it plots just fine.  What am
> I doing wrong, as this would be a nice, quick, and easy way to whip up
> multiple curves from a factorial dataset!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Fri Nov  4 18:02:19 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 4 Nov 2005 09:02:19 -0800 (PST)
Subject: [R] question about precision, floor, and powers of two.
In-Reply-To: <436B6C61.7070606@statistik.uni-dortmund.de>
References: <e89bb7ac0510310903m77d159e6nb9f2e7188ff82d90@mail.gmail.com>
	<436B6C61.7070606@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.63a.0511040900000.20625@homer23.u.washington.edu>

On Fri, 4 Nov 2005, Uwe Ligges wrote:

> Dr Carbon wrote:
>
>> At the risk of being beaten about the face and body, can somebody explain
>> why the middle example: log2(2^3); floor(log2(2^3)) is different than
>> examples 1 and 3?
>
>
> Because
>
> > log2(2^3) - 3
> [1] -4.440892e-16
>


This is a less satisfactory answer than usual, because both 2^3 and 
log(2^3) are integers and thus exactly representable in the R numeric 
type.  You could reasonably expect log(8) to be exactly 2, just as sqrt(4) 
is exactly 2.

The problem is that we compute all logarithms via the natural log, and 
this introduces the problem of limited precision.

 	-thomas



From andy_liaw at merck.com  Fri Nov  4 18:28:08 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 4 Nov 2005 12:28:08 -0500
Subject: [R] random forests and memory allocation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED568@usctmx1106.merck.com>

Please give more info on your problem, as the Posting Guide asked.  How
exactly did you call randomForest?  How many cases are in the data?  What
options did you request in randomForest?

People have run randomForest on data with more variables than that on lesser
machines without problem.  You should look in the archive for some hints.

Andy

> From: Kim Anh L?? Cao
> 
> Dear all,
> 
> I am trying to run a random forest on a 4-class dataset with 
> 10 215 variables.
> Whatever the number of trees (I was thinking of combining the 
> trees), I 
> first got the error on Windows:
> 
> Error: cannot allocate vector of size 407562 Kb
> In addition: Warning messages:
> 1: Reached total allocation of 511Mb: see help(memory.size)
> 2: Reached total allocation of 511Mb: see help(memory.size)
> 
> We did then try on linux on a super calculator with 130 Go of 
> RAM and 68 
> processors of 1.5Ghz each.
> 
> We then got the following error:
> Error: protect(): protection stack overflow
> Execution halted
> 
> Is there anything that can be done to run this random forest ? In the 
> ?Startup I did not understand how to increase the stack size...
> 
> Regards,
> 
> Kim-Anh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Fri Nov  4 18:46:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Nov 2005 17:46:12 +0000 (GMT)
Subject: [R] question about precision, floor, and powers of two.
In-Reply-To: <Pine.LNX.4.63a.0511040900000.20625@homer23.u.washington.edu>
References: <e89bb7ac0510310903m77d159e6nb9f2e7188ff82d90@mail.gmail.com>
	<436B6C61.7070606@statistik.uni-dortmund.de>
	<Pine.LNX.4.63a.0511040900000.20625@homer23.u.washington.edu>
Message-ID: <Pine.LNX.4.61.0511041742570.2857@gannet.stats>

On Fri, 4 Nov 2005, Thomas Lumley wrote:

> On Fri, 4 Nov 2005, Uwe Ligges wrote:
>
>> Dr Carbon wrote:
>>
>>> At the risk of being beaten about the face and body, can somebody explain
>>> why the middle example: log2(2^3); floor(log2(2^3)) is different than
>>> examples 1 and 3?
>>
>>
>> Because
>>
>>> log2(2^3) - 3
>> [1] -4.440892e-16
>>
>
>
> This is a less satisfactory answer than usual, because both 2^3 and
> log(2^3) are integers and thus exactly representable in the R numeric
> type.  You could reasonably expect log(8) to be exactly 2, just as sqrt(4)
> is exactly 2.
>
> The problem is that we compute all logarithms via the natural log, and
> this introduces the problem of limited precision.

Well, we _did_ (and as others have noted, most machines manage to get 
log(8) to be exactly 2).

R-devel now uses log2 for this, so

> log2(2^29) - 29
[1] 0


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From DePrengM at botanicgardens.org  Fri Nov  4 18:59:17 2005
From: DePrengM at botanicgardens.org (Michelle DePrenger-Levin)
Date: Fri, 04 Nov 2005 10:59:17 -0700
Subject: [R] Stress in multidimensional scaling
Message-ID: <s36b3f13.032@DBG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051104/df82eba7/attachment.pl

From zev at zevross.com  Fri Nov  4 20:25:31 2005
From: zev at zevross.com (Zev Ross)
Date: Fri, 04 Nov 2005 14:25:31 -0500
Subject: [R] Default keyboard shortcuts in JGR
Message-ID: <436BB5AB.2090908@zevross.com>


   Hi R List,
   A  very  minor  item,  but  I  was just wondering if there is a way to
   change  the default keyboard shortcuts in the JGR GUI. By default, the
   "Run" command is CTRL-R which requires more finger gymnastics than the
   F-10 in S-PLUS. Any suggestions? Thank you, Zev

   --

   Zev Ross
   ZevRoss Spatial Analysis
   303 Fairmount Ave
   Ithaca, NY 14850
   (607) 277-0004 (phone)
   (866) 877-3690 (fax toll-free)
   [1]zev at zevross.com
   [2]www.zevross.com

References

   1. mailto:zev at zevross.com
   2. http://www.zevross.com/


From tom at maladmin.com  Fri Nov  4 15:37:24 2005
From: tom at maladmin.com (tom wright)
Date: Fri, 04 Nov 2005 09:37:24 -0500
Subject: [R] problem in waveslim library?
Message-ID: <1131115045.4570.7.camel@localhost.localdomain>

This code consistenly segfaults for me. Can someone please take a look
and tell me if the problem is due to something I am doing or is there a
problems with the dwt (idwt) functions in the waveslim library.
Thanks
tom


library(waveslim)

data<-c(936.944,936.944,936.944,936.944,936.944,936.944,936.944,936.944,936.944,936.944,936.944,936.944,936.944,916.994666666667,909.592,941.8,961.933333333333,987.8,969.2,934.866666666667,912.8,913.266666666667,928.2,949.533333333333,933.4,948.533333333333,929.866666666667,935.2,937,934.666666666667,923.733333333333,911.8,937.066666666667,921.133333333333,914.6,941.933333333333,961.933333333333,950.333333333333,925.6,921.066666666667,916.2,927.266666666667,921.266666666667,938.066666666667,927.133333333333,949.866666666667,938.533333333333,934.933333333333,945.8,962.466666666667,967.266666666667,965,935.533333333333,935,927.933333333333,919.733333333333,930.466666666667,948,945.733333333333,972.933333333333,942.066666666667,921.533333333333,921.2,920.4,956.333333333333,945.666666666667,971.4,980,965.6,964.133333333333,960.066666666667,963,932.866666666667,952.466666666667,938.8,924.8,925.466666666667,949.4,931.466666666667,924.066666666667,926.866666666667,931.133333333333,937.266666666667,961.333333333333,952.2,958.133333333333,964,941.733333333333,936.133333333333,936.2,955.866666666667,964.6,962.333333333333,946.866666666667,971.533333333333,952.666666666667,954.933333333333,959.866666666667,969.2,970.266666666667,954.8,974.266666666667,947.133333333333,941.466666666667,949.066666666667,974.333333333333,943,929.333333333333,933.666666666667,926.666666666667,939.933333333333,936.6,952.4,954.6,947.466666666667,945.533333333333,948.733333333333,950.066666666667,941.666666666667,926.466666666667,934.333333333333,948.933333333333,946.733333333333,933.266666666667,913.266666666667,924.466666666667,937.866666666667,980.6,975.6,954.266666666667,943.266666666667,954,948.133333333333,947.066666666667,915,924.933333333333,938.2,942.666666666667,941.266666666667,956.066666666667,964.066666666667,958.666666666667,947.066666666667,940.4,926.133333333333,931.066666666667,940.933333333333,956.466666666667,958.2,941.866666666667,941.4,935.066666666667,943.933333333333,939,948.666666666667,964.133333333333,996.533333333333,957.466666666667,945.733333333333,922.533333333333,919.2,931.733333333333,952.2,921.066666666667,933.466666666667,948.666666666667,950.8,954,958.333333333333,942.6,936.266666666667,936.8,938.266666666667,943.666666666667,936.533333333333,920.133333333333,902.266666666667,933,948.733333333333,924.866666666667,941.866666666667,943.8,961.533333333333,968.333333333333,984.466666666667,949.4,933.266666666667,931.266666666667,934.533333333333,937.066666666667,959.533333333333,942.4,953.266666666667,975.866666666667,983.6,966.2,953.866666666667,938.066666666667,936.666666666667,931.733333333333,926.866666666667,965.8,970.133333333333,944.6,956.8,937.266666666667,933.066666666667,947.333333333333,935.733333333333,944.933333333333,946,943.6,969.733333333333,965.8,979.2,973.533333333333,970.666666666667,960.533333333333,939.266666666667,929.8,919.266666666667,925.333333333333,913.333333333333,924.266666666667,927.666666666667,936.6,919.066666666667,919.333333333333,936.2,954,938.066666666667,947.266666666667,945.066666666667,943.8,941.733333333333,942.133333333333,935.8,940.066666666667,942.466666666667,950.6,932.6,923.866666666667,920.2,916.6,939.866666666667,922.2,941.066666666667,927.466666666667,916.6,914.4,924.533333333333,958.533333333333,960.333333333333,932.733333333333,937,971.733333333333,981.866666666667,966.666666666667,949.333333333333,930.733333333333,918.933333333333,917.533333333333,948.133333333333,941.066666666667,952.8,966.533333333333,971.333333333333,962.866666666667,948.333333333333,970.066666666667,952.333333333333,937.2,951.933333333333,928.133333333333,937,948.066666666667,944.066666666667,963.733333333333,960.533333333333,931.266666666667,958.933333333333,971.4,969.2,963.133333333333,963.933333333333,965.133333333333,963,962.066666666667,938.6,945.733333333333,938.933333333333,936.8,954.2,961.133333333333,973.466666666667,989.933333333333,959.4,946.866666666667,959.733333333333,953.666666666667,953.8,945.466666666667,954.533333333333,957.6,951.533333333333,954.6,959.733333333333,943.466666666667,938.333333333333,954,966.466666666667,944.733333333333,941.666666666667,936.733333333333,938.933333333333,930.2,947.933333333333,927.333333333333,923.866666666667,908.533333333333,942.466666666667,950.066666666667,931.266666666667,935.2,941.933333333333,941.4,944.866666666667,956.8,935.266666666667,945.333333333333,963.666666666667,968.8,962.4,942.2,953.666666666667,932.8,920,921.2,933.933333333333,918.733333333333,933.8,945.933333333333,964.2,968.733333333333,961.666666666667,962.2,978.933333333333,973.866666666667,953.6,957.133333333333,940.333333333333,946.733333333333,950.733333333333,938.2,942.933333333333,951.866666666667,934.4,939.266666666667,936,916.533333333333,921.133333333333,913.2,903.466666666667,920.866666666667,920.466666666667,919.2,958.133333333333,954.866666666667,965.8,960.066666666667,943,970.4,976.8,967.933333333333,960.333333333333,934.6,953.466666666667,949.266666666667,934.4,936.666666666667,949.933333333333,931.933333333333,941.666666666667,957.266666666667,939.933333333333,925.733333333333,899.066666666667,932.733333333333,984.266666666667,969.866666666667,962.733333333333,945.333333333333,939.133333333333,932.8,933.666666666667,943.8,941.066666666667,937.2,942.8,938.333333333333,940.733333333333,938.466666666667,930.066666666667,941,937.066666666667,941.333333333333,954,937.733333333333,927.666666666667,918.933333333333,918.066666666667,938.666666666667,924.4,931,931.733333333333,922.866666666667,921.6,933.4,929.866666666667,926.066666666667,949.266666666667,967.866666666667,949.933333333333,947.666666666667,937.8,938.733333333333,926.733333333333,922,937.133333333333,940.6,911.666666666667,924.533333333333,952.066666666667,932.8,921.733333333333,917.466666666667,921.466666666667,955.4,967.933333333333,943.133333333333,896.666666666667,899.6,897.666666666667,917.733333333333,918.133333333333,919.8,923.666666666667,942.733333333333,942.266666666667,935,941.266666666667,951,960.6,949.4,964.8,947.466666666667,957.866666666667,952.466666666667,940.266666666667,927.133333333333,948.2,971,978.733333333333,954.333333333333,939.066666666667,936.866666666667,942.066666666667,941.733333333333,946.666666666667,947.866666666667,927.066666666667,953.266666666667,946,936.066666666667,925.266666666667,957.333333333333,965.333333333333,958.333333333333,942.6,927.866666666667,951.8,963.733333333333,982.8,994.266666666667,982.333333333333,966.2,957.333333333333,959.4,991,986.066666666667,973.733333333333,996.4,1012.2,1023,1049.2,1066.46666666667,1083.2,1085.73333333333,1113.8,1167.6,1241.8,1352.53333333333,1461.4,1597.8,1713,1764.33333333333,1776.86666666667,1744.26666666667,1696.06666666667,1642.73333333333,1609.8,1616.4,1613.2,1589.06666666667,1577.33333333333,1568.2,1553,1528.66666666667,1521.2,1512.8,1485.66666666667,1427.06666666667,1344.06666666667,1306.2,1279.46666666667,1264.6,1260.46666666667,1263.4,1233.6,1203.4,1187,1149.4,1158.46666666667,1150.66666666667,1154.8,1137.4,1109,1114.8,1108.06666666667,1118.4,1117.33333333333,1120.66666666667,1110,1131.66666666667,1149.6,1169.53333333333,1166.73333333333,1163,1130.6,1106.2,1113.66666666667,1119.06666666667,1101.06666666667,1118.06666666667,1137.93333333333,1193.6,1221.13333333333,1264.93333333333,1279.6,1250,1211.06666666667,1179.53333333333,1162.8,1176.13333333333,1172.66666666667,1167.93333333333,1186.73333333333,1193.33333333333,1194.8,1184.06666666667,1167.26666666667,1165.2,1165.86666666667,1183.13333333333,1208.86666666667,1201.53333333333,1196.66666666667,1183.8,1167.26666666667,1156,1121.06666666667,1115.06666666667,1105.4,1087.86666666667,1132.13333333333,1152.8,1152.8,1121.4,1095.86666666667,1107,1113.53333333333,1118.13333333333,1120.73333333333,1117.4,1113.86666666667,1094.46666666667,1102.4,1122.4,1132.6,1157.8,1155.33333333333,1124,1129.13333333333,1144.13333333333,1127.13333333333,1129.73333333333,1175.93333333333,1201.66666666667,1199.4,1196.2,1176.66666666667,1150.33333333333,1154,1170.53333333333,1193,1180.13333333333,1147.46666666667,1094.53333333333,1096.26666666667,1116.6,1147.33333333333,1130.8,1111.6,1094.73333333333,1092.33333333333,1104,1081,1085.53333333333,1098.33333333333,1089,1054.2,1042.4,1073.73333333333,1091.53333333333,1119.4,1081.06666666667,1080.66666666667,1050.66666666667,1073.53333333333,1082.4,1119.53333333333,1132.86666666667,1143.8,1197.13333333333,1206.93333333333,1206.46666666667,1202.6,1206.26666666667,1213.46666666667,1221.06666666667,1248.4,1277.2,1319.66666666667,1378.4,1447.06666666667,1532.33333333333,1636.13333333333,1727.6,1773.26666666667,1772.8,1755.66666666667,1738.46666666667,1709.4,1684.4,1673,1741.4,1806.13333333333,1857.2,1891.53333333333,1902.86666666667,1894.6,1860.13333333333,1809.6,1762.93333333333,1705.2,1656.93333333333,1652.46666666667,1671.46666666667,1720.6,1748.4,1753.06666666667,1750.2,1737.73333333333,1701.4,1636.66666666667,1549.2,1492.13333333333,1454.66666666667,1395.26666666667,1347.6,1320,1307.06666666667,1296.13333333333,1267.46666666667,1241.66666666667,1212.33333333333,1179.66666666667,1159.06666666667,1160.73333333333,1173.13333333333,1184.93333333333,1197.26666666667,1168.6,1135,1129.13333333333,1095.33333333333,1100.53333333333,1101.86666666667,1099.46666666667,1076.46666666667,1070.93333333333,1076.6,1051,1056.06666666667,1074.2,1072.6,1047.6,1066.6,1095.46666666667,1121.6,1135.93333333333,1152.53333333333,1151.86666666667,1152.33333333333,1173.06666666667,1199.33333333333,1219.53333333333,1262.06666666667,1265.93333333333,1252.13333333333,1206.26666666667,1170.13333333333,1123.33333333333,1141.86666666667,1108.4,1080.86666666667,1090.26666666667,1077.66666666667,1056.8,1068.6,1085.86666666667,1081,1059.06666666667,1072.26666666667,1050.8,1069.66666666667,1055,1046,1004.06666666667,984.333333333333,991.066666666667,993.133333333333,999.066666666667,998.4,991,975.533333333333,954.533333333333,932.933333333333,919.533333333333,956.866666666667,961.933333333333,961.8,957,951,956.733333333333,985.866666666667,962,961.666666666667,967.533333333333,978.866666666667,990,989,972.266666666667,978.8,956.6,963.8,975.4,976,962.8,958.333333333333,968.4,942.066666666667,931.133333333333,949.4,936.733333333333,970.466666666667,998.933333333333,971.333333333333,961.733333333333,954.666666666667,952.933333333333,951.2,963.2,975.266666666667,967.666666666667,940,959.066666666667,944.466666666667,968.133333333333,965.066666666667,956,961.533333333333,965.8,952,934.533333333333,945.066666666667,952.466666666667,953.133333333333,956.266666666667,952.866666666667,967.2,955.2,951.133333333333,955.533333333333,959.933333333333,949.266666666667,952.133333333333,948.533333333333,959.733333333333,950.4,960.866666666667,944.533333333333,940.2,945.066666666667,951.133333333333,958.666666666667,943.4,936.466666666667,937.6,941.333333333333,950.533333333333,967.066666666667,962.466666666667,975.2,977.6,959.733333333333,928.466666666667,949.533333333333,975.8,976.133333333333,959.933333333333,948.8,986.133333333333,976.333333333333,945.866666666667,947.266666666667,928.8,925.266666666667,956.466666666667,965.6,959.8,972.4,969.866666666667,948.533333333333,943.133333333333,962.133333333333,968.066666666667,945.8,931.6,940.066666666667,952.066666666667,922.666666666667,926.6,945.466666666667,952.933333333333,964.933333333333,986.533333333333,986.866666666667,977.933333333333,960.466666666667,944.933333333333,925.466666666667,947.666666666667,956.2,957.4,973.466666666667,977.066666666667,967.866666666667,973.4,968.333333333333,973.866666666667,963.266666666667,940.4,943.4,955.6,948.333333333333,932.066666666667,943.933333333333,946.266666666667,946.866666666667,933.733333333333,934.6,958.866666666667,950,940.266666666667,937.2,927.333333333333,931.2,942.4,961.866666666667,938.6,947.6,939.6,949.933333333333,948.666666666667,957.533333333333,952.733333333333,932.933333333333,931.2,938.4,921.066666666667,922.6,923.666666666667,933.2,953.066666666667,970.933333333333,970,973.733333333333,938.4,920.4,939.333333333333,945.133333333333,949.866666666667,963.866666666667,973.8,968.466666666667,950.933333333333,937.133333333333,934.8,924.733333333333,949.066666666667,930.466666666667,920,919.866666666667,925.666666666667,923.266666666667,929.2,947.2,935.4,952.133333333333,965,953.733333333333,940.4,955.133333333333,935.2,936.8,934.4,935.666666666667,935.8,943.6,954.666666666667,927.866666666667,924.733333333333,928.2,919.4,937,933.066666666667,927.333333333333,920.8,922.8,934.6,954.666666666667,976.933333333333,950.6,928.133333333333,930.8,914.4,926.866666666667,912.666666666667,926.466666666667,955.933333333333,957.933333333333,937.866666666667,928,922,946.066666666667,960.333333333333,971.4,938.933333333333,914.533333333333,930.133333333333,919.6,931.866666666667,936.266666666667,919.266666666667,923.466666666667,945.066666666667,955.733333333333,939.666666666667,935.533333333333,932.933333333333,935.266666666667,939.8,940.066666666667,941.666666666667,938.8,969.066666666667,972.2,932.933333333333,923.666666666667,931.266666666667,933.8,923.533333333333,938.333333333333,936.133333333333,913.866666666667,921.066666666667,952.4,951.133333333333,932.666666666667,924.6,959.4,947.333333333333,924.266666666667,906.666666666667,922.533333333333,950,985.266666666667,983.533333333333,964.2,945.6,945.6,947.733333333333,958.8)

setZeros<-function(data,factors=list()){
    for(factor in factors){
        sFac<-paste('d',factor,sep='')
        data[sFac]<-rep(0,length(data[sFac]))
    }
    return(data)
}

data.dwt<-dwt(data[[2]],n.levels=8)
opar<-par(mfrow=c(4,2),mar=c(2,2,2,2))
mlist<-c(1:8)
for(iFac in 1:8){
    #flist<-mlist[mlist!=iFac]
    ndata<-setZeros(data.dwt,iFac)
    plot(idwt(ndata),type='l')
}



From efg at stowers-institute.org  Fri Nov  4 20:44:12 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Fri, 4 Nov 2005 13:44:12 -0600
Subject: [R] RODBC and Excel: Wrong Data Type Assumed on Import
References: <adf71a630511030726v10f1dc77sa7c760b96a27480f@mail.gmail.com>
	<971536df0511031753y6eb4e271o797e862bc2740e1a@mail.gmail.com>
Message-ID: <dkgdmd$t5o$1@sea.gmane.org>

"Gabor Grothendieck" <ggrothendieck at gmail.com> wrote in message
news:971536df0511031753y6eb4e271o797e862bc2740e1a at mail.gmail.com...
> You could try using the COM interface rather than the ODBC
> interface.  Try code such as this:
>
> library(RDCOMClient)
> xls <- COMCreate("Excel.Application")
> xls[["Workbooks"]]$Open("MySpreadsheet.xls")
> sheet <- xls[["ActiveSheet"]]
> mydata <- sheet[["UsedRange"]][["value"]]
> xls$Quit()
>
> # convert mydata to a character matrix
> mydata.char <- matrix(unlist(mydata), nc = length(xx))

Gabor,

Thank you for that suggestion.  I try to avoid COM, but it seems to work
well with this problem.

Because I have empty cells, which are treated as NULLS, the unlist didn't
quite work.

Here's what I did:

library(RDCOMClient)
xls <- COMCreate("Excel.Application")
xls[["Workbooks"]]$Open("U:/efg/lab/R/Plasmid/construct list.xls")
sheet <- xls[["ActiveSheet"]]
mydata <- sheet[["UsedRange"]][["value"]]
xls$Quit()

for (column in 1:length(mydata))
{
  cat(column, " ", length(mydata[[column]]), " ",
length(unlist(mydata[[column]])), "\n")
}

The results show that while mydata is a list of columns, if you unlist each
column you'll be short by the number of NULL values.

1   1251   1251
2   1251   1198
3   1251   870
4   1251   327
5   1251   1250

This seemed a bit crude to fix that problem (can someone suggest a more
elegant way?):

mymatrix <- NULL
for (column in 1:length(mydata))
{
 # Use lappy to replace NULLs with "" strings, column-by-column
  mymatrix <- cbind(mymatrix, lapply(mydata[[column]], function(cell) {
ifelse(is.null(cell), "", cell) } ))
}
# Fix column names
colnames(mymatrix) <- mymatrix[1,]
mymatrix <- mymatrix[-1,]

> mymatrix[273:276,]
     Plasmid Number Plasmid
Concentration Comments Lost
[1,] 274            "yxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxy"         "1 ug/ul"
"4 mg"   ""
[2,] "275a"         "xyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyx" "1 ug/2
ul"   ""       ""
[3,] "275b"         "xyxyxyxyxyxyxyxyxyxyxyxyx"                   "1 ug/5
ul"   ""       ""
[4,] 276            "xyxyxyxyxyxyxyxyxyxyxyxyxyxy"                "1 ug/5
ul"   ""       "Assumed Lost"

Thank you for preserving "275a" and "275b" as the names here.

So, I'd recommend RDCOMClient over RODBC with Excel files.  "Being lucky"
shouldn't be part of processing Excel files.

efg



From droberts at montana.edu  Fri Nov  4 21:55:21 2005
From: droberts at montana.edu (Dave Roberts)
Date: Fri, 04 Nov 2005 13:55:21 -0700
Subject: [R] Stress in multidimensional scaling
In-Reply-To: <s36b3f13.032@DBG>
References: <s36b3f13.032@DBG>
Message-ID: <436BCAB9.4080402@montana.edu>

cmdscale calculates an eigenanalysis of the dissimilarity matrix, and 
does not employ "stress" per se.  Rather, it attempts to maximize 
variability along axes.

If you call the the cmdscale() function with "eig=TRUE" it returns a 
list object with the coordinates called "points" and the eigenvalues 
called "eig".  Accordingly, if you have an object returned from cmdscale 
you can look at the dimensionality of the solution with

 > x <- cmdscale(dissim.matrix,eig=TRUE)
 > barplot(x$eig)

If you really want "stress" you should use the isoMDS() function from 
MASS which performs nonmetric multi-dimensional scaling, and does 
directly employ stress.

You might also want to look at functions metaMDS in package vegan, and 
nmds in package labdsv.  There is a page at

http://ecology.msu.montana.edu/labdsv/R/lab7/lab7.html

and

http://ecology.msu.montana.edu/labdsv/R/lab8/lab8.html

that attempts to explain this.

Dave R.


Michelle DePrenger-Levin wrote:
> Hello,
> 
> We are trying to find a function to compute "stress" in our
> multidimensional scaling analysis of a dissimilarity matrix. We've used
> "dist()" to create the matrix and "cmdscale()" for the scaling. In order
> to determine the number of dimensions we would like to plot stress vs.
> dimensions. However, we cannot find a pre-made command. It seems that
> other programs may test stress as part of the algorithm as opposed to
> cmdscale() which allows us to chose the number of dimensions. Do we need
> to just code in a computation of stress? 
>  
> Thanks,
> Michelle
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From dhinds at sonic.net  Fri Nov  4 22:37:19 2005
From: dhinds at sonic.net (dhinds@sonic.net)
Date: Fri, 4 Nov 2005 21:37:19 +0000 (UTC)
Subject: [R] Problem installing ROracle package under R
References: <6140980.1131022611391.JavaMail.root@pswm18.cp.tin.it>
Message-ID: <dkgkaf$hin$1@sea.gmane.org>

In gmane.comp.lang.r.general Vittorio <vdemart1 at tin.it> wrote:

> configuration warning:
> Oracle pre-compiler proc not in 
> /usr/local/oracle8-client/bin/proc
> you may not be able to compile 
> ROracle

You don't have the Oracle Pro*C precompiler installed.  You can't
build ROracle without that.  This tool is part of a regular Oracle
client installation; but the FreeBSD client for Oracle appears to be a
port of the Linux client and it looks like all the pieces were not
included.

-- Dave



From evan at ausvet.com.au  Fri Nov  4 23:16:22 2005
From: evan at ausvet.com.au (Evan Sergeant)
Date: Sat, 05 Nov 2005 09:16:22 +1100
Subject: [R] RODBC error
In-Reply-To: <1130929257.4362.6.camel@localhost.localdomain>
References: <200511012155.jA1LtK7v055916@server.ausvet.com.au>
	<1130929257.4362.6.camel@localhost.localdomain>
Message-ID: <6.2.0.14.0.20051105091516.05f413d0@mail.ausvet.com.au>

Hi Tom,

Thanks for this suggestion. It now works perfectly. You have been a great help

Cheers

Evan Sergeant

At 10:00 PM 02/11/2005, tom wright wrote:


>On Wed, 2005-02-11 at 07:55 +1000, Evan Sergeant wrote:
> > Hi,
> >
> > I hope that someone can help me with the following problem with RODBC
> > connection to a MySQL database
> >
> > I am running R version 2.2.0 on windows XP, and have the MySQL database
> > registered in Windows ODBC.
> >
> > I have set up a web interface on my personal ISS web server using PhP
> > to accept input values and then call Rterm with an R script to access
> > the database and return a summary of the analysed data.
> >
> > This works perfectly if I run it from Rgui, or if I run Rterm from the
> > dos prompt using the same command line arguments, but returns an RODBC
> > error (below) when calling Rterm from the web interface
> >
> > 1: [RODBC] ERROR: state IM002, code 0, message [Microsoft][ODBC Driver
> > Manager] Data source name not found and no default driver specified
> >
> > There appears to be some problem with recognising the database when
> > called from the web page, even though it is not a problem at any other
> > times.
> >
> > Does anyone have any suggestions as to what I can do to overcome this
> > error
> >
> > Thanks for your help
> >
> > cheers
> >
> > Evan Sergeant
>Evan,
>It sounds like your problem is occuring when you try to access the
>database as a different user than yourself. Its been a while since I've
>been familiar with ms systems but IRC pws runs as its own user
>(ISUSR_...???). At the very least you will have to ensure that the
>datasource is registered as a system DSN not a user DSN. Appart from
>that you may run into other issues with access privileges.
>
>good luck
>tom
>
>
>
>
>--
>No virus found in this incoming message.
>Checked by AVG Anti-Virus.
>Version: 7.1.362 / Virus Database: 267.12.8/161 - Release Date: 03/11/2005

Evan Sergeant

AusVet Animal Health Services
69 Turner Cr,
Orange NSW 2800
Australia

Phone +61 2 6362 1598
Fax      +61 2 6369 1473
Email:  evan at ausvet.com.au
Web site: http://www.ausvet.com.au
MLA's Q Fever Register: www.qfever.org

This transmission is for the intended addressee only and is confidential
information. If you have received this transmission in error, please delete
it and notify the sender. The contents of this email are the opinion of the
writer only and are not endorsed by AusVet Animal Health Services unless
expressly stated otherwise. AusVet uses virus scanning software, but does
not accept liability for viruses or other defects in any attachments.



--



From maillist at roomity.com  Fri Nov  4 23:39:15 2005
From: maillist at roomity.com (shenanigans)
Date: Fri,  4 Nov 2005 14:39:15 -0800 (PST)
Subject: [R] [OTAnn] Groups:New Developments at Roomity
Message-ID: <8239258.291131143955486.JavaMail.tomcat5@slave1.roomity.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051104/02e6bc50/attachment.pl

From ywang at fhcrc.org  Sat Nov  5 03:04:45 2005
From: ywang at fhcrc.org (Wang, Yan)
Date: Fri, 4 Nov 2005 18:04:45 -0800
Subject: [R] problem with the assignment function
Message-ID: <000134EA228A1A49A552F75B9A13A5C901BBA341@groucho.fhcrc.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051104/fc947715/attachment.pl

From spencer.graves at pdf.com  Sat Nov  5 03:02:46 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 04 Nov 2005 18:02:46 -0800
Subject: [R] AR(1) with NLME
In-Reply-To: <BAY110-DAV7A19D678B677CB463A3519C710@phx.gbl>
References: <BAY110-DAV7A19D678B677CB463A3519C710@phx.gbl>
Message-ID: <436C12C6.8020509@pdf.com>

	  Have you plotted the residuals (without the AR(1))?  Lack of fit can 
masquerade as highly autocorrelated residuals.

	  Alternatively, can you still estimate your model using the first 
differences of your data?  If yes, do you have a problem?

	  Have you studied the appropriate portions of Pinheiro and Bates 
(2000) Mixed-Effects Models in S and S-PLUS (Springer)?  If you'd stiill 
like help from this list, PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html", especially the part about 
providing a toy example in a few lines of code that someone can copy 
from your email into R, try a few things, and craft a response in a 
minute or two.  Doing that will likely increase the speed and utility of 
replies.

	  Best Wishes,
	  Spencer Graves

Robert Schneider wrote:

> I am trying to calibrate a non linear mixed model with a AR(1)
> autocorrelation structure. This structure is indicated by the ACF plot. When
> calibrating the model without the AR(1), everything comes out nicely.
> However, when the AR(1) structure is included in the model, the Phi
> parameter estimate comes out to be 1. This result occurs with various
> starting values in the correlation statement. Am I missing something, or the
> model cannot be calibrated with the AR(1)?  
> 
>  
> 
> Thanks for you input.
> 
>  
> 
> Robert Schneider
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Sat Nov  5 08:04:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 5 Nov 2005 02:04:00 -0500
Subject: [R] RODBC and Excel: Wrong Data Type Assumed on Import
In-Reply-To: <dkgdmd$t5o$1@sea.gmane.org>
References: <adf71a630511030726v10f1dc77sa7c760b96a27480f@mail.gmail.com>
	<971536df0511031753y6eb4e271o797e862bc2740e1a@mail.gmail.com>
	<dkgdmd$t5o$1@sea.gmane.org>
Message-ID: <971536df0511042304ja638fbahd4a0bf71dc4e9616@mail.gmail.com>

On 11/4/05, Earl F. Glynn <efg at stowers-institute.org> wrote:
> "Gabor Grothendieck" <ggrothendieck at gmail.com> wrote in message
> news:971536df0511031753y6eb4e271o797e862bc2740e1a at mail.gmail.com...
> > You could try using the COM interface rather than the ODBC
> > interface.  Try code such as this:
> >
> > library(RDCOMClient)
> > xls <- COMCreate("Excel.Application")
> > xls[["Workbooks"]]$Open("MySpreadsheet.xls")
> > sheet <- xls[["ActiveSheet"]]
> > mydata <- sheet[["UsedRange"]][["value"]]
> > xls$Quit()
> >
> > # convert mydata to a character matrix
> > mydata.char <- matrix(unlist(mydata), nc = length(xx))
>
> Gabor,
>
> Thank you for that suggestion.  I try to avoid COM, but it seems to work
> well with this problem.
>
> Because I have empty cells, which are treated as NULLS, the unlist didn't
> quite work.
>
> Here's what I did:
>
> library(RDCOMClient)
> xls <- COMCreate("Excel.Application")
> xls[["Workbooks"]]$Open("U:/efg/lab/R/Plasmid/construct list.xls")
> sheet <- xls[["ActiveSheet"]]
> mydata <- sheet[["UsedRange"]][["value"]]
> xls$Quit()
>
> for (column in 1:length(mydata))
> {
>  cat(column, " ", length(mydata[[column]]), " ",
> length(unlist(mydata[[column]])), "\n")
> }
>
> The results show that while mydata is a list of columns, if you unlist each
> column you'll be short by the number of NULL values.
>
> 1   1251   1251
> 2   1251   1198
> 3   1251   870
> 4   1251   327
> 5   1251   1250
>
> This seemed a bit crude to fix that problem (can someone suggest a more
> elegant way?):
>
> mymatrix <- NULL
> for (column in 1:length(mydata))
> {
>  # Use lappy to replace NULLs with "" strings, column-by-column
>  mymatrix <- cbind(mymatrix, lapply(mydata[[column]], function(cell) {
> ifelse(is.null(cell), "", cell) } ))
> }
> # Fix column names
> colnames(mymatrix) <- mymatrix[1,]
> mymatrix <- mymatrix[-1,]
>
> > mymatrix[273:276,]
>     Plasmid Number Plasmid
> Concentration Comments Lost
> [1,] 274            "yxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxy"         "1 ug/ul"
> "4 mg"   ""
> [2,] "275a"         "xyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyxyx" "1 ug/2
> ul"   ""       ""
> [3,] "275b"         "xyxyxyxyxyxyxyxyxyxyxyxyx"                   "1 ug/5
> ul"   ""       ""
> [4,] 276            "xyxyxyxyxyxyxyxyxyxyxyxyxyxy"                "1 ug/5
> ul"   ""       "Assumed Lost"
>
> Thank you for preserving "275a" and "275b" as the names here.
>
> So, I'd recommend RDCOMClient over RODBC with Excel files.  "Being lucky"
> shouldn't be part of processing Excel files.
>

You could try something like this which turns the data into
a textConnection which is read using read.table:

con <- textConnection(do.call("paste", mydata))
dd <- read.table(con, header = TRUE, na.strings = "NULL", as.is = TRUE)

You might need to vary the arguments to read.table depending on what
it is you want to get out.  Also, I have assumed that none of the
strings contain
spaces though using a sep= arg on paste and read.table could handle that
too.



From dhinds at sonic.net  Sat Nov  5 08:04:05 2005
From: dhinds at sonic.net (dhinds@sonic.net)
Date: Sat, 5 Nov 2005 07:04:05 +0000 (UTC)
Subject: [R] problem with the assignment function
References: <000134EA228A1A49A552F75B9A13A5C901BBA341@groucho.fhcrc.org>
Message-ID: <dkhlh4$quf$1@sea.gmane.org>

Wang, Yan <ywang at fhcrc.org> wrote:

> I run into the most weird problem I have ever met. I wrote a function
> "rhopair", which calls a .C function. I cannot assign its value to a
> variable using either "=" nor "<-". After I did the assignment,
> "rhopair" cannot reproduce the same result as before with the same
> argument. Here is the code and results:

...

> Any hint or suggestions will be much appreciated.

Some sort of memory allocation bug in the C code?  Use of unprotected
SEXP objects, maybe?

-- Dave



From pburns at pburns.seanet.com  Sat Nov  5 10:35:45 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 05 Nov 2005 09:35:45 +0000
Subject: [R] problem with the assignment function
In-Reply-To: <000134EA228A1A49A552F75B9A13A5C901BBA341@groucho.fhcrc.org>
References: <000134EA228A1A49A552F75B9A13A5C901BBA341@groucho.fhcrc.org>
Message-ID: <436C7CF1.4060806@pburns.seanet.com>

Since you are on Linux, I suggest that you use valgrind:
http://www.valgrind.org/

Download it, build it, then do:

R -d valgrind

and run your function as usual.

It is hard to emphasize enough how useful this is for tracking
down problems in C code.  When developing on Windows,
it would often be worthwhile to get a Linux machine just to be
able to use valgrind.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Wang, Yan wrote:

>Hello,
>
> 
>
>I run into the most weird problem I have ever met. I wrote a function
>"rhopair", which calls a .C function. I cannot assign its value to a
>variable using either "=" nor "<-". After I did the assignment,
>"rhopair" cannot reproduce the same result as before with the same
>argument. Here is the code and results:
>
> 
>
>  
>
>>rhopair(x=SNP,bp=bpp[1:700],A=93,B=BB)
>>    
>>
>
>[1] 0.1012352
>
>  
>
>>rhopair(x=SNP,bp=bpp[1:700],A=93,B=BB)
>>    
>>
>
>[1] 0.1012352
>
>  
>
>>a=rhopair(x=SNP,bp=bpp[1:700],A=93,B=BB)
>>    
>>
>
>  
>
>>a
>>    
>>
>
>[1] 0.004524671
>
>  
>
>>rhopair(x=SNP,bp=bpp[1:700],A=93,B=BB)
>>    
>>
>
>[1] 5.241946e-06
>
>  
>
>>a=rhopair(x=SNP,bp=bpp[1:700],A=93,B=BB)
>>    
>>
>
>  
>
>>a
>>    
>>
>
>[1] 0.004524671
>
>  
>
>>sessionInfo()
>>    
>>
>
>R version 2.2.0, 2005-10-06, i686-pc-linux-gnu
>
> 
>
>attached base packages:
>
>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
>"datasets"
>
>[7] "base"
>
> 
>
>other attached packages:
>
>  fineld   hopach  cluster   fields  lattice
>
> "1.0.0"  "1.2.1" "1.10.2"    "2.0" "0.12-9"
>
> 
>
> 
>
>Any hint or suggestions will be much appreciated.
>
> 
>
>Yan
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From pcampbell at econ.bbk.ac.uk  Sat Nov  5 14:31:22 2005
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Sat, 5 Nov 2005 13:31:22 -0000
Subject: [R] Stochastic Volatility
Message-ID: <NGECIFANPOJAGABBAEAPGEHGFEAA.pcampbell@econ.bbk.ac.uk>

Has anybody implemented or tried to implement a stochastic volatility model
using the Kalman filter following a series of papers by Harvey, Ruiz and
Shepard?

This is a sophisticated approach for estimating an important class of
models, so I am surprised that no implementation exists, is this because
there are unforeseeable problems?

In a related but off topic question, it has been a while since I looked at
the non homoskedastic time series literature but back then you couldn't pick
up a journal without reading another stochastic volatility paper, does
anybody have any ideas why the literature has drifted back toward less
satisfactory GARCH and EGARCH models?

This question is somewhat moot as if I choose to pursue this I will
implement a model myself.


Phineas Campbell



From shaboe at bezeqint.net  Tue Nov  1 11:39:28 2005
From: shaboe at bezeqint.net (Erez)
Date: Tue, 1 Nov 2005 12:39:28 +0200
Subject: [R] (no subject)
Message-ID: <000a01c5ded0$89700170$0300000a@sfoyzrs7vbe2ba>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051101/fe4978d9/attachment.pl

From hristostoichkov at gmail.com  Sat Nov  5 15:41:34 2005
From: hristostoichkov at gmail.com (Robert Roig)
Date: Sat, 5 Nov 2005 15:41:34 +0100
Subject: [R] Problem executing C code in R
In-Reply-To: <c4b5ebd40511050508k34313518gfac8539868a3263d@mail.gmail.com>
References: <c4b5ebd40511050508k34313518gfac8539868a3263d@mail.gmail.com>
Message-ID: <c4b5ebd40511050641t1356f3bfx667e917df3dba860@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051105/f37598f5/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Sat Nov  5 15:55:35 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 05 Nov 2005 14:55:35 -0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <000a01c5ded0$89700170$0300000a@sfoyzrs7vbe2ba>
Message-ID: <XFMail.051105145535.Ted.Harding@nessie.mcc.ac.uk>

On 01-Nov-05 Erez wrote:
> Hi
> 
> I need an advise if any one can help me.
> i have mass of data in 2 array A and B:
> A = 0 1 0 0 1 1 0 0 
> B = 0 0 0 1 0 1 1 1
> and i have 3 rules to merge them into 3rd array C:
> if A[i] + B[i] == 0 then C[i]=0 
> if A[i] + B[i] == 1 then C[i]=1
> if A[i] + B[i] == 2 then C[i]=2
> it looks easy but with the regular way (loop) with large data it takes
> days (i test it).
> If any one can advise me what to do i'll be happy.
> 
> Thanks
> Erez  

Maybe there is a hidden complication in your context, but if those
are the only possibilities (as you have stated it above), then
what is wrong with:

  C = A + B

??

On the other hand, for instance, if C has values and you only want
to change those values as above for the relevant values of i, then
you could do

  ix <- (A+B==0)|(A+B==1)|(A+B==2)
  C[ix] <- A[ix] + B[ix]

or some similar possibility, depending on what you really want to do.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Nov-05                                       Time: 14:55:33
------------------------------ XFMail ------------------------------



From jfox at mcmaster.ca  Sat Nov  5 15:57:56 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 5 Nov 2005 09:57:56 -0500
Subject: [R] (no subject)
In-Reply-To: <000a01c5ded0$89700170$0300000a@sfoyzrs7vbe2ba>
Message-ID: <20051105145754.JCVB26550.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Erez,

How about C <- A + B ?

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Erez
> Sent: Tuesday, November 01, 2005 5:39 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] (no subject)
> 
> Hi
> 
> I need an advise if any one can help me.
> i have mass of data in 2 array A and B:
> A = 0 1 0 0 1 1 0 0
> B = 0 0 0 1 0 1 1 1
> and i have 3 rules to merge them into 3rd array C:
> if A[i] + B[i] == 0 then C[i]=0
> if A[i] + B[i] == 1 then C[i]=1
> if A[i] + B[i] == 2 then C[i]=2
> it looks easy but with the regular way (loop) with large data 
> it takes days (i test it).
> If any one can advise me what to do i'll be happy.
> 
> Thanks
> Erez  
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From MSchwartz at mn.rr.com  Sat Nov  5 15:59:25 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 05 Nov 2005 08:59:25 -0600
Subject: [R] Adding Two Vectors (was: no subject)
In-Reply-To: <000a01c5ded0$89700170$0300000a@sfoyzrs7vbe2ba>
References: <000a01c5ded0$89700170$0300000a@sfoyzrs7vbe2ba>
Message-ID: <1131202766.4173.7.camel@localhost.localdomain>

On Tue, 2005-11-01 at 12:39 +0200, Erez wrote:
> Hi
> 
> I need an advise if any one can help me.
> i have mass of data in 2 array A and B:
> A = 0 1 0 0 1 1 0 0 
> B = 0 0 0 1 0 1 1 1
> and i have 3 rules to merge them into 3rd array C:
> if A[i] + B[i] == 0 then C[i]=0 
> if A[i] + B[i] == 1 then C[i]=1
> if A[i] + B[i] == 2 then C[i]=2
> it looks easy but with the regular way (loop) with large data it takes days (i test it).
> If any one can advise me what to do i'll be happy.
> 
> Thanks
> Erez  
>  

[I did not see a reply to this. The date indicates that it was sent on
Tuesday, but I just got it today]


Please use an informative subject.

What's wrong with just adding the two vectors, if the rules are as
simple as you indicate?

> A
[1] 0 1 0 0 1 1 0 0

> B
[1] 0 0 0 1 0 1 1 1

> C <- A + B

> C
[1] 0 1 0 1 1 2 1 1


HTH,

Marc Schwartz



From Ted.Harding at nessie.mcc.ac.uk  Sat Nov  5 16:15:12 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 05 Nov 2005 15:15:12 -0000 (GMT)
Subject: [R] Hypatia?? [Was: Adding Two Vectors (was: no subject)]
In-Reply-To: <1131202766.4173.7.camel@localhost.localdomain>
Message-ID: <XFMail.051105151512.Ted.Harding@nessie.mcc.ac.uk>

On 05-Nov-05 Marc Schwartz wrote:
> On Tue, 2005-11-01 at 12:39 +0200, Erez wrote:
>> Hi
>> 
>> I need an advise if any one can help me.
>> i have mass of data in 2 array A and B:
>> A = 0 1 0 0 1 1 0 0 
>> B = 0 0 0 1 0 1 1 1
>> and i have 3 rules to merge them into 3rd array C:
>> if A[i] + B[i] == 0 then C[i]=0 
>> if A[i] + B[i] == 1 then C[i]=1
>> if A[i] + B[i] == 2 then C[i]=2
>> it looks easy but with the regular way (loop) with large data it takes
>> days (i test it).
>> If any one can advise me what to do i'll be happy.
>> 
>> Thanks
>> Erez  
>>  
> 
> [I did not see a reply to this. The date indicates that it was sent on
> Tuesday, but I just got it today]

This looks like a problam with hypatia:

[2]:
Received: from hypatia.math.ethz.ch (hypatia [129.132.145.15]) by
 hypatia.math.ethz.ch (8.13.4/8.13.4) with ESMTP id jA5ERVxZ007442;
 Sat, 5 Nov 2005 15:31:49 +0100

[1]:
Received: from sa4.bezeqint.net (sa4.bezeqint.net [192.115.104.18]) by
 hypatia.math.ethz.ch (8.13.4/8.13.4) with ESMTP id jA1AdRni004238 for
 <r-help at stat.math.ethz.ch>; Tue, 1 Nov 2005 11:39:27 +0100


Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Nov-05                                       Time: 15:15:02
------------------------------ XFMail ------------------------------



From MSchwartz at mn.rr.com  Sat Nov  5 16:20:40 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 05 Nov 2005 09:20:40 -0600
Subject: [R] Hypatia?? [Was: Adding Two Vectors (was: no subject)]
In-Reply-To: <XFMail.051105151512.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051105151512.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1131204040.4173.16.camel@localhost.localdomain>

On Sat, 2005-11-05 at 15:15 +0000, Ted Harding wrote:
> On 05-Nov-05 Marc Schwartz wrote:
> > On Tue, 2005-11-01 at 12:39 +0200, Erez wrote:
> >> Hi
> >> 
> >> I need an advise if any one can help me.
> >> i have mass of data in 2 array A and B:
> >> A = 0 1 0 0 1 1 0 0 
> >> B = 0 0 0 1 0 1 1 1
> >> and i have 3 rules to merge them into 3rd array C:
> >> if A[i] + B[i] == 0 then C[i]=0 
> >> if A[i] + B[i] == 1 then C[i]=1
> >> if A[i] + B[i] == 2 then C[i]=2
> >> it looks easy but with the regular way (loop) with large data it takes
> >> days (i test it).
> >> If any one can advise me what to do i'll be happy.
> >> 
> >> Thanks
> >> Erez  
> >>  
> > 
> > [I did not see a reply to this. The date indicates that it was sent on
> > Tuesday, but I just got it today]
> 
> This looks like a problam with hypatia:
> 
> [2]:
> Received: from hypatia.math.ethz.ch (hypatia [129.132.145.15]) by
>  hypatia.math.ethz.ch (8.13.4/8.13.4) with ESMTP id jA5ERVxZ007442;
>  Sat, 5 Nov 2005 15:31:49 +0100
> 
> [1]:
> Received: from sa4.bezeqint.net (sa4.bezeqint.net [192.115.104.18]) by
>  hypatia.math.ethz.ch (8.13.4/8.13.4) with ESMTP id jA1AdRni004238 for
>  <r-help at stat.math.ethz.ch>; Tue, 1 Nov 2005 11:39:27 +0100
> 
> 
> Ted.

Ted,

Agreed. There were a couple of other delayed posts that came through as
well.

I added Martin here, as an FYI.

Thanks,

Marc



From patrick.giraudoux at univ-fcomte.fr  Sat Nov  5 18:03:05 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 05 Nov 2005 18:03:05 +0100
Subject: [R] nlme, predict.nlme, levels not allowed
Message-ID: <436CE5C9.9050602@univ-fcomte.fr>

Dear listers,

I am trying to fit a nlme model with "age" and "pds" as reals, and 
"zone" a factor  with two levels "Annaba" and "Boumalek" . The "best" 
model found is the following:

 > modm3
Nonlinear mixed-effects model fit by maximum likelihood
  Model: pds ~ Asym/(1 + exp((xmid - age)/scal))
  Data: croispulm
  Log-likelihood: -91.86667
  Fixed: list(Asym ~ zone, xmid ~ zone, scal ~ 1)
 Asym.(Intercept) Asym.zoneBoumalek  xmid.(Intercept) 
xmid.zoneBoumalek              scal
       9.99551079        0.39423966        4.97981027        
0.06969807        2.23116661

Random effects:
 Formula: list(Asym ~ 1, xmid ~ 1)
 Level: nichoir
 Structure: General positive-definite, Log-Cholesky parametrization
                 StdDev       Corr 
Asym.(Intercept) 1.796565e-06 As.(I)
xmid.(Intercept) 1.219400e-04 0    
Residual         6.163282e-01      

Correlation Structure: Continuous AR(1)
 Formula: ~age | nichoir
 Parameter estimate(s):
      Phi
0.3395242
Number of Observations: 102
Number of Groups: 17


Everything normal so far.

Things come to be strange when I try to compute predicted values:

 > pred<-predict(modm3,newdata=mydata,type="response")
Error in predict.nlme(modm3, newdata = mydata, type = "response") :
        Levels Annaba,Boumalek not allowed for zone

I have checked and re-checked that zone in the newdata is well a factor 
with the "good" levels, and I can hardly understand why these two levels 
used when fitting the model are now rejected when used for computing 
predicted values.

Any hint welcome,

Best regards,

Patrick



From claus.atzenbeck at freenet.de  Sat Nov  5 18:04:34 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Sat, 5 Nov 2005 18:04:34 +0100 (CET)
Subject: [R] sort table
Message-ID: <Pine.OSX.4.61.0511051754380.25658@cirrus.local>

Hi,

I have a data frame named "questions" that I use to get a subset and
then a table:

failcondonly <- subset(questions, errorreason=="condition")
failcondtab <- table(failcondonly$type, failcondonly$qid, exclude=c("ORGA", "skipped"))

The failcondtab looks like this:

        6 11 12 13 14 15 17 26 30 31 39 41
gave up 0  1  1  1  1  0  0  0  0  8  0  2
wrong   3  3  7  3  3  1  4  1  3  4  1  2

I want to sort this table according to the *sum* of the values in "gave
up" and "wrong" in reverse order. The highest value should be first.
First would be column "31" (value 8+4=12), then column "12" (value
1+7=8), etc.

sort(failcondtab) does not help, since it sorts the column names, not
the sum of the values of each column.

Thanks for any hint.
Claus



From maechler at stat.math.ethz.ch  Sat Nov  5 17:52:13 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 5 Nov 2005 17:52:13 +0100
Subject: [R] Hypatia?? [Was: Adding Two Vectors (was: no subject)]
In-Reply-To: <1131204040.4173.16.camel@localhost.localdomain>
References: <XFMail.051105151512.Ted.Harding@nessie.mcc.ac.uk>
	<1131204040.4173.16.camel@localhost.localdomain>
Message-ID: <17260.58173.119304.313578@stat.math.ethz.ch>

>>>>> "Marc" == Marc Schwartz <MSchwartz at mn.rr.com>
>>>>>     on Sat, 05 Nov 2005 09:20:40 -0600 writes:

    Marc> On Sat, 2005-11-05 at 15:15 +0000, Ted Harding wrote:
    >> On 05-Nov-05 Marc Schwartz wrote: > On Tue, 2005-11-01 at
    >> 12:39 +0200, Erez wrote: >> Hi

    >> > [I did not see a reply to this. The date indicates that
    >>    it was sent on  Tuesday, but I just got it today]
    >> 
    >> This looks like a problam with hypatia:
    >> 
    >> [2]: Received: from hypatia.math.ethz.ch (hypatia
    >> [129.132.145.15]) by hypatia.math.ethz.ch (8.13.4/8.13.4)
    >> with ESMTP id jA5ERVxZ007442; Sat, 5 Nov 2005 15:31:49
    >> +0100
    >> 
    >> [1]: Received: from sa4.bezeqint.net (sa4.bezeqint.net
    >> [192.115.104.18]) by hypatia.math.ethz.ch (8.13.4/8.13.4)
    >> with ESMTP id jA1AdRni004238 for
    >> <r-help at stat.math.ethz.ch>; Tue, 1 Nov 2005 11:39:27
    >> +0100
    >> 
    >> 
    >> Ted.

    Marc> Ted,

    Marc> Agreed. There were a couple of other delayed posts
    Marc> that came through as well.

    Marc> I added Martin here, as an FYI.

    Marc> Thanks,

    Marc> Marc

No problems with Hypatia, just with Martin ...
He has been away at a nice workshop in Treviso
"Robust Statistics and R" and then at the CSDA conference in
Cyprus.  After coming back he has been quite busy and only today
found time to act as mailing list moderator to approve those
e-mails that were filtered accidentally ("False positives").   
For some reasons, our spam filter recently gives e-mails from
Yahoo and Hotmail a relatively too high spammyness score;  if
you additionally send "HTML"ified e-mails, chances have become pretty
high your mail won't get through.  And, BTW, only some of them will
occasionally be manually approved by me.

Martin



From MSchwartz at mn.rr.com  Sat Nov  5 18:40:58 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 05 Nov 2005 11:40:58 -0600
Subject: [R] Hypatia?? [Was: Adding Two Vectors (was: no subject)]
In-Reply-To: <17260.58173.119304.313578@stat.math.ethz.ch>
References: <XFMail.051105151512.Ted.Harding@nessie.mcc.ac.uk>
	<1131204040.4173.16.camel@localhost.localdomain>
	<17260.58173.119304.313578@stat.math.ethz.ch>
Message-ID: <1131212459.4173.22.camel@localhost.localdomain>

On Sat, 2005-11-05 at 17:52 +0100, Martin Maechler wrote:
> >>>>> "Marc" == Marc Schwartz <MSchwartz at mn.rr.com>
> >>>>>     on Sat, 05 Nov 2005 09:20:40 -0600 writes:
> 
>     Marc> On Sat, 2005-11-05 at 15:15 +0000, Ted Harding wrote:
>     >> On 05-Nov-05 Marc Schwartz wrote: > On Tue, 2005-11-01 at
>     >> 12:39 +0200, Erez wrote: >> Hi
> 
>     >> > [I did not see a reply to this. The date indicates that
>     >>    it was sent on  Tuesday, but I just got it today]
>     >> 
>     >> This looks like a problam with hypatia:
>     >> 
>     >> [2]: Received: from hypatia.math.ethz.ch (hypatia
>     >> [129.132.145.15]) by hypatia.math.ethz.ch (8.13.4/8.13.4)
>     >> with ESMTP id jA5ERVxZ007442; Sat, 5 Nov 2005 15:31:49
>     >> +0100
>     >> 
>     >> [1]: Received: from sa4.bezeqint.net (sa4.bezeqint.net
>     >> [192.115.104.18]) by hypatia.math.ethz.ch (8.13.4/8.13.4)
>     >> with ESMTP id jA1AdRni004238 for
>     >> <r-help at stat.math.ethz.ch>; Tue, 1 Nov 2005 11:39:27
>     >> +0100
>     >> 
>     >> 
>     >> Ted.
> 
>     Marc> Ted,
> 
>     Marc> Agreed. There were a couple of other delayed posts
>     Marc> that came through as well.
> 
>     Marc> I added Martin here, as an FYI.
> 
>     Marc> Thanks,
> 
>     Marc> Marc
> 
> No problems with Hypatia, just with Martin ...
> He has been away at a nice workshop in Treviso
> "Robust Statistics and R" and then at the CSDA conference in
> Cyprus.  After coming back he has been quite busy and only today
> found time to act as mailing list moderator to approve those
> e-mails that were filtered accidentally ("False positives").   
> For some reasons, our spam filter recently gives e-mails from
> Yahoo and Hotmail a relatively too high spammyness score;  if
> you additionally send "HTML"ified e-mails, chances have become pretty
> high your mail won't get through.  And, BTW, only some of them will
> occasionally be manually approved by me.
> 
> Martin

Thanks for the clarification Martin.

Is there a benign (time sparing) way of tagging these messages when they
come through, so that is can be known, first and foremost, that they
required manual intervention? The goal being to modify posting behavior
and to ultimately minimize the amount of your time that is required on
these?

Hope that your travels and the meetings were enjoyable!

Best regards,

Marc



From MSchwartz at mn.rr.com  Sat Nov  5 18:44:27 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 05 Nov 2005 11:44:27 -0600
Subject: [R] sort table
In-Reply-To: <Pine.OSX.4.61.0511051754380.25658@cirrus.local>
References: <Pine.OSX.4.61.0511051754380.25658@cirrus.local>
Message-ID: <1131212668.4173.26.camel@localhost.localdomain>

On Sat, 2005-11-05 at 18:04 +0100, Claus Atzenbeck wrote:
> Hi,
> 
> I have a data frame named "questions" that I use to get a subset and
> then a table:
> 
> failcondonly <- subset(questions, errorreason=="condition")
> failcondtab <- table(failcondonly$type, failcondonly$qid, exclude=c("ORGA", "skipped"))
> 
> The failcondtab looks like this:
> 
>         6 11 12 13 14 15 17 26 30 31 39 41
> gave up 0  1  1  1  1  0  0  0  0  8  0  2
> wrong   3  3  7  3  3  1  4  1  3  4  1  2
> 
> I want to sort this table according to the *sum* of the values in "gave
> up" and "wrong" in reverse order. The highest value should be first.
> First would be column "31" (value 8+4=12), then column "12" (value
> 1+7=8), etc.
> 
> sort(failcondtab) does not help, since it sorts the column names, not
> the sum of the values of each column.
> 
> Thanks for any hint.
> Claus


Something like the following should work:

> failcondtab[, rev(order(colSums(failcondtab)))]
        31 12 41 17 14 13 11 30 6 39 26 15
gave up  8  1  2  0  1  1  1  0 0  0  0  0
wrong    4  7  2  4  3  3  3  3 3  1  1  1


See ?order, ?rev and ?colSums.

HTH,

Marc Schwartz



From MSchwartz at mn.rr.com  Sat Nov  5 18:56:19 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 05 Nov 2005 11:56:19 -0600
Subject: [R] sort table
In-Reply-To: <1131212668.4173.26.camel@localhost.localdomain>
References: <Pine.OSX.4.61.0511051754380.25658@cirrus.local>
	<1131212668.4173.26.camel@localhost.localdomain>
Message-ID: <1131213380.4173.29.camel@localhost.localdomain>

On Sat, 2005-11-05 at 11:44 -0600, Marc Schwartz wrote:
> On Sat, 2005-11-05 at 18:04 +0100, Claus Atzenbeck wrote:
> > Hi,
> > 
> > I have a data frame named "questions" that I use to get a subset and
> > then a table:
> > 
> > failcondonly <- subset(questions, errorreason=="condition")
> > failcondtab <- table(failcondonly$type, failcondonly$qid, exclude=c("ORGA", "skipped"))
> > 
> > The failcondtab looks like this:
> > 
> >         6 11 12 13 14 15 17 26 30 31 39 41
> > gave up 0  1  1  1  1  0  0  0  0  8  0  2
> > wrong   3  3  7  3  3  1  4  1  3  4  1  2
> > 
> > I want to sort this table according to the *sum* of the values in "gave
> > up" and "wrong" in reverse order. The highest value should be first.
> > First would be column "31" (value 8+4=12), then column "12" (value
> > 1+7=8), etc.
> > 
> > sort(failcondtab) does not help, since it sorts the column names, not
> > the sum of the values of each column.
> > 
> > Thanks for any hint.
> > Claus
> 
> 
> Something like the following should work:
> 
> > failcondtab[, rev(order(colSums(failcondtab)))]
>         31 12 41 17 14 13 11 30 6 39 26 15
> gave up  8  1  2  0  1  1  1  0 0  0  0  0
> wrong    4  7  2  4  3  3  3  3 3  1  1  1
> 
> 
> See ?order, ?rev and ?colSums.

Or another, better variation, would be:

> failcondtab[, order(colSums(failcondtab), decreasing = TRUE)]
        31 12 11 13 14 17 41 6 30 15 26 39
gave up  8  1  1  1  1  0  2 0  0  0  0  0
wrong    4  7  3  3  3  4  2 3  3  1  1  1

which removes the requirement to use rev().

Marc



From tariq.khan at gmail.com  Sat Nov  5 19:08:12 2005
From: tariq.khan at gmail.com (=?ISO-8859-1?Q?=A8Tariq_Khan?=)
Date: Sat, 5 Nov 2005 18:08:12 +0000
Subject: [R] R CMD INSTALL --help crashes R for Windows
Message-ID: <2310043c0511051008w579cda68y6c87fa3a3418a5d6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051105/2e790f33/attachment.pl

From sfalcon at fhcrc.org  Sat Nov  5 20:02:29 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Sat, 05 Nov 2005 11:02:29 -0800
Subject: [R] Search within a file
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F414E@us-arlington-0668.mail.saic.com>
	(Jaroslaw W. Tuszynski's message of "Fri, 4 Nov 2005 10:53:40 -0500")
References: <CA0BCF3BED56294AB91E3AD74B849FD57F414E@us-arlington-0668.mail.saic.com>
Message-ID: <m2y842rk8q.fsf@macaroni.local>

On  4 Nov 2005, JAROSLAW.W.TUSZYNSKI at saic.com wrote:

> Thanks for a great suggestions. I guess the code you suggested would
> look something like this:

Glad you found it useful.  A quick note on a potentially serious
problem:

It seems entirely possibly that you split an instance of pattern in
the buffer.  That is, if you are searching for "right" you might read
in "I have a ri" in the first read and then "ght to a trial by jury"
in the second read.

If you know the pattern you are looking for won't be split over
multiple lines, you could use readLines instead of readChar.

Depending on the application you could also read "words" or use some
other info about the strucutre of the data you are reading to ensure
you don't ever split a pattern across a read.

+ seth



From patrick_hawkin at yahoo.com  Sat Nov  5 20:10:44 2005
From: patrick_hawkin at yahoo.com (Patrick Ho)
Date: Sat, 5 Nov 2005 19:10:44 +0000 (GMT)
Subject: [R] Goodness of fit tests
Message-ID: <20051105191044.82743.qmail@web50803.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051105/f9470d11/attachment.pl

From maechler at stat.math.ethz.ch  Sat Nov  5 20:18:43 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 5 Nov 2005 20:18:43 +0100
Subject: [R] Hypatia?? [Was: Adding Two Vectors (was: no subject)]
In-Reply-To: <1131212459.4173.22.camel@localhost.localdomain>
References: <XFMail.051105151512.Ted.Harding@nessie.mcc.ac.uk>
	<1131204040.4173.16.camel@localhost.localdomain>
	<17260.58173.119304.313578@stat.math.ethz.ch>
	<1131212459.4173.22.camel@localhost.localdomain>
Message-ID: <17261.1427.592860.555583@stat.math.ethz.ch>

>>>>> "Marc" == Marc Schwartz <MSchwartz at mn.rr.com>
>>>>>     on Sat, 05 Nov 2005 11:40:58 -0600 writes:

    Marc> On Sat, 2005-11-05 at 17:52 +0100, Martin Maechler wrote:

    ....................

    Marc> Agreed. There were a couple of other delayed posts
    Marc> that came through as well.
    >> 
    Marc> I added Martin here, as an FYI.
    >> 
    Marc> Thanks,
    >> 
    Marc> Marc
    >> 
    >> No problems with Hypatia, just with Martin ...
    >> He has been away at a nice workshop in Treviso
    >> "Robust Statistics and R" and then at the CSDA conference in
    >> Cyprus.  After coming back he has been quite busy and only today
    >> found time to act as mailing list moderator to approve those
    >> e-mails that were filtered accidentally ("False positives").   
    >> For some reasons, our spam filter recently gives e-mails from
    >> Yahoo and Hotmail a relatively too high spammyness score;  if
    >> you additionally send "HTML"ified e-mails, chances have become pretty
    >> high your mail won't get through.  And, BTW, only some of them will
    >> occasionally be manually approved by me.
    >> 
    >> Martin

    Marc> Thanks for the clarification Martin.

    Marc> Is there a benign (time sparing) way of tagging these messages when they
    Marc> come through, so that is can be known, first and foremost, that they
    Marc> required manual intervention? The goal being to modify posting behavior
    Marc> and to ultimately minimize the amount of your time that is required on
    Marc> these?

Thank you for the questions, Marc, 
but indeed, I had posed them to myself more than once in the
past.  AFAIK, it's not easily possible, at least not for those
that are caught by Mailman's filters {which for some lists I
made trigger on the spamassassin "*"s}.
Of course we could file a mailman feature request for this.. any
volunteers?

The 'much more probable' spam is not fed to Mailman at all but
put into spam-mailboxes which I skim through somewhat regularly
but not entirely systematically. Also, these (4 and more '*')
messages are very rarely false positives.  These then need even
more manual work, and are actually repostings (though not
visibly in the visible mail headers IIRC) with the advantage
that I could easily add "editorial remarks".

Martin



From spencer.graves at pdf.com  Sat Nov  5 20:26:59 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 05 Nov 2005 11:26:59 -0800
Subject: [R] predictive interval in nlme
In-Reply-To: <20051018174550.62991.qmail@web51806.mail.yahoo.com>
References: <20051018174550.62991.qmail@web51806.mail.yahoo.com>
Message-ID: <436D0783.7080400@pdf.com>

	  This is a difficult problem.  After several false starts with 
RSiteSearch, I finally got 15 hits from 'RSiteSearch("predict lme with 
confidence")', including the following 3 which looked to me like they 
might help with your question:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/27860.html
This outlines the mathematics required to do this conditional on the 
variance components estimates, and includes a comment from Doug Bates 
saying, "The new version of the lme4 package has a method for the "vcov"
generic. This method returns" the "Sig" matrix required to do this 
computation.

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/42666.html
Suggests using estimable() from the gmodels part of the gregmisc package.

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/44675.html
This includes a comment by Doug Bates saying, 'That's not currently 
implemented in lme. It's on the "To Do" list but it is not very close to 
the top.'

	  It also might be interesting and useful to check the accuracy of the 
conditioning using Monte Carlo.

	  Spencer Graves

Cunningham Kerry wrote:

> Suppose I have the following data:
> 
> y x id
> 44 0 104
> 48 58 104
> 48 55 204
> 47 105 204
> 41 275 206
> 18 67 209
> .......
> 
> I fit the model
> 
> 
>>fit=lme(y~x+I(x^2),random=~1|id)
> 
> 
> Now I want to make a prediction plot:
> 
> 
>>time=seq(0,300,len=100)
>>plot(predict(fit,data.frame(x=time),level=0))
> 
> 
> Very fine. It gives me the prediction curve based on
> the model. My further request is to make a confidence
> bands around the curve. I guess I can derive its
> mathematical form analytically and implement it
> myself. But I just hope some experts can point out one
> simple way in R to avoid my redundant work.
> 
> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From syy2004 at gmail.com  Sat Nov  5 20:38:28 2005
From: syy2004 at gmail.com (Yuying Shi)
Date: Sat, 5 Nov 2005 14:38:28 -0500
Subject: [R] question about R code
Message-ID: <91d269c60511051138g1d3cb5bfmf52a4b49775fa55@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051105/200dae11/attachment.pl

From spencer.graves at pdf.com  Sat Nov  5 20:42:25 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 05 Nov 2005 11:42:25 -0800
Subject: [R] nlme  Singularity in backsolve at level 0, block 1
In-Reply-To: <20051019152706.20153.qmail@web32111.mail.mud.yahoo.com>
References: <20051019152706.20153.qmail@web32111.mail.mud.yahoo.com>
Message-ID: <436D0B21.5000907@pdf.com>

	  RSiteSearch("Singularity in backsolve") produced 33 hits, the second 
of which was the following:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/62691.html

	  This reply from Peter Dalgaard dates 8 Oct. 2005 asks, Which version 
of R and NLME? R 2.2.0 ships with a version where the internal optimizer 
is changed to nlminb(). As I understand it, this was in response to 
reports where code that worked in S-PLUS refused to
work in R."

	  If you are using R 2.2.0 and the latest version of nlme, then it's 
possible (likely?) that your model is overparameterized, and can't be 
fit with nlme.  Can you fit the model using, e.g., nls ignoring the 
random effects model, as suggested in ch. 8 of Pinheiro and Bates (2000) 
Mixed-Effects Models for S and S-Plus (Springer)?  If no, it's virtually 
certain that nlme won't work, either.  If I had trouble diagnosing the 
problem, I might recode it for optim(..., hessian=T), then look at the 
eigenvalues and vectors of the hessian to understand the deficiencies in 
the model.

	  If nls works for you but not nlsList, there still might be hope, but 
it would be more difficult.  If I had that problem and it were 
sufficiently important, I would step through nlme until I could figure 
out how to modify the code so it would continue, as does optim, and 
provide answers that would help me diagnose the singularity.  (I'd also 
include a facility for providing a prior distribution that should 
eliminate the singularities.)

	  Hope this helps.
	  spencer graves

Elizabeth Lawson wrote:

> Hi,
>  
> I am hoping some one can help with this.
>  
> I am using nlme to fit a random coefficients model. It ran for hours before returning 
>  
> Error: Singularity in backsolve at level 0, block 1
> 
> The model is 
> 
>>plavix.nlme<-nlme(PLX_NRX~loglike(
PLX_NRX,PD4_42D,GAT_34D,VIS_42D,MSL_42D,SPE_ROL,XM2_DUM,THX_DUM,b0,b1,b2,b3,b4,b5,b6,b7,alpha),
> 
> + data=data,
> + fixed=list(b0 + b1+b2+b3+b4+b5+b6+b7+alpha~1),
> + random=b0+b1+b2+b3+b4+b5+b6+b7~1|menum,
> + 
> + start=c(b0=0,b1=0,b2=0,b3=0,b4=0,b5=0,b6=0,b7=0,alpha=5)
> + )
> 
> Can anyone tell me what this error means and how I can run the model?
>  
> Thanks,
>  
> Elizabeth Lawson
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From syy2004 at gmail.com  Sat Nov  5 20:43:55 2005
From: syy2004 at gmail.com (Yuying Shi)
Date: Sat, 5 Nov 2005 14:43:55 -0500
Subject: [R] question about R code
Message-ID: <91d269c60511051143h21dc8be1odd0dd8bbbba3304c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051105/f24bf0c5/attachment.pl

From ripley at stats.ox.ac.uk  Sat Nov  5 20:49:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Nov 2005 19:49:19 +0000 (GMT)
Subject: [R] R CMD INSTALL --help crashes R for Windows
In-Reply-To: <2310043c0511051008w579cda68y6c87fa3a3418a5d6@mail.gmail.com>
References: <2310043c0511051008w579cda68y6c87fa3a3418a5d6@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0511051943040.27180@gannet.stats>

On Sat, 5 Nov 2005, ?Tariq Khan wrote:

> Greetings,
>
> This is potentially a very simple question that arose as I am trying my best
> to learn how to compile dll's using SHLIB, and also finally how to compile
> packages from source (after being a windows user for 2 years it is about
> time).
>
> The question: I am trying to figure out why on my machine R for Windows
> Front-end crashes when on the command prompt I type any of the following:
> R CMD INSTALL --help
> R CMD SHLIB --help
> ...
>
> I get a windows dialog box asking if i want to send a bug report to
> microsoft.

This is specific to you: it works correctly for me and the alpha- and 
beta-testers.  Do you have Perl installed correctly, and does it work? 
(Those are Perl scripts.) What does just-in-time debugging show as the 
cause of the crash (see the rw-FAQ for how to enable this)?

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Ted.Harding at nessie.mcc.ac.uk  Sat Nov  5 21:25:42 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 05 Nov 2005 20:25:42 -0000 (GMT)
Subject: [R] question about R code
In-Reply-To: <91d269c60511051138g1d3cb5bfmf52a4b49775fa55@mail.gmail.com>
Message-ID: <XFMail.051105202542.Ted.Harding@nessie.mcc.ac.uk>

On 05-Nov-05 Yuying Shi wrote:
> Dear Sir/Madam,
> If I have matrics as follows:
>> a <- c(1,1,0,0)
>> b <- c(4,4,0,0)
>> c <- c(3,5,5,6)
> How can I use R code to solve the equation ax^2+bx+c=0.
> thanks!
> yuying shi

I'm not sure what your question really should be, but taking
it at face value and not looking at it too hard, specifically
taking a, b and c to be vectors and x a scalar (so that you have
four distinct quandratic equations wrapped into vectors), then

  a <- as.complex(a) ; b <- as.complex(b) ; c <- as.complex(c)

followed by

  (-b + sqrt(b^2 - 4*a*c)/(2*a)

will give you 4 distinct solutions (one for each quadratic equation)
Two of these solutions, in your case, are meaningful (the others
consisting of NaNs).

Likewise

  (-b - sqrt(b^2 - 4*a*c)/(2*a)

will give you 4 more solutions, again only two meaningful. Taking
these together gives you the four solution-pairs (two meaningful)
for the four quadratic equations.

However, in your equations as given above you should ignore the
3rd and 4th components with a=0 and b=0, since the corresponding
equations reduce to

  5 = 0

  6 = 0

and these have no solution in x (or anything else) ...

If that is not the solution to your question, please give more
detail about your question!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Nov-05                                       Time: 20:20:18
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Sat Nov  5 21:25:42 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 05 Nov 2005 20:25:42 -0000 (GMT)
Subject: [R] Hypatia?? [Was: Adding Two Vectors (was: no subject)]
In-Reply-To: <17260.58173.119304.313578@stat.math.ethz.ch>
Message-ID: <XFMail.051105202542.Ted.Harding@nessie.mcc.ac.uk>

On 05-Nov-05 Martin Maechler wrote:
> 
> No problems with Hypatia, just with Martin ...
> He has been away at a nice workshop in Treviso
> "Robust Statistics and R" and then at the CSDA conference in
> Cyprus.

I don't think there are problems with Martin either!

  After coming back he has been quite busy and only today
> found time to act as mailing list moderator to approve those
> e-mails that were filtered accidentally ("False positives").   
> For some reasons, our spam filter recently gives e-mails from
> Yahoo and Hotmail a relatively too high spammyness score;  if
> you additionally send "HTML"ified e-mails, chances have become pretty
> high your mail won't get through.  And, BTW, only some of them will
> occasionally be manually approved by me.
> 
> Martin

And thanks for the continual (and usually not noticed) housekeeping
of the list.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Nov-05                                       Time: 20:25:39
------------------------------ XFMail ------------------------------



From maechler at stat.math.ethz.ch  Sat Nov  5 22:02:27 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 5 Nov 2005 22:02:27 +0100
Subject: [R] problem in waveslim *package* ?
In-Reply-To: <1131115045.4570.7.camel@localhost.localdomain>
References: <1131115045.4570.7.camel@localhost.localdomain>
Message-ID: <17261.7651.107821.767086@stat.math.ethz.ch>

>>>>> "tom" == tom wright <tom at maladmin.com>
>>>>>     on Fri, 04 Nov 2005 09:37:24 -0500 writes:

    tom> This code consistenly segfaults for me. Can someone
    tom> please take a look and tell me if the problem is due to
    tom> something I am doing or is there a problems with the
    tom> dwt (idwt) functions in the waveslim library.  

it's the waveslim *package* (!!)  Try
	install.packages("fortunes")
	fortune(58)
in order to understand the exclamation marks ;-) 
{ I still hope for the next version
  of R to have a use() function for attaching packages so you'll
  have no excuse to call them "li....s" (I'm not going to spell out the abomination :-)
}

Back to the track: I can confirm the seg.fault;  
The reasons are simple:

1) Your code is slightly incorrect:  setZeros()  doesn't do what you
   thought it would do.  Look at

    > str(nd <- setZeros(data.dwt, 1))
    List of 9
     $ d1: num 0
     $ d2: num [1:256]  24.92 -32.17  -2.32   1.21   1.70 ...
     $ d3: num [1:128]   5.21 -41.61 -19.99   6.93  -6.23 ...
     $ d4: num [1:64]  -5.47 -28.69  49.38  -9.44 -17.04 ...
     $ d5: num [1:32]  0.723 -5.647  5.397  3.534 14.719 ...
     $ d6: num [1:16]  28.40   1.82  14.12 -45.44  18.70 ...
     $ d7: num [1:8]  92.8 183.5 -19.4  36.0 -18.2 ...
     $ d8: num [1:4] -1382  1895  -187   260
     $ s8: num [1:4] 19164 16690 15083 14648
     - attr(*, "class")= chr "dwt"
     - attr(*, "wavelet")= chr "la8"
     - attr(*, "boundary")= chr "periodic"

 i.e.,  $d1 is of length 1 instead of 512 .
 The  mistake was to assume that  data.dwt["d1"] would be a
 vector; however it is still a list but with only one component;
 instead  [["d1"]] (or " $d1 " ) really gives the list  *component*.


This fixes your problem {but read on!} :

setZeros <- function(data, factors = list()) {
    for(factor in factors) {
        sFac <- paste('d',factor,sep = '')
        ## wrong: data[sFac]   <- rep(0,length(data[sFac]))
        ## good:  data[[sFac]] <- rep(0,length(data[[sFac]]))
        ## more elegant(?); note the '[]' !
        data[[sFac]][] <- 0
    }
    return(data)
}


2) The idwt() function is not programmed robustly enough:
   It ensures that its main argument is of S3 class "dwt" -- which
   the above is; but with the S3 class system, you have no
   guarantee; there's no  validObject() function like with the
   nice formal classes in S4.   There, in S4, this problem could easily
   be prevented, but not here:  After checking for the "dwt"
   class, idwt() just assumes that it has a proper object and
   quickly passes stuff to .C(.....) where it bombs because 'd1'
   is not of length 512 but of length 1.

So another "moral of the story" could be that  S4 classes are
really something to consider, maybe inspite of what others told
you...

Martin

    tom> library(waveslim)

    tom> data<-c(936.944,936.944,936.944,936.944,.............................)

 [ of length 1024 -- doesn't really matter ]

    tom> setZeros<-function(data,factors=list()){
    tom>  for(factor in factors){
    tom>    sFac<-paste('d',factor,sep='')
    tom>    data[sFac]<-rep(0,length(data[sFac]))
    tom>   }
    tom>  return(data)
    tom> }

    tom> data.dwt<-dwt(data[[2]],n.levels=8)

should probably have been

         data.dwt <- dwt(data, n.levels=8)

    tom> opar<-par(mfrow=c(4,2),mar=c(2,2,2,2))
    tom> mlist<-c(1:8)
    tom> for(iFac in 1:8){
    tom> #flist<-mlist[mlist!=iFac]
    tom> ndata<-setZeros(data.dwt,iFac)
    tom> plot(idwt(ndata),type='l')
    tom> }



From rolf at math.unb.ca  Sat Nov  5 22:35:59 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 5 Nov 2005 17:35:59 -0400 (AST)
Subject: [R] Problem with installing home-made package under Windoze.
Message-ID: <200511052135.jA5LZxcE021426@erdos.math.unb.ca>

I am using/trying to use R 2.2.0.

I have created a package of ``supplementary'' time series functions
for use by my students in a time series course.  The package involves
only raw R code; no dynamic loading to complicate matters.  I need to
install this package in a location on a local area network where the
students can access it under Windoze.

In the past I learned that I could do this by:

	o installing the package under Linux
	o changing directories to the local library
	o executing

		zip -r9l ts.sup.zip ts.sup

	o uploading the resulting ts.sup.zip file to a convenient
	  Windoze-running machine

	o excuting

	  > install.packages("ts.sup.zip",repos=NULL,lib="L:/statdata/Rlib")

	the value of the ``lib'' argument being a location in the LAN
	where I have write permission and which is accessible by the
	students.

In the past this worked.  Now when I try this I get the error message

	``Error in .readRDS(pfile) : cannot read workspace version
	  167772160 written by R 512.2.2; need R 0.1.4 or newer''

I experimented with other small locally built packages and got the
same phenomenon.  Then I tried installing a package from CRAN
choosing to install ``aaMI'' just because it was first on the
list.  Got the same error message.

I tried firing up R version 2.1.1, which is still around.  Strangely,
this allowed me to install ``aaMI'' from CRAN, but threw up the same
error message when I tried to install from local zip files.

This is very frustrating --- I'm trying to install an updated package
with some bugs removed and new facilities added.  Can anyone suggest
what might be causing the problem and how I might fix it?

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From MSchwartz at mn.rr.com  Sat Nov  5 23:37:43 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 05 Nov 2005 16:37:43 -0600
Subject: [R] Problem with installing home-made package under Windoze.
In-Reply-To: <200511052135.jA5LZxcE021426@erdos.math.unb.ca>
References: <200511052135.jA5LZxcE021426@erdos.math.unb.ca>
Message-ID: <1131230263.4173.123.camel@localhost.localdomain>

On Sat, 2005-11-05 at 17:35 -0400, Rolf Turner wrote:
> I am using/trying to use R 2.2.0.
> 
> I have created a package of ``supplementary'' time series functions
> for use by my students in a time series course.  The package involves
> only raw R code; no dynamic loading to complicate matters.  I need to
> install this package in a location on a local area network where the
> students can access it under Windoze.
> 
> In the past I learned that I could do this by:
> 
> 	o installing the package under Linux
> 	o changing directories to the local library
> 	o executing
> 
> 		zip -r9l ts.sup.zip ts.sup
> 
> 	o uploading the resulting ts.sup.zip file to a convenient
> 	  Windoze-running machine
> 
> 	o excuting
> 
> 	  > install.packages("ts.sup.zip",repos=NULL,lib="L:/statdata/Rlib")
> 
> 	the value of the ``lib'' argument being a location in the LAN
> 	where I have write permission and which is accessible by the
> 	students.
> 
> In the past this worked.  Now when I try this I get the error message
> 
> 	``Error in .readRDS(pfile) : cannot read workspace version
> 	  167772160 written by R 512.2.2; need R 0.1.4 or newer''
> 
> I experimented with other small locally built packages and got the
> same phenomenon.  Then I tried installing a package from CRAN
> choosing to install ``aaMI'' just because it was first on the
> list.  Got the same error message.
> 
> I tried firing up R version 2.1.1, which is still around.  Strangely,
> this allowed me to install ``aaMI'' from CRAN, but threw up the same
> error message when I tried to install from local zip files.
> 
> This is very frustrating --- I'm trying to install an updated package
> with some bugs removed and new facilities added.  Can anyone suggest
> what might be causing the problem and how I might fix it?

Rolf,

What happens if you start up R on Windows from the command line using
 
  Rgui --vanilla

?

Any chance that you have a corrupted workspace getting restored on
startup that is subtle enough to not be immediately evident, but causes
this problem in later steps?

You may need to remove any .RData files that are getting loaded on
startup and recreate your package(s) in a clean R session before further
attempts at installation.

HTH,

Marc Schwartz



From spencer.graves at pdf.com  Sun Nov  6 00:38:22 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 05 Nov 2005 15:38:22 -0800
Subject: [R] Confidence Intervals for Mixed Effects
In-Reply-To: <OFFFF32B17.CCCC7801-ON882570A5.0072FEA4-882570A5.0073B740@bayer.com>
References: <OFFFF32B17.CCCC7801-ON882570A5.0072FEA4-882570A5.0073B740@bayer.com>
Message-ID: <436D426E.3050503@pdf.com>

	  Are you familiar with "intervals" and "lme" in the nlme package? 
These are documented in Pinheiro and Bates (2000) Mixed-Effects Models 
in S and S-Plus (Springer).  I'm not familiar with the algorithm, but if 
it's different from Satterthwaite's method, I suspect that Prof. Bates 
had a good reason for choosing something different.  Since R is open 
source, you could read the code and modify it to use Sattherthwaite and 
compare the two side by side.  The nlme package includes a 
"simulate.lme" function, which you could use to compare the different 
methods.

	  hope this helps.
	  spencer graves

Michel Friesenhahn wrote:
> I'm fairly new to R and am wondering if anybody knows of R code to 
> calculate confidence intervals for parameters (fixed effects and variance 
> components) from mixed effects models based on Sattherthwaite's method? 
> I'm also interested in Satterthwaite-based confidence intervals for linear 
> combinations (mostly sums) of various variance components.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Nov  6 01:04:17 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 05 Nov 2005 16:04:17 -0800
Subject: [R] aov() and lme()
In-Reply-To: <4360B6B3.30302@tuebingen.mpg.de>
References: <4360B6B3.30302@tuebingen.mpg.de>
Message-ID: <436D4881.6070601@pdf.com>

	  Have you solved this problem yet?  If no, and if you would still like 
a solution, I suggest you try to develop a much simpler example that 
still produces results like this that you don't understand.  Either use 
data that are already in a standard R library or are a Monte Carlo that 
a reader can generate with a very few lines of code and can run and get 
the same answers as you get (using set.seed).  The send that to r-help 
(after reading the posting guide 
"www.R-project.org/posting-guide.html").  Doing that should increase the 
chances that you can solve the problem yourself and, failing that, will 
get quicker and more useful replies from this list.

	  Viel Glueck
	  spencer graves

Jan Wiener wrote:

> Sorry for reposting, but even after extensive search I still did not 
> find any answers.
> 
> using: 
> summary(aov(pointErrorAbs~noOfSegments*turnAngle+Error(subj/(noOfSegments+turnAngle)), 
> data=anovaAllData ))
> 
> with subj being a random factor and noOfSegments and turnAngle being 
> fixed factors, I get the following results:
> 
> ----------------------------------------------
> Error: subj
>            Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 17 246606   14506
> 
> Error: subj:noOfSegments
>               Df  Sum Sq Mean Sq F value   Pr(>F)
> noOfSegments  3  7806.6  2602.2  5.3257 0.002864 **
> Residuals    51 24919.4   488.6
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Error: subj:turnAngle
>            Df Sum Sq Mean Sq F value  Pr(>F)
> turnAngle  5  14660    2932  3.1707 0.01131 *
> Residuals 85  78600     925
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Error: Within
>                          Df Sum Sq Mean Sq F value    Pr(>F)
> noOfSegments:turnAngle  15  19637    1309  2.9135 0.0001711 ***
> Residuals              687 308687     449
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> ------------------------------------------------------------------
> 
> all is fine, and I get  exactly the same results as with unix anova.
> 
> 
> No I trying to fit the same data with lme and using the following call:
> 
> anova(lme(fixed=pointErrorAbs~noOfSegments*turnAngle, random=~1|subj, 
> data=anovaAllData))
> 
> Unfortunately the results are 'really' different from the aov() 
> procedure (I guess I have the call wrong):
> 
> ----------------------------------------------------
> (Intercept)                1   823 42.10888  <.0001
> noOfSegments               3   823  5.19549  0.0015
> turnAngle                  5   823  5.85379  <.0001
> noOfSegments:turnAngle    15   823  2.61373  0.0007
> ----------------------------------------------------
> 
> I, however, need a comparable method for lme(), because in a different 
> data set I have single empty cells and can therefore not use aov().
> 
> does anyone know how to fit with lme() to obtain the same results (for 
> this balanced data set) as with aov().
> 
> Thanks in advance,
> Jan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Nov  6 01:25:45 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 05 Nov 2005 16:25:45 -0800
Subject: [R] syntax of nlme with nesting
In-Reply-To: <002401c5db39$064919c0$9a1ad284@BIO041>
References: <002401c5db39$064919c0$9a1ad284@BIO041>
Message-ID: <436D4D89.1000807@pdf.com>

	  If you have not yet solved this problem and are still interested in a 
solution, I have three suggestions:

	  1.  Try to simplify the problem as much as possible.  If this does 
not lead you to a solution to your problem, when you get an example that 
is completely self-contained in a few lines of code, please submit that 
to this group.  I can't comment on your problem in part because I have 
neither your function NRhyperbola nor lit.data.

	  2.  Copy the "nlme" function into a script file and walk through it 
line by line, either manually or using "debug".  The "nlme" function is 
a bit cryptic to people unfamiliar with R, consisting of 
'UseMethod("nlme")'.  To move beyond this, I used 'methods("nlme")', 
which identified "nlme.formula" and "nlme.nlsList".  Then entering 
"nlme.formula" produced the desired listing.

	  3.  PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html".  This might help in other ways.

	  Good Luck,
	  spencer graves

Bill Shipley wrote:

> This may appear too elementary to some on this list, but not to me.  My
> apologies if this is the case.  I have mastered the lme function but the
> nlme function has me stumped.
> 
>  
> 
> I am attempting to fit a nonlinear mixed model with 4 levels of nesting.
> I am getting a cryptic error message and do not know what is wrong with
> the syntax of the call.  This is the call:
> 
>  
> 
> 
>>nlme(Photosynthese~NRhyperbola(Irr,theta,Am,alpha,Rd),
> 
> 
> + fixed=theta+Am+alpha+Rd~1,
> 
> + random=theta~1|Reference/Espece/Plante/Groupe,
> 
> + data=lit.data)
> 
>  
> 
> NRhyperbola is a self-starting function with one variable (Irr) and four
> parameters (theta,Am,alpha,Rd).  The data set (lit.data) contains
> Photosynthese (dependent variable) and Irr, as well as the grouping
> structure, which is Reference, Espece nested in Reference, Plante nested
> in Espece and Groupe nested in Plante.  I want to allow only the
> parameter theta to vary randomly.  I get the following error message:
> "Error: subscript out of bounds".
> 
>  
> 
> What does this mean?  There are some "Plante" for which there is only
> one "Groupe" , some "Espece" for which there is only one "Plante" etc.
> Is this the source of the error?  If so, how can one solve this?
> 
>  
> 
> Bill Shipley
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From syy2004 at gmail.com  Sun Nov  6 02:53:24 2005
From: syy2004 at gmail.com (Yuying Shi)
Date: Sat, 5 Nov 2005 20:53:24 -0500
Subject: [R] R help
Message-ID: <91d269c60511051753v1390bc7dqe40d349fecdfcb07@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051105/52fd7c0f/attachment.pl

From xiaofan.mlist at gmail.com  Sun Nov  6 04:28:19 2005
From: xiaofan.mlist at gmail.com (Xiaofan Li)
Date: Sun, 6 Nov 2005 03:28:19 -0000
Subject: [R] How can I assign an argument to transfer whether by ref or by
	value?
Message-ID: <436d7854.2fa050a0.24d8.77a6@mx.gmail.com>

Hello guys,

I am wondering the default way of transferring arguments in R. Is it by
value or by ref in default case, or could that be changed explicitly?

Cheers,
Xiaofan

---
Xiaofan Li
Department of Applied Mathematics and Theoretical Physics
University of Cambridge



From xiaofan.mlist at gmail.com  Sun Nov  6 04:37:37 2005
From: xiaofan.mlist at gmail.com (Xiaofan Li)
Date: Sun, 6 Nov 2005 03:37:37 -0000
Subject: [R] dataTable manipulation in R
Message-ID: <436d7a83.3c0f1d71.6914.790f@mx.gmail.com>


Dear colleague,

Is it able in R to manipulate data tables as in SQL-based databases?
Is there any good method in R to locate a special subset of data rows, which
satisfy a certain expression of restrictions (e.g. dataTable$peakValue <
3.0)? Then if located, is it able for manipulations, for example, to delete
them from the dataTable?

Cheers,
Xiaofan

---
Xiaofan Li
Department of Applied Mathematics and Theoretical Physics
University of Cambridge



From murdoch at stats.uwo.ca  Sun Nov  6 05:50:00 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 05 Nov 2005 23:50:00 -0500
Subject: [R] How can I assign an argument to transfer whether by ref or
 by	value?
In-Reply-To: <436d7854.2fa050a0.24d8.77a6@mx.gmail.com>
References: <436d7854.2fa050a0.24d8.77a6@mx.gmail.com>
Message-ID: <436D8B78.9090203@stats.uwo.ca>

Xiaofan Li wrote:
> Hello guys,
> 
> I am wondering the default way of transferring arguments in R. Is it by
> value or by ref in default case, or could that be changed explicitly?

R passes by value.  It's worth reading the language definition manual to 
find out about the subtleties (e.g. lazy evaluation).

Duncan Murdohc
> 
> Cheers,
> Xiaofan
> 
> ---
> Xiaofan Li
> Department of Applied Mathematics and Theoretical Physics
> University of Cambridge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From patrick.giraudoux at univ-fcomte.fr  Sun Nov  6 09:03:38 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sun, 06 Nov 2005 09:03:38 +0100
Subject: [R] nlme, predict.nlme, levels not allowed
In-Reply-To: <436CE5C9.9050602@univ-fcomte.fr>
References: <436CE5C9.9050602@univ-fcomte.fr>
Message-ID: <436DB8DA.1020602@univ-fcomte.fr>

Going through the R-Dev list, I have found this (from Pedro Afalo), 
dated 8 April 2004:

> Dear Richard,
>
> The problem that you report is documented (but no solution given) in the
> file ch08.R in the scripts directory of nlme package.
>
> I have found the following workaround just by chance, but it may
> give a clue of what is the problem to those who know how to program
> in R.
>
> The solution is to add an explicit call to factor in the nlme call.
> In the case of the error reported by Richard the following call to nlme
> in the ch08.R file can be used:
>
> fm4CO2.nlme <- update( fm3CO2.nlme,
>    fixed = list(Asym + lrc ~ factor(Type) * factor(Treatment), c0 ~ 1),
>    start = c(fm3CO2.fix[1:5], 0, 0, 0, fm3CO2.fix[6]) )
>
> instead of:
>
> fm4CO2.nlme <- update( fm3CO2.nlme,
>    fixed = list(Asym + lrc ~ Type * Treatment, c0 ~ 1),
>    start = c(fm3CO2.fix[1:5], 0, 0, 0, fm3CO2.fix[6]) )
>
> I hope this helps,
>
> Pedro. 


I have tried the workaround for my own case and it works...

Any news since then about fixing the problem?

Patrick


Patrick Giraudoux a ??crit :

> Dear listers,
>
> I am trying to fit a nlme model with "age" and "pds" as reals, and 
> "zone" a factor  with two levels "Annaba" and "Boumalek" . The "best" 
> model found is the following:
>
> > modm3
> Nonlinear mixed-effects model fit by maximum likelihood
>  Model: pds ~ Asym/(1 + exp((xmid - age)/scal))
>  Data: croispulm
>  Log-likelihood: -91.86667
>  Fixed: list(Asym ~ zone, xmid ~ zone, scal ~ 1)
> Asym.(Intercept) Asym.zoneBoumalek  xmid.(Intercept) 
> xmid.zoneBoumalek              scal
>       9.99551079        0.39423966        4.97981027        
> 0.06969807        2.23116661
>
> Random effects:
> Formula: list(Asym ~ 1, xmid ~ 1)
> Level: nichoir
> Structure: General positive-definite, Log-Cholesky parametrization
>                 StdDev       Corr Asym.(Intercept) 1.796565e-06 As.(I)
> xmid.(Intercept) 1.219400e-04 0    Residual         6.163282e-01     
> Correlation Structure: Continuous AR(1)
> Formula: ~age | nichoir
> Parameter estimate(s):
>      Phi
> 0.3395242
> Number of Observations: 102
> Number of Groups: 17
>
>
> Everything normal so far.
>
> Things come to be strange when I try to compute predicted values:
>
> > pred<-predict(modm3,newdata=mydata,type="response")
> Error in predict.nlme(modm3, newdata = mydata, type = "response") :
>        Levels Annaba,Boumalek not allowed for zone
>
> I have checked and re-checked that zone in the newdata is well a 
> factor with the "good" levels, and I can hardly understand why these 
> two levels used when fitting the model are now rejected when used for 
> computing predicted values.
>
> Any hint welcome,
>
> Best regards,
>
> Patrick
>
>
>
>



From leaflovesun at yahoo.ca  Sun Nov  6 09:10:50 2005
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Sun, 6 Nov 2005 01:10:50 -0700
Subject: [R] OLS variables
Message-ID: <200511060811.jA68BYCl028466@hypatia.math.ethz.ch>

Dear all,

Is there any simple way in R that can I put the all the interactions of the variables in the OLS model?

e.g.

I have a bunch of variables, x1,x2,.... x20... I expect then to have interaction (e.g. x1*x2, x3*x4*x5... ) with some combinations(2 way or higher dimensions). 

Is there any way that I can write the model simpler?

Thanks!

Leaf



From ripley at stats.ox.ac.uk  Sun Nov  6 10:08:51 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 6 Nov 2005 09:08:51 +0000 (GMT)
Subject: [R] Problem with installing home-made package under Windoze.
In-Reply-To: <1131230263.4173.123.camel@localhost.localdomain>
References: <200511052135.jA5LZxcE021426@erdos.math.unb.ca>
	<1131230263.4173.123.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0511060845370.22830@gannet.stats>

I tried this with aaMI and had no problems, from either i386 or x86_64 
Linux.  (I think we have only ever said that i386 Linux will work and I 
suspect that a big-endian host would not.)

It look as if the transferred files have got corrupted. Marc's suggestion 
is unlikely, as these errors are coming from C code and it does look as if 
.readRDS is calling the right C code.

If you create a MD5 checksum file in Linux, it can be checked on Windows. 
See the help on checkMD5sums in package tools.  I believe that if you put 
a MD5 file in the package on Linux, it will automatically be checked when 
installing on Windows: it certainly happens for installs from a 
repository.

On Sat, 5 Nov 2005, Marc Schwartz wrote:

> On Sat, 2005-11-05 at 17:35 -0400, Rolf Turner wrote:
>> I am using/trying to use R 2.2.0.
>>
>> I have created a package of ``supplementary'' time series functions
>> for use by my students in a time series course.  The package involves
>> only raw R code; no dynamic loading to complicate matters.  I need to
>> install this package in a location on a local area network where the
>> students can access it under Windoze.
>>
>> In the past I learned that I could do this by:
>>
>> 	o installing the package under Linux
>> 	o changing directories to the local library
>> 	o executing
>>
>> 		zip -r9l ts.sup.zip ts.sup
>>
>> 	o uploading the resulting ts.sup.zip file to a convenient
>> 	  Windoze-running machine
>>
>> 	o excuting
>>
>> 	 > install.packages("ts.sup.zip",repos=NULL,lib="L:/statdata/Rlib")
>>
>> 	the value of the ``lib'' argument being a location in the LAN
>> 	where I have write permission and which is accessible by the
>> 	students.
>>
>> In the past this worked.  Now when I try this I get the error message
>>
>> 	``Error in .readRDS(pfile) : cannot read workspace version
>> 	  167772160 written by R 512.2.2; need R 0.1.4 or newer''
>>
>> I experimented with other small locally built packages and got the
>> same phenomenon.  Then I tried installing a package from CRAN
>> choosing to install ``aaMI'' just because it was first on the
>> list.  Got the same error message.
>>
>> I tried firing up R version 2.1.1, which is still around.  Strangely,
>> this allowed me to install ``aaMI'' from CRAN, but threw up the same
>> error message when I tried to install from local zip files.
>>
>> This is very frustrating --- I'm trying to install an updated package
>> with some bugs removed and new facilities added.  Can anyone suggest
>> what might be causing the problem and how I might fix it?
>
> Rolf,
>
> What happens if you start up R on Windows from the command line using
>
>  Rgui --vanilla
>
> ?
>
> Any chance that you have a corrupted workspace getting restored on
> startup that is subtle enough to not be immediately evident, but causes
> this problem in later steps?
>
> You may need to remove any .RData files that are getting loaded on
> startup and recreate your package(s) in a clean R session before further
> attempts at installation.
>
> HTH,
>
> Marc Schwartz

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bbdoc at free.Fr  Sun Nov  6 10:21:05 2005
From: bbdoc at free.Fr (bertrand)
Date: Sun, 6 Nov 2005 10:21:05 +0100
Subject: [R] cox models
Message-ID: <0c0b7ad44e8cf28bb7b06fc163d9b181@free.Fr>

Hello,
i'm a french student of medical oncology and i'm working on breast 
cancer. I have a variable with the histologic type of tumor wich is 
between 1 and 5. I use as.factor function to make some variable with 
level between 1 and 5. When i put it in the cox model i have only the 
level between 2 and 5. The level 1 doesn't appear. I think i have to 
change the number of level but i don't really know. Please can you help 
me?


Class
Levels: 1 2 3 4 5
coxph(formula = Surv(delai.etat, etat) ~ class, data = igr1)


          coef exp(coef) se(coef)     z       p
class2 -0.093     0.911    0.245 -0.38 7.0e-01
class3 -0.749     0.473    0.286 -2.62 8.9e-03
class4 -0.600     0.549    0.343 -1.75 8.0e-02
class5 -0.874     0.417    0.197 -4.44 8.9e-06

Likelihood ratio test=24.9  on 4 df, p=5.28e-05  n=740 (1 observations 
deleted due to missing)



Thanks for your help


Bertrand



From dieter.menne at menne-biomed.de  Sun Nov  6 13:50:01 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 6 Nov 2005 13:50:01 +0100
Subject: [R] R (2.2.0), R-DCOM and Delphi
Message-ID: <LPEJLJACLINDNMBMFAFIOEEICBAA.dieter.menne@menne-biomed.de>

In response to a few private e-mails, here a summary of using Delphi, R-DCOM
and R 2.2.0

1) As Earl Glynn noted
( http://finzi.psych.upenn.edu/R/Rhelp02a/archive/50705.html ),

there were a few paths specific to my installation in

http://www.menne-biomed.de/download/RDComDelphi.zip

leading to path errors on compilation. Hopefully, this has been corrected in
the new zip file.

2) Always check if the test samples (e.g. simple.exe) coming with R(D)COM
work before trying with Delphi. Parameters passing fails with R 2.2.0 and
old R(D)COM, but graphs still work.

For R 2.2.0, please read

http://mailman.csd.univie.ac.at/pipermail/rcom-l/2005-October/000764.html

and download the new versions.

After installing the new versions, my Delphi talks again with R.

Dieter



From jfox at mcmaster.ca  Sun Nov  6 14:27:27 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 6 Nov 2005 08:27:27 -0500
Subject: [R] OLS variables
In-Reply-To: <200511060811.jA68BYCl028466@hypatia.math.ethz.ch>
Message-ID: <20051106132725.WVPZ2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Leaf,

I assume that you're using lm() to fit the model, and that you don't really
want *all* of the interactions among 20 predictors: You'd need quite a lot
of data to fit a model with 2^20 terms in it, and might have trouble
interpreting the results. 

If you know which interactions you're looking for, then why not specify them
directly, as in lm(y ~  x1*x2 + x3*x4*x5 + etc.)? On the other hand, it you
want to include all interactions, say, up to three-way, and you've put the
variables in a data frame, then lm(y ~ .^3, data=DataFrame) will do it.
There are many terms in this model, however, if not quite 2^20.

The introductory manual that comes with R has information on model formulas
in Section 11.

I hope this helps,
 John 

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leaf Sun
> Sent: Sunday, November 06, 2005 3:11 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] OLS variables
> 
> Dear all,
> 
> Is there any simple way in R that can I put the all the 
> interactions of the variables in the OLS model?
> 
> e.g.
> 
> I have a bunch of variables, x1,x2,.... x20... I expect then 
> to have interaction (e.g. x1*x2, x3*x4*x5... ) with some 
> combinations(2 way or higher dimensions). 
> 
> Is there any way that I can write the model simpler?
> 
> Thanks!
> 
> Leaf
> 
>



From jfox at mcmaster.ca  Sun Nov  6 14:37:03 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 6 Nov 2005 08:37:03 -0500
Subject: [R] cox models
In-Reply-To: <0c0b7ad44e8cf28bb7b06fc163d9b181@free.Fr>
Message-ID: <20051106133701.TRWG26102.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Bertrand,

There are only 4 degrees of freedom for a 5-level factor. By default, R
creates contrasts for an unordered factor in a model formula using the
function contr.treatment(), which, also by default, treats the first level
of the factor (class, in your case) as the baseline or reference level. To
see the contrasts, enter the command contrasts(igr1$class). 

A bit more information on contrasts is in Section 11.1.1 of the introductory
manual that comes with R.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bertrand
> Sent: Sunday, November 06, 2005 4:21 AM
> To: r-help-request at stat.math.ethz.ch
> Cc: r-help at stat.math.ethz.ch
> Subject: [R] cox models
> 
> Hello,
> i'm a french student of medical oncology and i'm working on 
> breast cancer. I have a variable with the histologic type of 
> tumor wich is between 1 and 5. I use as.factor function to 
> make some variable with level between 1 and 5. When i put it 
> in the cox model i have only the level between 2 and 5. The 
> level 1 doesn't appear. I think i have to change the number 
> of level but i don't really know. Please can you help me?
> 
> 
> Class
> Levels: 1 2 3 4 5
> coxph(formula = Surv(delai.etat, etat) ~ class, data = igr1)
> 
> 
>           coef exp(coef) se(coef)     z       p
> class2 -0.093     0.911    0.245 -0.38 7.0e-01
> class3 -0.749     0.473    0.286 -2.62 8.9e-03
> class4 -0.600     0.549    0.343 -1.75 8.0e-02
> class5 -0.874     0.417    0.197 -4.44 8.9e-06
> 
> Likelihood ratio test=24.9  on 4 df, p=5.28e-05  n=740 (1 
> observations deleted due to missing)
> 
> 
> 
> Thanks for your help
> 
> 
> Bertrand
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From xcong at stat.rice.edu  Sun Nov  6 17:13:27 2005
From: xcong at stat.rice.edu (Xiuyu Cong)
Date: Sun, 6 Nov 2005 10:13:27 -0600 (CST)
Subject: [R] cox models
In-Reply-To: <0c0b7ad44e8cf28bb7b06fc163d9b181@free.Fr>
References: <0c0b7ad44e8cf28bb7b06fc163d9b181@free.Fr>
Message-ID: <Pine.GSO.4.58.0511061011450.16101@thor.stat.rice.edu>

Level 1 is treated as the "baseline". The coefficient for level 2 for
example is for the hazard ratio between level 2 and level 1. Same thing
for levels 3,4,5 in the output.

HTH,

On Sun, 6 Nov 2005, bertrand wrote:

> Hello,
> i'm a french student of medical oncology and i'm working on breast
> cancer. I have a variable with the histologic type of tumor wich is
> between 1 and 5. I use as.factor function to make some variable with
> level between 1 and 5. When i put it in the cox model i have only the
> level between 2 and 5. The level 1 doesn't appear. I think i have to
> change the number of level but i don't really know. Please can you help
> me?
>
>
> Class
> Levels: 1 2 3 4 5
> coxph(formula = Surv(delai.etat, etat) ~ class, data = igr1)
>
>
>           coef exp(coef) se(coef)     z       p
> class2 -0.093     0.911    0.245 -0.38 7.0e-01
> class3 -0.749     0.473    0.286 -2.62 8.9e-03
> class4 -0.600     0.549    0.343 -1.75 8.0e-02
> class5 -0.874     0.417    0.197 -4.44 8.9e-06
>
> Likelihood ratio test=24.9  on 4 df, p=5.28e-05  n=740 (1 observations
> deleted due to missing)
>
>
>
> Thanks for your help
>
>
> Bertrand
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> !DSPAM:436dcb0b22713230529779!
>



From ramasamy at cancer.org.uk  Sun Nov  6 18:44:15 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 06 Nov 2005 17:44:15 +0000
Subject: [R] dataTable manipulation in R
In-Reply-To: <436d7a83.3c0f1d71.6914.790f@mx.gmail.com>
References: <436d7a83.3c0f1d71.6914.790f@mx.gmail.com>
Message-ID: <1131299055.7881.8.camel@localhost.localdomain>

Check out the R Data Import/Export manual, especially
http://cran.r-project.org/doc/manuals/R-data.html#DBI-_002f-RMySQL
and the RMySQL package.


On Sun, 2005-11-06 at 03:37 +0000, Xiaofan Li wrote: 
> Dear colleague,
> 
> Is it able in R to manipulate data tables as in SQL-based databases?
> Is there any good method in R to locate a special subset of data rows, which
> satisfy a certain expression of restrictions (e.g. dataTable$peakValue <
> 3.0)? Then if located, is it able for manipulations, for example, to delete
> them from the dataTable?
> 
> Cheers,
> Xiaofan
> 
> ---
> Xiaofan Li
> Department of Applied Mathematics and Theoretical Physics
> University of Cambridge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Sun Nov  6 18:48:27 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 06 Nov 2005 17:48:27 +0000
Subject: [R] How can I assign an argument to transfer whether by ref
	or	by value?
In-Reply-To: <436d7854.2fa050a0.24d8.77a6@mx.gmail.com>
References: <436d7854.2fa050a0.24d8.77a6@mx.gmail.com>
Message-ID: <1131299307.7881.13.camel@localhost.localdomain>

I do not understand what your question is. Can you clarify with an
example or analogies to other programming language.

 my.fun <- function(x, y=1){ x^y }

 my.fun(5)        # returns 5 
 my.fun(5, 2)     # returns 25
 my.fun(y=2, x=5) # returns 25

Regards, Adai


On Sun, 2005-11-06 at 03:28 +0000, Xiaofan Li wrote:
> Hello guys,
> 
> I am wondering the default way of transferring arguments in R. Is it by
> value or by ref in default case, or could that be changed explicitly?
> 
> Cheers,
> Xiaofan
> 
> ---
> Xiaofan Li
> Department of Applied Mathematics and Theoretical Physics
> University of Cambridge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Wolfson College Annexe                  Tel : 01865 284 408
Linton Road, Oxford OX2 6UD             Fax : 01865 284 424



From MSchwartz at mn.rr.com  Sun Nov  6 18:59:04 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 06 Nov 2005 11:59:04 -0600
Subject: [R] dataTable manipulation in R
In-Reply-To: <1131299055.7881.8.camel@localhost.localdomain>
References: <436d7a83.3c0f1d71.6914.790f@mx.gmail.com>
	<1131299055.7881.8.camel@localhost.localdomain>
Message-ID: <1131299944.4173.176.camel@localhost.localdomain>

Adai,

I suspect that Xiaofan was looking for the use of a function like
subset(), which of course does enable one to specify filtering criteria
and return a subset of rows in a data frame that satisfy the criteria.

I am presuming that the SQL reference was more towards using SQL
Select/Where type statements within a database as an example, as opposed
to wanting to query a SQL database from within R.

Xiaofan, see ?subset for more information. The function will also let
you specify a subset of columns using the 'select' argument.

In addition to subset(), there is also the use of [.data.frame, which
will enable you to specify criteria using the typical "[row, column]"
extraction approach. See ?"[.data.frame" for more information.

Which approach is taken is to some extent personal preference. I find it
easier to use subset() when the criteria are more complex boolean
expressions and/or where I only want a subset of the columns.

HTH,

Marc Schwartz


On Sun, 2005-11-06 at 17:44 +0000, Adaikalavan Ramasamy wrote:
> Check out the R Data Import/Export manual, especially
> http://cran.r-project.org/doc/manuals/R-data.html#DBI-_002f-RMySQL
> and the RMySQL package.
> 
> 
> On Sun, 2005-11-06 at 03:37 +0000, Xiaofan Li wrote: 
> > Dear colleague,
> > 
> > Is it able in R to manipulate data tables as in SQL-based databases?
> > Is there any good method in R to locate a special subset of data rows, which
> > satisfy a certain expression of restrictions (e.g. dataTable$peakValue <
> > 3.0)? Then if located, is it able for manipulations, for example, to delete
> > them from the dataTable?
> > 
> > Cheers,
> > Xiaofan
> >



From ramasamy at cancer.org.uk  Sun Nov  6 19:07:31 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 06 Nov 2005 18:07:31 +0000
Subject: [R] OLS variables
In-Reply-To: <20051106132725.WVPZ2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>
References: <20051106132725.WVPZ2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <1131300451.13720.0.camel@localhost.localdomain>

IMHO, the details section of help(formula) provides a nicer help.

Regards, Adai


On Sun, 2005-11-06 at 08:27 -0500, John Fox wrote:
> Dear Leaf,
> 
> I assume that you're using lm() to fit the model, and that you don't really
> want *all* of the interactions among 20 predictors: You'd need quite a lot
> of data to fit a model with 2^20 terms in it, and might have trouble
> interpreting the results. 
> 
> If you know which interactions you're looking for, then why not specify them
> directly, as in lm(y ~  x1*x2 + x3*x4*x5 + etc.)? On the other hand, it you
> want to include all interactions, say, up to three-way, and you've put the
> variables in a data frame, then lm(y ~ .^3, data=DataFrame) will do it.
> There are many terms in this model, however, if not quite 2^20.
> 
> The introductory manual that comes with R has information on model formulas
> in Section 11.
> 
> I hope this helps,
>  John 
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leaf Sun
> > Sent: Sunday, November 06, 2005 3:11 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] OLS variables
> > 
> > Dear all,
> > 
> > Is there any simple way in R that can I put the all the 
> > interactions of the variables in the OLS model?
> > 
> > e.g.
> > 
> > I have a bunch of variables, x1,x2,.... x20... I expect then 
> > to have interaction (e.g. x1*x2, x3*x4*x5... ) with some 
> > combinations(2 way or higher dimensions). 
> > 
> > Is there any way that I can write the model simpler?
> > 
> > Thanks!
> > 
> > Leaf
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Wolfson College Annexe                  Tel : 01865 284 408
Linton Road, Oxford OX2 6UD             Fax : 01865 284 424



From ramasamy at cancer.org.uk  Sun Nov  6 19:15:50 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 06 Nov 2005 18:15:50 +0000
Subject: [R] dataTable manipulation in R
In-Reply-To: <1131299944.4173.176.camel@localhost.localdomain>
References: <436d7a83.3c0f1d71.6914.790f@mx.gmail.com>
	<1131299055.7881.8.camel@localhost.localdomain>
	<1131299944.4173.176.camel@localhost.localdomain>
Message-ID: <1131300950.13720.5.camel@localhost.localdomain>

Marc, thanks for pointing out. I completely misread Xiaofan's post.

In addition to your advice, I like to point out the function which()
which selects only the true values. This is comes in handy when one has
missing values or if the condition generates missing values in the data.

Regards, Adai


On Sun, 2005-11-06 at 11:59 -0600, Marc Schwartz wrote:
> Adai,
> 
> I suspect that Xiaofan was looking for the use of a function like
> subset(), which of course does enable one to specify filtering criteria
> and return a subset of rows in a data frame that satisfy the criteria.
> 
> I am presuming that the SQL reference was more towards using SQL
> Select/Where type statements within a database as an example, as opposed
> to wanting to query a SQL database from within R.
> 
> Xiaofan, see ?subset for more information. The function will also let
> you specify a subset of columns using the 'select' argument.
> 
> In addition to subset(), there is also the use of [.data.frame, which
> will enable you to specify criteria using the typical "[row, column]"
> extraction approach. See ?"[.data.frame" for more information.
> 
> Which approach is taken is to some extent personal preference. I find it
> easier to use subset() when the criteria are more complex boolean
> expressions and/or where I only want a subset of the columns.
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> On Sun, 2005-11-06 at 17:44 +0000, Adaikalavan Ramasamy wrote:
> > Check out the R Data Import/Export manual, especially
> > http://cran.r-project.org/doc/manuals/R-data.html#DBI-_002f-RMySQL
> > and the RMySQL package.
> > 
> > 
> > On Sun, 2005-11-06 at 03:37 +0000, Xiaofan Li wrote: 
> > > Dear colleague,
> > > 
> > > Is it able in R to manipulate data tables as in SQL-based databases?
> > > Is there any good method in R to locate a special subset of data rows, which
> > > satisfy a certain expression of restrictions (e.g. dataTable$peakValue <
> > > 3.0)? Then if located, is it able for manipulations, for example, to delete
> > > them from the dataTable?
> > > 
> > > Cheers,
> > > Xiaofan
> > > 
> 
> 
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Wolfson College Annexe                  Tel : 01865 284 408
Linton Road, Oxford OX2 6UD             Fax : 01865 284 424



From spencer.graves at pdf.com  Sun Nov  6 19:46:43 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Nov 2005 10:46:43 -0800
Subject: [R] question on adding confidence intervals
In-Reply-To: <f7dc8e940510292100w3cbb0ccfuc8ca8f1eb43cc894@mail.gmail.com>
References: <f7dc8e940510292100w3cbb0ccfuc8ca8f1eb43cc894@mail.gmail.com>
Message-ID: <436E4F93.3010600@pdf.com>

	  Have you considered "arima" and "predict.Arima"?

	  spencer graves

Renuka Sane wrote:

> I am trying to do a forecasting exercise for a series, x. My forecast
> model consists of the following
> 
> I first regress log(x) on time and dummy variables for each month.
> lm(log(x) ~ time + monthly dummies)
> I then use predict() to obtain a prediction for the next year.
> 
> I then fit an AR(6)/AR(12) model on the residuals of the regression.
> I use predict() here also to obtain the prediction for the next year
> 
> My final forecast is the sum of the two predictions i.e. predict.lm
> (from the regression, interval="prediction") and p$pred (from the AR
> model). I then take the exp(of the sum) to plot it against my orginal
> time series. (I assume that the Cov(x,y)=0 and therefore take the exp(sum)).
> 
> I now want to draw a confidence interval for my forecast. I realise that
> I will have to add the standard errors from both the regression and the
> AR() model to obtain the final standard error after which I will
> calculate the confidence interval. I realise that I cannot just add the
> two standard errors and take the exp() as in the case of the forecast.
> My question is the following
> 
> Can someone help me to obtain the standard error for the final series?
> How do I incorporate the SEs from the AR model? Or is there some other way I
> can
> calculate the confidence interval?
> 
> I know that if in predict.lm() I use interval="prediction", I get the
> prediction interval for the yhat. However this is for the regression of
> log(x) on time and monthly dummies. I can make an interval using p$pred
> +-1.96*p$se from the AR model. Again this is for log(x). How can I use these
> two to obtain the final interval for x as opposed to log(x)?
> 
> Thanks,
> 
> --
> Renuka Sane
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Sun Nov  6 19:49:08 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 6 Nov 2005 13:49:08 -0500
Subject: [R] How can I assign an argument to transfer whether by ref or
	by value?
In-Reply-To: <1131299307.7881.13.camel@localhost.localdomain>
References: <436d7854.2fa050a0.24d8.77a6@mx.gmail.com>
	<1131299307.7881.13.camel@localhost.localdomain>
Message-ID: <971536df0511061049w7853495fq2a56365c33979be7@mail.gmail.com>

i think this question was already answered but just to elaborate,
pass by value means that a copy of the argument is passed to the
function so if the argument is changed in the function its not changed
in the caller.  Pass by reference means its changed in the caller too.
R passes by value although there are workarounds to pass by
reference.  In the following f uses pass by value and g pass by
reference.

> y <- 1
> f <- function(x) x<-x+1
> f(y)
> y
[1] 1

> g <- function(x) eval.parent(substitute(x <- x+1))
> g(y)
> y
[1] 2


On 11/6/05, Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:
> I do not understand what your question is. Can you clarify with an
> example or analogies to other programming language.
>
>  my.fun <- function(x, y=1){ x^y }
>
>  my.fun(5)        # returns 5
>  my.fun(5, 2)     # returns 25
>  my.fun(y=2, x=5) # returns 25
>
> Regards, Adai
>
>
> On Sun, 2005-11-06 at 03:28 +0000, Xiaofan Li wrote:
> > Hello guys,
> >
> > I am wondering the default way of transferring arguments in R. Is it by
> > value or by ref in default case, or could that be changed explicitly?
> >
> > Cheers,
> > Xiaofan
> >
> > ---
> > Xiaofan Li
> > Department of Applied Mathematics and Theoretical Physics
> > University of Cambridge
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> --
> Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
> Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
> Wolfson College Annexe                  Tel : 01865 284 408
> Linton Road, Oxford OX2 6UD             Fax : 01865 284 424
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From deleeuw at stat.ucla.edu  Sun Nov  6 19:59:57 2005
From: deleeuw at stat.ucla.edu (Jan de Leeuw)
Date: Sun, 6 Nov 2005 10:59:57 -0800
Subject: [R] R for Psychometrics
Message-ID: <B0A814C7-DCE7-4D62-80E5-68D5209EF077@stat.ucla.edu>

Over the last couple of years I have written quite a few
R programs for various "psychometric" techniques, and I am
regularly updating and expanding what is there. I now
have (wholly or partially), or have planned

-- gifi package (update to homals on CRAN). Code for
    multiple correspondence analysis, nonlinear principal
    component analysis, nonlinear multiset canonical
    correlation analysis. Implements everything in Gifi (1990)
    or SPSS Categories, and then some. Status: done.

-- aspect package (optimizing functions of correlation
    matrices over transformations/quantifications of the variables --
    functions implemented are sums of eigenvalues, determinants,  
multiple
    correlations, power sums). Status: done.

-- optim package (miscellaneous optimization routines -- currently has
    pooled-adjacent-violaters for monotone regression and
    the hildreth-desopo coordinate descent method for quadratic
    programming). Status: will grow.

-- ca package (simple correspondence analysis with various
    plotting options). Status: done.

-- smacof package (metric and nonmetric multidimensional
    scaling, individual difference models, rectangular matrices,
    constrained scaling, metric nearness problem, Shepard-Luce
    models). Status: mostly done.

-- logitfold package (logistic likelihood solution to nonmetric
    unfolding for binary and multicategory data, multidimensional
    choice models, logistic principal component analysis and
    canonical analysis, roll-call models, multidimensional Rasch  
models).
    Status: partly done.

-- lssem package (linear structural equation models with latent
    variables, reformulated as matrix decomposition problems, and
    solved by majorizing least squares loss functions). Status:
    planning stage.

-- threeway package (multiway generalizations of principal component
    analysis with various constraints on the decomposition). Status:
    on the horizon.

All of this is currently in straightforward R, without any compiled C  
code,
and without any OOP.

The idea is that eventually these will be nicely organized in
R packages that pass the checks and are internally documented.
But "eventually" can take a pretty long time. Also, I expect that
some of this will wind up on www.jstatsoft.org, but again,
probably not any time soon.

Also, I would be more than willing to embed this stuff in general
multi-person projects such as

	R in Psychometrics
	R in Econometrics
	R in Social Statistics

but that will require someone else running these shows.

For the time being, if you want to receive information about updates  
of this
and related R code and papers in the area of multidimensional
scaling, item response theory, choice models, factor analysis,
and simultaneous equation models, please subscribe to

http://lists.stat.ucla.edu/mailman/listinfo/albertgifi

===
Jan de Leeuw; Distinguished Professor and Chair, UCLA Department of  
Statistics;
Editor: Journal of Multivariate Analysis, Journal of Statistical  
Software
US mail: 8130 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095-1554
phone (310)-825-9550;  fax (310)-206-5658;  email: deleeuw at stat.ucla.edu
.mac: jdeleeuw ++++++  aim: deleeuwjan ++++++ skype: j_deleeuw
homepages: http://gifi.stat.ucla.edu ++++++ http://www.cuddyvalley.org
   
------------------------------------------------------------------------ 
-------------------------
           No matter where you go, there you are. --- Buckaroo Banzai
                    http://gifi.stat.ucla.edu/sounds/nomatter.au



From spencer.graves at pdf.com  Sun Nov  6 20:15:11 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Nov 2005 11:15:11 -0800
Subject: [R] coding nesting in data for nlme example of Wafer data set.
In-Reply-To: <004501c5df2f$ab2ff130$9a1ad284@BIO041>
References: <004501c5df2f$ab2ff130$9a1ad284@BIO041>
Message-ID: <436E563F.5080908@pdf.com>

	  Doug Bates made an important comment that might help you with this. 
I found it via RSiteSearch("nesting in lmer"): 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/47423.html".

	  If you still have questions after this, please submit another post, 
preferably illustrating your question with a self-contained example in a 
few lines of R code that illustrate the problem you are having (as 
suggested in the posting guide "www.R-project.org/posting-guide.html"). 
  Doing so might increase your chances of getting a quick and useful 
reply.

	  spencer graves

Bill Shipley wrote:
> I am trying to understand the proper way to encode the nesting structure
> for data in the context of nlme, in the specific case of individuals
> nested within species for which each individual is unique.  I have
> searched through Pinheiro & Bates and also past postings, but without
> success.
> 
>  
> 
> Take the Wafer data set which has 2 levels: Wafer (8 values) and Site
> nested within Wafer (10 values for each value of Wafer)  (Pinheiro &
> Bates book).  The data set has Sites coded as values from 1 to 8 for
> Wafer 1, values 1 to 8 for Wafer 2 etc.  Does this mean that the SAME
> sites are used for each Wafer (i.e. site 1 of water 1 is the same as
> site 1 of wafer 2)?  If different sites were chosen for each of the
> wafers would one have to code the sites = 1 to 8 for Wafer 1, sites = 9
> to 16 for Wafer 2, and so on?
> 
> In the case of individuals nested within species, each individual is
> unique - analogous to different sites being chosen for each wafter.  
> 
>  
> 
> Thanks.
> 
>  
> 
> Bill Shipley
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From MSchwartz at mn.rr.com  Sun Nov  6 20:26:17 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 06 Nov 2005 13:26:17 -0600
Subject: [R] How can I assign an argument to transfer whether
	by	ref	or	by value?
In-Reply-To: <1131299307.7881.13.camel@localhost.localdomain>
References: <436d7854.2fa050a0.24d8.77a6@mx.gmail.com>
	<1131299307.7881.13.camel@localhost.localdomain>
Message-ID: <1131305178.4173.222.camel@localhost.localdomain>

Adai,

Duncan posted a reply to Xiaofan's query, indicating that R is generally
based upon pass by value.

The difference being that within R, either explicit values or explicit
copies of objects are passed as function arguments, as opposed to
passing a memory location reference to the original value or object.

In C, this would be the difference between passing a value or named
variable versus passing a memory pointer to the value or named variable.

Consistent with R's lexical scoping, when for example an object is
passed to a function as an argument, a copy of the object, not a pointer
to the original object itself is passed.

This means that the original object is not modified, but the copy within
the function is or may be.

Try the following:

swap <- function(x, y)
{ 
  x.save <- x
  x <- y
  y <- x.save
  sprintf("x = %d, y = %d", x, y)
}


> x <- 1
> y <- 2

> swap(x, y)
[1] "x = 2, y = 1"

> x
[1] 1
> y
[1] 2


The original x and y values are unchanged, even though they were swapped
within the function.

Now, I could re-write the swap() function to something like the
following using "<<-":

swap <- function(x, y)
{ 
  x.save <- x
  x <<- y
  y <<- x.save
  sprintf("x = %d, y = %d", x, y)
}

and end up with:

> x
[1] 1
> y
[1] 2

> swap(x, y)
[1] "x = 1, y = 2"

> x
[1] 2
> y
[1] 1

Here we get the opposite behavior, where the local copies of x and y in
the function are unchanged, but the original values are.

One could also use something like eval.parent() to play around with this
behavior as well, which I just noted that Gabor has pointed out in his
reply.

HTH,

Marc Schwartz


On Sun, 2005-11-06 at 17:48 +0000, Adaikalavan Ramasamy wrote:
> I do not understand what your question is. Can you clarify with an
> example or analogies to other programming language.
> 
>  my.fun <- function(x, y=1){ x^y }
> 
>  my.fun(5)        # returns 5 
>  my.fun(5, 2)     # returns 25
>  my.fun(y=2, x=5) # returns 25
> 
> Regards, Adai
> 
> 
> On Sun, 2005-11-06 at 03:28 +0000, Xiaofan Li wrote:
> > Hello guys,
> > 
> > I am wondering the default way of transferring arguments in R. Is it by
> > value or by ref in default case, or could that be changed explicitly?
> > 
> > Cheers,
> > Xiaofan



From spencer.graves at pdf.com  Sun Nov  6 20:34:46 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Nov 2005 11:34:46 -0800
Subject: [R] information matrix in random effects model
In-Reply-To: <BF8BDCD0.9ED3%I.Visser@uva.nl>
References: <BF8BDCD0.9ED3%I.Visser@uva.nl>
Message-ID: <436E5AD6.8070400@pdf.com>

	  If you walk through the code line by line, you can probably find what 
you want, then make relatively simple changes to the code so these 
things are exported.  However, it may not be easy to use, because you 
need to understand the parameterization.  To obtain that, you may need 
to study the code more carefully.  It may also help to study Pinheiro 
and Bates (2000) Mixed-Effects Models in S and S-PLUS (Springer) and 
some of Bates' more recent publications cited in the help files in the 
nlme and lme4 packages.

	  What problem are you trying to solve?  Hessian matrices are most 
useful for normal approximations, which are however sometimes not very 
good.  Better approximations are obtained from profile likelihood, and 
even better ones are available from Monte Carlo.  If you'd like more 
help from this list, you might increase your chances of getting a quick, 
useful reply by considering the suggestions in the posting guide, 
"www.R-project.org/posting-guide.html".

	  hope this helps.
	  spencer graves 	

Ingmar Visser wrote:
> I use the lme function from the nlme library (or alternatively from the
> Matrix library) to estimate a random effects model. Both functions return
> the covariance matrix of the estimated parameters. I have the following
> question: 
> Is it possible to retrieve the information matrix of such a model (ie from
> the fitted object)? In particular, the information matrix can be computed as
> a sum of individual contributions:
> Sum_i H_i = H,
> where H_i are the contributions of each case to the Hessian matrix H.
> Are these individual contributions computed somewhere in either of the
> functions to estimate random effects models, and is it possible to extract
> them somehow from the fitted objects?
> 
> ingmar

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From D.Childs at sheffield.ac.uk  Sun Nov  6 20:39:10 2005
From: D.Childs at sheffield.ac.uk (Dylan Childs)
Date: Sun, 6 Nov 2005 19:39:10 +0000
Subject: [R] Problem defining a system of odes as a C library with lsoda
Message-ID: <8f10e27b82c536d008f57a3c441b780c@sheffield.ac.uk>

I have been trying to make use of the odesolve library on my 
university's Linux grid - currently R version 2.0.1 is installed and 
the system runs 64-bit Scientific Linux based on Redhat. I cannot seem 
to get lsoda working when I define the model as a shared C library. For 
example, the following snippet uses the mymod.c example bundled with 
the package:

### START
rm(list=ls())
setwd("~/projects/test_odesolve")
invisible(file.remove("mymod.o","mymod.so"))
system("R CMD SHLIB mymod.c")
dyn.load("mymod.so")

require(odesolve)
parms <- c(k1 = 0.04, k2 = 1e4, k3=3e7)
my.atol <- c(1e-6, 1e-10, 1e-6)
times <- c(0, 4*10^(-1:10))
print(system.time(
                   out1 <- lsoda(c(1.0,0.0,0.0),times,"myderivs",
                                 parms,
                                 rtol=1e-4,atol=my.atol,jacfunc="myjac",
                                 dllname="mymod")
                   )
       )
### END

When I run this mymod.c compiles without any errors or warnings and the 
mymod.so library seems to load fine via dyn.load (the call 
is.loaded("mymod") returns TRUE for example). However, the call to 
lsoda fails with the following error:

Error in lsoda(c(1, 0, 0), times, "myderivs", parms, rtol = 1e-04, atol 
= my.atol,  :
	Confusion over the length of parms

This seems a very odd error message as the length of the parameter 
vector is definitely correct and if I run exactly the same script on my 
Mac everything works fine. Have I missed something obvious - is this 
something to do with using an older version of R for example?

Dr. Dylan Z. Childs
Department of Animal and Plant Sciences,
University of Sheffield,
Sheffield,
S10 2TN, UK.



From leaflovesun at yahoo.ca  Sun Nov  6 21:03:55 2005
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Sun, 6 Nov 2005 13:03:55 -0700
Subject: [R] OLS variables
Message-ID: <200511062004.jA6K4d6H002269@hypatia.math.ethz.ch>

Thanks for the information!

Leaf
  
======= At 2005-11-06, 11:07:31 you wrote: =======

>IMHO, the details section of help(formula) provides a nicer help.
>
>Regards, Adai
>
>
>On Sun, 2005-11-06 at 08:27 -0500, John Fox wrote:
>> Dear Leaf,
>> 
>> I assume that you're using lm() to fit the model, and that you don't really
>> want *all* of the interactions among 20 predictors: You'd need quite a lot
>> of data to fit a model with 2^20 terms in it, and might have trouble
>> interpreting the results. 
>> 
>> If you know which interactions you're looking for, then why not specify them
>> directly, as in lm(y ~  x1*x2 + x3*x4*x5 + etc.)? On the other hand, it you
>> want to include all interactions, say, up to three-way, and you've put the
>> variables in a data frame, then lm(y ~ .^3, data=DataFrame) will do it.
>> There are many terms in this model, however, if not quite 2^20.
>> 
>> The introductory manual that comes with R has information on model formulas
>> in Section 11.
>> 
>> I hope this helps,
>>  John 
>> 
>> --------------------------------
>> John Fox
>> Department of Sociology
>> McMaster University
>> Hamilton, Ontario
>> Canada L8S 4M4
>> 905-525-9140x23604
>> http://socserv.mcmaster.ca/jfox 
>> -------------------------------- 
>> 
>> > -----Original Message-----
>> > From: r-help-bounces at stat.math.ethz.ch 
>> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leaf Sun
>> > Sent: Sunday, November 06, 2005 3:11 AM
>> > To: r-help at stat.math.ethz.ch
>> > Subject: [R] OLS variables
>> > 
>> > Dear all,
>> > 
>> > Is there any simple way in R that can I put the all the 
>> > interactions of the variables in the OLS model?
>> > 
>> > e.g.
>> > 
>> > I have a bunch of variables, x1,x2,.... x20... I expect then 
>> > to have interaction (e.g. x1*x2, x3*x4*x5... ) with some 
>> > combinations(2 way or higher dimensions). 
>> > 
>> > Is there any way that I can write the model simpler?
>> > 
>> > Thanks!
>> > 
>> > Leaf
>> > 
>> >
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>-- 
>Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
>Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
>Wolfson College Annexe                  Tel : 01865 284 408
>Linton Road, Oxford OX2 6UD             Fax : 01865 284 424
>
>.

= = = = = = = = = = = = = = = = = = = =
			
Leaf Sun
leaflovesun at yahoo.ca
2005-11-06



From p.connolly at hortresearch.co.nz  Sun Nov  6 21:37:48 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Mon, 7 Nov 2005 09:37:48 +1300
Subject: [R] kinship package example data
Message-ID: <20051106203748.GP18619@hortresearch.co.nz>

I've been looking at the kinship package which looks as though it
might be appropriate for my purposes.  What I can't find is any
reference to the data that is used in the example code.  A dataframe
called d10 with column names, upn, dadid, momid, sex and affect is
required.  One can get an idea of what sort of values should be in
most columns from the description in the pedigree function, but I
don't see anything to indicate what upn should look like.

My efforts to contact the package maintainer have been unsuccessful,
so this is a request to anyone who has used kinship to let me know if
there is a real dataframe called d10, and if not, what sort of
relationship does upn have to the dadid and momid?

Thank you.


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ggrothendieck at gmail.com  Sun Nov  6 21:46:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 6 Nov 2005 15:46:07 -0500
Subject: [R] kinship package example data
In-Reply-To: <20051106203748.GP18619@hortresearch.co.nz>
References: <20051106203748.GP18619@hortresearch.co.nz>
Message-ID: <971536df0511061246i5ad7137sc8066a5e67bfa887@mail.gmail.com>

You might look through the tests/testcoxme subdirectory.  Maybe
the file called cdata.dput ?


On 11/6/05, Patrick Connolly <p.connolly at hortresearch.co.nz> wrote:
> I've been looking at the kinship package which looks as though it
> might be appropriate for my purposes.  What I can't find is any
> reference to the data that is used in the example code.  A dataframe
> called d10 with column names, upn, dadid, momid, sex and affect is
> required.  One can get an idea of what sort of values should be in
> most columns from the description in the pedigree function, but I
> don't see anything to indicate what upn should look like.
>
> My efforts to contact the package maintainer have been unsuccessful,
> so this is a request to anyone who has used kinship to let me know if
> there is a real dataframe called d10, and if not, what sort of
> relationship does upn have to the dadid and momid?
>
> Thank you.
>
>
> --
> Patrick Connolly
> HortResearch
> Mt Albert
> Auckland
> New Zealand
> Ph: +64-9 815 4200 x 7188
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
> I have the world`s largest collection of seashells. I keep it on all
> the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From lotze at hcs.harvard.edu  Sun Nov  6 22:42:04 2005
From: lotze at hcs.harvard.edu (Thomas Lotze)
Date: Sun, 6 Nov 2005 16:42:04 -0500 (EST)
Subject: [R] Box's M/likelihood ratio test for equal v-c matrices
Message-ID: <Pine.LNX.4.64.0511061524260.25953@hcs.harvard.edu>

Hopefully a quick question:

is there a package/routine to perform Box's M (like proc discrim in SAS) 
or some other test for whether two multivariate samples (presumed 
multivariate normal) have the same v-c matrix?  I've looked, but can't 
find it.

If not, I can certainly code it up myself (and perhaps submit it for 
inclusion in the mvtnormtest package), but I wanted to check if it exists.

Many thanks,
Thomas Lotze

"Isn't sanity just a one-trick pony anyway?  I mean, all you get
is that one trick--rational thinking--but when you're good and crazy,
well, the sky's the limit!"
 				-The Tick



From spencer.graves at pdf.com  Sun Nov  6 23:03:06 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Nov 2005 14:03:06 -0800
Subject: [R] Help with Subtracting an effect from a Mixed Model
In-Reply-To: <5c049bb37946ef4afc9fabec9b4a3fcb@life.bio.sunysb.edu>
References: <5c049bb37946ef4afc9fabec9b4a3fcb@life.bio.sunysb.edu>
Message-ID: <436E7D9A.2090900@pdf.com>

	  1.  Have you considered specifying the random effect as a list?  This 
is mentioned in the help file and discussed in Pinheiro and Bates (2000) 
Mixed-Effects Models in S and S-Plus (Springer).  On p. 40, they discuss 
the following example that might help you:

lme(pixel~day+day^2, data=Pixel, random=list(Dog=~day, Side=~1).

(I highly recommend this book, by the way.)

	  2.  Doug Bates has made several contributions to this within the past 
year or so, which you can find by purusing the documentation for the 
functions in the lme4 and using RSiteSearch.  The following was useful 
for me with an apparently similar issue: 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/47423.html".

	  3.  If you'd like further help from this list, I encourage you to 
read the posting guide, "www.R-project.org/posting-guide.html", 
especially the part about submitting a self-contained toy example that 
illustrates your question.  If someone can copy a few lines of code from 
your email into R and try a few things in a couple of minutes, I believe 
it will increase the chances that you will get quick, useful answers.

	  hope this helps.
	  spencer graves	

Matthew Forister wrote:
> Hi Everyone,
> 
> I posted a similar question about a week ago, but haven't gotten any 
> replies -- I'm afraid that's because my previous question was too 
> vague.  Let me try again with a more specific question, and I hope 
> someone can help.  NOTE, I know I should be using the newer lme4 
> package, I just haven't had a chance to update my version of R yet, so 
> the question below relates to nmle.
> 
> I have data from a classical quantitative genetics experiment, with 33 
> sires mated each to 2 dams, with 15 progeny from each dam raised on 5 
> host plants (3 larvae per host).  So the model I would like to run has 
> the following factors:
> Host (fixed)
> Sire (random)
> Dam [nested within sire] (random)
> Host * Sire (random interaction)
> Host * Dam [nested within sire] (random interaction)
> 
> This is the code I am using for that complete model:
> lme1=lme(gain~host,random=~host|sire/dam)
> I would like to try the model with and without each of those random 
> interactions (host*sire and host*dam-nested-within-sire).
> 
> WHAT IS THE SYNTAX FOR SUBTRACTING AN EFFECT FROM THE RANDOM STATEMENT?
> I have tried:
> lme1=lme(gain~host,random=~host|sire/dam-sire:host)
> But I get an "Invalid formula for groups" error.
> Alternatively, I tried reorganizing the code, so that the interaction 
> would be left out.  But there is no way to take out host*sire while 
> still leaving in host*dam nested within sire.
> 
> any help will be greatly appreciated!
> Matt
> 
> - - -
> Matthew L Forister
> Department of Ecology and Evolution
> State University of New York at Stony Brook
> 650 Life Sciences Building
> Stony Brook, New York 11794-5245
> Email: forister at life.bio.sunysb.edu
> Webpage: http://life.bio.sunysb.edu/~forister/
> Lab phone: (631) 632-8609
> Fax: (631) 632-7626
> - - -
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From p.connolly at hortresearch.co.nz  Sun Nov  6 23:26:33 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Mon, 7 Nov 2005 11:26:33 +1300
Subject: [R] kinship package example data
In-Reply-To: <971536df0511061246i5ad7137sc8066a5e67bfa887@mail.gmail.com>
References: <20051106203748.GP18619@hortresearch.co.nz> 
	<971536df0511061246i5ad7137sc8066a5e67bfa887@mail.gmail.com>
Message-ID: <20051106222633.GQ18619@hortresearch.co.nz>

On Sun, 06-Nov-2005 at 03:46PM -0500, Gabor Grothendieck wrote:

|> You might look through the tests/testcoxme subdirectory.  Maybe
|> the file called cdata.dput ?

Close, but not quite.  However, it got me looking in the right
place. It's in the tests/testpedigree directory.

Thank you for the hint.


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From kerryrekky at yahoo.com  Sun Nov  6 23:47:17 2005
From: kerryrekky at yahoo.com (Cunningham Kerry)
Date: Sun, 6 Nov 2005 14:47:17 -0800 (PST)
Subject: [R] solving a complicated equation
Message-ID: <20051106224717.43489.qmail@web51813.mail.yahoo.com>

I want to solve the following equation for x

p=a*exp(-x^2/2)+b*P(Z>x)

where p,a,b are known, Z is a standard normal
variable. Clearly there is no analytic form for
P(Z>x). 

I am wondering if any expert could direct one easy way
on this. Thank you.



From reilly at stat.auckland.ac.nz  Mon Nov  7 00:09:19 2005
From: reilly at stat.auckland.ac.nz (James Reilly)
Date: Mon, 07 Nov 2005 12:09:19 +1300
Subject: [R] solving a complicated equation
In-Reply-To: <20051106224717.43489.qmail@web51813.mail.yahoo.com>
References: <20051106224717.43489.qmail@web51813.mail.yahoo.com>
Message-ID: <436E8D1F.2070406@stat.auckland.ac.nz>

Try ?uniroot and ?pnorm.

On 7/11/2005 11:47 a.m., Cunningham Kerry wrote:
> I want to solve the following equation for x
> 
> p=a*exp(-x^2/2)+b*P(Z>x)
> 
> where p,a,b are known, Z is a standard normal
> variable. Clearly there is no analytic form for
> P(Z>x). 
> 
> I am wondering if any expert could direct one easy way
> on this. Thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
James Reilly
Department of Statistics, University of Auckland
Private Bag 92019, Auckland, New Zealand



From ggrothendieck at gmail.com  Mon Nov  7 00:18:16 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 6 Nov 2005 18:18:16 -0500
Subject: [R] solving a complicated equation
In-Reply-To: <20051106224717.43489.qmail@web51813.mail.yahoo.com>
References: <20051106224717.43489.qmail@web51813.mail.yahoo.com>
Message-ID: <971536df0511061518p21a9a4a9qab328c6c51f00c4c@mail.gmail.com>

Try this:

f <- function(x, a, b, p) a * exp(-x*x/2)+ b * pnorm(x, lower.tail = FALSE) - p
uniroot(f, lower = -3, upper = 3, a = 1, b = 1, p = 0.5)

On 11/6/05, Cunningham Kerry <kerryrekky at yahoo.com> wrote:
> I want to solve the following equation for x
>
> p=a*exp(-x^2/2)+b*P(Z>x)
>
> where p,a,b are known, Z is a standard normal
> variable. Clearly there is no analytic form for
> P(Z>x).
>
> I am wondering if any expert could direct one easy way
> on this. Thank you.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Mon Nov  7 00:43:28 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Nov 2005 15:43:28 -0800
Subject: [R] nlme error message
In-Reply-To: <web-11223574@mail3.rug.nl>
References: <web-11223574@mail3.rug.nl>
Message-ID: <436E9520.5060203@pdf.com>

	  You need repeated measures for a random effect to make any sense.  I 
modified your example as follows, and the error went away.

 > mytable$RIL2 <- rep(1:4, 1:4)
 > cs2 <- corCompSymm(value=0.5, form=~1|RIL2)
 > model2<-lme(mytrait~myloc, data=mytable, random=~1|RIL2,
+             correlation=cs2)

	  (I've made similar mistakes and had great difficulty finding the 
problem.)

	  spencer graves

J.Fu wrote:
> Dear Friends,
> 
> I am seeking for any help on an error message in lme 
> functions. I use mixed model to analyze a data with 
> compound symmetric correlation structure. But I get an 
> error message: "Error in corMatrix.corCompSymm(object) : 
> NA/NaN/Inf in foreign function call (arg 1)". If I change 
> the correlation structure to corAR1, then no error. I have 
> no clue how to solve this problem. I would highly 
> appreciate any help.
> Thanks in advance and looking forward to any help.
> 
> JY
> 
> 
> I attached my data and codes here:
> 
> # data: mytable
>           mytrait myloc RIL
> A1 0.590950330 0 1
> A2 -0.315469846 -1 2
> A3 -0.265690115 0 3
> A4 0.342885046 0 4
> A5 0.007613402 1 5
> A6 0.285997884 0 6
> A7 0.333841975 0 7
> A8 -0.599817735 -1 8
> A9 0.242621036 0 9
> A10 0.518959588 1 10
> 
> cs<-corCompSymm(0.5, form=~1|RIL, fixed=T)
> model<-lme(mytrait~myloc, data=mytable, random=~1|RIL, 
> na.action=na.omit, correlation=cs)
> 
> Error in corMatrix.corCompSymm(object) : NA/NaN/Inf in 
> foreign function call (arg 1)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Mon Nov  7 02:24:04 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Nov 2005 17:24:04 -0800
Subject: [R] how to optimise cross-correlation plot to study time lag
 between time-series?
In-Reply-To: <000001c5ddf9$50f5ac00$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000001c5ddf9$50f5ac00$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <436EACB4.8080305@pdf.com>

	  If one series is "input" and the other "output", the traditional 
advice (Box and Jenkins 1970 Time Series Analysis, Forecasting and 
Control, sec. 11.2.1) is as follows:

	  1.  Fit an ARIMA model to the "input".

	  2.  Prewhiten the "output" series using the model for the "input". 
Then compute the cross correlation function between the residuals from 
the input and the prewhitened output.

	  This could, of course, be done in R, but I don't know if it has 
already been programmed as a standard function. 
RSiteSearch("pre-whitening") and RSiteSearch("prewhitening") produced 5 
hits between them, and my cursory review of them didn't lead to 
immediate enlightenment.  If such a function exists, it's available 
under a different name.  There may be better techniques available today, 
but I'm not familiar with them.

	  spencer graves

Jan Verbesselt wrote:
> Dear R-help,
> 
> How could a cross-correlation plot be optimized such that the relationship
> between seasonal time-series can be studied?
> 
> We are working with strong seasonal time-series and derived a
> cross-correlation plot to study the relationship between time-series. The
> seasonal variation however strongly influences the cross-correlation plot
> and the plot seems to be “rather” symmetrical (max cross-correlation
> coefficient occurs at lag 0). We would like to visualize the deviation from
> the symmetrical shape such that the relationship between these two time
> series can be studied. How can the symmetry be investigated by using a
> cross-corr. plot (ccf())?
> 
> We tried the following:
> 
>     cross <- ccf(TS1, int.TS2, main= "")
>     
>     # produce the standard shape by correlating TS1 with TS1
>     test <- ccf(TS1, TS1)
> 
>     # add the standard shape on the cross-correlation plot of TS1 with TS2
> 
>     plot(cross)
>     par(new = T)
>     plot(test$lag, test$acf, axes=F, xlab="", ylab="", col=2)
>     
>    
> Is there another technique to visualize the difference from the symmetrical
> shape? Is ts1 lagged vs. ts2?
> 
> Jan
> (*Version R 2.2)
> 
> 
> Ps. -We tried also ccf() after differencing and decompositioning but
> seasonality remains in the residuals.
>      -max cross-correlation mostly occurs at lag 0.
> 
> 
> _______________________________________________________________________
> Ir. Jan Verbesselt
> Research Associate
> Biosystems Department ~ M³-BIORES
> Vital Decosterstraat 102, 3000 Leuven, Belgium
> Tel: +32-16-329750   Fax: +32-16-329760
> http://gloveg.kuleuven.ac.be/
> _______________________________________________________________________
> 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From kjetil at redcotel.bo  Sun Nov  6 14:34:16 2005
From: kjetil at redcotel.bo (Kjetil Brinchmann halvorsen)
Date: Sun, 06 Nov 2005 09:34:16 -0400
Subject: [R] Use of paste with apply()
Message-ID: <436E0658.5030106@redcotel.bo>

I was surprised by:

 > test <- matrix( as.character(1:4), 2)
 > test
      [,1] [,2]
[1,] "1"  "3"
[2,] "2"  "4"
 > apply(test, 1, paste, sep="+")
      [,1] [,2]
[1,] "1"  "2"
[2,] "3"  "4"
 > apply(test, 1, paste, sep="*")
      [,1] [,2]
[1,] "1"  "2"
[2,] "3"  "4"
 > te <- matrix(1:4, 2)
 > te
      [,1] [,2]
[1,]    1    3
[2,]    2    4
 > apply(te, 1, sum)
[1] 4 6

Why doesn't paste behave in apply as sum?

Kjetil


-- 

Checked by AVG Free Edition.



From kjetil at redcotel.bo  Sun Nov  6 16:37:27 2005
From: kjetil at redcotel.bo (Kjetil Brinchmann halvorsen)
Date: Sun, 06 Nov 2005 11:37:27 -0400
Subject: [R] OLS variables
In-Reply-To: <20051106132725.WVPZ2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>
References: <20051106132725.WVPZ2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <436E2337.9080302@redcotel.bo>

John Fox wrote:
> Dear Leaf,
> 
> I assume that you're using lm() to fit the model, and that you don't really
> want *all* of the interactions among 20 predictors: You'd need quite a lot
> of data to fit a model with 2^20 terms in it, and might have trouble
> interpreting the results. 
> 
> If you know which interactions you're looking for, then why not specify them
> directly, as in lm(y ~  x1*x2 + x3*x4*x5 + etc.)? On the other hand, it you
> want to include all interactions, say, up to three-way, and you've put the
> variables in a data frame, then lm(y ~ .^3, data=DataFrame) will do it.  

This is nice with factors, but with continuous variables, and need of a
response-surface type, of model, will not do. For instance, with 
variables x, y, z in data frame dat
    lm( y ~ (x+z)^2, data=dat )
gives a model mwith the terms x, z and x*z, not the square terms.
There is a need for a semi-automatic way to get these, for instance,
use poly() or polym() as in:

lm(y ~ polym(x,z,degree=2), data=dat)

Kjetil

> There are many terms in this model, however, if not quite 2^20.
> 
> The introductory manual that comes with R has information on model formulas
> in Section 11.
> 
> I hope this helps,
>  John 
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leaf Sun
>> Sent: Sunday, November 06, 2005 3:11 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] OLS variables
>>
>> Dear all,
>>
>> Is there any simple way in R that can I put the all the 
>> interactions of the variables in the OLS model?
>>
>> e.g.
>>
>> I have a bunch of variables, x1,x2,.... x20... I expect then 
>> to have interaction (e.g. x1*x2, x3*x4*x5... ) with some 
>> combinations(2 way or higher dimensions). 
>>
>> Is there any way that I can write the model simpler?
>>
>> Thanks!
>>
>> Leaf
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> 



-- 

Checked by AVG Free Edition.



From maustin at amgen.com  Mon Nov  7 03:33:27 2005
From: maustin at amgen.com (Austin, Matt)
Date: Sun, 6 Nov 2005 18:33:27 -0800 
Subject: [R] Use of paste with apply()
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD3DA@teal-exch.amgen.com>

The apply function is passing each row of you matrix as a single vector into
paste.  If paste receives a single vector and collapse is NULL, it will
simply coerce the vector into a character vector. 

However, when you collapse instead of sep

> test <- matrix( as.character(1:4), 2)
> apply(test, 1, paste, sep="+")
     [,1] [,2]
[1,] "1"  "2" 
[2,] "3"  "4" 

> apply(test, 1, paste, collapse="+")
[1] "1+3" "2+4"

Which may be closer to what you were expecting, but I'm just guessing.

--Matt


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Kjetil 
> Brinchmann
> halvorsen
> Sent: Sunday, November 06, 2005 5:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Use of paste with apply()
> 
> 
> I was surprised by:
> 
>  > test <- matrix( as.character(1:4), 2)
>  > test
>       [,1] [,2]
> [1,] "1"  "3"
> [2,] "2"  "4"
>  > apply(test, 1, paste, sep="+")
>       [,1] [,2]
> [1,] "1"  "2"
> [2,] "3"  "4"
>  > apply(test, 1, paste, sep="*")
>       [,1] [,2]
> [1,] "1"  "2"
> [2,] "3"  "4"
>  > te <- matrix(1:4, 2)
>  > te
>       [,1] [,2]
> [1,]    1    3
> [2,]    2    4
>  > apply(te, 1, sum)
> [1] 4 6
> 
> Why doesn't paste behave in apply as sum?
> 
> Kjetil
> 
> 
> -- 
> 
> Checked by AVG Free Edition.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From stuart.macgregor at qimr.edu.au  Mon Nov  7 08:04:12 2005
From: stuart.macgregor at qimr.edu.au (Stuart Macgregor)
Date: Mon, 07 Nov 2005 17:04:12 +1000
Subject: [R] slow R start up
Message-ID: <1131347052.15914.16.camel@straddie.qimr.edu.au>

Hi,
   I want to use R within a unix shell script where I repeatedly open
and close R (doing some computation within R whilst it is open). i.e.
something like

#!/bin/sh
R --vanilla << EOF
# some R commands
EOF
# some unix commands
R --vanilla << EOF
# some R commands
EOF
# some unix commands
...etc

the problem is that since I open and close R many times, the shell
script takes a long time to run (R takes ~1 second to start up, even on
a fast machine; I've tried various linux installations and versions of R
from 1.7 to 2.2). Is there some way of making R start up faster? I've
tried altering the memory start options but this makes little
difference.
Note I have various other programs called from the shell script and
don't want to do everything from "within" R. 
One possible solution may be to send the relevant commands to an already
running copy of R but I've yet to figure out how to achieve this.

Any suggestions gratefully received.

Stuart.



From ggrothendieck at gmail.com  Mon Nov  7 08:26:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 7 Nov 2005 02:26:22 -0500
Subject: [R] slow R start up
In-Reply-To: <1131347052.15914.16.camel@straddie.qimr.edu.au>
References: <1131347052.15914.16.camel@straddie.qimr.edu.au>
Message-ID: <971536df0511062326g26b311b1wdfb540faa022bb11@mail.gmail.com>

Or you could call those other programs from within R.  See
?system


On 11/7/05, Stuart Macgregor <stuart.macgregor at qimr.edu.au> wrote:
> Hi,
>   I want to use R within a unix shell script where I repeatedly open
> and close R (doing some computation within R whilst it is open). i.e.
> something like
>
> #!/bin/sh
> R --vanilla << EOF
> # some R commands
> EOF
> # some unix commands
> R --vanilla << EOF
> # some R commands
> EOF
> # some unix commands
> ...etc
>
> the problem is that since I open and close R many times, the shell
> script takes a long time to run (R takes ~1 second to start up, even on
> a fast machine; I've tried various linux installations and versions of R
> from 1.7 to 2.2). Is there some way of making R start up faster? I've
> tried altering the memory start options but this makes little
> difference.
> Note I have various other programs called from the shell script and
> don't want to do everything from "within" R.
> One possible solution may be to send the relevant commands to an already
> running copy of R but I've yet to figure out how to achieve this.
>
> Any suggestions gratefully received.
>
> Stuart.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Nov  7 08:44:16 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Nov 2005 07:44:16 +0000 (GMT)
Subject: [R] slow R start up
In-Reply-To: <1131347052.15914.16.camel@straddie.qimr.edu.au>
References: <1131347052.15914.16.camel@straddie.qimr.edu.au>
Message-ID: <Pine.LNX.4.61.0511070721460.7471@gannet.stats>

On Mon, 7 Nov 2005, Stuart Macgregor wrote:

> Hi,
>   I want to use R within a unix shell script where I repeatedly open
> and close R (doing some computation within R whilst it is open). i.e.
> something like
>
> #!/bin/sh
> R --vanilla << EOF
> # some R commands
> EOF
> # some unix commands
> R --vanilla << EOF
> # some R commands
> EOF
> # some unix commands
> ...etc
>
> the problem is that since I open and close R many times, the shell
> script takes a long time to run (R takes ~1 second to start up, even on
> a fast machine; I've tried various linux installations and versions of R
> from 1.7 to 2.2). Is there some way of making R start up faster? I've
> tried altering the memory start options but this makes little
> difference.

[I don't think your machine _is_ 'fast'!  I am seeing 0.3s for a complete 
R session on an Opteron 250.  We have worked hard to reduce the start-up 
time to those levels over the period you quote: please do not belittle 
those efforts.  R is fast (several times at least) compared to all other 
software I use of similar complexity.]

On my 2-year-old i386 box a complete empty session takes 0.5s.  You can 
reduce that a lot by using fewer default packages.  For example, if you 
don't need methods or graphics or datasets, 
R_DEFAULT_PACKAGES="utils,stats" reduces that to 0.3s, and 
R_DEFAULT_PACKAGES=NULL reduces it to 0.05s.  We use the latter when using 
R for scripting (e.g. in R CMD INSTALL).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov  7 09:57:21 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 07 Nov 2005 08:57:21 -0000 (GMT)
Subject: [R] slow R start up
In-Reply-To: <971536df0511062326g26b311b1wdfb540faa022bb11@mail.gmail.com>
Message-ID: <XFMail.051107085721.Ted.Harding@nessie.mcc.ac.uk>


On 07-Nov-05 Gabor Grothendieck wrote:
> Or you could call those other programs from within R.  See
> ?system

A possibility which might work, and if so would work as you intend,
is to use a FIFO ("named pipe") or two to communicate with R.

In R, see "?connections"; in Linux, see "man mkfifo".

Then you should be able to start R up so that it reads commands
from the FIFO, (and optionally writes output to a FIFO which can
be read by the shell script; though it might be simpler to write
the output to a regular file).

This would be the converse of running systen commands from within
R using 'system()'.

However, there can be issues with this approach since, in my
experience, you have to ensure that the program reading the
FIFO knows when there is more to come on the FIFO, and also
when to stop reading. In principle it's straightforward,
though, and rather like reading the commamnds simply from
a file.

Or you might emulate the whole thing using regular files:
Put R into a loop which reads commands from a file (testing
whether it has changed) which the shell script updates as
needed. But you'd need to be careful that R does not read
a partially updated file.

Best wishes,
Ted.

> On 11/7/05, Stuart Macgregor <stuart.macgregor at qimr.edu.au> wrote:
>> Hi,
>>   I want to use R within a unix shell script where I repeatedly open
>> and close R (doing some computation within R whilst it is open). i.e.
>> something like
>>
>> #!/bin/sh
>> R --vanilla << EOF
>> # some R commands
>> EOF
>> # some unix commands
>> R --vanilla << EOF
>> # some R commands
>> EOF
>> # some unix commands
>> ...etc
>>
>> the problem is that since I open and close R many times, the shell
>> script takes a long time to run (R takes ~1 second to start up, even
>> on
>> a fast machine; I've tried various linux installations and versions of
>> R
>> from 1.7 to 2.2). Is there some way of making R start up faster? I've
>> tried altering the memory start options but this makes little
>> difference.
>> Note I have various other programs called from the shell script and
>> don't want to do everything from "within" R.
>> One possible solution may be to send the relevant commands to an
>> already
>> running copy of R but I've yet to figure out how to achieve this.
>>
>> Any suggestions gratefully received.
>>
>> Stuart.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Nov-05                                       Time: 08:57:16
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Mon Nov  7 10:05:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Nov 2005 09:05:15 +0000 (GMT)
Subject: [R] OLS variables
In-Reply-To: <436E2337.9080302@redcotel.bo>
References: <20051106132725.WVPZ2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>
	<436E2337.9080302@redcotel.bo>
Message-ID: <Pine.LNX.4.61.0511070744300.7471@gannet.stats>

On Sun, 6 Nov 2005, Kjetil Brinchmann halvorsen wrote:

> John Fox wrote:
>>
>> I assume that you're using lm() to fit the model, and that you don't really
>> want *all* of the interactions among 20 predictors: You'd need quite a lot
>> of data to fit a model with 2^20 terms in it, and might have trouble
>> interpreting the results.
>>
>> If you know which interactions you're looking for, then why not specify them
>> directly, as in lm(y ~  x1*x2 + x3*x4*x5 + etc.)? On the other hand, it you
>> want to include all interactions, say, up to three-way, and you've put the
>> variables in a data frame, then lm(y ~ .^3, data=DataFrame) will do it.
>
> This is nice with factors, but with continuous variables, and need of a
> response-surface type, of model, will not do. For instance, with
> variables x, y, z in data frame dat
>    lm( y ~ (x+z)^2, data=dat )
> gives a model mwith the terms x, z and x*z, not the square terms.
> There is a need for a semi-automatic way to get these, for instance,
> use poly() or polym() as in:
>
> lm(y ~ polym(x,z,degree=2), data=dat)

This is an R-S difference (FAQ 3.3.2).  R's formula parser always takes 
x^2 = x whereas the S one does so only for factors.  This makes sense it 
you interpret `interaction' strictly as in John's description - S chose 
to see an interaction of any two continuous variables as multiplication
(something which puzzled me when I first encountered it, as it was not 
well documented back in 1991).

I have often wondered if this difference was thought to be an improvement, 
or if it just a different implementation of the Rogers-Wilkinson syntax.
Should we consider changing it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From B.Rowlingson at lancaster.ac.uk  Mon Nov  7 10:10:50 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 07 Nov 2005 09:10:50 +0000
Subject: [R] slow R start up
In-Reply-To: <XFMail.051107085721.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051107085721.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <436F1A1A.2020504@lancaster.ac.uk>

Some helpful person wrote:

> A possibility which might work, and if so would work as you intend,
> is to use a FIFO ("named pipe") or two to communicate with R.

>>>One possible solution may be to send the relevant commands to an
>>>already
>>>running copy of R but I've yet to figure out how to achieve this.

  You could try one of the R server-based solutions:

http://franklin.imgen.bcm.tmc.edu/R.web.servers/

  You could try Rserve, but you'll probably have to write some C++ 
wrapper so you can do a command from the shell, so you can do something 
like:

  echo "x=runif(1000);hist(x)" | RserveShell

from your shell script. Its probably only ten lines of C++.

http://stats.math.uni-augsburg.de/Rserve/doc.shtml

  If you rewrite your script in Python, I may have some Python-Rserve 
bindings working soon!

  Do you need to get any of R's computational results back into your shell?

  Whenever I write shell scripts, once it gets to about 20 lines I think 
to myself "Uh oh, I shoulda written this in perl". And when I've written 
200 lines of perl I find myself thinking "Uh oh, I shoulda written this 
in Python".

Baz



From andylehnert at gmx.de  Mon Nov  7 11:45:56 2005
From: andylehnert at gmx.de (Andreas Lehnert)
Date: Mon, 7 Nov 2005 11:45:56 +0100 (MET)
Subject: [R] levelplot and layout
Message-ID: <22830.1131360356@www73.gmx.net>


Dear R,

I know that I cannot use filled.contour() with layout,
so I tried levelplot. But it looks like I cannot use levelplot also.
The only way to get more than one graphic in the window is 
by levelplot(panel=). but there is only one colorkey and I need one for
every 
graphic.
(I??m also not very satisfied with levelplot, because I would like to have
a "soft" graphic like filled.contour)

Could anyone kindly suggest anything?

Thanks, Andy

-- 
Telefonieren Sie schon oder sparen Sie noch?



From dlvanbrunt at gmail.com  Mon Nov  7 12:16:24 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Mon, 7 Nov 2005 06:16:24 -0500
Subject: [R] R seems to "stall" after several hours on a long series of
	analyses... where to start?
Message-ID: <d332d3e10511070316j7028fffdw14bbfc94636dfdf2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051107/1c1a81a2/attachment.pl

From murdoch at stats.uwo.ca  Mon Nov  7 13:00:58 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 07 Nov 2005 07:00:58 -0500
Subject: [R] R seems to "stall" after several hours on a long series of
 analyses... where to start?
In-Reply-To: <d332d3e10511070316j7028fffdw14bbfc94636dfdf2@mail.gmail.com>
References: <d332d3e10511070316j7028fffdw14bbfc94636dfdf2@mail.gmail.com>
Message-ID: <436F41FA.2080003@stats.uwo.ca>

David L. Van Brunt, Ph.D. wrote:
> Not sure where to even start on this.... I'm hoping there's some debugging I
> can do...
> 
> I have a loop that cycles through several different data sets (same
> structure, different info), performing randomForest growth and
> predictions... saving out the predictions for later study...
> 
> I get about 5 hours in (9%... of the planned iterations.. yikes!) and R just
> freezes.
> 
> This happens in interactive and batch mode execution (I can see from the
> ".Rout" file that it gets about 9% through in Batch mode, and about 6% if in
> interactive mode... does that suggest memory problems?)
> 
> I'm thinking of re-executing this same code on a different platform to see
> if that's the issue (currently using OS X)... any other suggestions on where
> to look, or what to try to get more information?
> 
> Sorry so vague... it's a LOT of code, runs fine without error for many
> iterations, so I didn't think the problem was syntax...

You could try running an external debugger to see whether it appears R 
is stuck in a loop.  I don't know what OS X debuggers are like, but on 
Windows, you can see routine names even without debugging information. 
Recompiling R with debugging info will make the results a lot easier to 
interpret.

Duncan Murdoch



From joseclaudio.faria at terra.com.br  Mon Nov  7 13:10:39 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Mon, 07 Nov 2005 09:10:39 -0300
Subject: [R] R (2.2.0), R-DCOM and Delphi
Message-ID: <436F443F.2010900@terra.com.br>

Dear Dieter,

First, thank you for your work!
Below the error message I got when trying to compile your RDCom.dpr

[Warning] STATCONNECTORCLNTLib_TLB.pas(319): Unsafe type 'EventDispIDs: Pointer'
[Warning] STATCONNECTORCLNTLib_TLB.pas(320): Unsafe type 'LicenseKey: Pointer'
[Warning] STATCONNECTORCLNTLib_TLB.pas(324): Unsafe code '@ operator'
[Warning] STATCONNECTORCLNTLib_TLB.pas(367): Unsafe type 'EventDispIDs: Pointer'
[Warning] STATCONNECTORCLNTLib_TLB.pas(368): Unsafe type 'LicenseKey: Pointer'
[Warning] STATCONNECTORCLNTLib_TLB.pas(372): Unsafe code '@ operator'
[Warning] STATCONNECTORCLNTLib_TLB.pas(374): Unsafe code '@ operator'
[Warning] STATCONNECTORSRVLib_TLB.pas(376): Unsafe type 'LicenseKey: Pointer'
[Warning] STATCONNECTORSRVLib_TLB.pas(379): Unsafe code '@ operator'
[Warning] RCom.pas(93): Unsafe code 'String index to var param'
[Error] RCom.pas(119): Undeclared identifier: 'VarType'
[Error] RCom.pas(124): Undeclared identifier: 'VarArrayDimCount'
[Error] RCom.pas(127): Undeclared identifier: 'VarArrayHighBound'
[Error] RCom.pas(140): Undeclared identifier: 'VarType'
[Error] RCom.pas(145): Undeclared identifier: 'VarArrayDimCount'
[Error] RCom.pas(148): Undeclared identifier: 'VarArrayHighBound'
[Error] RCom.pas(161): Undeclared identifier: 'VarType'
[Error] RCom.pas(166): Undeclared identifier: 'VarArrayDimCount'
[Error] RCom.pas(169): Undeclared identifier: 'VarArrayHighBound'
[Error] RCom.pas(181): Undeclared identifier: 'VarArrayCreate'
[Error] RCom.pas(196): Undeclared identifier: 'VarArrayCreate'
[Error] RCom.pas(211): Undeclared identifier: 'VarArrayCreate'
[Error] RCom.pas(226): Undeclared identifier: 'VarArrayCreate'
[Error] RCom.pas(253): Undeclared identifier: 'VarType'
[Error] RCom.pas(258): Undeclared identifier: 'VarArrayDimCount'
[Error] RCom.pas(261): Undeclared identifier: 'VarArrayHighBound'
[Fatal Error] RDComMain.pas(14): Could not compile used unit 'RCom.pas'

I'm using Delphi 7 under WinXP pro/SP2.
Could you give me a tip?

Regards,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From FowlerM at mar.dfo-mpo.gc.ca  Mon Nov  7 13:14:54 2005
From: FowlerM at mar.dfo-mpo.gc.ca (Fowler, Mark)
Date: Mon, 07 Nov 2005 08:14:54 -0400
Subject: [R] R seems to "stall" after several hours on a long series o	f
 analyses... where to start?
Message-ID: <1A4AC4BAB9C50A42854582B69B08C0340949B845@MSGMARBIO05>

You can test if the problem is accumulation in memory registers, which is
certainly what this sounds like. Just do a loop over a reasonably small
number of iterations and store or print the time between each iteration. If
memory accumulation it will run optimally for the first few iterations,
after which the time will increase noticeably (essentially exponentially,
hence ultimately freezes up). If this is the problem you may need to switch
to a For loop approach, or effect the loop as a DOS script to do each
iteration as a batch job (open/close R each iteration, old bytes can't
clutter the memory).


>	Mark Fowler
		Population Ecology Division
>	Bedford Inst of Oceanography
>	Dept Fisheries & Oceans
>	Dartmouth NS Canada
		B2Y 4A2
		Tel. (902) 426-3529
		Fax (902) 426-9710
		Email fowlerm at mar.dfo-mpo.gc.ca
		Home Tel. (902) 461-0708
		Home Email mark.fowler at ns.sympatico.ca


-----Original Message-----
From: David L. Van Brunt, Ph.D. [mailto:dlvanbrunt at gmail.com] 
Sent: November 7, 2005 7:16 AM
To: r-help at stat.math.ethz.ch
Subject: [R] R seems to "stall" after several hours on a long series of
analyses... where to start?

Not sure where to even start on this.... I'm hoping there's some debugging I
can do...

I have a loop that cycles through several different data sets (same
structure, different info), performing randomForest growth and
predictions... saving out the predictions for later study...

I get about 5 hours in (9%... of the planned iterations.. yikes!) and R just
freezes.

This happens in interactive and batch mode execution (I can see from the
".Rout" file that it gets about 9% through in Batch mode, and about 6% if in
interactive mode... does that suggest memory problems?)

I'm thinking of re-executing this same code on a different platform to see
if that's the issue (currently using OS X)... any other suggestions on where
to look, or what to try to get more information?

Sorry so vague... it's a LOT of code, runs fine without error for many
iterations, so I didn't think the problem was syntax...

--
---------------------------------------
David L. Van Brunt, Ph.D.
mailto:dlvanbrunt at gmail.com

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dieter.menne at menne-biomed.de  Mon Nov  7 14:31:59 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 7 Nov 2005 13:31:59 +0000 (UTC)
Subject: [R] R (2.2.0), R-DCOM and Delphi
References: <436F443F.2010900@terra.com.br>
Message-ID: <loom.20051107T143033-989@post.gmane.org>

Jose Claudio Faria <joseclaudio.faria <at> terra.com.br> writes:

> 
> Dear Dieter,
> 
> Below the error message I got when trying to compile your RDCom.dpr
> 
...
> [Warning] RCom.pas(93): Unsafe code 'String index to var param'
> [Error] RCom.pas(119): Undeclared identifier: 'VarType'
> [Error] RCom.pas(124): Undeclared identifier: 'VarArrayDimCount'
..
> I'm using Delphi 7 under WinXP pro/SP2.

Looks like a missing file in "uses". I compiled it under Delphi 5, and it looks 
like the variants are no longer included in my "uses".

Try to find out via help file where VarType is declared, add it to "uses". And 
please tell me about the result when you got it compiled, I will add your 
finding to the archive.

Dieter



From jfontain at free.fr  Mon Nov  7 14:55:37 2005
From: jfontain at free.fr (jfontain@free.fr)
Date: Mon, 07 Nov 2005 14:55:37 +0100
Subject: [R] R-2.2.0: aggregate problem
Message-ID: <1131371737.436f5cd980ed3@imp1-g19.free.fr>

> aggregate(as.ts(c(1,2,3,4,5,6,7,8,9,10)),1/2,mean)
Time Series:
Start = 1
End = 9
Frequency = 0.5
[1] 1.5 3.5 5.5 7.5 9.5
> aggregate(as.ts(c(1,2,3,4,5,6,7,8,9,10)),1/5,mean)
Error in sprintf(gettext(fmt, domain = domain), ...) :
	use format %f, %e or %g for numeric objects
>

I must be doing something wrong, but what?

Many thanks for your help,

--
Jean-Luc



From andy_liaw at merck.com  Mon Nov  7 15:18:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 7 Nov 2005 09:18:18 -0500
Subject: [R] Box's M/likelihood ratio test for equal v-c matrices
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED574@usctmx1106.merck.com>

Doing RSiteSearch("Box's M test") ought to help:  There's code in 
the 3rd hit.

Andy

> From: Thomas Lotze
> 
> Hopefully a quick question:
> 
> is there a package/routine to perform Box's M (like proc 
> discrim in SAS) 
> or some other test for whether two multivariate samples (presumed 
> multivariate normal) have the same v-c matrix?  I've looked, 
> but can't 
> find it.
> 
> If not, I can certainly code it up myself (and perhaps submit it for 
> inclusion in the mvtnormtest package), but I wanted to check 
> if it exists.
> 
> Many thanks,
> Thomas Lotze
> 
> "Isn't sanity just a one-trick pony anyway?  I mean, all you get
> is that one trick--rational thinking--but when you're good and crazy,
> well, the sky's the limit!"
>  				-The Tick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abigi at agrsci.unibo.it  Mon Nov  7 15:37:07 2005
From: abigi at agrsci.unibo.it (Alessandro Bigi)
Date: Mon, 07 Nov 2005 15:37:07 +0100
Subject: [R] how to export density_function output?
Message-ID: <6.0.0.22.0.20051107152807.01c81270@pop.agrsci.unibo.it>

Dear all,

quite a naive question: I have a data frame and I computed "the kernel 
density estimate" with density on each column.
Now I'd like to export in a txt file the density function output for each 
column, but, when if I use write.table, I get a message "Error in 
as.data.frame.default(x[[i]], optional = TRUE) : can't coerce class 
"density" into a data.frame"
How should I do?
Thanks,

	Alessandro


-- 







--



From wsetzer at mindspring.com  Mon Nov  7 15:42:11 2005
From: wsetzer at mindspring.com (Woodrow Setzer)
Date: Mon, 7 Nov 2005 09:42:11 -0500 (GMT-05:00)
Subject: [R] Problem defining a system of odes as a C library with lsoda
Message-ID: <25259439.1131374531422.JavaMail.root@mswamui-bichon.atl.sa.earthlink.net>

I think the problem is in odesolve (something I thought I'd already fixed).  I don't have a 64-bit system to test on, so could you try these changes and let me know (offline) if they fix the problem?  There is a type mismatch in odesolve; I want to know if that is the cause of your problem.  In any case, I'll get an updated version of odesolve on CRAN ASAP.

in mymod.c:
change the line 
mymod(void(* odeparms)(int *, double *))
to
mymod(void(* odeparms)(long int *, double *))

and the line
int N=3;
to 
long int N=3;

Woody

-----Original Message-----
From: Dylan Childs <D.Childs at sheffield.ac.uk>
Sent: Nov 6, 2005 2:39 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Problem defining a system of odes as a C library with lsoda

I have been trying to make use of the odesolve library on my 
university's Linux grid - currently R version 2.0.1 is installed and 
the system runs 64-bit Scientific Linux based on Redhat.  

... [deleted]

However, the call to 
lsoda fails with the following error:

Error in lsoda(c(1, 0, 0), times, "myderivs", parms, rtol = 1e-04, atol 
= my.atol,  :
	Confusion over the length of parms

... [deleted]

Dr. Dylan Z. Childs
Department of Animal and Plant Sciences,
University of Sheffield,
Sheffield,
S10 2TN, UK.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Woodrow Setzer
National Center for Computational Toxicology
US Environmental Protection Agency
Research Triangle Park, NC 27711



From murdoch at stats.uwo.ca  Mon Nov  7 15:48:36 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 07 Nov 2005 09:48:36 -0500
Subject: [R] how to export density_function output?
In-Reply-To: <6.0.0.22.0.20051107152807.01c81270@pop.agrsci.unibo.it>
References: <6.0.0.22.0.20051107152807.01c81270@pop.agrsci.unibo.it>
Message-ID: <436F6944.9090609@stats.uwo.ca>

Alessandro Bigi wrote:
> Dear all,
> 
> quite a naive question: I have a data frame and I computed "the kernel 
> density estimate" with density on each column.
> Now I'd like to export in a txt file the density function output for each 
> column, but, when if I use write.table, I get a message "Error in 
> as.data.frame.default(x[[i]], optional = TRUE) : can't coerce class 
> "density" into a data.frame"
> How should I do?

When you get errors like that, it's a good idea to look inside the 
object to see what's there.  For example,

 > d <- density(precip, bw = bw)
 > str(d)
List of 7
  $ x        : num [1:512] -4.82 -4.65 -4.49 -4.32 -4.16 ...
  $ y        : num [1:512] 4.79e-05 5.46e-05 6.19e-05 7.00e-05 7.91e-05 ...
  $ bw       : num 3.94
  $ n        : int 70
  $ call     : language density.default(x = precip, bw = bw)
  $ data.name: chr "precip"
  $ has.na   : logi FALSE
  - attr(*, "class")= chr "density"

So the result is a list with x and y components giving the density, plus 
other components that describe what is there.  You probably only want 
the y components from various fits.  You can extract them as

results <- data.frame(precip = d$y, ...)

(where the ... has you extracting densities from other objects).

Duncan Murdoch



From u08adh at hotmail.com  Mon Nov  7 00:05:05 2005
From: u08adh at hotmail.com (Andreas Hary)
Date: Sun, 6 Nov 2005 23:05:05 -0000
Subject: [R] solving a complicated equation
References: <20051106224717.43489.qmail@web51813.mail.yahoo.com>
Message-ID: <BAY103-DAV13216A8B9208145DAE7147DF620@phx.gbl>

Try something like this:

> g <- function(x) return(p-a*exp(-x^2/2)+b*pnorm(x,0,1,lower.tail=F))
> p <- -0.5
> a <- 1
> b <- 1

> uniroot(g,c(-100,10))
$root
[1] -1.336607

$f.root
[1] 4.738e-06

$iter
[1] 11

$estim.prec
[1] 6.103516e-05


Regards,

Andreas






----- Original Message ----- 
From: "Cunningham Kerry" <kerryrekky at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Sunday, November 06, 2005 10:47 PM
Subject: [R] solving a complicated equation


>I want to solve the following equation for x
>
> p=a*exp(-x^2/2)+b*P(Z>x)
>
> where p,a,b are known, Z is a standard normal
> variable. Clearly there is no analytic form for
> P(Z>x).
>
> I am wondering if any expert could direct one easy way
> on this. Thank you.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From syy2004 at gmail.com  Sat Nov  5 20:28:42 2005
From: syy2004 at gmail.com (Yuying Shi)
Date: Sat, 5 Nov 2005 14:28:42 -0500
Subject: [R] solve the quadratic equation ax^2+bx+c=0
Message-ID: <91d269c60511051128r4b502115tdb0575a579df4e73@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051105/b23592f5/attachment.pl

From Dominik.Sydler at eawag.ch  Mon Nov  7 16:09:37 2005
From: Dominik.Sydler at eawag.ch (Sydler, Dominik)
Date: Mon, 7 Nov 2005 16:09:37 +0100
Subject: [R] Time-measurement in milliseconds
Message-ID: <59484F5CC089B4499DDAC9503DA0BC6706A4FE@EA-MAIL.eawag.wroot.emp-eaw.ch>

Hi there

I'm loking for a time-measurement to measure time-differences in
milliseconds.
On my search, I only found the following:
 - package "base": Sys.time()
    -> only second-accuracy
 - package "R.utils": System$currentTimeMillis()
    -> returns integer of milliseconds, but accuracy is only whole
seconds too.
At the moment I run every bit of code to measure 1000-times to be able
to calculate time in milliseconds... ;-)

Has anyone a method to get milliseconds?

Thanks for any help.



From dlvanbrunt at gmail.com  Mon Nov  7 16:09:47 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Mon, 7 Nov 2005 10:09:47 -0500
Subject: [R] R seems to "stall" after several hours on a long series of
	analyses... where to start?
In-Reply-To: <644e1f320511070557n4e20b03ei11dae22140c7b93@mail.gmail.com>
References: <d332d3e10511070316j7028fffdw14bbfc94636dfdf2@mail.gmail.com>
	<644e1f320511070557n4e20b03ei11dae22140c7b93@mail.gmail.com>
Message-ID: <d332d3e10511070709k5eb74dd2l5222aef52288ef90@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051107/5373b530/attachment.pl

From mmiller at nassp.uct.ac.za  Mon Nov  7 16:12:07 2005
From: mmiller at nassp.uct.ac.za (Mark Miller)
Date: Mon, 7 Nov 2005 17:12:07 +0200
Subject: [R] Scale of  plots
Message-ID: <200511071712.08134.mmiller@nassp.uct.ac.za>

I would like to know how I can change the axis to run from 0 to 50 say, 
instead of the 0 to 200 it gives me.

Many thanks
Mark Miller



From dieter.menne at menne-biomed.de  Mon Nov  7 16:08:02 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 7 Nov 2005 15:08:02 +0000 (UTC)
Subject: [R] =?utf-8?q?how_to_export_density=5Ffunction_output=3F?=
References: <6.0.0.22.0.20051107152807.01c81270@pop.agrsci.unibo.it>
Message-ID: <loom.20051107T160752-62@post.gmane.org>

Alessandro Bigi <abigi <at> agrsci.unibo.it> writes:

> 
> Dear all,
> 
> quite a naive question: I have a data frame and I computed "the kernel 
> density estimate" with density on each column.
> Now I'd like to export in a txt file the density function output for each 
> column, but, when if I use write.table, I get a message "Error in 
> as.data.frame.default(x[[i]], optional = TRUE) : can't coerce class 
> "density" into a data.frame"

dens = density(c(-20,rep(0,98),20))
str(dens) # If in doubt, write str()
#List of 7
# $ x        : num [1:512] -23.1 -23.0 -22.9 -22.8 -22.7 ...
# $ y        : num [1:512] 4.46e-05 5.77e-05 7.40e-05 9.45e-05 1.20e-04 ...
# $ bw       : num 1.02
# $ n        : int 100
# $ call     : language density.default(x = c(-20, rep(0, 98), 20))
# $ data.name: chr "c(-20, rep(0, 98), 20)"
# $ has.na   : logi FALSE
# - attr(*, "class")= chr "density"
# You probably need x and y
write.table(cbind(dens$x,dens$y),file="dens.txt")



From tlumley at u.washington.edu  Mon Nov  7 16:20:30 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 7 Nov 2005 07:20:30 -0800 (PST)
Subject: [R] R seems to "stall" after several hours on a long series o f
 analyses... where to start?
In-Reply-To: <1A4AC4BAB9C50A42854582B69B08C0340949B845@MSGMARBIO05>
References: <1A4AC4BAB9C50A42854582B69B08C0340949B845@MSGMARBIO05>
Message-ID: <Pine.LNX.4.63a.0511070719290.7562@homer22.u.washington.edu>

On Mon, 7 Nov 2005, Fowler, Mark wrote:

> You can test if the problem is accumulation in memory registers, which is
> certainly what this sounds like. Just do a loop over a reasonably small
> number of iterations and store or print the time between each iteration.

If you are worried about memory accumulation it would make more sense to 
print memory use statistics after each iteration rather than the time (eg 
gc(,reset=TRUE))

 	-thomas



> If
> memory accumulation it will run optimally for the first few iterations,
> after which the time will increase noticeably (essentially exponentially,
> hence ultimately freezes up). If this is the problem you may need to switch
> to a For loop approach, or effect the loop as a DOS script to do each
> iteration as a batch job (open/close R each iteration, old bytes can't
> clutter the memory).
>
>
>> 	Mark Fowler
> 		Population Ecology Division
>> 	Bedford Inst of Oceanography
>> 	Dept Fisheries & Oceans
>> 	Dartmouth NS Canada
> 		B2Y 4A2
> 		Tel. (902) 426-3529
> 		Fax (902) 426-9710
> 		Email fowlerm at mar.dfo-mpo.gc.ca
> 		Home Tel. (902) 461-0708
> 		Home Email mark.fowler at ns.sympatico.ca
>
>
> -----Original Message-----
> From: David L. Van Brunt, Ph.D. [mailto:dlvanbrunt at gmail.com]
> Sent: November 7, 2005 7:16 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R seems to "stall" after several hours on a long series of
> analyses... where to start?
>
> Not sure where to even start on this.... I'm hoping there's some debugging I
> can do...
>
> I have a loop that cycles through several different data sets (same
> structure, different info), performing randomForest growth and
> predictions... saving out the predictions for later study...
>
> I get about 5 hours in (9%... of the planned iterations.. yikes!) and R just
> freezes.
>
> This happens in interactive and batch mode execution (I can see from the
> ".Rout" file that it gets about 9% through in Batch mode, and about 6% if in
> interactive mode... does that suggest memory problems?)
>
> I'm thinking of re-executing this same code on a different platform to see
> if that's the issue (currently using OS X)... any other suggestions on where
> to look, or what to try to get more information?
>
> Sorry so vague... it's a LOT of code, runs fine without error for many
> iterations, so I didn't think the problem was syntax...
>
> --
> ---------------------------------------
> David L. Van Brunt, Ph.D.
> mailto:dlvanbrunt at gmail.com
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Mon Nov  7 16:21:59 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 7 Nov 2005 07:21:59 -0800 (PST)
Subject: [R] Use of paste with apply()
In-Reply-To: <436E0658.5030106@redcotel.bo>
References: <436E0658.5030106@redcotel.bo>
Message-ID: <Pine.LNX.4.63a.0511070721030.7562@homer22.u.washington.edu>

On Sun, 6 Nov 2005, Kjetil Brinchmann halvorsen wrote:
<examples snipped>
> Why doesn't paste behave in apply as sum?
>

Because sum maps a vector of inputs to a single output, but paste does 
not, unless you use collapse=

 	-thomas



From Gary.Nelson at state.ma.us  Mon Nov  7 16:39:15 2005
From: Gary.Nelson at state.ma.us (Nelson, Gary (FWE))
Date: Mon, 7 Nov 2005 10:39:15 -0500
Subject: [R] Help with downloading clim.pact
Message-ID: <74BDE31AFD6EC54DB026E6CD11FF0A7E9651BD@ES-MSG-008.es.govt.state.ma.us>


I am wondering if anyone might know why the package clim.pact won't
install properly.  I have tried many URL sites and the same thing
happens. I get the error message below.  I also tried downloading the
ZIP from the CRAN site and extracting the file myself, but an error
message (something like, "not an archive file") appears.  I operate
through Windows.

Thanks.

Gary Nelson.

******************************
trying URL
'http://cran.cnr.Berkeley.edu/bin/windows/contrib/2.2/clim.pact_2.2-0.zi
p'
Content type 'application/zip' length 5857349 bytes
opened URL
downloaded 2525Kb

Error in gzfile(file, "r") : unable to open connection
In addition: Warning messages:
1: downloaded length 2586064 != reported length 5857349 
2: error 1 in extracting from zip file 
3: cannot open compressed file 'clim.pact/DESCRIPTION' 

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.0            
year     2005           
month    10             
day      06             
svn rev  35749          
language R         


************************************************************************
*
Gary A. Nelson, Ph.D
Massachusetts Division of Marine Fisheries
30 Emerson Avenue
Gloucester, MA 01930
Phone: (978) 282-0308 x114
Fax: (617) 727-3337
Email: Gary.Nelson at state.ma.us



From andylehnert at gmx.de  Mon Nov  7 16:53:18 2005
From: andylehnert at gmx.de (Andreas Lehnert)
Date: Mon, 7 Nov 2005 16:53:18 +0100 (MET)
Subject: [R] someone knows how to title a image.plot() in a layout?
Message-ID: <21208.1131378798@www29.gmx.net>


Hello R,

I tried to get 4 image.plot() in one layout.
But when I try to label it with "main" or "sub" there is an error.
What am I doing wrong?

Please help

Thanks, Andy

--



From jfox at mcmaster.ca  Mon Nov  7 17:06:31 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 7 Nov 2005 11:06:31 -0500
Subject: [R] OLS variables
In-Reply-To: <Pine.LNX.4.61.0511070744300.7471@gannet.stats>
Message-ID: <20051107160629.MFGD28424.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Brian,

I don't have a strong opinion, but R's interpretation seems more consistent
to me, and as Kjetil points out, one can use polym() to specify a
full-polynomial model. It occurs to me that ^ and ** could be differentiated
in model formulae to provide both.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Monday, November 07, 2005 4:05 AM
> To: Kjetil Brinchmann halvorsen
> Cc: John Fox; r-help at stat.math.ethz.ch
> Subject: Re: [R] OLS variables
> 
> On Sun, 6 Nov 2005, Kjetil Brinchmann halvorsen wrote:
> 
> > John Fox wrote:
> >>
> >> I assume that you're using lm() to fit the model, and that 
> you don't 
> >> really want *all* of the interactions among 20 predictors: 
> You'd need 
> >> quite a lot of data to fit a model with 2^20 terms in it, 
> and might 
> >> have trouble interpreting the results.
> >>
> >> If you know which interactions you're looking for, then why not 
> >> specify them directly, as in lm(y ~  x1*x2 + x3*x4*x5 + 
> etc.)? On the 
> >> other hand, it you want to include all interactions, say, up to 
> >> three-way, and you've put the variables in a data frame, 
> then lm(y ~ .^3, data=DataFrame) will do it.
> >
> > This is nice with factors, but with continuous variables, 
> and need of 
> > a response-surface type, of model, will not do. For instance, with 
> > variables x, y, z in data frame dat
> >    lm( y ~ (x+z)^2, data=dat )
> > gives a model mwith the terms x, z and x*z, not the square terms.
> > There is a need for a semi-automatic way to get these, for 
> instance, 
> > use poly() or polym() as in:
> >
> > lm(y ~ polym(x,z,degree=2), data=dat)
> 
> This is an R-S difference (FAQ 3.3.2).  R's formula parser 
> always takes
> x^2 = x whereas the S one does so only for factors.  This 
> makes sense it you interpret `interaction' strictly as in 
> John's description - S chose to see an interaction of any two 
> continuous variables as multiplication (something which 
> puzzled me when I first encountered it, as it was not well 
> documented back in 1991).
> 
> I have often wondered if this difference was thought to be an 
> improvement, or if it just a different implementation of the 
> Rogers-Wilkinson syntax.
> Should we consider changing it?
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Nov  7 17:13:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Nov 2005 16:13:45 +0000 (GMT)
Subject: [R] OLS variables
In-Reply-To: <20051107160629.MFGD28424.tomts25-srv.bellnexxia.net@JohnDesktop8300>
References: <20051107160629.MFGD28424.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.LNX.4.61.0511071609480.24171@gannet.stats>

On Mon, 7 Nov 2005, John Fox wrote:

> Dear Brian,
>
> I don't have a strong opinion, but R's interpretation seems more consistent
> to me, and as Kjetil points out, one can use polym() to specify a
> full-polynomial model. It occurs to me that ^ and ** could be differentiated
> in model formulae to provide both.

However, poly[m] only provide orthogonal polynomials, and I have from time 
to time considered extending them to provide raw polynomials too.
Is that a better-supported idea?

>
> Regards,
> John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
>
>> -----Original Message-----
>> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>> Sent: Monday, November 07, 2005 4:05 AM
>> To: Kjetil Brinchmann halvorsen
>> Cc: John Fox; r-help at stat.math.ethz.ch
>> Subject: Re: [R] OLS variables
>>
>> On Sun, 6 Nov 2005, Kjetil Brinchmann halvorsen wrote:
>>
>>> John Fox wrote:
>>>>
>>>> I assume that you're using lm() to fit the model, and that
>> you don't
>>>> really want *all* of the interactions among 20 predictors:
>> You'd need
>>>> quite a lot of data to fit a model with 2^20 terms in it,
>> and might
>>>> have trouble interpreting the results.
>>>>
>>>> If you know which interactions you're looking for, then why not
>>>> specify them directly, as in lm(y ~  x1*x2 + x3*x4*x5 +
>> etc.)? On the
>>>> other hand, it you want to include all interactions, say, up to
>>>> three-way, and you've put the variables in a data frame,
>> then lm(y ~ .^3, data=DataFrame) will do it.
>>>
>>> This is nice with factors, but with continuous variables,
>> and need of
>>> a response-surface type, of model, will not do. For instance, with
>>> variables x, y, z in data frame dat
>>>    lm( y ~ (x+z)^2, data=dat )
>>> gives a model mwith the terms x, z and x*z, not the square terms.
>>> There is a need for a semi-automatic way to get these, for
>> instance,
>>> use poly() or polym() as in:
>>>
>>> lm(y ~ polym(x,z,degree=2), data=dat)
>>
>> This is an R-S difference (FAQ 3.3.2).  R's formula parser
>> always takes
>> x^2 = x whereas the S one does so only for factors.  This
>> makes sense it you interpret `interaction' strictly as in
>> John's description - S chose to see an interaction of any two
>> continuous variables as multiplication (something which
>> puzzled me when I first encountered it, as it was not well
>> documented back in 1991).
>>
>> I have often wondered if this difference was thought to be an
>> improvement, or if it just a different implementation of the
>> Rogers-Wilkinson syntax.
>> Should we consider changing it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From matthew_wiener at merck.com  Mon Nov  7 17:35:56 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Mon, 7 Nov 2005 11:35:56 -0500
Subject: [R] Scale of  plots
Message-ID: <4E9A692D8755DF478B56A2892388EE1F2F5F3D@usctmx1118.merck.com>

See "xlim" and "ylim" in the documentation on the plot command.
Hope this helps,

Matt Wiener
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mark Miller
Sent: Monday, November 07, 2005 10:12 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Scale of plots


I would like to know how I can change the axis to run from 0 to 50 say, 
instead of the 0 to 200 it gives me.

Many thanks
Mark Miller

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Mon Nov  7 17:37:29 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 7 Nov 2005 08:37:29 -0800
Subject: [R] Scale of  plots
In-Reply-To: <200511071712.08134.mmiller@nassp.uct.ac.za>
Message-ID: <200511071637.jA7GbTWh010265@ohm.gene.com>

Please read the docs! ?plot  --> ?plot.default --> ylim

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mark Miller
> Sent: Monday, November 07, 2005 7:12 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Scale of plots
> 
> I would like to know how I can change the axis to run from 0 
> to 50 say, 
> instead of the 0 to 200 it gives me.
> 
> Many thanks
> Mark Miller
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Mon Nov  7 17:49:21 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 07 Nov 2005 08:49:21 -0800
Subject: [R] nlme error message
In-Reply-To: <web-11223574@mail3.rug.nl>
References: <web-11223574@mail3.rug.nl>
Message-ID: <436F8591.7050709@pdf.com>

p.s.  You may also find useful the process I followed to diagnose this 
problem.

	  1.  I copied your example into R and confirmed that I could replicate 
the error.

	  2.  I read the documentation, invoked debug, and tried different 
things to isolate the problem.  For example, I listed the code for 
corCompSymm.  The documentation led me to "Initialize(cs)", which gave 
me the same error message.

	  3.  Ultimately, I found in Pinhiero and Bates (2000) Mixed-Effects 
Models for S and S-Plus (Springer) an example that looked exactly like 
yours but did NOT produce the same error message to "Initialize".  By 
comparing the example that worked with the superficially identical case 
that didn't, I found the difference.

	  hope this helps.
	  spencer graves

#######################
	  You need repeated measures for a random effect to make any sense.  I
modified your example as follows, and the error went away.

> mytable$RIL2 <- rep(1:4, 1:4)
> cs2 <- corCompSymm(value=0.5, form=~1|RIL2)
> model2<-lme(mytrait~myloc, data=mytable, random=~1|RIL2,
+             correlation=cs2)

	  (I've made similar mistakes and had great difficulty finding the
problem.)

	  spencer graves

J.Fu wrote:
> Dear Friends,
> 
> I am seeking for any help on an error message in lme 
> functions. I use mixed model to analyze a data with 
> compound symmetric correlation structure. But I get an 
> error message: "Error in corMatrix.corCompSymm(object) : 
> NA/NaN/Inf in foreign function call (arg 1)". If I change 
> the correlation structure to corAR1, then no error. I have 
> no clue how to solve this problem. I would highly 
> appreciate any help.
> Thanks in advance and looking forward to any help.
> 
> JY
> 
> 
> I attached my data and codes here:
> 
> # data: mytable
>           mytrait myloc RIL
> A1 0.590950330 0 1
> A2 -0.315469846 -1 2
> A3 -0.265690115 0 3
> A4 0.342885046 0 4
> A5 0.007613402 1 5
> A6 0.285997884 0 6
> A7 0.333841975 0 7
> A8 -0.599817735 -1 8
> A9 0.242621036 0 9
> A10 0.518959588 1 10
> 
> cs<-corCompSymm(0.5, form=~1|RIL, fixed=T)
> model<-lme(mytrait~myloc, data=mytable, random=~1|RIL, 
> na.action=na.omit, correlation=cs)
> 
> Error in corMatrix.corCompSymm(object) : NA/NaN/Inf in 
> foreign function call (arg 1)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From efg at stowers-institute.org  Mon Nov  7 18:19:54 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Mon, 7 Nov 2005 11:19:54 -0600
Subject: [R] R (2.2.0), R-DCOM and Delphi
References: <436F443F.2010900@terra.com.br>
	<loom.20051107T143033-989@post.gmane.org>
Message-ID: <dko2br$op6$1@sea.gmane.org>

"Dieter Menne" <dieter.menne at menne-biomed.de> wrote in message
news:<loom.20051107T143033-989 at post.gmane.org>...
> Jose Claudio Faria <joseclaudio.faria <at> terra.com.br> writes:

> Looks like a missing file in "uses". I compiled it under Delphi 5, and it
looks
> like the variants are no longer included in my "uses".
>
> Try to find out via help file where VarType is declared, add it to "uses".
And
> please tell me about the result when you got it compiled, I will add your
> finding to the archive.

Here's what worked for me:

(1) Install the new (D)COM server (the old one gives the error "Method '~'
of object '~' failed" with R 2.2.0):

     For anybody who wants to try the R(D)COM server alone, I recommend
     http://sunsite.univie.ac.at/rcom/download/RSrv200beta.exe
     It is reasonably stable and will work with R 2.2.0.


(2) In C:\Program Files\R\(D)COM Server\samples\Simple run the simple.exe to
verify the server is working.


(3) Download and unzip:  http://www.menne-biomed.de/download/RDComDelphi.zip

Like Jose Claudio Faria described, a number of warnings and errors will be
seen in Delphi 7 in compiling the unmodified RDCom.dpr project.

To fix the compilation errors, add the "Variants" unit to the uses:

uses
  Windows, Messages, SysUtils,Classes, Dialogs,
STATCONNECTORSRVLib_TLB,Forms, Variants;

I don't remember why Borland made this change.

You can make the Warnings go away by selecting Project | Options | Compiler
Messages and unchecking the boxes for

- Unsafe type
- Unsafe code

These warning were introduced with Borland made changes to support .NET.
The code should be quite fine for Win32.

Note:  The Delphi source code could be modified to use {$IFDEF} directives
to take care of the differences by compiler version.  See the "Compiler
Versions" section of this page:
http://www.efg2.com/Lab/Library/Delphi/Miscellany/index.html.
Unfortunately, Borland has not been very helpful in promoting the use of a
common "Versions.INC" file or any other mechanism to take care of such
compiler version problems in a uniform way.  The file "Versions.INC" must be
updated with every new release by Borland.


(4) Run the RDCom.exe program. Select the "Run" and "Plot" buttons and see a
wonderful demo of Delphi and R working together.  (Now I can get busy and
use this in other Delphi applications with R as a "backend".)


Thanks for the work on R-DCOM and this great example!

efg
Scientific Programmer
Stowers Institute for Medical Research



From ripley at stats.ox.ac.uk  Mon Nov  7 18:27:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Nov 2005 17:27:10 +0000 (GMT)
Subject: [R] Time-measurement in milliseconds
In-Reply-To: <59484F5CC089B4499DDAC9503DA0BC6706A4FE@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <59484F5CC089B4499DDAC9503DA0BC6706A4FE@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <Pine.LNX.4.61.0511071726390.5518@gannet.stats>

Be aware that the measurement will itself take more than a millisecond
from an interpreted language like R.

Please see the help page for Sys.time, and its suggestion of proc.time.
for(i in 1:100) print(proc.time()[3]) suggests this takes 3ms on my box.

On Mon, 7 Nov 2005, Sydler, Dominik wrote:

> Hi there
>
> I'm loking for a time-measurement to measure time-differences in
> milliseconds.
> On my search, I only found the following:
> - package "base": Sys.time()
>    -> only second-accuracy
> - package "R.utils": System$currentTimeMillis()
>    -> returns integer of milliseconds, but accuracy is only whole
> seconds too.
> At the moment I run every bit of code to measure 1000-times to be able
> to calculate time in milliseconds... ;-)
>
> Has anyone a method to get milliseconds?
>
> Thanks for any help.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From buser at stat.math.ethz.ch  Mon Nov  7 18:34:22 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 7 Nov 2005 18:34:22 +0100
Subject: [R] solve the quadratic equation ax^2+bx+c=0
In-Reply-To: <91d269c60511051128r4b502115tdb0575a579df4e73@mail.gmail.com>
References: <91d269c60511051128r4b502115tdb0575a579df4e73@mail.gmail.com>
Message-ID: <17263.36894.22896.259169@stat.math.ethz.ch>

Hi

Have a look at ?polyroot. This might be helpful.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Yuying Shi writes:
 > If I have matrics as follows:
 > > a <- c(1,1,0,0)
 > > b <- c(4,4,0,0)
 > > c <- c(3,5,5,6)
 > How can I use R code to solve the equation ax^2+bx+c=0.
 > thanks!
 >  yuying shi
 > 
 > 	[[alternative HTML version deleted]]
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Nov  7 18:35:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Nov 2005 17:35:05 +0000 (GMT)
Subject: [R] Help with downloading clim.pact
In-Reply-To: <74BDE31AFD6EC54DB026E6CD11FF0A7E9651BD@ES-MSG-008.es.govt.state.ma.us>
References: <74BDE31AFD6EC54DB026E6CD11FF0A7E9651BD@ES-MSG-008.es.govt.state.ma.us>
Message-ID: <Pine.LNX.4.61.0511071730480.5518@gannet.stats>

On Mon, 7 Nov 2005, Nelson, Gary (FWE) wrote:

> I am wondering if anyone might know why the package clim.pact won't
> install properly.  I have tried many URL sites and the same thing
> happens. I get the error message below.  I also tried downloading the
> ZIP from the CRAN site and extracting the file myself, but an error
> message (something like, "not an archive file") appears.  I operate
> through Windows.

Perhaps you are hitting a download limit?  This works for me from the 
master site.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ManuelPerera-Chang at fmc-ag.com  Mon Nov  7 18:36:39 2005
From: ManuelPerera-Chang at fmc-ag.com (ManuelPerera-Chang@fmc-ag.com)
Date: Mon, 7 Nov 2005 18:36:39 +0100
Subject: [R] lattice chart: different definitions for series
Message-ID: <OF918AF1D7.C02B4AE4-ONC12570B2.005C6944-C12570B2.0060BE43@notes.fresenius.de>





Hi enthusiasts,

Trying to create a single chart in  lattice with different plotting
definitions for the different series (two series should be drawn with lines
and the other without them)

I am using a dataset, which includes a grouping variable e.g. clinic with
three levels, the variable "year" and a continous variable: "mct".

In the graph the variable "year" is in the x axis, with "mct" represented
in the y axis.

The diagram should include two line diagrams(representing two of the
groups) , with the third group represented only with symbols(no lines).

Until now I was using white lines to eliminate the lines drawn in the third
group, but this solution is not optimal, as the grids are sometimes not
visible

sp<-list(superpose.symbol=list(pch=c(1,2,1),col=c("blue","red","green")),
       superpose.line=list(col=c("blue","red","white"),lty=c(1,2,)))

... and then including

print(xyplot(mct~trend.data$year,groups=clinic,
  scales=list(x=list(at=c(15:pno),labels=per.labels)),
  main=main.title,
  sub=sub.title,
  xlab=x.label,
  ylab=y.label,
  xlim=c(pno-12,pno+1),
  panel=function(x,y,...){panel.grid(h=-1,v=-1,col="grey",lty=2,cex=0.1);
                  panel.superpose(x,y,type="l",lwd=1.8,...);
                  panel.superpose(x,y,type="p",cex=1.8,...))},
  key=sk,
  par.settings=sp));

... was also experimenting, and searching a lot in the WWW for

panel.superpose.2 and type=c("b","b","p"), but without success.

Many thanks,

Manuel



From gerifalte28 at hotmail.com  Mon Nov  7 19:08:01 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 07 Nov 2005 18:08:01 +0000
Subject: [R] someone knows how to title a image.plot() in a layout?
In-Reply-To: <21208.1131378798@www29.gmx.net>
Message-ID: <BAY103-F14D1406CD4F9D074C97EB2A6650@phx.gbl>

What error are you getting?  What did you try?  If you want to get 
meaningful help you need to provide examples of what you did and didn't 
work.

Read the bottom of the message that you just sent and you will notice that 
we ask you to read the posting guide....

Cheers

Francisco

>From: "Andreas Lehnert" <andylehnert at gmx.de>
>To: r-help at stat.math.ethz.ch
>Subject: [R] someone knows how to title a image.plot() in a layout?
>Date: Mon, 7 Nov 2005 16:53:18 +0100 (MET)
>
>
>Hello R,
>
>I tried to get 4 image.plot() in one layout.
>But when I try to label it with "main" or "sub" there is an error.
>What am I doing wrong?
>
>Please help
>
>Thanks, Andy
>
>--
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From secchi at sssup.it  Mon Nov  7 19:10:32 2005
From: secchi at sssup.it (Angelo Secchi)
Date: Mon, 7 Nov 2005 19:10:32 +0100
Subject: [R] Newbie on functions
Message-ID: <20051107191032.8ad37063.secchi@sssup.it>


Hi,
I'm trying to write a simple function like

case1 <- function (m, cov, Q, R) {
  theta     <- (acos(R/sqrt(Q^3)))
  beta      <- (-2)*sqrt(Q)*cos(theta/3)+m[1]/3
  rho1      <- (-2)*sqrt(Q)*cos((theta+2*pi)/3)+m[1]/3
  rho2     <- (-2)*sqrt(Q)*cos((theta-2*pi)/3)+m[1]/3
  stderrb   <-  deltamethod( ~(-2)*sqrt(Q)*cos(theta/3)+x1/3,m,cov)
  stderrr1  <-  deltamethod( ~(-2)*sqrt(Q)*cos((theta+2*pi)/3)+x1/3, m,
cov) stderrr2  <-  deltamethod( ~(-2)*sqrt(Q)*cos((theta-2*pi)/3)+x1/3,
m, cov) stderr    <- c(stderrb,stderrr1,stderrr2)
  results   <- c(beta,rho1,rho2,stderr)
  results2  <- t(results)
  results2
}

When I call the function in an IF statement like

if (Q^3>R^2) results2 <- case1() else print('ciccio')

I get 

Error in eval(expr, envir, enclos) : Object "theta" not found

I do not understand why, any help?
Thanks


-- 
========================================================
 Angelo Secchi                     PGP Key ID:EA280337



From deepayan.sarkar at gmail.com  Mon Nov  7 19:52:37 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 7 Nov 2005 12:52:37 -0600
Subject: [R] levelplot and layout
In-Reply-To: <22830.1131360356@www73.gmx.net>
References: <22830.1131360356@www73.gmx.net>
Message-ID: <eb555e660511071052p4a21febfs36012e193685d5e2@mail.gmail.com>

On 11/7/05, Andreas Lehnert <andylehnert at gmx.de> wrote:
>
> Dear R,
>
> I know that I cannot use filled.contour() with layout,
> so I tried levelplot. But it looks like I cannot use levelplot also.

Have you looked at help(print.trellis) ?

> The only way to get more than one graphic in the window is
> by levelplot(panel=). but there is only one colorkey and I need one for
> every
> graphic.
> (I??m also not very satisfied with levelplot, because I would like to have
> a "soft" graphic like filled.contour)

That's not implemented.

Deepayan



From Olaf.Schenk at unibas.ch  Mon Nov  7 19:58:06 2005
From: Olaf.Schenk at unibas.ch (Olaf.Schenk@unibas.ch)
Date: Mon,  7 Nov 2005 19:58:06 +0100
Subject: [R] Is R thread safe?
In-Reply-To: <Pine.LNX.4.61.0511071726390.5518@gannet.stats>
References: <59484F5CC089B4499DDAC9503DA0BC6706A4FE@EA-MAIL.eawag.wroot.emp-eaw.ch>
	<Pine.LNX.4.61.0511071726390.5518@gannet.stats>
Message-ID: <1131389886.436fa3bee6877@webmail.unibas.ch>

Dear R-help,

I would like to accelerate my R computation by using parallel OpenMP compilers
(e.g from Pathscale) on a 2-processor AMD server and I would like to know
whether R is a tread safe library. The main kernel of the OpenMP
parallelization is a C SEXP function that performs the computational routine in
parallel with:

*******************
SEXP example(SEXP list, SEXP expr, SEXP rho)
     {
       R_len_t i, n = length(list);
       SEXP ans, alocal;

       omp_lock_t lck;
       PROTECT(ans = allocVector(VECSXP, n));
       ans = allocVector(VECSXP, n);
       omp_init_lock(&lck);
#pragma omp parallel for default(none) private(i, alocal) shared(list,
lck,rho, ans, n, expr)
       for(i = 0; i < n; i++) {

          omp_set_lock(&lck);
             PROTECT(alocal = allocVector(VECSXP, 1));
             alocal = allocVector(VECSXP, 1);
             defineVar(install("x"), VECTOR_ELT(list, i), rho);
          omp_unset_lock(&lck);

          /* do computational kernel in parallel */
          alocal = eval(expr, rho);

          omp_set_lock(&lck);
             SET_VECTOR_ELT(ans, i, alocal);
             UNPROTECT(1);
          omp_unset_lock(&lck);

       }
       setAttrib(ans, R_NamesSymbol, getAttrib(list, R_NamesSymbol));
       UNPROTECT(1);
       return(ans);
}

***********

The code works fine using one thread and breaks currently down with 2 threads. 
I am using a recent R distribution and  the complete R code is compile with
"-openmp" and the Pathscale compiler suite.

Thanks in advance,
Olaf



From Olaf.Schenk at unibas.ch  Mon Nov  7 20:08:00 2005
From: Olaf.Schenk at unibas.ch (Olaf.Schenk@unibas.ch)
Date: Mon,  7 Nov 2005 20:08:00 +0100
Subject: [R] Is R thread safe?
Message-ID: <1131390480.436fa610c431e@webmail.unibas.ch>

Dear R-help,

I would like to accelerate my R computation by using parallel OpenMP compilers
(e.g from Pathscale) on a 2-processor AMD server and I would like to know
whether R is a tread safe library. The main kernel of the OpenMP
parallelization is a C SEXP function that performs the computational routine in
parallel with:

*******************
SEXP example(SEXP list, SEXP expr, SEXP rho)
     {
       R_len_t i, n = length(list);
       SEXP ans, alocal;

       omp_lock_t lck;
       PROTECT(ans = allocVector(VECSXP, n));
       ans = allocVector(VECSXP, n);
       omp_init_lock(&lck);
#pragma omp parallel for default(none) private(i, alocal) shared(list,
lck,rho, ans, n, expr)
       for(i = 0; i < n; i++) {

          omp_set_lock(&lck);
             PROTECT(alocal = allocVector(VECSXP, 1));
             alocal = allocVector(VECSXP, 1);
             defineVar(install("x"), VECTOR_ELT(list, i), rho);
          omp_unset_lock(&lck);

          /* do computational kernel in parallel */
          alocal = eval(expr, rho);

          omp_set_lock(&lck);
             SET_VECTOR_ELT(ans, i, alocal);
             UNPROTECT(1);
          omp_unset_lock(&lck);

       }
       setAttrib(ans, R_NamesSymbol, getAttrib(list, R_NamesSymbol));
       UNPROTECT(1);
       return(ans);
}

***********

The code works fine using one thread and breaks currently down with 2 threads.
I am using a recent R distribution and  the complete R code is compile with
"-openmp" and the Pathscale compiler suite.

Thanks in advance,
Olaf



From ivo_welch at brown.edu  Mon Nov  7 20:16:44 2005
From: ivo_welch at brown.edu (ivo welch)
Date: Mon, 07 Nov 2005 14:16:44 -0500
Subject: [R] pdf device and TeXencoding?
Message-ID: <436FA81C.3030902@brown.edu>


Dear R wizards:

[a] I believe that the pdf device does not yet fully support TeXencoding.  (under R-2.2.0, the pdf file created with Textext as font encoding still dies when post-processed by ghostscript.)  are there any workarounds, or are there utilities that would allow a TeXencoded font to be re-encoded/converted into ISOLatin, perhaps, which R could then handle beautifully?

[b] is there a way to use an arbitrary postscript font and position it into a ps or pdf graphic (i.e., without it being in the family {5 fonts} that I am using for the main drawing)?  in a weird latex/R mix, my intent is something like

	pdf(file="test.pdf");
	plot( c(0,1),c(0,1) );
	test.font = fontis("postscriptfont.pfb");  # may need a tfm specification, too?
	text( 0.5, 0.5, fontobject("some text", test.font));
	dev.off();

help would be highly appreciated, as always.

sincerely,

/ivo welch



From patrick.giraudoux at univ-fcomte.fr  Mon Nov  7 20:27:30 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Mon, 07 Nov 2005 20:27:30 +0100
Subject: [R] repeated values, nlme, correlation structures
Message-ID: <436FAAA2.8090109@univ-fcomte.fr>

Dear listers,

As an exercise, I am trying to fit a logistic model with nlme.  Blue tit 
pulli (youngs) were weighted occasionnally (for field reasons) along 
time in 17 nestboxes. Individuals where not idenfied but their age was 
known. This means that for a given age several measurements were done 
but individuals could not be identified from a time to the other. This 
makes repeated values for a given age group in each nestbox. The aim is 
to get an acceptable growth curve (weight against age).

As far as repeated values cannot be handled with standard coStruct 
classes of nlme, I have done a first fit with nlme using the mean of 
each group. Comparing several models, the best fit is:

modm0c<-nlme(pds~Asym/(1+exp((xmid-age)/scal)),
    fixed=list(Asym~1,xmid~1,scal~1),
    random=Asym+xmid~1|nichoir,data=croispulm,
    start=list(fixed=c(10,5,2.2)),
    method="ML",
    corr=corCAR1()
    )

with pds = weight, age = mean age of each age group, nichoir = nestbox 
(a factor of 17 levels)

Based on the empirical autocorrelation function of the normalised 
residuals drawn from this model one can acceptaly assume that  the 
normalized residuals behave like uncorrelated noise.

Though this could be quite satisfying at first sight,  I am quite 
frustrated with starting from the mean weight of each age group, thus 
not being capable to manage and incorporate the variability around the 
mean weight of each age group in the model. My bible is Pinheiro & Bates 
(2000), but I did not find an example to board this problem.

Is there an affordable way (=not that much complicated for a biologist 
familiar to some general statistics) to handle this in nlme?

Any hint?

Patrick



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov  7 21:04:05 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 07 Nov 2005 20:04:05 -0000 (GMT)
Subject: [R] solve the quadratic equation ax^2+bx+c=0
In-Reply-To: <91d269c60511051128r4b502115tdb0575a579df4e73@mail.gmail.com>
Message-ID: <XFMail.051107200405.Ted.Harding@nessie.mcc.ac.uk>

On 05-Nov-05 Yuying Shi wrote:
> If I have matrics as follows:
>> a <- c(1,1,0,0)
>> b <- c(4,4,0,0)
>> c <- c(3,5,5,6)
> How can I use R code to solve the equation ax^2+bx+c=0.
> thanks!
>  yuying shi

Here is a solution, using the more interesting example in an
ealrier mail of yours:

  a b c
  1 4 3
  1 4 5
  0 2 5
  0 0 6

  qs<-function(a,b,c){
    a<-as.complex(a); b<-as.complex(b); c<-as.complex(c)
    i2<-(a!=0); i1<-((a==0)&(b!=0));
    solns<-as.complex(rep(NA,length(a)))
      solns<-cbind(solns,solns); colnames(solns)<-c("soln 1","soln 2")
    a2<-a[i2]; b2<-b[i2]; c2<-c[i2]
      solns[i2,1]<-(-b2 + sqrt(b2^2 - 4*a2*c2))/(2*a2)
      solns[i2,2]<-(-b2 - sqrt(b2^2 - 4*a2*c2))/(2*a2)
    b1<-b[i1]; c1<-c[i1]
      solns[i1,1]<-(-c1)/b1
    solns
  }

  a<-c(1,1,0,0); b<-c(4,4,2,0); c<-c(3,5,5,6)

  qs(a,b,c)
          soln 1 soln 2
    [1,] -1.0+0i -3+0i
    [2,] -2.0+1i -2-1i
    [3,] -2.5+0i     NA
    [4,]      NA     NA


Check that a*s^2 + b*s + c = 0:

  s1<-solns[,1]; s2<-solns[,2]

  diag(cbind(a,b,c)%*%rbind(s1^2,s1,c(1,1,1,1)))
    [1]  0.000000e+00-2.449213e-16i -8.881784e-16-1.776357e-15i
    [3]  0.000000e+00+0.000000e+00i                          NA

  diag(cbind(a,b,c)%*%rbind(s2^2,s2,c(1,1,1,1)))
    [1]  1.776357e-15-2.204291e-15i -8.881784e-16+1.776357e-15i
    [3]                          NA                          NA

which is as close to 0 as you can expect to get.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Nov-05                                       Time: 20:04:00
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov  7 21:14:09 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 07 Nov 2005 20:14:09 -0000 (GMT)
Subject: [R] Newbie on functions
In-Reply-To: <20051107191032.8ad37063.secchi@sssup.it>
Message-ID: <XFMail.051107201409.Ted.Harding@nessie.mcc.ac.uk>

On 07-Nov-05 Angelo Secchi wrote:
> 
> Hi,
> I'm trying to write a simple function like
> 
> case1 <- function (m, cov, Q, R) {
>   theta     <- (acos(R/sqrt(Q^3)))
>   beta      <- (-2)*sqrt(Q)*cos(theta/3)+m[1]/3
>   rho1      <- (-2)*sqrt(Q)*cos((theta+2*pi)/3)+m[1]/3
>   rho2     <- (-2)*sqrt(Q)*cos((theta-2*pi)/3)+m[1]/3
>   stderrb   <-  deltamethod( ~(-2)*sqrt(Q)*cos(theta/3)+x1/3,m,cov)
>   stderrr1  <-  deltamethod( ~(-2)*sqrt(Q)*cos((theta+2*pi)/3)+x1/3, m,
> cov) stderrr2  <-  deltamethod( ~(-2)*sqrt(Q)*cos((theta-2*pi)/3)+x1/3,
> m, cov) stderr    <- c(stderrb,stderrr1,stderrr2)
>   results   <- c(beta,rho1,rho2,stderr)
>   results2  <- t(results)
>   results2
> }
> 
> When I call the function in an IF statement like
> 
> if (Q^3>R^2) results2 <- case1() else print('ciccio')
> 
> I get 
> 
> Error in eval(expr, envir, enclos) : Object "theta" not found
> 
> I do not understand why, any help?

Because when the function 'case1' is called as "case1()" it
has no information on the values of m, cov, Q and R since
it only looks in its argument list for these values (and
not in the environment from which you called it).

It would be different if, in the function definition, you
had assigned default values for arguments not mentioned in
the function call, since it would then use these values
instead, for example

  case1 <- function (m=1, cov=1, Q=1, R=1) {
  ...
  }

but probably you do not want to do anything like that in this case.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Nov-05                                       Time: 20:14:07
------------------------------ XFMail ------------------------------



From quantpm at yahoo.com  Mon Nov  7 21:16:33 2005
From: quantpm at yahoo.com (t c)
Date: Mon, 7 Nov 2005 12:16:33 -0800 (PST)
Subject: [R] percent rank by an index key?
In-Reply-To: <4367A525.6080804@pdf.com>
Message-ID: <20051107201633.79752.qmail@web35009.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051107/bdd4294b/attachment.pl

From deepayan.sarkar at gmail.com  Mon Nov  7 21:23:32 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 7 Nov 2005 14:23:32 -0600
Subject: [R] lattice chart: different definitions for series
In-Reply-To: <OF918AF1D7.C02B4AE4-ONC12570B2.005C6944-C12570B2.0060BE43@notes.fresenius.de>
References: <OF918AF1D7.C02B4AE4-ONC12570B2.005C6944-C12570B2.0060BE43@notes.fresenius.de>
Message-ID: <eb555e660511071223iab918d8w57a927e2ff09f946@mail.gmail.com>

On 11/7/05, ManuelPerera-Chang at fmc-ag.com <ManuelPerera-Chang at fmc-ag.com> wrote:
>
>
>
>
> Hi enthusiasts,
>
> Trying to create a single chart in  lattice with different plotting
> definitions for the different series (two series should be drawn with lines
> and the other without them)
>
> I am using a dataset, which includes a grouping variable e.g. clinic with
> three levels, the variable "year" and a continous variable: "mct".
>
> In the graph the variable "year" is in the x axis, with "mct" represented
> in the y axis.
>
> The diagram should include two line diagrams(representing two of the
> groups) , with the third group represented only with symbols(no lines).
>
> Until now I was using white lines to eliminate the lines drawn in the third
> group, but this solution is not optimal, as the grids are sometimes not
> visible
>
> sp<-list(superpose.symbol=list(pch=c(1,2,1),col=c("blue","red","green")),
>        superpose.line=list(col=c("blue","red","white"),lty=c(1,2,)))
>
> ... and then including
>
> print(xyplot(mct~trend.data$year,groups=clinic,
>   scales=list(x=list(at=c(15:pno),labels=per.labels)),
>   main=main.title,
>   sub=sub.title,
>   xlab=x.label,
>   ylab=y.label,
>   xlim=c(pno-12,pno+1),
>   panel=function(x,y,...){panel.grid(h=-1,v=-1,col="grey",lty=2,cex=0.1);
>                   panel.superpose(x,y,type="l",lwd=1.8,...);
>                   panel.superpose(x,y,type="p",cex=1.8,...))},
>   key=sk,
>   par.settings=sp));
>
> ... was also experimenting, and searching a lot in the WWW for
>
> panel.superpose.2 and type=c("b","b","p"), but without success.

I don't know what experiments you did, but the following seems to work
fine for me:

library(lattice)

mydf <-
    data.frame(year = rep(1991:2000, 3),
               mct =
               rnorm(30,
                     mean = rep(1:3, each = 10),
                     sd = 0.5),
               clinic = gl(3, 10))

xyplot(mct ~ year, mydf, groups = clinic,
       panel = panel.superpose.2,
       type = c("b", "b", "p"))

-Deepayan



From dlvanbrunt at gmail.com  Mon Nov  7 21:49:36 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Mon, 7 Nov 2005 15:49:36 -0500
Subject: [R] R seems to "stall" after several hours on a long series of
	analyses... where to start?
In-Reply-To: <436F82AF.1010409@bank-banque-canada.ca>
References: <d332d3e10511070316j7028fffdw14bbfc94636dfdf2@mail.gmail.com>
	<644e1f320511070557n4e20b03ei11dae22140c7b93@mail.gmail.com>
	<d332d3e10511070709k5eb74dd2l5222aef52288ef90@mail.gmail.com>
	<436F82AF.1010409@bank-banque-canada.ca>
Message-ID: <d332d3e10511071249i4620e2cbx8b48e3776e301269@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051107/0e208d20/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Mon Nov  7 22:03:35 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 07 Nov 2005 21:03:35 -0000 (GMT)
Subject: [R] slow R start up
In-Reply-To: <XFMail.051107085721.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.051107210335.Ted.Harding@nessie.mcc.ac.uk>

On 07-Nov-05 Ted Harding wrote:
> 
> On 07-Nov-05 Gabor Grothendieck wrote:
>> Or you could call those other programs from within R.  See
>> ?system
> 
> A possibility which might work, and if so would work as you intend,
> is to use a FIFO ("named pipe") or two to communicate with R.
> 
> In R, see "?connections"; in Linux, see "man mkfifo".
> 
> Then you should be able to start R up so that it reads commands
> from the FIFO, (and optionally writes output to a FIFO which can
> be read by the shell script; though it might be simpler to write
> the output to a regular file).

Here is a bare and thin skeleton, which you may be able to develop.
You'll need two windows for this demo, one for R and one for the
Linux shell, both operating in the same directory.


In the R window, start up R


In the Linux window, execute

mkfifo -m 666 Rin


In R:

Rin<-fifo("Rin",open="T")
online<-TRUE
while(online){source(Rin)}


In Linux:

cat > Rin  << EOF
x<-0.1*(0:10)
plot(x,cos(2*pi*x))
EOF
sleep 10
cat > Rin  << EOF
x<-0.1*(0:20)
plot(x,sin(2*pi*x))
online<-FALSE
EOF

You can then "rm Rin" in Linux.


That is a little script which sends commends to R via the FIFO
to do two (slightly) different things, with a pause between them.

R reads the FIFO so long as "online" is TRUE, and then stops
reading when told that "online" is FALSE. Whenever it has emptied
the FIFO for the moment (as happens at the "sleep 10" in Linux)
it simply waits in the loop for more to come through.

You could add a line in R following the "while" to

close(Rin)

or to quit() so that R closes down at that point.

>From that beginning you should be able to develop scripts which
send arbitrary commands to R from Linux.

PS: This is my first venture into the FIFO world where using R
is concerned, so I may have under-exploited the possibilities
of R in this respect. It is simply based on how I've done it
with other software, and this is a very generic way to do it.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Nov-05                                       Time: 21:03:32
------------------------------ XFMail ------------------------------



From xiaofan.mlist at gmail.com  Mon Nov  7 22:07:26 2005
From: xiaofan.mlist at gmail.com (Xiaofan Li)
Date: Mon, 7 Nov 2005 21:07:26 -0000
Subject: [R] Deleting Rows/Columns
Message-ID: <436fc216.65e9de72.5bfb.ffffa1d6@mx.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051107/0d589f9c/attachment.pl

From maillist at roomity.com  Mon Nov  7 22:11:11 2005
From: maillist at roomity.com (shenanigans)
Date: Mon,  7 Nov 2005 13:11:11 -0800 (PST)
Subject: [R] [OTAnn] Feedback
Message-ID: <781844.521131397871147.JavaMail.tomcat5@slave1.roomity.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051107/9c4d3788/attachment.pl

From helprhelp at gmail.com  Mon Nov  7 22:26:21 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 7 Nov 2005 15:26:21 -0600
Subject: [R] Deleting Rows/Columns
In-Reply-To: <436fc216.65e9de72.5bfb.ffffa1d6@mx.gmail.com>
References: <436fc216.65e9de72.5bfb.ffffa1d6@mx.gmail.com>
Message-ID: <cdf817830511071326m7140d3eepea74847a394ed24c@mail.gmail.com>

put - before the column id you want to delete.

like this:
> a<-data.frame(c(1,2,3), c(4,5,6), c(7,8,9))
> a
  c.1..2..3. c.4..5..6. c.7..8..9.
1          1          4          7
2          2          5          8
3          3          6          9
> a[,-1]
  c.4..5..6. c.7..8..9.
1          4          7
2          5          8
3          6          9

HTH

On 11/7/05, Xiaofan Li <xiaofan.mlist at gmail.com> wrote:
> Sorry to bother the group but I am wondering if there are some official ways
> to delete a row/column, i.e., some functions of dataTable manipulation? For
> rows operation I use subset() but what about columns?
>
>
>
> Any advice is welcome and I will be more than grateful if somebody could
> make a summary on this issue.
>
>
>
> Xiaofan
>
>
>
> ---------------------------------------------------------
>
> Xiaofan Li
>
> Cambridge Computational Biology Institute
>
> Department of Applied Mathematics and Theoretical Physics
>
> University of Cambridge, CB3 0WA, United Kingdom
>
> Tel +44 7886 614030, Email xl252 at cam.ac.uk
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From ripley at stats.ox.ac.uk  Mon Nov  7 22:41:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Nov 2005 21:41:14 +0000 (GMT)
Subject: [R] R-2.2.0: aggregate problem
In-Reply-To: <1131371737.436f5cd980ed3@imp1-g19.free.fr>
References: <1131371737.436f5cd980ed3@imp1-g19.free.fr>
Message-ID: <Pine.LNX.4.61.0511071555320.24171@gannet.stats>

On Mon, 7 Nov 2005 jfontain at free.fr wrote:

>> aggregate(as.ts(c(1,2,3,4,5,6,7,8,9,10)),1/2,mean)
> Time Series:
> Start = 1
> End = 9
> Frequency = 0.5
> [1] 1.5 3.5 5.5 7.5 9.5
>> aggregate(as.ts(c(1,2,3,4,5,6,7,8,9,10)),1/5,mean)
> Error in sprintf(gettext(fmt, domain = domain), ...) :
> 	use format %f, %e or %g for numeric objects
>>
>
> I must be doing something wrong, but what?

It is rounding error: 1 is just under a multiple of 1/5 (in computer 
representation).  Try

aggregate(as.ts(c(1,2,3,4,5,6,7,8,9,10)),1/5.000000000000001,mean)

Clearly it needs an internal fix (and for the reporting error), and 
R-patched now has one.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Nov  7 22:44:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Nov 2005 21:44:58 +0000 (GMT)
Subject: [R] pdf device and TeXencoding?
In-Reply-To: <436FA81C.3030902@brown.edu>
References: <436FA81C.3030902@brown.edu>
Message-ID: <Pine.LNX.4.61.0511072120380.7607@gannet.stats>

On Mon, 7 Nov 2005, ivo welch wrote:

> [a] I believe that the pdf device does not yet fully support 
> TeXencoding.  (under R-2.2.0, the pdf file created with Textext as font 
> encoding still dies when post-processed by ghostscript.)  are there any 
> workarounds, or are there utilities that would allow a TeXencoded font 
> to be re-encoded/converted into ISOLatin, perhaps, which R could then 
> handle beautifully?

1) What does `dies' mean?
2) See the following R-patched NEWS entry

     o	pdf() was not writing details of the encoding to the file
 	correctly.  (Spotted by Alexey Shipunov in Russian encodings.)

so this may well be solved in current R (R-patched/R-devel).

> [b] is there a way to use an arbitrary postscript font and position it 
> into a ps or pdf graphic (i.e., without it being in the family {5 fonts} 
> that I am using for the main drawing)?  in a weird latex/R mix, my 
> intent is something like

Yes, see par(family=) and (in 2.2.x)  ?postscriptFont to create a family.

> 	pdf(file="test.pdf");
> 	plot( c(0,1),c(0,1) );
> 	test.font = fontis("postscriptfont.pfb");  # may need a tfm specification, too?
> 	text( 0.5, 0.5, fontobject("some text", test.font));
> 	dev.off();
>
> help would be highly appreciated, as always.

This area is all under development as we allow for CJK fonts.  In R-devel, 
something like

myfam <- Type1Font("test", rep("postscriptfont.afm"), 4)
pdf(file="test.pdf", fonts=myfam)
plot( c(0,1),c(0,1) )
text( 0.5, 0.5, "some text", family=myfam)
dev.off()

will work (and with amendments if this is a TeX-encoded font).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ivo_welch at brown.edu  Mon Nov  7 23:10:58 2005
From: ivo_welch at brown.edu (ivo welch)
Date: Mon, 07 Nov 2005 17:10:58 -0500
Subject: [R] pdf device and TeXencoding?
In-Reply-To: <Pine.LNX.4.61.0511072120380.7607@gannet.stats>
References: <436FA81C.3030902@brown.edu>
	<Pine.LNX.4.61.0511072120380.7607@gannet.stats>
Message-ID: <436FD0F2.4050603@brown.edu>


hi:  as usual, I am trying to be brief, and end up being too brief.  let me be more specific on [a].

$ cat test.R
luafmfiles <- c("/usr/share/texmf/fonts/afm/yandy/lubright/lbr.afm",
                "/usr/share/texmf/fonts/afm/yandy/lubright/lbd.afm",
                "/usr/share/texmf/fonts/afm/yandy/lubright/lbi.afm",
                "/usr/share/texmf/fonts/afm/yandy/lubright/lbdi.afm",
                "/usr/share/texmf/fonts/afm/yandy/lumath/lbms.afm")

grDevices::postscriptFonts(lucida=grDevices::postscriptFont("Lucida", metrics=luafmfiles, encoding="TeXtext.enc"));

pdf(file="test.pdf", fonts="lucida", version="1.4");
par(family="lucida");
plot( c(0,1),c(0,1) );
text( 0.5, 0.5, "this is some text" );
dev.off();

$ R CMD BATCH test.R

$ pdffonts test.pdf
name                                 type         emb sub uni object ID
------------------------------------ ------------ --- --- --- ---------
Error: Wrong type in font encoding resource differences (array)
Error: Wrong type in font encoding resource differences (array)
Error: Wrong type in font encoding resource differences (array)
Error: Wrong type in font encoding resource differences (array)
ZapfDingbats                         Type 1       no  no  no       5  0
Helvetica                            Type 1       no  no  no      11  0
Helvetica-Bold                       Type 1       no  no  no      12  0
Helvetica-Oblique                    Type 1       no  no  no      13  0
Helvetica-BoldOblique                Type 1       no  no  no      14  0
Symbol                               Type 1       no  no  no      15  0
LucidaBright                         Type 1       no  no  no      16  0
LucidaBright-Demi                    Type 1       no  no  no      17  0
LucidaBright-Italic                  Type 1       no  no  no      18  0
LucidaBright-DemiItalic              Type 1       no  no  no      19  0
LucidaNewMath-Symbol                 Type 1       no  no  no      20  0


$ ps2pdf13 -dPDFSETTINGS=/printer test.pdf a.pdf
Error: /typecheck in --.max--
Operand stack:
   --dict:4/4(L)--   F7   1   --dict:5/5(L)--   --dict:5/5(L)--   --dict:11/11(ro)(G)--   --nostringval--   --nostringval--   --nostringval--   256
Execution stack:
   %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   --nostringval--   false   1   %stopped_push   1   3   %oparray_pop   1   3   %oparray_pop   --nostringval--   2   1   1   --nostringval--   %for_pos_int_continue   --nostringval--   --nostringval--   --nostringval--   %array_continue   --nostringval--   false   1   %stopped_push   --nostringval--   %loop_continue   --nostringval--   --nostringval--   --nostringval--   --nostringval--   --nostringval--   --nostringval--
Dictionary stack:
   --dict:1052/1417(ro)(G)--   --dict:0/20(G)--   --dict:73/200(L)--   --dict:73/200(L)--   --dict:97/127(ro)(G)--   --dict:229/230(ro)(G)--   --dict:19/24(L)--   --dict:4/6(L)--   --dict:19/20(L)--   --dict:12/13(L)--
Current allocation mode is local
ESP Ghostscript 7.07.1: Unrecoverable error, exit code 1


if I use the postscript device,  this error does not occur.  if I use the standard helvetica fonts, this error does not occur.  I believe this error is from the textext encoding used in the pdf device.


I am sorry, I know so little about (postscript) fonts.  (I also need to end up with all fonts embedded in my figures.)  I was really trying to avoid complexity by avoiding "families of fonts".  I really want the most simple possible access given that I have exactly one .pfb font---I just want R to write a string in this font to a particular location.  [if it could embed the font itself, it would be much better, but I believe R cannot embed fonts itself; instead it requires an external tool like ghostscript.]  long story, short moral:  my desire for simplicity here is not just my stupidity (although there is plenty), but other tools (e.g., ghostscript) are sometimes quiet on errors or just substitute other fonts without asking ('feature'), and it takes a lot of experimenting to get the basics right.  so, the more basic, the better.

regards,

/iaw


Prof Brian Ripley wrote:

>> On Mon, 7 Nov 2005, ivo welch wrote:
>> 
>
>>>> [a] I believe that the pdf device does not yet fully support
>>>> TeXencoding.  (under R-2.2.0, the pdf file created with Textext as
>>>> font encoding still dies when post-processed by ghostscript.)  are
>>>> there any workarounds, or are there utilities that would allow a
>>>> TeXencoded font to be re-encoded/converted into ISOLatin, perhaps,
>>>> which R could then handle beautifully?
>
>> 
>> 
>> 1) What does `dies' mean?
>> 2) See the following R-patched NEWS entry
>> 
>>     o    pdf() was not writing details of the encoding to the file
>>     correctly.  (Spotted by Alexey Shipunov in Russian encodings.)
>> 
>> so this may well be solved in current R (R-patched/R-devel).
>> 
>
>>>> [b] is there a way to use an arbitrary postscript font and position it
>>>> into a ps or pdf graphic (i.e., without it being in the family {5
>>>> fonts} that I am using for the main drawing)?  in a weird latex/R mix,
>>>> my intent is something like
>
>> 
>> 
>> Yes, see par(family=) and (in 2.2.x)  ?postscriptFont to create a family.
>> 
>
>>>>     pdf(file="test.pdf");
>>>>     plot( c(0,1),c(0,1) );
>>>>     test.font = fontis("postscriptfont.pfb");  # may need a tfm
>>>> specification, too?
>>>>     text( 0.5, 0.5, fontobject("some text", test.font));
>>>>     dev.off();
>>>>
>>>> help would be highly appreciated, as always.
>
>> 
>> 
>> This area is all under development as we allow for CJK fonts.  In
>> R-devel, something like
>> 
>> myfam <- Type1Font("test", rep("postscriptfont.afm"), 4)
>> pdf(file="test.pdf", fonts=myfam)
>> plot( c(0,1),c(0,1) )
>> text( 0.5, 0.5, "some text", family=myfam)
>> dev.off()
>> 
>> will work (and with amendments if this is a TeX-encoded font).
>> 
>>



From KKIII at Indiana.Edu  Mon Nov  7 23:17:44 2005
From: KKIII at Indiana.Edu (Ken Kelley)
Date: Mon, 07 Nov 2005 17:17:44 -0500
Subject: [R] Modifying Internal C Files
Message-ID: <436FD288.8000600@Indiana.Edu>

Hi All.
I want to tweak a few minor things inside of internal C code. I have my 
Win. XP machine set-up to build packages (including C code), but I'm 
having problems getting the package to run correctly. In particular, I 
want to modify a few things inside of pnbeta.c (namely errmax and 
itrmax), which is what the pbeta() function calls upon when there is a 
noncentral parameter. I copied the pnbeta.c C code, changed its name [to 
pnbeta2.c], included the nmath.h, dpq.h files, lgamma.c, and pbeta.c in 
my src folder (since the .h files were called upon and the .c files 
were). I then created an R function that I thought would call upon the 
new C code:

pnbeta2 <- function(x, a, b, lambda, low.tail, log.p)
{
res <- .C("pnbeta2", as.double(x), as.double(a), as.double(b), 
as.double(lambda), as.integer(low.tail), as.integer(log.p))
return(list(res))
}

But after I built the package and loaded it it, the function doesn't 
work (it isn't even recognized). I have no idea why this is failing. Any 
information to help me figure out what I need to do to modify internal C 
code generally, or specifically as it applies to this scenario would be 
most helpful.

Thanks,
Ken

-- 
Ken Kelley, Ph.D.
Inquiry Methodology Program
Indiana University
201 North Rose Avenue, Room 4004
Bloomington, Indiana 47405
http://www.indiana.edu/~kenkel



From murdoch at stats.uwo.ca  Mon Nov  7 23:30:32 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 07 Nov 2005 17:30:32 -0500
Subject: [R] Modifying Internal C Files
In-Reply-To: <436FD288.8000600@Indiana.Edu>
References: <436FD288.8000600@Indiana.Edu>
Message-ID: <436FD588.7040400@stats.uwo.ca>

On 11/7/2005 5:17 PM, Ken Kelley wrote:
> Hi All.
> I want to tweak a few minor things inside of internal C code. I have my 
> Win. XP machine set-up to build packages (including C code), but I'm 
> having problems getting the package to run correctly. In particular, I 
> want to modify a few things inside of pnbeta.c (namely errmax and 
> itrmax), which is what the pbeta() function calls upon when there is a 
> noncentral parameter. I copied the pnbeta.c C code, changed its name [to 
> pnbeta2.c], included the nmath.h, dpq.h files, lgamma.c, and pbeta.c in 
> my src folder (since the .h files were called upon and the .c files 
> were). I then created an R function that I thought would call upon the 
> new C code:
> 
> pnbeta2 <- function(x, a, b, lambda, low.tail, log.p)
> {
> res <- .C("pnbeta2", as.double(x), as.double(a), as.double(b), 
> as.double(lambda), as.integer(low.tail), as.integer(log.p))
> return(list(res))
> }
> 
> But after I built the package and loaded it it, the function doesn't 
> work (it isn't even recognized). I have no idea why this is failing. Any 
> information to help me figure out what I need to do to modify internal C 
> code generally, or specifically as it applies to this scenario would be 
> most helpful.

You didn't say that you changed the name of the function, only the file 
that contained it.  If this wasn't an oversight, then you should put

double pnbeta2(double x, double a, double b, double lambda,
	      int lower_tail, int log_p)

in the appropriate place in your pnbeta2.c file.

The other thing you should do is to add a PACKAGE argument to your .C 
call, just in case there is already a pnbeta2 function somewhere else 
(or will be some day).  In fact, if you do this, there should be no need 
to change the name of the function:  R will look in the package DLL 
rather than R.dll to find it.  Just make sure that the name in the .C 
call matches the declared name in the source.

Duncan Murdoch



From rcmcll at yahoo.com  Tue Nov  8 00:11:24 2005
From: rcmcll at yahoo.com (bob mccall)
Date: Mon, 7 Nov 2005 15:11:24 -0800 (PST)
Subject: [R] frequency() source code
Message-ID: <20051107231124.69813.qmail@web60017.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051107/590f33f2/attachment.pl

From murdoch at stats.uwo.ca  Tue Nov  8 00:30:25 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 07 Nov 2005 18:30:25 -0500
Subject: [R] frequency() source code
In-Reply-To: <20051107231124.69813.qmail@web60017.mail.yahoo.com>
References: <20051107231124.69813.qmail@web60017.mail.yahoo.com>
Message-ID: <436FE391.5070701@stats.uwo.ca>

On 11/7/2005 6:11 PM, bob mccall wrote:
>   Greetings:
>  
>     I am looking for the source code for the frequency function. I 
>     grepped  the following dirs  but no luck. 
> 
>     R-2.2.0/src/appl/*
>     R-2.2.0/src/main/*
>     R-2.2.0/src/nmath/*
>     R-2.2.0/src/library/stats/*
> 
>     Does anybody know the file name??

Here's how I would look for it:

In R, type frequency, and I get:

 > frequency
function (x, ...)
UseMethod("frequency")
<environment: namespace:stats>

So frequency is a generic function.  That's all the source there is.

Now I'm probably interested in a particular method.  Let's say the 
default one.  So I try

 > frequency.default
Error: object "frequency.default" not found

Oops, looks like it's hidden in a namespace.  Try again:

 > getAnywhere("frequency.default")
A single object matching 'frequency.default' was found
It was found in the following places
   registered S3 method for frequency from namespace stats
   namespace:stats
with value

function (x, ...)
if (!is.null(xtsp <- attr(x, "tsp"))) xtsp[3] else 1
<environment: namespace:stats>

Now that's probably enough information, but if I really wanted to see 
the source (as opposed to the above deparsed version, which won't have 
any comments in it), then I'd know to look in src/library/stats/R, since 
it's in namespace:stats, and it's R code.  I'd grep or do another search 
of the files there and find it in src/library/stats/R/ts.R (and it turns 
out there aren't any comments after all, but it might be useful to look 
at the other functions in that file anyway).

Now if someone were running from a binary install in Windows, they 
wouldn't have a full copy of the source directories; they need to 
download that separately from the binary builds.

Duncan Murdoch



From jfox at mcmaster.ca  Tue Nov  8 00:35:57 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 7 Nov 2005 18:35:57 -0500
Subject: [R] OLS variables
In-Reply-To: <Pine.LNX.4.61.0511071609480.24171@gannet.stats>
Message-ID: <20051107233555.ZUW1799.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Brian,

I like the idea of providing support for raw polynomials in poly() and
polym(), if only for pedagogical reasons.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Monday, November 07, 2005 11:14 AM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch; 'Kjetil Brinchmann halvorsen'
> Subject: RE: [R] OLS variables
> 
> On Mon, 7 Nov 2005, John Fox wrote:
> 
> > Dear Brian,
> >
> > I don't have a strong opinion, but R's interpretation seems more 
> > consistent to me, and as Kjetil points out, one can use polym() to 
> > specify a full-polynomial model. It occurs to me that ^ and 
> ** could 
> > be differentiated in model formulae to provide both.
> 
> However, poly[m] only provide orthogonal polynomials, and I 
> have from time to time considered extending them to provide 
> raw polynomials too.
> Is that a better-supported idea?
> 
> >
> > Regards,
> > John
> >
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> >
> >> -----Original Message-----
> >> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> >> Sent: Monday, November 07, 2005 4:05 AM
> >> To: Kjetil Brinchmann halvorsen
> >> Cc: John Fox; r-help at stat.math.ethz.ch
> >> Subject: Re: [R] OLS variables
> >>
> >> On Sun, 6 Nov 2005, Kjetil Brinchmann halvorsen wrote:
> >>
> >>> John Fox wrote:
> >>>>
> >>>> I assume that you're using lm() to fit the model, and that
> >> you don't
> >>>> really want *all* of the interactions among 20 predictors:
> >> You'd need
> >>>> quite a lot of data to fit a model with 2^20 terms in it,
> >> and might
> >>>> have trouble interpreting the results.
> >>>>
> >>>> If you know which interactions you're looking for, then why not 
> >>>> specify them directly, as in lm(y ~  x1*x2 + x3*x4*x5 +
> >> etc.)? On the
> >>>> other hand, it you want to include all interactions, say, up to 
> >>>> three-way, and you've put the variables in a data frame,
> >> then lm(y ~ .^3, data=DataFrame) will do it.
> >>>
> >>> This is nice with factors, but with continuous variables,
> >> and need of
> >>> a response-surface type, of model, will not do. For 
> instance, with 
> >>> variables x, y, z in data frame dat
> >>>    lm( y ~ (x+z)^2, data=dat )
> >>> gives a model mwith the terms x, z and x*z, not the square terms.
> >>> There is a need for a semi-automatic way to get these, for
> >> instance,
> >>> use poly() or polym() as in:
> >>>
> >>> lm(y ~ polym(x,z,degree=2), data=dat)
> >>
> >> This is an R-S difference (FAQ 3.3.2).  R's formula parser always 
> >> takes
> >> x^2 = x whereas the S one does so only for factors.  This 
> makes sense 
> >> it you interpret `interaction' strictly as in John's 
> description - S 
> >> chose to see an interaction of any two continuous variables as 
> >> multiplication (something which puzzled me when I first 
> encountered 
> >> it, as it was not well documented back in 1991).
> >>
> >> I have often wondered if this difference was thought to be an 
> >> improvement, or if it just a different implementation of the 
> >> Rogers-Wilkinson syntax.
> >> Should we consider changing it?
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From claus.atzenbeck at freenet.de  Tue Nov  8 00:52:04 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Tue, 8 Nov 2005 00:52:04 +0100 (CET)
Subject: [R] reduce levels
Message-ID: <Pine.OSX.4.61.0511080041460.26719@cirrus.local>

Hi all:

I have an example that shows my problem:

    > test <- data.frame(c("a", "b", "c"))
    > colnames(test) <- "mm"
    > sub <- subset(test, mm=="b")
    > sub$mm
    [1] b
    Levels: a b c
    > levels(sub$mm)
    [1] "a" "b" "c"

How can I reduce the levels to exclusively those which are part of the
data frame? That is in the above named example exclusively "b".

Reason: I want to iterate exclusively through those levels that exist
within a subset, but leave away all others.

Thanks for any hint.
Claus



From gerifalte28 at hotmail.com  Tue Nov  8 01:30:19 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 08 Nov 2005 00:30:19 +0000
Subject: [R] frequency() source code
In-Reply-To: <436FE391.5070701@stats.uwo.ca>
Message-ID: <BAY103-F2932AD7CDD337B663565A4A6640@phx.gbl>




>From: Duncan Murdoch <murdoch at stats.uwo.ca>
>To: bob mccall <rcmcll at yahoo.com>
>CC: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
>Subject: Re: [R] frequency() source code
>Date: Mon, 07 Nov 2005 18:30:25 -0500
>
>On 11/7/2005 6:11 PM, bob mccall wrote:
> >   Greetings:
> >
> >     I am looking for the source code for the frequency function. I
> >     grepped  the following dirs  but no luck.
> >
> >     R-2.2.0/src/appl/*
> >     R-2.2.0/src/main/*
> >     R-2.2.0/src/nmath/*
> >     R-2.2.0/src/library/stats/*
> >
> >     Does anybody know the file name??
>
>Here's how I would look for it:
>
>In R, type frequency, and I get:
>
>  > frequency
>function (x, ...)
>UseMethod("frequency")
><environment: namespace:stats>
>
>So frequency is a generic function.  That's all the source there is.
>
>Now I'm probably interested in a particular method.  Let's say the
>default one.  So I try
>
>  > frequency.default
>Error: object "frequency.default" not found
>
>Oops, looks like it's hidden in a namespace.  Try again:
>
>  > getAnywhere("frequency.default")
>A single object matching 'frequency.default' was found
>It was found in the following places
>    registered S3 method for frequency from namespace stats
>    namespace:stats
>with value
>
>function (x, ...)
>if (!is.null(xtsp <- attr(x, "tsp"))) xtsp[3] else 1
><environment: namespace:stats>
>
>Now that's probably enough information, but if I really wanted to see
>the source (as opposed to the above deparsed version, which won't have
>any comments in it), then I'd know to look in src/library/stats/R, since
>it's in namespace:stats, and it's R code.  I'd grep or do another search
>of the files there and find it in src/library/stats/R/ts.R (and it turns
>out there aren't any comments after all, but it might be useful to look
>at the other functions in that file anyway).
>
>Now if someone were running from a binary install in Windows, they
>wouldn't have a full copy of the source directories; they need to
>download that separately from the binary builds.

...or, they can look at the source code directly at 
https://svn.r-project.org/R/trunk/src/library/stats/R/ts.R

Cheers

Francisco

>
>Duncan Murdoch
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Tue Nov  8 02:48:06 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 07 Nov 2005 17:48:06 -0800
Subject: [R] Help on model selection using AICc
In-Reply-To: <200511031602.jA3G2nuX018575@aitana.cpd.ua.es>
References: <200511031602.jA3G2nuX018575@aitana.cpd.ua.es>
Message-ID: <437003D6.7050605@pdf.com>

	  I don't think AICc is available in a standard script.  A couple of 
years ago, I modified the stepAIC script from the MASS package to use 
AICc.  If and when I get time for this again, I plan to redo it using a 
more complete Bayesian analysis.

	  However, if you want to use AICc, you might consider modifying 
stepAIC or downloading my "stepAIC.c" from "www.prodsyse.com".  This 
code was last used under S-Plus 6.2 and would doubtless require some 
effort to make it work under R.

	  Before you do either, however, I suggest you review earlier 
discussions on this issue, especially comments by Prof. Ripley, if you 
haven't already.  You can find numerous comments by looking for "AICc", 
"AIC.c", "stepAIC.c", and "Burnham and Anderson" with RSiteSearch.

	  Buena Suerte
	  Spencer Graves

german.lopez at ua.es wrote:

> Hi,
>    I'm fitting poisson regression models to counts of birds in 
> 1x1 km squares using several environmental variables as predictors. 
> I do this in a stepwise way, using the stepAIC function. However the 
> resulting models appear to be overparametrized, since too much 
> variables were included. 
>   I would like to know if there is the possibility of fitting models 
> by steps but using the AICc instead of AIC. Or at least I wonder if it 
> would be possible to save the AIC value and number of parameters of 
> models fitted in each step and to calculate AICc afterward.
>    Help on this will be very much appreciated
>    German Lopez
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From mbmiller at taxa.epi.umn.edu  Tue Nov  8 02:52:18 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Mon, 7 Nov 2005 19:52:18 -0600 (CST)
Subject: [R] writing R shell scripts?
Message-ID: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>

I'm new to the list.  I've used R and S-PLUS a bit for about 15 years but 
am now working to make R my main program for all numerical and statistical 
computing.  I also use Octave for this kind of work and I recommend it (it 
is also under the GPL).  Here's my question:  In Octave I can write shell 
scripts in the Linux/UNIX environment that begin with a line like this...

#!/usr/local/bin/octave -q

...and the remaining lines are octave commands.  Is it possible to do this 
sort of thing in R using something like this?:

#!/usr/lib/R/bin/R.bin

Well, that isn't quite it because I tried it and it didn't work!

Any advice greatly appreciated.  Thanks in advance.

Mike

-- 
Michael B. Miller, Ph.D.
Assistant Professor
Division of Epidemiology and Community Health
and Institute of Human Genetics
University of Minnesota
http://taxa.epi.umn.edu/~mbmiller/



From stratja at auburn.edu  Tue Nov  8 03:01:22 2005
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Mon, 07 Nov 2005 20:01:22 -0600
Subject: [R] Poisson/negbin followed by jackknife
Message-ID: <436FB292020000F200000C8B@TMIA1.AUBURN.EDU>

Folks,

Thanks for the help with the hier.part analysis.  All the problems
stemmed from an import problem which was solved with file.chose().

Now that I have the variables that I'd like to use I need to run some
GLM models.  I think I have that part under control but I'd like to use
a jackknife approach to model validation (I was using a hold out sample
but this seems to have fallen out of favor).  

I'd appreciate it if someone could just point me in the right direction
for the jackkife analysis given a particular distribution, coefficients,
etc.

Thanks,

Jeff

****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja



From Bill.Venables at csiro.au  Tue Nov  8 03:11:39 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Tue, 8 Nov 2005 13:11:39 +1100
Subject: [R] reduce levels
Message-ID: <B998A44C8986644EA8029CFE6396A924304C4C@exqld2-bne.qld.csiro.au>

sub$mm <- factor(sub$mm)

is the simplest way to change a single factor in this way.  To do a
whole data frame you just need a loop:

drop_unused_levels <- function(data) 
	as.data.frame(lapply(data, function(x) if(is.factor(x))
factor(x) else x ))

Here's your example again, but witn a slightly different idiom...

test <- data.frame(mm = letters[1:3])
sub <- subset(test, mm == "b")

fixedSub <- drop_unused_levels(sub)

Bill Venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Claus Atzenbeck
Sent: Tuesday, 8 November 2005 9:52 AM
To: R-help
Subject: [R] reduce levels


Hi all:

I have an example that shows my problem:

    > test <- data.frame(c("a", "b", "c"))
    > colnames(test) <- "mm"
    > sub <- subset(test, mm=="b")
    > sub$mm
    [1] b
    Levels: a b c
    > levels(sub$mm)
    [1] "a" "b" "c"

How can I reduce the levels to exclusively those which are part of the
data frame? That is in the above named example exclusively "b".

Reason: I want to iterate exclusively through those levels that exist
within a subset, but leave away all others.

Thanks for any hint.
Claus

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From duncan at wald.ucdavis.edu  Tue Nov  8 03:15:18 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Mon, 07 Nov 2005 18:15:18 -0800
Subject: [R] writing R shell scripts?
In-Reply-To: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>
Message-ID: <43700A36.8090801@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


This has been done and I said I would integrate it into R.
It is on my list of things to do relatively soon. I have never
been entirely happy with the way arguments are handled and that is what
has put things on hold. And now that the R startup is a lot quicker,
this is more worthwhile.

Mike Miller wrote:
> I'm new to the list.  I've used R and S-PLUS a bit for about 15 years but 
> am now working to make R my main program for all numerical and statistical 
> computing.  I also use Octave for this kind of work and I recommend it (it 
> is also under the GPL).  Here's my question:  In Octave I can write shell 
> scripts in the Linux/UNIX environment that begin with a line like this...
> 
> #!/usr/local/bin/octave -q
> 
> ...and the remaining lines are octave commands.  Is it possible to do this 
> sort of thing in R using something like this?:
> 
> #!/usr/lib/R/bin/R.bin
> 
> Well, that isn't quite it because I tried it and it didn't work!
> 
> Any advice greatly appreciated.  Thanks in advance.
> 
> Mike
> 

- --
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (Darwin)
Comment: Using GnuPG with Thunderbird - http://enigmail.mozdev.org

iD8DBQFDcAo19p/Jzwa2QP4RAjuPAJ48um++JJ9f1g62i6jBm7w8yZY7wwCfXU1J
ceVWi2zr1oEcXV8BQyD2gdM=
=i8JE
-----END PGP SIGNATURE-----



From KKIII at Indiana.Edu  Tue Nov  8 04:25:09 2005
From: KKIII at Indiana.Edu (Ken Kelley)
Date: Mon, 07 Nov 2005 22:25:09 -0500
Subject: [R] Modifying Internal C Files
In-Reply-To: <436FD588.7040400@stats.uwo.ca>
References: <436FD288.8000600@Indiana.Edu> <436FD588.7040400@stats.uwo.ca>
Message-ID: <43701A95.9030207@Indiana.Edu>

Hi Duncan and others.

Thanks for your insight. Actually, I did change the function name in the 
pnbeta2.c code (I should have said so). When I check the package I get 
no errors. I build the package and all seems well. When I install and 
then load the package in R, I get:

 > pnbeta2
function (x, a, b, lambda, low.tail, log.p)
{
     res <- .Internal("pnbeta2", as.double(x), as.double(a), 
as.double(b), as.double(lambda), as.integer(low.tail), as.integer(log.p))
     return(list(res))
}
<environment: namespace:try>

which seems good, but the function does not work:

pnbeta2(.1, 10, 25, 2, TRUE, FALSE)
Error in pnbeta2(0.1, 10, 25, 2, TRUE, FALSE) :
         7 arguments passed to '.Internal' which requires 1

When I try to run the .Internal file directly is perhaps where my 
problems begin:
 > .Internal(pnbeta2(.1, 10, 25, 2, TRUE, FALSE))
Error in .Internal(pnbeta2(0.1, 10, 25, 2, TRUE, FALSE)) :
         no internal function "pnbeta2"

But, when I do the same thing to the pnbeta internal function (which 
pnbeta2 is identical to at this point) I get the result:
 > .Internal(pnbeta(.1, 10, 25, 2, TRUE, FALSE))
[1] 0.0006837318

So, I'm at a loss for what is going on in this situation. my pnbeta2 
internal function doesn't seem to be there. What I want to do is simple 
(modify an interal C file), but it is proving to be quite difficult for 
me to implement.

Thanks for any thoughts,
Ken


Duncan Murdoch wrote:
> On 11/7/2005 5:17 PM, Ken Kelley wrote:
> 
>> Hi All.
>> I want to tweak a few minor things inside of internal C code. I have 
>> my Win. XP machine set-up to build packages (including C code), but 
>> I'm having problems getting the package to run correctly. In 
>> particular, I want to modify a few things inside of pnbeta.c (namely 
>> errmax and itrmax), which is what the pbeta() function calls upon when 
>> there is a noncentral parameter. I copied the pnbeta.c C code, changed 
>> its name [to pnbeta2.c], included the nmath.h, dpq.h files, lgamma.c, 
>> and pbeta.c in my src folder (since the .h files were called upon and 
>> the .c files were). I then created an R function that I thought would 
>> call upon the new C code:
>>
>> pnbeta2 <- function(x, a, b, lambda, low.tail, log.p)
>> {
>> res <- .C("pnbeta2", as.double(x), as.double(a), as.double(b), 
>> as.double(lambda), as.integer(low.tail), as.integer(log.p))
>> return(list(res))
>> }
>>
>> But after I built the package and loaded it it, the function doesn't 
>> work (it isn't even recognized). I have no idea why this is failing. 
>> Any information to help me figure out what I need to do to modify 
>> internal C code generally, or specifically as it applies to this 
>> scenario would be most helpful.
> 
> 
> You didn't say that you changed the name of the function, only the file 
> that contained it.  If this wasn't an oversight, then you should put
> 
> double pnbeta2(double x, double a, double b, double lambda,
>           int lower_tail, int log_p)
> 
> in the appropriate place in your pnbeta2.c file.
> 
> The other thing you should do is to add a PACKAGE argument to your .C 
> call, just in case there is already a pnbeta2 function somewhere else 
> (or will be some day).  In fact, if you do this, there should be no need 
> to change the name of the function:  R will look in the package DLL 
> rather than R.dll to find it.  Just make sure that the name in the .C 
> call matches the declared name in the source.
> 
> Duncan Murdoch
> 

-- 
Ken Kelley, Ph.D.
Inquiry Methodology Program
Indiana University
201 North Rose Avenue, Room 4004
Bloomington, Indiana 47405
http://www.indiana.edu/~kenkel



From spencer.graves at pdf.com  Tue Nov  8 04:23:00 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 07 Nov 2005 19:23:00 -0800
Subject: [R] multidimensional integration not over
	a	multidimensionalrectangle
In-Reply-To: <BAY102-F2C6A03021F1D099B0C406B3610@phx.gbl>
References: <BAY102-F2C6A03021F1D099B0C406B3610@phx.gbl>
Message-ID: <43701A14.2050801@pdf.com>

	  The (non)respoonse that I've seen to your question combined with my 
own search suggests that R does not yet have a multidimensional 
numerical integration function of the generality you desire.

	  Would you care to tell us more about the problem you are trying to 
solve?  For example, if it can be parameterized as an integral over R^k 
where the integrand is unimodal, you could approximate the integrand 
with a parabolic then do Monte Carlo from the obvious approximation of 
that integral by a multivariate normal.  For each selected point, you 
compute the ratio of your integrand to the approximating normal density. 
  Then compute mean and standard deviation of these ratios.  The mean is 
the evaluation of the integral, and stdev/N estimates the error.  For 
more information, see any good reference on "importance sampling", e.g.:

	  Evans and Swartz (2000) Approximating Integral via Monte Carlo and 
Deterministic Methods (Oxford).

	  hope this helps.
	  spencer graves

Lynette Sun wrote:

> Hi,
> 
> anyone knows about any functions in R can get multidimensional integration
> not over a multidimensional rectangle (not adapt).
> 
> For example, I tried the following function f(x,n)=x^n/n!
> 
> phi.fun<-function(x,n)
> { if (n==1) {
> 	x
> 	}else{
> 		integrate(phi.fun, lower=0, upper=x, n=n-1)$value
> 		}
> }
> 
> I could get f(4,2)=4^2/2!=8, but failed in f(4,3)=4^3/3! Thanks
> 
> Best,
> Lynette
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From redbeard at arrr.net  Tue Nov  8 04:57:14 2005
From: redbeard at arrr.net (Jarrett Byrnes)
Date: Mon, 7 Nov 2005 19:57:14 -0800
Subject: [R] Simple Nesting question/Odd error message
Message-ID: <e6ad7a3edf43b711cd1edce8bd5cbd50@arrr.net>

I'm attempting to analyze some survey data comparing multiple docks.  I 
surveyed all of the slips within each dock, but as slips are nested 
within docks, getting multiple samples per slip, and don't really 
represent any meaningful gradient, slip is a random effect.  There are 
also an unequal number of slips at each dock.

I'm having syntactical issues, however.  When I try

dock.lme<-lme(X.open ~ Dock, random=Slip|~Dock, data=my.data)

I get

Error in inherits(object, "reStruct") : Object "Slip" not found

I'm uncertain as to what this means, as Slip most certainly is there 
(yes, I checked) - any pointers, or pointers about syntax for this as a 
general topic.

And while I'm on it, is there a way to fit models with random effects 
using least squares instead of REML?  Thanks!

-Jarrett



From rahmank at frim.gov.my  Wed Nov  9 05:43:13 2005
From: rahmank at frim.gov.my (Abd. Rahman Kassim)
Date: Wed, 9 Nov 2005 12:43:13 +0800
Subject: [R] Sort a dataframe
Message-ID: <000601c5e4e8$17f38cb0$2f01a8c0@rahmandt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/153ac05a/attachment.pl

From A.Robinson at ms.unimelb.edu.au  Tue Nov  8 05:53:12 2005
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 8 Nov 2005 15:53:12 +1100
Subject: [R] Sort a dataframe
In-Reply-To: <000601c5e4e8$17f38cb0$2f01a8c0@rahmandt>
References: <000601c5e4e8$17f38cb0$2f01a8c0@rahmandt>
Message-ID: <20051108045312.GH54438@ms.unimelb.edu.au>

Try: 

dataframeName <- dataframeName[order(dataframeName$ColumnName),]

Cheers

Andrew

On Wed, Nov 09, 2005 at 12:43:13PM +0800, Abd. Rahman Kassim wrote:
> 
> Dear All,
> 
> How can I sort a data frame (using one of the column)?
> 
> Thanks for your support.
> 
> Regards.
> 
> Abd. Rahman Kassim (PhD)
> Head Forest Ecology Branch
> Forest Management & Ecology Program
> Forestry and Conservation Division
> Forest Research Institute Malaysia
> Kepong 52109
> Selangor, Malaysia
> 
> *****************************************
> 
> Checked by TrendMicro Interscan Messaging Security.
> For any enquiries, please contact FRIM IT Department.
> *****************************************
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344-4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au



From ripley at stats.ox.ac.uk  Tue Nov  8 07:22:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Nov 2005 06:22:40 +0000 (GMT)
Subject: [R] Modifying Internal C Files
In-Reply-To: <43701A95.9030207@Indiana.Edu>
References: <436FD288.8000600@Indiana.Edu> <436FD588.7040400@stats.uwo.ca>
	<43701A95.9030207@Indiana.Edu>
Message-ID: <Pine.LNX.4.61.0511080608230.13011@gannet.stats>

In your first post you said you called .C.  Here you say you call 
.Internal.  Users cannot add .Internal functions, nor can they be used in 
packages. In any case, The syntax is not as you show, but

     .Internal(pnbeta(q, shape1, shape2, ncp, lower.tail, log.p))

(And to be definite, that calls the C function do_math4, not pnbeta.)

Final comment:  Duncan mentioned

>> double pnbeta2(double x, double a, double b, double lambda,
>>           int lower_tail, int log_p)

but a .C call has to be to a function with all arguments as pointers, so 
there would need a wrapper function.

The reason you are finding this difficult is that you are not dealing with 
the C function behind .Internal, but one deeper into the system. The 
simplest thing to do would be to compile R from the sources, then make 
modifications and rebuild.  But in this case I believe Thomas Lumley has 
already modified the R-devel sources and Duncan Murdoch has provided 
Windows binaries of R-devel, so perhaps there is a much simpler route?


On Mon, 7 Nov 2005, Ken Kelley wrote:

> Hi Duncan and others.
>
> Thanks for your insight. Actually, I did change the function name in the
> pnbeta2.c code (I should have said so). When I check the package I get
> no errors. I build the package and all seems well. When I install and
> then load the package in R, I get:
>
> > pnbeta2
> function (x, a, b, lambda, low.tail, log.p)
> {
>     res <- .Internal("pnbeta2", as.double(x), as.double(a),
> as.double(b), as.double(lambda), as.integer(low.tail), as.integer(log.p))
>     return(list(res))
> }
> <environment: namespace:try>
>
> which seems good, but the function does not work:
>
> pnbeta2(.1, 10, 25, 2, TRUE, FALSE)
> Error in pnbeta2(0.1, 10, 25, 2, TRUE, FALSE) :
>         7 arguments passed to '.Internal' which requires 1
>
> When I try to run the .Internal file directly is perhaps where my
> problems begin:
> > .Internal(pnbeta2(.1, 10, 25, 2, TRUE, FALSE))
> Error in .Internal(pnbeta2(0.1, 10, 25, 2, TRUE, FALSE)) :
>         no internal function "pnbeta2"
>
> But, when I do the same thing to the pnbeta internal function (which
> pnbeta2 is identical to at this point) I get the result:
> > .Internal(pnbeta(.1, 10, 25, 2, TRUE, FALSE))
> [1] 0.0006837318
>
> So, I'm at a loss for what is going on in this situation. my pnbeta2
> internal function doesn't seem to be there. What I want to do is simple
> (modify an interal C file), but it is proving to be quite difficult for
> me to implement.
>
> Thanks for any thoughts,
> Ken
>
>
> Duncan Murdoch wrote:
>> On 11/7/2005 5:17 PM, Ken Kelley wrote:
>>
>>> Hi All.
>>> I want to tweak a few minor things inside of internal C code. I have
>>> my Win. XP machine set-up to build packages (including C code), but
>>> I'm having problems getting the package to run correctly. In
>>> particular, I want to modify a few things inside of pnbeta.c (namely
>>> errmax and itrmax), which is what the pbeta() function calls upon when
>>> there is a noncentral parameter. I copied the pnbeta.c C code, changed
>>> its name [to pnbeta2.c], included the nmath.h, dpq.h files, lgamma.c,
>>> and pbeta.c in my src folder (since the .h files were called upon and
>>> the .c files were). I then created an R function that I thought would
>>> call upon the new C code:
>>>
>>> pnbeta2 <- function(x, a, b, lambda, low.tail, log.p)
>>> {
>>> res <- .C("pnbeta2", as.double(x), as.double(a), as.double(b),
>>> as.double(lambda), as.integer(low.tail), as.integer(log.p))
>>> return(list(res))
>>> }
>>>
>>> But after I built the package and loaded it it, the function doesn't
>>> work (it isn't even recognized). I have no idea why this is failing.
>>> Any information to help me figure out what I need to do to modify
>>> internal C code generally, or specifically as it applies to this
>>> scenario would be most helpful.
>>
>>
>> You didn't say that you changed the name of the function, only the file
>> that contained it.  If this wasn't an oversight, then you should put
>>
>> double pnbeta2(double x, double a, double b, double lambda,
>>           int lower_tail, int log_p)
>>
>> in the appropriate place in your pnbeta2.c file.
>>
>> The other thing you should do is to add a PACKAGE argument to your .C
>> call, just in case there is already a pnbeta2 function somewhere else
>> (or will be some day).  In fact, if you do this, there should be no need
>> to change the name of the function:  R will look in the package DLL
>> rather than R.dll to find it.  Just make sure that the name in the .C
>> call matches the declared name in the source.
>>
>> Duncan Murdoch
>>
>
> -- 
> Ken Kelley, Ph.D.
> Inquiry Methodology Program
> Indiana University
> 201 North Rose Avenue, Room 4004
> Bloomington, Indiana 47405
> http://www.indiana.edu/~kenkel
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov  8 07:31:11 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Nov 2005 06:31:11 +0000 (GMT)
Subject: [R] Poisson/negbin followed by jackknife
In-Reply-To: <436FB292020000F200000C8B@TMIA1.AUBURN.EDU>
References: <436FB292020000F200000C8B@TMIA1.AUBURN.EDU>
Message-ID: <Pine.LNX.4.61.0511080627240.13011@gannet.stats>

Do you really mean a jackknife or leave-one-out crossvalidation?  They are 
not the same, but the second is often incorrectly called the first.

In either case, I would point you at the book the 'boot' package supports.
See for example its cv.glm function.

On Mon, 7 Nov 2005, Jeffrey Stratford wrote:

> Thanks for the help with the hier.part analysis.  All the problems
> stemmed from an import problem which was solved with file.chose().
>
> Now that I have the variables that I'd like to use I need to run some
> GLM models.  I think I have that part under control but I'd like to use
> a jackknife approach to model validation (I was using a hold out sample
> but this seems to have fallen out of favor).
>
> I'd appreciate it if someone could just point me in the right direction
> for the jackkife analysis given a particular distribution, coefficients,
> etc.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rahmank at frim.gov.my  Wed Nov  9 08:25:52 2005
From: rahmank at frim.gov.my (Abd. Rahman Kassim)
Date: Wed, 9 Nov 2005 15:25:52 +0800
Subject: [R] Sort a dataframe
References: <000601c5e4e8$17f38cb0$2f01a8c0@rahmandt> 
	<20051108045312.GH54438@ms.unimelb.edu.au>
Message-ID: <001601c5e4fe$d19dc110$2f01a8c0@rahmandt>


Dear Andrew,

Thanks for the response. It works.

Abd. Rahman
----- Original Message ----- 
From: "Andrew Robinson" <A.Robinson at ms.unimelb.edu.au>
To: "Abd. Rahman Kassim" <rahmank at frim.gov.my>
Cc: "R-Help Discussion" <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 08, 2005 12:53 PM
Subject: Re: [R] Sort a dataframe


> Try:
>
> dataframeName <- dataframeName[order(dataframeName$ColumnName),]
>
> Cheers
>
> Andrew
>
> On Wed, Nov 09, 2005 at 12:43:13PM +0800, Abd. Rahman Kassim wrote:
>>
>> Dear All,
>>
>> How can I sort a data frame (using one of the column)?
>>
>> Thanks for your support.
>>
>> Regards.
>>
>> Abd. Rahman Kassim (PhD)
>> Head Forest Ecology Branch
>> Forest Management & Ecology Program
>> Forestry and Conservation Division
>> Forest Research Institute Malaysia
>> Kepong 52109
>> Selangor, Malaysia
>>
>> *****************************************
>>
>> Checked by TrendMicro Interscan Messaging Security.
>> For any enquiries, please contact FRIM IT Department.
>> *****************************************
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> -- 
> Andrew Robinson
> Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
> Department of Mathematics and Statistics            Fax: +61-3-8344-4599
> University of Melbourne, VIC 3010 Australia
> Email: a.robinson at ms.unimelb.edu.au    Website: 
> http://www.ms.unimelb.edu.au 


*****************************************

Checked by TrendMicro Interscan Messaging Security. 
For any enquiries, please contact FRIM IT Department.



From Dominik.Sydler at eawag.ch  Tue Nov  8 08:32:04 2005
From: Dominik.Sydler at eawag.ch (Sydler, Dominik)
Date: Tue, 8 Nov 2005 08:32:04 +0100
Subject: [R] Time-measurement in milliseconds
Message-ID: <59484F5CC089B4499DDAC9503DA0BC6706A51C@EA-MAIL.eawag.wroot.emp-eaw.ch>

Thanks a lot, that works fine!
Strange that I didn't find this function in the help...

Bye
    Dominic 

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Montag, 7. November 2005 18:27
To: Sydler, Dominik
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Time-measurement in milliseconds

Be aware that the measurement will itself take more than a millisecond
from an interpreted language like R.

Please see the help page for Sys.time, and its suggestion of proc.time.
for(i in 1:100) print(proc.time()[3]) suggests this takes 3ms on my box.

On Mon, 7 Nov 2005, Sydler, Dominik wrote:

> Hi there
>
> I'm loking for a time-measurement to measure time-differences in 
> milliseconds.
> On my search, I only found the following:
> - package "base": Sys.time()
>    -> only second-accuracy
> - package "R.utils": System$currentTimeMillis()
>    -> returns integer of milliseconds, but accuracy is only whole 
> seconds too.
> At the moment I run every bit of code to measure 1000-times to be able

> to calculate time in milliseconds... ;-)
>
> Has anyone a method to get milliseconds?
>
> Thanks for any help.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tura at centroin.com.br  Tue Nov  8 09:52:53 2005
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Tue, 08 Nov 2005 06:52:53 -0200
Subject: [R] Distribution fitting problem
In-Reply-To: <200511021432.52723.mmiller@nassp.uct.ac.za>
References: <200511021432.52723.mmiller@nassp.uct.ac.za>
Message-ID: <6.1.2.0.2.20051108065101.0282d380@centroin.com.br>

At 10:32 2/11/2005, you wrote:

>I am using the MASS library function
>
>fitdistr(x, dpois, list(lambda=2))
>
>but I get
>
>Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
>         Function cannot be evaluated at initial parameters
>In addition: There were 50 or more warnings (use warnings() to see the first
>50)
>
>and all the first 50 warnings say
>
>1:  non-integer x = 1.452222
>etc
>
>Can anyone tell me what I am doing wrong. p.s. the data was read in from
>a .csv file that I wrote using octave

Mark,

Try fitdistr(x, "Poisson")

I think this is enough for fit Poisson distribuition for your data


Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil 


--



From dieter.menne at menne-biomed.de  Tue Nov  8 11:06:44 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 8 Nov 2005 10:06:44 +0000 (UTC)
Subject: [R] Simple Nesting question/Odd error message
References: <e6ad7a3edf43b711cd1edce8bd5cbd50@arrr.net>
Message-ID: <loom.20051108T110550-647@post.gmane.org>

Jarrett Byrnes <redbeard <at> arrr.net> writes:

> I'm having syntactical issues, however.  When I try
> 
> dock.lme<-lme(X.open ~ Dock, random=Slip|~Dock, data=my.data)
> 
> I get
> 
> Error in inherits(object, "reStruct") : Object "Slip" not found
> 

Syntax is wrong, should be something like

dock.lme<-lme(X.open ~ Dock, random=~Slip|Dock, data=my.data)



From c.marziliano at ec.univaq.it  Tue Nov  8 12:02:52 2005
From: c.marziliano at ec.univaq.it (Ciro Marziliano)
Date: Tue, 08 Nov 2005 12:02:52 +0100
Subject: [R] Output glm
Message-ID: <437085DC.2000100@ec.univaq.it>

Hello,
How can I obtain the likelihood ratio of a Poisson regression model?
Regards.


_____________________________________________
	dr. Marziliano Ciro
	Facolta' di Economia
	Universita' degli Studi di L'Aquila
	p.zza del Santuario, 19
	67040 Roio Poggio, L'Aquila
	tel.: 0862 434836
	fax: 0862 434803



From ManuelPerera-Chang at fmc-ag.com  Tue Nov  8 12:12:11 2005
From: ManuelPerera-Chang at fmc-ag.com (ManuelPerera-Chang@fmc-ag.com)
Date: Tue, 8 Nov 2005 12:12:11 +0100
Subject: [R] lattice chart: different definitions for series
Message-ID: <OF20D49190.7DBEB7A0-ONC12570B3.0037C24E-C12570B3.003D8AC1@notes.fresenius.de>





Dear Deepayan,

Thank you very much for the example.

Now I am missing the grids inside my graph. As shown in my sample code
yesterday, before I had them defined in the

panel=function(x,y,...){panel.grid(h=-1,v=-1,col="grey",lty=2,cex=0.1)}

and they were properly drawn, but you did not use this approach in your
example, and my guesses this morning to place them together with your code
failed.

Sorry for my newbee question ..

My best regards,

Manuel

PS.: As for my "experiments", I was trying to use the document "Panel
Function for Display Marked by groups", that you wrote. In this document I
cannot see that e.g. all the arguments for panel.superpose.2 are optionals,
as in your example below.

Further in this documentation you explained:

"panel.groups - the panel function to be used for each group of points.
Defaults to panel.xyplot (behavior in S) "

and then ..

"type -  usually a character vector specifying what should be drawn for
each group, passed on to the panel.groups function, which must know what to
do with it. By default this is panel.xyplot ..."

consecuently I was trying to use "type" as an argument to panel.groups, but
without success.




                                                                                                                                       
                      Deepayan Sarkar                                                                                                  
                      <deepayan.sarkar@        To:       ManuelPerera-Chang at fmc-ag.com                                                 
                      gmail.com>               cc:       r-help at stat.math.ethz.ch                                                      
                                               Subject:  Re: lattice chart: different definitions for series                           
                      07.11.2005 21:23                                                                                                 
                                                                                                                                       
                                                                                                                                       




On 11/7/05, ManuelPerera-Chang at fmc-ag.com <ManuelPerera-Chang at fmc-ag.com>
wrote:
>
>
>
>
> Hi enthusiasts,
>
> Trying to create a single chart in  lattice with different plotting
> definitions for the different series (two series should be drawn with
lines
> and the other without them)
>
> I am using a dataset, which includes a grouping variable e.g. clinic with
> three levels, the variable "year" and a continous variable: "mct".
>
> In the graph the variable "year" is in the x axis, with "mct" represented
> in the y axis.
>
> The diagram should include two line diagrams(representing two of the
> groups) , with the third group represented only with symbols(no lines).
>
> Until now I was using white lines to eliminate the lines drawn in the
third
> group, but this solution is not optimal, as the grids are sometimes not
> visible
>
> sp<-list(superpose.symbol=list(pch=c(1,2,1),col=c("blue","red","green")),
>        superpose.line=list(col=c("blue","red","white"),lty=c(1,2,)))
>
> ... and then including
>
> print(xyplot(mct~trend.data$year,groups=clinic,
>   scales=list(x=list(at=c(15:pno),labels=per.labels)),
>   main=main.title,
>   sub=sub.title,
>   xlab=x.label,
>   ylab=y.label,
>   xlim=c(pno-12,pno+1),
>   panel=function(x,y,...){panel.grid(h=-1,v=-1,col="grey",lty=2,cex=0.1);
>                   panel.superpose(x,y,type="l",lwd=1.8,...);
>                   panel.superpose(x,y,type="p",cex=1.8,...))},
>   key=sk,
>   par.settings=sp));
>
> ... was also experimenting, and searching a lot in the WWW for
>
> panel.superpose.2 and type=c("b","b","p"), but without success.

I don't know what experiments you did, but the following seems to work
fine for me:

library(lattice)

mydf <-
    data.frame(year = rep(1991:2000, 3),
               mct =
               rnorm(30,
                     mean = rep(1:3, each = 10),
                     sd = 0.5),
               clinic = gl(3, 10))

xyplot(mct ~ year, mydf, groups = clinic,
       panel = panel.superpose.2,
       type = c("b", "b", "p"))

-Deepayan



From KKIII at Indiana.Edu  Tue Nov  8 12:33:41 2005
From: KKIII at Indiana.Edu (Ken Kelley)
Date: Tue, 08 Nov 2005 06:33:41 -0500
Subject: [R] Modifying Internal C Files
In-Reply-To: <Pine.LNX.4.61.0511080608230.13011@gannet.stats>
References: <436FD288.8000600@Indiana.Edu> <436FD588.7040400@stats.uwo.ca>
	<43701A95.9030207@Indiana.Edu>
	<Pine.LNX.4.61.0511080608230.13011@gannet.stats>
Message-ID: <43708D15.1000709@Indiana.Edu>

Thanks for your thoughts Prof. Ripley.

When I set out to do this I had no idea it would be so difficult. 
Nevertheless, perhaps the best thing would be for me to say why I want 
to change internal parts of the function and why the best solution might 
be for it to be done in the next release of R. At the beginning of the 
pnbeta.c file we see:

     /* change errmax and itrmax if desired */

     const static double errmax = 1.0e-9;
     const int    itrmax = 100;

Which obviously sets the limits for the error and for the number of 
iterations. The comment says to change if desired, but there is no way 
for the user to modify these values. If you recall, I posted on problems 
with the pf() function for large noncentral parameters (On Oct. 24 and 
filed a bug #8251). Since pf() uses the pnbeta.c, the limitations in 
pnbeta.c filter though to the pf() function. My thoughts about the 
limitations of the iterations where confirmed when Chattamvelli and 
Shanmugam (1997; _Applied Statistics_) illustrate in their table that 
the method R uses (Length, 1987, and the Frick, 1990, update) that 
indeed the number of iterations can go well above 100. Dr. T. Lumley 
emailed the list saying increasing the iterations does in fact seem to 
solve the problems I described (presumably he modifying the source code 
directly).

So, can the itrmax be changed inside of pnbeta.c to some large value 
(e.g., 5000; realizing most problems will converge after only a few 
iterations). Or can the pbeta() function (possibly pf() since it uses 
pbeta.c) have parameters added that allows user control over these 
values? I would very much appreciate if this could be done, and 
obviously it would make R a bit more stable for large noncentral values 
(which are growing more important in some areas). That way everyone has 
a working function and not just me (If I were to be able to modify this 
internal function anyway!). As an aside, the Chattamvelli and Shanmugam 
(1997) paper update the algorithms that pnbeta.c is based (Algorithm AS 
310). It would be nice to incorporate this into R at some point, but 
first taking care of the iteration issue would be easy to take care of 
right now.

Thanks,
Ken

Prof Brian Ripley wrote:
> In your first post you said you called .C.  Here you say you call 
> .Internal.  Users cannot add .Internal functions, nor can they be used 
> in packages. In any case, The syntax is not as you show, but
> 
>     .Internal(pnbeta(q, shape1, shape2, ncp, lower.tail, log.p))
> 
> (And to be definite, that calls the C function do_math4, not pnbeta.)
> 
> Final comment:  Duncan mentioned
> 
>>> double pnbeta2(double x, double a, double b, double lambda,
>>>           int lower_tail, int log_p)
> 
> 
> but a .C call has to be to a function with all arguments as pointers, so 
> there would need a wrapper function.
> 
> The reason you are finding this difficult is that you are not dealing 
> with the C function behind .Internal, but one deeper into the system. 
> The simplest thing to do would be to compile R from the sources, then 
> make modifications and rebuild.  But in this case I believe Thomas 
> Lumley has already modified the R-devel sources and Duncan Murdoch has 
> provided Windows binaries of R-devel, so perhaps there is a much simpler 
> route?
> 
> 
> On Mon, 7 Nov 2005, Ken Kelley wrote:
> 
>> Hi Duncan and others.
>>
>> Thanks for your insight. Actually, I did change the function name in the
>> pnbeta2.c code (I should have said so). When I check the package I get
>> no errors. I build the package and all seems well. When I install and
>> then load the package in R, I get:
>>
>> > pnbeta2
>> function (x, a, b, lambda, low.tail, log.p)
>> {
>>     res <- .Internal("pnbeta2", as.double(x), as.double(a),
>> as.double(b), as.double(lambda), as.integer(low.tail), as.integer(log.p))
>>     return(list(res))
>> }
>> <environment: namespace:try>
>>
>> which seems good, but the function does not work:
>>
>> pnbeta2(.1, 10, 25, 2, TRUE, FALSE)
>> Error in pnbeta2(0.1, 10, 25, 2, TRUE, FALSE) :
>>         7 arguments passed to '.Internal' which requires 1
>>
>> When I try to run the .Internal file directly is perhaps where my
>> problems begin:
>> > .Internal(pnbeta2(.1, 10, 25, 2, TRUE, FALSE))
>> Error in .Internal(pnbeta2(0.1, 10, 25, 2, TRUE, FALSE)) :
>>         no internal function "pnbeta2"
>>
>> But, when I do the same thing to the pnbeta internal function (which
>> pnbeta2 is identical to at this point) I get the result:
>> > .Internal(pnbeta(.1, 10, 25, 2, TRUE, FALSE))
>> [1] 0.0006837318
>>
>> So, I'm at a loss for what is going on in this situation. my pnbeta2
>> internal function doesn't seem to be there. What I want to do is simple
>> (modify an interal C file), but it is proving to be quite difficult for
>> me to implement.
>>
>> Thanks for any thoughts,
>> Ken
>>
>>
>> Duncan Murdoch wrote:
>>
>>> On 11/7/2005 5:17 PM, Ken Kelley wrote:
>>>
>>>> Hi All.
>>>> I want to tweak a few minor things inside of internal C code. I have
>>>> my Win. XP machine set-up to build packages (including C code), but
>>>> I'm having problems getting the package to run correctly. In
>>>> particular, I want to modify a few things inside of pnbeta.c (namely
>>>> errmax and itrmax), which is what the pbeta() function calls upon when
>>>> there is a noncentral parameter. I copied the pnbeta.c C code, changed
>>>> its name [to pnbeta2.c], included the nmath.h, dpq.h files, lgamma.c,
>>>> and pbeta.c in my src folder (since the .h files were called upon and
>>>> the .c files were). I then created an R function that I thought would
>>>> call upon the new C code:
>>>>
>>>> pnbeta2 <- function(x, a, b, lambda, low.tail, log.p)
>>>> {
>>>> res <- .C("pnbeta2", as.double(x), as.double(a), as.double(b),
>>>> as.double(lambda), as.integer(low.tail), as.integer(log.p))
>>>> return(list(res))
>>>> }
>>>>
>>>> But after I built the package and loaded it it, the function doesn't
>>>> work (it isn't even recognized). I have no idea why this is failing.
>>>> Any information to help me figure out what I need to do to modify
>>>> internal C code generally, or specifically as it applies to this
>>>> scenario would be most helpful.
>>>
>>>
>>>
>>> You didn't say that you changed the name of the function, only the file
>>> that contained it.  If this wasn't an oversight, then you should put
>>>
>>> double pnbeta2(double x, double a, double b, double lambda,
>>>           int lower_tail, int log_p)
>>>
>>> in the appropriate place in your pnbeta2.c file.
>>>
>>> The other thing you should do is to add a PACKAGE argument to your .C
>>> call, just in case there is already a pnbeta2 function somewhere else
>>> (or will be some day).  In fact, if you do this, there should be no need
>>> to change the name of the function:  R will look in the package DLL
>>> rather than R.dll to find it.  Just make sure that the name in the .C
>>> call matches the declared name in the source.
>>>
>>> Duncan Murdoch
>>>
>>
>> -- 
>> Ken Kelley, Ph.D.
>> Inquiry Methodology Program
>> Indiana University
>> 201 North Rose Avenue, Room 4004
>> Bloomington, Indiana 47405
>> http://www.indiana.edu/~kenkel
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 

-- 
Ken Kelley, Ph.D.
Inquiry Methodology Program
Indiana University
201 North Rose Avenue, Room 4004
Bloomington, Indiana 47405
http://www.indiana.edu/~kenkel



From azzalini at stat.unipd.it  Tue Nov  8 13:04:58 2005
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Tue, 8 Nov 2005 13:04:58 +0100
Subject: [R] Output glm
In-Reply-To: <437085DC.2000100@ec.univaq.it>
References: <437085DC.2000100@ec.univaq.it>
Message-ID: <20051108130458.1e4218c2.azzalini@stat.unipd.it>

On Tue, 08 Nov 2005 12:02:52 +0100, Ciro Marziliano wrote:

CM> 
CM>  How can I obtain the likelihood ratio of a Poisson regression
CM>  model?

likelihood ratio for testing which hypothesis?

however, in GLM the deviance provides the essential ingredient
for the job, since
  deviance = 2(log L(saturated model) - log L(fitted model))

best wishes

Adelchi Azzalini

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit?? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/



From ivjuli at mail.ru  Tue Nov  8 12:42:37 2005
From: ivjuli at mail.ru (Julia Ivanova)
Date: Tue, 08 Nov 2005 14:42:37 +0300
Subject: [R] The problem with rcom pakage
Message-ID: <E1EZRs5-000Ifq-00.ivjuli-mail-ru@f36.mail.ru>

I have the problem with rcom 1.2.2 package. It does't work with R 2.2.0.
Is there any opportunities to correct the work of this package?
Thanks for your help.
Yulia



From dingjia at gmail.com  Tue Nov  8 14:08:15 2005
From: dingjia at gmail.com (jia ding)
Date: Tue, 8 Nov 2005 14:08:15 +0100
Subject: [R] question about R graphics-example plot attached
In-Reply-To: <91ae6e350511020703x34dc80dbj2bb9a92b07efc631@mail.gmail.com>
References: <91ae6e350511020703x34dc80dbj2bb9a92b07efc631@mail.gmail.com>
Message-ID: <91ae6e350511080508nb688ac7l647051c35ae29db8@mail.gmail.com>

---------- Forwarded message ----------
From: jia ding <dingjia at gmail.com>
Date: Nov 2, 2005 4:03 PM
Subject: question about R graphics-example plot attached
To: r-help at lists.r-project.org

Suppose I have the data set like this:
A 1
3
7
10

B 5
9
13
The numbers here actually is A or B's occurence positions.So it means,
position 1,3,7,10 is A;position 5,9,13 is B's occurence.

I want the plot is a line with a little dot (or bar) at the position
1(A-show red), position 3(A-red),position
5(B-blue),7(A-red),10(A-red),13(B-blue)

I am not sure, If I explained very clearly about my question. What a
pity google group not support to attach a file. Otherwise,I can provide
an example.

Thanks for the help!
Thanks Marc.
 DJ
-------------- next part --------------
A non-text attachment was scrubbed...
Name: output.png
Type: image/png
Size: 1605 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051108/5d290440/output.png

From maechler at stat.math.ethz.ch  Tue Nov  8 14:10:53 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 8 Nov 2005 14:10:53 +0100
Subject: [R] Modifying Internal C Files
In-Reply-To: <43708D15.1000709@Indiana.Edu>
References: <436FD288.8000600@Indiana.Edu> <436FD588.7040400@stats.uwo.ca>
	<43701A95.9030207@Indiana.Edu>
	<Pine.LNX.4.61.0511080608230.13011@gannet.stats>
	<43708D15.1000709@Indiana.Edu>
Message-ID: <17264.41949.642263.250531@stat.math.ethz.ch>

Hi Ken,
just to repeat Prof Ripley's answer:

- R-devel has already increased itrmax (from 100 to 1000),
  and if you don't want to compile R-devel yourself, there are precompiled
  R-devel versions for Windows {maybe MacOS X too}.
 
 [Also, there's an E-mail on Nov 3 from Thomas Lumley to you and R-help
  where he replied to your posting from Oct 24, and told you
  exactly this; did you somehow not get that mail? (please
  answer offline; this gets *way* off-topic!)]

  Indeed, we are interested to hear if the change has been
  sufficient for all cases.  Personally, I think the real
  solution would have to be a different formula / algorithm
  for "large" ncp values.

- If you want to change R's internals, do so and recompile R;
  that's considerably less work (for you) than trying to put the
  changes into a package {particulary since you should also
  do the vectorization that R's internal code does for you}.

- BTW: This topic has been quite techncal from the start, and I
       think the posting guide advises you to use the "R-devel"
       mailing list instead of R-help { __ next time __ }

BTW, thank you for your orginal report about the pf(*, ncp = "large")
misbehavior!

Regards,
Martin <Maechler at stat.math.ethz.ch>  http://stat.ethz.ch/people/maechler
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: +41-44-632-3408		fax: ...-1228			<><



From ligges at statistik.uni-dortmund.de  Tue Nov  8 14:24:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Nov 2005 14:24:45 +0100
Subject: [R] The problem with rcom pakage
In-Reply-To: <E1EZRs5-000Ifq-00.ivjuli-mail-ru@f36.mail.ru>
References: <E1EZRs5-000Ifq-00.ivjuli-mail-ru@f36.mail.ru>
Message-ID: <4370A71D.7040904@statistik.uni-dortmund.de>

Julia Ivanova wrote:

> I have the problem with rcom 1.2.2 package. It does't work with R 2.2.0.
> Is there any opportunities to correct the work of this package?

Why do you think it does not work? It passes the check under Windows.
Please read the posting guide which suggests what information is 
relevant to get appropriate help. Without giving any examples, we do not 
know what does not work for you. If you found a bug in the package, 
please report it to the package maintainer.

Uwe Ligges


> Thanks for your help.
> Yulia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ssk2031 at columbia.edu  Tue Nov  8 14:29:12 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Tue, 08 Nov 2005 08:29:12 -0500
Subject: [R] The problem with rcom pakage
In-Reply-To: <E1EZRs5-000Ifq-00.ivjuli-mail-ru@f36.mail.ru>
References: <E1EZRs5-000Ifq-00.ivjuli-mail-ru@f36.mail.ru>
Message-ID: <op.szxbqyiicrygw3@suresh1.mahoney.cpmc.columbia.edu>


You need the 2.0 beta package available at:

http://sunsite.univie.ac.at/rcom/download/RSrv200beta.exe

Also, there is a rcom mailing list at  
http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l for more help.

Suresh

On Tue, 08 Nov 2005 06:42:37 -0500, Julia Ivanova <ivjuli at mail.ru> wrote:

> I have the problem with rcom 1.2.2 package. It does't work with R 2.2.0.
> Is there any opportunities to correct the work of this package?
> Thanks for your help.
> Yulia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html



From ssk2031 at columbia.edu  Tue Nov  8 14:31:58 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Tue, 08 Nov 2005 08:31:58 -0500
Subject: [R] The problem with rcom pakage
In-Reply-To: <op.szxbqyiicrygw3@suresh1.mahoney.cpmc.columbia.edu>
References: <E1EZRs5-000Ifq-00.ivjuli-mail-ru@f36.mail.ru>
	<op.szxbqyiicrygw3@suresh1.mahoney.cpmc.columbia.edu>
Message-ID: <op.szxbvkcgcrygw3@suresh1.mahoney.cpmc.columbia.edu>


Sorry, I wrote too soon. I thought you meant rdcom. My fault... I dont  
know of anything wrong with rcom 1.2.2, it works for me.

Suresh

On Tue, 08 Nov 2005 08:29:12 -0500, Suresh Krishna <ssk2031 at columbia.edu>  
wrote:

>
> You need the 2.0 beta package available at:
>
> http://sunsite.univie.ac.at/rcom/download/RSrv200beta.exe
>
> Also, there is a rcom mailing list at  
> http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l for more help.
>
> Suresh
>
> On Tue, 08 Nov 2005 06:42:37 -0500, Julia Ivanova <ivjuli at mail.ru> wrote:
>
>> I have the problem with rcom 1.2.2 package. It does't work with R 2.2.0.
>> Is there any opportunities to correct the work of this package?
>> Thanks for your help.
>> Yulia
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!  
>> http://www.R-project.org/posting-guide.html
>



From andylehnert at gmx.de  Tue Nov  8 14:35:30 2005
From: andylehnert at gmx.de (Andreas Lehnert)
Date: Tue, 8 Nov 2005 14:35:30 +0100 (MET)
Subject: [R] setting labels on axis
Message-ID: <18160.1131456930@www52.gmx.net>


Dear R,

When I try to plot e.g.

a<-matrix(1:100,10,10)
image.plot(a,xlim=c(0,100),ylim=c(0,100))

I get the axis but without filling.
I tried a few things, but it only works if I delete the 
xlim and ylim. Then I have the range from 0:1.

Can anyone help me get the right labeling?

Thank you,

Andy

--



From ripley at stats.ox.ac.uk  Tue Nov  8 14:35:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Nov 2005 13:35:46 +0000 (GMT)
Subject: [R] The problem with rcom pakage
In-Reply-To: <E1EZRs5-000Ifq-00.ivjuli-mail-ru@f36.mail.ru>
References: <E1EZRs5-000Ifq-00.ivjuli-mail-ru@f36.mail.ru>
Message-ID: <Pine.LNX.4.61.0511081326170.7184@gannet.stats>

On Tue, 8 Nov 2005, Julia Ivanova wrote:

> I have the problem with rcom 1.2.2 package. It does't work with R 2.2.0.

It was released to coincide with R 2.2.0: it does not work with earlier 
versions ....

Did you read and follow the README in the package?

> Is there any opportunities to correct the work of this package?
> Thanks for your help.
> Yulia

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

What did the maintainer say when you contacted him as the posting guide 
suggests?  The posting guide suggests how to provide enough information to 
allow others to help you, which you have not done so here.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lisawang at uhnres.utoronto.ca  Tue Nov  8 14:59:41 2005
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Tue, 08 Nov 2005 08:59:41 -0500
Subject: [R] how to draw cumulative histogram
Message-ID: <4370AF4D.BFE81520@uhnres.utoronto.ca>

Hello there,

I am using R to plot some cumulative histogram for my data. Please help
in this case.

Thank you

Lisa Wang
Princess Margaret Hospital
Toronto
phone 416 946 4501 ext.4883



From murdoch at stats.uwo.ca  Tue Nov  8 15:01:21 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 08 Nov 2005 09:01:21 -0500
Subject: [R] setting labels on axis
In-Reply-To: <18160.1131456930@www52.gmx.net>
References: <18160.1131456930@www52.gmx.net>
Message-ID: <4370AFB1.4090108@stats.uwo.ca>

On 11/8/2005 8:35 AM, Andreas Lehnert wrote:
> Dear R,
> 
> When I try to plot e.g.
> 
> a<-matrix(1:100,10,10)
> image.plot(a,xlim=c(0,100),ylim=c(0,100))
> 
> I get the axis but without filling.
> I tried a few things, but it only works if I delete the 
> xlim and ylim. Then I have the range from 0:1.
> 
> Can anyone help me get the right labeling?

You need to tell us where you found that function.  Is it from the 
fields package?  If so, it treats its arguments the way the image() 
function does (from the graphics package), i.e. if you only supply a 
matrix, it assumes the axes run from 0 to 1.

If you want the axes to run from 0 to 100, then you call it as

image.plot(x=(0:10)*10, y=(0:10)*10, z=a)

Duncan Murdoch



From MSchwartz at mn.rr.com  Tue Nov  8 15:03:11 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 08 Nov 2005 08:03:11 -0600
Subject: [R] question about R graphics-example plot attached
In-Reply-To: <91ae6e350511080508nb688ac7l647051c35ae29db8@mail.gmail.com>
References: <91ae6e350511020703x34dc80dbj2bb9a92b07efc631@mail.gmail.com>
	<91ae6e350511080508nb688ac7l647051c35ae29db8@mail.gmail.com>
Message-ID: <1131458592.6252.17.camel@localhost.localdomain>

On Tue, 2005-11-08 at 14:08 +0100, jia ding wrote:
> ---------- Forwarded message ----------
> From: jia ding <dingjia at gmail.com>
> Date: Nov 2, 2005 4:03 PM
> Subject: question about R graphics-example plot attached
> To: r-help at lists.r-project.org
> 
> Suppose I have the data set like this:
> A 1
> 3
> 7
> 10
> 
> B 5
> 9
> 13
> The numbers here actually is A or B's occurence positions.So it means,
> position 1,3,7,10 is A;position 5,9,13 is B's occurence.
> 
> I want the plot is a line with a little dot (or bar) at the position
> 1(A-show red), position 3(A-red),position
> 5(B-blue),7(A-red),10(A-red),13(B-blue)
> 
> I am not sure, If I explained very clearly about my question. What a
> pity google group not support to attach a file. Otherwise,I can provide
> an example.
> 
> Thanks for the help!
> Thanks Marc.
>  DJ

DJ,

In follow up to your post to sci.stat.math and your e-mail to me, let me
put forth an alternative to the solution that I initially posted to
sci.stat.math.

First, let's set up the data again and repeat (for the sake of readers
here) my initial solution, followed by a second option, given your
example plot.


# Create a data frame with your data as above. 
# Use the I() construct to keep the colors as 
# characters, rather than factors 

data <- data.frame(pos = c(1, 3, 7, 10, 5, 9, 13), 
                   col = I(c(rep("red", 4), 
                             rep("blue", 3)))) 


# Data now looks like this: 

> data 
  pos  col 
1   1  red 
2   3  red 
3   7  red 
4  10  red 
5   5 blue 
6   9 blue 
7  13 blue 


# We need to sort the rows by the position number 
# for plotting in sequence 

data <- data[order(data$pos), ] 


# Data now looks like this: 

> data 
  pos  col 
1   1  red 
2   3  red 
5   5 blue 
3   7  red 
6   9 blue 
4  10  red 
7  13 blue 


# Now generate the plot, using the 'pos' values 
# for the x axis and fix the y axis at 1 
# Also set the plot type for both lines and points 
# The colors for the points will be set in sequence 
# Do not plot any annotation or axis values 

plot(data$pos, rep(1, 7), type = "b", 
     bg = data$col, pch = 21, ann = FALSE, 
     axes = FALSE) 

#  Now create the x axis with tick marks and labels 
# at the 'pos' values 

axis(1, at = data$pos) 


So that generates a plot using colored points with connecting lines. Now
seeing your example, where I took the word "line" to mean one connecting
the points, I see now you are referring to the x axis.

Here is the second option:

First start with the same approach to configuring your data so that you
end up with 'data' as per above. Now, we'll use plot() with type 'h',
which creates vertical lines. We will set the line widths (lwd) wider so
that they appear more as bars and adjust the line ends so that they are
square rather than round.

# Set line ends to square
par(lend = 2)

# Use the same approach as above, but type = 'h'
# and set line width to be fairly wide
plot(data$pos, rep(1, 7), type = "h", col = data$col, lwd = 20, 
     ann = FALSE, axes = FALSE)

# Now do the x axis at data values
axis(1, at = data$pos)


Does that get you closer to what you intended?

HTH,

Marc Schwartz



From andy_liaw at merck.com  Tue Nov  8 15:29:45 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 8 Nov 2005 09:29:45 -0500
Subject: [R] how to draw cumulative histogram
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED57C@usctmx1106.merck.com>

I'm not sure what a cumulative histogram is, but does the following help?

x <- rnorm(100)
plot(ecdf(x))

Andy

> From: Lisa Wang
> 
> Hello there,
> 
> I am using R to plot some cumulative histogram for my data. 
> Please help
> in this case.
> 
> Thank you
> 
> Lisa Wang
> Princess Margaret Hospital
> Toronto
> phone 416 946 4501 ext.4883
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From h.wickham at gmail.com  Tue Nov  8 15:37:47 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 8 Nov 2005 08:37:47 -0600
Subject: [R] how to draw cumulative histogram
In-Reply-To: <4370AF4D.BFE81520@uhnres.utoronto.ca>
References: <4370AF4D.BFE81520@uhnres.utoronto.ca>
Message-ID: <f8e6ff050511080637x7e75fd66ha555155098254ca@mail.gmail.com>

> I am using R to plot some cumulative histogram for my data. Please help
> in this case.

It's not exactly a cumulative histogram, but perhaps ?ecdf is what you want:
plot(ecdf(islands))

It's also possible to abuse hist to get a cumulative histogram:
h <- hist(islands)
h$counts <- cumsum(h$counts)
plot(h)

(but that was just a quick attempt so may break other code)

Hadley



From dlvanbrunt at gmail.com  Tue Nov  8 15:43:58 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Tue, 8 Nov 2005 09:43:58 -0500
Subject: [R] writing R shell scripts?
In-Reply-To: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>
Message-ID: <d332d3e10511080643w13773509rf5b7aacae8e0ac6e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051108/6a847bfb/attachment.pl

From pbarros at ualg.pt  Tue Nov  8 15:47:16 2005
From: pbarros at ualg.pt (Pedro de Barros)
Date: Tue, 08 Nov 2005 14:47:16 +0000
Subject: [R] Interpretation of output from glm
Message-ID: <6.1.2.0.2.20051108144644.026b6ec0@pop.ualg.pt>

I am fitting a logistic model to binary data. The response variable is a 
factor (0 or 1) and all predictors are continuous variables. The main 
predictor is LT (I expect a logistic relation between LT and the 
probability of being mature) and the other are variables I expect to modify 
this relation.

I want to test if all predictors contribute significantly for the fit or not
I fit the full model, and get these results

 > summary(HMMaturation.glmfit.Full)

Call:
glm(formula = Mature ~ LT + CondF + Biom + LT:CondF + LT:Biom,
     family = binomial(link = "logit"), data = HMIndSamples)

Deviance Residuals:
     Min       1Q   Median       3Q      Max
-3.0983  -0.7620   0.2540   0.7202   2.0292

Coefficients:
               Estimate Std. Error z value Pr(>|z|)
(Intercept) -8.789e-01  3.694e-01  -2.379  0.01735 *
LT           5.372e-02  1.798e-02   2.987  0.00281 **
CondF       -6.763e-02  9.296e-03  -7.275 3.46e-13 ***
Biom        -1.375e-02  2.005e-03  -6.856 7.07e-12 ***
LT:CondF     2.434e-03  3.813e-04   6.383 1.74e-10 ***
LT:Biom      7.833e-04  9.614e-05   8.148 3.71e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

     Null deviance: 10272.4  on 8224  degrees of freedom
Residual deviance:  7185.8  on 8219  degrees of freedom
AIC: 7197.8

Number of Fisher Scoring iterations: 8

However, when I run anova on the fit, I get
 > anova(HMMaturation.glmfit.Full, test='Chisq')
Analysis of Deviance Table

Model: binomial, link: logit

Response: Mature

Terms added sequentially (first to last)


            Df Deviance Resid. Df Resid. Dev P(>|Chi|)
NULL                        8224    10272.4
LT          1   2873.8      8223     7398.7       0.0
CondF       1      0.1      8222     7398.5       0.7
Biom        1      0.2      8221     7398.3       0.7
LT:CondF    1    142.1      8220     7256.3 9.413e-33
LT:Biom     1     70.4      8219     7185.8 4.763e-17
Warning message:
fitted probabilities numerically 0 or 1 occurred in: method(x = x[, varseq 
<= i, drop = FALSE], y = object$y, weights = object$prior.weights,


I am having a little difficulty interpreting these results.
The result from the fit tells me that all predictors are significant, while 
the anova indicates that besides LT (the main variable), only the 
interaction of the other terms is significant, but the main effects are not.
I believe that in the first output (on the glm object), the significance of 
all terms is calculated considering each of them alone in the model (i.e. 
removing all other terms), while the anova output is (as it says) 
considering the sequential addition of the terms.

So, there are 2 questions:
a) Can I tell that the interactions are significant, but not the main effects?
b) Is it legitimate to consider a model where the interactions are 
considered, but not the main effects CondF and Biom?

Thanks for any help,

Pedro



From francoisromain at free.fr  Tue Nov  8 15:56:40 2005
From: francoisromain at free.fr (francoisromain@free.fr)
Date: Tue, 08 Nov 2005 15:56:40 +0100
Subject: [R] how to draw cumulative histogram
In-Reply-To: <4370AF4D.BFE81520@uhnres.utoronto.ca>
References: <4370AF4D.BFE81520@uhnres.utoronto.ca>
Message-ID: <1131461800.4370bca8643bd@imp1-g19.free.fr>

Selon Lisa Wang <lisawang at uhnres.utoronto.ca>:

> Hello there,
>
> I am using R to plot some cumulative histogram for my data. Please help
> in this case.
>
> Thank you
>
> Lisa Wang

Hi Lisa,

Here is one way (if i am right in what is a cumulative histogram), using the
existing possibilities

cumhist <- function(x, plot=TRUE, ...){
  h <- hist(x, plot=FALSE, ...)
  h$counts <- cumsum(h$counts)
  h$density <- cumsum(h$density)
  h$itensities <- cumsum(h$itensities)

  if(plot)
    plot(h)
  h
}


R> x <- rnorm(100)
R> cumhist(x)
R> # hist(x, add=TRUE, col="cyan")
R> # if you want to overlay the original histogram

-- Romain



From koen.hufkens at telenet.be  Tue Nov  8 16:15:03 2005
From: koen.hufkens at telenet.be (Koen Hufkens)
Date: Tue, 08 Nov 2005 16:15:03 +0100
Subject: [R] wavethresh and multiresolutional decomposition (mallat)
Message-ID: <4370C0F7.4020908@telenet.be>

Hi list,

I'm trying to calculate and playing around with multiresolutional 
decomposition (MRD, Mallat 1989) using wavelets and the wavethresh3 
package. Therefor I should "clear" all the unwanted wavelet coefficient 
data from the wavelet coefficient matrix/vector? that's produced by 
wavethresh3 and the imwd() command.

All data can be called for with waveletcoeff$wXLY where X and Y are the 
level and the direction (horizontal, vertical, diagonal) in 2d wavelet 
decompositions.

waveletcoeff$wXLY <- 0 will clear the vector and replace it with just 
one 0 giving strange results when using the inverse wavelet transform.

So I was thinking of exporting waveletcoeff$wXLY to some variable, and 
clearing it

somelevel <- waveletcoeff$wXLY
somelevel[1:n,] <- 0
decomposition$wXLY <- somelevel

But this gives as strange a result as just setting everything to 0 in 
the first place.

So if anyone has any clues to how to do this and how to calculate the 
MRD using R in general?

Cheers,
Koen



From nassar at noos.fr  Tue Nov  8 16:31:59 2005
From: nassar at noos.fr (Naji)
Date: Tue, 08 Nov 2005 16:31:59 +0100
Subject: [R] Hybrid Monte Carlo algorithm (MCMC)
Message-ID: <BF96837F.7B37%nassar@noos.fr>

Hi all,

I'm trying to estimate a nested model (purchase decision, cloglog formula, &
quantity bought given a purchase, truncated Poisson). Some of the parameters
are mixed (6) and 4 are fixed for all the respondent.
The simulated ML (500 simulations) method forwards highly correlated
estimates.
After some research, Hybrid Monte Carlo seems to be a good alternative to
estimate the model. I found neither article nor reference in order to
establish some code.

I would appreciate a good reference describing the algorithm in detail or
some code. Other ideas performing the model estimation are welcome.


I have established the log-likelihood function and its gradient.
Best regards
Naji



From rcmcll at yahoo.com  Tue Nov  8 17:24:13 2005
From: rcmcll at yahoo.com (bob mccall)
Date: Tue, 8 Nov 2005 08:24:13 -0800 (PST)
Subject: [R] frequency() source code
In-Reply-To: <BAY103-F2932AD7CDD337B663565A4A6640@phx.gbl>
Message-ID: <20051108162414.98173.qmail@web60012.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051108/77f66410/attachment.pl

From chris.jackson at imperial.ac.uk  Tue Nov  8 17:25:48 2005
From: chris.jackson at imperial.ac.uk (Chris Jackson)
Date: Tue, 08 Nov 2005 16:25:48 +0000
Subject: [R] Newbie on functions
In-Reply-To: <XFMail.051107201409.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051107201409.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <4370D18C.1000707@imperial.ac.uk>

(Ted Harding) wrote:
> On 07-Nov-05 Angelo Secchi wrote:
>> 
>> Hi,
>> I'm trying to write a simple function like
>> 
>> case1 <- function (m, cov, Q, R) {
>>   theta     <- (acos(R/sqrt(Q^3)))
>>   beta      <- (-2)*sqrt(Q)*cos(theta/3)+m[1]/3
>>   rho1      <- (-2)*sqrt(Q)*cos((theta+2*pi)/3)+m[1]/3
>>   rho2     <- (-2)*sqrt(Q)*cos((theta-2*pi)/3)+m[1]/3
>>   stderrb   <-  deltamethod( ~(-2)*sqrt(Q)*cos(theta/3)+x1/3,m,cov)
>>   stderrr1  <-  deltamethod( ~(-2)*sqrt(Q)*cos((theta+2*pi)/3)+x1/3, m,
>> cov) stderrr2  <-  deltamethod( ~(-2)*sqrt(Q)*cos((theta-2*pi)/3)+x1/3,
>> m, cov) stderr    <- c(stderrb,stderrr1,stderrr2)
>>   results   <- c(beta,rho1,rho2,stderr)
>>   results2  <- t(results)
>>   results2
>> }
>> 
>> When I call the function in an IF statement like
>> 
>> if (Q^3>R^2) results2 <- case1() else print('ciccio')
>> 
>> I get 
>> 
>> Error in eval(expr, envir, enclos) : Object "theta" not found
>> 
>> I do not understand why, any help?
> 
> Because when the function 'case1' is called as "case1()" it
> has no information on the values of m, cov, Q and R since
> it only looks in its argument list for these values (and
> not in the environment from which you called it).

In that case the error message would be something like "Argument m is
missing, with no default".

But in this case it looks like the "eval" error message is from within
the deltamethod() function from my CRAN package msm.  Unfortunately, a
limitation of this function is that user-defined variables are not
visible in the first "formula" argument.  I should make the
documentation clearer.

You could work around this as follows.  Build a string from the
values of Q and theta using sprintf(), then convert it into a
formula to pass to deltamethod():

form <- sprintf("~(-2)*sqrt(%f)*cos(%f/3)+x1/3", Q, theta)
stderrb <- deltamethod(as.formula(form), m, cov)

Mail me off list if you want any further help, as this is quite
specialised.

Chris

-- 
Christopher Jackson <chris.jackson at imperial.ac.uk>, Research Associate,
Department of Epidemiology and Public Health, Imperial College
School of Medicine, Norfolk Place, London W2 1PG



From Jan.Wijffels at ucs.kuleuven.be  Tue Nov  8 17:29:31 2005
From: Jan.Wijffels at ucs.kuleuven.be (Jan Wijffels)
Date: Tue, 8 Nov 2005 17:29:31 +0100
Subject: [R] proportional odds assumption with mixed model
Message-ID: <00e101c5e481$98fb0930$2c70210a@UCSPC32>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051108/8ded89e0/attachment.pl

From vdemart1 at tin.it  Tue Nov  8 17:31:58 2005
From: vdemart1 at tin.it (Vittorio)
Date: Tue, 8 Nov 2005 17:31:58 +0100 (GMT+01:00)
Subject: [R] RODBC fails to build
Message-ID: <26568880.1131467518408.JavaMail.root@pswm16.cp.tin.it>

Context:Pentium 4, FreeBSD 5.4, R 2.2.0

I updated the extra packages I 
had downloaded but the upgrading of RODBC failed complaining:

checking 
for unistd.h... yes
checking sql.h usability... no
checking sql.h 
presence... no
checking for sql.h... no
checking sqlext.h usability... 
no
checking sqlext.h presence... no
checking for sqlext.h... no
configure: error: "ODBC headers sql.h and sqlext.h not found"
ERROR: 
configuration failed for package 'RODBC'

The two header files are 
present in my box under /usr/local/include; I tried to symlink them to 
/usr/include and to /usr/local/lib/R/include to no avail.

Please help.

Vittorio



From ggrothendieck at gmail.com  Tue Nov  8 17:45:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 8 Nov 2005 11:45:55 -0500
Subject: [R] frequency() source code
In-Reply-To: <20051108162414.98173.qmail@web60012.mail.yahoo.com>
References: <BAY103-F2932AD7CDD337B663565A4A6640@phx.gbl>
	<20051108162414.98173.qmail@web60012.mail.yahoo.com>
Message-ID: <971536df0511080845o7476a20cu841e365a6b83ed79@mail.gmail.com>

This depends on how you constructed your time series in the
first place.  If you do it like this:

x <- ts(1:10, freq = 4)
frequency(x)

then frequency will be 4.

Also, note that the zoo package contains a yearqtr class which
may or may not be of interest to you.

On 11/8/05, bob mccall <rcmcll at yahoo.com> wrote:
>
>
> Thanks for the replys and the link. I'll look through that file. I couldn't understand why frequency() returns 1 when quarterly seasonal data is used. Maybe I can find out looking at the code.
>
>
>
> Thanks again,
>
> Bob
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From claus.atzenbeck at freenet.de  Tue Nov  8 18:10:02 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Tue, 8 Nov 2005 18:10:02 +0100 (CET)
Subject: [R] reduce levels
In-Reply-To: <B998A44C8986644EA8029CFE6396A924304C4C@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A924304C4C@exqld2-bne.qld.csiro.au>
Message-ID: <Pine.OSX.4.61.0511081808160.14679@cirrus.aue.aau.dk>

On Tue, 8 Nov 2005 Bill.Venables at csiro.au wrote:

> sub$mm <- factor(sub$mm)

Great. This is exactly what I need.

> is the simplest way to change a single factor in this way.  To do a
> whole data frame you just need a loop:
>
> drop_unused_levels <- function(data)
> 	as.data.frame(lapply(data, function(x) if(is.factor(x))
> factor(x) else x ))

Thanks for this additional information. Maybe I'll be in need of it
later.

Greetings,
Claus



From christophe.pouzat at univ-paris5.fr  Tue Nov  8 18:18:13 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Tue, 08 Nov 2005 18:18:13 +0100
Subject: [R] Hybrid Monte Carlo algorithm (MCMC)
In-Reply-To: <BF96837F.7B37%nassar@noos.fr>
References: <BF96837F.7B37%nassar@noos.fr>
Message-ID: <4370DDD5.4090808@univ-paris5.fr>

Hi,

A clear introduction to Hybrid MC can be found in Neal (1993) and an 
example of its use in Chen et al (2001). A good alternative to it when 
you have slowly relaxing chains is the Replica Exchange / Parallel 
Tempering Method, which was introduced by Geyer (1991) and Hukushima and 
Nemoto (1996). Its well discribed in Iba (2001).

HTH,

Christophe.

PS: The references:

@TechReport{Neal_1993,
  Author = {Neal, Radford M},
  Title= {Probabilistic {I}nference {U}sing {M}arkov {C}hain
                   {M}onte {C}arlo {M}ethods},
  Institution= {Department of Computer Science. University of Toronto},
  Number = {CRG-TR-91-1},
  url        = {http://www.cs.toronto.edu/~radford/papers-online.html},
  year       = 1993
}

@InProceedings{ChenEtAl_2001,
  Author = {Chen, Lingyu and Qin, Zhaohui and Liu, Jun S.},
  Title = {Exploring hybrid {M}onte {C}arlo in {B}ayesian
                   computation},
  BookTitle = {I{SBA} 2000, {P}roceedings},
  url = {http://www.people.fas.harvard.edu/~junliu/TechRept/01.html},
  year = 2001
}

@InProceedings{Geyer_1991b,
  Author = {Geyer, C. J.},
  Title = {Markov chain {M}onte {C}arlo maximum likelihood},
  BookTitle = {Computing {S}cience and {S}tatistics: {P}roc. 23rd
                   {S}ymp. {I}nterface},
  Pages = {156-163},
  year = 1991
}

@Article{HukushimaNemoto_1996,
  Author = {Hukushima, K. and Nemoto, K.},
  Title = {Exchange {M}onte {C}arlo and {A}pplication to {S}pin
                   {G}lass {S}imulations},
  Journal = {J Phys Soc Japan},
  Volume = {65},
  Pages = {1604-1608},
  eprint = {cond-mat/9512035},
  year = 1996
}

@Article{Iba_2001,
  Author = "Iba, Yukito",
  Title = "Extended Ensemble Monte Carlo",
  Journal = "Int. J. Mod. Phys.",
  Volume = "C12",
  Pages = "623-656",
  eprint = "cond-mat/0012323",
  url = {http://fr.arxiv.org/abs/cond-mat/0012323},
  year = 2001
}


Naji wrote:

>Hi all,
>
>I'm trying to estimate a nested model (purchase decision, cloglog formula, &
>quantity bought given a purchase, truncated Poisson). Some of the parameters
>are mixed (6) and 4 are fixed for all the respondent.
>The simulated ML (500 simulations) method forwards highly correlated
>estimates.
>After some research, Hybrid Monte Carlo seems to be a good alternative to
>estimate the model. I found neither article nor reference in order to
>establish some code.
>
>I would appreciate a good reference describing the algorithm in detail or
>some code. Other ideas performing the model estimation are welcome.
>
>
>I have established the log-likelihood function and its gradient.
>Best regards
>Naji
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From ripley at stats.ox.ac.uk  Tue Nov  8 18:18:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Nov 2005 17:18:59 +0000 (GMT)
Subject: [R] frequency() source code
In-Reply-To: <20051108162414.98173.qmail@web60012.mail.yahoo.com>
References: <20051108162414.98173.qmail@web60012.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0511081715400.13847@gannet.stats>

On Tue, 8 Nov 2005, bob mccall wrote:

> Thanks for the replys and the link. I'll look through that file. I 
> couldn't understand why frequency() returns 1 when quarterly seasonal 
> data is used. Maybe I can find out looking at the code.

Note that frequency only reads off the information in an object, e.g.

> frequency(presidents)
[1] 4

comes from

> tsp(presidents)
[1] 1945.00 1974.75    4.00

You may know you have quarterly data, but did you tell R?  Is it even 
known to R as a time series?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov  8 18:34:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Nov 2005 17:34:30 +0000 (GMT)
Subject: [R] RODBC fails to build
In-Reply-To: <26568880.1131467518408.JavaMail.root@pswm16.cp.tin.it>
References: <26568880.1131467518408.JavaMail.root@pswm16.cp.tin.it>
Message-ID: <Pine.LNX.4.61.0511081731080.14230@gannet.stats>

Did you read the package's README?  It contains two ways to set these 
paths.

On Tue, 8 Nov 2005, Vittorio wrote:

> Context:Pentium 4, FreeBSD 5.4, R 2.2.0
>
> I updated the extra packages I
> had downloaded but the upgrading of RODBC failed complaining:
>
> checking
> for unistd.h... yes
> checking sql.h usability... no
> checking sql.h
> presence... no
> checking for sql.h... no
> checking sqlext.h usability...
> no
> checking sqlext.h presence... no
> checking for sqlext.h... no
> configure: error: "ODBC headers sql.h and sqlext.h not found"
> ERROR:
> configuration failed for package 'RODBC'
>
> The two header files are
> present in my box under /usr/local/include; I tried to symlink them to
> /usr/include and to /usr/local/lib/R/include to no avail.
>
> Please help.
>
> Vittorio

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From redbeard at arrr.net  Tue Nov  8 18:39:05 2005
From: redbeard at arrr.net (Jarrett Byrnes)
Date: Tue, 8 Nov 2005 09:39:05 -0800
Subject: [R] Simple Nesting question/Odd error message
In-Reply-To: <loom.20051108T110550-647@post.gmane.org>
References: <e6ad7a3edf43b711cd1edce8bd5cbd50@arrr.net>
	<loom.20051108T110550-647@post.gmane.org>
Message-ID: <df1cfa611078fd3a05734e90e33618d4@arrr.net>

That works, but I am still puzzled as to why
lme(fixed = X.open ~ Dock, random=~Slip%in%Dock, data=my.data)

Does not work.  It yields the error

Error in getGroups.data.frame(dataMix, groups) :
	Invalid formula for groups

Whereas were I to treat slip as a fixed factor

aov(X.open ~ Dock + slipfactor%in%Dock, data=my.data)

works just fine.

Is there something fundamental that I am missing in the syntax?

On Nov 8, 2005, at 2:06 AM, Dieter Menne wrote:

> Jarrett Byrnes <redbeard <at> arrr.net> writes:
>
>> I'm having syntactical issues, however.  When I try
>>
>> dock.lme<-lme(X.open ~ Dock, random=Slip|~Dock, data=my.data)
>>
>> I get
>>
>> Error in inherits(object, "reStruct") : Object "Slip" not found
>>
>
> Syntax is wrong, should be something like
>
> dock.lme<-lme(X.open ~ Dock, random=~Slip|Dock, data=my.data)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rajnmsu79 at gmail.com  Tue Nov  8 18:43:19 2005
From: rajnmsu79 at gmail.com (Raja Jayaraman)
Date: Tue, 8 Nov 2005 11:43:19 -0600
Subject: [R] Can someone Help in nls() package
Message-ID: <a666434f0511080943g434e0dcfg9527bd9736222085@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051108/f3302853/attachment.pl

From tlumley at u.washington.edu  Tue Nov  8 18:53:39 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 8 Nov 2005 09:53:39 -0800 (PST)
Subject: [R] Simple Nesting question/Odd error message
In-Reply-To: <e6ad7a3edf43b711cd1edce8bd5cbd50@arrr.net>
References: <e6ad7a3edf43b711cd1edce8bd5cbd50@arrr.net>
Message-ID: <Pine.LNX.4.63a.0511080951210.27500@homer21.u.washington.edu>

On Mon, 7 Nov 2005, Jarrett Byrnes wrote:

> I'm attempting to analyze some survey data comparing multiple docks.  I
> surveyed all of the slips within each dock, but as slips are nested
> within docks, getting multiple samples per slip, and don't really
> represent any meaningful gradient, slip is a random effect.  There are
> also an unequal number of slips at each dock.
>
> I'm having syntactical issues, however.  When I try
>
> dock.lme<-lme(X.open ~ Dock, random=Slip|~Dock, data=my.data)

You want ~Slip|Dock, not Slip|~Dock

> I get
>
> Error in inherits(object, "reStruct") : Object "Slip" not found
>
> I'm uncertain as to what this means, as Slip most certainly is there
> (yes, I checked) - any pointers, or pointers about syntax for this as a
> general topic.
>
> And while I'm on it, is there a way to fit models with random effects
> using least squares instead of REML?  Thanks!

I'm not sure what you mean by "using least squares". You can use maximum 
likelihood in lme().

Alternatively, if the random effects are just a nuisance rather than the 
target of inference, and if your survey has a well-defined sampling scheme 
you might want to use svyglm() in the "survey" package.

 	-thomas



From leaflovesun at yahoo.ca  Tue Nov  8 18:13:43 2005
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Tue, 8 Nov 2005 10:13:43 -0700
Subject: [R] Variogram
Message-ID: <200511081754.jA8Hsg84032615@hypatia.math.ethz.ch>

Dear All,

Is there anybody has the experience in using variogram(gstat) ? Please kindly give me some hints about the results.


I used variogram() to build a semivariogram plot as:

tr.var=variogram(Incr~1,loc=~X+Y,data=TRI2TU,width=5)

then fir the variogram to get the parameters as:

 v.fit = fit.variogram(tr.var,vgm(0.5,"Exp",300,1))

v.fit
  model    psill    range
1   Nug 1.484879  0.00000
2   Exp 3.476700 29.70914

This is the output of v.fit. Can anybody help me write the exponential formula for this variogram?  I have the problem in understanding the result.

Thanks!

Leaf



From deepayan.sarkar at gmail.com  Tue Nov  8 19:19:09 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 8 Nov 2005 12:19:09 -0600
Subject: [R] lattice chart: different definitions for series
In-Reply-To: <OF20D49190.7DBEB7A0-ONC12570B3.0037C24E-C12570B3.003D8AC1@notes.fresenius.de>
References: <OF20D49190.7DBEB7A0-ONC12570B3.0037C24E-C12570B3.003D8AC1@notes.fresenius.de>
Message-ID: <eb555e660511081019g581c9631y7233cb7907257d3@mail.gmail.com>

On 11/8/05, ManuelPerera-Chang at fmc-ag.com <ManuelPerera-Chang at fmc-ag.com> wrote:
>
>
>
>
> Dear Deepayan,
>
> Thank you very much for the example.
>
> Now I am missing the grids inside my graph. As shown in my sample code
> yesterday, before I had them defined in the
>
> panel=function(x,y,...){panel.grid(h=-1,v=-1,col="grey",lty=2,cex=0.1)}
>
> and they were properly drawn, but you did not use this approach in your
> example, and my guesses this morning to place them together with your code
> failed.
>
> Sorry for my newbee question ..

Starting from my example, and going one step at a time:

0. My example was

xyplot(mct ~ year, mydf, groups = clinic,
       panel = panel.superpose.2,
       type = c("b", "b", "p"))

Since this works, it means that when 'panel.superpose.2' is eventually
called, it gets all the arguments it needs (the details are not
important).

1. You want a new panel function that adds something to the existing
one. So as a first try, let's reproduce the above plot with an
explicit panel function.

xyplot(mct ~ year, mydf, groups = clinic,
       panel = function(...) {
           panel.superpose.2(...)
       },
       type = c("b", "b", "p"))

This new panel function just calls panel.superpose.2, passing on all
its arguments. If you start  worrying about what those arguments are,
things will get very confusing very soon, so don't.

3. You need to add a grid, so modify your panel function accordingly.

xyplot(mct ~ year, mydf, groups = clinic,
       panel = function(...) {
           panel.grid(h = -1, v = -1)
           panel.superpose.2(...)
       },
       type = c("b", "b", "p"))

And, you're done. There's another alternative described in
?panel.superpose.2 that avoids writing a panel function, but I'll skip
that here.


4. For the record, suppose you needed to do something new in your
panel function that actually used the x, y, groups and subscripts
arguments. That's simple too, just do

xyplot(mct ~ year, mydf, groups = clinic,
       panel = function(x, y = NULL, subscripts, groups, ...) {
           ## do things using x, y, etc
           print(length(groups[subscripts]))
           panel.grid(h = -1, v = -1)
           panel.superpose.2(x = x, y = y,
                             subscripts = subscripts,
                             groups = groups, ...)
       },
       type = c("b", "b", "p"))



> My best regards,
>
> Manuel
>
> PS.: As for my "experiments", I was trying to use the document "Panel
> Function for Display Marked by groups", that you wrote. In this document I
> cannot see that e.g. all the arguments for panel.superpose.2 are optionals,
> as in your example below.

Hmm. If you mean that the graphical parameters have defaults, then
yes, that's a bit confusing. The defaults are left out in the
documentation because they are long, but you can see them using

args(panel.superpose.2)

> Further in this documentation you explained:
>
> "panel.groups - the panel function to be used for each group of points.
> Defaults to panel.xyplot (behavior in S) "
>
> and then ..
>
> "type -  usually a character vector specifying what should be drawn for
> each group, passed on to the panel.groups function, which must know what to
> do with it. By default this is panel.xyplot ..."
>
> consecuently I was trying to use "type" as an argument to panel.groups, but
> without success.
>
>                       Deepayan Sarkar
>
>                       <deepayan.sarkar@        To:
> ManuelPerera-Chang at fmc-ag.com
>
>                       gmail.com>               cc:
> r-help at stat.math.ethz.ch
>
>                                                Subject:  Re: lattice chart:
> different definitions for series
>                       07.11.2005 21:23
>
>
>
>
>
>
>
> On 11/7/05, ManuelPerera-Chang at fmc-ag.com <ManuelPerera-Chang at fmc-ag.com>
> wrote:
> >
> >
> >
> >
> > Hi enthusiasts,
> >
> > Trying to create a single chart in  lattice with different plotting
> > definitions for the different series (two series should be drawn with
> lines
> > and the other without them)
> >
> > I am using a dataset, which includes a grouping variable e.g. clinic with
> > three levels, the variable "year" and a continous variable: "mct".
> >
> > In the graph the variable "year" is in the x axis, with "mct" represented
> > in the y axis.
> >
> > The diagram should include two line diagrams(representing two of the
> > groups) , with the third group represented only with symbols(no lines).
> >
> > Until now I was using white lines to eliminate the lines drawn in the
> third
> > group, but this solution is not optimal, as the grids are sometimes not
> > visible
> >
> > sp<-list(superpose.symbol=list(pch=c(1,2,1),col=c("blue","red","green")),
> >        superpose.line=list(col=c("blue","red","white"),lty=c(1,2,)))
> >
> > ... and then including
> >
> > print(xyplot(mct~trend.data$year,groups=clinic,
> >   scales=list(x=list(at=c(15:pno),labels=per.labels)),
> >   main=main.title,
> >   sub=sub.title,
> >   xlab=x.label,
> >   ylab=y.label,
> >   xlim=c(pno-12,pno+1),
> >   panel=function(x,y,...){panel.grid(h=-1,v=-1,col="grey",lty=2,cex=0.1);
> >                   panel.superpose(x,y,type="l",lwd=1.8,...);
> >                   panel.superpose(x,y,type="p",cex=1.8,...))},
> >   key=sk,
> >   par.settings=sp));
> >
> > ... was also experimenting, and searching a lot in the WWW for
> >
> > panel.superpose.2 and type=c("b","b","p"), but without success.
>
> I don't know what experiments you did, but the following seems to work
> fine for me:
>
> library(lattice)
>
> mydf <-
>     data.frame(year = rep(1991:2000, 3),
>                mct =
>                rnorm(30,
>                      mean = rep(1:3, each = 10),
>                      sd = 0.5),
>                clinic = gl(3, 10))
>
> xyplot(mct ~ year, mydf, groups = clinic,
>        panel = panel.superpose.2,
>        type = c("b", "b", "p"))
>
> -Deepayan
>
>
>
>



From peteoutside at yahoo.com  Tue Nov  8 20:28:05 2005
From: peteoutside at yahoo.com (Pete Cap)
Date: Tue, 8 Nov 2005 11:28:05 -0800 (PST)
Subject: [R] Quickest way to match two vectors besides %in%?
Message-ID: <20051108192805.68111.qmail@web52401.mail.yahoo.com>

Hello list,

I have two data frames, X (48469,2) and Y (79771,5).

X[,1] contains distinct values of Y[,2].
I want to match values in X[,1] and Y[,2], then take
the corresponding value in [X,2] and place it in
Y[,4].

So far I have been doing it like so:
for(i in 1:48469) {
y[which(x[i,1]==y[,3]),4]<-x[i,2]
}

But it chunks along so very slowly that I can't help
but wonder if there's a faster way, mainly because on
my box it takes R about 30 seconds to simply COUNT to
48,469 in the for loop.

I have already tried using %in%.  It tells me if the
values in X[,1] are IN Y[,2], which is useful in
removing unnecessary values from X[,1].  But it does
not tell me exactly where they match.  which(X[,1]
%in% Y[,2]) does but it only matches on the first
instance.

This is the slowest part of the script I'm working
on--if I could improve it I could shave off some
serious operating time.  Any pointers?

Regards,

Pete



From pauljohn at ku.edu  Tue Nov  8 20:41:32 2005
From: pauljohn at ku.edu (Paul Johnson)
Date: Tue, 08 Nov 2005 13:41:32 -0600
Subject: [R] Need advice about models with ordinal input variables
Message-ID: <4370FF6C.9090708@ku.edu>

Dear colleagues:

I've been storing up this question for a long time and apologize for the 
length and verbosity of it.  I am having trouble in consulting with 
graduate students on their research projects.  They are using surveys to 
investigate the sources of voter behavior or attitudes.  They have 
predictors that are factors, some ordered, but I am never confident in 
telling them what they ought to do.  Usually, they come in with a 
regression model fitted as though these variables are numerical, and if 
one looks about in the social science literature, one finds that many 
people have published doing the same.

I want to ask your advice about some cases.

1. An ordered factor that masquerades as a numerical "interval level" score.

In the research journals, these are the ones most often treated as 
numerical variables in regressions. For example: "Thermometer scores" 
for Presidential candidates range from 0 to 100 in integer units.

What's a better idea?  In an OLS model with just one input variable, a 
plot will reveal if there is a significant "nonlinearity". One can 
recode the assigned values to linearize the final model or take the 
given values and make a nonlinear model.

In the R package "acepack" I found avas, which works like a "rubber 
ruler" and recodes variables in order to make relationships as linear 
and homoskedastic as possible.  I've never seen this used in the social 
science literature.  It works like magic.  Take an ugly scatterplot and 
shazam, out come transformed variables that have a beautiful plot.  But 
what do you make of these things?  There is so much going on in these 
transformations that interpretation of the results is very difficult. 
You can't say "a one unit increase in x causes a b increase in y". 
Furthermore, if the model is a survival model, a logistic regression, or 
other non-OLS model, I don't see how the avas approach will help.

I've tried fiddling about with smoothers, treating the input scores as 
if they were numerical.  I got this idea from Prof. Harrell's Regression 
Modeling Strategies.  In his Design package for R, one can include a 
cubic spline for a variable in a model by replacing x with rcs(x). Very 
convenient. If the results say the relationship is mostly linear, then 
we might as well treat the input variable as a numerical score and save 
some degrees of freedom.

But if the higher order terms are statistically significant, it is 
difficult to know what to do. The best strategy I have found so far is 
to calculate fitted values for particular inputs and then try to tell a 
story about them.


2. Ordinal variables with less than 10 values.

Consider variables like self-reported ideology, where respondents are 
asked to place themselves on a 7 point scale ranging from "very 
conservative" to "very liberal".  Or Party Identification on a 7 point 
scale, ranging (in the US) from "Strong Democrat" to "Strong Republican".

It has been quite common to see these thrown into regression models as 
if they were numerical.

I've sometimes found it useful to run a regression treating them as 
unordered factors, and then I attempt to glean a pattern in the 
coefficients.  If the parameter estimates step up by a fixed proportion, 
then one might think there's no damage from treating them as numerical 
variables.

Yesterday, it occurred to us that there should be a signifance test to 
determine if one looses predictive power by replacing the 
factor-treatment of x with x itself.  Is there a non-nested model test 
that is most appropriate?

3. Truly numericals variable that are reported as "grouped" ordinal 
scales. THese variables are aweful in many ways.

Income is often reported in a form like this:

1) Less than 20000
2) 20000 to 35000
3) 35001 to 50000
4) 50001 to 100000
5) above 100000

Education often appears in a form that has
1) 8 years or less
2) 9 years
3) 10 years
4) 11 years
5) 12 years
6) some college completed
7) undergraduate degree completed
8) graduate degree completed

These predictors pose many problems.  We have dissimilar people grouped 
together, so there are "errors in variables" and it seems obvious that 
the scores should be recoded somehow to reflect the substance of the 
differences among groups. But how?


4. Ordered variables with a small number of scores.

For example, "has your economic situation been
    1) worse
    2) same
    3) better"

or "how do you feel when you see the American flag?"
    1) no effect
    2) OK
    3) great
    4) extatic

Anyway, in an R model, I think the right thing to do is to enter them 
into a regression with as.ordered(x).

But I don't know what to say about the results.  Has anybody written an 
"idiots guide to orthogonal polynomials"?  Aside from calculating fitted 
values, how do you interpret these things?  Is there ever a point when 
you would say "we should treat that as a numerical variable with scores 
1-2-3-4" rather than as an ordered factor?


If you have advice, I would be delighted to hear it.

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From sun at cae.wisc.edu  Tue Nov  8 20:54:56 2005
From: sun at cae.wisc.edu (Hongyu Sun)
Date: Tue, 08 Nov 2005 13:54:56 -0600
Subject: [R] spatial epidemiology
In-Reply-To: <20051028094157.78516.qmail@web26606.mail.ukl.yahoo.com>
References: <20051028094157.78516.qmail@web26606.mail.ukl.yahoo.com>
Message-ID: <43710290.4070503@cae.wisc.edu>

Hi, All: Does R have such packages for spatial epidemiology? Many thanks,



From p.connolly at hortresearch.co.nz  Tue Nov  8 20:54:08 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 9 Nov 2005 08:54:08 +1300
Subject: [R] question about precision, floor, and powers of two.
In-Reply-To: <x2fyqcxv4x.fsf@viggo.kubism.ku.dk>
References: <e89bb7ac0510310903m77d159e6nb9f2e7188ff82d90@mail.gmail.com> 
	<436B6C61.7070606@statistik.uni-dortmund.de> 
	<x2fyqcxv4x.fsf@viggo.kubism.ku.dk>
Message-ID: <20051108195408.GS18619@hortresearch.co.nz>

On Fri, 04-Nov-2005 at 04:58PM +0100, Peter Dalgaard wrote:


|> In this particular case, it is slightly odd that we can't get an exact
|> answer for operations that could in principle be carried out using
|> integer arithmetic, but we're actually calculating log(8)/log(2).
|> 
|> (Curiously, the same effect is not seen on Linux or Solaris until 
|> 
|>  > log2(2^29)-29
|> [1] 3.552714e-15

"Until" is not quite the word:

> x <- 2:100
> data.frame(x, D = log2(2^x) -x)
     x            D
1    2 0.000000e+00
2    3 0.000000e+00
3    4 0.000000e+00
4    5 0.000000e+00
5    6 0.000000e+00
6    7 0.000000e+00
7    8 0.000000e+00
8    9 0.000000e+00
9   10 0.000000e+00
10  11 0.000000e+00
11  12 0.000000e+00
12  13 0.000000e+00
13  14 0.000000e+00
14  15 0.000000e+00
15  16 0.000000e+00
16  17 0.000000e+00
17  18 0.000000e+00
18  19 0.000000e+00
19  20 0.000000e+00
20  21 0.000000e+00
21  22 0.000000e+00
22  23 0.000000e+00
23  24 0.000000e+00
24  25 0.000000e+00
25  26 0.000000e+00
26  27 0.000000e+00
27  28 0.000000e+00
28  29 3.552714e-15
29  30 0.000000e+00
30  31 3.552714e-15
31  32 0.000000e+00
32  33 0.000000e+00
33  34 0.000000e+00
34  35 0.000000e+00
35  36 0.000000e+00
36  37 0.000000e+00
37  38 0.000000e+00
38  39 7.105427e-15
39  40 0.000000e+00
40  41 0.000000e+00
41  42 0.000000e+00
42  43 0.000000e+00
43  44 0.000000e+00
44  45 0.000000e+00
45  46 0.000000e+00
46  47 7.105427e-15
47  48 0.000000e+00
48  49 0.000000e+00
49  50 0.000000e+00
50  51 7.105427e-15
51  52 0.000000e+00
52  53 0.000000e+00
53  54 0.000000e+00
54  55 7.105427e-15
55  56 0.000000e+00
56  57 0.000000e+00
57  58 7.105427e-15
58  59 7.105427e-15
59  60 0.000000e+00
60  61 0.000000e+00
61  62 7.105427e-15
62  63 0.000000e+00
63  64 0.000000e+00
64  65 0.000000e+00
65  66 0.000000e+00
66  67 0.000000e+00
67  68 0.000000e+00
68  69 0.000000e+00
69  70 0.000000e+00
70  71 0.000000e+00
71  72 0.000000e+00
72  73 0.000000e+00
73  74 0.000000e+00
74  75 0.000000e+00
75  76 0.000000e+00
76  77 0.000000e+00
77  78 1.421085e-14
78  79 0.000000e+00
79  80 0.000000e+00
80  81 0.000000e+00
81  82 0.000000e+00
82  83 0.000000e+00
83  84 0.000000e+00
84  85 0.000000e+00
85  86 0.000000e+00
86  87 0.000000e+00
87  88 0.000000e+00
88  89 0.000000e+00
89  90 0.000000e+00
90  91 0.000000e+00
91  92 0.000000e+00
92  93 1.421085e-14
93  94 1.421085e-14
94  95 1.421085e-14
95  96 0.000000e+00
96  97 0.000000e+00
97  98 0.000000e+00
98  99 0.000000e+00
99 100 0.000000e+00

Is there a pattern?


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From murdoch at stats.uwo.ca  Tue Nov  8 21:00:16 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 08 Nov 2005 15:00:16 -0500
Subject: [R] Quickest way to match two vectors besides %in%?
In-Reply-To: <20051108192805.68111.qmail@web52401.mail.yahoo.com>
References: <20051108192805.68111.qmail@web52401.mail.yahoo.com>
Message-ID: <437103D0.6000904@stats.uwo.ca>

On 11/8/2005 2:28 PM, Pete Cap wrote:
> Hello list,
> 
> I have two data frames, X (48469,2) and Y (79771,5).
> 
> X[,1] contains distinct values of Y[,2].
> I want to match values in X[,1] and Y[,2], then take
> the corresponding value in [X,2] and place it in
> Y[,4].
> 
> So far I have been doing it like so:
> for(i in 1:48469) {
> y[which(x[i,1]==y[,3]),4]<-x[i,2]
> }
> 
> But it chunks along so very slowly that I can't help
> but wonder if there's a faster way, mainly because on
> my box it takes R about 30 seconds to simply COUNT to
> 48,469 in the for loop.
> 
> I have already tried using %in%.  It tells me if the
> values in X[,1] are IN Y[,2], which is useful in
> removing unnecessary values from X[,1].  But it does
> not tell me exactly where they match.  which(X[,1]
> %in% Y[,2]) does but it only matches on the first
> instance.
> 
> This is the slowest part of the script I'm working
> on--if I could improve it I could shave off some
> serious operating time.  Any pointers?

Look at the merge() function to add the X and Y columns to a new 
dataframe, then process that to merge the X[,2] and Y[,4] values.

It will be something like

Z <- merge(X, Y, by.x=1, by.y=2, all.y=TRUE)

changes <- !is.na(Z[,2])
Z[changes,5] <- Z[changes,2]

but you are almost certainly better off (from a maintenance point of 
view) to use the names of the columns, rather than guessing at column 
numbers.

Duncan Murdoch



From helprhelp at gmail.com  Tue Nov  8 21:15:52 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 8 Nov 2005 14:15:52 -0600
Subject: [R] Quickest way to match two vectors besides %in%?
In-Reply-To: <20051108192805.68111.qmail@web52401.mail.yahoo.com>
References: <20051108192805.68111.qmail@web52401.mail.yahoo.com>
Message-ID: <cdf817830511081215o2113ab37ke3c8000afcb559b5@mail.gmail.com>

?match

> x
  X1 X2
1  1  5
2  2  6
3  3  7
4  4  8

> y
  Y1 Y4
1  1  8
2  2  9
3  3 10
4  4 11
5  1 12
6  2 13
7  3 14
8  4 15

> y.orig<-y # backup

> y$Y4<-x$X2[match(y$Y1, x$X1)]
> y
  Y1 Y4
1  1  5
2  2  6
3  3  7
4  4  8
5  1  5
6  2  6
7  3  7
8  4  8


HTH,

Weiwei

On 11/8/05, Pete Cap <peteoutside at yahoo.com> wrote:
> Hello list,
>
> I have two data frames, X (48469,2) and Y (79771,5).
>
> X[,1] contains distinct values of Y[,2].
> I want to match values in X[,1] and Y[,2], then take
> the corresponding value in [X,2] and place it in
> Y[,4].
>
> So far I have been doing it like so:
> for(i in 1:48469) {
> y[which(x[i,1]==y[,3]),4]<-x[i,2]
> }
>
> But it chunks along so very slowly that I can't help
> but wonder if there's a faster way, mainly because on
> my box it takes R about 30 seconds to simply COUNT to
> 48,469 in the for loop.
>
> I have already tried using %in%.  It tells me if the
> values in X[,1] are IN Y[,2], which is useful in
> removing unnecessary values from X[,1].  But it does
> not tell me exactly where they match.  which(X[,1]
> %in% Y[,2]) does but it only matches on the first
> instance.
>
> This is the slowest part of the script I'm working
> on--if I could improve it I could shave off some
> serious operating time.  Any pointers?
>
> Regards,
>
> Pete
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From tgibson at augustcouncil.com  Tue Nov  8 21:20:17 2005
From: tgibson at augustcouncil.com (Todd A. Gibson)
Date: Tue, 8 Nov 2005 13:20:17 -0700
Subject: [R] Using split and sapply to return entire lines
Message-ID: <20051108202017.GA30278@bluefish.augustcouncil.com>

Hello,
I have a data manipulation problem that I can easily resolve by using
perl or python to pre-process the data, but I would prefer to do it
directly in R.

Given, for example:

  month length ratio monthly1 monthly2
1 Jan   23     0.1   9        6
2 Jan   45     0.2   9        6
3 Jan   16     0.3   9        6
4 Feb   14     0.2   1        9
5 Mar   98     0.4   2        2
6 Mar   02     0.6   2        2

(FWIW, monthly1 and monthly2 are unchanged for each month)

I understand how to do aggregations on single fields using split and
sapply, but how can I get entire lines.  For example, For the maximum
of data$length grouped by data$month I would like to get back some
form of:

2 Jan 45 0.2 9 6
4 Feb 14 0.2 1 9
5 Mar 98 0.4 2 2

For mean, I would like to average all columns:

Jan 28 0.2 9 6
Feb 14 0.2 1 9
Mar 50 0.5 2 2

Thank you,
-TAG
Todd A. Gibson



From redbeard at arrr.net  Tue Nov  8 21:26:38 2005
From: redbeard at arrr.net (Jarrett Byrnes)
Date: Tue, 8 Nov 2005 12:26:38 -0800
Subject: [R] Type II and III sums of squares with Error in AOV
Message-ID: <854e3fa432e69f6a30cda3bd28f8ccd6@arrr.net>

I've recently run into the problem of using aov with nested factors, 
and wanting to get the type II and III sums of squares.  Normally Anova 
from the car package would do fine, but it doesn't like having an Error 
included, so

my.aov <-aov(Response ~ Treatment + Error(Treatment:Replicate))
Anova(my.aov, type="II")

yields

Error in Anova(nested.anova) : no applicable method for "Anova"

And lm does not take Error as an argument.

Error in eval(expr, envir, enclos) : couldn't find function "Error"

Is there a way to get these additional types of sums of squares when 
Error is included in an aov model?



From jfox at mcmaster.ca  Tue Nov  8 21:40:12 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 8 Nov 2005 15:40:12 -0500
Subject: [R] Need advice about models with ordinal input variables
In-Reply-To: <4370FF6C.9090708@ku.edu>
Message-ID: <20051108204009.XIQE2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Paul,

I'll try to answer these question briefly.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Johnson
> Sent: Tuesday, November 08, 2005 2:42 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Need advice about models with ordinal input variables
> 
> Dear colleagues:
> 
> I've been storing up this question for a long time and 
> apologize for the length and verbosity of it.  I am having 
> trouble in consulting with graduate students on their 
> research projects.  They are using surveys to investigate the 
> sources of voter behavior or attitudes.  They have predictors 
> that are factors, some ordered, but I am never confident in 
> telling them what they ought to do.  Usually, they come in 
> with a regression model fitted as though these variables are 
> numerical, and if one looks about in the social science 
> literature, one finds that many people have published doing the same.
> 
> I want to ask your advice about some cases.
> 
> 1. An ordered factor that masquerades as a numerical 
> "interval level" score.
> 
> In the research journals, these are the ones most often 
> treated as numerical variables in regressions. For example: 
> "Thermometer scores" 
> for Presidential candidates range from 0 to 100 in integer units.
> 

In my opinion, such scales aren't even really measurments, but they have a
prima-facie reasonableness, and I wouldn't hesitate to use them in the
absence of something better.

> What's a better idea?  In an OLS model with just one input 
> variable, a plot will reveal if there is a significant 
> "nonlinearity". One can recode the assigned values to 
> linearize the final model or take the given values and make a 
> nonlinear model.
> 
> In the R package "acepack" I found avas, which works like a 
> "rubber ruler" and recodes variables in order to make 
> relationships as linear and homoskedastic as possible.  I've 
> never seen this used in the social science literature.  It 
> works like magic.  Take an ugly scatterplot and shazam, out 
> come transformed variables that have a beautiful plot.  But 
> what do you make of these things?  There is so much going on 
> in these transformations that interpretation of the results 
> is very difficult. 
> You can't say "a one unit increase in x causes a b increase in y". 
> Furthermore, if the model is a survival model, a logistic 
> regression, or other non-OLS model, I don't see how the avas 
> approach will help.
> 

As a general matter, the central issue seems to me the functional form of
the relationship between the variables, whether or not X really has a unit
of measurement. If you use a nonparametric fit (such as avas) or even if you
use a complex parametric fit, such as a regression (rather than smoothing)
spline, then why not think of the graph of the fit as the best description?

> I've tried fiddling about with smoothers, treating the input 
> scores as if they were numerical.  I got this idea from Prof. 
> Harrell's Regression Modeling Strategies.  In his Design 
> package for R, one can include a cubic spline for a variable 
> in a model by replacing x with rcs(x). Very convenient. If 
> the results say the relationship is mostly linear, then we 
> might as well treat the input variable as a numerical score 
> and save some degrees of freedom.
> 
> But if the higher order terms are statistically significant, 
> it is difficult to know what to do. The best strategy I have 
> found so far is to calculate fitted values for particular 
> inputs and then try to tell a story about them.
> 

This seems reasonable, though I'd usually prefer a graph to a table.

> 
> 2. Ordinal variables with less than 10 values.
> 
> Consider variables like self-reported ideology, where 
> respondents are asked to place themselves on a 7 point scale 
> ranging from "very conservative" to "very liberal".  Or Party 
> Identification on a 7 point scale, ranging (in the US) from 
> "Strong Democrat" to "Strong Republican".
> 
> It has been quite common to see these thrown into regression 
> models as if they were numerical.
> 
> I've sometimes found it useful to run a regression treating 
> them as unordered factors, and then I attempt to glean a 
> pattern in the coefficients.  If the parameter estimates step 
> up by a fixed proportion, then one might think there's no 
> damage from treating them as numerical variables.
> 

Since linear (and other similar) models don't make distributional
assumptions about the X's (other than independence from the errors), nothing
in principle changes.

> Yesterday, it occurred to us that there should be a 
> signifance test to determine if one looses predictive power 
> by replacing the factor-treatment of x with x itself.  Is 
> there a non-nested model test that is most appropriate?
> 

Actually, the models are nested, so, e.g., for a linear model, you could do
an incremental F-test. That is, a linear relationship is a special case of
any relationship at all, which is what you get by treating X as a factor.

> 3. Truly numericals variable that are reported as "grouped" 
> ordinal scales. THese variables are aweful in many ways.
> 
> Income is often reported in a form like this:
> 
> 1) Less than 20000
> 2) 20000 to 35000
> 3) 35001 to 50000
> 4) 50001 to 100000
> 5) above 100000
> 
> Education often appears in a form that has
> 1) 8 years or less
> 2) 9 years
> 3) 10 years
> 4) 11 years
> 5) 12 years
> 6) some college completed
> 7) undergraduate degree completed
> 8) graduate degree completed
> 
> These predictors pose many problems.  We have dissimilar 
> people grouped together, so there are "errors in variables" 
> and it seems obvious that the scores should be recoded 
> somehow to reflect the substance of the differences among 
> groups. But how?
> 

I don't see a difference in principle here, although if the intervals are
very wide, there is as you suggest a measurement-error problem. Since the
information is irretrievably lost at the point of data collection, there
isn't much to be done.

> 
> 4. Ordered variables with a small number of scores.
> 
> For example, "has your economic situation been
>     1) worse
>     2) same
>     3) better"
> 
> or "how do you feel when you see the American flag?"
>     1) no effect
>     2) OK
>     3) great
>     4) extatic
> 
> Anyway, in an R model, I think the right thing to do is to 
> enter them into a regression with as.ordered(x).
> 

That seems reasonable. Again, I don't see this as special. Treating the
variable as ordered would allow you to look at linear, quadratic, etc.,
terms.
 
> But I don't know what to say about the results.  Has anybody 
> written an "idiots guide to orthogonal polynomials"?  Aside 
> from calculating fitted values, how do you interpret these 
> things?  Is there ever a point when you would say "we should 
> treat that as a numerical variable with scores 1-2-3-4" 
> rather than as an ordered factor?
> 

If the linear term were the only important one, then that would be
equivalent to saying that you could treat the variable in this manner.
Otherwise you should combine the terms to interpret them, as is generally
the case when regressors are related by marginality.

I hope this helps,
 John

> 
> If you have advice, I would be delighted to hear it.
> 
> -- 
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66044-3177           FAX: (785) 864-5700
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From sourceforge at metrak.com  Tue Nov  8 21:47:08 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Wed, 09 Nov 2005 07:47:08 +1100
Subject: [R] Quickest way to match two vectors besides %in%?
In-Reply-To: <20051108192805.68111.qmail@web52401.mail.yahoo.com>
References: <20051108192805.68111.qmail@web52401.mail.yahoo.com>
Message-ID: <43710ECC.1060003@metrak.com>

Pete Cap wrote:
> Hello list,
> 
> I have two data frames, X (48469,2) and Y (79771,5).
> 
> X[,1] contains distinct values of Y[,2].
> I want to match values in X[,1] and Y[,2], then take
> the corresponding value in [X,2] and place it in
> Y[,4].
> 
> So far I have been doing it like so:
> for(i in 1:48469) {
> y[which(x[i,1]==y[,3]),4]<-x[i,2]
> }

I'm not sure but isn't that a case where merge() can help?

cheers



From grupo_logit at yahoo.es  Tue Nov  8 22:33:11 2005
From: grupo_logit at yahoo.es (Grupo Logit)
Date: Tue, 8 Nov 2005 22:33:11 +0100 (CET)
Subject: [R] (no subject)
Message-ID: <20051108213312.8249.qmail@web26904.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051108/693b13cc/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Tue Nov  8 22:35:49 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 08 Nov 2005 21:35:49 -0000 (GMT)
Subject: [R] how to draw cumulative histogram
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED57C@usctmx1106.merck.com>
Message-ID: <XFMail.051108213549.Ted.Harding@nessie.mcc.ac.uk>

On 08-Nov-05 Liaw, Andy wrote:
> I'm not sure what a cumulative histogram is, but does the following
> help?
> 
> x <- rnorm(100)
> plot(ecdf(x))
> 
> Andy

Maybe what is wanted is like the following:

  x<-rnorm(1000)
  h<-hist(x)
  h$counts<-cumsum(h$counts)
  plot.histogram(h)

Best wishes,
Ted.

>> From: Lisa Wang
>> 
>> Hello there,
>> 
>> I am using R to plot some cumulative histogram for my data. 
>> Please help in this case.
>> 
>> Thank you
>> 
>> Lisa Wang
>> Princess Margaret Hospital
>> Toronto
>> phone 416 946 4501 ext.4883


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Nov-05                                       Time: 21:35:45
------------------------------ XFMail ------------------------------



From tts_boopathy at yahoo.com  Tue Nov  8 22:36:00 2005
From: tts_boopathy at yahoo.com (shanmuha boopathy)
Date: Tue, 8 Nov 2005 13:36:00 -0800 (PST)
Subject: [R] how to plot the circles in matrix form
Message-ID: <20051108213600.68346.qmail@web33802.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051108/ed553433/attachment.pl

From vdemart1 at tin.it  Tue Nov  8 23:45:47 2005
From: vdemart1 at tin.it (vittorio)
Date: Tue, 8 Nov 2005 22:45:47 +0000
Subject: [R] RODBC fails to build
In-Reply-To: <Pine.LNX.4.61.0511081731080.14230@gannet.stats>
References: <26568880.1131467518408.JavaMail.root@pswm16.cp.tin.it>
	<Pine.LNX.4.61.0511081731080.14230@gannet.stats>
Message-ID: <200511082245.48134.vdemart1@tin.it>

yes, I read it. Nevertheless I can't make head or tail of it.
Can anyone out there help me with a step by step explanation?
Ciao
Vittorio

Alle 17:34, marted?? 08 novembre 2005, Prof Brian Ripley ha scritto:
> Did you read the package's README?  It contains two ways to set these
> paths.
>
> On Tue, 8 Nov 2005, Vittorio wrote:
> > Context:Pentium 4, FreeBSD 5.4, R 2.2.0
> >
> > I updated the extra packages I
> > had downloaded but the upgrading of RODBC failed complaining:
> >
> > checking
> > for unistd.h... yes
> > checking sql.h usability... no
> > checking sql.h
> > presence... no
> > checking for sql.h... no
> > checking sqlext.h usability...
> > no
> > checking sqlext.h presence... no
> > checking for sqlext.h... no
> > configure: error: "ODBC headers sql.h and sqlext.h not found"
> > ERROR:
> > configuration failed for package 'RODBC'
> >
> > The two header files are
> > present in my box under /usr/local/include; I tried to symlink them to
> > /usr/include and to /usr/local/lib/R/include to no avail.
> >
> > Please help.
> >
> > Vittorio



From itsme_410 at yahoo.com  Tue Nov  8 22:46:56 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Tue, 8 Nov 2005 13:46:56 -0800 (PST)
Subject: [R] OT: on the DFT of a periodic Gaussian kernel (analytic
	representation?)
Message-ID: <20051108214656.18611.qmail@web54515.mail.yahoo.com>

Hi,

I have a one-dimensional N-coordinate periodic Gaussian kernel defined by 

dnorm(c(0:N/2,(N/2-1):1),sd = h)

where h is the bandwidth/smoothing parameter/FWHM.

I want to calculate the Fourier transform of this, which can be done using a
FFT. But my question is: is it possible to get the Discrete Fourier transform
of this sequence analytically? Do there exist references on this?

Many thanks and best wishes,
Aarem



From fridolin.wild at wu-wien.ac.at  Wed Nov  9 00:03:51 2005
From: fridolin.wild at wu-wien.ac.at (Fridolin Wild)
Date: Wed, 09 Nov 2005 00:03:51 +0100
Subject: [R] sorting during xtabs? sorting by "individual" order?
Message-ID: <43712ED7.3000406@wu-wien.ac.at>


Hey alltogether,

refacturing a package (before it will be released),
I ran across the following problem.

I have two directories with different text files,
I want to read the first and construct a document-term
matrix from it (every term=word in a row, every file in
a column, occurrence frequencies form the values).

The second directory contains different files. It
needs to be read in to also construct a document-term
matrix -- however, in the same "term-order" to enable
similarity comparisons in a vector space of the
same format.

Let's make a (fake) example:

(1) support function

    # directory 1 contains 2 files (F1 & F2):
       F1 = c("word4", "word3", "word2")
       F2 = c("word1", "word4", "word2")

    # directory 2 contains also 2 files (F3 & F4):
       F3 = c("word1", "word2", "bla")
       F4 = c("word1", "word2", "word3")

    # I file in the first directory, file by file,
    # create triples of the format (file, word, 1)

        F1tab = sort(table(F1), decreasing = TRUE)
        F2tab = sort(table(F2), decreasing = TRUE)

    # and create a dataframe

        F1frame = data.frame( docs="F1", terms=names(F1tab),
                              Freq = F1tab, row.names = NULL)
        F2frame = data.frame( docs="F2", terms = names(F2tab),
                              Freq = F2tab, row.names = NULL)

(2) textmatrix function

    ... to be bound together for every file and to be
    converted with xtabs into a document term matrix:

        dummy = list(F1frame, F2frame)
        dtm = t(xtabs(Freq ~ ., data = do.call("rbind", dummy)))

        =>
               docs
        terms   F1 F2
          word2  1  1
          word3  1  0
          word4  1  1
          word1  0  1

    Now, when I want to re-use this to construct another
    document-term matrix from files F3&F4 -- with the same terms
    in the exactly same order, firstly, I need to add

        F3clean = F3[F3 %in% rownames(dtm)]
        F4clean = F4[F4 %in% rownames(dtm)]

    to keep "unwanted" terms from getting into the tabs.

    And here is my problem:

    I need to reformat the output document-term matrix
    (as it would be given by another time running step 2
    with F3clean and F4clean) to correspond with the given
    order of the rownames(dtm) of the first directory.

    How can I do this (not costly, the matrices I have to
    deal with are usually really big)? Hopefully just
    by adding s.th. to the xtabs function?

    To make an example of what I need: I need dtm2
    to look exactly like this (doc-order is not important):

        =>
               docs
        terms   F3 F4
          word2  1  1
          word3  1  1
          word4  0  0
          word1  1  1

    Can anybody help me?

Best,
Fridolin

-- 
Fridolin Wild, Institute for Information Systems and New Media,
Vienna University of Economics and Business Administration (WUW),
Augasse 2-6, A-1090 Wien, Austria
fon +43-1-31336-4488, fax +43-1-31336-746



From srini_iyyer_bio at yahoo.com  Wed Nov  9 00:09:13 2005
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Tue, 8 Nov 2005 15:09:13 -0800 (PST)
Subject: [R] Can I run both R and Python through Emacs
In-Reply-To: <eb555e660511081019g581c9631y7233cb7907257d3@mail.gmail.com>
Message-ID: <20051108230913.91650.qmail@web31603.mail.mud.yahoo.com>

Dear Group, 
 I am new to R and Emacs both.  Until now I have been
working through R's interactive space.  Today I
configured R+XEmacs by following John Fox help file -
how to configure Xemacs + R.  

I am very happy to do this. Now the question I have is
can I make XEmacs to be the editor for both R and
Python with syntax highliting etc. 

How can I do that. Could any one help me please.  

Thanks
Sri



From redbeard at arrr.net  Wed Nov  9 00:40:58 2005
From: redbeard at arrr.net (Jarrett Byrnes)
Date: Tue, 8 Nov 2005 15:40:58 -0800
Subject: [R] A Quick and (Very) Dirty Intro to Stats in R
Message-ID: <ef6bf117ece7120997e59453e7f1de7f@arrr.net>

Greetings to all,
	First off, I want to thank you all for answering any nagging questions 
I've had over the past few days.  I've been in the process of putting 
together A Quick and (Very) Dirty Intro to Doing Your Statistics in R  
(which I have posted to http://didemnid.ucdavis.edu/rtutorial.html ) in 
order to teach an R workshop for the graduate students in my 
department.  This is a guide for your everyday stats crunchers who want 
to free themselves from the cycle of SAS updates, have more flexibility 
than JMP or Statview will allow, but are not hardcore 
programming/think-about-stats-allday types.  These are people who get 
data from the natural world, and then find out what it's telling them.

So, to that end, I've put the guide together, and would be very 
interested in any comments you all would have.  Are there any 
statistical methods that you think I really should have included for 
this type of audience that I didn't (and if it's over my head, would 
you be interested in contributing)?  Is there anything just blatantly 
wrong or is unclear to a casual reader?

Most importantly, there are still a few holes that need to be filled - 
if they can be

1) A SIMPLE explanation for how to do mixed models using lme.  I am 
quite unsatisfied with most of what I've seen on the net, and if it 
even comes close to going over my head, it really won't fly with most 
folk I know.  I've done the best I can, but I know if falls short.
2) A method of looking at type II and III sums of squares for aov if 
there is a different error term included.
3) How does one plot canonical values and centroid groupings for a 
MANOVA?
4) How does one use manova to do repeated measures?  I've got the 
univariate method down, but would like to use manova a la the repeated 
statement in SAS.
5) Better output for post-hocs, and a Ryan's Q implementation.

Thanks in advance for any input, and I hope this can be a resource to a 
lot of people!


----------------------------------------
Jarrett Byrnes
Population Biology Graduate Group, UC Davis
Bodega Marine Lab
707-875-1969
http://www-eve.ucdavis.edu/stachowicz/byrnes.shtml



From sunyata at uvic.ca  Wed Nov  9 00:46:06 2005
From: sunyata at uvic.ca (Graham Watt-Gremm)
Date: Tue, 8 Nov 2005 15:46:06 -0800
Subject: [R] retrieve most abundant species by sample unit
Message-ID: <B0C5E226-9D56-4131-AAD3-3DEA8A7DC8EF@uvic.ca>

Hi R-users:
[R 2.2 on OSX 10.4.3]
I have a (sparse) vegetation data frame with 500 rows (sampling  
units) and 177 columns (plant species) where the data represent %  
cover. I need to summarize the cover data by returning the names of  
the most dominant and the second most dominant species per plot. I  
reduced the data frame to omit cover below 5%; this is what it looks  
like stacked. I have experimented with tapply(), by(), and some  
functions mentioned in archived postings, but I haven't seen anything  
that answers to this directly. Does anybody have any ideas?

      OBJECTID       PolygonID SpeciesCod AbundanceP
1       15006     ANT-CBG-rr1     Leymol    5.00000
3       15008     ANT-CBG-rr1     Ambcha    5.00000
5       15010      ANT-ESH-27     Atrpat   20.00000
6       15011      ANT-ESH-27     Ambcha   10.00000
11      15016      ANT-ESH-28     Salvir   20.00000
14      15019      ANT-ESH-28     Atrpat    5.00000
18      15023 ANT-POR-Rubarm5     Rubarm   60.00000
19      15024 ANT-POR-Rubarm5     Hedhel   40.00000
25      15030      ECO-CBG-A2     Griint    5.00000
27      15032      ECO-CBG-A2     Anngra    5.00000
38      15043      ECO-CBG-A4     Sperub   50.00000

Regards,
Graham Watt-Gremm



From p.dalgaard at biostat.ku.dk  Wed Nov  9 01:00:05 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Nov 2005 01:00:05 +0100
Subject: [R] A Quick and (Very) Dirty Intro to Stats in R
In-Reply-To: <ef6bf117ece7120997e59453e7f1de7f@arrr.net>
References: <ef6bf117ece7120997e59453e7f1de7f@arrr.net>
Message-ID: <x2ek5q8zcq.fsf@turmalin.kubism.ku.dk>

Jarrett Byrnes <redbeard at arrr.net> writes:

> 4) How does one use manova to do repeated measures?  I've got the 
> univariate method down, but would like to use manova a la the repeated 
> statement in SAS.

The examples for anova.mlm should detail this rather explicitly.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From jmburgos at u.washington.edu  Wed Nov  9 01:40:42 2005
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Tue, 08 Nov 2005 16:40:42 -0800
Subject: [R] Variograms and large distances
In-Reply-To: <mailman.9.1131447601.27031.r-help@stat.math.ethz.ch>
References: <mailman.9.1131447601.27031.r-help@stat.math.ethz.ch>
Message-ID: <4371458A.9020300@u.washington.edu>

Hello R list,
I need to compute empirical variograms using data from a large 
geographic area (~10^6 km2).  Although I could not find a specific 
reference, I assume that both geoR and gstat calculate distances among 
data points assuming points are on a flat surface (using the Pythagorean 
Theorem).  Because the location of my data is large and located near the 
pole, assuming that latitude and longitude are coordinates on a flat 
surface would introduce a -possibly large- bias in the empirical 
variogram estimate.  My questions are the following:

a)  Does geoR and gstat assume that points are on a flat surface?

b) If I first calculate the distances among points using the Haversine 
formula, it is possible to calculate the variogram with a matrix of 
distances among points (where n is the number of observations) and a 
vector of observation values?

Any help would be appreciated.

Julian

Julian M. Burgos

Fisheries Acoustics Research Lab
School of Aquatic and Fishery Science
University of Washington

1122 NE Boat Street
Seattle, WA  98105 

Phone: 206-221-6864



From cjbroz at ucdavis.edu  Wed Nov  9 01:48:52 2005
From: cjbroz at ucdavis.edu (Craig Brozinsky)
Date: Tue, 8 Nov 2005 16:48:52 -0800
Subject: [R] contrasts
Message-ID: <083DEA13-91AF-4B59-9F85-2A34684E2C3A@ucdavis.edu>

Hi,

I'm having difficulty specifying contrasts for a within subjects  
factor with 3 levels. I can do it correctly for my factors with 2- 
levels, but i'm not getting the correct results for a 3-level factor.

My design has 1 between subjects factor (gp) and 3 within subjects  
factors. Within factor "w" has 2 levels, within factor "x" has two  
factors, and within
factor "y" has 3 factors. i first ran an ombibus F-test using aov. i  
then compared all of my contrasts to that output to determine if i  
was calculating the
contrasts correctly.

i first calculated the within factor w's main effect and interaction  
with the aov and then using a contrast:
 > summary(aov(effect ~ gp * w * x * y + Error(subj/(w+x+y)),  
data=both.uni))
Error: subj:w
           Df Sum Sq Mean Sq F value Pr(>F)
w          1 1.3889  1.3889  1.1364 0.3465
gp:w       1 0.2222  0.2222  0.1818 0.6918
Residuals  4 4.8889  1.2222
...

with contrasts i got the same results using the following code:
contr <- matrix(c(
1, 1, 1, 1, 1, 1,-1, -1, -1,-1,-1,-1,), ncol=1)

taov <- aov(cbind (
w1x1y, w1x1y2, w1x1y3, w1x2y, w1x2y2, w1x2y3,w2x1y, w2x1y2, w2x1y3,  
w2x2y, w2x2y2, w2x2y3)  %*% contr ~ gp, data = both.mul)
summary(taov,intercept=T)

now, things get a little more complicated for the three-level factor,  
'y.' If my memory serves me correctly, i can achieve this by writing  
a contrast that compares level 1 with level3 and level 2 with level  
3. is that correct? if so the code should look something like:

contr <- matrix(c(
1, 0,
0, 1,
-1, -1,
1, 0,
0, 1,
-1, -1,
1, 0,
0, 1,
-1, -1,
1, 0,
0, 1,
-1, -1, ), nrow=12, byrow=T)

tmp<-manova(cbind(
w1x1y, w1x1y2, w1x1y3, w1x2y, w1x2y2, w1x2y3, w2x1y, w2x1y2,  
w2x1y3,w2x2y, w2x2y2, w2x2y3)  %*% contr ~ gp, data = both.mul)
summary(tmp, test="Wilks", intercept=T)

when i run this code, it gives me the incorrect degrees of freedom  
and a p value different from what i obtained in my omnibus anova.  
does anyone have an idea of where i went wrong?

if access to my dataset is useful, i list it below.

# set up matrix for control subjects
cont.mat <- c 
(3,1,5,2,1,3,2,3,5,2,2,4,1,2,4,1,2,6,0,1,4,1,1,4,3,3,4,3,1,5,3,2,6,4,4,5 
,)
nsubs_c <- length(cont.mat)/12
con.mat <- matrix(cont.mat,  nsubs_c, 12, T)
con.mul <- cbind.data.frame(subj=1:nsubs_c, conds=factor(rep(1,rep 
(nsubs_c,1))), con.mat)

dimnames(con.mul)[[2]] <-
c("subj","gp",
"w1x1y", "w1x1y2", "w1x1y3",
"w1x2y", "w1x2y2", "w1x2y3",
"w2x1y", "w2x1y2", "w2x1y3",
"w2x2y", "w2x2y2", "w2x2y3",)

con.uni <- data.frame(effect = as.vector(con.mat),
subj = factor(paste("s", rep(1:nsubs_c, 12), sep="")),
gp = factor( paste("gp", rep(c(1), nsubs_c*12), sep="")),
w = factor(paste("w", rep(c(1,2), c(nsubs_c*3*2,nsubs_c*3*2)), sep="")),
x = factor( paste("x", rep(rep(c(1, 2), c(nsubs_c*3, nsubs_c*3)),2),  
sep="")),
y = factor(paste("y", rep(rep(c(1:3), rep(c(nsubs_c),3)),4), sep="")),
row.names = NULL)

# set up matrix for patients
pati.mat <- c 
(3,1,4,0,2,3,2,2,5,1,1,5,5,0,3,1,1,2,3,1,4,0,1,4,1,1,4,1,0,3,0,2,4,2,1,4 
,)

nsubs_p <- length(pati.mat)/12
pat.mat <- matrix(pati.mat,  nsubs_p, 12, T)
pat.mul <- cbind.data.frame(subj=1:nsubs_p, conds=factor(rep(2,rep 
(nsubs_p,1))), pat.mat)

dimnames(pat.mul)[[2]] <-
c("subj","gp",
"w1x1y", "w1x1y2", "w1x1y3",
"w1x2y", "w1x2y2", "w1x2y3",
"w2x1y", "w2x1y2", "w2x1y3",
"w2x2y", "w2x2y2", "w2x2y3",)

pat.uni <- data.frame(effect = as.vector(pat.mat),
subj = factor(paste("s", rep((nsubs_c+1):(nsubs_c+nsubs_p), 12),  
sep="")),
gp = factor( paste("gp", rep(c(2), nsubs_p*12), sep="")),
w = factor(paste("w", rep(c(1,2), c(nsubs_p*3*2,nsubs_p*3*2)), sep="")),
x = factor( paste("x", rep(rep(c(1, 2), c(nsubs_p*3, nsubs_p*3)),2),  
sep="")),
y = factor(paste("y", rep(rep(c(1:3), rep(c(nsubs_p),3)),4), sep="")),
row.names = NULL)

both.uni <- rbind(con.uni, pat.uni) # univariate file containing  
everone's data
both.mul <- rbind(con.mul, pat.mul) # multivariate file containing  
everone's data

############ 1: give me the whole anova ##########

summary(aov(effect ~ gp * w * x * y + Error(subj/(w+x+y)),  
data=both.uni))

############ 2: give me the w main effect and interaction with gp  
interaction ############
contr <- matrix(c(
-1, -1, -1,
-1,-1,-1,
1, 1, 1,
1, 1, 1,
), ncol=1)

taov <- aov(cbind (
w1x1y, w1x1y2, w1x1y3,
w1x2y, w1x2y2, w1x2y3,
w2x1y, w2x1y2, w2x1y3,
w2x2y, w2x2y2, w2x2y3)  %*% contr ~ gp, data = both.mul)
summary(taov,intercept=T)

############ 3: give me the y main effect and interaction with gp  
interaction ############

contr <- matrix(c(
1, 0,
0, 1,
-1, -1,
1, 0,
0, 1,
-1, -1,
1, 0,
0, 1,
-1, -1,
1, 0,
0, 1,
-1, -1, ), nrow=12, byrow=T)

tmp<-manova(cbind(
w1x1y, w1x1y2, w1x1y3,
w1x2y, w1x2y2, w1x2y3,
w2x1y, w2x1y2, w2x1y3,
w2x2y, w2x2y2, w2x2y3)  %*% contr ~ gp, data = both.mul)

summary(tmp, test="Wilks", intercept=T)



From E.Catchpole at adfa.edu.au  Wed Nov  9 01:49:20 2005
From: E.Catchpole at adfa.edu.au (ecatchpole)
Date: Wed, 09 Nov 2005 11:49:20 +1100
Subject: [R] Can I run both R and Python through Emacs
In-Reply-To: <20051108230913.91650.qmail@web31603.mail.mud.yahoo.com>
References: <20051108230913.91650.qmail@web31603.mail.mud.yahoo.com>
Message-ID: <43714790.8060004@adfa.edu.au>

Try sending this question to ess-help at stat.math.ethz.ch.

Ted.

On 09/11/05 10:09,  Srinivas Iyyer wrote,:
> Dear Group, 
>  I am new to R and Emacs both.  Until now I have been
> working through R's interactive space.  Today I
> configured R+XEmacs by following John Fox help file -
> how to configure Xemacs + R.  
> 
> I am very happy to do this. Now the question I have is
> can I make XEmacs to be the editor for both R and
> Python with syntax highliting etc. 
> 
> How can I do that. Could any one help me please.  
> 
> Thanks
> Sri
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr E.A. Catchpole
Visiting Fellow
Univ of New South Wales at ADFA, Canberra, Australia
and University of Kent, Canterbury, England
- www.ma.adfa.edu.au/~eac
- fax: +61 2 6268 8786		
- ph:  +61 2 6268 8895



From ramasamy at cancer.org.uk  Wed Nov  9 01:52:27 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 09 Nov 2005 00:52:27 +0000
Subject: [R] retrieve most abundant species by sample unit
In-Reply-To: <B0C5E226-9D56-4131-AAD3-3DEA8A7DC8EF@uvic.ca>
References: <B0C5E226-9D56-4131-AAD3-3DEA8A7DC8EF@uvic.ca>
Message-ID: <1131497547.3893.34.camel@localhost.localdomain>

Your example does not appear to match your description of the problem.

If you want have a 500x177 matrix and want to find the largest and
second largest, you can try something like

 m <- matrix( sample( 101:115 ), nc=3 )

      [,1] [,2] [,3]
 [1,]  102  112  110 
 [2,]  111  106  104
 [3,]  108  101  103
 [4,]  114  115  105
 [5,]  113  107  109

 t( apply( m, 1, function(x){ 
           r <- rank(-x); c( which(r==1), which(r==2) ) } ) )

      [,1] [,2]
 [1,]    2    3
 [2,]    1    2
 [3,]    1    3
 [4,]    2    1 
 [5,]    1    3

This uses the fact that all entries in a column is always refers to the
same species. If you have stacked data (especially where the species
appear in a non-regular manner), then it becomes slightly more tricky to
find an elegant solution.

Regards, Adai



On Tue, 2005-11-08 at 15:46 -0800, Graham Watt-Gremm wrote:
> Hi R-users:
> [R 2.2 on OSX 10.4.3]
> I have a (sparse) vegetation data frame with 500 rows (sampling  
> units) and 177 columns (plant species) where the data represent %  
> cover. I need to summarize the cover data by returning the names of  
> the most dominant and the second most dominant species per plot. I  
> reduced the data frame to omit cover below 5%; this is what it looks  
> like stacked. I have experimented with tapply(), by(), and some  
> functions mentioned in archived postings, but I haven't seen anything  
> that answers to this directly. Does anybody have any ideas?
> 
>       OBJECTID       PolygonID SpeciesCod AbundanceP
> 1       15006     ANT-CBG-rr1     Leymol    5.00000
> 3       15008     ANT-CBG-rr1     Ambcha    5.00000
> 5       15010      ANT-ESH-27     Atrpat   20.00000
> 6       15011      ANT-ESH-27     Ambcha   10.00000
> 11      15016      ANT-ESH-28     Salvir   20.00000
> 14      15019      ANT-ESH-28     Atrpat    5.00000
> 18      15023 ANT-POR-Rubarm5     Rubarm   60.00000
> 19      15024 ANT-POR-Rubarm5     Hedhel   40.00000
> 25      15030      ECO-CBG-A2     Griint    5.00000
> 27      15032      ECO-CBG-A2     Anngra    5.00000
> 38      15043      ECO-CBG-A4     Sperub   50.00000
> 
> Regards,
> Graham Watt-Gremm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Wed Nov  9 02:04:08 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 09 Nov 2005 01:04:08 +0000
Subject: [R] how to plot the circles in matrix form
In-Reply-To: <20051108213600.68346.qmail@web33802.mail.mud.yahoo.com>
References: <20051108213600.68346.qmail@web33802.mail.mud.yahoo.com>
Message-ID: <1131498248.3893.41.camel@localhost.localdomain>

 m <- matrix(0, nc=4, nr=3)

 plot( 0, xlim=c( 1, ncol(m) ), ylim=c( 1, nrow(m) ), 
       type="n", ann=F, axes=F )

 points( expand.grid( 1:ncol(m), 1:nrow(m) ), pch=1 )


Run the last example in help(points) for more refinements and options.

Regards, Adai



On Tue, 2005-11-08 at 13:36 -0800, shanmuha boopathy wrote:
> Could you help me 
> to plot the circles in the form of matrix like
>  
> O  O  O  O
> O  O  O  O
> O  O  O  O
> 
> thank you..
>  
> with regards,
> boopathy.
> 
> 
> Thirumalai Shanmuha Boopathy, 
> Zimmer no : 07-15,
> Rtscher strasse 165, 
> 52072  Aachen . 
> Germany.
>  
> Home zone   :  0049 - 241 - 9813409
> Mobile zone :  0049 - 176 - 23567867
> 
> 
> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Wed Nov  9 02:25:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 8 Nov 2005 20:25:35 -0500
Subject: [R] how to plot the circles in matrix form
In-Reply-To: <20051108213600.68346.qmail@web33802.mail.mud.yahoo.com>
References: <20051108213600.68346.qmail@web33802.mail.mud.yahoo.com>
Message-ID: <971536df0511081725s329c550ap14f7f8e1a29cc907@mail.gmail.com>

Check out balloonplot in package gplots.

On 11/8/05, shanmuha boopathy <tts_boopathy at yahoo.com> wrote:
> Could you help me
> to plot the circles in the form of matrix like
>
> O  O  O  O
> O  O  O  O
> O  O  O  O
>
> thank you..
>
> with regards,
> boopathy.
>
>
> Thirumalai Shanmuha Boopathy,
> Zimmer no : 07-15,
> R??tscher strasse 165,
> 52072  Aachen .
> Germany.
>
> Home zone   :  0049 - 241 - 9813409
> Mobile zone :  0049 - 176 - 23567867
>
>
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From mbmiller at taxa.epi.umn.edu  Wed Nov  9 02:39:09 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Tue, 8 Nov 2005 19:39:09 -0600 (CST)
Subject: [R] writing R shell scripts?
In-Reply-To: <d332d3e10511080643w13773509rf5b7aacae8e0ac6e@mail.gmail.com>
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>
	<d332d3e10511080643w13773509rf5b7aacae8e0ac6e@mail.gmail.com>
Message-ID: <Pine.GSO.4.60.0511081919030.19524@taxa.epi.umn.edu>

Many thanks for the suggestions.  Here is a related question:

When I do things like this...

echo "matrix(rnorm(25*2),c(25,2))" | R --slave --no-save

...I get the desired result except that I would like to suppress the 
"[,1]" row and column labels so that only the values go to stdout.  What 
is the trick to making that work?

By the way, I find it useful to have a script in my path that does this:

#!/bin/sh
echo "$1" | /usr/local/bin/R --slave --no-save

Suppose that script was called "doR", then one could do things like this 
from the Linux/UNIX command line:

# doR 'sqrt(35.6)'
[1] 5.966574

# doR 'runif(1)'
[1] 0.8881654

Which I find to be handy for quick arithmetic and even for much more 
sophisticated things.  I'd like to get rid of the "[1]" though!

Mike

-- 
Michael B. Miller, Ph.D.
Assistant Professor
Division of Epidemiology and Community Health
and Institute of Human Genetics
University of Minnesota
http://taxa.epi.umn.edu/~mbmiller/



From jfox at mcmaster.ca  Wed Nov  9 02:49:46 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 8 Nov 2005 20:49:46 -0500
Subject: [R] Interpretation of output from glm
In-Reply-To: <6.1.2.0.2.20051108144644.026b6ec0@pop.ualg.pt>
Message-ID: <20051109014943.RCHG28424.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Pedro,


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pedro de Barros
> Sent: Tuesday, November 08, 2005 9:47 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Interpretation of output from glm
> Importance: High
> 
> I am fitting a logistic model to binary data. The response 
> variable is a factor (0 or 1) and all predictors are 
> continuous variables. The main predictor is LT (I expect a 
> logistic relation between LT and the probability of being 
> mature) and the other are variables I expect to modify this relation.
> 
> I want to test if all predictors contribute significantly for 
> the fit or not I fit the full model, and get these results
> 
>  > summary(HMMaturation.glmfit.Full)
> 
> Call:
> glm(formula = Mature ~ LT + CondF + Biom + LT:CondF + LT:Biom,
>      family = binomial(link = "logit"), data = HMIndSamples)
> 
> Deviance Residuals:
>      Min       1Q   Median       3Q      Max
> -3.0983  -0.7620   0.2540   0.7202   2.0292
> 
> Coefficients:
>                Estimate Std. Error z value Pr(>|z|)
> (Intercept) -8.789e-01  3.694e-01  -2.379  0.01735 *
> LT           5.372e-02  1.798e-02   2.987  0.00281 **
> CondF       -6.763e-02  9.296e-03  -7.275 3.46e-13 ***
> Biom        -1.375e-02  2.005e-03  -6.856 7.07e-12 ***
> LT:CondF     2.434e-03  3.813e-04   6.383 1.74e-10 ***
> LT:Biom      7.833e-04  9.614e-05   8.148 3.71e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> (Dispersion parameter for binomial family taken to be 1)
> 
>      Null deviance: 10272.4  on 8224  degrees of freedom 
> Residual deviance:  7185.8  on 8219  degrees of freedom
> AIC: 7197.8
> 
> Number of Fisher Scoring iterations: 8
> 
> However, when I run anova on the fit, I get  > 
> anova(HMMaturation.glmfit.Full, test='Chisq') Analysis of 
> Deviance Table
> 
> Model: binomial, link: logit
> 
> Response: Mature
> 
> Terms added sequentially (first to last)
> 
> 
>             Df Deviance Resid. Df Resid. Dev P(>|Chi|)
> NULL                        8224    10272.4
> LT          1   2873.8      8223     7398.7       0.0
> CondF       1      0.1      8222     7398.5       0.7
> Biom        1      0.2      8221     7398.3       0.7
> LT:CondF    1    142.1      8220     7256.3 9.413e-33
> LT:Biom     1     70.4      8219     7185.8 4.763e-17
> Warning message:
> fitted probabilities numerically 0 or 1 occurred in: method(x 
> = x[, varseq <= i, drop = FALSE], y = object$y, weights = 
> object$prior.weights,
> 
> 
> I am having a little difficulty interpreting these results.
> The result from the fit tells me that all predictors are 
> significant, while 
> the anova indicates that besides LT (the main variable), only the 
> interaction of the other terms is significant, but the main 
> effects are not.
> I believe that in the first output (on the glm object), the 
> significance of 
> all terms is calculated considering each of them alone in the 
> model (i.e. 
> removing all other terms), while the anova output is (as it says) 
> considering the sequential addition of the terms.
> 
> So, there are 2 questions:
> a) Can I tell that the interactions are significant, but not 
> the main effects?

In a model with this structure, the "main effects" represent slopes over the
origin (i.e., where the other variables in the product terms are 0), and
aren't meaningfully interpreted as main effects. (Is there even any data
near the origin?)
 
> b) Is it legitimate to consider a model where the interactions are 
> considered, but not the main effects CondF and Biom?

Generally, no: That is, such a model is interpretable, but it places strange
constraints on the regression surface -- that the CondF and Biom slopes are
0 over the origin.

None of this is specific to logistic regression -- it applies generally to
generalized linear models, including linear models.

I hope this helps,
 John



From jfox at mcmaster.ca  Wed Nov  9 02:52:11 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 8 Nov 2005 20:52:11 -0500
Subject: [R] A Quick and (Very) Dirty Intro to Stats in R
In-Reply-To: <ef6bf117ece7120997e59453e7f1de7f@arrr.net>
Message-ID: <20051109015209.DBER25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Jarrett,


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jarrett Byrnes
> Sent: Tuesday, November 08, 2005 6:41 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] A Quick and (Very) Dirty Intro to Stats in R
> 

. . .

> Most importantly, there are still a few holes that need to be 
> filled - if they can be
> 
> 1) A SIMPLE explanation for how to do mixed models using lme. 
>  I am quite unsatisfied with most of what I've seen on the 
> net, and if it even comes close to going over my head, it 
> really won't fly with most folk I know.  I've done the best I 
> can, but I know if falls short.

Possibly take a look at the appendix on mixed models to my R and S-PLUS
Companion to Applied Regression, available at
<http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix-mixed-model
s.pdf>. This was intended to be a simple explanation.

Regards,
 John



From leog at anicca-vijja.de  Wed Nov  9 02:54:21 2005
From: leog at anicca-vijja.de (=?ISO-8859-1?Q?Leo_G=FCrtler?=)
Date: Wed, 09 Nov 2005 02:54:21 +0100
Subject: [R] error in NORM lib
Message-ID: <437156CD.4040804@anicca-vijja.de>

Dear alltogether,

I experience very strange behavior of imputation of NA's with the NORM 
library. I use R 2.2.0, win32.
The code is below and the same dataset was also tried with MICE and 
aregImpute() from HMISC _without_ any problem.
The problem is as follows:

(1) using the whole dataset results in very strange imputations - values 
far beyond the maximum of the respective column, > 200%! and this is 
reproducible and true for the whole set of imputed NAs
(2) using just part (i.e. columns) of the dataset results in the fact 
that some NAs are not imputed at all, i.e. NAs are still in the dataset 
- but there is neither a warning nor an error
(3) data.augmentation with da.norm() fails, but not after the first 
step, mostly 3-5 steps are ok, then it stops (see below)

The dataset is from educational research and should be almost normal 
distributed (slight deviations, but not really that heavy to explain the 
strange results).
I don't understand this, because the dataset works well with MICE and 
aregImpute() and other statistics _and_ I checked the manpages and it 
does not seem that the calls are wrong.
Thus, either it depends on the dataset (but why?) or it is maybe a bug.

I appreciate every help,

thanks,

leo g??rtler

<---snip--->

library(norm)
rngseed(1234)
load(url("http://www.anicca-vijja.de/lg/dframe.Rdata"))   # load object 
"dframe"
dim(dframe)
apply(dframe,2,function(x) sum(is.na(x))) # check how many NAs in the 
dataset
#dframe <- 
subset(dframe,select=-c(alter,grpzugeh,is1,is4,is6,klassenstufe,mmit,vorai,vorap,voras,vorkf,vorsg,vorvb))
s1 <- prelim.norm(dframe)
s1$nmis   # re-check of NAs should be identical to above
s2 <- prelim.norm(dframe[,1:32])# see below -> still NAs are available - 
_not_ imputed
thetahat1 <- em.norm(s1)
theta1 <- da.norm(s1,thetahat1,steps=20,showits=TRUE)  # error:
                                                       # Steps of Data 
Augmentation:
                                                       # 
1...2...3...4...5...6...7...8...Fehler: NA/NaN/Inf in externem 
Funktionsaufruf (arg 2)
thetahat2 <- em.norm(s2)
( imputed1 <- imp.norm(s1,thetahat1,dframe) )    # very strange imputed 
values
                                                 # almost >200% to big 
than expected
( imputed1.1 <- imp.norm(s1,theta1,dframe)  )    # not possible - 
because da.norm gives no result!
( imputed2 <- imp.norm(s2,thetahat2,dframe) )    # still NAs in the matrix

# visualize the strange values
par(mfrow=c(2,1))
hist(dframe,prob=TRUE)      # histogramm data set with NAs - original values
lines(density(na.omit(dframe)))
hist(imputed1,prob=TRUE)   # histogramm of dataset with imputed values
lines(density(imputed1))


</---snip--->



From gerifalte28 at hotmail.com  Wed Nov  9 03:07:04 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 09 Nov 2005 02:07:04 +0000
Subject: [R] spatial epidemiology
In-Reply-To: <43710290.4070503@cae.wisc.edu>
Message-ID: <BAY103-F7B26F0D5FCC14842477AFA6670@phx.gbl>

It depends on what you want to do.  There are many spatial packages that can 
be used for a variety of epidemiological analyses.  Take a look at this link 
http://sal.uiuc.edu/csiss/Rgeo// for a nice summary of the current spatial 
tools in R.

If you are looking for something that will perform scan statistics along the 
lines of SaTScan, the package DCluster will probably be your best option.  
Follow this link 
http://finzi.psych.upenn.edu/R/library/DCluster/html/00Index.html for a list 
of options available in this package

Finally, try RSiteSearch("spatial epidemiology") for some interesting 
threads on the subject

I hope this helps

Francisco


>From: Hongyu Sun <sun at cae.wisc.edu>
>To: rHELP <R-help at stat.math.ethz.ch>
>Subject: [R] spatial epidemiology
>Date: Tue, 08 Nov 2005 13:54:56 -0600
>
>Hi, All: Does R have such packages for spatial epidemiology? Many thanks,
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From jholtman at gmail.com  Wed Nov  9 03:51:42 2005
From: jholtman at gmail.com (jim holtman)
Date: Tue, 8 Nov 2005 21:51:42 -0500
Subject: [R] Using split and sapply to return entire lines
In-Reply-To: <20051108202017.GA30278@bluefish.augustcouncil.com>
References: <20051108202017.GA30278@bluefish.augustcouncil.com>
Message-ID: <644e1f320511081851w544c257ds4c3169cde4100d29@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051108/a4385847/attachment.pl

From ggrothendieck at gmail.com  Wed Nov  9 04:08:11 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 8 Nov 2005 22:08:11 -0500
Subject: [R] Using split and sapply to return entire lines
In-Reply-To: <20051108202017.GA30278@bluefish.augustcouncil.com>
References: <20051108202017.GA30278@bluefish.augustcouncil.com>
Message-ID: <971536df0511081908n48fde0b1o335735bd2518f3b7@mail.gmail.com>

Using package doBy at http://genetics.agrsci.dk/~sorenh/misc/index.html
try this:

summaryBy(cbind(length, ratio, monthly1, monthly2) ~ month, DF, max)



On 11/8/05, Todd A. Gibson <tgibson at augustcouncil.com> wrote:
> Hello,
> I have a data manipulation problem that I can easily resolve by using
> perl or python to pre-process the data, but I would prefer to do it
> directly in R.
>
> Given, for example:
>
>  month length ratio monthly1 monthly2
> 1 Jan   23     0.1   9        6
> 2 Jan   45     0.2   9        6
> 3 Jan   16     0.3   9        6
> 4 Feb   14     0.2   1        9
> 5 Mar   98     0.4   2        2
> 6 Mar   02     0.6   2        2
>
> (FWIW, monthly1 and monthly2 are unchanged for each month)
>
> I understand how to do aggregations on single fields using split and
> sapply, but how can I get entire lines.  For example, For the maximum
> of data$length grouped by data$month I would like to get back some
> form of:
>
> 2 Jan 45 0.2 9 6
> 4 Feb 14 0.2 1 9
> 5 Mar 98 0.4 2 2
>
> For mean, I would like to average all columns:
>
> Jan 28 0.2 9 6
> Feb 14 0.2 1 9
> Mar 50 0.5 2 2
>
> Thank you,
> -TAG
> Todd A. Gibson
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed Nov  9 04:47:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 8 Nov 2005 22:47:26 -0500
Subject: [R] Using split and sapply to return entire lines
In-Reply-To: <971536df0511081908n48fde0b1o335735bd2518f3b7@mail.gmail.com>
References: <20051108202017.GA30278@bluefish.augustcouncil.com>
	<971536df0511081908n48fde0b1o335735bd2518f3b7@mail.gmail.com>
Message-ID: <971536df0511081947x4ae7e912l5cf2cc19fbe0a5e3@mail.gmail.com>

Also, one can use aggregate:

aggregate(DF[,-1], list(month = DF$month), max)

On 11/8/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Using package doBy at http://genetics.agrsci.dk/~sorenh/misc/index.html
> try this:
>
> summaryBy(cbind(length, ratio, monthly1, monthly2) ~ month, DF, max)
>
>
>
> On 11/8/05, Todd A. Gibson <tgibson at augustcouncil.com> wrote:
> > Hello,
> > I have a data manipulation problem that I can easily resolve by using
> > perl or python to pre-process the data, but I would prefer to do it
> > directly in R.
> >
> > Given, for example:
> >
> >  month length ratio monthly1 monthly2
> > 1 Jan   23     0.1   9        6
> > 2 Jan   45     0.2   9        6
> > 3 Jan   16     0.3   9        6
> > 4 Feb   14     0.2   1        9
> > 5 Mar   98     0.4   2        2
> > 6 Mar   02     0.6   2        2
> >
> > (FWIW, monthly1 and monthly2 are unchanged for each month)
> >
> > I understand how to do aggregations on single fields using split and
> > sapply, but how can I get entire lines.  For example, For the maximum
> > of data$length grouped by data$month I would like to get back some
> > form of:
> >
> > 2 Jan 45 0.2 9 6
> > 4 Feb 14 0.2 1 9
> > 5 Mar 98 0.4 2 2
> >
> > For mean, I would like to average all columns:
> >
> > Jan 28 0.2 9 6
> > Feb 14 0.2 1 9
> > Mar 50 0.5 2 2
> >
> > Thank you,
> > -TAG
> > Todd A. Gibson
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From hb at maths.lth.se  Wed Nov  9 04:56:25 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 09 Nov 2005 14:56:25 +1100
Subject: [R] writing R shell scripts?
In-Reply-To: <Pine.GSO.4.60.0511081919030.19524@taxa.epi.umn.edu>
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>	<d332d3e10511080643w13773509rf5b7aacae8e0ac6e@mail.gmail.com>
	<Pine.GSO.4.60.0511081919030.19524@taxa.epi.umn.edu>
Message-ID: <43717369.6070108@maths.lth.se>

Mike Miller wrote:

>Many thanks for the suggestions.  Here is a related question:
>
>When I do things like this...
>
>echo "matrix(rnorm(25*2),c(25,2))" | R --slave --no-save
>
>...I get the desired result except that I would like to suppress the 
>"[,1]" row and column labels so that only the values go to stdout.  What 
>is the trick to making that work?
>  
>
What you ask R to do is

matrix(rnorm(25*2),c(25,2))

which is equivalent to

print(matrix(rnorm(25*2),c(25,2)))

What you really want to do might be solved by write.table(), e.g.

x <- matrix(rnorm(25*2),c(25,2));
write.table(file=stdout(), x, row.names=FALSE, col.names=FALSE);

A note of concern: When writing batch scripts like this, be explicit and use the print() statement.  A counter example to compare

echo "1; 2" | R --slave --no-save

and

echo "print(1); print(2)" | R --slave --no-save



>By the way, I find it useful to have a script in my path that does this:
>
>#!/bin/sh
>echo "$1" | /usr/local/bin/R --slave --no-save
>
>Suppose that script was called "doR", then one could do things like this 
>from the Linux/UNIX command line:
>
># doR 'sqrt(35.6)'
>[1] 5.966574
>
># doR 'runif(1)'
>[1] 0.8881654
>
>Which I find to be handy for quick arithmetic and even for much more 
>sophisticated things.  I'd like to get rid of the "[1]" though!
>
>  
>
If you want to be lazy and not use, say, doR 'cat(runif(1),"\n")' above, 
maybe a simple Unix sed in your shell script can fix that?!

/Henrik

>Mike
>
>  
>



From mbmiller at taxa.epi.umn.edu  Wed Nov  9 06:04:55 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Tue, 8 Nov 2005 23:04:55 -0600 (CST)
Subject: [R] writing R shell scripts?
In-Reply-To: <43717369.6070108@maths.lth.se>
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>
	<d332d3e10511080643w13773509rf5b7aacae8e0ac6e@mail.gmail.com>
	<Pine.GSO.4.60.0511081919030.19524@taxa.epi.umn.edu>
	<43717369.6070108@maths.lth.se>
Message-ID: <Pine.GSO.4.60.0511082210360.29031@taxa.epi.umn.edu>

On Wed, 9 Nov 2005, Henrik Bengtsson wrote:

> What you really want to do might be solved by write.table(), e.g.
>
> x <- matrix(rnorm(25*2),c(25,2));
> write.table(file=stdout(), x, row.names=FALSE, col.names=FALSE);

Thanks.  That does what I want.

There is one remaining problem for my "echo" method.  While write.table 
seems to do the right thing for me, R seems to add an extra newline to the 
output when it closes.  You can see that this produces exactly one newline 
and nothing else:

# echo '' | R --slave --no-save | wc -c
       1

Is there any way to stop R from sending an extra newline?  It's funny 
because normal running of R doesn't seem to terminate by sending a newline 
to stdout.  Oops -- I just figured it out.  If I send "quit()", there is 
no extra newline!  Examples that send no extra newline:

echo 'quit()' | R --slave --no-save

echo "x <- matrix(rnorm(25*2),c(25,2)); write.table(file=stdout(), x, row.names=FALSE, col.names=FALSE); quit()" | R --slave --no-save

I suppose we can live with that as it is.  Is this an intentional feature?


> A note of concern: When writing batch scripts like this, be explicit and 
> use the print() statement.  A counter example to compare
>
> echo "1; 2" | R --slave --no-save
>
> and
>
> echo "print(1); print(2)" | R --slave --no-save

I guess you are saying that sometimes R will fail if I don't use print(). 
Can you give an example of how it can fail?


>> By the way, I find it useful to have a script in my path that does 
>> this:
>> 
>> #!/bin/sh
>> echo "$1" | /usr/local/bin/R --slave --no-save
>> 
>> Suppose that script was called "doR", then one could do things like 
>> this from the Linux/UNIX command line:
>> 
>> # doR 'sqrt(35.6)'
>> [1] 5.966574
>> 
>> # doR 'runif(1)'
>> [1] 0.8881654
>> 
>> Which I find to be handy for quick arithmetic and even for much more 
>> sophisticated things.  I'd like to get rid of the "[1]" though!
>
> If you want to be lazy and not use, say, doR 'cat(runif(1),"\n")' above, 
> maybe a simple Unix sed in your shell script can fix that?!


It looks like I can rewrite the script like this:

#!/bin/sh
echo "$1 ; write.table(file=stdout(), out, row.names=FALSE, col.names=FALSE); quit()" | /usr/local/bin/R --slave --no-save

Then I have to always include something like this...

out <- some_operation

...as part of my command.  Example:

# doR 'A <- matrix(rnorm(100*5),c(100,5)); out <- chol(cov(A))'
1.08824564637869 0.00749462665482204 -0.109577665309141 0.123824503621501 0.0420504647142321
0 0.969304154505745 0.0689085053799411 0.143273894584171 -0.0204348333174425
0 0 0.995383836907855 0.0860782051613422 0.056980680914183
0 0 0 0.94180592438191 0.0534651651371964
0 0 0 0 0.907266109886987

Now we've got it!!

The output above is nice and compact, and it doesn't have an extra 
newline, but if you want it to look nice on the screen, my friend Stephen 
Montgomery-Smith (Math, U Missouri) made me this nice little perl script 
for aligning numbers neatly and easily (but it doesn't work if there are 
letters in there (e.g., NA or 1.3e-6):

http://taxa.epi.umn.edu/misc/numalign

# ./doR 'A <- matrix(rnorm(100*5),c(100,5)); out <- chol(cov(A))' | numalign
0.903339952680364 -0.088773840144205 -0.223677935069773  -0.0736286093726908  0.0457396703130186
0                  1.08096548052082   0.0800540640587432 -0.0457840266135511 -0.0311210293661459
0                  0                  0.93834307353671    0.0665017259723313 -0.0825698771035788
0                  0                  0                   1.03303581434252    0.118372967026342
0                  0                  0                   0                   0.972768611955302

Thanks very much for all of the help!!!

Mike

-- 
Michael B. Miller, Ph.D.
Assistant Professor
Division of Epidemiology and Community Health
and Institute of Human Genetics
University of Minnesota
http://taxa.epi.umn.edu/~mbmiller/



From tgibson at augustcouncil.com  Wed Nov  9 06:33:16 2005
From: tgibson at augustcouncil.com (Todd A. Gibson)
Date: Tue, 8 Nov 2005 22:33:16 -0700
Subject: [R] Using split and sapply to return entire lines
In-Reply-To: <971536df0511081947x4ae7e912l5cf2cc19fbe0a5e3@mail.gmail.com>
References: <20051108202017.GA30278@bluefish.augustcouncil.com>
	<971536df0511081908n48fde0b1o335735bd2518f3b7@mail.gmail.com>
	<971536df0511081947x4ae7e912l5cf2cc19fbe0a5e3@mail.gmail.com>
Message-ID: <20051109053316.GA32337@bluefish.augustcouncil.com>

On Tue, Nov 08, 2005 at 10:47:26PM -0500, Gabor Grothendieck wrote:
> Also, one can use aggregate:
> 
> aggregate(DF[,-1], list(month = DF$month), max)

The issue here is that I need the row corresponding to the month with
the maximum length.  That is, aggregate(.) is returning both the
maximum month and maximum ratio for all rows with month=Jan.  I need
the single row in month=Jan which has the maximum length.

Thanks,
-TAG

> On 11/8/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Using package doBy at http://genetics.agrsci.dk/~sorenh/misc/index.html
> > try this:
> >
> > summaryBy(cbind(length, ratio, monthly1, monthly2) ~ month, DF, max)
> >
> >
> >
> > On 11/8/05, Todd A. Gibson <tgibson at augustcouncil.com> wrote:
> > > Hello,
> > > I have a data manipulation problem that I can easily resolve by using
> > > perl or python to pre-process the data, but I would prefer to do it
> > > directly in R.
> > >
> > > Given, for example:
> > >
> > >  month length ratio monthly1 monthly2
> > > 1 Jan   23     0.1   9        6
> > > 2 Jan   45     0.2   9        6
> > > 3 Jan   16     0.3   9        6
> > > 4 Feb   14     0.2   1        9
> > > 5 Mar   98     0.4   2        2
> > > 6 Mar   02     0.6   2        2
> > >
> > > (FWIW, monthly1 and monthly2 are unchanged for each month)
> > >
> > > I understand how to do aggregations on single fields using split and
> > > sapply, but how can I get entire lines.  For example, For the maximum
> > > of data$length grouped by data$month I would like to get back some
> > > form of:
> > >
> > > 2 Jan 45 0.2 9 6
> > > 4 Feb 14 0.2 1 9
> > > 5 Mar 98 0.4 2 2
> > >
> > > For mean, I would like to average all columns:
> > >
> > > Jan 28 0.2 9 6
> > > Feb 14 0.2 1 9
> > > Mar 50 0.5 2 2
> > >
> > > Thank you,
> > > -TAG
> > > Todd A. Gibson
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >



From hb at maths.lth.se  Wed Nov  9 06:45:28 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 09 Nov 2005 16:45:28 +1100
Subject: [R] writing R shell scripts?
In-Reply-To: <Pine.GSO.4.60.0511082210360.29031@taxa.epi.umn.edu>
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>	<d332d3e10511080643w13773509rf5b7aacae8e0ac6e@mail.gmail.com>	<Pine.GSO.4.60.0511081919030.19524@taxa.epi.umn.edu>	<43717369.6070108@maths.lth.se>
	<Pine.GSO.4.60.0511082210360.29031@taxa.epi.umn.edu>
Message-ID: <43718CF8.3020401@maths.lth.se>

Mike Miller wrote:

>On Wed, 9 Nov 2005, Henrik Bengtsson wrote:
>
>  
>
>>What you really want to do might be solved by write.table(), e.g.
>>
>>x <- matrix(rnorm(25*2),c(25,2));
>>write.table(file=stdout(), x, row.names=FALSE, col.names=FALSE);
>>    
>>
>
>Thanks.  That does what I want.
>
>There is one remaining problem for my "echo" method.  While write.table 
>seems to do the right thing for me, R seems to add an extra newline to the 
>output when it closes.  You can see that this produces exactly one newline 
>and nothing else:
>
># echo '' | R --slave --no-save | wc -c
>       1
>
>Is there any way to stop R from sending an extra newline?  It's funny 
>because normal running of R doesn't seem to terminate by sending a newline 
>to stdout.  Oops -- I just figured it out.  If I send "quit()", there is 
>no extra newline!  Examples that send no extra newline:
>
>echo 'quit()' | R --slave --no-save
>
>echo "x <- matrix(rnorm(25*2),c(25,2)); write.table(file=stdout(), x, row.names=FALSE, col.names=FALSE); quit()" | R --slave --no-save
>
>I suppose we can live with that as it is.  Is this an intentional feature?
>
>
>  
>
>>A note of concern: When writing batch scripts like this, be explicit and 
>>use the print() statement.  A counter example to compare
>>
>>echo "1; 2" | R --slave --no-save
>>
>>and
>>
>>echo "print(1); print(2)" | R --slave --no-save
>>    
>>
>
>I guess you are saying that sometimes R will fail if I don't use print(). 
>Can you give an example of how it can fail?
>  
>
What I was really try to say is that if you running the R terminal, that 
is, you "sending" commands via the R prompt, R will take the *last* 
value and call print()  on it.  This is why you get the result when you type

 > 1+1
[1] 2

Without this "feature" you would have had to type
 > print(1+1)
[1] 2

to get any results.  Note that it is only the last value calculate, that 
will be output this way, cf.
 > 1+1; 2+2
[1] 4

In other words, in the latter example '1+1' is pretty useless and does 
nothing but taking up CPU time.  So, whenever you write batch scripts 
like yours or standard R scripts called by source(), do use print() 
explicitly (or cat() or write.table() or ...) if want output!

>>>By the way, I find it useful to have a script in my path that does 
>>>this:
>>>
>>>#!/bin/sh
>>>echo "$1" | /usr/local/bin/R --slave --no-save
>>>
>>>Suppose that script was called "doR", then one could do things like 
>>>this from the Linux/UNIX command line:
>>>
>>># doR 'sqrt(35.6)'
>>>[1] 5.966574
>>>
>>># doR 'runif(1)'
>>>[1] 0.8881654
>>>
>>>Which I find to be handy for quick arithmetic and even for much more 
>>>sophisticated things.  I'd like to get rid of the "[1]" though!
>>>      
>>>
>>If you want to be lazy and not use, say, doR 'cat(runif(1),"\n")' above, 
>>maybe a simple Unix sed in your shell script can fix that?!
>>    
>>
>
>
>It looks like I can rewrite the script like this:
>
>#!/bin/sh
>echo "$1 ; write.table(file=stdout(), out, row.names=FALSE, col.names=FALSE); quit()" | /usr/local/bin/R --slave --no-save
>
>Then I have to always include something like this...
>
>out <- some_operation
>
>...as part of my command.  Example:
>
>  
>
Or go get the last value calculated by .Last.value, see

echo "$1; out <- .Last.value; write.table(file=stdout(), out, row.names=FALSE, col.names=FALSE); quit()" | /usr/local/bin/R --slave --no-save

You may also want to create your own method for returning/outputting 
data.  An ideal way for doing this is to use create a so called generic 
function, say, returnOutput(), and then special functions for each class 
of object that you might get returned, e.g. returnOutput.matrix(), 
returnOutput.list(), etc. Don't forget returnOutput.default().  If you 
do not understand what I'm talking about here, please read up on 
S3/UseMethod in R documentation.  It's all in there.  Then you'll also 
get a much deeper understanding of how print() (and R) works.  Please 
don't ask me to explain it on the mailing list.

Cheers

Henrik

># doR 'A <- matrix(rnorm(100*5),c(100,5)); out <- chol(cov(A))'
>1.08824564637869 0.00749462665482204 -0.109577665309141 0.123824503621501 0.0420504647142321
>0 0.969304154505745 0.0689085053799411 0.143273894584171 -0.0204348333174425
>0 0 0.995383836907855 0.0860782051613422 0.056980680914183
>0 0 0 0.94180592438191 0.0534651651371964
>0 0 0 0 0.907266109886987
>
>Now we've got it!!
>
>The output above is nice and compact, and it doesn't have an extra 
>newline, but if you want it to look nice on the screen, my friend Stephen 
>Montgomery-Smith (Math, U Missouri) made me this nice little perl script 
>for aligning numbers neatly and easily (but it doesn't work if there are 
>letters in there (e.g., NA or 1.3e-6):
>
>http://taxa.epi.umn.edu/misc/numalign
>
># ./doR 'A <- matrix(rnorm(100*5),c(100,5)); out <- chol(cov(A))' | numalign
>0.903339952680364 -0.088773840144205 -0.223677935069773  -0.0736286093726908  0.0457396703130186
>0                  1.08096548052082   0.0800540640587432 -0.0457840266135511 -0.0311210293661459
>0                  0                  0.93834307353671    0.0665017259723313 -0.0825698771035788
>0                  0                  0                   1.03303581434252    0.118372967026342
>0                  0                  0                   0                   0.972768611955302
>
>Thanks very much for all of the help!!!
>
>Mike
>
>  
>



From ggrothendieck at gmail.com  Wed Nov  9 06:57:02 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 9 Nov 2005 00:57:02 -0500
Subject: [R] Using split and sapply to return entire lines
In-Reply-To: <20051109053316.GA32337@bluefish.augustcouncil.com>
References: <20051108202017.GA30278@bluefish.augustcouncil.com>
	<971536df0511081908n48fde0b1o335735bd2518f3b7@mail.gmail.com>
	<971536df0511081947x4ae7e912l5cf2cc19fbe0a5e3@mail.gmail.com>
	<20051109053316.GA32337@bluefish.augustcouncil.com>
Message-ID: <971536df0511082157l3a67acc8qb5f20cc6cd901694@mail.gmail.com>

On 11/9/05, Todd A. Gibson <tgibson at augustcouncil.com> wrote:
> On Tue, Nov 08, 2005 at 10:47:26PM -0500, Gabor Grothendieck wrote:
> > Also, one can use aggregate:
> >
> > aggregate(DF[,-1], list(month = DF$month), max)
>
> The issue here is that I need the row corresponding to the month with
> the maximum length.  That is, aggregate(.) is returning both the
> maximum month and maximum ratio for all rows with month=Jan.  I need
> the single row in month=Jan which has the maximum length.

Try this to calculate the index, idx, of the largest in each group
and then put it all together in the last line:

f <- function(x) rownames(x)[which.max(x$length)]
idx <- by(DF, DF$month, f)
cbind(index = c(idx), DF[idx,])



From rahmank at frim.gov.my  Thu Nov 10 07:34:27 2005
From: rahmank at frim.gov.my (Abd. Rahman Kassim)
Date: Thu, 10 Nov 2005 14:34:27 +0800
Subject: [R] polynomials transformation
Message-ID: <001601c5e5c0$cc7fdb60$2f01a8c0@rahmandt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/64a0da4e/attachment.pl

From mbmiller at taxa.epi.umn.edu  Wed Nov  9 07:58:05 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Wed, 9 Nov 2005 00:58:05 -0600 (CST)
Subject: [R] writing R shell scripts?
In-Reply-To: <43718CF8.3020401@maths.lth.se>
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>
	<d332d3e10511080643w13773509rf5b7aacae8e0ac6e@mail.gmail.com>
	<Pine.GSO.4.60.0511081919030.19524@taxa.epi.umn.edu>
	<43717369.6070108@maths.lth.se>
	<Pine.GSO.4.60.0511082210360.29031@taxa.epi.umn.edu>
	<43718CF8.3020401@maths.lth.se>
Message-ID: <Pine.GSO.4.60.0511090030390.11831@taxa.epi.umn.edu>

On Wed, 9 Nov 2005, Henrik Bengtsson wrote:

>>> A note of concern: When writing batch scripts like this, be explicit 
>>> and use the print() statement.  A counter example to compare
>>> 
>>> echo "1; 2" | R --slave --no-save
>>> 
>>> and
>>> 
>>> echo "print(1); print(2)" | R --slave --no-save
>>> 
>> 
>> I guess you are saying that sometimes R will fail if I don't use 
>> print(). Can you give an example of how it can fail?

This may have been a misunderstanding because it looks like my R and your 
R are not functioning in the same ways.  More below...


> What I was really try to say is that if you running the R terminal, that 
> is, you "sending" commands via the R prompt, R will take the *last* 
> value and call print()  on it.  This is why you get the result when you 
> type
>
>> 1+1
> [1] 2
>
> Without this "feature" you would have had to type
>> print(1+1)
> [1] 2
>
> to get any results.  Note that it is only the last value calculate, that will 
> be output this way, cf.
>> 1+1; 2+2
> [1] 4


My version of R works differently:

# echo "1+1; 2+2" | R --slave --no-save
[1] 2
[1] 4

It does the same thing from the interactive prompt.  This holds in both of 
these versions of R on Red Hat Linux:

R 1.8.1 (2003-11-21).
R 2.2.0 (2005-10-06).


If I assign the results of earlier computations to variables, then they 
are not printed:

# echo "x <- 1+1; 2+2" | R --slave --no-save
[1] 4


> Or go get the last value calculated by .Last.value, see
>
> echo "$1; out <- .Last.value; write.table(file=stdout(), out, row.names=FALSE, col.names=FALSE); quit()" | /usr/local/bin/R --slave --no-save

Beautiful.  I didn't know about .Last.value, but now that I do, I think we 
can shorten that script to this...

echo "$1; write.table(file=stdout(), .Last.value, row.names=FALSE, col.names=FALSE); quit()" | /usr/local/bin/R --slave --no-save

...because we no longer need the "out" variable.  It seems like one 
problem I'm having is that R returns results of every computation, and not 
just the last one, unless I assign the result to a variable.  Example 
using the doR one-line script above:

# doR 'chol(cov(matrix(rnorm(100*5),c(100,5))))'
          [,1]       [,2]        [,3]          [,4]        [,5]
[1,] 1.021414 0.09806281 0.003275454  0.0009819654  0.05031847
[2,] 0.000000 1.10031274 0.002696835 -0.0990352880  0.17356877
[3,] 0.000000 0.00000000 0.822075977 -0.0353553332 -0.04559222
[4,] 0.000000 0.00000000 0.000000000  0.9367890692 -0.01513027
[5,] 0.000000 0.00000000 0.000000000  0.0000000000  0.97588119
1.02141394873274 0.0980628119885006 0.00327545419626209 0.000981965434760053 0.050318470112499
0 1.10031274450895 0.00269683530006245 -0.0990352879929318 0.173568771318532
0 0 0.82207597738982 -0.0353553332133034 -0.0455922206141078
0 0 0 0.936789069194909 -0.0151302741201435
0 0 0 0 0.975881188029811

# doR 'x <- chol(cov(matrix(rnorm(100*5),c(100,5))))'
1.09005225946311 0.183719241993361 -0.211250918511775 -0.0148273333266647 -0.097633753471306
0 0.990599902490968 0.0546812452445389 -0.0255188599622241 0.0502929718369168
0 0 0.982263267444303 -0.0587151164554906 -0.046018923176493
0 0 0 1.00433563628640 0.222340686806836
0 0 0 0 0.976420329786668

I suppose I can live with that.  Is my R really working differently from 
the R other people are using?


> You may also want to create your own method for returning/outputting 
> data. An ideal way for doing this is to use create a so called generic 
> function, say, returnOutput(), and then special functions for each class 
> of object that you might get returned, e.g. returnOutput.matrix(), 
> returnOutput.list(), etc. Don't forget returnOutput.default().  If you 
> do not understand what I'm talking about here, please read up on 
> S3/UseMethod in R documentation.  It's all in there.  Then you'll also 
> get a much deeper understanding of how print() (and R) works.

Thanks yet again for another great tip!

Mike

-- 
Michael B. Miller, Ph.D.
Assistant Professor
Division of Epidemiology and Community Health
and Institute of Human Genetics
University of Minnesota
http://taxa.epi.umn.edu/~mbmiller/



From ripley at stats.ox.ac.uk  Wed Nov  9 08:32:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Nov 2005 07:32:21 +0000 (GMT)
Subject: [R] error in NORM lib
In-Reply-To: <437156CD.4040804@anicca-vijja.de>
References: <437156CD.4040804@anicca-vijja.de>
Message-ID: <Pine.LNX.4.61.0511090727530.28141@gannet.stats>

You really need to send such issues to the _package_ maintainer: please 
see the posting guide.  He will need a completely reproducible example.

On Wed, 9 Nov 2005, Leo G?rtler wrote:

> Dear alltogether,
>
> I experience very strange behavior of imputation of NA's with the NORM
> library. I use R 2.2.0, win32.
> The code is below and the same dataset was also tried with MICE and
> aregImpute() from HMISC _without_ any problem.
> The problem is as follows:
>
> (1) using the whole dataset results in very strange imputations - values
> far beyond the maximum of the respective column, > 200%! and this is
> reproducible and true for the whole set of imputed NAs
> (2) using just part (i.e. columns) of the dataset results in the fact
> that some NAs are not imputed at all, i.e. NAs are still in the dataset
> - but there is neither a warning nor an error
> (3) data.augmentation with da.norm() fails, but not after the first
> step, mostly 3-5 steps are ok, then it stops (see below)
>
> The dataset is from educational research and should be almost normal
> distributed (slight deviations, but not really that heavy to explain the
> strange results).
> I don't understand this, because the dataset works well with MICE and
> aregImpute() and other statistics _and_ I checked the manpages and it
> does not seem that the calls are wrong.
> Thus, either it depends on the dataset (but why?) or it is maybe a bug.
>
> I appreciate every help,
>
> thanks,
>
> leo g?rtler
>
> <---snip--->
>
> library(norm)
> rngseed(1234)
> load(url("http://www.anicca-vijja.de/lg/dframe.Rdata"))   # load object
> "dframe"
> dim(dframe)
> apply(dframe,2,function(x) sum(is.na(x))) # check how many NAs in the
> dataset
> #dframe <-
> subset(dframe,select=-c(alter,grpzugeh,is1,is4,is6,klassenstufe,mmit,vorai,vorap,voras,vorkf,vorsg,vorvb))
> s1 <- prelim.norm(dframe)
> s1$nmis   # re-check of NAs should be identical to above
> s2 <- prelim.norm(dframe[,1:32])# see below -> still NAs are available -
> _not_ imputed
> thetahat1 <- em.norm(s1)
> theta1 <- da.norm(s1,thetahat1,steps=20,showits=TRUE)  # error:
>                                                       # Steps of Data
> Augmentation:
>                                                       #
> 1...2...3...4...5...6...7...8...Fehler: NA/NaN/Inf in externem
> Funktionsaufruf (arg 2)
> thetahat2 <- em.norm(s2)
> ( imputed1 <- imp.norm(s1,thetahat1,dframe) )    # very strange imputed
> values
>                                                 # almost >200% to big
> than expected
> ( imputed1.1 <- imp.norm(s1,theta1,dframe)  )    # not possible -
> because da.norm gives no result!
> ( imputed2 <- imp.norm(s2,thetahat2,dframe) )    # still NAs in the matrix
>
> # visualize the strange values
> par(mfrow=c(2,1))
> hist(dframe,prob=TRUE)      # histogramm data set with NAs - original values
> lines(density(na.omit(dframe)))
> hist(imputed1,prob=TRUE)   # histogramm of dataset with imputed values
> lines(density(imputed1))
>
>
> </---snip--->
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Wed Nov  9 08:36:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Nov 2005 07:36:57 +0000 (GMT)
Subject: [R] writing R shell scripts?
In-Reply-To: <Pine.GSO.4.60.0511082210360.29031@taxa.epi.umn.edu>
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>
	<d332d3e10511080643w13773509rf5b7aacae8e0ac6e@mail.gmail.com>
	<Pine.GSO.4.60.0511081919030.19524@taxa.epi.umn.edu>
	<43717369.6070108@maths.lth.se>
	<Pine.GSO.4.60.0511082210360.29031@taxa.epi.umn.edu>
Message-ID: <Pine.LNX.4.61.0511090734090.28141@gannet.stats>

On Tue, 8 Nov 2005, Mike Miller wrote:

> On Wed, 9 Nov 2005, Henrik Bengtsson wrote:
>
>> What you really want to do might be solved by write.table(), e.g.
>>
>> x <- matrix(rnorm(25*2),c(25,2));
>> write.table(file=stdout(), x, row.names=FALSE, col.names=FALSE);
>
> Thanks.  That does what I want.
>
> There is one remaining problem for my "echo" method.  While write.table
> seems to do the right thing for me, R seems to add an extra newline to the
> output when it closes.  You can see that this produces exactly one newline
> and nothing else:
>
> # echo '' | R --slave --no-save | wc -c
>       1
>
> Is there any way to stop R from sending an extra newline?  It's funny
> because normal running of R doesn't seem to terminate by sending a newline
> to stdout.  Oops -- I just figured it out.  If I send "quit()", there is
> no extra newline!  Examples that send no extra newline:
>
> echo 'quit()' | R --slave --no-save
>
> echo "x <- matrix(rnorm(25*2),c(25,2)); write.table(file=stdout(), x, row.names=FALSE, col.names=FALSE); quit()" | R --slave --no-save
>
> I suppose we can live with that as it is.  Is this an intentional feature?

Yes.  You get a prompt (suppressed here) and did not give an input for it, 
so a newline is needed to finish the line.

R scripts should end in 'q()'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Wed Nov  9 08:42:22 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 9 Nov 2005 08:42:22 +0100 (CET)
Subject: [R] Variograms and large distances
In-Reply-To: <4371458A.9020300@u.washington.edu>
Message-ID: <Pine.LNX.4.44.0511090838040.13747-100000@reclus.nhh.no>

On Tue, 8 Nov 2005, Julian Burgos wrote:

> Hello R list,
> I need to compute empirical variograms using data from a large 
> geographic area (~10^6 km2).  Although I could not find a specific 
> reference, I assume that both geoR and gstat calculate distances among 
> data points assuming points are on a flat surface (using the Pythagorean 
> Theorem).  Because the location of my data is large and located near the 
> pole, assuming that latitude and longitude are coordinates on a flat 
> surface would introduce a -possibly large- bias in the empirical 
> variogram estimate.  My questions are the following:
> 
> a)  Does geoR and gstat assume that points are on a flat surface?

Yes.

> 
> b) If I first calculate the distances among points using the Haversine 
> formula, it is possible to calculate the variogram with a matrix of 
> distances among points (where n is the number of observations) and a 
> vector of observation values?
> 

I don't think so.

I suggest that you at least consider the fields package, partly because 
its disciplinary heritage makes large area coverage more natural, and 
because it has rdist.earth() and exp.earth.cov() functions. Whether it can 
do everything you need straightaway is another question, though.

> Any help would be appreciated.
> 
> Julian
> 
> Julian M. Burgos
> 
> Fisheries Acoustics Research Lab
> School of Aquatic and Fishery Science
> University of Washington
> 
> 1122 NE Boat Street
> Seattle, WA  98105 
> 
> Phone: 206-221-6864
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From anil_rohilla at rediffmail.com  Wed Nov  9 08:55:16 2005
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 9 Nov 2005 07:55:16 -0000
Subject: [R] problem with Running Locfit
Message-ID: <20051109075516.27673.qmail@webmail34.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/b03d3c7a/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Wed Nov  9 08:57:18 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 09 Nov 2005 07:57:18 -0000 (GMT)
Subject: [R] error in NORM lib
In-Reply-To: <Pine.LNX.4.61.0511090727530.28141@gannet.stats>
Message-ID: <XFMail.051109075718.Ted.Harding@nessie.mcc.ac.uk>

On 09-Nov-05 Prof Brian Ripley wrote:
> You really need to send such issues to the _package_ maintainer: please
> see the posting guide.  He will need a completely reproducible example.
> 
> On Wed, 9 Nov 2005, Leo G??rtler wrote:
> 
>> Dear alltogether,
>>
>> I experience very strange behavior of imputation of NA's with the NORM
>> library. I use R 2.2.0, win32.
>> The code is below and the same dataset was also tried with MICE and
>> aregImpute() from HMISC _without_ any problem.

The Shafer-derived CAT, NORM and MIX require the data to be passed
as a *matrix*. As far as I can see from your code below, you have
passed the data as a dataframe (at least, that is what the name
"dframe" suggests).

First convert dframe to a matrix bbefore running *any* of the NORM
commands on it, e.g.

  dmatrix <- as.matrix(dframe)

and then try your NORM commands again.

The "matrix" requirement is in fact given in the documentation
for all three, which is essentially as in Shafer's original.
For example:

  library(norm)
  ?prelim.norm

--> "Usage:

     prelim.norm(x)

     Arguments:

       x: data matrix containing missing values. The rows of x 
          correspond to observational units, and the columns to
          variables.  Missing values are denoted by `NA'."

Note "matrix".

Hoping this helps,
Ted.

>> The problem is as follows:
>>
>> (1) using the whole dataset results in very strange imputations -
>> values
>> far beyond the maximum of the respective column, > 200%! and this is
>> reproducible and true for the whole set of imputed NAs
>> (2) using just part (i.e. columns) of the dataset results in the fact
>> that some NAs are not imputed at all, i.e. NAs are still in the
>> dataset
>> - but there is neither a warning nor an error
>> (3) data.augmentation with da.norm() fails, but not after the first
>> step, mostly 3-5 steps are ok, then it stops (see below)
>>
>> The dataset is from educational research and should be almost normal
>> distributed (slight deviations, but not really that heavy to explain
>> the
>> strange results).
>> I don't understand this, because the dataset works well with MICE and
>> aregImpute() and other statistics _and_ I checked the manpages and it
>> does not seem that the calls are wrong.
>> Thus, either it depends on the dataset (but why?) or it is maybe a
>> bug.
>>
>> I appreciate every help,
>>
>> thanks,
>>
>> leo g??rtler
>>
>> <---snip--->
>>
>> library(norm)
>> rngseed(1234)
>> load(url("http://www.anicca-vijja.de/lg/dframe.Rdata"))   # load
>> object
>> "dframe"
>> dim(dframe)
>> apply(dframe,2,function(x) sum(is.na(x))) # check how many NAs in the
>> dataset
>> #dframe <-
>> subset(dframe,select=-c(alter,grpzugeh,is1,is4,is6,klassenstufe,mmit,vo
>> rai,vorap,voras,vorkf,vorsg,vorvb))
>> s1 <- prelim.norm(dframe)
>> s1$nmis   # re-check of NAs should be identical to above
>> s2 <- prelim.norm(dframe[,1:32])# see below -> still NAs are available
>> -
>> _not_ imputed
>> thetahat1 <- em.norm(s1)
>> theta1 <- da.norm(s1,thetahat1,steps=20,showits=TRUE)  # error:
>>                                                       # Steps of Data
>> Augmentation:
>>                                                       #
>> 1...2...3...4...5...6...7...8...Fehler: NA/NaN/Inf in externem
>> Funktionsaufruf (arg 2)
>> thetahat2 <- em.norm(s2)
>> ( imputed1 <- imp.norm(s1,thetahat1,dframe) )    # very strange
>> imputed
>> values
>>                                                 # almost >200% to big
>> than expected
>> ( imputed1.1 <- imp.norm(s1,theta1,dframe)  )    # not possible -
>> because da.norm gives no result!
>> ( imputed2 <- imp.norm(s2,thetahat2,dframe) )    # still NAs in the
>> matrix
>>
>> # visualize the strange values
>> par(mfrow=c(2,1))
>> hist(dframe,prob=TRUE)      # histogramm data set with NAs - original
>> values
>> lines(density(na.omit(dframe)))
>> hist(imputed1,prob=TRUE)   # histogramm of dataset with imputed values
>> lines(density(imputed1))
>>
>>
>> </---snip--->
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 09-Nov-05                                       Time: 07:57:11
------------------------------ XFMail ------------------------------



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Nov  9 09:02:15 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 9 Nov 2005 09:02:15 +0100 (CET)
Subject: [R] useR! 2006: submission & registration started!
Message-ID: <Pine.LNX.4.51.0511090856450.17628@artemis.imbe.med.uni-erlangen.de>


We are happy to inform you that the online abstract submission and
registration for `useR! 2006' is now available online from
  http://www.R-project.org/useR-2006/

This second world meeting of the R user community will take place at
the Wirtschaftsuniversitaet Wien, Vienna, Austria, June 15 to 17 2006.
The conference schedule comprises keynote lectures and user-contributed
sessions as well as half-day tutorials presented by R experts on June 14,
2006, prior to the conference.

Keynote lectures addressing hot topics including data mining, graphics,
marketing or teaching with R will be presented by prominent speakers
including John Chambers, Jan de Leeuw, Brian Everitt, Travor Hastie, John
Fox, Stefano Iacus, Uwe Ligges, Paul Murrell, Peter Rossi, Simon Urbanek
and Sanford Weisberg.

The spectrum of user-contributed sessions will depend on your submissions.
Hence, we invite you to submit abstracts on topics presenting innovations
or exciting applications of R. The call for papers along with the link to
the online abstract submission is available at
  http://www.R-project.org/useR-2006/#Call

Before the start of the official program, half-day tutorials will be
offered on Wednesday, June 14th, a list of topics and speakers can be
found at
  http://www.R-project.org/useR-2006/Tutorials/

A special highlight of the conference will be a panel discussion on
`Getting recognition for excellence in computational statistics'. Editors
of well established journals in both computational and applied statistics
will discuss the impact of recent developments in computational statistics
on peer-reviewed journal publications. Currently, the panelists include
Jan de Leeuw (JSS), Brian Everitt (SMMR), Wolfgang Haerdle (CS), Nicholas
Jewell (SMGMB), Erricos Konthogiorges (CSDA), and Luke Tierney (JCGS).

Early birds fly until January 31st 2006, so now is the perfect time to
write and submit an abstract, register as a participant and plan your trip
to Vienna. The conference web page
  http://www.R-project.org/useR-2006/
has a link to a local hotel booking service and much more news.

See you in Vienna!
Torsten, Achim, David, Bettina, Kurt and Fritz



From sb at ihe.se  Wed Nov  9 09:27:51 2005
From: sb at ihe.se (Sixten Borg)
Date: Wed, 09 Nov 2005 09:27:51 +0100
Subject: [R] R seems to "stall" after several hours on a long	series of
	analyses... where to start?
Message-ID: <s371c123.042@gwmail.ihe.se>

Hi,

I saw something similar, when I had R to look in a file every half minute if there was a request to do something, and if so, do that something and empty the file. (This was my way of testing if I coud do an interactive web page, somehow I managed to get the web page to write the requests to the file that R would look in. R would update a graph that was visible on that same web page).

Anyway, this ran smoothly for while (40 minutes I think), then it just stopped. When I examined the situation, R suddenly woke up and continued its task as if nothing had happened (which was quite correct).

My amateur interpretation was that the system put R to sleep since it appeared to be inactive according to the system. When I swithed to R, it became interactive and was given CPU time again. 

Maybe this gives some inspiration to solve the problem. The system was Windows NT, R version 1.8, I think.

Kind regards.
Sixten


>>> "David L. Van Brunt, Ph.D." <dlvanbrunt at gmail.com> 2005-11-07 16:09 >>>
Great suggestions, all.

I do have a timer in there, and it looks like the time to complete a loop is
not increasing as it goes. From your comments, I take it that suggests there
is not a memory leak. I could try scripting the loop from the shell, rather
than R, to see if that works, but will do that as a last resort as it will
require a good deal of re-writing (the loop follows some "setup" code that
builds a pretty large data set... the loop then slaps several new columns on
a copy of that data set, and analyses that...)

I'll still try the other platform as well, see if the same problem occurs
there.

On 11/7/05, jim holtman <jholtman at gmail.com> wrote:
>
> Here is some code that I use to track the progress of my scripts. This
> will print out the total cpu time and the memory that is being used. You
> call it with 'my.stats("message")' to print out "message" on the console.
>  Also, have you profiled your code to see where the time is being spent?
> Can you break it up into multiple runs so that you can start with a "fresh"
> version of memory?
>  ======script===========
> "my.stats" <- local({
> # local variables to hold the last times
> # first two variables are the elasped and CPU times from the last report
> lastTime <- lastCPU <- 0
> function(text = "stats", reset=F)
> {
> procTime <- proc.time()[1:3] # get current metrics
> if (reset){ # setup to mark timing from this point
> lastTime <<- procTime[3]
> lastCPU <<- procTime[1] + procTime[2]
> } else {
> cat(text, "-",sys.call(sys.parent())[[1]], ": <",
> round((procTime[1] + procTime[2]) - lastCPU,1),
> round(procTime[3] - lastTime,1), ">", procTime,
> " : ", round(memory.size()/2.^20., 1.), "MB\n")
> invisible(flush.console()) # force a write to the console
> }
> }
> })
>  ========= here is some sample output=========
> > my.stats(reset=TRUE) # reset counters
> > x <- runif(1e6) # generate 1M random numbers
> > my.stats('random')
> random - my.stats : < 0.3 31.8 > 96.17 11.7 230474.9 : 69.5 MB
> > y <- x*x+sqrt(x) # just come calculation
> > my.stats('calc')
> calc - my.stats : < 0.7 71.2 > 96.52 11.74 230514.3 : 92.4 MB
> >
>  You can see that memory is growing. The first number is the CPU time and
> the second (in <>) is the elapsed time.
>  HTH
>
>
>  On 11/7/05, David L. Van Brunt, Ph.D. <dlvanbrunt at gmail.com> wrote:
>
> > Not sure where to even start on this.... I'm hoping there's some
> > debugging I
> > can do...
> >
> > I have a loop that cycles through several different data sets (same
> > structure, different info), performing randomForest growth and
> > predictions... saving out the predictions for later study...
> >
> > I get about 5 hours in (9%... of the planned iterations.. yikes!) and R
> > just
> > freezes.
> >
> > This happens in interactive and batch mode execution (I can see from the
> > ".Rout" file that it gets about 9% through in Batch mode, and about 6%
> > if in
> > interactive mode... does that suggest memory problems?)
> >
> > I'm thinking of re-executing this same code on a different platform to
> > see
> > if that's the issue (currently using OS X)... any other suggestions on
> > where
> > to look, or what to try to get more information?
> >
> > Sorry so vague... it's a LOT of code, runs fine without error for many
> > iterations, so I didn't think the problem was syntax...
> >
> > --
> > ---------------------------------------
> > David L. Van Brunt, Ph.D.
> > mailto: dlvanbrunt at gmail.com 
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 
> >
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
>
> What the problem you are trying to solve?




--
---------------------------------------
David L. Van Brunt, Ph.D.
mailto:dlvanbrunt at gmail.com 

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mmiller at nassp.uct.ac.za  Wed Nov  9 09:46:44 2005
From: mmiller at nassp.uct.ac.za (Mark Miller)
Date: Wed, 9 Nov 2005 10:46:44 +0200
Subject: [R] Title and axis
Message-ID: <200511091046.44909.mmiller@nassp.uct.ac.za>

I was wandering how to set the labels for the axis and how to set the title so 
that the default title name is not displayed under the title I set. I also 
use the following to save the plot to a file but it does not seem to save (I 
use postscript for use in my Latex document)

x = seq(0,30,0.01)
postscript("cumIAT.ps")
plot(ecdf(w1a), do.point=FALSE)
lines(x, pexp(x,0.405156250))
title('Cummlative Plot of Monday IATs for entire 15 Weeks')
dev.off()

Many thanks
Mark Miler



From mmiller at nassp.uct.ac.za  Wed Nov  9 09:47:21 2005
From: mmiller at nassp.uct.ac.za (Mark Miller)
Date: Wed, 9 Nov 2005 10:47:21 +0200
Subject: [R] Title and axis
Message-ID: <200511091047.21275.mmiller@nassp.uct.ac.za>

I was wandering how to set the labels for the axis and how to set the title so 
that the default title name is not displayed under the title I set. I also 
use the following to save the plot to a file but it does not seem to save (I 
use postscript for use in my Latex document)

x = seq(0,30,0.01)
postscript("cumIAT.ps")
plot(ecdf(w1a), do.point=FALSE)
lines(x, pexp(x,0.405156250))
title('Cummlative Plot of Monday IATs for entire 15 Weeks')
dev.off()

Many thanks
Mark Miler



From anil_rohilla at rediffmail.com  Wed Nov  9 09:53:42 2005
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 9 Nov 2005 08:53:42 -0000
Subject: [R] Problem with Running Locfit
Message-ID: <20051109085342.3531.qmail@webmail36.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/6d227621/attachment.pl

From hb at maths.lth.se  Wed Nov  9 10:01:23 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 09 Nov 2005 20:01:23 +1100
Subject: [R] writing R shell scripts?
In-Reply-To: <Pine.GSO.4.60.0511090030390.11831@taxa.epi.umn.edu>
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>	<d332d3e10511080643w13773509rf5b7aacae8e0ac6e@mail.gmail.com>	<Pine.GSO.4.60.0511081919030.19524@taxa.epi.umn.edu>	<43717369.6070108@maths.lth.se>	<Pine.GSO.4.60.0511082210360.29031@taxa.epi.umn.edu>	<43718CF8.3020401@maths.lth.se>
	<Pine.GSO.4.60.0511090030390.11831@taxa.epi.umn.edu>
Message-ID: <4371BAE3.4010801@maths.lth.se>

Mike Miller wrote:

>On Wed, 9 Nov 2005, Henrik Bengtsson wrote:
>
>  
>
>>>>A note of concern: When writing batch scripts like this, be explicit 
>>>>and use the print() statement.  A counter example to compare
>>>>
>>>>echo "1; 2" | R --slave --no-save
>>>>
>>>>and
>>>>
>>>>echo "print(1); print(2)" | R --slave --no-save
>>>>
>>>>        
>>>>
>>>I guess you are saying that sometimes R will fail if I don't use 
>>>print(). Can you give an example of how it can fail?
>>>      
>>>
>
>This may have been a misunderstanding because it looks like my R and your 
>R are not functioning in the same ways.  More below...
>
>
>  
>
>>What I was really try to say is that if you running the R terminal, that 
>>is, you "sending" commands via the R prompt, R will take the *last* 
>>value and call print()  on it.  This is why you get the result when you 
>>type
>>
>>    
>>
>>>1+1
>>>      
>>>
>>[1] 2
>>
>>Without this "feature" you would have had to type
>>    
>>
>>>print(1+1)
>>>      
>>>
>>[1] 2
>>
>>to get any results.  Note that it is only the last value calculate, that will 
>>be output this way, cf.
>>    
>>
>>>1+1; 2+2
>>>      
>>>
>>[1] 4
>>    
>>
>
>
>My version of R works differently:
>
># echo "1+1; 2+2" | R --slave --no-save
>[1] 2
>[1] 4
>
>It does the same thing from the interactive prompt.  This holds in both of 
>these versions of R on Red Hat Linux:
>
>R 1.8.1 (2003-11-21).
>R 2.2.0 (2005-10-06).
>
>
>If I assign the results of earlier computations to variables, then they 
>are not printed:
>
># echo "x <- 1+1; 2+2" | R --slave --no-save
>[1] 4
>
>  
>
Hmm... You're completely correct.  I did not know this, that is, that 
print() seems to be called by the R terminal also after the completion 
of each expression and not just newlines.  Anyway, my overall message is 
that you should still be explicit about what you want to output in your 
code.  The following does definitely show what I want to stress:

foo <- function() {
 1+1;
 2+2;
}

 > foo()
[1] 4

and even

 > { 1+1; 2+2; }
[1] 4

>>Or go get the last value calculated by .Last.value, see
>>
>>echo "$1; out <- .Last.value; write.table(file=stdout(), out, row.names=FALSE, col.names=FALSE); quit()" | /usr/local/bin/R --slave --no-save
>>    
>>
>
>Beautiful.  I didn't know about .Last.value, but now that I do, I think we 
>can shorten that script to this...
>
>echo "$1; write.table(file=stdout(), .Last.value, row.names=FALSE, col.names=FALSE); quit()" | /usr/local/bin/R --slave --no-save
>
>...because we no longer need the "out" variable.  It seems like one 
>problem I'm having is that R returns results of every computation, and not 
>just the last one, unless I assign the result to a variable.  Example 
>using the doR one-line script above:
>
># doR 'chol(cov(matrix(rnorm(100*5),c(100,5))))'
>          [,1]       [,2]        [,3]          [,4]        [,5]
>[1,] 1.021414 0.09806281 0.003275454  0.0009819654  0.05031847
>[2,] 0.000000 1.10031274 0.002696835 -0.0990352880  0.17356877
>[3,] 0.000000 0.00000000 0.822075977 -0.0353553332 -0.04559222
>[4,] 0.000000 0.00000000 0.000000000  0.9367890692 -0.01513027
>[5,] 0.000000 0.00000000 0.000000000  0.0000000000  0.97588119
>1.02141394873274 0.0980628119885006 0.00327545419626209 0.000981965434760053 0.050318470112499
>0 1.10031274450895 0.00269683530006245 -0.0990352879929318 0.173568771318532
>0 0 0.82207597738982 -0.0353553332133034 -0.0455922206141078
>0 0 0 0.936789069194909 -0.0151302741201435
>0 0 0 0 0.975881188029811
>
># doR 'x <- chol(cov(matrix(rnorm(100*5),c(100,5))))'
>1.09005225946311 0.183719241993361 -0.211250918511775 -0.0148273333266647 -0.097633753471306
>0 0.990599902490968 0.0546812452445389 -0.0255188599622241 0.0502929718369168
>0 0 0.982263267444303 -0.0587151164554906 -0.046018923176493
>0 0 0 1.00433563628640 0.222340686806836
>0 0 0 0 0.976420329786668
>
>I suppose I can live with that.  
>
Put everything in curly brackets as my above example show.

>Is my R really working differently from 
>the R other people are using?
>
>  
>
No. Sorry about the confusion.

Cheers

Henrik

>  
>
>>You may also want to create your own method for returning/outputting 
>>data. An ideal way for doing this is to use create a so called generic 
>>function, say, returnOutput(), and then special functions for each class 
>>of object that you might get returned, e.g. returnOutput.matrix(), 
>>returnOutput.list(), etc. Don't forget returnOutput.default().  If you 
>>do not understand what I'm talking about here, please read up on 
>>S3/UseMethod in R documentation.  It's all in there.  Then you'll also 
>>get a much deeper understanding of how print() (and R) works.
>>    
>>
>
>Thanks yet again for another great tip!
>
>Mike
>
>  
>



From e.pebesma at geog.uu.nl  Wed Nov  9 10:14:21 2005
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Wed, 09 Nov 2005 10:14:21 +0100
Subject: [R]  Variogram
Message-ID: <4371BDED.8050106@geog.uu.nl>

Leaf, please note that r-help is not the appropriate place to ask
package-specific questions. We have r-sig-geo for questions related
to geographic data in R, and gstat has a mailing list on its own.

The answer is below.
--
Edzer


Leaf wrote:

Dear All,

Is there anybody has the experience in using variogram(gstat) ? Please kindly give me some hints about the results.


I used variogram() to build a semivariogram plot as:

tr.var=variogram(Incr~1,loc=~X+Y,data=TRI2TU,width=5)

then fir the variogram to get the parameters as:

 v.fit = fit.variogram(tr.var,vgm(0.5,"Exp",300,1))

v.fit
  model    psill    range
1   Nug 1.484879  0.00000
2   Exp 3.476700 29.70914

This is the output of v.fit. Can anybody help me write the exponential formula for this variogram?  I have the problem in understanding the result.

BTW

The equation you're looking for is:

if h = 0, gamma(h) = 0
if h > 0, gamma(h) = 1.484879 + 3.4767 (1 - exp(-h/29.70914))



From cg.pettersson at evp.slu.se  Wed Nov  9 10:56:14 2005
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Wed, 9 Nov 2005 10:56:14 +0100 (CET)
Subject: [R] using abline and a fitted 2nd degree formula
Message-ID: <33769.62.119.38.100.1131530174.squirrel@webmail.slu.se>

Hello all,

R2.1.1, Wk2

I am doing some two-step plotting, first using plot() to illustrate the
datapoints and then using abline() to place a trend line from a fitted
model into the plot.

Everything works well as long as the formula of the fitted model i of the
type:

m1 <- lm(Dependent ~ Independent)
then abline(m1) puts the proper straight line into the plot.

But if I use:

m2 <- lm(Dependent ~ Independent + I(Independent^2))
abline(m2) produces a straight line, only from the first order term.

Why, and what should I do about it?

Cheers
/CG




-- 
CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences (SLU)
Dep. of Crop Production Ekology. Box 7043.
SE-750 07 Uppsala, Sweden
cg.pettersson at evp.slu.se



From ccleland at optonline.net  Wed Nov  9 11:04:50 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 09 Nov 2005 05:04:50 -0500
Subject: [R] polynomials transformation
In-Reply-To: <001601c5e5c0$cc7fdb60$2f01a8c0@rahmandt>
References: <001601c5e5c0$cc7fdb60$2f01a8c0@rahmandt>
Message-ID: <4371C9C2.5090503@optonline.net>

The coefficients are in the attribute list:

 > x <- rnorm(100)
 > attributes(poly(x, 2))$coefs
$alpha
[1] -0.1585783 -0.1193990

$norm2
[1]   1.0000 100.0000 110.3589 254.5965

?attributes

Abd. Rahman Kassim wrote:
> Dear All,
> 
> Need some help in polynomials transformation to get the coefficients. I have tried "poly.transform" as applied in S-plus but it does not work.
> 
> Thanks in advanced for any helps.
> 
> Regards.
> 
> 
> Abd. Rahman Kassim (PhD)
> Head Forest Ecology Branch
> Forest Management & Ecology Program
> Forestry and Conservation Division
> Forest Research Institute Malaysia
> Kepong 52109
> Selangor, Malaysia
> 
> *****************************************
> 
> Checked by TrendMicro Interscan Messaging Security.
> For any enquiries, please contact FRIM IT Department.
> *****************************************
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From arunkrlal at yahoo.co.in  Wed Nov  9 11:21:34 2005
From: arunkrlal at yahoo.co.in (arun lal)
Date: Wed, 9 Nov 2005 02:21:34 -0800 (PST)
Subject: [R] annot have zero distances in "corSpatial"
Message-ID: <20051109102134.7875.qmail@web8413.mail.in.yahoo.com>

Hi all,

I have been attempting this repeated measure analysis
where correlation among observations (within each
subject) is spatial. 

My command is:
>fdat.lme1<-lme(Lint~Rep+Treatment,data=fdat,random=~1|fldPos,correlation=corLin(form=~lat+lon))

It is generating follwoing error:

Error in getCovariate.corSpatial(object, data = data)
: 
	Cannot have zero distances in "corSpatial"

first I thought that decimal values of
latitude/longitude are resulting in extremely small
values of distances, causing R to ignore it. So, I
scaled it up and made the coordinates integer values.
But I am still getting the same error.

Anyone can help? I don't know if you will need more
info to answer this question. Please let me know.

arun @LSU



From ripley at stats.ox.ac.uk  Wed Nov  9 11:36:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Nov 2005 10:36:15 +0000 (GMT)
Subject: [R] using abline and a fitted 2nd degree formula
In-Reply-To: <33769.62.119.38.100.1131530174.squirrel@webmail.slu.se>
References: <33769.62.119.38.100.1131530174.squirrel@webmail.slu.se>
Message-ID: <Pine.LNX.4.61.0511091030540.13417@gannet.stats>

On Wed, 9 Nov 2005, CG Pettersson wrote:

> Hello all,
>
> R2.1.1, Wk2
           ^^^  week 2?
> I am doing some two-step plotting, first using plot() to illustrate the
> datapoints and then using abline() to place a trend line from a fitted
> model into the plot.
>
> Everything works well as long as the formula of the fitted model i of the
> type:
>
> m1 <- lm(Dependent ~ Independent)
> then abline(m1) puts the proper straight line into the plot.
>
> But if I use:
>
> m2 <- lm(Dependent ~ Independent + I(Independent^2))
> abline(m2) produces a straight line, only from the first order term.
>
> Why, and what should I do about it?

Why: because that is what it is documented to do:

      'reg' is a regression object which contains 'reg$coef'.  If it is
      of length 1 then the value is taken to be the slope of a line
      through the origin, otherwise, the first 2 values are taken to be
      the intercept and slope.

What to do about it: first, read the help page and second, use predict and 
lines.  There is an example of the latter on ?cars.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From DOCGArmelini at iese.edu  Wed Nov  9 11:38:29 2005
From: DOCGArmelini at iese.edu (Armelini, Guillermo)
Date: Wed, 9 Nov 2005 11:38:29 +0100
Subject: [R] Query
Message-ID: <2DA68F10815D6E49956E3DC2CE18603E2240DD@SRVSTAFF.iese.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/650d54b5/attachment.pl

From huan.huang at uk.bnpparibas.com  Wed Nov  9 11:39:07 2005
From: huan.huang at uk.bnpparibas.com (huan.huang@uk.bnpparibas.com)
Date: Wed, 9 Nov 2005 10:39:07 +0000
Subject: [R] moc function, normal distribution peak capture
In-Reply-To: <59484F5CC089B4499DDAC9503DA0BC6706A51C@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <OFADC55A3E.1F3699D2-ON802570B4.003A4358-802570B4.003A83A0@bnpparibas.com>

Hi

With moc function (in moc library), I try to apply a mixture of 2 normal
distribution for univariate data which has 2 peaks,
however I found it came down to 1 normal distribution (weights became 1 and
0) and could not capture the 2nd peaks in many cases.
I set the first mode and second mode as initial values of mean of normal
distribution, and set an appropriate sd as initial values of sd of normal
distribution.

Has anyone experienced this problem? If anyone could let me know how to
avoid this problem, it would be appreciated.

Thanks

Huan


This message and any attachments (the "message") is\ intende...{{dropped}}



From Rau at demogr.mpg.de  Wed Nov  9 11:49:00 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Wed, 9 Nov 2005 11:49:00 +0100
Subject: [R] Can I run both R and Python through Emacs
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E69FDFA2@HERMES.demogr.mpg.de>

Dear Sri,

probably this is the wrong list to address this question. For example,
there are the newsgroups comp.emacs.xemacs or comp.lang.python which are
probably more appropriate.
Nevertheless, I don't think you need to do anything to configure XEmacs
for Python.
Just open or create a file with .py as extension ("C-x C-f") and syntax
highlighting should be automatically available.
You can also start the python interpreter (given python is installed)
via "C-c !"
Marked parts of your buffer can be submitted to the evaluator via "C-c
|"
The whole buffer can be evaluated via "C-c C-c"
....

I don't know which XEmacs version you have running ("M-x version") on
which platform, I just tried it quickly here using XEmacs 21.4.13 on
i586-pc-win32 and XEmacs 21.4.17 on i686-pc-cygwin.

But this is problably not a direction the help-mailing list of R should
pursue.

Best,
Roland


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Srinivas Iyyer
> Sent: Wednesday, November 09, 2005 12:09 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Can I run both R and Python through Emacs
> 
> Dear Group, 
>  I am new to R and Emacs both.  Until now I have been
> working through R's interactive space.  Today I
> configured R+XEmacs by following John Fox help file -
> how to configure Xemacs + R.  
> 
> I am very happy to do this. Now the question I have is
> can I make XEmacs to be the editor for both R and
> Python with syntax highliting etc. 
> 
> How can I do that. Could any one help me please.  
> 
> Thanks
> Sri
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Nov  9 12:06:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Nov 2005 11:06:02 +0000 (GMT)
Subject: [R] Query
In-Reply-To: <2DA68F10815D6E49956E3DC2CE18603E2240DD@SRVSTAFF.iese.org>
References: <2DA68F10815D6E49956E3DC2CE18603E2240DD@SRVSTAFF.iese.org>
Message-ID: <Pine.LNX.4.61.0511091102260.14836@gannet.stats>

On Wed, 9 Nov 2005, Armelini, Guillermo wrote:

> Dear all,
>
> My name is Guillermo I'm trying to write a short program to find all the
> possible combination in fitting an ARIMA Model. My script is the
> following:
>
> n=rnorm(100)+5
> Aux.M=matrix(NA,nrow=100,ncol=2)
> L=1
> for(i in 0:2)
> {for(j in 0:2) {for(k in 0:2)

need { here

> Aux.M[L,2]=arima(n,order=c(i,j,k),include.mean=TRUE)$aic
>
> x1=paste("AR=",i,sep=" ")
> x2=paste("I=",j,sep=" ")
> x3=paste("MA=",k,sep=" ")
> x4=paste(x1,x2,sep=" ")
> x5=paste(x4,x3,sep=" ")
>
> Aux.M[L,1]=x5
> L=L+1}}

need another } here

> After running in R, you will realize that the last for does not work. So

It does, but you did not store the resutls from it as you probably 
intended.

> I have problem using three 'for command' at the same time, and I do not
> know exactly how to solve the problem. Are there any more efficient
> approach? (i.e. using a combination of while and for) Could somebody
> help me?

You have forgotten the braces for the body of the inner for() loop.
Also, 27 combinations does not need 100 rows to store.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From secchi at sssup.it  Wed Nov  9 12:10:39 2005
From: secchi at sssup.it (Angelo Secchi)
Date: Wed, 9 Nov 2005 12:10:39 +0100
Subject: [R] Command line and R
Message-ID: <20051109121039.ecfa2de9.secchi@sssup.it>



Hi,
I wrote a small R script (delta.R) using commandArgs(). The script
works from the shell in usual way

R --no-save arg1 < delta2.R

Suppose arg1 is the output of another shell command (e.g. gawk,
sed ...). Is there a way to tell R to read arg1 from the
output of the previous command? Any other workaround?
Thanks


-- 
========================================================
 Angelo Secchi                     PGP Key ID:EA280337



From ripley at stats.ox.ac.uk  Wed Nov  9 12:12:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Nov 2005 11:12:05 +0000 (GMT)
Subject: [R] Type II and III sums of squares with Error in AOV
In-Reply-To: <854e3fa432e69f6a30cda3bd28f8ccd6@arrr.net>
References: <854e3fa432e69f6a30cda3bd28f8ccd6@arrr.net>
Message-ID: <Pine.LNX.4.61.0511082155340.21306@gannet.stats>

A multistratum aov() fit is just a list of aov() fits, so you can apply 
functions such as Anova to the individual strata.

However, why do you want types II and III sums of squares?  It is usual 
to do this type of analysis only with balanced designs.  In the cases I 
can envisage that these make any sense, they are the same as type I
(and in cases with only one treatment effect, they always are).

On Tue, 8 Nov 2005, Jarrett Byrnes wrote:

> I've recently run into the problem of using aov with nested factors,
> and wanting to get the type II and III sums of squares.  Normally Anova
> from the car package would do fine, but it doesn't like having an Error
> included, so
>
> my.aov <-aov(Response ~ Treatment + Error(Treatment:Replicate))
> Anova(my.aov, type="II")
>
> yields
>
> Error in Anova(nested.anova) : no applicable method for "Anova"
>
> And lm does not take Error as an argument.

Nor does log()!  Is that relevant?

> Error in eval(expr, envir, enclos) : couldn't find function "Error"
>
> Is there a way to get these additional types of sums of squares when
> Error is included in an aov model?

Have you read the reference on the aov help page?  That might be a good 
step to deepening your understanding of what Error() does.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From RRoa at fisheries.gov.fk  Wed Nov  9 11:32:49 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Wed, 9 Nov 2005 08:32:49 -0200
Subject: [R] Variograms and large distances
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC5DC@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From:	r-help-bounces at stat.math.ethz.ch [SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of Julian Burgos
> Sent:	Tuesday, November 08, 2005 10:41 PM
> To:	r-help at stat.math.ethz.ch
> Subject:	[R] Variograms and large distances
> 
> Hello R list,
> I need to compute empirical variograms using data from a large 
> geographic area (~10^6 km2).  Although I could not find a specific 
> reference, I assume that both geoR and gstat calculate distances among 
> data points assuming points are on a flat surface (using the Pythagorean 
> Theorem).  Because the location of my data is large and located near the 
> pole, assuming that latitude and longitude are coordinates on a flat 
> surface would introduce a -possibly large- bias in the empirical 
> variogram estimate.  My questions are the following:
> 
> a)  Does geoR and gstat assume that points are on a flat surface?
> 
> b) If I first calculate the distances among points using the Haversine 
> formula, it is possible to calculate the variogram with a matrix of 
> distances among points (where n is the number of observations) and a 
> vector of observation values?
> 
> Any help would be appreciated.
> 
> Julian
> 
> 
Hi Julian,

I suggest you transform your coordinates to UTM. I use Eino Uikannen's 
GeoConv program
http://www.kolumbus.fi/eino.uikkanen/geoconvgb/index.htm
but i believe there are packages and functions in R to carry out the 
transformation. GeoConv is a DOS program that runs in batch 
mode from the DOS console.
Ruben



From mineoeli at UNIPA.IT  Wed Nov  9 12:34:03 2005
From: mineoeli at UNIPA.IT (Elio Mineo)
Date: Wed, 09 Nov 2005 12:34:03 +0100
Subject: [R] R, Apache and PHP (was no subject)
In-Reply-To: <20051108213312.8249.qmail@web26904.mail.ukl.yahoo.com>
References: <20051108213312.8249.qmail@web26904.mail.ukl.yahoo.com>
Message-ID: <4371DEAB.8080809@unipa.it>

There are several projects on this. Look at

http://franklin.imgen.bcm.tmc.edu/R.web.servers/

Bye

Grupo Logit wrote:

>hello.
>My name is kerlim arturo.
>i was search into internet about R apache y PhP and  i found your mail into foro.
>i want create a stadistic's software on line with language R,apache y php .
> 
>i want know, what is the steps for developer this stadistic's software? where can i get information about language R, apache y php; and examples between language R and php ('source').
> 
>can I configurate  Language R,apache y php on windows xp, 2000,?
> 
>i don't write in english very good.
>excuse me
> 
>thank you   
>
>		
>---------------------------------
>
>
>Comprueba qu?? es nuevo, aqu??
>
>	[[alternative HTML version deleted]]
>
>  
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
----------------------------------------------------------------------------------
Angelo M. Mineo
Dipartimento di Scienze Statistiche e Matematiche "S. Vianelli"
Universit?? degli Studi di Palermo
Viale delle Scienze
90128 Palermo
url: http://dssm.unipa.it/elio



From Roger.Bivand at nhh.no  Wed Nov  9 12:39:11 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 9 Nov 2005 12:39:11 +0100 (CET)
Subject: [R] Command line and R
In-Reply-To: <20051109121039.ecfa2de9.secchi@sssup.it>
Message-ID: <Pine.LNX.4.44.0511091237030.13871-100000@reclus.nhh.no>

On Wed, 9 Nov 2005, Angelo Secchi wrote:

> 
> 
> Hi,
> I wrote a small R script (delta.R) using commandArgs(). The script
> works from the shell in usual way
> 
> R --no-save arg1 < delta2.R
> 
> Suppose arg1 is the output of another shell command (e.g. gawk,
> sed ...). Is there a way to tell R to read arg1 from the
> output of the previous command? Any other workaround?

Use shell variables, possibly also Sys.getenv() within R as well as or 
instead of commandArgs().

> Thanks
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From gsmatos1 at ig.com.br  Wed Nov  9 13:00:02 2005
From: gsmatos1 at ig.com.br (gsmatos1)
Date: Wed, 9 Nov 2005 09:00:02 -0300
Subject: [R] About: Error in FUN(X[[1]], ...) : symbol print-name too long
Message-ID: <20051109_120002_092149.gsmatos1@ig.com.br>

Hi, 

I??m trying to use the Win2BUGS package from R and I have a similar problem 
that reurns with the message: 

Error in FUN(X[[1]], ...) : symbol print-name too long 

But, there is no stray ` character in the file ( Sugestions given by: Duncan 
Temple Lang <duncan> 
Date: Mon, 26 Sep 2005 07:31:08 -0700 ) 

The progam in R is: 

library(R2WinBUGS) 
library(rbugs) 

dat <- 
list(x=c(49,48,50,44,54,56,48,48,51,51,50,53,51,50,51,54,50,53,50,49,51,47,53,50,49,55,53,48,54,46), 
y=c(50,49,57,52,47,52,58,45,55,54,51,54,56,53,52,47,51,54,50,47,46,44,54,55,52,57,52,48,48,51)) 

dat  <- format4Bugs(dat, digits = 0) 
parm <- c("lbda") 

bugs(dat, inits=list(NULL), parm, "d2.bug", 
n.chains = 1, n.iter = 5000, n.burnin = floor(n.iter/2), 
n.thin = max(1, floor(n.chains * (n.iter - n.burnin)/1000)), 
bin = (n.iter - n.burnin) / n.thin, 
debug = TRUE, DIC = TRUE, digits = 5, codaPkg = FALSE, 
bugs.directory = "C:/WinBUGS14/", 
working.directory = NULL, clearWD = FALSE) 

	The objective of the program is to compare means of two independent samples 
that results 
	in Beherens-Fisher posterior and in the model.file of WinBUGS "d2.bug" 
there is the following codes: 

  model 
{ 
   for( i in 1 : 30 ) { 
      x[i] ~ dnorm(mu1,sig1) 
   } 
   for( i in 1 : 30 ) { 
      y[i] ~ dnorm(mu2,sig2) 
   } 
   mu1 ~ dnorm(50,1.0E-6) 
   sig1 ~ dgamma(0.001,0.001) 
   mu2 ~ dnorm(50,1.0E-6) 
   sig2 ~ dgamma(0.001,0.001) 
   lbda <- mu1 - mu2 
} 

  I??m a new user of WinBUGS and if someone detect error in the model codes 
too, I??m grateful. 

	Thanks for help! 
	Gilberto Matos. 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/5490e606/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Wed Nov  9 13:25:37 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 09 Nov 2005 12:25:37 -0000 (GMT)
Subject: [R] Command line and R
In-Reply-To: <Pine.LNX.4.44.0511091237030.13871-100000@reclus.nhh.no>
Message-ID: <XFMail.051109122537.Ted.Harding@nessie.mcc.ac.uk>

On 09-Nov-05 Roger Bivand wrote:
> On Wed, 9 Nov 2005, Angelo Secchi wrote:
>> Hi,
>> I wrote a small R script (delta.R) using commandArgs(). The script
>> works from the shell in usual way
>> 
>> R --no-save arg1 < delta2.R
>> 
>> Suppose arg1 is the output of another shell command (e.g. gawk,
>> sed ...). Is there a way to tell R to read arg1 from the
>> output of the previous command? Any other workaround?
> 
> Use shell variables, possibly also Sys.getenv() within R as well as or 
> instead of commandArgs().

If it's a fairly simple shell comand (and even if it isn't, though
it could get tricky for complicated ones) you can use the "backquote"
trick (called, in well-spoken circles, "command substitution"):

  R --no-save `shellcmd` < delta2.R

As in all shell command lines, wherever you have a command (including
arguments etc.) between backquotes, as exemplified by "`shellcmd`" above,
the output of the command (as sent to stdout) replaces "`shellcmd`" in
the command-line. This could be a lot of stuff (depending on what
"shellcmd" is), or just one value, or whatever.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 09-Nov-05                                       Time: 12:25:32
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Wed Nov  9 13:38:20 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Nov 2005 13:38:20 +0100
Subject: [R] Type II and III sums of squares with Error in AOV
In-Reply-To: <Pine.LNX.4.61.0511082155340.21306@gannet.stats>
References: <854e3fa432e69f6a30cda3bd28f8ccd6@arrr.net>
	<Pine.LNX.4.61.0511082155340.21306@gannet.stats>
Message-ID: <x23bm6uhc3.fsf@viggo.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> A multistratum aov() fit is just a list of aov() fits, so you can apply 
> functions such as Anova to the individual strata.
> 
> However, why do you want types II and III sums of squares?  It is usual 
> to do this type of analysis only with balanced designs.  In the cases I 
> can envisage that these make any sense, they are the same as type I
> (and in cases with only one treatment effect, they always are).


I was about to make a similar comment. A possible exception is ANCOVA
where you likely want to test both the within-stratum effect of a
covariate and the effect of design factors adjusted for the covariate.

 
> On Tue, 8 Nov 2005, Jarrett Byrnes wrote:
> 
> > I've recently run into the problem of using aov with nested factors,
> > and wanting to get the type II and III sums of squares.  Normally Anova
> > from the car package would do fine, but it doesn't like having an Error
> > included, so
> >
> > my.aov <-aov(Response ~ Treatment + Error(Treatment:Replicate))
> > Anova(my.aov, type="II")
> >
> > yields
> >
> > Error in Anova(nested.anova) : no applicable method for "Anova"
> >
> > And lm does not take Error as an argument.
> 
> Nor does log()!  Is that relevant?
> 
> > Error in eval(expr, envir, enclos) : couldn't find function "Error"
> >
> > Is there a way to get these additional types of sums of squares when
> > Error is included in an aov model?
> 
> Have you read the reference on the aov help page?  That might be a good 
> step to deepening your understanding of what Error() does.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From rpeng at jhsph.edu  Wed Nov  9 14:41:43 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 09 Nov 2005 08:41:43 -0500
Subject: [R] how to plot the circles in matrix form
In-Reply-To: <20051108213600.68346.qmail@web33802.mail.mud.yahoo.com>
References: <20051108213600.68346.qmail@web33802.mail.mud.yahoo.com>
Message-ID: <4371FC97.6010208@jhsph.edu>

Would 'symbols()' be of use?

-roger

shanmuha boopathy wrote:
> Could you help me 
> to plot the circles in the form of matrix like
>  
> O  O  O  O
> O  O  O  O
> O  O  O  O
> 
> thank you..
>  
> with regards,
> boopathy.
> 
> 
> Thirumalai Shanmuha Boopathy, 
> Zimmer no : 07-15,
> R??tscher strasse 165, 
> 52072  Aachen . 
> Germany.
>  
> Home zone   :  0049 - 241 - 9813409
> Mobile zone :  0049 - 176 - 23567867
> 
> 
> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/



From secchi at sssup.it  Wed Nov  9 14:48:58 2005
From: secchi at sssup.it (Angelo Secchi)
Date: Wed, 9 Nov 2005 14:48:58 +0100
Subject: [R] Command line and R
In-Reply-To: <XFMail.051109122537.Ted.Harding@nessie.mcc.ac.uk>
References: <Pine.LNX.4.44.0511091237030.13871-100000@reclus.nhh.no>
	<XFMail.051109122537.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20051109144858.0e885248.secchi@sssup.it>




On Wed, 09 Nov 2005 12:25:37 -0000 (GMT)
(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> wrote:

> On 09-Nov-05 Roger Bivand wrote:
> > On Wed, 9 Nov 2005, Angelo Secchi wrote:
> >> Hi,
> >> I wrote a small R script (delta.R) using commandArgs(). The script
> >> works from the shell in usual way
> >> 
> >> R --no-save arg1 < delta2.R
> >> 
> >> Suppose arg1 is the output of another shell command (e.g. gawk,
> >> sed ...). Is there a way to tell R to read arg1 from the
> >> output of the previous command? Any other workaround?
> > 
> > Use shell variables, possibly also Sys.getenv() within R as well as or 
> > instead of commandArgs().
> 
> If it's a fairly simple shell comand (and even if it isn't, though
> it could get tricky for complicated ones) you can use the "backquote"
> trick (called, in well-spoken circles, "command substitution"):
> 
>   R --no-save `shellcmd` < delta2.R
> 
> As in all shell command lines, wherever you have a command (including
> arguments etc.) between backquotes, as exemplified by "`shellcmd`" above,
> the output of the command (as sent to stdout) replaces "`shellcmd`" in
> the command-line. This could be a lot of stuff (depending on what
> "shellcmd" is), or just one value, or whatever.
> 
> Best wishes,
> Ted.

Thanks.
A.



> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 09-Nov-05                                       Time: 12:25:32
> ------------------------------ XFMail ------------------------------


-- 
========================================================
 Angelo Secchi                     PGP Key ID:EA280337
========================================================
  Current Position:
  Research Fellow Scuola Superiore S.Anna
  Piazza Martiri della Liberta' 33, Pisa, 56127 Italy
  ph.: +39 050 883365
  email: secchi at sssup.it	www.sssup.it/~secchi/



From MSchwartz at mn.rr.com  Wed Nov  9 14:51:19 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 09 Nov 2005 07:51:19 -0600
Subject: [R] Title and axis
In-Reply-To: <200511091047.21275.mmiller@nassp.uct.ac.za>
References: <200511091047.21275.mmiller@nassp.uct.ac.za>
Message-ID: <1131544279.4155.36.camel@localhost.localdomain>

On Wed, 2005-11-09 at 10:47 +0200, Mark Miller wrote:
> I was wandering how to set the labels for the axis and how to set the title so 
> that the default title name is not displayed under the title I set. I also 
> use the following to save the plot to a file but it does not seem to save (I 
> use postscript for use in my Latex document)
> 
> x = seq(0,30,0.01)
> postscript("cumIAT.ps")
> plot(ecdf(w1a), do.point=FALSE)
> lines(x, pexp(x,0.405156250))
> title('Cummlative Plot of Monday IATs for entire 15 Weeks')
> dev.off()
> 
> Many thanks
> Mark Miler

Since we cannot fully replicate your code, I'll offer some hints here:

1. For the axis labels and plot title, you can set 'xlab', 'ylab' and
'main' in the plot() call:

plot(ecdf(w1a), do.points = FALSE, 
     xlab = "X Label", ylab = "Y Label",
     main = "Cumulative Plot of Monday IATs\nfor entire 15 Weeks")

Note that I inserted a '\n' (newline) in the title to put it on to two
lines.  If you want it on one line, you will likely need to set
'cex.main' to a number < 1 to reduce the font size.

See ?plot.default for more information.


2. If you are going to use the plot file in LaTeX, you need to create an
Encapsulated Postscript file (EPS), which requires the following
settings:

postscript("cumIAT.eps", horizontal = FALSE, height = 4, width = 4,
           onefile = FALSE, paper = "special")

The critical arguments are 'horizontal', 'onefile' and 'paper'. You can
adjust the 'height' and 'width' arguments as you require.

See the Details section of ?postscript for more information.

The reason that you are not getting a PS file using your code above is
that you are likely getting an error message after the plot() call
indicating that the figure margins are too large. You need to define
larger height and width arguments in the postscript() call to resolve
that issue, as the defaults for your system are not large enough.

BTW, no need to post twice. :-)

HTH,

Marc Schwartz



From bcutayar at lfdj.com  Wed Nov  9 15:01:14 2005
From: bcutayar at lfdj.com (Bruno Cutayar)
Date: Wed, 09 Nov 2005 15:01:14 +0100
Subject: [R]  dataframe without repetition
Message-ID: <4372012A.3010909@lfdj.com>



Hello,
with a data.frame like this :
 > toto <- 
data.frame(id=c("id1","id1","id2","id3","id3","id3"),dpt=c("13","13","34","30","30","30"))
 > toto
   id dpt
1 id1  13
2 id1  13
3 id2  34
4 id3  30
5 id3  30
6 id3  30

what is the most efficient ways to obtain :
   id dpt
1 id1  13
2 id2  34
3 id3  30
?
thanks in advance for your reply
Bruno



Si vous n'etes pas destinataires de ce message, merci d'avertir l'expediteur de l'erreur de distribution et de le detruire immediatement.
Ce message contient des informations confidentielles ou appartenant a La Francaise des Jeux. Il est etabli a l'intention exclusive de ses destinataires. Toute divulgation, utilisation, diffusion ou reproduction (totale ou partielle) de ce message ou des informations qu'il contient, doit etre prealablement autorisee.
Tout message electronique est susceptible d'alteration et son integrite ne peut etre assuree. La Francaise des Jeux decline toute responsabilite au titre de ce message s'il a ete modifie ou falsifie.

If you are not the intended recipient of this e-mail, please notify the sender of the wrong delivery and delete it immediately from your system.
This e-mail contains confidential information or information belonging to La Francaise des Jeux and is intended solely for the addressees. The unauthorised disclosure, use, dissemination or copying (either whole or partial) of this e-mail, or any information it contains, is prohibited.
E-mails are susceptible to alteration and their integrity cannot be guaranteed. La Francaise des Jeux shall not be liable for this e-mail if modified or falsified.



From ccleland at optonline.net  Wed Nov  9 15:05:16 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 09 Nov 2005 09:05:16 -0500
Subject: [R] dataframe without repetition
In-Reply-To: <4372012A.3010909@lfdj.com>
References: <4372012A.3010909@lfdj.com>
Message-ID: <4372021C.9030705@optonline.net>

?unique

 > unique(toto)
    id dpt
1 id1  13
3 id2  34
4 id3  30

Bruno Cutayar wrote:
> 
> Hello,
> with a data.frame like this :
>  > toto <- 
> data.frame(id=c("id1","id1","id2","id3","id3","id3"),dpt=c("13","13","34","30","30","30"))
>  > toto
>    id dpt
> 1 id1  13
> 2 id1  13
> 3 id2  34
> 4 id3  30
> 5 id3  30
> 6 id3  30
> 
> what is the most efficient ways to obtain :
>    id dpt
> 1 id1  13
> 2 id2  34
> 3 id3  30
> ?
> thanks in advance for your reply
> Bruno
> 
> 
> 
> Si vous n'etes pas destinataires de ce message, merci d'avertir l'expediteur de l'erreur de distribution et de le detruire immediatement.
> Ce message contient des informations confidentielles ou appartenant a La Francaise des Jeux. Il est etabli a l'intention exclusive de ses destinataires. Toute divulgation, utilisation, diffusion ou reproduction (totale ou partielle) de ce message ou des informations qu'il contient, doit etre prealablement autorisee.
> Tout message electronique est susceptible d'alteration et son integrite ne peut etre assuree. La Francaise des Jeux decline toute responsabilite au titre de ce message s'il a ete modifie ou falsifie.
> 
> If you are not the intended recipient of this e-mail, please notify the sender of the wrong delivery and delete it immediately from your system.
> This e-mail contains confidential information or information belonging to La Francaise des Jeux and is intended solely for the addressees. The unauthorised disclosure, use, dissemination or copying (either whole or partial) of this e-mail, or any information it contains, is prohibited.
> E-mails are susceptible to alteration and their integrity cannot be guaranteed. La Francaise des Jeux shall not be liable for this e-mail if modified or falsified.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From julie.lejeune at psl.ap-hop-paris.fr  Wed Nov  9 15:15:29 2005
From: julie.lejeune at psl.ap-hop-paris.fr (Julie Lejeune)
Date: Wed, 09 Nov 2005 15:15:29 +0100
Subject: [R] Tendancy Chi test?
Message-ID: <003101c5e538$0a1e62e0$3bc3a00a@user7ad6eb3b98>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/78d89d7f/attachment.pl

From jholtman at gmail.com  Wed Nov  9 15:16:46 2005
From: jholtman at gmail.com (jim holtman)
Date: Wed, 9 Nov 2005 09:16:46 -0500
Subject: [R] dataframe without repetition
In-Reply-To: <4372012A.3010909@lfdj.com>
References: <4372012A.3010909@lfdj.com>
Message-ID: <644e1f320511090616h19e2c9cct4a7560457ccdd200@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/362992c1/attachment.pl

From ripley at stats.ox.ac.uk  Wed Nov  9 15:17:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Nov 2005 14:17:05 +0000 (GMT)
Subject: [R] dataframe without repetition
In-Reply-To: <4372012A.3010909@lfdj.com>
References: <4372012A.3010909@lfdj.com>
Message-ID: <Pine.LNX.4.61.0511091415260.24874@gannet.stats>

unique(): see its help page.  (It has a data-frame method.)

On Wed, 9 Nov 2005, Bruno Cutayar wrote:

>
>
> Hello,
> with a data.frame like this :
> > toto <-
> data.frame(id=c("id1","id1","id2","id3","id3","id3"),dpt=c("13","13","34","30","30","30"))
> > toto
>   id dpt
> 1 id1  13
> 2 id1  13
> 3 id2  34
> 4 id3  30
> 5 id3  30
> 6 id3  30
>
> what is the most efficient ways to obtain :
>   id dpt
> 1 id1  13
> 2 id2  34
> 3 id3  30
> ?
> thanks in advance for your reply
> Bruno

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vdemart1 at tin.it  Wed Nov  9 15:18:59 2005
From: vdemart1 at tin.it (Vittorio)
Date: Wed, 9 Nov 2005 15:18:59 +0100 (GMT+01:00)
Subject: [R] R: Re:  RODBC fails to build
Message-ID: <18975192.1131545939684.JavaMail.root@pswm15.cp.tin.it>

Dear All,

Thanks to the suggestion of Prof Ripley I progressively 
solved the many libraries problems connected to the installation of the 
R package RODBC under FreeBSD 5.4. 
*** As far as this OS is concerned 
*** please take notice that the sentence in the README file:".....
Use 
the configure options --with-odbc-include and --with-odbc-lib or
environment variables ODBC_INCLUDE and ODBC_LIBS to set the include
and 
library paths as needed...."
doesn't give a complete piece of info. As 
a matter of fact, as well as setting $ODBC_INCLUDE=/usr/local/include 
and $ODBC_LIBS=/usr/local/lib I had also to put ** $LIBS=-
L/usr/local/lib ** which wasn't in the README (and wasn't that 
intuitive, was it?).
In so doing I solved the sql.h, sqlext.h headers 
problem and a successive problem with the SQLTables libraries.
This set 
things straight and RODBC was compiled flawlessly.
I think it would be 
worth your while adding this piece of information in the README file.

Ciao
Vittorio
 


>----Messaggio originale----
>Da: vdemart1 at tin.it
>Data: 8-nov-2005 11.45 PM
>A: <r-help at stat.math.ethz.ch>
>Ogg: Re: [R] 
RODBC fails to build
>
>yes, I read it. Nevertheless I can't make head 
or tail of it.
>Can anyone out there help me with a step by step 
explanation?
>Ciao
>Vittorio
>
>Alle 17:34, martedÃ¬ 08 novembre 2005, 
Prof Brian Ripley ha scritto:
>> Did you read the package's README?  It 
contains two ways to set these
>> paths.
>>
>> On Tue, 8 Nov 2005, 
Vittorio wrote:
>> > Context:Pentium 4, FreeBSD 5.4, R 2.2.0
>> >
>> > 
I updated the extra packages I
>> > had downloaded but the upgrading of 
RODBC failed complaining:
>> >
>> > checking
>> > for unistd.h... yes
>> > checking sql.h usability... no
>> > checking sql.h
>> > 
presence... no
>> > checking for sql.h... no
>> > checking sqlext.h 
usability...
>> > no
>> > checking sqlext.h presence... no
>> > 
checking for sqlext.h... no
>> > configure: error: "ODBC headers sql.h 
and sqlext.h not found"
>> > ERROR:
>> > configuration failed for 
package 'RODBC'
>> >
>> > The two header files are
>> > present in my 
box under /usr/local/include; I tried to symlink them to
>> > 
/usr/include and to /usr/local/lib/R/include to no avail.
>> >
>> > 
Please help.
>> >
>> > Vittorio
>
>______________________________________________
>R-help at stat.math.ethz.
ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE 
do read the posting guide! http://www.R-project.org/posting-guide.html
>



From redbeard at arrr.net  Wed Nov  9 15:54:29 2005
From: redbeard at arrr.net (Jarrett Byrnes)
Date: Wed, 9 Nov 2005 06:54:29 -0800
Subject: [R] Type II and III sums of squares with Error in AOV
In-Reply-To: <x23bm6uhc3.fsf@viggo.kubism.ku.dk>
References: <854e3fa432e69f6a30cda3bd28f8ccd6@arrr.net>
	<Pine.LNX.4.61.0511082155340.21306@gannet.stats>
	<x23bm6uhc3.fsf@viggo.kubism.ku.dk>
Message-ID: <6c82d55776d48062c07e36f46fd2797e@arrr.net>

While my original design was balanced, I lost several replicates due to 
a storm, making the whole thing unbalanced.  Ah, the realities of 
ecology.

So, how does one look at individual strata, and then how would one 
report an aggregate test of the effect in general?

On Nov 9, 2005, at 4:38 AM, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> A multistratum aov() fit is just a list of aov() fits, so you can 
>> apply
>> functions such as Anova to the individual strata.
>>
>> However, why do you want types II and III sums of squares?  It is 
>> usual
>> to do this type of analysis only with balanced designs.  In the cases 
>> I
>> can envisage that these make any sense, they are the same as type I
>> (and in cases with only one treatment effect, they always are).
>
>
> I was about to make a similar comment. A possible exception is ANCOVA
> where you likely want to test both the within-stratum effect of a
> covariate and the effect of design factors adjusted for the covariate.
>
>
>> On Tue, 8 Nov 2005, Jarrett Byrnes wrote:
>>
>>> I've recently run into the problem of using aov with nested factors,
>>> and wanting to get the type II and III sums of squares.  Normally 
>>> Anova
>>> from the car package would do fine, but it doesn't like having an 
>>> Error
>>> included, so
>>>
>>> my.aov <-aov(Response ~ Treatment + Error(Treatment:Replicate))
>>> Anova(my.aov, type="II")
>>>
>>> yields
>>>
>>> Error in Anova(nested.anova) : no applicable method for "Anova"
>>>
>>> And lm does not take Error as an argument.
>>
>> Nor does log()!  Is that relevant?
>>
>>> Error in eval(expr, envir, enclos) : couldn't find function "Error"
>>>
>>> Is there a way to get these additional types of sums of squares when
>>> Error is included in an aov model?
>>
>> Have you read the reference on the aov help page?  That might be a 
>> good
>> step to deepening your understanding of what Error() does.
>>
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 
> 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bill.shipley at usherbrooke.ca  Wed Nov  9 15:57:31 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Wed, 9 Nov 2005 09:57:31 -0500
Subject: [R] source of "susbcript out of bounds error" in nmle
Message-ID: <001601c5e53d$e9a1fbc0$9f1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/2bd48ea4/attachment.pl

From verena.s.hoffmann at web.de  Wed Nov  9 14:59:18 2005
From: verena.s.hoffmann at web.de (Verena Hoffmann)
Date: Wed, 09 Nov 2005 14:59:18 +0100
Subject: [R] two Kaplan-Meier Curves in one plot
Message-ID: <437200B6.1030601@web.de>

Hello!
Maybe someone is able to help me here:

I want to compare the survival probabilities of patients treated with 
two different  kinds of medicine.
So I did the     KM <- survfit(Surv(Time, Status), type="Kaplan-Meier")
                 plot(KM)
for each group seperatly. But now I want to compare both treatments and 
have both Kaplan-Meier Curves in one plot.
HOW CAN I DO THAT?
I tried add=TRUE, but that didn't work.

Thousand thanks in advance,
Verena



From Chris.Planet at gmx.de  Wed Nov  9 16:05:23 2005
From: Chris.Planet at gmx.de (Christian Hinz)
Date: Wed, 9 Nov 2005 16:05:23 +0100 (MET)
Subject: [R]  Is there no LU-Decomposition in R?
References: <12760.1125405438@www70.gmx.net>
Message-ID: <6203.1131548723@www95.gmx.net>

I need the LU_Decomposition for my conversion of the DIRK/SDIRK (singly
diagonally Runge Kutta implicit methods) algorithm.

thank you in advance.

Christian Hinz


-- 



--



From fcombes at gmail.com  Wed Nov  9 16:07:39 2005
From: fcombes at gmail.com (Florence Combes)
Date: Wed, 9 Nov 2005 16:07:39 +0100
Subject: [R] read.table error with R 2.2.0
Message-ID: <73dae3060511090707s706cf91bp80161533bbfffd03@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/32b3dccd/attachment.pl

From Christoph.Scherber at uni-jena.de  Wed Nov  9 16:24:23 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Wed, 09 Nov 2005 16:24:23 +0100
Subject: [R] trellis: positioning of key
Message-ID: <437214A7.4020509@uni-jena.de>

Dear R users,

Using xyplot(), how can I position the key in the *margin* outside the 
plotting area ?

My problem is that the key always overlaps with the x axis labels, no 
matter how I try to specify any of the par() arguments (e.g. oma()).

Many thanks for any suggestions!
Christoph


###
for information, here??s the code I use

par(oma=c(0,0,3,0)) ###this, I think, is what should be changed

xyplot(jitter(height,factor=1) ~ sowndiv | time2,
       groups=treatment, data=caging04051, subscripts=T,
       xlab=list("Diversity",cex=1.5),
       ylab=list("Height",cex=1.5),
       layout=c(3, 1),

       key=list(space="top",border=TRUE,
         columns=1,
         transparent=FALSE,
         points=list(pch=c(1,2),cex=2),
         lines=list(col=c(1,1), lty=c(1,2), lwd=c(1.5,1.5)),
         text=list(levels(treatment), cex=1.2)),

       par.strip.text=list(cex=1.2),
           strip = function(..., strip.names)
           strip.default(..., strip.names=c(FALSE, TRUE),
                     style = 1),

       scales=list(  x=list(at=c(1, 2, 4, 8, 16, 60),
                       labels=c(1,2,4,8,16,60),log=TRUE,cex=1.4,tck=0.02),
                     y=list(tck=0.02)),

       panel=function(x, y, subscripts, groups){
         panel.superpose(x, y, subscripts, groups,cex=1.4)
         which <- groups[subscripts]
                panel.loess(x[which=="C"],y[which=="C"],lty=1)
                panel.loess(x[which=="H"],y[which=="H"],lty=2)
       })



From dargosch at gmail.com  Wed Nov  9 16:33:38 2005
From: dargosch at gmail.com (Fredrik Karlsson)
Date: Wed, 9 Nov 2005 16:33:38 +0100
Subject: [R] Problems with Shapiro Wilk's test of normality.
Message-ID: <376e97ec0511090733q4eb0f7ccy1bbbc873b8339842@mail.gmail.com>

Hi,

I am trying to create a table with information from Shapiro Wilk's
test of normality.
However, it fails due to lack of sample size, it says, but the way I
see it, this is not a problem.
(See the table of sample sizes (almost) at the bottom).

Applying a different function using a similar ftable call is not a
problem (See the bottom table).

This is R 2.1.0 on Linux (Gentoo).

/Fredrik

> shapiro.p.value <- function(x){
+   if(length(! is.na(x)) > 3 & length(! is.na(x)) < 5000 ){
+     p <- shapiro.test(x)$p.value
+     return(p)
+    }else{
+      return(NA)
+      }
+ }
>
> distribution.table.fun <- function(x,na.rm=T,digits=1){
+
+   if(length(! is.na(x)) > 3 & length(! is.na(x)) < 5000){
+    # shapTest <- shapiro.test(x)
+    # W <- shapTest$statistic
+     W <- "W"
+   }
+
+
+
+   shap <- shapiro.p.value(x)
+   stars <- ''
+   premark <- ''
+   postmark <- ''
+   if(length(x) < 10){
+       premark <- '\\textit{'
+       postmark <- '}'
+   }
+
+   #skapa stj??rnor
+   if(! is.na(shap)){
+     if( shap <= 0.001 ){
+       stars <- '***'
+     }else{
+       if( shap <= 0.01 ){
+         stars <- '**'
+       }else{
+         if( shap <= 0.05 ){
+           stars <- '*'
+         }
+
+       }
+
+     }
+
+     outstr <- paste(premark,'W=',W,',p=',shap,postmark,stars,sep="")
+   }
+   else{
+     outstr <-  ""
+   }
+
+
+   return(outstr)
+
+ }
>
> ftable(tapply(aspvotwork$ampratio,list(Place=aspvotwork$Place,Age=aspvotwork$agemF,voicetype=aspvotwork$Type),FUN="length" ))
               voicetype Voiced Voiceless unaspirated Voiceless aspirated
Place  Age
Velar  18 - 24               44                    41                  34
       24 - 30               70                    81                  71
       30 - 36               59                    66                  64
       36 - 42               25                    27                  22
       42 - 48               22                    23                  23
       48 - 54               12                     9                   7
Dental 18 - 24               48                    61                  54
       24 - 30               82                   101                  89
       30 - 36               57                    82                  72
       36 - 42               19                    31                  34
       42 - 48               25                    33                  31
       48 - 54               10                    12                  14
Labial 18 - 24               74                   141                  84
       24 - 30              142                   264                 162
       30 - 36              124                   213                 148
       36 - 42               50                    91                  50
       42 - 48               49                    82                  64
       48 - 54               17                    26                  16
> ftable(tapply(aspvotwork$ampratio,list(Place=aspvotwork$Place,Age=aspvotwork$agemF,voicetype=aspvotwork$Type),FUN="distribution.table.fun",digits=4))
Error in shapiro.test(x) : sample size must be between 3 and 5000
>
  > ftable(tapply(aspvotwork$ampratio,list(Place=aspvotwork$Place,Age=aspvotwork$agemF,voicetype=aspvotwork$Type),FUN="mean",digits=4,na.rm=TRUE
))
               voicetype    Voiced Voiceless unaspirated Voiceless aspirated
Place  Age
Velar  18 - 24           0.4816810             0.4461307           0.4513994
       24 - 30           0.5289028             0.4778686           0.4888445
       30 - 36           0.5452949             0.5208633           0.4756369
       36 - 42           0.5631310             0.4697789           0.4709779
       42 - 48           0.4968318             0.4174068           0.4088855
       48 - 54           0.3057712             0.4483639           0.4561953
Dental 18 - 24           0.4058078             0.4596251           0.4091731
       24 - 30           0.4609731             0.4502778           0.4483340
       30 - 36           0.5095430             0.4726149           0.4315419
       36 - 42           0.4935719             0.4687774           0.4528758
       42 - 48           0.4344465             0.4220429           0.4362018
       48 - 54           0.3697664             0.4338549           0.4897856
Labial 18 - 24           0.4327926             0.4879985           0.4503917
       24 - 30           0.5309634             0.4839031           0.5927699
       30 - 36           0.4094516             0.4444757           0.3964693
       36 - 42           0.5010130             0.4855550           0.4540598
       42 - 48           0.4949510             0.4329442           0.3935921
       48 - 54           0.5217893             0.5124186           0.5011346
>



From mbmiller at taxa.epi.umn.edu  Wed Nov  9 16:36:16 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Wed, 9 Nov 2005 09:36:16 -0600 (CST)
Subject: [R] Is there no LU-Decomposition in R?
In-Reply-To: <6203.1131548723@www95.gmx.net>
References: <12760.1125405438@www70.gmx.net> <6203.1131548723@www95.gmx.net>
Message-ID: <Pine.GSO.4.60.0511090934270.7619@taxa.epi.umn.edu>

On Wed, 9 Nov 2005, Christian Hinz wrote:

> I need the LU_Decomposition for my conversion of the DIRK/SDIRK (singly 
> diagonally Runge Kutta implicit methods) algorithm.


I think you need the Matrix package:

http://cran.r-project.org/src/contrib/Descriptions/Matrix.html

Mike



From murdoch at stats.uwo.ca  Wed Nov  9 16:44:20 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 09 Nov 2005 10:44:20 -0500
Subject: [R] read.table error with R 2.2.0
In-Reply-To: <73dae3060511090707s706cf91bp80161533bbfffd03@mail.gmail.com>
References: <73dae3060511090707s706cf91bp80161533bbfffd03@mail.gmail.com>
Message-ID: <43721954.8000209@stats.uwo.ca>

On 11/9/2005 10:07 AM, Florence Combes wrote:
> Dear all,
> 
> I just upgraded version of R to R 2.2.0, and I have a problem with a script
> that did not happen with my previous version.
> Here is the error :
> 
> -----------------------------------------
>> param<-read.table(file="param.dat",sep ="\t",header=TRUE,fill=TRUE,
> na.strings="NA")
> Erreur dans read.table.default(file = "param.dat", sep = "\t", header =
> TRUE, :
> 5 arguments passed to 'readTableHead' which requires 6
> -----------------------------------------
> 
> whereas all was OK before. I cannot understand what's happening.
> 
> Has someone already encountered this ??
> Any help greatly appreciated,

There is no "read.table.default" in standard R 2.2.0, so it appears that 
you have installed a replacement for read.table, and it no longer works. 
   If you type

getAnywhere("read.table")$where

and

getAnywhere("read.table.default")$where

you are likely to see where those functions came from.  (I see

 > getAnywhere("read.table")$where
[1] "package:base"   "namespace:base"

 > getAnywhere("read.table.default")$where
character(0)

indicating that read.table comes from the base package, and 
read.table.default doesn't exist.

Duncan Murdoch



From pbarros at ualg.pt  Wed Nov  9 16:45:06 2005
From: pbarros at ualg.pt (Pedro de Barros)
Date: Wed, 09 Nov 2005 15:45:06 +0000
Subject: [R] Interpretation of output from glm
In-Reply-To: <20051109014943.RCHG28424.tomts25-srv.bellnexxia.net@JohnDe
	sktop8300>
References: <6.1.2.0.2.20051108144644.026b6ec0@pop.ualg.pt>
	<20051109014943.RCHG28424.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <6.1.2.0.2.20051109154310.0240a708@pop.ualg.pt>

Dear John,

Thanks for the quick reply. I did indeed have these ideas, but somehow 
"floating", and all I could find about this mentioned categorical 
predictors. Can you suggest a good book where I could try to learn more 
about this?

Thanks again,

Pedro
At 01:49 09/11/2005, you wrote:
>Dear Pedro,
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pedro de Barros
> > Sent: Tuesday, November 08, 2005 9:47 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Interpretation of output from glm
> > Importance: High
> >
> > I am fitting a logistic model to binary data. The response
> > variable is a factor (0 or 1) and all predictors are
> > continuous variables. The main predictor is LT (I expect a
> > logistic relation between LT and the probability of being
> > mature) and the other are variables I expect to modify this relation.
> >
> > I want to test if all predictors contribute significantly for
> > the fit or not I fit the full model, and get these results
> >
> >  > summary(HMMaturation.glmfit.Full)
> >
> > Call:
> > glm(formula = Mature ~ LT + CondF + Biom + LT:CondF + LT:Biom,
> >      family = binomial(link = "logit"), data = HMIndSamples)
> >
> > Deviance Residuals:
> >      Min       1Q   Median       3Q      Max
> > -3.0983  -0.7620   0.2540   0.7202   2.0292
> >
> > Coefficients:
> >                Estimate Std. Error z value Pr(>|z|)
> > (Intercept) -8.789e-01  3.694e-01  -2.379  0.01735 *
> > LT           5.372e-02  1.798e-02   2.987  0.00281 **
> > CondF       -6.763e-02  9.296e-03  -7.275 3.46e-13 ***
> > Biom        -1.375e-02  2.005e-03  -6.856 7.07e-12 ***
> > LT:CondF     2.434e-03  3.813e-04   6.383 1.74e-10 ***
> > LT:Biom      7.833e-04  9.614e-05   8.148 3.71e-16 ***
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > (Dispersion parameter for binomial family taken to be 1)
> >
> >      Null deviance: 10272.4  on 8224  degrees of freedom
> > Residual deviance:  7185.8  on 8219  degrees of freedom
> > AIC: 7197.8
> >
> > Number of Fisher Scoring iterations: 8
> >
> > However, when I run anova on the fit, I get  >
> > anova(HMMaturation.glmfit.Full, test='Chisq') Analysis of
> > Deviance Table
> >
> > Model: binomial, link: logit
> >
> > Response: Mature
> >
> > Terms added sequentially (first to last)
> >
> >
> >             Df Deviance Resid. Df Resid. Dev P(>|Chi|)
> > NULL                        8224    10272.4
> > LT          1   2873.8      8223     7398.7       0.0
> > CondF       1      0.1      8222     7398.5       0.7
> > Biom        1      0.2      8221     7398.3       0.7
> > LT:CondF    1    142.1      8220     7256.3 9.413e-33
> > LT:Biom     1     70.4      8219     7185.8 4.763e-17
> > Warning message:
> > fitted probabilities numerically 0 or 1 occurred in: method(x
> > = x[, varseq <= i, drop = FALSE], y = object$y, weights =
> > object$prior.weights,
> >
> >
> > I am having a little difficulty interpreting these results.
> > The result from the fit tells me that all predictors are
> > significant, while
> > the anova indicates that besides LT (the main variable), only the
> > interaction of the other terms is significant, but the main
> > effects are not.
> > I believe that in the first output (on the glm object), the
> > significance of
> > all terms is calculated considering each of them alone in the
> > model (i.e.
> > removing all other terms), while the anova output is (as it says)
> > considering the sequential addition of the terms.
> >
> > So, there are 2 questions:
> > a) Can I tell that the interactions are significant, but not
> > the main effects?
>
>In a model with this structure, the "main effects" represent slopes over the
>origin (i.e., where the other variables in the product terms are 0), and
>aren't meaningfully interpreted as main effects. (Is there even any data
>near the origin?)
>
> > b) Is it legitimate to consider a model where the interactions are
> > considered, but not the main effects CondF and Biom?
>
>Generally, no: That is, such a model is interpretable, but it places strange
>constraints on the regression surface -- that the CondF and Biom slopes are
>0 over the origin.
>
>None of this is specific to logistic regression -- it applies generally to
>generalized linear models, including linear models.
>
>I hope this helps,
>  John



From nali at umn.edu  Wed Nov  9 16:46:57 2005
From: nali at umn.edu (Na Li)
Date: Wed, 09 Nov 2005 09:46:57 -0600
Subject: [R] writing R shell scripts?
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>
Message-ID: <5j64r1x1qm.fsf@bass.biostat.umn.edu>

On  7 Nov 2005, Mike Miller wrote:

> Is it possible to do this sort of thing in R using something like this?:
> 
> #!/usr/lib/R/bin/R.bin
> 
> Well, that isn't quite it because I tried it and it didn't work!
 
Mike,

I use a shell script to do this which also allows passing in command line
options.

http://www.biostat.umn.edu/~nali/computing/Rstuff.html#sec2

It is a hack of course.  Hopefully soon this will be implemented in R itself.

Michael



From fcombes at gmail.com  Wed Nov  9 16:57:27 2005
From: fcombes at gmail.com (Florence Combes)
Date: Wed, 9 Nov 2005 16:57:27 +0100
Subject: [R] read.table error with R 2.2.0
In-Reply-To: <43721954.8000209@stats.uwo.ca>
References: <73dae3060511090707s706cf91bp80161533bbfffd03@mail.gmail.com>
	<43721954.8000209@stats.uwo.ca>
Message-ID: <73dae3060511090757x9587d45mff9193cd2f18b93@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/22957fb5/attachment.pl

From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Nov  9 16:57:35 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 9 Nov 2005 16:57:35 +0100 (CET)
Subject: [R] Tendancy Chi test?
In-Reply-To: <003101c5e538$0a1e62e0$3bc3a00a@user7ad6eb3b98>
References: <003101c5e538$0a1e62e0$3bc3a00a@user7ad6eb3b98>
Message-ID: <Pine.LNX.4.51.0511091656260.1413@artemis.imbe.med.uni-erlangen.de>


On Wed, 9 Nov 2005, Julie Lejeune wrote:

> Hello,
> I would like to know what I do it to test correlation between ordered categorical variables.
> Tendancy  Chi test?

a linear-by-linear association test is one possibilty, see `lbl_test' in
package `coin'.

Best,

Torsten

> Thank you,
> Melle Lejeune Julie
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From p.dalgaard at biostat.ku.dk  Wed Nov  9 17:01:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Nov 2005 17:01:27 +0100
Subject: [R] read.table error with R 2.2.0
In-Reply-To: <73dae3060511090707s706cf91bp80161533bbfffd03@mail.gmail.com>
References: <73dae3060511090707s706cf91bp80161533bbfffd03@mail.gmail.com>
Message-ID: <x2acgdu7xk.fsf@viggo.kubism.ku.dk>

Florence Combes <fcombes at gmail.com> writes:

> Dear all,
> 
> I just upgraded version of R to R 2.2.0, and I have a problem with a script
> that did not happen with my previous version.
> Here is the error :
> 
> -----------------------------------------
> > param<-read.table(file="param.dat",sep ="\t",header=TRUE,fill=TRUE,
> na.strings="NA")
> Erreur dans read.table.default(file = "param.dat", sep = "\t", header =
> TRUE, :
> 5 arguments passed to 'readTableHead' which requires 6
> -----------------------------------------
> 
> whereas all was OK before. I cannot understand what's happening.

read.table is not generic in 2.2.0 and it calls 

.Internal(readTableHead(file, nlines, comment.char,
        blank.lines.skip, quote, sep))


So where did  read.table.default() come into play? Is it picking up a
version that you yourself have modified perhaps? Or are you using some
package which redefines read.table and needs to be updated for 2.2.0?

        -p

 
> Has someone already encountered this ??
> Any help greatly appreciated,
> 
> Thanks a lot,
> 
> Florence.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From deepayan.sarkar at gmail.com  Wed Nov  9 17:03:46 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 9 Nov 2005 10:03:46 -0600
Subject: [R] trellis: positioning of key
In-Reply-To: <437214A7.4020509@uni-jena.de>
References: <437214A7.4020509@uni-jena.de>
Message-ID: <eb555e660511090803p37e1f68cma815abc7c86ebf23@mail.gmail.com>



On 11/9/05, Christoph Scherber <Christoph.Scherber at uni-jena.de> wrote:
> Dear R users,
> 
> Using xyplot(), how can I position the key in the *margin* outside the 
> plotting area ?
> 
> My problem is that the key always overlaps with the x axis labels, no 
> matter how I try to specify any of the par() arguments (e.g. oma()).

Evidently that doesn't happen regularly, or someone would have noticed by now. In other words, there's something specific to your example that's causing it. Unless you give us a reproducible example, preferably with all the irrelevant junk edited out, I don't see how we can help.

> ###
> for information, here??s the code I use
> 
> par(oma=c(0,0,3,0)) ###this, I think, is what should be changed

?lattice has the following note:

     Lattice plots are highly customizable via user-modifiable
     settings. However, these are completely unrelated to base graphics
     settings; in particular, changing 'par()' settings usually have no
     effect on lattice plots.

-Deepayan



From mbmiller at taxa.epi.umn.edu  Wed Nov  9 17:06:57 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Wed, 9 Nov 2005 10:06:57 -0600 (CST)
Subject: [R] writing R shell scripts?
In-Reply-To: <4371BAE3.4010801@maths.lth.se>
References: <Pine.GSO.4.60.0511071937020.1739@taxa.epi.umn.edu>
	<d332d3e10511080643w13773509rf5b7aacae8e0ac6e@mail.gmail.com>
	<Pine.GSO.4.60.0511081919030.19524@taxa.epi.umn.edu>
	<43717369.6070108@maths.lth.se>
	<Pine.GSO.4.60.0511082210360.29031@taxa.epi.umn.edu>
	<43718CF8.3020401@maths.lth.se>
	<Pine.GSO.4.60.0511090030390.11831@taxa.epi.umn.edu>
	<4371BAE3.4010801@maths.lth.se>
Message-ID: <Pine.GSO.4.60.0511090959080.22352@taxa.epi.umn.edu>

On Wed, 9 Nov 2005, Henrik Bengtsson wrote:

> Put everything in curly brackets as my above example show.

Ah ha! This leads to the latest version of my little one-liner (called 
"doR"):

#!/bin/sh
echo "output <- { $1 }; write.table(file=stdout(), .Last.value, row.names=FALSE, col.names=FALSE); q()" | /usr/local/bin/R --slave --no-save


The use of "output <-" suppresses the unwanted intermediate computations 
and the curly brackets make it so that ".Last.value" does what we want. 
So I can now do a series of semi-colon separated commands in my one-liner 
and pump out the result of only the last step.  Like so:

# doR 'dims <- c(100,5) ; A <- matrix(rnorm(100*5)*4,dims); chol(cov(A))'
3.94658490894107 0.317366981840069 1.06924803070620 0.340120807287771 -0.497063892955837
0 3.8326282049957 -0.675108240711913 -0.0199506782900556 -0.306406513059080
0 0 3.56097699699067 -0.52407121204737 0.107635052917745
0 0 0 3.88866072385884 -0.365208065096497
0 0 0 0 3.81017455630087

That isn't a great example, but you get the idea.  I use this kind of 
thing pretty often.

Mike

-- 
Michael B. Miller, Ph.D.
Assistant Professor
Division of Epidemiology and Community Health
and Institute of Human Genetics
University of Minnesota
http://taxa.epi.umn.edu/~mbmiller/



From MikeJones at westat.com  Wed Nov  9 17:14:14 2005
From: MikeJones at westat.com (Mike Jones)
Date: Wed, 9 Nov 2005 11:14:14 -0500
Subject: [R] elements in a matrix to a vector
Message-ID: <403593359CA56C4CAE1F8F4F00DCFE7D02722D44@MAILBE2.westat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/dc034143/attachment.pl

From arnaud.dowkiw at orleans.inra.fr  Wed Nov  9 17:20:38 2005
From: arnaud.dowkiw at orleans.inra.fr (Arnaud Dowkiw)
Date: Wed, 09 Nov 2005 17:20:38 +0100
Subject: [R] Replace missing values in spatial design using moving average
Message-ID: <5.2.0.9.0.20051109171407.02087318@orleans.inra.fr>

Dera R helpers,

I have a (x,y,z) data file where x and y are spatial coordinates and z a 
variable. I have some missing values in the z column and I would like to 
replace them with an optimized estimation from the neighbour cells. I could 
not find any function in R to do that. Is anybody aware of such function ? 
Maybe someone knows how to implement the Papadakis method in R...
Thanks for your help,

Arnaud DOWKIW

- - - - - - - - - - - - - - - - - - - - - - -
Arnaud DOWKIW
INRA
Unit?? Am??lioration G??n??tique et Physiologie Foresti??res
2163 avenue de la Pomme de Pin
BP 20619 ARDON
45166 OLIVET CEDEX
FRANCE
Tel. + 33 2 38 41 78 00
Fax. + 33 2 38 41 48 09
- - - - - - - - - - - - - - - - - - - - - - -



From ivo_welch at mailblocks.com  Wed Nov  9 17:24:18 2005
From: ivo_welch at mailblocks.com (ivo welch)
Date: Wed, 09 Nov 2005 11:24:18 -0500
Subject: [R] short suggestion to the R development team
Message-ID: <437222B2.2090508@mailblocks.com>


Dear R developpment team:  would it be easy to introduce a global variable (in options()) that contains the name of the R file that is currently being processed?  (ala \jobname in latex.)  if this is difficult, would it be easy to introduce something like "$0" in perl (i.e., argv[0] in C), which contains the current R file?  I just checked what perl does interactively, when no file is being processed.  It fills $0 with "-", though it would seem to me that just "" would be more appropriate.

sincerely,

/iaw



From idimakos at upatras.gr  Wed Nov  9 17:53:43 2005
From: idimakos at upatras.gr (=?iso-8859-7?Q?=C3=E9=DC=ED=ED=E7=F2_=C4=E7=EC=DC=EA=EF=F2?=)
Date: Wed, 9 Nov 2005 18:53:43 +0200 (EET)
Subject: [R] Type II and III sums of squares with Error in AOV
In-Reply-To: <x23bm6uhc3.fsf@viggo.kubism.ku.dk>
References: <854e3fa432e69f6a30cda3bd28f8ccd6@arrr.net>
	<Pine.LNX.4.61.0511082155340.21306@gannet.stats>
	<x23bm6uhc3.fsf@viggo.kubism.ku.dk>
Message-ID: <36259.84.254.10.74.1131555223.squirrel@mail.upatras.gr>

On Wed, November 9, 2005 14:38, Peter Dalgaard wrote:
> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> A multistratum aov() fit is just a list of aov() fits, so you can apply
>> functions such as Anova to the individual strata.
>>
>> However, why do you want types II and III sums of squares?  It is usual
>> to do this type of analysis only with balanced designs.  In the cases I
>> can envisage that these make any sense, they are the same as type I
>> (and in cases with only one treatment effect, they always are).
>
>
> I was about to make a similar comment. A possible exception is ANCOVA
> where you likely want to test both the within-stratum effect of a
> covariate and the effect of design factors adjusted for the covariate.

May I offer a more mundane reason for asking for Type II and PRIMARILY
Type III SS?

The editor of that particular journal who is hooked on a specific type of
software and knows only about what he/she gets from the software?

Sincerely,

Ioannis
-- 
Ioannis C. Dimakos
University of Patras
Department of Elementary Education
Patras, GR-26500 GREECE
http://www.elemedu.upatras.gr/dimakos/
http://yannishome.port5.com/



From droberts at montana.edu  Wed Nov  9 18:02:37 2005
From: droberts at montana.edu (Dave Roberts)
Date: Wed, 09 Nov 2005 10:02:37 -0700
Subject: [R] retrieve most abundant species by sample unit
In-Reply-To: <B0C5E226-9D56-4131-AAD3-3DEA8A7DC8EF@uvic.ca>
References: <B0C5E226-9D56-4131-AAD3-3DEA8A7DC8EF@uvic.ca>
Message-ID: <43722BAD.6060404@montana.edu>

Graham,

It's relatively easily done, especially the first one.

Let's suppose your veg data frame is called veg

 > dom1 <- apply(veg,1,which.max)

returns a vector with the column number of the species with the highest 
abundance for each sample (if there are ties, it returns the first one).
If you're concerned about ties, you can check to see how many there are 
with

for (i in 1:nrow(veg)) print(sum(veg[i,]==dom1[i]))

There may be ways to eliminate the for loop, but this works

If you want the names of the species, rather than column number, you can do

 > names(veg)[dom1]

which will return the species names (assuming they are the column names 
of the data.frame).  Now to get the next most abundant species, zero out 
the dominant species and repeat

 > tmp <- veg
 > for (i in 1:nrow(veg)) tmp[i,dom1[i]] <- 0
 > dom2 <- apply(veg,1,which.max)

HTH Dave R

Graham Watt-Gremm wrote:
> Hi R-users:
> [R 2.2 on OSX 10.4.3]
> I have a (sparse) vegetation data frame with 500 rows (sampling  
> units) and 177 columns (plant species) where the data represent %  
> cover. I need to summarize the cover data by returning the names of  
> the most dominant and the second most dominant species per plot. I  
> reduced the data frame to omit cover below 5%; this is what it looks  
> like stacked. I have experimented with tapply(), by(), and some  
> functions mentioned in archived postings, but I haven't seen anything  
> that answers to this directly. Does anybody have any ideas?
> 
>       OBJECTID       PolygonID SpeciesCod AbundanceP
> 1       15006     ANT-CBG-rr1     Leymol    5.00000
> 3       15008     ANT-CBG-rr1     Ambcha    5.00000
> 5       15010      ANT-ESH-27     Atrpat   20.00000
> 6       15011      ANT-ESH-27     Ambcha   10.00000
> 11      15016      ANT-ESH-28     Salvir   20.00000
> 14      15019      ANT-ESH-28     Atrpat    5.00000
> 18      15023 ANT-POR-Rubarm5     Rubarm   60.00000
> 19      15024 ANT-POR-Rubarm5     Hedhel   40.00000
> 25      15030      ECO-CBG-A2     Griint    5.00000
> 27      15032      ECO-CBG-A2     Anngra    5.00000
> 38      15043      ECO-CBG-A4     Sperub   50.00000
> 
> Regards,
> Graham Watt-Gremm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From p.dalgaard at biostat.ku.dk  Wed Nov  9 18:13:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Nov 2005 18:13:00 +0100
Subject: [R] elements in a matrix to a vector
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D02722D44@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D02722D44@MAILBE2.westat.com>
Message-ID: <x21x1pu4mb.fsf@viggo.kubism.ku.dk>

"Mike Jones" <MikeJones at westat.com> writes:

> hi all,
> 
> i'm trying to get elements in a matrix into a vector.  i need a
> "streamlined" way to do it as the way i'm doing it is not very
> serviceable.  an example is a 3x3 matrix like
> 
> 0 0 3
> 2 0 0
> 0 4 0
> 
> to a vector like
> 
> 3 2 4

You need to explain the rules of the game better. Do you mean like


> M <- matrix(c(0,0,3,2,0,0,0,4,0),3,byrow=T)
> rowSums(M)
[1] 3 2 4

or

> x <- c(t(M)) ; x[x!=0]
[1] 3 2 4

or

...?

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From rkoenker at uiuc.edu  Wed Nov  9 18:14:33 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 9 Nov 2005 11:14:33 -0600
Subject: [R] elements in a matrix to a vector
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D02722D44@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D02722D44@MAILBE2.westat.com>
Message-ID: <C3AEBA48-6611-462D-9E43-047BBF0FE200@uiuc.edu>

If you are really looking for a way to extract the non-zero elements you
can use something like the following:

 > library(SparseM)
 >  A
      [,1] [,2] [,3]
[1,]    0    0    3
[2,]    2    0    0
[3,]    0    4    0
 > as.matrix.csr(A)@ra
[1] 3 2 4

there is a tolerance parameter in the coercion to sparse representation
to decide what is really "zero"  -- by default this is  eps = .Machine 
$double.eps.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Nov 9, 2005, at 10:14 AM, Mike Jones wrote:

> hi all,
>
> i'm trying to get elements in a matrix into a vector.  i need a
> "streamlined" way to do it as the way i'm doing it is not very
> serviceable.  an example is a 3x3 matrix like
>
> 0 0 3
> 2 0 0
> 0 4 0
>
> to a vector like
>
> 3 2 4
>
> thanks...mj
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From gunter.berton at gene.com  Wed Nov  9 18:17:01 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 9 Nov 2005 09:17:01 -0800
Subject: [R] elements in a matrix to a vector
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D02722D44@MAILBE2.westat.com>
Message-ID: <200511091717.jA9HH1B5002363@faraday.gene.com>

A matrix is a vector with a dim() attribute. Values are stored in column
major order (first column on top of second column on top of ...). Thus, if
mymatrix is your matrix,

mymatrix[as.logical(mymatrix)] 

gives c(2,4,3), because as.logical(0) = FALSE, as.logical(nonzero) = TRUE.

If you are really doing serious work with sparse matrices , you should look
at the sparseM package.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Jones
> Sent: Wednesday, November 09, 2005 8:14 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] elements in a matrix to a vector
> 
> hi all,
> 
> i'm trying to get elements in a matrix into a vector.  i need a
> "streamlined" way to do it as the way i'm doing it is not very
> serviceable.  an example is a 3x3 matrix like
> 
> 0 0 3
> 2 0 0
> 0 4 0
> 
> to a vector like
> 
> 3 2 4
> 
> thanks...mj
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bill.shipley at usherbrooke.ca  Wed Nov  9 18:33:23 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Wed, 9 Nov 2005 12:33:23 -0500
Subject: [R] strategies to obtain convergence using nlme
Message-ID: <002101c5e553$afd252d0$9f1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/f5f6b8ae/attachment.pl

From droberts at montana.edu  Wed Nov  9 18:27:35 2005
From: droberts at montana.edu (Dave Roberts)
Date: Wed, 09 Nov 2005 10:27:35 -0700
Subject: [R] elements in a matrix to a vector
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D02722D44@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D02722D44@MAILBE2.westat.com>
Message-ID: <43723187.5010500@montana.edu>

Mike,

    It's not clear whaty way you are doing it now, but this works

 > x <- matrix(c(0,2,0,0,0,4,3,0,0),nrow=3)
 > x
      [,1] [,2] [,3]
[1,]    0    0    3
[2,]    2    0    0
[3,]    0    4    0
 > y <- as.vector(t(x))
 > z <- y[y!=0]
 > z
[1] 3 2 4


Dave



Mike Jones wrote:
> hi all,
> 
> i'm trying to get elements in a matrix into a vector.  i need a
> "streamlined" way to do it as the way i'm doing it is not very
> serviceable.  an example is a 3x3 matrix like
> 
> 0 0 3
> 2 0 0
> 0 4 0
> 
> to a vector like
> 
> 3 2 4
> 
> thanks...mj
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From elvis at xlsolutions-corp.com  Wed Nov  9 18:38:01 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed, 09 Nov 2005 10:38:01 -0700
Subject: [R] Course***R/Splus Fundamentals and Programming Techniques,
	December 2005
Message-ID: <20051109103800.a108dc04937c07ba67766dad37185406.a9b0bda3c7.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" in New York and Boston: www.xlsolutions-corp.com/Rfund.htm


****Boston           ------------------------------------ December
15th-16th, 2005
****New York       ------------------------------------ December
19th-20th, 2005

Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From duncan at wald.ucdavis.edu  Wed Nov  9 18:39:11 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Wed, 09 Nov 2005 09:39:11 -0800
Subject: [R] short suggestion to the R development team
In-Reply-To: <437222B2.2090508@mailblocks.com>
References: <437222B2.2090508@mailblocks.com>
Message-ID: <4372343F.5070402@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

I assume you are talking about source() when you say "R file that is
currently being processed".
A few weeks ago I worked on an enhanced version of source.
It allows
* nested source() with relative links,
* looking for files in a pre-defined set of directorys,
* determining the current stack of files/URIs being sourced() in the
nested calls
* recognizing URIs from strings to avoid the need to say
source(url("http://...")

I have this in a package that I have not released, but that I have just
posted at
 http://www.omegahat.org/Prerelease/RSource_0.1-0.tar.gz

It also has a simple implementation of using gpg to manage
encrypted files using an external application.

I have tested the package a little bit and there is some documentation,
but not an excessive amount, shall we say.

And there are some example S files at http://www.omegahat.org/RIO/test
that one can use to test nested source() calls for URIs and examining
the stack of source() calls.

We may put something like this into the R distribution, but
for the moment, it is better to do these things as packages.

 D.

ivo welch wrote:
> Dear R developpment team:  would it be easy to introduce a global variable (in options()) that contains the name 
> of the R file that is currently being processed?  (ala \jobname in latex.) 
> if this is difficult, would it be easy to introduce something like "$0" in perl (i.e., argv[0] in C), 
> which contains the current R file?  I just checked what perl does interactively, when no file is being processed. 
> It fills $0 with "-", though it would seem to me that just "" would be more appropriate.
> 
> sincerely,
> 
> /iaw
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

- --
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (Darwin)
Comment: Using GnuPG with Thunderbird - http://enigmail.mozdev.org

iD8DBQFDcjQ/9p/Jzwa2QP4RAlQVAJ9U9GR8l0YVnOi3SYahfqmLN208XwCfar8Y
7HiDI2Us+j3LRnN6TaTRA5c=
=eEkJ
-----END PGP SIGNATURE-----



From leaflovesun at yahoo.ca  Wed Nov  9 18:42:28 2005
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Wed, 9 Nov 2005 10:42:28 -0700
Subject: [R] Variogram
Message-ID: <200511091742.jA9HgpDX026771@hypatia.math.ethz.ch>

Sorry about this, I didn't know. I guess I have posted too much garbage on here.

Thanks to Edzer for your answers!

Leaf
======= At 2005-11-09, 02:14:21 you wrote: =======

>Leaf, please note that r-help is not the appropriate place to ask
>package-specific questions. We have r-sig-geo for questions related
>to geographic data in R, and gstat has a mailing list on its own.
>
>The answer is below.
>--
>Edzer
>
>
>Leaf wrote:
>
>Dear All,
>
>Is there anybody has the experience in using variogram(gstat) ? Please kindly give me some hints about the results.
>
>
>I used variogram() to build a semivariogram plot as:
>
>tr.var=variogram(Incr~1,loc=~X+Y,data=TRI2TU,width=5)
>
>then fir the variogram to get the parameters as:
>
> v.fit = fit.variogram(tr.var,vgm(0.5,"Exp",300,1))
>
>v.fit
>  model    psill    range
>1   Nug 1.484879  0.00000
>2   Exp 3.476700 29.70914
>
>This is the output of v.fit. Can anybody help me write the exponential formula for this variogram?  I have the problem in understanding the result.
>
>BTW
>
>The equation you're looking for is:
>
>if h = 0, gamma(h) = 0
>if h > 0, gamma(h) = 1.484879 + 3.4767 (1 - exp(-h/29.70914))
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =



From MikeJones at westat.com  Wed Nov  9 18:44:23 2005
From: MikeJones at westat.com (Mike Jones)
Date: Wed, 9 Nov 2005 12:44:23 -0500
Subject: [R] elements in a matrix to a vector
Message-ID: <403593359CA56C4CAE1F8F4F00DCFE7D02722D4B@MAILBE2.westat.com>

my apologies to everyone taking the time to answer this question.  i
didn't explain myself very well.  

i have the indices of the elements of the matrix and the matrix won't
necessarily have zeros everywhere else.  is there a way to use the
indices to strip the elements i need?

thanks again...mj

-----Original Message-----
From: Dave Roberts [mailto:droberts at montana.edu] 
Sent: Wednesday, November 09, 2005 12:28 PM
To: Mike Jones
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] elements in a matrix to a vector


Mike,

    It's not clear whaty way you are doing it now, but this works

 > x <- matrix(c(0,2,0,0,0,4,3,0,0),nrow=3)
 > x
      [,1] [,2] [,3]
[1,]    0    0    3
[2,]    2    0    0
[3,]    0    4    0
 > y <- as.vector(t(x))
 > z <- y[y!=0]
 > z
[1] 3 2 4


Dave



Mike Jones wrote:
> hi all,
> 
> i'm trying to get elements in a matrix into a vector.  i need a 
> "streamlined" way to do it as the way i'm doing it is not very 
> serviceable.  an example is a 3x3 matrix like
> 
> 0 0 3
> 2 0 0
> 0 4 0
> 
> to a vector like
> 
> 3 2 4
> 
> thanks...mj
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From v.gomezrubio at imperial.ac.uk  Wed Nov  9 19:00:58 2005
From: v.gomezrubio at imperial.ac.uk (=?ISO-8859-1?Q?=22G=F3mez_Rubio=2C_Virgilio=22?=)
Date: Wed, 09 Nov 2005 18:00:58 +0000
Subject: [R] Small Area Estimation with R
Message-ID: <4372395A.9090203@imperial.ac.uk>

Dear R users,

First of all, excuse for the cross posting. I am working on Small Area 
Estimation and I was
wondering if someone knows about software to compute GREG, syntehtic and/or
(E)BLUP estimators with R.  I know that it should be posible to compute 
those
estimators using package nlme (for example), but I  thought that perhaps 
someone had already
written  some code to include spatial and/or spatio-temporal random effects.

Thanks in advance. Best regards,

Virgilio



From rcmcll at yahoo.com  Wed Nov  9 19:18:24 2005
From: rcmcll at yahoo.com (bob mccall)
Date: Wed, 9 Nov 2005 10:18:24 -0800 (PST)
Subject: [R] frequency() source code
In-Reply-To: <Pine.LNX.4.61.0511081715400.13847@gannet.stats>
Message-ID: <20051109181825.53310.qmail@web60015.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/19b30228/attachment.pl

From Michael.Gates at barclaysglobal.com  Wed Nov  9 20:26:13 2005
From: Michael.Gates at barclaysglobal.com (Gates, Michael BGI SF)
Date: Wed, 9 Nov 2005 11:26:13 -0800
Subject: [R] Element-by-element multiplication operator?
Message-ID: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>

Is there an element-by-element multiplication in R, like the .* operator in Matlab?

eg: A (2x3)
     B (2x3) 
C=A.*B
C (2x3)
C = [[a11*b11  a12*b12  a13*b13]; [a21*b21  a22*b22  a23*b23]]

I can't find one...

Thanks

-Mike Gates



From ray at mcs.vuw.ac.nz  Wed Nov  9 20:33:31 2005
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Thu, 10 Nov 2005 08:33:31 +1300 (NZDT)
Subject: [R] elements in a matrix to a vector
Message-ID: <200511091933.jA9JXVIx016664@tahi.mcs.vuw.ac.nz>

> my apologies to everyone taking the time to answer this question.  i
> didn't explain myself very well.  
> 
> i have the indices of the elements of the matrix and the matrix won't
> necessarily have zeros everywhere else.  is there a way to use the
> indices to strip the elements i need?
> 
RTFM ?"[".

> x <- matrix(c(0, 2, 0, 0, 0, 4, 3, 0, 0), nrow=3)
> x
     [,1] [,2] [,3]
[1,]    0    0    3
[2,]    2    0    0
[3,]    0    4    0
> ?"["
> xx <- matrix(c(1, 2, 3, 3, 1, 2), ncol=2)
> xx
     [,1] [,2]
[1,]    1    3
[2,]    2    1
[3,]    3    2
> x[xx]
[1] 3 2 4
>                                       

HTH,
Ray



From ggrothendieck at gmail.com  Wed Nov  9 20:56:17 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 9 Nov 2005 14:56:17 -0500
Subject: [R] short suggestion to the R development team
In-Reply-To: <437222B2.2090508@mailblocks.com>
References: <437222B2.2090508@mailblocks.com>
Message-ID: <971536df0511091156u4f515cefs57cbb9f4c9c31c21@mail.gmail.com>

In R 2.2.0 (it might be different in other versions) just put this
in your source file:

this.file <- parent.frame(2)$ofile

(If you place it into a function in your sourced file then will need
to increase the number 2 to take into account of the extra call.)

On 11/9/05, ivo welch <ivo_welch at mailblocks.com> wrote:
>
> Dear R developpment team:  would it be easy to introduce a global variable (in options()) that contains the name of the R file that is currently being processed?  (ala \jobname in latex.)  if this is difficult, would it be easy to introduce something like "$0" in perl (i.e., argv[0] in C), which contains the current R file?  I just checked what perl does interactively, when no file is being processed.  It fills $0 with "-", though it would seem to me that just "" would be more appropriate.
>
> sincerely,
>
> /iaw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed Nov  9 20:58:02 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 9 Nov 2005 14:58:02 -0500
Subject: [R] Element-by-element multiplication operator?
In-Reply-To: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
References: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
Message-ID: <971536df0511091158q2fa63404i1724fd67e1b8a5bb@mail.gmail.com>

See:

http://cran.r-project.org/doc/contrib/R-and-octave-2.txt


On 11/9/05, Gates, Michael BGI SF <Michael.Gates at barclaysglobal.com> wrote:
> Is there an element-by-element multiplication in R, like the .* operator in Matlab?
>
> eg: A (2x3)
>     B (2x3)
> C=A.*B
> C (2x3)
> C = [[a11*b11  a12*b12  a13*b13]; [a21*b21  a22*b22  a23*b23]]
>
> I can't find one...
>
> Thanks
>
> -Mike Gates
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From drf5n at maplepark.com  Wed Nov  9 20:59:55 2005
From: drf5n at maplepark.com (David Forrest)
Date: Wed, 9 Nov 2005 13:59:55 -0600 (CST)
Subject: [R] Element-by-element multiplication operator?
In-Reply-To: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
References: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
Message-ID: <Pine.LNX.4.58.0511091359041.1333@maplepark.com>

On Wed, 9 Nov 2005, Gates, Michael BGI SF wrote:

> Is there an element-by-element multiplication in R, like the .* operator in Matlab?
>
> eg: A (2x3)
>      B (2x3)
> C=A.*B
> C (2x3)
> C = [[a11*b11  a12*b12  a13*b13]; [a21*b21  a22*b22  a23*b23]]
>
> I can't find one...

It is the default:

matrix(1:6,nrow=2) * cbind(1:2,1:2,1:2)
     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    4    8   12

>
> Thanks
>
> -Mike Gates
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From murdoch at stats.uwo.ca  Wed Nov  9 21:01:38 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 09 Nov 2005 15:01:38 -0500
Subject: [R] Element-by-element multiplication operator?
In-Reply-To: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
References: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
Message-ID: <437255A2.8020202@stats.uwo.ca>

On 11/9/2005 2:26 PM, Gates, Michael BGI SF wrote:
> Is there an element-by-element multiplication in R, like the .* operator in Matlab?
> 
> eg: A (2x3)
>      B (2x3) 
> C=A.*B
> C (2x3)
> C = [[a11*b11  a12*b12  a13*b13]; [a21*b21  a22*b22  a23*b23]]
> 
> I can't find one...

Just use "*".  If you want matrix multiplication, you'd use %*%.

Duncan Murdoch



From h.wickham at gmail.com  Wed Nov  9 21:06:37 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 9 Nov 2005 14:06:37 -0600
Subject: [R] Element-by-element multiplication operator?
In-Reply-To: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
References: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
Message-ID: <f8e6ff050511091206u6a17dae8o2734278ca63e9bb3@mail.gmail.com>

> Is there an element-by-element multiplication in R, like the .* operator in Matlab?

How about *?

a <- matrix(1:4, 2, 2)
b <- matrix(4:7, 2, 2,)
a * b

Hadley



From tobias.verbeke at telenet.be  Wed Nov  9 21:07:39 2005
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Wed, 09 Nov 2005 21:07:39 +0100
Subject: [R] Element-by-element multiplication operator?
In-Reply-To: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
References: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
Message-ID: <4372570B.8010101@telenet.be>

Gates, Michael BGI SF wrote:

>Is there an element-by-element multiplication in R, like the .* operator in Matlab?
>
>eg: A (2x3)
>     B (2x3) 
>C=A.*B
>C (2x3)
>C = [[a11*b11  a12*b12  a13*b13]; [a21*b21  a22*b22  a23*b23]]
>
>I can't find one...
>  
>
mym <- matrix(1:4,2)
myt <- matrix(5:8,2)

 > mym
     [,1] [,2]
[1,]    1    3
[2,]    2    4
 > myt
     [,1] [,2]
[1,]    5    7
[2,]    6    8
 > mym * myt
     [,1] [,2]
[1,]    5   21
[2,]   12   32

HTH,
Tobias



From p.dalgaard at biostat.ku.dk  Wed Nov  9 21:23:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Nov 2005 21:23:46 +0100
Subject: [R] Element-by-element multiplication operator?
In-Reply-To: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
References: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
Message-ID: <x2vez1in8t.fsf@turmalin.kubism.ku.dk>

"Gates, Michael BGI SF" <Michael.Gates at barclaysglobal.com> writes:

> Is there an element-by-element multiplication in R, like the .* operator in Matlab?
> 
> eg: A (2x3)
>      B (2x3) 
> C=A.*B
> C (2x3)
> C = [[a11*b11  a12*b12  a13*b13]; [a21*b21  a22*b22  a23*b23]]
> 
> I can't find one...

It's * , plain and simple. The _other_ one is %*%.
 
> Thanks
> 
> -Mike Gates
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From davidr at rhotrading.com  Wed Nov  9 21:24:16 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 9 Nov 2005 14:24:16 -0600
Subject: [R] Element-by-element multiplication operator?
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A7CEAB2@rhosvr02.rhotrading.com>

A*B

David L. Reiner
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Gates, Michael BGI SF
> Sent: Wednesday, November 09, 2005 1:26 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Element-by-element multiplication operator?
> 
> Is there an element-by-element multiplication in R, like the .*
operator
> in Matlab?
> 
> eg: A (2x3)
>      B (2x3)
> C=A.*B
> C (2x3)
> C = [[a11*b11  a12*b12  a13*b13]; [a21*b21  a22*b22  a23*b23]]
> 
> I can't find one...
> 
> Thanks
> 
> -Mike Gates
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From rpeng at jhsph.edu  Wed Nov  9 21:35:09 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 09 Nov 2005 15:35:09 -0500
Subject: [R] Element-by-element multiplication operator?
In-Reply-To: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
References: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
Message-ID: <43725D7D.3070104@jhsph.edu>

What about A * B?

-roger

Gates, Michael BGI SF wrote:
> Is there an element-by-element multiplication in R, like the .* operator in Matlab?
> 
> eg: A (2x3)
>      B (2x3) 
> C=A.*B
> C (2x3)
> C = [[a11*b11  a12*b12  a13*b13]; [a21*b21  a22*b22  a23*b23]]
> 
> I can't find one...
> 
> Thanks
> 
> -Mike Gates
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/



From oliver at bic.mni.mcgill.ca  Wed Nov  9 21:48:40 2005
From: oliver at bic.mni.mcgill.ca (Oliver Lyttelton)
Date: Wed, 9 Nov 2005 15:48:40 -0500
Subject: [R] Order of terms in a model specification...
Message-ID: <HJEOLCCIEHBKJJLPOEBFOEHACCAA.oliver@bic.mni.mcgill.ca>




Hi,

Sorry for this one as its pretty basic but I've taken a look for info and
couldn't find any...

My question is, does the order of main effect terms in a model specification
have any impact on the model R fits or not. (in particular when using lm).
ie

Can A~X+Y+Z lead to different results to A~Z+Y+X, and if so in what
circumstances, and how much should I worry about it?

I believe this is an implementation detail as it depends on the way the
fitting algorithm works, but it would be great to have a few lines to plug
this gap in my knowledge...

Many thanks,

Oliver



From illyese at freemail.hu  Wed Nov  9 21:55:44 2005
From: illyese at freemail.hu (Illyes Eszter)
Date: Wed, 9 Nov 2005 21:55:44 +0100 (CET)
Subject: [R] how to convert  strings back to values?
Message-ID: <freemail.20051009215544.81277@fm08.freemail.hu>

Dear All, 

It's Eszter from Hungary, a total beginner with R. My problem is the 
following: 

I have a dataset with binary values as a comma separated textfile. The 
samples are in the coloumns and the species are in the rows. 

I have to transpose it for the further PCoA analysis. There is no 
problem with reading the dataset. 

When I transpose the dataset, the original values become strings 
(instead of 0,1,0,0,1 I have "0","1","0","0","1"). The distance matrix 
cannot be counted from the transposed dataset, I have 2 error 
messages: 

<Warning in vegdist(tdf1, method = "jaccard", binary = FALSE, diag = 
FALSE,  : results may be meaningless because input data have 
negative entries>

<Error in rowSums(x, prod(dn), p, na.rm) : 'x' must be numeric>

I do not understand the first, since I have only 1 and 0 in the dataset. I 
guess I have the second because of the strings instead of values in the 
dataset. 

Could you please help me solving these problems? I could not find 
anything about these in the manuals. 

Thank you, cheers:

Eszter

p.s. This is a new problem, last week I worked with a similar dataset 
and I did not get any error message like these. 
 


_____________________________________________________________________
Men?? cseng??hangok (MP3 is!) ??s sz??nes k??pek a mobilodra. 
N??lunk szinte mindent megtal??lsz, KLIKK IDE! www.7777oplogo.hu



From statsfay at hotmail.com  Wed Nov  9 21:56:31 2005
From: statsfay at hotmail.com (Gao Fay)
Date: Wed, 09 Nov 2005 15:56:31 -0500
Subject: [R] How to find statistics like that.
Message-ID: <BAY104-F11BD7F7D981E77A1D264C8D0670@phx.gbl>

Hi there,

Suppose mu is constant, and error is normally distributed with mean 0 and 
fixed variance s. I need to find a statistics that:
Y_i = mu + beta1* I1_i beta2*I2_i + beta3*I1_i*I2_i + +error, where I_i is 
1 Y_i is from group A, and 0 if Y_i is from group B.

It is large when  beta1=beta2=0
It is small when beta1 and/or beta2 is not equal to 0

How can I find it by R? Thank you very much for your time.
Fay



From mbmiller at taxa.epi.umn.edu  Wed Nov  9 22:20:05 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Wed, 9 Nov 2005 15:20:05 -0600 (CST)
Subject: [R] R-and-octave (was "Element-by-element multiplication operator?")
In-Reply-To: <971536df0511091158q2fa63404i1724fd67e1b8a5bb@mail.gmail.com>
References: <551D17AE9FCFB245B067E06916A5357C0247813A@calnte2k034.insidelive.net>
	<971536df0511091158q2fa63404i1724fd67e1b8a5bb@mail.gmail.com>
Message-ID: <Pine.GSO.4.60.0511091515550.7828@taxa.epi.umn.edu>

On Wed, 9 Nov 2005, Gabor Grothendieck wrote:

> See:
>
> http://cran.r-project.org/doc/contrib/R-and-octave-2.txt


That is a really beautiful page.  If the author is willing to update it to 
include some of the new Octave functionality -- specifically N-dimensional 
arrays -- I'm sure I can help out.  If there is new R functionality to 
include, I probably wouldn't know about it (I'm still learning R) but 
maybe someone else on the list would know.

Mike



From dingjia at gmail.com  Wed Nov  9 22:23:53 2005
From: dingjia at gmail.com (jia ding)
Date: Wed, 9 Nov 2005 22:23:53 +0100
Subject: [R] library(lattice)
Message-ID: <91ae6e350511091323u192fda78n3c53b5ecb7c4a9e4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/ccfd13c5/attachment.pl

From dingjia at gmail.com  Wed Nov  9 22:35:45 2005
From: dingjia at gmail.com (jia ding)
Date: Wed, 9 Nov 2005 22:35:45 +0100
Subject: [R] accident modified dataset. How can I recovery it?!
Message-ID: <91ae6e350511091335m4bac64c9rd7b2de441f6958dc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/f5abcf27/attachment.pl

From toby.m at mail.utexas.edu  Wed Nov  9 22:44:46 2005
From: toby.m at mail.utexas.edu (Toby Muhlhofer)
Date: Wed, 09 Nov 2005 15:44:46 -0600
Subject: [R] Compiling R on Sparc-Solaris-2.9
Message-ID: <43726DCE.5020107@mail.utexas.edu>

Hi, all!

I'm trying to compile R on a Solaris machine. The default C compiler is 
cc (although gcc is available) and the default Fortran compiler is f95 
(although g77 is available).

Without defining the F77 environment variable, configure defaults to f95 
as a Fortran compiler and eventually fails with the following output:

---------------------------------
checking whether mixed C/Fortran code can be run... configure: WARNING: 
cannot run mixed C/Fortan code
configure: error: Maybe check LDFLAGS for paths to Fortran libraries?
---------------------------------

Setting LDFLAGS to the path where the Fortran libraries sit makes the C 
compiler complain.

If I give the value g77 to F77, there are two interesting issues:

1)
-------------------------
defining F77 to be g77
checking whether we are using the GNU Fortran 77 compiler... no
checking whether g77 accepts -g... yes
-------------------------

Why does configure think we are not using the GNU Fortran 77 compiler?

But more importantly

2)
-------------------------------------
checking how to get verbose linking output from g77... configure: 
WARNING: compilation failed

checking for Fortran libraries of g77...
checking how to get verbose linking output from cc... -###
checking for C libraries of cc...  -L/usr/local/lib -lthread
checking for dummy main to link with Fortran libraries... none
checking for Fortran name-mangling scheme... configure: error: cannot 
compile a simple Fortran program
-------------------------------------

I tried to compile a simple "Hello World" program with either compiler 
and both work without a problem.

Any ideas?

Toby



From dingjia at gmail.com  Wed Nov  9 22:46:54 2005
From: dingjia at gmail.com (jia ding)
Date: Wed, 9 Nov 2005 22:46:54 +0100
Subject: [R] accident modified dataset. How can I recovery it?!
In-Reply-To: <91ae6e350511091335m4bac64c9rd7b2de441f6958dc@mail.gmail.com>
References: <91ae6e350511091335m4bac64c9rd7b2de441f6958dc@mail.gmail.com>
Message-ID: <91ae6e350511091346l3bd5b226j1c6f96dc3079dd9a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051109/3d607697/attachment.pl

From murdoch at stats.uwo.ca  Wed Nov  9 23:55:16 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 09 Nov 2005 17:55:16 -0500
Subject: [R] Order of terms in a model specification...
In-Reply-To: <HJEOLCCIEHBKJJLPOEBFOEHACCAA.oliver@bic.mni.mcgill.ca>
References: <HJEOLCCIEHBKJJLPOEBFOEHACCAA.oliver@bic.mni.mcgill.ca>
Message-ID: <43727E54.2000204@stats.uwo.ca>

On 11/9/2005 3:48 PM, Oliver Lyttelton wrote:
> 
> 
> Hi,
> 
> Sorry for this one as its pretty basic but I've taken a look for info and
> couldn't find any...
> 
> My question is, does the order of main effect terms in a model specification
> have any impact on the model R fits or not. (in particular when using lm).
> ie
> 
> Can A~X+Y+Z lead to different results to A~Z+Y+X, and if so in what
> circumstances, and how much should I worry about it?
> 
> I believe this is an implementation detail as it depends on the way the
> fitting algorithm works, but it would be great to have a few lines to plug
> this gap in my knowledge...

Definitely yes, in the case of collinear terms.  For example,

 > X <- rnorm(10)
 > Y <- rnorm(10)
 > Z <- X
 > A <- rnorm(10)
 > lm(A ~ X+Y+Z)

Call:
lm(formula = A ~ X + Y + Z)

Coefficients:
(Intercept)            X            Y            Z
     -0.3474      -0.1166      -0.2203           NA

 > lm(A ~ Z+Y+X)

Call:
lm(formula = A ~ Z + Y + X)

Coefficients:
(Intercept)            Z            Y            X
     -0.3474      -0.1166      -0.2203           NA


In one case X gets a coefficient and Z doesn't, but the other is the 
opposite.

I suspect there would be differences due to rounding in other 
situations, and they might be noticeable in the case of near-collinearity.

Duncan Murdoch



From mbmiller at taxa.epi.umn.edu  Thu Nov 10 00:09:45 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Wed, 9 Nov 2005 17:09:45 -0600 (CST)
Subject: [R] How to find statistics like that.
In-Reply-To: <BAY104-F11BD7F7D981E77A1D264C8D0670@phx.gbl>
References: <BAY104-F11BD7F7D981E77A1D264C8D0670@phx.gbl>
Message-ID: <Pine.GSO.4.60.0511091707290.2930@taxa.epi.umn.edu>

On Wed, 9 Nov 2005, Gao Fay wrote:

> Hi there,
>
> Suppose mu is constant, and error is normally distributed with mean 0 and 
> fixed variance s. I need to find a statistics that:
> Y_i = mu + beta1* I1_i beta2*I2_i + beta3*I1_i*I2_i + +error, where I_i is 1 
> Y_i is from group A, and 0 if Y_i is from group B.
>
> It is large when  beta1=beta2=0
> It is small when beta1 and/or beta2 is not equal to 0
>
> How can I find it by R? Thank you very much for your time.


That's a funny question.  Usually we want a statistic that is small when 
beta1=beta2=0 and large otherwise.

Why not compute the usual F statistic for the null beta1=beta2=0 and then 
use 1/F as your statistic?

Mike



From wzhao6898 at gmail.com  Thu Nov 10 00:11:12 2005
From: wzhao6898 at gmail.com (David Zhao)
Date: Wed, 9 Nov 2005 15:11:12 -0800
Subject: [R] help with legacy R code
Message-ID: <4e6115a50511091511o511ad49crd84908ee6fa09da2@mail.gmail.com>

Hi there,


Could somebody help me disect this legacy R script I inherited at work, I
have two questions:
1. I've tried to upgrade our R version from 1.6.2 (yeah, I know), to R 2.0,
but some of the lines in this script are not compatible with R 2.0, could
someone help me figure out where the problem is?
2. the jpeg generated (attached) seems to be off on some of the data, is
there a better way of doing this.

Thanks very much in advance!

David


 library(MASS)
 jpeg(filename = "diswrong.jpg", width = 800, height = 600, pointsize = 12,
quality = 75, bg = "white")

 myfunc <- function(x, mean, sd, nfalse, ntotal, shape, rate) {
 (nfalse*dgamma(x,shape,rate)+(ntotal-nfalse)*dnorm(x,mean,sd))/ntotal
 }

 wrong <- scan("wrongrawdata.txt", list(x=0))
 wrongfit <- fitdistr(wrong$x, "gamma")
 wrongmean <- mean(wrong$x)
 wrongshape <- wrongfit[[1]][1]
 wrongrate <- wrongfit[[1]][2]

 good <- scan("rawdata.txt", list(x=0))
 xmin = 0
 newx = good$x
 xmean = mean(newx)


 xmax = max(newx)+0.15
 goodhist <- hist(newx, br=seq(from=0,to=xmax,by=0.15), probability=T,
col="lightyellow")

 initmean <- (min(newx)+max(newx))/2
 totalx <- length(newx)

 wrongmeanshift <- wrongmean + 0.2
 wrongper <- pgamma(wrongmeanshift, wrongshape, wrongrate)
 nfalseundermean <-
which(abs(newx-wrongmeanshift)==min(abs(newx-wrongmeanshift)))
 initnfalse <- nfalseundermean / wrongper

 fitmean <- -1
 fitsd <- 0
 fitnfalse <- initnfalse
 fitshape <- wrongshape
 fitrate <- wrongrate

 curve((fitnfalse*dgamma(x,fitshape,fitrate))/totalx, add=T, col="red",
lwd=2)

 breaksllength <- length(goodhist$breaks)
 endi = breaksllength - 1
 binprob = c(1)
 for (i in 1:endi) {
 expnegative <- fitnfalse * (pgamma(goodhist$breaks[i+1],wrongshape,
wrongrate)-pgamma(goodhist$breaks[i],wrongshape, wrongrate))
 if (goodhist$counts[i] == 0)
 binprob[i] = 0
 else
 binprob[i] = (goodhist$counts[i] - expnegative) / goodhist$counts[i]
 }

 result = data.frame(newx)
 prob = c(1)
 for (i in 1:totalx) {
 bini = which ((goodhist$breaks < newx[i]) & (goodhist$breaks > newx[i]-0.15
))
 if ((binprob[bini] < 0.8) | (newx[i] < wrongmean))
 prob[i] = -1
 else
 prob[i] = binprob[bini]*100
 }

 result = data.frame(result, prob)
 write.table(result, file="probwrong.txt", sep=" ", row.name=F, col.name=F)
 fitpars = c(fitmean, fitsd, fitnfalse, fitshape, fitrate, totalx)
 result = data.frame(fitpars)
 write.table(result,file="parwrong.txt", sep=" ", row.name=F, col.name=F)
 dev.off()

From gerifalte28 at hotmail.com  Thu Nov 10 00:14:18 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 09 Nov 2005 23:14:18 +0000
Subject: [R] how to convert strings back to values?
In-Reply-To: <freemail.20051009215544.81277@fm08.freemail.hu>
Message-ID: <BAY103-F293C80E61C6939D6D7F07CA6670@phx.gbl>

Examples of the code you used would have helped i.e. We don't know how you 
transposed your matrix.  Did you use t()?  In any event, as.integer() may be 
what you need.

Francisco


>From: Illyes Eszter <illyese at freemail.hu>
>To: r-help at stat.math.ethz.ch
>Subject: [R] how to convert  strings back to values?
>Date: Wed, 9 Nov 2005 21:55:44 +0100 (CET)
>
>Dear All,
>
>It's Eszter from Hungary, a total beginner with R. My problem is the
>following:
>
>I have a dataset with binary values as a comma separated textfile. The
>samples are in the coloumns and the species are in the rows.
>
>I have to transpose it for the further PCoA analysis. There is no
>problem with reading the dataset.
>
>When I transpose the dataset, the original values become strings
>(instead of 0,1,0,0,1 I have "0","1","0","0","1"). The distance matrix
>cannot be counted from the transposed dataset, I have 2 error
>messages:
>
><Warning in vegdist(tdf1, method = "jaccard", binary = FALSE, diag =
>FALSE,  : results may be meaningless because input data have
>negative entries>
>
><Error in rowSums(x, prod(dn), p, na.rm) : 'x' must be numeric>
>
>I do not understand the first, since I have only 1 and 0 in the dataset. I
>guess I have the second because of the strings instead of values in the
>dataset.
>
>Could you please help me solving these problems? I could not find
>anything about these in the manuals.
>
>Thank you, cheers:
>
>Eszter
>
>p.s. This is a new problem, last week I worked with a similar dataset
>and I did not get any error message like these.
>
>
>
>_____________________________________________________________________
>Menõ csengõhangok (MP3 is!) és színes képek a mobilodra.
>Nálunk szinte mindent megtalálsz, KLIKK IDE! www.7777oplogo.hu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From hb at maths.lth.se  Thu Nov 10 00:15:26 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 10 Nov 2005 10:15:26 +1100
Subject: [R] read.table error with R 2.2.0
In-Reply-To: <73dae3060511090757x9587d45mff9193cd2f18b93@mail.gmail.com>
References: <73dae3060511090707s706cf91bp80161533bbfffd03@mail.gmail.com>	<43721954.8000209@stats.uwo.ca>
	<73dae3060511090757x9587d45mff9193cd2f18b93@mail.gmail.com>
Message-ID: <4372830E.8020806@maths.lth.se>

Florence Combes wrote:

>Thanks a lot for your answer.
>In fact I found the solution, it's seems strange to me so I put it here if
>it could bu useful for other people ...
>
>I have the same as you
>  
>
>>getAnywhere("read.table")$where
>>    
>>
>[1] "package:base" "namespace:base"
>  
>
>>getAnywhere("read.table.default")$where
>>    
>>
>character(0)
>
>when I run it in a "native" R console, and the line :
>param<-read.table(file="param.dat",sep ="\t",header=TRUE,fill=TRUE,
>na.strings="NA")
>
>works very well. (I didn't test this before).
>
>
>BUT when I load the aroma package (which I need for what I want to do), then
>I have :
>  
>
>>getAnywhere("read.table")$where
>>    
>>
>[1] "package:aroma"
>[2] "package:base"
>[3] "registered S3 method for read from namespace base"
>[4] "namespace:base"
>  
>
>>getAnywhere("read.table.default")$where
>>    
>>
>[1] "package:aroma" "registered S3 method for read"
>
>and the "read.table" didn't work.
>
>So I reinstall the aroma package (even if I had the latest version) and it
>works well now).
>
>best regards,
>
>Florence.
>
>  
>
Hi, author of aroma here.  This was fixed a few months ago.  From 
showHistory(aroma):
Version: 0.84 [2005-07-01]
...
o BUG FIX: GenePixData$read() would give "Error in read.table.default(...
  ...): 5 arguments passed to 'readTableHead' which requires 6".

What I have done/did is that I created a read.table.QuantArrayData() 
function, rename base::read.table() to read.table.default() and made 
read.table() generic.  This should make things rather backward 
compatible.  When I looked at my source code history, the reason for 
this was:

# 2002-08-18
# o Since the 'Measurements' section in QuantArray files seems to contain
#   rows with tailing TAB's (that just should be ignored) read.table() fails
#   to read them. read.table() is making use of scan() and scan() has the
#   argument 'flush' which flushes such trailing cells, but it is not used
#   by read.table(). For this reason I created the static read.table()
#   method of QuantArrrayData which has the 'flush' argument.

I other words, I just added the argument 'flush=FALSE' to 
read.table[.QuantArrayData]() and passes 'flush=flush' to its internal 
calls to scan().  I'll send R-devel a note and see if it is possible to 
add this argument to the default read.table().  However, everything 
should work correctly as it is now.

Cheers

Henrik

>
>
>
>
>On 11/9/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>  
>
>>On 11/9/2005 10:07 AM, Florence Combes wrote:
>>    
>>
>>>Dear all,
>>>
>>>I just upgraded version of R to R 2.2.0, and I have a problem with a
>>>      
>>>
>>script
>>    
>>
>>>that did not happen with my previous version.
>>>Here is the error :
>>>
>>>-----------------------------------------
>>>      
>>>
>>>>param<-read.table(file="param.dat",sep ="\t",header=TRUE,fill=TRUE,
>>>>        
>>>>
>>>na.strings="NA")
>>>Erreur dans read.table.default(file = "param.dat", sep = "\t", header =
>>>TRUE, :
>>>5 arguments passed to 'readTableHead' which requires 6
>>>-----------------------------------------
>>>
>>>whereas all was OK before. I cannot understand what's happening.
>>>
>>>Has someone already encountered this ??
>>>Any help greatly appreciated,
>>>      
>>>
>>There is no "read.table.default" in standard R 2.2.0, so it appears that
>>you have installed a replacement for read.table, and it no longer works.
>>If you type
>>
>>getAnywhere("read.table")$where
>>
>>and
>>
>>getAnywhere("read.table.default")$where
>>
>>you are likely to see where those functions came from. (I see
>>
>>    
>>
>>>getAnywhere("read.table")$where
>>>      
>>>
>>[1] "package:base" "namespace:base"
>>
>>    
>>
>>>getAnywhere("read.table.default")$where
>>>      
>>>
>>character(0)
>>
>>indicating that read.table comes from the base package, and
>>read.table.default doesn't exist.
>>
>>Duncan Murdoch
>>
>>    
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From Duncan.Mackay at flinders.edu.au  Thu Nov 10 00:16:14 2005
From: Duncan.Mackay at flinders.edu.au (Duncan Mackay)
Date: Thu, 10 Nov 2005 09:46:14 +1030
Subject: [R] [Rd] bug in windows GUI/script editor (PR#8288)
Message-ID: <000401c5e583$af873f40$f4e66081@duncanlt>

Here's how I can reproduce this bug, running under MDI under WinXP

1) start Rgui
2) open a script window using File>New script
3) click back in the console window and open a help window (e.g. by typing
"?merge") in the console window
4) click in the close box of the help window
5) click in the close box of the script window (which was visible even
though the script window was largely behind the console window)
6) click on the File menu ..........CRASH!!! "R for Windows GUI front-end
has encountered a problem and needs to close.  We are sorry for the
inconvenience."


Cheers,
Duncan



> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.0            
year     2005           
month    10             
day      06             
svn rev  35749          
language R  


*****************************************
Dr. Duncan Mackay
School of Biological Sciences
Flinders University
GPO Box 2100
Adelaide
S.A.    5001
AUSTRALIA

Ph (08) 8201 2627    FAX (08) 8201 3015

http://www.scieng.flinders.edu.au/biology/people/mackay_d/index.html



From skougeo at iit.edu  Thu Nov 10 00:50:59 2005
From: skougeo at iit.edu (George Skountrianos)
Date: Wed, 09 Nov 2005 17:50:59 -0600
Subject: [R] basic mac question
Message-ID: <10df02110e1de7.10e1de710df021@iit.edu>

ok i got r 2.2.0 to work on my mac just fine but im having problems 
with when i try to create a new data set. when i go to new data...new 
data set it opens up the window. so i name my set but when i click ok 
the window does not go away. is this normal. also the data editor will 
not allow me to name my variables. for example i click on the create 
new variable columb and row button and i get var1, var2  and then n/a 
n/a. ok well im able to put numbers into the n/a slots but i cannot 
change the var1, var2, ect names. any suggestions. ooo i have tiger 
10.4. thanks



From droberts at montana.edu  Thu Nov 10 01:04:38 2005
From: droberts at montana.edu (Dave Roberts)
Date: Wed, 09 Nov 2005 17:04:38 -0700
Subject: [R] how to convert strings back to values?
In-Reply-To: <BAY103-F293C80E61C6939D6D7F07CA6670@phx.gbl>
References: <BAY103-F293C80E61C6939D6D7F07CA6670@phx.gbl>
Message-ID: <43728E96.3030502@montana.edu>

Eszter,

I suspect the problem is different than you think.  It's possible that 
when you read in the data it assumed that the first column was data, not 
row names, and so when you transpose the first column becomes the first 
row. Since it is alpha, all the columns become factors rather then 
integers.  Check to see on the non-transposed matrix wheter the number 
of rows and columns is correct.  If it incorrectly treated the first 
column as data (as it would if you have the same number of column 
headings as columns), you could read it back in with

 > x <- read.table('file_name',header=TRUE, row.names=1)

and then transpose it

 > y <- t(x)

Often at that point the row and column names are stripped off, and you 
have to put them back.

 > y <- data.frame(y)
 > row.names(y) <- names(x)
 > names(y) <- row.names(x)

I'm interested in PCoA in ecology, so let me know how it goes.

HTH Dave

Francisco J. Zagmutt wrote:
> Examples of the code you used would have helped i.e. We don't know how 
> you transposed your matrix.  Did you use t()?  In any event, 
> as.integer() may be what you need.
> 
> Francisco
> 
> 
>> From: Illyes Eszter <illyese at freemail.hu>
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] how to convert  strings back to values?
>> Date: Wed, 9 Nov 2005 21:55:44 +0100 (CET)
>>
>> Dear All,
>>
>> It's Eszter from Hungary, a total beginner with R. My problem is the
>> following:
>>
>> I have a dataset with binary values as a comma separated textfile. The
>> samples are in the coloumns and the species are in the rows.
>>
>> I have to transpose it for the further PCoA analysis. There is no
>> problem with reading the dataset.
>>
>> When I transpose the dataset, the original values become strings
>> (instead of 0,1,0,0,1 I have "0","1","0","0","1"). The distance matrix
>> cannot be counted from the transposed dataset, I have 2 error
>> messages:
>>
>> <Warning in vegdist(tdf1, method = "jaccard", binary = FALSE, diag =
>> FALSE,  : results may be meaningless because input data have
>> negative entries>
>>
>> <Error in rowSums(x, prod(dn), p, na.rm) : 'x' must be numeric>
>>
>> I do not understand the first, since I have only 1 and 0 in the 
>> dataset. I
>> guess I have the second because of the strings instead of values in the
>> dataset.
>>
>> Could you please help me solving these problems? I could not find
>> anything about these in the manuals.
>>
>> Thank you, cheers:
>>
>> Eszter
>>
>> p.s. This is a new problem, last week I worked with a similar dataset
>> and I did not get any error message like these.
>>
>>
>>
>> _____________________________________________________________________
>> Men?? cseng??hangok (MP3 is!) ??s sz??nes k??pek a mobilodra.
>> N??lunk szinte mindent megtal??lsz, KLIKK IDE! www.7777oplogo.hu
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From Duncan.Mackay at flinders.edu.au  Thu Nov 10 01:48:29 2005
From: Duncan.Mackay at flinders.edu.au (Duncan Mackay)
Date: Thu, 10 Nov 2005 11:18:29 +1030
Subject: [R] [Rd] bug in windows GUI/script editor (PR#8288)
Message-ID: <000801c5e590$778392d0$f4e66081@duncanlt>

P.S.  I should have added that this crash occurred when the MDI toolbar was
OFF.

Here's how I can reproduce this bug, running under MDI under WinXP

1) start Rgui
2) open a script window using File>New script
3) click back in the console window and open a help window (e.g. by typing
"?merge") in the console window
4) click in the close box of the help window
5) click in the close box of the script window (which was visible even
though the script window was largely behind the console window)
6) click on the File menu ..........CRASH!!! "R for Windows GUI front-end
has encountered a problem and needs to close.  We are sorry for the
inconvenience."


Cheers,
Duncan



> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.0            
year     2005           
month    10             
day      06             
svn rev  35749          
language R  


*****************************************
Dr. Duncan Mackay
School of Biological Sciences
Flinders University
GPO Box 2100
Adelaide
S.A.    5001
AUSTRALIA

Ph (08) 8201 2627    FAX (08) 8201 3015

http://www.scieng.flinders.edu.au/biology/people/mackay_d/index.html



From rahmank at frim.gov.my  Thu Nov 10 02:06:09 2005
From: rahmank at frim.gov.my (Abd. Rahman Kassim)
Date: Thu, 10 Nov 2005 09:06:09 +0800
Subject: [R] polynomials transformation
References: <001601c5e5c0$cc7fdb60$2f01a8c0@rahmandt> 
	<4371C9C2.5090503@optonline.net>
Message-ID: <002001c5e592$efa6a930$2f01a8c0@rahmandt>


Dear Chuck,

Thanks for your prompt reply.

Abd. Rahman
----- Original Message ----- 
From: "Chuck Cleland" <ccleland at optonline.net>
To: "Abd. Rahman Kassim" <rahmank at frim.gov.my>
Cc: "R-Help Discussion" <r-help at stat.math.ethz.ch>
Sent: Wednesday, November 09, 2005 6:04 PM
Subject: Re: [R] polynomials transformation


> The coefficients are in the attribute list:
>
> > x <- rnorm(100)
> > attributes(poly(x, 2))$coefs
> $alpha
> [1] -0.1585783 -0.1193990
>
> $norm2
> [1]   1.0000 100.0000 110.3589 254.5965
>
> ?attributes
>
> Abd. Rahman Kassim wrote:
>> Dear All,
>>
>> Need some help in polynomials transformation to get the coefficients. I 
>> have tried "poly.transform" as applied in S-plus but it does not work.
>>
>> Thanks in advanced for any helps.
>>
>> Regards.
>>
>>
>> Abd. Rahman Kassim (PhD)
>> Head Forest Ecology Branch
>> Forest Management & Ecology Program
>> Forestry and Conservation Division
>> Forest Research Institute Malaysia
>> Kepong 52109
>> Selangor, Malaysia
>>
>> *****************************************
>>
>> Checked by TrendMicro Interscan Messaging Security.
>> For any enquiries, please contact FRIM IT Department.
>> *****************************************
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894 


*****************************************

Checked by TrendMicro Interscan Messaging Security. 
For any enquiries, please contact FRIM IT Department.



From Mike.Prager at noaa.gov  Thu Nov 10 02:33:06 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Wed, 09 Nov 2005 20:33:06 -0500
Subject: [R] solve the quadratic equation ax^2+bx+c=0
In-Reply-To: <91d269c60511051128r4b502115tdb0575a579df4e73@mail.gmail.com>
References: <91d269c60511051128r4b502115tdb0575a579df4e73@mail.gmail.com>
Message-ID: <4372A352.50800@noaa.gov>

The inquiry below and similar ones seem a lot like homework problems.  
They are distinguished by showing no evidence of the OP's ever having 
read a manual and also by being problems of apparently pedagogical 
interest. The Fortran group (comp.lang.fortran) has a long tradition of 
not giving more than the broadest of  hints in response to such 
inquiries, and I would encourage r-help readers to consider doing the same.

MHP


Yuying Shi wrote:

>If I have matrics as follows:
>  
>
>>a <- c(1,1,0,0)
>>b <- c(4,4,0,0)
>>c <- c(3,5,5,6)
>>    
>>
>How can I use R code to solve the equation ax^2+bx+c=0.
>thanks!
> yuying shi
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From jfox at mcmaster.ca  Thu Nov 10 02:54:46 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 9 Nov 2005 20:54:46 -0500
Subject: [R] basic mac question
In-Reply-To: <10df02110e1de7.10e1de710df021@iit.edu>
Message-ID: <20051110015443.SYOX21026.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear George,

I'm afraid that people won't understand the context of your question -- that
is, that you're using the Rcmdr GUI, but that the crux of your problem as do
with changing the names of variables in the data editor using R on a Mac. 

(As you know, I'm not a Mac user and unable to answer that question. I did
answer the first part of your question in our private email correspondence,
which is that the New Data Set window goes away only after the data editor
is closed.)

I hope that, with this clarification, a Mac R user will come to your
assistance.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of George 
> Skountrianos
> Sent: Wednesday, November 09, 2005 6:51 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] basic mac question
> 
> ok i got r 2.2.0 to work on my mac just fine but im having 
> problems with when i try to create a new data set. when i go 
> to new data...new data set it opens up the window. so i name 
> my set but when i click ok the window does not go away. is 
> this normal. also the data editor will not allow me to name 
> my variables. for example i click on the create new variable 
> columb and row button and i get var1, var2  and then n/a n/a. 
> ok well im able to put numbers into the n/a slots but i 
> cannot change the var1, var2, ect names. any suggestions. ooo 
> i have tiger 10.4. thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Thu Nov 10 03:33:16 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 09 Nov 2005 21:33:16 -0500
Subject: [R] [Rd]  bug in windows GUI/script editor (PR#8288)
In-Reply-To: <000401c5e583$af873f40$f4e66081@duncanlt>
References: <000401c5e583$af873f40$f4e66081@duncanlt>
Message-ID: <4372B16C.8030106@stats.uwo.ca>

On 11/9/2005 6:16 PM, Duncan Mackay wrote:
> Here's how I can reproduce this bug, running under MDI under WinXP
> 
> 1) start Rgui
> 2) open a script window using File>New script
> 3) click back in the console window and open a help window (e.g. by typing
> "?merge") in the console window
> 4) click in the close box of the help window
> 5) click in the close box of the script window (which was visible even
> though the script window was largely behind the console window)
> 6) click on the File menu ..........CRASH!!! "R for Windows GUI front-end
> has encountered a problem and needs to close.  We are sorry for the
> inconvenience."

Thanks, I can reproduce this.  I'll track it down.

Duncan Murdoch
> 
> 
> Cheers,
> Duncan
> 
> 
> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    2.0            
> year     2005           
> month    10             
> day      06             
> svn rev  35749          
> language R  
> 
> 
> *****************************************
> Dr. Duncan Mackay
> School of Biological Sciences
> Flinders University
> GPO Box 2100
> Adelaide
> S.A.    5001
> AUSTRALIA
> 
> Ph (08) 8201 2627    FAX (08) 8201 3015
> 
> http://www.scieng.flinders.edu.au/biology/people/mackay_d/index.html
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



From andy_liaw at merck.com  Thu Nov 10 03:53:08 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 9 Nov 2005 21:53:08 -0500
Subject: [R] Problem with Running Locfit
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED586@usctmx1106.merck.com>

Anil,

If you can send me some data (off-list), I can try to see what's tripping
you up.

Andy

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of anil 
> kumar rohilla
> Sent: Wednesday, November 09, 2005 3:54 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Problem with Running Locfit
> 
> 
>   
>   Dear R users,
>   i am using locfit package developed by loader in R 
> software, my problem is that as i am doing independont 
> forecast using locfit object , i am able to do independont 
> forecast for more than one years simultaniously. But when i 
> am doing one year forecast(single) this code is giving 
> following error...
> "Warning message:
> 'newdata' had 1 rows but variable(s) found have 24 rows "
> 
> here is the code i am using for locfit.........
> 
> xt<-read.table("x.dat",header=FALSE)
> yt<-read.table("y.dat",header=FALSE)
> tst<-read.table("test.dat",header=FALSE)
> 
> 
> fitt<-locfit.raw(sapply(xt,"[",1:24),sapply(yt,"[",1:24),
> 
> deg=1,maxk=700)
> fr<-fitted(fitt)
> result<-fr
> 
> hltt<-predict(fitt,newdata=tst)
> hltt<-predict.locfit(fitt,newdata=sapply(tst,"[",1:1))
> res_tt<-hltt
> 
> 
> cat(file="prd_model.dat",result, sep="\n")
> cat(file="prd_independent.dat",res_tt,sep="\n")
> 
> 
> Any help is much apreciated...
> thanks in advance
> 
> 
> ANIL KUMAR( METEOROLOGIST GR -II)
> LRF SECTION 
> NATIONAL CLIMATE CENTER 
> ADGM(RESEARCH)
> INDIA METEOROLOGICAL DEPARTMENT
> SHIVIJI NAGAR
> PUNE-411005 INDIA
> MOBILE +919422023277
> anilkumar at imdpune.gov.in
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ramasamy at cancer.org.uk  Thu Nov 10 03:56:58 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 10 Nov 2005 02:56:58 +0000
Subject: [R] how to convert  strings back to values?
In-Reply-To: <freemail.20051009215544.81277@fm08.freemail.hu>
References: <freemail.20051009215544.81277@fm08.freemail.hu>
Message-ID: <1131591418.4865.4.camel@dhcp-82.wolf.ox.ac.uk>


Problems like these could be caused by improperly spaced columns. Try
table(tdf1). If you see only "0" and "1", then you should be fine.
However I suspect that you might see things like " 0", "0", " 1", "1"
which means that there is a an extra space between the delimiters.

Report back what you get and we can work around a solution if need be.

Regards, Adai



On Wed, 2005-11-09 at 21:55 +0100, Illyes Eszter wrote:
> Dear All, 
> 
> It's Eszter from Hungary, a total beginner with R. My problem is the 
> following: 
> 
> I have a dataset with binary values as a comma separated textfile. The 
> samples are in the coloumns and the species are in the rows. 
> 
> I have to transpose it for the further PCoA analysis. There is no 
> problem with reading the dataset. 
> 
> When I transpose the dataset, the original values become strings 
> (instead of 0,1,0,0,1 I have "0","1","0","0","1"). The distance matrix 
> cannot be counted from the transposed dataset, I have 2 error 
> messages: 
> 
> <Warning in vegdist(tdf1, method = "jaccard", binary = FALSE, diag = 
> FALSE,  : results may be meaningless because input data have 
> negative entries>
> 
> <Error in rowSums(x, prod(dn), p, na.rm) : 'x' must be numeric>
> 
> I do not understand the first, since I have only 1 and 0 in the dataset. I 
> guess I have the second because of the strings instead of values in the 
> dataset. 
> 
> Could you please help me solving these problems? I could not find 
> anything about these in the manuals. 
> 
> Thank you, cheers:
> 
> Eszter
> 
> p.s. This is a new problem, last week I worked with a similar dataset 
> and I did not get any error message like these. 
>  
> 
> 
> _____________________________________________________________________
> MenÅ‘ csengÅ‘hangok (MP3 is!) Ã©s szÃ­nes kÃ©pek a mobilodra. 
> NÃ¡lunk szinte mindent megtalÃ¡lsz, KLIKK IDE! www.7777oplogo.hu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Thu Nov 10 04:01:27 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 10 Nov 2005 03:01:27 +0000
Subject: [R] How to find statistics like that.
In-Reply-To: <Pine.GSO.4.60.0511091707290.2930@taxa.epi.umn.edu>
References: <BAY104-F11BD7F7D981E77A1D264C8D0670@phx.gbl>
	<Pine.GSO.4.60.0511091707290.2930@taxa.epi.umn.edu>
Message-ID: <1131591687.4865.9.camel@dhcp-82.wolf.ox.ac.uk>

I think an alternative is to use a p-value from F distribution. Even
tough it is not a statistics, it is much easier to explain and popular
than 1/F. Better yet to report the confidence intervals.

Regards, Adai



On Wed, 2005-11-09 at 17:09 -0600, Mike Miller wrote:
> On Wed, 9 Nov 2005, Gao Fay wrote:
> 
> > Hi there,
> >
> > Suppose mu is constant, and error is normally distributed with mean 0 and 
> > fixed variance s. I need to find a statistics that:
> > Y_i = mu + beta1* I1_i beta2*I2_i + beta3*I1_i*I2_i + +error, where I_i is 1 
> > Y_i is from group A, and 0 if Y_i is from group B.
> >
> > It is large when  beta1=beta2=0
> > It is small when beta1 and/or beta2 is not equal to 0
> >
> > How can I find it by R? Thank you very much for your time.
> 
> 
> That's a funny question.  Usually we want a statistic that is small when 
> beta1=beta2=0 and large otherwise.
> 
> Why not compute the usual F statistic for the null beta1=beta2=0 and then 
> use 1/F as your statistic?
> 
> Mike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Thu Nov 10 04:10:16 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 10 Nov 2005 03:10:16 +0000
Subject: [R] accident modified dataset. How can I recovery it?!
In-Reply-To: <91ae6e350511091346l3bd5b226j1c6f96dc3079dd9a@mail.gmail.com>
References: <91ae6e350511091335m4bac64c9rd7b2de441f6958dc@mail.gmail.com>
	<91ae6e350511091346l3bd5b226j1c6f96dc3079dd9a@mail.gmail.com>
Message-ID: <1131592216.4865.14.camel@dhcp-82.wolf.ox.ac.uk>

Please do not post thrice, especially within 23 min of the first post.

Your problem is that cuckoos is located in DAAG package not the lattice
package. I am guessing that at some point you loaded DAAG in the initial
session but did not realise this on subsequent sessions.

Next time, search http://finzi.psych.upenn.edu/nmz.html first.

Regards, Adai



On Wed, 2005-11-09 at 22:46 +0100, jia ding wrote:
> I tried to reinstall the package. but my R version is too old.
> 
> dj at ubuntu:~$ sudo R CMD INSTALL -l /usr/lib/R/library
> /home/dj/Desktop/lattice_0.12-11.tar.gz
> Password:
> ERROR: This R is version 2.1.1
> package 'lattice' needs R >= 2.2.0
> 
> So, *my question being, how do I upgrade from R version *R >= 2.2.0
> *> to R *2.1.1* and keep all of my libraries intact? *
> 
> 
> On 11/9/05, jia ding <dingjia at gmail.com> wrote:
> >
> > I first try these command, it works quite well.
> > library(lattice)
> > data(cuckoos)
> > levnam <- strsplit(levels(cuckoos$species), "\\.")
> >
> > BUT, i want to try :
> > levnam <- strsplit(levels(cuckoos$species), ".")
> >
> > to see the difference.
> >
> > They maybe I modified the data file, because when I try again, it says:
> > > data(cuckoos)
> > Warning message:
> > data set 'cuckoos' not found in: data(cuckoos)
> >
> > would you please tell me how to deal with this problem?
> >
> > I have already tried update.packages()
> > it doesn't help.
> >
> > Thanks.
> > DJ
> >
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From LI at nsabp.pitt.edu  Thu Nov 10 04:10:32 2005
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Wed, 9 Nov 2005 22:10:32 -0500
Subject: [R] About double-bootstrap
Message-ID: <D70CBC108DFBD446862A6E1F6F0B4A152AC9F0@nsabpmail.nsabp.pitt.edu>

Dear all,
 
I have to use double-bootstrap to resample a data set. I am not familiar with double-bootstrap in R, so I am wondering if there is a function that is related to it
 
Thanks!



From ripley at stats.ox.ac.uk  Thu Nov 10 08:18:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Nov 2005 07:18:43 +0000 (GMT)
Subject: [R] Order of terms in a model specification...
In-Reply-To: <43727E54.2000204@stats.uwo.ca>
References: <HJEOLCCIEHBKJJLPOEBFOEHACCAA.oliver@bic.mni.mcgill.ca>
	<43727E54.2000204@stats.uwo.ca>
Message-ID: <Pine.LNX.4.61.0511100708520.2716@gannet.stats>

On Wed, 9 Nov 2005, Duncan Murdoch wrote:

> On 11/9/2005 3:48 PM, Oliver Lyttelton wrote:
>>
>> Sorry for this one as its pretty basic but I've taken a look for info and
>> couldn't find any...

The only reasonably comprehensive account of this of which I am aware is 
in Chapter 6 of MASS (the book).  That is for S, and there are S/R 
differences (not all of which I suspect any one person is aware off).

>> My question is, does the order of main effect terms in a model specification
>> have any impact on the model R fits or not. (in particular when using lm).
>> ie
>>
>> Can A~X+Y+Z lead to different results to A~Z+Y+X, and if so in what
>> circumstances, and how much should I worry about it?

Depemds what you mean by `different results'.   It will specify the same 
model subspace, but not the same basis vectors for that space.

Collinearity is one example, as Duncan points out.  Factors with no 
intercept are another, so A + B - 1 and B + A - 1 will be different
representations in A and B are factors.

Interactions introduce futher complications. Note that unless you jump 
though hoops, the model-fitting does reorder terms in a formula (see the 
keep.order argument to terms.formula). Under some circumstances the order 
in which interactions are specified (A:B vs B:A) can matter.

>> I believe this is an implementation detail as it depends on the way the
>> fitting algorithm works, but it would be great to have a few lines to plug
>> this gap in my knowledge...
>
> Definitely yes, in the case of collinear terms.  For example,
>
> > X <- rnorm(10)
> > Y <- rnorm(10)
> > Z <- X
> > A <- rnorm(10)
> > lm(A ~ X+Y+Z)
>
> Call:
> lm(formula = A ~ X + Y + Z)
>
> Coefficients:
> (Intercept)            X            Y            Z
>     -0.3474      -0.1166      -0.2203           NA
>
> > lm(A ~ Z+Y+X)
>
> Call:
> lm(formula = A ~ Z + Y + X)
>
> Coefficients:
> (Intercept)            Z            Y            X
>     -0.3474      -0.1166      -0.2203           NA
>
>
> In one case X gets a coefficient and Z doesn't, but the other is the
> opposite.
>
> I suspect there would be differences due to rounding in other
> situations, and they might be noticeable in the case of near-collinearity.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Nov 10 08:38:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Nov 2005 07:38:00 +0000 (GMT)
Subject: [R] About double-bootstrap
In-Reply-To: <D70CBC108DFBD446862A6E1F6F0B4A152AC9F0@nsabpmail.nsabp.pitt.edu>
References: <D70CBC108DFBD446862A6E1F6F0B4A152AC9F0@nsabpmail.nsabp.pitt.edu>
Message-ID: <Pine.LNX.4.61.0511100729310.3400@gannet.stats>

On Wed, 9 Nov 2005, Li, Jia wrote:

> Dear all,
>
> I have to use double-bootstrap to resample a data set. I am not familiar 
> with double-bootstrap in R, so I am wondering if there is a function 
> that is related to it

Double bootstrapping is not a way `to resample a data set' (it is rather 
about adjusting estimators).  It is discussed with worked examples in 
Davison and Hinkley (1997) which is the book that package 'boot' supports 
(and they will have used that package, albeit in S-PLUS).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kihwang.lee at gmail.com  Thu Nov 10 09:39:12 2005
From: kihwang.lee at gmail.com (Kihwang Lee)
Date: Thu, 10 Nov 2005 17:39:12 +0900
Subject: [R] Low level algorithm conrol in Fisher's exact test
Message-ID: <43730730.9050703@gmail.com>

Hi folks,

Forgive me if this question is a trivial issue.

I was doing a series of Fishers' exact test using the fisher.test
function in stats package.
Since the counts I have were quite large (c(64, 3070, 2868, 4961135)), R
suggested me to use
*other algorithms* for the test which can be specified through the
'control' argument of the
fisher.test function as I understood. But where can I find other
algorithms that I can use?
I hoped I could find relevant information in the manual but could not.

Can anybody help me out there?

Many thanks in advance.

Kihwang



From sourceforge at metrak.com  Thu Nov 10 09:41:13 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Thu, 10 Nov 2005 19:41:13 +1100
Subject: [R] Command line and R
In-Reply-To: <20051109144858.0e885248.secchi@sssup.it>
References: <Pine.LNX.4.44.0511091237030.13871-100000@reclus.nhh.no>	<XFMail.051109122537.Ted.Harding@nessie.mcc.ac.uk>
	<20051109144858.0e885248.secchi@sssup.it>
Message-ID: <437307A9.5050200@metrak.com>

Angelo Secchi wrote:
> 
> On Wed, 09 Nov 2005 12:25:37 -0000 (GMT)
> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> wrote:
> 
> 
>>On 09-Nov-05 Roger Bivand wrote:
>>
>>>On Wed, 9 Nov 2005, Angelo Secchi wrote:
>>>
>>>>Hi,
>>>>I wrote a small R script (delta.R) using commandArgs(). The script
>>>>works from the shell in usual way
>>>>
>>>>R --no-save arg1 < delta2.R
>>>>
>>>>Suppose arg1 is the output of another shell command (e.g. gawk,
>>>>sed ...). Is there a way to tell R to read arg1 from the
>>>>output of the previous command? Any other workaround?
>>>
>>>Use shell variables, possibly also Sys.getenv() within R as well as or 
>>>instead of commandArgs().
>>
>>If it's a fairly simple shell comand (and even if it isn't, though
>>it could get tricky for complicated ones) you can use the "backquote"
>>trick (called, in well-spoken circles, "command substitution"):
>>
>>  R --no-save `shellcmd` < delta2.R
>>
>>As in all shell command lines, wherever you have a command (including
>>arguments etc.) between backquotes, as exemplified by "`shellcmd`" above,
>>the output of the command (as sent to stdout) replaces "`shellcmd`" in
>>the command-line. This could be a lot of stuff (depending on what
>>"shellcmd" is), or just one value, or whatever.

... and this behaviour is OS (or at least command shell specific) for 
anyone trying this on Windows and wondering why it doesn't work.



From ligges at statistik.uni-dortmund.de  Thu Nov 10 10:32:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Nov 2005 10:32:03 +0100
Subject: [R] help with legacy R code
In-Reply-To: <4e6115a50511091511o511ad49crd84908ee6fa09da2@mail.gmail.com>
References: <4e6115a50511091511o511ad49crd84908ee6fa09da2@mail.gmail.com>
Message-ID: <43731393.50903@statistik.uni-dortmund.de>

David Zhao wrote:

> Hi there,
> 
> 
> Could somebody help me disect this legacy R script I inherited at work, I
> have two questions:
> 1. I've tried to upgrade our R version from 1.6.2 (yeah, I know), to R 2.0,
> but some of the lines in this script are not compatible with R 2.0, could
> someone help me figure out where the problem is?
> 2. the jpeg generated (attached) seems to be off on some of the data, is
> there a better way of doing this.

1a. R 2.0 must be a software I am not familar with, since for the R I 
know such a version has never been released.
1b. We are unable to reproduce the stuff given below. Not even an error 
message is given.
1c. Do you expect anybody has the time to make your own homework, in 
particular on an unreproducible example? There are very convenient 
debugging tools made available for you in R.
2. We do not see where your jpeg produced with your data is "off".


Uwe Ligges

PS: Let me quote
 > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



> Thanks very much in advance!
> 
> David
> 
> 
>  library(MASS)
>  jpeg(filename = "diswrong.jpg", width = 800, height = 600, pointsize = 12,
> quality = 75, bg = "white")
> 
>  myfunc <- function(x, mean, sd, nfalse, ntotal, shape, rate) {
>  (nfalse*dgamma(x,shape,rate)+(ntotal-nfalse)*dnorm(x,mean,sd))/ntotal
>  }
> 
>  wrong <- scan("wrongrawdata.txt", list(x=0))
>  wrongfit <- fitdistr(wrong$x, "gamma")
>  wrongmean <- mean(wrong$x)
>  wrongshape <- wrongfit[[1]][1]
>  wrongrate <- wrongfit[[1]][2]
> 
>  good <- scan("rawdata.txt", list(x=0))
>  xmin = 0
>  newx = good$x
>  xmean = mean(newx)
> 
> 
>  xmax = max(newx)+0.15
>  goodhist <- hist(newx, br=seq(from=0,to=xmax,by=0.15), probability=T,
> col="lightyellow")
> 
>  initmean <- (min(newx)+max(newx))/2
>  totalx <- length(newx)
> 
>  wrongmeanshift <- wrongmean + 0.2
>  wrongper <- pgamma(wrongmeanshift, wrongshape, wrongrate)
>  nfalseundermean <-
> which(abs(newx-wrongmeanshift)==min(abs(newx-wrongmeanshift)))
>  initnfalse <- nfalseundermean / wrongper
> 
>  fitmean <- -1
>  fitsd <- 0
>  fitnfalse <- initnfalse
>  fitshape <- wrongshape
>  fitrate <- wrongrate
> 
>  curve((fitnfalse*dgamma(x,fitshape,fitrate))/totalx, add=T, col="red",
> lwd=2)
> 
>  breaksllength <- length(goodhist$breaks)
>  endi = breaksllength - 1
>  binprob = c(1)
>  for (i in 1:endi) {
>  expnegative <- fitnfalse * (pgamma(goodhist$breaks[i+1],wrongshape,
> wrongrate)-pgamma(goodhist$breaks[i],wrongshape, wrongrate))
>  if (goodhist$counts[i] == 0)
>  binprob[i] = 0
>  else
>  binprob[i] = (goodhist$counts[i] - expnegative) / goodhist$counts[i]
>  }
> 
>  result = data.frame(newx)
>  prob = c(1)
>  for (i in 1:totalx) {
>  bini = which ((goodhist$breaks < newx[i]) & (goodhist$breaks > newx[i]-0.15
> ))
>  if ((binprob[bini] < 0.8) | (newx[i] < wrongmean))
>  prob[i] = -1
>  else
>  prob[i] = binprob[bini]*100
>  }
> 
>  result = data.frame(result, prob)
>  write.table(result, file="probwrong.txt", sep=" ", row.name=F, col.name=F)
>  fitpars = c(fitmean, fitsd, fitnfalse, fitshape, fitrate, totalx)
>  result = data.frame(fitpars)
>  write.table(result,file="parwrong.txt", sep=" ", row.name=F, col.name=F)
>  dev.off()
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Nov 10 10:36:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Nov 2005 10:36:59 +0100
Subject: [R] Low level algorithm conrol in Fisher's exact test
In-Reply-To: <43730730.9050703@gmail.com>
References: <43730730.9050703@gmail.com>
Message-ID: <437314BB.4050402@statistik.uni-dortmund.de>

Kihwang Lee wrote:

> Hi folks,
> 
> Forgive me if this question is a trivial issue.
> 
> I was doing a series of Fishers' exact test using the fisher.test
> function in stats package.
> Since the counts I have were quite large (c(64, 3070, 2868, 4961135)), R
> suggested me to use
> *other algorithms* for the test which can be specified through the
> 'control' argument of the
> fisher.test function as I understood. But where can I find other
> algorithms that I can use?
> I hoped I could find relevant information in the manual but could not.
> 
> Can anybody help me out there?

What about a chisq.test? And honestly, I know the answer before 
calculating anything ....

Uwe Ligges


> Many thanks in advance.
> 
> Kihwang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Nov 10 10:42:08 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Nov 2005 10:42:08 +0100
Subject: [R] About: Error in FUN(X[[1]],
 ...) : symbol print-name too long
In-Reply-To: <20051109_120002_092149.gsmatos1@ig.com.br>
References: <20051109_120002_092149.gsmatos1@ig.com.br>
Message-ID: <437315F0.9080807@statistik.uni-dortmund.de>

gsmatos1 wrote:

> Hi, 
> 
> I??m trying to use the Win2BUGS package from R and I have a similar problem 

R2WinBUGS???


> that reurns with the message: 
> 
> Error in FUN(X[[1]], ...) : symbol print-name too long 
> 
> But, there is no stray ` character in the file ( Sugestions given by: Duncan 
> Temple Lang <duncan> 
> Date: Mon, 26 Sep 2005 07:31:08 -0700 ) 
> 
> The progam in R is: 
> 
> library(R2WinBUGS) 
> library(rbugs) 

Hmm, mixing these two packages might not be a good idea...


> dat <- 
> list(x=c(49,48,50,44,54,56,48,48,51,51,50,53,51,50,51,54,50,53,50,49,51,47,53,50,49,55,53,48,54,46), 
> y=c(50,49,57,52,47,52,58,45,55,54,51,54,56,53,52,47,51,54,50,47,46,44,54,55,52,57,52,48,48,51)) 
> 
> dat  <- format4Bugs(dat, digits = 0) 

What happens if you omit the line above?

Anyway, I can take closer look, but not within the next 24 hours ...

Uwe Ligges


> parm <- c("lbda") 
> 
> bugs(dat, inits=list(NULL), parm, "d2.bug", 
> n.chains = 1, n.iter = 5000, n.burnin = floor(n.iter/2), 
> n.thin = max(1, floor(n.chains * (n.iter - n.burnin)/1000)), 
> bin = (n.iter - n.burnin) / n.thin, 
> debug = TRUE, DIC = TRUE, digits = 5, codaPkg = FALSE, 
> bugs.directory = "C:/WinBUGS14/", 
> working.directory = NULL, clearWD = FALSE) 
> 
> 	The objective of the program is to compare means of two independent samples 
> that results 
> 	in Beherens-Fisher posterior and in the model.file of WinBUGS "d2.bug" 
> there is the following codes: 
> 
>   model 
> { 
>    for( i in 1 : 30 ) { 
>       x[i] ~ dnorm(mu1,sig1) 
>    } 
>    for( i in 1 : 30 ) { 
>       y[i] ~ dnorm(mu2,sig2) 
>    } 
>    mu1 ~ dnorm(50,1.0E-6) 
>    sig1 ~ dgamma(0.001,0.001) 
>    mu2 ~ dnorm(50,1.0E-6) 
>    sig2 ~ dgamma(0.001,0.001) 
>    lbda <- mu1 - mu2 
> } 
> 
>   I??m a new user of WinBUGS and if someone detect error in the model codes 
> too, I??m grateful. 
> 
> 	Thanks for help! 
> 	Gilberto Matos. 
> 
> 
> ------------------------------------------------------------------------
> 
> model
> {
>    for( i in 1 : 30 ) {
>       x[i] ~ dnorm(mu1,sig1)
>    }
>    for( i in 1 : 30 ) {
>       y[i] ~ dnorm(mu2,sig2)
>    }
>    mu1 ~ dnorm(50,1.0E-6)
>    sig1 ~ dgamma(0.001,0.001)
>    mu2 ~ dnorm(50,1.0E-6)
>    sig2 ~ dgamma(0.001,0.001)
>    lbda <- mu1 - mu2
> }
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dargosch at gmail.com  Thu Nov 10 10:56:34 2005
From: dargosch at gmail.com (Fredrik Karlsson)
Date: Thu, 10 Nov 2005 10:56:34 +0100
Subject: [R] Problems with Shapiro Wilk's test of normality.
In-Reply-To: <376e97ec0511090733q4eb0f7ccy1bbbc873b8339842@mail.gmail.com>
References: <376e97ec0511090733q4eb0f7ccy1bbbc873b8339842@mail.gmail.com>
Message-ID: <376e97ec0511100156v24a35f18waed3ddd2ad876a0a@mail.gmail.com>

Never mind,

I solved it myself. It was an NA problem.

/Fredrik

On 11/9/05, Fredrik Karlsson <dargosch at gmail.com> wrote:
> Hi,
>
> I am trying to create a table with information from Shapiro Wilk's
> test of normality.
> However, it fails due to lack of sample size, it says, but the way I
> see it, this is not a problem.
> (See the table of sample sizes (almost) at the bottom).
>
> Applying a different function using a similar ftable call is not a
> problem (See the bottom table).
>
> This is R 2.1.0 on Linux (Gentoo).
>
> /Fredrik
>
> > shapiro.p.value <- function(x){
> +   if(length(! is.na(x)) > 3 & length(! is.na(x)) < 5000 ){
> +     p <- shapiro.test(x)$p.value
> +     return(p)
> +    }else{
> +      return(NA)
> +      }
> + }
> >
> > distribution.table.fun <- function(x,na.rm=T,digits=1){
> +
> +   if(length(! is.na(x)) > 3 & length(! is.na(x)) < 5000){
> +    # shapTest <- shapiro.test(x)
> +    # W <- shapTest$statistic
> +     W <- "W"
> +   }
> +
> +
> +
> +   shap <- shapiro.p.value(x)
> +   stars <- ''
> +   premark <- ''
> +   postmark <- ''
> +   if(length(x) < 10){
> +       premark <- '\\textit{'
> +       postmark <- '}'
> +   }
> +
> +   #skapa stj??rnor
> +   if(! is.na(shap)){
> +     if( shap <= 0.001 ){
> +       stars <- '***'
> +     }else{
> +       if( shap <= 0.01 ){
> +         stars <- '**'
> +       }else{
> +         if( shap <= 0.05 ){
> +           stars <- '*'
> +         }
> +
> +       }
> +
> +     }
> +
> +     outstr <- paste(premark,'W=',W,',p=',shap,postmark,stars,sep="")
> +   }
> +   else{
> +     outstr <-  ""
> +   }
> +
> +
> +   return(outstr)
> +
> + }
> >
> > ftable(tapply(aspvotwork$ampratio,list(Place=aspvotwork$Place,Age=aspvotwork$agemF,voicetype=aspvotwork$Type),FUN="length" ))
>                voicetype Voiced Voiceless unaspirated Voiceless aspirated
> Place  Age
> Velar  18 - 24               44                    41                  34
>        24 - 30               70                    81                  71
>        30 - 36               59                    66                  64
>        36 - 42               25                    27                  22
>        42 - 48               22                    23                  23
>        48 - 54               12                     9                   7
> Dental 18 - 24               48                    61                  54
>        24 - 30               82                   101                  89
>        30 - 36               57                    82                  72
>        36 - 42               19                    31                  34
>        42 - 48               25                    33                  31
>        48 - 54               10                    12                  14
> Labial 18 - 24               74                   141                  84
>        24 - 30              142                   264                 162
>        30 - 36              124                   213                 148
>        36 - 42               50                    91                  50
>        42 - 48               49                    82                  64
>        48 - 54               17                    26                  16
> > ftable(tapply(aspvotwork$ampratio,list(Place=aspvotwork$Place,Age=aspvotwork$agemF,voicetype=aspvotwork$Type),FUN="distribution.table.fun",digits=4))
> Error in shapiro.test(x) : sample size must be between 3 and 5000
> >
>   > ftable(tapply(aspvotwork$ampratio,list(Place=aspvotwork$Place,Age=aspvotwork$agemF,voicetype=aspvotwork$Type),FUN="mean",digits=4,na.rm=TRUE
> ))
>                voicetype    Voiced Voiceless unaspirated Voiceless aspirated
> Place  Age
> Velar  18 - 24           0.4816810             0.4461307           0.4513994
>        24 - 30           0.5289028             0.4778686           0.4888445
>        30 - 36           0.5452949             0.5208633           0.4756369
>        36 - 42           0.5631310             0.4697789           0.4709779
>        42 - 48           0.4968318             0.4174068           0.4088855
>        48 - 54           0.3057712             0.4483639           0.4561953
> Dental 18 - 24           0.4058078             0.4596251           0.4091731
>        24 - 30           0.4609731             0.4502778           0.4483340
>        30 - 36           0.5095430             0.4726149           0.4315419
>        36 - 42           0.4935719             0.4687774           0.4528758
>        42 - 48           0.4344465             0.4220429           0.4362018
>        48 - 54           0.3697664             0.4338549           0.4897856
> Labial 18 - 24           0.4327926             0.4879985           0.4503917
>        24 - 30           0.5309634             0.4839031           0.5927699
>        30 - 36           0.4094516             0.4444757           0.3964693
>        36 - 42           0.5010130             0.4855550           0.4540598
>        42 - 48           0.4949510             0.4329442           0.3935921
>        48 - 54           0.5217893             0.5124186           0.5011346
> >
>


--
My Gentoo + PVR-350 + IVTV + MythTV blog is on
http://gentoomythtv.blogspot.com/



From ripley at stats.ox.ac.uk  Thu Nov 10 11:04:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Nov 2005 10:04:40 +0000 (GMT)
Subject: [R] Command line and R
In-Reply-To: <437307A9.5050200@metrak.com>
References: <Pine.LNX.4.44.0511091237030.13871-100000@reclus.nhh.no>
	<XFMail.051109122537.Ted.Harding@nessie.mcc.ac.uk>
	<20051109144858.0e885248.secchi@sssup.it>
	<437307A9.5050200@metrak.com>
Message-ID: <Pine.LNX.4.61.0511100958300.10879@gannet.stats>

On Thu, 10 Nov 2005, paul sorenson wrote:

> Angelo Secchi wrote:
>>
>> On Wed, 09 Nov 2005 12:25:37 -0000 (GMT)
>> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> wrote:
>>
>>
>>> On 09-Nov-05 Roger Bivand wrote:
>>>
>>>> On Wed, 9 Nov 2005, Angelo Secchi wrote:
>>>>
>>>>> Hi,
>>>>> I wrote a small R script (delta.R) using commandArgs(). The script
>>>>> works from the shell in usual way
>>>>>
>>>>> R --no-save arg1 < delta2.R
>>>>>
>>>>> Suppose arg1 is the output of another shell command (e.g. gawk,
>>>>> sed ...). Is there a way to tell R to read arg1 from the
>>>>> output of the previous command? Any other workaround?
>>>>
>>>> Use shell variables, possibly also Sys.getenv() within R as well as or
>>>> instead of commandArgs().
>>>
>>> If it's a fairly simple shell comand (and even if it isn't, though
>>> it could get tricky for complicated ones) you can use the "backquote"
>>> trick (called, in well-spoken circles, "command substitution"):
>>>
>>>  R --no-save `shellcmd` < delta2.R
>>>
>>> As in all shell command lines, wherever you have a command (including
>>> arguments etc.) between backquotes, as exemplified by "`shellcmd`" above,
>>> the output of the command (as sent to stdout) replaces "`shellcmd`" in
>>> the command-line. This could be a lot of stuff (depending on what
>>> "shellcmd" is), or just one value, or whatever.
>
> ... and this behaviour is OS (or at least command shell specific) for
> anyone trying this on Windows and wondering why it doesn't work.

But it does work on Windows if you have a reasonable shell.  Cmd.exe is 
and (especially) command.com are not shells in the usually accepted sense.
Better to use Rterm than incur the additional overhead of R, though.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Nov 10 11:17:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Nov 2005 10:17:17 +0000 (GMT)
Subject: [R] Low level algorithm conrol in Fisher's exact test
In-Reply-To: <43730730.9050703@gmail.com>
References: <43730730.9050703@gmail.com>
Message-ID: <Pine.LNX.4.61.0511101007320.10879@gannet.stats>

On Thu, 10 Nov 2005, Kihwang Lee wrote:

> Hi folks,
>
> Forgive me if this question is a trivial issue.
>
> I was doing a series of Fishers' exact test using the fisher.test
> function in stats package.
> Since the counts I have were quite large (c(64, 3070, 2868, 4961135)), R
> suggested me to use
> *other algorithms* for the test which can be specified through the
> 'control' argument of the
> fisher.test function as I understood.

Not that I can reproduce.  You cannot change the algorithm that way.

> But where can I find other algorithms that I can use? I hoped I could 
> find relevant information in the manual but could not.
>
> Can anybody help me out there?

What *exactly* did you see?  I get

> fisher.test(matrix(c(64, 3070, 2868, 4961135), 2))
         FEXACT error 40.
Out of workspace.

> fisher.test(matrix(c(64, 3070, 2868, 4961135), 2), workspace=20e6)
         FEXACT error 501.
The hash table key cannot be computed because the largest key
is larger than the largest representable int.
The algorithm cannot proceed.
Reduce the workspace size or use another algorithm.

Where does it say anything about using control= ?


AFAIK R does not have a means of doing Fisher's test on such a table, and 
it really does not make much statistical sense to do so.  With such 
numbers, the null hypothesis is almost always rejected (try the chisq 
test), even for negligible dependence.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jmoreira at fe.up.pt  Thu Nov 10 11:53:35 2005
From: jmoreira at fe.up.pt (jmoreira@fe.up.pt)
Date: Thu, 10 Nov 2005 10:53:35 +0000
Subject: [R] Choosing the data type to improve accuracy in SVM,
	PPR and	randomForest
Message-ID: <20051110105335.ls37fjejwkgw8gwg@webmail.fe.up.pt>

Dear all,

This question is not a pure R question but I believe it is quite related.

I am trying to find some literature (without success) about the most appropriate
type for the data I am using. For example: day of the week is it better
represented as a factor or as a number? I am trying to answer this for SVM, PPR
and randomForest.

Thanks for any help

Joao Moreira



From p.dalgaard at biostat.ku.dk  Thu Nov 10 11:32:56 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Nov 2005 11:32:56 +0100
Subject: [R] Low level algorithm conrol in Fisher's exact test
In-Reply-To: <437314BB.4050402@statistik.uni-dortmund.de>
References: <43730730.9050703@gmail.com>
	<437314BB.4050402@statistik.uni-dortmund.de>
Message-ID: <x23bm4yeqv.fsf@viggo.kubism.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Kihwang Lee wrote:
> 
> > Hi folks,
> > 
> > Forgive me if this question is a trivial issue.
> > 
> > I was doing a series of Fishers' exact test using the fisher.test
> > function in stats package.
> > Since the counts I have were quite large (c(64, 3070, 2868, 4961135)), R
> > suggested me to use
> > *other algorithms* for the test which can be specified through the
> > 'control' argument of the
> > fisher.test function as I understood. But where can I find other
> > algorithms that I can use?
> > I hoped I could find relevant information in the manual but could not.
> > 
> > Can anybody help me out there?
> 
> What about a chisq.test? And honestly, I know the answer before 
> calculating anything ....

Actually, chisq.test complains that the expected values are too low...
I.e. you expected less than 5 and got 64! So the chisquare
approximation might not be perfect, but p < 2e-16 should be close
enough for jazz.

There's a buglet in the internal FEXACT code that causes it to
allocate a workspace that is way too big for cases like this. If you
really want to know what the p value is, phyper() is less sensitive:

> phyper(63,2932,4964205,3134,lower=FALSE)
[1] 4.512776e-74

(and in cases where one group is much larger than the other, you're
not far off by assuming that the probability in that group is known,
leading to a binomial test:

> binom.test(64,3134,p=2868/4961135)$p.value
[1] 2.368985e-74
)

The control= argument is not too well documented, but according to my
reading of the code, it is only used to set the "mult" argument to
.C("fexact", ...) and has no effect on the current issue.

Actually, the fexact C code is only used if or=1 (the default), so
another way out is

> fisher.test(M,or=1+1e-15)$p.value
[1] 4.512776e-74
> fisher.test(M,or=1-1e-15)$p.value
[1] 4.512776e-74

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Thu Nov 10 11:50:36 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Nov 2005 11:50:36 +0100
Subject: [R] Low level algorithm conrol in Fisher's exact test
In-Reply-To: <Pine.LNX.4.61.0511101007320.10879@gannet.stats>
References: <43730730.9050703@gmail.com>
	<Pine.LNX.4.61.0511101007320.10879@gannet.stats>
Message-ID: <x2y83wwzcz.fsf@viggo.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> 
> AFAIK R does not have a means of doing Fisher's test on such a table, and 
> it really does not make much statistical sense to do so.  With such 
> numbers, the null hypothesis is almost always rejected (try the chisq 
> test), even for negligible dependence.

I have to disagree a little here. If the count in the smaller group
had been smaller we would have been well inside the scope of exact
testing, e.g.


> fisher.test(matrix(c(4, 3070, 2868, 4961135), 2),or=1+1e-15)

        Fisher's Exact Test for Count Data

data:  matrix(c(4, 3070, 2868, 4961135), 2)
p-value = 0.105
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
 0.6132965 5.7830438
sample estimates:
odds ratio
  2.253824

(And the workspace issue still applies, hence the or= fiddle)

A professional statistician would know enough to switch to the
binomial (or Poisson) approximation, but others might need help.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From bitwrit at ozemail.com.au  Fri Nov 11 03:56:32 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 10 Nov 2005 21:56:32 -0500
Subject: [R] Low level algorithm conrol in Fisher's exact test
Message-ID: <43740860.70508@ozemail.com.au>

Prof Brian Ripley wrote:
 >
 > Where does it say anything about using control= ?

fisher.test(x, y = NULL, workspace = 200000, hybrid = FALSE,
             control = list(), or = 1, alternative = "two.sided",
             conf.int = TRUE, conf.level = 0.95)
...
control  a list with named components for low level algorithm control.

I could not make any sense out of this, but it seems to indicate that 
this argument either selects or modifies the algorithm used to compute 
the output.

Jim



From murdoch at stats.uwo.ca  Thu Nov 10 12:05:04 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 10 Nov 2005 06:05:04 -0500
Subject: [R] How to find statistics like that.
In-Reply-To: <1131591687.4865.9.camel@dhcp-82.wolf.ox.ac.uk>
References: <BAY104-F11BD7F7D981E77A1D264C8D0670@phx.gbl>	<Pine.GSO.4.60.0511091707290.2930@taxa.epi.umn.edu>
	<1131591687.4865.9.camel@dhcp-82.wolf.ox.ac.uk>
Message-ID: <43732960.8030806@stats.uwo.ca>

On 11/9/2005 10:01 PM, Adaikalavan Ramasamy wrote:
> I think an alternative is to use a p-value from F distribution. Even
> tough it is not a statistics, it is much easier to explain and popular
> than 1/F. Better yet to report the confidence intervals.

Just curious about your usage:  why do you say a p-value is not a statistic?

Duncan Murdoch

> 
> Regards, Adai
> 
> 
> 
> On Wed, 2005-11-09 at 17:09 -0600, Mike Miller wrote:
> 
>>On Wed, 9 Nov 2005, Gao Fay wrote:
>>
>>
>>>Hi there,
>>>
>>>Suppose mu is constant, and error is normally distributed with mean 0 and 
>>>fixed variance s. I need to find a statistics that:
>>>Y_i = mu + beta1* I1_i beta2*I2_i + beta3*I1_i*I2_i + +error, where I_i is 1 
>>>Y_i is from group A, and 0 if Y_i is from group B.
>>>
>>>It is large when  beta1=beta2=0
>>>It is small when beta1 and/or beta2 is not equal to 0
>>>
>>>How can I find it by R? Thank you very much for your time.
>>
>>
>>That's a funny question.  Usually we want a statistic that is small when 
>>beta1=beta2=0 and large otherwise.
>>
>>Why not compute the usual F statistic for the null beta1=beta2=0 and then 
>>use 1/F as your statistic?
>>
>>Mike
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From luciana.alves at ensp.fiocruz.br  Thu Nov 10 12:06:15 2005
From: luciana.alves at ensp.fiocruz.br (Luciana Correia Alves)
Date: Thu, 10 Nov 2005 08:06:15 -0300
Subject: [R] Help to multinomial analyses
In-Reply-To: <20051109165759.M84413@ensp.fiocruz.br>
References: <20051109165634.M87268@ensp.fiocruz.br>
	<20051109165759.M84413@ensp.fiocruz.br>
Message-ID: <20051110110520.M14497@ensp.fiocruz.br>

Dear Sirs,
Could you please be so kind as to send us some information on residuals in 
multinomial logistic models? Is it possible to use R software?
We thank you in advance.

Sincerely yours

Luciana Alves,MSc
Beatriz Leimann, MD




--
Luciana Correia Alves
Doutoranda em Sa??de P??blica
ENSP - Fiocruz



From bernat at creaf.uab.es  Thu Nov 10 12:19:29 2005
From: bernat at creaf.uab.es (Bernat Claramunt)
Date: Thu, 10 Nov 2005 12:19:29 +0100
Subject: [R] "silly" question on covariable declaration
Message-ID: <001f01c5e5e8$9e192ab0$973e6d9e@creaf.uab.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/9982a0e0/attachment.pl

From Luisr at frs.fo  Thu Nov 10 12:33:33 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Thu, 10 Nov 2005 11:33:33 +0000
Subject: [R] match and %in%
Message-ID: <s3733010.080@ffdata.setur.fo>

R-help,

I have two data frames with a commom column (but with different size)
What I want is to get a column (say df1$mycolumn ) according to the
matches of common columns in both data frames.

I have tried this but it is not working:

transform(fb, breidd = ifelse (match (as.character(df1$puntar),
as.character(df2$puntar) )
, df2$breidd, "no" ) ) )

transform(fb, breidd = ifelse(as.character(df1$puntar) %in%
as.character(df2$puntar)
, df2$breidd, "no" ) ) )


Thank you in advance



From sdavis2 at mail.nih.gov  Thu Nov 10 13:11:25 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 10 Nov 2005 07:11:25 -0500
Subject: [R] match and %in%
In-Reply-To: <s3733010.080@ffdata.setur.fo>
Message-ID: <BF98A31D.1299C%sdavis2@mail.nih.gov>

On 11/10/05 6:33 AM, "Luis Ridao Cruz" <Luisr at frs.fo> wrote:

> R-help,
> 
> I have two data frames with a commom column (but with different size)
> What I want is to get a column (say df1$mycolumn ) according to the
> matches of common columns in both data frames.
> 
> I have tried this but it is not working:
> 
> transform(fb, breidd = ifelse (match (as.character(df1$puntar),
> as.character(df2$puntar) )
> , df2$breidd, "no" ) ) )
> 
> transform(fb, breidd = ifelse(as.character(df1$puntar) %in%
> as.character(df2$puntar)
> , df2$breidd, "no" ) ) )


Try looking at ?merge or ?union to see if either will do what you like.

Sean



From ripley at stats.ox.ac.uk  Thu Nov 10 13:17:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Nov 2005 12:17:14 +0000 (GMT)
Subject: [R] help with legacy R code
In-Reply-To: <4e6115a50511091511o511ad49crd84908ee6fa09da2@mail.gmail.com>
References: <4e6115a50511091511o511ad49crd84908ee6fa09da2@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0511100719560.2716@gannet.stats>

1) There is no 'R 2.0'.  What version did you mean?

2) We cannot reproduce your script (no data files), and JPEGs are not 
allowed on R-help: see http://www.r-project.org/mail.html.  (PNGs are, 
though).

3) You have given us no indication of what the problems are nor in which 
lines.

Please give us more usable information.


On Wed, 9 Nov 2005, David Zhao wrote:

> Could somebody help me disect this legacy R script I inherited at work, I
> have two questions:
> 1. I've tried to upgrade our R version from 1.6.2 (yeah, I know), to R 2.0,
> but some of the lines in this script are not compatible with R 2.0, could
> someone help me figure out where the problem is?
> 2. the jpeg generated (attached) seems to be off on some of the data, is
> there a better way of doing this.


> library(MASS)
> jpeg(filename = "diswrong.jpg", width = 800, height = 600, pointsize = 12,
> quality = 75, bg = "white")
>
> myfunc <- function(x, mean, sd, nfalse, ntotal, shape, rate) {
> (nfalse*dgamma(x,shape,rate)+(ntotal-nfalse)*dnorm(x,mean,sd))/ntotal
> }
>
> wrong <- scan("wrongrawdata.txt", list(x=0))

...


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Thu Nov 10 13:31:21 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 10 Nov 2005 12:31:21 +0000
Subject: [R] How to find statistics like that.
In-Reply-To: <43732960.8030806@stats.uwo.ca>
References: <BAY104-F11BD7F7D981E77A1D264C8D0670@phx.gbl>
	<Pine.GSO.4.60.0511091707290.2930@taxa.epi.umn.edu>
	<1131591687.4865.9.camel@dhcp-82.wolf.ox.ac.uk>
	<43732960.8030806@stats.uwo.ca>
Message-ID: <1131625881.3155.27.camel@dhcp-82.wolf.ox.ac.uk>

If my usage is wrong please correct me. Thank you.

Here are my reason :

1. p-value is a (cumulative) probability and always ranges from 0 to 1.
A test statistic depending on its definition can wider range of possible
values.

2. A test statistics is one that is calculated from the data without the
need of assuming a null distribution. Whereas to calculate p-values, you
need to assume a null distribution or estimate it empirically using
permutation techniques.

3. The directionality of a test statistics may be ignored. For example a
t-statistics of -5 and 5 are equally interesting in a two-sided testing.
But the smaller the p-value, more evidence against the null hypothesis.

Regards, Adai



On Thu, 2005-11-10 at 06:05 -0500, Duncan Murdoch wrote:
> On 11/9/2005 10:01 PM, Adaikalavan Ramasamy wrote:
> > I think an alternative is to use a p-value from F distribution. Even
> > tough it is not a statistics, it is much easier to explain and popular
> > than 1/F. Better yet to report the confidence intervals.
> 
> Just curious about your usage:  why do you say a p-value is not a statistic?
> 
> Duncan Murdoch
> 
> > 
> > Regards, Adai
> > 
> > 
> > 
> > On Wed, 2005-11-09 at 17:09 -0600, Mike Miller wrote:
> > 
> >>On Wed, 9 Nov 2005, Gao Fay wrote:
> >>
> >>
> >>>Hi there,
> >>>
> >>>Suppose mu is constant, and error is normally distributed with mean 0 and 
> >>>fixed variance s. I need to find a statistics that:
> >>>Y_i = mu + beta1* I1_i beta2*I2_i + beta3*I1_i*I2_i + +error, where I_i is 1 
> >>>Y_i is from group A, and 0 if Y_i is from group B.
> >>>
> >>>It is large when  beta1=beta2=0
> >>>It is small when beta1 and/or beta2 is not equal to 0
> >>>
> >>>How can I find it by R? Thank you very much for your time.
> >>
> >>
> >>That's a funny question.  Usually we want a statistic that is small when 
> >>beta1=beta2=0 and large otherwise.
> >>
> >>Why not compute the usual F statistic for the null beta1=beta2=0 and then 
> >>use 1/F as your statistic?
> >>
> >>Mike
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Thu Nov 10 13:53:44 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 10 Nov 2005 07:53:44 -0500
Subject: [R] How to find statistics like that.
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED587@usctmx1106.merck.com>

The definition of a statistic that I learned in grad school is that it's a
function of a random sample from a population.  Any p-value would fit that
definition.

Andy

> From: Adaikalavan Ramasamy
> 
> If my usage is wrong please correct me. Thank you.
> 
> Here are my reason :
> 
> 1. p-value is a (cumulative) probability and always ranges 
> from 0 to 1.
> A test statistic depending on its definition can wider range 
> of possible
> values.
> 
> 2. A test statistics is one that is calculated from the data 
> without the
> need of assuming a null distribution. Whereas to calculate 
> p-values, you
> need to assume a null distribution or estimate it empirically using
> permutation techniques.
> 
> 3. The directionality of a test statistics may be ignored. 
> For example a
> t-statistics of -5 and 5 are equally interesting in a 
> two-sided testing.
> But the smaller the p-value, more evidence against the null 
> hypothesis.
> 
> Regards, Adai
> 
> 
> 
> On Thu, 2005-11-10 at 06:05 -0500, Duncan Murdoch wrote:
> > On 11/9/2005 10:01 PM, Adaikalavan Ramasamy wrote:
> > > I think an alternative is to use a p-value from F 
> distribution. Even
> > > tough it is not a statistics, it is much easier to 
> explain and popular
> > > than 1/F. Better yet to report the confidence intervals.
> > 
> > Just curious about your usage:  why do you say a p-value is 
> not a statistic?
> > 
> > Duncan Murdoch
> > 
> > > 
> > > Regards, Adai
> > > 
> > > 
> > > 
> > > On Wed, 2005-11-09 at 17:09 -0600, Mike Miller wrote:
> > > 
> > >>On Wed, 9 Nov 2005, Gao Fay wrote:
> > >>
> > >>
> > >>>Hi there,
> > >>>
> > >>>Suppose mu is constant, and error is normally 
> distributed with mean 0 and 
> > >>>fixed variance s. I need to find a statistics that:
> > >>>Y_i = mu + beta1* I1_i beta2*I2_i + beta3*I1_i*I2_i + 
> +error, where I_i is 1 
> > >>>Y_i is from group A, and 0 if Y_i is from group B.
> > >>>
> > >>>It is large when  beta1=beta2=0
> > >>>It is small when beta1 and/or beta2 is not equal to 0
> > >>>
> > >>>How can I find it by R? Thank you very much for your time.
> > >>
> > >>
> > >>That's a funny question.  Usually we want a statistic 
> that is small when 
> > >>beta1=beta2=0 and large otherwise.
> > >>
> > >>Why not compute the usual F statistic for the null 
> beta1=beta2=0 and then 
> > >>use 1/F as your statistic?
> > >>
> > >>Mike
> > >>
> > >>______________________________________________
> > >>R-help at stat.math.ethz.ch mailing list
> > >>https://stat.ethz.ch/mailman/listinfo/r-help
> > >>PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > >>
> > > 
> > > 
> > 
> > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From bady at univ-lyon1.fr  Thu Nov 10 14:03:56 2005
From: bady at univ-lyon1.fr (bady@univ-lyon1.fr)
Date: Thu, 10 Nov 2005 14:03:56 +0100
Subject: [R] Help to multinomial analyses
In-Reply-To: <20051110110520.M14497@ensp.fiocruz.br>
References: <20051109165634.M87268@ensp.fiocruz.br>
	<20051109165759.M84413@ensp.fiocruz.br>
	<20051110110520.M14497@ensp.fiocruz.br>
Message-ID: <1131627836.4373453c9cac6@webmail.univ-lyon1.fr>


hi, hi all,

> Dear Sirs,
> Could you please be so kind as to send us some information on residuals in
> multinomial logistic models?

here are some references to multinomial models:

Agresti A.(1996) An Introduction to Categorical Data Analysis

Agresti A. (2002) Categorical Data Analysis, 2nd Edition

McCullagh P. and Nelder J. (1989). Generalized Linear Models. Chapman and Hall,
London.

http://data.princeton.edu/wws509/notes/c6.pdf

http://www.statslab.cam.ac.uk/~pat/Splusdiscrete2.pdf
(see chapter 11 for Multinomial response)

etc .......

> Is it possible to use R software?

You can consult these links :
http://www.stat.ufl.edu/~aa/cda/software.html
http://www.statslab.cam.ac.uk/~pat/Splusdiscrete2.pdf
(see chapter 11 for Multinomial response)


cheers,

P.BADY



From RRoa at fisheries.gov.fk  Thu Nov 10 13:24:17 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Thu, 10 Nov 2005 10:24:17 -0200
Subject: [R] How to find statistics like that.
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC5ED@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From:	r-help-bounces at stat.math.ethz.ch [SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adaikalavan Ramasamy
> Sent:	Thursday, November 10, 2005 10:31 AM
> To:	Duncan Murdoch
> Cc:	r-help at stat.math.ethz.ch
> Subject:	Re: [R] How to find statistics like that.
> 
> If my usage is wrong please correct me. Thank you.
> 
> Here are my reason :
> 
> 1. p-value is a (cumulative) probability and always ranges from 0 to 1.
> A test statistic depending on its definition can wider range of possible
> values.
> 
> 2. A test statistics is one that is calculated from the data without the
> need of assuming a null distribution. Whereas to calculate p-values, you
> need to assume a null distribution or estimate it empirically using
> permutation techniques.
> 
> 3. The directionality of a test statistics may be ignored. For example a
> t-statistics of -5 and 5 are equally interesting in a two-sided testing.
> But the smaller the p-value, more evidence against the null hypothesis.
> 
> Regards, Adai
> 
--------
Hi:
A statistic is any real-valued or vector-valued function whose
domain includes the sample space of a random sample. The
p-value is a real-valued function and its domain includes the 
sample space of a random sample. The p-value has a sampling
distribution. The code below, found with Google ("sampling distribution
of the p-value" "R command") shows the sampling
distribution of the p-value for a t-test of a mean when the null hypothesis
is true.
Ruben

n<-18
mu<-40
pop.var<-100
n.draw<-200
alpha<-0.05
draws<-matrix(rnorm(n.draw * n, mu, sqrt(pop.var)), n)
get.p.value<-function(x) t.test(x, mu = mu)$p.value
pvalues<-apply(draws, 2, get.p.value)
hist(pvalues)
sum(pvalues <= alpha)
[1] 6



From Luisr at frs.fo  Thu Nov 10 14:28:25 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Thu, 10 Nov 2005 13:28:25 +0000
Subject: [R] paste argument of a function as a file name
Message-ID: <s3734afe.043@ffdata.setur.fo>

R-help,

I have a function which is exporting the output to a file via
write.table(df, file =  "file name.xls" )

What I want is to paste the file name (above) by taking the argument to
the function as a file name 

something like this:

MY.function<- function(df)
{
...
...
write.table(df,"argument.xls")
}
MY.function(argument)


Thank you



From flom at ndri.org  Thu Nov 10 14:29:18 2005
From: flom at ndri.org (Peter Flom)
Date: Thu, 10 Nov 2005 08:29:18 -0500
Subject: [R] Help to multinomial analyses
Message-ID: <s37304fd.021@MAIL.NDRI.ORG>

Luciana Alves asked

> Dear Sirs,
> Could you please be so kind as to send us some information on
residuals in
> multinomial logistic models?



Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
http://cduhr.ndri.org
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



>>> <bady at univ-lyon1.fr> 11/10/2005 8:03:56 AM >>> gave some very good
references

here are some references to multinomial models:

Agresti A.(1996) An Introduction to Categorical Data Analysis

Agresti A. (2002) Categorical Data Analysis, 2nd Edition

McCullagh P. and Nelder J. (1989). Generalized Linear Models. Chapman
and Hall,
London.

http://data.princeton.edu/wws509/notes/c6.pdf 

http://www.statslab.cam.ac.uk/~pat/Splusdiscrete2.pdf 
(see chapter 11 for Multinomial response)

etc .......

> Is it possible to use R software?

You can consult these links :
http://www.stat.ufl.edu/~aa/cda/software.html 
http://www.statslab.cam.ac.uk/~pat/Splusdiscrete2.pdf 
(see chapter 11 for Multinomial response)
>>>

I would suggest, in addition,

Hosmer & Lemeshow Applied Logistic Regression esp. p 280-288, and
references therein.
In particular, they cite 

Lesaffre & Albert, Multiple-group regression diagnostics, Applied
Statistics, 38, 425-440

but note that the techniques recommended there are not implemented in
'available software'.  I would be interested to know if these techniques
have been implemented since H and L.

Regards

Peter

cheers,

P.BADY

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bernarduse1 at yahoo.fr  Thu Nov 10 14:32:35 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Thu, 10 Nov 2005 14:32:35 +0100 (CET)
Subject: [R] Remove levels
Message-ID: <20051110133235.67767.qmail@web25811.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/358bf0b4/attachment.pl

From sdavis2 at mail.nih.gov  Thu Nov 10 14:39:16 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 10 Nov 2005 08:39:16 -0500
Subject: [R] paste argument of a function as a file name
In-Reply-To: <s3734afe.043@ffdata.setur.fo>
Message-ID: <BF98B7B4.129BA%sdavis2@mail.nih.gov>

On 11/10/05 8:28 AM, "Luis Ridao Cruz" <Luisr at frs.fo> wrote:

> R-help,
> 
> I have a function which is exporting the output to a file via
> write.table(df, file =  "file name.xls" )
> 
> What I want is to paste the file name (above) by taking the argument to
> the function as a file name

help('paste')

Sean



From JeeBee at troefpunt.nl  Thu Nov 10 14:21:56 2005
From: JeeBee at troefpunt.nl (JeeBee)
Date: Thu, 10 Nov 2005 14:21:56 +0100
Subject: [R] write.table read.table with Dates
Message-ID: <pan.2005.11.10.13.21.54.785817@troefpunt.nl>

I've found several similar issues with write.table/read.table
with Dates on this list, but trying to follow this advice I still
get an error.

First, I read in data from several files, constructing several date/time
columns using ISOdatetime

> str(Tall$Begin)
'POSIXct', format: chr [1:40114] "2005-10-02 00:00:00" "2005-10-02
00:00:00" ...
> length(Tall$Begin)
[1] 40114
> class(Tall$Begin)
[1] "POSIXt"  "POSIXct"

This looks good (time is not always 00:00:00 ...)
This data came from several files, now I want to store the result I have
in data.frame Tall and be able to retrieve this quickly some other time.

This is what I do:
write.table(Tall, file="somefile.csv", sep=",", qmethod="double",
row.names=FALSE)

Later, I do this to read the file again:
fieldnames=c("Begin","test-a","test-b","Eind")
T=read.table(file = "somefile.csv", col.names = fieldnames,
  header = TRUE, sep = ",", quote="\"", fill=FALSE)

I understand T$Begin now is a factor. I tried to simply convert it
again using (as I read on this mailinglist ...):
Q = strptime(as.character(T$Begin),format="%Y-%m-%d %H:%M:%S")

Q is looking good, though its length I don't understand .. is it a list or
something? It seems there are 40114 values in there somewhere...

> class(Q)
[1] "POSIXt"  "POSIXlt"
> length(Q)
[1] 9
> str(Q)
'POSIXlt', format: chr [1:40114] "2005-10-02 00:00:00" "2005-10-02 00:00:00" ...

T$Begin = Q ### yields this error
Error in "$<-.data.frame"(`*tmp*`, "Begin", value = list(sec = c(0, 0,  :
        replacement has 9 rows, data has 40114

Could somebody explain me how to convert the date column?
Or perhaps there is an easier way?

Thanks in advance for your time.



From francoisromain at free.fr  Thu Nov 10 14:49:28 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 10 Nov 2005 14:49:28 +0100
Subject: [R] paste argument of a function as a file name
In-Reply-To: <s3734afe.043@ffdata.setur.fo>
References: <s3734afe.043@ffdata.setur.fo>
Message-ID: <43734FE8.8090302@free.fr>

Le 10.11.2005 14:28, Luis Ridao Cruz a ??crit :

>R-help,
>
>I have a function which is exporting the output to a file via
>write.table(df, file =  "file name.xls" )
>
>What I want is to paste the file name (above) by taking the argument to
>the function as a file name 
>
>something like this:
>
>MY.function<- function(df)
>{
>...
>...
>write.table(df,"argument.xls")
>}
>MY.function(argument)
>
>
>Thank you
>  
>
Hi,

Maybe sprintf or paste.

MY.function<- function(df, arg="argument")
{
...
...
write.table(df,paste(arg,".xls",sep=""))
# or : 
# write.table(df,sprintf("%s.xls",arg))
}
MY.function(argument)



-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From B.Rowlingson at lancaster.ac.uk  Thu Nov 10 14:47:21 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 10 Nov 2005 13:47:21 +0000
Subject: [R] paste argument of a function as a file name
In-Reply-To: <s3734afe.043@ffdata.setur.fo>
References: <s3734afe.043@ffdata.setur.fo>
Message-ID: <43734F69.3000801@lancaster.ac.uk>

Luis Ridao Cruz wrote:
> R-help,
> 
> I have a function which is exporting the output to a file via
> write.table(df, file =  "file name.xls" )
> 
> What I want is to paste the file name (above) by taking the argument to
> the function as a file name 
> 
> something like this:

  More like this:

foo = function(df){
  fn=paste(deparse(substitute(df)),'.xls',sep='')
  write.table(df,file=fn)
}

Then:

  x=1:10
  foo(x)

produces a file: x.xls

  deparse(substitute(df)) is used in plot() to label the Y-axis with the 
name of the object passed to plot(), which is similar to what you want 
to do here.

Baz



From ramasamy at cancer.org.uk  Thu Nov 10 14:50:31 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 10 Nov 2005 13:50:31 +0000
Subject: [R] paste argument of a function as a file name
In-Reply-To: <s3734afe.043@ffdata.setur.fo>
References: <s3734afe.043@ffdata.setur.fo>
Message-ID: <1131630631.3155.77.camel@dhcp-82.wolf.ox.ac.uk>

my.write <- function( obj, name ){

  filename <- file=paste( name, ".txt", sep="")
  write.table( obj, file=filename, sep="\t", quote=F)

}

my.write( df, "output" )

Regards, Adai


On Thu, 2005-11-10 at 13:28 +0000, Luis Ridao Cruz wrote:
> R-help,
> 
> I have a function which is exporting the output to a file via
> write.table(df, file =  "file name.xls" )
> 
> What I want is to paste the file name (above) by taking the argument to
> the function as a file name 
> 
> something like this:
> 
> MY.function<- function(df)
> {
> ...
> ...
> write.table(df,"argument.xls")
> }
> MY.function(argument)
> 
> 
> Thank you
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From FredeA.Togersen at agrsci.dk  Thu Nov 10 14:57:02 2005
From: FredeA.Togersen at agrsci.dk (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Thu, 10 Nov 2005 14:57:02 +0100
Subject: [R] paste argument of a function as a file name
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC03725D0F@DJFPOST01.djf.agrsci.dk>

Why not use something like


MY.function <- function(x){
  filn <- deparse(substitute(x))
  filename <- paste(filn,"xls",sep=".")
  ...
  ...
  write.table(x,file=filename)
}


Med venlig hilsen
Frede Aakmann T??gersen
 

 

> -----Oprindelig meddelelse-----
> Fra: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] P?? vegne af Luis Ridao Cruz
> Sendt: 10. november 2005 14:28
> Til: r-help at stat.math.ethz.ch
> Emne: [R] paste argument of a function as a file name
> 
> R-help,
> 
> I have a function which is exporting the output to a file via 
> write.table(df, file =  "file name.xls" )
> 
> What I want is to paste the file name (above) by taking the 
> argument to the function as a file name 
> 
> something like this:
> 
> MY.function<- function(df)
> {
> ...
> ...
> write.table(df,"argument.xls")
> }
> MY.function(argument)
> 
> 
> Thank you
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Thu Nov 10 15:10:33 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Nov 2005 15:10:33 +0100
Subject: [R] Remove levels
In-Reply-To: <20051110133235.67767.qmail@web25811.mail.ukl.yahoo.com>
References: <20051110133235.67767.qmail@web25811.mail.ukl.yahoo.com>
Message-ID: <437354D9.50907@statistik.uni-dortmund.de>

Marc Bernard wrote:

> Daer All,
>  
> I have a factor  variable, X with 5 levels. When I type tables(X) it gives me:
>  
> table(X)
> 1     2   3    4   5 
> 10   5    0   0   0
>  
> How to drop the levels with zeros such that when I will type:
> table(X) it will give me:
>  
> table(X)
> 1     2   
> 10   5    


table(X[,drop=TRUE])

Uwe Ligges


>  
> Thank a lot,
>  
> Bernard
>  
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Thu Nov 10 15:07:39 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 10 Nov 2005 09:07:39 -0500
Subject: [R] How to find statistics like that.
In-Reply-To: <1131625881.3155.27.camel@dhcp-82.wolf.ox.ac.uk>
References: <BAY104-F11BD7F7D981E77A1D264C8D0670@phx.gbl>	
	<Pine.GSO.4.60.0511091707290.2930@taxa.epi.umn.edu>	
	<1131591687.4865.9.camel@dhcp-82.wolf.ox.ac.uk>	
	<43732960.8030806@stats.uwo.ca>
	<1131625881.3155.27.camel@dhcp-82.wolf.ox.ac.uk>
Message-ID: <4373542B.3010604@stats.uwo.ca>

On 11/10/2005 7:31 AM, Adaikalavan Ramasamy wrote:
> If my usage is wrong please correct me. Thank you.
> 
> Here are my reason :
> 
> 1. p-value is a (cumulative) probability and always ranges from 0 to 1.
> A test statistic depending on its definition can wider range of possible
> values.
> 
> 2. A test statistics is one that is calculated from the data without the
> need of assuming a null distribution. Whereas to calculate p-values, you
> need to assume a null distribution or estimate it empirically using
> permutation techniques.
> 
> 3. The directionality of a test statistics may be ignored. For example a
> t-statistics of -5 and 5 are equally interesting in a two-sided testing.
> But the smaller the p-value, more evidence against the null hypothesis.
> 
> Regards, Adai

Thanks for your explanation.  I think your interpretation is one that is 
sometimes taught, but I think it's more useful to think of a p-value as 
just another statistic, whose null distribution (in the ideal case, but 
not always in practice) is a uniform distribution on (0,1), and whose 
distribution when the alternative is true (again, ideally) tends to be 
more concentrated near 0.  This takes a lot of the mysticism out of them.

Duncan Murdoch
> 
> 
> On Thu, 2005-11-10 at 06:05 -0500, Duncan Murdoch wrote:
> 
>>On 11/9/2005 10:01 PM, Adaikalavan Ramasamy wrote:
>>
>>>I think an alternative is to use a p-value from F distribution. Even
>>>tough it is not a statistics, it is much easier to explain and popular
>>>than 1/F. Better yet to report the confidence intervals.
>>
>>Just curious about your usage:  why do you say a p-value is not a statistic?
>>
>>Duncan Murdoch
>>
>>
>>>Regards, Adai
>>>
>>>
>>>
>>>On Wed, 2005-11-09 at 17:09 -0600, Mike Miller wrote:
>>>
>>>
>>>>On Wed, 9 Nov 2005, Gao Fay wrote:
>>>>
>>>>
>>>>
>>>>>Hi there,
>>>>>
>>>>>Suppose mu is constant, and error is normally distributed with mean 0 and 
>>>>>fixed variance s. I need to find a statistics that:
>>>>>Y_i = mu + beta1* I1_i beta2*I2_i + beta3*I1_i*I2_i + +error, where I_i is 1 
>>>>>Y_i is from group A, and 0 if Y_i is from group B.
>>>>>
>>>>>It is large when  beta1=beta2=0
>>>>>It is small when beta1 and/or beta2 is not equal to 0
>>>>>
>>>>>How can I find it by R? Thank you very much for your time.
>>>>
>>>>
>>>>That's a funny question.  Usually we want a statistic that is small when 
>>>>beta1=beta2=0 and large otherwise.
>>>>
>>>>Why not compute the usual F statistic for the null beta1=beta2=0 and then 
>>>>use 1/F as your statistic?
>>>>
>>>>Mike
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>



From ligges at statistik.uni-dortmund.de  Thu Nov 10 15:11:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Nov 2005 15:11:45 +0100
Subject: [R] paste argument of a function as a file name
In-Reply-To: <s3734afe.043@ffdata.setur.fo>
References: <s3734afe.043@ffdata.setur.fo>
Message-ID: <43735521.8040900@statistik.uni-dortmund.de>

Luis Ridao Cruz wrote:

> R-help,
> 
> I have a function which is exporting the output to a file via
> write.table(df, file =  "file name.xls" )
> 
> What I want is to paste the file name (above) by taking the argument to
> the function as a file name 
> 
> something like this:
> 
> MY.function<- function(df)
> {
> ...
> ...
> write.table(df,"argument.xls")
> }
> MY.function(argument)

Has been asked hundreds of times on this list. Please check the archives 
as the posting guide asks to do  ...

Hint: paste()

Uwe Ligges


> 
> Thank you
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Nov 10 15:11:59 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Nov 2005 15:11:59 +0100
Subject: [R] Remove levels
In-Reply-To: <20051110133235.67767.qmail@web25811.mail.ukl.yahoo.com>
References: <20051110133235.67767.qmail@web25811.mail.ukl.yahoo.com>
Message-ID: <x2lkzwwq1c.fsf@viggo.kubism.ku.dk>

Marc Bernard <bernarduse1 at yahoo.fr> writes:

> Daer All,
>  
> I have a factor  variable, X with 5 levels. When I type tables(X) it gives me:
>  
> table(X)
> 1     2   3    4   5 
> 10   5    0   0   0
>  
> How to drop the levels with zeros such that when I will type:
> table(X) it will give me:
>  
> table(X)
> 1     2   
> 10   5    

table(factor(X)) or table(X[drop=TRUE]) should do it. The latter runs
the former, but the intention might be clearer.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From mbmiller at taxa.epi.umn.edu  Thu Nov 10 15:32:03 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Thu, 10 Nov 2005 08:32:03 -0600 (CST)
Subject: [R] How to find statistics like that.
In-Reply-To: <03DCBBA079F2324786E8715BE538968A3DC5ED@FIGMAIL-CLUS01.FIG.FK>
References: <03DCBBA079F2324786E8715BE538968A3DC5ED@FIGMAIL-CLUS01.FIG.FK>
Message-ID: <Pine.GSO.4.60.0511100826500.28235@taxa.epi.umn.edu>

On Thu, 10 Nov 2005, Ruben Roa wrote:

> A statistic is any real-valued or vector-valued function whose
> domain includes the sample space of a random sample. The
> p-value is a real-valued function and its domain includes the
> sample space of a random sample. The p-value has a sampling
> distribution. The code below, found with Google ("sampling distribution
> of the p-value" "R command") shows the sampling
> distribution of the p-value for a t-test of a mean when the null hypothesis
> is true.
> Ruben
>
> n<-18
> mu<-40
> pop.var<-100
> n.draw<-200
> alpha<-0.05
> draws<-matrix(rnorm(n.draw * n, mu, sqrt(pop.var)), n)
> get.p.value<-function(x) t.test(x, mu = mu)$p.value
> pvalues<-apply(draws, 2, get.p.value)
> hist(pvalues)
> sum(pvalues <= alpha)
> [1] 6


The sampling distribution of a p-value when the null hypothesis is true 
can be given more simply by this R code:

runif()

That holds for any valid test, not just a t test, that produces p-values 
distributed continuously on [0,1].  Discrete distributions can't quite do 
that without special tweaking.

Mike



From ggrothendieck at gmail.com  Thu Nov 10 15:32:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Nov 2005 09:32:26 -0500
Subject: [R] write.table read.table with Dates
In-Reply-To: <pan.2005.11.10.13.21.54.785817@troefpunt.nl>
References: <pan.2005.11.10.13.21.54.785817@troefpunt.nl>
Message-ID: <971536df0511100632g2e0a6e4bv53ee08ce355d55b7@mail.gmail.com>

On 11/10/05, JeeBee <JeeBee at troefpunt.nl> wrote:
> I've found several similar issues with write.table/read.table
> with Dates on this list, but trying to follow this advice I still
> get an error.
>
> First, I read in data from several files, constructing several date/time
> columns using ISOdatetime
>
> > str(Tall$Begin)
> 'POSIXct', format: chr [1:40114] "2005-10-02 00:00:00" "2005-10-02
> 00:00:00" ...
> > length(Tall$Begin)
> [1] 40114
> > class(Tall$Begin)
> [1] "POSIXt"  "POSIXct"
>
> This looks good (time is not always 00:00:00 ...)
> This data came from several files, now I want to store the result I have
> in data.frame Tall and be able to retrieve this quickly some other time.
>
> This is what I do:
> write.table(Tall, file="somefile.csv", sep=",", qmethod="double",
> row.names=FALSE)
>
> Later, I do this to read the file again:
> fieldnames=c("Begin","test-a","test-b","Eind")
> T=read.table(file = "somefile.csv", col.names = fieldnames,
>  header = TRUE, sep = ",", quote="\"", fill=FALSE)
>
> I understand T$Begin now is a factor. I tried to simply convert it
> again using (as I read on this mailinglist ...):
> Q = strptime(as.character(T$Begin),format="%Y-%m-%d %H:%M:%S")
>
> Q is looking good, though its length I don't understand .. is it a list or
> something? It seems there are 40114 values in there somewhere...
>
> > class(Q)
> [1] "POSIXt"  "POSIXlt"
> > length(Q)
> [1] 9
> > str(Q)
> 'POSIXlt', format: chr [1:40114] "2005-10-02 00:00:00" "2005-10-02 00:00:00" ...
>
> T$Begin = Q ### yields this error
> Error in "$<-.data.frame"(`*tmp*`, "Begin", value = list(sec = c(0, 0,  :
>        replacement has 9 rows, data has 40114
>
> Could somebody explain me how to convert the date column?
> Or perhaps there is an easier way?
>

You are converting it to POSIXlt (which represents date/times as a 9
element structure) but its likely you really wanted to convert it to
POSIXct.

as.POSIXct(T$Begin)

Also, you might need to use the tz= argument depending on what result
you want.

See the Help Desk article in RNews 4/1 for more info.



From RRoa at fisheries.gov.fk  Thu Nov 10 14:41:30 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Thu, 10 Nov 2005 11:41:30 -0200
Subject: [R] How to find statistics like that.
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC5EF@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From:	Mike Miller [SMTP:mbmiller at taxa.epi.umn.edu]
> Sent:	Thursday, November 10, 2005 12:32 PM
> To:	Ruben Roa
> Cc:	ramasamy at cancer.org.uk; Duncan Murdoch; r-help at stat.math.ethz.ch
> Subject:	Re: [R] How to find statistics like that.
> 
> On Thu, 10 Nov 2005, Ruben Roa wrote:
> 
> > A statistic is any real-valued or vector-valued function whose
> > domain includes the sample space of a random sample. The
> > p-value is a real-valued function and its domain includes the
> > sample space of a random sample. The p-value has a sampling
> > distribution. The code below, found with Google ("sampling distribution
> > of the p-value" "R command") shows the sampling
> > distribution of the p-value for a t-test of a mean when the null hypothesis
> > is true.
> > Ruben
> >
> > n<-18
> > mu<-40
> > pop.var<-100
> > n.draw<-200
> > alpha<-0.05
> > draws<-matrix(rnorm(n.draw * n, mu, sqrt(pop.var)), n)
> > get.p.value<-function(x) t.test(x, mu = mu)$p.value
> > pvalues<-apply(draws, 2, get.p.value)
> > hist(pvalues)
> > sum(pvalues <= alpha)
> > [1] 6
> 
> 
> The sampling distribution of a p-value when the null hypothesis is true 
> can be given more simply by this R code:
> 
> runif()
> 
> That holds for any valid test, not just a t test, that produces p-values 
> distributed continuously on [0,1].  Discrete distributions can't quite do 
> that without special tweaking.
> 
> Mike
> 
------------
Theorem 2.1.4 in Casella and Berger (1990, p. 52).
Ruben



From jfox at mcmaster.ca  Thu Nov 10 15:41:58 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 10 Nov 2005 09:41:58 -0500
Subject: [R] Interpretation of output from glm
In-Reply-To: <6.1.2.0.2.20051109154310.0240a708@pop.ualg.pt>
Message-ID: <20051110144155.JKZJ25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Pedro,

The basic point, which relates to the principle of marginality in
formulating linear models, applies whether the predictors are factors,
covariates, or both. I think that this is a common topic in books on linear
models; I certainly discuss it in my Applied Regression, Linear Models, and
Related Methods.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pedro de Barros
> Sent: Wednesday, November 09, 2005 10:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Interpretation of output from glm
> Importance: High
> 
> Dear John,
> 
> Thanks for the quick reply. I did indeed have these ideas, 
> but somehow "floating", and all I could find about this 
> mentioned categorical predictors. Can you suggest a good book 
> where I could try to learn more about this?
> 
> Thanks again,
> 
> Pedro
> At 01:49 09/11/2005, you wrote:
> >Dear Pedro,
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pedro de 
> > > Barros
> > > Sent: Tuesday, November 08, 2005 9:47 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] Interpretation of output from glm
> > > Importance: High
> > >
> > > I am fitting a logistic model to binary data. The 
> response variable 
> > > is a factor (0 or 1) and all predictors are continuous variables. 
> > > The main predictor is LT (I expect a logistic relation between LT 
> > > and the probability of being
> > > mature) and the other are variables I expect to modify 
> this relation.
> > >
> > > I want to test if all predictors contribute significantly for the 
> > > fit or not I fit the full model, and get these results
> > >
> > >  > summary(HMMaturation.glmfit.Full)
> > >
> > > Call:
> > > glm(formula = Mature ~ LT + CondF + Biom + LT:CondF + LT:Biom,
> > >      family = binomial(link = "logit"), data = HMIndSamples)
> > >
> > > Deviance Residuals:
> > >      Min       1Q   Median       3Q      Max
> > > -3.0983  -0.7620   0.2540   0.7202   2.0292
> > >
> > > Coefficients:
> > >                Estimate Std. Error z value Pr(>|z|)
> > > (Intercept) -8.789e-01  3.694e-01  -2.379  0.01735 *
> > > LT           5.372e-02  1.798e-02   2.987  0.00281 **
> > > CondF       -6.763e-02  9.296e-03  -7.275 3.46e-13 ***
> > > Biom        -1.375e-02  2.005e-03  -6.856 7.07e-12 ***
> > > LT:CondF     2.434e-03  3.813e-04   6.383 1.74e-10 ***
> > > LT:Biom      7.833e-04  9.614e-05   8.148 3.71e-16 ***
> > > ---
> > > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> > >
> > > (Dispersion parameter for binomial family taken to be 1)
> > >
> > >      Null deviance: 10272.4  on 8224  degrees of freedom Residual 
> > > deviance:  7185.8  on 8219  degrees of freedom
> > > AIC: 7197.8
> > >
> > > Number of Fisher Scoring iterations: 8
> > >
> > > However, when I run anova on the fit, I get  > 
> > > anova(HMMaturation.glmfit.Full, test='Chisq') Analysis of 
> Deviance 
> > > Table
> > >
> > > Model: binomial, link: logit
> > >
> > > Response: Mature
> > >
> > > Terms added sequentially (first to last)
> > >
> > >
> > >             Df Deviance Resid. Df Resid. Dev P(>|Chi|)
> > > NULL                        8224    10272.4
> > > LT          1   2873.8      8223     7398.7       0.0
> > > CondF       1      0.1      8222     7398.5       0.7
> > > Biom        1      0.2      8221     7398.3       0.7
> > > LT:CondF    1    142.1      8220     7256.3 9.413e-33
> > > LT:Biom     1     70.4      8219     7185.8 4.763e-17
> > > Warning message:
> > > fitted probabilities numerically 0 or 1 occurred in: 
> method(x = x[, 
> > > varseq <= i, drop = FALSE], y = object$y, weights = 
> > > object$prior.weights,
> > >
> > >
> > > I am having a little difficulty interpreting these results.
> > > The result from the fit tells me that all predictors are 
> > > significant, while the anova indicates that besides LT (the main 
> > > variable), only the interaction of the other terms is 
> significant, 
> > > but the main effects are not.
> > > I believe that in the first output (on the glm object), the 
> > > significance of all terms is calculated considering each of them 
> > > alone in the model (i.e.
> > > removing all other terms), while the anova output is (as it says) 
> > > considering the sequential addition of the terms.
> > >
> > > So, there are 2 questions:
> > > a) Can I tell that the interactions are significant, but not the 
> > > main effects?
> >
> >In a model with this structure, the "main effects" represent slopes 
> >over the origin (i.e., where the other variables in the 
> product terms 
> >are 0), and aren't meaningfully interpreted as main effects. 
> (Is there 
> >even any data near the origin?)
> >
> > > b) Is it legitimate to consider a model where the 
> interactions are 
> > > considered, but not the main effects CondF and Biom?
> >
> >Generally, no: That is, such a model is interpretable, but it places 
> >strange constraints on the regression surface -- that the CondF and 
> >Biom slopes are 0 over the origin.
> >
> >None of this is specific to logistic regression -- it 
> applies generally 
> >to generalized linear models, including linear models.
> >
> >I hope this helps,
> >  John
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ivo_welch at mailblocks.com  Thu Nov 10 15:44:49 2005
From: ivo_welch at mailblocks.com (ivo welch)
Date: Thu, 10 Nov 2005 09:44:49 -0500
Subject: [R] Fonts, Plus
Message-ID: <43735CE1.6010103@mailblocks.com>


Dear R Wizards:

sorry, I need more help.  hopefully, it will help others in the future. 
I am using R 2.2.0 Patched (2005-11-07 r36217).


[a]

# copy from the postscriptFont documentation
CMitalic <- postscriptFont("ComputerModern",
               c("CM_regular_10.afm", "CM_boldx_10.afm",
                 "cmti10.afm", "cmbxti10.afm",
                 "CM_symbol_10.afm"))
postscriptFonts(CMitalic=CMitalic)

# trying this one out.  I copied the syntax that worked for lucida
pdf(file="test.pdf", fonts="CMitalic", version="1.4");
par(family="CMitalic");
plot( c(0,1),c(0,1) );
myeq <- bquote((w[I]==.(1/7)));
text( 0.5, 0.3, myeq );
text( 0.5, 0.7, "this is computer modern");
dev.off();

Now, pdffonts test.pdf (from the xpdf distribution) gives me

$ pdffonts test.pdf
name                                 type         emb sub uni object ID
------------------------------------ ------------ --- --- --- ---------
Error (4149): Dictionary key must be a name object
Error (4152): Dictionary key must be a name object
ZapfDingbats                         Type 1       no  no  no       5  0
Helvetica                            Type 1       no  no  no      10  0
Helvetica-Bold                       Type 1       no  no  no      11  0
Helvetica-Oblique                    Type 1       no  no  no      12  0
Helvetica-BoldOblique                Type 1       no  no  no      13  0
Symbol                               Type 1       no  no  no      14  0
CMR10                                Type 1       no  no  no      15  0
CMBX10                               Type 1       no  no  no      16  0
CMTI10                               Type 1       no  no  no      17  0
CMBXTI10                             Type 1       no  no  no      18  0
Error (4149): Dictionary key must be a name object
Error (4152): Dictionary key must be a name object
CMSY10                               Type 1       no  no  no      19  0

so, something is still wrong.

[b]

I am looking at the docs for postscriptFonts.  ?postscriptFonts.  May I
suggest that we add two or three more lines to show usage?  something
like "plot(c(0,1),c(0,1)); text(0.2, 0.5, "hello", font=2); dev.off()". 
More generally, a documented sample example file that shows usage of
many/multiple postscript fonts and families within one graph would be a
great help.

This is of course all just my own ignorance.  In general, I am not yet
sure about the whole font syntax.  I wonder what a font="something"
statement in the plot statement itself does.  I believe the
"par(family=)" changes the font used for the figure [e.g., axis labels],
although I am wondering why I am giving a string ["CMItalic"] rather
than a variable [CMItalic].  the ?text documentation does not have an
example of font selection, especially if I want to mix multiple fonts
and from different font families.


[c]

how do I tell a CMD BATCH not to execute the site file?  "R
--no-init-file" works only interactively.  ( Would it not make sense to
allow this options also for CMD BATCH?)  I probably have an incorrect
installation, because the suggestion from "R --help" fails for me

$ R CMD command --help
/usr/local/lib64/R/bin/Rcmd: line 45: exec: command: not found

However, R CMD BATCH my.R works just fine.

Further suggestion: let's have an abbreviation for --no-init-file, too;
e.g., "-I".


[d] regarding my earlier suggestion of a variable that contains the
currently executing file, I know I can put an argv0 <- "filename" into
each file, but it would be nice if this happened automatically and was
available everywhere.  just a suggestion...


please don't see the above as a complaint.  R is great, and the effort
you guys put in is terrific.  It's just that I am struggling with the
syntax here, and this one is not easy to figure out.

Regards,

/ivo



From JeeBee at troefpunt.nl  Thu Nov 10 15:40:53 2005
From: JeeBee at troefpunt.nl (JeeBee)
Date: Thu, 10 Nov 2005 15:40:53 +0100
Subject: [R] write.table read.table with Dates
References: <pan.2005.11.10.13.21.54.785817@troefpunt.nl>
Message-ID: <pan.2005.11.10.14.40.53.780299@troefpunt.nl>


I see that strptime returns a list of
year, mon, mday, hour, min, sec, etc.

The following works for me (for each column that is a date/time field
in my imported file)

cat("Converting date/time fields...\n")
Q = strptime(as.character(data$myfield), format="%Y-%m-%d%H:%M:%S")
data$myfield = ISOdatetime(year = Q$year + 1900,
               month = Q$mon + 1, day = Q$mday,
               hour =Q$hour, min = Q$min, sec = Q$sec, tz = "")

ISOdatetime does return a vector, which is, I guess, what I want.
It is quite slow like this though, and I don't think it's the best way.



From ripley at stats.ox.ac.uk  Thu Nov 10 15:51:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Nov 2005 14:51:08 +0000 (GMT)
Subject: [R] write.table read.table with Dates
In-Reply-To: <pan.2005.11.10.13.21.54.785817@troefpunt.nl>
References: <pan.2005.11.10.13.21.54.785817@troefpunt.nl>
Message-ID: <Pine.LNX.4.61.0511101445300.16385@gannet.stats>

On Thu, 10 Nov 2005, JeeBee wrote:

> I've found several similar issues with write.table/read.table
> with Dates on this list, but trying to follow this advice I still
> get an error.
>
> First, I read in data from several files, constructing several date/time
> columns using ISOdatetime
>
>> str(Tall$Begin)
> 'POSIXct', format: chr [1:40114] "2005-10-02 00:00:00" "2005-10-02
> 00:00:00" ...
>> length(Tall$Begin)
> [1] 40114
>> class(Tall$Begin)
> [1] "POSIXt"  "POSIXct"
>
> This looks good (time is not always 00:00:00 ...)
> This data came from several files, now I want to store the result I have
> in data.frame Tall and be able to retrieve this quickly some other time.
>
> This is what I do:
> write.table(Tall, file="somefile.csv", sep=",", qmethod="double",
> row.names=FALSE)
>
> Later, I do this to read the file again:
> fieldnames=c("Begin","test-a","test-b","Eind")
> T=read.table(file = "somefile.csv", col.names = fieldnames,
>  header = TRUE, sep = ",", quote="\"", fill=FALSE)

You can avoid all this trouble by using colClasses as documented on the 
help page.

> I understand T$Begin now is a factor. I tried to simply convert it
> again using (as I read on this mailinglist ...):
> Q = strptime(as.character(T$Begin),format="%Y-%m-%d %H:%M:%S")

Or just as.POSIXct(as.character(T$Begin))

> Q is looking good, though its length I don't understand .. is it a list or
> something? It seems there are 40114 values in there somewhere...

It is a list of length 9.  Try names(Q)

>> class(Q)
> [1] "POSIXt"  "POSIXlt"
>> length(Q)
> [1] 9
>> str(Q)
> 'POSIXlt', format: chr [1:40114] "2005-10-02 00:00:00" "2005-10-02 00:00:00" ...
>
> T$Begin = Q ### yields this error
> Error in "$<-.data.frame"(`*tmp*`, "Begin", value = list(sec = c(0, 0,  :
>        replacement has 9 rows, data has 40114
>
> Could somebody explain me how to convert the date column?
> Or perhaps there is an easier way?

You started with POSIXct, and you need to convert back to POSIXct
with as.POSIXct(Q).

Reading ?DateTimeClasses should explain to you what you are missing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Antje.Doering at komdat.com  Thu Nov 10 15:53:42 2005
From: Antje.Doering at komdat.com (=?iso-8859-1?Q?Antje_D=F6ring?=)
Date: Thu, 10 Nov 2005 15:53:42 +0100
Subject: [R] R-help: conversion of long decimal numbers into hexadecimal
Message-ID: <686C1FDE894539418C5668E5E6DE12DE050B68@muc-exch-tmp.komdat.intern>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/d3164aa6/attachment.pl

From huh at rti.org  Thu Nov 10 15:56:41 2005
From: huh at rti.org (Huh, Seungho)
Date: Thu, 10 Nov 2005 09:56:41 -0500
Subject: [R] question about the dataset fgl
Message-ID: <54535CBB16ABDA469D72C7614960429BCD5A9D@rtpwexc04.RCC_NT.RTI.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/102d397f/attachment.pl

From balaji.rajashekar at gmail.com  Thu Nov 10 16:07:08 2005
From: balaji.rajashekar at gmail.com (balaji)
Date: Thu, 10 Nov 2005 16:07:08 +0100
Subject: [R] estimating significance P-value between 2 matricesv
Message-ID: <aae8ee9e0511100707g175f78a5j6a0b518af4f9ccd7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/68202a4b/attachment.pl

From ron.ophir at weizmann.ac.il  Thu Nov 10 16:25:50 2005
From: ron.ophir at weizmann.ac.il (Ron Ophir)
Date: Thu, 10 Nov 2005 17:25:50 +0200
Subject: [R] different functions on different vector subsets
Message-ID: <s37382ac.089@wisemail.weizmann.ac.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/8f471093/attachment.pl

From illyese at freemail.hu  Thu Nov 10 16:49:56 2005
From: illyese at freemail.hu (Illyes Eszter)
Date: Thu, 10 Nov 2005 16:49:56 +0100 (CET)
Subject: [R] error in rowSums:'x' must be numeric
Message-ID: <freemail.20051010164956.40084@fm07.freemail.hu>

Dear All, 

It's Eszter again from Hungary. I could not solve my problem form 
yesterday, so I still have to ask your help.

I have a binary dataset of vegetation samples and species as a comma 
separated file. I would like to calculate the Jaccard distance of the 
dataset. I have the following error message: 

Error in rowSums(x, prod(dn), p, na.rm) : 'x' must be numeric
In addition: Warning message:
results may be meaningless because input data have negative entries
 in: vegdist(t2, method = "jaccard", binary = FALSE, diag = FALSE,  

Do you have any idea what can be the problem? I have only 0 and 1 in 
the dataset. 

Thank you very much! All the best:


Eszter


_______________________________________________________________________
KGFB 2006 - Garant??ltan a legjobb ??r! Nyerje meg az ??j Swiftet + 
garant??lt 10,000,-  Ft ??rt??k?? aj??nd??k. WWW.NETRISK.HU



From murdoch at stats.uwo.ca  Thu Nov 10 17:17:02 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 10 Nov 2005 11:17:02 -0500
Subject: [R] How to find statistics like that.
In-Reply-To: <Pine.GSO.4.60.0511100826500.28235@taxa.epi.umn.edu>
References: <03DCBBA079F2324786E8715BE538968A3DC5ED@FIGMAIL-CLUS01.FIG.FK>
	<Pine.GSO.4.60.0511100826500.28235@taxa.epi.umn.edu>
Message-ID: <4373727E.7090303@stats.uwo.ca>

On 11/10/2005 9:32 AM, Mike Miller wrote:
> On Thu, 10 Nov 2005, Ruben Roa wrote:
> 
> 
>>A statistic is any real-valued or vector-valued function whose
>>domain includes the sample space of a random sample. The
>>p-value is a real-valued function and its domain includes the
>>sample space of a random sample. The p-value has a sampling
>>distribution. The code below, found with Google ("sampling distribution
>>of the p-value" "R command") shows the sampling
>>distribution of the p-value for a t-test of a mean when the null hypothesis
>>is true.
>>Ruben
>>
>>n<-18
>>mu<-40
>>pop.var<-100
>>n.draw<-200
>>alpha<-0.05
>>draws<-matrix(rnorm(n.draw * n, mu, sqrt(pop.var)), n)
>>get.p.value<-function(x) t.test(x, mu = mu)$p.value
>>pvalues<-apply(draws, 2, get.p.value)
>>hist(pvalues)
>>sum(pvalues <= alpha)
>>[1] 6
> 
> 
> 
> The sampling distribution of a p-value when the null hypothesis is true 
> can be given more simply by this R code:
> 
> runif()
> 
> That holds for any valid test, not just a t test, that produces p-values 
> distributed continuously on [0,1].  Discrete distributions can't quite do 
> that without special tweaking.

Nor can most composite null hypotheses, e.g.

H0: mu <= 0 versus H1: mu > 0

A t-test may be an appropriate test, but its p-value is not uniformly 
distributed when mu is -1, even though the null is true.

Duncan Murdoch



From abunn at whrc.org  Thu Nov 10 17:19:26 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 10 Nov 2005 11:19:26 -0500
Subject: [R] ltext  - adding text to each panel from a matrix
Message-ID: <NEBBIPHDAMMOKDKPOFFIIEMFDMAA.abunn@whrc.org>

Hi all (really probably just Deepayan):

In the plot below I want to add text on either side of each violin plot that
indicates the number of observations that are either positive or negative.
I'm trying to do this with ltext() and I've also monkeyed about with
panel.text(). The code below is generally what I want but my calls to
ltext() are wrong and I'm not sure how to fix them. Right now they replicate
the first column of the matrices obs.pos and obs.neg for each panel. How do
I tell ltext to advance to the next column when the next panel is plotted? I
don't see how subscripts can do it, but I bet it's something along that
line...

Thanks, Andy

rm(list = ls())
set.seed(354)
# make a bimodal dataset with three groups and three treatments
foo <- c(rnorm(150,-1,0.5), rnorm(150,1,0.25))
treatment <- factor(rep(seq(1,3),100), labels = c("Treatment 1", "Treatment
2", "Treatment 3"))
group <- factor(rep(seq(1,3),100), labels = c("Group A", "Group B", "Group
C"))
group <- sample(group)
# corrupt Group A, Treatment 2 for fun.
foo[group=="Group A" & treatment=="Treatment 2"][1:8] <- rnorm(8,1,1)
dat <- data.frame(foo,treatment,group)

# set the limits for the plot, which also tells where to put the text
my.xlim <- c(-6, 6)

# make a matrix that counts the number of obs greater or less than zero
obs.pos <- tapply(dat[dat[,1] > 0,1], dat[dat[,1] > 0,-1], length)
obs.neg <- tapply(dat[dat[,1] <= 0,1], dat[dat[,1] <= 0,-1], length)
#write some coordinate data
x.obs.pos <- rep(my.xlim[2],dim(obs.pos)[2])
y.obs.pos <- 1:dim(obs.pos)[2]
x.obs.neg <- rep(my.xlim[1],dim(obs.neg)[2])
y.obs.neg <- 1:dim(obs.neg)[2]

bwplot(treatment~foo|group, data = dat,
       panel=function(...) {
           panel.violin(..., col = "transparent", varwidth = F)
           panel.abline(v=0, lty = "dotted")
           ltext(x.obs.pos, y.obs.pos, obs.pos, pos = 2)
           ltext(x.obs.neg, y.obs.neg, obs.neg, pos = 4)
       },
       par.strip.text = list(cex = 0.8),  xlim = my.xlim)
obs.pos
obs.neg

# note that the numbers in the plot only match the matrices for Group A,
# which is the first panel. Alas.



From Antje.Doering at komdat.com  Thu Nov 10 17:22:04 2005
From: Antje.Doering at komdat.com (=?iso-8859-1?Q?Antje_D=F6ring?=)
Date: Thu, 10 Nov 2005 17:22:04 +0100
Subject: [R] R-help: conversion of long decimal numbers into hexadecimal
	numbers
Message-ID: <686C1FDE894539418C5668E5E6DE12DE050B85@muc-exch-tmp.komdat.intern>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/f2d9f076/attachment.pl

From andy_liaw at merck.com  Thu Nov 10 17:27:21 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 10 Nov 2005 11:27:21 -0500
Subject: [R] question about the dataset fgl
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED58B@usctmx1106.merck.com>

`VR' is a bundle consisting of `MASS', `class', `nnet' and `spatial', as the
description says.  The fgl data is in the MASS package, so you need to load
that one.  In any case, data() would have told you that after the bundle is
installed.

Andy

> From: Huh, Seungho
> 
> Dear sir or ma'am,
> 
>  
> 
> I have a question about the dataset "fgl." The dataset seems to be in
> the "VR" package, so I tried to download it from CRAN. However, after
> downloading, when I tried to load the package, it was not in 
> my package
> list. I am wondering what is wrong.
> 
>  
> 
> Any advice on how to access the fgl dataset would be appreciated.
> Thanks. 
> 
>  
> 
> Seungho Huh
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From nayeemquayum at gmail.com  Thu Nov 10 17:51:10 2005
From: nayeemquayum at gmail.com (Nayeem Quayum)
Date: Thu, 10 Nov 2005 09:51:10 -0700
Subject: [R] Help regarding mas5 normalization
Message-ID: <af6d2ccc0511100851j21d22c6ra7a8d58bc6e5c372@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/18e37895/attachment.pl

From DAVID.BICKEL at pioneer.com  Thu Nov 10 17:51:06 2005
From: DAVID.BICKEL at pioneer.com (Bickel, David)
Date: Thu, 10 Nov 2005 10:51:06 -0600
Subject: [R] order statistics / sample quantiles
Message-ID: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2BA4@jhms08.phibred.com>

Are there any R functions or packages that can compute distributions,
expectations, or quantiles of order statistics (or sample quantiles or
extreme values) for a given distribution such as a normal distribution?
Both exact and asymptotic calculations are of interest. I am already
aware of the 'quantile' function of 'stats'.

David
_______________________________________
David R. Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International (DuPont)
Bioinformatics and Exploratory Research
7200 NW 62nd Ave.; PO Box 184
Johnston, IA 50131-0184
515-334-4739 Tel
515-334-4473 Fax
david.bickel at pioneer.com, bickel at prueba.info

This communication is for use by the intended recipient and ...{{dropped}}



From gunter.berton at gene.com  Thu Nov 10 18:00:18 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 10 Nov 2005 09:00:18 -0800
Subject: [R] different functions on different vector subsets
In-Reply-To: <s37382ac.089@wisemail.weizmann.ac.il>
Message-ID: <200511101700.jAAH0ITa024767@hertz.gene.com>

The error messages mean what they say.

> I am trying to apply two different functions on on a vector as follow:
> a<-c(NA,1,2,3,-3,-4,-6)
> if a>0 I would like to raise it by the power of 2: 2^a and if 
> the a<0 I
> would like to have the inverse value, i.e., -1/2^a.
## I assume you mean 1/(2^a). If not, modify the following appropriately.

2^(a*sign(a))  ## will do

As for your error message for:
> a[a<0]<-(-1)/2^a[a<0]

a<0 has an NA at the first index and so R doesn't know what index you want
to assign the value to. Ergo the error message.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ron Ophir
> Sent: Thursday, November 10, 2005 7:26 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] different functions on different vector subsets
> 
> Hi,
> I am trying to apply two different functions on on a vector as follow:
> a<-c(NA,1,2,3,-3,-4,-6)
> if a>0 I would like to raise it by the power of 2: 2^a and if 
> the a<0 I
> would like to have the inverse value, i.e., -1/2^a.
> so I thought of doing it two steps:
> a[a>0]<-2^[a>0]
> a[a<0]<-(-1)/2^a[a<0]
> I got the following error
> Error: NAs are not allowed in subscripted assignments
> any other ma>nupulation that I did with is.na() but did not succeed.
> What is funny that the two sides of the assignment work and return the
> same vector size:
> > 2^a[a>0]
> [1] NA  2  4  8
> > a[a>0]
> [1] NA  1  2  3
> 
> I found a solution in term of:
> sapply(a,function(x) if (is(s.na)) NA else if (x<0) (-1)/2^x else 2^x)
> but still I would like to understand why the solution above did not
> work. I think is more ellegant.
> my R version is:
> > sessionInfo()
> R version 2.2.0, 2005-10-06, i386-pc-mingw32 
>  
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"    
> "datasets" 
> [7] "base" 
> Thanks,
> Ron
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Thu Nov 10 18:04:49 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 10 Nov 2005 09:04:49 -0800 (PST)
Subject: [R] different functions on different vector subsets
In-Reply-To: <s37382ac.089@wisemail.weizmann.ac.il>
References: <s37382ac.089@wisemail.weizmann.ac.il>
Message-ID: <Pine.LNX.4.63a.0511100856280.10487@homer22.u.washington.edu>

On Thu, 10 Nov 2005, Ron Ophir wrote:

> Hi,
> I am trying to apply two different functions on on a vector as follow:
> a<-c(NA,1,2,3,-3,-4,-6)
> if a>0 I would like to raise it by the power of 2: 2^a and if the a<0 I
> would like to have the inverse value, i.e., -1/2^a.
> so I thought of doing it two steps:
> a[a>0]<-2^[a>0]
> a[a<0]<-(-1)/2^a[a<0]
> I got the following error
> Error: NAs are not allowed in subscripted assignments
> any other ma>nupulation that I did with is.na() but did not succeed.
> What is funny that the two sides of the assignment work and return the
> same vector size:
>> 2^a[a>0]
> [1] NA  2  4  8
>> a[a>0]
> [1] NA  1  2  3

The reason NAs are not allowed in subscripted assignments is based on 
numeric rather than logical subscripts.

For numeric subscripts the problem is ambiguity about what the NA index 
should do (we know there is ambiguity because two parts of the R code did 
different things).  For logical subscripts you could argue that the 
ambiguity isn't present and that if the index was NA the element should 
just be set to NA. This change might be worth making.


> I found a solution in term of:
> sapply(a,function(x) if (is(s.na)) NA else if (x<0) (-1)/2^x else 2^x)
> but still I would like to understand why the solution above did not
> work. I think is more ellegant.

A better general solution is

  a<-ifelse(a<0, -1/2^a, 2^a)

An alternative for this problem that is faster when a is very large is
  a<-sign(a)*2^abs(a)

 	-thomas



From gunter.berton at gene.com  Thu Nov 10 18:04:53 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 10 Nov 2005 09:04:53 -0800
Subject: [R] different functions on different vector subsets
Message-ID: <200511101704.jAAH4rad006209@compton.gene.com>


Oops. Sorry. Should be:

sign(a)*2^a

where I assume you meant the inverse value should be -1/2^|a| = - 2^a for
a<0

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: bgunter 
> Sent: Thursday, November 10, 2005 9:00 AM
> To: Ron Ophir; r-help at stat.math.ethz.ch
> Subject: RE: [R] different functions on different vector subsets
> 
> The error messages mean what they say.
> 
> > I am trying to apply two different functions on on a vector 
> as follow:
> > a<-c(NA,1,2,3,-3,-4,-6)
> > if a>0 I would like to raise it by the power of 2: 2^a and if 
> > the a<0 I
> > would like to have the inverse value, i.e., -1/2^a.
> ## I assume you mean 1/(2^a). If not, modify the following 
> appropriately.
> 
> 2^(a*sign(a))  ## will do
> 
> As for your error message for:
> > a[a<0]<-(-1)/2^a[a<0]
> 
> a<0 has an NA at the first index and so R doesn't know what 
> index you want to assign the value to. Ergo the error message.
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the 
> scientific learning process."  - George E. P. Box
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ron Ophir
> > Sent: Thursday, November 10, 2005 7:26 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] different functions on different vector subsets
> > 
> > Hi,
> > I am trying to apply two different functions on on a vector 
> as follow:
> > a<-c(NA,1,2,3,-3,-4,-6)
> > if a>0 I would like to raise it by the power of 2: 2^a and if 
> > the a<0 I
> > would like to have the inverse value, i.e., -1/2^a.
> > so I thought of doing it two steps:
> > a[a>0]<-2^[a>0]
> > a[a<0]<-(-1)/2^a[a<0]
> > I got the following error
> > Error: NAs are not allowed in subscripted assignments
> > any other ma>nupulation that I did with is.na() but did not succeed.
> > What is funny that the two sides of the assignment work and 
> return the
> > same vector size:
> > > 2^a[a>0]
> > [1] NA  2  4  8
> > > a[a>0]
> > [1] NA  1  2  3
> > 
> > I found a solution in term of:
> > sapply(a,function(x) if (is(s.na)) NA else if (x<0) 
> (-1)/2^x else 2^x)
> > but still I would like to understand why the solution above did not
> > work. I think is more ellegant.
> > my R version is:
> > > sessionInfo()
> > R version 2.2.0, 2005-10-06, i386-pc-mingw32 
> >  
> > attached base packages:
> > [1] "methods"   "stats"     "graphics"  "grDevices" "utils"    
> > "datasets" 
> > [7] "base" 
> > Thanks,
> > Ron
> >  
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
>



From kubovy at virginia.edu  Thu Nov 10 18:13:15 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 10 Nov 2005 12:13:15 -0500
Subject: [R] Concurrent structures
In-Reply-To: <4317B920.5070802@pdf.com>
References: <mailman.10.1125136801.20354.r-help@stat.math.ethz.ch>
	<C04F70B0-91E8-464B-82BA-1C18A3690D6B@virginia.edu>
	<4317B920.5070802@pdf.com>
Message-ID: <2F3E67DD-9892-458B-A78C-2953F2FF61C9@virginia.edu>

@BOOK{Mandel1995,
   title = {Analysis of two-way layouts},
   publisher = {Chapman \& Hall},
   year = {1995},
   address = {New York, NY, USA},
   author = {John Mandel},
}

describes diagnostics for identifying concurrent structures (i.e., y_ 
{ij} = A + B_{i} * C_{j} + e_{ij} ). Does anyone know of an  
implementation in R?


_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 10 18:28:11 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 10 Nov 2005 17:28:11 -0000 (GMT)
Subject: [R] R-help: conversion of long decimal numbers into hexadeci
In-Reply-To: <686C1FDE894539418C5668E5E6DE12DE050B68@muc-exch-tmp.komdat.intern>
Message-ID: <XFMail.051110172811.Ted.Harding@nessie.mcc.ac.uk>

On 10-Nov-05 Antje D??ring wrote:
> Hi there,
> 
> could somebody help me to convert a decimal number into a hexadecimal
> number? I know that there is the function "sprintf", but the numbers I
> want to convert consist of  20 or more numbers. "Spintf" is not able to
> convert these big numbers.

If I understand aright, you have decimal integers with 20 or more
digits (and you want to get these as hexadecimal).

You are probably out of luck for a direct approach, since
10^20 > 2^64 (indeed > 2^66), so you will have overflowed a 64-bit
integer. However, I'm not sure what the limitations on integer
types are in R on all platforms.

If, however, all you need is to do these conversions, and you
do not really need to use R (how off-topic can I get ... ?),
then (at any rate on Linux/Unix systems where the program is
installed by default) you can use the aribitrary-precision
calculator 'bc'.

Session:

$ bc
bc 1.06
Copyright 1991-1994, 1997, 1998, 2000 Free Software Foundation, Inc.
This is free software with ABSOLUTELY NO WARRANTY.
For details type `warranty'. 
obase=16

1234567898765432123456789
1056E0F555A18EBDA7D15

123456789876543212345678987654321
6163E6712EBBAA4E3D62B41F4B1

12345678987654321234567898765432123456789876543212345678987654321
1E02BC221DC9369C8981C6F859501BD313D339F09180862B41F4B1

quit


Und so weiter ... and of course you can go in the opposite
direction by "ibase=16" (to set hex as the input base) and
"obase=10" (to set decimal as the output base).

'bc' is a classic Unix tool, and features as an illoustration
of complex programming in C, with lex and yacc and all, in
"The Unix Programming Environment" (as I recall) by Kernighan
and Ritchie.

I don't need it often, but when you need it it's very handy
(e.g. now).

Hoping this helps,
Ted.

PS:

$ bc -l
bc 1.06
Copyright 1991-1994, 1997, 1998, 2000 Free Software Foundation, Inc.
This is free software with ABSOLUTELY NO WARRANTY.
For details type `warranty'. 
scale=1000
pi=4*a(1)
pi
3.141592653589793238462643383279502884197169399375105820974944592307\
81640628620899862803482534211706798214808651328230664709384460955058\
22317253594081284811174502841027019385211055596446229489549303819644\
28810975665933446128475648233786783165271201909145648566923460348610\
......................
08302642522308253344685035261931188171010003137838752886587533208381\
42061717766914730359825349042875546873115956286388235378759375195778\
18577805321712268066130019278766111959092164201988

(last digit wrong because of truncation)


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-Nov-05                                       Time: 17:28:05
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Thu Nov 10 18:32:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Nov 2005 17:32:45 +0000 (GMT)
Subject: [R] different functions on different vector subsets
In-Reply-To: <s37382ac.089@wisemail.weizmann.ac.il>
References: <s37382ac.089@wisemail.weizmann.ac.il>
Message-ID: <Pine.LNX.4.61.0511101725030.17764@gannet.stats>

On Thu, 10 Nov 2005, Ron Ophir wrote:

> Hi,
> I am trying to apply two different functions on on a vector as follow:
> a<-c(NA,1,2,3,-3,-4,-6)
> if a>0 I would like to raise it by the power of 2: 2^a and if the a<0 I
> would like to have the inverse value, i.e., -1/2^a.
> so I thought of doing it two steps:
> a[a>0]<-2^[a>0]
> a[a<0]<-(-1)/2^a[a<0]
> I got the following error
> Error: NAs are not allowed in subscripted assignments
> any other ma>nupulation that I did with is.na() but did not succeed.
> What is funny that the two sides of the assignment work and return the
> same vector size:
>> 2^a[a>0]
> [1] NA  2  4  8
>> a[a>0]
> [1] NA  1  2  3
>
> I found a solution in term of:
> sapply(a,function(x) if (is(s.na)) NA else if (x<0) (-1)/2^x else 2^x)
> but still I would like to understand why the solution above did not
> work. I think is more ellegant.

What do you think the NA value in

> a > 0
[1]    NA  TRUE  TRUE  TRUE FALSE FALSE FALSE

means?  Should you replace a[1] or not?  You are saying you don't know, so 
what is R to do?  It tells you to make up your mind.

Try

ind <- !is.na(a) & a > 0
a[ind] <- 2^a[ind]
ind <- !is.na(a) & a < 0
a[ind] <- (-1)/2^a[ind]

or use ifelse as in

ifelse(a > 0, 2^a, -1/2^a)

which is a lot more elegant.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maustin at amgen.com  Thu Nov 10 19:07:07 2005
From: maustin at amgen.com (Austin, Matt)
Date: Thu, 10 Nov 2005 10:07:07 -0800
Subject: [R] ltext  - adding text to each panel from a matrix
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD41B@teal-exch.amgen.com>

Haven't checked it too carefully, but how about:

bwplot(treatment~foo|group, data = dat,
       panel=function(x,y,...) {
           panel.violin(x,y, ..., col = "transparent", varwidth = F)
           gt0 <- table( x > 0, y)
           panel.abline(v=0, lty = "dotted")
           grid.text(as.character(gt0[1,]), unit(1, 'lines'), unit(1:3,
'native'), just='left')
           grid.text(as.character(gt0[2,]), unit(1, 'npc') - unit(1,
'lines'), unit(1:3, 'native'), just='left')
       },
       par.strip.text = list(cex = 0.8),  xlim = my.xlim)


--Matt

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Andy Bunn
> Sent: Thursday, November 10, 2005 8:19 AM
> To: R-Help
> Subject: [R] ltext - adding text to each panel from a matrix
> 
> 
> Hi all (really probably just Deepayan):
> 
> In the plot below I want to add text on either side of each 
> violin plot that
> indicates the number of observations that are either positive 
> or negative.
> I'm trying to do this with ltext() and I've also monkeyed about with
> panel.text(). The code below is generally what I want but my calls to
> ltext() are wrong and I'm not sure how to fix them. Right now 
> they replicate
> the first column of the matrices obs.pos and obs.neg for each 
> panel. How do
> I tell ltext to advance to the next column when the next 
> panel is plotted? I
> don't see how subscripts can do it, but I bet it's something 
> along that
> line...
> 
> Thanks, Andy
> 
> rm(list = ls())
> set.seed(354)
> # make a bimodal dataset with three groups and three treatments
> foo <- c(rnorm(150,-1,0.5), rnorm(150,1,0.25))
> treatment <- factor(rep(seq(1,3),100), labels = c("Treatment 
> 1", "Treatment
> 2", "Treatment 3"))
> group <- factor(rep(seq(1,3),100), labels = c("Group A", 
> "Group B", "Group
> C"))
> group <- sample(group)
> # corrupt Group A, Treatment 2 for fun.
> foo[group=="Group A" & treatment=="Treatment 2"][1:8] <- rnorm(8,1,1)
> dat <- data.frame(foo,treatment,group)
> 
> # set the limits for the plot, which also tells where to put the text
> my.xlim <- c(-6, 6)
> 
> # make a matrix that counts the number of obs greater or less 
> than zero
> obs.pos <- tapply(dat[dat[,1] > 0,1], dat[dat[,1] > 0,-1], length)
> obs.neg <- tapply(dat[dat[,1] <= 0,1], dat[dat[,1] <= 0,-1], length)
> #write some coordinate data
> x.obs.pos <- rep(my.xlim[2],dim(obs.pos)[2])
> y.obs.pos <- 1:dim(obs.pos)[2]
> x.obs.neg <- rep(my.xlim[1],dim(obs.neg)[2])
> y.obs.neg <- 1:dim(obs.neg)[2]
> 
> bwplot(treatment~foo|group, data = dat,
>        panel=function(...) {
>            panel.violin(..., col = "transparent", varwidth = F)
>            panel.abline(v=0, lty = "dotted")
>            ltext(x.obs.pos, y.obs.pos, obs.pos, pos = 2)
>            ltext(x.obs.neg, y.obs.neg, obs.neg, pos = 4)
>        },
>        par.strip.text = list(cex = 0.8),  xlim = my.xlim)
> obs.pos
> obs.neg
> 
> # note that the numbers in the plot only match the matrices 
> for Group A,
> # which is the first panel. Alas.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gunter.berton at gene.com  Thu Nov 10 19:17:56 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 10 Nov 2005 10:17:56 -0800
Subject: [R] specifying a key for a trellis display
Message-ID: <200511101818.jAAIHuGX006461@compton.gene.com>

Folks:

The "key" argument of trellis commands (e.g. xyplot()) allows one to place a
key at the top of a trellis display using 

key=list(space='top',...)

I would like to increase the space between the bottom of the key and the
trellis plots beyond the default. Is there a simple way to do this? At
present, I add an empty row (e.g. text = '', point colored in background
color) to the bottom of each key column. This seems a bit of a kludge. Is
there a slicker way to do it, i.e. a parameter that I have missed?

Cheers,
Bert

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From deepayan.sarkar at gmail.com  Thu Nov 10 19:28:57 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 10 Nov 2005 12:28:57 -0600
Subject: [R] ltext - adding text to each panel from a matrix
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIIEMFDMAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIIEMFDMAA.abunn@whrc.org>
Message-ID: <eb555e660511101028o12724c58ofccf99bc23b87055@mail.gmail.com>

On 11/10/05, Andy Bunn <abunn at whrc.org> wrote:
> Hi all (really probably just Deepayan):
>
> In the plot below I want to add text on either side of each violin plot
> that
> indicates the number of observations that are either positive or negative.
> I'm trying to do this with ltext() and I've also monkeyed about with
> panel.text().

They are the same (at least for now).

> The code below is generally what I want but my calls to
> ltext() are wrong and I'm not sure how to fix them. Right now they
> replicate
> the first column of the matrices obs.pos and obs.neg for each panel. How do
> I tell ltext to advance to the next column when the next panel is plotted?
> I
> don't see how subscripts can do it, but I bet it's something along that
> line...

Maybe, but there's a more direct solution (somewhat artificial perhaps):

bwplot(treatment~foo|group, data = dat,
       panel=function(..., packet.number) {
           panel.violin(..., col = "transparent", varwidth = F)
           panel.abline(v=0, lty = "dotted")
           ltext(x.obs.pos, y.obs.pos, obs.pos[, packet.number], pos = 2)
           ltext(x.obs.neg, y.obs.neg, obs.neg[, packet.number], pos = 4)
       },
       par.strip.text = list(cex = 0.8),  xlim = my.xlim)

This is documented under 'panel' in ?xyplot.

Deepayan



From macq at llnl.gov  Thu Nov 10 19:44:33 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 10 Nov 2005 10:44:33 -0800
Subject: [R] write.table read.table with Dates
In-Reply-To: <pan.2005.11.10.13.21.54.785817@troefpunt.nl>
References: <pan.2005.11.10.13.21.54.785817@troefpunt.nl>
Message-ID: <p06210203bf994464415d@[128.115.153.6]>

In addition to the solutions already provided, note that if *all* you 
want to do is save your dataframe in a file, and later recreate it 
from that file, you can use dump().

dump('Tall',file='Tall.r')
rm(Tall)  ## just to demonstrate that the next command will recreate Tall
source('Tall.r')

-Don

At 2:21 PM +0100 11/10/05, JeeBee wrote:
>I've found several similar issues with write.table/read.table
>with Dates on this list, but trying to follow this advice I still
>get an error.
>
>First, I read in data from several files, constructing several date/time
>columns using ISOdatetime
>
>>  str(Tall$Begin)
>'POSIXct', format: chr [1:40114] "2005-10-02 00:00:00" "2005-10-02
>00:00:00" ...
>>  length(Tall$Begin)
>[1] 40114
>>  class(Tall$Begin)
>[1] "POSIXt"  "POSIXct"
>
>This looks good (time is not always 00:00:00 ...)
>This data came from several files, now I want to store the result I have
>in data.frame Tall and be able to retrieve this quickly some other time.
>
>This is what I do:
>write.table(Tall, file="somefile.csv", sep=",", qmethod="double",
>row.names=FALSE)
>
>Later, I do this to read the file again:
>fieldnames=c("Begin","test-a","test-b","Eind")
>T=read.table(file = "somefile.csv", col.names = fieldnames,
>   header = TRUE, sep = ",", quote="\"", fill=FALSE)
>
>I understand T$Begin now is a factor. I tried to simply convert it
>again using (as I read on this mailinglist ...):
>Q = strptime(as.character(T$Begin),format="%Y-%m-%d %H:%M:%S")
>
>Q is looking good, though its length I don't understand .. is it a list or
>something? It seems there are 40114 values in there somewhere...
>
>>  class(Q)
>[1] "POSIXt"  "POSIXlt"
>>  length(Q)
>[1] 9
>>  str(Q)
>'POSIXlt', format: chr [1:40114] "2005-10-02 00:00:00" "2005-10-02 
>00:00:00" ...
>
>T$Begin = Q ### yields this error
>Error in "$<-.data.frame"(`*tmp*`, "Begin", value = list(sec = c(0, 0,  :
>         replacement has 9 rows, data has 40114
>
>Could somebody explain me how to convert the date column?
>Or perhaps there is an easier way?
>
>Thanks in advance for your time.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From tlumley at u.washington.edu  Thu Nov 10 20:28:02 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 10 Nov 2005 11:28:02 -0800 (PST)
Subject: [R] order statistics / sample quantiles
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2BA4@jhms08.phibred.com>
References: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2BA4@jhms08.phibred.com>
Message-ID: <Pine.LNX.4.63a.0511101121410.10487@homer22.u.washington.edu>

On Thu, 10 Nov 2005, Bickel, David wrote:

> Are there any R functions or packages that can compute distributions,
> expectations, or quantiles of order statistics (or sample quantiles or
> extreme values) for a given distribution such as a normal distribution?
> Both exact and asymptotic calculations are of interest. I am already
> aware of the 'quantile' function of 'stats'.
>

The density function of the order statistics is given as an example in the 
FAQ.  This can then be integrated with integrate() to give expectations.

 	-thomas



From deepayan.sarkar at gmail.com  Thu Nov 10 20:31:08 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 10 Nov 2005 13:31:08 -0600
Subject: [R] specifying a key for a trellis display
In-Reply-To: <200511101818.jAAIHuGX006461@compton.gene.com>
References: <200511101818.jAAIHuGX006461@compton.gene.com>
Message-ID: <eb555e660511101131k5ef1a1a5tbef52d8360a7f793@mail.gmail.com>

On 11/10/05, Berton Gunter <gunter.berton at gene.com> wrote:
> Folks:
>
> The "key" argument of trellis commands (e.g. xyplot()) allows one to place
> a
> key at the top of a trellis display using
>
> key=list(space='top',...)
>
> I would like to increase the space between the bottom of the key and the
> trellis plots beyond the default. Is there a simple way to do this? At
> present, I add an empty row (e.g. text = '', point colored in background
> color) to the bottom of each key column. This seems a bit of a kludge. Is
> there a slicker way to do it, i.e. a parameter that I have missed?

Yes, trellis.par.get("layout.heights"). e.g.

xyplot(1 ~ 1, key = list(text = list(letters[1:3]), points = list(col = 1:3)),
       par.settings = list(layout.heights = list(key.axis.padding = 5)))

-Deepayan



From gunter.berton at gene.com  Thu Nov 10 20:44:21 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 10 Nov 2005 11:44:21 -0800
Subject: [R] specifying a key for a trellis display
In-Reply-To: <eb555e660511101131k5ef1a1a5tbef52d8360a7f793@mail.gmail.com>
Message-ID: <200511101944.jAAJiL2D022302@volta.gene.com>

Many thanks, Deepayan. As I suspected... I'll fool around with these key
arguments to see what they do.

-- Bert
 

> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
> Sent: Thursday, November 10, 2005 11:31 AM
> To: Berton Gunter
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: specifying a key for a trellis display
> 
> On 11/10/05, Berton Gunter <gunter.berton at gene.com> wrote:
> > Folks:
> >
> > The "key" argument of trellis commands (e.g. xyplot()) 
> allows one to place
> > a
> > key at the top of a trellis display using
> >
> > key=list(space='top',...)
> >
> > I would like to increase the space between the bottom of 
> the key and the
> > trellis plots beyond the default. Is there a simple way to 
> do this? At
> > present, I add an empty row (e.g. text = '', point colored 
> in background
> > color) to the bottom of each key column. This seems a bit 
> of a kludge. Is
> > there a slicker way to do it, i.e. a parameter that I have missed?
> 
> Yes, trellis.par.get("layout.heights"). e.g.
> 
> xyplot(1 ~ 1, key = list(text = list(letters[1:3]), points = 
> list(col = 1:3)),
>        par.settings = list(layout.heights = 
> list(key.axis.padding = 5)))
> 
> -Deepayan
>



From ron.ophir at weizmann.ac.il  Thu Nov 10 21:04:21 2005
From: ron.ophir at weizmann.ac.il (Ron Ophir)
Date: Thu, 10 Nov 2005 22:04:21 +0200
Subject: [R] different functions on different vector subsets
Message-ID: <s373c3ee.080@wisemail.weizmann.ac.il>

Thanks Thomas,

"...For logical subscripts you could argue that the 
ambiguity isn't present and that if the index was NA the element should 
just be set to NA. This change might be worth making."

I see you got my point. NA should return NA no matter what the
comparison is. But any way thanks Brian, Jim, and Berton, I have leaned
a lot. It was a good practice.
Ron

Ron Ophir, Ph.D.
Bioinformatician,
Biological Services
Weizmann Institute of Science
POB 26
Rehovot 76100
Israel
e-mail: Ron.Ophir at weizmann.ac.il
Phone: 972-8-9342614
Fax:972-8-9344113
>>> Thomas Lumley <tlumley at u.washington.edu> 11/10/05 7:04 PM >>>
On Thu, 10 Nov 2005, Ron Ophir wrote:

> Hi,
> I am trying to apply two different functions on on a vector as follow:
> a<-c(NA,1,2,3,-3,-4,-6)
> if a>0 I would like to raise it by the power of 2: 2^a and if the a<0
I
> would like to have the inverse value, i.e., -1/2^a.
> so I thought of doing it two steps:
> a[a>0]<-2^[a>0]
> a[a<0]<-(-1)/2^a[a<0]
> I got the following error
> Error: NAs are not allowed in subscripted assignments
> any other ma>nupulation that I did with is.na() but did not succeed.
> What is funny that the two sides of the assignment work and return the
> same vector size:
>> 2^a[a>0]
> [1] NA  2  4  8
>> a[a>0]
> [1] NA  1  2  3

The reason NAs are not allowed in subscripted assignments is based on 
numeric rather than logical subscripts.

For numeric subscripts the problem is ambiguity about what the NA index 
should do (we know there is ambiguity because two parts of the R code
did 
different things).  For logical subscripts you could argue that the 
ambiguity isn't present and that if the index was NA the element should 
just be set to NA. This change might be worth making.


> I found a solution in term of:
> sapply(a,function(x) if (is(s.na)) NA else if (x<0) (-1)/2^x else 2^x)
> but still I would like to understand why the solution above did not
> work. I think is more ellegant.

A better general solution is

  a<-ifelse(a<0, -1/2^a, 2^a)

An alternative for this problem that is faster when a is very large is
  a<-sign(a)*2^abs(a)

 	-thomas



From ripley at stats.ox.ac.uk  Thu Nov 10 21:37:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Nov 2005 20:37:45 +0000 (GMT)
Subject: [R] different functions on different vector subsets
In-Reply-To: <Pine.LNX.4.63a.0511100856280.10487@homer22.u.washington.edu>
References: <s37382ac.089@wisemail.weizmann.ac.il>
	<Pine.LNX.4.63a.0511100856280.10487@homer22.u.washington.edu>
Message-ID: <Pine.LNX.4.61.0511101859210.24937@gannet.stats>

On Thu, 10 Nov 2005, Thomas Lumley wrote:

> On Thu, 10 Nov 2005, Ron Ophir wrote:
>
>> Hi,
>> I am trying to apply two different functions on on a vector as follow:
>> a<-c(NA,1,2,3,-3,-4,-6)
>> if a>0 I would like to raise it by the power of 2: 2^a and if the a<0 I
>> would like to have the inverse value, i.e., -1/2^a.
>> so I thought of doing it two steps:
>> a[a>0]<-2^[a>0]
>> a[a<0]<-(-1)/2^a[a<0]
>> I got the following error
>> Error: NAs are not allowed in subscripted assignments
>> any other ma>nupulation that I did with is.na() but did not succeed.
>> What is funny that the two sides of the assignment work and return the
>> same vector size:
>>> 2^a[a>0]
>> [1] NA  2  4  8
>>> a[a>0]
>> [1] NA  1  2  3
>
> The reason NAs are not allowed in subscripted assignments is based on
> numeric rather than logical subscripts.
>
> For numeric subscripts the problem is ambiguity about what the NA index
> should do (we know there is ambiguity because two parts of the R code did
> different things).  For logical subscripts you could argue that the
> ambiguity isn't present and that if the index was NA the element should
> just be set to NA. This change might be worth making.

That presumes NA is a valid value, but in general it is not. (Not for raw, 
not for lists, not for data frames, ....)  I don't think we want such 
inconsistent behaviour.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From zpzhang at uchicago.edu  Thu Nov 10 21:44:11 2005
From: zpzhang at uchicago.edu (Zepu Zhang)
Date: Thu, 10 Nov 2005 14:44:11 -0600
Subject: [R] grid.remove() doesn't remove output
Message-ID: <4373B11B.5050801@uchicago.edu>

I've found that grid.remove() doesn't clear the output when the grob is 
the only one on the device (or viewport; I didn't test it). For example:

library(grid)
grid.newpage()

grid.circle(name="cir", x=.5, y=.5, r=.3, gp=gpar(lwd=5))
grid.lines(c(.2, .8), c(.3, .7), name="lin")
grid.remove("cir")      # circle disappears
grid.remove("lin")      # object deleted, but line remains on output

If I now draw another primitive, "lin" disappears.

If I plotted only one thing, say "cir", in the first place,
grid.remove() won't clear the output.

I'm using R 2.2. Any pointer is appreciated.

Zepu



From tlumley at u.washington.edu  Thu Nov 10 22:02:13 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 10 Nov 2005 13:02:13 -0800 (PST)
Subject: [R] different functions on different vector subsets
In-Reply-To: <s373c3ee.081@wisemail.weizmann.ac.il>
References: <s373c3ee.081@wisemail.weizmann.ac.il>
Message-ID: <Pine.LNX.4.63a.0511101258540.10487@homer22.u.washington.edu>

On Thu, 10 Nov 2005, Ron Ophir wrote:

> Thanks Thomas,
>
> "...For logical subscripts you could argue that the
> ambiguity isn't present and that if the index was NA the element should
> just be set to NA. This change might be worth making."
>
> I see you got my point. NA should return NA no matter what the
> comparison is.

I'm not sure that I did get your point.  As Brian said, you aren't 
specifying whether or not to set the value. In your example it didn't 
matter because it would end up NA either way.

I was saying that for eg

a<-c(1,2,3,4)
b<-c(NA,T,F,T)

a[b]<-7

we could relax the prohibition on NA indexing to give c(NA,7,7,7) as the 
result. In your case that would give what you wanted, but in other cases 
it might not.


 	-thomas



From tlumley at u.washington.edu  Thu Nov 10 22:03:23 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 10 Nov 2005 13:03:23 -0800 (PST)
Subject: [R] different functions on different vector subsets
In-Reply-To: <Pine.LNX.4.63a.0511101258540.10487@homer22.u.washington.edu>
References: <s373c3ee.081@wisemail.weizmann.ac.il>
	<Pine.LNX.4.63a.0511101258540.10487@homer22.u.washington.edu>
Message-ID: <Pine.LNX.4.63a.0511101302510.10487@homer22.u.washington.edu>

On Thu, 10 Nov 2005, Thomas Lumley wrote:

> On Thu, 10 Nov 2005, Ron Ophir wrote:
>
>> Thanks Thomas,
>> 
>> "...For logical subscripts you could argue that the
>> ambiguity isn't present and that if the index was NA the element should
>> just be set to NA. This change might be worth making."
>> 
>> I see you got my point. NA should return NA no matter what the
>> comparison is.
>
> I'm not sure that I did get your point.  As Brian said, you aren't specifying 
> whether or not to set the value. In your example it didn't matter because it 
> would end up NA either way.
>

And as Brian later pointed out, this approach wouldn't work for things 
other than simple vectors.

 	-thomas



From andy_liaw at merck.com  Thu Nov 10 22:12:39 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 10 Nov 2005 16:12:39 -0500
Subject: [R] make check failed on linux-amd64 using PGI compilers
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED590@usctmx1106.merck.com>

Dear R-help,

I am trying to build R-2.2.0-patched (2005-11-07 r36217) on the head node of
a Scyld cluster (dual Opteron 250s) using PGI compilers (v6.0).  I used the
flags suggested by Jennifer Lai on R-devel (taken from R-admin, except that
I had to add -L/usr/X11R6/lib64 to LDFLAGS).  The build went fine, but make
check-all failed when running tests/Examples/graphics-Ex.R, at:

> plot(1:2, xaxs = "i") # 'inner-axis' w/o extra space
> stopifnot(par("xaxp")[1:2] == 1:2 &&
+           par("usr")[1:2] == 1:2)
Error: par("xaxp")]1:2] == 1:2 && par("usr")[1:2] == 1:2 is not TRUE

The above looks a bit strange to me, as running the R built as above,
par("xaxp")[1:2] - 1:2 gives

[1] -1.110223e-16 -2.220446e-16

Is my R build faulty?  I'd very much appreciate any advise.

Best,
Andy



From Cameron.Guenther at MyFWC.com  Thu Nov 10 22:36:07 2005
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Thu, 10 Nov 2005 16:36:07 -0500
Subject: [R]  IF/Else
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F304C8650@FWC-TLEX3.fwc.state.fl.us>

Hi,
I am trying to write a for loop with if else statements to calculate
biomass density estimates for different types of sampling gear.  
My code is:

bmd=for (i in 1:length(Gear)){
if (Gear==20) {bioden=Biomass/141}
else {if (Gear==23) {bioden=Biomass/68}}
else {if (Gear==160) {bioden=Biomass/4120}}
else {if (Gear==170) {bioden=Biomass/2210}}
else {if (Gear==300) {bioden=Biomass/(DIST_TOW*4*1853)}}
else {if (Gear==301) {bioden=Biomass/(DIST_TOW*4*1853)}}
}

The syntax that is returned is:

> bmd=for (i in 1:length(Gear)){
+ if (Gear==20) {bioden=Biomass/141}
+ else {if (Gear==23) {bioden=Biomass/68}}
+ else {if (Gear==160) {bioden=Biomass/4120}}
Error: syntax error in:
"else {if (Gear==23) {bioden=Biomass/68}}
else"
> else {if (Gear==170) {bioden=Biomass/2210}}
Error: syntax error in "else"
> else {if (Gear==300) {bioden=Biomass/(DIST_TOW*4*1853)}}
Error: syntax error in "else"
> else {if (Gear==301) {bioden=Biomass/(DIST_TOW*4*1853)}}
Error: syntax error in "else"
> }
Error: syntax error in "}"

It appears that the code works for the first two if/else statements and
then fails there after.  Any suggestions?

Cameron Guenther 
Associate Research Scientist
FWC/FWRI, Marine Fisheries Research
100 8th Avenue S.E.
St. Petersburg, FL 33701
(727)896-8626 Ext. 4305
cameron.guenther at myfwc.com



From alice.0309 at yahoo.com.tw  Thu Nov 10 22:38:05 2005
From: alice.0309 at yahoo.com.tw (alice.0309)
Date: Thu, 10 Nov 2005 13:38:05 -0800 (PST)
Subject: [R] How to export multiple files using write.table in the loop?
Message-ID: <20051110213805.75673.qmail@web17012.mail.tpe.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/511decf3/attachment.pl

From Charles.Annis at StatisticalEngineering.com  Thu Nov 10 22:46:16 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 10 Nov 2005 16:46:16 -0500
Subject: [R] converting a character string to a subscripted numeric variable
Message-ID: <200511102146.jAALkI6n016993@hypatia.math.ethz.ch>

Dear R-helpers:

It seems that I have a mental block.  (Some say that it sits atop my
shoulders.)

For reasons too tedious to retell I have an R object:

> input.line[7]
[1] "-13.24, -11.24, -9.24, -7.24, -5.24, -3.24, -1.24, 0.76, 2.76, 4.76,
6.76, 8.76, 10.76, 12.76, 14.76, 16.76, 18.76, 20.76, 22.76, 24.76, 26.76,
28.76, 30.76, 32.76, 34.76, 36.76, 38.76, 40.76, 42.76, 44.76, "
>

I would like to convert this into a subscripted variable, Beta, something
that should be straightforward if I had *almost* what I have.

I'd like to say

Beta <- c(-13.24, -11.24, -9.24, -7.24, -5.24, -3.24, -1.24, 0.76, 2.76,
4.76, 6.76, 8.76, 10.76, 12.76, 14.76, 16.76, 18.76, 20.76, 22.76, 24.76,
26.76, 28.76, 30.76, 32.76, 34.76, 36.76, 38.76, 40.76, 42.76, 44.76)

but I can't because:

input.line[7] is a character string, and

it ends in a comma.


This cannot be as difficult as I have found it to be.  Can anyone help?

Copious Thanks.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:?? 614-455-3265
http://www.StatisticalEngineering.com
??



From sundar.dorai-raj at pdf.com  Thu Nov 10 23:00:21 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 10 Nov 2005 16:00:21 -0600
Subject: [R] converting a character string to a subscripted numeric
	variable
In-Reply-To: <200511102146.jAALkI6n016993@hypatia.math.ethz.ch>
References: <200511102146.jAALkI6n016993@hypatia.math.ethz.ch>
Message-ID: <4373C2F5.1030903@pdf.com>



Charles Annis, P.E. wrote:
> Dear R-helpers:
> 
> It seems that I have a mental block.  (Some say that it sits atop my
> shoulders.)
> 
> For reasons too tedious to retell I have an R object:
> 
> 
>>input.line[7]
> 
> [1] "-13.24, -11.24, -9.24, -7.24, -5.24, -3.24, -1.24, 0.76, 2.76, 4.76,
> 6.76, 8.76, 10.76, 12.76, 14.76, 16.76, 18.76, 20.76, 22.76, 24.76, 26.76,
> 28.76, 30.76, 32.76, 34.76, 36.76, 38.76, 40.76, 42.76, 44.76, "
> 
> 
> I would like to convert this into a subscripted variable, Beta, something
> that should be straightforward if I had *almost* what I have.
> 
> I'd like to say
> 
> Beta <- c(-13.24, -11.24, -9.24, -7.24, -5.24, -3.24, -1.24, 0.76, 2.76,
> 4.76, 6.76, 8.76, 10.76, 12.76, 14.76, 16.76, 18.76, 20.76, 22.76, 24.76,
> 26.76, 28.76, 30.76, 32.76, 34.76, 36.76, 38.76, 40.76, 42.76, 44.76)
> 
> but I can't because:
> 
> input.line[7] is a character string, and
> 
> it ends in a comma.
> 
> 
> This cannot be as difficult as I have found it to be.  Can anyone help?
> 
> Copious Thanks.

How about:

x <- "-13.24, -11.24, "
x <- as.numeric(strsplit(x, ",")[[1]])
x[!is.na(x)]

--sundar



From Charles.Annis at StatisticalEngineering.com  Thu Nov 10 23:10:40 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 10 Nov 2005 17:10:40 -0500
Subject: [R] converting a character string to a subscripted numeric
	variable
In-Reply-To: <4373C2F5.1030903@pdf.com>
Message-ID: <200511102210.jAAMAfpq023429@hypatia.math.ethz.ch>

Eternal gratitude to Sunbar and Matt and Patrick!

The easy solution is

Beta <- as.numeric(strsplit(input.line [7], ",")[[1]])
Beta <- Beta[!is.na(Beta)]
Beta

I have a slew of files to interrogate and need to know from some of the
input, what to look for in the remainder of the input.

Thanks to all!


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com] 
Sent: Thursday, November 10, 2005 5:00 PM
To: Charles.Annis at statisticalengineering.com
Cc: R-help at r-project.org
Subject: Re: [R] converting a character string to a subscripted numeric
variable



Charles Annis, P.E. wrote:
> Dear R-helpers:
> 
> It seems that I have a mental block.  (Some say that it sits atop my
> shoulders.)
> 
> For reasons too tedious to retell I have an R object:
> 
> 
>>input.line[7]
> 
> [1] "-13.24, -11.24, -9.24, -7.24, -5.24, -3.24, -1.24, 0.76, 2.76, 4.76,
> 6.76, 8.76, 10.76, 12.76, 14.76, 16.76, 18.76, 20.76, 22.76, 24.76, 26.76,
> 28.76, 30.76, 32.76, 34.76, 36.76, 38.76, 40.76, 42.76, 44.76, "
> 
> 
> I would like to convert this into a subscripted variable, Beta, something
> that should be straightforward if I had *almost* what I have.
> 
> I'd like to say
> 
> Beta <- c(-13.24, -11.24, -9.24, -7.24, -5.24, -3.24, -1.24, 0.76, 2.76,
> 4.76, 6.76, 8.76, 10.76, 12.76, 14.76, 16.76, 18.76, 20.76, 22.76, 24.76,
> 26.76, 28.76, 30.76, 32.76, 34.76, 36.76, 38.76, 40.76, 42.76, 44.76)
> 
> but I can't because:
> 
> input.line[7] is a character string, and
> 
> it ends in a comma.
> 
> 
> This cannot be as difficult as I have found it to be.  Can anyone help?
> 
> Copious Thanks.

How about:

x <- "-13.24, -11.24, "
x <- as.numeric(strsplit(x, ",")[[1]])
x[!is.na(x)]

--sundar



From p.dalgaard at biostat.ku.dk  Thu Nov 10 23:11:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Nov 2005 23:11:46 +0100
Subject: [R] make check failed on linux-amd64 using PGI compilers
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED590@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED590@usctmx1106.merck.com>
Message-ID: <x2mzkcjgpp.fsf@turmalin.kubism.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> Dear R-help,
> 
> I am trying to build R-2.2.0-patched (2005-11-07 r36217) on the head node of
> a Scyld cluster (dual Opteron 250s) using PGI compilers (v6.0).  I used the
> flags suggested by Jennifer Lai on R-devel (taken from R-admin, except that
> I had to add -L/usr/X11R6/lib64 to LDFLAGS).  The build went fine, but make
> check-all failed when running tests/Examples/graphics-Ex.R, at:
> 
> > plot(1:2, xaxs = "i") # 'inner-axis' w/o extra space
> > stopifnot(par("xaxp")[1:2] == 1:2 &&
> +           par("usr")[1:2] == 1:2)
> Error: par("xaxp")]1:2] == 1:2 && par("usr")[1:2] == 1:2 is not TRUE
> 
> The above looks a bit strange to me, as running the R built as above,
> par("xaxp")[1:2] - 1:2 gives
> 
> [1] -1.110223e-16 -2.220446e-16
> 
> Is my R build faulty?  I'd very much appreciate any advise.

Relative errors of 1 ULP would not ruin my sleep.  It's not obvious to
me which is the floating point operation responsible for the effect,
though, and it is a bit suspicious given that it should just be the
x-range of the data.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Thu Nov 10 23:23:44 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Nov 2005 23:23:44 +0100
Subject: [R] converting a character string to a subscripted numeric
	variable
In-Reply-To: <200511102210.jAAMAfpq023429@hypatia.math.ethz.ch>
References: <200511102210.jAAMAfpq023429@hypatia.math.ethz.ch>
Message-ID: <x2irv0jg5r.fsf@turmalin.kubism.ku.dk>

"Charles Annis, P.E." <Charles.Annis at statisticalengineering.com> writes:

> Eternal gratitude to Sunbar and Matt and Patrick!
> 
> The easy solution is
> 
> Beta <- as.numeric(strsplit(input.line [7], ",")[[1]])
> Beta <- Beta[!is.na(Beta)]
> Beta

Also

x <- input.line [7]

eval(parse(text=paste("c(", x, ")")))
 
or 

x <- sub(", *$","",x)
scan(textConnection(x), sep=",")


> I have a slew of files to interrogate and need to know from some of the
> input, what to look for in the remainder of the input.
> 
> Thanks to all!
> 
> 
> Charles Annis, P.E.
> 
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>  
> -----Original Message-----
> From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com] 
> Sent: Thursday, November 10, 2005 5:00 PM
> To: Charles.Annis at statisticalengineering.com
> Cc: R-help at r-project.org
> Subject: Re: [R] converting a character string to a subscripted numeric
> variable
> 
> 
> 
> Charles Annis, P.E. wrote:
> > Dear R-helpers:
> > 
> > It seems that I have a mental block.  (Some say that it sits atop my
> > shoulders.)
> > 
> > For reasons too tedious to retell I have an R object:
> > 
> > 
> >>input.line[7]
> > 
> > [1] "-13.24, -11.24, -9.24, -7.24, -5.24, -3.24, -1.24, 0.76, 2.76, 4.76,
> > 6.76, 8.76, 10.76, 12.76, 14.76, 16.76, 18.76, 20.76, 22.76, 24.76, 26.76,
> > 28.76, 30.76, 32.76, 34.76, 36.76, 38.76, 40.76, 42.76, 44.76, "
> > 
> > 
> > I would like to convert this into a subscripted variable, Beta, something
> > that should be straightforward if I had *almost* what I have.
> > 
> > I'd like to say
> > 
> > Beta <- c(-13.24, -11.24, -9.24, -7.24, -5.24, -3.24, -1.24, 0.76, 2.76,
> > 4.76, 6.76, 8.76, 10.76, 12.76, 14.76, 16.76, 18.76, 20.76, 22.76, 24.76,
> > 26.76, 28.76, 30.76, 32.76, 34.76, 36.76, 38.76, 40.76, 42.76, 44.76)
> > 
> > but I can't because:
> > 
> > input.line[7] is a character string, and
> > 
> > it ends in a comma.
> > 
> > 
> > This cannot be as difficult as I have found it to be.  Can anyone help?
> > 
> > Copious Thanks.
> 
> How about:
> 
> x <- "-13.24, -11.24, "
> x <- as.numeric(strsplit(x, ",")[[1]])
> x[!is.na(x)]
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 10 23:45:05 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 10 Nov 2005 22:45:05 -0000 (GMT)
Subject: [R] IF/Else
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F304C8650@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <XFMail.051110224505.Ted.Harding@nessie.mcc.ac.uk>


On 10-Nov-05 Guenther, Cameron wrote:
> Hi,
> I am trying to write a for loop with if else statements to calculate
> biomass density estimates for different types of sampling gear.  
> My code is:
> 
> bmd=for (i in 1:length(Gear)){
> if (Gear==20) {bioden=Biomass/141}
> else {if (Gear==23) {bioden=Biomass/68}}
> else {if (Gear==160) {bioden=Biomass/4120}}
> else {if (Gear==170) {bioden=Biomass/2210}}
> else {if (Gear==300) {bioden=Biomass/(DIST_TOW*4*1853)}}
> else {if (Gear==301) {bioden=Biomass/(DIST_TOW*4*1853)}}
> }

In the above, you have in effect written a string of "else"s
following a single if, since the {if(Gear==160){...}} isolates
its "if" from the preceding else, etc.. But there are other
things wrong.

The following should work:

  bmd=for (i in 1:length(Gear)) {
   if (Gear==20) {bioden=Biomass/141}  else 
    if (Gear==23) {bioden=Biomass/68}   else 
     if (Gear==160) {bioden=Biomass/4120}  else 
      if (Gear==170) {bioden=Biomass/2210}  else 
       if (Gear==300) {bioden=Biomass/(DIST_TOW*4*1853)}  else 
        if (Gear==301) {bioden=Biomass/(DIST_TOW*4*1853)}
  }

where I have removed superfluous "{" on the left and "}" on the
right.

Putting the "else" at the end of the line, rather than on the
next, is needed since otherwise the line is a completed
statement befor the "else" is encountered and then the "else"
is a syntax error. With "else" at the end of the line, the
end of line is reached on an incomplete statement so R goes
on to the next line before making up its mind about it.

I.e. Good:

  A<-2
  if(A==1){B<-1} else
    if(A==2){B<-2} else
       if(A==3){B<-3}

while Bad:
  A<-2
  if(A==1){B<-1}
  else if(A==2){B<-2}
       else if(A==3){B<-3}

Also Bad:
  A<-2
  if(A==1){B<-1} else
    {if(A==2){B<-2}} else
       {if(A==3){B<-3}}

since each "{if(A==2){B<-2}}" is a complete statement delimited
by "{...}" and the "else" is then again a syntax error, The final
statement is OK, however.

Mind you, if the cases in your code are the only possibilites
for Gear, then you don't need the final "if" at all -- just the
assignment, since if you get that far it's the only case left,
and you'll only reach it by falling through the preceding "else".

Indeed, you could even dispense with all of the "else"s and
just use "if" statements, since only one of them will be
satisfied; but of course that is a bit inefficient in that
every statement will be tested even if one has already been
satisfied.

Hoping this helps,
Ted.

> The syntax that is returned is:
> 
>> bmd=for (i in 1:length(Gear)){
> + if (Gear==20) {bioden=Biomass/141}
> + else {if (Gear==23) {bioden=Biomass/68}}
> + else {if (Gear==160) {bioden=Biomass/4120}}
> Error: syntax error in:
> "else {if (Gear==23) {bioden=Biomass/68}}
> else"
>> else {if (Gear==170) {bioden=Biomass/2210}}
> Error: syntax error in "else"
>> else {if (Gear==300) {bioden=Biomass/(DIST_TOW*4*1853)}}
> Error: syntax error in "else"
>> else {if (Gear==301) {bioden=Biomass/(DIST_TOW*4*1853)}}
> Error: syntax error in "else"
>> }
> Error: syntax error in "}"
> 
> It appears that the code works for the first two if/else statements and
> then fails there after.  Any suggestions?

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-Nov-05                                       Time: 22:44:59
------------------------------ XFMail ------------------------------



From andy_liaw at merck.com  Thu Nov 10 23:46:17 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 10 Nov 2005 17:46:17 -0500
Subject: [R] make check failed on linux-amd64 using PGI compilers
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED593@usctmx1106.merck.com>

> From: pd at pubhealth.ku.dk
> 
> "Liaw, Andy" <andy_liaw at merck.com> writes:
> 
> > Dear R-help,
> > 
> > I am trying to build R-2.2.0-patched (2005-11-07 r36217) on 
> the head node of
> > a Scyld cluster (dual Opteron 250s) using PGI compilers 
> (v6.0).  I used the
> > flags suggested by Jennifer Lai on R-devel (taken from 
> R-admin, except that
> > I had to add -L/usr/X11R6/lib64 to LDFLAGS).  The build 
> went fine, but make
> > check-all failed when running tests/Examples/graphics-Ex.R, at:
> > 
> > > plot(1:2, xaxs = "i") # 'inner-axis' w/o extra space
> > > stopifnot(par("xaxp")[1:2] == 1:2 &&
> > +           par("usr")[1:2] == 1:2)
> > Error: par("xaxp")]1:2] == 1:2 && par("usr")[1:2] == 1:2 is not TRUE
> > 
> > The above looks a bit strange to me, as running the R built 
> as above,
> > par("xaxp")[1:2] - 1:2 gives
> > 
> > [1] -1.110223e-16 -2.220446e-16
> > 
> > Is my R build faulty?  I'd very much appreciate any advise.
> 
> Relative errors of 1 ULP would not ruin my sleep.  It's not obvious to
> me which is the floating point operation responsible for the effect,
> though, and it is a bit suspicious given that it should just be the
> x-range of the data.

It's the par() output that has storage mode "double", as that stores the
coordinates.  1:2 has storage mode "integer".

Those comparisons looked a bit suspicious to me.  Isn't it recommended time
and again on this list not to test for exact equality of numerics?
all.equal() of the above does return TRUE.

Andy

 
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
> (+45) 35327907
> 
>



From pbarros at ualg.pt  Fri Nov 11 00:47:14 2005
From: pbarros at ualg.pt (Pedro de Barros)
Date: Thu, 10 Nov 2005 23:47:14 +0000
Subject: [R] Interpretation of output from glm
In-Reply-To: <20051110144155.JKZJ25800.tomts13-srv.bellnexxia.net@JohnDe
	sktop8300>
References: <6.1.2.0.2.20051109154310.0240a708@pop.ualg.pt>
	<20051110144155.JKZJ25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <6.1.2.0.2.20051110234557.023e1ec0@pop.ualg.pt>

Dear John,

Thanks for the pointers. I will read this.

Pedro
At 14:41 10/11/2005, you wrote:
>Dear Pedro,
>
>The basic point, which relates to the principle of marginality in
>formulating linear models, applies whether the predictors are factors,
>covariates, or both. I think that this is a common topic in books on linear
>models; I certainly discuss it in my Applied Regression, Linear Models, and
>Related Methods.
>
>Regards,
>  John
>
>--------------------------------
>John Fox
>Department of Sociology
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>905-525-9140x23604
>http://socserv.mcmaster.ca/jfox
>--------------------------------
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pedro de Barros
> > Sent: Wednesday, November 09, 2005 10:45 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Interpretation of output from glm
> > Importance: High
> >
> > Dear John,
> >
> > Thanks for the quick reply. I did indeed have these ideas,
> > but somehow "floating", and all I could find about this
> > mentioned categorical predictors. Can you suggest a good book
> > where I could try to learn more about this?
> >
> > Thanks again,
> >
> > Pedro
> > At 01:49 09/11/2005, you wrote:
> > >Dear Pedro,
> > >
> > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pedro de
> > > > Barros
> > > > Sent: Tuesday, November 08, 2005 9:47 AM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: [R] Interpretation of output from glm
> > > > Importance: High
> > > >
> > > > I am fitting a logistic model to binary data. The
> > response variable
> > > > is a factor (0 or 1) and all predictors are continuous variables.
> > > > The main predictor is LT (I expect a logistic relation between LT
> > > > and the probability of being
> > > > mature) and the other are variables I expect to modify
> > this relation.
> > > >
> > > > I want to test if all predictors contribute significantly for the
> > > > fit or not I fit the full model, and get these results
> > > >
> > > >  > summary(HMMaturation.glmfit.Full)
> > > >
> > > > Call:
> > > > glm(formula = Mature ~ LT + CondF + Biom + LT:CondF + LT:Biom,
> > > >      family = binomial(link = "logit"), data = HMIndSamples)
> > > >
> > > > Deviance Residuals:
> > > >      Min       1Q   Median       3Q      Max
> > > > -3.0983  -0.7620   0.2540   0.7202   2.0292
> > > >
> > > > Coefficients:
> > > >                Estimate Std. Error z value Pr(>|z|)
> > > > (Intercept) -8.789e-01  3.694e-01  -2.379  0.01735 *
> > > > LT           5.372e-02  1.798e-02   2.987  0.00281 **
> > > > CondF       -6.763e-02  9.296e-03  -7.275 3.46e-13 ***
> > > > Biom        -1.375e-02  2.005e-03  -6.856 7.07e-12 ***
> > > > LT:CondF     2.434e-03  3.813e-04   6.383 1.74e-10 ***
> > > > LT:Biom      7.833e-04  9.614e-05   8.148 3.71e-16 ***
> > > > ---
> > > > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> > > >
> > > > (Dispersion parameter for binomial family taken to be 1)
> > > >
> > > >      Null deviance: 10272.4  on 8224  degrees of freedom Residual
> > > > deviance:  7185.8  on 8219  degrees of freedom
> > > > AIC: 7197.8
> > > >
> > > > Number of Fisher Scoring iterations: 8
> > > >
> > > > However, when I run anova on the fit, I get  >
> > > > anova(HMMaturation.glmfit.Full, test='Chisq') Analysis of
> > Deviance
> > > > Table
> > > >
> > > > Model: binomial, link: logit
> > > >
> > > > Response: Mature
> > > >
> > > > Terms added sequentially (first to last)
> > > >
> > > >
> > > >             Df Deviance Resid. Df Resid. Dev P(>|Chi|)
> > > > NULL                        8224    10272.4
> > > > LT          1   2873.8      8223     7398.7       0.0
> > > > CondF       1      0.1      8222     7398.5       0.7
> > > > Biom        1      0.2      8221     7398.3       0.7
> > > > LT:CondF    1    142.1      8220     7256.3 9.413e-33
> > > > LT:Biom     1     70.4      8219     7185.8 4.763e-17
> > > > Warning message:
> > > > fitted probabilities numerically 0 or 1 occurred in:
> > method(x = x[,
> > > > varseq <= i, drop = FALSE], y = object$y, weights =
> > > > object$prior.weights,
> > > >
> > > >
> > > > I am having a little difficulty interpreting these results.
> > > > The result from the fit tells me that all predictors are
> > > > significant, while the anova indicates that besides LT (the main
> > > > variable), only the interaction of the other terms is
> > significant,
> > > > but the main effects are not.
> > > > I believe that in the first output (on the glm object), the
> > > > significance of all terms is calculated considering each of them
> > > > alone in the model (i.e.
> > > > removing all other terms), while the anova output is (as it says)
> > > > considering the sequential addition of the terms.
> > > >
> > > > So, there are 2 questions:
> > > > a) Can I tell that the interactions are significant, but not the
> > > > main effects?
> > >
> > >In a model with this structure, the "main effects" represent slopes
> > >over the origin (i.e., where the other variables in the
> > product terms
> > >are 0), and aren't meaningfully interpreted as main effects.
> > (Is there
> > >even any data near the origin?)
> > >
> > > > b) Is it legitimate to consider a model where the
> > interactions are
> > > > considered, but not the main effects CondF and Biom?
> > >
> > >Generally, no: That is, such a model is interpretable, but it places
> > >strange constraints on the regression surface -- that the CondF and
> > >Biom slopes are 0 over the origin.
> > >
> > >None of this is specific to logistic regression -- it
> > applies generally
> > >to generalized linear models, including linear models.
> > >
> > >I hope this helps,
> > >  John
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Fri Nov 11 01:00:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Nov 2005 19:00:32 -0500
Subject: [R] IF/Else
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F304C8650@FWC-TLEX3.fwc.state.fl.us>
References: <BA6FF017E924044A9BF748AFAEEA6F304C8650@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <971536df0511101600l61b24894sce628ede271654ce@mail.gmail.com>

Try using switch:

bioden <- Biomass /
  switch(paste(Gear),
    `20` = 141,
    `23` = 68,
    # ... fill in the ones I have omitted ...,
    `301` = DIST_TOW*4*1853)


On 11/10/05, Guenther, Cameron <Cameron.Guenther at myfwc.com> wrote:
> Hi,
> I am trying to write a for loop with if else statements to calculate
> biomass density estimates for different types of sampling gear.
> My code is:
>
> bmd=for (i in 1:length(Gear)){
> if (Gear==20) {bioden=Biomass/141}
> else {if (Gear==23) {bioden=Biomass/68}}
> else {if (Gear==160) {bioden=Biomass/4120}}
> else {if (Gear==170) {bioden=Biomass/2210}}
> else {if (Gear==300) {bioden=Biomass/(DIST_TOW*4*1853)}}
> else {if (Gear==301) {bioden=Biomass/(DIST_TOW*4*1853)}}
> }
>
> The syntax that is returned is:
>
> > bmd=for (i in 1:length(Gear)){
> + if (Gear==20) {bioden=Biomass/141}
> + else {if (Gear==23) {bioden=Biomass/68}}
> + else {if (Gear==160) {bioden=Biomass/4120}}
> Error: syntax error in:
> "else {if (Gear==23) {bioden=Biomass/68}}
> else"
> > else {if (Gear==170) {bioden=Biomass/2210}}
> Error: syntax error in "else"
> > else {if (Gear==300) {bioden=Biomass/(DIST_TOW*4*1853)}}
> Error: syntax error in "else"
> > else {if (Gear==301) {bioden=Biomass/(DIST_TOW*4*1853)}}
> Error: syntax error in "else"
> > }
> Error: syntax error in "}"
>
> It appears that the code works for the first two if/else statements and
> then fails there after.  Any suggestions?
>
> Cameron Guenther
> Associate Research Scientist
> FWC/FWRI, Marine Fisheries Research
> 100 8th Avenue S.E.
> St. Petersburg, FL 33701
> (727)896-8626 Ext. 4305
> cameron.guenther at myfwc.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Fri Nov 11 01:10:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Nov 2005 19:10:01 -0500
Subject: [R] How to export multiple files using write.table in the loop?
In-Reply-To: <20051110213805.75673.qmail@web17012.mail.tpe.yahoo.com>
References: <20051110213805.75673.qmail@web17012.mail.tpe.yahoo.com>
Message-ID: <971536df0511101610v663dcea1gd186489bfba9e24d@mail.gmail.com>

Try this:

col.list <- list(1:3, c(1,4:5))
for(cols in col.list) write.table(data1[,cols], ...whatever...)


On 11/10/05, alice.0309 <alice.0309 at yahoo.com.tw> wrote:
> Hi,
>
> I tried to split a big file into some small files seperately by R.  I can only do that writing duplicated codes.  When I tried to write a loop, I only got one appned or destroyed exported file.   For example:
>
> data1<- read.table(file = "C:\\Alice\\MBEI.txt", sep="\t", check.names=FALSE)
> a<-subset(data1,select=c(V1,V2,V3))
> write.table<-(a,file="C:\\Alice\\aa1.txt",quote=FALSE,row.names=FALSE,col.name=FALSE,sep="\t")
> a<-subset(data1,select=c(V1,V4,V5))
> write.table(a,file="C:\\Alice\\aa2.txt",quote=FALSE,row.names=FALSE,col.name=FALSE,sep="\t")
>
> or
> data1<- read.table(file = "C:\\Alice\\MBEI.txt", sep="\t", check.names=FALSE)
> a<-data.frame(data1[,1],data1[,2],data1[,3]
> write.table<-(a,file="C:\\Alice\\aa1.txt",quote=FALSE,row.names=FALSE,col.name=FALSE,sep="\t")
> a<-data.frame(data1[,1],data1[,4],data1[,5]
> write.table(a,file="C:\\Alice\\aa2.txt",quote=FALSE,row.names=FALSE,col.name=FALSE,sep="\t")
>
> I tried to write a loop for it like :
>
>
> i <- 1
> while (i <3)
> {
> qq<-data.frame(test[,1],test[,2[i],test[,2[i]+1])
> write.table(qq,file="C:\\Alice\\bb.txt",quote=FALSE,row.names=FALSE,col.name=FALSE,sep="\t")
> i <- i+1;
> }
>
> But it's not right.  I tried hard to revise it but I just can't make it.  If someone can help me out here, your help is really greatly appreciated.  Thank you sooo much!
>
> Best,
>
> Alice
>
>
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From durbano at shbano.com  Fri Nov 11 01:26:01 2005
From: durbano at shbano.com (durbano@shbano.com)
Date: Fri, 11 Nov 2005 01:26:01 +0100
Subject: [R] =?iso-8859-1?q?following_Appendix_A_results_in_=22plot=2Enew_?=
	=?iso-8859-1?q?has_not_been_called_yet=22?=
Message-ID: <0MKoyl-1EaMlu1ka3-00009e@mrelay.perfora.net>


Hello. I was exploring the R software package and received the error
message "plot.new has not been called yet" when following "Appendix A A
Sample Session" of R-intro.pdf.  I searched the message archives and
found no similiar report to mine.

I am using R on CentOS and I am using the latest from R-project, version
2.2.0.

Here is my .Rhistory file recorded when following the appendix after the
first removal of variables (e.g. "rm(x,y)" ):

x <- 1:20
w <- 1 + sqrt(x)/2
dummy <- data.frame(x=x, y=x + rnorm(x)*w)
dummy
fm <- lm(y ~ x, data=dummy)
summary(fm)
fm1 <- lm(y ~ x, data=dummy, weight=1/w^2)
summary(fm1)
attach(dummy)
lrf <- lowess(x,y)
plot(x,y)
lines(x, lrf$y)

Results in:

Error in plot.xy(xy.coords(x, y), type = type, col = col, lty = lty,
...) :
        plot.new has not been called yet

If I continue along with the list of commands, I yield similiar errors.

I am unsure if this is related to using CentOS, my installation, bad
steps in the pdf file, or a bug in the software.  I am leaning towards
the steps in the pdf.

Regards,

Daniel



From ggrothendieck at gmail.com  Fri Nov 11 02:04:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Nov 2005 20:04:15 -0500
Subject: [R] following Appendix A results in "plot.new has not been
	called yet"
In-Reply-To: <0MKoyl-1EaMlu1ka3-00009e@mrelay.perfora.net>
References: <0MKoyl-1EaMlu1ka3-00009e@mrelay.perfora.net>
Message-ID: <971536df0511101704p567b081cu3d6a959a5eef4d27@mail.gmail.com>

You left out

y <- rnorm(x)



On 11/10/05, durbano at shbano.com <durbano at shbano.com> wrote:
>
> Hello. I was exploring the R software package and received the error
> message "plot.new has not been called yet" when following "Appendix A A
> Sample Session" of R-intro.pdf.  I searched the message archives and
> found no similiar report to mine.
>
> I am using R on CentOS and I am using the latest from R-project, version
> 2.2.0.
>
> Here is my .Rhistory file recorded when following the appendix after the
> first removal of variables (e.g. "rm(x,y)" ):
>
> x <- 1:20
> w <- 1 + sqrt(x)/2
> dummy <- data.frame(x=x, y=x + rnorm(x)*w)
> dummy
> fm <- lm(y ~ x, data=dummy)
> summary(fm)
> fm1 <- lm(y ~ x, data=dummy, weight=1/w^2)
> summary(fm1)
> attach(dummy)
> lrf <- lowess(x,y)
> plot(x,y)
> lines(x, lrf$y)
>
> Results in:
>
> Error in plot.xy(xy.coords(x, y), type = type, col = col, lty = lty,
> ...) :
>        plot.new has not been called yet
>
> If I continue along with the list of commands, I yield similiar errors.
>
> I am unsure if this is related to using CentOS, my installation, bad
> steps in the pdf file, or a bug in the software.  I am leaning towards
> the steps in the pdf.
>
> Regards,
>
> Daniel
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Fri Nov 11 02:18:33 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 11 Nov 2005 01:18:33 +0000
Subject: [R] Help regarding mas5 normalization
In-Reply-To: <af6d2ccc0511100851j21d22c6ra7a8d58bc6e5c372@mail.gmail.com>
References: <af6d2ccc0511100851j21d22c6ra7a8d58bc6e5c372@mail.gmail.com>
Message-ID: <1131671913.3155.110.camel@dhcp-82.wolf.ox.ac.uk>

Please do not post to both BioConductor and R.



On Thu, 2005-11-10 at 09:51 -0700, Nayeem Quayum wrote:
> Hello everybody,
> I am trying to use mas5 to normalize some array data and using mas5 and
> mas5calls. But I received these warning message. If anybody can explain the
> problem I would really appreciate that. Thanks in advance.
> background correction: mas
> PM/MM correction : mas
> expression values: mas
> background correcting...Warning message:
> 'loadURL' is deprecated.
> Use 'load(url())' instead.
> See help("Deprecated")
> Warning message:
> 'loadURL' is deprecated.
> Use 'load(url())' instead.
> See help("Deprecated")
> Warning message:
> 'loadURL' is deprecated.
> Use 'load(url())' instead.
> See help("Deprecated")
> There were 14 warnings (use warnings() to see them)
> Note: http://www.bioconductor.org/repository/devel/package/Win32 does not
> seem to have a valid repository, skipping
> Note: You did not specify a download type. Using a default value of: Source
> This will be fine for almost all users
> 
> Error in FUN(X[[1]], ...) : no slot of name "Uses" for this object of class
> "localPkg"
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From alice.0309 at yahoo.com.tw  Fri Nov 11 02:47:41 2005
From: alice.0309 at yahoo.com.tw (alice.0309)
Date: Thu, 10 Nov 2005 17:47:41 -0800 (PST)
Subject: [R] How to export multiple files using write.table in the loop?
In-Reply-To: <971536df0511101610v663dcea1gd186489bfba9e24d@mail.gmail.com>
Message-ID: <20051111014741.69776.qmail@web17008.mail.tpe.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/a415cea6/attachment.pl

From alice.0309 at yahoo.com.tw  Fri Nov 11 03:23:39 2005
From: alice.0309 at yahoo.com.tw (alice.0309)
Date: Thu, 10 Nov 2005 18:23:39 -0800 (PST)
Subject: [R] How to export multiple files using write.table in the loop?
In-Reply-To: <971536df0511101610v663dcea1gd186489bfba9e24d@mail.gmail.com>
Message-ID: <20051111022339.65289.qmail@web17007.mail.tpe.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051110/0d9095c7/attachment.pl

From ggrothendieck at gmail.com  Fri Nov 11 04:06:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Nov 2005 22:06:42 -0500
Subject: [R] How to export multiple files using write.table in the loop?
In-Reply-To: <20051111022339.65289.qmail@web17007.mail.tpe.yahoo.com>
References: <971536df0511101610v663dcea1gd186489bfba9e24d@mail.gmail.com>
	<20051111022339.65289.qmail@web17007.mail.tpe.yahoo.com>
Message-ID: <971536df0511101906t3903e564o5703ecb1550cf9d1@mail.gmail.com>

In that case you can simplify it a bit like this:

fn <- "/s.txt"
for(i in 1:14)
  write.table(data1[, c(1, 2*i, 2*i+1)], paste(fn, i, sep = "."),
           quote = FALSE, row.names = FALSE, col.names = FALSE)


On 11/10/05, alice.0309 <alice.0309 at yahoo.com.tw> wrote:
> Hello!
>
> I've solved the problems so don't bother.  I can use Paste function like
> this:
> i <- 1
> while (i <15)
> {
> s<-data.frame(data1[,1],data1[,(2*i)],data1[,(2*i+1)])
> write.table(s,paste("F:\\s.txt",i,sep="."),quote=FALSE,row.names=FALSE,col.names=FALSE)
> i <- i+1;
> }
>
> Thank you very much and sorry for multiple e-mails!
>
> Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> col.list <- list(1:3, c(1,4:5))
> for(cols in col.list) write.table(data1[,cols], ...whatever...)
>
>
> On 11/10/05, alice.0309 wrote:
> > Hi,
> >
> > I tried to split a big file into some small files seperately by R. I can
> only do that writing duplicated codes. When I tried to write a loop, I only
> got one appned or destroyed exported file. For example:
> >
> > data1<- read.table(file = "C:\\Alice\\MBEI.txt", sep="\t",
> check.names=FALSE)
> > a<-subset(data1,select=c(V1,V2,V3))
> >
> write.table<-(a,file="C:\\Alice\\aa1.txt",quote=FALSE,row.names=FALSE,col.name=FALSE,sep="\t")
> > a<-subset(data1,select=c(V1,V4,V5))
> >
> write.table(a,file="C:\\Alice\\aa2.txt",quote=FALSE,row.names=FALSE,col.name=FALSE,sep="\t")
> >
> > or
> > data1<- read.table(file = "C:\\Alice\\MBEI.txt", sep="\t",
> check.names=FALSE)
> > a<-data.frame(data1[,1],data1[,2],data1[,3]
> >
> write.table<-(a,file="C:\\Alice\\aa1.txt",quote=FALSE,row.names=FALSE,col.name=FALSE,sep="\t")
> > a<-data.frame(data1[,1],data1[,4],data1[,5]
> >
> write.table(a,file="C:\\Alice\\aa2.txt",quote=FALSE,row.names=FALSE,col.name=FALSE,sep="\t")
> >
> > I tried to write a loop for it like :
> >
> >
> > i <- 1
> > while (i <3)
> > {
> > qq<-data.frame(test[,1],test[,2[i],test[,2[i]+1])
> >
> write.table(qq,file="C:\\Alice\\bb.txt",quote=FALSE,row.names=FALSE,col.name=FALSE,sep="\t")
> > i <- i+1;
> > }
> >
> > But it's not right. I tried hard to revise it but I just can't make it. If
> someone can help me out here, your help is really greatly appreciated. Thank
> you sooo much!
> >
> > Best,
> >
> > Alice
> >
> >
> >
> >
> >
> > ---------------------------------
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>
> ________________________________
> Yahoo! FareChase - Search multiple travel sites in one click.
>
>



From jpablo.romero at gmail.com  Fri Nov 11 06:16:51 2005
From: jpablo.romero at gmail.com (Juan Pablo Romero)
Date: Thu, 10 Nov 2005 23:16:51 -0600
Subject: [R] undefined symbol in grDevices.so
Message-ID: <e6507ac70511102116m132fa22bv@mail.gmail.com>

Hello

I'm trying to use rpy with latest R (2.2.0), but unfortunately it
seems there is some kind of undefined symbol in grDevices.so
(utf8locale)

Within python, this message appears:

>>> import rpy
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
'/usr/local/lib/R/library/grDevices/libs/grDevices.so':
  /usr/local/lib/R/library/grDevices/libs/grDevices.so: undefined
symbol: utf8locale
Loading required package: grDevices
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
'/usr/local/lib/R/library/grDevices/libs/grDevices.so':
  /usr/local/lib/R/library/grDevices/libs/grDevices.so: undefined
symbol: utf8locale
In addition: Warning message:
package grDevices in options("defaultPackages") was not found
Error: package 'grDevices' could not be loaded

Any suggestions?

Thanks in advance.

  Juan Pablo



From ggrothendieck at gmail.com  Fri Nov 11 07:30:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 11 Nov 2005 01:30:01 -0500
Subject: [R] strange classification behaviour
In-Reply-To: <20051110201053.6eadfea0@zygiella.local>
References: <20051110201053.6eadfea0@zygiella.local>
Message-ID: <971536df0511102230m1bad470cvaf658f70523b9144@mail.gmail.com>

You could use cut.  The key calculation would be:

   w <- .05; eps <- 1e-5
   breakpoints <- seq(min(kk), max(kk), .05)
   breakpoints <- floor( (breakpoints + (w/2) + eps) / w) * w
   values <- cut(kk, c(breakpoints, Inf), right = FALSE)
   values <- ordered(values)

If you don't like the labels produced add lab = breakpoints as a cut arg.

On 11/10/05, RenE J.V. Bertin <rjvbertin at gmail.com> wrote:
> Hello,
>
> I've written a routine that takes an input vector and returns a 'binned' version with a requested bin width and converted to an ordered factor by default. It also attempts to make sure that all factor levels intermediate to the input range are present.
>
> This is the code as I currently have it:
>
> Classify <- function( values, ClassWidth=0.05, ordered.factor=TRUE, all=TRUE )
> {
>     valuesName <- deparse(substitute(values))
>     if( is.numeric(values) ){
>          values <- floor( (values+ (ClassWidth/2) ) / ClassWidth ) * ClassWidth
>          # determine the numerical range of the input
>          levels <- range( values, finite=TRUE )
>          if( ordered.factor ){
>               if( all ){
>                    # if we want all levels, construct a levels vector that can be passed to factor's levels argument:
>                    levels <- seq( levels[1], levels[2], by=ClassWidth )
>                    values <- factor(values, levels=levels, ordered=TRUE )
>               }
>               else{
>                    values <- factor(values, ordered=TRUE )
>               }
>          }
>     }
>     else{
>          levels <- range( values, finite=TRUE )
>          if( all ){
>               levels <- seq( levels[1], levels[2], by=ClassWidth )
>               values <- factor( values, levels=levels, ordered=ordered.factor )
>          }
>          else{
>               values <- factor( values, ordered=ordered.factor )
>          }
>     }
>     comment(values) <- paste( comment(values),
>          "; Classify(", valuesName, ", ClassWidth=", ClassWidth, ", ordered.factor=", ordered.factor, ")",
>          sep="")
>     values
> }
>
> This does work, but has some strange side-effects that I think might be due to rounding errors:
>
> ##> kk<-c(  0.854189  0.374423  0.522893  0.670796  0.913540  0.979011  0.510378  0.320440 -0.576764  0.940343 )
>
> ##> Classify( kk, ClassWidth=0.05, all=FALSE )
>  [1] 0.85 0.35 0.5  0.65 0.9  1    0.5  0.3  -0.6 0.95
> Levels: -0.6 < 0.3 < 0.35 < 0.5 < 0.65 < 0.85 < 0.9 < 0.95 < 1
> ### result as expected, but using this on the hor. axis of a graph can be ... surprising.
>
> ##> Classify( kk, ClassWidth=0.05, all=TRUE )
>  [1] 0.85 <NA> 0.5  <NA> <NA> 1    0.5  <NA> -0.6 <NA>
> 33 Levels: -0.6 < -0.55 < -0.5 < -0.45 < -0.4 < -0.35 < -0.3 < -0.25 < -0.2 < -0.15 < -0.1 < -0.05 < 0 < ... < 1
> ##> summary( Classify( kk, ClassWidth=0.05, all=TRUE ) )
>              -0.6              -0.55               -0.5              -0.45               -0.4              -0.35
>                 1                  0                  0                  0                  0                  0
>              -0.3              -0.25               -0.2              -0.15               -0.1              -0.05
>                 0                  0                  0                  0                  0                  0
>                 0 0.0499999999999999                0.1               0.15                0.2               0.25
>                 0                  0                  0                  0                  0                  0
>               0.3               0.35                0.4               0.45                0.5               0.55
>                 0                  0                  0                  0                  2                  0
>               0.6               0.65                0.7               0.75                0.8               0.85
>                 0                  0                  0                  0                  0                  1
>               0.9               0.95                  1               NA's
>                 0                  0                  1                  5
>
> ### ???
>
> What happens is probably that the value in my input that classify to 0.3 or 0.35 are not found in the list of levels that I calculate due to rounding errors. Adding an element 0.05 to kk supports this idea.
>
> Is there a way around this, for instance a more robust way to do what I'm trying to do here (or a function provided by R)?
>
> When I modify the relevant code above to
>
>                    levels <- floor( (seq( levels[1], levels[2], by=ClassWidth ) + (ClassWidth/2)) / ClassWidth ) * ClassWidth
>                    values <- factor( values, levels=levels, ordered=TRUE )
>
> the result is as expected, but I find that not very elegant...
>
> Thanks in advance,
> RenE Bertin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fengtai at biostat.umn.edu  Fri Nov 11 07:45:10 2005
From: fengtai at biostat.umn.edu (Feng Tai)
Date: Fri, 11 Nov 2005 00:45:10 -0600 (CST)
Subject: [R] R on Windows XP x64
Message-ID: <Pine.GSO.4.44.0511110031380.17622-100000@smelt.biostat.umn.edu>

Hi,

I am running R 2.2.0 on the Windows XP x64. The mechanism of error hanlder
seems different. It will take a very long time to pop up a error message
diaglog box, even when some simple errors happen such as "Syntax error" or
"object xxxx not found". Does anybody have the similar experience? Thanks
a lot.

BTW: everything works fine under 32-bit Windows XP, error messages come
out immediately.

Feng



From jarioksa at sun3.oulu.fi  Fri Nov 11 08:48:25 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Fri, 11 Nov 2005 09:48:25 +0200
Subject: [R] error in rowSums:'x' must be numeric
In-Reply-To: <freemail.20051010164956.40084@fm07.freemail.hu>
References: <freemail.20051010164956.40084@fm07.freemail.hu>
Message-ID: <1131695305.19414.10.camel@biol102145.oulu.fi>

On Thu, 2005-11-10 at 16:49 +0100, Illyes Eszter wrote:
> Dear All, 
> 
> It's Eszter again from Hungary. I could not solve my problem form 
> yesterday, so I still have to ask your help.
> 
> I have a binary dataset of vegetation samples and species as a comma 
> separated file. I would like to calculate the Jaccard distance of the 
> dataset. I have the following error message: 
> 
> Error in rowSums(x, prod(dn), p, na.rm) : 'x' must be numeric
> In addition: Warning message:
> results may be meaningless because input data have negative entries
>  in: vegdist(t2, method = "jaccard", binary = FALSE, diag = FALSE,  
> 
> Do you have any idea what can be the problem? I have only 0 and 1 in 
> the dataset. 
> 
Eszter, 

An old truth is that if The Computer is always right and you are wrong
when The Computer says that you have non-numeric data. Check your data
first. An obvious way of checking this is to repeat the command that
found the problem: rowSums(t2). After that (probably) reports the same
error, you can check your data sayin, e.g., str(t2) which displays you
the variables in a very compact form.

Now some wild speculation. When you read your data as comma separated
file, very often the column names are taken as the first variable. Check
this and remove the first column if needed. This is so common that I've
even thought that I perhaps need to write a sanitizing function for cvs
files to do the following:

   rownames(x) <- x[,1]
   x <- x[,-1]

or to take the first column as rownames and then remove the non-numeric
first column.

A pertinent problem in R communication with cvs files is that R assumes
that with header=TRUE the header line has one entry less than data rows.
However, popular software (read Excel) refuses to write data so: even if
you make the first column empty in your header line, the popular
software adds a comma before the first intended entry, and so you have
the same number of entries in the header line and in the data. The
result is that the column that intended as rownames is taken as a
non-numeric variable. 

This is such a common problem that an innocent user (not me: I'm no more
innocent) would expect R cope with that kind of input format.

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/



From ripley at stats.ox.ac.uk  Fri Nov 11 08:58:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Nov 2005 07:58:39 +0000 (GMT)
Subject: [R] make check failed on linux-amd64 using PGI compilers
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED593@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED593@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.61.0511110746100.531@gannet.stats>

On Thu, 10 Nov 2005, Liaw, Andy wrote:

>> From: pd at pubhealth.ku.dk
>>
>> "Liaw, Andy" <andy_liaw at merck.com> writes:
>>
>>> Dear R-help,
>>>
>>> I am trying to build R-2.2.0-patched (2005-11-07 r36217) on
>> the head node of
>>> a Scyld cluster (dual Opteron 250s) using PGI compilers
>> (v6.0).  I used the
>>> flags suggested by Jennifer Lai on R-devel (taken from
>> R-admin, except that
>>> I had to add -L/usr/X11R6/lib64 to LDFLAGS).  The build
>> went fine, but make
>>> check-all failed when running tests/Examples/graphics-Ex.R, at:
>>>
>>>> plot(1:2, xaxs = "i") # 'inner-axis' w/o extra space
>>>> stopifnot(par("xaxp")[1:2] == 1:2 &&
>>> +           par("usr")[1:2] == 1:2)
>>> Error: par("xaxp")]1:2] == 1:2 && par("usr")[1:2] == 1:2 is not TRUE
>>>
>>> The above looks a bit strange to me, as running the R built
>> as above,
>>> par("xaxp")[1:2] - 1:2 gives
>>>
>>> [1] -1.110223e-16 -2.220446e-16
>>>
>>> Is my R build faulty?  I'd very much appreciate any advise.
>>
>> Relative errors of 1 ULP would not ruin my sleep.  It's not obvious to
>> me which is the floating point operation responsible for the effect,
>> though, and it is a bit suspicious given that it should just be the
>> x-range of the data.
>
> It's the par() output that has storage mode "double", as that stores the
> coordinates.  1:2 has storage mode "integer".
>
> Those comparisons looked a bit suspicious to me.  Isn't it recommended time
> and again on this list not to test for exact equality of numerics?
> all.equal() of the above does return TRUE.

Yes.  It is also not the intention to have examples be regression tests, 
something we have moved away from in general.

But I think Peter's point was that par("usr") should be exactly the range 
of the data (it is), and par("xaxp") should be par("usr") with xaxs="i" 
when the end points are round numbers. Now setting par("xaxp") is a 
tangled tale and involves a call to GPretty, and the latter is what will 
be doing floating-point computations.  It is a little strange that they 
are not exact, but although all.equal with the default tolerance would be 
too loose, a tolerance of say 100 ulp would suffice.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Nov 11 09:09:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Nov 2005 08:09:32 +0000 (GMT)
Subject: [R] undefined symbol in grDevices.so
In-Reply-To: <e6507ac70511102116m132fa22bv@mail.gmail.com>
References: <e6507ac70511102116m132fa22bv@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0511110759240.531@gannet.stats>

On Thu, 10 Nov 2005, Juan Pablo Romero wrote:

> Hello
>
> I'm trying to use rpy with latest R (2.2.0), but unfortunately it
> seems there is some kind of undefined symbol in grDevices.so
> (utf8locale)

It is defined in libR.so which is presumably what 'rpy' uses (you haven't 
said what rpy is), provided R was built with --enable-mbcs (the default if 
there is enough OS support - another thing you failed to mention was your 
OS).

gannet% nm -pg lib/libR.so | grep utf8locale
001ef824 B utf8locale

Look like your rpy installation is mixed up.  I presume R itself runs, in 
which case I suspect rpy is pointing at the wrong libR.so.  Maybe it needs 
updating for current R?

Suggestion: ask programming questions on R-devel as the posting guide 
says, and send 'rpy' questions to the maintainer (as the posting guide 
says), or its mailing list (if it is the project on Sourceforge, it has 
one).


> Within python, this message appears:
>
>>>> import rpy
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library
> '/usr/local/lib/R/library/grDevices/libs/grDevices.so':
>  /usr/local/lib/R/library/grDevices/libs/grDevices.so: undefined
> symbol: utf8locale
> Loading required package: grDevices
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library
> '/usr/local/lib/R/library/grDevices/libs/grDevices.so':
>  /usr/local/lib/R/library/grDevices/libs/grDevices.so: undefined
> symbol: utf8locale
> In addition: Warning message:
> package grDevices in options("defaultPackages") was not found
> Error: package 'grDevices' could not be loaded


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mmiller at nassp.uct.ac.za  Fri Nov 11 09:14:59 2005
From: mmiller at nassp.uct.ac.za (Mark Miller)
Date: Fri, 11 Nov 2005 10:14:59 +0200
Subject: [R] Colour Lines
Message-ID: <200511111014.59606.mmiller@nassp.uct.ac.za>

I am plotting a cumulative plot of data with the fitted distribution's 
cumulative plot and I want to make it so that one or the other is in a 
different colour so that I can talk about which is which and, if possible, 
add a legend to the graph

x = seq(0,30,0.01)
plot(ecdf(IAT), do.point=FALSE, main = 'Cummlative Plot of Monday IATs\n for 
entire 15 Weeks')
lines(x, pexp(x,0.41562500))

Many thanks
Mark Miller



From ripley at stats.ox.ac.uk  Fri Nov 11 09:27:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Nov 2005 08:27:57 +0000 (GMT)
Subject: [R] R on Windows XP x64
In-Reply-To: <Pine.GSO.4.44.0511110031380.17622-100000@smelt.biostat.umn.edu>
References: <Pine.GSO.4.44.0511110031380.17622-100000@smelt.biostat.umn.edu>
Message-ID: <Pine.LNX.4.61.0511110821270.531@gannet.stats>

On Fri, 11 Nov 2005, Feng Tai wrote:

> I am running R 2.2.0 on the Windows XP x64. The mechanism of error hanlder
> seems different. It will take a very long time to pop up a error message
> diaglog box, even when some simple errors happen such as "Syntax error" or
> "object xxxx not found". Does anybody have the similar experience? Thanks
> a lot.

R does not use dialog boxes for such error messages: it reports them in 
the console (RGui) or on the terminal (Rterm).

So it seems there is something you are not telling us about what you are 
doing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fengtai at biostat.umn.edu  Fri Nov 11 11:05:20 2005
From: fengtai at biostat.umn.edu (Feng Tai)
Date: Fri, 11 Nov 2005 04:05:20 -0600 (CST)
Subject: [R] R on Windows XP x64
In-Reply-To: <Pine.LNX.4.61.0511110821270.531@gannet.stats>
Message-ID: <Pine.GSO.4.44.0511110400560.18841-100000@smelt.biostat.umn.edu>

I checked again, the dialog box pops up after loading package "pamr".
Thanks a lot!!

Feng
On Fri, 11 Nov 2005, Prof Brian Ripley wrote:

> On Fri, 11 Nov 2005, Feng Tai wrote:
>
> > I am running R 2.2.0 on the Windows XP x64. The mechanism of error hanlder
> > seems different. It will take a very long time to pop up a error message
> > diaglog box, even when some simple errors happen such as "Syntax error" or
> > "object xxxx not found". Does anybody have the similar experience? Thanks
> > a lot.
>
> R does not use dialog boxes for such error messages: it reports them in
> the console (RGui) or on the terminal (Rterm).
>
> So it seems there is something you are not telling us about what you are
> doing.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From fengtai at biostat.umn.edu  Fri Nov 11 11:13:22 2005
From: fengtai at biostat.umn.edu (Feng Tai)
Date: Fri, 11 Nov 2005 04:13:22 -0600 (CST)
Subject: [R] R on Windows XP x64
In-Reply-To: <Pine.LNX.4.61.0511110821270.531@gannet.stats>
Message-ID: <Pine.GSO.4.44.0511110410570.18841-100000@smelt.biostat.umn.edu>

In "pamr", I found that they wrote their own error handler function and
set as .Options$error. I set .Options$error back to NULL, problem solved.
Thanks again.

Feng

On Fri, 11 Nov 2005, Prof Brian Ripley wrote:

> On Fri, 11 Nov 2005, Feng Tai wrote:
>
> > I am running R 2.2.0 on the Windows XP x64. The mechanism of error hanlder
> > seems different. It will take a very long time to pop up a error message
> > diaglog box, even when some simple errors happen such as "Syntax error" or
> > "object xxxx not found". Does anybody have the similar experience? Thanks
> > a lot.
>
> R does not use dialog boxes for such error messages: it reports them in
> the console (RGui) or on the terminal (Rterm).
>
> So it seems there is something you are not telling us about what you are
> doing.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From florent.baty at unibas.ch  Fri Nov 11 12:48:09 2005
From: florent.baty at unibas.ch (Florent Baty)
Date: Fri, 11 Nov 2005 12:48:09 +0100
Subject: [R] Snow parLapply
Message-ID: <437484F9.6000805@unibas.ch>

Dear R-user,

I am trying to use the function 'parLapply' from the 'snow' package 
which is supposed to work the same wys as 'lapply' but for a 
parallelized cluster of computers. The function I am trying to call in 
parallel is 'dudi.pca' (from the 'ade4' package) which performs 
principal component analyses. When I call this function on a list of 
dataframes with the regular lapply function it works correctly. If use 
the 'parLapply' there is an error message.

Example:
 > library("snow")
 > library(ade4)
 > a <- matrix(rnorm(2500),50)
 > b <- matrix(rnorm(10^4),10^2)
 > l1 <- list(a=a,b=b)
 > mycluster <- makeCluster(2,type="MPI")
Loading required package: Rmpi

        Rmpi version: 0.4-9
        Rmpi is an interface (wrapper) to MPI APIs
        with interactive R slave functionalities.
        See `library (help=Rmpi)' for details.
        2 slaves are spawned successfully. 0 failed.
 > lapply(list(a=as.data.frame(a),b=as.data.frame(b)),dudi.pca,scannf=F)
$a
Duality diagramm
class: pca dudi
$call: FUN(df = X[[1]], scannf = ..1)
..

$b
Duality diagramm
class: pca dudi
$call: FUN(df = X[[2]], scannf = ..1)

..

 > 
parLapply(mycluster,list(a=as.data.frame(a),b=as.data.frame(b)),dudi.pca,scannf=F)
[1] "Error in FUN(X[[1]], ...) : couldn't find function \"as.dudi\"\n"
[2] "Error in FUN(X[[1]], ...) : couldn't find function \"as.dudi\"\n"


On the other hand, if I call 'parLapply' with the function 'princomp' 
(which also performs PCA) everything works fine.

Example:

 > parLapply(mycluster,list(a=as.data.frame(a),b=as.data.frame(b)),princomp)
$a
Call:
princomp(x = X[[1]])
..

$b
Call:
princomp(x = X[[1]])
..

Does anybody knows why 'parLapply' does not work correctly with some 
functions?

Thanks a lot for your help,
 
Florent

-- 
--------------------------------------------------
		Dr Florent BATY
Pulmonary Gene Research, Universit??tsspital Basel
   Petersgraben 4, CH-4031 Basel, Switzerland
 tel: +41 61 265 57 27 - fax: +41 61 265 45 87



From petr.pikal at precheza.cz  Fri Nov 11 12:08:09 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 11 Nov 2005 12:08:09 +0100
Subject: [R] Remove levels
In-Reply-To: <20051110133235.67767.qmail@web25811.mail.ukl.yahoo.com>
Message-ID: <437489A9.15757.12C4756@localhost>

Hi

On 10 Nov 2005 at 14:32, Marc Bernard wrote:

Date sent:      	Thu, 10 Nov 2005 14:32:35 +0100 (CET)
From:           	Marc Bernard <bernarduse1 at yahoo.fr>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Remove levels

> Daer All,
> 
> I have a factor  variable, X with 5 levels. When I type tables(X) it
> gives me:
> 
> table(X)
> 1     2   3    4   5 
> 10   5    0   0   0
> 
> How to drop the levels with zeros such that when I will type:
> table(X) it will give me:


see ?factor
table(factor(X))

HTH
Petr


> 
> table(X)
> 1     2   
> 10   5    
> 
> 
> Thank a lot,
> 
> Bernard
> 
> 
> 
> 
> ---------------------------------
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Fri Nov 11 12:13:14 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 11 Nov 2005 12:13:14 +0100
Subject: [R] error in rowSums:'x' must be numeric
In-Reply-To: <freemail.20051010164956.40084@fm07.freemail.hu>
Message-ID: <43748ADA.19220.130EE8F@localhost>

Hi

try

str(x)

which will show you how your data look like.
Obviously during some reading/manipulation your data became non 
numeric.

HTH
Petr



On 10 Nov 2005 at 16:49, Illyes Eszter wrote:

Date sent:      	Thu, 10 Nov 2005 16:49:56 +0100 (CET)
From:           	Illyes Eszter <illyese at freemail.hu>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] error in rowSums:'x' must be numeric

> Dear All, 
> 
> It's Eszter again from Hungary. I could not solve my problem form
> yesterday, so I still have to ask your help.
> 
> I have a binary dataset of vegetation samples and species as a comma
> separated file. I would like to calculate the Jaccard distance of the
> dataset. I have the following error message: 
> 
> Error in rowSums(x, prod(dn), p, na.rm) : 'x' must be numeric
> In addition: Warning message:
> results may be meaningless because input data have negative entries
>  in: vegdist(t2, method = "jaccard", binary = FALSE, diag = FALSE,  
> 
> Do you have any idea what can be the problem? I have only 0 and 1 in
> the dataset. 
> 
> Thank you very much! All the best:
> 
> 
> Eszter
> 
> 
> ______________________________________________________________________
> _ KGFB 2006 - Garant??ltan a legjobb ??r! Nyerje meg az ??j Swiftet +
> garant??lt 10,000,-  Ft ??rt??k?? aj??nd??k. WWW.NETRISK.HU
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Allan at STATS.uct.ac.za  Fri Nov 11 12:22:01 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Fri, 11 Nov 2005 13:22:01 +0200
Subject: [R] R: method of moments
Message-ID: <43747ED9.3207AFF@STATS.uct.ac.za>

hi all

does anyone know how one would calculate the covariance matrix of a
vector of estimated parameters if we use the method of moments
technique? one could use bootstrapping techniques but there should be
some asymptotic results that could be used as per MLE estimates.

a reference would be much appreciated.

From sdavis2 at mail.nih.gov  Fri Nov 11 12:29:49 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 11 Nov 2005 06:29:49 -0500
Subject: [R] Snow parLapply
In-Reply-To: <437484F9.6000805@unibas.ch>
Message-ID: <BF99EADD.12AE9%sdavis2@mail.nih.gov>

On 11/11/05 6:48 AM, "Florent Baty" <florent.baty at unibas.ch> wrote:

> Dear R-user,
> 
> I am trying to use the function 'parLapply' from the 'snow' package
> which is supposed to work the same wys as 'lapply' but for a
> parallelized cluster of computers. The function I am trying to call in
> parallel is 'dudi.pca' (from the 'ade4' package) which performs
> principal component analyses. When I call this function on a list of
> dataframes with the regular lapply function it works correctly. If use
> the 'parLapply' there is an error message.
> 
> Example:
>> library("snow")
>> library(ade4)
>> a <- matrix(rnorm(2500),50)
>> b <- matrix(rnorm(10^4),10^2)
>> l1 <- list(a=a,b=b)
>> mycluster <- makeCluster(2,type="MPI")
> Loading required package: Rmpi
> 
>       Rmpi version: 0.4-9
>       Rmpi is an interface (wrapper) to MPI APIs
>       with interactive R slave functionalities.
>       See `library (help=Rmpi)' for details.
>       2 slaves are spawned successfully. 0 failed.
>> lapply(list(a=as.data.frame(a),b=as.data.frame(b)),dudi.pca,scannf=F)
> $a
> Duality diagramm
> class: pca dudi
> $call: FUN(df = X[[1]], scannf = ..1)
> ..
> 
> $b
> Duality diagramm
> class: pca dudi
> $call: FUN(df = X[[2]], scannf = ..1)
> 
> ..
> 
>> 
> parLapply(mycluster,list(a=as.data.frame(a),b=as.data.frame(b)),dudi.pca,scann
> f=F)
> [1] "Error in FUN(X[[1]], ...) : couldn't find function \"as.dudi\"\n"
> [2] "Error in FUN(X[[1]], ...) : couldn't find function \"as.dudi\"\n"
> 
> 
> On the other hand, if I call 'parLapply' with the function 'princomp'
> (which also performs PCA) everything works fine.
> 
> Example:
> 
>> parLapply(mycluster,list(a=as.data.frame(a),b=as.data.frame(b)),princomp)
> $a
> Call:
> princomp(x = X[[1]])
> ..
> 
> $b
> Call:
> princomp(x = X[[1]])
> ..
> 
> Does anybody knows why 'parLapply' does not work correctly with some
> functions?

Florent,

Keep in mind how snow and parLapply work.  They start slaves on each node.
Each slave has its own workspace, and that workspace doesn't contain
dudi.pca unless you tell it to.  Therefore, you will have to do:

clusterEvalQ(mycluster,library(ade4))

To load the ade4 library in all the slaves.  After that, I think your code
will behave as expected.  I highly recommend this website for learning the
basics of snow:

http://www.sfu.ca/~sblay/R/snow.html

Hope that helps.

Sean



From ron.ophir at weizmann.ac.il  Fri Nov 11 12:48:23 2005
From: ron.ophir at weizmann.ac.il (Ron Ophir)
Date: Fri, 11 Nov 2005 13:48:23 +0200
Subject: [R] different functions on different vector subsets
Message-ID: <s374a12f.057@wisemail.weizmann.ac.il>

I thought about other cases but I have to dissagree with you. For
logical vector NA is no decision and that should be the results of it.
Let's say that -b- is a result of comparison not of -a- to something 
rather a compsrison of -c- to -d-. In this case NA in the first position
is a result of NA in either -c- or -d- or both. Now, if the result I
wanted to get from your example

a<-c(1,2,3,4)
b<-c(NA,T,F,T)

a[b]<-7
 is c(1,7,7,7) I should replace the NA (no decision) with F and if to
get c(7,7,7,7)  the NA should be replaced by T otherwise the result
should be c(NA,7,7,7). The first two option are possible to perform in R
 the third is not and that is to the user to decide which to choose.
Ron

>>> Thomas Lumley <tlumley at u.washington.edu> 11/10/05 11:02 PM >>>
On Thu, 10 Nov 2005, Ron Ophir wrote:

> Thanks Thomas,
>
> "...For logical subscripts you could argue that the
> ambiguity isn't present and that if the index was NA the element
should
> just be set to NA. This change might be worth making."
>
> I see you got my point. NA should return NA no matter what the
> comparison is.

I'm not sure that I did get your point.  As Brian said, you aren't 
specifying whether or not to set the value. In your example it didn't 
matter because it would end up NA either way.

I was saying that for eg

a<-c(1,2,3,4)
b<-c(NA,T,F,T)

a[b]<-7

we could relax the prohibition on NA indexing to give c(NA,7,7,7) as the

result. In your case that would give what you wanted, but in other cases

it might not.


 	-thomas



From charoentong at gmail.com  Fri Nov 11 12:58:17 2005
From: charoentong at gmail.com (Pornpimol Kay)
Date: Fri, 11 Nov 2005 12:58:17 +0100
Subject: [R] R: method of moments
In-Reply-To: <43747ED9.3207AFF@STATS.uct.ac.za>
References: <43747ED9.3207AFF@STATS.uct.ac.za>
Message-ID: <b6c60d270511110358h2d103b1chc7089605fb2f3de8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051111/040701e0/attachment.pl

From illyese at freemail.hu  Fri Nov 11 13:10:51 2005
From: illyese at freemail.hu (Illyes Eszter)
Date: Fri, 11 Nov 2005 13:10:51 +0100 (CET)
Subject: [R] Error in rowSums : 'x' must be numeric IS SOLVED!
Message-ID: <freemail.20051011131051.67282@fm13.freemail.hu>

Dear All, 

It's Eszter again. Thank you for all, who tried to help me. 
In this mail I would like to inform you that I could solve my yesterday 
problem 

"Error in rowSums(x, prod(dn), p, na.rm) : 'x' must be numeric"

only with creating a new datafile. Most probably the problem was that I 
opened my dataset before in MS Word for editing and MS Word created 
some transformation with the data which caused R to look at the 
numbers as strings. If I excluded MS Word from the process, everything 
was fine. 

All the best, 

Eszter


_______________________________________________________________________
KGFB 2006 - Garant??ltan a legjobb ??r! Nyerje meg az ??j Swiftet + 
garant??lt 10,000,-  Ft ??rt??k?? aj??nd??k. WWW.NETRISK.HU



From roger.bos at gmail.com  Fri Nov 11 14:26:18 2005
From: roger.bos at gmail.com (roger bos)
Date: Fri, 11 Nov 2005 08:26:18 -0500
Subject: [R] A Quick and (Very) Dirty Intro to Stats in R
In-Reply-To: <20051109015209.DBER25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>
References: <ef6bf117ece7120997e59453e7f1de7f@arrr.net>
	<20051109015209.DBER25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <1db726800511110526s36677870ydecf300c7f158e61@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051111/d64e3717/attachment.pl

From spencer.graves at pdf.com  Fri Nov 11 14:38:20 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Nov 2005 05:38:20 -0800
Subject: [R] no package 'Matrix' at the repositories
Message-ID: <43749ECC.6000206@pdf.com>

	  Yesterday, I installed R2.2.0 for Windows [Version 2.2.0 (2005-10-06 
r35749)].  Unfortunately, 'install.packages("Matrix")' produced the 
following message:

Warning in download.packages(pkgs, destdir = tmpd, available = 
available,  :
          no package 'Matrix' at the repositories

	  I installed lme4, maps, mapproj, CircStats, scatterplot3d, gregmisc, 
Hmisc without problems.  To confirm, 'library(lme4)' produced the 
following error:

Error: package 'Matrix' required by 'lme4' could not be found

	  What do you suggest?
	  Spencer Graves
p.s.  I get the same result using several different (US) mirrors.
-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From HDoran at air.org  Fri Nov 11 14:53:44 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 11 Nov 2005 08:53:44 -0500
Subject: [R] no package 'Matrix' at the repositories
Message-ID: <F5ED48890E2ACB468D0F3A64989D335ACDCFEA@dc1ex3.air.org>

I don't have an answer, but also want to point out that whenever I try
and download the pdf documentation associated with a package, my Acrobat
opens but tells me the file is corrupted.  I did this for a random
selection of packages and the same problem seems to reoccur. I'm not
sure if this is just me or something on CRAN

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
Sent: Friday, November 11, 2005 8:38 AM
To: R-help at stat.math.ethz.ch; Douglas Bates
Subject: [R] no package 'Matrix' at the repositories

	  Yesterday, I installed R2.2.0 for Windows [Version 2.2.0
(2005-10-06 r35749)].  Unfortunately, 'install.packages("Matrix")'
produced the following message:

Warning in download.packages(pkgs, destdir = tmpd, available =
available,  :
          no package 'Matrix' at the repositories

	  I installed lme4, maps, mapproj, CircStats, scatterplot3d,
gregmisc, Hmisc without problems.  To confirm, 'library(lme4)' produced
the following error:

Error: package 'Matrix' required by 'lme4' could not be found

	  What do you suggest?
	  Spencer Graves
p.s.  I get the same result using several different (US) mirrors.
--
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sdavis2 at mail.nih.gov  Fri Nov 11 14:54:03 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 11 Nov 2005 08:54:03 -0500
Subject: [R] no package 'Matrix' at the repositories
In-Reply-To: <43749ECC.6000206@pdf.com>
Message-ID: <BF9A0CAB.12AF8%sdavis2@mail.nih.gov>

On 11/11/05 8:38 AM, "Spencer Graves" <spencer.graves at pdf.com> wrote:

>  Yesterday, I installed R2.2.0 for Windows [Version 2.2.0 (2005-10-06
> r35749)].  Unfortunately, 'install.packages("Matrix")' produced the
> following message:
> 
> Warning in download.packages(pkgs, destdir = tmpd, available =
> available,  :
>         no package 'Matrix' at the repositories
> 
>  I installed lme4, maps, mapproj, CircStats, scatterplot3d, gregmisc,
> Hmisc without problems.  To confirm, 'library(lme4)' produced the
> following error:
> 
> Error: package 'Matrix' required by 'lme4' could not be found
> 
>  What do you suggest?
>  Spencer Graves
> p.s.  I get the same result using several different (US) mirrors.

I found it here:

http://cran.cnr.berkeley.edu/src/contrib/Matrix_0.99-1.tar.gz

and here:

http://cran.us.r-project.org/src/contrib/Matrix_0.99-1.tar.gz

at least.  These are the only two I checked.

Sean



From costas.magnuse at gmail.com  Fri Nov 11 15:07:40 2005
From: costas.magnuse at gmail.com (Constantine Tsardounis)
Date: Fri, 11 Nov 2005 16:07:40 +0200
Subject: [R] Inputing data from multiple files as time series objects
Message-ID: <30ddfdae0511110607l565e0dagd5da6041b82f9b24@mail.gmail.com>

Hello to everyone,...

I am a new R ambitious user. I would like to be the first at my
department using R, but I have encountered a difficulty during the
last days that I cannot overcome reading help() and searching over the
net.
Problem:
I have multiple files with financial data like the following (header included):
E.g.:
filename: AOL.txt
aol.txt
4
3
5
3...

filename: IBM.txt
ibm.txt
6
2
5
2...

I would like to input these data in R as time-series objects with
their corresponding names automatically, so that I can manipulate them
(exempli gratia plot acf, pacf, differntiate them, make adf tests,
etc)
For example: I could do that by hand using:
AOL <- ts(read.table(AOL.txt, header = TRUE))))
IBM <- ts(read.table(IBM.txt, header = TRUE))))
but is there another way to achieve the same actions as above with a
more versatile way? (for example loops?)
Or would you suggest inputing these data with another way or as other objects?

Thank you very much in advance,...

Tsardounis Costas



From andy_liaw at merck.com  Fri Nov 11 15:13:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 11 Nov 2005 09:13:12 -0500
Subject: [R] no package 'Matrix' at the repositories
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED597@usctmx1106.merck.com>

If you are installing from Windows, the current version of Matrix apparently
doesn't build automatically on Windows.  See:

http://cran.r-project.org/bin/windows/contrib/2.3/check/Matrix-check.log

(That's for R-devel.  There're similar problems on R-2.2.0.)

I'm sure Doug is aware of this...

Andy

> From: Sean Davis
> 
> On 11/11/05 8:38 AM, "Spencer Graves" <spencer.graves at pdf.com> wrote:
> 
> >  Yesterday, I installed R2.2.0 for Windows [Version 2.2.0 
> (2005-10-06
> > r35749)].  Unfortunately, 'install.packages("Matrix")' produced the
> > following message:
> > 
> > Warning in download.packages(pkgs, destdir = tmpd, available =
> > available,  :
> >         no package 'Matrix' at the repositories
> > 
> >  I installed lme4, maps, mapproj, CircStats, scatterplot3d, 
> gregmisc,
> > Hmisc without problems.  To confirm, 'library(lme4)' produced the
> > following error:
> > 
> > Error: package 'Matrix' required by 'lme4' could not be found
> > 
> >  What do you suggest?
> >  Spencer Graves
> > p.s.  I get the same result using several different (US) mirrors.
> 
> I found it here:
> 
http://cran.cnr.berkeley.edu/src/contrib/Matrix_0.99-1.tar.gz

and here:

http://cran.us.r-project.org/src/contrib/Matrix_0.99-1.tar.gz

at least.  These are the only two I checked.

Sean

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From wilks at dial.pipex.com  Fri Nov 11 15:26:26 2005
From: wilks at dial.pipex.com (John Wilkinson (pipex))
Date: Fri, 11 Nov 2005 14:26:26 -0000
Subject: [R] How to find statistics like that.
Message-ID: <JCEIJNOHMNBPLMGFDHNDGEEACBAA.wilks@dial.pipex.com>

Adai,

I recently came across the following definition of a statistic
which may be relevent to the discussion.

John
-----

Beran?s (2003) provocative definition of statistics as ?the study of
algorithms for data analysis? elevates computational considerations to the
forefront of the field. It is apparent that the evolutionary success of
statistical methods is to a significant degree determined by considerations
of computational convenience. As a result,design and dissemination of
statistical software has become an integral part of statistical research.

from this it follows that a 'Statistic' is

 " A mathematical function or  algorithm for data analysis"


--------------------
Duncan Murdoch wrote
--------------------

On 11/9/2005 10:01 PM, Adaikalavan Ramasamy wrote:
> I think an alternative is to use a p-value from F distribution. Even
> tough it is not a statistics, it is much easier to explain and popular
> than 1/F. Better yet to report the confidence intervals.

Just curious about your usage:  why do you say a p-value is not a statistic?

Duncan Murdoch

Adaikalavan Ramasamy replied
-----------------------------

If my usage is wrong please correct me. Thank you.

Here are my reason :

1. p-value is a (cumulative) probability and always ranges from 0 to 1.
A test statistic depending on its definition can wider range of possible
values.

2. A test statistics is one that is calculated from the data without the
need of assuming a null distribution. Whereas to calculate p-values, you
need to assume a null distribution or estimate it empirically using
permutation techniques.

3. The directionality of a test statistics may be ignored. For example a
t-statistics of -5 and 5 are equally interesting in a two-sided testing.
But the smaller the p-value, more evidence against the null hypothesis.

Regards, Adai



From sdavis2 at mail.nih.gov  Fri Nov 11 15:24:31 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 11 Nov 2005 09:24:31 -0500
Subject: [R] Inputing data from multiple files as time series objects
In-Reply-To: <30ddfdae0511110607l565e0dagd5da6041b82f9b24@mail.gmail.com>
Message-ID: <BF9A13CF.12B00%sdavis2@mail.nih.gov>

On 11/11/05 9:07 AM, "Constantine Tsardounis" <costas.magnuse at gmail.com>
wrote:

> Hello to everyone,...
> 
> I am a new R ambitious user. I would like to be the first at my
> department using R, but I have encountered a difficulty during the
> last days that I cannot overcome reading help() and searching over the
> net.
> Problem:
> I have multiple files with financial data like the following (header
> included):
> E.g.:
> filename: AOL.txt
> aol.txt
> 4
> 3
> 5
> 3...
> 
> filename: IBM.txt
> ibm.txt
> 6
> 2
> 5
> 2...
> 
> I would like to input these data in R as time-series objects with
> their corresponding names automatically, so that I can manipulate them
> (exempli gratia plot acf, pacf, differntiate them, make adf tests,
> etc)
> For example: I could do that by hand using:
> AOL <- ts(read.table(AOL.txt, header = TRUE))))
> IBM <- ts(read.table(IBM.txt, header = TRUE))))
> but is there another way to achieve the same actions as above with a
> more versatile way? (for example loops?)
> Or would you suggest inputing these data with another way or as other objects?

myts <- list()
mydir <- 'path/to/files'
# put all files in the same directory
myfiles <- dir('path/to/files',pattern='.txt')
for (i in myfiles) {
  myts[[sub('.txt','',i)]] <-
    ts(read.table(paste(mydir,myfiles,sep='/'),header=T))
}

This will give you back a list of ts objects based on all txt files in a
directory.  I haven't tested the code, but I hope you get the idea.

Sean



From Michaell.Taylor at boxwoodmeans.com  Fri Nov 11 15:41:22 2005
From: Michaell.Taylor at boxwoodmeans.com (Michaell Taylor)
Date: Fri, 11 Nov 2005 08:41:22 -0600
Subject: [R] Inputing data from multiple files as time series objects
In-Reply-To: <30ddfdae0511110607l565e0dagd5da6041b82f9b24@mail.gmail.com>
References: <30ddfdae0511110607l565e0dagd5da6041b82f9b24@mail.gmail.com>
Message-ID: <200511110841.22871.Michaell.Taylor@boxwoodmeans.com>



Assuming all the data are in a subdirectory called "Data" and all the files 
have the '.txt' extension, you could do something like.

files <- list.files('Data')
for (file in files){
	temp <- ts(read.table(file, header = T))
	vname <- sub('.txt','',file)
	assign(vname,temp,envir=.Globalenv)
	}


On Friday 11 November 2005 08:07 am, Constantine Tsardounis wrote:
> Hello to everyone,...
>
> I am a new R ambitious user. I would like to be the first at my
> department using R, but I have encountered a difficulty during the
> last days that I cannot overcome reading help() and searching over the
> net.
> Problem:
> I have multiple files with financial data like the following (header
> included): E.g.:
> filename: AOL.txt
> aol.txt
> 4
> 3
> 5
> 3...
>
> filename: IBM.txt
> ibm.txt
> 6
> 2
> 5
> 2...
>
> I would like to input these data in R as time-series objects with
> their corresponding names automatically, so that I can manipulate them
> (exempli gratia plot acf, pacf, differntiate them, make adf tests,
> etc)
> For example: I could do that by hand using:
> AOL <- ts(read.table(AOL.txt, header = TRUE))))
> IBM <- ts(read.table(IBM.txt, header = TRUE))))
> but is there another way to achieve the same actions as above with a
> more versatile way? (for example loops?)
> Or would you suggest inputing these data with another way or as other
> objects?
>
> Thank you very much in advance,...
>
> Tsardounis Costas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
=======================================
Michaell Taylor, PhD.
Principal
Boxwood Means, Inc.
203.653.4100



From pgreen at umich.edu  Fri Nov 11 15:49:50 2005
From: pgreen at umich.edu (Paul E. Green)
Date: Fri, 11 Nov 2005 09:49:50 -0500
Subject: [R] Lattice (Trellis)  plot margins
Message-ID: <000c01c5e6cf$2e153e10$62bbd38d@adsroot.itcs.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051111/15185701/attachment.pl

From chrysopa at gmail.com  Fri Nov 11 16:12:48 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Fri, 11 Nov 2005 13:12:48 -0200
Subject: [R] problems with for: warnings and segfault
Message-ID: <200511111312.48549.chrysopa@gmail.com>

Hi,

I have two problem with a for looping using R Version 2.1.1  (2005-06-20) on a 
Debian Linux Testing.

The first problem: warnings messages

Look:

> xcoord <- 5
> ycoord <- 5
> indice <- 1
> for(i in c(1:5)) {indice <- indice+1;xcoord[indice] <- xcoord+i; 
ycoord[indice] <- ycoord }
Warning messages:
1: number of items to replace is not a multiple of replacement length 
2: number of items to replace is not a multiple of replacement length 
3: number of items to replace is not a multiple of replacement length 
4: number of items to replace is not a multiple of replacement length 
5: number of items to replace is not a multiple of replacement length 
6: number of items to replace is not a multiple of replacement length 
7: number of items to replace is not a multiple of replacement length 
8: number of items to replace is not a multiple of replacement length 

> xcoord
[1]  5  6  7  8  9 10
> ycoord
[1] 5 5 5 5 5 5
> 

The results are OK, but I dont understand the warning message

The second problem: The segfault

> xcoord <- 5
> ycoord <- 5
> indice <- 1
> for(i in c(1:100)) {indice <- indice+1;xcoord[indice] <- xcoord+i; 
ycoord[indice] <- ycoord }
Segmentation fault

This is a R bug or an error in my for function?

Thanks
Ronaldo

-- 

Ningu??m ?? t??o mais inteligente que o outro para que o c??rebro conte mais que a 
perseveran??a

--Alan Green
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From dray at biomserv.univ-lyon1.fr  Fri Nov 11 16:24:40 2005
From: dray at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?St=E9phane_Dray?=)
Date: Fri, 11 Nov 2005 16:24:40 +0100
Subject: [R] A 'sweave' strange problem !!!
Message-ID: <4374B7B8.2020904@biomserv.univ-lyon1.fr>

Hello list,
I have found a problem (bug?) with Sweave. I hope that someone could 
help me.
Try this little example :
\documentclass[a4paper]{article}

\title{toto}
\author{toto}

\begin{document}

\maketitle



<<ni1, fig=T, eval=TRUE, echo=TRUE, debug=TRUE, results=verbatim, 
include =FALSE, width=7, height=7>>=
a<-rnorm(1)+1
a
plot(1:10)
@
patati


<<ni2, fig=F, eval=TRUE, echo=TRUE, debug=TRUE, results=verbatim, 
include =FALSE, width=7, height=7>>=
a
@
\end{document}

The value of 'a' has change between the two Schunks. It seems that the 
problem only appear when there are plot (fig=T) in the first one. 
Without plot, there are no problems: a remains unchanged. Is it a bug or 
have I misundertsood something ?

Thanks a lot !
R version 2.1.0 (Debian).

-- 
St??phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://www.steph280.freesurf.fr/



From ManuelPerera-Chang at fmc-ag.com  Fri Nov 11 16:24:48 2005
From: ManuelPerera-Chang at fmc-ag.com (ManuelPerera-Chang@fmc-ag.com)
Date: Fri, 11 Nov 2005 16:24:48 +0100
Subject: [R] Inputing data from multiple files as time series objects
Message-ID: <OF64A758AD.1407B5B6-ONC12570B6.004F4E50-C12570B6.0054AB5B@notes.fresenius.de>





Hi Constantine,

I have not tested it but try ...

y<-c("IBM","MS","DELL","SIEMENS","SUN")

read.data.myfunction<-function(x){
                  for(i in 1:length(x))
                  {
                  paste(x[i],sep="") <-
ts(read.table(paste(x[i],".txt",sep=""), header = TRUE))
                  }
                  }

read.data.myfunction(y)


Manuel



                                                                                                                                           
                      Constantine                                                                                                          
                      Tsardounis                   To:       r-help at stat.math.ethz.ch                                                      
                      <costas.magnuse at gmail        cc:                                                                                     
                      .com>                        Subject:  [R] Inputing data from multiple files as time series objects                  
                      Sent by:                                                                                                             
                      r-help-bounces at stat.m                                                                                                
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      11.11.2005 15:07                                                                                                     
                                                                                                                                           
                                                                                                                                           




Hello to everyone,...

I am a new R ambitious user. I would like to be the first at my
department using R, but I have encountered a difficulty during the
last days that I cannot overcome reading help() and searching over the
net.
Problem:
I have multiple files with financial data like the following (header
included):
E.g.:
filename: AOL.txt
aol.txt
4
3
5
3...

filename: IBM.txt
ibm.txt
6
2
5
2...

I would like to input these data in R as time-series objects with
their corresponding names automatically, so that I can manipulate them
(exempli gratia plot acf, pacf, differntiate them, make adf tests,
etc)
For example: I could do that by hand using:
AOL <- ts(read.table(AOL.txt, header = TRUE))))
IBM <- ts(read.table(IBM.txt, header = TRUE))))
but is there another way to achieve the same actions as above with a
more versatile way? (for example loops?)
Or would you suggest inputing these data with another way or as other
objects?

Thank you very much in advance,...

Tsardounis Costas

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Fri Nov 11 16:30:40 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 11 Nov 2005 07:30:40 -0800
Subject: [R] Lattice (Trellis)  plot margins
In-Reply-To: <000c01c5e6cf$2e153e10$62bbd38d@adsroot.itcs.umich.edu>
Message-ID: <200511111530.jABFUexP008436@meitner.gene.com>

To paraphrase a recent reply by the inimitable Deepayan Sarkar, take a look
at trellis.par.get('layout.heights'). Playing with some of those arguments
(especially the "padding" ones) may get you what you want.

Subject to correction by DS, himself, of course ...

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul E. Green
> Sent: Friday, November 11, 2005 6:50 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Lattice (Trellis) plot margins
> 
> Is there an equivalent way to set plotting parameters
> as in
> 
> par(mai=c(......))
> 
> in the Lattice package? I searched trellis.par.get(), but am not
> sure which one can be used.
> 
> Paul E. Green
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tom at maladmin.com  Fri Nov 11 11:39:16 2005
From: tom at maladmin.com (tom wright)
Date: Fri, 11 Nov 2005 05:39:16 -0500
Subject: [R] curve fitting question
Message-ID: <1131705556.4819.28.camel@localhost.localdomain>

I'd appreciate some direction here.
I have a model for a system with two independant variables (i1,i2) and
one dependant variable (d).
I have experimental data recorded at multiple levels of the dependant
variable (x).
I need to work out the values for the independant variables that best
fit the experimental data recorded for all the dependant variables.
I assume I'm going to need the glm() function but I'd really appreciate
some pointers in how to actually do this.
Do I need to calculate the fits for a range of values of i1 and i2 for
each value of d and then look to see what values of i1 and i2 give the
best fit over all the experimental data or is there a way of doing this
automatically?
Please feel free to point me at other reading sources.

Many thanks
Tom



From p.dalgaard at biostat.ku.dk  Fri Nov 11 16:34:10 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Nov 2005 16:34:10 +0100
Subject: [R] problems with for: warnings and segfault
In-Reply-To: <200511111312.48549.chrysopa@gmail.com>
References: <200511111312.48549.chrysopa@gmail.com>
Message-ID: <x21x1ni4gd.fsf@viggo.kubism.ku.dk>

"Ronaldo Reis-Jr." <chrysopa at gmail.com> writes:

> Hi,
> 
> I have two problem with a for looping using R Version 2.1.1  (2005-06-20) on a 
> Debian Linux Testing.
> 
> The first problem: warnings messages
> 
> Look:
> 
> > xcoord <- 5
> > ycoord <- 5
> > indice <- 1
> > for(i in c(1:5)) {indice <- indice+1;xcoord[indice] <- xcoord+i; 
> ycoord[indice] <- ycoord }
> Warning messages:
> 1: number of items to replace is not a multiple of replacement length 
> 2: number of items to replace is not a multiple of replacement length 
> 3: number of items to replace is not a multiple of replacement length 
> 4: number of items to replace is not a multiple of replacement length 
> 5: number of items to replace is not a multiple of replacement length 
> 6: number of items to replace is not a multiple of replacement length 
> 7: number of items to replace is not a multiple of replacement length 
> 8: number of items to replace is not a multiple of replacement length 
> 
> > xcoord
> [1]  5  6  7  8  9 10
> > ycoord
> [1] 5 5 5 5 5 5
> > 
> 
> The results are OK, but I dont understand the warning message

Look at

  xcoord[indice] <- xcoord+i

The left hand side is a single element, the right hand side is a
vector so I don't think it means what I think you think it means.

> The second problem: The segfault
> 
> > xcoord <- 5
> > ycoord <- 5
> > indice <- 1
> > for(i in c(1:100)) {indice <- indice+1;xcoord[indice] <- xcoord+i; 
> ycoord[indice] <- ycoord }
> Segmentation fault
> 
> This is a R bug or an error in my for function?

R shouldn't segfault no matter how silly the code....

[I see this on 2.2.0/Linux too, on the 2nd try of the for loop]

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Fri Nov 11 16:46:48 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 11 Nov 2005 16:46:48 +0100
Subject: [R] [Fwd: Re:  no package 'Matrix' at the repositories]
Message-ID: <4374BCE8.3050602@statistik.uni-dortmund.de>

[Resend the stuff below since initial one has been blocked from R-help.]

I have moved the "old" Matrix_0.98-7.zip to the main repository for the 
meantime.

Best,
Uwe Ligges



-------- Original Message --------
Subject: Re: [R] no package 'Matrix' at the repositories
Date: Fri, 11 Nov 2005 15:45:19 +0100
From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
Organization: Fachbereich Statistik, Universitaet Dortmund
To: Liaw, Andy <andy_liaw at merck.com>
CC: 'Sean Davis' <sdavis2 at mail.nih.gov>,  Spencer Graves 
<spencer.graves at pdf.com>, r-help <R-help at stat.math.ethz.ch>, Douglas 
Bates <bates at stat.wisc.edu>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED597 at usctmx1106.merck.com>

Liaw, Andy wrote:

> If you are installing from Windows, the current version of Matrix apparently
> doesn't build automatically on Windows.  See:
> 
> http://cran.r-project.org/bin/windows/contrib/2.3/check/Matrix-check.log
> 
> (That's for R-devel.  There're similar problems on R-2.2.0.)
> 
> I'm sure Doug is aware of this...


Indeed, Doug himself as well as others are aware of it and a solution is
not far away, as far as I can tell ...

The last working version of Matrix for R-2.2.x is (according to the
ReadMe at the repository) available at:

CRAN/bin/windows/contrib/2.2/last/

Uwe Ligges





> Andy
> 
> 
>>From: Sean Davis
>>
>>On 11/11/05 8:38 AM, "Spencer Graves" <spencer.graves at pdf.com> wrote:
>>
>>
>>> Yesterday, I installed R2.2.0 for Windows [Version 2.2.0 
>>
>>(2005-10-06
>>
>>>r35749)].  Unfortunately, 'install.packages("Matrix")' produced the
>>>following message:
>>>
>>>Warning in download.packages(pkgs, destdir = tmpd, available =
>>>available,  :
>>>        no package 'Matrix' at the repositories
>>>
>>> I installed lme4, maps, mapproj, CircStats, scatterplot3d, 
>>
>>gregmisc,
>>
>>>Hmisc without problems.  To confirm, 'library(lme4)' produced the
>>>following error:
>>>
>>>Error: package 'Matrix' required by 'lme4' could not be found
>>>
>>> What do you suggest?
>>> Spencer Graves
>>>p.s.  I get the same result using several different (US) mirrors.
>>
>>I found it here:
>>
> 
> http://cran.cnr.berkeley.edu/src/contrib/Matrix_0.99-1.tar.gz
> 
> and here:
> 
> http://cran.us.r-project.org/src/contrib/Matrix_0.99-1.tar.gz
> 
> at least.  These are the only two I checked.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chrysopa at gmail.com  Fri Nov 11 16:54:55 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Fri, 11 Nov 2005 13:54:55 -0200
Subject: [R] problems with for: warnings and segfault
In-Reply-To: <x21x1ni4gd.fsf@viggo.kubism.ku.dk>
References: <200511111312.48549.chrysopa@gmail.com>
	<x21x1ni4gd.fsf@viggo.kubism.ku.dk>
Message-ID: <200511111354.55452.chrysopa@gmail.com>

Em Sex 11 Nov 2005 13:34, Peter Dalgaard escreveu:
>
> Look at
>
>   xcoord[indice] <- xcoord+i
>
> The left hand side is a single element, the right hand side is a
> vector so I don't think it means what I think you think it means.

I'm stupid, of course. The correct is:

> xcoord[indice] <- xcoord[indice-1]+i
> ycoord[indice] <- ycoord[indice-1]

This error is the segfault.

Thanks
Ronaldo

-- 
So so is good, very good, very excellent good:
and yet it is not; it is but so so.
		-- William Shakespeare, "As You Like It"
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From ripley at stats.ox.ac.uk  Fri Nov 11 17:05:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Nov 2005 16:05:44 +0000 (GMT)
Subject: [R] problems with for: warnings and segfault
In-Reply-To: <200511111312.48549.chrysopa@gmail.com>
References: <200511111312.48549.chrysopa@gmail.com>
Message-ID: <Pine.LNX.4.61.0511111556040.19106@gannet.stats>

On Fri, 11 Nov 2005, Ronaldo Reis-Jr. wrote:

> Hi,
>
> I have two problem with a for looping using R Version 2.1.1  (2005-06-20) on a
> Debian Linux Testing.
>
> The first problem: warnings messages
>
> Look:
>
>> xcoord <- 5
>> ycoord <- 5
>> indice <- 1
>> for(i in c(1:5)) {indice <- indice+1;xcoord[indice] <- xcoord+i;
> ycoord[indice] <- ycoord }
> Warning messages:
> 1: number of items to replace is not a multiple of replacement length
> 2: number of items to replace is not a multiple of replacement length
> 3: number of items to replace is not a multiple of replacement length
> 4: number of items to replace is not a multiple of replacement length
> 5: number of items to replace is not a multiple of replacement length
> 6: number of items to replace is not a multiple of replacement length
> 7: number of items to replace is not a multiple of replacement length
> 8: number of items to replace is not a multiple of replacement length
>
>> xcoord
> [1]  5  6  7  8  9 10
>> ycoord
> [1] 5 5 5 5 5 5
>>
>
> The results are OK, but I dont understand the warning message

At step one you have xcoord[2] <- 5+1, so xcoord is c(5,6)
At step two you have xcoord[3] <- xcoord+2, so you are trying to replace 
one value with two, and the same for ycoord.
...

It is much better practice to create a vector with the size you are going 
to need it.

> The second problem: The segfault
>
>> xcoord <- 5
>> ycoord <- 5
>> indice <- 1
>> for(i in c(1:100)) {indice <- indice+1;xcoord[indice] <- xcoord+i;
> ycoord[indice] <- ycoord }
> Segmentation fault
>
> This is a R bug or an error in my for function?

Both since R should not segfault, but mainly the latter.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From stratja at auburn.edu  Fri Nov 11 17:58:41 2005
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Fri, 11 Nov 2005 10:58:41 -0600
Subject: [R] glm x^2
Message-ID: <43747961020000F2000010F0@TMIA1.AUBURN.EDU>

R-users,

I'm having some trouble getting .glm and glm.nb to run a polynomial. 
I've used x*x and x^2 and neither works.  I've checked out the archives
and they refer to an archive that's no longer working.  

I've seen that they use poly() but I'm following up my analysis with
cv.glm so I'd prefer to keep using glm.  It's easier to just add a
column to my data but I'd rather code it.

Thanks for the response... I appreciate the people that work on the
list.

Jeff

****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja



From sundar.dorai-raj at pdf.com  Fri Nov 11 18:05:37 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 11 Nov 2005 11:05:37 -0600
Subject: [R] glm x^2
In-Reply-To: <43747961020000F2000010F0@TMIA1.AUBURN.EDU>
References: <43747961020000F2000010F0@TMIA1.AUBURN.EDU>
Message-ID: <4374CF61.2020002@pdf.com>

I think you want:

glm(y ~ x + I(x^2), ...)

This is shown as an example on pg 50 of "An Introduction to R" 
(R-2.2.0-pdf).

HTH,

--sundar

Jeffrey Stratford wrote:
> R-users,
> 
> I'm having some trouble getting .glm and glm.nb to run a polynomial. 
> I've used x*x and x^2 and neither works.  I've checked out the archives
> and they refer to an archive that's no longer working.  
> 
> I've seen that they use poly() but I'm following up my analysis with
> cv.glm so I'd prefer to keep using glm.  It's easier to just add a
> column to my data but I'd rather code it.
> 
> Thanks for the response... I appreciate the people that work on the
> list.
> 
> Jeff
> 
> ****************************************
> Jeffrey A. Stratford, Ph.D.
> Postdoctoral Associate
> 331 Funchess Hall
> Department of Biological Sciences
> Auburn University
> Auburn, AL 36849
> 334-329-9198
> FAX 334-844-9234
> http://www.auburn.edu/~stratja
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Fri Nov 11 18:25:55 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 11 Nov 2005 09:25:55 -0800 (PST)
Subject: [R] different functions on different vector subsets
In-Reply-To: <s374a12f.057@wisemail.weizmann.ac.il>
References: <s374a12f.057@wisemail.weizmann.ac.il>
Message-ID: <Pine.LNX.4.63a.0511110922060.21453@homer22.u.washington.edu>

On Fri, 11 Nov 2005, Ron Ophir wrote:

> I thought about other cases but I have to dissagree with you. For
> logical vector NA is no decision and that should be the results of it.

I would say NA is "missing": it means that the result could be any valid 
value and we don't know which one it is.  That's how NA works in all other 
computations. It doesn't really affect your argument below, though.

> Let's say that -b- is a result of comparison not of -a- to something
> rather a compsrison of -c- to -d-. In this case NA in the first position
> is a result of NA in either -c- or -d- or both. Now, if the result I
> wanted to get from your example
>
> a<-c(1,2,3,4)
> b<-c(NA,T,F,T)
>
> a[b]<-7
> is c(1,7,7,7) I should replace the NA (no decision) with F and if to
> get c(7,7,7,7)  the NA should be replaced by T otherwise the result
> should be c(NA,7,7,7). The first two option are possible to perform in R
> the third is not and that is to the user to decide which to choose.

But as Brian pointed out, what if a is a list? There is no NA value for 
the list type.  So assignment with an NA subscript cannot be made 
consistent across even the basic vector types.

 	-thomas


> Ron
>
>>>> Thomas Lumley <tlumley at u.washington.edu> 11/10/05 11:02 PM >>>
> On Thu, 10 Nov 2005, Ron Ophir wrote:
>
>> Thanks Thomas,
>>
>> "...For logical subscripts you could argue that the
>> ambiguity isn't present and that if the index was NA the element
> should
>> just be set to NA. This change might be worth making."
>>
>> I see you got my point. NA should return NA no matter what the
>> comparison is.
>
> I'm not sure that I did get your point.  As Brian said, you aren't
> specifying whether or not to set the value. In your example it didn't
> matter because it would end up NA either way.
>
> I was saying that for eg
>
> a<-c(1,2,3,4)
> b<-c(NA,T,F,T)
>
> a[b]<-7
>
> we could relax the prohibition on NA indexing to give c(NA,7,7,7) as the
>
> result. In your case that would give what you wanted, but in other cases
>
> it might not.
>
>
> 	-thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From jjmichael at cc.usu.edu  Fri Nov 11 18:37:01 2005
From: jjmichael at cc.usu.edu (Jake Michaelson)
Date: Fri, 11 Nov 2005 10:37:01 -0700
Subject: [R] conditional coloring of image labels
Message-ID: <200511111037.01615.jjmichael@cc.usu.edu>

Hi all,

I am interested in plotting a heatmap of a set of genes.  I would like the 
text labels of these genes to be colored red rather than black if they meet  
certain statistical criteria (using an if statement).  I'm not sure how to 
change individual color labels without changing them all.  Can anyone provide 
some insight on how to do this?

Thanks in advance,

Jake



From dhiren22 at hotmail.com  Fri Nov 11 19:12:37 2005
From: dhiren22 at hotmail.com (Dhiren DSouza)
Date: Fri, 11 Nov 2005 13:12:37 -0500
Subject: [R]  split data into training and testing sets
Message-ID: <BAY102-F14F8E3BC4EAFCE81D01FA7D3590@phx.gbl>

How can I split a dataset randomly into a training and testing set.  I would 
like to have the ability to specify the size of the training set and use the 
remaining data as the testing set.

For example 90% training data and 10% testing data split.  Is there a 
function that will accomplish this?

Thank you,

-Dhiren

Rutgers University
Graduate Student



From sundar.dorai-raj at pdf.com  Fri Nov 11 19:18:06 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 11 Nov 2005 12:18:06 -0600
Subject: [R] split data into training and testing sets
In-Reply-To: <BAY102-F14F8E3BC4EAFCE81D01FA7D3590@phx.gbl>
References: <BAY102-F14F8E3BC4EAFCE81D01FA7D3590@phx.gbl>
Message-ID: <4374E05E.9070909@pdf.com>



Dhiren DSouza wrote:
> How can I split a dataset randomly into a training and testing set.  I would 
> like to have the ability to specify the size of the training set and use the 
> remaining data as the testing set.
> 
> For example 90% training data and 10% testing data split.  Is there a 
> function that will accomplish this?
> 
> Thank you,
> 
> -Dhiren
> 
> Rutgers University
> Graduate Student
> 

See ?sample.

sub <- sample(nrow(x), floor(nrow(x) * 0.9))
training <- x[sub, ]
testing <- x[-sub, ]

HTH,

--sundar



From mbmiller at taxa.epi.umn.edu  Fri Nov 11 19:25:27 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Fri, 11 Nov 2005 12:25:27 -0600 (CST)
Subject: [R] problems with for: warnings and segfault
In-Reply-To: <200511111312.48549.chrysopa@gmail.com>
References: <200511111312.48549.chrysopa@gmail.com>
Message-ID: <Pine.GSO.4.60.0511111223560.29260@taxa.epi.umn.edu>

On Fri, 11 Nov 2005, Ronaldo Reis-Jr. wrote:

> Segmentation fault
>
> This is a R bug or an error in my for function?


All seg faults are bugs.

Mike



From ripley at stats.ox.ac.uk  Fri Nov 11 19:30:29 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Nov 2005 18:30:29 +0000 (GMT)
Subject: [R] split data into training and testing sets
In-Reply-To: <BAY102-F14F8E3BC4EAFCE81D01FA7D3590@phx.gbl>
References: <BAY102-F14F8E3BC4EAFCE81D01FA7D3590@phx.gbl>
Message-ID: <Pine.LNX.4.61.0511111828250.21948@gannet.stats>

On Fri, 11 Nov 2005, Dhiren DSouza wrote:

> How can I split a dataset randomly into a training and testing set.  I would
> like to have the ability to specify the size of the training set and use the
> remaining data as the testing set.
>
> For example 90% training data and 10% testing data split.  Is there a
> function that will accomplish this?

Yes, see ?sample: use it to sample indices.
There are lots of examples around, e.g. in ?lda.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Nov 11 19:46:51 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Nov 2005 19:46:51 +0100
Subject: [R] problems with for: warnings and segfault
In-Reply-To: <Pine.GSO.4.60.0511111223560.29260@taxa.epi.umn.edu>
References: <200511111312.48549.chrysopa@gmail.com>
	<Pine.GSO.4.60.0511111223560.29260@taxa.epi.umn.edu>
Message-ID: <x2k6ffggys.fsf@viggo.kubism.ku.dk>

Mike Miller <mbmiller at taxa.epi.umn.edu> writes:

> On Fri, 11 Nov 2005, Ronaldo Reis-Jr. wrote:
> 
> > Segmentation fault
> >
> > This is a R bug or an error in my for function?
> 
> 
> All seg faults are bugs.

Within reason... If users go out of their way to cause havoc, e.g.
calling C/Fortran entry points with incorrect parameters, they deserve
what they get. "Normal" abuse of the language shouldn't crash R. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From patrick.giraudoux at univ-fcomte.fr  Fri Nov 11 20:12:17 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Fri, 11 Nov 2005 20:12:17 +0100
Subject: [R] repeated values, nlme, correlation structures
Message-ID: <4374ED11.6010401@univ-fcomte.fr>

Dear listers,

My request of last week seems not to have drawn someone's attention. 
Suppose it was not clear enough.

I am coping with an observational study where people's aim was to fit 
growth curve for a population of young blue tits. For logistic reasons, 
people have not been capable to number each individual, but they have a 
method to assess their age. Thus, nestboxes were visited occasionnally, 
youngs aged and weighted.

This makes a multilevel data set, with two classification factors:

- the nestbox (youngs shared the same parents and general feeding 
conditions)
- age in each nestbox (animals from the same nestbox have been weighed 
along time, which likely leads to time correlation)

Life would have been heaven if individuals were numbered, and thus nlme 
correlation structure implemented in the package be used easy. As 
mentioned above, this could not be the case. In a first approach, I 
actually used the mean weight of the youngs weighed at each age in nest 
boxes for the variable "age", and could get a nice fit with "nestbox" as 
random variable and corCAR1(form=~age|nestbox) as covariation structure.

modm0c<-nlme(pds~Asym/(1+exp((xmid-age)/scal)),
    fixed=list(Asym~1,xmid~1,scal~1),
    random=Asym+xmid~1|nestbox,data=croispulm,
    start=list(fixed=c(10,5,2.2)),
    method="ML",
    corr=corCAR1(form=~age|nestbox)
    )

Assuming that I did not commited some error in setting model parameters 
(?), this way of doing is not fully satisfying, since using the mean of 
each age category as variable  leads to a  loss of information regarding 
the variance on the weight at each age and nestbox.

My question is: is there a way to handle repeated values per group (here 
several youngs in an age category in each nestbox) in such a case?

I would really appreciate an answer, even negative...

Kind regards,

Patrick



From jjmichael at cc.usu.edu  Fri Nov 11 20:36:44 2005
From: jjmichael at cc.usu.edu (Jake Michaelson)
Date: Fri, 11 Nov 2005 12:36:44 -0700
Subject: [R] conditional coloring of image labels
In-Reply-To: <200511111037.01615.jjmichael@cc.usu.edu>
References: <200511111037.01615.jjmichael@cc.usu.edu>
Message-ID: <200511111236.44511.jjmichael@cc.usu.edu>

On Friday 11 November 2005 10:37 am, Jake Michaelson wrote:

I'll clarify a little and hopefully this will make more sense (thanks for the 
friendly encouragement):

Let's say I have 6 samples and am looking at 3 genes, with intensities in a 
matrix as follows:

> genes=cbind(ABC1=c(3,4,4,5,6,3), ABC2=c(4,3,4,7,7,8), ABC3=c(8,7,8,6,3,2))
> genes
     ABC1 ABC2 ABC3
[1,]    3    4    8
[2,]    4    3    7
[3,]    4    4    8
[4,]    5    7    6
[5,]    6    7    3
[6,]    3    8    2

###plot the image
>image(1:nrow(genes), 1:ncol(genes), genes, axes = FALSE, xlab = "", ylab = 
"", col=cm.colors(256))
 
###label the axes
>axis(1, 1:nrow(genes), labels = rownames(genes), las = 2, line = -0.5, tick = 
0)

>axis(2, 1:ncol(genes), labels = colnames(genes), las = 2, line = -0.5, tick = 
0)

Now let's say (I'm just making these numbers and the scenario up here -- for 
simplicity's sake) that I had run a statistical analysis previously and 
wanted to label the genes that showed a significance of p< 0.05.  Let's say 
that ABC1 and ABC3 had p<0.05 (assume that these values would be in a 
two-column matrix with the gene name and its p-value). 

> sig=cbind(name=c("ABC1", "ABC2", "ABC3"), pvalue=c(0.005, 0.1, 0.001))
> sig
     name   pvalue 
[1,] "ABC1" "0.005"
[2,] "ABC2" "0.1"  
[3,] "ABC3" "0.001"

 I now want these (the names of the significant genes) to be labeled in red 
rather than black on the plot.  I would eventually write a script that would 
generate a large number of these images, each with a different set of genes. 
I would like to insert some sort of conditional formatting so that if that 
gene meets the significance threshold, the name is automatically plotted in 
red on the plot.

I hope this is more clear and effective in explaining what I'm looking for.


Thanks,

--Jake




> Hi all,
>
> I am interested in plotting a heatmap of a set of genes.  I would like the
> text labels of these genes to be colored red rather than black if they meet
> certain statistical criteria (using an if statement).  I'm not sure how to
> change individual color labels without changing them all.  Can anyone
> provide some insight on how to do this?
>
> Thanks in advance,
>
> Jake
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From dejongroel at gmail.com  Fri Nov 11 20:39:32 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Fri, 11 Nov 2005 20:39:32 +0100
Subject: [R] simulation study using AD model builder to fit a GLMM under the
 binomial probit link
Message-ID: <4374F374.6000307@gmail.com>

I recently took Dave Fournier up on his offer to evaluate his AD Model 
builder package (http://otter-rsch.com/admodel.html) when fitting a GLMM 
under the binomial probit link.

I conducted a simulation study in which I drawed 500 samples each 
containing 1500 observations from the following model specification:

y = (intercept*f1+pred2*f2+pred3*f3)+(intercept*ri+pred2*rs)
	where pred2 and pred3 are predictors distributed N(0,1)
	f1..f3 are fixed effects, f1=-1, f2=1.5, f3=0.5
	ri is random intercept with associated variance var_ri=0.2
	rs is random slope with associated variance var_rs=0.4
	the covariance between ri and rs=0

we have 50 level 2 units, so 30 observations/level 2 unit

I then proceeded with the analysis of the 500 samples with the AD Model 
builder package. To check for bias, I calculated the average of the 
parameter estimates of the 500 samples and compared them to the true 
population parameters. There was virtually no bias:

parameter	average	parameter estimate		true value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
f1		-1.001					-1.000
f2		1.510					1.500
f3		0.499					0.500
var_ri		0.197					0.200
var_rs		0.396					0.400

Then I checked the coverage with alpha=0.95, where asymmetrical 
confidence intervals were calculated for the variance components:

parameter	coverage (alpha=0.95)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
f1		.928
f2		.948
f3		.956
var_ri		.960
var_rs 		.984

The coverages are quite good, only the variance of the random slope is 
high, which suggests that the associated standard error is too large.

Where AD model builder really shines is the fact that convergence was 
reached without problems in all 500 samples, where R alternatives like 
lmer and glmmPQL, which use Penalized Quasi Likelihood, tend to run in 
computational problems. I therefore highly recommend the software for 
analyzing binomial mixed models, and I encourage Dave to add it to his 
existing negative binomial package for R.

Regards,
	Roel de Jong



From jjmichael at cc.usu.edu  Fri Nov 11 21:17:19 2005
From: jjmichael at cc.usu.edu (Jake Michaelson)
Date: Fri, 11 Nov 2005 13:17:19 -0700
Subject: [R] Fwd: Re:  conditional coloring of image labels
Message-ID: <200511111317.19063.jjmichael@cc.usu.edu>



----------  Forwarded Message  ----------

Subject: Re: [R] conditional coloring of image labels
Date: Friday 11 November 2005 1:04 pm
From: jim holtman <jholtman at gmail.com>
To: Jake Michaelson <jjmichael at cc.usu.edu>

Use 'mtext':

genes=cbind(ABC1=c(3,4,4,5,6,3), ABC2=c(4,3,4,7,7,8), ABC3=c(8,7,8,6,3,2))
###plot the image
image(1:nrow(genes), 1:ncol(genes), genes, axes = FALSE, xlab = "", ylab =
"", col=cm.colors(256))

sig=cbind(name=c("ABC1", "ABC2", "ABC3"), pvalue=c(0.005, 0.1, 0.001))

###label the axes
axis(1, 1:nrow(genes), labels = rownames(genes), las = 2, line = -0.5, tick
=
0)

mtext(colnames(genes), side=2, las = 2, line = 1, at=1:3,
col=ifelse(sig[, 'pvalue'] == '0.1', 'red', 'black'))

 On 11/11/05, Jake Michaelson <jjmichael at cc.usu.edu> wrote:
> On Friday 11 November 2005 10:37 am, Jake Michaelson wrote:
>
> I'll clarify a little and hopefully this will make more sense (thanks for
> the
> friendly encouragement):
>
> Let's say I have 6 samples and am looking at 3 genes, with intensities in
> a
>
> matrix as follows:
> > genes=cbind(ABC1=c(3,4,4,5,6,3), ABC2=c(4,3,4,7,7,8),
>
> ABC3=c(8,7,8,6,3,2))
>
> > genes
>
> ABC1 ABC2 ABC3
> [1,] 3 4 8
> [2,] 4 3 7
> [3,] 4 4 8
> [4,] 5 7 6
> [5,] 6 7 3
> [6,] 3 8 2
>
> ###plot the image
>
> >image(1:nrow(genes), 1:ncol(genes), genes, axes = FALSE, xlab = "", ylab
>
> =
> "", col=cm.colors(256))
>
> ###label the axes
>
> >axis(1, 1:nrow(genes), labels = rownames(genes), las = 2, line = -0.5,
>
> tick =
> 0)
>
> >axis(2, 1:ncol(genes), labels = colnames(genes), las = 2, line = -0.5,
>
> tick =
> 0)
>
> Now let's say (I'm just making these numbers and the scenario up here --
> for
> simplicity's sake) that I had run a statistical analysis previously and
> wanted to label the genes that showed a significance of p< 0.05. Let's say
> that ABC1 and ABC3 had p<0.05 (assume that these values would be in a
> two-column matrix with the gene name and its p-value).
>
> > sig=cbind(name=c("ABC1", "ABC2", "ABC3"), pvalue=c(0.005, 0.1, 0.001))
> > sig
>
> name pvalue
> [1,] "ABC1" "0.005"
> [2,] "ABC2" "0.1"
> [3,] "ABC3" "0.001"
>
> I now want these (the names of the significant genes) to be labeled in red
> rather than black on the plot. I would eventually write a script that
> would
> generate a large number of these images, each with a different set of
> genes.
> I would like to insert some sort of conditional formatting so that if that
> gene meets the significance threshold, the name is automatically plotted
> in
> red on the plot.
>
> I hope this is more clear and effective in explaining what I'm looking
> for.
>
>
> Thanks,
>
> --Jake
>
> > Hi all,
> >
> > I am interested in plotting a heatmap of a set of genes. I would like
>
> the
>
> > text labels of these genes to be colored red rather than black if they
>
> meet
>
> > certain statistical criteria (using an if statement). I'm not sure how
>
> to
>
> > change individual color labels without changing them all. Can anyone
> > provide some insight on how to do this?
> >
> > Thanks in advance,
> >
> > Jake
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html

--
Jim Holtman
Cincinnati, OH
+1 513 247 0281

What the problem you are trying to solve?



From ripley at stats.ox.ac.uk  Fri Nov 11 21:23:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Nov 2005 20:23:15 +0000 (GMT)
Subject: [R] problems with for: warnings and segfault
In-Reply-To: <x2k6ffggys.fsf@viggo.kubism.ku.dk>
References: <200511111312.48549.chrysopa@gmail.com>
	<Pine.GSO.4.60.0511111223560.29260@taxa.epi.umn.edu>
	<x2k6ffggys.fsf@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0511112017480.26830@gannet.stats>

On Fri, 11 Nov 2005, Peter Dalgaard wrote:

> Mike Miller <mbmiller at taxa.epi.umn.edu> writes:
>
>> On Fri, 11 Nov 2005, Ronaldo Reis-Jr. wrote:
>>
>>> Segmentation fault
>>>
>>> This is a R bug or an error in my for function?
>>
>>
>> All seg faults are bugs.
>
> Within reason... If users go out of their way to cause havoc, e.g.
> calling C/Fortran entry points with incorrect parameters, they deserve
> what they get. "Normal" abuse of the language shouldn't crash R.

Also, they are not necessarily bugs *in R*.  They could well be bugs in 
the compiler or the libc (or equivalent) or even the OS running out of 
resources ungracefully.  We've seen quite a few which were compiler bugs, 
and cases where a malloc failure was a segfault in the OS and not
a NULL return.

This one was a bug in R, only happening with incorrect usage.  It has now 
been fixed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lforsber at hsph.harvard.edu  Fri Nov 11 21:59:04 2005
From: lforsber at hsph.harvard.edu (Laura Forsberg White)
Date: Fri, 11 Nov 2005 15:59:04 -0500
Subject: [R] optim not giving correct minima
Message-ID: <5.2.0.9.2.20051111152820.00d0eff8@hsph.harvard.edu>

Hello,

I am trying to use optim() on a function involving a summation.  My 
function basically is a thinned poisson likelihood.  I have two parameters 
and in most cases optim() does a fine job of getting the minima.  I am 
simulating my data based on pre specified parameters, so I know what I 
should be getting.  However when my true parameters fall in a particular 
range, optim() gives incorrect results.  I have generated a grid of 
parameter values and calculated my likelihood through those to see that the 
values that optim() gives is clearly not correct.  I can see that my 
likelihood does in fact have a unique maximum.  Any ideas why this might be?

The data given below was generated such that the true parameters should be 
(0.3001, -1.8971).  Here is an example piece of data and the function:

#function to maximize
likN_alpha <- function(params,N){

     thetas <- exp(params)

     k <- length(thetas)
     N <- c(rep(0,(k-1)),N)
     l <- length(N)

     lik <- 0

     for(i in (k):(l-1)){
         lambda <- thetas%*%N[i:(i-k+1)]
         lik <- -lambda + N[i+1]*log(lambda) + lik
     }

     return(-lik)
}

# data to maximize over
N <- c( 3, 3, 10, 19, 36, 54, 78,116,177, 265, 388, 598, 
890,1328,1910,2736,3982,5908,8471,12440,17964,26207,37688,54795,79270,114752,166594,242438, 
352753,512054)

#optim() command

optim(log(1.5*rep(1/2,2)),likN.alpha,N=N)

# If I use constrained optimization and a slightly different 
parameterization, then the results are fine, at least in this case, but not 
always.

Thanks for any help you might be able to offer!

laura



From ripley at stats.ox.ac.uk  Fri Nov 11 22:22:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Nov 2005 21:22:27 +0000 (GMT)
Subject: [R] optim not giving correct minima
In-Reply-To: <5.2.0.9.2.20051111152820.00d0eff8@hsph.harvard.edu>
References: <5.2.0.9.2.20051111152820.00d0eff8@hsph.harvard.edu>
Message-ID: <Pine.LNX.4.61.0511112113230.487@gannet.stats>

Try the other methods that optim() supplies, and try supplying a 
analytical derviative (which looks easy enough).  On a problem like this I 
would expect to use BFGS with analytical derivatives.

If you don't want to do any of that, at least explore the control 
parameters.  It doesn't look to me as if you have attempted to scale the 
problem as the optim help page suggests you should.  Also, you say a 
`likelihood', but it is usual to maximize a log-likelihood. (Without 
knowing what you are trying to do in detail, I cannot tell if you are in 
fact using a log-likelihood.)

On Fri, 11 Nov 2005, Laura Forsberg White wrote:

> Hello,
>
> I am trying to use optim() on a function involving a summation.  My
> function basically is a thinned poisson likelihood.  I have two parameters
> and in most cases optim() does a fine job of getting the minima.  I am
> simulating my data based on pre specified parameters, so I know what I
> should be getting.  However when my true parameters fall in a particular
> range, optim() gives incorrect results.  I have generated a grid of
> parameter values and calculated my likelihood through those to see that the
> values that optim() gives is clearly not correct.  I can see that my
> likelihood does in fact have a unique maximum.  Any ideas why this might be?
>
> The data given below was generated such that the true parameters should be
> (0.3001, -1.8971).  Here is an example piece of data and the function:
>
> #function to maximize
> likN_alpha <- function(params,N){
>
>     thetas <- exp(params)
>
>     k <- length(thetas)
>     N <- c(rep(0,(k-1)),N)
>     l <- length(N)
>
>     lik <- 0
>
>     for(i in (k):(l-1)){
>         lambda <- thetas%*%N[i:(i-k+1)]
>         lik <- -lambda + N[i+1]*log(lambda) + lik
>     }
>
>     return(-lik)
> }
>
> # data to maximize over
> N <- c( 3, 3, 10, 19, 36, 54, 78,116,177, 265, 388, 598,
> 890,1328,1910,2736,3982,5908,8471,12440,17964,26207,37688,54795,79270,114752,166594,242438,
> 352753,512054)
>
> #optim() command
>
> optim(log(1.5*rep(1/2,2)),likN.alpha,N=N)
>
> # If I use constrained optimization and a slightly different
> parameterization, then the results are fine, at least in this case, but not
> always.
>
> Thanks for any help you might be able to offer!
>
> laura
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Fri Nov 11 15:40:45 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 11 Nov 2005 09:40:45 -0500
Subject: [R] no package 'Matrix' at the repositories
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED597@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED597@usctmx1106.merck.com>
Message-ID: <4374AD6D.5030509@stats.uwo.ca>

On 11/11/2005 9:13 AM, Liaw, Andy wrote:
> If you are installing from Windows, the current version of Matrix apparently
> doesn't build automatically on Windows.  See:
> 
> http://cran.r-project.org/bin/windows/contrib/2.3/check/Matrix-check.log
> 
> (That's for R-devel.  There're similar problems on R-2.2.0.)
> 
> I'm sure Doug is aware of this...

Yes, he is.  Brian Ripley put together a copy that will build and 
install on Windows; I think it is just a matter of a short wait before 
those patches are incorporated in the main copy and it is sent to CRAN.

Matrix is a big package, and it tests lots of code in R.  I spent a day 
earlier this week tracking down a bug in R-devel that it revealed.  It 
was a tricky one, because it only generated a warning; when warnings 
were converted to errors, the problem went away.  Luke Tierney pointed 
out a nice trick that made it easier to find such a thing, and I've 
added it to my debugging web page here:

<http://www.stats.uwo.ca/faculty/murdoch/software/debuggingR/#tryCatch>

Duncan Murdoch

> 
> Andy
> 
>> From: Sean Davis
>> 
>> On 11/11/05 8:38 AM, "Spencer Graves" <spencer.graves at pdf.com> wrote:
>> 
>> >  Yesterday, I installed R2.2.0 for Windows [Version 2.2.0 
>> (2005-10-06
>> > r35749)].  Unfortunately, 'install.packages("Matrix")' produced the
>> > following message:
>> > 
>> > Warning in download.packages(pkgs, destdir = tmpd, available =
>> > available,  :
>> >         no package 'Matrix' at the repositories
>> > 
>> >  I installed lme4, maps, mapproj, CircStats, scatterplot3d, 
>> gregmisc,
>> > Hmisc without problems.  To confirm, 'library(lme4)' produced the
>> > following error:
>> > 
>> > Error: package 'Matrix' required by 'lme4' could not be found
>> > 
>> >  What do you suggest?
>> >  Spencer Graves
>> > p.s.  I get the same result using several different (US) mirrors.
>> 
>> I found it here:
>> 
> http://cran.cnr.berkeley.edu/src/contrib/Matrix_0.99-1.tar.gz
> 
> and here:
> 
> http://cran.us.r-project.org/src/contrib/Matrix_0.99-1.tar.gz
> 
> at least.  These are the only two I checked.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Nov 11 15:48:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Nov 2005 06:48:01 -0800
Subject: [R] no package 'Matrix' at the repositories
In-Reply-To: <4374AD6D.5030509@stats.uwo.ca>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED597@usctmx1106.merck.com>
	<4374AD6D.5030509@stats.uwo.ca>
Message-ID: <4374AF21.5090300@pdf.com>

	  Thank you all for your replies and for all your hard work to make R 
what it is.  The wise course for me is probably to use R 2.1.1 when I 
need the Matrix package until this issue gets fixed.

	  Best Wishes,
	  spencer graves

Duncan Murdoch wrote:

> On 11/11/2005 9:13 AM, Liaw, Andy wrote:
> 
>> If you are installing from Windows, the current version of Matrix 
>> apparently
>> doesn't build automatically on Windows.  See:
>>
>> http://cran.r-project.org/bin/windows/contrib/2.3/check/Matrix-check.log
>>
>> (That's for R-devel.  There're similar problems on R-2.2.0.)
>>
>> I'm sure Doug is aware of this...
> 
> 
> Yes, he is.  Brian Ripley put together a copy that will build and 
> install on Windows; I think it is just a matter of a short wait before 
> those patches are incorporated in the main copy and it is sent to CRAN.
> 
> Matrix is a big package, and it tests lots of code in R.  I spent a day 
> earlier this week tracking down a bug in R-devel that it revealed.  It 
> was a tricky one, because it only generated a warning; when warnings 
> were converted to errors, the problem went away.  Luke Tierney pointed 
> out a nice trick that made it easier to find such a thing, and I've 
> added it to my debugging web page here:
> 
> <http://www.stats.uwo.ca/faculty/murdoch/software/debuggingR/#tryCatch>
> 
> Duncan Murdoch
> 
>>
>> Andy
>>
>>> From: Sean Davis
>>>
>>> On 11/11/05 8:38 AM, "Spencer Graves" <spencer.graves at pdf.com> wrote:
>>>
>>> >  Yesterday, I installed R2.2.0 for Windows [Version 2.2.0 (2005-10-06
>>> > r35749)].  Unfortunately, 'install.packages("Matrix")' produced the
>>> > following message:
>>> > > Warning in download.packages(pkgs, destdir = tmpd, available =
>>> > available,  :
>>> >         no package 'Matrix' at the repositories
>>> > >  I installed lme4, maps, mapproj, CircStats, scatterplot3d, 
>>> gregmisc,
>>> > Hmisc without problems.  To confirm, 'library(lme4)' produced the
>>> > following error:
>>> > > Error: package 'Matrix' required by 'lme4' could not be found
>>> > >  What do you suggest?
>>> >  Spencer Graves
>>> > p.s.  I get the same result using several different (US) mirrors.
>>>
>>> I found it here:
>>>
>> http://cran.cnr.berkeley.edu/src/contrib/Matrix_0.99-1.tar.gz
>>
>> and here:
>>
>> http://cran.us.r-project.org/src/contrib/Matrix_0.99-1.tar.gz
>>
>> at least.  These are the only two I checked.
>>
>> Sean
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From heil at caesar.de  Fri Nov 11 15:55:40 2005
From: heil at caesar.de (Burkhard Heil)
Date: Fri, 11 Nov 2005 15:55:40 +0100
Subject: [R] precision of double in R
Message-ID: <002101c5e6cf$dae69350$cc00030a@caesar.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051111/dbfd21e0/attachment.pl

From murdoch at stats.uwo.ca  Sat Nov 12 00:01:06 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 11 Nov 2005 18:01:06 -0500
Subject: [R] precision of double in R
In-Reply-To: <002101c5e6cf$dae69350$cc00030a@caesar.local>
References: <002101c5e6cf$dae69350$cc00030a@caesar.local>
Message-ID: <437522B2.7020708@stats.uwo.ca>

On 11/11/2005 9:55 AM, Burkhard Heil wrote:
> Hello Everybody!
> 
> 
> 
> I'm working on biological sequences and their alignements. I can't get
> around calculating something in the size of choose(1000,500) or 0.02^250.
> The result is either INF or 0 in the latter case. Is their any data type in
> R to calculate these things. Or some other solution?

Use logs.  log(0.02^250) is easy to calculate as 250*log(0.02).  There's 
also an lchoose() function; it gives

 > lchoose( 1000, 500)
[1] 689.4673

without complaining.

Duncan Murdoch



From ivo_welch at mailblocks.com  Sat Nov 12 04:13:34 2005
From: ivo_welch at mailblocks.com (ivo welch)
Date: Fri, 11 Nov 2005 22:13:34 -0500
Subject: [R] absolute position in plot()
References: <43755DDE.7070405@mailblocks.com>
Message-ID: <ivo_welch-0YfZQBNmxhP4u31v8NGQ+dLAjQJdJtx@mailblocks.com>


dear R wizards:  could you please point me into the right direction?  I
would like to write a general function arrow(x1,y1,x2,y2), which
naturally draws an arrow.

the basics of writing this function are of course easy.  the only
complication is that I would like the arrow not to exactly hit (x2,y2),
but to stop short just a "tiny bit."  alas, because I want my arrow()
function to be general, I would rather not have to tell it the xlim and
ylim plot parameters.  So, I need something more akin to the absolute
positioning by which the argument "pos" works within the text()
function.  In fact, optimally, I would adopt the default distance that
the "pos=x" uses, and have an optional parameter that allows the user to
change this.

could someone please point me to an example of how to do something like
this?

regards,

/iaw



From ggrothendieck at gmail.com  Sat Nov 12 04:29:57 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 11 Nov 2005 22:29:57 -0500
Subject: [R] absolute position in plot()
In-Reply-To: <ivo_welch-0YfZQBNmxhP4u31v8NGQ+dLAjQJdJtx@mailblocks.com>
References: <43755DDE.7070405@mailblocks.com>
	<ivo_welch-0YfZQBNmxhP4u31v8NGQ+dLAjQJdJtx@mailblocks.com>
Message-ID: <971536df0511111929w341fdd5bk496852470c50307f@mail.gmail.com>

See getcell at:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/63579.html

which converts between absolute and user coordinates.  It
should give you insight to allow you to develop this functionality.

On 11/11/05, ivo welch <ivo_welch at mailblocks.com> wrote:
>
> dear R wizards:  could you please point me into the right direction?  I
> would like to write a general function arrow(x1,y1,x2,y2), which
> naturally draws an arrow.
>
> the basics of writing this function are of course easy.  the only
> complication is that I would like the arrow not to exactly hit (x2,y2),
> but to stop short just a "tiny bit."  alas, because I want my arrow()
> function to be general, I would rather not have to tell it the xlim and
> ylim plot parameters.  So, I need something more akin to the absolute
> positioning by which the argument "pos" works within the text()
> function.  In fact, optimally, I would adopt the default distance that
> the "pos=x" uses, and have an optional parameter that allows the user to
> change this.
>
> could someone please point me to an example of how to do something like
> this?
>
> regards,
>
> /iaw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From roebuck at mdanderson.org  Sat Nov 12 08:03:15 2005
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Sat, 12 Nov 2005 01:03:15 -0600 (CST)
Subject: [R] sibling list element reference during list definition
Message-ID: <Pine.OSF.4.58.0511120051001.400324@wotan.mdacc.tmc.edu>

Can the value of a list element be referenced from a
sibling list element during list creation without the use
of a temporary variable?

The following doesn't work but it's the general idea.

> list(value = 2, plusplus = $value+1)

such that the following would be the output from str()

List of 2
 $ value   : num 2
 $ plusplus: num 3

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From berwin at maths.uwa.edu.au  Sat Nov 12 09:34:18 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Sat, 12 Nov 2005 16:34:18 +0800
Subject: [R] A 'sweave' strange problem !!!
In-Reply-To: <4374B7B8.2020904@biomserv.univ-lyon1.fr>
References: <4374B7B8.2020904@biomserv.univ-lyon1.fr>
Message-ID: <17269.43274.69732.248398@bossiaea.maths.uwa.edu.au>

G'day St??phane,

>>>>> "SD" == St??phane Dray <dray at biomserv.univ-lyon1.fr> writes:

    SD> I have found a problem (bug?) with Sweave.  [...]  The value
    SD> of 'a' has change between the two Schunks. It seems that the
    SD> problem only appear when there are plot (fig=T) in the first
    SD> one.  Without plot, there are no problems: a remains
    SD> unchanged. Is it a bug or have I misundertsood something ?
You have misunderstood something :) But it is quite subtle and it took
me some time to realise what was going on.

Note, that if an Sweave chunk produces a figure, then by default a PDF
and an EPS version of the figure is produced.  But R can produce only
one figure at a time, thus the chunk will be executed at least twice
if you set 'fig=TRUE' (and do not change any of the other arguments).

I noticed that even if I add 'eps=FALSE' or 'pdf=FALSE', the value of
'a' changed, if 'fig=TRUE'.  Only with 'fig=TRUE' and 'eps=FALSE' and
'pdf=FALSE' (don't ask me why I tried it), did the value of 'a' not
change.  Hence, my guess is that chunks that have 'fig=TRUE' are
executed once to produce the output for the .tex file and then they
are executed again to produce EPS and/or PDF output.  Thus such a
chunk is executed once, twice or thrice; depending on the settings of
'eps'and 'pdf'.

Thus, it is not a good idea to have statements in such chunks that
produce (pseudo-)random results.  

    SD> Thanks a lot !
My pleasure. HTH.

    SD> R version 2.1.0 (Debian).
Well, I guess the standard on this mailing list is to point out that
this is quite an old version of R and that the current one is R
2.2.0 (but that one has the same behaviour). :)

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From kubovy at virginia.edu  Sat Nov 12 12:47:17 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 12 Nov 2005 06:47:17 -0500
Subject: [R] Updating in attempt to rotate ylab
In-Reply-To: <mailman.6.1131793201.12198.r-help@stat.math.ethz.ch>
References: <mailman.6.1131793201.12198.r-help@stat.math.ethz.ch>
Message-ID: <28812A15-2277-4FCC-BF1B-40BEBDD4F0E5@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051112/043147f9/attachment.pl

From ligges at statistik.uni-dortmund.de  Sat Nov 12 13:34:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 12 Nov 2005 13:34:59 +0100
Subject: [R] no package 'Matrix' at the repositories
In-Reply-To: <4374AF21.5090300@pdf.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED597@usctmx1106.merck.com>	<4374AD6D.5030509@stats.uwo.ca>
	<4374AF21.5090300@pdf.com>
Message-ID: <4375E173.2000201@statistik.uni-dortmund.de>

Spencer Graves wrote:

> 	  Thank you all for your replies and for all your hard work to make R 
> what it is.  The wise course for me is probably to use R 2.1.1 when I 
> need the Matrix package until this issue gets fixed.


No, you can use R-2.2.0, but simply use the last working version of 
Matrix (which is in the Windows repository again, as mentioned 
yesterday) rather than the most recent one.
Exactly the same applies for R-2.1.1, hence no reason to switch back!

Uwe Ligges


> 	  Best Wishes,
> 	  spencer graves
> 
> Duncan Murdoch wrote:
> 
> 
>>On 11/11/2005 9:13 AM, Liaw, Andy wrote:
>>
>>
>>>If you are installing from Windows, the current version of Matrix 
>>>apparently
>>>doesn't build automatically on Windows.  See:
>>>
>>>http://cran.r-project.org/bin/windows/contrib/2.3/check/Matrix-check.log
>>>
>>>(That's for R-devel.  There're similar problems on R-2.2.0.)
>>>
>>>I'm sure Doug is aware of this...
>>
>>
>>Yes, he is.  Brian Ripley put together a copy that will build and 
>>install on Windows; I think it is just a matter of a short wait before 
>>those patches are incorporated in the main copy and it is sent to CRAN.
>>
>>Matrix is a big package, and it tests lots of code in R.  I spent a day 
>>earlier this week tracking down a bug in R-devel that it revealed.  It 
>>was a tricky one, because it only generated a warning; when warnings 
>>were converted to errors, the problem went away.  Luke Tierney pointed 
>>out a nice trick that made it easier to find such a thing, and I've 
>>added it to my debugging web page here:
>>
>><http://www.stats.uwo.ca/faculty/murdoch/software/debuggingR/#tryCatch>
>>
>>Duncan Murdoch
>>
>>
>>>Andy
>>>
>>>
>>>>From: Sean Davis
>>>>
>>>>On 11/11/05 8:38 AM, "Spencer Graves" <spencer.graves at pdf.com> wrote:
>>>>
>>>>
>>>>> Yesterday, I installed R2.2.0 for Windows [Version 2.2.0 (2005-10-06
>>>>>r35749)].  Unfortunately, 'install.packages("Matrix")' produced the
>>>>>following message:
>>>>>
>>>>>>Warning in download.packages(pkgs, destdir = tmpd, available =
>>>>>
>>>>>available,  :
>>>>>        no package 'Matrix' at the repositories
>>>>>
>>>>>> I installed lme4, maps, mapproj, CircStats, scatterplot3d, 
>>>>
>>>>gregmisc,
>>>>
>>>>>Hmisc without problems.  To confirm, 'library(lme4)' produced the
>>>>>following error:
>>>>>
>>>>>>Error: package 'Matrix' required by 'lme4' could not be found
>>>>>> What do you suggest?
>>>>>
>>>>> Spencer Graves
>>>>>p.s.  I get the same result using several different (US) mirrors.
>>>>
>>>>I found it here:
>>>>
>>>
>>>http://cran.cnr.berkeley.edu/src/contrib/Matrix_0.99-1.tar.gz
>>>
>>>and here:
>>>
>>>http://cran.us.r-project.org/src/contrib/Matrix_0.99-1.tar.gz
>>>
>>>at least.  These are the only two I checked.
>>>
>>>Sean
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>
>>
>



From ramasamy at cancer.org.uk  Sat Nov 12 14:14:08 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sat, 12 Nov 2005 13:14:08 +0000
Subject: [R] sibling list element reference during list definition
In-Reply-To: <Pine.OSF.4.58.0511120051001.400324@wotan.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0511120051001.400324@wotan.mdacc.tmc.edu>
Message-ID: <1131801248.7407.26.camel@dhcp-82.wolf.ox.ac.uk>

It would be more interesting to ask why does this does not work.

   mylist <- list( value=5, plusplus = mylist$value + 1 )

I think this is because plusplus cannot be evaluated because mylist does
not exist and mylist cannot be created until plusplus is evaluated.

There are people on this list who can explain in more technical terms.
But I think reading this page might help
http://cran.r-project.org/doc/manuals/R-lang.html#index-evaluation_002c-symbol-166


Here is one option :

 mylist <- eval( expression( list( value=x, plusplus=x+1) ), list(x=5) )
 mylist
 $value
 [1] 5
 $plusplus
 [1] 6


Or a bit easier to read is :

 myfun  <- function(x) list( value=x, plusplus=x+1 )
 mylist <- myfun(5)


Regards, Adai


On Sat, 2005-11-12 at 01:03 -0600, Paul Roebuck wrote:
> Can the value of a list element be referenced from a
> sibling list element during list creation without the use
> of a temporary variable?
> 
> The following doesn't work but it's the general idea.
> 
> > list(value = 2, plusplus = $value+1)
> 
> such that the following would be the output from str()
> 
> List of 2
>  $ value   : num 2
>  $ plusplus: num 3
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Sat Nov 12 15:24:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 12 Nov 2005 09:24:32 -0500
Subject: [R] sibling list element reference during list definition
In-Reply-To: <Pine.OSF.4.58.0511120051001.400324@wotan.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0511120051001.400324@wotan.mdacc.tmc.edu>
Message-ID: <971536df0511120624k566e1114uf865d7b3f839226c@mail.gmail.com>

You can do this:

   L <- list(value = 2)
   L$plusplus <- L$value + 1

or use the proto package which does support this sort of
manipulation:

   library(proto)
   as.list(proto(,{value = 2; plusplus = value+1}))

That creates a proto object with the indicated two components
and then converts it to a list.

On 11/12/05, Paul Roebuck <roebuck at mdanderson.org> wrote:
> Can the value of a list element be referenced from a
> sibling list element during list creation without the use
> of a temporary variable?
>
> The following doesn't work but it's the general idea.
>
> > list(value = 2, plusplus = $value+1)
>
> such that the following would be the output from str()
>
> List of 2
>  $ value   : num 2
>  $ plusplus: num 3
>
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From deepayan.sarkar at gmail.com  Sat Nov 12 16:15:46 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 12 Nov 2005 09:15:46 -0600
Subject: [R] Updating in attempt to rotate ylab
In-Reply-To: <28812A15-2277-4FCC-BF1B-40BEBDD4F0E5@virginia.edu>
References: <mailman.6.1131793201.12198.r-help@stat.math.ethz.ch>
	<28812A15-2277-4FCC-BF1B-40BEBDD4F0E5@virginia.edu>
Message-ID: <eb555e660511120715p36a73b47s1e019a046e26f2c9@mail.gmail.com>

On 11/12/05, Michael Kubovy <kubovy at virginia.edu> wrote:
> What am I doing wrong? (please cc me when replying)
>
> yy <- 1:10
> xx <- yy*2
> xYplot(yy~xx,ylab="") #this plots as it should
>
> text(.4,5,expression(paste(log, frac(p(b),p(a)) )))
>
> Error in text.default(0.4, 13, expression(paste(log, frac(p(b), p
> (a))))) :
> 	plot.new has not been called yet

You are trying to mix grid and base graphics, which is not allowed (at
least not without some further work). In any case, this approach won't
get you anywhere.

> This is the case even when I write:
>
> xxyy <- xYplot(yy~xx,ylab="")
> xxyy <- update(xxyy, text(.4,5,expression(paste(log, frac(p(b),p
> (a)) ))) )
>
> More generally: is there a simple method for rotating ylab?

No, but you can specify an arbitrary (unrotated text in this case) ylab as

 ylab = grid::textGrob(expression(paste(log, frac(p(b),p(a)))) )

[ This works in xyplot etc, and should in xYplot as well, but I
haven't checked ]

Deepayan



From vincent at 7d4.com  Sat Nov 12 20:24:58 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Sat, 12 Nov 2005 20:24:58 +0100
Subject: [R] matrix subset
Message-ID: <4376418A.7020207@7d4.com>

Dear R-helpers,

I apologize for this certainly simple question.
I have the following R lines :

 > m  = matrix(1:12 , 3 , 4);
 > m
      [,1] [,2] [,3] [,4]
[1,]    1    4    7   10
[2,]    2    5    8   11
[3,]    3    6    9   12

 > m1 = subset(m , m[,2]>=5);
 > m1
[1]  2  3  5  6  8  9 11 12

but in fact I would appreciate m1 to be also a matrix,
and thus would like to get :

 > m1
      [,1] [,2] [,3] [,4]
[2,]    2    5    8   11
[3,]    3    6    9   12

... but I don't find how to do ?
(probably it is very simple ! double shame.).
Thanks for any hint or pointer.
Vincent



From kristel.joossens at econ.kuleuven.ac.be  Sat Nov 12 20:36:09 2005
From: kristel.joossens at econ.kuleuven.ac.be (Kristel Joossens)
Date: Sat, 12 Nov 2005 20:36:09 +0100
Subject: [R] matrix subset
In-Reply-To: <4376418A.7020207@7d4.com>
References: <4376418A.7020207@7d4.com>
Message-ID: <43764429.7060808@econ.kuleuven.ac.be>

Do you mean simply m[m[,2]>=5,] ?

Kristel

vincent at 7d4.com wrote:
> Dear R-helpers,
> 
> I apologize for this certainly simple question.
> I have the following R lines :
> 
>  > m  = matrix(1:12 , 3 , 4);
>  > m
>       [,1] [,2] [,3] [,4]
> [1,]    1    4    7   10
> [2,]    2    5    8   11
> [3,]    3    6    9   12
> 
>  > m1 = subset(m , m[,2]>=5);
>  > m1
> [1]  2  3  5  6  8  9 11 12
> 
> but in fact I would appreciate m1 to be also a matrix,
> and thus would like to get :
> 
>  > m1
>       [,1] [,2] [,3] [,4]
> [2,]    2    5    8   11
> [3,]    3    6    9   12
> 
> ... but I don't find how to do ?
> (probably it is very simple ! double shame.).
> Thanks for any hint or pointer.
> Vincent
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From MSchwartz at mn.rr.com  Sat Nov 12 20:46:03 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 12 Nov 2005 13:46:03 -0600
Subject: [R] matrix subset
In-Reply-To: <4376418A.7020207@7d4.com>
References: <4376418A.7020207@7d4.com>
Message-ID: <1131824763.4144.38.camel@localhost.localdomain>

On Sat, 2005-11-12 at 20:24 +0100, vincent at 7d4.com wrote:
> Dear R-helpers,
> 
> I apologize for this certainly simple question.
> I have the following R lines :
> 
>  > m  = matrix(1:12 , 3 , 4);
>  > m
>       [,1] [,2] [,3] [,4]
> [1,]    1    4    7   10
> [2,]    2    5    8   11
> [3,]    3    6    9   12
> 
>  > m1 = subset(m , m[,2]>=5);
>  > m1
> [1]  2  3  5  6  8  9 11 12
> 
> but in fact I would appreciate m1 to be also a matrix,
> and thus would like to get :
> 
>  > m1
>       [,1] [,2] [,3] [,4]
> [2,]    2    5    8   11
> [3,]    3    6    9   12
> 
> ... but I don't find how to do ?
> (probably it is very simple ! double shame.).
> Thanks for any hint or pointer.
> Vincent


What version of R are you using?  You did not indicate this in your post
as you are asked to do in the posting guide.

In R version 2.1.0, a matrix method was added to the subset() function,
so I am guessing that you are several versions out of date. Please
upgrade to the latest version, which is 2.2.0, where you will get:

> subset(m, m[, 2] >= 5)
     [,1] [,2] [,3] [,4]
[1,]    2    5    8   11
[2,]    3    6    9   12


Also, you do not need the semi-colons at the end of each line. They are
only generally used if you wish to place more than one R statement on a
single line.

HTH,

Marc Schwartz



From vincent at 7d4.com  Sat Nov 12 21:27:17 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Sat, 12 Nov 2005 21:27:17 +0100
Subject: [R] matrix subset
In-Reply-To: <1131824763.4144.38.camel@localhost.localdomain>
References: <4376418A.7020207@7d4.com>
	<1131824763.4144.38.camel@localhost.localdomain>
Message-ID: <43765025.2050905@7d4.com>

Marc Schwartz a ??crit :

> What version of R are you using?  You did not indicate this in your post
> as you are asked to do in the posting guide.

I apologize for that, I use Version 2.0.1

> In R version 2.1.0, a matrix method was added to the subset() function,
> so I am guessing that you are several versions out of date. Please
> upgrade to the latest version, which is 2.2.0, where you will get:
> 
>>subset(m, m[, 2] >= 5)
> 
>      [,1] [,2] [,3] [,4]
> [1,]    2    5    8   11
> [2,]    3    6    9   12

So, I will upgrade, thanks.

> Also, you do not need the semi-colons at the end of each line. They are
> only generally used if you wish to place more than one R statement on a
> single line.

I'm an old C programmer ... and it's an old (good) behavior.
Without semicolons at the end of lines I feel naked like a worm in the
desert. (I'll keep them).

Thanks for your kind help.

Vincent



From vincent at 7d4.com  Sat Nov 12 21:28:54 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Sat, 12 Nov 2005 21:28:54 +0100
Subject: [R] matrix subset
In-Reply-To: <43764429.7060808@econ.kuleuven.ac.be>
References: <4376418A.7020207@7d4.com> <43764429.7060808@econ.kuleuven.ac.be>
Message-ID: <43765086.5030803@7d4.com>

Kristel Joossens a ??crit :

> Do you mean simply m[m[,2]>=5,] ?

yes. Arghh !!!
Thanks to all of you for your helpful answers.
Vincent



From subianto at gmail.com  Sat Nov 12 21:37:27 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Sat, 12 Nov 2005 21:37:27 +0100
Subject: [R] how to make automatically each level from data.frame to vector
Message-ID: <3635ddc20511121237y7cf7b442nb3cc2e9eaf8af366@mail.gmail.com>

Dear R-helpers,
Suppose I have dataset like this below:
data(HairEyeColor)
dfHEC <- as.data.frame(as.table(HairEyeColor))
my.dfHEC <- data.frame(Hair=rep(dfHEC$Hair,dfHEC$Freq),
                       Eye=rep(dfHEC$Eye,dfHEC$Freq),
                       Sex=rep(dfHEC$Sex,dfHEC$Freq))
my.dfHEC
my.dfHEC$Hair
my.dfHEC$Eye
my.dfHEC$Sex

and I know all levels for Hair, Eye and Sex.
In my case, I want to "expand.grid" all attributes but in Hair I only
include "Black" hair:

Hair.e <- c("Black")
Eye.e <- c("Brown","Blue","Hazel","Green")
Sex.e <- c("Male","Female")

#I can do like,
dfHEC.Black <- expand.grid(Hair.e,Eye.e,Sex.e)
dfHEC.Black

My question is how to make automatically each level from data.frame to vector.
I don't want to make definition for each level again (Hair.e, Eye.e, Sex.e).
In the others word, how can I make "expand.grid" (each level) from
data.frame automatically if Hair is only ("Black").
The result I need like,

   Var1  Var2   Var3
1 Black Brown   Male
2 Black  Blue   Male
3 Black Hazel   Male
4 Black Green   Male
5 Black Brown Female
6 Black  Blue Female
7 Black Hazel Female
8 Black Green Female

Thanks in advance.
Best, Muhammad Subianto



From gerifalte28 at hotmail.com  Sat Nov 12 22:08:03 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Sat, 12 Nov 2005 21:08:03 +0000
Subject: [R] how to make automatically each level from data.frame to
	vector
In-Reply-To: <3635ddc20511121237y7cf7b442nb3cc2e9eaf8af366@mail.gmail.com>
Message-ID: <BAY103-F2282F4DB65D63A0389996A6580@phx.gbl>

Looking at the results that you are expecing I think that you just want to 
have only the record from black colored people.  If that't the case, for 
this dataset the easiest way is to subset the data i.e

data(HairEyeColor)
x=as.data.frame(HairEyeColor)
x2=x[x$Hair=="Black",1:3]
x2
    Hair   Eye    Sex
1  Black Brown   Male
5  Black  Blue   Male
9  Black Hazel   Male
13 Black Green   Male
17 Black Brown Female
21 Black  Blue Female
25 Black Hazel Female
29 Black Green Female


Is this what you needed?

Francisco


>From: Muhammad Subianto <subianto at gmail.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] how to make automatically each level from data.frame to vector
>Date: Sat, 12 Nov 2005 21:37:27 +0100
>
>Dear R-helpers,
>Suppose I have dataset like this below:
>data(HairEyeColor)
>dfHEC <- as.data.frame(as.table(HairEyeColor))
>my.dfHEC <- data.frame(Hair=rep(dfHEC$Hair,dfHEC$Freq),
>                        Eye=rep(dfHEC$Eye,dfHEC$Freq),
>                        Sex=rep(dfHEC$Sex,dfHEC$Freq))
>my.dfHEC
>my.dfHEC$Hair
>my.dfHEC$Eye
>my.dfHEC$Sex
>

>and I know all levels for Hair, Eye and Sex.
>In my case, I want to "expand.grid" all attributes but in Hair I only
>include "Black" hair:
>
>Hair.e <- c("Black")
>Eye.e <- c("Brown","Blue","Hazel","Green")
>Sex.e <- c("Male","Female")
>
>#I can do like,
>dfHEC.Black <- expand.grid(Hair.e,Eye.e,Sex.e)
>dfHEC.Black
>
>My question is how to make automatically each level from data.frame to 
>vector.
>I don't want to make definition for each level again (Hair.e, Eye.e, 
>Sex.e).
>In the others word, how can I make "expand.grid" (each level) from
>data.frame automatically if Hair is only ("Black").
>The result I need like,
>
>    Var1  Var2   Var3
>1 Black Brown   Male
>2 Black  Blue   Male
>3 Black Hazel   Male
>4 Black Green   Male
>5 Black Brown Female
>6 Black  Blue Female
>7 Black Hazel Female
>8 Black Green Female
>
>Thanks in advance.
>Best, Muhammad Subianto
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From german.lopez at ua.es  Sat Nov 12 22:17:52 2005
From: german.lopez at ua.es (german.lopez@ua.es)
Date: Sat, 12 Nov 2005 22:17:52 +0100
Subject: [R] Error message in polr
Message-ID: <200511122117.jACLHqcO000419@aitana.cpd.ua.es>

Dear members of the list,
  I'm fitting ordinal regressions using polr, and in some models I 
get the error copied below. Dependent variable is an ordered factor 
of bird abundance categories, and predictors are continuous habitat 
variables.

> ro6 <- polr(formula = abun ~ InOmbrot + Oliva.OC + ToCultAr + 
DivCulArb + AltitMax + COORXY)
> summary(ro6)

Re-fitting to get Hessian

Error in La.svd(x, nu, nv, method) : error code 8 from Lapack 
routine dgesdd
In addition: Warning messages: 
1: NaNs produced in: dlogis(x, location, scale, log) 
2: NaNs produced in: dlogis(x, location, scale, log) 

But if I call for the "ro6" object I get the coefficients but 
without standard errors:

> ro6
Call:
polr(formula = abun ~ InOmbrot + Oliva.OC + ToCultAr + DivCulArb + 
    AltitMax + COORXY)

Coefficients:
     InOmbrot      Oliva.OC      ToCultAr     DivCulArb      
AltitMax 
-2.721242e-01  2.590153e-02  2.098157e-02  7.908437e-01  2.088895e-
03 
       COORXY 
 5.937940e-06 

Intercepts:
     0|1      1|2      2|3 
18.16425 18.97070 20.94103 

Residual Deviance: 817.667 
AIC: 835.667 

It seems that the error is related to the number of predictors 
involved since if I use a lower number, say five predictors, polr 
does not produce this error message in summary.
I have been looking at previous messages concerning errors in polr 
but I haven't found the solution. I would ackowledge very much any 
help.

  Germ??n L??pez



From claus.atzenbeck at freenet.de  Sat Nov 12 23:41:33 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Sat, 12 Nov 2005 23:41:33 +0100 (CET)
Subject: [R] computation on a table
Message-ID: <Pine.OSX.4.61.0511122304570.12962@cirrus.local>

Hello,

I have a table (1) of the form

      q1 q3 q4 q8 q9
    A  5  2  0  1  3
    B  2  0  2  4  4

I have another table (2):

      q1 q2 q3 q4 q5 q6 q7 q8 q9
    C 10  7  4  2  6  9  3  1  2

I would like to divide the numbers in table (1) by the number of the
appropriate column in table (2):

         q1   q3   q4   q8   q9
    A  5/10  2/4  0/2  1/1  3/2
    B  2/10  0/4  2/2  4/1  4/2

The result would look lie this:

         q1   q3   q4   q8   q9
    A   0.5  0.5    0    1  1.5
    B   0.2    0    1    4    2

BACKGROUND: I have a data frame with measured times for answering
questions. I want to know how many PERCENT of the answers are wrong,
caused by reason A or B.

This gives me the subset of false answers. The table looks like table (1):

    fail <- subset(questions, type=="wrong")
    fail$qid <- factor(fail$qid)
    failtab <- table(fail$failtype, fail$qid)

The following gives me information about how often a specific question
was asked. This is similar to table (2) above.

    count <- table(questions$failtype, questions$qid)
    count <- colSums(count)

One solution would be to delete the line that calls factor(...) on the
subset and calculate failtab/count. However, then I have the problem
that I have to get rid of all columns of the table that have '0' in all
rows.

Thanks for any hint.
Claus



From jholtman at gmail.com  Sun Nov 13 03:05:50 2005
From: jholtman at gmail.com (jim holtman)
Date: Sat, 12 Nov 2005 21:05:50 -0500
Subject: [R] computation on a table
In-Reply-To: <Pine.OSX.4.61.0511122304570.12962@cirrus.local>
References: <Pine.OSX.4.61.0511122304570.12962@cirrus.local>
Message-ID: <644e1f320511121805j7f9adbc2qe95a560737734f7e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051112/cdad4986/attachment.pl

From p.dalgaard at biostat.ku.dk  Sun Nov 13 03:26:11 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Nov 2005 03:26:11 +0100
Subject: [R] computation on a table
In-Reply-To: <644e1f320511121805j7f9adbc2qe95a560737734f7e@mail.gmail.com>
References: <Pine.OSX.4.61.0511122304570.12962@cirrus.local>
	<644e1f320511121805j7f9adbc2qe95a560737734f7e@mail.gmail.com>
Message-ID: <x2u0ehgu64.fsf@turmalin.kubism.ku.dk>

jim holtman <jholtman at gmail.com> writes:

> This will work if you are using matrices (if you have data frames, convert
> them to matrix):
>  > table1
> q1 q3 q4 q8 q9
> A 5 2 0 1 3
> B 2 0 2 4 4
> > table2
> q1 q2 q3 q4 q5 q6 q7 q8 q9
> C 10 7 4 2 6 9 3 1 2
> > index <- match(colnames(table2), colnames(table1), nomatch=0)
> > t(t(table1[,index]) / table2[index != 0, drop=FALSE])
> q1 q3 q4 q8 q9
> A 0.5 0.5 0 1 1.5
> B 0.2 0.0 1 4 2.0

or even

> sweep(table1, 2, table2[colnames(table1)], "/")
   q1  q3 q4 q8  q9
A 0.5 0.5  0  1 1.5
B 0.2 0.0  1  4 2.0


 
> 
>  On 11/12/05, Claus Atzenbeck <claus.atzenbeck at freenet.de> wrote:
> >
> > Hello,
> >
> > I have a table (1) of the form
> >
> > q1 q3 q4 q8 q9
> > A 5 2 0 1 3
> > B 2 0 2 4 4
> >
> > I have another table (2):
> >
> > q1 q2 q3 q4 q5 q6 q7 q8 q9
> > C 10 7 4 2 6 9 3 1 2
> >
> > I would like to divide the numbers in table (1) by the number of the
> > appropriate column in table (2):
> >
> > q1 q3 q4 q8 q9
> > A 5/10 2/4 0/2 1/1 3/2
> > B 2/10 0/4 2/2 4/1 4/2
> >
> > The result would look lie this:
> >
> > q1 q3 q4 q8 q9
> > A 0.5 0.5 0 1 1.5
> > B 0.2 0 1 4 2
> >
> > BACKGROUND: I have a data frame with measured times for answering
> > questions. I want to know how many PERCENT of the answers are wrong,
> > caused by reason A or B.
> >
> > This gives me the subset of false answers. The table looks like table (1):
> >
> > fail <- subset(questions, type=="wrong")
> > fail$qid <- factor(fail$qid)
> > failtab <- table(fail$failtype, fail$qid)
> >
> > The following gives me information about how often a specific question
> > was asked. This is similar to table (2) above.
> >
> > count <- table(questions$failtype, questions$qid)
> > count <- colSums(count)
> >
> > One solution would be to delete the line that calls factor(...) on the
> > subset and calculate failtab/count. However, then I have the problem
> > that I have to get rid of all columns of the table that have '0' in all
> > rows.
> >
> > Thanks for any hint.
> > Claus
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> 
> 
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
> 
> What the problem you are trying to solve?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From samrobertsmith at yahoo.com  Sun Nov 13 06:12:26 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sat, 12 Nov 2005 21:12:26 -0800 (PST)
Subject: [R] voronoi
Message-ID: <20051113051226.1677.qmail@web30611.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051112/962506e7/attachment.pl

From samrobertsmith at yahoo.com  Sun Nov 13 07:17:45 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sat, 12 Nov 2005 22:17:45 -0800 (PST)
Subject: [R] assign values
In-Reply-To: <20051113051226.1677.qmail@web30611.mail.mud.yahoo.com>
Message-ID: <20051113061745.72092.qmail@web30604.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051112/8b28b809/attachment.pl

From ggrothendieck at gmail.com  Sun Nov 13 07:40:20 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 13 Nov 2005 01:40:20 -0500
Subject: [R] assign values
In-Reply-To: <20051113061745.72092.qmail@web30604.mail.mud.yahoo.com>
References: <20051113051226.1677.qmail@web30611.mail.mud.yahoo.com>
	<20051113061745.72092.qmail@web30604.mail.mud.yahoo.com>
Message-ID: <971536df0511122240n199d0cp61ffa0848f4f1918@mail.gmail.com>

<- can be used in places where = but otherwise they are the same.
See ?"<-"


On 11/13/05, Robert <samrobertsmith at yahoo.com> wrote:
>
> Any difference between <- and =
>
> to assign the values?
>
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Sun Nov 13 07:46:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 13 Nov 2005 01:46:32 -0500
Subject: [R] assign values
In-Reply-To: <971536df0511122240n199d0cp61ffa0848f4f1918@mail.gmail.com>
References: <20051113051226.1677.qmail@web30611.mail.mud.yahoo.com>
	<20051113061745.72092.qmail@web30604.mail.mud.yahoo.com>
	<971536df0511122240n199d0cp61ffa0848f4f1918@mail.gmail.com>
Message-ID: <971536df0511122246wbe1e088k2e45ade86cb665f2@mail.gmail.com>

Sorry, I meant to say <- can be used in places where = can't.  Seem
to have left out the word can't.

On 11/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> <- can be used in places where = but otherwise they are the same.
> See ?"<-"
>
>
> On 11/13/05, Robert <samrobertsmith at yahoo.com> wrote:
> >
> > Any difference between <- and =
> >
> > to assign the values?
> >
> >
> >
> >
> > ---------------------------------
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From ripley at stats.ox.ac.uk  Sun Nov 13 09:17:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 13 Nov 2005 08:17:15 +0000 (GMT)
Subject: [R] assign values
In-Reply-To: <20051113061745.72092.qmail@web30604.mail.mud.yahoo.com>
References: <20051113061745.72092.qmail@web30604.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0511130811210.26993@gannet.stats>

Please do read the posting guide.
The answer to your question can be found by help("<-") or help("=").

      The '<-' can be used anywhere, but the '=' is
      only allowed at the top level (that is, in the complete expression
      typed by the user) or as one of the subexpressions in a braced
      list of expressions.

If you don't fully understand that, just use <- always.

On Sat, 12 Nov 2005, someone not signing his posting wrote:

> Any difference between <- and =
>
> to assign the values?
>
> 	[[alternative HTML version deleted]]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

and no HTML mail please, as we ask.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From renaud.lancelot at gmail.com  Sun Nov 13 09:17:33 2005
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Sun, 13 Nov 2005 09:17:33 +0100
Subject: [R] voronoi
In-Reply-To: <20051113051226.1677.qmail@web30611.mail.mud.yahoo.com>
References: <20051113051226.1677.qmail@web30611.mail.mud.yahoo.com>
Message-ID: <c2ee56800511130017o479a3a9dq@mail.gmail.com>

See package tripack. You can find this information with

RSiteSearch("voronoi", restrict = "functions")

Best,

Renaud

2005/11/13, Robert <samrobertsmith at yahoo.com>:
> Is there any pure r code to do delaunay or voronoi diagrams?
> Thanks!
>
> ---------------------------------
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
Renaud LANCELOT
D??partement Elevage et M??decine V??t??rinaire (EMVT) du CIRAD
Directeur adjoint charg?? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B??t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T??l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95



From claus.atzenbeck at freenet.de  Sun Nov 13 11:43:47 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Sun, 13 Nov 2005 11:43:47 +0100 (CET)
Subject: [R] computation on a table
In-Reply-To: <x2u0ehgu64.fsf@turmalin.kubism.ku.dk>
References: <Pine.OSX.4.61.0511122304570.12962@cirrus.local>
	<644e1f320511121805j7f9adbc2qe95a560737734f7e@mail.gmail.com>
	<x2u0ehgu64.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.OSX.4.61.0511131139260.1608@cirrus.local>

On Sun, 13 Nov 2005, Peter Dalgaard wrote:

> jim holtman <jholtman at gmail.com> writes:
[...]
> > > index <- match(colnames(table2), colnames(table1), nomatch=0)
> > > t(t(table1[,index]) / table2[index != 0, drop=FALSE])
[...]
> or even
>
> > sweep(table1, 2, table2[colnames(table1)], "/")

Perfect. I was not aware of sweep. The R help pages I read to solve this
problem did not refer to that.

Thanks to Jim and Peter.

Claus



From jari.oksanen at oulu.fi  Sun Nov 13 13:16:37 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Sun, 13 Nov 2005 14:16:37 +0200
Subject: [R] voronoi
In-Reply-To: <c2ee56800511130017o479a3a9dq@mail.gmail.com>
References: <20051113051226.1677.qmail@web30611.mail.mud.yahoo.com>
	<c2ee56800511130017o479a3a9dq@mail.gmail.com>
Message-ID: <3c7ca041dbbae9ec592993ff7f5a673e@oulu.fi>


On 13 Nov 2005, at 10:17, Renaud Lancelot wrote:

> See package tripack. You can find this information with
>
> RSiteSearch("voronoi", restrict = "functions")
>
So the answer is "no". The question was about "pure r code" (with this 
capitalization), and voronoi.mosaic() in 'tripack'  uses .Fortran.

cheers, jari oksanen

> 2005/11/13, Robert <samrobertsmith at yahoo.com>:
>> Is there any pure r code to do delaunay or voronoi diagrams?
>> Thanks!
>>
>> ---------------------------------
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
>
> --
> Renaud LANCELOT
> D??partement Elevage et M??decine V??t??rinaire (EMVT) du CIRAD
> Directeur adjoint charg?? des affaires scientifiques
>
> CIRAD, Animal Production and Veterinary Medicine Department
> Deputy director for scientific affairs
>
> Campus international de Baillarguet
> TA 30 / B (B??t. B, Bur. 214)
> 34398 Montpellier Cedex 5 - France
> T??l   +33 (0)4 67 59 37 17
> Secr. +33 (0)4 67 59 39 04
> Fax   +33 (0)4 67 59 37 95
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
--
Jari Oksanen, Oulu, Finland



From mmiller at nassp.uct.ac.za  Sun Nov 13 13:37:11 2005
From: mmiller at nassp.uct.ac.za (Mark Miller)
Date: Sun, 13 Nov 2005 14:37:11 +0200
Subject: [R] Legend
Message-ID: <200511131437.11935.mmiller@nassp.uct.ac.za>

I use the following to plot two graphs over each other and then insert a 
legend, but the two items in the legend both come up the same colour

x = seq(0,30,0.01)
plot(ecdf(complete), do.point=FALSE, main = 'Cummlative Plot of Monday IATs 
for Data and\n Fitted PDF over Entire 15 Weeks')
lines(x, pexp(x,0.415694806),col="red")
legend(x=5,y=0.2 , legend=c("Data Set","Fitted PDF"),col=c("black","red"))

Many thanks
Mark Miller



From sundar.dorai-raj at pdf.com  Sun Nov 13 13:46:59 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sun, 13 Nov 2005 06:46:59 -0600
Subject: [R] Legend
In-Reply-To: <200511131437.11935.mmiller@nassp.uct.ac.za>
References: <200511131437.11935.mmiller@nassp.uct.ac.za>
Message-ID: <437735C3.9060301@pdf.com>



Mark Miller wrote:
> I use the following to plot two graphs over each other and then insert a 
> legend, but the two items in the legend both come up the same colour
> 
> x = seq(0,30,0.01)
> plot(ecdf(complete), do.point=FALSE, main = 'Cummlative Plot of Monday IATs 
> for Data and\n Fitted PDF over Entire 15 Weeks')
> lines(x, pexp(x,0.415694806),col="red")
> legend(x=5,y=0.2 , legend=c("Data Set","Fitted PDF"),col=c("black","red"))
> 
> Many thanks
> Mark Miller
> 

Hi, Mark,

You want to use "text.col" in legend instead of "col":

set.seed(1)
z <- rexp(30, 0.415694806)
x <- seq(0, 30, 0.1)
plot(ecdf(z), do.point = FALSE)
lines(x, pexp(x, 0.415694806), col="red")
legend(x = 5, y = 0.2, legend = c("Data Set", "Fitted PDF"),
        text.col = c("black", "red"))

--sundar



From ramasamy at cancer.org.uk  Sun Nov 13 14:07:28 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 13 Nov 2005 13:07:28 +0000
Subject: [R] Legend
In-Reply-To: <437735C3.9060301@pdf.com>
References: <200511131437.11935.mmiller@nassp.uct.ac.za>
	<437735C3.9060301@pdf.com>
Message-ID: <1131887249.3559.7.camel@dhcp-82.wolf.ox.ac.uk>

And you want to have different colored lines but black texts, try

 legend(x = 5, y = 0.2, legend = c("Data Set", "Fitted PDF"),
        col = c("black", "red"), lty=1)

The advantage of this is that you can use dotted (lty option) or lines
with different weights (lwd option).

Regards, Adai



On Sun, 2005-11-13 at 06:46 -0600, Sundar Dorai-Raj wrote:
> 
> Mark Miller wrote:
> > I use the following to plot two graphs over each other and then insert a 
> > legend, but the two items in the legend both come up the same colour
> > 
> > x = seq(0,30,0.01)
> > plot(ecdf(complete), do.point=FALSE, main = 'Cummlative Plot of Monday IATs 
> > for Data and\n Fitted PDF over Entire 15 Weeks')
> > lines(x, pexp(x,0.415694806),col="red")
> > legend(x=5,y=0.2 , legend=c("Data Set","Fitted PDF"),col=c("black","red"))
> > 
> > Many thanks
> > Mark Miller
> > 
> 
> Hi, Mark,
> 
> You want to use "text.col" in legend instead of "col":
> 
> set.seed(1)
> z <- rexp(30, 0.415694806)
> x <- seq(0, 30, 0.1)
> plot(ecdf(z), do.point = FALSE)
> lines(x, pexp(x, 0.415694806), col="red")
> legend(x = 5, y = 0.2, legend = c("Data Set", "Fitted PDF"),
>         text.col = c("black", "red"))
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From liyue.journal at gmail.com  Sun Nov 13 14:27:37 2005
From: liyue.journal at gmail.com (Yue Li)
Date: Sun, 13 Nov 2005 21:27:37 +0800
Subject: [R] How to show numerical values on boxplots
Message-ID: <8c676d260511130527k2c76f58dr8f30e85a160718cd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051113/2e344aef/attachment.pl

From p.dalgaard at biostat.ku.dk  Sun Nov 13 14:28:30 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Nov 2005 14:28:30 +0100
Subject: [R] computation on a table
In-Reply-To: <Pine.OSX.4.61.0511131139260.1608@cirrus.local>
References: <Pine.OSX.4.61.0511122304570.12962@cirrus.local>
	<644e1f320511121805j7f9adbc2qe95a560737734f7e@mail.gmail.com>
	<x2u0ehgu64.fsf@turmalin.kubism.ku.dk>
	<Pine.OSX.4.61.0511131139260.1608@cirrus.local>
Message-ID: <x2iruw65j5.fsf@turmalin.kubism.ku.dk>

Claus Atzenbeck <claus.atzenbeck at freenet.de> writes:

> On Sun, 13 Nov 2005, Peter Dalgaard wrote:
> 
> > jim holtman <jholtman at gmail.com> writes:
> [...]
> > > > index <- match(colnames(table2), colnames(table1), nomatch=0)
> > > > t(t(table1[,index]) / table2[index != 0, drop=FALSE])
> [...]
> > or even
> >
> > > sweep(table1, 2, table2[colnames(table1)], "/")
> 
> Perfect. I was not aware of sweep. The R help pages I read to solve this
> problem did not refer to that.

So you didn't look at ?prop.table, I guess....

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Sun Nov 13 14:36:28 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 13 Nov 2005 14:36:28 +0100
Subject: [R] How to show numerical values on boxplots
In-Reply-To: <8c676d260511130527k2c76f58dr8f30e85a160718cd@mail.gmail.com>
References: <8c676d260511130527k2c76f58dr8f30e85a160718cd@mail.gmail.com>
Message-ID: <4377415C.7050902@statistik.uni-dortmund.de>

Yue Li wrote:
> Hi, dear all,
>  I want to show numerical values with decimal points on the boxplots. Here
> is what I did:
>  For example;
>  x1<-rnorm(100,2,2); x2<-rexp(100); label<-rep(1:2, rep(100,2))
> median<-round(c(median(x1), median(x2)),3)
> boxplot(c(x1, x2)~label, medpch=paste(median), medcex=1.2)

medpch issimilar as pch for a plotting character, but not a string.
Hence I'd add the median values by a call to text() as in:

bpo <- boxplot(c(x1, x2) ~ label, medcex=1.2)
text(1:2, bpo$stats[3,], median, pos=3)

Uwe Ligges



>   It only shows the integers at the median position in the boxplots. How to
> make it show more decimal points pls?
>  Thanks a lot.
> Lily
> NUS
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun Nov 13 15:21:07 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 13 Nov 2005 15:21:07 +0100
Subject: [R] About: Error in FUN(X[[1]],
 ...) : symbol print-name too long
In-Reply-To: <20051109_120002_092149.gsmatos1@ig.com.br>
References: <20051109_120002_092149.gsmatos1@ig.com.br>
Message-ID: <43774BD3.40700@statistik.uni-dortmund.de>

gsmatos1 wrote:
> Hi, 
> 
> I??m trying to use the Win2BUGS package from R and I have a similar problem 
> that reurns with the message: 
> 
> Error in FUN(X[[1]], ...) : symbol print-name too long 


I took a first look. The above looks like a bug in package "rbugs" which 
does some really strange things by pasting huge strings. It is 
completely unrelated with R2WinBUGS.



> But, there is no stray ` character in the file ( Sugestions given by: Duncan 
> Temple Lang <duncan> 
> Date: Mon, 26 Sep 2005 07:31:08 -0700 ) 
> 
> The progam in R is: 
> 
> library(R2WinBUGS) 
> library(rbugs) 

Please omit the line above.


> dat <- 
> list(x=c(49,48,50,44,54,56,48,48,51,51,50,53,51,50,51,54,50,53,50,49,51,47,53,50,49,55,53,48,54,46), 
> y=c(50,49,57,52,47,52,58,45,55,54,51,54,56,53,52,47,51,54,50,47,46,44,54,55,52,57,52,48,48,51)) 
> 
> dat  <- format4Bugs(dat, digits = 0) 

Please omit the line above.


> parm <- c("lbda") 
> 
> bugs(dat, inits=list(NULL), parm, "d2.bug", 
> n.chains = 1, n.iter = 5000, n.burnin = floor(n.iter/2), 
> n.thin = max(1, floor(n.chains * (n.iter - n.burnin)/1000)), 
> bin = (n.iter - n.burnin) / n.thin, 
> debug = TRUE, DIC = TRUE, digits = 5, codaPkg = FALSE, 
> bugs.directory = "C:/WinBUGS14/", 
> working.directory = NULL, clearWD = FALSE) 

The above should simply read:

bugs(dat, NULL, parm, "d2.bug",
     n.chains = 1, n.iter = 5000, debug = TRUE,
     bugs.directory = "C:/WinBUGS14/")

which reveals a bug in the function bugs():
the first lines do not special case inits=NULL and should read

    if (!missing(inits) && !is.function(inits) && !is.null(inits) && 
(length(inits) != n.chains))

rather than

    if (!missing(inits) && !is.function(inits) && (length(inits) != 
n.chains))

Please change it simply by calling fix(bugs) for the meantime.
I will submit a patched version of R2WinBUGS to CRAN within next week.

Applying your model with fixed versions of your code and R2WinBUGS also 
shows that you get some errors in WinBUGS and you have to change your 
model file, but that's another issue...

Uwe Ligges



> 	The objective of the program is to compare means of two independent samples 
> that results 
> 	in Beherens-Fisher posterior and in the model.file of WinBUGS "d2.bug" 
> there is the following codes: 
> 
>   model 
> { 
>    for( i in 1 : 30 ) { 
>       x[i] ~ dnorm(mu1,sig1) 
>    } 
>    for( i in 1 : 30 ) { 
>       y[i] ~ dnorm(mu2,sig2) 
>    } 
>    mu1 ~ dnorm(50,1.0E-6) 
>    sig1 ~ dgamma(0.001,0.001) 
>    mu2 ~ dnorm(50,1.0E-6) 
>    sig2 ~ dgamma(0.001,0.001) 
>    lbda <- mu1 - mu2 
> } 
> 
>   I??m a new user of WinBUGS and if someone detect error in the model codes 
> too, I??m grateful. 
> 
> 	Thanks for help! 
> 	Gilberto Matos. 
> 
> 
> ------------------------------------------------------------------------
> 
> model
> {
>    for( i in 1 : 30 ) {
>       x[i] ~ dnorm(mu1,sig1)
>    }
>    for( i in 1 : 30 ) {
>       y[i] ~ dnorm(mu2,sig2)
>    }
>    mu1 ~ dnorm(50,1.0E-6)
>    sig1 ~ dgamma(0.001,0.001)
>    mu2 ~ dnorm(50,1.0E-6)
>    sig2 ~ dgamma(0.001,0.001)
>    lbda <- mu1 - mu2
> }
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Sun Nov 13 15:32:33 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Nov 2005 15:32:33 +0100
Subject: [R] assign values
In-Reply-To: <Pine.LNX.4.61.0511130811210.26993@gannet.stats>
References: <20051113061745.72092.qmail@web30604.mail.mud.yahoo.com>
	<Pine.LNX.4.61.0511130811210.26993@gannet.stats>
Message-ID: <x2acg862ke.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> Please do read the posting guide.
> The answer to your question can be found by help("<-") or help("=").
> 
>       The '<-' can be used anywhere, but the '=' is
>       only allowed at the top level (that is, in the complete expression
>       typed by the user) or as one of the subexpressions in a braced
>       list of expressions.
> 
> If you don't fully understand that, just use <- always.

The text is not completely accurate though:

> if ((x=2)) 2
[1] 2
> x
[1] 2
> if (x=2) 2
Error: syntax error in "if(x="
> if(x<-2) 2
[1] 2
> if(x < -2) 2
>

We do allow "=" inside parentheses too, it's only in function
arguments (where it is ambiguous) and inside conditional clauses
(where users might use it incorrectly to test for equality) that we
disallow it. 

As the last example shows, we do leave people to fry in their own fat
if they use "<-" for comparisons with negative numbers, though (been
there, done that...)


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From julien.ruiz at airfrance.fr  Sun Nov 13 16:00:45 2005
From: julien.ruiz at airfrance.fr (julien.ruiz@airfrance.fr)
Date: Sun, 13 Nov 2005 16:00:45 +0100
Subject: [R] Julien Ruiz est absent.
Message-ID: <OF0700B1F0.C5DBE0D3-ONC12570B8.005277B5-C12570B8.005277B7@airfrance.fr>

Je serai absent(e) du  12/11/2005 au 16/11/2005.

Je r??pondrai ?? votre message d??s mon retour.

I will be out of the office from 14-MAR-2005 until 18-MAR-2005
I will reply to your message on my return.

     Julien Ruiz
----------------
L'acces immediat aux meilleurs tarifs Air France et au billet electronique
sur http://www.airfrance.com
For immediate access to the best Air France fares and to electronic
tickets, visit our website http://www.airfrance.com

----------------
Les donnees et renseignements contenus dans ce message sont personnels,
confidentiels et secrets. Ce message est adresse a l'individu ou l'entite
dont les coordonnees figurent ci-dessus. Si vous n'etes pas le bon
destinataire, nous vous demandons de ne pas lire, copier, utiliser ou
divulguer cette communication. Nous vous prions de notifier cette erreur a
l'expediteur et d'effacer immediatement cette communication de votre
systeme.
The information contained in this message is privileged, confidential, and
protected from disclosure. This message is intended for the individual or
entity adressed herein. If you are not the intended recipient, please do
not read, copy, use or disclose this communication to others; also please
notify the sender by replying to this message, and then delete it from your
system.



From barbora.kocurova at atlas.cz  Sun Nov 13 16:53:33 2005
From: barbora.kocurova at atlas.cz (=?iso-8859-2?B?QmFyYm9yYSBLb2P6cm924Q==?=)
Date: Sun, 13 Nov 2005 16:53:33 +0100
Subject: [R] simulation of compound Poisson and Cox process
Message-ID: <de046207147844c081deaa7640c27cdf@atlas.cz>


		
	Hello.
	
	I have this problem. It is modeling high-frequency financial data.
	The gamma OU process X (t), BDLP compound Poisson with intensity h > 0 and
	E(a) exponential (a) distribution of jump. L??vy density w of Z (1):
	w(x) = ahexp(-ax), x is more or equal than 0,
	f(R) < infinity,
	g(z) = izh/(a-iz).
	OUCP rejection sampling
	x(t) = x(0)exp(-ct) + suma(0<t(j)is less or equal to T)[z(j)exp(t(j)-ct)],
	cf. shot noise Cox process, 0 < t1 < .. < tk is less or equal to T jump times of BDLP Z,
	z(j) jumps.
	Let
	B = max (0<t<T)[x(t)] = max (1<j<k)[[x(0) + suma(l goes from 1 to j)z(l)exp(t(l))]exp(-t(j))],
	simulate Poisson (BT) = m, then m uniform points on [0,T].
	Each point s is let with probability x(s) / B, 0 < s1 < s2 < ....... sn < T, n < m.
	
	Could you please help me to simulate this process? I hope that it is possible to do it in R.
	
	Thanks in advance
	
	Barbora Kocurova



From zpzhang at uchicago.edu  Sun Nov 13 18:39:03 2005
From: zpzhang at uchicago.edu (Zepu Zhang)
Date: Sun, 13 Nov 2005 23:39:03 +0600
Subject: [R] problem with grid animation
Message-ID: <ce82b33d.5aab9592.819df00@m4500-03.uchicago.edu>

I'm trying to do animation with grid. Basically it's a vector field, like what 'quiver' 
in Matlab creates. I need to update it with grid.edit(). It seems grid erases the 
whole thing first, then redraws. Therefore the evident 'flash' between frames.

Any way to avoid this flash? Thanks.



From billemont at cegetel.net  Sun Nov 13 18:40:30 2005
From: billemont at cegetel.net (billemont@cegetel.net)
Date: Sun, 13 Nov 2005 18:40:30 +0100
Subject: [R] selection of missing data
Message-ID: <5f48b1d3ae76dbf745eaa9a5e69e1033@cegetel.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051113/1ae88f42/attachment.pl

From ramasamy at cancer.org.uk  Sun Nov 13 19:56:09 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 13 Nov 2005 18:56:09 +0000
Subject: [R] selection of missing data
In-Reply-To: <5f48b1d3ae76dbf745eaa9a5e69e1033@cegetel.net>
References: <5f48b1d3ae76dbf745eaa9a5e69e1033@cegetel.net>
Message-ID: <1131908169.30342.19.camel@dhcp-82.wolf.ox.ac.uk>

I do not quite follow your post but here are some suggestions. 


1) You can the na.strings argument to simplify things 

   df <- read.delim(file="lala.txt", na.strings="-" )


2) If you can count the number of metastasis per row first, then find
the rows with zero sum.

   met.cols      <- c(11,12,14,21,23,24) # metastasis columns
   number.of.met <- rowSums( mela[ , met.cols ] == "-" )
   have.no.met   <- which( number.of.met == 0 )
   mela.no.met   <- mela[ have.no.met , ]

If you had coded your "-" as NA during read in then, the second line
needs to be changed to

   number.of.met <- rowSums( is.na( mela[ , met.cols ] ) )

or simply use complete.cases

   met.cols      <- c(11,12,14,21,23,24) # metastasis columns
   mela.no.met   <- mela[ which( complete.cases(mela[ , met.cols]) ) , ]


3) If you name your columns in a systematic fashion, then you can easily
extract and specify those columns. For example if your columns were
named 

   cn <- c( "age", "colon.met", "PSA.level", "prostate.met", "gender",
            "hospitalisation.days", "status", "liver.met", "ethnicity")

Then you can extract those names ending with ".met" as

   met.cols <- grep( "\\.met$", cn )
   met.cols
   [1] 2 4 8


Regards, Adai



On Sun, 2005-11-13 at 18:40 +0100, billemont at cegetel.net wrote:
> Hi i'm a french medical student,
> i have some data that i import from excel. My colomn of the datafram 
> are the localisations of metastasis. If there is a metatsasis there is 
> the symbol "_". i want to exclude the row without metastasis wich 
> represent the NA data.
> 
> so, i wrote this
> 
> mela is the data fram
> 
> mela1=ifelse(mela[,c(11:12,14:21,23,24)]=="_",1,0) # selection of the 
> colomn of metastasis localisation
> 
> mela4=subset(mela3,Skin ==0 & s.c == 0 & Mucosa ==0 & Soft.ti ==0 & 
> Ln.peri==0 & Ln.med==0 & Ln.abdo==0 & Lung==0 & Liver==0 & 
> Other.Visc==0 & Bone==0 & Marrow==0 & Brain==0 & Other==0) ## selection 
> of the row with no metastasis localisation
> nrow(mela4)
> 
> but i dont now if it is possible to make the same thin as 
> ifelse(mela3,Skin & s.c== 0, 0,NA) with more than colomn and after to 
> exclude of my data the Na with na.omit.
> 
> The last question is how can i omit only the row which are NA value for 
> the colomn metastasis c(11:12,14:21,23,24))
> 
> Thank you for your help
> 
> 
> 
> Bertrand billemont
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wolfgang.keller.nospam at gmx.de  Sun Nov 13 20:11:19 2005
From: wolfgang.keller.nospam at gmx.de (Wolfgang Keller)
Date: Sun, 13 Nov 2005 20:11:19 +0100
Subject: [R] R for reliability analysis (with censored samples)?
Message-ID: <1648838080.20051113201119@gmx.de>

Hello,

first: I am a newbie to this list, so if this isn't the right place to
ask, thanks for pointing me to the right direction...

I'm  currently  working  on  adding a degree in RAMS engineering to my
general  engineering  education,  and  consequently  I'm  looking  for
software  to  get my future work done. This will have to handle mostly
right-/left-/intervall-censored  samples,  which  seems  to  be      a
non-trivial  feature  from what I've read and heard so far. Apart from
this, RAMS work will be only part of my job and I will be the only one
to do RAMS work in the (tiny) company I work for, so paying multi-kEUR
every  year  in  license  fees  for  some  commercial application will
probably  not be the preferred choice for us. So I'm planning to use R
together with Python and GNUmeric/OO Calc instead.

So here are the questions:

How suitable is R for this kind of work (reliability analysis)?

Does it handle (right-/left-/interval-)censored samples by default?

Are  there  any information sources (on- or off-line) dedicated to the
use of open-source software for reliability analysis and especially R?

TIA,

Sincerely,

Wolfgang Keller

-- 
P.S.: My From-address is correct



From samrobertsmith at yahoo.com  Sun Nov 13 20:32:33 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sun, 13 Nov 2005 11:32:33 -0800 (PST)
Subject: [R] assign values
In-Reply-To: <x2acg862ke.fsf@turmalin.kubism.ku.dk>
Message-ID: <20051113193233.81050.qmail@web30614.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051113/555817e5/attachment.pl

From samrobertsmith at yahoo.com  Sun Nov 13 20:43:20 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sun, 13 Nov 2005 11:43:20 -0800 (PST)
Subject: [R] voronoi
In-Reply-To: <c2ee56800511130017o479a3a9dq@mail.gmail.com>
Message-ID: <20051113194320.52053.qmail@web30609.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051113/203669ed/attachment.pl

From p.murrell at auckland.ac.nz  Sun Nov 13 20:53:37 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 14 Nov 2005 08:53:37 +1300
Subject: [R] problem with grid animation
In-Reply-To: <ce82b33d.5aab9592.819df00@m4500-03.uchicago.edu>
References: <ce82b33d.5aab9592.819df00@m4500-03.uchicago.edu>
Message-ID: <437799C1.8080201@stat.auckland.ac.nz>

Hi


Zepu Zhang wrote:
> I'm trying to do animation with grid. Basically it's a vector field, like what 'quiver' 
> in Matlab creates. I need to update it with grid.edit(). It seems grid erases the 
> whole thing first, then redraws. Therefore the evident 'flash' between frames.


Correct.


> Any way to avoid this flash? Thanks.


There are a couple of ways, but unfortunately they may not be to your 
taste ...

1)  Use windows (where double-buffering has been implemented)
2)  Find some way to convince someone to modify the X11 device (or 
whichever device you are using) to support double-buffering
3)  Provide a patch for the X11 device yourself ...

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From Roger.Bivand at nhh.no  Sun Nov 13 21:02:45 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 13 Nov 2005 21:02:45 +0100 (CET)
Subject: [R] voronoi
In-Reply-To: <20051113194320.52053.qmail@web30609.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.44.0511132101300.31230-100000@reclus.nhh.no>

On Sun, 13 Nov 2005, Robert wrote:

> It uses FORTRAN code and not in pure R.

The same applies to deldir - it also includes Fortran. So the answer seems 
to be no, there is no voronoi function only written in R.

> 
> Renaud Lancelot <renaud.lancelot at gmail.com> wrote:See package tripack. You can find this information with
> 
> RSiteSearch("voronoi", restrict = "functions")
> 
> Best,
> 
> Renaud
> 
> 2005/11/13, Robert :
> > Is there any pure r code to do delaunay or voronoi diagrams?
> > Thanks!
> >
> > ---------------------------------
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> 
> --
> Renaud LANCELOT
> D??partement Elevage et M??decine V??t??rinaire (EMVT) du CIRAD
> Directeur adjoint charg?des affaires scientifiques
> 
> CIRAD, Animal Production and Veterinary Medicine Department
> Deputy director for scientific affairs
> 
> Campus international de Baillarguet
> TA 30 / B (B??t. B, Bur. 214)
> 34398 Montpellier Cedex 5 - France
> T??l +33 (0)4 67 59 37 17
> Secr. +33 (0)4 67 59 39 04
> Fax +33 (0)4 67 59 37 95
> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From p.dalgaard at biostat.ku.dk  Sun Nov 13 21:44:51 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Nov 2005 21:44:51 +0100
Subject: [R] assign values
In-Reply-To: <20051113193233.81050.qmail@web30614.mail.mud.yahoo.com>
References: <20051113193233.81050.qmail@web30614.mail.mud.yahoo.com>
Message-ID: <x2slu046rg.fsf@turmalin.kubism.ku.dk>

Robert <samrobertsmith at yahoo.com> writes:

> Professor,
> What does if ((x=2)) 2 mean?
> Thanks,
> Robert

it assigns 2 to x, then uses the result (i.e. whether it is non-zero)
as the condition for the if() construct. So in this case, the
condition is always TRUE and the result is always 2.
 
> Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Prof Brian Ripley writes:
> 
> > Please do read the posting guide.
> > The answer to your question can be found by help("<-") or help("=").
> > 
> > The '<-' can be used anywhere, but the '=' is
> > only allowed at the top level (that is, in the complete expression
> > typed by the user) or as one of the subexpressions in a braced
> > list of expressions.
> > 
> > If you don't fully understand that, just use <- always.
> 
> The text is not completely accurate though:
> 
> > if ((x=2)) 2
> [1] 2
> > x
> [1] 2
> > if (x=2) 2
> Error: syntax error in "if(x="
> > if(x<-2) 2
> [1] 2
> > if(x < -2) 2
> >
> 
> We do allow "=" inside parentheses too, it's only in function
> arguments (where it is ambiguous) and inside conditional clauses
> (where users might use it incorrectly to test for equality) that we
> disallow it. 
> 
> As the last example shows, we do leave people to fry in their own fat
> if they use "<-" for comparisons with negative numbers, though (been
> there, done that...)
> 
> 
> -- 
> O__ ---- Peter Dalgaard ??ster Farimagsgade 5, Entr.B
> c/ /'_ --- Dept. of Biostatistics PO Box 2099, 1014 Cph. K
> (*) \(*) -- University of Copenhagen Denmark Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk) FAX: (+45) 35327907
> 
> 		
> ---------------------------------
>  Yahoo! FareChase - Search multiple travel sites in one click.  

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From samrobertsmith at yahoo.com  Sun Nov 13 22:01:41 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sun, 13 Nov 2005 13:01:41 -0800 (PST)
Subject: [R] assign values
In-Reply-To: <x2slu046rg.fsf@turmalin.kubism.ku.dk>
Message-ID: <20051113210141.57974.qmail@web30602.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051113/84d3cacb/attachment.pl

From sourceforge at metrak.com  Sun Nov 13 22:20:10 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Mon, 14 Nov 2005 08:20:10 +1100
Subject: [R] correlating irregular time series
Message-ID: <4377AE0A.2090702@metrak.com>

I have some time stamped events that are supposed to be unrelated.

I have plotted them and that assumption does not appear to be valid. 
http://metrak.com/tmp/sevents.png is a plot showing three sets of events 
over time.  For the purpose of this exercise, the Y value is irrelevant. 
  The series are not sampled at the same time and are not equispaced 
(just events in a log file).

The plot is already pretty convincing but requires a human-in-the-loop 
to zoom in on "hot" areas and then visually interpret the result.  I 
want to calculate some index of the events' temporal relationship.

I think the question I am trying to ask is something like: "If event B 
occurs, how likely is it that an event A occurred at almost the same time?".

Can anyone suggest an established approach that could provide some 
further insight into this relationship?  I can think of a fairly basic 
approach where I start out with the ecdf of the time differences but I 
am guessing I would be reinventing some wheel.

Any tips would be most appreciated.

cheers



From samrobertsmith at yahoo.com  Sun Nov 13 22:24:49 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sun, 13 Nov 2005 13:24:49 -0800 (PST)
Subject: [R] open source and R
In-Reply-To: <Pine.LNX.4.44.0511132101300.31230-100000@reclus.nhh.no>
Message-ID: <20051113212449.62570.qmail@web30602.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051113/e8132847/attachment.pl

From Roger.Bivand at nhh.no  Sun Nov 13 22:25:33 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 13 Nov 2005 22:25:33 +0100 (CET)
Subject: [R] assign values
In-Reply-To: <20051113210141.57974.qmail@web30602.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.44.0511132223010.31230-100000@reclus.nhh.no>

On Sun, 13 Nov 2005, Robert wrote:

> 
> 
> Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Robert writes:
> 
> > Professor,
> > What does if ((x=2)) 2 mean?
> > Thanks,
> > Robert
> 
> it assigns 2 to x, then uses the result (i.e. whether it is non-zero)
> as the condition for the if() construct. So in this case, the
> condition is always TRUE and the result is always 2.
> 
> I see. 
> 
> > if ((x=2)) 2
> [1] 2
> > if ((x=0)) 2
> > 

Look at the text above "(i.e. whether it is non-zero)", and:

as.logical(seq(-2,2,1))

and then at your result - numeric x with value 0 is being cast to logical 
FALSE, so this is as described.

> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Roger.Bivand at nhh.no  Sun Nov 13 22:34:37 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 13 Nov 2005 22:34:37 +0100 (CET)
Subject: [R] open source and R
In-Reply-To: <20051113212449.62570.qmail@web30602.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.44.0511132226450.31230-100000@reclus.nhh.no>

On Sun, 13 Nov 2005, Robert wrote:

> Roger Bivand <Roger.Bivand at nhh.no> wrote: 
> On Sun, 13 Nov 2005, Robert wrote:
> 
> > It uses FORTRAN code and not in pure R.
> 
> The same applies to deldir - it also includes Fortran. So the answer seems 
> to be no, there is no voronoi function only written in R.
> 

Robert wrote:

> 
> I am curious about one thing: since the reason for using r is r is a
> easy-to-learn language and it is good for getting more people involved.
> Why most of the packages written in r use other languages such as
> FORTRAN's code? I understand some functions have already been written in
> other language or it is faster to be implemented in other language. But
> my understanding is if the user does not know that language (for
> example, FORTRAN), the package is still a black box to him because he
> can not improve the package and can not be involved in the development.  
> When I searched the packages of R, I saw many packages with duplicated
> or similar functions. the main difference among them are the different
> functions implemented using other languages, which are always a black
> box to the users. So it is very hard for users to believe the package
> will run something they need, let alone getting involved in the
> development. My comments are not to disregard these efforts. But it is
> good to see the packages written in pure R.
> 

Please indent your replies, they are very difficult to read sensibly.

Although surprisingly much of R is written in R, quite a lot is written in
Fortran and C. One very good reason, apart from efficiency, is code re-use
- BLAS and LAPACK among many others are excellent implementations of what
we need for numerical linear algebra. R is very typical of good scientific
software, it tries to avoid re-implementing functions that are used by the
community, are well-supported by the community, and work. Packages by and
large do the same - if existing software does the required job, package
authors attempt to port that software to R, providing interfaces to
underlying C or Fortran libraries. 

It's about standing on the shoulders of giants.

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From samrobertsmith at yahoo.com  Sun Nov 13 22:45:49 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sun, 13 Nov 2005 13:45:49 -0800 (PST)
Subject: [R] open source and R
In-Reply-To: <Pine.LNX.4.44.0511132226450.31230-100000@reclus.nhh.no>
Message-ID: <20051113214549.54433.qmail@web30604.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051113/2110da0d/attachment.pl

From hvermei1 at vrcbe.jnj.com  Sun Nov 13 22:47:31 2005
From: hvermei1 at vrcbe.jnj.com (Vermeiren, Hans [VRCBE])
Date: Sun, 13 Nov 2005 22:47:31 +0100
Subject: [R] Robust Non-linear Regression
Message-ID: <9AC105024CEA64458BF66D1DE13CA50D070FB3B2@tibbemeexs1.eu.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051113/dadc076e/attachment.pl

From Roger.Bivand at nhh.no  Sun Nov 13 23:01:41 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 13 Nov 2005 23:01:41 +0100 (CET)
Subject: [R] open source and R
In-Reply-To: <20051113214549.54433.qmail@web30604.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.44.0511132300060.31230-100000@reclus.nhh.no>

On Sun, 13 Nov 2005, Robert wrote:

> If I do not know C or FORTRAN, how can I fully understand the package or
> possibly improve it?

By learning enough to see whether that makes a difference for your 
purposes. Life is hard, but that's what makes life interesting ...

> Robert.
> 
> Roger Bivand <Roger.Bivand at nhh.no> wrote:
> On Sun, 13 Nov 2005, Robert wrote:
> 
> > Roger Bivand wrote: 
> > On Sun, 13 Nov 2005, Robert wrote:
> > 
> > > It uses FORTRAN code and not in pure R.
> > 
> > The same applies to deldir - it also includes Fortran. So the answer seems 
> > to be no, there is no voronoi function only written in R.
> > 
> 
> Robert wrote:
> 
> > 
> > I am curious about one thing: since the reason for using r is r is a
> > easy-to-learn language and it is good for getting more people involved.
> > Why most of the packages written in r use other languages such as
> > FORTRAN's code? I understand some functions have already been written in
> > other language or it is faster to be implemented in other language. But
> > my understanding is if the user does not know that language (for
> > example, FORTRAN), the package is still a black box to him because he
> > can not improve the package and can not be involved in the development. 
> > When I searched the packages of R, I saw many packages with duplicated
> > or similar functions. the main difference among them are the different
> > functions implemented using other languages, which are always a black
> > box to the users. So it is very hard for users to believe the package
> > will run something they need, let alone getting involved in the
> > development. My comments are not to disregard these efforts. But it is
> > good to see the packages written in pure R.
> > 
> 
> Please indent your replies, they are very difficult to read sensibly.
> 
> Although surprisingly much of R is written in R, quite a lot is written in
> Fortran and C. One very good reason, apart from efficiency, is code re-use
> - BLAS and LAPACK among many others are excellent implementations of what
> we need for numerical linear algebra. R is very typical of good scientific
> software, it tries to avoid re-implementing functions that are used by the
> community, are well-supported by the community, and work. Packages by and
> large do the same - if existing software does the required job, package
> authors attempt to port that software to R, providing interfaces to
> underlying C or Fortran libraries. 
> 
> It's about standing on the shoulders of giants.
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From roger at ysidro.econ.uiuc.edu  Sun Nov 13 23:02:32 2005
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sun, 13 Nov 2005 16:02:32 -0600
Subject: [R] Robust Non-linear Regression
In-Reply-To: <9AC105024CEA64458BF66D1DE13CA50D070FB3B2@tibbemeexs1.eu.jnj.com>
References: <9AC105024CEA64458BF66D1DE13CA50D070FB3B2@tibbemeexs1.eu.jnj.com>
Message-ID: <508F4F88-A5B3-4788-ADC8-B874AECB8CB0@ysidro.econ.uiuc.edu>

you might consider nlrq() in the quantreg package, which does median
regression for nonlinear response functions....


url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Nov 13, 2005, at 3:47 PM, Vermeiren, Hans [VRCBE] wrote:

> Hi,
>
> I'm trying to use Robust non-linear regression to fit dose response  
> curves.
> Maybe I didnt look good enough, but I dind't find robust methods  
> for NON
> linear regression implemented in R. A method that looked good to me  
> but is
> unfortunately not (yet) implemented in R is described in
> http://www.graphpad.com/articles/RobustNonlinearRegression_files/ 
> frame.htm
> <http://www.graphpad.com/articles/RobustNonlinearRegression_files/ 
> frame.htm>
>
>
> in short: instead of using the premise that the residuals are  
> gaussian they
> propose a Lorentzian distribution,
> in stead of minimizing the squared residus SUM (Y-Yhat)^2, the  
> objective
> function is now
> SUM log(1+(Y-Yhat)^2/ RobustSD)
>
> where RobustSD is the 68th percentile of the absolute value of the  
> residues
>
> my question is: is there a smart and elegant way to change to  
> objective
> function from squared Distance to log(1+D^2/Rsd^2) ?
>
> or alternatively to write this as a weighted non-linear regression  
> where the
> weights are recalculated during the iterations
> in nlme it is possible to specify weights, possibly that is the way  
> to do
> it, but I didn't manage to get it working
> the weights should then be something like:
>
> SUM (log(1+(resid(.)/quantile(all_residuals,0.68))^2)) / SUM (resid 
> (.))
>
> the test data I use :
> x<-seq(-5,-2,length=50)
> x<-rep(x,4)
> y<-SSfpl(x,0,100,-3.5,1)
> y<-y+rnorm(length(y),sd=5)
> y[sample(1:length(y),floor(length(y)/50))]<-200 # add 2% outliers  
> at 200
>
> thanks a lot
>
> Hans Vermeiren
>
>
>     [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From roger at ysidro.econ.uiuc.edu  Sun Nov 13 23:03:34 2005
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sun, 13 Nov 2005 16:03:34 -0600
Subject: [R] open source and R
In-Reply-To: <F5DEA368-4808-4DBD-9A51-12066AE16335@ysidro.econ.uiuc.edu>
References: <20051113212449.62570.qmail@web30602.mail.mud.yahoo.com>
	<F5DEA368-4808-4DBD-9A51-12066AE16335@ysidro.econ.uiuc.edu>
Message-ID: <1EE475E5-3B52-4D22-9B39-B162CD8D08B4@ysidro.econ.uiuc.edu>

>
> On Nov 13, 2005, at 3:24 PM, Robert wrote:
>
>
>> I am curious about one thing: since the reason for using r is r is  
>> a easy-to-learn language and it is good for getting more people  
>> involved. Why most of the packages written in r use other  
>> languages such as FORTRAN's code? I understand some functions have  
>> already been written in other language or it is faster to be  
>> implemented in other language. But my understanding is if the user  
>> does not know that language (for example, FORTRAN), the package is  
>> still a black box to him  because he can not improve the package  
>> and can not be involved in the development.
>> When I searched the packages of R, I saw many packages with  
>> duplicated or similar functions. the main difference among them  
>> are the different functions implemented using other languages,  
>> which are always a black box to the users. So it is very hard for  
>> users to believe the package will run something they need, let  
>> alone getting involved in the development.
>>
>
>
> No, the box is not black, it is utterly transparent.  Of course,  
> what you can recognize and understand
> inside depends on you,.  Just say "no"  to linguistic chauvinism   
> -- even R-ism.
>
>
> url:    www.econ.uiuc.edu/~roger                Roger Koenker
> email   rkoenker at uiuc.edu                       Department of  
> Economics
> vox:    217-333-4558                            University of Illinois
> fax:    217-244-6678                            Champaign, IL 61820
>
>



From mbmiller at taxa.epi.umn.edu  Sun Nov 13 23:23:33 2005
From: mbmiller at taxa.epi.umn.edu (Mike Miller)
Date: Sun, 13 Nov 2005 16:23:33 -0600 (CST)
Subject: [R] open source and R
In-Reply-To: <Pine.LNX.4.44.0511132300060.31230-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0511132300060.31230-100000@reclus.nhh.no>
Message-ID: <Pine.GSO.4.60.0511131613001.3018@taxa.epi.umn.edu>

On Sun, 13 Nov 2005, Roger Bivand wrote:

> On Sun, 13 Nov 2005, Robert wrote:
>
>> If I do not know C or FORTRAN, how can I fully understand the package 
>> or possibly improve it?
>
> By learning enough to see whether that makes a difference for your 
> purposes. Life is hard, but that's what makes life interesting ...


None of us "fully understands" what we are doing with computer software. 
If you understand R code, that's great, but then there is the R 
interpreter -- do you understand how it works?  That interpreter was 
written in another language that was then compiled by a compiler which was 
written by someone else for some other purpose -- do you understand the 
compiler?  Then it all gets processed by some very complex hardware that 
practically none of us *fully* understands.  We have to accept that we 
can't have a complete grasp of what R is doing, but we can still read the 
R docs and test R in many ways.

When functions are written in R, they may be easier for you to read, but 
they may run much slower than code written in C, C++ or FORTRAN.  I don't 
think it is wise to forgo the speed improvement so that people who don't 
know FORTRAN can enjoy contributing to R development.  The contribution of 
FORTRAN libraries R functionality and efficiency is probably much greater 
than the contributions would be from any group of people who could code in 
R but could't code in C or FORTRAN.

That said, I appreciate the sentiment and I think we should prefer 
straight R code for many functions, but some things just run too slowly 
when written that way.

Mike



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov 14 00:14:01 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 13 Nov 2005 23:14:01 -0000 (GMT)
Subject: [R] open source and R
In-Reply-To: <Pine.LNX.4.44.0511132300060.31230-100000@reclus.nhh.no>
Message-ID: <XFMail.051113231401.Ted.Harding@nessie.mcc.ac.uk>

On 13-Nov-05 Roger Bivand wrote:
> On Sun, 13 Nov 2005, Robert wrote:
> 
>> If I do not know C or FORTRAN, how can I fully understand the package
>> or possibly improve it?
> 
> By learning enough to see whether that makes a difference for your 
> purposes. Life is hard, but that's what makes life interesting ...
> 
>> Robert.
>> 
>> Roger Bivand <Roger.Bivand at nhh.no> wrote:
>> On Sun, 13 Nov 2005, Robert wrote:
>> 
>> > Roger Bivand wrote: 
>> > On Sun, 13 Nov 2005, Robert wrote:
>> > 
>> > > It uses FORTRAN code and not in pure R.
>> > 
>> > The same applies to deldir - it also includes Fortran. So the
>> > answer seems to be no, there is no voronoi function only
>> > written in R.
>> > 
>> 
>> Robert wrote:
>> 
>> > 
>> > I am curious about one thing: since the reason for using r
>> > is r is a easy-to-learn language and it is good for getting
>> > more people involved.
>> >
>> > Why most of the packages written in r use other languages
>> > such as FORTRAN's code? I understand some functions have
>> > already been written in other language or it is faster to
>> > be implemented in other language.
>> >
>> > But my understanding is if the user does not know that
>> > language (for example, FORTRAN), the package is still a
>> > black box to him because he can not improve the package and
>> > can not be involved in the development. 
>> >
>> > When I searched the packages of R, I saw many packages with
>> > duplicated or similar functions. the main difference among
>> > them are the different functions implemented using other
>> >languages, which are always a black box to the users. So it
>> > is very hard for users to believe the package will run
>> > something they need, let alone getting involved in the
>> > development. My comments are not to disregard these efforts.
>> > But it is good to see the packages written in pure R.
>> > 
>> 
>> Although surprisingly much of R is written in R, quite a lot is
>> written in Fortran and C. One very good reason, apart from
>> efficiency, is code
>> re-use
>> - BLAS and LAPACK among many others are excellent implementations
>> of what we need for numerical linear algebra. R is very typical
>> of good scientific software, it tries to avoid re-implementing
>> functions that are used by the community, are well-supported by
>> the community, and work. Packages by and large do the same - if
>> existing software does the required job, package authors attempt
>> to port that software to R, providing interfaces to underlying
>> C or Fortran libraries. 
>> 
>> It's about standing on the shoulders of giants.

Those are very strong points. Some comments:

It would be possible to implement in "pure R" a matrix inversion
or eigenvalue/vector function, for instance, and I'm sure it would
be done (if it were done) to very high quality. However, it would
run like an elephant in quicksands. BLAS and LAPACK have, over the
years, become highly optimised not just for accuracy and robustness,
but for speed and efficiency.

Also, you will hit the "other language" problem sooner or
later. Robert's complaint is that he does not like black
boxes. But R itself is a black box. You cannot write R in R,
all the way down to the bottom. At the bottom is machine
code, and languages like assember, C, C++, FORTRAN and
their compilers provide "black box" wrappers for this.

That is not a whimsical comment either -- all those discussions
about why  2 - sqrt(2)^2 is not equal to 0 come down to this
sort of issue. Sooner or later, if you really want to understand
what is going on, you have to get beneath the shiny smooth
surface and swim amongst the molecules!

So, Robert, try to be positive about C and FORTRAN etc., rather
than feeling put off by the fact that they are yet more things
to learn and seem to get in the way of understanding how the
functions work. C and FORTRAN are your friends, as well as
the R langauge itself, and great deal more friemdly than
the raw machine code. 

There is one aspect though where R users are in the cold when
it comes to C and FORTAN. If you want to understand the function
'eigen', say, then you can "?eigen" to learn about its usage.
You can enter "eigen" to see the R code, and indeed that is
not too imcomprehensible. But then you find

  .Fortran("ch", n, n, xr, xi, values = dbl.n, 
           !only.values, vectors = xr, ivectors = xi, dbl.n, 
           dbl.n, double(2 * n), ierr = integer(1),
           PACKAGE = "base")

and similar for "rs", "cg" and "rg". Where's the help for
these? Nowhere obvious! In fact you have to go to the source
code, locate the FORTRAN routines, and study these, hoping
that enough helpful comments have been included to steer
your study. So it is a much more formidable task, especially
if you are having to learn the language at the same time.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Nov-05                                       Time: 23:13:58
------------------------------ XFMail ------------------------------



From murdoch at stats.uwo.ca  Mon Nov 14 02:25:38 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 13 Nov 2005 20:25:38 -0500
Subject: [R] poker package -- comments?
Message-ID: <4377E792.9020008@stats.uwo.ca>

Over the weekend I wrote a small package to evaluate poker hands and to 
do some small simulations with them.  If anyone is interested in looking 
at it, I'd appreciate comments and/or contributions.

The package is available at 
http://www.stats.uwo.ca/faculty/murdoch/software.  (Look at the bottom 
of the list.)

So far only the Texas Hold'em variation has been programmed.  There's 
support for wild cards and fairly general schemes of putting together
hands for evaluation, so it wouldn't be too hard to add other games. 
There's no support for betting or simulating different strategies, but 
again, if you want to write that, it should be possible.

Here's a quick example, where I've asked it to simulate hands until it 
came up with one I won.  In the first case I start with a pair of aces 
and won on the first hand; in the second another player started with 
aces, and it took 7 hands to find me a winner.

poker> select.hand(pocket = card("Ah As"), players = 4)
Showing: 4H 3S 2D 6S 4C
   Rank Name Cards      Value
1    1 Self AH AS   Two pair
2    2    1 8S 3C   Two pair
3    3    2 QD KH Pair of 4s
4    4    3 8H 9D Pair of 4s
Would win  4  person game
Required 1 hand.

poker> select.hand(players = list(card("Ah As"), NULL, NULL))
Showing: AD 4H 7D 2C 8S
   Rank Name Cards       Value
1    1 Self 6H 5H    Straight
2    2    1 AH AS 3 of a kind
3    3    2 AC 3C  Pair of As
4    4    3 9D 6D      A high
Would win  4  person game
Required 7 hands.

Duncan Murdoch



From tlumley at u.washington.edu  Mon Nov 14 04:05:12 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 13 Nov 2005 19:05:12 -0800 (PST)
Subject: [R] R for reliability analysis (with censored samples)?
In-Reply-To: <1648838080.20051113201119@gmx.de>
References: <1648838080.20051113201119@gmx.de>
Message-ID: <Pine.LNX.4.63a.0511131902440.4356@homer24.u.washington.edu>

On Sun, 13 Nov 2005, Wolfgang Keller wrote:

> So here are the questions:
>
> How suitable is R for this kind of work (reliability analysis)?
>
> Does it handle (right-/left-/interval-)censored samples by default?
>

The survreg() function in the "survival" package fits accelerated failure 
models to right/left/interval censored data.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From samrobertsmith at yahoo.com  Mon Nov 14 05:51:17 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sun, 13 Nov 2005 20:51:17 -0800 (PST)
Subject: [R] open source and R
In-Reply-To: <XFMail.051113231401.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20051114045117.990.qmail@web30606.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051113/beb63a7e/attachment.pl

From ripley at stats.ox.ac.uk  Mon Nov 14 08:20:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Nov 2005 07:20:55 +0000 (GMT)
Subject: [R] open source and R
In-Reply-To: <XFMail.051113231401.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051113231401.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0511140712440.26098@gannet.stats>

On Sun, 13 Nov 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

[...]

> There is one aspect though where R users are in the cold when
> it comes to C and FORTAN. If you want to understand the function
> 'eigen', say, then you can "?eigen" to learn about its usage.
> You can enter "eigen" to see the R code, and indeed that is
> not too imcomprehensible. But then you find
>
>  .Fortran("ch", n, n, xr, xi, values = dbl.n,
>           !only.values, vectors = xr, ivectors = xi, dbl.n,
>           dbl.n, double(2 * n), ierr = integer(1),
>           PACKAGE = "base")
>
> and similar for "rs", "cg" and "rg". Where's the help for
> these? Nowhere obvious! In fact you have to go to the source
> code, locate the FORTRAN routines, and study these, hoping
> that enough helpful comments have been included to steer
> your study. So it is a much more formidable task, especially
> if you are having to learn the language at the same time.

That is an unfair comment.  The help page for eigen explains what 
routines are used and gives you references to books describing them.
So the help _is_ in the most obvious place.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov 14 09:55:38 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Nov 2005 08:55:38 -0000 (GMT)
Subject: [R] open source and R
In-Reply-To: <Pine.LNX.4.61.0511140712440.26098@gannet.stats>
Message-ID: <XFMail.051114085538.Ted.Harding@nessie.mcc.ac.uk>

On 14-Nov-05 Prof Brian Ripley wrote:
> On Sun, 13 Nov 2005 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
> [...]
> 
>> There is one aspect though where R users are in the cold when
>> it comes to C and FORTAN. If you want to understand the function
>> 'eigen', say, then you can "?eigen" to learn about its usage.
>> You can enter "eigen" to see the R code, and indeed that is
>> not too imcomprehensible. But then you find
>>
>>  .Fortran("ch", n, n, xr, xi, values = dbl.n,
>>           !only.values, vectors = xr, ivectors = xi, dbl.n,
>>           dbl.n, double(2 * n), ierr = integer(1),
>>           PACKAGE = "base")
>>
>> and similar for "rs", "cg" and "rg". Where's the help for
>> these? Nowhere obvious! In fact you have to go to the source
>> code, locate the FORTRAN routines, and study these, hoping
>> that enough helpful comments have been included to steer
>> your study. So it is a much more formidable task, especially
>> if you are having to learn the language at the same time.
> 
> That is an unfair comment.  The help page for eigen explains what 
> routines are used and gives you references to books describing them.
> So the help _is_ in the most obvious place.

Apologies for misleading wording. This was not meant as a criticism
of R in any way, but as an illustration of the theme that, sooner
or later, you "drop through the floor" of what R can provide in
the way of explicit explanation. So, while R's help is indeed helpful
in this case in indicating an orientation for your travels in that
outside world, "out in the cold" as it were, users are then on
their own as far as R is concerned. This is of course inevitable,
and the comment was intended as part of the general response to
Robert that if you want to study how R does things you are led
to study how other software, on which R depends, does things.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 14-Nov-05                                       Time: 08:55:35
------------------------------ XFMail ------------------------------



From christophe.pouzat at univ-paris5.fr  Mon Nov 14 10:06:28 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Mon, 14 Nov 2005 10:06:28 +0100
Subject: [R] correlating irregular time series
In-Reply-To: <4377AE0A.2090702@metrak.com>
References: <4377AE0A.2090702@metrak.com>
Message-ID: <43785394.6090506@univ-paris5.fr>

Hi Paul,

Here is how an amateur statistician deals with this problem when 
analyzing spike trains from simultaneously recorded neurons.

Start by estimating the "hazard function" h(t) of your several point 
processes (if you have a copy of MASS, check out the chapter 13, If you 
have a copy of Jim Lindsey, "The Statistical Analysis of Stochastic 
Processes in Time", check out chap 3 & 4; the hazard function is also 
called the "conditional intensity" or the "stochastic intensity").

In practice if you have a renewal process, meaning that the successive 
intervals between your events times are independent, you can first 
estimate the "Inter Event Interval" pdf, f(t), and its cumulative 
distribution function F(t). h(t) is then given by:

h(t) = f(t) / (1-F(t)),

where the quantity S(t) = 1-F(t) is often called the survivor function.

Fine, now if your processes are well approximated by renewal processes, 
you can look for the distribution of "time to next event" (TTN) and 
"time to former event" (TTF). By that I mean that for each of the black 
events of your figure, you must get the interval separating it from the 
last red event preceding it (the time to former) and the next red event 
following it (the time to next). Under the null hypothesis of no 
correlation these to random variables have the same pdf given by:

TTN(i) = S(i) / <IEI>,

where S(i) in that case is the survivor function of the red (test) 
process and <IEI> is its inter event interval expected value.
Using this approach I typically estimate the TTN and TTF pdfs with 
histograms and compare these histograms to their expected values under 
the null hypothesis. A warning though, I have most of the time much more 
events than you seem to have on your figure.

Let me know if any of this makes sense.

Christophe.

paul sorenson wrote:

>I have some time stamped events that are supposed to be unrelated.
>
>I have plotted them and that assumption does not appear to be valid. 
>http://metrak.com/tmp/sevents.png is a plot showing three sets of events 
>over time.  For the purpose of this exercise, the Y value is irrelevant. 
>  The series are not sampled at the same time and are not equispaced 
>(just events in a log file).
>
>The plot is already pretty convincing but requires a human-in-the-loop 
>to zoom in on "hot" areas and then visually interpret the result.  I 
>want to calculate some index of the events' temporal relationship.
>
>I think the question I am trying to ask is something like: "If event B 
>occurs, how likely is it that an event A occurred at almost the same time?".
>
>Can anyone suggest an established approach that could provide some 
>further insight into this relationship?  I can think of a fairly basic 
>approach where I start out with the ecdf of the time differences but I 
>am guessing I would be reinventing some wheel.
>
>Any tips would be most appreciated.
>
>cheers
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From vincent at 7d4.com  Mon Nov 14 10:16:54 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Mon, 14 Nov 2005 10:16:54 +0100
Subject: [R] matrix subset
In-Reply-To: <1131824763.4144.38.camel@localhost.localdomain>
References: <4376418A.7020207@7d4.com>
	<1131824763.4144.38.camel@localhost.localdomain>
Message-ID: <43785606.70701@7d4.com>

Marc Schwartz a ??crit :

> In R version 2.1.0, a matrix method was added to the subset() function,
> so I am guessing that you are several versions out of date. Please
> upgrade to the latest version, which is 2.2.0, where you will get:

I upgraded and it works fine.
Thanks for the hint.

Many thanks also to the authors of the function, Peter Dalgaard
and prof Ripley (I suspect prof Ripley is responsible for
the matrix improvment ?).

Many thanks also to all the contributors to the last
version of this wonderful software.

Vincent



From krankenversicherung at help.ch  Mon Nov 14 10:39:19 2005
From: krankenversicherung at help.ch (krankenversicherung@help.ch)
Date: Mon, 14 Nov 2005 10:39:19 +0100
Subject: [R] www.krankenversicherung.ch News - Krankenkassen - Information
	Newsletter
Message-ID: <200511141045109.SM00940@195.141.204.149>

Guten Tag
 
Weitere Krankenkassen-Spar-Tipps fuer Sie!


INHALT DIESES NEWSLETTERS
1) Trend 2006 - Der Trend geht Richtung Hausarzt-Model
2) Angebot Assura - Kennen Sie die guenstigen Praemien der Assura
3) Vergleich der Praemien
4) Voting - Jetzt koennen Sie Ihre Versicherung bewerten
5) Wettbewerb - schon gewonnen?
6) Kuendigungstermin ist der 30.11.2005


Gerne informieren wir Sie ueber die aktuellen Sparmoeglichkeiten fuer das Jahr 2006.


>>> AKTUELLER TREND FUER 2006
Vor einigen Tagen haben wir den Trend fuer das kommende Jahr ausgewertet. Der Trend geht Richtung 
Hausarztmodelle. Weitere Details und Statistiken finden Sie auf folgendem Link:
http://www.krankenversicherung.ch/helpart2.cfm?art=trend2006



>>> PRAEMIEN ASSURA
Die Assura hat eine eigene Philosophie. Diese verhilft seit Jahren zu guenstigen Praemien fuer die 
Assura-Versicherten.
Erhalten Sie hier Ihre persoenliche und unverbindliche Offerte:
http://www.assura.krankenversicherung.ch



>>> VERGLEICHEN SIE JETZT
Unter http://vergleich.krankenversicherung.ch vergleichen Sie Ihre Praemien fuer das Jahr 2006.



>>> VOTING - BEWERTEN SIE JETZT IHRE KRANKENKASSE
Bewerten Sie jetzt Ihre Krankenkasse unter http://voting.krankenversicherung.ch


>>> WETTBEWERB - GEWINNEN SIE EINE REISE - HABEN SIE SCHON EINEN TAGESPREIS GEWONNEN?
100 Sofortpreise und eine Flugreise sind zu gewinnen unter 
http://wettbewerb.krankenversicherung.ch . Viel Glueck!


>>> KUENDIGUNGSTERMIN
Nicht vergessen! Der Kuendigungstermin fuer die Grundversicherung ist der 30.11.2005.


>>> NAECHSTE INFORMATION
Noch einmal erinnern wir Sie rechtzeitig an den letzten Kuendigungstermin. Ein kostenloser Service 
von Krankenversicherung.ch


>>> INFORMATION - Kein Spam
Sie erhalten diesen Newsletter aufgrund einer Bestellung oder Eintrages auf 
www.krankenversicherung.ch oder www.help.ch. Sie koennen diesen Newsletter jederzeit sofort 
abbestellen mit Nutzung des untenstehenden Links.
 
 
>>> ABMELDUNG
http://www.krankenversicherung.ch/unsubscribe.cfm



Mit freundlichen Gruessen und viel Erfolg beim Wettbewerb
www.krankenversicherung.ch und www.help.ch
 
Ihr Newsletter-Team

___________________________________________________________________

Die Schweizer Firmen-Suchmaschine
HELP Searchengines AG - Badenerstrasse 75 - 8004 Zuerich
www.help.ch  -  www.firmenscout.ch  -  www.produktesuche.ch
mailto:info at help.ch



From sourceforge at metrak.com  Mon Nov 14 11:06:35 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Mon, 14 Nov 2005 21:06:35 +1100
Subject: [R] correlating irregular time series
In-Reply-To: <43785394.6090506@univ-paris5.fr>
References: <4377AE0A.2090702@metrak.com> <43785394.6090506@univ-paris5.fr>
Message-ID: <437861AB.60306@metrak.com>

I don't have the texts you mention but I get the general idea.  The 
diagram I posted shows only a small fraction of the events I have.

Thank you

Christophe Pouzat wrote:
> Hi Paul,
> 
> Here is how an amateur statistician deals with this problem when 
> analyzing spike trains from simultaneously recorded neurons.
> 
> Start by estimating the "hazard function" h(t) of your several point 
> processes (if you have a copy of MASS, check out the chapter 13, If you 
> have a copy of Jim Lindsey, "The Statistical Analysis of Stochastic 
> Processes in Time", check out chap 3 & 4; the hazard function is also 
> called the "conditional intensity" or the "stochastic intensity").
> 
> In practice if you have a renewal process, meaning that the successive 
> intervals between your events times are independent, you can first 
> estimate the "Inter Event Interval" pdf, f(t), and its cumulative 
> distribution function F(t). h(t) is then given by:
> 
> h(t) = f(t) / (1-F(t)),
> 
> where the quantity S(t) = 1-F(t) is often called the survivor function.
> 
> Fine, now if your processes are well approximated by renewal processes, 
> you can look for the distribution of "time to next event" (TTN) and 
> "time to former event" (TTF). By that I mean that for each of the black 
> events of your figure, you must get the interval separating it from the 
> last red event preceding it (the time to former) and the next red event 
> following it (the time to next). Under the null hypothesis of no 
> correlation these to random variables have the same pdf given by:
> 
> TTN(i) = S(i) / <IEI>,
> 
> where S(i) in that case is the survivor function of the red (test) 
> process and <IEI> is its inter event interval expected value.
> Using this approach I typically estimate the TTN and TTF pdfs with 
> histograms and compare these histograms to their expected values under 
> the null hypothesis. A warning though, I have most of the time much more 
> events than you seem to have on your figure.
> 
> Let me know if any of this makes sense.
> 
> Christophe.
> 
> paul sorenson wrote:
> 
>> I have some time stamped events that are supposed to be unrelated.
>>
>> I have plotted them and that assumption does not appear to be valid. 
>> http://metrak.com/tmp/sevents.png is a plot showing three sets of 
>> events over time.  For the purpose of this exercise, the Y value is 
>> irrelevant.  The series are not sampled at the same time and are not 
>> equispaced (just events in a log file).
>>
>> The plot is already pretty convincing but requires a human-in-the-loop 
>> to zoom in on "hot" areas and then visually interpret the result.  I 
>> want to calculate some index of the events' temporal relationship.
>>
>> I think the question I am trying to ask is something like: "If event B 
>> occurs, how likely is it that an event A occurred at almost the same 
>> time?".
>>
>> Can anyone suggest an established approach that could provide some 
>> further insight into this relationship?  I can think of a fairly basic 
>> approach where I start out with the ecdf of the time differences but I 
>> am guessing I would be reinventing some wheel.
>>
>> Any tips would be most appreciated.
>>
>> cheers
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>  
>>
> 
>



From christophe.pouzat at univ-paris5.fr  Mon Nov 14 11:44:04 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Mon, 14 Nov 2005 11:44:04 +0100
Subject: [R] correlating irregular time series
In-Reply-To: <437861AB.60306@metrak.com>
References: <4377AE0A.2090702@metrak.com> <43785394.6090506@univ-paris5.fr>
	<437861AB.60306@metrak.com>
Message-ID: <43786A74.70506@univ-paris5.fr>

Paul,

You can get a version of Lindsey's course at the following address:

http://popgen.unimaas.nl/~jlindsey/manuscripts.html

At the bottom of the page under heading "Courses".

Christophe.

paul sorenson wrote:

> I don't have the texts you mention but I get the general idea.  The 
> diagram I posted shows only a small fraction of the events I have.
>
> Thank you



-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From maechler at stat.math.ethz.ch  Mon Nov 14 12:41:16 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Nov 2005 12:41:16 +0100
Subject: [R] Robust Non-linear Regression
In-Reply-To: <9AC105024CEA64458BF66D1DE13CA50D070FB3B2@tibbemeexs1.eu.jnj.com>
References: <9AC105024CEA64458BF66D1DE13CA50D070FB3B2@tibbemeexs1.eu.jnj.com>
Message-ID: <17272.30684.543345.857234@stat.math.ethz.ch>

Package 'sfsmisc' has had a function  'rnls()' for a while 
which does robust non-linear regression via M-estimation.

[The name of the function is probably *really* a misnomer,
 since the 'ls' part stands for "least squares"!]

Two weeks ago, there's been a small workshop
"Robustness and R" in Treviso (It),
http://www.dst.unive.it/rsr/

where we've talked about available and missing robustness
functionality ``in R''.  One consequence of the workshop is the
new mailing list "R-SIG-Robust" {to which I CC this message} 
and another planned and hopefully even more consequential
consequence will be collaboration on producing more widely
available robustness functionality for R.  Do subscribe to the
list if you are interested.

>>>>> "Vermeiren" == Vermeiren, Hans [VRCBE] <Vermeiren>
>>>>>     on Sun, 13 Nov 2005 22:47:31 +0100 writes:

    Vermeiren> Hi, I'm trying to use Robust non-linear
    Vermeiren> regression to fit dose response curves.  Maybe I
    Vermeiren> didnt look good enough, but I dind't find robust
    Vermeiren> methods for NON linear regression implemented in
    Vermeiren> R. A method that looked good to me but is
    Vermeiren> unfortunately not (yet) implemented in R is
    Vermeiren> described in
    Vermeiren> http://www.graphpad.com/articles/RobustNonlinearRegression_files/frame.htm

 
    Vermeiren> in short: instead of using the premise that the
    Vermeiren> residuals are gaussian they propose a Lorentzian
    Vermeiren> distribution, in stead of minimizing the squared
    Vermeiren> residus SUM (Y-Yhat)^2, the objective function is
    Vermeiren> now SUM log(1+(Y-Yhat)^2/ RobustSD)
 
    Vermeiren> where RobustSD is the 68th percentile of the
    Vermeiren> absolute value of the residues

 
    Vermeiren> my question is: is there a smart and elegant way
    Vermeiren> to change to objective function from squared
    Vermeiren> Distance to log(1+D^2/Rsd^2) ?

no; not easily.
 
    Vermeiren> or alternatively to write this as a weighted
    Vermeiren> non-linear regression where the weights are
    Vermeiren> recalculated during the iterations in nlme it is
    Vermeiren> possible to specify weights, possibly that is the
    Vermeiren> way to do it, but I didn't manage to get it
    Vermeiren> working the weights should then be something
    Vermeiren> like:
 
    Vermeiren> SUM (log(1+(resid(.)/quantile(all_residuals,0.68))^2))
    Vermeiren>   / SUM (resid(.))
 
rnls() mentioned does use robust weights and IRLS (iteratively
reweighted LS) making use of  nls() and rlm(),
similarly to your suggestion.

    Vermeiren> the test data I use :

    Vermeiren> x<-seq(-5,-2,length=50)
    Vermeiren> x<-rep(x,4)
    Vermeiren> y<-SSfpl(x,0,100,-3.5,1)
    Vermeiren> y<-y+rnorm(length(y),sd=5)
    Vermeiren> y[sample(1:length(y),floor(length(y)/50))]<-200 # add 2% outliers at 200

Since you have only outliers in 'y' and none in 'x',
you could use the 'nlrq' (nonlinear regression quantiles)
package that Roger Koenker mentioned.

To really robustify such self starting models as the 4-parameter
logistic 'SSfpl' above, you would also need to provide a robust
initial estimator; 
maybe that could be done pretty easily 'rlm()' instead of 'lm()' and
using 'rnls()' instead of 'nls()' also for the "initial" part in
something like

SSfpl.rob <-
selfStart(~ A + (B - A)/(1 + exp((xmid - input)/scal)), 
          initial = function( ...) { ...... },
          parameters= c("A","B","xmid","scal"))
 
{look at 'SSfpl() for the initial estimator}.

However, BTW, currently the "plinear" version fails for our robust
nonlinear procedure 'rnls()'.

Martin Maechler, ETH Zurich



From RRoa at fisheries.gov.fk  Mon Nov 14 12:00:06 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Mon, 14 Nov 2005 09:00:06 -0200
Subject: [R] Robust Non-linear Regression
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC5FD@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From:	r-help-bounces at stat.math.ethz.ch [SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vermeiren, Hans [VRCBE]
> Sent:	Sunday, November 13, 2005 7:48 PM
> To:	'r-help at stat.math.ethz.ch'
> Subject:	[R] Robust Non-linear Regression
> 
> Hi,
>  
> I'm trying to use Robust non-linear regression to fit dose response curves.
> Maybe I didnt look good enough, but I dind't find robust methods for NON
> linear regression implemented in R. A method that looked good to me but is
> unfortunately not (yet) implemented in R is described in 
> http://www.graphpad.com/articles/RobustNonlinearRegression_files/frame.htm
> <http://www.graphpad.com/articles/RobustNonlinearRegression_files/frame.htm>
> 
> 
> in short: instead of using the premise that the residuals are gaussian they
> propose a Lorentzian distribution,
> in stead of minimizing the squared residus SUM (Y-Yhat)^2, the objective
> function is now
> SUM log(1+(Y-Yhat)^2/ RobustSD)
>  
> where RobustSD is the 68th percentile of the absolute value of the residues
>  
> my question is: is there a smart and elegant way to change to objective
> function from squared Distance to log(1+D^2/Rsd^2) ?
>  
-----------
I do not know about in-built robustness options in R but I have found that 
Dave Fournier's robust likelihood for nonlinear regression in ADMB does
a pretty good job in detecting and counter-acting the influence of outliers
(in my applications this has been used to counter-act the effect of reading 
errors in determination of the age of fish based on rings in bones). 
It relies on a likelihood function based on a mixture of a normal and another
distribution with fatter tails. You can find the documentation in the ADMB manual 
at the ADMB website: http://otter-rsch.com/admodel.htm
Ruben



From eric_wzl at yahoo.com  Mon Nov 14 13:16:05 2005
From: eric_wzl at yahoo.com (peter eric)
Date: Mon, 14 Nov 2005 04:16:05 -0800 (PST)
Subject: [R] how to plot matrix  in graphs
Message-ID: <20051114121605.66643.qmail@web36406.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051114/729fe078/attachment.pl

From K.Soetaert at nioo.knaw.nl  Mon Nov 14 13:20:24 2005
From: K.Soetaert at nioo.knaw.nl (Soetaert, Karline)
Date: Mon, 14 Nov 2005 13:20:24 +0100
Subject: [R] (no subject)
Message-ID: <65F6E1EC64DCA6489800C09A2007FC6E6AA48A@cememail1.nioo.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051114/22c74999/attachment.pl

From dmb at mrc-dunn.cam.ac.uk  Mon Nov 14 13:39:14 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 14 Nov 2005 12:39:14 +0000
Subject: [R] poker package -- comments?
In-Reply-To: <4377E792.9020008@stats.uwo.ca>
References: <4377E792.9020008@stats.uwo.ca>
Message-ID: <43788572.3060508@mrc-dunn.cam.ac.uk>

Duncan Murdoch wrote:
> Over the weekend I wrote a small package to evaluate poker hands and to 
> do some small simulations with them.  If anyone is interested in looking 
> at it, I'd appreciate comments and/or contributions.

How do I install this package?

A README or a hint on th webpage below would be great.


> The package is available at 
> http://www.stats.uwo.ca/faculty/murdoch/software.  (Look at the bottom 
> of the list.)
> 
> So far only the Texas Hold'em variation has been programmed.  There's 
> support for wild cards and fairly general schemes of putting together
> hands for evaluation, so it wouldn't be too hard to add other games. 
> There's no support for betting or simulating different strategies, but 
> again, if you want to write that, it should be possible.
> 
> Here's a quick example, where I've asked it to simulate hands until it 
> came up with one I won.  In the first case I start with a pair of aces 
> and won on the first hand; in the second another player started with 
> aces, and it took 7 hands to find me a winner.
> 
> poker> select.hand(pocket = card("Ah As"), players = 4)
> Showing: 4H 3S 2D 6S 4C
>    Rank Name Cards      Value
> 1    1 Self AH AS   Two pair
> 2    2    1 8S 3C   Two pair
> 3    3    2 QD KH Pair of 4s
> 4    4    3 8H 9D Pair of 4s
> Would win  4  person game
> Required 1 hand.
> 
> poker> select.hand(players = list(card("Ah As"), NULL, NULL))
> Showing: AD 4H 7D 2C 8S
>    Rank Name Cards       Value
> 1    1 Self 6H 5H    Straight
> 2    2    1 AH AS 3 of a kind
> 3    3    2 AC 3C  Pair of As
> 4    4    3 9D 6D      A high
> Would win  4  person game
> Required 7 hands.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From claus.atzenbeck at freenet.de  Mon Nov 14 13:47:11 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Mon, 14 Nov 2005 13:47:11 +0100 (CET)
Subject: [R] name of object
Message-ID: <Pine.OSX.4.61.0511141338220.29081@cirrus.aue.aau.dk>

Hi,

I have the following function:

    test <- function(x)
    {
        print(shapiro.test(x))
        ...
    }

The output for "test(sample1$sec)" is:

    Shapiro-Wilk normality test

    data:  x
    W = 0.9447, p-value = 0.5767
    ...

I would like to see "data: sample1$sec" instead of "data: x", as it
would be when directly called "shapiro.test(sample1$sec)".

How can I do that? I browsed the documentation and other literature, but
did not find any solution.

Thanks.
Claus



From ripley at stats.ox.ac.uk  Mon Nov 14 14:15:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Nov 2005 13:15:59 +0000 (GMT)
Subject: [R] name of object
In-Reply-To: <Pine.OSX.4.61.0511141338220.29081@cirrus.aue.aau.dk>
References: <Pine.OSX.4.61.0511141338220.29081@cirrus.aue.aau.dk>
Message-ID: <Pine.LNX.4.61.0511141310410.9938@gannet.stats>

On Mon, 14 Nov 2005, Claus Atzenbeck wrote:

> Hi,
>
> I have the following function:
>
>    test <- function(x)
>    {
>        print(shapiro.test(x))
>        ...
>    }
>
> The output for "test(sample1$sec)" is:
>
>    Shapiro-Wilk normality test
>
>    data:  x
>    W = 0.9447, p-value = 0.5767
>    ...
>
> I would like to see "data: sample1$sec" instead of "data: x", as it
> would be when directly called "shapiro.test(sample1$sec)".
>
> How can I do that? I browsed the documentation and other literature, but
> did not find any solution.

Use substitute().  Something like

test <- function(x)
{
    xlab <- substitute(x)
    print(eval.parent(substitute(shapiro.test(x), list(x=xlab))))
}

See S Programming section 3.5.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Mon Nov 14 14:49:56 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Nov 2005 08:49:56 -0500
Subject: [R] how to plot matrix  in graphs
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED59F@usctmx1106.merck.com>

See ?image.

Andy

> From: peter eric
> 
> halo,
>  
> how to plot a matrix (i have a multiple matrix ) in graphs in 
> terms of colored boxes or circles.
> my matrix looks like
>  
>            A           B          C
>  
>        6 2 3     4 3 2      2 1 7
>   A    4 3 1     4 6 8      2 1 6
>         2 7 8     7 8 0      2 3 5
>  
>        5 2 3     4 7 2      2 1 7 B    4 3 1     4 8 8      3 1 6
>         9 7 8     7 8 0      6 3 5
> 
>  
>         1 2 3     4 3 2      2 1 7  C   4 3 1     4 6 8      2 1 6
>         2 7 8     7 8 0      2 3 5
>  
> And my graph should looks like(in terms of colored boxes or 
> circles according to the magnitude of the nos)
>  
>  
>              A           B          C
>  
>           O O O    O O O   O O O
>  A       O O O    O O O   O O O  
>           O O O    O O O   O O O
>  
>           O O O    O O O   O O O B       O O O    O O O   O O O  
>           O O O    O O O   O O O
>  
>           O O O    O O O   O O O C       O O O    O O O   O O O  
>           O O O    O O O   O O O
>  
> Can you suggest me some ways of doing this..
>  
> thank you....
>  
> best regards,
> peter.
>  
> Research student,
> Fraunhofer IPT,
> Germany.
> 
> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From murdoch at stats.uwo.ca  Mon Nov 14 15:04:41 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 14 Nov 2005 09:04:41 -0500
Subject: [R] poker package -- comments?
In-Reply-To: <43788572.3060508@mrc-dunn.cam.ac.uk>
References: <4377E792.9020008@stats.uwo.ca>
	<43788572.3060508@mrc-dunn.cam.ac.uk>
Message-ID: <43789979.2030006@stats.uwo.ca>

On 11/14/2005 7:39 AM, Dan Bolser wrote:
> Duncan Murdoch wrote:
>> Over the weekend I wrote a small package to evaluate poker hands and to 
>> do some small simulations with them.  If anyone is interested in looking 
>> at it, I'd appreciate comments and/or contributions.
> 
> How do I install this package?

It depends on which version of R you're running, but on windows, you
just download the zip file, and then in R, choose "Install package from
local zip file".

By the way, I've rearranged the files on my web page, so if you need to
download it again you'll need to follow a few different links.  It still
starts at http://www.stats.uwo.ca/faculty/murdoch/software.

Duncan
> 
> A README or a hint on th webpage below would be great.
> 
> 
>> The package is available at 
>> http://www.stats.uwo.ca/faculty/murdoch/software.  (Look at the bottom 
>> of the list.)
>> 
>> So far only the Texas Hold'em variation has been programmed.  There's 
>> support for wild cards and fairly general schemes of putting together
>> hands for evaluation, so it wouldn't be too hard to add other games. 
>> There's no support for betting or simulating different strategies, but 
>> again, if you want to write that, it should be possible.
>> 
>> Here's a quick example, where I've asked it to simulate hands until it 
>> came up with one I won.  In the first case I start with a pair of aces 
>> and won on the first hand; in the second another player started with 
>> aces, and it took 7 hands to find me a winner.
>> 
>> poker> select.hand(pocket = card("Ah As"), players = 4)
>> Showing: 4H 3S 2D 6S 4C
>>    Rank Name Cards      Value
>> 1    1 Self AH AS   Two pair
>> 2    2    1 8S 3C   Two pair
>> 3    3    2 QD KH Pair of 4s
>> 4    4    3 8H 9D Pair of 4s
>> Would win  4  person game
>> Required 1 hand.
>> 
>> poker> select.hand(players = list(card("Ah As"), NULL, NULL))
>> Showing: AD 4H 7D 2C 8S
>>    Rank Name Cards       Value
>> 1    1 Self 6H 5H    Straight
>> 2    2    1 AH AS 3 of a kind
>> 3    3    2 AC 3C  Pair of As
>> 4    4    3 9D 6D      A high
>> Would win  4  person game
>> Required 7 hands.
>> 
>> Duncan Murdoch
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Mon Nov 14 15:46:06 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Nov 2005 15:46:06 +0100
Subject: [R] odesolve with banded Jacobian [was "no subject"]
In-Reply-To: <65F6E1EC64DCA6489800C09A2007FC6E6AA48A@cememail1.nioo.int>
References: <65F6E1EC64DCA6489800C09A2007FC6E6AA48A@cememail1.nioo.int>
Message-ID: <17272.41774.72970.156563@stat.math.ethz.ch>

>>>>> "KSoet" == Soetaert, Karline <K.Soetaert at nioo.knaw.nl>
>>>>>     on Mon, 14 Nov 2005 13:20:24 +0100 writes:

    KSoet> Hi, I am trying to solve a model that consists of
    KSoet> rather stiff ODEs in R.

    KSoet> I use the package ODEsolve (lsoda) to solve these
    KSoet> ODEs.
 
    KSoet> To speed up the integration, the jacobian is also
    KSoet> specified.
 
    KSoet> Basically, the model is a one-dimensional
    KSoet> advection-diffusion problem, and thus the jacobian is
    KSoet> a tridiagonal matrix.

    KSoet> The size of this jacobian is 100*100.

    KSoet> In the original package LSODA it is possible to
    KSoet> specify that the jacobian is banded, which makes its
    KSoet> inversion very efficient.

    KSoet> However, this feature seems to have been removed in
    KSoet> the R version.
 
    KSoet> Is there a way to overcome this limitation?

Yes.  But probably not a very easy one; maybe even a very
cumbersome one... ;-)

Note however that questions like these should typically be
addressed at the package author - which you can always quickly
find out via

  > packageDescription("odesolve")
  Package: odesolve
  Version: 0.5-12
  Date: 2004/10/25
  Title: Solvers for Ordinary Differential Equations
  Author: R. Woodrow Setzer <setzer.woodrow at epa.gov>
  Maintainer: R. Woodrow Setzer <setzer.woodrow at epa.gov>
  Depends: R (>= 1.4.0)
  Description: This package provides an interface for the ODE solver
	  lsoda. ODEs are expressed as R functions or as compiled code.
  .......................

 
I've CC'ed this e-mail to Woodrow to help you for once


 <..........>

    KSoet> 	[[alternative HTML version deleted]]

    KSoet> ______________________________________________
    KSoet> .........
    KSoet> PLEASE do read the posting guide!
    KSoet> http://www.R-project.org/posting-guide.html

if you do read that guide, it will tell you 

- why you should always use a 'Subject' for your e-mails
- why HTML-ified e-mails are not much liked and what you can do
   about it.

Regards,
Martin Maechler, ETH Zurich



From karin.lagesen at medisin.uio.no  Mon Nov 14 15:55:09 2005
From: karin.lagesen at medisin.uio.no (Karin Lagesen)
Date: Mon, 14 Nov 2005 15:55:09 +0100
Subject: [R] bug/feature with barplot?
Message-ID: <ypx6veyv5lf6.fsf@uracil.uio.no>


I have found a bug/feature with barplot that at least to me shows
undesireable behaviour. When using barplot and plotting fewer
groups/levels/factors(I am unsure what they are called) than the number
of colors stated in a col statement, the colors wrap around such that
the colors are not fixed to one group. This is mostly problematic when
I make R figures using scripts, since I sometimes have empty input
groups. I have in these cases experienced labeling the empty group as
red, and then seeing a bar being red when that bar is actually from a
different group.

Reproducible example (I hope):

barplot(VADeaths, beside=TRUE, col=c("red", "green", "blue", "yellow", "black"))
barplot(VADeaths[1:4,], beside=TRUE, col=c("red", "green", "blue", "yellow", "black"))

Now, I don't know if this is a bug or a feature, but it sure bugged me...:)

Karin
-- 
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://www.cmbn.no/rognes/



From asr at ufl.edu  Mon Nov 14 16:41:51 2005
From: asr at ufl.edu (Allen S. Rout)
Date: Mon, 14 Nov 2005 10:41:51 -0500
Subject: [R] Curve fitting tutorial / clue stick?
Message-ID: <200511141541.jAEFfpbm060478@nersp.nerdc.ufl.edu>



Working through the R archives and webspace, I've mostly proved to myself that
I don't know enough about what statisticians call "Curve Fitting" to even
begin translating the basics.


I'm a sysadmin, and have collected a variety of measurements of my systems,
and I can draw pretty pictures in R showing what has happened.  People are
happy, customers feel empowered.  Whee!


Now, I want to take my corpus of data and make a prediction based on it; In
statistics-moron speak, I want to draw a line or a simple curve across my
extant graph, and figure out where the predictive curve passes threshold 'T',
and then graph that too.


I thought I'd be telling R something like:  

- I think this is exponential.  Here's the data.  Give me the best function
  you can come up with, and tell me "how good" the fit is.

- I think this is quadratic.  Here's the data.  Give me the best function
  you can come up with, and tell me "how good" the fit is.



Can someone point me at a spot in the docs which might be suitable for my
level of ignorance?  


- Allen S. Rout



From TobiasBr at Taquanta.com  Mon Nov 14 16:47:29 2005
From: TobiasBr at Taquanta.com (Brandt, T. (Tobias))
Date: Mon, 14 Nov 2005 17:47:29 +0200
Subject: [R] [<- and indexing for zoo objects
Message-ID: <A77412E534FCD248A93A81F37CC75B7A033F8C0B@waxbill.africa.nedcor.net>

Hi
 
I've been greatly enjoying the functionality the zoo package offers.
However I've hit a snag with the following code
 
> a <- zoo(matrix(1:10,5,2), 2001:2005)
> a
          
2001  1  6
2002  2  7
2003  3  8
2004  4  9
2005  5 10
> a[I(2003), 2]
      
2003 8
> a[I(2003), 2] <- NA
Error: subscript out of bounds
> 

I've also tried
 
> coredata(a[I(2003), 2]) <- NA
Error: subscript out of bounds
> 
 
but that doesn't work either.  I can of course do
 
> ix <- which(index(a)==2003)
> a[ix, 2] <- NA
> a
          
2001  1  6
2002  2  7
2003  3 NA
2004  4  9
2005  5 10
> 

which gives me the desired result but I feel that a timeseries class should
really be able to handle the first syntax since with the workaround I'm back
to the way of doing things before I had timeseries objects.
 
Am I missing something or any comments?
 

Regards

Tobias Brandt
 

********************
Nedbank Limited Reg No 1951/000009/06
Directors: WAM Clewlow (Chairman)  Prof MM Katz (Vice-chairman)  ML Ndlovu (Vice-chairman)  TA Boardman (Chief Executive)
CJW Ball  MWT Brown  RG Cottrell  BE Davison  N Dennis (British)  MA Enus-Brey  Prof B de L Figaji  RM Head (British)
RJ Khoza  JB Magwaza  ME Mkwanazi  JVF Roberts (British)  CML Savage  GT Serobe  JH Sutcliffe (British)
Company Secretary: GS Nienaber     16.08.2005

This email and any accompanying attachments may contain confidential and proprietary information.  This information is private and protected by law and, accordingly, if you are not the intended recipient, you are requested to delete this entire communication immediately and are notified that any disclosure, copying or distribution of or taking any action based on this information is prohibited.

Emails cannot be guaranteed to be secure or free of errors or viruses.  The sender does not accept any liability or responsibility for any interception, corruption, destruction, loss, late arrival or incompleteness of or tampering or interference with any of the information contained in this email or for its incorrect delivery or non-delivery for whatsoever reason or for its effect on any electronic device of the recipient.

If verification of this email or any attachment is required, please request a hard-copy version.



From TobiasBr at Taquanta.com  Mon Nov 14 16:53:41 2005
From: TobiasBr at Taquanta.com (Brandt, T. (Tobias))
Date: Mon, 14 Nov 2005 17:53:41 +0200
Subject: [R] Coercion of percentages by as.numeric
Message-ID: <A77412E534FCD248A93A81F37CC75B7A033F8C0C@waxbill.africa.nedcor.net>

Hi

Given that things like the following work
 
 > a <- c("-.1"," 2.7 ","B")
> a
[1] "-.1"   " 2.7 " "B"    
> as.numeric(a)
[1] -0.1  2.7   NA
Warning message:
NAs introduced by coercion 
> 

I naively expected that the following would behave differently.
 
 > b <- c('10%', '-20%', '30.0%', '.40%')
> b
[1] "10%"   "-20%"  "30.0%" ".40%" 
> as.numeric(b)
[1] NA NA NA NA
Warning message:
NAs introduced by coercion 
> 

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.0            
year     2005           
month    10             
day      06             
svn rev  35749          
language R              
> 

Various RSiteSearches with terms like "percentage" and "coercion" yielded
nothing.
 
Does anyone know how to do this elegantly?
 
 
Thanks
 
Tobias Brandt
 
 
P.S. Apologies if this appears on the list twice but I suspect an earlier
post was blocked since it was in html format.


********************
Nedbank Limited Reg No 1951/000009/06
Directors: WAM Clewlow (Chairman)  Prof MM Katz (Vice-chairman)  ML Ndlovu (Vice-chairman)  TA Boardman (Chief Executive)
CJW Ball  MWT Brown  RG Cottrell  BE Davison  N Dennis (British)  MA Enus-Brey  Prof B de L Figaji  RM Head (British)
RJ Khoza  JB Magwaza  ME Mkwanazi  JVF Roberts (British)  CML Savage  GT Serobe  JH Sutcliffe (British)
Company Secretary: GS Nienaber     16.08.2005

This email and any accompanying attachments may contain confidential and proprietary information.  This information is private and protected by law and, accordingly, if you are not the intended recipient, you are requested to delete this entire communication immediately and are notified that any disclosure, copying or distribution of or taking any action based on this information is prohibited.

Emails cannot be guaranteed to be secure or free of errors or viruses.  The sender does not accept any liability or responsibility for any interception, corruption, destruction, loss, late arrival or incompleteness of or tampering or interference with any of the information contained in this email or for its incorrect delivery or non-delivery for whatsoever reason or for its effect on any electronic device of the recipient.

If verification of this email or any attachment is required, please request a hard-copy version.



From mschwartz at mn.rr.com  Mon Nov 14 16:59:21 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 14 Nov 2005 09:59:21 -0600
Subject: [R] bug/feature with barplot?
In-Reply-To: <ypx6veyv5lf6.fsf@uracil.uio.no>
References: <ypx6veyv5lf6.fsf@uracil.uio.no>
Message-ID: <1131983962.5104.12.camel@localhost.localdomain>

On Mon, 2005-11-14 at 15:55 +0100, Karin Lagesen wrote:
> I have found a bug/feature with barplot that at least to me shows
> undesireable behaviour. When using barplot and plotting fewer
> groups/levels/factors(I am unsure what they are called) than the number
> of colors stated in a col statement, the colors wrap around such that
> the colors are not fixed to one group. This is mostly problematic when
> I make R figures using scripts, since I sometimes have empty input
> groups. I have in these cases experienced labeling the empty group as
> red, and then seeing a bar being red when that bar is actually from a
> different group.
> 
> Reproducible example (I hope):
> 
> barplot(VADeaths, beside=TRUE, col=c("red", "green", "blue", "yellow", "black"))
> barplot(VADeaths[1:4,], beside=TRUE, col=c("red", "green", "blue", "yellow", "black"))
> 
> Now, I don't know if this is a bug or a feature, but it sure bugged me...:)
> 
> Karin

Most definitely not a bug.

As with many vectorized function arguments, they will be recycled as
required to match the length of other appropriate arguments.

In this case, the number of colors (5) does not match the number of
groups (4). Thus, they are "out of synch" with each other and you get
the result you have.

Not unexpected behavior.

You should adjust your code and the function call so that the number of
groups matches the number of colors. Something along the lines of the
following:

col <- c("red", "green", "blue", "yellow", "black")
no.groups <- 4
barplot(VADeaths[1:no.groups, ], beside = TRUE, col = col[1:no.groups])


Now try:

 no.groups <- 5
 barplot(VADeaths[1:no.groups, ], beside = TRUE, col = col[1:no.groups])

 no.groups <- 3
 barplot(VADeaths[1:no.groups, ], beside = TRUE, col = col[1:no.groups])


HTH,

Marc Schwartz



From claus.atzenbeck at freenet.de  Mon Nov 14 17:01:08 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Mon, 14 Nov 2005 17:01:08 +0100 (CET)
Subject: [R] effect sizes for Wilcoxon tests
Message-ID: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>

Hello,

I use t.test for normal distributed and wilcox.test for non-normal
distributed samples.

It is easy to write a function for t.test that calculates the effect
size, because all parts of the formula are available from the t.test
result: r = sqrt(t*t / (t*t + df))

However, for Wilcoxon tests, the formula for effect sizes is:
r = Z / sqrt(N)

I wonder how I can calculate the Z-score in R for a Wilcoxon test.

BTW, would it be correct to name "wilcox.test(..., paired=F)" a
"Mann-Whitney test" in a report?  If I understand the documentation
(?wilcox.test) correctly, R does actually not use Mann-Whitney, but the
equivalent Wilcoxon test.

Thanks for clarification.
Claus



From Achim.Zeileis at R-project.org  Mon Nov 14 17:06:02 2005
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Mon, 14 Nov 2005 17:06:02 +0100
Subject: [R] [<- and indexing for zoo objects
In-Reply-To: <A77412E534FCD248A93A81F37CC75B7A033F8C0B@waxbill.africa.nedcor.net>
References: <A77412E534FCD248A93A81F37CC75B7A033F8C0B@waxbill.africa.nedcor.net>
Message-ID: <20051114170602.254d0ba5.Achim.Zeileis@R-project.org>

Tobias,

thanks for the report:

> > a[I(2003), 2] <- NA
> Error: subscript out of bounds

Yes, we would have to write a [<-.zoo method for that, currently we
rely on the corresponding methods for matrices and vectors. I'll add it
to the WISHLIST and try to add this functionality for the next zoo
release.

All the indexing can also be done via window<-

window(a, start = 2003, end = 2003)[,2] <- NA

which would currently be the preferred solution.

thx,
Z



From ggrothendieck at gmail.com  Mon Nov 14 17:16:18 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 14 Nov 2005 11:16:18 -0500
Subject: [R] [<- and indexing for zoo objects
In-Reply-To: <A77412E534FCD248A93A81F37CC75B7A033F8C0B@waxbill.africa.nedcor.net>
References: <A77412E534FCD248A93A81F37CC75B7A033F8C0B@waxbill.africa.nedcor.net>
Message-ID: <971536df0511140816q5eee4a75u622be7e8db92c990@mail.gmail.com>

On 11/14/05, Brandt, T. (Tobias) <TobiasBr at taquanta.com> wrote:
>
>
> Hi
>
> I've been greatly enjoying the functionality the zoo package offers.
> However I've hit a snag with the following code
>
> > a <- zoo(matrix(1:10,5,2), 2001:2005)
> > a
>
> 2001  1  6
> 2002  2  7
> 2003  3  8
> 2004  4  9
> 2005  5 10
> > a[I(2003), 2]
>
> 2003 8
> > a[I(2003), 2] <- NA
> Error: subscript out of bounds
> >
>
> I've also tried
>
> > coredata(a[I(2003), 2]) <- NA
> Error: subscript out of bounds
> >
>
> but that doesn't work either.  I can of course do


Try this:

   window(a, 2003)[,2] <- NA

See

   ?"window<-.zoo"

for more info.



From ggrothendieck at gmail.com  Mon Nov 14 17:20:33 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 14 Nov 2005 11:20:33 -0500
Subject: [R] Coercion of percentages by as.numeric
In-Reply-To: <A77412E534FCD248A93A81F37CC75B7A033F8C0C@waxbill.africa.nedcor.net>
References: <A77412E534FCD248A93A81F37CC75B7A033F8C0C@waxbill.africa.nedcor.net>
Message-ID: <971536df0511140820g7a7f70a9k2b45d84ab79c0cc1@mail.gmail.com>

On 11/14/05, Brandt, T. (Tobias) <TobiasBr at taquanta.com> wrote:
> Hi
>
> Given that things like the following work
>
>  > a <- c("-.1"," 2.7 ","B")
> > a
> [1] "-.1"   " 2.7 " "B"
> > as.numeric(a)
> [1] -0.1  2.7   NA
> Warning message:
> NAs introduced by coercion
> >
>
> I naively expected that the following would behave differently.
>
>  > b <- c('10%', '-20%', '30.0%', '.40%')
> > b
> [1] "10%"   "-20%"  "30.0%" ".40%"
> > as.numeric(b)
> [1] NA NA NA NA
> Warning message:
> NAs introduced by coercion

Try this:

as.numeric(sub("%", "e-2", b))



From jfontain at free.fr  Mon Nov 14 17:31:03 2005
From: jfontain at free.fr (Jean-Luc Fontaine)
Date: Mon, 14 Nov 2005 17:31:03 +0100
Subject: [R] Curve fitting tutorial / clue stick?
In-Reply-To: <200511141541.jAEFfpbm060478@nersp.nerdc.ufl.edu>
References: <200511141541.jAEFfpbm060478@nersp.nerdc.ufl.edu>
Message-ID: <4378BBC7.8040204@free.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Allen S. Rout wrote:

>
> Working through the R archives and webspace, I've mostly proved to
> myself that I don't know enough about what statisticians call
> "Curve Fitting" to even begin translating the basics.
>
>
> I'm a sysadmin,

I have just the thing for you (citing myself):
(http://moodss.sourceforge.net/ <http://moodss.sourceforge.net/>)
The major new feature planned and being worked on is... predicting the
future. With the help of the R project statistical engine (a
remarkable piece of software), the user will be able to receive emails
such as: "the disk on server S... is likely to become full in 3
weeks". The statistical model will be automatically determined by
moodss in the new predictor viewer, from traditional models (ARIMA,
...) and neural networks... Expect the new release by the end of 2005.

Unfortunately, that'll take a few months...

- --
Jean-Luc Fontaine  http://jfontain.free.fr/
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.1 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFDeLvGkG/MMvcT1qQRAtZUAKCYlqJP77sD4yeS747uvoNrtljHiwCfebkd
w+uE4Ip++2oabUWJjFqoZU4=
=hCLa
-----END PGP SIGNATURE-----



From rohitvk at vsnl.com  Mon Nov 14 17:40:38 2005
From: rohitvk at vsnl.com (Rohit Vishal Kumar)
Date: Mon, 14 Nov 2005 22:10:38 +0530
Subject: [R] Little's Chi Square test for MCAR?
In-Reply-To: <mailman.11.1131966002.7606.r-help@stat.math.ethz.ch>
References: <mailman.11.1131966002.7606.r-help@stat.math.ethz.ch>
Message-ID: <4378BE06.8040609@vsnl.com>

Hi.

Can anyone point me to any module in R which implements "Little's Chi 
Square test" for MCAR.
The problem is that i have around 60 behavioural variables on a 6 point 
categorical scale which i need to test for MCAR and MAR. What i can make 
out from preliminary analysis is that moderate (0.30 to 0.60) 
correlations  may be present in several variable pairs leading me to 
suspect that the data may not be MCAR or MAR. However i need some more 
"concrete" proof.

Any help - onlist or offlist - would be greatly appreciated.

Thanks in Advance

Rohit Vishal Kumar
Ph.D. Student (Calcutta) India



From dimitris.rizopoulos at med.kuleuven.be  Mon Nov 14 18:02:55 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 14 Nov 2005 18:02:55 +0100
Subject: [R] Little's Chi Square test for MCAR?
References: <mailman.11.1131966002.7606.r-help@stat.math.ethz.ch>
	<4378BE06.8040609@vsnl.com>
Message-ID: <00b201c5e93d$41977910$0540210a@www.domain>

This depends on the analysis you want to do; Maximum Likelihood will 
give you unbiased results even under MAR. In this case the more 
relevant question is whether the missing data mechanism is MNAR, in 
which case ML might give you biased results. Unfortunately you cannot 
test MNAR without making, some times very strong, assumptions.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rohit Vishal Kumar" <rohitvk at vsnl.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, November 14, 2005 5:40 PM
Subject: [R] Little's Chi Square test for MCAR?


> Hi.
>
> Can anyone point me to any module in R which implements "Little's 
> Chi
> Square test" for MCAR.
> The problem is that i have around 60 behavioural variables on a 6 
> point
> categorical scale which i need to test for MCAR and MAR. What i can 
> make
> out from preliminary analysis is that moderate (0.30 to 0.60)
> correlations  may be present in several variable pairs leading me to
> suspect that the data may not be MCAR or MAR. However i need some 
> more
> "concrete" proof.
>
> Any help - onlist or offlist - would be greatly appreciated.
>
> Thanks in Advance
>
> Rohit Vishal Kumar
> Ph.D. Student (Calcutta) India
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From TobiasBr at Taquanta.com  Mon Nov 14 18:07:42 2005
From: TobiasBr at Taquanta.com (Brandt, T. (Tobias))
Date: Mon, 14 Nov 2005 19:07:42 +0200
Subject: [R] Coercion of percentages by as.numeric
Message-ID: <A77412E534FCD248A93A81F37CC75B7A033F8C10@waxbill.africa.nedcor.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051114/0e5131c0/attachment.pl

From ggrothendieck at gmail.com  Mon Nov 14 18:36:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 14 Nov 2005 12:36:13 -0500
Subject: [R] Coercion of percentages by as.numeric
In-Reply-To: <A77412E534FCD248A93A81F37CC75B7A033F8C10@waxbill.africa.nedcor.net>
References: <A77412E534FCD248A93A81F37CC75B7A033F8C10@waxbill.africa.nedcor.net>
Message-ID: <971536df0511140936m31ad21c6la496209e932ebbce@mail.gmail.com>

On 11/14/05, Brandt, T. (Tobias) <TobiasBr at taquanta.com> wrote:
>
>
>
>
> >-----Original Message-----
> >From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> >Sent: 14 November 2005 06:21 PM
> >
> >On 11/14/05, Brandt, T. (Tobias) <TobiasBr at taquanta.com> wrote:
> >> Hi
> >>
> >> Given that things like the following work
> >>
> >>  > a <- c("-.1"," 2.7 ","B")
> >> > a
> >> [1] "-.1"   " 2.7 " "B"
> >> > as.numeric(a)
> >> [1] -0.1  2.7   NA
> >> Warning message:
> >> NAs introduced by coercion
> >> >
> >>
> >> I naively expected that the following would behave differently.
> >>
> >>  > b <- c('10%', '-20%', '30.0%', '.40%')
> >> > b
> >> [1] "10%"   "-20%"  "30.0%" ".40%"
> >> > as.numeric(b)
> >> [1] NA NA NA NA
> >> Warning message:
> >> NAs introduced by coercion
> >
> >Try this:
> >
> >as.numeric(sub("%", "e-2", b))
> >
>
> Thank you, that accomplishes what I had intended.
>
> I would have thought though that the expression "53%" would be a fairly
> standard representation of the number 0.53 and might be handled as such.  Is
> there a specific reason for avoiding this behaviour?
>
> I can imagine that it might add unnecessary overhead to routines like
> "as.numeric" which one would like to keep as fast as possible.
>
> Perhaps there are other areas though where it might be desirable?  For
> example I'm thinking of the read.table function for reading in csv files
> since I have many of these that have been saved from excel and now contain
> numbers in the "%" format.

Assuming a .csv file with trailing percents after some numbers
you could try this:

Lines <- readLines(myfile)
Lines <- gsub("%", "e-2", Lines)
mydata <- read.csv(textConnection(Lines))



From creemts at TNC.ORG  Mon Nov 14 18:57:37 2005
From: creemts at TNC.ORG (Charlotte Reemts)
Date: Mon, 14 Nov 2005 11:57:37 -0600
Subject: [R] point pattern interactions (Gcross and Kcross)
Message-ID: <IHEPIDGPPELJMBFKIEBAAEJACAAA.creemts@tnc.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051114/b6e8a360/attachment.pl

From mschwartz at mn.rr.com  Mon Nov 14 19:02:31 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 14 Nov 2005 12:02:31 -0600
Subject: [R] Coercion of percentages by as.numeric
In-Reply-To: <A77412E534FCD248A93A81F37CC75B7A033F8C10@waxbill.africa.nedcor.net>
References: <A77412E534FCD248A93A81F37CC75B7A033F8C10@waxbill.africa.nedcor.net>
Message-ID: <1131991351.5104.45.camel@localhost.localdomain>

On Mon, 2005-11-14 at 19:07 +0200, Brandt, T. (Tobias) wrote:
>  
> >-----Original Message-----
> >From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
> >Sent: 14 November 2005 06:21 PM
> >
> >On 11/14/05, Brandt, T. (Tobias) <TobiasBr at taquanta.com> wrote:
> >> Hi
> >>
> >> Given that things like the following work
> >>
> >>  > a <- c("-.1"," 2.7 ","B")
> >> > a
> >> [1] "-.1"   " 2.7 " "B"
> >> > as.numeric(a)
> >> [1] -0.1  2.7   NA
> >> Warning message:
> >> NAs introduced by coercion
> >> >
> >>
> >> I naively expected that the following would behave differently.
> >>
> >>  > b <- c('10%', '-20%', '30.0%', '.40%')
> >> > b
> >> [1] "10%"   "-20%"  "30.0%" ".40%"
> >> > as.numeric(b)
> >> [1] NA NA NA NA
> >> Warning message:
> >> NAs introduced by coercion
> >
> >Try this:
> >
> >as.numeric(sub("%", "e-2", b))
> >
> 
> Thank you, that accomplishes what I had intended.
> 
> I would have thought though that the expression "53%" would be a fairly
> standard representation of the number 0.53 and might be handled as such.  Is
> there a specific reason for avoiding this behaviour?  

"53%" is a 'shorthand' character representation of a mathematical
concept. To wit, the specific representation of a fraction using 100 as
the denominator (ie. 53 / 100). The symbol '%' can be replaced by the
word "percent", such as "53 percent", which is also a character
representation.

0.53, in context, is a numeric representation of a proportion in the
range of 0 - 1.0.

> I can imagine that it might add unnecessary overhead to routines like
> "as.numeric" which one would like to keep as fast as possible.
> 
> Perhaps there are other areas though where it might be desirable?  For
> example I'm thinking of the read.table function for reading in csv files
> since I have many of these that have been saved from excel and now contain
> numbers in the "%" format.

In Excel, numbers displayed with a '%' are what you see visually.
However, the internal representation (how the value is actually stored
in the program) is still as a floating point value, without the '%'. 

For example:

> a <- 53
> a
[1] 53

> sprintf("%.0f%%", a)
[1] "53%"

> is.numeric(a)
[1] TRUE

> is.numeric(sprintf("%.0f%%", a))
[1] FALSE


Unfortunately (depending upon your perspective), Excel, and other
similar programs, tend to export the visually displayed values and not
the internal representations of them. Thus, as Gabor pointed out, you
will need to do some 'editing' of the values before using them in R. You
can either do this in Excel, by removing the "%" formatting, or
post-import in R as Gabor has described.

You need to keep separate the internal representation of a value and its
printed or displayed representation for human readable consumption.

as.numeric() does basically one thing and it does it well and properly.
It is up to the user to ensure that it is passed the proper values. When
that is not the case, it issues an appropriate warning message and
returns NA.

Of course, using Gabor's hint, you can also write your own variation of
as.numeric(), creating a function that takes percent formatted values
and converts them as you require. One of the many strengths of R, is
that you can extend it to meet your own specific requirements when the
base functions do not.

HTH,

Marc Schwartz



From B.Rowlingson at lancaster.ac.uk  Mon Nov 14 19:08:12 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 14 Nov 2005 18:08:12 +0000
Subject: [R] point pattern interactions (Gcross and Kcross)
In-Reply-To: <IHEPIDGPPELJMBFKIEBAAEJACAAA.creemts@tnc.org>
References: <IHEPIDGPPELJMBFKIEBAAEJACAAA.creemts@tnc.org>
Message-ID: <4378D28C.7060402@lancaster.ac.uk>

Charlotte Reemts wrote:


> marked planar point pattern: 628 points
> multitype, with levels = w2004  w2005
> window: rectangle = [ 607200 , 634800 ] x [ 3438400 , 3460400 ]

  I just created something as close as possible to that using random 
poisson points:

  > oakfake
  marked planar point pattern: 628 points
  multitype, with levels = w2004  w2005
  window: rectangle = [ 607200 , 634800 ] x [ 3438400 , 3460400 ]

but Gcross works fine:

  > g.0405<-Gcross(oakfake, i="w2004", j="w2005")
  > plot(g.0405)

as does Kcross - so it must be something wrong with your data beyond 
what you've told us.

  Do you see it okay if you do plot(oakwilt)?

Baz



From veen at stat.ucla.edu  Mon Nov 14 19:23:02 2005
From: veen at stat.ucla.edu (Alejandro Veen)
Date: Mon, 14 Nov 2005 10:23:02 -0800
Subject: [R] roots of a function
In-Reply-To: <mailman.11.1131966002.7606.r-help@stat.math.ethz.ch>
Message-ID: <000301c5e948$730c24e0$d3376180@Osiris>

For finding the root of the following function I have been using 'uniroot':

f(p) = log(p-1) - log(p) + 1/(p-1) - log(A) - B = 0

where 'p' is a scalar.  However, I will have to find this root repeatedly,
so I would like to suggest a starting value, which is not possible with
'uniroot'.  'nlm' allows the use of starting values, so I have been thinking
of applying 'nlm' to abs(f(p)).  Is that the way to go or is there a better
way I don't know about?

Thanks for your help,

Alejandro



From wzhao6898 at gmail.com  Mon Nov 14 19:35:11 2005
From: wzhao6898 at gmail.com (David Zhao)
Date: Mon, 14 Nov 2005 10:35:11 -0800
Subject: [R] library MASS fitdistri() funciotn question
Message-ID: <4e6115a50511141035v4fadb999i93bb4e25c26fb6d7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051114/81d00315/attachment.pl

From william.astle at imperial.ac.uk  Mon Nov 14 20:01:42 2005
From: william.astle at imperial.ac.uk (William Astle)
Date: Mon, 14 Nov 2005 19:01:42 +0000
Subject: [R] as.integer with base other than ten.
Message-ID: <1131994902.9475.16.camel@fh-wja.sm.med.ic.ac.uk>

Is there an R function analogous to the C function strtol? 
I would like to convert a binary string into an integer.

Cheers for advice

Will


-- 
__________________________________________________
William Astle
Statistical Genetics,
David Balding's Group.
Imperial College,
St Mary's Hospital Campus,
147 Norfolk Place,
Paddington.
London.
W2 1PG



From creemts at TNC.ORG  Mon Nov 14 20:15:20 2005
From: creemts at TNC.ORG (Charlotte Reemts)
Date: Mon, 14 Nov 2005 13:15:20 -0600
Subject: [R] point pattern interactions (Gcross and Kcross)
In-Reply-To: <4378D28C.7060402@lancaster.ac.uk>
Message-ID: <IHEPIDGPPELJMBFKIEBAOEJACAAA.creemts@tnc.org>

In going over the creation of the point pattern (again), I discovered a typo
that switched x and y data.  Once I fixed that, the code worked just fine.
Thanks for your help!




Charlotte Reemts wrote:


> marked planar point pattern: 628 points
> multitype, with levels = w2004  w2005
> window: rectangle = [ 607200 , 634800 ] x [ 3438400 , 3460400 ]

  I just created something as close as possible to that using random
poisson points:

  > oakfake
  marked planar point pattern: 628 points
  multitype, with levels = w2004  w2005
  window: rectangle = [ 607200 , 634800 ] x [ 3438400 , 3460400 ]

but Gcross works fine:

  > g.0405<-Gcross(oakfake, i="w2004", j="w2005")
  > plot(g.0405)

as does Kcross - so it must be something wrong with your data beyond
what you've told us.

  Do you see it okay if you do plot(oakwilt)?

Baz



From mschwartz at mn.rr.com  Mon Nov 14 20:30:29 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 14 Nov 2005 13:30:29 -0600
Subject: [R] as.integer with base other than ten.
In-Reply-To: <1131994902.9475.16.camel@fh-wja.sm.med.ic.ac.uk>
References: <1131994902.9475.16.camel@fh-wja.sm.med.ic.ac.uk>
Message-ID: <1131996629.5104.59.camel@localhost.localdomain>

On Mon, 2005-11-14 at 19:01 +0000, William Astle wrote:
> Is there an R function analogous to the C function strtol? 
> I would like to convert a binary string into an integer.
> 
> Cheers for advice
> 
> Will


There was some discussion in the past and you might want to search the
archive for a more generic solution for any base to any base, but for
binary to decimal specifically, something like the following will work:

bin2dec <- function(x)
{
  b <- as.numeric(unlist(strsplit(x, "")))
  pow <- 2 ^ ((length(b) - 1):0)
  sum(pow[b == 1])
}


The function takes the binary string and splits it up into individual
numbers ('b'). It then creates a vector of powers of 2 as long as 'b'
less one through 0 ('pow').  It then takes the sum of the values of pow,
indexed by 'b == 1'.


> bin2dec("101")
[1] 5

> bin2dec("1111")
[1] 15

> bin2dec("1011111")
[1] 95


HTH,

Marc Schwartz



From andy_liaw at merck.com  Mon Nov 14 20:38:48 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Nov 2005 14:38:48 -0500
Subject: [R] open source and R
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5A3@usctmx1106.merck.com>

Here comes a not-so-nice one:  Sorry to be blunt, but I think the current
reality is that one's effectiveness in scientific computing is not likely to
be high if s/he can't read C for Fortran code.

The mode of development for new methods, I believe, should be:

- Write it in R (or S-PLUS or Matlab or ...) because one can usually do that
quite quickly.

- Check and make sure the code produces correct result.

- See if the code can be improved for efficiency.  Use the profiling
facility in R to see where the bottlenecks really are, and try to improve
those parts.

- If no significant improvement is possible in R, move only the
time-consuming part of the computation to C/Fortran/C++.

The above mode is not always followed, because many of the packages on CRAN
are simply R interfaces to _existing_ C/Fortran code.  One would be happy to
be able to use them at the R level, but to rewrite the whole thing in R, one
better have _very_ good reason!

For some algorithms, efficient code can be written in pure R, but the
resulting code can be less readable than one written more legibly in C for
Fortran.

Just my $0.02...

Andy

> From: Robert
> 
> Thanks for all the nice discussions. 
>   Though different users have various needs from R, It's 
> always good to stand on the shoulders of giants (as roger 
> said). How far we will see depends our ability to understand 
> what have been done by other languages. 
>   The package written in pure R might be good for education 
> in starting OOP in research but not effective in scientific 
> computing as suggested.
>   
> 
> Ted.Harding at nessie.mcc.ac.uk wrote:
>   On 13-Nov-05 Roger Bivand wrote:
> > On Sun, 13 Nov 2005, Robert wrote:
> > 
> >> If I do not know C or FORTRAN, how can I fully understand 
> the package
> >> or possibly improve it?
> > 
> > By learning enough to see whether that makes a difference for your 
> > purposes. Life is hard, but that's what makes life interesting ...
> > 
> >> Robert.
> >> 
> >> Roger Bivand wrote:
> >> On Sun, 13 Nov 2005, Robert wrote:
> >> 
> >> > Roger Bivand wrote: 
> >> > On Sun, 13 Nov 2005, Robert wrote:
> >> > 
> >> > > It uses FORTRAN code and not in pure R.
> >> > 
> >> > The same applies to deldir - it also includes Fortran. So the
> >> > answer seems to be no, there is no voronoi function only
> >> > written in R.
> >> > 
> >> 
> >> Robert wrote:
> >> 
> >> > 
> >> > I am curious about one thing: since the reason for using r
> >> > is r is a easy-to-learn language and it is good for getting
> >> > more people involved.
> >> >
> >> > Why most of the packages written in r use other languages
> >> > such as FORTRAN's code? I understand some functions have
> >> > already been written in other language or it is faster to
> >> > be implemented in other language.
> >> >
> >> > But my understanding is if the user does not know that
> >> > language (for example, FORTRAN), the package is still a
> >> > black box to him because he can not improve the package and
> >> > can not be involved in the development. 
> >> >
> >> > When I searched the packages of R, I saw many packages with
> >> > duplicated or similar functions. the main difference among
> >> > them are the different functions implemented using other
> >> >languages, which are always a black box to the users. So it
> >> > is very hard for users to believe the package will run
> >> > something they need, let alone getting involved in the
> >> > development. My comments are not to disregard these efforts.
> >> > But it is good to see the packages written in pure R.
> >> > 
> >> 
> >> Although surprisingly much of R is written in R, quite a lot is
> >> written in Fortran and C. One very good reason, apart from
> >> efficiency, is code
> >> re-use
> >> - BLAS and LAPACK among many others are excellent implementations
> >> of what we need for numerical linear algebra. R is very typical
> >> of good scientific software, it tries to avoid re-implementing
> >> functions that are used by the community, are well-supported by
> >> the community, and work. Packages by and large do the same - if
> >> existing software does the required job, package authors attempt
> >> to port that software to R, providing interfaces to underlying
> >> C or Fortran libraries. 
> >> 
> >> It's about standing on the shoulders of giants.
> 
> Those are very strong points. Some comments:
> 
> It would be possible to implement in "pure R" a matrix inversion
> or eigenvalue/vector function, for instance, and I'm sure it would
> be done (if it were done) to very high quality. However, it would
> run like an elephant in quicksands. BLAS and LAPACK have, over the
> years, become highly optimised not just for accuracy and robustness,
> but for speed and efficiency.
> 
> Also, you will hit the "other language" problem sooner or
> later. Robert's complaint is that he does not like black
> boxes. But R itself is a black box. You cannot write R in R,
> all the way down to the bottom. At the bottom is machine
> code, and languages like assember, C, C++, FORTRAN and
> their compilers provide "black box" wrappers for this.
> 
> That is not a whimsical comment either -- all those discussions
> about why 2 - sqrt(2)^2 is not equal to 0 come down to this
> sort of issue. Sooner or later, if you really want to understand
> what is going on, you have to get beneath the shiny smooth
> surface and swim amongst the molecules!
> 
> So, Robert, try to be positive about C and FORTRAN etc., rather
> than feeling put off by the fact that they are yet more things
> to learn and seem to get in the way of understanding how the
> functions work. C and FORTRAN are your friends, as well as
> the R langauge itself, and great deal more friemdly than
> the raw machine code. 
> 
> There is one aspect though where R users are in the cold when
> it comes to C and FORTAN. If you want to understand the function
> 'eigen', say, then you can "?eigen" to learn about its usage.
> You can enter "eigen" to see the R code, and indeed that is
> not too imcomprehensible. But then you find
> 
> .Fortran("ch", n, n, xr, xi, values = dbl.n, 
> !only.values, vectors = xr, ivectors = xi, dbl.n, 
> dbl.n, double(2 * n), ierr = integer(1),
> PACKAGE = "base")
> 
> and similar for "rs", "cg" and "rg". Where's the help for
> these? Nowhere obvious! In fact you have to go to the source
> code, locate the FORTRAN routines, and study these, hoping
> that enough helpful comments have been included to steer
> your study. So it is a much more formidable task, especially
> if you are having to learn the language at the same time.
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) 
> Fax-to-email: +44 (0)870 094 0861
> Date: 13-Nov-05 Time: 23:13:58
> ------------------------------ XFMail ------------------------------
>   
> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Markus.Gesmann at lloyds.com  Mon Nov 14 21:05:38 2005
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Mon, 14 Nov 2005 20:05:38 +0000
Subject: [R] change some levels of a factor column in data frame according
 to a condition
Message-ID: <321C3EEBDB00C24185705B8BF733DADD0BE5A362@LNVCNTEXCH01.corp.lloydsnet>

Dear R-users,

I am looking for an elegant way to change some levels of a factor column
in data frame according to a condition.
Lets look at the following data frame:

> data.frame(crit1=gl(2,5), crit2=factor(letters[1:10]), x=rnorm(10))
   crit1 crit2           x
1      1     a -1.06957692
2      1     b  0.24368402
3      1     c -0.24958322
4      1     d -1.37577955
5      1     e -0.01713288
6      2     f -1.25203573
7      2     g -1.94348533
8      2     h -0.16041719
9      2     i -1.91572616
10     2     j -0.20256478

Now I would like to find for each level in crit1 the two smallest values
of x and change the levels of crit2 to "small", so the result would look
like this:

   crit1 crit2           x
1      1     small -1.06957692
2      1     b  	0.24368402
3      1     c	 -0.24958322
4      1     small 	-1.37577955
5      1     e 	-0.01713288
6      2     f	 -1.25203573
7      2     small 	-1.94348533
8      2     h	 -0.16041719
9      2     small 	-1.91572616
10     2     j 	-0.20256478

Thank you for advice!

Markus Gesmann

************LNSCNTMCS01***************************************************
The information in this E-Mail and in any attachments is CON...{{dropped}}



From maechler at stat.math.ethz.ch  Mon Nov 14 21:46:47 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Nov 2005 21:46:47 +0100
Subject: [R] roots of a function
In-Reply-To: <000301c5e948$730c24e0$d3376180@Osiris>
References: <mailman.11.1131966002.7606.r-help@stat.math.ethz.ch>
	<000301c5e948$730c24e0$d3376180@Osiris>
Message-ID: <17272.63415.164459.109875@stat.math.ethz.ch>

>>>>> "Alejandro" == Alejandro Veen <veen at stat.ucla.edu>
>>>>>     on Mon, 14 Nov 2005 10:23:02 -0800 writes:

    Alejandro> For finding the root of the following function I
    Alejandro> have been using 'uniroot': f(p) = log(p-1) -
    Alejandro> log(p) + 1/(p-1) - log(A) - B = 0

    Alejandro> where 'p' is a scalar.  However, I will have to
    Alejandro> find this root repeatedly, so I would like to
    Alejandro> suggest a starting value, which is not possible
    Alejandro> with 'uniroot'.  'nlm' allows the use of starting
    Alejandro> values, so I have been thinking of applying 'nlm'
    Alejandro> to abs(f(p)).  Is that the way to go or is there
    Alejandro> a better way I don't know about?

No, using minimization instead of root finding is typically not
as efficient (particularly in one dimension).

But in some ways you *have* been using  starting values for
uniroot() contrary to what you said:

uniroot even needs an starting *interval* in which to search.
If you know a lot about your function and its zero, just make
that initial interval appropriately small.

    Alejandro> Thanks for your help,

you're welcome!
Martin



From p.connolly at hortresearch.co.nz  Mon Nov 14 21:58:31 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 15 Nov 2005 09:58:31 +1300
Subject: [R] Tidiest way of modifying S4 classes?
Message-ID: <20051114205831.GV18619@hortresearch.co.nz>

I wish to make modifications to the plot.pedigree function in the
kinship package.  My attempts to contact the maintainer have been
unsuccessful, but my question is general, so specifics of the kinship
package might not be an issue.

My first attempt was to make a new function Plot.pedigree in the
.GlobalEnv which mostly achieved what I wanted to.  However, I'm sure
that's not the tidiest way to do it.  We don't have the green book,
but there's lots of interesting information I found here:

http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/S4Objects

However, there's something I'm missing in connecting that information
into knowledge of how I go about making a new method or slot or
whatever is sensible in this case.  What does one make of this:


> getClass(class(kinship:::plot.pedigree))

No Slots, prototype of class "function"

Extends: "OptionalFunction", "PossibleMethod"

Known Subclasses: 
Class "MethodDefinition", from data part
Class "genericFunction", from data part
Class "functionWithTrace", from data part
Class "derivedDefaultMethod", by class "MethodDefinition"
Class "MethodWithNext", by class "MethodDefinition"
Class "SealedMethodDefinition", by class "MethodDefinition"
Class "standardGeneric", by class "genericFunction"
Class "nonstandardGenericFunction", by class "genericFunction"
Class "groupGenericFunction", by class "genericFunction"
> 


If I want a new plot.pedigree function, do I make a slot, or what is
the approach to take?

Suggestions most welcome.

Thanks

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From jholtman at gmail.com  Mon Nov 14 22:02:38 2005
From: jholtman at gmail.com (jim holtman)
Date: Mon, 14 Nov 2005 16:02:38 -0500
Subject: [R] change some levels of a factor column in data frame
	according to a condition
In-Reply-To: <321C3EEBDB00C24185705B8BF733DADD0BE5A362@LNVCNTEXCH01.corp.lloydsnet>
References: <321C3EEBDB00C24185705B8BF733DADD0BE5A362@LNVCNTEXCH01.corp.lloydsnet>
Message-ID: <644e1f320511141302p1a71eed3k1c3fcbd308833cdd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051114/7e3ae155/attachment.pl

From elvis at xlsolutions-corp.com  Mon Nov 14 22:03:56 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Mon, 14 Nov 2005 14:03:56 -0700
Subject: [R] November Course In San Francisco***R/Splus Fundamentals and
	Programming Techniques
Message-ID: <20051114140356.a108dc04937c07ba67766dad37185406.7a12405f3f.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" in San Francisco: www.xlsolutions-corp.com/Rfund.htm

**** San Francisco,   November 17 - 18, 2005

Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From gerifalte28 at hotmail.com  Mon Nov 14 23:01:38 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 14 Nov 2005 22:01:38 +0000
Subject: [R] change some levels of a factor column in data frame
	according to a condi
In-Reply-To: <321C3EEBDB00C24185705B8BF733DADD0BE5A362@LNVCNTEXCH01.corp.lloydsnet>
Message-ID: <BAY103-F102FFA5FB3F8A0F5ABFE03A65A0@phx.gbl>

Hi Gesman

There may be more elegant ways to do this but here is one option:

d=data.frame(crit1=gl(2,5), crit2=factor(letters[1:10]), x=rnorm(10)) 
#Creates data

levels(d$crit2)=c(levels(d$crit2),"Small")#Adds the level "Small" to the 
factor crit2.

d2=d[order(d$crit1,d$x),]#Sorts x ascending, by crit1

idx=do.call("rbind",by(d2,d$crit1,head,2))#selects the 2 smallest by crit1 
and merges the results by row

d2[d2$x %in% idx$x,'crit2']="Small" #Changes the desired crit2 to "Small"


Cheers

Francisco


>From: "Gesmann, Markus" <Markus.Gesmann at lloyds.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] change some levels of a factor column in data frame according 
>to a condition
>Date: Mon, 14 Nov 2005 20:05:38 +0000
>
>Dear R-users,
>
>I am looking for an elegant way to change some levels of a factor column
>in data frame according to a condition.
>Lets look at the following data frame:
>
> > data.frame(crit1=gl(2,5), crit2=factor(letters[1:10]), x=rnorm(10))
>    crit1 crit2           x
>1      1     a -1.06957692
>2      1     b  0.24368402
>3      1     c -0.24958322
>4      1     d -1.37577955
>5      1     e -0.01713288
>6      2     f -1.25203573
>7      2     g -1.94348533
>8      2     h -0.16041719
>9      2     i -1.91572616
>10     2     j -0.20256478
>
>Now I would like to find for each level in crit1 the two smallest values
>of x and change the levels of crit2 to "small", so the result would look
>like this:
>
>    crit1 crit2           x
>1      1     small -1.06957692
>2      1     b  	0.24368402
>3      1     c	 -0.24958322
>4      1     small 	-1.37577955
>5      1     e 	-0.01713288
>6      2     f	 -1.25203573
>7      2     small 	-1.94348533
>8      2     h	 -0.16041719
>9      2     small 	-1.91572616
>10     2     j 	-0.20256478
>
>Thank you for advice!
>
>Markus Gesmann
>
>************LNSCNTMCS01***************************************************
>The information in this E-Mail and in any attachments is CON...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Mon Nov 14 23:46:05 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Nov 2005 17:46:05 -0500
Subject: [R] open source and R
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5A6@usctmx1106.merck.com>

However code readability can not be over-emphasized.  I must admit to have
written R code in such a supposedly `clever' way that I can't figure out
what I was trying to do (or how I did it) a week later...

Andy

> From: Ernesto Jardim 
> 
> Hi,
> 
> One single comment about the subject of this message. Open source is 
> about making the code _available_ for all, not making the code 
> _understandable_ for all.
> 
> Regards
> 
> EJ
> 
>



From dataanalytics at earthlink.net  Mon Nov 14 23:55:00 2005
From: dataanalytics at earthlink.net (Walter R. Paczkowski)
Date: Mon, 14 Nov 2005 22:55:00 +0000
Subject: [R] Using pakage foreign and to import SAS file
Message-ID: <E1EbnEC-0006GI-Um@smtpauth01.mail.atl.earthlink.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051114/1512f22a/attachment.pl

From dushoff at eno.princeton.edu  Tue Nov 15 00:08:16 2005
From: dushoff at eno.princeton.edu (Jonathan Dushoff)
Date: Mon, 14 Nov 2005 18:08:16 -0500 (EST)
Subject: [R] Trouble with aovlist and Tukey test
Message-ID: <Pine.LNX.4.61.0511141753190.326@tahawus.Princeton.EDU>

I am having what I think is a strange problem with applying TukeyHSD to
an aov fit with error strata.

TukeyHSD is supposed to take "A fitted model object, usually an 'aov'
fit."  aov (with error strata) is supposed to generate an object of type
aovlist, which is a list of objects of type aov.  But I can't seem to
feed components of my aovlist to TukeyHSD.  I guess I wouldn't expect to
be able to use the error strata, but I did expect to be able to use the
final stratum.

I have posted a complete example, which I hope explains why I am
confused, below.  Any help will be appreciated.

Jonathan Dushoff

----------------------------------------------------------------------


> morley$Expt = factor(morley$Expt)
> morley$Run = factor(morley$Run)
>
> mod =  aov(Speed~Expt+Run, data=morley)
> class(mod)
[1] "aov" "lm"
>
> TukeyHSD(mod)$Expt
      diff        lwr        upr
2-1 -53.0 -117.91627  11.916268
3-1 -64.0 -128.91627   0.916268
4-1 -88.5 -153.41627 -23.583732
5-1 -77.5 -142.41627 -12.583732
3-2 -11.0  -75.91627  53.916268
4-2 -35.5 -100.41627  29.416268
5-2 -24.5  -89.41627  40.416268
4-3 -24.5  -89.41627  40.416268
5-3 -13.5  -78.41627  51.416268
5-4  11.0  -53.91627  75.916268
>
> errmod =  aov(Speed~Expt+Error(Run), data=morley)
> names(errmod)
[1] "(Intercept)" "Run"         "Within"
> basemod = errmod$W
>
> class(basemod)
[1] "aov" "lm"
> TukeyHSD(basemod)
Error in sort(unique.default(x), na.last = TRUE) :
    'x' must be atomic



From gunter.berton at gene.com  Tue Nov 15 01:00:49 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 14 Nov 2005 16:00:49 -0800
Subject: [R] open source and R
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5A6@usctmx1106.merck.com>
Message-ID: <200511150000.jAF00nNG007811@ohm.gene.com>

Andy:

Ah, don't feel bad, Andy; this is a universal problem in programming that
despite all kinds of efforts in "lucid programming", OOP, etc. no one has
figured out. So while "code readability cannot be overemphasized," what this
actually means also apparently cannot be defined.

From: http://www.jeffgainer.com/lucid_code/lc_cover.html

"If you are a software professional, you know how software is created.
Surely you recognize it. Chances are you live it: chaos."

;-)

-- Bert
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Monday, November 14, 2005 2:46 PM
> To: 'Ernesto Jardim'
> Cc: ted.harding at nessie.mcc.ac.uk; r-help at stat.math.ethz.ch
> Subject: Re: [R] open source and R
> 
> However code readability can not be over-emphasized.  I must 
> admit to have
> written R code in such a supposedly `clever' way that I can't 
> figure out
> what I was trying to do (or how I did it) a week later...
> 
> Andy
> 
> > From: Ernesto Jardim 
> > 
> > Hi,
> > 
> > One single comment about the subject of this message. Open 
> source is 
> > about making the code _available_ for all, not making the code 
> > _understandable_ for all.
> > 
> > Regards
> > 
> > EJ
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From usenet at s-boehringer.de  Tue Nov 15 01:17:16 2005
From: usenet at s-boehringer.de (usenet@s-boehringer.de)
Date: Tue, 15 Nov 2005 01:17:16 +0100
Subject: [R] (no subject)
Message-ID: <E1EboVg-0007Zf-00@magellan.synserver.de>

Dear all,

just a little problem report for R 2.2.0 on OpenSuse 10.0-64. Gcc version is 4.0.2
Installing fortran packages runs into: 'cc1' command not found.
I apparently got away with:
sudo ln -s /usr/bin/cc /usr/bin/cc1
which causes other warnings but the packages seem to function well. Obviously cc1 does no longer exist in gcc 4.0.2.

Stefan



From Ted.Harding at nessie.mcc.ac.uk  Tue Nov 15 01:45:48 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Nov 2005 00:45:48 -0000 (GMT)
Subject: [R] error in NORM lib
In-Reply-To: <437910D3.6040704@anicca-vijja.de>
Message-ID: <XFMail.051115004548.Ted.Harding@nessie.mcc.ac.uk>

Folks,

Leo G??rther and I have been privately discussing the
problems with imputation using NORM which he originally
described on 9 November. Essentially, he observed that
many of the imputed missing values were totally absurd,
being well out of any range comatible with the observed
values of the variables.

After following a few false trails, we have discovered
the reason. People interested in using NORM (and CAT and
MIX and maybe PAN) may well be interested in this reason!

The dataset, which can be downloaded from his URL

  http://www.anicca-vijja.de/lg/dframe.Rdata

consists of a matrix with 74 columns and 200 rows.
There are 553 missing values out of the 14800 (less
than 4%), and the distributions of the observed values
of the variables are well-behaved. So this should not
be a problematic dataset.

61 of the 74 columns have missing values (NAs) in them,
and this is the reason why NORM fails.

Specifically, the first few lines of the code of the
function prelim.norm() are as follows:

    if (is.vector(x)) 
        x <- matrix(x, length(x), 1)
    n <- nrow(x)
    p <- ncol(x)
    storage.mode(x) <- "double"
    r <- 1 * is.na(x)
    nmis <- as.integer(apply(r, 2, sum))
    names(nmis) <- dimnames(x)[[2]]
    mdp <- as.integer((r %*% (2^((1:ncol(x)) - 1))) + 1)

and, as can be seen from the last line, if there are
missing values in a column with index > 31 then

  (r %*% (2^((1:ncol(x)) - 1))) + 1 >= 2^31

and then applying as.integer() to this value returns NA
since as.integer only works for numbers no greater than
.Machine$integer.max, normally 2^31 - 1. (Is the situation
different for R on say 64-bit machines?)

The value of mdp[i] is a "packed" binary encoding of the
column positions of any NAs in row i: if bit j-1 (counting
from 0) in the binary representation of mdp[i] is 1, then
there is an NA in column j of row i.

The vector mdp is used at various places in the NORM routines,
and the effect on the imputations of having NAs in it, when
the functioning of the routines depends on unpacking the
encoding, is catastrophic. (Experiment had shown, indeed,
that imputing with a subset of fewer than 32 columns always
gave acceptable results).

The upshot of this is that NORM cannot be used for multiple
imputations if there are more than 31 columns in the data
which have NAs in them.

You could have more than 31 columns of data -- indeed Leo's
74 would have worked then -- if the columns are re-ordered
so that all the columns with NAs are at the left, provided
there are fewer than 32 with NAs. Unfortunately Leo has 61.

There is in principle no necessity to represent NA positions
in this way, but that is how Shafer did it and it was carried
over into R. An alternative method would simply be to have
a 0/1 matrix of NA indicators, but the code for the NORM
functions would have to be picked through to replace the
unpacking of mdp -- and this includes FORTRAN routines
(Oh dear, echoes of the "open source and R" discussion)!

So removing this limitation would not be trivial.

I have not noticed mention of the limitation in the documentation
of the NORM functions.

Exactly the same construction of mdp, and therefore exactly the
same problem, occurs in prelim.cat in CAT, for which I'm joint
maintainer with Fernando Tusell, so we had better try to look
into that! Any lessons we learn will be broadcast, so should be
useful for NORM as well.

And, for good measure, in MIX it occurs twice over in prelim.mix:
once in constructing mdpz for the continuous variables, and
once in mdpw for the categorical variables. This is perhaps
less likely in practice to cause the problem in MIX, since it
would arise only if either there were more than 31 columns of
continuous variables with NAs, or more than 31 of categorical
variables; so MIXers can spread their bets.

Again, I have not noticed that the limitation is mentioned
in the documentation of MIX; and I'm pretty sure it is not
in the documentation of CAT!

Any suggestions or guidance from people who are familiar with
NORM and MIX will be most welcome.

I should add that I have not looked into PAN, but would not
be surprised if it were there as well.

I've written this explanation in consultation with Leo G??rtler,
and he has proposed that I should publish it to the R List;
but please consider that it is a joint effort.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 15-Nov-05                                       Time: 00:45:46
------------------------------ XFMail ------------------------------



From zhuzhaoxuan at hotmail.com  Tue Nov 15 05:06:18 2005
From: zhuzhaoxuan at hotmail.com (Z ZX)
Date: Tue, 15 Nov 2005 00:06:18 -0400
Subject: [R]   Linear model mixed with an ARIMA model
Message-ID: <BAY114-F1587630248F626573C354DB65D0@phx.gbl>


   Dear all,

   I'm looking for how can I input a linear model with an arma model,like

   log(y) = 8.95756 + 0.0346414^t - 0.1*t^2   + ut
   ut=-0.296ut-1+at-0.68at-1

   where log(y) is qudratic function ,for the time series trend,

   and get then get the residuals from the first function.

    " obersvations value - the fit value = ut"

   and fit an ARIMA(1,1,1) model for ut.

   anyway,how can I combine this two models together as a group ?

   my purpose is to  to use this mixed model forecast  'y'

   can you help me?  I will very appreciate it.


     _________________________________________________________________

   Don't just Search. Find! [1]The new MSN Search: Fast. Clear. Easy.

References

   1. http://g.msn.com/8HMAENCA/2749??PS=47575


From e8a91 at unb.ca  Tue Nov 15 05:14:39 2005
From: e8a91 at unb.ca (Zhu, Zhaoxuan)
Date: Tue, 15 Nov 2005 00:14:39 -0400
Subject: [R]  Linear model mixed with an ARIMA model
Message-ID: <1132028079.437960af2bf43@webmail.unb.ca>

Dear all, 


I'm looking for how can I input a linear model with an arma model,like

log(y) = 8.95756 + 0.0346414^t - 0.1*t^2   + ut           
ut=-0.296ut-1+at-0.68at-1  

where log(y) is qudratic function ,for the time series trend,

and get then get the residuals from the first function. 

 " obersvations value - the fit value = ut"

and fit an ARIMA(1,1,1) model for ut.

anyway,how can I combine this two models together as a group ? 

my purpose is to  to use this mixed model forecast  'y'

can you help me?  I will very appreciate it.



From ripley at stats.ox.ac.uk  Tue Nov 15 05:21:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Nov 2005 04:21:34 +0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <E1EboVg-0007Zf-00@magellan.synserver.de>
References: <E1EboVg-0007Zf-00@magellan.synserver.de>
Message-ID: <Pine.LNX.4.61.0511150415280.27867@gannet.stats>

In what sense is this a problem report for R?  R does not know about
cc1, unless some user told it to use it.

cc1 is an internal part of gcc (the C front-end), usually found in

/usr/libexec/gcc/i686-pc-linux-gnu/4.0.2

or some such path. As my path shows, it is part of gcc 4.0.2, so this 
looks like a error in your compiler installation.  It is not to do 
with Fortran, whose front-end is f951 in the same directory.

On Tue, 15 Nov 2005, usenet at s-boehringer.de wrote:

> just a little problem report for R 2.2.0 on OpenSuse 10.0-64. Gcc version is 4.0.2
> Installing fortran packages runs into: 'cc1' command not found.
> I apparently got away with:
> sudo ln -s /usr/bin/cc /usr/bin/cc1
> which causes other warnings but the packages seem to function well. Obviously cc1 does no longer exist in gcc 4.0.2.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rab45 at pitt.edu  Tue Nov 15 05:22:16 2005
From: rab45 at pitt.edu (Rick Bilonick)
Date: Mon, 14 Nov 2005 23:22:16 -0500
Subject: [R] Using pakage foreign and to import SAS file
In-Reply-To: <E1EbnEC-0006GI-Um@smtpauth01.mail.atl.earthlink.net>
References: <E1EbnEC-0006GI-Um@smtpauth01.mail.atl.earthlink.net>
Message-ID: <1132028537.3257.2.camel@localhost.localdomain>

On Mon, 2005-11-14 at 22:55 +0000, Walter R. Paczkowski wrote:
> Hi,
> 
> I'm struggling with foreign to import a SAS file.  The file, for lack of imagination, is d.sas7bdat and is in my root directory (c:\) under Windows XP.  When I type
> 
> read.ssd("c:\\", "d")
> 
> which I think I'm suppose to enter, I get
> 
> SAS failed.  SAS program at C:\DOCUME~1\Owner\LOCALS~1\Temp\Rtmp32758\file19621.sas 
> The log file will be file19621.log in the current directory
> NULL
> Warning messages:
> 1: "sas" not found 
> 2: SAS return code was -1 in: read.ssd("c:\\", "d") 
> 
> I have SAS 9.1 running on my computer so SAS is there.  What am I doing wrong?
> 
> Thanks,
> 
> Walt
> 
> 
I've not used read.ssd but I've had good results with sas.get in Hmisc.

Rick B.



From maustin at amgen.com  Tue Nov 15 05:39:28 2005
From: maustin at amgen.com (Austin, Matt)
Date: Mon, 14 Nov 2005 20:39:28 -0800
Subject: [R] Using pakage foreign and to import SAS file
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD45D@teal-exch.amgen.com>

If sas isn't in the path, then you might have trouble with sas.get or
read.ssd.

Assuming you are using windows, go to the Start menu, select run and type
"sas".  If sas fires up it's in your path, if not then that is the reason.

--Matt

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Rick Bilonick
> Sent: Monday, November 14, 2005 8:22 PM
> To: Walter R. Paczkowski
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Using pakage foreign and to import SAS file
> 
> 
> On Mon, 2005-11-14 at 22:55 +0000, Walter R. Paczkowski wrote:
> > Hi,
> > 
> > I'm struggling with foreign to import a SAS file.  The 
> file, for lack of imagination, is d.sas7bdat and is in my 
> root directory (c:\) under Windows XP.  When I type
> > 
> > read.ssd("c:\\", "d")
> > 
> > which I think I'm suppose to enter, I get
> > 
> > SAS failed.  SAS program at 
> C:\DOCUME~1\Owner\LOCALS~1\Temp\Rtmp32758\file19621.sas 
> > The log file will be file19621.log in the current directory
> > NULL
> > Warning messages:
> > 1: "sas" not found 
> > 2: SAS return code was -1 in: read.ssd("c:\\", "d") 
> > 
> > I have SAS 9.1 running on my computer so SAS is there.  
> What am I doing wrong?
> > 
> > Thanks,
> > 
> > Walt
> > 
> > 
> I've not used read.ssd but I've had good results with sas.get 
> in Hmisc.
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Nov 15 07:17:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Nov 2005 06:17:52 +0000 (GMT)
Subject: [R] Using pakage foreign and to import SAS file
In-Reply-To: <E7D5AB4811D20B489622AABA9C53859109DAD45D@teal-exch.amgen.com>
References: <E7D5AB4811D20B489622AABA9C53859109DAD45D@teal-exch.amgen.com>
Message-ID: <Pine.LNX.4.61.0511150609200.7825@gannet.stats>

It is highly unlikely that SAS is on the path, as it does not put itself 
there.

read.ssd () has a 'sascmd' argument to give the path to SAS.  This is 
explained  *with a functioning Windows example*, on the help page for 
read.ssd.

On Mon, 14 Nov 2005, Austin, Matt wrote:

> If sas isn't in the path, then you might have trouble with sas.get or
> read.ssd.
>
> Assuming you are using windows, go to the Start menu, select run and type
> "sas".  If sas fires up it's in your path, if not then that is the reason.
>
> --Matt
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Rick Bilonick
>> Sent: Monday, November 14, 2005 8:22 PM
>> To: Walter R. Paczkowski
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] Using pakage foreign and to import SAS file
>>
>>
>> On Mon, 2005-11-14 at 22:55 +0000, Walter R. Paczkowski wrote:
>>> Hi,
>>>
>>> I'm struggling with foreign to import a SAS file.  The
>> file, for lack of imagination, is d.sas7bdat and is in my
>> root directory (c:\) under Windows XP.  When I type
>>>
>>> read.ssd("c:\\", "d")
>>>
>>> which I think I'm suppose to enter, I get
>>>
>>> SAS failed.  SAS program at
>> C:\DOCUME~1\Owner\LOCALS~1\Temp\Rtmp32758\file19621.sas
>>> The log file will be file19621.log in the current directory
>>> NULL
>>> Warning messages:
>>> 1: "sas" not found
>>> 2: SAS return code was -1 in: read.ssd("c:\\", "d")
>>>
>>> I have SAS 9.1 running on my computer so SAS is there.
>> What am I doing wrong?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Nov 15 09:08:06 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2005 09:08:06 +0100
Subject: [R] (no subject)
In-Reply-To: <Pine.LNX.4.61.0511150415280.27867@gannet.stats>
References: <E1EboVg-0007Zf-00@magellan.synserver.de>
	<Pine.LNX.4.61.0511150415280.27867@gannet.stats>
Message-ID: <x2acg649ll.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> In what sense is this a problem report for R?  R does not know about
> cc1, unless some user told it to use it.
> 
> cc1 is an internal part of gcc (the C front-end), usually found in
> 
> /usr/libexec/gcc/i686-pc-linux-gnu/4.0.2
> 
> or some such path. As my path shows, it is part of gcc 4.0.2, so this 
> looks like a error in your compiler installation.  It is not to do 
> with Fortran, whose front-end is f951 in the same directory.

Could still be Fortran. Apparently upgrading SUSE to 10.0 will upgrade
the C compiler to gcc-4.x and g77 to compat-g77-3.x and you need an
explicit install of gfortran, aka gcc-fortran. Mixing 3.x and 4.x
compilers won't work. I haven't seen the cc1 symptom though.

 
> On Tue, 15 Nov 2005, usenet at s-boehringer.de wrote:
> 
> > just a little problem report for R 2.2.0 on OpenSuse 10.0-64. Gcc version is 4.0.2
> > Installing fortran packages runs into: 'cc1' command not found.
> > I apparently got away with:
> > sudo ln -s /usr/bin/cc /usr/bin/cc1
> > which causes other warnings but the packages seem to function well. Obviously cc1 does no longer exist in gcc 4.0.2.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dejongroel at gmail.com  Tue Nov 15 10:15:26 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Tue, 15 Nov 2005 10:15:26 +0100
Subject: [R] error in NORM lib
In-Reply-To: <XFMail.051115004548.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051115004548.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <4379A72E.3010805@gmail.com>

try MICE package :)

regards,
	Roel de Jong

(Ted Harding) wrote:
> Folks,
> 
> Leo G??rther and I have been privately discussing the
> problems with imputation using NORM which he originally
> described on 9 November. Essentially, he observed that
> many of the imputed missing values were totally absurd,
> being well out of any range comatible with the observed
> values of the variables.
> 
> After following a few false trails, we have discovered
> the reason. People interested in using NORM (and CAT and
> MIX and maybe PAN) may well be interested in this reason!
> 
> The dataset, which can be downloaded from his URL
> 
>   http://www.anicca-vijja.de/lg/dframe.Rdata
> 
> consists of a matrix with 74 columns and 200 rows.
> There are 553 missing values out of the 14800 (less
> than 4%), and the distributions of the observed values
> of the variables are well-behaved. So this should not
> be a problematic dataset.
> 
> 61 of the 74 columns have missing values (NAs) in them,
> and this is the reason why NORM fails.
> 
> Specifically, the first few lines of the code of the
> function prelim.norm() are as follows:
> 
>     if (is.vector(x)) 
>         x <- matrix(x, length(x), 1)
>     n <- nrow(x)
>     p <- ncol(x)
>     storage.mode(x) <- "double"
>     r <- 1 * is.na(x)
>     nmis <- as.integer(apply(r, 2, sum))
>     names(nmis) <- dimnames(x)[[2]]
>     mdp <- as.integer((r %*% (2^((1:ncol(x)) - 1))) + 1)
> 
> and, as can be seen from the last line, if there are
> missing values in a column with index > 31 then
> 
>   (r %*% (2^((1:ncol(x)) - 1))) + 1 >= 2^31
> 
> and then applying as.integer() to this value returns NA
> since as.integer only works for numbers no greater than
> .Machine$integer.max, normally 2^31 - 1. (Is the situation
> different for R on say 64-bit machines?)
> 
> The value of mdp[i] is a "packed" binary encoding of the
> column positions of any NAs in row i: if bit j-1 (counting
> from 0) in the binary representation of mdp[i] is 1, then
> there is an NA in column j of row i.
> 
> The vector mdp is used at various places in the NORM routines,
> and the effect on the imputations of having NAs in it, when
> the functioning of the routines depends on unpacking the
> encoding, is catastrophic. (Experiment had shown, indeed,
> that imputing with a subset of fewer than 32 columns always
> gave acceptable results).
> 
> The upshot of this is that NORM cannot be used for multiple
> imputations if there are more than 31 columns in the data
> which have NAs in them.
> 
> You could have more than 31 columns of data -- indeed Leo's
> 74 would have worked then -- if the columns are re-ordered
> so that all the columns with NAs are at the left, provided
> there are fewer than 32 with NAs. Unfortunately Leo has 61.
> 
> There is in principle no necessity to represent NA positions
> in this way, but that is how Shafer did it and it was carried
> over into R. An alternative method would simply be to have
> a 0/1 matrix of NA indicators, but the code for the NORM
> functions would have to be picked through to replace the
> unpacking of mdp -- and this includes FORTRAN routines
> (Oh dear, echoes of the "open source and R" discussion)!
> 
> So removing this limitation would not be trivial.
> 
> I have not noticed mention of the limitation in the documentation
> of the NORM functions.
> 
> Exactly the same construction of mdp, and therefore exactly the
> same problem, occurs in prelim.cat in CAT, for which I'm joint
> maintainer with Fernando Tusell, so we had better try to look
> into that! Any lessons we learn will be broadcast, so should be
> useful for NORM as well.
> 
> And, for good measure, in MIX it occurs twice over in prelim.mix:
> once in constructing mdpz for the continuous variables, and
> once in mdpw for the categorical variables. This is perhaps
> less likely in practice to cause the problem in MIX, since it
> would arise only if either there were more than 31 columns of
> continuous variables with NAs, or more than 31 of categorical
> variables; so MIXers can spread their bets.
> 
> Again, I have not noticed that the limitation is mentioned
> in the documentation of MIX; and I'm pretty sure it is not
> in the documentation of CAT!
> 
> Any suggestions or guidance from people who are familiar with
> NORM and MIX will be most welcome.
> 
> I should add that I have not looked into PAN, but would not
> be surprised if it were there as well.
> 
> I've written this explanation in consultation with Leo G??rtler,
> and he has proposed that I should publish it to the R List;
> but please consider that it is a joint effort.
> 
> Best wishes to all,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 15-Nov-05                                       Time: 00:45:46
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From loncard at who.int  Tue Nov 15 10:31:31 2005
From: loncard at who.int (Loncar, Dejan)
Date: Tue, 15 Nov 2005 10:31:31 +0100
Subject: [R] conversion from RData to R file
Message-ID: <349865A6DFCF704488DCA7BB5CC83BED6DCC03@HQSWKAKI01.hq.intra.who.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051115/93663e39/attachment.pl

From maechler at stat.math.ethz.ch  Tue Nov 15 10:42:33 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 15 Nov 2005 10:42:33 +0100
Subject: [R] Tidiest way of modifying S4 classes?
In-Reply-To: <20051114205831.GV18619@hortresearch.co.nz>
References: <20051114205831.GV18619@hortresearch.co.nz>
Message-ID: <17273.44425.4221.840824@stat.math.ethz.ch>

>>>>> "PaCo" == Patrick Connolly <p.connolly at hortresearch.co.nz>
>>>>>     on Tue, 15 Nov 2005 09:58:31 +1300 writes:

    PaCo> I wish to make modifications to the plot.pedigree function in the
    PaCo> kinship package.  My attempts to contact the maintainer have been
    PaCo> unsuccessful, but my question is general, so specifics of the kinship
    PaCo> package might not be an issue.

well, but a quick look confirms that "pedigree" is an S3 class
and not a formal S4 one.  And, as the name  plot.pedigree
suggests, this is an S3 method for the 'plot' generic.

(And  ?plot.pedigree  shows a ``wrong'' usage, namely
 'plot.pedigree(.....)' instead of   'plot(..........)'
 because the authors didn't use the recommended
 \method{plot}{pedigree}(....)   syntax in their documentation
)

Unfortunately, the examples from the main help pages are not
executable either...  {and I'd vote to "forbid" that..}

After all my grumbling: 
The quick answer is: You'd have to redefine  plot.pedigree {no
new name!} in your own code and then call plot(<pedigree-object>, ...)
unless you really go for a new name such as 'Plot' {which would
need a 'Plot' generic and a 'Plot.pedigree' method}.
In your redefinition you may explicitly call  kinship:::plot.pedigree(..)
which might be useful.  
Because of the 'kinship' namespace protection,
note that  kinship-internal functions calling  plot(<pedigree>, ..)
or plot.pedigree(<pedigree>, ..)  will always call kinship:::plot.pedigree
and not your modified version.
There are ways around that as well, but we don't advertize them
much, because there's probably too much slicksand (aka "rope to
hang yourself") here...

Unfortunately, this has nothing to do with the niceties of S4
classes and methods.

Martin

    PaCo> My first attempt was to make a new function Plot.pedigree in the
    PaCo> .GlobalEnv which mostly achieved what I wanted to.  However, I'm sure
    PaCo> that's not the tidiest way to do it.  We don't have the green book,
    PaCo> but there's lots of interesting information I found here:

    PaCo> http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/S4Objects

    PaCo> However, there's something I'm missing in connecting that information
    PaCo> into knowledge of how I go about making a new method or slot or
    PaCo> whatever is sensible in this case.  What does one make of this:


    >> getClass(class(kinship:::plot.pedigree))

    PaCo> No Slots, prototype of class "function"
          ^^^^^^^^

    PaCo> Extends: "OptionalFunction", "PossibleMethod"

    PaCo> Known Subclasses: 
    PaCo> Class "MethodDefinition", from data part
    PaCo> Class "genericFunction", from data part
    PaCo> Class "functionWithTrace", from data part
    PaCo> Class "derivedDefaultMethod", by class "MethodDefinition"
    PaCo> Class "MethodWithNext", by class "MethodDefinition"
    PaCo> Class "SealedMethodDefinition", by class "MethodDefinition"
    PaCo> Class "standardGeneric", by class "genericFunction"
    PaCo> Class "nonstandardGenericFunction", by class "genericFunction"
    PaCo> Class "groupGenericFunction", by class "genericFunction"

Here, simply

> class(kinship:::plot.pedigree)
[1] "function"

and getClass("function") treats it as an S4 class:
When the 'methods' package is loaded (as always per default),
many S3 classes are in some sense also S4 classes; this is
necessary such that  proper S4 classes can have slots containing
these S3 (i.e. "pseudo") classes.

    PaCo> If I want a new plot.pedigree function, do I make a slot, or what is
    PaCo> the approach to take?

(see above).

    PaCo> Suggestions most welcome.

    PaCo> Thanks

    PaCo> -- 
    PaCo> Patrick Connolly
    PaCo> HortResearch
    PaCo> Mt Albert
    PaCo> Auckland
    PaCo> New Zealand 
    PaCo> Ph: +64-9 815 4200 x 7188



From hypan at scbit.org  Tue Nov 15 10:49:53 2005
From: hypan at scbit.org (Haiyan Pan)
Date: Tue, 15 Nov 2005 17:49:53 +0800
Subject: [R] (no subject)
Message-ID: <200511150953.jAF9roNG032339@hypatia.math.ethz.ch>

Hi,r-help£¬

	
      when I install rw2011 version ,the language is Chinese, because the language of my OS is Chinese, how can I change R language? 

 Thanks

 Haiyan

= = = = = = = = = = = = = = = = = = = = 
  
 Haiyan Pan
 hypan at scbit.org

 Tel: 021-64363311-123
 Shanghai Center for Bioinformatics Technology
 Floor 12th,100# QinZhou Road
 Shanghai,China,200235



From B.Rowlingson at lancaster.ac.uk  Tue Nov 15 10:54:45 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 15 Nov 2005 09:54:45 +0000
Subject: [R] open source and R
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5A6@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5A6@usctmx1106.merck.com>
Message-ID: <4379B065.109@lancaster.ac.uk>

Liaw, Andy wrote:
> However code readability can not be over-emphasized.  I must admit to have
> written R code in such a supposedly `clever' way that I can't figure out
> what I was trying to do (or how I did it) a week later...

  The solution to that is to make sure this sort of code is adequately 
commented! Be as clever as you like - make your R look like a runner-up 
in the obfuscated perl programming contest if you want - but a 
well-placed comment will hopefully prevent that stupid feeling a week later.

Baz



From pkhomski at wiwi.uni-bielefeld.de  Tue Nov 15 11:00:11 2005
From: pkhomski at wiwi.uni-bielefeld.de (Pavel Khomski)
Date: Tue, 15 Nov 2005 11:00:11 +0100
Subject: [R] cannot.allocate.memory.again and 32bit<--->64bit
Message-ID: <4379B1AB.7080407@wiwi.uni-bielefeld.de>

hello!
------

i use 32bit.Linux(SuSe)Server, so i'm limited with 3.5Gb of memory
i demonstrate, that there is times to times a problem with allocating of 
objects of large size, for example


0.state (no objects yet created)
------------------------------------

 > gc()
                  used (Mb)    gc trigger (Mb)    max used (Mb)
Ncells   162070  4.4          350000  9.4          350000  9.4
Vcells      59921  0.5          786432  6.0          281974  2.2



1.state:  let create now a vector of large size
--------------------------------------------------

 > my.vector<-rnorm(100000*500)
 > object.size(my.vector)/1024^2
[1] 381.4698
 > 100000*500*8/1024^2   #calculate object.size directly
[1] 381.4697
 > gc()
                      used   (Mb)      gc trigger  (Mb)      max used  (Mb)
Ncells       162257     4.4             350000   9.4             
350000   9.4
Vcells   50060239 382.0      50412232 384.7     50060419 382.0



3.state:  well, let create a matrix of the same size from this vector
--------------------------------------------------------------------------

 > my.matrix<-matrix(my.vector,nrow=100000,ncol=500)
 > gc()
                     used      (Mb)       gc trigger    (Mb)     max 
used    (Mb)
Ncells       162264       4.4           350000        9.4          
350000        9.4
Vcells  100060241  763.4    150315042 1146.9   150060261 1144.9
 > object.size(my.matrix)/1024^2    #calculate object.size directly
[1] 381.4698


so, the matrix actually - according to the used.Mb - needs the same Mb 
as the vector.
but, the trigger.Mb - and i still have problems with understanding of 
this - grows ennormously.
and i can sure, i had received the "cannot allocate  the vector of  
xxxKb"-error  last time, trying the same experiment.

if we know, that the matrix (or array generally) is acctually alloccated 
as a vector (with removed dimensions), why do we need so much trigger.Mb 
for it?

is it a problem for R only on a 32bit? what is the difference with 
recpect to trigger.memory  if i use 64bit (i didn't yet)?


thanks for your advice
--------------------------





 

From ligges at statistik.uni-dortmund.de  Tue Nov 15 11:15:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 15 Nov 2005 11:15:40 +0100
Subject: [R] setting LANGUAGE of messages (no subject)
In-Reply-To: <200511150953.jAF9roNG032339@hypatia.math.ethz.ch>
References: <200511150953.jAF9roNG032339@hypatia.math.ethz.ch>
Message-ID: <4379B54C.3020107@statistik.uni-dortmund.de>

Haiyan Pan wrote:

> Hi,r-help£¬
> 
> 	
>       when I install rw2011 version ,the language is Chinese, because the language of my OS is Chinese, how can I change R language? 


Please read the posting guide ehich suggests to
1. use a sensible subject line (now done for you),
2. read the manuals (now done for you, see below)

The R Installation and Administration manual has a section on
localization od messages which suggests to use the environment variable
LANGUAGE in order to use another than the default language of your OS
for R messages.

Uwe Ligges






>  Thanks
> 
>  Haiyan
> 
> = = = = = = = = = = = = = = = = = = = = 
>   
>  Haiyan Pan
>  hypan at scbit.org
> 
>  Tel: 021-64363311-123
>  Shanghai Center for Bioinformatics Technology
>  Floor 12th,100# QinZhou Road
>  Shanghai,China,200235
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jarioksa at sun3.oulu.fi  Tue Nov 15 11:25:17 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Tue, 15 Nov 2005 12:25:17 +0200
Subject: [R] open source and R
In-Reply-To: <4379B065.109@lancaster.ac.uk>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5A6@usctmx1106.merck.com>
	<4379B065.109@lancaster.ac.uk>
Message-ID: <1132050317.6763.72.camel@biol102145.oulu.fi>

On Tue, 2005-11-15 at 09:54 +0000, Barry Rowlingson wrote:
> Liaw, Andy wrote:
> > However code readability can not be over-emphasized.  I must admit to have
> > written R code in such a supposedly `clever' way that I can't figure out
> > what I was trying to do (or how I did it) a week later...
> 
>   The solution to that is to make sure this sort of code is adequately 
> commented! Be as clever as you like - make your R look like a runner-up 
> in the obfuscated perl programming contest if you want - but a 
> well-placed comment will hopefully prevent that stupid feeling a week later.
> 
Unfortunately the comments don't stick well with the R code. They would
if you always edit the source code, but not with my preferred toolbox.
For me the most natural way to work on a function is to install the
package with the function, and then use Emacs+ESS to edit, test and
debug the the function within an R session. That really ruins all decent
commenting: comments may be misplaced, and the default formatting of
comments is really bad in ESS. So my choice is to uncomment R code, but
comment C (and Fortran).

cheers, jari oksanen



From mostafa.ghaderi at inw.agrl.ethz.ch  Tue Nov 15 11:48:56 2005
From: mostafa.ghaderi at inw.agrl.ethz.ch (Mostafa Ghaderi)
Date: Tue, 15 Nov 2005 11:48:56 +0100
Subject: [R] latex table and R codes
Message-ID: <4379BD18.8080507@inw.agrl.ethz.ch>

Dear R-help assistance;
may you help me regrding to following inquiry!?
you know what, i have generated three tables by xtable R function, right 
now i am trying to make a single table by putting these tables 
togethere; actully i am going to come upt with *.tex (latex) file. 
because i have more extera non-R material, i am using  Sweave to read R 
instructions, and finally i hope to end up with a single table 
containing all three tables in one table, would you let me know how i 
should go for it!?( i mean combining Sweave arguments and Latex expressions)
thanks for your time;
Mostafa



From alkauffm at rz.uni-potsdam.de  Tue Nov 15 12:22:30 2005
From: alkauffm at rz.uni-potsdam.de (Albrecht Kauffmann)
Date: Tue, 15 Nov 2005 12:22:30 +0100 (CET)
Subject: [R] Temporal disaggregation using interpolation splines
Message-ID: <Pine.GSO.4.58.0511151207180.22999@persius.rz.uni-potsdam.de>

Hi,

this is a newbie question. Would it be able to convert e.g. annual
time series of flow data (or an index series) into quarterly data using
interpolation splines by means of an existing R-function? The problem is,
that the average value of the computed quarterly values must be the annual
value, i.e. the spline should cross the annual values (in a stairs-line
plot) in the middle of every annual step.

With many thanks for every hint
Albrecht Kauffmann



From ripley at stats.ox.ac.uk  Tue Nov 15 12:50:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Nov 2005 11:50:21 +0000 (GMT)
Subject: [R] cannot.allocate.memory.again and 32bit<--->64bit
In-Reply-To: <4379B1AB.7080407@wiwi.uni-bielefeld.de>
References: <4379B1AB.7080407@wiwi.uni-bielefeld.de>
Message-ID: <Pine.LNX.4.61.0511151125070.17950@gannet.stats>

You need to understand that your process has (normally) 3GB of user 
address space, not of memory.

The `gc trigger' is the value at which an automated gc is triggered, but 
one is also triggered if there is no large enough memory block left to 
allocate.  So it is unrelated to the message about not being able to 
allocate memory.  And it does need to be bigger than the maximum actual 
use: you have had three copies of your object in use at once (and now have 
two).

The problem with a small address space is that it can easily become 
fragmented, and a 64-bit system avoids that.  Generally we would not want 
more than about 1/4 of the address space used to avoid fragmentation.

BTW, setting the dim on your vector is a much more efficient way to do this

> my.vector<-rnorm(100000*500)
> dim(my.vector) <- c(100000,500)
> gc()
            used  (Mb) gc trigger  (Mb) max used  (Mb)
Ncells   169775   4.6     350000   9.4   350000   9.4
Vcells 50063257 382.0   50415499 384.7 50064028 382.0

so perhaps it is time for you to learn some of these tricks.  If you 
study matrix() you will see where the extra copy comes from.  (Hint: it is 
more-or-less needed if byrow=TRUE.)

On Tue, 15 Nov 2005, someone with a broken shift key wrote:

> hello!
> ------
>
> i use 32bit.Linux(SuSe)Server, so i'm limited with 3.5Gb of memory
> i demonstrate, that there is times to times a problem with allocating of 
> objects of large size, for example
>
>
> 0.state (no objects yet created)
> ------------------------------------
>
>> gc()
>                 used (Mb)    gc trigger (Mb)    max used (Mb)
> Ncells   162070  4.4          350000  9.4          350000  9.4
> Vcells      59921  0.5          786432  6.0          281974  2.2
>
>
>
> 1.state:  let create now a vector of large size
> --------------------------------------------------
>
>> my.vector<-rnorm(100000*500)
>> object.size(my.vector)/1024^2
> [1] 381.4698
>> 100000*500*8/1024^2   #calculate object.size directly
> [1] 381.4697
>> gc()
>                     used   (Mb)      gc trigger  (Mb)      max used  (Mb)
> Ncells       162257     4.4             350000   9.4             350000   9.4
> Vcells   50060239 382.0      50412232 384.7     50060419 382.0
>
>
>
> 3.state:  well, let create a matrix of the same size from this vector
> --------------------------------------------------------------------------
>
>> my.matrix<-matrix(my.vector,nrow=100000,ncol=500)
>> gc()
>                    used      (Mb)       gc trigger    (Mb)     max used 
> (Mb)
> Ncells       162264       4.4           350000        9.4          350000 
> 9.4
> Vcells  100060241  763.4    150315042 1146.9   150060261 1144.9
>> object.size(my.matrix)/1024^2    #calculate object.size directly
> [1] 381.4698
>
>
> so, the matrix actually - according to the used.Mb - needs the same Mb as the 
> vector.
> but, the trigger.Mb - and i still have problems with understanding of this - 
> grows ennormously.
> and i can sure, i had received the "cannot allocate  the vector of 
> xxxKb"-error  last time, trying the same experiment.
>
> if we know, that the matrix (or array generally) is acctually alloccated as a 
> vector (with removed dimensions), why do we need so much trigger.Mb for it?
>
> is it a problem for R only on a 32bit? what is the difference with recpect to 
> trigger.memory  if i use 64bit (i didn't yet)?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Tue Nov 15 12:52:11 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 15 Nov 2005 06:52:11 -0500
Subject: [R] conversion from RData to R file
In-Reply-To: <349865A6DFCF704488DCA7BB5CC83BED6DCC03@HQSWKAKI01.hq.intra.who.int>
Message-ID: <BF9F361B.12E43%sdavis2@mail.nih.gov>

On 11/15/05 4:31 AM, "Loncar, Dejan" <loncard at who.int> wrote:

> 
> Dear all
> I am beginner in R coding and have a problem to figure out how to
> convert RData format into R format.
> After I converted csv file using read.csv  I got RData file but to run
> some R code need R format

You can simply load the Rdata file into R.  Then, whatever was in that file
will be in your workspace, and if there was R source code in the Rdata file,
it will available as the objects it represents.

See ?load.  Also, you will likely need to spend some time with the Intro to
R documentation.

Sean



From subianto at gmail.com  Tue Nov 15 12:54:28 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Tue, 15 Nov 2005 12:54:28 +0100
Subject: [R] How can I put the object name in list
Message-ID: <4379CC74.2070503@gmail.com>

Dear R-helpers,
How can I put the object name in list.

 > Hair <- c("Black","Brown","Red","Blond")
 > Eye <- c("Brown","Blue","Hazel","Green")
 > Sex <- c("Male","Female")
 >
 > HEC.list <- list(Hair,Eye,Sex)
 > HEC.list
[[1]]
[1] "Black" "Brown" "Red"   "Blond"

[[2]]
[1] "Brown" "Blue"  "Hazel" "Green"

[[3]]
[1] "Male"   "Female"
 >

I expect the result like this,

$Hair
[1] "Black" "Brown" "Red"   "Blond"

$Eye
[1] "Brown" "Blue"  "Hazel" "Green"

$Sex
[1] "Male"   "Female"

Best, Muhammad Subianto



From B.Rowlingson at lancaster.ac.uk  Tue Nov 15 13:10:20 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 15 Nov 2005 12:10:20 +0000
Subject: [R] How can I put the object name in list
In-Reply-To: <4379CC74.2070503@gmail.com>
References: <4379CC74.2070503@gmail.com>
Message-ID: <4379D02C.20202@lancaster.ac.uk>

Muhammad Subianto wrote:

>  > Hair <- c("Black","Brown","Red","Blond")
>  > Eye <- c("Brown","Blue","Hazel","Green")
>  > Sex <- c("Male","Female")
>  >
>  > HEC.list <- list(Hair,Eye,Sex)
>  > HEC.list
> [[1]]
> [1] "Black" "Brown" "Red"   "Blond"
> 
> [[2]]
> [1] "Brown" "Blue"  "Hazel" "Green"
> 
> [[3]]
> [1] "Male"   "Female"

After constructing the list you can give it names this way:

  > names(HEC.list)=c("Hair","Eye","Sex")

Or do it at the time you create the list:

  > HEC.list <- list(Hair=Hair,Eye=Eye,Sex=Sex)

Barry



From jmacdon at med.umich.edu  Tue Nov 15 13:11:24 2005
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 15 Nov 2005 07:11:24 -0500
Subject: [R] How can I put the object name in list
In-Reply-To: <4379CC74.2070503@gmail.com>
References: <4379CC74.2070503@gmail.com>
Message-ID: <4379D06C.6080704@med.umich.edu>

Muhammad Subianto wrote:
> Dear R-helpers,
> How can I put the object name in list.
> 
>  > Hair <- c("Black","Brown","Red","Blond")
>  > Eye <- c("Brown","Blue","Hazel","Green")
>  > Sex <- c("Male","Female")

HEC.list <- list(Hair = c("Black","Brown","Red","Blond"), Eye = 
c("Brown","Blue","Hazel","Green"), Sex = c("Male","Female"))

Best,

Jim


>  >
>  > HEC.list <- list(Hair,Eye,Sex)
>  > HEC.list
> [[1]]
> [1] "Black" "Brown" "Red"   "Blond"
> 
> [[2]]
> [1] "Brown" "Blue"  "Hazel" "Green"
> 
> [[3]]
> [1] "Male"   "Female"
>  >
> 
> I expect the result like this,
> 
> $Hair
> [1] "Black" "Brown" "Red"   "Blond"
> 
> $Eye
> [1] "Brown" "Blue"  "Hazel" "Green"
> 
> $Sex
> [1] "Male"   "Female"
> 
> Best, Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
James W. MacDonald
University of Michigan
Affymetrix and cDNA Microarray Core
1500 E Medical Center Drive
Ann Arbor MI 48109
734-647-5623



**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.



From sundar.dorai-raj at pdf.com  Tue Nov 15 13:12:56 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 15 Nov 2005 06:12:56 -0600
Subject: [R] How can I put the object name in list
In-Reply-To: <4379CC74.2070503@gmail.com>
References: <4379CC74.2070503@gmail.com>
Message-ID: <4379D0C8.4030702@pdf.com>



Muhammad Subianto wrote:
> Dear R-helpers,
> How can I put the object name in list.
> 
>  > Hair <- c("Black","Brown","Red","Blond")
>  > Eye <- c("Brown","Blue","Hazel","Green")
>  > Sex <- c("Male","Female")
>  >
>  > HEC.list <- list(Hair,Eye,Sex)
>  > HEC.list
> [[1]]
> [1] "Black" "Brown" "Red"   "Blond"
> 
> [[2]]
> [1] "Brown" "Blue"  "Hazel" "Green"
> 
> [[3]]
> [1] "Male"   "Female"
>  >
> 
> I expect the result like this,
> 
> $Hair
> [1] "Black" "Brown" "Red"   "Blond"
> 
> $Eye
> [1] "Brown" "Blue"  "Hazel" "Green"
> 
> $Sex
> [1] "Male"   "Female"
> 
> Best, Muhammad Subianto
> 

Try:

HEC.list <- list(Hare = Hair, Eye = Eye, Sex = Sex)

This is described in the Details section of ?list.

--sundar



From subianto at gmail.com  Tue Nov 15 13:31:12 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Tue, 15 Nov 2005 13:31:12 +0100
Subject: [R] How can I put the object name in list
In-Reply-To: <4379CC74.2070503@gmail.com>
References: <4379CC74.2070503@gmail.com>
Message-ID: <4379D510.8050203@gmail.com>

Yes, thanks you very much.
Regards, Muhammad Subianto

 > HEC.list <- list(Hair=Hair,Eye=Eye,Sex=Sex)
 > ?list

On this day 15/11/2005 12:54 PM, Muhammad Subianto wrote:
> Dear R-helpers,
> How can I put the object name in list.
> 
>  > Hair <- c("Black","Brown","Red","Blond")
>  > Eye <- c("Brown","Blue","Hazel","Green")
>  > Sex <- c("Male","Female")
>  >
>  > HEC.list <- list(Hair,Eye,Sex)
>  > HEC.list
> [[1]]
> [1] "Black" "Brown" "Red"   "Blond"
> 
> [[2]]
> [1] "Brown" "Blue"  "Hazel" "Green"
> 
> [[3]]
> [1] "Male"   "Female"
>  >
> 
> I expect the result like this,
> 
> $Hair
> [1] "Black" "Brown" "Red"   "Blond"
> 
> $Eye
> [1] "Brown" "Blue"  "Hazel" "Green"
> 
> $Sex
> [1] "Male"   "Female"
> 
> Best, Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From hvermei1 at vrcbe.jnj.com  Tue Nov 15 13:56:14 2005
From: hvermei1 at vrcbe.jnj.com (Vermeiren, Hans [VRCBE])
Date: Tue, 15 Nov 2005 13:56:14 +0100
Subject: [R] Robust Non-linear Regression
Message-ID: <9AC105024CEA64458BF66D1DE13CA50D070FB3BF@tibbemeexs1.eu.jnj.com>

thank you all for the valuable suggestions
rnls() is indeed what I was looking for
I've to apologize to Roger Koenker for not mentioning that I did try
quantile regression (saw his answer in a previous post with a similar
question, yes i did my homework) however, least medians regression gave not
always satisfying results, I now understand that this is in fact due to
variability in the concentrations (x-axis) (thanks to Martin Maechlers
remark), my example dataset was in that sense a bit unfortunate
regards
Hans Vermeiren

-----Original Message-----
From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
Sent: Monday, November 14, 2005 12:41 PM
To: Vermeiren, Hans [VRCBE]
Cc: 'r-help at stat.math.ethz.ch'; R-SIG-robust at stat.math.ethz.ch
Subject: Re: [R] Robust Non-linear Regression


Package 'sfsmisc' has had a function  'rnls()' for a while 
which does robust non-linear regression via M-estimation.

Since you have only outliers in 'y' and none in 'x',
you could use the 'nlrq' (nonlinear regression quantiles)
package that Roger Koenker mentioned.



From r.hankin at noc.soton.ac.uk  Tue Nov 15 14:17:54 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 15 Nov 2005 13:17:54 +0000
Subject: [R] Lerch Phi function
Message-ID: <B2D61527-FAEC-4FF2-AF46-7D6613F29E4D@soc.soton.ac.uk>

Hi

before I reinvent the wheel, has anyone coded up the Lerch Phi function?


[
The Lerch Phi function is defined as

phi(a,s,b) = \sum_{k=1}^\infty\frac{a^k}{(b+k)^s}
]


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From maechler at stat.math.ethz.ch  Tue Nov 15 15:04:55 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 15 Nov 2005 15:04:55 +0100
Subject: [R] open source and R
In-Reply-To: <1132050317.6763.72.camel@biol102145.oulu.fi>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5A6@usctmx1106.merck.com>
	<4379B065.109@lancaster.ac.uk>
	<1132050317.6763.72.camel@biol102145.oulu.fi>
Message-ID: <17273.60167.440238.98057@stat.math.ethz.ch>

>>>>> "Jari" == Jari Oksanen <jarioksa at sun3.oulu.fi>
>>>>>     on Tue, 15 Nov 2005 12:25:17 +0200 writes:

    Jari> On Tue, 2005-11-15 at 09:54 +0000, Barry Rowlingson wrote:
    >> Liaw, Andy wrote:

    >>> However code readability can not be over-emphasized.  I
    >>> must admit to have written R code in such a supposedly
    >>> `clever' way that I can't figure out what I was trying
    >>> to do (or how I did it) a week later...

    Baz> The solution to that is to make sure this sort of code
    Baz> is adequately commented! Be as clever as you like -
    Baz> make your R look like a runner-up in the obfuscated
    Baz> perl programming contest if you want - but a
    Baz> well-placed comment will hopefully prevent that stupid
    Baz> feeling a week later.

Exactly!

    Jari> Unfortunately the comments don't stick well with the R
    Jari> code. They would if you always edit the source code,
    Jari> but not with my preferred toolbox.  For me the most
    Jari> natural way to work on a function is to install the
    Jari> package with the function, and then use Emacs+ESS to
    Jari> edit, test and debug the the function within an R
    Jari> session. 

But why?  Why on earth are you not working with the *.R files in your
    <pkg>/R/ directory?
Or make a copy of these and work with the copy?

Also, you can use  library(<....>,  keep.source = TRUE)
and this helps for all those packages that did *not* use
lazy-loading or saved images at their installation time;
unfortunately, that excludes many packages.

Once you've seen the light, i.e. ESS :-) ,
using edit() , fix() and all those abominations is just plainly
wrong in my (biased) view!

    Jari> session. That really ruins all decent commenting:
    Jari> comments may be misplaced, and the default formatting
    Jari> of comments is really bad in ESS. 

Huh??  I'd have expected you to say the contrary here.
I assume you have never heard of the difference between "#", "##"
and "###" in comments.  Most of the R core developers adhere to
it which you can see when browsing the R source files.

Since one version of the ESS manual is online, look here:
http://ESS.R-project.org/Manual/ess.html#index-comments-in-S-154

    Jari> So my choice is to uncomment R code, but comment C
    Jari> (and Fortran).

ooh;  I hope you can be convinced to use comments in R code...
and maybe adapt to the "##" vs "#" (or "###") scheme.

Regards,
Martin Maechler, ETH Zurich



From dmbates at gmail.com  Tue Nov 15 15:09:54 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 15 Nov 2005 08:09:54 -0600
Subject: [R] R News, volume 5, issue 2 is now available
Message-ID: <40e66e0b0511150609p2593627ble4377f0c29f6ddcc@mail.gmail.com>

The November 2005 issue of R News is now available on CRAN under the
Documentation/Newsletter link.  Our thanks to all the contributoRs.

For the editorial board,
Douglas Bates



From suling at bips.uni-bremen.de  Tue Nov 15 15:39:58 2005
From: suling at bips.uni-bremen.de (suling@bips.uni-bremen.de)
Date: Tue, 15 Nov 2005 15:39:58 +0100
Subject: [R] Darstellung mit Nachkommastellen
Message-ID: <437A014F.7066.13714B2@localhost>

Hi!
I got a rather stupid question (I think):

Is there ANY option that makes R display numericals not like
   
   "1e-8"

but as

   "0.00000001"	

by default ?

And I need the outcome to be really numerical, so formatC(...) which produces a 
character or something like this won't be acceptable.

Any help on this would be appreciated, thanx.

Marc



From fred-l at poleto.com  Tue Nov 15 15:41:04 2005
From: fred-l at poleto.com (Frederico Zanqueta Poleto)
Date: Tue, 15 Nov 2005 12:41:04 -0200
Subject: [R] Little's Chi Square test for MCAR?
In-Reply-To: <00b201c5e93d$41977910$0540210a@www.domain>
References: <mailman.11.1131966002.7606.r-help@stat.math.ethz.ch>	<4378BE06.8040609@vsnl.com>
	<00b201c5e93d$41977910$0540210a@www.domain>
Message-ID: <4379F380.9050707@poleto.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051115/ff004d81/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Nov 15 15:47:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 15 Nov 2005 15:47:49 +0100
Subject: [R] Darstellung mit Nachkommastellen
In-Reply-To: <437A014F.7066.13714B2@localhost>
References: <437A014F.7066.13714B2@localhost>
Message-ID: <4379F515.6040809@statistik.uni-dortmund.de>

suling at bips.uni-bremen.de wrote:

> Hi!
> I got a rather stupid question (I think):
> 
> Is there ANY option that makes R display numericals not like
>    
>    "1e-8"
> 
> but as
> 
>    "0.00000001"	
> 
> by default ?

What about

  options(scipen=100)
  1e-8


Uwe Ligges


> And I need the outcome to be really numerical, so formatC(...) which produces a 
> character or something like this won't be acceptable.
> 
> Any help on this would be appreciated, thanx.
> 
> Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From suling at bips.uni-bremen.de  Tue Nov 15 15:52:55 2005
From: suling at bips.uni-bremen.de (suling@bips.uni-bremen.de)
Date: Tue, 15 Nov 2005 15:52:55 +0100
Subject: [R] Darstellung mit Nachkommastellen
In-Reply-To: <4379F515.6040809@statistik.uni-dortmund.de>
References: <437A014F.7066.13714B2@localhost>
Message-ID: <437A0458.8033.142EE79@localhost>

Yep. That does the trick.
Like I said, it was a rather stupid question... if you know the answer :-)
Thank you, Uwe!


Am 15 Nov 2005 um 15:47 hat Uwe Ligges geschrieben:

> suling at bips.uni-bremen.de wrote:
> 
> > Hi!
> > I got a rather stupid question (I think):
> > 
> > Is there ANY option that makes R display numericals not like
> >    
> >    "1e-8"
> > 
> > but as
> > 
> >    "0.00000001"	
> > 
> > by default ?
> 
> What about
> 
>   options(scipen=100)
>   1e-8
> 
> 
> Uwe Ligges
> 
> 
> > And I need the outcome to be really numerical, so formatC(...) which produces a 
> > character or something like this won't be acceptable.
> > 
> > Any help on this would be appreciated, thanx.
> > 
> > Marc
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From szlevine at nana.co.il  Tue Nov 15 15:59:40 2005
From: szlevine at nana.co.il (Stephen)
Date: Tue, 15 Nov 2005 16:59:40 +0200
Subject: [R] Item response theory
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD65D@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051115/3d2dfd6c/attachment.pl

From ccleland at optonline.net  Tue Nov 15 16:02:37 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 15 Nov 2005 10:02:37 -0500
Subject: [R] Darstellung mit Nachkommastellen
In-Reply-To: <437A014F.7066.13714B2@localhost>
References: <437A014F.7066.13714B2@localhost>
Message-ID: <4379F88D.9060007@optonline.net>

Look at the scipen argument to options().  For example:

 > 0.00000001
[1] 1e-08

 > options(scipen=10)

 > 0.00000001
[1] 0.00000001

suling at bips.uni-bremen.de wrote:
> Hi!
> I got a rather stupid question (I think):
> 
> Is there ANY option that makes R display numericals not like
>    
>    "1e-8"
> 
> but as
> 
>    "0.00000001"	
> 
> by default ?
> 
> And I need the outcome to be really numerical, so formatC(...) which produces a 
> character or something like this won't be acceptable.
> 
> Any help on this would be appreciated, thanx.
> 
> Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From gchappi at gmail.com  Tue Nov 15 16:10:35 2005
From: gchappi at gmail.com (Hans-Peter)
Date: Tue, 15 Nov 2005 16:10:35 +0100
Subject: [R] www.krankenversicherung.ch News - Krankenkassen -
	Information Newsletter
In-Reply-To: <200511141045109.SM00940@195.141.204.149>
References: <200511141045109.SM00940@195.141.204.149>
Message-ID: <47fce0650511150710y408e996dx@mail.gmail.com>

Am 14.11.05 schrieb krankenversicherung at help.ch <krankenversicherung at help.ch>:
> [snip some spam]

Being located only some blocks away from these spammers, I just asked
them in person to remove the R list from their database.

Regards,
Hans-Peter



From MikeJones at westat.com  Tue Nov 15 16:18:05 2005
From: MikeJones at westat.com (Mike Jones)
Date: Tue, 15 Nov 2005 10:18:05 -0500
Subject: [R] Reading in a table with unequal columns
Message-ID: <403593359CA56C4CAE1F8F4F00DCFE7D02722DAB@MAILBE2.westat.com>

Hi, 

Wasn't sure how to explain this problem succinctly in a title.  I am
trying to read in a text file that looks like:

0   1000  175  1  2  3
1   1000  58   0  2  9
2   1000  35   0  1  3 10
3   1000  300  0  2  4  5  10  11  18
4   1000  150  3  5  6
5   1000  100 3  4  6  7  18
6   1000   50  4  5  7  8
7   1000  155  5  6  8  19
8   1000  255  6  7 19
9   1000  200  1 10 12
10  1000  52   2  3  9  11  12  13
11  1000  70  3  10 14 15  16  17  18  19
12  1000  250 9  10 13
13  1000  40 10 12 14
14  1000  235 11 13 15
15  1000  127 11 14 16 17
16  1000  177 11 15 17
17  1000  358 11 15 16
18  1000  296 3  5  11  19
19  1000  120 7  8  11  18

The problem with this is that the 12th row (row with 11 in the first
column) doesn't get read in correctly.  To read into R, I'm using a
command like:

matrix(unlist(read.table(datafile, sep="",fill=T)),
             ncol=max(count.fields(datafile, sep="")),byrow=F)

but that gives

      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
 [1,]    0   19 1000  358   11   14   15   NA   NA    NA    18
 [2,]    1 1000 1000  296   11   15   16   NA   NA    NA    NA
 [3,]    2 1000  175  120    3   15   17   17   NA    NA    NA
 [4,]    3 1000   58    1    7    5   16   NA   NA    NA    NA
 [5,]    4 1000   35    0    2    8   11   NA   NA    NA    NA
 [6,]    5 1000  300    0    2    3   11   19   NA    NA    NA
 [7,]    6 1000  150    0    1    9   NA   18   NA    NA    NA
 [8,]    7 1000  100    3    2    3   NA   NA   NA    NA    NA
 [9,]    8 1000   50    3    5    4   10   NA   NA    NA    NA
[10,]    9 1000  155    4    4    6    5   NA   NA    NA    NA
[11,]   10 1000  255    5    5    6   NA   10   NA    NA     0
[12,]   11 1000  200    6    6    7    7   NA   11    NA     1
[13,]   19 1000   52    1    7    8    8   18   NA    18     2
[14,]   12   NA   70    2   10   19   19   NA   NA    NA     3
[15,]   13 1000   NA    3    3   12   NA   NA   NA    NA     4
[16,]   14 1000  250   NA   10    9   NA   NA   NA    NA     5
[17,]   15 1000   40    9   NA   14   11   NA   NA    NA     6
[18,]   16 1000  235   10   10   NA   15   12   NA    NA     7
[19,]   17 1000  127   11   12   13   NA   16   13    NA     8
[20,]   18 1000  177   11   13   14   NA   NA   17    NA     9

I've tried other things, but this is as close as I've been able to get
and I'm at a loss at this point.  Any input would be
helpful...thanks...mj



From sundar.dorai-raj at pdf.com  Tue Nov 15 16:44:37 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 15 Nov 2005 09:44:37 -0600
Subject: [R] Reading in a table with unequal columns
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D02722DAB@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D02722DAB@MAILBE2.westat.com>
Message-ID: <437A0265.7040003@pdf.com>



Mike Jones wrote:
> Hi, 
> 
> Wasn't sure how to explain this problem succinctly in a title.  I am
> trying to read in a text file that looks like:
> 
> 0   1000  175  1  2  3
> 1   1000  58   0  2  9
> 2   1000  35   0  1  3 10
> 3   1000  300  0  2  4  5  10  11  18
> 4   1000  150  3  5  6
> 5   1000  100 3  4  6  7  18
> 6   1000   50  4  5  7  8
> 7   1000  155  5  6  8  19
> 8   1000  255  6  7 19
> 9   1000  200  1 10 12
> 10  1000  52   2  3  9  11  12  13
> 11  1000  70  3  10 14 15  16  17  18  19
> 12  1000  250 9  10 13
> 13  1000  40 10 12 14
> 14  1000  235 11 13 15
> 15  1000  127 11 14 16 17
> 16  1000  177 11 15 17
> 17  1000  358 11 15 16
> 18  1000  296 3  5  11  19
> 19  1000  120 7  8  11  18
> 
> The problem with this is that the 12th row (row with 11 in the first
> column) doesn't get read in correctly.  To read into R, I'm using a
> command like:
> 
> matrix(unlist(read.table(datafile, sep="",fill=T)),
>              ncol=max(count.fields(datafile, sep="")),byrow=F)
> 
> but that gives
> 
>       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
>  [1,]    0   19 1000  358   11   14   15   NA   NA    NA    18
>  [2,]    1 1000 1000  296   11   15   16   NA   NA    NA    NA
>  [3,]    2 1000  175  120    3   15   17   17   NA    NA    NA
>  [4,]    3 1000   58    1    7    5   16   NA   NA    NA    NA
>  [5,]    4 1000   35    0    2    8   11   NA   NA    NA    NA
>  [6,]    5 1000  300    0    2    3   11   19   NA    NA    NA
>  [7,]    6 1000  150    0    1    9   NA   18   NA    NA    NA
>  [8,]    7 1000  100    3    2    3   NA   NA   NA    NA    NA
>  [9,]    8 1000   50    3    5    4   10   NA   NA    NA    NA
> [10,]    9 1000  155    4    4    6    5   NA   NA    NA    NA
> [11,]   10 1000  255    5    5    6   NA   10   NA    NA     0
> [12,]   11 1000  200    6    6    7    7   NA   11    NA     1
> [13,]   19 1000   52    1    7    8    8   18   NA    18     2
> [14,]   12   NA   70    2   10   19   19   NA   NA    NA     3
> [15,]   13 1000   NA    3    3   12   NA   NA   NA    NA     4
> [16,]   14 1000  250   NA   10    9   NA   NA   NA    NA     5
> [17,]   15 1000   40    9   NA   14   11   NA   NA    NA     6
> [18,]   16 1000  235   10   10   NA   15   12   NA    NA     7
> [19,]   17 1000  127   11   12   13   NA   16   13    NA     8
> [20,]   18 1000  177   11   13   14   NA   NA   17    NA     9
> 
> I've tried other things, but this is as close as I've been able to get
> and I'm at a loss at this point.  Any input would be
> helpful...thanks...mj
> 


There are two ways that I know of to get around this. I'm sure there are 
others:

## read in the file to determine the max number of columns
x <- scan("file.txt", what = "", sep = "\n")
x <- strsplit(x, "[ \t]+") # split string by white space
max.col <- max(sapply(x, length))

## option 1
## specify col.names as ?read.table suggests
cn <- paste("V", 1:max.col, sep = "")
z1 <- read.table("file.txt", fill = TRUE, col.names = cn)

## option 2
## parse `x' yourself and construct a matrix
z2 <- t(sapply(x, function(i) {
   n <- length(i)
   y <- rep(NA, max.col)
   y[1:n] <- as.numeric(i)
   y
}))



From ggrothendieck at gmail.com  Tue Nov 15 16:54:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 15 Nov 2005 10:54:01 -0500
Subject: [R] Reading in a table with unequal columns
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D02722DAB@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D02722DAB@MAILBE2.westat.com>
Message-ID: <971536df0511150754n562d52bdldbed1a506724f2c@mail.gmail.com>

On 11/15/05, Mike Jones <MikeJones at westat.com> wrote:
> Hi,
>
> Wasn't sure how to explain this problem succinctly in a title.  I am
> trying to read in a text file that looks like:
>
> 0   1000  175  1  2  3
> 1   1000  58   0  2  9
> 2   1000  35   0  1  3 10
> 3   1000  300  0  2  4  5  10  11  18
> 4   1000  150  3  5  6
> 5   1000  100 3  4  6  7  18
> 6   1000   50  4  5  7  8
> 7   1000  155  5  6  8  19
> 8   1000  255  6  7 19
> 9   1000  200  1 10 12
> 10  1000  52   2  3  9  11  12  13
> 11  1000  70  3  10 14 15  16  17  18  19
> 12  1000  250 9  10 13
> 13  1000  40 10 12 14
> 14  1000  235 11 13 15
> 15  1000  127 11 14 16 17
> 16  1000  177 11 15 17
> 17  1000  358 11 15 16
> 18  1000  296 3  5  11  19
> 19  1000  120 7  8  11  18
>
> The problem with this is that the 12th row (row with 11 in the first
> column) doesn't get read in correctly.  To read into R, I'm using a
> command like:
>
> matrix(unlist(read.table(datafile, sep="",fill=T)),
>             ncol=max(count.fields(datafile, sep="")),byrow=F)
>
> but that gives
>
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
>  [1,]    0   19 1000  358   11   14   15   NA   NA    NA    18
>  [2,]    1 1000 1000  296   11   15   16   NA   NA    NA    NA
>  [3,]    2 1000  175  120    3   15   17   17   NA    NA    NA
>  [4,]    3 1000   58    1    7    5   16   NA   NA    NA    NA
>  [5,]    4 1000   35    0    2    8   11   NA   NA    NA    NA
>  [6,]    5 1000  300    0    2    3   11   19   NA    NA    NA
>  [7,]    6 1000  150    0    1    9   NA   18   NA    NA    NA
>  [8,]    7 1000  100    3    2    3   NA   NA   NA    NA    NA
>  [9,]    8 1000   50    3    5    4   10   NA   NA    NA    NA
> [10,]    9 1000  155    4    4    6    5   NA   NA    NA    NA
> [11,]   10 1000  255    5    5    6   NA   10   NA    NA     0
> [12,]   11 1000  200    6    6    7    7   NA   11    NA     1
> [13,]   19 1000   52    1    7    8    8   18   NA    18     2
> [14,]   12   NA   70    2   10   19   19   NA   NA    NA     3
> [15,]   13 1000   NA    3    3   12   NA   NA   NA    NA     4
> [16,]   14 1000  250   NA   10    9   NA   NA   NA    NA     5
> [17,]   15 1000   40    9   NA   14   11   NA   NA    NA     6
> [18,]   16 1000  235   10   10   NA   15   12   NA    NA     7
> [19,]   17 1000  127   11   12   13   NA   16   13    NA     8
> [20,]   18 1000  177   11   13   14   NA   NA   17    NA     9
>

Try this:

nf <- max(count.fields(datafile))
read.table(datafile, fill = TRUE, col.names = 1:nf)



From gavin.simpson at ucl.ac.uk  Tue Nov 15 17:43:08 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 15 Nov 2005 16:43:08 +0000
Subject: [R] plots in a matrix
Message-ID: <1132072988.5163.11.camel@gsimpson.geog.ucl.ac.uk>

Hi,

consider the following example:

I have a matrix like this:

spp.mds$points
                 [,1]         [,2]
CLAP0      1.79703164 -11.66716182
CLAP30     3.87034797  -7.48168377
YBI0000   10.27538316  -3.32226184
YBI0100    0.58463806  -1.25748701
hir10000   5.82907036  -4.09695960
hir10016  -2.22113472  -4.86563557
gyn30000   5.14643671  -3.58706541
gyn30032   2.99552434  -6.21815132
...

Each pair of rows represents a single sample (now and past say). I want
to draw the rowname for the past sample (row 2 of a pair) and an arrow
from the past sample pointing to the now sample (row 1 of a pair). These
are the coords of each point on axis 1 and 2 of an nmds.

Because there are so many pairs (52) the plot is crowded so I subset the
points, choosing which I want to plot on each sub-plot by hand to make
the plot less crowded:

rows <- list(c(1,3,7,9,11,13,15,17,19,21,25,35,37,43,97,99,93),
             c(5,27,29,31,33,45,47,49,51,53,39,71),
             c(55,57,59,61,63,65,67,69,73,75,77,79,41),
             c(23,81,83,85,87,89,91,95,101,103))

listing the row 1 of each pair, so for I can do:

tops <- rows[[1]]
bottoms <- rows[[1]] + 1
plot(spp.mds, type = "n", display = "site", ann = FALSE, axes = FALSE)
axis(side = 2, at = c(10, 5, 0, -5, -10))
box()
text(spp.mds$points[bottoms,],
     labels = rep(sacs.code, each = 2)[bottoms],
     cex = 0.5)
arrows(spp.mds$points[bottoms, 1], spp.mds$points[bottoms, 2],
       spp.mds$points[tops, 1], spp.mds$points[tops, 2],
       col = "blue", length = 0.03)

for plot 1, and so on for the other subsets.

I was plotting using par(mfrow = c(2,2)), and so I have 4 plots
separated by lots of space. I want to get rid of all that space between
plots, turning off axis labels and axes for the internal axes. I know
how to do all of this except how to remove all the internal space whilst
keeping each plot the same size and shape. Simply setting two of the mar
(gin)s to be 0 for the internal margins for each plot is almost what I
want, but the plots come out slightly different sizes.

I'm sure there is something simple I'm missing, but I've tried various
combinations of split.screen(), layout(), changing oma, mar but I still
can't quite get this right.

Any suggestions?

Thanks in advance,

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From tom at maladmin.com  Tue Nov 15 12:51:58 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 15 Nov 2005 06:51:58 -0500
Subject: [R] strsplit
Message-ID: <1132055518.4819.37.camel@localhost.localdomain>

I'm stuck on what I feel should be a minor problem.
I have a dataseries obtained from a MS Access database that consists of
a series of numbers seperated by carridge returns (\r)
Currently this data is in R as mode numeric???

I want to separate this into a vector of the componant numbers.
Perhaps a little code will help describe what I've got here!!

library(rodbc)
s_sql<-'SELECT Data from table where id=1' #only one record returned
oWave<-sqlQuery(oConn,s_sql)

wave.data<-oWave$data

> mode(data)
[1] "numeric"

Any clues will be much appreciated
thanks
Tom



From 042045003 at fudan.edu.cn  Tue Nov 15 17:56:06 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Wed, 16 Nov 2005 00:56:06 +0800
Subject: [R] Reading in a table with unequal columns
Message-ID: <0IQ000JVZ98HGV@mail.fudan.edu.cn>


======= 2005-11-15 23:18:05 ÄúÔÚÀ´ÐÅÖÐÐ´µÀ£º=======

>Hi, 
>
>Wasn't sure how to explain this problem succinctly in a title.  I am
>trying to read in a text file that looks like:
>
>0   1000  175  1  2  3
>1   1000  58   0  2  9
>2   1000  35   0  1  3 10
>3   1000  300  0  2  4  5  10  11  18
>4   1000  150  3  5  6
>5   1000  100 3  4  6  7  18
>6   1000   50  4  5  7  8
>7   1000  155  5  6  8  19
>8   1000  255  6  7 19
>9   1000  200  1 10 12
>10  1000  52   2  3  9  11  12  13
>11  1000  70  3  10 14 15  16  17  18  19
>12  1000  250 9  10 13
>13  1000  40 10 12 14
>14  1000  235 11 13 15
>15  1000  127 11 14 16 17
>16  1000  177 11 15 17
>17  1000  358 11 15 16
>18  1000  296 3  5  11  19
>19  1000  120 7  8  11  18
>
>The problem with this is that the 12th row (row with 11 in the first
>column) doesn't get read in correctly.  To read into R, I'm using a
>command like:
>
>matrix(unlist(read.table(datafile, sep="",fill=T)),
>             ncol=max(count.fields(datafile, sep="")),byrow=F)
?read.table will find
   The number of data columns is determined by looking at the first
     five lines of input (or the whole file if it has less than five
     lines), or from the length of 'col.names' if it is specified and
     is longer.  This could conceivably be wrong if 'fill' or
     'blank.lines.skip' are true, so specify 'col.names' if necessary.
So try:
nc<-max(count.fields(datafile, sep="")
x<-read.table(datafile,sep="",col.names=paste("v",1:nc,sep="."),fill=T)
matrix(unlist(x),ncol=nc)

>but that gives
>
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
> [1,]    0   19 1000  358   11   14   15   NA   NA    NA    18
> [2,]    1 1000 1000  296   11   15   16   NA   NA    NA    NA
> [3,]    2 1000  175  120    3   15   17   17   NA    NA    NA
> [4,]    3 1000   58    1    7    5   16   NA   NA    NA    NA
> [5,]    4 1000   35    0    2    8   11   NA   NA    NA    NA
> [6,]    5 1000  300    0    2    3   11   19   NA    NA    NA
> [7,]    6 1000  150    0    1    9   NA   18   NA    NA    NA
> [8,]    7 1000  100    3    2    3   NA   NA   NA    NA    NA
> [9,]    8 1000   50    3    5    4   10   NA   NA    NA    NA
>[10,]    9 1000  155    4    4    6    5   NA   NA    NA    NA
>[11,]   10 1000  255    5    5    6   NA   10   NA    NA     0
>[12,]   11 1000  200    6    6    7    7   NA   11    NA     1
>[13,]   19 1000   52    1    7    8    8   18   NA    18     2
>[14,]   12   NA   70    2   10   19   19   NA   NA    NA     3
>[15,]   13 1000   NA    3    3   12   NA   NA   NA    NA     4
>[16,]   14 1000  250   NA   10    9   NA   NA   NA    NA     5
>[17,]   15 1000   40    9   NA   14   11   NA   NA    NA     6
>[18,]   16 1000  235   10   10   NA   15   12   NA    NA     7
>[19,]   17 1000  127   11   12   13   NA   16   13    NA     8
>[20,]   18 1000  177   11   13   14   NA   NA   17    NA     9
>
>I've tried other things, but this is as close as I've been able to get
>and I'm at a loss at this point.  Any input would be
>helpful...thanks...mj
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-11-16

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From p.dalgaard at biostat.ku.dk  Tue Nov 15 18:03:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2005 18:03:39 +0100
Subject: [R] strsplit
In-Reply-To: <1132055518.4819.37.camel@localhost.localdomain>
References: <1132055518.4819.37.camel@localhost.localdomain>
Message-ID: <x2k6f9vo5w.fsf@turmalin.kubism.ku.dk>

tom wright <tom at maladmin.com> writes:

> I'm stuck on what I feel should be a minor problem.
> I have a dataseries obtained from a MS Access database that consists of
> a series of numbers seperated by carridge returns (\r)
> Currently this data is in R as mode numeric???
> 
> I want to separate this into a vector of the componant numbers.
> Perhaps a little code will help describe what I've got here!!
> 
> library(rodbc)
> s_sql<-'SELECT Data from table where id=1' #only one record returned
> oWave<-sqlQuery(oConn,s_sql)
> 
> wave.data<-oWave$data
> 
> > mode(data)
> [1] "numeric"
> 
> Any clues will be much appreciated

Er, did you mean to say mode(wave.data)? 

Anyways, you say that you want to obtain a vector of numbers, and you
got an object of mode numeric, which means that it is a vector of
numbers. So what was the problem in the first place?

If you had data with carriage returns inside, then you should have
them as mode "character".

So there seems to be something that you're not telling us...

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tom at maladmin.com  Tue Nov 15 13:21:50 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 15 Nov 2005 07:21:50 -0500
Subject: [R] strsplit
In-Reply-To: <x2k6f9vo5w.fsf@turmalin.kubism.ku.dk>
References: <1132055518.4819.37.camel@localhost.localdomain>
	<x2k6f9vo5w.fsf@turmalin.kubism.ku.dk>
Message-ID: <1132057310.4819.46.camel@localhost.localdomain>

Peter, Your correct I did mean to say
mode(wave.data)
but mode is still numeric???

I've made some changes so I can show you a trucated value for the data,
please see the following code...
getWave<-function(connection,id=NULL){
    if(is.null(id)){
        stop('Must supply record#')
    }
    s_sql1<-'SELECT
Samplerate,Sampleswave,Prestimbaseline,Numberaveraged,LEFT(Data,100) as
wavedata'
    s_sql2<-'FROM [Patient Information]'
    s_sql3<-paste('WHERE [Record#]=',id,sep='')
    
    s_sql<-paste(s_sql1,s_sql2,s_sql3)

    wave<-sqlQuery(connection,s_sql)
    return(wave)
}
> owave<-getWave(oConn,89) 
> owave$wavedata
[1] 161.1328\r158.6914\r162.3535\r159.9121\r158.6914\r159.9121\r153.8086
\r151.3672\r146.4844\r142.8223\r140.3809\r1
Levels: 161.1328\r158.6914\r162.3535\r159.9121\r158.6914\r159.9121
\r153.8086\r151.3672\r146.4844\r142.8223\r140.3809\r1

> mode(owave$wavedata)
[1] "numeric"




On Tue, 2005-15-11 at 18:03 +0100, Peter Dalgaard wrote:
> tom wright <tom at maladmin.com> writes:
> 
> > I'm stuck on what I feel should be a minor problem.
> > I have a dataseries obtained from a MS Access database that consists of
> > a series of numbers seperated by carridge returns (\r)
> > Currently this data is in R as mode numeric???
> > 
> > I want to separate this into a vector of the componant numbers.
> > Perhaps a little code will help describe what I've got here!!
> > 
> > library(rodbc)
> > s_sql<-'SELECT Data from table where id=1' #only one record returned
> > oWave<-sqlQuery(oConn,s_sql)
> > 
> > wave.data<-oWave$data
> > 
> > > mode(data)
> > [1] "numeric"
> > 
> > Any clues will be much appreciated
> 
> Er, did you mean to say mode(wave.data)? 
> 
> Anyways, you say that you want to obtain a vector of numbers, and you
> got an object of mode numeric, which means that it is a vector of
> numbers. So what was the problem in the first place?
> 
> If you had data with carriage returns inside, then you should have
> them as mode "character".
> 
> So there seems to be something that you're not telling us...
>



From msw10 at duke.edu  Tue Nov 15 18:22:15 2005
From: msw10 at duke.edu (Michael Wolosin)
Date: Tue, 15 Nov 2005 12:22:15 -0500
Subject: [R] changing the value of a variable from inside a function
Message-ID: <5.2.1.1.1.20051115115224.0198f800@imap.duke.edu>

All -

I am trying to write R code to implement a recursive algorithm.  I've 
solved the problem in a klunky way that works, but uses  more memory and 
computing time than it should.

A more elegant solution than my current one would require updating the 
values of a variable that is located in what I will call the "root" 
environment - that environment from which the original call to the 
recursive function was issued.  Certainly, I could pass the variable into 
the function, update it inside, and return it.  However, the variable I am 
updating is a large matrix, and the recursion could end up several hundred 
levels deep.  Passing the matrix around would create a copy in the 
environment for each call, wasting memory, time, and space.

I've read the help on the "sys.{}" family of functions, and "eval", and 
although I can't claim to have absorbed it all, it seems like it is much 
easier to access the value of a variable in a parent frame than it is to 
update that value with assignment.
If you make an assignment inside a function, even if it is to a section of 
a variable that exists in a parent frame, the variable is only created or 
updated in the current environment - never in the parent frame.

For example:

test <- matrix(NA,nrow=4,ncol=3)
test[1,] <- c(1,2,3)
blah <- function(i){
   test[i,] <- c(0,1,2) + i
   return(test)
}
test
blah(2)
test

So the real question is, how do I write the function like "blah" above that 
updates "test" in the parent or root frame?

blah <- function(i){
   test[i,] <- c(1,2,3) + i  #modify this line somehow
   return(NULL)
}
If done "correctly", we will get:
 > blah(2)
 > test
       [,1] [,2] [,3]
  [1,]    1    2    3
  [2,]    2    3    4
  [3,]   NA   NA   NA
  [4,]   NA   NA   NA

And given an example that works from within a single function call, does it 
have to be modified to work recursively?

blah <- function(i){
   if (i<4) {blah(i + 1)}
   test[i,] <- c(0,1,2) + i  #modify this line somehow
   return(NULL)
}
If written "correctly", the following would be the output:
 > blah(2)
 > test
       [,1] [,2] [,3]
  [1,]    1    2    3
  [2,]    2    3    4
  [3,]    3    4    5
  [4,]    4    5    6

One idea would be to write out to a file.  The filename could reside in the 
root environment, and that is all that is needed.  But  this also seems 
inelegant (and slow).  If I can read and write to a file, I should be able 
to read and write to a memory location.

I suspect that the solution lies somewhere in the "sys" functions, but I 
was having trouble seeing it.  Any help would be appreciated.

Thank you in advance,

Mike



From tom at maladmin.com  Tue Nov 15 13:35:24 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 15 Nov 2005 07:35:24 -0500
Subject: [R] changing the value of a variable from inside a function
In-Reply-To: <5.2.1.1.1.20051115115224.0198f800@imap.duke.edu>
References: <5.2.1.1.1.20051115115224.0198f800@imap.duke.edu>
Message-ID: <1132058124.4819.51.camel@localhost.localdomain>

Michael,
I feel sure people are going to come up with other better suggestions
here but does <- work for you?
test <- matrix(NA,nrow=4,ncol=3)
test[1,] <- c(1,2,3)
blah <- function(i){
   test[i,] <<- c(0,1,2) + i
   return(test)
}


On Tue, 2005-15-11 at 12:22 -0500, Michael Wolosin wrote:
> test <- matrix(NA,nrow=4,ncol=3)
> test[1,] <- c(1,2,3)
> blah <- function(i){
>    test[i,] <- c(0,1,2) + i
>    return(test)
> }
> test
> blah(2)
> test



From ripley at stats.ox.ac.uk  Tue Nov 15 18:29:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Nov 2005 17:29:43 +0000 (GMT)
Subject: [R] strsplit
In-Reply-To: <x2k6f9vo5w.fsf@turmalin.kubism.ku.dk>
References: <1132055518.4819.37.camel@localhost.localdomain>
	<x2k6f9vo5w.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0511151723270.10875@gannet.stats>

On Tue, 15 Nov 2005, Peter Dalgaard wrote:

> tom wright <tom at maladmin.com> writes:
>
>> I'm stuck on what I feel should be a minor problem.
>> I have a dataseries obtained from a MS Access database that consists of
>> a series of numbers seperated by carridge returns (\r)
>> Currently this data is in R as mode numeric???
>>
>> I want to separate this into a vector of the componant numbers.
>> Perhaps a little code will help describe what I've got here!!
>>
>> library(rodbc)
>> s_sql<-'SELECT Data from table where id=1' #only one record returned
>> oWave<-sqlQuery(oConn,s_sql)
>>
>> wave.data<-oWave$data
>>
>>> mode(data)
>> [1] "numeric"
>>
>> Any clues will be much appreciated
>
> Er, did you mean to say mode(wave.data)?
>
> Anyways, you say that you want to obtain a vector of numbers, and you
> got an object of mode numeric, which means that it is a vector of
> numbers. So what was the problem in the first place?
>
> If you had data with carriage returns inside, then you should have
> them as mode "character".
>
> So there seems to be something that you're not telling us...

or Access is not telling R.  One possibility is that Access is reporting 
this field as numeric:  use sqlColumns() to find out.  If so, you need to 
sort this out in Access.  If not, you can use the as.is argument: see 
?sqlGetResults referenced from ?sqlQuery (although I would be surprised if 
type.convert was getting confused).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov 15 18:32:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Nov 2005 17:32:34 +0000 (GMT)
Subject: [R] strsplit
In-Reply-To: <1132057310.4819.46.camel@localhost.localdomain>
References: <1132055518.4819.37.camel@localhost.localdomain>
	<x2k6f9vo5w.fsf@turmalin.kubism.ku.dk>
	<1132057310.4819.46.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0511151730310.10875@gannet.stats>

On Tue, 15 Nov 2005, tom wright wrote:

> Peter, Your correct I did mean to say
> mode(wave.data)
> but mode is still numeric???

That is because it is a factor, and they have mode = "numeric".

Don't use mode() for this: use str() or typeof(). mode() is a rather 
confusing piece of code, written I think for S-compatibility.

Use as.is, as I suggested a few minutes ago.

> I've made some changes so I can show you a trucated value for the data,
> please see the following code...
> getWave<-function(connection,id=NULL){
>    if(is.null(id)){
>        stop('Must supply record#')
>    }
>    s_sql1<-'SELECT
> Samplerate,Sampleswave,Prestimbaseline,Numberaveraged,LEFT(Data,100) as
> wavedata'
>    s_sql2<-'FROM [Patient Information]'
>    s_sql3<-paste('WHERE [Record#]=',id,sep='')
>
>    s_sql<-paste(s_sql1,s_sql2,s_sql3)
>
>    wave<-sqlQuery(connection,s_sql)
>    return(wave)
> }
>> owave<-getWave(oConn,89)
>> owave$wavedata
> [1] 161.1328\r158.6914\r162.3535\r159.9121\r158.6914\r159.9121\r153.8086
> \r151.3672\r146.4844\r142.8223\r140.3809\r1
> Levels: 161.1328\r158.6914\r162.3535\r159.9121\r158.6914\r159.9121
> \r153.8086\r151.3672\r146.4844\r142.8223\r140.3809\r1
>
>> mode(owave$wavedata)
> [1] "numeric"
>
>
>
>
> On Tue, 2005-15-11 at 18:03 +0100, Peter Dalgaard wrote:
>> tom wright <tom at maladmin.com> writes:
>>
>>> I'm stuck on what I feel should be a minor problem.
>>> I have a dataseries obtained from a MS Access database that consists of
>>> a series of numbers seperated by carridge returns (\r)
>>> Currently this data is in R as mode numeric???
>>>
>>> I want to separate this into a vector of the componant numbers.
>>> Perhaps a little code will help describe what I've got here!!
>>>
>>> library(rodbc)
>>> s_sql<-'SELECT Data from table where id=1' #only one record returned
>>> oWave<-sqlQuery(oConn,s_sql)
>>>
>>> wave.data<-oWave$data
>>>
>>>> mode(data)
>>> [1] "numeric"
>>>
>>> Any clues will be much appreciated
>>
>> Er, did you mean to say mode(wave.data)?
>>
>> Anyways, you say that you want to obtain a vector of numbers, and you
>> got an object of mode numeric, which means that it is a vector of
>> numbers. So what was the problem in the first place?
>>
>> If you had data with carriage returns inside, then you should have
>> them as mode "character".
>>
>> So there seems to be something that you're not telling us...
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tom at maladmin.com  Tue Nov 15 13:47:03 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 15 Nov 2005 07:47:03 -0500
Subject: [R] strsplit
In-Reply-To: <Pine.LNX.4.61.0511151723270.10875@gannet.stats>
References: <1132055518.4819.37.camel@localhost.localdomain>
	<x2k6f9vo5w.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.61.0511151723270.10875@gannet.stats>
Message-ID: <1132058823.4819.55.camel@localhost.localdomain>

Thanks everyone

Using the sqlQuery line
    wave<-sqlQuery(connection,s_sql,as.is=c(TRUE,TRUE,TRUE,TRUE,TRUE))

Then using strsplit(data,'\r') instead of split() seems to give what I
need



On Tue, 2005-15-11 at 17:29 +0000, Prof Brian Ripley wrote:
> On Tue, 15 Nov 2005, Peter Dalgaard wrote:
> 
> > tom wright <tom at maladmin.com> writes:
> >
> >> I'm stuck on what I feel should be a minor problem.
> >> I have a dataseries obtained from a MS Access database that consists of
> >> a series of numbers seperated by carridge returns (\r)
> >> Currently this data is in R as mode numeric???
> >>
> >> I want to separate this into a vector of the componant numbers.
> >> Perhaps a little code will help describe what I've got here!!
> >>
> >> library(rodbc)
> >> s_sql<-'SELECT Data from table where id=1' #only one record returned
> >> oWave<-sqlQuery(oConn,s_sql)
> >>
> >> wave.data<-oWave$data
> >>
> >>> mode(data)
> >> [1] "numeric"
> >>
> >> Any clues will be much appreciated
> >
> > Er, did you mean to say mode(wave.data)?
> >
> > Anyways, you say that you want to obtain a vector of numbers, and you
> > got an object of mode numeric, which means that it is a vector of
> > numbers. So what was the problem in the first place?
> >
> > If you had data with carriage returns inside, then you should have
> > them as mode "character".
> >
> > So there seems to be something that you're not telling us...
> 
> or Access is not telling R.  One possibility is that Access is reporting 
> this field as numeric:  use sqlColumns() to find out.  If so, you need to 
> sort this out in Access.  If not, you can use the as.is argument: see 
> ?sqlGetResults referenced from ?sqlQuery (although I would be surprised if 
> type.convert was getting confused).
>



From redbeard at arrr.net  Tue Nov 15 19:10:19 2005
From: redbeard at arrr.net (Jarrett Byrnes)
Date: Tue, 15 Nov 2005 10:10:19 -0800
Subject: [R] Repeates Measures MANOVA for Time*Treatment Interactions
Message-ID: <f74b268753a7de710fd368d8f709c488@arrr.net>

Dear R folk,
	First off I want to thank those of you who responded with comments for 
my R quick and dirty stats tutorial.  They've been quite helpful, and 
I'm in the process of revising them.  When it comes to repeated 
measures MANOVA, I'm in a bit of a bind, however.  I'm beginning to see 
that all of the documentation is written for psychologists, who have a 
slightly different mind-set behind their experiments than, say, an 
ecologist, who is interested in the effects of time per se, and not 
just the effects of a treatment.  For example, here's my dataset, say, 
looking at plant height in cm with and without fertilizer

Treatment, Time1, Time2, Time3, Time4, Time5
Fertilizer, 1, 4, 8, 10, 12
Control,1,2,3,4,5
Fertilizer,1,8,10,12,20
Control,1,3,5,6,6
Fertilizer,2,5,10,20,25
Control,1,2,4,4,4


Clearly there is a time*treatment interaction (just eyeballing the 
dataset)

My question is, how does one set this up using the anova.mlm approach 
so that in the end I can write up a table that says

Treatment
Time
Time*Treatment

I can see from ?anova.mlm how one would get the Treatment effect using 
something like

response<-with(my.data, rbind(Time1, Time2, Time3, Time4, Time5))
mlmfit<-lm(response~1)
mlmfit0<-update(mlmfit, ~0)
anova(mlmfit, mlmfit0, X = ~ Treatment, idata=my.data, test="Spherical")

Although this yields the result that, after correction, it's not 
significant - perhaps due to the low DF from this simple example
--
Analysis of Variance Table

Model 1: response ~ 1
Model 2: response ~ 1 - 1

Contrasts orthogonal to
~Treatment

Greenhouse-Geisser epsilon: 0.3565
Huynh-Feldt epsilon:        0.4982

   Res.Df Df Gen.var.      F num Df den Df  Pr(>F)  G-G Pr  H-F Pr
1      4     0.43167
2      5  1  0.50937 3.7939      4     16 0.02356 0.09620 0.06966
--

But, I still want to get my time and time*treatment interactions - what 
would be the appropriate anova statements here?

Thanks so much, and hopefully this will resolve the confusion both for 
myself and for LOTS of other ecology types!

-Jarrett


----------------------------------------
Jarrett Byrnes
Population Biology Graduate Group, UC Davis
Bodega Marine Lab
707-875-1969
http://www-eve.ucdavis.edu/stachowicz/byrnes.shtml



From ggrothendieck at gmail.com  Tue Nov 15 19:16:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 15 Nov 2005 13:16:00 -0500
Subject: [R] changing the value of a variable from inside a function
In-Reply-To: <5.2.1.1.1.20051115115224.0198f800@imap.duke.edu>
References: <5.2.1.1.1.20051115115224.0198f800@imap.duke.edu>
Message-ID: <971536df0511151016g4f6c8457r370569ff7d63169c@mail.gmail.com>

Use eval.parent as shown in example 1.  Note that you might
be tempted to use example 2 but it does not actually fulfill
the letter of the original post since it changes test in the lexical
environment of f, i.e.the environment where f is defined,
rather than the calling frame of f, i.e. the environment from where
f is called.  To get <<- to work with example 2 we must
create a new f that is the same as the original f but whose
lexical environment has been changed to be the caller frame
as shown in example 3.

# example 1.  ok.  test changed in caller frame.
test <- 11:13
f <- function(i) eval.parent(substitute(test[i] <- 99))
g <- function() { test <- 1:3; f(2); print(test) }
g()  # 1 99 3
test # 11 12 13

# example 2.  Same except f has been changed.
# Note that this changes test in the lexical environment
# rather than in the caller frame.
test <- 11:13
f <- function(i) test[i] <<- 99
g <- function() { test <- 1:3; f(2); print(test) }
g()  # 1 2 3
test  # 11 99 13

# example 3. same as example 2 but the lexical environment of f is
# forced to be the caller frame so that it works as in example 1.
# f is the same as in example 1 and g has been changed to
# create a new f like the original f but with the caller frame as its
# lexical environment.
test <- 11:13
f <- function(i) test[i] <<- 99
g <- function() { test <- 1:3; environment(f) <- environment(); f(2);
print(test) }
g()  # 1 99 3
test # 11 12 13


Another possibility, which is similar in effect to example 1, would be
to use defmacro in package gtools.


On 11/15/05, Michael Wolosin <msw10 at duke.edu> wrote:
> All -
>
> I am trying to write R code to implement a recursive algorithm.  I've
> solved the problem in a klunky way that works, but uses  more memory and
> computing time than it should.
>
> A more elegant solution than my current one would require updating the
> values of a variable that is located in what I will call the "root"
> environment - that environment from which the original call to the
> recursive function was issued.  Certainly, I could pass the variable into
> the function, update it inside, and return it.  However, the variable I am
> updating is a large matrix, and the recursion could end up several hundred
> levels deep.  Passing the matrix around would create a copy in the
> environment for each call, wasting memory, time, and space.
>
> I've read the help on the "sys.{}" family of functions, and "eval", and
> although I can't claim to have absorbed it all, it seems like it is much
> easier to access the value of a variable in a parent frame than it is to
> update that value with assignment.
> If you make an assignment inside a function, even if it is to a section of
> a variable that exists in a parent frame, the variable is only created or
> updated in the current environment - never in the parent frame.
>
> For example:
>
> test <- matrix(NA,nrow=4,ncol=3)
> test[1,] <- c(1,2,3)
> blah <- function(i){
>   test[i,] <- c(0,1,2) + i
>   return(test)
> }
> test
> blah(2)
> test
>
> So the real question is, how do I write the function like "blah" above that
> updates "test" in the parent or root frame?
>
> blah <- function(i){
>   test[i,] <- c(1,2,3) + i  #modify this line somehow
>   return(NULL)
> }
> If done "correctly", we will get:
>  > blah(2)
>  > test
>       [,1] [,2] [,3]
>  [1,]    1    2    3
>  [2,]    2    3    4
>  [3,]   NA   NA   NA
>  [4,]   NA   NA   NA
>
> And given an example that works from within a single function call, does it
> have to be modified to work recursively?
>
> blah <- function(i){
>   if (i<4) {blah(i + 1)}
>   test[i,] <- c(0,1,2) + i  #modify this line somehow
>   return(NULL)
> }
> If written "correctly", the following would be the output:
>  > blah(2)
>  > test
>       [,1] [,2] [,3]
>  [1,]    1    2    3
>  [2,]    2    3    4
>  [3,]    3    4    5
>  [4,]    4    5    6
>
> One idea would be to write out to a file.  The filename could reside in the
> root environment, and that is all that is needed.  But  this also seems
> inelegant (and slow).  If I can read and write to a file, I should be able
> to read and write to a memory location.
>
> I suspect that the solution lies somewhere in the "sys" functions, but I
> was having trouble seeing it.  Any help would be appreciated.
>
> Thank you in advance,
>
> Mike
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Tue Nov 15 19:55:18 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2005 19:55:18 +0100
Subject: [R] Repeates Measures MANOVA for Time*Treatment Interactions
In-Reply-To: <f74b268753a7de710fd368d8f709c488@arrr.net>
References: <f74b268753a7de710fd368d8f709c488@arrr.net>
Message-ID: <x2fypxvizt.fsf@turmalin.kubism.ku.dk>

Jarrett Byrnes <redbeard at arrr.net> writes:

> Dear R folk,
> 	First off I want to thank those of you who responded with comments for 
> my R quick and dirty stats tutorial.  They've been quite helpful, and 
> I'm in the process of revising them.  When it comes to repeated 
> measures MANOVA, I'm in a bit of a bind, however.  I'm beginning to see 
> that all of the documentation is written for psychologists, who have a 
> slightly different mind-set behind their experiments than, say, an 
> ecologist, who is interested in the effects of time per se, and not 
> just the effects of a treatment.  For example, here's my dataset, say, 
> looking at plant height in cm with and without fertilizer
> 
> Treatment, Time1, Time2, Time3, Time4, Time5
> Fertilizer, 1, 4, 8, 10, 12
> Control,1,2,3,4,5
> Fertilizer,1,8,10,12,20
> Control,1,3,5,6,6
> Fertilizer,2,5,10,20,25
> Control,1,2,4,4,4
> 
> 
> Clearly there is a time*treatment interaction (just eyeballing the 
> dataset)
> 
> My question is, how does one set this up using the anova.mlm approach 
> so that in the end I can write up a table that says
> 
> Treatment
> Time
> Time*Treatment
> 
> I can see from ?anova.mlm how one would get the Treatment effect using 
> something like
> 
> response<-with(my.data, rbind(Time1, Time2, Time3, Time4, Time5))

cbind() would be more like it

> mlmfit<-lm(response~1)
> mlmfit0<-update(mlmfit, ~0)
> anova(mlmfit, mlmfit0, X = ~ Treatment, idata=my.data, test="Spherical")

No... idata is the *intra*-block structure, so it should just be your
five times. I'm somewhat baffled that you're getting away with
supplying a data frame that doesn't have the right number of rows, but
maybe that's because you're supplying the transposed matrix as a
response. 

Try 

mlmfit <- lm(response ~ Treatment)
mlmfit1 <- lm(response ~ 1)
mlmfit0 <- lm(response ~ 0)
 
Then 

anova.mlm(mlmfit, mlmfit1, X=~1, test="Spherical")

tests whether time contrasts depend on treatment, which is the
time*treatment effect.

anova.mlm(mlmfit, mlmfit1, M=~1) 

is the test for overall treatment effect

and 

anova.mlm(mlmfit1, mlmfit0, X=~1, test="Spherical")

is the test for overall time effect.

(If Time1 is a pre-randomization baseline measurement you should
consider including it as a covariate, but that is another matter)

> Although this yields the result that, after correction, it's not 
> significant - perhaps due to the low DF from this simple example
> --
> Analysis of Variance Table
> 
> Model 1: response ~ 1
> Model 2: response ~ 1 - 1
> 
> Contrasts orthogonal to
> ~Treatment
> 
> Greenhouse-Geisser epsilon: 0.3565
> Huynh-Feldt epsilon:        0.4982
> 
>    Res.Df Df Gen.var.      F num Df den Df  Pr(>F)  G-G Pr  H-F Pr
> 1      4     0.43167
> 2      5  1  0.50937 3.7939      4     16 0.02356 0.09620 0.06966
> --
> 
> But, I still want to get my time and time*treatment interactions - what 
> would be the appropriate anova statements here?
> 
> Thanks so much, and hopefully this will resolve the confusion both for 
> myself and for LOTS of other ecology types!
> 
> -Jarrett
> 
> 
> ----------------------------------------
> Jarrett Byrnes
> Population Biology Graduate Group, UC Davis
> Bodega Marine Lab
> 707-875-1969
> http://www-eve.ucdavis.edu/stachowicz/byrnes.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Tue Nov 15 20:08:53 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2005 20:08:53 +0100
Subject: [R] Repeates Measures MANOVA for Time*Treatment Interactions
In-Reply-To: <x2fypxvizt.fsf@turmalin.kubism.ku.dk>
References: <f74b268753a7de710fd368d8f709c488@arrr.net>
	<x2fypxvizt.fsf@turmalin.kubism.ku.dk>
Message-ID: <x2br0lvid6.fsf@turmalin.kubism.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> No... idata is the *intra*-block structure, so it should just be your
> five times. I'm somewhat baffled that you're getting away with

To clarify: Your five times, if anything. the default is a dataframe
containing the single vector 1:p where p is the number of columns.

This allows you to fit simple polynomials in time and test them for
interaction with treatment, etc., but of course it requires that the
times are equispaced, so if they are not, then you should use
(say) idata=data.frame(time=c(0,3,6,9,12,18,24))

<snip rest>
-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From typhoon_zy at hotmail.com  Tue Nov 15 20:21:26 2005
From: typhoon_zy at hotmail.com (Feng Tai)
Date: Tue, 15 Nov 2005 13:21:26 -0600
Subject: [R] changing the value of a variable from inside a function
In-Reply-To: <5.2.1.1.1.20051115115224.0198f800@imap.duke.edu>
Message-ID: <BAY103-F29975C42E7BF85C82D48BAE85D0@phx.gbl>

You could use 'ref' package.

Feng


>From: Michael Wolosin <msw10 at duke.edu>
>To: r-help at stat.math.ethz.ch
>Subject: [R] changing the value of a variable from inside a function
>Date: Tue, 15 Nov 2005 12:22:15 -0500
>
>All -
>
>I am trying to write R code to implement a recursive algorithm.  I've
>solved the problem in a klunky way that works, but uses  more memory and
>computing time than it should.
>
>A more elegant solution than my current one would require updating the
>values of a variable that is located in what I will call the "root"
>environment - that environment from which the original call to the
>recursive function was issued.  Certainly, I could pass the variable into
>the function, update it inside, and return it.  However, the variable I am
>updating is a large matrix, and the recursion could end up several hundred
>levels deep.  Passing the matrix around would create a copy in the
>environment for each call, wasting memory, time, and space.
>
>I've read the help on the "sys.{}" family of functions, and "eval", and
>although I can't claim to have absorbed it all, it seems like it is much
>easier to access the value of a variable in a parent frame than it is to
>update that value with assignment.
>If you make an assignment inside a function, even if it is to a section of
>a variable that exists in a parent frame, the variable is only created or
>updated in the current environment - never in the parent frame.
>
>For example:
>
>test <- matrix(NA,nrow=4,ncol=3)
>test[1,] <- c(1,2,3)
>blah <- function(i){
>    test[i,] <- c(0,1,2) + i
>    return(test)
>}
>test
>blah(2)
>test
>
>So the real question is, how do I write the function like "blah" above that
>updates "test" in the parent or root frame?
>
>blah <- function(i){
>    test[i,] <- c(1,2,3) + i  #modify this line somehow
>    return(NULL)
>}
>If done "correctly", we will get:
>  > blah(2)
>  > test
>        [,1] [,2] [,3]
>   [1,]    1    2    3
>   [2,]    2    3    4
>   [3,]   NA   NA   NA
>   [4,]   NA   NA   NA
>
>And given an example that works from within a single function call, does it
>have to be modified to work recursively?
>
>blah <- function(i){
>    if (i<4) {blah(i + 1)}
>    test[i,] <- c(0,1,2) + i  #modify this line somehow
>    return(NULL)
>}
>If written "correctly", the following would be the output:
>  > blah(2)
>  > test
>        [,1] [,2] [,3]
>   [1,]    1    2    3
>   [2,]    2    3    4
>   [3,]    3    4    5
>   [4,]    4    5    6
>
>One idea would be to write out to a file.  The filename could reside in the
>root environment, and that is all that is needed.  But  this also seems
>inelegant (and slow).  If I can read and write to a file, I should be able
>to read and write to a memory location.
>
>I suspect that the solution lies somewhere in the "sys" functions, but I
>was having trouble seeing it.  Any help would be appreciated.
>
>Thank you in advance,
>
>Mike
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From redbeard at arrr.net  Tue Nov 15 20:27:07 2005
From: redbeard at arrr.net (Jarrett Byrnes)
Date: Tue, 15 Nov 2005 11:27:07 -0800
Subject: [R] Repeates Measures MANOVA for Time*Treatment Interactions
In-Reply-To: <x2br0lvid6.fsf@turmalin.kubism.ku.dk>
References: <f74b268753a7de710fd368d8f709c488@arrr.net>
	<x2fypxvizt.fsf@turmalin.kubism.ku.dk>
	<x2br0lvid6.fsf@turmalin.kubism.ku.dk>
Message-ID: <4380e28e1f63253e9bfd95ab9b48cbf3@arrr.net>

Ah, so, if, say, I sampled on the 1st, 4th, 19th, 20th, and 30th day of 
a month, I should use:
idata=data.frame(time=c(1,4,19,20,30))


On Nov 15, 2005, at 11:08 AM, Peter Dalgaard wrote:

> Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:
>
>> No... idata is the *intra*-block structure, so it should just be your
>> five times. I'm somewhat baffled that you're getting away with
>
> To clarify: Your five times, if anything. the default is a dataframe
> containing the single vector 1:p where p is the number of columns.
>
> This allows you to fit simple polynomials in time and test them for
> interaction with treatment, etc., but of course it requires that the
> times are equispaced, so if they are not, then you should use
> (say) idata=data.frame(time=c(0,3,6,9,12,18,24))
>
> <snip rest>
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 
> 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tom at maladmin.com  Tue Nov 15 15:40:05 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 15 Nov 2005 09:40:05 -0500
Subject: [R] Subtracting timeseries objects
Message-ID: <1132065605.4819.65.camel@localhost.localdomain>

Sorry to keep posting but I want to do this right and I'm hoping for
some pointers
I now have two time series objects which I need to subtract.
Unfortunatly the two series dont have the same sample rates.
When I try to subtract them

avgSub<-avg1-avg2

The time series object is clever enough to object.
So I guess I need to write a function for subtraction of the time series
objects which will need to interpolate the samples to the same sampling
time (this linear interpolation should be ok here)
I would like to make this function the default one to be used when a ts
subtractin is attempted. Unfortunatly I dont really have much idea where
to start with this. If someone could point me in the direction of some
good reading I'll be grateful.
Many thanks (again)
Tom



From michael_graber at gmx.de  Tue Nov 15 21:10:04 2005
From: michael_graber at gmx.de (Michael Graber)
Date: Tue, 15 Nov 2005 21:10:04 +0100
Subject: [R] y-axis in histograms
Message-ID: <437A409C.1060005@gmx.de>

Dear R- list,
I have some data to present with histograms. Therefore I used hist(...). 
I have few values with almost 80% of
the frequencies (totaly 800) and some other values with low frequencies 
( totaly 5 -10 )
that I want to emphasize. Therefore I want to "cut" the y-axis on 100, 
but I
don't know how to deal with this.

Thanks in advance,

Michael Graber



From jeanniet at geo.umass.edu  Tue Nov 15 21:23:40 2005
From: jeanniet at geo.umass.edu (Jeanne Thibeault)
Date: Tue, 15 Nov 2005 15:23:40 -0500
Subject: [R] labeling contours that are output from contourLines
Message-ID: <200511152023.jAFKNLE2018204@eclogite.geo.umass.edu>

Hello,

I have been trying to map some climate data using contourLines on a
projected map. The following code seems to work well to add the lines, but
the lines are not labeled:

res <- contourLines(data.li)
contours_x <- unlist(sapply(res, function(x) c(x$x, NA))) 
contours_y <- unlist(sapply(res, function(x) c(x$y, NA))) 
contours <- list(x= contours_x, y = contours_y)
lines(mapproject(contours, proj = ""), type="l")

I was thinking about trying to adapt some of the code from
panel.levelplot(), but was wondering if someone else had already written a
labeling function.

Thanks for the help,
Jeanne



From claus.atzenbeck at freenet.de  Tue Nov 15 21:27:00 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Tue, 15 Nov 2005 21:27:00 +0100 (CET)
Subject: [R] effect sizes for Wilcoxon tests
In-Reply-To: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
References: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
Message-ID: <Pine.OSX.4.61.0511152122290.17240@cirrus.local>

On Mon, 14 Nov 2005, Claus Atzenbeck wrote:

> However, for Wilcoxon tests, the formula for effect sizes is:
> r = Z / sqrt(N)
>
> I wonder how I can calculate the Z-score in R for a Wilcoxon test.

Does anyone know how to calculate effect sizes for Wilcoxon tests as
SPSS can do? I wonder if it would be very difficult to do so with R.

Thanks...
Claus



From ggrothendieck at gmail.com  Tue Nov 15 21:33:21 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 15 Nov 2005 15:33:21 -0500
Subject: [R] Subtracting timeseries objects
In-Reply-To: <1132065605.4819.65.camel@localhost.localdomain>
References: <1132065605.4819.65.camel@localhost.localdomain>
Message-ID: <971536df0511151233t19843833v246e34f769fc19ae@mail.gmail.com>

On 11/15/05, tom wright <tom at maladmin.com> wrote:
> Sorry to keep posting but I want to do this right and I'm hoping for
> some pointers
> I now have two time series objects which I need to subtract.
> Unfortunatly the two series dont have the same sample rates.
> When I try to subtract them
>
> avgSub<-avg1-avg2
>
> The time series object is clever enough to object.
> So I guess I need to write a function for subtraction of the time series
> objects which will need to interpolate the samples to the same sampling
> time (this linear interpolation should be ok here)

Convert it to zoo, perform the calculation and convert it back:

   library(zoo)
   z <- na.approx(merge(as.zoo(ts1), as.zoo(ts2)))
   as.ts(z[,1] - z[,2])

> I would like to make this function the default one to be used when a ts
> subtractin is attempted. Unfortunatly I dont really have much idea where
> to start with this. If someone could point me in the direction of some
> good reading I'll be grateful.

Its not a good idea to change the definition of
subtraction in ts but you could create a subclass
of ts and then define your own subtraction method
for that.

"-.myts" <- function(x, y) {
   z <- na.approx(merge(as.zoo(ts1), as.zoo(ts2)))
   myts <- as.ts(z[,1] - z[,2])
   class(myts) <- c("myts", class(myts))
   myts
}

class(ts1) <- c("myts", class(ts1))
class(ts2) <- c("myts", class(ts2))
ts1 - ts2



From ripley at stats.ox.ac.uk  Tue Nov 15 21:47:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Nov 2005 20:47:18 +0000 (GMT)
Subject: [R] Subtracting timeseries objects
In-Reply-To: <1132065605.4819.65.camel@localhost.localdomain>
References: <1132065605.4819.65.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0511152044570.14601@gannet.stats>

On Tue, 15 Nov 2005, tom wright wrote:

> Sorry to keep posting but I want to do this right and I'm hoping for
> some pointers
> I now have two time series objects which I need to subtract.
> Unfortunatly the two series dont have the same sample rates.
> When I try to subtract them
>
> avgSub<-avg1-avg2
>
> The time series object is clever enough to object.
> So I guess I need to write a function for subtraction of the time series
> objects which will need to interpolate the samples to the same sampling
> time (this linear interpolation should be ok here)
> I would like to make this function the default one to be used when a ts
> subtractin is attempted. Unfortunatly I dont really have much idea where
> to start with this. If someone could point me in the direction of some
> good reading I'll be grateful.

Subtraction is done by Ops.ts, so you need to learn up on S3 group 
generics.  That leaves you just two choices I believe, the White book and 
S Programming (details of both in the FAQ).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Nov 15 21:54:03 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2005 21:54:03 +0100
Subject: [R] effect sizes for Wilcoxon tests
In-Reply-To: <Pine.OSX.4.61.0511152122290.17240@cirrus.local>
References: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
	<Pine.OSX.4.61.0511152122290.17240@cirrus.local>
Message-ID: <x27jb9vdhw.fsf@turmalin.kubism.ku.dk>

Claus Atzenbeck <claus.atzenbeck at freenet.de> writes:

> On Mon, 14 Nov 2005, Claus Atzenbeck wrote:
> 
> > However, for Wilcoxon tests, the formula for effect sizes is:
> > r = Z / sqrt(N)
> >
> > I wonder how I can calculate the Z-score in R for a Wilcoxon test.
> 
> Does anyone know how to calculate effect sizes for Wilcoxon tests as
> SPSS can do? I wonder if it would be very difficult to do so with R.

If the above formula is correct, it can't be hard. What I wonder is
whether (and if so, how) it makes sense. (& the fact that SPSS does it
is no guarantee...)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From rolf at math.unb.ca  Tue Nov 15 21:50:52 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 15 Nov 2005 16:50:52 -0400 (AST)
Subject: [R] An optim() mystery.
Message-ID: <200511152050.jAFKoqdJ025227@erdos.math.unb.ca>


I have a Master's student working on a project which involves
estimating parameters of a certain model via maximum likelihood,
with the maximization being done via optim().

A phenomenon has occurred which I am at a loss to explain.

If we use certain pairs of starting values for optim(), it
simply returns those values as the ``optimal'' values, although
they are definitely not optimal.

With other starting values, optim() goes off and finds a reasonable,
optimum, and seems to do so consistently --- i.e. it gets essentially
the same estimates irrespective of starting values.

We have plotted the log likelihood surface and it appears smooth
and relatively innocuous.

The phenomenon only occurs with the "L-BFGS-B"; the default
(Nelder-Mead simplex) method, with a heavy penalty for violating
constraints, seems to work just fine.  So we can get solutions;
it just makes me uneasy that there's this funny going on.

Can anyone shed any light on what the problem is?  I have enclosed
below code to reproduce the phenomenon.  It is probably advisable
to save it in two separate parts:  The first part does some
setting up - creating the data and defining the function to be
optimized.  The second part consists of various calls to optim().
Note that the code ***minimizes*** minus twice the log likelihood.

Be aware that the calls to optim() take quite a while to execute;
like 40 seconds on my laptop.

Eternal gratitude for any words of wisdom on this matter!

BTW, the recalcitrant parameter pair, used in the script below, is
the pair of values which were used to simulate the data ``resi''.  So
they are the ``correct'' values.  But optim() can't know that unless
magic is loose in the world!  Anyhow, there are other pairs, e.g.
c(0.04,0.15),that result in the same behaviour.

				cheers,

					Rolf Turner
					rolf at math.unb.ca

P. S.  The help on optim() says the Nelder-Mead method is
``relatively slow''.  However for this problem optim() seems to come
in about 10 seconds when Nelder-Mead is used, as opposed to about 40
when "L-BFGS-B" is used and runs successfully.

					R. T.

#==+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
# Script #1:
#==+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===

#
# Preliminary stuff, to set things up:
#

resi <- c(0.0603646889,0.1363603779,0.1523925363,0.2145877518,0.1440157480,
          0.0844998235,-0.1297248347,-0.0552557629,0.0481005640,0.0754532293,
          0.2119027608,0.1851255809,-0.0380954844,-0.0118705151,-0.1094305971,
         -0.0862933436,-0.0275531242,-0.0626877959,-0.0923180186,-0.2725409915,
         -0.2733103385,-0.2806762169,-0.3713376801,-0.4138431049,-0.3037150478,
         -0.3347483204,-0.1598387374,-0.1869063413,-0.1734677727,-0.1610389684,
         -0.2239938894,-0.2186303230,-0.1686406340,-0.2381844523,-0.2115563556,
         -0.3134484958,-0.2570423412,-0.1447961173,0.0321965240,0.0073261698,
         -0.0559209305,-0.0219575807,0.0073407870,0.2049894668,0.0997092007,
          0.2281661594,0.1474324246,0.2203087759,0.1902474412,0.2909445296,
          0.1869771602,0.1673737821,0.0644645886,0.1107366215,0.0222997552,
         -0.0202454896,0.0458960824,0.0082990521,0.0069920731,-0.0400939212,
         -0.1366096522,-0.1103078421,-0.0539834505,-0.0002650046,0.0163391466,
         -0.0713795007,0.1324350576,0.1937169442,0.3131844274,0.3325063151,
          0.4348740892,0.4551717728,0.4049963331,0.4482280578,0.2167847952,
          0.2168151604,0.2233674124,0.2472087180,0.1983512503,0.1823975372,
          0.0483989112,-0.0421543344,-0.0526425804,-0.0590416757,-0.0406552389,
          0.1728161804,0.2281930355,0.2136595821,0.0647976863,0.0781707139,
          0.0860909779,-0.0236073305,0.0817706100,0.1411715048,0.1305741598,
          0.2838075993,0.1642797706,0.1328162057,0.3099161357,0.1859591497)

theta <- seq(0,2*pi,length=101)[-101]

foo <- function(pars,res,theta) {
		eps <- sqrt(.Machine$double.eps)
		big <- sqrt(.Machine$double.xmax)
                k   <- pars[1]
                rho <- pars[2]
		# This line is needed in the Nelder-Mead case:
                if(k < 0 | rho < 0 | rho > 1) return(big)
                Sigma <- k*rho^outer(theta,theta,cdist)
                udv   <- svd(Sigma)
                d     <- udv$d
                if(min(d) < eps) return(big)
                w <- t(udv$u)%*%res
		# Minus twice the log likelihood:
                sum(log(d)) + sum(w^2/d)
        }

cdist <- function(theta1,theta2) {
        d <- abs(theta1-theta2)%%(2*pi)
        ifelse(d>pi,2*pi-d,d)
}

#==+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
# Script #2:
#==+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===

#
# Demonstration of the problem.
#

# Using the ``bad'' starting values:
pars <- c(0.06,0.25)
R1 <- optim(pars,foo,res=resi,theta=theta, method="L-BFGS-B",
      lower=c(0,0),upper=c(Inf,1))
print(R1)

# Using roughly (rather badly!) estimated starting values:
pars <- c(var(resi),cor(resi[-1],resi[-100]))
R2 <- optim(pars,foo,res=resi,theta=theta, method="L-BFGS-B",
      lower=c(0,0),upper=c(Inf,1))
print(R2)

# Using other starting values:
pars <- c(0.05,0.5)
R3 <- optim(pars,foo,res=resi,theta=theta, method="L-BFGS-B",
      lower=c(0,0),upper=c(Inf,1))
print(R3)

# Using Nelder-Mead and the ``bad'' starting values.
pars <- c(0.06,0.25)
R4 <- optim(pars,foo,res=resi,theta=theta)
print(R4)



From claus.atzenbeck at freenet.de  Tue Nov 15 22:17:18 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Tue, 15 Nov 2005 22:17:18 +0100 (CET)
Subject: [R] effect sizes for Wilcoxon tests
In-Reply-To: <x27jb9vdhw.fsf@turmalin.kubism.ku.dk>
References: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
	<Pine.OSX.4.61.0511152122290.17240@cirrus.local>
	<x27jb9vdhw.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.OSX.4.61.0511152206110.19274@cirrus.local>

On Tue, 15 Nov 2005, Peter Dalgaard wrote:

> > > However, for Wilcoxon tests, the formula for effect sizes is:
> > > r = Z / sqrt(N)
> > >
> > > I wonder how I can calculate the Z-score in R for a Wilcoxon test.
> >
> > Does anyone know how to calculate effect sizes for Wilcoxon tests as
> > SPSS can do? I wonder if it would be very difficult to do so with R.
>
> If the above formula is correct, it can't be hard. What I wonder is
> whether (and if so, how) it makes sense. (& the fact that SPSS does it
> is no guarantee...)

According to my references, the formula is OK.

However, how do I get Z from a Wilcoxon test in R?

(BTW, I have to correct myself: SPSS delivers Z. The effect size was
calculated by the author of one of my books using the above mentioned
formula.)

Claus



From jeanniet at geo.umass.edu  Tue Nov 15 22:30:08 2005
From: jeanniet at geo.umass.edu (Jeanne Thibeault)
Date: Tue, 15 Nov 2005 16:30:08 -0500
Subject: [R] labeling contour lines
Message-ID: <200511152129.jAFLTnI3021669@eclogite.geo.umass.edu>

Hello again,

In my previous message I neglected to acknowledge that Roger Bivand had
previously provided me with the code for the contours. My apologies.

Jeanne



From p.dalgaard at biostat.ku.dk  Tue Nov 15 22:51:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2005 22:51:46 +0100
Subject: [R] An optim() mystery.
In-Reply-To: <200511152050.jAFKoqdJ025227@erdos.math.unb.ca>
References: <200511152050.jAFKoqdJ025227@erdos.math.unb.ca>
Message-ID: <x23blxvatp.fsf@turmalin.kubism.ku.dk>

Rolf Turner <rolf at math.unb.ca> writes:

> I have a Master's student working on a project which involves
> estimating parameters of a certain model via maximum likelihood,
> with the maximization being done via optim().
> 
> A phenomenon has occurred which I am at a loss to explain.
> 
> If we use certain pairs of starting values for optim(), it
> simply returns those values as the ``optimal'' values, although
> they are definitely not optimal.
> 
> With other starting values, optim() goes off and finds a reasonable,
> optimum, and seems to do so consistently --- i.e. it gets essentially
> the same estimates irrespective of starting values.
> 
> We have plotted the log likelihood surface and it appears smooth
> and relatively innocuous.
> 
> The phenomenon only occurs with the "L-BFGS-B"; the default
> (Nelder-Mead simplex) method, with a heavy penalty for violating
> constraints, seems to work just fine.  So we can get solutions;
> it just makes me uneasy that there's this funny going on.
> 
> Can anyone shed any light on what the problem is?  I have enclosed
> below code to reproduce the phenomenon.  It is probably advisable
> to save it in two separate parts:  The first part does some
> setting up - creating the data and defining the function to be
> optimized.  The second part consists of various calls to optim().
> Note that the code ***minimizes*** minus twice the log likelihood.
> 
> Be aware that the calls to optim() take quite a while to execute;
> like 40 seconds on my laptop.
> 
> Eternal gratitude for any words of wisdom on this matter!

The gradient calculations used by optim and friends sometimes cause
trouble if the objective function is not sufficiently smooth. Have a
look at this:

fval<-sapply(seq(-1e-6,1e-6,,100),
     function(x)foo(pars+c(0,x),res=resi,theta=theta))
plot(diff(diff(fval)),type="l") 

notice that the second order differences occasionally go negative. If
this is commensurate with the differences used for the numerical
gradient, then you have a bunch of local optima on your hands. I'm not
quite sure that they really are commensurate (and I'm not going to dig
into the optim sources just now) but if this is the cause, you could
supply your own gradient. 

(
I did some similar considerations for the IBC in 2004, you might want
to peek at my slides from then:

http://www.biostat.ku.dk/~pd/slides/cairns04-slides.pdf 
)

> BTW, the recalcitrant parameter pair, used in the script below, is
> the pair of values which were used to simulate the data ``resi''.  So
> they are the ``correct'' values.  But optim() can't know that unless
> magic is loose in the world!  Anyhow, there are other pairs, e.g.
> c(0.04,0.15),that result in the same behaviour.
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca
> 
> P. S.  The help on optim() says the Nelder-Mead method is
> ``relatively slow''.  However for this problem optim() seems to come
> in about 10 seconds when Nelder-Mead is used, as opposed to about 40
> when "L-BFGS-B" is used and runs successfully.
> 
> 					R. T.
> 
> #==+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
> # Script #1:
> #==+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
> 
> #
> # Preliminary stuff, to set things up:
> #
> 
> resi <- c(0.0603646889,0.1363603779,0.1523925363,0.2145877518,0.1440157480,
>           0.0844998235,-0.1297248347,-0.0552557629,0.0481005640,0.0754532293,
>           0.2119027608,0.1851255809,-0.0380954844,-0.0118705151,-0.1094305971,
>          -0.0862933436,-0.0275531242,-0.0626877959,-0.0923180186,-0.2725409915,
>          -0.2733103385,-0.2806762169,-0.3713376801,-0.4138431049,-0.3037150478,
>          -0.3347483204,-0.1598387374,-0.1869063413,-0.1734677727,-0.1610389684,
>          -0.2239938894,-0.2186303230,-0.1686406340,-0.2381844523,-0.2115563556,
>          -0.3134484958,-0.2570423412,-0.1447961173,0.0321965240,0.0073261698,
>          -0.0559209305,-0.0219575807,0.0073407870,0.2049894668,0.0997092007,
>           0.2281661594,0.1474324246,0.2203087759,0.1902474412,0.2909445296,
>           0.1869771602,0.1673737821,0.0644645886,0.1107366215,0.0222997552,
>          -0.0202454896,0.0458960824,0.0082990521,0.0069920731,-0.0400939212,
>          -0.1366096522,-0.1103078421,-0.0539834505,-0.0002650046,0.0163391466,
>          -0.0713795007,0.1324350576,0.1937169442,0.3131844274,0.3325063151,
>           0.4348740892,0.4551717728,0.4049963331,0.4482280578,0.2167847952,
>           0.2168151604,0.2233674124,0.2472087180,0.1983512503,0.1823975372,
>           0.0483989112,-0.0421543344,-0.0526425804,-0.0590416757,-0.0406552389,
>           0.1728161804,0.2281930355,0.2136595821,0.0647976863,0.0781707139,
>           0.0860909779,-0.0236073305,0.0817706100,0.1411715048,0.1305741598,
>           0.2838075993,0.1642797706,0.1328162057,0.3099161357,0.1859591497)
> 
> theta <- seq(0,2*pi,length=101)[-101]
> 
> foo <- function(pars,res,theta) {
> 		eps <- sqrt(.Machine$double.eps)
> 		big <- sqrt(.Machine$double.xmax)
>                 k   <- pars[1]
>                 rho <- pars[2]
> 		# This line is needed in the Nelder-Mead case:
>                 if(k < 0 | rho < 0 | rho > 1) return(big)
>                 Sigma <- k*rho^outer(theta,theta,cdist)
>                 udv   <- svd(Sigma)
>                 d     <- udv$d
>                 if(min(d) < eps) return(big)
>                 w <- t(udv$u)%*%res
> 		# Minus twice the log likelihood:
>                 sum(log(d)) + sum(w^2/d)
>         }
> 
> cdist <- function(theta1,theta2) {
>         d <- abs(theta1-theta2)%%(2*pi)
>         ifelse(d>pi,2*pi-d,d)
> }
> 
> #==+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
> # Script #2:
> #==+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
> 
> #
> # Demonstration of the problem.
> #
> 
> # Using the ``bad'' starting values:
> pars <- c(0.06,0.25)
> R1 <- optim(pars,foo,res=resi,theta=theta, method="L-BFGS-B",
>       lower=c(0,0),upper=c(Inf,1))
> print(R1)
> 
> # Using roughly (rather badly!) estimated starting values:
> pars <- c(var(resi),cor(resi[-1],resi[-100]))
> R2 <- optim(pars,foo,res=resi,theta=theta, method="L-BFGS-B",
>       lower=c(0,0),upper=c(Inf,1))
> print(R2)
> 
> # Using other starting values:
> pars <- c(0.05,0.5)
> R3 <- optim(pars,foo,res=resi,theta=theta, method="L-BFGS-B",
>       lower=c(0,0),upper=c(Inf,1))
> print(R3)
> 
> # Using Nelder-Mead and the ``bad'' starting values.
> pars <- c(0.06,0.25)
> R4 <- optim(pars,foo,res=resi,theta=theta)
> print(R4)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tlumley at u.washington.edu  Tue Nov 15 23:06:31 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 15 Nov 2005 14:06:31 -0800 (PST)
Subject: [R] effect sizes for Wilcoxon tests
In-Reply-To: <Pine.OSX.4.61.0511152206110.19274@cirrus.local>
References: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
	<Pine.OSX.4.61.0511152122290.17240@cirrus.local>
	<x27jb9vdhw.fsf@turmalin.kubism.ku.dk>
	<Pine.OSX.4.61.0511152206110.19274@cirrus.local>
Message-ID: <Pine.LNX.4.63a.0511151355370.20530@homer23.u.washington.edu>

On Tue, 15 Nov 2005, Claus Atzenbeck wrote:

> On Tue, 15 Nov 2005, Peter Dalgaard wrote:
>
>>>> However, for Wilcoxon tests, the formula for effect sizes is:
>>>> r = Z / sqrt(N)
>>>>
>>>> I wonder how I can calculate the Z-score in R for a Wilcoxon test.
>>>
>>> Does anyone know how to calculate effect sizes for Wilcoxon tests as
>>> SPSS can do? I wonder if it would be very difficult to do so with R.
>>
>> If the above formula is correct, it can't be hard. What I wonder is
>> whether (and if so, how) it makes sense. (& the fact that SPSS does it
>> is no guarantee...)
>
> According to my references, the formula is OK.
>

I think you have misinterpreted the direction of Peter's scepticism. The 
question is whether an effect size defined this way means anything useful. 
For example, even if your data are Normally distributed with equal 
variance this definition will not agree with the definition based on the 
mean. This is only the start of the potential problems, especially if the 
distributions do not have the same shape.


> However, how do I get Z from a Wilcoxon test in R?

wtest <- wilcox.test(y~group,data=d, alternative="greater")
qnorm(wtest$p.value)


 	-thomas



From p.dalgaard at biostat.ku.dk  Tue Nov 15 23:07:20 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2005 23:07:20 +0100
Subject: [R] effect sizes for Wilcoxon tests
In-Reply-To: <Pine.OSX.4.61.0511152206110.19274@cirrus.local>
References: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
	<Pine.OSX.4.61.0511152122290.17240@cirrus.local>
	<x27jb9vdhw.fsf@turmalin.kubism.ku.dk>
	<Pine.OSX.4.61.0511152206110.19274@cirrus.local>
Message-ID: <x2y83ptvjb.fsf@turmalin.kubism.ku.dk>

Claus Atzenbeck <claus.atzenbeck at freenet.de> writes:

> On Tue, 15 Nov 2005, Peter Dalgaard wrote:
> 
> > > > However, for Wilcoxon tests, the formula for effect sizes is:
> > > > r = Z / sqrt(N)
> > > >
> > > > I wonder how I can calculate the Z-score in R for a Wilcoxon test.
> > >
> > > Does anyone know how to calculate effect sizes for Wilcoxon tests as
> > > SPSS can do? I wonder if it would be very difficult to do so with R.
> >
> > If the above formula is correct, it can't be hard. What I wonder is
> > whether (and if so, how) it makes sense. (& the fact that SPSS does it
> > is no guarantee...)
> 
> According to my references, the formula is OK.
> 
> However, how do I get Z from a Wilcoxon test in R?

Well it's the quantity that is fed to pnorm in the calculation of the
approximate p-value, so how about inverting it using e.g.

qnorm(wilcox.test(....., exact=FALSE, alternative="less")$p.value)
 
if you don't want to pick stats:::wilcox.test.default apart. You might
also throw in correct=FALSE, but I see no objective argument for
doing so or not.

> (BTW, I have to correct myself: SPSS delivers Z. The effect size was
> calculated by the author of one of my books using the above mentioned
> formula.)
> 
> Claus
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Murraypu at aimnsw.com.au  Tue Nov 15 23:11:25 2005
From: Murraypu at aimnsw.com.au (Murray Pung)
Date: Wed, 16 Nov 2005 09:11:25 +1100
Subject: [R] y-axis in histograms
Message-ID: <3028F4C4647C9043B870276E28C69FD601343863@syd05.aimnsw.com.au>

Just quickly - Try 'ylim'

hist(x, ylim = 100,...)

-----Original Message-----
From: Michael Graber [mailto:michael_graber at gmx.de]
Sent: Wednesday, 16 November 2005 7:10 AM
To: R-Mailingliste
Subject: [R] y-axis in histograms


Dear R- list,
I have some data to present with histograms. Therefore I used hist(...). 
I have few values with almost 80% of
the frequencies (totaly 800) and some other values with low frequencies 
( totaly 5 -10 )
that I want to emphasize. Therefore I want to "cut" the y-axis on 100, 
but I
don't know how to deal with this.

Thanks in advance,

Michael Graber

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ross at biostat.ucsf.edu  Tue Nov 15 23:35:55 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 15 Nov 2005 14:35:55 -0800
Subject: [R] Apparent problem with \eqn
Message-ID: <1132094155.16298.10.camel@iron.psg.net>

  \eqn{{\bf\beta}_j}{b(j)} in my .Rd file produces this error
--------------------------------------------
! Missing $ inserted.
<inserted text> 
                $
l.7 \eqn{{\bf\beta}_j}{\bf\beta}_
                                 jnormal-bracket5bracket-normal{b(j)}
--
! Missing $ inserted.
<inserted text> 
                $
l.16 
     
--
! Missing } inserted.
<inserted text> 
                }
l.16 
     
--
! Extra }, or forgotten \endgroup.
\par ...m \@noitemerr {\@@par }\fi \else {\@@par }
                                                  \fi 
l.16 
-------------------------------
Is this a bug, or am I missing something?  Note that \bf\beta seems to
have been doubled.

Currently on R 2.2.0.final-4 on Debian.  I think I've seen this with
many prior versions too.
     
-- 
Ross Boylan                                      wk:  (415) 514-8146
185 Berry St #5700                               ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 514-8150
University of California, San Francisco
San Francisco, CA 94107-1739                     hm:  (415) 550-1062



From davidr at rhotrading.com  Wed Nov 16 00:02:53 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Tue, 15 Nov 2005 17:02:53 -0600
Subject: [R] origin and "origin<-" functions on chron
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A7CEB96@rhosvr02.rhotrading.com>

I'm trying to use/modify some code I found (at Omegahat, but I've seem similar usage elsewhere.)
It contains the lines:
   if(any(origin(chronDate)!=orig))
      origin(chronDate) <- orig

Let's say:
> require("chron")
[1] TRUE
> chronDate <- chron("11/15/2005", format="m/d/y", origin.=c(12,31,1899))
> orig <- c(month=12, day=31, year=1899)
> origin(chronDate)
Error: couldn't find function "origin"
> origin(chronDate) <- orig
Error: couldn't find function "origin<-"

I'm sure I'm missing something simple here, but what? I've looked in the archives and docs quite a lot....

Thanks for the help!

R-2.2.0 on Windows XP (SP2)

> R.Version()
$platform
[1] "i386-pc-mingw32"
$arch
[1] "i386"
$os
[1] "mingw32"
$system
[1] "i386, mingw32"
$status
[1] ""
$major
[1] "2"
$minor
[1] "2.0"
$year
[1] "2005"
$month
[1] "10"
$day
[1] "06"
$"svn rev"
[1] "35749"
$language
[1] "R"

David L. Reiner
??
Rho Trading
440 S. LaSalle St.
Chicago?? IL?? 60605
312-362-4963
??



From berr0179 at umn.edu  Wed Nov 16 00:43:13 2005
From: berr0179 at umn.edu (Erin Berryman)
Date: Tue, 15 Nov 2005 17:43:13 -0600
Subject: [R] combination xyplot and barchart?
Message-ID: <47ecce16e65cc88e51077229befaf07b@umn.edu>

Dear R community,

I am having trouble determining how to create the graph I want 
utilizing my relatively limited knowledge of R. So far I have been 
using the lattice library to create most of what I need.
The dataset (enviro) consists of 2 variables (Temp and Precip) for each 
Day of a 2-yr period (Year). I wish to display Temp and Precip along 
the y axis plotted by Day on the x axis to allow comparison (one year's 
data in each of 2 panels stacked on top of each other) between the 
years. Essentially what I want it to look like is an xyplot (Temp ~ Day 
| Year, type='l') superimposed onto a barchart(Precip ~ Day | Year, 
horizontal=F), with scales adjusted so one can see detail in both 
variables.
The closest I have come to what I need is by the following code :

library(lattice)
barchart(Precip + Temp ~ Day | Year, data=enviro, layout=c(1,2), 
horizontal=F, origin=0, 
panel=function(x,y,subscripts,...){panel.xyplot(x=enviro$Day, 
y=enviro$Temp, type='l',subscripts=subscripts, ...); 
panel.barchart(x=enviro$Day, y=enviro$Precip, subscripts=subscripts, 
...)})

Two panels are produced; however, both years' data are plotted in each 
panel (panels look identical). And I get this error:

Error in grid.Call.graphics("L_rect", x$x, x$y, x$width, x$height, 
resolveHJust(x$just,  :
	invalid line type

 From the documentation or the help archives, I cannot understand how to:
1) indicate a conditioning variable (Year) for panel.barchart and 
panel.xyplot
2) have 2 y axes with different scales in one panel

Is this even possible to do using lattice? Any insight is greatly 
appreciated.

Thanks,

Erin


Erin M. Berryman
Graduate Research Assistant
Department of Soil, Water, and Climate
University of Minnesota



From ggrothendieck at gmail.com  Wed Nov 16 00:33:37 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 15 Nov 2005 18:33:37 -0500
Subject: [R] origin and "origin<-" functions on chron
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A7CEB96@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A7CEB96@rhosvr02.rhotrading.com>
Message-ID: <971536df0511151533k2a2de54cg7a99f6ef08dcecf8@mail.gmail.com>

chron has a namespace so try this:

   chron:::origin
   getAnywhere("origin<-")

Having said that I would recommend that you don't use origins in chron.
The situation may have changed but when I wrote the Help Desk article
in R News 4/1 I encountered problems with using origins in some
situations and as discussed in the article its so easy to avoid using them
that there is really no good reason I can see not to avoid them.


On 11/15/05, davidr at rhotrading.com <davidr at rhotrading.com> wrote:
> I'm trying to use/modify some code I found (at Omegahat, but I've seem similar usage elsewhere.)
> It contains the lines:
>   if(any(origin(chronDate)!=orig))
>      origin(chronDate) <- orig
>
> Let's say:
> > require("chron")
> [1] TRUE
> > chronDate <- chron("11/15/2005", format="m/d/y", origin.=c(12,31,1899))
> > orig <- c(month=12, day=31, year=1899)
> > origin(chronDate)
> Error: couldn't find function "origin"
> > origin(chronDate) <- orig
> Error: couldn't find function "origin<-"
>
> I'm sure I'm missing something simple here, but what? I've looked in the archives and docs quite a lot....
>
> Thanks for the help!
>
> R-2.2.0 on Windows XP (SP2)
>
> > R.Version()
> $platform
> [1] "i386-pc-mingw32"
> $arch
> [1] "i386"
> $os
> [1] "mingw32"
> $system
> [1] "i386, mingw32"
> $status
> [1] ""
> $major
> [1] "2"
> $minor
> [1] "2.0"
> $year
> [1] "2005"
> $month
> [1] "10"
> $day
> [1] "06"
> $"svn rev"
> [1] "35749"
> $language
> [1] "R"
>
> David L. Reiner
>
> Rho Trading
> 440 S. LaSalle St.
> Chicago IL 60605
> 312-362-4963
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Hong.Ooi at iag.com.au  Wed Nov 16 02:01:38 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 16 Nov 2005 12:01:38 +1100
Subject: [R] Unexpected result of names<-
Message-ID: <200511160102.jAG12FxT008655@hypatia.math.ethz.ch>


_______________________________________________________________________________________

Note: This e-mail is subject to the disclaimer contained at the bottom of this message.
_______________________________________________________________________________________


Hi,

I came across some rather unexpected behaviour the other day with
assigning names and lists. Here's an example.

> z <- list(aaa=1, bbb=2)
> z
$aaa
[1] 1

$bbb
[1] 2

Note that z has members named "aaa" and "bbb". Now:

> names(z$a) <- "X"
> z
$aaa
[1] 1

$bbb
[1] 2

$a
X 
1

I would have expected that trying to name z$a would either give an error
(because z doesn't have an element "a") or would cause z$aaa to be
modified (due to partial name matching).  I didn't expect that a new
list member would be created.

I've checked that this is consistent across SPlus 2000, SPlus 7, and R
2.2 for Windows. Can someone give an explanation for why this is
happening?


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
(02) 9292 1566


_______________________________________________________________________________________

The information transmitted in this message and its attachments (if any) is intended 
only for the person or entity to which it is addressed.
The message may contain confidential and/or privileged material. Any review, 
retransmission, dissemination or other use of, or taking of any action in reliance 
upon this information, by persons or entities other than the intended recipient is 
prohibited.

If you have received this in error, please contact the sender and delete this e-mail 
and associated material from any computer.

The intended recipient of this e-mail may only use, reproduce, disclose or distribute 
the information contained in this e-mail and any attached files, with the permission 
of the sender.

This message has been scanned for viruses with Symantec Scan Engine and cleared by 
MailMarshal.



From cberry at tajo.ucsd.edu  Wed Nov 16 02:24:48 2005
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 15 Nov 2005 17:24:48 -0800
Subject: [R] latex table and R codes
In-Reply-To: <4379BD18.8080507@inw.agrl.ethz.ch>
References: <4379BD18.8080507@inw.agrl.ethz.ch>
Message-ID: <Pine.LNX.4.64.0511151649100.19281@tajo.ucsd.edu>

On Tue, 15 Nov 2005, Mostafa Ghaderi wrote:

> Dear R-help assistance;
> may you help me regrding to following inquiry!?
> you know what, i have generated three tables by xtable R function, right now 
> i am trying to make a single table by putting these tables togethere; actully 
> i am going to come upt with *.tex (latex) file. because i have more extera 
> non-R material, i am using  Sweave to read R instructions, and finally i hope 
> to end up with a single table containing all three tables in one table, would 
> you let me know how i should go for it!?( i mean combining Sweave arguments 
> and Latex expressions)
> thanks for your time;
> Mostafa

There are probably lots of ways to combine xtable()'d R objects to form a 
pleasing result using Sweave.

Some suggestions:

---
in R

> RSiteSearch("Sweave xtable")

will lead to many useful threads (but none specifically addressing your 
question, I think) about formatting tables.

---

capture.output() can be used in a code chunk to save the results of
print.xtable() calls and then later combine them -- see the example below.

see ?capture.output, too.
---

Explicit calls to print.xtable() are helpful in tuning or 
suppressing floats -- also in the example  --- and other things.

see ?print.table and print a copy of the function to see what it is doing 
in more detail.

---

A private copy of print.table() may help you add useful features.

---

You can use Sweave to make files to be included in other *.tex documents, 
but it also works the other way round.

-----

Here is an Sweave example of some of these tactics:

\documentclass{article}

\begin{document}

Start by putting together a (tabular) table:

@
<<make-tab1,echo=T,results=hide>>=

library(xtable)

tab1 <- capture.output(print.xtable(xtable(diag(2)),float=F))

@ %def

And now another table

@
<<make-tab2,echo=T,results=hide>>=

tab2 <- capture.output(print.xtable(xtable(diag(2)+2),float=F))

@ %def

Now print out a super table:


@
<<super-tab,echo=T,eval=F,results=verbatim>>=

cat( tab1, "  & upper right blank  \\\\ ", sep='\n' )

cat( "lower left blank & ", tab2, "\\\\" ,sep='\n' )

@ %def


\begin{tabular}[c]{|c|c|}

@
<<eval-super-tab,echo=F,results=tex>>=

<<super-tab>>

@ %def


\end{tabular}

\end{document}


--
Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717



From deepayan.sarkar at gmail.com  Wed Nov 16 04:09:32 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 15 Nov 2005 21:09:32 -0600
Subject: [R] combination xyplot and barchart?
In-Reply-To: <47ecce16e65cc88e51077229befaf07b@umn.edu>
References: <47ecce16e65cc88e51077229befaf07b@umn.edu>
Message-ID: <eb555e660511151909k5796fc49y691f25709e690b0b@mail.gmail.com>

On 11/15/05, Erin Berryman <berr0179 at umn.edu> wrote:
> Dear R community,
>
> I am having trouble determining how to create the graph I want
> utilizing my relatively limited knowledge of R. So far I have been
> using the lattice library to create most of what I need.
> The dataset (enviro) consists of 2 variables (Temp and Precip) for each
> Day of a 2-yr period (Year). I wish to display Temp and Precip along
> the y axis plotted by Day on the x axis to allow comparison (one year's
> data in each of 2 panels stacked on top of each other) between the
> years. Essentially what I want it to look like is an xyplot (Temp ~ Day
> | Year, type='l') superimposed onto a barchart(Precip ~ Day | Year,
> horizontal=F), with scales adjusted so one can see detail in both
> variables.
> The closest I have come to what I need is by the following code :
>
> library(lattice)
> barchart(Precip + Temp ~ Day | Year, data=enviro, layout=c(1,2),
> horizontal=F, origin=0,
> panel=function(x,y,subscripts,...){panel.xyplot(x=enviro$Day,
> y=enviro$Temp, type='l',subscripts=subscripts, ...);
> panel.barchart(x=enviro$Day, y=enviro$Precip, subscripts=subscripts,
> ...)})
>
> Two panels are produced; however, both years' data are plotted in each
> panel (panels look identical). And I get this error:
>
> Error in grid.Call.graphics("L_rect", x$x, x$y, x$width, x$height,
> resolveHJust(x$just,  :
> 	invalid line type

A reproducible example, even if it's a toy one, would have been helpful.

Your usage is confused. In particular, panel.xyplot ignores the
subscripts argument, so you end up giving exactly the same set of
values to panel.xyplot for both panels (so it's not surprising that
your panels show the same data). It seems that you are looking for
something like the following:


enviro <-
    data.frame(Year = rep(2001:2002, each = 365),
               Day = rep(1:365, 2),
               Precip = pmax(0, rnorm(365 * 2)),
               Temp = 2 + 0.2 * rnorm(365 * 2))


xyplot(Precip + Temp ~ Day | Year, data=enviro,
       layout = c(1, 2),
       panel = panel.superpose.2,
       type = c('h', 'l'))

In case it helps, this is shorthand for

xyplot(Precip + Temp ~ Day | Year, data=enviro,
       layout = c(1, 2),
       panel = function(x, y, groups, subscripts, ...) {
           panel.superpose.2(x = x, y = y,
                             groups = groups,
                             subscripts = subscripts,
                             ...)
       },
       type = c('h', 'l'))

Note that the panel function is defined in terms of arguments it gets,
and does not explicitly refer to any external variables (like the data
frame 'enviro'). If you find yourself writing code that does, it's a
likely sign that you are doing something wrong (or at least
unnecessarily convoluted).


>  From the documentation or the help archives, I cannot understand how to:
> 1) indicate a conditioning variable (Year) for panel.barchart and
> panel.xyplot
> 2) have 2 y axes with different scales in one panel

You can't easily. A panel has one set of scales, and that's it. You
can of course fake it by transforming the relevant part of your data
and adding a set of tick marks with appropriate (fake) labels (see
?panel.axis).

-Deepayan



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Nov 16 09:13:56 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 16 Nov 2005 09:13:56 +0100 (CET)
Subject: [R] effect sizes for Wilcoxon tests
In-Reply-To: <Pine.LNX.4.63a.0511151355370.20530@homer23.u.washington.edu>
References: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
	<Pine.OSX.4.61.0511152122290.17240@cirrus.local>
	<x27jb9vdhw.fsf@turmalin.kubism.ku.dk>
	<Pine.OSX.4.61.0511152206110.19274@cirrus.local>
	<Pine.LNX.4.63a.0511151355370.20530@homer23.u.washington.edu>
Message-ID: <Pine.LNX.4.51.0511160910230.24561@artemis.imbe.med.uni-erlangen.de>



On Tue, 15 Nov 2005, Thomas Lumley wrote:

> On Tue, 15 Nov 2005, Claus Atzenbeck wrote:
>
> > On Tue, 15 Nov 2005, Peter Dalgaard wrote:
> >
> >>>> However, for Wilcoxon tests, the formula for effect sizes is:
> >>>> r = Z / sqrt(N)
> >>>>
> >>>> I wonder how I can calculate the Z-score in R for a Wilcoxon test.
> >>>
> >>> Does anyone know how to calculate effect sizes for Wilcoxon tests as
> >>> SPSS can do? I wonder if it would be very difficult to do so with R.
> >>
> >> If the above formula is correct, it can't be hard. What I wonder is
> >> whether (and if so, how) it makes sense. (& the fact that SPSS does it
> >> is no guarantee...)
> >
> > According to my references, the formula is OK.
> >
>
> I think you have misinterpreted the direction of Peter's scepticism. The
> question is whether an effect size defined this way means anything useful.
> For example, even if your data are Normally distributed with equal
> variance this definition will not agree with the definition based on the
> mean. This is only the start of the potential problems, especially if the
> distributions do not have the same shape.
>
>
> > However, how do I get Z from a Wilcoxon test in R?
>
> wtest <- wilcox.test(y~group,data=d, alternative="greater")
> qnorm(wtest$p.value)
>

or

library("coin")
statistic(wilcox_test(y ~ group, data = d, ...), type = "standardized")

where the variance `estimator' takes care of tied observations.

Best,

Torsten

>
>  	-thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From claus.atzenbeck at freenet.de  Wed Nov 16 10:34:11 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Wed, 16 Nov 2005 10:34:11 +0100 (CET)
Subject: [R] effect sizes for Wilcoxon tests
In-Reply-To: <Pine.LNX.4.63a.0511151355370.20530@homer23.u.washington.edu>
References: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
	<Pine.OSX.4.61.0511152122290.17240@cirrus.local>
	<x27jb9vdhw.fsf@turmalin.kubism.ku.dk>
	<Pine.OSX.4.61.0511152206110.19274@cirrus.local>
	<Pine.LNX.4.63a.0511151355370.20530@homer23.u.washington.edu>
Message-ID: <Pine.OSX.4.61.0511160917420.675@cirrus.aue.aau.dk>

On Tue, 15 Nov 2005, Thomas Lumley wrote:

> I think you have misinterpreted the direction of Peter's scepticism.
> The question is whether an effect size defined this way means anything
> useful. For example, even if your data are Normally distributed with
> equal variance this definition will not agree with the definition
> based on the mean. This is only the start of the potential problems,
> especially if the distributions do not have the same shape.

I see the point. The book that I use (it is a kind of practical guide)
calculates the effect sizes of Mann-Whitney tests  and compares it to
Cohen's benchmarks (small effect: r=.10 -- medium effect: r=.30 -- large
effect: r=.50). This was also my plan. However, as far as I understand
you and Peter, the formula is not appropriate for comparison with
Cohen's benchmarks?

> > However, how do I get Z from a Wilcoxon test in R?
>
> wtest <- wilcox.test(y~group,data=d, alternative="greater")
> qnorm(wtest$p.value)

Thanks, also to Peter.

Claus



From maechler at stat.math.ethz.ch  Wed Nov 16 11:04:18 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 16 Nov 2005 11:04:18 +0100
Subject: [R] www.krankenversicherung.ch News - Krankenkassen -
 Information Newsletter
In-Reply-To: <47fce0650511150710y408e996dx@mail.gmail.com>
References: <200511141045109.SM00940@195.141.204.149>
	<47fce0650511150710y408e996dx@mail.gmail.com>
Message-ID: <17275.1058.104817.93218@stat.math.ethz.ch>

>>>>> "Hans-Peter" == Hans-Peter  <gchappi at gmail.com>
>>>>>     on Tue, 15 Nov 2005 16:10:35 +0100 writes:

    Hans-Peter> Am 14.11.05 schrieb krankenversicherung at help.ch <krankenversicherung at help.ch>:
    >> [snip some spam]

    Hans-Peter> Being located only some blocks away from these spammers, I just asked
    Hans-Peter> them in person to remove the R list from their database.


Vielen Dank,
Hans-Peter.

Es betrifft auch noch andere  R-<..>@stat.math.ethz.ch  
Listen.

Und dann k??nnte man doch gleich alle @...ethz.ch auch streichen...

Aber eigentlich m??ssten die davon ??berzeugt werden k??nnen, nicht
mehr zu spammen.

Mit liebem Gruss,
Martin M??chler
(R- Mailing List Administrator)

Martin <Maechler at stat.math.ethz.ch>  http://stat.ethz.ch/people/maechler
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: +41-44-632-3408		fax: ...-1228			<><



From koen.hufkens at telenet.be  Wed Nov 16 11:03:43 2005
From: koen.hufkens at telenet.be (Koen Hufkens)
Date: Wed, 16 Nov 2005 11:03:43 +0100
Subject: [R] spatial statistics on images, any packages?
Message-ID: <437B03FF.4050208@telenet.be>

Hi list,

Is there a package that covers the evaluation of spatial statistics on 
images and not on point data? I've converted an image matrix to x, y 
coordinates and a measurement value but evaluation with the package 
spdep (not really designed for image data I suppose) is unworkable. Any 
suggestions?

Regards,
Koen



From r.hankin at noc.soton.ac.uk  Wed Nov 16 11:10:27 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 16 Nov 2005 10:10:27 +0000
Subject: [R] matrix assembly
Message-ID: <1A3766A5-32BB-4EEA-B9AC-643B57726A62@soc.soton.ac.uk>

Hi

I have a function f(k,l) which returns a matrix for integer k and l.
I want to call f(.,.) for each combination of a supplied vector (as  
in expand.grid())
and then I want to assemble these matrices into one big one using
k and l to index the position of the submatrices returned by f(k,l).

Toy example follows.


f <- function(k,l){
matrix(k+l , k , l)
}

and I want to call f() with each combination of c(1,3)
for the first argument and c(2,4,5) for the second
and then assemble the resulting matrices.

I can do it piecemeal:

a <- c(1,3)
b <- c(2,4,5)
expand.grid(a,b)
   Var1 Var2
1    1    2
2    3    2
3    1    4
4    3    4
5    1    5
6    3    5
  cbind(rbind(f(1,2),f(3,2)) , rbind(f(1,4),f(3,4)) , rbind(f(1,5),f 
(3,5)))
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
[1,]    3    3    5    5    5    5    6    6    6     6     6
[2,]    5    5    7    7    7    7    8    8    8     8     8
[3,]    5    5    7    7    7    7    8    8    8     8     8
[4,]    5    5    7    7    7    7    8    8    8     8     8

[see how the calls to f(. , .) follow the rows of expand.grid(a,b)]


How to do this in a nice vectorized manner?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From p.dalgaard at biostat.ku.dk  Wed Nov 16 11:20:02 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Nov 2005 11:20:02 +0100
Subject: [R] effect sizes for Wilcoxon tests
In-Reply-To: <Pine.LNX.4.51.0511160910230.24561@artemis.imbe.med.uni-erlangen.de>
References: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
	<Pine.OSX.4.61.0511152122290.17240@cirrus.local>
	<x27jb9vdhw.fsf@turmalin.kubism.ku.dk>
	<Pine.OSX.4.61.0511152206110.19274@cirrus.local>
	<Pine.LNX.4.63a.0511151355370.20530@homer23.u.washington.edu>
	<Pine.LNX.4.51.0511160910230.24561@artemis.imbe.med.uni-erlangen.de>
Message-ID: <x21x1gyjvx.fsf@viggo.kubism.ku.dk>

Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de> writes:
[snip]

> > > However, how do I get Z from a Wilcoxon test in R?
> >
> > wtest <- wilcox.test(y~group,data=d, alternative="greater")
> > qnorm(wtest$p.value)
> >
> 
> or
> 
> library("coin")
> statistic(wilcox_test(y ~ group, data = d, ...), type = "standardized")
> 
> where the variance `estimator' takes care of tied observations.

Doesn't it do that in the same way as inside wilcox.test(...,exact=FALSE)?

Just wondering.

        -p

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From chris at willmot.org.uk  Wed Nov 16 11:46:43 2005
From: chris at willmot.org.uk (Christopher Willmot)
Date: Wed, 16 Nov 2005 10:46:43 +0000
Subject: [R] Histogram font
Message-ID: <op.s0bxj5my56vhoe@skylark.home.net>

The hist() command produces this message on my machine...

Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
         X11 font at size 14 could not be loaded

How can I either (a) determine what font is required,
  or (b) specify one of the fonts I have available?

This problem is specific to hist(), plot() works fine.
I am using R on SuSE Linux v9.3, from the KDE desktop.

-- 
Christopher Willmot



From maechler at stat.math.ethz.ch  Wed Nov 16 11:53:31 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 16 Nov 2005 11:53:31 +0100
Subject: [R] www.... News - Krankenkassen - ....
In-Reply-To: <17275.1058.104817.93218@stat.math.ethz.ch>
References: <200511141045109.SM00940@195.141.204.149>
	<47fce0650511150710y408e996dx@mail.gmail.com>
	<17275.1058.104817.93218@stat.math.ethz.ch>
Message-ID: <17275.4011.635449.500881@stat.math.ethz.ch>

Sorry to everyone -- I'm deeply embarrassed that this private
reply (to Hans-Peter) accidentally went to the whole R-help list.

It's bad enough (and inavoidable as long as we keep the list
open) that real spam occasionally goes through, but 
of all people, I should know better and be more careful ....

Martin Maechler, ETH Zurich



From torsten at hothorn.de  Wed Nov 16 11:58:09 2005
From: torsten at hothorn.de (torsten@hothorn.de)
Date: Wed, 16 Nov 2005 11:58:09 +0100 (CET)
Subject: [R] effect sizes for Wilcoxon tests
In-Reply-To: <x21x1gyjvx.fsf@viggo.kubism.ku.dk>
References: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
	<Pine.OSX.4.61.0511152122290.17240@cirrus.local>
	<x27jb9vdhw.fsf@turmalin.kubism.ku.dk>
	<Pine.OSX.4.61.0511152206110.19274@cirrus.local>
	<Pine.LNX.4.63a.0511151355370.20530@homer23.u.washington.edu>
	<Pine.LNX.4.51.0511160910230.24561@artemis.imbe.med.uni-erlangen.de>
	<x21x1gyjvx.fsf@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.51.0511161127430.28580@artemis.imbe.med.uni-erlangen.de>


On Wed, 16 Nov 2005, Peter Dalgaard wrote:

> Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de> writes:
> [snip]
>
> > > > However, how do I get Z from a Wilcoxon test in R?
> > >
> > > wtest <- wilcox.test(y~group,data=d, alternative="greater")
> > > qnorm(wtest$p.value)
> > >
> >
> > or
> >
> > library("coin")
> > statistic(wilcox_test(y ~ group, data = d, ...), type = "standardized")
> >
> > where the variance `estimator' takes care of tied observations.
>
> Doesn't it do that in the same way as inside wilcox.test(...,exact=FALSE)?
>

My understanding was that `wilcox.test' implements the unconditional version
(with unconditional variance estimator and some `adjustment' for ties) and
`wilcox_test' implements the conditional version of the test (of course both
coincide when there are no ties).

However, some quick experiments suggest that the standardized statistic is
the same for both versions (with correct = FALSE) for tied observations.
One needs to check if the expectation and variance formulae in
`wilcox.test' are equivalent with the conditional versions used in
`wilcox_test' (in contrast to my initial opinion).

Best,

Torsten

> Just wondering.
>
>         -p
>
> --
>    O__  ---- Peter Dalgaard             Øster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>



From f.calboli at imperial.ac.uk  Wed Nov 16 12:10:58 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 16 Nov 2005 11:10:58 +0000
Subject: [R] natural selection coefficient S
Message-ID: <1132139458.3978.22.camel@localhost.localdomain>

Hi everyone,

before I embark in some coding, can I ask if there is any function in
any of the packages on CRAN dealing with the natural selection
coefficient S?

At the moment I am more curious to see it has been implemented already
in R, rather than what specific implementation.

Regards,

Federico Calboli
-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From abigi at agrsci.unibo.it  Wed Nov 16 12:24:26 2005
From: abigi at agrsci.unibo.it (Alessandro Bigi)
Date: Wed, 16 Nov 2005 12:24:26 +0100
Subject: [R] Friedman test with replicated samples
Message-ID: <6.0.0.22.0.20051116121400.01c59840@pop.agrsci.unibo.it>

Dear R-users,
	I would need to do a Friedman test with replicated samples, and 
Friedman.test(y...) currently works only for unreplicated designs.
Is there a script or a function available?
Thanks,

	Alessandro


-- 







--



From bitwrit at ozemail.com.au  Thu Nov 17 04:33:27 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 16 Nov 2005 22:33:27 -0500
Subject: [R] y-axis in histograms
In-Reply-To: <437A409C.1060005@gmx.de>
References: <437A409C.1060005@gmx.de>
Message-ID: <437BFA07.6070303@ozemail.com.au>

Michael Graber wrote:
> Dear R- list,
> I have some data to present with histograms. Therefore I used hist(...). 
> I have few values with almost 80% of
> the frequencies (totaly 800) and some other values with low frequencies 
> ( totaly 5 -10 )
> that I want to emphasize. Therefore I want to "cut" the y-axis on 100, 
> but I
> don't know how to deal with this.
> 
If you mean that you would like to have an axis break from 10 to 100, 
you can try this:

# be creative, make up some data
fakedat<-c(sample(1:6,50,TRUE),sample(7:9,1000,TRUE))
# get the histogram counts
# note that I have to explicitly specify breaks
fakehist<-hist(fakedat,breaks=0:10,type="n")
# here's a rough function
gap.barplot<-function(y,gap,x=NA,xlabels=NA,col=NULL,...) {
  if(missing(y) || missing(gap))
   stop("Must pass at least y and gap")
  littleones<-which(y<=gap[1])
  bigones<-which(y>=gap[2])
  if(length(bigones)+length(littleones) != length(y))
   warning("gap includes some values of y")
  gapsize<-gap[2]-gap[1]
  if(is.na(x)) x<-1:length(y)
  if(is.na(xlabels)) xlabels<-as.character(x)
  xlim<-range(x)
  ylim<-c(min(y),max(y)-gapsize)
  plot(0,xlim=xlim,ylim=ylim,axes=FALSE,type="n",...)
  box()
  axis(1,at=x,labels=xlabels)
  ytics<-pretty(y)
  littletics<-which(ytics<gap[1])
  bigtics<-which(ytics>=gap[2])
  axis(2,at=c(ytics[littletics],ytics[bigtics]-gapsize),
   labels=c(ytics[littletics],ytics[bigtics]))
  axis.break(2,gap[1])
  halfwidth<-min(diff(x))/2
  plot.lim<-par("usr")
  rect(x-halfwidth,plot.lim[3],x+halfwidth,
   c(y[littleones],y[bigones]-gapsize),
   col=col)
  abline(h=gap[1],col="white",lwd=5)
}
gap.barplot(fakehist$counts,gap=c(100,295))

Jim



From Roger.Bivand at nhh.no  Wed Nov 16 12:53:29 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 16 Nov 2005 12:53:29 +0100 (CET)
Subject: [R] spatial statistics on images, any packages?
In-Reply-To: <437B03FF.4050208@telenet.be>
Message-ID: <Pine.LNX.4.44.0511161249580.1056-100000@reclus.nhh.no>

On Wed, 16 Nov 2005, Koen Hufkens wrote:

> Hi list,
> 
> Is there a package that covers the evaluation of spatial statistics on 
> images and not on point data? I've converted an image matrix to x, y 
> coordinates and a measurement value but evaluation with the package 
> spdep (not really designed for image data I suppose) is unworkable. Any 
> suggestions?

It depends what you are trying to do, also please consider re-posting on 
the R-sig-geo list. Yes, constructing a neighbour list for a large image - 
millions of points and thus potentially very many neighbours - is not what 
spdep is dimensioned for. Advice will depend on your research problem.

> 
> Regards,
> Koen
> 
-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Roger.Bivand at nhh.no  Wed Nov 16 12:55:55 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 16 Nov 2005 12:55:55 +0100 (CET)
Subject: [R] Histogram font
In-Reply-To: <op.s0bxj5my56vhoe@skylark.home.net>
Message-ID: <Pine.LNX.4.44.0511161254450.1056-100000@reclus.nhh.no>

On Wed, 16 Nov 2005, Christopher Willmot wrote:

> The hist() command produces this message on my machine...
> 
> Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
>          X11 font at size 14 could not be loaded
> 
> How can I either (a) determine what font is required,
>   or (b) specify one of the fonts I have available?
> 
> This problem is specific to hist(), plot() works fine.
> I am using R on SuSE Linux v9.3, from the KDE desktop.

Try searching the mailing list, this feels like:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/20511.html

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Roger.Bivand at nhh.no  Wed Nov 16 13:00:18 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 16 Nov 2005 13:00:18 +0100 (CET)
Subject: [R] Replace missing values in spatial design using moving
	average
In-Reply-To: <5.2.0.9.0.20051109171407.02087318@orleans.inra.fr>
Message-ID: <Pine.LNX.4.44.0511161256520.1056-100000@reclus.nhh.no>

On Wed, 9 Nov 2005, Arnaud Dowkiw wrote:

> Dera R helpers,
> 
> I have a (x,y,z) data file where x and y are spatial coordinates and z a 
> variable. I have some missing values in the z column and I would like to 
> replace them with an optimized estimation from the neighbour cells. I could 
> not find any function in R to do that. Is anybody aware of such function ? 
> Maybe someone knows how to implement the Papadakis method in R...
> Thanks for your help,

I'm not sure about the Papadakis method, I don't think it is there, though 
could be implemented using standard tools. Ripley (1981) relates the 
method to SAR and CAR models, but predicting with them is far from easy 
(there are papers by Martin from the 1980's about this). Are 
geostatistical methods an option for you? That is, what is the support of 
your cells?

> 
> Arnaud DOWKIW
> 
> - - - - - - - - - - - - - - - - - - - - - - -
> Arnaud DOWKIW
> INRA
> Unit?? Am??lioration G??n??tique et Physiologie Foresti??res
> 2163 avenue de la Pomme de Pin
> BP 20619 ARDON
> 45166 OLIVET CEDEX
> FRANCE
> Tel. + 33 2 38 41 78 00
> Fax. + 33 2 38 41 48 09
> - - - - - - - - - - - - - - - - - - - - - - -
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From p.dalgaard at biostat.ku.dk  Wed Nov 16 13:12:35 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Nov 2005 13:12:35 +0100
Subject: [R] effect sizes for Wilcoxon tests
In-Reply-To: <Pine.LNX.4.51.0511161127430.28580@artemis.imbe.med.uni-erlangen.de>
References: <Pine.OSX.4.61.0511141401380.329@cirrus.aue.aau.dk>
	<Pine.OSX.4.61.0511152122290.17240@cirrus.local>
	<x27jb9vdhw.fsf@turmalin.kubism.ku.dk>
	<Pine.OSX.4.61.0511152206110.19274@cirrus.local>
	<Pine.LNX.4.63a.0511151355370.20530@homer23.u.washington.edu>
	<Pine.LNX.4.51.0511160910230.24561@artemis.imbe.med.uni-erlangen.de>
	<x21x1gyjvx.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.51.0511161127430.28580@artemis.imbe.med.uni-erlangen.de>
Message-ID: <x2oe4kx03w.fsf@viggo.kubism.ku.dk>

torsten at hothorn.de writes:

> On Wed, 16 Nov 2005, Peter Dalgaard wrote:
> 
> > Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de> writes:
> > [snip]
> >
> > > > > However, how do I get Z from a Wilcoxon test in R?
> > > >
> > > > wtest <- wilcox.test(y~group,data=d, alternative="greater")
> > > > qnorm(wtest$p.value)
> > > >
> > >
> > > or
> > >
> > > library("coin")
> > > statistic(wilcox_test(y ~ group, data = d, ...), type = "standardized")
> > >
> > > where the variance `estimator' takes care of tied observations.
> >
> > Doesn't it do that in the same way as inside wilcox.test(...,exact=FALSE)?
> >
> 
> My understanding was that `wilcox.test' implements the unconditional version
> (with unconditional variance estimator and some `adjustment' for ties) and
> `wilcox_test' implements the conditional version of the test (of course both
> coincide when there are no ties).
> 
> However, some quick experiments suggest that the standardized statistic is
> the same for both versions (with correct = FALSE) for tied observations.
> One needs to check if the expectation and variance formulae in
> `wilcox.test' are equivalent with the conditional versions used in
> `wilcox_test' (in contrast to my initial opinion).


I think you'll find that they are the same. There isn't really an
unconditional variance formula in the presence of ties - I don't think
you can do that without knowing what the point masses are in the
underlying distribution. The question is only whether the tie
corrected statistic is an asymptotic approximation or an exact formula
for the variance. I believe it is the latter.

What you need to calculate is the expectation and variance of the
(possibly tied) rank of a particular observation, given the sets of
tied observations. In principle, also the covariance between two of
them, but this is easily seen to be equal to -1/(N-1) times the
variance since they are all equal and the rows/columns of the
covariance sums to zero.

The expectation is a no-brainer: tie-breaking preserves the sum of
ranks so the average rank is left unchanged by ties. 

The fun bit is trying to come up with an elegant argument why the
correction term for the variances, involving sum(NTIES.CI^3 -
NTIES.CI) is exact. I think you can do it by saying that breaking a
set of tied ranks randomly corresponds to adding a term which has a
variance related to that of a random number between 1 and d, with
probability d/N . Notice that sum((1:d)^2) - sum(1:d)^2 is (d^3-d)/3.
After breaking the ties at random, you should end up with the untied
situation, so you get the tied variance by subtracting the variance of
the tie-breaking terms.

Tying up the loose ends is left as an exercise....


> Best,
> 
> Torsten
> 
> > Just wondering.
> >
> >         -p
> >
> > --
> >    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> >
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dimitris.rizopoulos at med.kuleuven.be  Wed Nov 16 13:17:51 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 16 Nov 2005 13:17:51 +0100
Subject: [R] matrix assembly
References: <1A3766A5-32BB-4EEA-B9AC-643B57726A62@soc.soton.ac.uk>
Message-ID: <00d401c5eaa7$c3b479f0$0540210a@www.domain>

a crude approach is the following:

f <- function(x){
    k <- x[1]
    l <- x[2]
    matrix(k + l, k , l)
}
a <- c(1, 3)
b <- c(2, 4, 5)
combs <- expand.grid(a, b)

do.call("cbind", lapply(split(combs, combs$Var2), function(x) 
do.call("rbind", apply(x, 1, f))))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Robin Hankin" <r.hankin at noc.soton.ac.uk>
To: "RHelp" <r-help at stat.math.ethz.ch>
Sent: Wednesday, November 16, 2005 11:10 AM
Subject: [R] matrix assembly


> Hi
>
> I have a function f(k,l) which returns a matrix for integer k and l.
> I want to call f(.,.) for each combination of a supplied vector (as
> in expand.grid())
> and then I want to assemble these matrices into one big one using
> k and l to index the position of the submatrices returned by f(k,l).
>
> Toy example follows.
>
>
> f <- function(k,l){
> matrix(k+l , k , l)
> }
>
> and I want to call f() with each combination of c(1,3)
> for the first argument and c(2,4,5) for the second
> and then assemble the resulting matrices.
>
> I can do it piecemeal:
>
> a <- c(1,3)
> b <- c(2,4,5)
> expand.grid(a,b)
>   Var1 Var2
> 1    1    2
> 2    3    2
> 3    1    4
> 4    3    4
> 5    1    5
> 6    3    5
>  cbind(rbind(f(1,2),f(3,2)) , rbind(f(1,4),f(3,4)) , rbind(f(1,5),f
> (3,5)))
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
> [1,]    3    3    5    5    5    5    6    6    6     6     6
> [2,]    5    5    7    7    7    7    8    8    8     8     8
> [3,]    5    5    7    7    7    7    8    8    8     8     8
> [4,]    5    5    7    7    7    7    8    8    8     8     8
>
> [see how the calls to f(. , .) follow the rows of expand.grid(a,b)]
>
>
> How to do this in a nice vectorized manner?
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From f_bresson at yahoo.fr  Wed Nov 16 13:26:45 2005
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Wed, 16 Nov 2005 13:26:45 +0100 (CET)
Subject: [R] numericDeriv
Message-ID: <20051116122645.61497.qmail@web26805.mail.ukl.yahoo.com>

I have to compute some standard errors using the delta
method and so have to use the command "numericDeriv"
to get the desired gradient. Befor using it on my
complicated function, I've done a try with a simple
exemple :

x <- 1:5
numericDeriv(quote(x^2),"x")

and i get :

[1]   1   8  27  64 125 216
attr(,"gradient")
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]  Inf    0    0  NaN    0    0
[2,]    0    0    0  NaN    0    0
[3,]    0  Inf    0  NaN    0    0
[4,]    0    0    0  NaN    0    0
[5,]    0    0  Inf  NaN    0    0
[6,]    0    0    0  NaN    0    0

I don't understand the result. I thought I will get :

[1]   1   8  27  64 125 216
attr(,"gradient")
     [,1]
[1,]  1
[2,]  4
[3,]  6
[4,]  8
[5,]  10
[6,]  12

The derivative of x^2 is still 2x, isn't it ?

Thanks for help



From ripley at stats.ox.ac.uk  Wed Nov 16 13:27:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Nov 2005 12:27:02 +0000 (GMT)
Subject: [R] Histogram font
In-Reply-To: <op.s0bxj5my56vhoe@skylark.home.net>
References: <op.s0bxj5my56vhoe@skylark.home.net>
Message-ID: <Pine.LNX.4.61.0511161221320.11774@gannet.stats>

On Wed, 16 Nov 2005, Christopher Willmot wrote:

> The hist() command produces this message on my machine...
>
> Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
>         X11 font at size 14 could not be loaded
>
> How can I either (a) determine what font is required,
>  or (b) specify one of the fonts I have available?

?options (see option X11fonts).

> This problem is specific to hist(), plot() works fine.
> I am using R on SuSE Linux v9.3, from the KDE desktop.

This usually means that 100dpi fonts are missing or not in the X11 font 
path.  The code assumes that the standard 75dpi and where necessary 100dpi 
fonts are installed.

The issue is title(), not hist() and not plot() as the error message 
shows.  So calling plot(main=) would also fail.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Wed Nov 16 13:27:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 16 Nov 2005 07:27:15 -0500
Subject: [R] matrix assembly
In-Reply-To: <1A3766A5-32BB-4EEA-B9AC-643B57726A62@soc.soton.ac.uk>
References: <1A3766A5-32BB-4EEA-B9AC-643B57726A62@soc.soton.ac.uk>
Message-ID: <971536df0511160427v1a49145ag792559d58f3a4829@mail.gmail.com>

On 11/16/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi
>
> I have a function f(k,l) which returns a matrix for integer k and l.
> I want to call f(.,.) for each combination of a supplied vector (as
> in expand.grid())
> and then I want to assemble these matrices into one big one using
> k and l to index the position of the submatrices returned by f(k,l).
>
> Toy example follows.
>
>
> f <- function(k,l){
> matrix(k+l , k , l)
> }
>
> and I want to call f() with each combination of c(1,3)
> for the first argument and c(2,4,5) for the second
> and then assemble the resulting matrices.
>
> I can do it piecemeal:
>
> a <- c(1,3)
> b <- c(2,4,5)
> expand.grid(a,b)
>   Var1 Var2
> 1    1    2
> 2    3    2
> 3    1    4
> 4    3    4
> 5    1    5
> 6    3    5
>  cbind(rbind(f(1,2),f(3,2)) , rbind(f(1,4),f(3,4)) , rbind(f(1,5),f
> (3,5)))
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
> [1,]    3    3    5    5    5    5    6    6    6     6     6
> [2,]    5    5    7    7    7    7    8    8    8     8     8
> [3,]    5    5    7    7    7    7    8    8    8     8     8
> [4,]    5    5    7    7    7    7    8    8    8     8     8
>
> [see how the calls to f(. , .) follow the rows of expand.grid(a,b)]
>
>
> How to do this in a nice vectorized manner?
>

Replacing your example with the appropriate do.call and lapply
calls gives this:

do.call("cbind", lapply(b, function(x) do.call("rbind", lapply(a, f, l = x))))

or one can do the cbind in the inner loop and the rbind in the outer
giving the same result:

 do.call("rbind", lapply(a, function(x) do.call("cbind", lapply(b, f, k = x))))



From tschoenhoff at gmail.com  Wed Nov 16 13:58:29 2005
From: tschoenhoff at gmail.com (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Wed, 16 Nov 2005 13:58:29 +0100
Subject: [R] www.... News - Krankenkassen - ....
In-Reply-To: <17275.4011.635449.500881@stat.math.ethz.ch>
References: <200511141045109.SM00940@195.141.204.149>
	<47fce0650511150710y408e996dx@mail.gmail.com>
	<17275.1058.104817.93218@stat.math.ethz.ch>
	<17275.4011.635449.500881@stat.math.ethz.ch>
Message-ID: <5ad2dec0511160458l1d5b52dfv@mail.gmail.com>

Hello Martin,

2005/11/16, Martin Maechler <maechler at stat.math.ethz.ch>:
> Sorry to everyone -- I'm deeply embarrassed that this private
> reply (to Hans-Peter) accidentally went to the whole R-help list.
>
> It's bad enough (and inavoidable as long as we keep the list
> open) that real spam occasionally goes through, but
> of all people, I should know better and be more careful ....

In the end there is only a very small segment of spam getting the spam
filters, not to say: Great job, no reason to worry.
After all, finally everone has the possiblity to also kill spam in its
only mail client aterwards!
On this occasion, thanks for setting up and running this good and helpful list.

sincerely

Thomas



From ripley at stats.ox.ac.uk  Wed Nov 16 14:05:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Nov 2005 13:05:55 +0000 (GMT)
Subject: [R] numericDeriv
In-Reply-To: <20051116122645.61497.qmail@web26805.mail.ukl.yahoo.com>
References: <20051116122645.61497.qmail@web26805.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0511161301510.12485@gannet.stats>

On Wed, 16 Nov 2005, Florent Bresson wrote:

> I have to compute some standard errors using the delta
> method and so have to use the command "numericDeriv"
> to get the desired gradient. Befor using it on my
> complicated function, I've done a try with a simple
> exemple :
>
> x <- 1:5
> numericDeriv(quote(x^2),"x")
>
> and i get :
>
> [1]   1   8  27  64 125 216
> attr(,"gradient")
>     [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]  Inf    0    0  NaN    0    0
> [2,]    0    0    0  NaN    0    0
> [3,]    0  Inf    0  NaN    0    0
> [4,]    0    0    0  NaN    0    0
> [5,]    0    0  Inf  NaN    0    0
> [6,]    0    0    0  NaN    0    0
>
> I don't understand the result. I thought I will get :
>
> [1]   1   8  27  64 125 216
> attr(,"gradient")
>     [,1]
> [1,]  1
> [2,]  4
> [3,]  6
> [4,]  8
> [5,]  10
> [6,]  12
>
> The derivative of x^2 is still 2x, isn't it ?

and (1:5)^2 is still

[1]  1  4  9 16 25

!

Try

> x <- as.numeric(1:5)
> numericDeriv(quote(x^2),"x")

since the author of numericDeriv has forgotten some coercions.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From chrysopa at gmail.com  Wed Nov 16 14:04:30 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Wed, 16 Nov 2005 11:04:30 -0200
Subject: [R] Difficulties with for() {while(){}}
Message-ID: <200511161104.31083.chrysopa@gmail.com>

Hi,

I have the follow function:

function() {

  ## Init of function
  ...

  for(i in test) {
    ...

    while(j <= test2) {
    ...

    }
  }
}

The problem is that sometimes, naturally, the while is not possible to be 
resolved, and so the program abort.

In this case I need that program return to the init of function and run again.

How I can make this? Abort the while, abort the for and run the function 
again?

Thanks
Ronaldo
-- 
	Os homens ficam terrivelmente chatos quando sao bons 
	maridos, e abominavelmente convencidos quando nao 
	sao.
		-- Oscar Wilde 
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 16 14:11:51 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 16 Nov 2005 13:11:51 -0000 (GMT)
Subject: [R] numericDeriv
In-Reply-To: <20051116122645.61497.qmail@web26805.mail.ukl.yahoo.com>
Message-ID: <XFMail.051116131151.Ted.Harding@nessie.mcc.ac.uk>

On 16-Nov-05 Florent Bresson wrote:
> I have to compute some standard errors using the delta
> method and so have to use the command "numericDeriv"
> to get the desired gradient. Befor using it on my
> complicated function, I've done a try with a simple
> exemple :
> 
> x <- 1:5
> numericDeriv(quote(x^2),"x")
> 
> and i get :
> 
> [1]   1   8  27  64 125 216
> attr(,"gradient")
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]  Inf    0    0  NaN    0    0
> [2,]    0    0    0  NaN    0    0
> [3,]    0  Inf    0  NaN    0    0
> [4,]    0    0    0  NaN    0    0
> [5,]    0    0  Inf  NaN    0    0
> [6,]    0    0    0  NaN    0    0
> 
> I don't understand the result. I thought I will get :
> 
> [1]   1   8  27  64 125 216
> attr(,"gradient")
>      [,1]
> [1,]  1
> [2,]  4
> [3,]  6
> [4,]  8
> [5,]  10
> [6,]  12
> 
> The derivative of x^2 is still 2x, isn't it ?

The trap you've fallen into is that "x <- 1:5" makes x of
integer type, and (believe it or not) you cannot differentiate
when the support of a function is the integers. Wrong topology
(though I'm not sure that this is quite how R thinks about it).

So give x a bit of elbow-room ("numeric" type has "continous"
-- well, nearly -- topology):

> x <- as.numeric(1:5)
> numericDeriv(quote(x^2),"x")
[1]  1  4  9 16 25
attr(,"gradient")
     [,1] [,2] [,3] [,4] [,5]
[1,]    2    0    0    0    0
[2,]    0    4    0    0    0
[3,]    0    0    6    0    0
[4,]    0    0    0    8    0
[5,]    0    0    0    0   10

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Nov-05                                       Time: 13:11:49
------------------------------ XFMail ------------------------------



From Matthias.Templ at statistik.gv.at  Wed Nov 16 14:32:09 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 16 Nov 2005 14:32:09 +0100
Subject: [R] Difficulties with for() {while(){}}
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BACEF@xchg1.statistik.local>

Is this possible:

function(){
  initialize <- function( ){ initialize }
  for(i in test){
   ...
   if( j <= test2 ) {i <- 1; initialize()} 
   ...
  }

Best,
Matthias

> Hi,
> 
> I have the follow function:
> 
> function() {
> 
>   ## Init of function
>   ...
> 
>   for(i in test) {
>     ...
> 
>     while(j <= test2) {
>     ...
> 
>     }
>   }
> }
> 
> The problem is that sometimes, naturally, the while is not 
> possible to be 
> resolved, and so the program abort.
> 
> In this case I need that program return to the init of 
> function and run again.
> 
> How I can make this? Abort the while, abort the for and run 
> the function 
> again?
> 
> Thanks
> Ronaldo
> -- 
> 	Os homens ficam terrivelmente chatos quando sao bons 
> 	maridos, e abominavelmente convencidos quando nao 
> 	sao.
> 		-- Oscar Wilde 
> --
> |>   // | \\   [***********************************]
> |   ( ??   ?? )  [Ronaldo Reis J??nior                ]
> |>      V      [UFV/DBA-Entomologia                ]
> |    /     \   [36570-000 Vi??osa - MG              ]
> |>  /(.''`.)\  [Fone: 31-3899-4007                 ]
> |  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |    ( `-  )   [***********************************]
> |>>  _/   \_Powered by GNU/Debian Woody/Sarge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From wxc203 at psu.edu  Wed Nov 16 14:39:56 2005
From: wxc203 at psu.edu (Vivien W. Chen)
Date: Wed, 16 Nov 2005 08:39:56 -0500
Subject: [R] Error in integrate
Message-ID: <000e01c5eab3$3b7d9a60$3502a8c0@VC>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/21e1a2b9/attachment.pl

From wxc203 at psu.edu  Wed Nov 16 14:48:49 2005
From: wxc203 at psu.edu (Vivien W. Chen)
Date: Wed, 16 Nov 2005 08:48:49 -0500
Subject: [R] Combine related plots
Message-ID: <001701c5eab4$78f45ae0$3502a8c0@VC>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/d0459ecb/attachment.pl

From sdavis2 at mail.nih.gov  Wed Nov 16 14:52:25 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 16 Nov 2005 08:52:25 -0500
Subject: [R] Combine related plots
In-Reply-To: <001701c5eab4$78f45ae0$3502a8c0@VC>
Message-ID: <BFA0A3C9.12FB5%sdavis2@mail.nih.gov>

On 11/16/05 8:48 AM, "Vivien W. Chen" <wxc203 at psu.edu> wrote:

> Dear R users,
> 
> If I have to combine plots which have the same independent and dependent
> variables in one graph. Which command should I use? Any example? Can I use
> "panel"?

Viven,

plot(x1,y1)
par(new=TRUE)
plot(x2,y2)

You may have to use xlim and ylim to get the plots to match axes.

Alternatively,

plot(x1,y1)
points(x2,y2) # or use lines, or whatnot

Sean



From matthew_wiener at merck.com  Wed Nov 16 15:09:30 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 16 Nov 2005 09:09:30 -0500
Subject: [R] Combine related plots
Message-ID: <4E9A692D8755DF478B56A2892388EE1F2F5FA6@usctmx1118.merck.com>

You can also look at xyplot in the lattice package.  You will have to set up
your data slightly differently than for the standard graphics package, but
it may well be worth learning to do so.  The lattice package has enormous
flexibility for combining multiple sets of data in one panel or plotting
them in separate panels.

Hope this helps,

Matt

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vivien W. Chen
Sent: Wednesday, November 16, 2005 8:49 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Combine related plots


Dear R users,

If I have to combine plots which have the same independent and dependent
variables in one graph. Which command should I use? Any example? Can I use
"panel"?

Your help will be deeply appreciated. Thanks!

-Vivien Chen-

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Nov 16 15:12:43 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Nov 2005 15:12:43 +0100
Subject: [R] numericDeriv
In-Reply-To: <XFMail.051116131151.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051116131151.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2hdacwujo.fsf@viggo.kubism.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 16-Nov-05 Florent Bresson wrote:
> > I have to compute some standard errors using the delta
> > method and so have to use the command "numericDeriv"
> > to get the desired gradient. Befor using it on my
> > complicated function, I've done a try with a simple
> > exemple :
> > 
> > x <- 1:5
> > numericDeriv(quote(x^2),"x")
> > 
> > and i get :
> > 
> > [1]   1   8  27  64 125 216
> > attr(,"gradient")
> >      [,1] [,2] [,3] [,4] [,5] [,6]
> > [1,]  Inf    0    0  NaN    0    0
> > [2,]    0    0    0  NaN    0    0
> > [3,]    0  Inf    0  NaN    0    0
> > [4,]    0    0    0  NaN    0    0
> > [5,]    0    0  Inf  NaN    0    0
> > [6,]    0    0    0  NaN    0    0
> > 
> > I don't understand the result. I thought I will get :
> > 
> > [1]   1   8  27  64 125 216
> > attr(,"gradient")
> >      [,1]
> > [1,]  1
> > [2,]  4
> > [3,]  6
> > [4,]  8
> > [5,]  10
> > [6,]  12
> > 
> > The derivative of x^2 is still 2x, isn't it ?
> 
> The trap you've fallen into is that "x <- 1:5" makes x of
> integer type, and (believe it or not) you cannot differentiate
> when the support of a function is the integers. Wrong topology
> (though I'm not sure that this is quite how R thinks about it).
> 
> So give x a bit of elbow-room ("numeric" type has "continous"
> -- well, nearly -- topology):
> 
> > x <- as.numeric(1:5)
> > numericDeriv(quote(x^2),"x")
> [1]  1  4  9 16 25
> attr(,"gradient")
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    2    0    0    0    0
> [2,]    0    4    0    0    0
> [3,]    0    0    6    0    0
> [4,]    0    0    0    8    0
> [5,]    0    0    0    0   10


Oho. That had me baffled for a while... We should probably also
explain that x is a vector and differentiation of a vector w.r.t. a
vector is a matrix. If you want a 5x1 result you should likely use
something like

> d<-0
> numericDeriv(quote((x+d)^2),"d")
[1]  1  4  9 16 25
attr(,"gradient")
     [,1]
[1,]    2
[2,]    4
[3,]    6
[4,]    8
[5,]   10

(I *hope* there's no way to get the result that Florent claims that he
expected...)


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From davidr at rhotrading.com  Wed Nov 16 15:41:27 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 16 Nov 2005 08:41:27 -0600
Subject: [R] COM dates (was origin and "origin<-" in chron)
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A7CEBA7@rhosvr02.rhotrading.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/4b4264c4/attachment.pl

From ggrothendieck at gmail.com  Wed Nov 16 15:51:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 16 Nov 2005 09:51:09 -0500
Subject: [R] COM dates (was origin and "origin<-" in chron)
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A7CEBA7@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A7CEBA7@rhosvr02.rhotrading.com>
Message-ID: <971536df0511160651x396747deh663ecf53b7bcf044@mail.gmail.com>

This is covered in the R News article.


On 11/16/05, davidr at rhotrading.com <davidr at rhotrading.com> wrote:
>
>
> I was just looking for an easy way to convert between COM datetime and chron
> datetime (both ways.)
>
> I found examples on the list, but they involved origin.
>
>
>
> Does anyone have functions for converting COM datetime <-> chron
> datetimethat work "safely"?
>
>
>
> David L. Reiner
>
>
>
>
>
> > -----Original Message-----
>
> > From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
>
> > Sent: Tuesday, November 15, 2005 5:34 PM
>
> > To: David Reiner <davidr at rhotrading.com>
>
> > Cc: r-help at stat.math.ethz.ch
>
> > Subject: [SPAM] - Re: [R] origin and "origin<-" functions on chron -
>
> > Bayesian Filter detected spam
>
> >
>
> > chron has a namespace so try this:
>
> >
>
> >    chron:::origin
>
> >    getAnywhere("origin<-")
>
> >
>
> > Having said that I would recommend that you don't use origins in chron.
>
> > The situation may have changed but when I wrote the Help Desk article
>
> > in R News 4/1 I encountered problems with using origins in some
>
> > situations and as discussed in the article its so easy to avoid using them
>
> > that there is really no good reason I can see not to avoid them.
>
> >
>
> >
>
> > On 11/15/05, davidr at rhotrading.com <davidr at rhotrading.com> wrote:
>
> > > I'm trying to use/modify some code I found (at Omegahat, but I've seem
>
> > similar usage elsewhere.)
>
> > > It contains the lines:
>
> > >   if(any(origin(chronDate)!=orig))
>
> > >      origin(chronDate) <- orig
>
> > >
>
> > > Let's say:
>
> > > > require("chron")
>
> > > [1] TRUE
>
> > > > chronDate <- chron("11/15/2005", format="m/d/y",
>
> > origin.=c(12,31,1899))
>
> > > > orig <- c(month=12, day=31, year=1899)
>
> > > > origin(chronDate)
>
> > > Error: couldn't find function "origin"
>
> > > > origin(chronDate) <- orig
>
> > > Error: couldn't find function "origin<-"
>
> > >
>
> > > I'm sure I'm missing something simple here, but what? I've looked in the
>
> > archives and docs quite a lot....
>
> > >
>
> > > Thanks for the help!
>
> > >
>
> > > R-2.2.0 on Windows XP (SP2)
>
> > >
>
> > > > R.Version()
>
> > > $platform
>
> > > [1] "i386-pc-mingw32"
>
> > > $arch
>
> > > [1] "i386"
>
> > > $os
>
> > > [1] "mingw32"
>
> > > $system
>
> > > [1] "i386, mingw32"
>
> > > $status
>
> > > [1] ""
>
> > > $major
>
> > > [1] "2"
>
> > > $minor
>
> > > [1] "2.0"
>
> > > $year
>
> > > [1] "2005"
>
> > > $month
>
> > > [1] "10"
>
> > > $day
>
> > > [1] "06"
>
> > > $"svn rev"
>
> > > [1] "35749"
>
> > > $language
>
> > > [1] "R"
>
> > >
>
> > > David L. Reiner
>
> > >
>
> > > Rho Trading
>
> > > 440 S. LaSalle St.
>
> > > Chicago IL 60605
>
> > > 312-362-4963
>
> > >
>
> > >
>
> > > ______________________________________________
>
> > > R-help at stat.math.ethz.ch mailing list
>
> > > https://stat.ethz.ch/mailman/listinfo/r-help
>
> > > PLEASE do read the posting guide!
> http://www.R-project.org/posting-
>
> > guide.html
>
> > >
>
>
>
>
>
> David L. Reiner
>
>
>
> Rho Trading
>
> 440 S. LaSalle St.
>
> Chicago  IL  60605
>
> 312-362-4963
>
>
>
>



From f_bresson at yahoo.fr  Wed Nov 16 16:05:39 2005
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Wed, 16 Nov 2005 16:05:39 +0100 (CET)
Subject: [R] numericDeriv
In-Reply-To: <Pine.LNX.4.61.0511161301510.12485@gannet.stats>
Message-ID: <20051116150539.10213.qmail@web26806.mail.ukl.yahoo.com>

Effectively, it's much better
thanks
--- Prof Brian Ripley <ripley at stats.ox.ac.uk> a
??crit??:

> On Wed, 16 Nov 2005, Florent Bresson wrote:
> 
> > I have to compute some standard errors using the
> delta
> > method and so have to use the command
> "numericDeriv"
> > to get the desired gradient. Befor using it on my
> > complicated function, I've done a try with a
> simple
> > exemple :
> >
> > x <- 1:5
> > numericDeriv(quote(x^2),"x")
> >
> > and i get :
> >
> > [1]   1   8  27  64 125 216
> > attr(,"gradient")
> >     [,1] [,2] [,3] [,4] [,5] [,6]
> > [1,]  Inf    0    0  NaN    0    0
> > [2,]    0    0    0  NaN    0    0
> > [3,]    0  Inf    0  NaN    0    0
> > [4,]    0    0    0  NaN    0    0
> > [5,]    0    0  Inf  NaN    0    0
> > [6,]    0    0    0  NaN    0    0
> >
> > I don't understand the result. I thought I will
> get :
> >
> > [1]   1   8  27  64 125 216
> > attr(,"gradient")
> >     [,1]
> > [1,]  1
> > [2,]  4
> > [3,]  6
> > [4,]  8
> > [5,]  10
> > [6,]  12
> >
> > The derivative of x^2 is still 2x, isn't it ?
> 
> and (1:5)^2 is still
> 
> [1]  1  4  9 16 25
> 
> !
> 
> Try
> 
> > x <- as.numeric(1:5)
> > numericDeriv(quote(x^2),"x")
> 
> since the author of numericDeriv has forgotten some
> coercions.
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
>



From aliscla at yahoo.com  Wed Nov 16 16:08:10 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Wed, 16 Nov 2005 07:08:10 -0800 (PST)
Subject: [R] x-axis in dendrogram
Message-ID: <20051116150810.57253.qmail@web61224.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/771cc97f/attachment.pl

From tlumley at u.washington.edu  Wed Nov 16 16:08:26 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 16 Nov 2005 07:08:26 -0800 (PST)
Subject: [R] Error in integrate
In-Reply-To: <000e01c5eab3$3b7d9a60$3502a8c0@VC>
References: <000e01c5eab3$3b7d9a60$3502a8c0@VC>
Message-ID: <Pine.LNX.4.63a.0511160705450.5647@homer24.u.washington.edu>

On Wed, 16 Nov 2005, Vivien W. Chen wrote:

> Hi!
>
> I am a beginner of R. I am trying to calculate integrate and draw a 
> graph of the output, but just kept on getting error messages. I list my 
> program and error message below. Please help. Many Thanks!
>
> =======================
> + > a<--11
>> b<-0.1
>> c<-0.012
>> x<-0:110

This is harmless but suspicious -- this x is completely unrelated to the x 
in integrand()

>> t<-0:15
>> integrand<-function(x) {exp(-exp(a-c*t)*(exp(b*x)-exp(c*x))/(b-c))}

The problem is that x is a vector of whatever length integrate() decides 
to pass, but t is a vector of length 16.  This mismatch of lengths is 
going to cause a problem.

>> cal<-integrate(integrand,0,Inf)
> Error in integrate(integrand, 0, Inf) : evaluation of function gave a result of wrong length
> In addition: Warning message:
> longer object length
>        is not a multiple of shorter object length in: -exp(a - c * t) * (exp(b * x) - exp(c * x))
>> plot(cal~t, xlab="Time", ylab="Pop at t")
> Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  :
>        invalid variable type
> ===========================
>
> - Vivien Chen -
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From afshart at exchange.sba.miami.edu  Wed Nov 16 16:39:24 2005
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Wed, 16 Nov 2005 10:39:24 -0500
Subject: [R] save to ascii
Message-ID: <6BCB4D493A447546A8126F24332056E8027C893F@school1.business.edu>


All,

Usually when I save a variable I have ascii = FALSE since I usually
load the variable via the load(variable) command and do not need to
view the variable outside of R.

When I set ascii = TRUE and view the variable outside of R in a text editor,
I notice additional characters (starting w/ RDA2 ...) before the first actual value
in the vector.

Is there are way to save to ascii such that this does not happen?
I checked the help under save and didn't see anything.  I apologize
in advance for this overly simplistic question.

Cheers,
Dave
ps - please reply directly to afshar at miami.edu



From rpeng at jhsph.edu  Wed Nov 16 16:40:46 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 16 Nov 2005 10:40:46 -0500
Subject: [R] save to ascii
In-Reply-To: <6BCB4D493A447546A8126F24332056E8027C893F@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E8027C893F@school1.business.edu>
Message-ID: <437B52FE.6020607@jhsph.edu>

You could try using functions like 'dput()' or 'dump()'.  These may or may not 
be equivalent to 'save()' depending on how complex the object to be saved is.

-roger

P.S. Asking people to respond to a different email address is not a good way to 
get responses!

Afshartous, David wrote:
> All,
> 
> Usually when I save a variable I have ascii = FALSE since I usually
> load the variable via the load(variable) command and do not need to
> view the variable outside of R.
> 
> When I set ascii = TRUE and view the variable outside of R in a text editor,
> I notice additional characters (starting w/ RDA2 ...) before the first actual value
> in the vector.
> 
> Is there are way to save to ascii such that this does not happen?
> I checked the help under save and didn't see anything.  I apologize
> in advance for this overly simplistic question.
> 
> Cheers,
> Dave
> ps - please reply directly to afshar at miami.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/



From jorg.schlingemann at genpat.uu.se  Wed Nov 16 17:46:05 2005
From: jorg.schlingemann at genpat.uu.se (=?iso-8859-1?Q?J=F6rg_Schlingemann?=)
Date: Wed, 16 Nov 2005 16:46:05 -0000
Subject: [R] invert y-axis in barplot
Message-ID: <000701c5eacd$3ccd4d10$a7d2ee82@rudbeck.uu.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/dcde9cb4/attachment.pl

From chrysopa at gmail.com  Wed Nov 16 16:47:11 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Wed, 16 Nov 2005 13:47:11 -0200
Subject: [R] Difficulties with for() {while(){}}
In-Reply-To: <644e1f320511160530q4ba17cccx3865fed381e0a357@mail.gmail.com>
References: <200511161104.31083.chrysopa@gmail.com>
	<644e1f320511160530q4ba17cccx3865fed381e0a357@mail.gmail.com>
Message-ID: <200511161347.11889.chrysopa@gmail.com>

Em Qua 16 Nov 2005 11:30, jim holtman escreveu:
> What do you mean by 'abort'? Does an 'error' occur that you want to catch?
> If so, look at 'try'. Otherwise if it is testable, then just test for the
> condition and restart.
>

Hi,

this is not a real error, is a situation whitout a resolution.

Is, I try to test the condition and restart, but how to restart all?

Matthias suggest the use of initialize, I dont undertande how to use this.

I try this:

myfunction <- function(...) {

  ## Init of function
  ...

  for(i in test) {
    ...

    while(j <= test2) {
    ...
      test3 <- make a test 
      if(test3 == error) {
      myfunction(...)
      }
    }
  }
}

This is the best way to make this?

In this case I need to put all arguments initialized in function(...) on the 
myfunction(...) .

Thanks
Ronaldo

-- 
Fa??a algo ?? prova de idiotas e algu??m far?? um idiota melhor.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From andy_liaw at merck.com  Wed Nov 16 16:58:33 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 16 Nov 2005 10:58:33 -0500
Subject: [R] save to ascii
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5AF@usctmx1106.merck.com>

Perhaps use dput?

HTH,
Andy

From: Afshartous, David
> 
> All,
> 
> Usually when I save a variable I have ascii = FALSE since I usually
> load the variable via the load(variable) command and do not need to
> view the variable outside of R.
> 
> When I set ascii = TRUE and view the variable outside of R in 
> a text editor,
> I notice additional characters (starting w/ RDA2 ...) before 
> the first actual value
> in the vector.
> 
> Is there are way to save to ascii such that this does not happen?
> I checked the help under save and didn't see anything.  I apologize
> in advance for this overly simplistic question.
> 
> Cheers,
> Dave
> ps - please reply directly to afshar at miami.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Nov 16 17:12:01 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 16 Nov 2005 11:12:01 -0500
Subject: [R] "Warning message: package '...' was built under R version 2.3.0"
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F415B@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/e6d3059c/attachment.pl

From mschwartz at mn.rr.com  Wed Nov 16 17:19:47 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 16 Nov 2005 10:19:47 -0600
Subject: [R] invert y-axis in barplot
In-Reply-To: <000701c5eacd$3ccd4d10$a7d2ee82@rudbeck.uu.local>
References: <000701c5eacd$3ccd4d10$a7d2ee82@rudbeck.uu.local>
Message-ID: <1132157987.7166.4.camel@localhost.localdomain>

On Wed, 2005-11-16 at 16:46 +0000, JÃ¶rg Schlingemann wrote:
> Hi!
> 
>  
> 
> This is probably a very trivial question. Is there an easy way to invert the
> y-axis (low values on top) when using the function barplot()? 
> 
>  
> 
> Thanks,
> 
> Jrg

You mean something like this?:

  barplot(1:10, ylim = rev(c(0, 12)))

or, something like this?:

  barplot(1:10, yaxt = "n") 
  axis(2, labels = rev(seq(0, 10, 2)), at = seq(0, 10, 2))


Note the use of rev() in each case.

HTH,

Marc Schwartz



From jtw2 at CDC.GOV  Wed Nov 16 17:01:21 2005
From: jtw2 at CDC.GOV (Wassell, James T., Ph.D.)
Date: Wed, 16 Nov 2005 11:01:21 -0500
Subject: [R] nlme question
Message-ID: <AF2DCD619279544BA454141F4A45B9E3F5E18F@m-niosh-3.niosh.cdc.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/2f653cab/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed Nov 16 17:23:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Nov 2005 17:23:42 +0100
Subject: [R] invert y-axis in barplot
In-Reply-To: <000701c5eacd$3ccd4d10$a7d2ee82@rudbeck.uu.local>
References: <000701c5eacd$3ccd4d10$a7d2ee82@rudbeck.uu.local>
Message-ID: <437B5D0E.5040009@statistik.uni-dortmund.de>

J??rg Schlingemann wrote:

> Hi!
> 
>  
> 
> This is probably a very trivial question. Is there an easy way to invert the
> y-axis (low values on top) when using the function barplot()? 


barplot(1:10, ylim=c(10, 0))

works for me ...

Uwe Ligges


>  
> 
> Thanks,
> 
> J??rg
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Nov 16 17:26:34 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Nov 2005 17:26:34 +0100
Subject: [R] "Warning message: package '...' was built under R version
 2.3.0"
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F415B@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F415B@us-arlington-0668.mail.saic.com>
Message-ID: <437B5DBA.3030109@statistik.uni-dortmund.de>

Tuszynski, Jaroslaw W. wrote:

> Hi,
> 
> While installing precompiled packages I often get warnings like the one in
> the subject. I usually ignore them, but I still do not understand why
> windows packages are build with unreleased versions of R. 
> Is there some way to get packages build under R-2.2.0?

Well, those in the corresponding "2.2" repository on CRAN should all be 
build with R-2.2.0. If not, please tell me which one is wrong.

Where did you get the packages for R-devel from?

Uwe Ligges


> What are potential problems that can result from that version mismatch?
> 
> My system: winXP
> R version: 2.2.0
> 
> Jarek Tuszynski 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Nov 16 17:52:00 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 16 Nov 2005 11:52:00 -0500
Subject: [R] "Warning message: package '...' was built under R version	
	2.3.0"
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F415C@us-arlington-0668.mail.saic.com>

Uwe,

I think I used following repositories: PA1, PA2 and NC, which are the
closest to me and I think I got the same results in all. The package I
downloaded was "coin" which downloaded other packages. The consol printout
follows:


> chooseCRANmirror()
> utils:::menuInstallPkgs()
trying URL
'http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.2/coin_0.
3-3.zip'
Content type 'application/zip' length 793299 bytes
opened URL
downloaded 774Kb

package 'coin' successfully unpacked and MD5 sums checked

The downloaded packages are in
        C:\Documents and Settings\tuszynskij\Local
Settings\Temp\Rtmp7594\downloaded_packages
updating HTML package descriptions
> library(coin)
Loading required package: survival
Loading required package: splines
Loading required package: mvtnorm
Loading required package: modeltools
Error in load(dataFile, ns) : ReadItem: unknown type 241
In addition: Warning messages:
1: package 'mvtnorm' was built under R version 2.3.0 
2: package 'modeltools' was built under R version 2.3.0 
Error: unable to load R code in package 'modeltools'
Error: package 'modeltools' could not be loaded

Jarek Tuszynski

-----Original Message-----
From: ligges at statistik.uni-dortmund.de
[mailto:ligges at statistik.uni-dortmund.de] 
Sent: Wednesday, November 16, 2005 11:27 AM
To: Tuszynski, Jaroslaw W.
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] "Warning message: package '...' was built under R version
2.3.0"


Tuszynski, Jaroslaw W. wrote:

> Hi,
> 
> While installing precompiled packages I often get warnings like the 
> one in the subject. I usually ignore them, but I still do not 
> understand why windows packages are build with unreleased versions of 
> R. Is there some way to get packages build under R-2.2.0?

Well, those in the corresponding "2.2" repository on CRAN should all be 
build with R-2.2.0. If not, please tell me which one is wrong.

Where did you get the packages for R-devel from?

Uwe Ligges


> What are potential problems that can result from that version 
> mismatch?
> 
> My system: winXP
> R version: 2.2.0
> 
> Jarek Tuszynski
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From berr0179 at umn.edu  Wed Nov 16 18:18:11 2005
From: berr0179 at umn.edu (Erin Berryman)
Date: Wed, 16 Nov 2005 11:18:11 -0600
Subject: [R] combination xyplot and barchart?
In-Reply-To: <eb555e660511151909k5796fc49y691f25709e690b0b@mail.gmail.com>
References: <47ecce16e65cc88e51077229befaf07b@umn.edu>
	<eb555e660511151909k5796fc49y691f25709e690b0b@mail.gmail.com>
Message-ID: <4bc98ef32c1d6eaf42a992162aac1898@umn.edu>

Thank you, that worked great. Per your suggestion, I transformed the 
Precip data to fit on the same scale as the Temp data. After changing 
trellis.par.set$layout.widths$right.padding to 5, I was able to fit a 
second axis and label for the Precip (using panel.axis) to achieve the 
plot I want.

Erin

On Nov 15, 2005, at 9:09 PM, Deepayan Sarkar wrote:

> On 11/15/05, Erin Berryman <berr0179 at umn.edu> wrote:
>> Dear R community,
>>
>> I am having trouble determining how to create the graph I want
>> utilizing my relatively limited knowledge of R. So far I have been
>> using the lattice library to create most of what I need.
>> The dataset (enviro) consists of 2 variables (Temp and Precip) for 
>> each
>> Day of a 2-yr period (Year). I wish to display Temp and Precip along
>> the y axis plotted by Day on the x axis to allow comparison (one 
>> year's
>> data in each of 2 panels stacked on top of each other) between the
>> years. Essentially what I want it to look like is an xyplot (Temp ~ 
>> Day
>> | Year, type='l') superimposed onto a barchart(Precip ~ Day | Year,
>> horizontal=F), with scales adjusted so one can see detail in both
>> variables.
>> The closest I have come to what I need is by the following code :
>>
>> library(lattice)
>> barchart(Precip + Temp ~ Day | Year, data=enviro, layout=c(1,2),
>> horizontal=F, origin=0,
>> panel=function(x,y,subscripts,...){panel.xyplot(x=enviro$Day,
>> y=enviro$Temp, type='l',subscripts=subscripts, ...);
>> panel.barchart(x=enviro$Day, y=enviro$Precip, subscripts=subscripts,
>> ...)})
>>
>> Two panels are produced; however, both years' data are plotted in each
>> panel (panels look identical). And I get this error:
>>
>> Error in grid.Call.graphics("L_rect", x$x, x$y, x$width, x$height,
>> resolveHJust(x$just,  :
>> 	invalid line type
>
> A reproducible example, even if it's a toy one, would have been 
> helpful.
>
> Your usage is confused. In particular, panel.xyplot ignores the
> subscripts argument, so you end up giving exactly the same set of
> values to panel.xyplot for both panels (so it's not surprising that
> your panels show the same data). It seems that you are looking for
> something like the following:
>
>
> enviro <-
>     data.frame(Year = rep(2001:2002, each = 365),
>                Day = rep(1:365, 2),
>                Precip = pmax(0, rnorm(365 * 2)),
>                Temp = 2 + 0.2 * rnorm(365 * 2))
>
>
> xyplot(Precip + Temp ~ Day | Year, data=enviro,
>        layout = c(1, 2),
>        panel = panel.superpose.2,
>        type = c('h', 'l'))
>
> In case it helps, this is shorthand for
>
> xyplot(Precip + Temp ~ Day | Year, data=enviro,
>        layout = c(1, 2),
>        panel = function(x, y, groups, subscripts, ...) {
>            panel.superpose.2(x = x, y = y,
>                              groups = groups,
>                              subscripts = subscripts,
>                              ...)
>        },
>        type = c('h', 'l'))
>
> Note that the panel function is defined in terms of arguments it gets,
> and does not explicitly refer to any external variables (like the data
> frame 'enviro'). If you find yourself writing code that does, it's a
> likely sign that you are doing something wrong (or at least
> unnecessarily convoluted).
>
>
>>  From the documentation or the help archives, I cannot understand how 
>> to:
>> 1) indicate a conditioning variable (Year) for panel.barchart and
>> panel.xyplot
>> 2) have 2 y axes with different scales in one panel
>
> You can't easily. A panel has one set of scales, and that's it. You
> can of course fake it by transforming the relevant part of your data
> and adding a set of tick marks with appropriate (fake) labels (see
> ?panel.axis).
>
> -Deepayan
>



From jtw2 at CDC.GOV  Wed Nov 16 18:15:56 2005
From: jtw2 at CDC.GOV (Wassell, James T., Ph.D.)
Date: Wed, 16 Nov 2005 12:15:56 -0500
Subject: [R] nmle question
Message-ID: <AF2DCD619279544BA454141F4A45B9E306A3CC@m-niosh-3.niosh.cdc.gov>

Hello.

I have 16 subjects with 1-4 obs per subject. 

I am using the package "nlme" to fit a simple random effects (variance
components model) with 3 parameters:  overall mean (fixed effect),
between subject variance (random) and within subject variance (random).
 
I need a 3x3 variance-covariance matrix that includes all 3 parameters
in order to compute the variance of a specific linear combination.

But I can't get the 3x3 matrix.  Should I specify the formulae in lme
differently or is there some other suggestion that I might try?

Thank you very much for any advice.  my data and code follows.  

"mydata" <-
structure(list(subject = c(17, 17, 17, 17, 5, 16, 16, 8, 8, 8, 
8, 7, 7, 7, 7, 9, 9, 9, 10, 10, 11, 11, 11, 12, 12, 12, 12, 14, 
14, 14, 14, 15, 15, 15, 15, 13, 13, 13, 1, 1, 1, 2, 2, 2, 2, 
3, 3, 3, 4, 4, 4), y = c(-2.944, -5.521, -4.644, -4.736, -5.799, 
-4.635, -5.986, -5.011, -3.989, -4.682, -6.975, -6.064, -5.991, 
-8.068, -5.075, -5.298, -6.446, -5.037, -6.534, -4.828, -5.886, 
-4.025, -6.607, -5.914, -4.159, -6.757, -4.564, -5.011, -5.416, 
-5.371, -5.768, -7.962, -5.635, -4.575, -5.268, -6.975, -5.598, 
-7.669, -8.292, -7.265, -5.858, -7.003, -3.807, -5.829, -5.613, 
-3.135, -5.136, -5.394, -5.011, -5.598, -4.174)), .Names = c("subject", 
"y"), class = "data.frame", row.names = c("1", "2", "3", "4", 
"5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", 
"16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", 
"27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", 
"38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", 
"49", "50", "51"))

lme.res<-lme(fixed=y~1,data=mydata,random=~1|subject,method="ML")
VarCorr(lme.res)



From andy_liaw at merck.com  Wed Nov 16 18:34:13 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 16 Nov 2005 12:34:13 -0500
Subject: [R] Unexpected result of names<-
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5B0@usctmx1106.merck.com>

That's because S does partial matching of component names, if it can be done
unambiguously:


> z <- list(aaa=1, bbb=2)
> z$a
[1] 1
> z[["a"]]
[1] 1
> z["a"]
$"NA"
NULL

> z["aaa"]
$aaa
[1] 1

Andy



From: Hong Ooi
> 
> Hi,
> 
> I came across some rather unexpected behaviour the other day with
> assigning names and lists. Here's an example.
> 
> > z <- list(aaa=1, bbb=2)
> > z
> $aaa
> [1] 1
> 
> $bbb
> [1] 2
> 
> Note that z has members named "aaa" and "bbb". Now:
> 
> > names(z$a) <- "X"
> > z
> $aaa
> [1] 1
> 
> $bbb
> [1] 2
> 
> $a
> X 
> 1
> 
> I would have expected that trying to name z$a would either 
> give an error
> (because z doesn't have an element "a") or would cause z$aaa to be
> modified (due to partial name matching).  I didn't expect that a new
> list member would be created.
> 
> I've checked that this is consistent across SPlus 2000, SPlus 7, and R
> 2.2 for Windows. Can someone give an explanation for why this is
> happening?
> 
> 
> -- 
> Hong Ooi
> Senior Research Analyst, IAG Limited
> 388 George St, Sydney NSW 2000
> (02) 9292 1566
> 
> 
> ______________________________________________________________
> _________________________
> 
> The information transmitted in this message and its 
> attachments (if any) is intended 
> only for the person or entity to which it is addressed.
> The message may contain confidential and/or privileged 
> material. Any review, 
> retransmission, dissemination or other use of, or taking of 
> any action in reliance 
> upon this information, by persons or entities other than the 
> intended recipient is 
> prohibited.
> 
> If you have received this in error, please contact the sender 
> and delete this e-mail 
> and associated material from any computer.
> 
> The intended recipient of this e-mail may only use, 
> reproduce, disclose or distribute 
> the information contained in this e-mail and any attached 
> files, with the permission 
> of the sender.
> 
> This message has been scanned for viruses with Symantec Scan 
> Engine and cleared by 
> MailMarshal.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From cznm4 at mizzou.edu  Wed Nov 16 18:42:40 2005
From: cznm4 at mizzou.edu (Zhu, Chao (UMC-Student))
Date: Wed, 16 Nov 2005 11:42:40 -0600
Subject: [R] Newton-Raphson
Message-ID: <C4A33886378A9447B5C05B3010868146FFA415@UM-EMAIL10.um.umsystem.edu>

Dear all,
 
I want to solve a score function by using Newton-Raphson algorithm. Is there such a fucntion in R? I know there's one called optim, but it seems only doing minimizing or maximizing. 
 
Thanks,
 
Jimmy



From sabolk at hotmail.com  Wed Nov 16 18:57:07 2005
From: sabolk at hotmail.com (Keith Sabol)
Date: Wed, 16 Nov 2005 12:57:07 -0500
Subject: [R] RODBC help
Message-ID: <BAY114-F12CBDEDD885260F9BE1AA7D75C0@phx.gbl>

I am using the RODBC package to read data from an Excel file. An excerpt of 
the file looks like this:
00103V206	AES Corporation	6.00%	42.87
00808N202	AES Trust III	6.75%	34.98
03748R861	Apartment Investment & Management	9.00%	#ERROR (I)
039380209	Arch Coal, Inc.	5.00%	61.51


My problem appears to be related to specification of data types by column.  
For instance, the CUSIPS in column 1 are read in correctly when they contain 
a letter, but as NA when they are purely numbers.  Similarly, in the fourth 
column the rows with "#ERROR (I)" are read in as such, but all other values 
become NA.

I have experimented with "as.is" but have not been able to arrive at a 
solution.  I also changed the nullstrings return value and each of the NAs 
generated above appear to be genrated by the query "seeing" null strings.

As always, your assistance is most appreciated.

_________________________________________________________________
Don’t just search. Find. Check out the new MSN Search!



From ligges at statistik.uni-dortmund.de  Wed Nov 16 19:03:54 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Nov 2005 19:03:54 +0100
Subject: [R] "Warning message: package '...' was built under R version
 2.3.0"
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F415C@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F415C@us-arlington-0668.mail.saic.com>
Message-ID: <437B748A.2000105@statistik.uni-dortmund.de>

Tuszynski, Jaroslaw W. wrote:

> Uwe,
> 
> I think I used following repositories: PA1, PA2 and NC, which are the
> closest to me and I think I got the same results in all. The package I
> downloaded was "coin" which downloaded other packages. The consol printout
> follows:
> 
> 
> 
>>chooseCRANmirror()
>>utils:::menuInstallPkgs()
> 
> trying URL
> 'http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.2/coin_0.
> 3-3.zip'
> Content type 'application/zip' length 793299 bytes
> opened URL
> downloaded 774Kb
> 
> package 'coin' successfully unpacked and MD5 sums checked
> 
> The downloaded packages are in
>         C:\Documents and Settings\tuszynskij\Local
> Settings\Temp\Rtmp7594\downloaded_packages
> updating HTML package descriptions
> 
>>library(coin)
> 
> Loading required package: survival
> Loading required package: splines
> Loading required package: mvtnorm
> Loading required package: modeltools
> Error in load(dataFile, ns) : ReadItem: unknown type 241
> In addition: Warning messages:
> 1: package 'mvtnorm' was built under R version 2.3.0 


I have just downloaded
http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.2/mvtnorm_0.7-2.zip
and its DESCRIPTION file correctly has:
"Built: R 2.2.0; i386-pc-mingw32; 2005-10-14 14:44:12; windows"


My guess is that you are using one library for two different R versions 
and you have installed some R-devel package into the library you are now 
using with R-2.2.0 ...

You cannot mix binary packages for R < 2.3.0 with those for R-devel due 
to changes in environment handling.

Uwe Ligges




> 2: package 'modeltools' was built under R version 2.3.0 
> Error: unable to load R code in package 'modeltools'
> Error: package 'modeltools' could not be loaded
> 
> Jarek Tuszynski
> 
> -----Original Message-----
> From: ligges at statistik.uni-dortmund.de
> [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: Wednesday, November 16, 2005 11:27 AM
> To: Tuszynski, Jaroslaw W.
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] "Warning message: package '...' was built under R version
> 2.3.0"
> 
> 
> Tuszynski, Jaroslaw W. wrote:
> 
> 
>>Hi,
>>
>>While installing precompiled packages I often get warnings like the 
>>one in the subject. I usually ignore them, but I still do not 
>>understand why windows packages are build with unreleased versions of 
>>R. Is there some way to get packages build under R-2.2.0?
> 
> 
> Well, those in the corresponding "2.2" repository on CRAN should all be 
> build with R-2.2.0. If not, please tell me which one is wrong.
> 
> Where did you get the packages for R-devel from?
> 
> Uwe Ligges
> 
> 
> 
>>What are potential problems that can result from that version 
>>mismatch?
>>
>>My system: winXP
>>R version: 2.2.0
>>
>>Jarek Tuszynski
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html



From olau at fas.harvard.edu  Wed Nov 16 19:18:58 2005
From: olau at fas.harvard.edu (Olivia Lau)
Date: Wed, 16 Nov 2005 13:18:58 -0500
Subject: [R] normal cdf over an interval
Message-ID: <001501c5eada$367785e0$0500a8c0@OliviaIBM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/c797d864/attachment.pl

From tom at maladmin.com  Wed Nov 16 15:16:03 2005
From: tom at maladmin.com (tom wright)
Date: Wed, 16 Nov 2005 09:16:03 -0500
Subject: [R] RODBC and Very long field lengths
Message-ID: <1132150563.4819.107.camel@localhost.localdomain>

I'm having a small problem using RODBC. I'm trying to retrieve a string
from a very long memo field (512*20*9=9360 characters = 74880 bytes) in
an MSAccess database. 
It appears that RODBC set a maximum buffer size for a single column of
65535 bytes.

########## cut from RODBC.c ##########
	} else { /* transfer as character */
	    int datalen = thisHandle->ColData[i].ColSize;
	    if (datalen <= 0 || datalen < COLMAX) datalen = COLMAX;
	    /* sanity check as the reports are sometimes unreliable */
	    if (datalen > 65535) datalen = 65535;
######################################

Can I increase this by just changeing the value in RODBC.c? If so how do
I get R  to re-compile the package?

Thanks for your advice 
Tom



From ripley at stats.ox.ac.uk  Wed Nov 16 20:21:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Nov 2005 19:21:14 +0000 (GMT)
Subject: [R] Unexpected result of names<-
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5B0@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5B0@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.61.0511161848240.22329@gannet.stats>

On Wed, 16 Nov 2005, Liaw, Andy wrote:

> That's because S does partial matching of component names, if it can be 
> done unambiguously:
>
>> z <- list(aaa=1, bbb=2)
>> z$a
> [1] 1
>> z[["a"]]
> [1] 1
>> z["a"]
> $"NA"
> NULL
[That could be done unambiguously, but [] does not partial match in R
(see ?"["), only [[]] and $ do.  That is different from S (Blue Book 
p.358), and I do not know why.]
>> z["aaa"]
> $aaa
> [1] 1

Well, *sometimes*.  For

> z <- list(aaa=1, bbb=2)
> z$a <- 3
> z
$aaa
[1] 1

$bbb
[1] 2

$a
[1] 3

It is that the rules are different for extraction and replacement that 
surprises people.  In this case we have effectively

z <- "$<-"("names<-"(z$a, "X"), "a")

and the replacement function "$<-" does not partially match (Blue Book, 
p.362).


>
> Andy
>
>
>
> From: Hong Ooi
>>
>> Hi,
>>
>> I came across some rather unexpected behaviour the other day with
>> assigning names and lists. Here's an example.
>>
>>> z <- list(aaa=1, bbb=2)
>>> z
>> $aaa
>> [1] 1
>>
>> $bbb
>> [1] 2
>>
>> Note that z has members named "aaa" and "bbb". Now:
>>
>>> names(z$a) <- "X"
>>> z
>> $aaa
>> [1] 1
>>
>> $bbb
>> [1] 2
>>
>> $a
>> X
>> 1
>>
>> I would have expected that trying to name z$a would either
>> give an error
>> (because z doesn't have an element "a") or would cause z$aaa to be
>> modified (due to partial name matching).  I didn't expect that a new
>> list member would be created.
>>
>> I've checked that this is consistent across SPlus 2000, SPlus 7, and R
>> 2.2 for Windows. Can someone give an explanation for why this is
>> happening?
>>
>>
>> --
>> Hong Ooi
>> Senior Research Analyst, IAG Limited
>> 388 George St, Sydney NSW 2000
>> (02) 9292 1566
>>
>>
>> ______________________________________________________________
>> _________________________
>>
>> The information transmitted in this message and its
>> attachments (if any) is intended
>> only for the person or entity to which it is addressed.
>> The message may contain confidential and/or privileged
>> material. Any review,
>> retransmission, dissemination or other use of, or taking of
>> any action in reliance
>> upon this information, by persons or entities other than the
>> intended recipient is
>> prohibited.
>>
>> If you have received this in error, please contact the sender
>> and delete this e-mail
>> and associated material from any computer.
>>
>> The intended recipient of this e-mail may only use,
>> reproduce, disclose or distribute
>> the information contained in this e-mail and any attached
>> files, with the permission
>> of the sender.
>>
>> This message has been scanned for viruses with Symantec Scan
>> Engine and cleared by
>> MailMarshal.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Nov 16 20:23:27 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 16 Nov 2005 14:23:27 -0500
Subject: [R] "Warning message: package '...' was built under R version	
	2.3.0"
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F415E@us-arlington-0668.mail.saic.com>

I do have single version of R (2.2.0) and all packages I have were installed
by downloading precompiled packages from PA or NC repositories within last
month. 

I also just managed to download 'modeltools' from PA2 and its DESCRIPTION
says:
Built: R 2.3.0; ; 2005-11-04 20:47:47; windows

R console:

	> utils:::menuInstallPkgs()
	--- Please select a CRAN mirror for use in this session ---
	trying URL
'http://lib.stat.cmu.edu/R/CRAN/bin/windows/contrib/2.2/modeltools_0.2-0.zip
'
	Content type 'application/zip' length 64426 bytes
	opened URL
	downloaded 62Kb

	package 'modeltools' successfully unpacked and MD5 sums checked

	The downloaded packages are in
	        C:\Documents and Settings\tuszynskij\Local
Settings\Temp\Rtmp4458\downloaded_packages
	updating HTML package descriptions
	> library(modeltools)
	Error in load(dataFile, ns) : ReadItem: unknown type 241
	In addition: Warning message:
	package 'modeltools' was built under R version 2.3.0 
	Error: unable to load R code in package 'modeltools'
	Error: package/namespace load failed for 'modeltools'

The same package downloaded from PA1 does not give me any warnings or
errors.

Jarek Tuszynski


-----Original Message-----
From: ligges at statistik.uni-dortmund.de
[mailto:ligges at statistik.uni-dortmund.de] 
Sent: Wednesday, November 16, 2005 1:04 PM
To: Tuszynski, Jaroslaw W.
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] "Warning message: package '...' was built under R version
2.3.0"


Tuszynski, Jaroslaw W. wrote:

> Uwe,
> 
> I think I used following repositories: PA1, PA2 and NC, which are the 
> closest to me and I think I got the same results in all. The package I 
> downloaded was "coin" which downloaded other packages. The consol 
> printout
> follows:
> 
> 
> 
>>chooseCRANmirror()
>>utils:::menuInstallPkgs()
> 
> trying URL 
> 'http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.2/c
> oin_0.
> 3-3.zip'
> Content type 'application/zip' length 793299 bytes
> opened URL
> downloaded 774Kb
> 
> package 'coin' successfully unpacked and MD5 sums checked
> 
> The downloaded packages are in
>         C:\Documents and Settings\tuszynskij\Local 
> Settings\Temp\Rtmp7594\downloaded_packages
> updating HTML package descriptions
> 
>>library(coin)
> 
> Loading required package: survival
> Loading required package: splines
> Loading required package: mvtnorm
> Loading required package: modeltools
> Error in load(dataFile, ns) : ReadItem: unknown type 241
> In addition: Warning messages:
> 1: package 'mvtnorm' was built under R version 2.3.0


I have just downloaded
http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.2/mvtnorm_
0.7-2.zip
and its DESCRIPTION file correctly has:
"Built: R 2.2.0; i386-pc-mingw32; 2005-10-14 14:44:12; windows"


My guess is that you are using one library for two different R versions 
and you have installed some R-devel package into the library you are now 
using with R-2.2.0 ...

You cannot mix binary packages for R < 2.3.0 with those for R-devel due 
to changes in environment handling.

Uwe Ligges



From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 16 21:37:50 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 16 Nov 2005 20:37:50 -0000 (GMT)
Subject: [R] normal cdf over an interval
In-Reply-To: <001501c5eada$367785e0$0500a8c0@OliviaIBM>
Message-ID: <XFMail.051116203750.Ted.Harding@nessie.mcc.ac.uk>

On 16-Nov-05 Olivia Lau wrote:
> Hi, 
> 
> I'm trying to find a way to take evaluate the Normal CDF over an
> interval and return the result on the log scale.  This works, but I
> think it isn't numerically stable:  
> 
> log(pnorm(a, mean = x, sd = y) - pnorm(b, mean = x, sd = y))
> 
> Does anyone know of a single function that does the above?  Or knows of
> a way to make it more stable?  I'd really appreciate any suggestions!  
> 
> Thanks, 
> 
> Olivia

A little more detail might be helpful.

Your formula is simple enough in itself, and you should not be
getting numerical stability problems for reasonable values of
(a-x)/y or (b-x)/y (say in the range -4 to 4). And I assume
you're being careful that a > b!

So for what values of your variables are problems arising?
And how do they manifest themselves?

If it's due to large values of (a-x)/y etc., then possibly
an asymptotic approximation may serve. There are some good
ones around.

Can you tell us a bit more?

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Nov-05                                       Time: 20:37:47
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Wed Nov 16 21:56:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Nov 2005 20:56:27 +0000 (GMT)
Subject: [R] RODBC and Very long field lengths
In-Reply-To: <1132150563.4819.107.camel@localhost.localdomain>
References: <1132150563.4819.107.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0511162050590.23695@gannet.stats>

On Wed, 16 Nov 2005, tom wright wrote:

> I'm having a small problem using RODBC. I'm trying to retrieve a string
> from a very long memo field (512*20*9=9360 characters = 74880 bytes) in
> an MSAccess database.
> It appears that RODBC set a maximum buffer size for a single column of
> 65535 bytes.

Well, it is documented in the ChangeLog.  (The limit used to be much 
lower.)

> ########## cut from RODBC.c ##########
> 	} else { /* transfer as character */
> 	    int datalen = thisHandle->ColData[i].ColSize;
> 	    if (datalen <= 0 || datalen < COLMAX) datalen = COLMAX;
> 	    /* sanity check as the reports are sometimes unreliable */
> 	    if (datalen > 65535) datalen = 65535;
> ######################################
>
> Can I increase this by just changeing the value in RODBC.c? If so how do
> I get R  to re-compile the package?

Yes, and you re-compile it just like any other package, see the rw-FAQ, 
the R-admin manual and the latest Helpdesk in R-news, for example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Nov 16 22:21:01 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Nov 2005 22:21:01 +0100
Subject: [R] "Warning message: package '...' was built under R version	
	2.3.0"
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F415E@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F415E@us-arlington-0668.mail.saic.com>
Message-ID: <x2wtj8uw5e.fsf@turmalin.kubism.ku.dk>

"Tuszynski, Jaroslaw W." <JAROSLAW.W.TUSZYNSKI at saic.com> writes:

> I do have single version of R (2.2.0) and all packages I have were installed
> by downloading precompiled packages from PA or NC repositories within last
> month. 
> 
> I also just managed to download 'modeltools' from PA2 and its DESCRIPTION
> says:
> Built: R 2.3.0; ; 2005-11-04 20:47:47; windows
> 
> R console:
> 
> 	> utils:::menuInstallPkgs()
> 	--- Please select a CRAN mirror for use in this session ---
> 	trying URL
> 'http://lib.stat.cmu.edu/R/CRAN/bin/windows/contrib/2.2/modeltools_0.2-0.zip
> '
> 	Content type 'application/zip' length 64426 bytes
> 	opened URL
> 	downloaded 62Kb
> 
> 	package 'modeltools' successfully unpacked and MD5 sums checked
> 
> 	The downloaded packages are in
> 	        C:\Documents and Settings\tuszynskij\Local
> Settings\Temp\Rtmp4458\downloaded_packages
> 	updating HTML package descriptions
> 	> library(modeltools)
> 	Error in load(dataFile, ns) : ReadItem: unknown type 241
> 	In addition: Warning message:
> 	package 'modeltools' was built under R version 2.3.0 
> 	Error: unable to load R code in package 'modeltools'
> 	Error: package/namespace load failed for 'modeltools'
> 
> The same package downloaded from PA1 does not give me any warnings or
> errors.
> 
> Jarek Tuszynski

Yes. The Statlib mirror (which I suppose is what you call PA2) appears
to be badly messed up.

The packages under 

http://lib.stat.cmu.edu/R/CRAN/bin/windows/contrib

seem not to have been updated since 2004, whereas (e.g.)

http://lib.stat.cmu.edu/R/CRAN/bin/windows/contrib/r-release/gRbase_0.1.23.zip

carries the Built: 2.3.0 line in the DESCRIPTION file.

Even stranger, looking in R/CRAN/bin/windows/contrib shows no
subfolder r-release, and R/CRAN/bin/windows/contrib/r-release contains
version 0.1.13 of gRbase!

In short, use another mirror until this one gets sorted out...



> 
> -----Original Message-----
> From: ligges at statistik.uni-dortmund.de
> [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: Wednesday, November 16, 2005 1:04 PM
> To: Tuszynski, Jaroslaw W.
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] "Warning message: package '...' was built under R version
> 2.3.0"
> 
> 
> Tuszynski, Jaroslaw W. wrote:
> 
> > Uwe,
> > 
> > I think I used following repositories: PA1, PA2 and NC, which are the 
> > closest to me and I think I got the same results in all. The package I 
> > downloaded was "coin" which downloaded other packages. The consol 
> > printout
> > follows:
> > 
> > 
> > 
> >>chooseCRANmirror()
> >>utils:::menuInstallPkgs()
> > 
> > trying URL 
> > 'http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.2/c
> > oin_0.
> > 3-3.zip'
> > Content type 'application/zip' length 793299 bytes
> > opened URL
> > downloaded 774Kb
> > 
> > package 'coin' successfully unpacked and MD5 sums checked
> > 
> > The downloaded packages are in
> >         C:\Documents and Settings\tuszynskij\Local 
> > Settings\Temp\Rtmp7594\downloaded_packages
> > updating HTML package descriptions
> > 
> >>library(coin)
> > 
> > Loading required package: survival
> > Loading required package: splines
> > Loading required package: mvtnorm
> > Loading required package: modeltools
> > Error in load(dataFile, ns) : ReadItem: unknown type 241
> > In addition: Warning messages:
> > 1: package 'mvtnorm' was built under R version 2.3.0
> 
> 
> I have just downloaded
> http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.2/mvtnorm_
> 0.7-2.zip
> and its DESCRIPTION file correctly has:
> "Built: R 2.2.0; i386-pc-mingw32; 2005-10-14 14:44:12; windows"
> 
> 
> My guess is that you are using one library for two different R versions 
> and you have installed some R-devel package into the library you are now 
> using with R-2.2.0 ...
> 
> You cannot mix binary packages for R < 2.3.0 with those for R-devel due 
> to changes in environment handling.
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From yz8 at georgetown.edu  Wed Nov 16 22:47:27 2005
From: yz8 at georgetown.edu (Jack Zhu)
Date: Wed, 16 Nov 2005 16:47:27 -0500
Subject: [R] X11 error in png
Message-ID: <001601c5eaf7$56ae91b0$ede9a18d@clarkepower>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/8b13eaf2/attachment.pl

From julio_semprones at yahoo.co.uk  Wed Nov 16 22:53:19 2005
From: julio_semprones at yahoo.co.uk (Julio Thomas)
Date: Wed, 16 Nov 2005 21:53:19 +0000 (GMT)
Subject: [R] How to choose a validation set
Message-ID: <20051116215319.57397.qmail@web26603.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/1e094bb1/attachment.pl

From andy_liaw at merck.com  Wed Nov 16 22:55:50 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 16 Nov 2005 16:55:50 -0500
Subject: [R] X11 error in png
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5B8@usctmx1106.merck.com>

Does this help?
http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-do-I-produce-PNG-graphics-i
n-batch-mode_003f

Andy

From: Jack Zhu
> 
> Hi all,
> 
> When I ran a script containing the following codes:
> 
>     png(paste(savepath,"a_rnaplot.png",sep = ""),width = 
> fwidth, height = fheight,pointsize = fpointsize);
>     data_deg <- AffyRNAdeg(data_cel)
>     plotAffyRNAdeg(data_deg,col=cols,lty=1,lwd = "2")
>     #a <- par("fin")
>     legend("bottomright",sampleNames(data_cel),col=cols,lty=1)
>     RNAdegSlope = cbind("Sample Names" = 
> data_deg$sample.names, "RNA Deg Slope" = data_deg$slope)
>     write.csv(RNAdegSlope, file = 
> paste(savepath,"RNAdegSlope.csv",sep = ""))
>     dev.off()
> 
> I got this error message:
>     Error in X11(paste("png::", filename, sep = ""), width, 
> height, pointsize,  : 
>             unable to start device PNG
>     In addition: Warning message:
>     unable to open connection to X11 display '' 
> 
> 
> My system:
> 
> Linux AS 4.0 64-bit
> 
> 
> 
> 
> 
> PATH=/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/bin:/bin:
> /usr/bin:/usr/X11
> 
> 
> 
> Jack Zhu, MD, MS
> 
> Lombardi Cancer center
> Georgetown University
> 3970 Reservoir Rd, NW, NRB, W405b
> Washington, DC 20057
> Email: yz8 at georgetown.edu
> Tel: (202)-687-7451
> Web: http://clarkelabs.georgetown.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Nov 16 23:08:49 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 16 Nov 2005 17:08:49 -0500
Subject: [R] "Warning message: package '...' was built under R version		
	2.3.0"
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4161@us-arlington-0668.mail.saic.com>


>> Yes. The Statlib mirror (which I suppose is what you call PA2) appears to

>> be badly messed up.
>>
>> -- 
>>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
35327918
>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
35327907

Peter,

Thanks for checking on this problem. 

My names for CRAN mirror sites (PA 1, PA 2, NC, etc.) came from that is
shown in windows RGui when you choose "Packages/Set CRAN mirror" menu. All
mirrors are represented by "Country (city)" for most of the world except US
which is shown as "USA (state abbreviation + number)". So what I call "PA 1"
means "USA/Pennsylvania #1", etc. Names you are using ("Statlib mirror") are
not visible in GUI. Just a clarification.

Jarek Tuszynski



From subianto at gmail.com  Wed Nov 16 23:19:12 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Wed, 16 Nov 2005 23:19:12 +0100
Subject: [R] update R packages in local repos
Message-ID: <3635ddc20511161419p1a8e2706k341f0566c15207c0@mail.gmail.com>

I try to update R packages via my local repository.
I put all R packages in g:/myFolder/myRepository, I do like

> library(tools)
> write_PACKAGES("g:/myFolder/myRepository")
> options(repos=c(LocalR="file://g:/myFolder/myRepository"))
> getOption("repos")
                           LocalR
"file://g:/myFolder/myRepository"
> update.packages(ask = "graphics")
Error in gzfile(file, "r") : unable to open connection
In addition: Warning message:
cannot open compressed file
':/myFolder/myRepository/bin/windows/contrib/2.2/PACKAGES'
>
> ?update.packages

It produces these file in g:/myFolder/myRepository
PACKAGES
PACKAGES.gz

Could I make this folder (bin/windows/contrib/2.2/)? Why?

Regards, Muhammad Subianto


> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    2.0
year     2005
month    10
day      06
svn rev  35749
language R
>



From ehlers at math.ucalgary.ca  Wed Nov 16 23:50:37 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Wed, 16 Nov 2005 15:50:37 -0700
Subject: [R] update R packages in local repos
In-Reply-To: <3635ddc20511161419p1a8e2706k341f0566c15207c0@mail.gmail.com>
References: <3635ddc20511161419p1a8e2706k341f0566c15207c0@mail.gmail.com>
Message-ID: <437BB7BD.6030906@math.ucalgary.ca>

This should work:

update.packages(ask = "graphics", repos = NULL,
     contriburl = "file:///g:/myFolder/myRepository"))

-peter

Muhammad Subianto wrote:
> I try to update R packages via my local repository.
> I put all R packages in g:/myFolder/myRepository, I do like
> 
> 
>>library(tools)
>>write_PACKAGES("g:/myFolder/myRepository")
>>options(repos=c(LocalR="file://g:/myFolder/myRepository"))
>>getOption("repos")
> 
>                            LocalR
> "file://g:/myFolder/myRepository"
> 
>>update.packages(ask = "graphics")
> 
> Error in gzfile(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open compressed file
> ':/myFolder/myRepository/bin/windows/contrib/2.2/PACKAGES'
> 
>>?update.packages
> 
> 
> It produces these file in g:/myFolder/myRepository
> PACKAGES
> PACKAGES.gz
> 
> Could I make this folder (bin/windows/contrib/2.2/)? Why?
> 
> Regards, Muhammad Subianto
> 
> 
> 
>>version
> 
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.0
> year     2005
> month    10
> day      06
> svn rev  35749
> language R
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Peter Ehlers
Department of Mathematics and Statistics
University of Calgary, 2500 University Dr. NW



From d.scott at auckland.ac.nz  Thu Nov 17 00:40:41 2005
From: d.scott at auckland.ac.nz (David Scott)
Date: Thu, 17 Nov 2005 12:40:41 +1300 (NZDT)
Subject: [R] X11 error in png
Message-ID: <Pine.LNX.4.60.0511171235050.7255@stat71.stat.auckland.ac.nz>


I managed to delete the original post about this and will no doubt mess up 
threading of responses. The post is shown below.

This looks like a problem I saw just a couple of days ago. I noticed that 
you used a path variable in determining the file location and name. If 
something is wrong with your path (e.g. the directories it specifies don't 
exist) I think you will get this error.

Check your path to make sure it is ok.

David Scott


*******************************
Original post
*******************************


From: Jack Zhu <yz8_at_georgetown.edu>
Date: Thu 17 Nov 2005 - 08:47:27 EST


Hi all,

When I ran a script containing the following codes:

     png(paste(savepath,"a_rnaplot.png",sep = ""),width = fwidth, height = 
fheight,pointsize = fpointsize);     data_deg <- AffyRNAdeg(data_cel)
     plotAffyRNAdeg(data_deg,col=cols,lty=1,lwd = "2")     #a <- par("fin")
     legend("bottomright",sampleNames(data_cel),col=cols,lty=1) 
RNAdegSlope = cbind("Sample Names" = data_deg$sample.names, "RNA Deg 
Slope" = data_deg$slope)     write.csv(RNAdegSlope, file = 
paste(savepath,"RNAdegSlope.csv",sep = ""))     dev.off()

I got this error message:

     Error in X11(paste("png::", filename, sep = ""), width, height, 
pointsize, :

             unable to start device PNG
     In addition: Warning message:
     unable to open connection to X11 display ''

My system:

Linux AS 4.0 64-bit

PATH=/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin:/usr/X11

Jack Zhu, MD, MS

Lombardi Cancer center
Georgetown University
3970 Reservoir Rd, NW, NRB, W405b
Washington, DC 20057
Email: yz8 at georgetown.edu
Tel: (202)-687-7451
Web: http://clarkelabs.georgetown.edu



-- 
_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From zwang at scharp.org  Thu Nov 17 00:46:52 2005
From: zwang at scharp.org (Zhu Wang)
Date: Wed, 16 Nov 2005 15:46:52 -0800
Subject: [R] GEE in Fortran
Message-ID: <437BC4EC.90805@scharp.org>

Dear all,

Does anybody happen to have written some GEE source code in Fortran to 
solve a Generalized Estimation Equation Model? Or kindly point out me a 
good starting point. Thanks.

 Zhu Wang
 SCHARP



From kpangolo at hotmail.com  Thu Nov 17 03:04:23 2005
From: kpangolo at hotmail.com (Louis Longchamps)
Date: Thu, 17 Nov 2005 02:04:23 +0000
Subject: [R] Is it feasible with R?
Message-ID: <BAY104-F34DE72ECCD4A48FAE7B201D05F0@phx.gbl>


   My data are (for one experiment):

   3 Types of plant (1,2,3)

   4  Species  per  Type (ex:for type 1(a,b,c,d), for type 2(e,f,g,h) and
   for type 3(i,j,k,l)

   8 Repetitions of each Species

   3 Stages (10, 20, 30)

   2  Measures per Stages, for the stages 20 and 30 (ex: for Stage 10(C),
   for Stage 20 (A and B) and for Stage 30 (A and B)

   3  Types  x  4 Species(Type) x 8 Repetitions x (1 Stage x 1 Mesure + 2
   Stages  x  2  Measures= 480 data. There are 10 data missing. The final
   number of data is 470.

   My questions that I ask to my data:

   1- Is there a significant difference between Measure A and Measure B?

   2- Is there a significant difference between Stages 10, 20 and 30?

   3- Is there a significant difference between the 3 plant Types?

   I haven't found  any way to consider the whole data lot in one shot, I
   decided to remove the first stage with the Measure "C" that comes with
   it. It bring the number of data to 384.

   Is it possible to consider the whole lot in one shot?

   Thus my model is

   Source                                                d.l.

   Types                                                  2

   Species (Type)                                      9

   Error a)                                                84


   Stage                                                  1

   Measure                                               1

   Stage x Measure                                   1

   Stage x Type                                        2

   Measure x Type                                     2

   Stage x Measure x Type                         2

   Stage x Species (Type)                          9

   Measure x Species (Type)                       9

   Stage x Measure x Species (Type)            9

   Erreur b)                                             252

   Total                                                   383

   How  can I write this in R? My problem is that when I use %in% to nest
   Species in Type, R looks, for exemple, for specie "a" in Type 2 and it
   finds nothing and returns "Na".

   Type1:Plante[T.a]  -1.51967    0.82774  -1.836   0.0672 .
   Type2:Plante[T.a]        NA         NA      NA       NA

   Can  you  help  me?  Can  R  consider  Species a,b,c,d only in Type 1,
   Species e,f,g,h only in Type 2 and Species i,j,k,l only in Type 3?

   Thank you very much.


From monch1962 at gmail.com  Thu Nov 17 04:16:53 2005
From: monch1962 at gmail.com (David Mitchell)
Date: Thu, 17 Nov 2005 14:16:53 +1100
Subject: [R] Portable R?
Message-ID: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>

Hello list,

A short time ago, I found
http://johnhaller.com/jh/useful_stuff/portable_apps_suite/, which
contains basically a complete set of office tools that can be run
*entirely* from a USB key.  The concept is:
- find a Windows PC
- put in your USB key
- run OpenOffice, Firefox, Gaim, Nvu, Thunderbird, ... directly from
your USB key, with no app installation required
- save your files wherever
- remove your USB key and leave, with nothing installed on the original PC

As a consultant who battles regularly with limited toolsets at
customer sites, this strikes me as an extremely handy way of working.

Has anyone managed to setup a base R configuration that runs entirely
from USB key?  Being a regular user, but no expert, with R, it'd be
very helpful for me if such a mechanism existed, but I've got no idea
where to begin in building such a thing.

Thanks in advance for any responses or suggestions

Dave M.



From liuwensui at gmail.com  Thu Nov 17 04:29:35 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 16 Nov 2005 22:29:35 -0500
Subject: [R] Portable R?
In-Reply-To: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>
References: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>
Message-ID: <1115a2b00511161929h111a6c3bid62b470bebcb7bfe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/33c1ba6b/attachment.pl

From edd at debian.org  Thu Nov 17 04:39:26 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 16 Nov 2005 21:39:26 -0600
Subject: [R] Portable R?
In-Reply-To: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>
References: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>
Message-ID: <17275.64366.678545.955642@basebud.nulle.part>


On 17 November 2005 at 14:16, David Mitchell wrote:
| Hello list,
| 
| A short time ago, I found
| http://johnhaller.com/jh/useful_stuff/portable_apps_suite/, which
| contains basically a complete set of office tools that can be run
| *entirely* from a USB key.  The concept is:
| - find a Windows PC
| - put in your USB key
| - run OpenOffice, Firefox, Gaim, Nvu, Thunderbird, ... directly from
| your USB key, with no app installation required
| - save your files wherever
| - remove your USB key and leave, with nothing installed on the original PC
| 
| As a consultant who battles regularly with limited toolsets at
| customer sites, this strikes me as an extremely handy way of working.
| 
| Has anyone managed to setup a base R configuration that runs entirely
| from USB key?  Being a regular user, but no expert, with R, it'd be
| very helpful for me if such a mechanism existed, but I've got no idea
| where to begin in building such a thing.

Short answer:
	Yes but using Linux, requiring a larger USB stick and some fiddling.

Longer answer: 
	Quantian (http://dirk.eddelbuettel.com/quantian) is a "everything, 
the kitchen sink and some" Linux distribution running off a DVD. Quantian is
focussed on quantitative / numeric apps, and tends to include R plus related
goodies -- the last release had an almost complete set of CRAN and
BioConductor packages. The raw size of the last release is around 2 GB,
corresponding to 6.6 GB expanded.  Marco Caliari, who often contributes
improved boot code to Quantian, has managed to boot Quantian off a USB
stick. I didn't manage to do that with my laptop, possibly because of
limitations in its bios. Some of this was discussed in past threads on the
quantian-general mailing list.

Lots-o-work suggestion:
	To not require a huge USB stick, you could try to shrink a given live
cdrom such as Knoppix or Ubuntu, then add R and other goodies such that
you're left with around 512 MB compressed. Then throw it onto a USB stick and
make it bootable.

Shortcut:
	Order a Quantian DVD. Some folks sell them pre-made for less than $5.
Experiment with that, If you like it, consider making your own mini-distro.
Or stick with the DVD and use it directly with the USB stick for your
configuration, data, demos, ...

Even shorter:
	R is perfectly "relocatable". If you install the Windows binary onto
the USB drive, it will run fine. You'll probably need to add editors and
other tools.

Hope this helps, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From leavestonebodt at yahoo.com  Thu Nov 17 04:55:05 2005
From: leavestonebodt at yahoo.com (yuying shi)
Date: Wed, 16 Nov 2005 19:55:05 -0800 (PST)
Subject: [R] some questions
Message-ID: <20051117035505.65229.qmail@web52505.mail.yahoo.com>

Dear R expert,
 The following is my questions:

1. How to generate two sequences of 150 uniform
deviates, called v1 and v2, in the range [-1,1]. 
2. How to compute  r=(v1)^2+(v2)^2
3.If r is outside the range of (0,1) then it will be
discarded. 
4 How to compute (v1)*sqrt(-2*log(r)/r) and output the
first 100 z values in the list. 
5. Plot histograms for the sequences from steps 1 and
2. 

Thanks very much for your help!

xingyu



From syy2004 at gmail.com  Thu Nov 17 04:57:34 2005
From: syy2004 at gmail.com (Yuying Shi)
Date: Wed, 16 Nov 2005 22:57:34 -0500
Subject: [R] R questions
Message-ID: <91d269c60511161957x525359f5h77db07ab2b9f7178@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051116/5014fcc0/attachment.pl

From monch1962 at gmail.com  Thu Nov 17 05:11:45 2005
From: monch1962 at gmail.com (David Mitchell)
Date: Thu, 17 Nov 2005 15:11:45 +1100
Subject: [R] Portable R?
In-Reply-To: <17275.64366.678545.955642@basebud.nulle.part>
References: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>
	<17275.64366.678545.955642@basebud.nulle.part>
Message-ID: <f6508a860511162011p7a029d51nca5ceaf29ce44bce@mail.gmail.com>

Thanks guys,

Dirk: I generally resort to using my laptop for R analysis, but it
usually involves dragging loads of data multiple times between a
customer system and my laptop.  Moving large amounts of data in this
fashion can be a problem, particularly when there's sensitivity issues
about the data itself.

I've used Quantian in the past, and I've customised my own Knoppix
CDs, but ideally I'd like to run *my* tools (i.e. R) on *their* system
so I'm not having to deal with the logistical and security issues that
come with moving data around between systems.

I didn't realise that the Windows version of R was "relocatable" that
easily - I just assumed the install did something more complex than
that.  I'll try copying the R files to a USB key later today.

Thanks again

Dave M.


On 11/17/05, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> On 17 November 2005 at 14:16, David Mitchell wrote:
> | Hello list,
> |
> | A short time ago, I found
> | http://johnhaller.com/jh/useful_stuff/portable_apps_suite/, which
> | contains basically a complete set of office tools that can be run
> | *entirely* from a USB key.  The concept is:
> | - find a Windows PC
> | - put in your USB key
> | - run OpenOffice, Firefox, Gaim, Nvu, Thunderbird, ... directly from
> | your USB key, with no app installation required
> | - save your files wherever
> | - remove your USB key and leave, with nothing installed on the original PC
> |
> | As a consultant who battles regularly with limited toolsets at
> | customer sites, this strikes me as an extremely handy way of working.
> |
> | Has anyone managed to setup a base R configuration that runs entirely
> | from USB key?  Being a regular user, but no expert, with R, it'd be
> | very helpful for me if such a mechanism existed, but I've got no idea
> | where to begin in building such a thing.
>
> Short answer:
>         Yes but using Linux, requiring a larger USB stick and some fiddling.
>
> Longer answer:
>         Quantian (http://dirk.eddelbuettel.com/quantian) is a "everything,
> the kitchen sink and some" Linux distribution running off a DVD. Quantian is
> focussed on quantitative / numeric apps, and tends to include R plus related
> goodies -- the last release had an almost complete set of CRAN and
> BioConductor packages. The raw size of the last release is around 2 GB,
> corresponding to 6.6 GB expanded.  Marco Caliari, who often contributes
> improved boot code to Quantian, has managed to boot Quantian off a USB
> stick. I didn't manage to do that with my laptop, possibly because of
> limitations in its bios. Some of this was discussed in past threads on the
> quantian-general mailing list.
>
> Lots-o-work suggestion:
>         To not require a huge USB stick, you could try to shrink a given live
> cdrom such as Knoppix or Ubuntu, then add R and other goodies such that
> you're left with around 512 MB compressed. Then throw it onto a USB stick and
> make it bootable.
>
> Shortcut:
>         Order a Quantian DVD. Some folks sell them pre-made for less than $5.
> Experiment with that, If you like it, consider making your own mini-distro.
> Or stick with the DVD and use it directly with the USB stick for your
> configuration, data, demos, ...
>
> Even shorter:
>         R is perfectly "relocatable". If you install the Windows binary onto
> the USB drive, it will run fine. You'll probably need to add editors and
> other tools.
>
> Hope this helps, Dirk
>
> --
> Statistics: The (futile) attempt to offer certainty about uncertainty.
>          -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'
>



From jjmichael at comcast.net  Thu Nov 17 05:19:03 2005
From: jjmichael at comcast.net (Jacob Michaelson)
Date: Wed, 16 Nov 2005 21:19:03 -0700
Subject: [R] Portable R?
In-Reply-To: <f6508a860511162011p7a029d51nca5ceaf29ce44bce@mail.gmail.com>
References: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>
	<17275.64366.678545.955642@basebud.nulle.part>
	<f6508a860511162011p7a029d51nca5ceaf29ce44bce@mail.gmail.com>
Message-ID: <249CDD52-C135-4778-B621-739AB38CEFD1@comcast.net>

I use SLAX on my USB stick (1GB) and I carry R, a host of contributed  
R packages, all the core BioC packages, rkward as the R frontend,  
Octave, and many many more.  SLAX is in my experience much smaller,  
lighter, faster, and more complete than any of the other Linux Live  
CDs out there (my installation with all these packages is about ~600  
MB).  Heck, I even installed my remastered SLAX onto my iPod nano!

Give it a shot.

--Jake

On Nov 16, 2005, at 9:11 PM, David Mitchell wrote:

> Thanks guys,
>
> Dirk: I generally resort to using my laptop for R analysis, but it
> usually involves dragging loads of data multiple times between a
> customer system and my laptop.  Moving large amounts of data in this
> fashion can be a problem, particularly when there's sensitivity issues
> about the data itself.
>
> I've used Quantian in the past, and I've customised my own Knoppix
> CDs, but ideally I'd like to run *my* tools (i.e. R) on *their* system
> so I'm not having to deal with the logistical and security issues that
> come with moving data around between systems.
>
> I didn't realise that the Windows version of R was "relocatable" that
> easily - I just assumed the install did something more complex than
> that.  I'll try copying the R files to a USB key later today.
>
> Thanks again
>
> Dave M.
>
>
> On 11/17/05, Dirk Eddelbuettel <edd at debian.org> wrote:
>>
>> On 17 November 2005 at 14:16, David Mitchell wrote:
>> | Hello list,
>> |
>> | A short time ago, I found
>> | http://johnhaller.com/jh/useful_stuff/portable_apps_suite/, which
>> | contains basically a complete set of office tools that can be run
>> | *entirely* from a USB key.  The concept is:
>> | - find a Windows PC
>> | - put in your USB key
>> | - run OpenOffice, Firefox, Gaim, Nvu, Thunderbird, ... directly  
>> from
>> | your USB key, with no app installation required
>> | - save your files wherever
>> | - remove your USB key and leave, with nothing installed on the  
>> original PC
>> |
>> | As a consultant who battles regularly with limited toolsets at
>> | customer sites, this strikes me as an extremely handy way of  
>> working.
>> |
>> | Has anyone managed to setup a base R configuration that runs  
>> entirely
>> | from USB key?  Being a regular user, but no expert, with R, it'd be
>> | very helpful for me if such a mechanism existed, but I've got no  
>> idea
>> | where to begin in building such a thing.
>>
>> Short answer:
>>         Yes but using Linux, requiring a larger USB stick and some  
>> fiddling.
>>
>> Longer answer:
>>         Quantian (http://dirk.eddelbuettel.com/quantian) is a  
>> "everything,
>> the kitchen sink and some" Linux distribution running off a DVD.  
>> Quantian is
>> focussed on quantitative / numeric apps, and tends to include R  
>> plus related
>> goodies -- the last release had an almost complete set of CRAN and
>> BioConductor packages. The raw size of the last release is around  
>> 2 GB,
>> corresponding to 6.6 GB expanded.  Marco Caliari, who often  
>> contributes
>> improved boot code to Quantian, has managed to boot Quantian off a  
>> USB
>> stick. I didn't manage to do that with my laptop, possibly because of
>> limitations in its bios. Some of this was discussed in past  
>> threads on the
>> quantian-general mailing list.
>>
>> Lots-o-work suggestion:
>>         To not require a huge USB stick, you could try to shrink a  
>> given live
>> cdrom such as Knoppix or Ubuntu, then add R and other goodies such  
>> that
>> you're left with around 512 MB compressed. Then throw it onto a  
>> USB stick and
>> make it bootable.
>>
>> Shortcut:
>>         Order a Quantian DVD. Some folks sell them pre-made for  
>> less than $5.
>> Experiment with that, If you like it, consider making your own  
>> mini-distro.
>> Or stick with the DVD and use it directly with the USB stick for your
>> configuration, data, demos, ...
>>
>> Even shorter:
>>         R is perfectly "relocatable". If you install the Windows  
>> binary onto
>> the USB drive, it will run fine. You'll probably need to add  
>> editors and
>> other tools.
>>
>> Hope this helps, Dirk
>>
>> --
>> Statistics: The (futile) attempt to offer certainty about  
>> uncertainty.
>>          -- Roger Koenker, 'Dictionary of Received Ideas of  
>> Statistics'
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From edd at debian.org  Thu Nov 17 05:33:02 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 16 Nov 2005 22:33:02 -0600
Subject: [R] Portable R?
In-Reply-To: <f6508a860511162011p7a029d51nca5ceaf29ce44bce@mail.gmail.com>
References: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>
	<17275.64366.678545.955642@basebud.nulle.part>
	<f6508a860511162011p7a029d51nca5ceaf29ce44bce@mail.gmail.com>
Message-ID: <17276.2046.639384.251176@basebud.nulle.part>


On 17 November 2005 at 15:11, David Mitchell wrote:
| I didn't realise that the Windows version of R was "relocatable" that
| easily - I just assumed the install did something more complex than
| that.  I'll try copying the R files to a USB key later today.

Yup. I've installed it onto Windows SMB shares to provide little R apps with
tcl/tk apps to colleagues. Works very well until you hit snags like requiring
ODBC entry on each client which one can't script ... (as far as I know).

Cheers, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From S.Eagleson at exchange.curtin.edu.au  Thu Nov 17 06:34:49 2005
From: S.Eagleson at exchange.curtin.edu.au (Serryn Eagleson)
Date: Thu, 17 Nov 2005 13:34:49 +0800
Subject: [R] Morans I for Spatial Surveillance
Message-ID: <50D7651CF10CA64F9ABD0DE89466AE3B012712DF@exmsp3.perth.ad.curtin.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/9e38563f/attachment.pl

From aleszib at gmail.com  Thu Nov 17 07:06:13 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Thu, 17 Nov 2005 07:06:13 +0100
Subject: [R] some questions
References: <20051117035505.65229.qmail@web52505.mail.yahoo.com>
Message-ID: <021101c5eb3d$054186c0$598debd4@ALES>

Firstly, these are all very basic questions, thah you coukld probobly answer 
yourself by searching the manual and help files. Nevertheless, here are the 
answers to give you a little head start.

> Dear R expert,
> The following is my questions:
>
> 1. How to generate two sequences of 150 uniform
> deviates, called v1 and v2, in the range [-1,1].
v1<-runif(n=150,min=-1,max=1)
v2<-runif(n=150,min=-1,max=1)

> 2. How to compute  r=(v1)^2+(v2)^2
r<-(v1)^2+(v2)^2

> 3.If r is outside the range of (0,1) then it will be
> discarded.
newr<-r[r<1]
newr<-newr[r>0]

> 4 How to compute (v1)*sqrt(-2*log(r)/r) and output the
> first 100 z values in the list.
z<-(v1)*sqrt(-2*log(r)/r)
z[1:100] #or
print(z[1:100])

> 5. Plot histograms for the sequences from steps 1 and
> 2.
hist(v1)
hist(v2)
hist(r)

>
> Thanks very much for your help!
I hope this helps,

Ales Ziberna

>
> xingyu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Thu Nov 17 08:20:35 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 17 Nov 2005 08:20:35 +0100 (CET)
Subject: [R] ECDF values
Message-ID: <20051117072035.2483.qmail@web36115.mail.mud.yahoo.com>

Dear UseRs,

maybe is a silly question: how can I get Empirical CDF
values from an object created with ecdf()?? Using
print I obtain:

Empirical CDF 
Call: ecdf(t)
 x[1:57] =    4.1,    4.4,    4.5,  ...,  491.3,
671.27

Thanks in advance.

Regards,

Vito

Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From ripley at stats.ox.ac.uk  Thu Nov 17 08:27:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Nov 2005 07:27:23 +0000 (GMT)
Subject: [R] update R packages in local repos
In-Reply-To: <437BB7BD.6030906@math.ucalgary.ca>
References: <3635ddc20511161419p1a8e2706k341f0566c15207c0@mail.gmail.com>
	<437BB7BD.6030906@math.ucalgary.ca>
Message-ID: <Pine.LNX.4.61.0511170718380.19598@gannet.stats>

On Wed, 16 Nov 2005, P Ehlers wrote:

> This should work:
>
> update.packages(ask = "graphics", repos = NULL,
>     contriburl = "file:///g:/myFolder/myRepository"))

Only if the so-called repository contains (only) binary builds under R 
2.2.x of packages for Windows.

It would be better to set up the 'repository' correctly as a repository. 
See my article in R-news 5/1 and the R-admin manual for the format of a 
repository.

>
> -peter
>
> Muhammad Subianto wrote:
>> I try to update R packages via my local repository.
>> I put all R packages in g:/myFolder/myRepository, I do like

Are these source packages or binary packages or what?

>>> library(tools)
>>> write_PACKAGES("g:/myFolder/myRepository")
>>> options(repos=c(LocalR="file://g:/myFolder/myRepository"))
>>> getOption("repos")
>>
>>                            LocalR
>> "file://g:/myFolder/myRepository"

Your syntax is incorrect here: Peter has silently corrected it.
file:// syntax has an element for 'machine' followed by a third slash
(although 'machine' is not supported in R, so it should be empty).

>>> update.packages(ask = "graphics")
>>
>> Error in gzfile(file, "r") : unable to open connection
>> In addition: Warning message:
>> cannot open compressed file
>> ':/myFolder/myRepository/bin/windows/contrib/2.2/PACKAGES'
>>
>>> ?update.packages
>>
>>
>> It produces these file in g:/myFolder/myRepository
>> PACKAGES
>> PACKAGES.gz
>>
>> Could I make this folder (bin/windows/contrib/2.2/)? Why?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan.sarkar at gmail.com  Thu Nov 17 08:52:13 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 17 Nov 2005 01:52:13 -0600
Subject: [R] nlme question
In-Reply-To: <AF2DCD619279544BA454141F4A45B9E3F5E18F@m-niosh-3.niosh.cdc.gov>
References: <AF2DCD619279544BA454141F4A45B9E3F5E18F@m-niosh-3.niosh.cdc.gov>
Message-ID: <eb555e660511162352s4112b8d4q960825ed59b6dde6@mail.gmail.com>

On 11/16/05, Wassell, James T., Ph.D. <jtw2 at cdc.gov> wrote:
> I am using the package nlme to fit a simple random effects (variance
> components model)
>
> with 3 parameters:  overall mean (fixed effect), between subject
> variance (random) and  within subject variance (random).

So to paraphrase, your model can be written as (with the index i
representing subject)

y_ij = \mu + b_i + e_ij

where

b_i ~ N(0, \tao^2)
e_ij ~ N(0, \sigma_2)
and all b_i's and e_ij's are mutually independent. The model has, as
you say, 3 parameters, \mu, \tao and \sigma.

> I have 16 subjects with 1-4 obs per subject.
>
> I need a 3x3 variance-covariance matrix that includes all 3 parameters
> in order to compute the variance of a specific linear combination.

Can you specify the 'linear combination' that you want to estimate in
terms of the model above?

Deepayan



From ripley at stats.ox.ac.uk  Thu Nov 17 08:53:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Nov 2005 07:53:49 +0000 (GMT)
Subject: [R] Portable R?
In-Reply-To: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>
References: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0511170738090.19598@gannet.stats>

See the rw-FAQ Q2.6 for a more precise answer than has yet appeared in 
this thread.  A while back (for 2.1.0) some aspects of R were optimized 
for usage from a slow (and possibly read-only) drive.

This does presume that the sites you visit will allow you to run programs 
from an external drive, and in my experience that is often explicitly 
disallowed.

On Thu, 17 Nov 2005, David Mitchell wrote:

> Hello list,
>
> A short time ago, I found
> http://johnhaller.com/jh/useful_stuff/portable_apps_suite/, which
> contains basically a complete set of office tools that can be run
> *entirely* from a USB key.  The concept is:
> - find a Windows PC
> - put in your USB key
> - run OpenOffice, Firefox, Gaim, Nvu, Thunderbird, ... directly from
> your USB key, with no app installation required
> - save your files wherever
> - remove your USB key and leave, with nothing installed on the original PC
>
> As a consultant who battles regularly with limited toolsets at
> customer sites, this strikes me as an extremely handy way of working.
>
> Has anyone managed to setup a base R configuration that runs entirely
> from USB key?  Being a regular user, but no expert, with R, it'd be
> very helpful for me if such a mechanism existed, but I've got no idea
> where to begin in building such a thing.
>
> Thanks in advance for any responses or suggestions

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From patrick.giraudoux at univ-fcomte.fr  Thu Nov 17 08:54:39 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Thu, 17 Nov 2005 08:54:39 +0100
Subject: [R] generalised linear mixed effect model, glmmPQL
Message-ID: <437C373F.2020501@univ-fcomte.fr>

Dear listers,

I am trying to get more familiar with concepts underlying generalised 
linear mixed models, mainly through Venables and Ripley (fourth edition) 
and the R-list archive. Of course, as a  possibly tool-user biologist I 
am not that easy with every d??tails of the mathematical aspects of the 
optimisation methods described. I am trying to sort out which could be 
an acceptable strategy for model comparisons and selection. I have 
understood from the R-list archive that AIC and similar approaches are 
not valid for model comparisons with PQL. On the other hand, table 10.4 
in Venables and Ripley compares the results of various fitting methods 
(thus the fitting methods), but my wonder is about  comparing  models 
(or parameter estimates) given a  method.

Can somebody put me on the track with some hints or  basic references 
for "dummies" if any...

Thanks in advance,

Patrick



From mmiller at nassp.uct.ac.za  Thu Nov 17 08:58:10 2005
From: mmiller at nassp.uct.ac.za (Mark Miller)
Date: Thu, 17 Nov 2005 09:58:10 +0200
Subject: [R] Fitdistr()
Message-ID: <200511170958.10823.mmiller@nassp.uct.ac.za>

When using fitdistr() with the exponential, log-normal and beta distributions, 
you get the relevent rate, mean, standard deviation, shape1 and shape2 but 
you get a number bellow those that are in () and I was wandering what exactly 
those numbers represent and how they relate to the data.

Many thanks
Mark Miller



From vito_ricci at yahoo.com  Thu Nov 17 08:54:53 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 17 Nov 2005 07:54:53 +0000 (UTC)
Subject: [R] ECDF values
Message-ID: <loom.20051117T085402-75@post.gmane.org>

Sorry, I solved by myself.
Thanks.
Vito


From: Vito Ricci <vito_ricci <at> yahoo.com>
Subject: [R] ECDF values
Date: 2005-11-17 07:20:35 GMT (33 minutes ago)

Dear UseRs,

maybe is a silly question: how can I get Empirical CDF
values from an object created with ecdf()?? Using
print I obtain:

Empirical CDF 
Call: ecdf(t)
 x[1:57] =    4.1,    4.4,    4.5,  ...,  491.3,
671.27

Thanks in advance.

Regards,

Vito



From deepayan.sarkar at gmail.com  Thu Nov 17 08:58:34 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 17 Nov 2005 01:58:34 -0600
Subject: [R] ECDF values
In-Reply-To: <20051117072035.2483.qmail@web36115.mail.mud.yahoo.com>
References: <20051117072035.2483.qmail@web36115.mail.mud.yahoo.com>
Message-ID: <eb555e660511162358p2f06c441t53bbf00661784da2@mail.gmail.com>

On 11/17/05, Vito Ricci <vito_ricci at yahoo.com> wrote:
> Dear UseRs,
>
> maybe is a silly question: how can I get Empirical CDF
> values from an object created with ecdf()??

The return value of ecdf is a function. Use it as you would any other function.

-Deepayan



From maechler at stat.math.ethz.ch  Thu Nov 17 09:22:54 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 17 Nov 2005 09:22:54 +0100
Subject: [R] ECDF values
In-Reply-To: <20051117072035.2483.qmail@web36115.mail.mud.yahoo.com>
References: <20051117072035.2483.qmail@web36115.mail.mud.yahoo.com>
Message-ID: <17276.15838.924522.822973@stat.math.ethz.ch>

>>>>> "Vito" == Vito Ricci <vito_ricci at yahoo.com>
>>>>>     on Thu, 17 Nov 2005 08:20:35 +0100 (CET) writes:

    Vito> Dear UseRs,
    Vito> maybe is a silly question: how can I get Empirical CDF
    Vito> values from an object created with ecdf()?? Using
    Vito> print I obtain:

    Vito> Empirical CDF 
    Vito> Call: ecdf(t)
    Vito> x[1:57] =    4.1,    4.4,    4.5,  ...,  491.3, 671.27

help(ecdf) supposedly has enough explanation; however
it seems the author of the help page (guess who) was
mistaken about the "simple" in

>> Examples:
>> 
>>      ##-- Simple didactical  ecdf  example:
>>      Fn <- ecdf(rnorm(12))
>>      Fn; summary(Fn)
>>      12*Fn(knots(Fn)) == 1:12 ## == 1:12  if and only if there are no ties !

The clue is that the result of ecdf() 
{ and also of stepfun(), approxfun() or splinefun() ! }
is a *function*  (with some additional attributes); hence it's
as simple as

 tt <- seq(-3, 3, by = 0.1)
 Fn(tt)

    Vito> Thanks in advance.

you're welcome.
Martin Maechler, ETH Zurich



From buser at stat.math.ethz.ch  Thu Nov 17 09:42:02 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 17 Nov 2005 09:42:02 +0100
Subject: [R] R questions
In-Reply-To: <91d269c60511161957x525359f5h77db07ab2b9f7178@mail.gmail.com>
References: <91d269c60511161957x525359f5h77db07ab2b9f7178@mail.gmail.com>
Message-ID: <17276.16986.175297.978209@stat.math.ethz.ch>

Hi

There is a very good introduction script to R on 

http://www.r-project.org/

under manuals, including an index of nice functions. For
example you will find the basic plots, how to sort vectors and
so on.

Furthermore have a look at

?RSiteSearch

It is very useful to search in R archives for similar
problems. Lots of answers are already there and only wait to be
found. 
So there is a good chance that you will find the information
that you are looking for in these archives.

Best regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------




Yuying Shi writes:
 > Dear Sir/Madam,
 >  I am a beginner in R. Here is my questions.
 >  1. Can you give me one test for randomness (a name and descriptive
 > paragraph is sufficient).
 > 2. I have learned a uniform random number generator [e.g. not the
 > algorithms: i)Wichmann-Hill, ii) Marsaglia-Multicarry, iii) Super-Duper
 > (Marsaglia), iv) Mersenne-Twister, v) TAOCP-1997 (Knuth), or vi) TAOCP-2002
 > (Knuth)] . Is there any other method besides that?
 > 3. How to generate 100 random standard normal deviates using the Box-Muller
 > method for standard normal random deviates and sort the sequence, smallest
 > to largest?
 >   Your kind help is greatly appreciated. Thanks in advance!
 >  best wishes
 > yuying shi
 > 
 > 	[[alternative HTML version deleted]]
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Nov 17 09:44:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Nov 2005 09:44:12 +0100
Subject: [R] R questions
In-Reply-To: <91d269c60511161957x525359f5h77db07ab2b9f7178@mail.gmail.com>
References: <91d269c60511161957x525359f5h77db07ab2b9f7178@mail.gmail.com>
Message-ID: <437C42DC.6090404@statistik.uni-dortmund.de>

Yuying Shi wrote:

> Dear Sir/Madam,
>  I am a beginner in R. Here is my questions.
>  1. Can you give me one test for randomness (a name and descriptive
> paragraph is sufficient).
> 2. I have learned a uniform random number generator [e.g. not the
> algorithms: i)Wichmann-Hill, ii) Marsaglia-Multicarry, iii) Super-Duper
> (Marsaglia), iv) Mersenne-Twister, v) TAOCP-1997 (Knuth), or vi) TAOCP-2002
> (Knuth)] . Is there any other method besides that?
> 3. How to generate 100 random standard normal deviates using the Box-Muller
> method for standard normal random deviates and sort the sequence, smallest
> to largest?
>   Your kind help is greatly appreciated. Thanks in advance!
>  best wishes
> yuying shi

Please do not ask the list to do your homework (I'd highly suspect this 
is homework, in particular from your first question). Excercises are 
there to be solved by yourself. And if that fails, what about asking 
your lecturer rather than thousands of others?

Uwe Ligges


> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From subianto at gmail.com  Thu Nov 17 11:16:53 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Thu, 17 Nov 2005 11:16:53 +0100
Subject: [R] update R packages in local repos
In-Reply-To: <Pine.LNX.4.61.0511170718380.19598@gannet.stats>
References: <3635ddc20511161419p1a8e2706k341f0566c15207c0@mail.gmail.com>	<437BB7BD.6030906@math.ucalgary.ca>
	<Pine.LNX.4.61.0511170718380.19598@gannet.stats>
Message-ID: <437C5895.80709@gmail.com>

Thanks you.
Now, I can install and update some packages from local repos.
I put all R packages (zip files, using W2K)
in h:/myFolder/myRepository,

library(tools)
write_PACKAGES("h:/myFolder/myRepository")
options(repos=c(LocalR="file:///h:/myFolder/myRepository"))
getOption("repos")
install.packages(lib=.libPaths()[3],
                  repos=NULL,
                  contriburl="file:///h:/myFolder/myRepository")
instead of
install.packages(choose.files('',filters=Filters[c('zip','All'),]), 
.libPaths()[3], repos = NULL)

where .libPaths()[3] my local library (added in Renviron.site) and 
update with

update.packages(ask="graphics",
                 repos=NULL,
                 contriburl="file:///h:/myFolder/myRepository")

Best, Muhammad Subianto

Cited:
- R News 5/1
- C:\Program Files\R\R-2.2.0\library\utils\html\update.packages.html

On this day 17/11/2005 08:27 AM, Prof Brian Ripley wrote:
> On Wed, 16 Nov 2005, P Ehlers wrote:
> 
> 
>>This should work:
>>
>>update.packages(ask = "graphics", repos = NULL,
>>    contriburl = "file:///g:/myFolder/myRepository"))
> 
> 
> Only if the so-called repository contains (only) binary builds under R 
> 2.2.x of packages for Windows.
> 
> It would be better to set up the 'repository' correctly as a repository. 
> See my article in R-news 5/1 and the R-admin manual for the format of a 
> repository.
> 
> 
>>-peter
>>
>>Muhammad Subianto wrote:
>>
>>>I try to update R packages via my local repository.
>>>I put all R packages in g:/myFolder/myRepository, I do like
> 
> 
> Are these source packages or binary packages or what?
> 
> 
>>>>library(tools)
>>>>write_PACKAGES("g:/myFolder/myRepository")
>>>>options(repos=c(LocalR="file://g:/myFolder/myRepository"))
>>>>getOption("repos")
>>>
>>>                           LocalR
>>>"file://g:/myFolder/myRepository"
> 
> 
> Your syntax is incorrect here: Peter has silently corrected it.
> file:// syntax has an element for 'machine' followed by a third slash
> (although 'machine' is not supported in R, so it should be empty).
> 
> 
>>>>update.packages(ask = "graphics")
>>>
>>>Error in gzfile(file, "r") : unable to open connection
>>>In addition: Warning message:
>>>cannot open compressed file
>>>':/myFolder/myRepository/bin/windows/contrib/2.2/PACKAGES'
>>>
>>>
>>>>?update.packages
>>>
>>>
>>>It produces these file in g:/myFolder/myRepository
>>>PACKAGES
>>>PACKAGES.gz
>>>
>>>Could I make this folder (bin/windows/contrib/2.2/)? Why?
> 
>



From jlandgr1 at gwdg.de  Thu Nov 17 11:40:01 2005
From: jlandgr1 at gwdg.de (jobst landgrebe)
Date: Thu, 17 Nov 2005 11:40:01 +0100
Subject: [R] discontinuous y-axis (ordinate with a -/ /-)
Message-ID: <20051117104001.GA27916@ukb2-09>

Dear List,

can anyone tell me how to plot a discontinuous y-axis (ordinate
with a -/ /- "break sign") to fit in data with a wide range without the
need of logarthimic transformation? My data are distributed like this:

(abscissa: 1:10)

1. vector to plot

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  4.030   5.987   6.865  19.520  16.200  88.000

2. vector to plot

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.000   2.112   2.620   2.976   4.303   7.030 

I have just this one 88 outlier and cannot log-transform the data
(reviewers want plain data).

I would be glad to get help.

Yours sincerely,

Jobst Landgrebe

-- 
Dr. Jobst Landgrebe
Universit??t G??ttingen
Abt. Biochemie 2
Heinrich-D??ker-Weg 12
37073 G??ttingen
Germany
-------------------------------
tel: 0551/39-5902 oder -2316
fax: 0551/39-5979
mail: jlandgr1 at gwdg.de



From markus.jantti at iki.fi  Thu Nov 17 12:17:10 2005
From: markus.jantti at iki.fi (Markus Jantti)
Date: Thu, 17 Nov 2005 13:17:10 +0200
Subject: [R] anova.gls from nlme on multiple arguments within a function
	fails
Message-ID: <1132226230.16749.11.camel@pallas.abo.fi>

Dear All -- 

I am trying to use within a little table producing code an anova
comparison of two gls fitted objects, contained in a list of such
object, obtained using nlme function gls.
The anova procedure fails to locate the second of the objects.

The following code, borrowed from the help page of anova.gls,
exemplifies:
--------------- start example code ---------------
library(nlme)

## stolen from example(anova.gls)
# AR(1) errors within each Mare
fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
           correlation = corAR1(form = ~ 1 | Mare))
anova(fm1)
# variance changes with a power of the absolute fitted values?
fm2 <- update(fm1, weights = varPower())
anova(fm1, fm2)

## now define a little function
dummy <- function(obj)
  {
    anova(obj[[1]], obj[[2]])
  }
dummy(list(fm1, fm2))

## compare with what happens in anova.lm:

lm1 <- lm(follicles ~ sin(2*pi*Time), Ovary)
lm2 <- lm(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary)
dummy(list(lm1, lm2))
------------- end example code ------------------

It is not the end of the world: I can easily work around this. 
But it would be nice to know why this does not work.

Digging around using options(error=recover) did not help my much, I'm
afraid.  

Best,

Markus 
-- 
Markus Jantti
Abo Akademi University
markus.jantti at iki.fi
http://www.iki.fi/~mjantti



From arnejol at hotmail.com  Thu Nov 17 12:33:29 2005
From: arnejol at hotmail.com (Arne Jol)
Date: Thu, 17 Nov 2005 11:33:29 +0000
Subject: [R] access standard errors from multinom model
Message-ID: <BAY106-F1130C24059DE70CFD5562BBE5F0@phx.gbl>

Dear R users,

I'm using a multinomial LOGIT model to analyse choice behaviour of consumers 
(as part of my masters thesis research).

Using the R documentation and search on the R website I have a working 
script now.
Parameters are estimated and I can access them via 
coefficients(multinom.out).

In order to see if the parameters are significant I like to access the 
standard errors  in the same way as the coefficiants but I cannot find out 
how to do this. The standard errors are shown in summary(multinom.out) but 
how can I access them to compute the z-score.

Apologies if I’m asking something really simple, just started with R one 
week ago...

Regards,
Arne Jol



From vito_ricci at yahoo.com  Thu Nov 17 13:08:50 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 17 Nov 2005 13:08:50 +0100 (CET)
Subject: [R] Fitdistr()
Message-ID: <20051117120850.69638.qmail@web36102.mail.mud.yahoo.com>

Hi,

values in parentesis below the estimate of a parameter
is the standard deviation of parameter, that's a
measure of variability.

Regards.

Vito

>  set.seed(123)
>      x <- rgamma(100, shape = 5, rate = 0.1)
>      fitdistr(x, "gamma")
     shape         rate   
  6.45947303   0.13593172 
 (0.89052006) (0.01948648)
   ^^^^^^^         ^^^^^^^

From: Mark Miller <mmiller <at> nassp.uct.ac.za>
Subject: [R] Fitdistr()
Newsgroups: gmane.comp.lang.r.general
Date: 2005-11-17 07:58:10 GMT (4 hours and 2 minutes
ago)

When using fitdistr() with the exponential, log-normal
and beta distributions, 
you get the relevent rate, mean, standard deviation,
shape1 and shape2 but 
you get a number bellow those that are in () and I was
wandering what exactly 
those numbers represent and how they relate to the
data.

Many thanks
Mark Miller



Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From mmiller at nassp.uct.ac.za  Thu Nov 17 13:15:34 2005
From: mmiller at nassp.uct.ac.za (Mark Miller)
Date: Thu, 17 Nov 2005 14:15:34 +0200
Subject: [R] Standard Error
Message-ID: <200511171415.34930.mmiller@nassp.uct.ac.za>

I have worked out that when I fit data I get an estimate and a standard error, 
but all the definitions I can find describe the standard error of a sample as 
the standard deviation over the square root of the sample size, so if I am 
fitting to a log-normal distribution, what is the standard error associated 
with the standard deviation and why is it different from the standard error 
of the mean.

Many thanks
Mark Miller



From ccleland at optonline.net  Thu Nov 17 13:33:05 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 17 Nov 2005 07:33:05 -0500
Subject: [R] access standard errors from multinom model
In-Reply-To: <BAY106-F1130C24059DE70CFD5562BBE5F0@phx.gbl>
References: <BAY106-F1130C24059DE70CFD5562BBE5F0@phx.gbl>
Message-ID: <437C7881.80209@optonline.net>

str(summary(multinom.out)) often helps.

summary(multinom.out)$standard.errors

Arne Jol wrote:
> Dear R users,
> 
> I'm using a multinomial LOGIT model to analyse choice behaviour of 
> consumers (as part of my masters thesis research).
> 
> Using the R documentation and search on the R website I have a working 
> script now.
> Parameters are estimated and I can access them via 
> coefficients(multinom.out).
> 
> In order to see if the parameters are significant I like to access the 
> standard errors  in the same way as the coefficiants but I cannot find 
> out how to do this. The standard errors are shown in 
> summary(multinom.out) but how can I access them to compute the z-score.
> 
> Apologies if I?m asking something really simple, just started with R one 
> week ago...
> 
> Regards,
> Arne Jol
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From RRoa at fisheries.gov.fk  Thu Nov 17 13:01:39 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Thu, 17 Nov 2005 10:01:39 -0200
Subject: [R] Standard Error
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC60F@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From:	r-help-bounces at stat.math.ethz.ch [SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mark Miller
> Sent:	Thursday, November 17, 2005 10:16 AM
> To:	r-help at stat.math.ethz.ch
> Subject:	[R] Standard Error
> 
> I have worked out that when I fit data I get an estimate and a standard error, 
> but all the definitions I can find describe the standard error of a sample as 
> the standard deviation over the square root of the sample size, so if I am 
> fitting to a log-normal distribution, what is the standard error associated 
> with the standard deviation and why is it different from the standard error 
> of the mean.
------------
One thing is the standard error of the 
estimate_of_a_mean_ 
         from 
a random_sample_from_a_population,
whose formula you mentioned. 
Another thing, though related of course, is the standard error of a 
parameter_estimate 
         from 
a model.

The standard error of a parameter estimate from a model is a measure 
of the precision with which the parameter was estimated. The standard 
lognormal distribution is a model with two parameters (there is another 
with three parameters): the mean and the standard deviation. When you 
fit that model -the lognormal distribution- to a sample, you are estimating 
these two parameters. If you maximise the likelihood for your data as a 
function of the two parameters the estimation process, if successful, will 
produce the two estimates and the corresponding standard errors of those 
estimates (plus the estimated covariance between the estimates). Both 
parameters, the lognormal mean and the lognormal standard deviation, are 
unknown and are estimated so that each one has its corresponding measure 
of precision. 

You can think of the standard error of a parameter estimate from a model at 
least in two ways.
(1) Because maximum likelihood estimates tend to distribute normally, then 
the standard errors of parameter estimates are the standard deviation parameter 
estimates in a normal distribution whose mean is estimated by the maximum 
likelihood estimate itself. For example the output report from the ADMB statistical
system simply put the header Standard Deviation in the column for standard errors
of parameter estimates. Presumably this is because the ADMB's author subscribe
to this interpretation.
(2) You can also think of standard error of parameter estimates as measuring the 
curvature of the likelihood function about the maximum likelihood estimate.
In the pure-likelihood theory of inference this is the preferred interpretation. So 
A.W.F. Edwards (1972, Likelihood, Cambridge UP) has renamed the standard 
errors calling them "the span".

I hope this makes sense to you.

Ruben



From r.hankin at noc.soton.ac.uk  Thu Nov 17 14:24:18 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 17 Nov 2005 13:24:18 +0000
Subject: [R] changing figure size in Sweave
Message-ID: <08FFA9F9-C468-45E1-A644-D0784305C6AE@soc.soton.ac.uk>

Hi

In Sweave, how does one change the size of the plots?

I tried using a hook:


<<echo=FALSE, print=FALSE, fig=TRUE>>=
options(SweaveHooks=list(fig=function() ps.options(width=1)))
library(graphics)
pairs(iris)
@

but this didn't change the size of the figure.  How to make the  
figures a
different size?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From murdoch at stats.uwo.ca  Thu Nov 17 14:24:35 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 17 Nov 2005 08:24:35 -0500
Subject: [R] Difficulties with for() {while(){}}
In-Reply-To: <200511161104.31083.chrysopa@gmail.com>
References: <200511161104.31083.chrysopa@gmail.com>
Message-ID: <437C8493.80500@stats.uwo.ca>

Ronaldo Reis-Jr. wrote:
> Hi,
> 
> I have the follow function:
> 
> function() {
> 
>   ## Init of function
>   ...
> 
>   for(i in test) {
>     ...
> 
>     while(j <= test2) {
>     ...
> 
>     }
>   }
> }
> 
> The problem is that sometimes, naturally, the while is not possible to be 
> resolved, and so the program abort.
> 
> In this case I need that program return to the init of function and run again.

Wrap the call to your function in try(), and if the result is a 
try-error, try it again.  E.g. if your function is named f, do something 
like this:

repeat {
   result <- try(f())
   if (!inherits(result, "try-error")) break
}

Duncan Murdoch

> 
> How I can make this? Abort the while, abort the for and run the function 
> again?
> 
> Thanks
> Ronaldo



From Christoph.Scherber at uni-jena.de  Thu Nov 17 14:25:27 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Thu, 17 Nov 2005 14:25:27 +0100
Subject: [R] Mean survival times
Message-ID: <437C84C7.7070908@uni-jena.de>

Dear list,

I have data on insect survival in different cages; these have the 
following structure:

  deathtime status id cage    S      F     G   L     S
        1.5      1  1 C1      8      2     1   1     1
        1.5      1  2 C1      8      2     1   1     1
       11.5      1  3 C1      8      2     1   1     1
       11.5      1  4 C1      8      2     1   1     1

There are 81 cages and each 20 individuals whose survival was followed 
over time. The columns S,F,G,L and S are experimentally manipulated 
factors thought to have an influence on survival.

Using survfit(Surv(deathtime,status)~cage) gives me the survivorship 
curves for every cage. But what I??d like to have is a mean survivorship 
value for every cage.

Obviously, using tapply (deathtime,cage,mean) gives me mean values, but 
I??d like to have a better estimate of this using a proper statistical 
model. I??ve tried a glm with poisson errors (as suggested in Crawley??s 
book, page 628), but the back-transformed estimates (using status as the 
response variable and deathtime as an offset) were totally unrealistic.

As I??m new to survival analysis, it would be great if anyone could give 
me some hints on what method would be best.

Thanks a lot!
Christoph



From Matthias.Templ at statistik.gv.at  Thu Nov 17 14:27:57 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Thu, 17 Nov 2005 14:27:57 +0100
Subject: [R] Building S4-classes, documents
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BACFA@xchg1.statistik.local>

Hello,

I have some troubles when building S4-class packages.

All my (S4-)code works well (without building a package).

When building a package, in the R prompt after 
checking S3 generic/method consistency
Following error occurs:
Fehler: Kann R Kode in Packet 'AddNoise' nicht laden (~Error: Can not
load R code from package 'AddNoise')
(and there are some warnings after the error)

So, I really need some reading material.

Does anybody know some good documents about building S4-class packages
beside "Writing R-Extensions", "S4 Classes in 15 pages, more or less"
and "S4 Classes and Methods" from Fritz Leisch, especially documents
which are detailed explains how to write .Rd files for S4-classes (with
examples)?
Probably, "Writing .Rd Files For S4 Classes, Generic Functions and
Methods" from Gordon Smyth, 2003" were interessing, but the link
http://bioinf.wehi.edu.au/limma/Rdocs.html is defenitly broken...

Thanks for some links or additional documentents,
Matthias



From sdavis2 at mail.nih.gov  Thu Nov 17 14:27:02 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 17 Nov 2005 08:27:02 -0500
Subject: [R] Combine related plots
In-Reply-To: <000b01c5eb77$a1a46bb0$4c02a8c0@VC>
Message-ID: <BFA1EF56.13119%sdavis2@mail.nih.gov>

On 11/17/05 8:05 AM, "Vivien W. Chen" <wxc203 at psu.edu> wrote:

> Sean,
> 
> Thanks!
> It works!
> I have another extended question: how to label the two lines in the graph?
> I tried many ways, including text, label, list, etc., just cannot give a
> very effective way to do.

If I were you, I would play with the "legend" command.  It places a legend
on the plot; you can have the two different line styles that you used to
draw the original lines labeled with the names.  See the help for legend and
just play with it a bit.

 x2 <- rnorm(100) + 1:100
 x1 <- rnorm(100) + 1:100
 plot(x1,type='l',col='red')
 lines(x2,col='green')
 legend(x=5,y=90,legend=c('line1','line2'),col=c('red','green'),lty=1)

Glad it worked for you.

Sean



From MSchwartz at mn.rr.com  Thu Nov 17 14:30:48 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 17 Nov 2005 07:30:48 -0600
Subject: [R] discontinuous y-axis (ordinate with a -/ /-)
In-Reply-To: <20051117104001.GA27916@ukb2-09>
References: <20051117104001.GA27916@ukb2-09>
Message-ID: <1132234249.4173.3.camel@localhost.localdomain>

On Thu, 2005-11-17 at 11:40 +0100, jobst landgrebe wrote:
> Dear List,
> 
> can anyone tell me how to plot a discontinuous y-axis (ordinate
> with a -/ /- "break sign") to fit in data with a wide range without the
> need of logarthimic transformation? My data are distributed like this:
> 
> (abscissa: 1:10)
> 
> 1. vector to plot
> 
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>   4.030   5.987   6.865  19.520  16.200  88.000
> 
> 2. vector to plot
> 
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
>   0.000   2.112   2.620   2.976   4.303   7.030 
> 
> I have just this one 88 outlier and cannot log-transform the data
> (reviewers want plain data).
> 
> I would be glad to get help.
> 
> Yours sincerely,
> 
> Jobst Landgrebe


See this post by Jim Lemon, whose plotrix package on CRAN has an
axis.break() function authored by Jim and Ben Bolker:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/56487.html

HTH,

Marc Schwartz



From murdoch at stats.uwo.ca  Thu Nov 17 14:32:55 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 17 Nov 2005 08:32:55 -0500
Subject: [R] "Warning message: package '...' was built under R version
 2.3.0"
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F415B@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F415B@us-arlington-0668.mail.saic.com>
Message-ID: <437C8687.2060504@stats.uwo.ca>

Tuszynski, Jaroslaw W. wrote:
> Hi,
> 
> While installing precompiled packages I often get warnings like the one in
> the subject. I usually ignore them, but I still do not understand why
> windows packages are build with unreleased versions of R. 
> Is there some way to get packages build under R-2.2.0?

I think this part of your question was determined to be a problem with a 
certain mirror (but I didn't read all the messages on this...)

> What are potential problems that can result from that version mismatch?

Newer versions of R may store things differently.  We try hard to make 
sure that newer versions can read data from older ones, but it's hard to 
work in the opposite direction.

For one important change, an environment of NULL in 2.2.x has been 
replaced with an environment of baseenv() in 2.3.x.  2.3.x can usually 
translate the NULLs, but 2.2.x won't know what a baseenv() object is.

Duncan Murdoch



From Achim.Zeileis at wu-wien.ac.at  Thu Nov 17 14:32:49 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 17 Nov 2005 14:32:49 +0100 (CET)
Subject: [R] changing figure size in Sweave
In-Reply-To: <08FFA9F9-C468-45E1-A644-D0784305C6AE@soc.soton.ac.uk>
References: <08FFA9F9-C468-45E1-A644-D0784305C6AE@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.58.0511171430450.438@thorin.ci.tuwien.ac.at>

On Thu, 17 Nov 2005, Robin Hankin wrote:

> Hi
>
> In Sweave, how does one change the size of the plots?

Of the actual .eps or .pdf files or of the included figures in the paper?
The former can be done easily via

  <<foo,fig=TRUE,height=4,width=8>>=
  ...
  @

etc., the latter can be done most conveniently by setting something like

  \setkeys{Gin}{width=0.75\textwidth}

in the LaTeX documentation.

hth,
Z

> I tried using a hook:
>
>
> <<echo=FALSE, print=FALSE, fig=TRUE>>=
> options(SweaveHooks=list(fig=function() ps.options(width=1)))
> library(graphics)
> pairs(iris)
> @
>
> but this didn't change the size of the figure.  How to make the
> figures a
> different size?
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Matthias.Templ at statistik.gv.at  Thu Nov 17 14:36:05 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Thu, 17 Nov 2005 14:36:05 +0100
Subject: [R] changing figure size in Sweave
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BACFB@xchg1.statistik.local>

The Sweave User Manual says (p. 12):

Attention: One thing that gets easily confused are the width/height
parameters of the R
graphics devices and the corresponding arguments to the LATEX
\includegraphics command.
The Sweave options width and height are passed to the R graphics
devices, and hence affect
the default size of the produced EPS and PDF files. They do not affect
the size of figures in the
document, by default they will always be 80% of the current text width.
Use \setkeys{Gin}
to modify figure sizes or use explicit \includegraphics commands in
combination with Sweave
option include=FALSE.

I hope this helps.

Best,
Matthias


> 
> Hi
> 
> In Sweave, how does one change the size of the plots?
> 
> I tried using a hook:
> 
> 
> <<echo=FALSE, print=FALSE, fig=TRUE>>=
> options(SweaveHooks=list(fig=function() ps.options(width=1)))
> library(graphics)
> pairs(iris)
> @
> 
> but this didn't change the size of the figure.  How to make the  
> figures a
> different size?
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From jlandgr1 at gwdg.de  Thu Nov 17 14:45:01 2005
From: jlandgr1 at gwdg.de (jobst landgrebe)
Date: Thu, 17 Nov 2005 14:45:01 +0100
Subject: [R] discontinuous y-axis (ordinate with a -/ /-)
In-Reply-To: <437D47E9.3080700@ozemail.com.au>
References: <20051117104001.GA27916@ukb2-09> <437D47E9.3080700@ozemail.com.au>
Message-ID: <20051117134500.GB8805@ukb2-09>

Dear Jim,

thanks a lot, that did it.

Jobst

On Thu, Nov 17, 2005 at 10:18:01PM -0500, Jim Lemon wrote:
> jobst landgrebe wrote:
> >Dear List,
> >
> >can anyone tell me how to plot a discontinuous y-axis (ordinate
> >with a -/ /- "break sign") to fit in data with a wide range without the
> >need of logarthimic transformation? My data are distributed like this:
> >
> >(abscissa: 1:10)
> >
> >1. vector to plot
> >
> >   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> >  4.030   5.987   6.865  19.520  16.200  88.000
> >
> >2. vector to plot
> >
> >   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
> >  0.000   2.112   2.620   2.976   4.303   7.030 
> >
> >I have just this one 88 outlier and cannot log-transform the data
> >(reviewers want plain data).
> >
> Have a look at axis.break in the plotrix package. Also, the function I 
> posted earlier (gap.barplot) could be modified to do something like this.
> 
> Jim

-- 
Dr. Jobst Landgrebe
Universit??t G??ttingen
Abt. Biochemie 2
Heinrich-D??ker-Weg 12
37073 G??ttingen
Germany
-------------------------------
tel: 0551/39-5902 oder -2316
fax: 0551/39-5979
mail: jlandgr1 at gwdg.de



From andy_liaw at merck.com  Thu Nov 17 14:49:07 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Nov 2005 08:49:07 -0500
Subject: [R] some questions
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5BB@usctmx1106.merck.com>

From: Ales Ziberna
> 
> Firstly, these are all very basic questions, thah you coukld 
> probobly answer 
> yourself by searching the manual and help files. 
> Nevertheless, here are the 
> answers to give you a little head start.
> 
> > Dear R expert,
> > The following is my questions:
> >
> > 1. How to generate two sequences of 150 uniform
> > deviates, called v1 and v2, in the range [-1,1].
> v1<-runif(n=150,min=-1,max=1)
> v2<-runif(n=150,min=-1,max=1)
> 
> > 2. How to compute  r=(v1)^2+(v2)^2
> r<-(v1)^2+(v2)^2
> 
> > 3.If r is outside the range of (0,1) then it will be
> > discarded.
> newr<-r[r<1]
> newr<-newr[r>0]

I guess you meant

  newr <- newr[newr > 0]

for the second line above?  I would do it in one line as 

  newr <- r[abs(r - 0.5) < 0.5]

Andy

 
> > 4 How to compute (v1)*sqrt(-2*log(r)/r) and output the
> > first 100 z values in the list.
> z<-(v1)*sqrt(-2*log(r)/r)
> z[1:100] #or
> print(z[1:100])
> 
> > 5. Plot histograms for the sequences from steps 1 and
> > 2.
> hist(v1)
> hist(v2)
> hist(r)
> 
> >
> > Thanks very much for your help!
> I hope this helps,
> 
> Ales Ziberna
> 
> >
> > xingyu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From bolker at ufl.edu  Thu Nov 17 14:50:09 2005
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 17 Nov 2005 13:50:09 +0000 (UTC)
Subject: [R] discontinuous y-axis (ordinate with a -/ /-)
References: <20051117104001.GA27916@ukb2-09>
Message-ID: <loom.20051117T143334-720@post.gmane.org>

jobst landgrebe <jlandgr1 <at> gwdg.de> writes:

> 
> Dear List,
> 
> can anyone tell me how to plot a discontinuous y-axis (ordinate
> with a -/ /- "break sign") to fit in data with a wide range without the
> need of logarthimic transformation? My data are distributed like this:
> 
> (abscissa: 1:10)
> 
> 1. vector to plot
> 
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>   4.030   5.987   6.865  19.520  16.200  88.000
> 
> 2. vector to plot
> 
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
>   0.000   2.112   2.620   2.976   4.303   7.030 
> 
> I have just this one 88 outlier and cannot log-transform the data
> (reviewers want plain data).
> 
> I would be glad to get help.
> 
> Yours sincerely,
> 
> Jobst Landgrebe
> 


  there is an axis.break() command in the plotrix
package that will draw the break itself for you; however,
it just draws the axis break -- 
you have to manipulate the axis labels etc. yourself.
Here is an example (perhaps the beginning of a
more automated version, although it would need
some work)

library(plotrix)
x = runif(20)
y = c(runif(18),1.5,1.8)
break.bottom = 1.0
break.top = 1.4
top.size = 1.3
lab1=pretty(c(min(y),break.bottom))
lab2=pretty(c(break.top*1.1,max(y)))
top.range = break.bottom*(top.size-1)
top.data = max(y)-break.top
rescale = function(y) {
  break.bottom+(y-break.top)/(top.data/top.range)
}
rescaled.y = rescale(y[y>break.top])
##
plot(x[y<break.bottom],y[y<break.bottom],
     ylim=c(min(y),break.bottom*top.size),axes=FALSE,
     xlab="x",ylab="y")
axis(side=1)
axis(side=2,at=lab1)
box()
axis.break(axis=2,breakpos=1.05)
## rescale top of plot
points(x[y>break.top],rescaled.y)
axis(side=2,at=rescale(lab2),labels=lab2)



From Matthias.Templ at statistik.gv.at  Thu Nov 17 15:01:43 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Thu, 17 Nov 2005 15:01:43 +0100
Subject: [R] changing figure size in Sweave
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BACFC@xchg1.statistik.local>

Hi Robin,

After \begin{document} of your Rnw file, you can easily set e.g.

\setkeys{Gin}{width=width=0.8\textwidth}

And then doing something like;

<<test>>=
pdf(file = ...)
@

Best,
Matthias


> Hi Matthias
> 
> thanks for this.  I have the manual here.
> Do you have a working example of \setkeys{Gin}
> in use that I could modify?
> 
> best wishes
> 
> Robin
> 
> 
> On 17 Nov 2005, at 13:36, TEMPL Matthias wrote:
> 
> > The Sweave User Manual says (p. 12):
> >
> > Attention: One thing that gets easily confused are the width/height 
> > parameters of the R graphics devices and the corresponding 
> arguments 
> > to the LATEX \includegraphics command.
> > The Sweave options width and height are passed to the R graphics
> > devices, and hence affect
> > the default size of the produced EPS and PDF files. They do 
> not affect
> > the size of figures in the
> > document, by default they will always be 80% of the current text  
> > width.
> > Use \setkeys{Gin}
> > to modify figure sizes or use explicit \includegraphics commands in
> > combination with Sweave
> > option include=FALSE.
> >
> > I hope this helps.
> >
> > Best,
> > Matthias
> >
> >
> >>
> >> Hi
> >>
> >> In Sweave, how does one change the size of the plots?
> >>
> >> I tried using a hook:
> >>
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
>



From murdoch at stats.uwo.ca  Thu Nov 17 15:39:05 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 17 Nov 2005 09:39:05 -0500
Subject: [R] changing the value of a variable from inside a function
In-Reply-To: <5.2.1.1.1.20051115115224.0198f800@imap.duke.edu>
References: <5.2.1.1.1.20051115115224.0198f800@imap.duke.edu>
Message-ID: <437C9609.3070303@stats.uwo.ca>

On 11/15/2005 12:22 PM, Michael Wolosin wrote:
> All -
> 
> I am trying to write R code to implement a recursive algorithm.  I've 
> solved the problem in a klunky way that works, but uses  more memory and 
> computing time than it should.
> 
> A more elegant solution than my current one would require updating the 
> values of a variable that is located in what I will call the "root" 
> environment - that environment from which the original call to the 
> recursive function was issued.  

That's tricky and ugly, but possible in various ways.  However, the 
clean easy way to do this is to wrap your recursive function in a 
non-recursive one, and refer to variables in the non-recursive one using 
lexical scoping.  For example,

wrapper <- function(test) {
    test <- test  # make a copy in the wrapper environment
    blah <- function() {
        # references here to test will see the one in wrapper
        # blah can call itself; each invocation will see the same test

        test[i,] <<- expr  #  use "super-assignment" to modify it
    }
    return(test)
}

This makes one copy of the matrix and works on that.  If you want to 
make zero copies, you need to get tricky.

Duncan Murdoch

Certainly, I could pass the variable into
> the function, update it inside, and return it.  However, the variable I am 
> updating is a large matrix, and the recursion could end up several hundred 
> levels deep.  Passing the matrix around would create a copy in the 
> environment for each call, wasting memory, time, and space.
> 
> I've read the help on the "sys.{}" family of functions, and "eval", and 
> although I can't claim to have absorbed it all, it seems like it is much 
> easier to access the value of a variable in a parent frame than it is to 
> update that value with assignment.
> If you make an assignment inside a function, even if it is to a section of 
> a variable that exists in a parent frame, the variable is only created or 
> updated in the current environment - never in the parent frame.
> 
> For example:
> 
> test <- matrix(NA,nrow=4,ncol=3)
> test[1,] <- c(1,2,3)
> blah <- function(i){
>    test[i,] <- c(0,1,2) + i
>    return(test)
> }
> test
> blah(2)
> test
> 
> So the real question is, how do I write the function like "blah" above that 
> updates "test" in the parent or root frame?
> 
> blah <- function(i){
>    test[i,] <- c(1,2,3) + i  #modify this line somehow
>    return(NULL)
> }
> If done "correctly", we will get:
>  > blah(2)
>  > test
>        [,1] [,2] [,3]
>   [1,]    1    2    3
>   [2,]    2    3    4
>   [3,]   NA   NA   NA
>   [4,]   NA   NA   NA
> 
> And given an example that works from within a single function call, does it 
> have to be modified to work recursively?
> 
> blah <- function(i){
>    if (i<4) {blah(i + 1)}
>    test[i,] <- c(0,1,2) + i  #modify this line somehow
>    return(NULL)
> }
> If written "correctly", the following would be the output:
>  > blah(2)
>  > test
>        [,1] [,2] [,3]
>   [1,]    1    2    3
>   [2,]    2    3    4
>   [3,]    3    4    5
>   [4,]    4    5    6
> 
> One idea would be to write out to a file.  The filename could reside in the 
> root environment, and that is all that is needed.  But  this also seems 
> inelegant (and slow).  If I can read and write to a file, I should be able 
> to read and write to a memory location.
> 
> I suspect that the solution lies somewhere in the "sys" functions, but I 
> was having trouble seeing it.  Any help would be appreciated.
> 
> Thank you in advance,
> 
> Mike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From friendly at yorku.ca  Thu Nov 17 15:57:39 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Thu, 17 Nov 2005 09:57:39 -0500
Subject: [R] loess: choose span to minimize AIC?
Message-ID: <437C9A63.8050107@yorku.ca>

Is there an R implementation of a scheme for automatic smoothing
parameter selection with loess, e.g., by minimizing one of the AIC/GCV 
statistics discussed by Hurvich, Simonoff & Tsai (1998)?

Below is a function that calculates the relevant values of AICC,
AICC1 and GCV--- I think, because I to guess from the names of the
components returned in a loess object.

I guess I could use optimize(), or do a simple line search on span=,
but I'm not sure how to use loess.aic to write a function
that would act as a wrapper for loess() and return the mimimizing
loess fit for a specified criterion.

loess.aic <- function (x) {
	# extract values from loess object
	if (!(inherits(x,"loess"))) stop("Error: argument must be a loess object")
	span <- x$pars$span
	n <- x$n
	traceL <- x$trace.hat
	sigma2 <- sum( x$residuals^2 ) / (n-1)
	delta1 <- x$one.delta
	delta2 <- x$two.delta
	enp <- x$enp

	aicc <- log(sigma2) + 1 + 2* (2*(traceL+1)) / (n-traceL-2)
	aicc1<- n*log(sigma2) + n* ( 
(delta1/(delta2*(n+enp)))/(delta1^2/delta2)-2 )
	gcv  <- n*sigma2 / (n-traceL)^2
	
	result <- list(span=span, aicc=aicc, aicc1=aicc1, gcv=gcv)
	return(result)
}


 > cars.lo <- loess(dist ~ speed, cars)
 >
 > (values <- loess.aic(cars.lo))
$span
[1] 0.75

$aicc
[1] 6.93678

$aicc1
[1] 167.7267

$gcv
[1] 5.275487

 >


-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From sfalcon at fhcrc.org  Thu Nov 17 16:07:54 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 17 Nov 2005 07:07:54 -0800
Subject: [R] Building S4-classes, documents
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BACFA@xchg1.statistik.local>
	(TEMPL Matthias's message of "Thu, 17 Nov 2005 14:27:57 +0100")
References: <83536658864BC243BE3C06D7E936ABD5027BACFA@xchg1.statistik.local>
Message-ID: <m2psozxqgl.fsf@fhcrc.org>

Hi Matthias,

On 17 Nov 2005, Matthias.Templ at statistik.gv.at wrote:
> Hello,
>
> I have some troubles when building S4-class packages.
>
> All my (S4-)code works well (without building a package).
>
> When building a package, in the R prompt after checking S3
> generic/method consistency Following error occurs: Fehler: Kann R
> Kode in Packet 'AddNoise' nicht laden (~Error: Can not load R code
> from package 'AddNoise') (and there are some warnings after the
> error)

I don't have documentation to recommend other than what you
mentioned.  However, a few things to look into:

1. If you have your R code in multiple files, you may need to use the
   DESCRIPTION file's Collate field to control the loading order.
   Basically, you want: Class defintions, generics, methods, other.

2. Put methods and any other packages you depend on in Depends.

3. Add SaveImage: yes.

Also, make sure R CMD INSTALL yourPackage/ works before trying R CMD
check yourPackage.

+ seth



From Peter.Rossi at chicagogsb.edu  Thu Nov 17 15:40:46 2005
From: Peter.Rossi at chicagogsb.edu (Rossi, Peter E.)
Date: Thu, 17 Nov 2005 08:40:46 -0600
Subject: [R] [R-pkgs] new version of bayesm
Message-ID: <1E7B167439290641966EB161D4330798AEF9BF@GSBEX.gsb.uchicago.edu>

 
 Version 2.0-2 of bayesm is available on CRAN.
 This version includes bug fixes for rhierMnlRwMixture and
 rhierLinearModel.

 peter rossi

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From tlumley at u.washington.edu  Thu Nov 17 16:35:31 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 17 Nov 2005 07:35:31 -0800 (PST)
Subject: [R] loess: choose span to minimize AIC?
In-Reply-To: <437C9A63.8050107@yorku.ca>
References: <437C9A63.8050107@yorku.ca>
Message-ID: <Pine.LNX.4.63a.0511170731230.26715@homer22.u.washington.edu>

On Thu, 17 Nov 2005, Michael Friendly wrote:

> Is there an R implementation of a scheme for automatic smoothing
> parameter selection with loess, e.g., by minimizing one of the AIC/GCV
> statistics discussed by Hurvich, Simonoff & Tsai (1998)?

If you particularly want loess smoothing then I don't know, but if 
penalised spline smoothing will do then in gam() in the mgcv package does 
minimize GCV.

 	-thomas

> Below is a function that calculates the relevant values of AICC,
> AICC1 and GCV--- I think, because I to guess from the names of the
> components returned in a loess object.
>
> I guess I could use optimize(), or do a simple line search on span=,
> but I'm not sure how to use loess.aic to write a function
> that would act as a wrapper for loess() and return the mimimizing
> loess fit for a specified criterion.
>
> loess.aic <- function (x) {
> 	# extract values from loess object
> 	if (!(inherits(x,"loess"))) stop("Error: argument must be a loess object")
> 	span <- x$pars$span
> 	n <- x$n
> 	traceL <- x$trace.hat
> 	sigma2 <- sum( x$residuals^2 ) / (n-1)
> 	delta1 <- x$one.delta
> 	delta2 <- x$two.delta
> 	enp <- x$enp
>
> 	aicc <- log(sigma2) + 1 + 2* (2*(traceL+1)) / (n-traceL-2)
> 	aicc1<- n*log(sigma2) + n* (
> (delta1/(delta2*(n+enp)))/(delta1^2/delta2)-2 )
> 	gcv  <- n*sigma2 / (n-traceL)^2
>
> 	result <- list(span=span, aicc=aicc, aicc1=aicc1, gcv=gcv)
> 	return(result)
> }
>
>
> > cars.lo <- loess(dist ~ speed, cars)
> >
> > (values <- loess.aic(cars.lo))
> $span
> [1] 0.75
>
> $aicc
> [1] 6.93678
>
> $aicc1
> [1] 167.7267
>
> $gcv
> [1] 5.275487
>
> >
>
>
> -- 
> Michael Friendly     Email: friendly AT yorku DOT ca
> Professor, Psychology Dept.
> York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> Toronto, ONT  M3J 1P3 CANADA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From andy_liaw at merck.com  Thu Nov 17 16:41:43 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Nov 2005 10:41:43 -0500
Subject: [R] [Rd] Scan data from a .txt file
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5BD@usctmx1106.merck.com>

[Re-directing to R-help, as this is more appropriate there.]

I tried copying the snippet of data into the windows clipboard and tried it:

> dat <- read.table("clipboard", header=T)
> dat
    Name Weight Height Gender
1   Anne    150     65      F
2    Rob    160     68      M
3 George    180     65      M
4   Greg    205     69      M
> str(dat)
`data.frame':   4 obs. of  4 variables:
 $ Name  : Factor w/ 4 levels "Anne","George",..: 1 4 2 3
 $ Weight: int  150 160 180 205
 $ Height: int  65 68 65 69
 $ Gender: Factor w/ 2 levels "F","M": 1 2 2 2
> dat <- read.table("clipboard", header=T, row=1)
> str(dat)
`data.frame':   4 obs. of  3 variables:
 $ Weight: int  150 160 180 205
 $ Height: int  65 68 65 69
 $ Gender: Factor w/ 2 levels "F","M": 1 2 2 2
> dat
       Weight Height Gender
Anne      150     65      F
Rob       160     68      M
George    180     65      M
Greg      205     69      M

Don't see how it "doesn't work".  Please give more detail on what "doesn't
work" means.

Andy

> From: Vasundhara Akkineni
> 
> Hi all,
> Am trying to read data from a .txt file in such a way that i 
> can access the
> column names too. For example, the data in the table.txt file 
> is as below:
>  Name Weight Height Gender
> Anne 150 65 F
> Rob 160 68 M
> George 180 65 M
> Greg 205 69 M
>  i used the following commands:
>  data<-scan("table.txt",list("",0,0,0),sep="")
> a<-data[[1]]
> b<-data[[2]]
> c<-data[[3]]
> d<-data[[4]]
>  But this doesn't work because of type mismatch. I want to 
> pull the col
> names also into the respective lists. For example i want 'b' to have
> (weight,150,160,180,205) so that i can access the col name 
> and also the
> induvidual weights. I tried using the read.table method too, 
> but couldn't
> get this working. Can someone suggest a way to do this.
> Thanks,
> Vasu.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
>



From jtw2 at CDC.GOV  Thu Nov 17 16:28:44 2005
From: jtw2 at CDC.GOV (Wassell, James T., Ph.D.)
Date: Thu, 17 Nov 2005 10:28:44 -0500
Subject: [R] nlme question
Message-ID: <AF2DCD619279544BA454141F4A45B9E3F5E195@m-niosh-3.niosh.cdc.gov>



-----Original Message-----
From: Wassell, James T., Ph.D. 
Sent: Thursday, November 17, 2005 9:40 AM
To: 'Deepayan Sarkar'
Subject: RE: nlme question

Deepayan, 

Thanks for your interest.   It's difficult in email but I need the
variance of Kappa = mu + 1.645*tau + 1.645* sigma

Just using the standard variance calculation Var(K) = Var(mu) +
(1.645)^2 * Var(tau) + 1.645^2 * var(sigma) + [three covariance terms
with constant multipliers]. 

So, I can get var(mu) [or actually the standard error] from the summary
function -- and var(tau) and var(sigma) using the VarCorr function, but
I still need the covariance terms.

I am trying to duplicate methods in a paper by Nicas & Neuhaus,
"Variability in Respiratory Protection and the Assigned Protection
Factor"  J. Occ & Environ Health, Feb. 2004.   p 99-109.  (see eqn. 12).


The authors used Proc Mixed, but I can't figure out how to get
covariance terms with SAS either.  

Thanks again. 

Terry Wassell

-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
Sent: Thursday, November 17, 2005 2:52 AM
To: Wassell, James T., Ph.D.
Cc: r-help at stat.math.ethz.ch
Subject: Re: nlme question

On 11/16/05, Wassell, James T., Ph.D. <jtw2 at cdc.gov> wrote:
> I am using the package nlme to fit a simple random effects (variance
> components model)
>
> with 3 parameters:  overall mean (fixed effect), between subject
> variance (random) and  within subject variance (random).

So to paraphrase, your model can be written as (with the index i
representing subject)

y_ij = \mu + b_i + e_ij

where

b_i ~ N(0, \tao^2)
e_ij ~ N(0, \sigma_2)
and all b_i's and e_ij's are mutually independent. The model has, as
you say, 3 parameters, \mu, \tao and \sigma.

> I have 16 subjects with 1-4 obs per subject.
>
> I need a 3x3 variance-covariance matrix that includes all 3 parameters
> in order to compute the variance of a specific linear combination.

Can you specify the 'linear combination' that you want to estimate in
terms of the model above?

Deepayan



From murdoch at stats.uwo.ca  Thu Nov 17 16:53:45 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 17 Nov 2005 10:53:45 -0500
Subject: [R] changing the value of a variable from inside a function
In-Reply-To: <437C9609.3070303@stats.uwo.ca>
References: <5.2.1.1.1.20051115115224.0198f800@imap.duke.edu>
	<437C9609.3070303@stats.uwo.ca>
Message-ID: <437CA789.3030400@stats.uwo.ca>

On 11/17/2005 9:39 AM, Duncan Murdoch wrote:
> On 11/15/2005 12:22 PM, Michael Wolosin wrote:
>> All -
>> 
>> I am trying to write R code to implement a recursive algorithm.  I've 
>> solved the problem in a klunky way that works, but uses  more memory and 
>> computing time than it should.
>> 
>> A more elegant solution than my current one would require updating the 
>> values of a variable that is located in what I will call the "root" 
>> environment - that environment from which the original call to the 
>> recursive function was issued.  
> 
> That's tricky and ugly, but possible in various ways.  However, the 
> clean easy way to do this is to wrap your recursive function in a 
> non-recursive one, and refer to variables in the non-recursive one using 
> lexical scoping.  For example,
> 
> wrapper <- function(test) {
>     test <- test  # make a copy in the wrapper environment

Whoops, as Martin Maechler pointed out to me, this line is unnecessary. 
  The fact that test is an argument to wrapper means that a local copy 
would have been made already.

(I am fairly sure this line wouldn't cost very much, since R would 
recognize that a third copy of test isn't needed, but I shouldn't have 
put it there.)

Duncan Murdoch

>     blah <- function() {
>         # references here to test will see the one in wrapper
>         # blah can call itself; each invocation will see the same test
> 
>         test[i,] <<- expr  #  use "super-assignment" to modify it
>     }
>     return(test)
> }
> 
> This makes one copy of the matrix and works on that.  If you want to 
> make zero copies, you need to get tricky.
> 
> Duncan Murdoch
> 
> Certainly, I could pass the variable into
>> the function, update it inside, and return it.  However, the variable I am 
>> updating is a large matrix, and the recursion could end up several hundred 
>> levels deep.  Passing the matrix around would create a copy in the 
>> environment for each call, wasting memory, time, and space.
>> 
>> I've read the help on the "sys.{}" family of functions, and "eval", and 
>> although I can't claim to have absorbed it all, it seems like it is much 
>> easier to access the value of a variable in a parent frame than it is to 
>> update that value with assignment.
>> If you make an assignment inside a function, even if it is to a section of 
>> a variable that exists in a parent frame, the variable is only created or 
>> updated in the current environment - never in the parent frame.
>> 
>> For example:
>> 
>> test <- matrix(NA,nrow=4,ncol=3)
>> test[1,] <- c(1,2,3)
>> blah <- function(i){
>>    test[i,] <- c(0,1,2) + i
>>    return(test)
>> }
>> test
>> blah(2)
>> test
>> 
>> So the real question is, how do I write the function like "blah" above that 
>> updates "test" in the parent or root frame?
>> 
>> blah <- function(i){
>>    test[i,] <- c(1,2,3) + i  #modify this line somehow
>>    return(NULL)
>> }
>> If done "correctly", we will get:
>>  > blah(2)
>>  > test
>>        [,1] [,2] [,3]
>>   [1,]    1    2    3
>>   [2,]    2    3    4
>>   [3,]   NA   NA   NA
>>   [4,]   NA   NA   NA
>> 
>> And given an example that works from within a single function call, does it 
>> have to be modified to work recursively?
>> 
>> blah <- function(i){
>>    if (i<4) {blah(i + 1)}
>>    test[i,] <- c(0,1,2) + i  #modify this line somehow
>>    return(NULL)
>> }
>> If written "correctly", the following would be the output:
>>  > blah(2)
>>  > test
>>        [,1] [,2] [,3]
>>   [1,]    1    2    3
>>   [2,]    2    3    4
>>   [3,]    3    4    5
>>   [4,]    4    5    6
>> 
>> One idea would be to write out to a file.  The filename could reside in the 
>> root environment, and that is all that is needed.  But  this also seems 
>> inelegant (and slow).  If I can read and write to a file, I should be able 
>> to read and write to a memory location.
>> 
>> I suspect that the solution lies somewhere in the "sys" functions, but I 
>> was having trouble seeing it.  Any help would be appreciated.
>> 
>> Thank you in advance,
>> 
>> Mike
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From fcombes at gmail.com  Thu Nov 17 17:03:15 2005
From: fcombes at gmail.com (Florence Combes)
Date: Thu, 17 Nov 2005 17:03:15 +0100
Subject: [R] dev.copy legend problem
Message-ID: <73dae3060511170803m70e31ad3t1a481e8fe8d21cba@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/8e03094f/attachment.pl

From HDoran at air.org  Thu Nov 17 17:05:06 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 17 Nov 2005 11:05:06 -0500
Subject: [R] nlme question
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A0100FCD4@dc1ex3.air.org>

James,

By assumption, sigma and tau are assumed uncorrelated, which I believe
Deepayan noted below.  Sigma is also random error so it is uncorrelated
with your fixed effects.  What are the covariance terms you are seeking?

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wassell, James
T., Ph.D.
Sent: Thursday, November 17, 2005 10:29 AM
To: r-help at stat.math.ethz.ch
Subject: [R] nlme question



-----Original Message-----
From: Wassell, James T., Ph.D. 
Sent: Thursday, November 17, 2005 9:40 AM
To: 'Deepayan Sarkar'
Subject: RE: nlme question

Deepayan, 

Thanks for your interest.   It's difficult in email but I need the
variance of Kappa = mu + 1.645*tau + 1.645* sigma

Just using the standard variance calculation Var(K) = Var(mu) +
(1.645)^2 * Var(tau) + 1.645^2 * var(sigma) + [three covariance terms
with constant multipliers]. 

So, I can get var(mu) [or actually the standard error] from the summary
function -- and var(tau) and var(sigma) using the VarCorr function, but
I still need the covariance terms.

I am trying to duplicate methods in a paper by Nicas & Neuhaus,
"Variability in Respiratory Protection and the Assigned Protection
Factor"  J. Occ & Environ Health, Feb. 2004.   p 99-109.  (see eqn. 12).


The authors used Proc Mixed, but I can't figure out how to get
covariance terms with SAS either.  

Thanks again. 

Terry Wassell

-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com]
Sent: Thursday, November 17, 2005 2:52 AM
To: Wassell, James T., Ph.D.
Cc: r-help at stat.math.ethz.ch
Subject: Re: nlme question

On 11/16/05, Wassell, James T., Ph.D. <jtw2 at cdc.gov> wrote:
> I am using the package nlme to fit a simple random effects (variance 
> components model)
>
> with 3 parameters:  overall mean (fixed effect), between subject 
> variance (random) and  within subject variance (random).

So to paraphrase, your model can be written as (with the index i
representing subject)

y_ij = \mu + b_i + e_ij

where

b_i ~ N(0, \tao^2)
e_ij ~ N(0, \sigma_2)
and all b_i's and e_ij's are mutually independent. The model has, as you
say, 3 parameters, \mu, \tao and \sigma.

> I have 16 subjects with 1-4 obs per subject.
>
> I need a 3x3 variance-covariance matrix that includes all 3 parameters

> in order to compute the variance of a specific linear combination.

Can you specify the 'linear combination' that you want to estimate in
terms of the model above?

Deepayan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From matthieu.cornec at gmail.com  Thu Nov 17 17:09:56 2005
From: matthieu.cornec at gmail.com (Matthieu Cornec)
Date: Thu, 17 Nov 2005 17:09:56 +0100
Subject: [R] Help with read.csv2
Message-ID: <8a83e5000511170809m492bffbao1f2b1a196eb8b667@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/51699a02/attachment.pl

From mschwartz at mn.rr.com  Thu Nov 17 17:24:47 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 17 Nov 2005 10:24:47 -0600
Subject: [R] [Rd] Scan data from a .txt file
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5BD@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5BD@usctmx1106.merck.com>
Message-ID: <1132244687.4391.24.camel@localhost.localdomain>

I have a feeling that Vasu wants (mistakenly) this:

dat <- read.table("clipboard", header = FALSE)

> dat
      V1     V2     V3     V4
1   Name Weight Height Gender
2   Anne    150     65      F
3    Rob    160     68      M
4 George    180     65      M
5   Greg    205     69      M

> str(dat)
`data.frame':   5 obs. of  4 variables:
 $ V1: Factor w/ 5 levels "Anne","George",..: 4 1 5 2 3
 $ V2: Factor w/ 5 levels "150","160","180",..: 5 1 2 3 4
 $ V3: Factor w/ 4 levels "65","68","69",..: 4 1 2 1 3
 $ V4: Factor w/ 3 levels "F","Gender","M": 2 1 3 3 3

> dat$V1
[1] Name   Anne   Rob    George Greg
Levels: Anne George Greg Name Rob

> dat$V2
[1] Weight 150    160    180    205
Levels: 150 160 180 205 Weight

> dat$V3
[1] Height 65     68     65     69
Levels: 65 68 69 Height

> dat$V4
[1] Gender F      M      M      M
Levels: F Gender M


So that the colnames are actually part of the data frame columns.

Vasu, note however that all values become factors or you can convert to
character, for example:

> as.character(dat$V1)
[1] "Name"   "Anne"   "Rob"    "George" "Greg"

neither of which I suspect is what you really want.


You can access the column names of the data frame using colnames():

> dat <- read.table("clipboard", header = TRUE)

> dat
    Name Weight Height Gender
1   Anne    150     65      F
2    Rob    160     68      M
3 George    180     65      M
4   Greg    205     69      M

> colnames(dat)
[1] "Name"   "Weight" "Height" "Gender"


This keeps the column names separate from the actual data, which unless
we are missing something here, is the proper way to do this. Think of a
data frame as a rectangular data set, which can contain more than one
data type across the columns, much like a spreadsheet.  The difference
here (unlike a spreadsheet) is that the first row does not contain the
column names/labels. These are separate from the data itself, which in a
typical spreadsheet would start on row 2.

Note as Andy pointed out, that in this case, you should use
read.table(), not scan().

Review "An Introduction To R" and the "R Data Import/Export" manuals for
more information. Both are available with your installation and/or from
the main R web site under Documentation.

HTH,

Marc Schwartz


On Thu, 2005-11-17 at 10:41 -0500, Liaw, Andy wrote:
> [Re-directing to R-help, as this is more appropriate there.]
> 
> I tried copying the snippet of data into the windows clipboard and tried it:
> 
> > dat <- read.table("clipboard", header=T)
> > dat
>     Name Weight Height Gender
> 1   Anne    150     65      F
> 2    Rob    160     68      M
> 3 George    180     65      M
> 4   Greg    205     69      M
> > str(dat)
> `data.frame':   4 obs. of  4 variables:
>  $ Name  : Factor w/ 4 levels "Anne","George",..: 1 4 2 3
>  $ Weight: int  150 160 180 205
>  $ Height: int  65 68 65 69
>  $ Gender: Factor w/ 2 levels "F","M": 1 2 2 2
> > dat <- read.table("clipboard", header=T, row=1)
> > str(dat)
> `data.frame':   4 obs. of  3 variables:
>  $ Weight: int  150 160 180 205
>  $ Height: int  65 68 65 69
>  $ Gender: Factor w/ 2 levels "F","M": 1 2 2 2
> > dat
>        Weight Height Gender
> Anne      150     65      F
> Rob       160     68      M
> George    180     65      M
> Greg      205     69      M
> 
> Don't see how it "doesn't work".  Please give more detail on what "doesn't
> work" means.
> 
> Andy
> 
> > From: Vasundhara Akkineni
> > 
> > Hi all,
> > Am trying to read data from a .txt file in such a way that i 
> > can access the
> > column names too. For example, the data in the table.txt file 
> > is as below:
> >  Name Weight Height Gender
> > Anne 150 65 F
> > Rob 160 68 M
> > George 180 65 M
> > Greg 205 69 M
> >  i used the following commands:
> >  data<-scan("table.txt",list("",0,0,0),sep="")
> > a<-data[[1]]
> > b<-data[[2]]
> > c<-data[[3]]
> > d<-data[[4]]
> >  But this doesn't work because of type mismatch. I want to 
> > pull the col
> > names also into the respective lists. For example i want 'b' to have
> > (weight,150,160,180,205) so that i can access the col name 
> > and also the
> > induvidual weights. I tried using the read.table method too, 
> > but couldn't
> > get this working. Can someone suggest a way to do this.
> > Thanks,
> > Vasu.
> >



From jfox at mcmaster.ca  Thu Nov 17 17:28:52 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 17 Nov 2005 11:28:52 -0500
Subject: [R] loess: choose span to minimize AIC?
In-Reply-To: <437C9A63.8050107@yorku.ca>
Message-ID: <20051117162851.XTSZ26967.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Mike,

You could try

bestLoess <- function(model, criterion=c("aicc", "aicc1", "gcv"),
spans=c(.05, .95)){
    criterion <- match.arg(criterion)
    f <- function(span) {
        mod <- update(model, span=span)
        loess.aic(mod)[[criterion]]
        }
    result <- optimize(f, spans)
    list(span=result$minimum, criterion=result$objective)
    } 

A little experimentation suggests that aicc1 doesn't seem to behave
reasonably.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Michael Friendly
> Sent: Thursday, November 17, 2005 9:58 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] loess: choose span to minimize AIC?
> 
> Is there an R implementation of a scheme for automatic 
> smoothing parameter selection with loess, e.g., by minimizing 
> one of the AIC/GCV statistics discussed by Hurvich, Simonoff 
> & Tsai (1998)?
> 
> Below is a function that calculates the relevant values of AICC,
> AICC1 and GCV--- I think, because I to guess from the names 
> of the components returned in a loess object.
> 
> I guess I could use optimize(), or do a simple line search on 
> span=, but I'm not sure how to use loess.aic to write a 
> function that would act as a wrapper for loess() and return 
> the mimimizing loess fit for a specified criterion.
> 
> loess.aic <- function (x) {
> 	# extract values from loess object
> 	if (!(inherits(x,"loess"))) stop("Error: argument must 
> be a loess object")
> 	span <- x$pars$span
> 	n <- x$n
> 	traceL <- x$trace.hat
> 	sigma2 <- sum( x$residuals^2 ) / (n-1)
> 	delta1 <- x$one.delta
> 	delta2 <- x$two.delta
> 	enp <- x$enp
> 
> 	aicc <- log(sigma2) + 1 + 2* (2*(traceL+1)) / (n-traceL-2)
> 	aicc1<- n*log(sigma2) + n* (
> (delta1/(delta2*(n+enp)))/(delta1^2/delta2)-2 )
> 	gcv  <- n*sigma2 / (n-traceL)^2
> 	
> 	result <- list(span=span, aicc=aicc, aicc1=aicc1, gcv=gcv)
> 	return(result)
> }
> 
> 
>  > cars.lo <- loess(dist ~ speed, cars)
>  >
>  > (values <- loess.aic(cars.lo))
> $span
> [1] 0.75
> 
> $aicc
> [1] 6.93678
> 
> $aicc1
> [1] 167.7267
> 
> $gcv
> [1] 5.275487
> 
>  >
> 
> 
> -- 
> Michael Friendly     Email: friendly AT yorku DOT ca
> Professor, Psychology Dept.
> York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> Toronto, ONT  M3J 1P3 CANADA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From arnejol at hotmail.com  Thu Nov 17 17:40:43 2005
From: arnejol at hotmail.com (Arne Jol)
Date: Thu, 17 Nov 2005 16:40:43 +0000
Subject: [R] Multinomial Conditional Logit Model
Message-ID: <BAY106-F313359100C1514E1F4557EBE5F0@phx.gbl>

Hello R Help,

I have a question regarding Multinomial (Conditional) LOGIT models in R.

For my masters thesis I like to use Multinomial LOGIT models to analyse 
consumer choice data.

After orientation on the R homepage and internet I found a method for 
multinomial LOGIT. This model is as follows:

P_s(X_i) = exp(B_s * X_i) / som_j(B_j * X_i)

s = Class or Response (bought brand A, B, C or D)
X_i = Characteristics that describe characteristics of purchase incident i.
B_s = Parameter vector for Class s.

This model estimates for each class a B_s vector. (or actually one less 
because B_0 = 1)
I use the function multinom from the nnet package.

Now I am also interested in Conditional Multinomial Logit model with the 
following formulation.

P_s(X_is) = exp(B * X_is) / som_j(B * X_is)
s = Class or Response (bought brand A, B, C or D)
X_is = Characteristics that describe characteristics of purchase incident I 
for Class s.
B = Parameter vector for all classes s (only one..).

This model estimates one B vector.

My question is: “Is it possible to estimate a Conditional Multinomial Logit 
model as described above with R”? And How??

Something that comes close is the implementation by John Hendrix in the 
CATSPEC package. This allows for constraints on the response variable, but 
it estimates a B_s vector for each Class and not one B vector. This is not 
what I like to have and not according to the specification of a Conditional 
LOGIT model as I found in literature.

I’m interested in commands on this. Is the information above correct because 
it could well be that I am wrong since I am new to this kind of models.

Thanks in advance for any responses or suggestions.

Arne Jol
Student Informatics & Economics
Erasmus University Rotterdam
E-mail: arnejol at gmail.com
Tel: +31 618088152



From pauljohn at ku.edu  Thu Nov 17 17:47:25 2005
From: pauljohn at ku.edu (Paul Johnson)
Date: Thu, 17 Nov 2005 10:47:25 -0600
Subject: [R] Predicting and  Plotting  "hypothetical" values of factors
Message-ID: <437CB41D.9080808@ku.edu>

Last Friday, I noticed that it is difficult to work with regression 
models in which there are factors.  It is easier to do the old fashioned 
thing of coding up "dummy" variables with 0-1 values.  The predict 
function's newdata argument is not suited to insertion of hypothetical 
values for the factor, whereas it has no trouble with numeric variables. 
  For example, if one uses a factor as a predictor in a logistic 
regression, then it is tough to plot the S-shaped curve that describes 
the probabilities.

Here is some code with comments describing the difficulties that we have 
found.  Are there simpler, less frustrating approaches?

-----------------------------
# Paul Johnson  <pauljohn at ku.edu> 2005-11-17
# factorFutzing-1.R


myfac <- factor(c(1.1, 4.5, 1.1, 1.1, 4.5, 1.1, 4.5, 1.1))

y <- c(0,1,0,1,0,0,1,0)

mymod1 <-glm (y~myfac, family=binomial)

p1 <- predict(mymod1, type="response")

plot(myfac,p1)

# Wait, that's not the picture I wanted. I want to see the
# proverbial S-shaped curve!
#

# The contrast variable that R creates is coded 0 and 1.
# I'd like to have a sequence from 0 to 1 (seq(0,1,length.out=10)) and
# get predicted values for each.  That would give the S-shaped curve.


# But the use of predict does not work because the factor has 2
# values and the predict function won't take my new data:

predict(mymod1, newdata=data.frame(myfac=seq(0,1,length.out=8)))

# Error: variable 'myfac' was fitted with class "factor" but class 
"numeric" was supplied
# In addition: Warning message:
# variable 'myfac' is not a factor in: model.frame.default(Terms, 
newdata, na.action = na.action, xlev = object$xlevels)

# Isn't there an easier way than this?

c1 <- coef(mymod1)

myseq <- seq(0,1, length.out=10)

newdf <- data.frame(1, myseq)

linearPredictor <- as.matrix(newdf) %*% c1

p2 <- 1/ (1 + exp(-linearPredictor))
# But there is a big disaster if you just try the obvious thing
# of plotting with
# lines(myseq,p2)
# The line does not show up where you hope in the plot.
# The plot "appears to" have the x-axis on the scale
# 1.1 to 4.5, So in order to see the s-shaped curve, it appears we have
# to re-scale. However, this is a big illusion.  To see what I mean,
# do

points(2,.4)

# you expected to see the point appear at (2,.4), but in the plot
# it appears at (4.5,.4).  Huh?

# The actual values being plotted are the integer-valued levels that
# R uses for factors, the numbers you get from as.numeric(myfac).
# So it is only necessary to re-scale the sequence by adding one.

myseq2 <- 1 +  myseq

lines( myseq2, p2, type="l" )

#Its not all that S-shaped, but you have to take what you can get! :)
-----------------------------

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From vasu.akkineni at gmail.com  Thu Nov 17 17:45:56 2005
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Thu, 17 Nov 2005 11:45:56 -0500
Subject: [R] [Rd] Scan data from a .txt file
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5BD@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5BD@usctmx1106.merck.com>
Message-ID: <3b67376c0511170845l6f03d4cawe17d87125cb5b036@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/0f4fd5f9/attachment.pl

From jtw2 at CDC.GOV  Thu Nov 17 17:51:39 2005
From: jtw2 at CDC.GOV (Wassell, James T., Ph.D.)
Date: Thu, 17 Nov 2005 11:51:39 -0500
Subject: [R] nlme question
Message-ID: <AF2DCD619279544BA454141F4A45B9E306A3CE@m-niosh-3.niosh.cdc.gov>

Thank you for taking the time to think about my problem. 

The reference states:  "The covariance structure must be considered,
because for unbalanced data the estimates" (i.e. mu, sigma and tau hats)
"are not typically independent." Page 105.  It would be nice to simply
assume zero covariance terms, but the authors reject this
simplification.



From vasu.akkineni at gmail.com  Thu Nov 17 17:56:47 2005
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Thu, 17 Nov 2005 11:56:47 -0500
Subject: [R] [Rd] Scan data from a .txt file
In-Reply-To: <1132244687.4391.24.camel@localhost.localdomain>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5BD@usctmx1106.merck.com>
	<1132244687.4391.24.camel@localhost.localdomain>
Message-ID: <3b67376c0511170856w378dba2ejb21e2da6976fa52c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/6c5366dd/attachment.pl

From p.dalgaard at biostat.ku.dk  Thu Nov 17 18:00:45 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Nov 2005 18:00:45 +0100
Subject: [R] Predicting and  Plotting  "hypothetical" values of factors
In-Reply-To: <437CB41D.9080808@ku.edu>
References: <437CB41D.9080808@ku.edu>
Message-ID: <x23blvfbuq.fsf@viggo.kubism.ku.dk>

Paul Johnson <pauljohn at ku.edu> writes:

> Last Friday, I noticed that it is difficult to work with regression 
> models in which there are factors.  It is easier to do the old fashioned 
> thing of coding up "dummy" variables with 0-1 values.  The predict 
> function's newdata argument is not suited to insertion of hypothetical 
> values for the factor, whereas it has no trouble with numeric variables. 
>   For example, if one uses a factor as a predictor in a logistic 
> regression, then it is tough to plot the S-shaped curve that describes 
> the probabilities.
> 
> Here is some code with comments describing the difficulties that we have 
> found.  Are there simpler, less frustrating approaches?

I think the point is that if you think a factor can take intermediate
values in between the groups (as in sex==1.245), then it is really a
numeric variable and should be treated as such in the model, whether
or not it has only two distinct values in your data set.

It is not obvious to me what the "S shaped curve" means in a model
which really only specifies two probabilities.

        -p

 
> -----------------------------
> # Paul Johnson  <pauljohn at ku.edu> 2005-11-17
> # factorFutzing-1.R
> 
> 
> myfac <- factor(c(1.1, 4.5, 1.1, 1.1, 4.5, 1.1, 4.5, 1.1))
> 
> y <- c(0,1,0,1,0,0,1,0)
> 
> mymod1 <-glm (y~myfac, family=binomial)
> 
> p1 <- predict(mymod1, type="response")
> 
> plot(myfac,p1)
> 
> # Wait, that's not the picture I wanted. I want to see the
> # proverbial S-shaped curve!
> #
> 
> # The contrast variable that R creates is coded 0 and 1.
> # I'd like to have a sequence from 0 to 1 (seq(0,1,length.out=10)) and
> # get predicted values for each.  That would give the S-shaped curve.
> 
> 
> # But the use of predict does not work because the factor has 2
> # values and the predict function won't take my new data:
> 
> predict(mymod1, newdata=data.frame(myfac=seq(0,1,length.out=8)))
> 
> # Error: variable 'myfac' was fitted with class "factor" but class 
> "numeric" was supplied
> # In addition: Warning message:
> # variable 'myfac' is not a factor in: model.frame.default(Terms, 
> newdata, na.action = na.action, xlev = object$xlevels)
> 
> # Isn't there an easier way than this?
> 
> c1 <- coef(mymod1)
> 
> myseq <- seq(0,1, length.out=10)
> 
> newdf <- data.frame(1, myseq)
> 
> linearPredictor <- as.matrix(newdf) %*% c1
> 
> p2 <- 1/ (1 + exp(-linearPredictor))
> # But there is a big disaster if you just try the obvious thing
> # of plotting with
> # lines(myseq,p2)
> # The line does not show up where you hope in the plot.
> # The plot "appears to" have the x-axis on the scale
> # 1.1 to 4.5, So in order to see the s-shaped curve, it appears we have
> # to re-scale. However, this is a big illusion.  To see what I mean,
> # do
> 
> points(2,.4)
> 
> # you expected to see the point appear at (2,.4), but in the plot
> # it appears at (4.5,.4).  Huh?
> 
> # The actual values being plotted are the integer-valued levels that
> # R uses for factors, the numbers you get from as.numeric(myfac).
> # So it is only necessary to re-scale the sequence by adding one.
> 
> myseq2 <- 1 +  myseq
> 
> lines( myseq2, p2, type="l" )
> 
> #Its not all that S-shaped, but you have to take what you can get! :)
> -----------------------------
> 
> -- 
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66044-3177           FAX: (785) 864-5700
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From friendly at yorku.ca  Thu Nov 17 18:05:10 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Thu, 17 Nov 2005 12:05:10 -0500
Subject: [R] loess: choose span to minimize AIC?
In-Reply-To: <20051117162851.XTSZ26967.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <20051117162851.XTSZ26967.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <437CB846.2030706@yorku.ca>

Thanks very much, John

The formula for AICC1 was transscribed from an ambiguously
rendered version (in the SAS documentation).  This is a
corrected version.

loess.aic <- function (x) {
	if (!(inherits(x,"loess"))) stop("Error: argument must be a loess object")
	# extract values from loess object
	span <- x$pars$span
	n <- x$n
	traceL <- x$trace.hat
	sigma2 <- sum( x$residuals^2 ) / (n-1)
	delta1 <- x$one.delta
	delta2 <- x$two.delta
	enp <- x$enp

	aicc <- log(sigma2) + 1 + 2* (2*(traceL+1)) / (n-traceL-2)
#	aicc1<- n*log(sigma2) + n* ( 
(delta1/(delta2*(n+enp)))/(delta1^2/delta2)-2 )
	aicc1<- n*log(sigma2) + n* ( (delta1/delta2)*(n+enp)/(delta1^2/delta2)-2 )
	gcv  <- n*sigma2 / (n-traceL)^2
	
	result <- list(span=span, aicc=aicc, aicc1=aicc1, gcv=gcv)
	return(result)
}


John Fox wrote:

> Dear Mike,
> 
> You could try
> 
> bestLoess <- function(model, criterion=c("aicc", "aicc1", "gcv"),
> spans=c(.05, .95)){
>     criterion <- match.arg(criterion)
>     f <- function(span) {
>         mod <- update(model, span=span)
>         loess.aic(mod)[[criterion]]
>         }
>     result <- optimize(f, spans)
>     list(span=result$minimum, criterion=result$objective)
>     } 
> 
> A little experimentation suggests that aicc1 doesn't seem to behave
> reasonably.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>>Michael Friendly
>>Sent: Thursday, November 17, 2005 9:58 AM
>>To: R-help at stat.math.ethz.ch
>>Subject: [R] loess: choose span to minimize AIC?
>>
>>Is there an R implementation of a scheme for automatic 
>>smoothing parameter selection with loess, e.g., by minimizing 
>>one of the AIC/GCV statistics discussed by Hurvich, Simonoff 
>>& Tsai (1998)?
>>
>>Below is a function that calculates the relevant values of AICC,
>>AICC1 and GCV--- I think, because I to guess from the names 
>>of the components returned in a loess object.
>>
>>I guess I could use optimize(), or do a simple line search on 
>>span=, but I'm not sure how to use loess.aic to write a 
>>function that would act as a wrapper for loess() and return 
>>the mimimizing loess fit for a specified criterion.
>>
>>loess.aic <- function (x) {
>>	# extract values from loess object
>>	if (!(inherits(x,"loess"))) stop("Error: argument must 
>>be a loess object")
>>	span <- x$pars$span
>>	n <- x$n
>>	traceL <- x$trace.hat
>>	sigma2 <- sum( x$residuals^2 ) / (n-1)
>>	delta1 <- x$one.delta
>>	delta2 <- x$two.delta
>>	enp <- x$enp
>>
>>	aicc <- log(sigma2) + 1 + 2* (2*(traceL+1)) / (n-traceL-2)
>>	aicc1<- n*log(sigma2) + n* (
>>(delta1/(delta2*(n+enp)))/(delta1^2/delta2)-2 )
>>	gcv  <- n*sigma2 / (n-traceL)^2
>>	
>>	result <- list(span=span, aicc=aicc, aicc1=aicc1, gcv=gcv)
>>	return(result)
>>}
>>
>>
>> > cars.lo <- loess(dist ~ speed, cars)
>> >
>> > (values <- loess.aic(cars.lo))
>>$span
>>[1] 0.75
>>
>>$aicc
>>[1] 6.93678
>>
>>$aicc1
>>[1] 167.7267
>>
>>$gcv
>>[1] 5.275487
>>
>> >
>>
>>
>>-- 
>>Michael Friendly     Email: friendly AT yorku DOT ca
>>Professor, Psychology Dept.
>>York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
>>4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
>>Toronto, ONT  M3J 1P3 CANADA
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From mschwartz at mn.rr.com  Thu Nov 17 18:20:14 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 17 Nov 2005 11:20:14 -0600
Subject: [R] dev.copy legend problem
In-Reply-To: <73dae3060511170803m70e31ad3t1a481e8fe8d21cba@mail.gmail.com>
References: <73dae3060511170803m70e31ad3t1a481e8fe8d21cba@mail.gmail.com>
Message-ID: <1132248015.4391.52.camel@localhost.localdomain>

On Thu, 2005-11-17 at 17:03 +0100, Florence Combes wrote:
> Dear all,
> 
> We are facing this problem for long, and so ask for your help.
> 
> We are plotting 2 graphs in a postscript device (left part -layout
> function-), and the common legend for these graphs on the right part.
> The legend in the postscript device looks ok: this is color lines with
> numbers on the right (6 columns) , see the code below:
> 
> > nblock<-c(1:48)
> > leg<-paste(c(1:npin)," ",sep=" ")
> > legend(0,19,legend = leg, col=rainbow(nblock), lty=1,
> merge=TRUE,ncol=6,bty="n",cex=0.6)
> 
> The problem we are facing is that we dev.copy to a pdf device and then, the
> legend doesn't look the same: numbers overlap a little lines.

The problem with using dev.copy() is that it can result in slightly (or
notably) different plotting characteristics, which are device dependent.

A plot that is viewed on the screen, for example, may or may not (most
likely not) look the same when created using the same code to a
postscript device or a pdf device. This can be further exacerbated if
the plot device dimensions and pointsizes are not explicitly defined, as
"device shrinkage" may occur. I suspect that this is what you are
experiencing.

If you want to end up with a PDF plot, use pdf() and set the various
plotting parameters (ie. font size, etc.) based upon what you see there,
not what you see on the screen or in another device.

One other possibility, just to throw it out there since you are on
Linux, is to use ps2pdf from a console to convert the PS file to PDF.
This uses Ghostscript to do the conversion and will generally work well,
but testing with your particular plot, given possible issues with fonts,
etc. would still be a good idea. See 'man ps2pdf' for more information.

> Has someone already encountered such a thing ?
> 
> Any help apreciated, thanks a lot.
> 
> Florence.
> 
> ----------------------------------------
> R 2.1.0 on a Linux Debian.

Have not been able to get your SysAdmin to upgrade you yet?

:-)

HTH,

Marc Schwartz



From guang.li at imperial.ac.uk  Thu Nov 17 18:43:00 2005
From: guang.li at imperial.ac.uk (Li, Guang Q)
Date: Thu, 17 Nov 2005 17:43:00 -0000
Subject: [R] Linking Fortran subroutines
Message-ID: <EFCF60A4595E9148A108047D78D8D66436813E@icex1.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/c89ba9e7/attachment.pl

From wzhao6898 at gmail.com  Thu Nov 17 18:44:53 2005
From: wzhao6898 at gmail.com (David Zhao)
Date: Thu, 17 Nov 2005 09:44:53 -0800
Subject: [R] Goodness fit test HELP!
Message-ID: <4e6115a50511170944g4401e7c1wb401ccd64607fa83@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/24ba1472/attachment.pl

From andy_liaw at merck.com  Thu Nov 17 18:46:56 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Nov 2005 12:46:56 -0500
Subject: [R] loess: choose span to minimize AIC?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5BE@usctmx1106.merck.com>

The locfit package (which, I believe, contains an independent implementation
of loess, plus more) contains the gcvplot() and aicplot() functions that I
think can do this.

Best,
Andy

> From: Michael Friendly
> 
> Thanks very much, John
> 
> The formula for AICC1 was transscribed from an ambiguously
> rendered version (in the SAS documentation).  This is a
> corrected version.
> 
> loess.aic <- function (x) {
> 	if (!(inherits(x,"loess"))) stop("Error: argument must 
> be a loess object")
> 	# extract values from loess object
> 	span <- x$pars$span
> 	n <- x$n
> 	traceL <- x$trace.hat
> 	sigma2 <- sum( x$residuals^2 ) / (n-1)
> 	delta1 <- x$one.delta
> 	delta2 <- x$two.delta
> 	enp <- x$enp
> 
> 	aicc <- log(sigma2) + 1 + 2* (2*(traceL+1)) / (n-traceL-2)
> #	aicc1<- n*log(sigma2) + n* ( 
> (delta1/(delta2*(n+enp)))/(delta1^2/delta2)-2 )
> 	aicc1<- n*log(sigma2) + n* ( 
> (delta1/delta2)*(n+enp)/(delta1^2/delta2)-2 )
> 	gcv  <- n*sigma2 / (n-traceL)^2
> 	
> 	result <- list(span=span, aicc=aicc, aicc1=aicc1, gcv=gcv)
> 	return(result)
> }
> 
> 
> John Fox wrote:
> 
> > Dear Mike,
> > 
> > You could try
> > 
> > bestLoess <- function(model, criterion=c("aicc", "aicc1", "gcv"),
> > spans=c(.05, .95)){
> >     criterion <- match.arg(criterion)
> >     f <- function(span) {
> >         mod <- update(model, span=span)
> >         loess.aic(mod)[[criterion]]
> >         }
> >     result <- optimize(f, spans)
> >     list(span=result$minimum, criterion=result$objective)
> >     } 
> > 
> > A little experimentation suggests that aicc1 doesn't seem to behave
> > reasonably.
> > 
> > Regards,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox 
> > -------------------------------- 
> > 
> > 
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> >>Michael Friendly
> >>Sent: Thursday, November 17, 2005 9:58 AM
> >>To: R-help at stat.math.ethz.ch
> >>Subject: [R] loess: choose span to minimize AIC?
> >>
> >>Is there an R implementation of a scheme for automatic 
> >>smoothing parameter selection with loess, e.g., by minimizing 
> >>one of the AIC/GCV statistics discussed by Hurvich, Simonoff 
> >>& Tsai (1998)?
> >>
> >>Below is a function that calculates the relevant values of AICC,
> >>AICC1 and GCV--- I think, because I to guess from the names 
> >>of the components returned in a loess object.
> >>
> >>I guess I could use optimize(), or do a simple line search on 
> >>span=, but I'm not sure how to use loess.aic to write a 
> >>function that would act as a wrapper for loess() and return 
> >>the mimimizing loess fit for a specified criterion.
> >>
> >>loess.aic <- function (x) {
> >>	# extract values from loess object
> >>	if (!(inherits(x,"loess"))) stop("Error: argument must 
> >>be a loess object")
> >>	span <- x$pars$span
> >>	n <- x$n
> >>	traceL <- x$trace.hat
> >>	sigma2 <- sum( x$residuals^2 ) / (n-1)
> >>	delta1 <- x$one.delta
> >>	delta2 <- x$two.delta
> >>	enp <- x$enp
> >>
> >>	aicc <- log(sigma2) + 1 + 2* (2*(traceL+1)) / (n-traceL-2)
> >>	aicc1<- n*log(sigma2) + n* (
> >>(delta1/(delta2*(n+enp)))/(delta1^2/delta2)-2 )
> >>	gcv  <- n*sigma2 / (n-traceL)^2
> >>	
> >>	result <- list(span=span, aicc=aicc, aicc1=aicc1, gcv=gcv)
> >>	return(result)
> >>}
> >>
> >>
> >> > cars.lo <- loess(dist ~ speed, cars)
> >> >
> >> > (values <- loess.aic(cars.lo))
> >>$span
> >>[1] 0.75
> >>
> >>$aicc
> >>[1] 6.93678
> >>
> >>$aicc1
> >>[1] 167.7267
> >>
> >>$gcv
> >>[1] 5.275487
> >>
> >> >
> >>
> >>
> >>-- 
> >>Michael Friendly     Email: friendly AT yorku DOT ca
> >>Professor, Psychology Dept.
> >>York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> >>4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> >>Toronto, ONT  M3J 1P3 CANADA
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> 
> -- 
> Michael Friendly     Email: friendly AT yorku DOT ca
> Professor, Psychology Dept.
> York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> Toronto, ONT  M3J 1P3 CANADA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Thu Nov 17 18:50:28 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Nov 2005 12:50:28 -0500
Subject: [R] Linking Fortran subroutines
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5BF@usctmx1106.merck.com>

Why not put the two subroutines into one file and compile them into one DLL?
Or compile them separately into .o, but make them into one DLL.

Andy

> From: Li, Guang Q
> 
> Hi,
> 
>  
> 
> I just started using R a few weeks ago and have a problem 
> with linking Fortran subroutines to R. For some reasons, I 
> need to compile a Fortran program in R (or Splus) and the 
> whole program consists a couple of subroutines, say, subA and 
> subB. There is no difficulty in linking the subroutines 
> individually, but two subroutines are nested, as shown below, 
> 
>  
> 
> SUBROUTINE subA(arg.)
> 
> EXTERNAL subB
> 
> ????.
> 
> CALL subB(arg.)
> 
> ????. 
> 
> RETURN
> 
> END
> 
>  
> 
> SUBROUTINE subB (arg.)
> 
> ????.
> 
> RETURN
> 
> END
> 
>  
> 
> Obviously, if subA(.dll) and subB(.dll) are individually 
> linked to R, subB will not be called within subA. In fact, R 
> does not recognise the command CALL subB( )! Further, in the 
> real situation, subB is called several times with different 
> inputs within subA, so it will be impossible to merge two 
> subroutines into one. 
> 
>  
> 
> I am here asking whether there is any way to solve this 
> problem, and I hope the question is clearly explained.
> 
>  
> 
> Thanks in advance for any help!
> 
>  
> 
> Philip
> 
> 
> 	[[alternative HTML version deleted]]
> 
>



From gunter.berton at gene.com  Thu Nov 17 18:56:38 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 17 Nov 2005 09:56:38 -0800
Subject: [R] Goodness fit test HELP!
In-Reply-To: <4e6115a50511170944g4401e7c1wb401ccd64607fa83@mail.gmail.com>
Message-ID: <200511171756.jAHHuchr008892@ohm.gene.com>

This is an "iceberg" question -- most of it (i.e. statistical issues) is
hidden beneath the surface.

To avoid a lengthy dissertation on statistical philosophy, I would merely
suggest:

1. 
require(lattice)
?qqmath

2. With that many points  **any** test for a specific distributional form
will be rejected. Goodness of fit tests are essentially meaningless in this
context. This is a somewhat contentious assertion that might generate heated
disagreement. What a lawyer would call "argumentative." ;-) 

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Zhao
> Sent: Thursday, November 17, 2005 9:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Goodness fit test HELP!
> 
> Hi there,
> 
> I'm a newbie, plesae bear with me.
> I have a dataset with about 10000 ~ 30000 data points. Would 
> like fit to
> both Gamma and Normal distribution to see which one fits 
> better. How do I do
> this in R? Or I could do a normality test of the data, if 
> it's normal, I
> then will do a normal fit, otherwise, a gamma fit. But again, 
> I don't know
> how to do this either.
> Please help!
> 
> David
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From HDoran at air.org  Thu Nov 17 19:02:29 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 17 Nov 2005 13:02:29 -0500
Subject: [R] nlme question
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A0100FD0A@dc1ex3.air.org>

I think the authors are mistaken. Sigma is random error, and due to its
randomness it cannot be systematically related to anything. It is this
ind. assumption that allows for the likelihood to be expressed as
described in Pinhiero and Bates p.62. 

If you are finding that sigma is systematically related to a fixed
effect, then your model is misspecified and there are omitted
characteristics that need to be accounted for. 

-----Original Message-----
From: Wassell, James T., Ph.D. [mailto:jtw2 at cdc.gov] 
Sent: Thursday, November 17, 2005 11:52 AM
To: Doran, Harold; r-help at stat.math.ethz.ch
Subject: RE: [R] nlme question

Thank you for taking the time to think about my problem. 

The reference states:  "The covariance structure must be considered,
because for unbalanced data the estimates" (i.e. mu, sigma and tau hats)
"are not typically independent." Page 105.  It would be nice to simply
assume zero covariance terms, but the authors reject this
simplification.



From mschwartz at mn.rr.com  Thu Nov 17 19:06:07 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 17 Nov 2005 12:06:07 -0600
Subject: [R] Help with read.csv2
In-Reply-To: <8a83e5000511170809m492bffbao1f2b1a196eb8b667@mail.gmail.com>
References: <8a83e5000511170809m492bffbao1f2b1a196eb8b667@mail.gmail.com>
Message-ID: <1132250767.4391.80.camel@localhost.localdomain>

On Thu, 2005-11-17 at 17:09 +0100, Matthieu Cornec wrote:
> Hello,
>  I am importing the following file
>   ;aa;bb;cc
> 1988;12;12;12
> 1989;78;78;12
> 1990;78;78;12
> 1991;78;78;12
> 1992;78;78;12
> 1993;78;78;12
> 1994;78;78;12
> ------------------------------------------------
> data<-read.csv2("test.csv",header=T)
> ------------------------------------------
> it gives
>   X aa bb cc
> 1 1988 12 12 12
> 2 1989 78 78 12
> 3 1990 78 78 12
> 4 1991 78 78 12
> 5 1992 78 78 12
> 6 1993 78 78 12
> 7 1994 78 78 12
>  How do I get :
> ------------------------
>  aa bb cc
> 1988 12 12 12
> 1989 78 78 12
> 1990 78 78 12
> 1991 78 78 12
> 1992 78 78 12
> 1993 78 78 12
> 1994 78 78 12
> ----------------------------

Are you indicating that you want the years to be the rownames and NOT a
column in the data frame?

If so, use the 'row.names' argument to indicate that the first column of
the incoming data file contains the rownames:

> dat <- read.csv2("clipboard", row.names = 1)

> dat
     aa bb cc
1988 12 12 12
1989 78 78 12
1990 78 78 12
1991 78 78 12
1992 78 78 12
1993 78 78 12
1994 78 78 12

> str(dat)
`data.frame':   7 obs. of  3 variables:
 $ aa: int  12 78 78 78 78 78 78
 $ bb: int  12 78 78 78 78 78 78
 $ cc: int  12 12 12 12 12 12 12

> rownames(dat)
[1] "1988" "1989" "1990" "1991" "1992" "1993" "1994"



Note that you are getting the "X" in your initial example above, since
you are missing the first column name in the text file. A better
approach here might be (depending upon your intent) to use the
'col.names' argument to specify the column names explicitly:

> dat <- read.csv2("clipboard", header = TRUE, 
                    col.names = c("Years", "aa", "bb", "cc"))

> dat
  Years aa bb cc
1  1988 12 12 12
2  1989 78 78 12
3  1990 78 78 12
4  1991 78 78 12
5  1992 78 78 12
6  1993 78 78 12
7  1994 78 78 12

This way, you are defining the first column name during the import and
your 'Years' are available as a data column. It all depends upon what
you want to do with the data at this point.

HTH,

Marc Schwartz



From ggrothendieck at gmail.com  Thu Nov 17 19:35:24 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 17 Nov 2005 13:35:24 -0500
Subject: [R] Predicting and Plotting "hypothetical" values of factors
In-Reply-To: <x23blvfbuq.fsf@viggo.kubism.ku.dk>
References: <437CB41D.9080808@ku.edu> <x23blvfbuq.fsf@viggo.kubism.ku.dk>
Message-ID: <971536df0511171035r58a3a8aaud8946b159542d0f8@mail.gmail.com>

On 17 Nov 2005 18:00:45 +0100, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Paul Johnson <pauljohn at ku.edu> writes:
>
> > Last Friday, I noticed that it is difficult to work with regression
> > models in which there are factors.  It is easier to do the old fashioned
> > thing of coding up "dummy" variables with 0-1 values.  The predict
> > function's newdata argument is not suited to insertion of hypothetical
> > values for the factor, whereas it has no trouble with numeric variables.
> >   For example, if one uses a factor as a predictor in a logistic
> > regression, then it is tough to plot the S-shaped curve that describes
> > the probabilities.
> >
> > Here is some code with comments describing the difficulties that we have
> > found.  Are there simpler, less frustrating approaches?
>
> I think the point is that if you think a factor can take intermediate
> values in between the groups (as in sex==1.245), then it is really a
> numeric variable and should be treated as such in the model, whether
> or not it has only two distinct values in your data set.
>
> It is not obvious to me what the "S shaped curve" means in a model
> which really only specifies two probabilities.
>
>        -p
>
>
> > -----------------------------
> > # Paul Johnson  <pauljohn at ku.edu> 2005-11-17
> > # factorFutzing-1.R
> >
> >
> > myfac <- factor(c(1.1, 4.5, 1.1, 1.1, 4.5, 1.1, 4.5, 1.1))
> >
> > y <- c(0,1,0,1,0,0,1,0)
> >
> > mymod1 <-glm (y~myfac, family=binomial)
> >
> > p1 <- predict(mymod1, type="response")
> >
> > plot(myfac,p1)
> >
> > # Wait, that's not the picture I wanted. I want to see the
> > # proverbial S-shaped curve!
> > #
> >
> > # The contrast variable that R creates is coded 0 and 1.
> > # I'd like to have a sequence from 0 to 1 (seq(0,1,length.out=10)) and
> > # get predicted values for each.  That would give the S-shaped curve.
> >
> >
> > # But the use of predict does not work because the factor has 2
> > # values and the predict function won't take my new data:
> >
> > predict(mymod1, newdata=data.frame(myfac=seq(0,1,length.out=8)))
> >
> > # Error: variable 'myfac' was fitted with class "factor" but class
> > "numeric" was supplied
> > # In addition: Warning message:
> > # variable 'myfac' is not a factor in: model.frame.default(Terms,
> > newdata, na.action = na.action, xlev = object$xlevels)
> >
> > # Isn't there an easier way than this?
> >
> > c1 <- coef(mymod1)
> >
> > myseq <- seq(0,1, length.out=10)
> >
> > newdf <- data.frame(1, myseq)
> >
> > linearPredictor <- as.matrix(newdf) %*% c1
> >
> > p2 <- 1/ (1 + exp(-linearPredictor))
> > # But there is a big disaster if you just try the obvious thing
> > # of plotting with
> > # lines(myseq,p2)
> > # The line does not show up where you hope in the plot.
> > # The plot "appears to" have the x-axis on the scale
> > # 1.1 to 4.5, So in order to see the s-shaped curve, it appears we have
> > # to re-scale. However, this is a big illusion.  To see what I mean,
> > # do
> >
> > points(2,.4)
> >
> > # you expected to see the point appear at (2,.4), but in the plot
> > # it appears at (4.5,.4).  Huh?
> >
> > # The actual values being plotted are the integer-valued levels that
> > # R uses for factors, the numbers you get from as.numeric(myfac).
> > # So it is only necessary to re-scale the sequence by adding one.
> >
> > myseq2 <- 1 +  myseq
> >
> > lines( myseq2, p2, type="l" )
> >
> > #Its not all that S-shaped, but you have to take what you can get! :)


Perhaps still too complicated but if interp is
an interpolation function and xx are the two levels and yy
are the corresponding linear predictors (not the responses)
then plot xx interpolated vs the inverse link of yy
interpolated:

# interpolation function
interp <- function(x, length = 100) seq(x[1], x[2], length = length)

# levels and predictions (on linear predictor scale) from levels
xx <- c(1.1, 4.5)
yy <- predict(mymod1, newdata = list(myfac = factor(levs)))

plot(interp(xx), family(mymod1)$linkinv(interp(yy)))



From stratja at auburn.edu  Thu Nov 17 18:44:06 2005
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Thu, 17 Nov 2005 11:44:06 -0600
Subject: [R] cv.glm help
Message-ID: <437C6D06020000F20000181A@TMIA1.AUBURN.EDU>

I would appreciate some help writing R code to plot predicted species
richness vs. observed species richness (nat_est) with 95% CI lines.  

I'm using glm to get model coefficients and remove-one cross validation
to get predicted (cv.glm). 

Here is what I have for the code so far:

SRCOUNT <- read.table(file.choose(),header=T) 
library(boot)
library(MASS)
quk.native <- glm.nb(nat_est ~ UK + I(UK^2), SRCOUNT, link=log)
cv.quk  <- cv.glm(SRCOUNT, quk)

Many thanks,

Jeff

****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja



From ripley at stats.ox.ac.uk  Thu Nov 17 20:09:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Nov 2005 19:09:57 +0000 (GMT)
Subject: [R] dev.copy legend problem
In-Reply-To: <73dae3060511170803m70e31ad3t1a481e8fe8d21cba@mail.gmail.com>
References: <73dae3060511170803m70e31ad3t1a481e8fe8d21cba@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0511171859360.26337@gannet.stats>

That's why we recommend that you do not use dev.copy but replot on the 
target device.

Different devices have different fonts and font metrics, and so the plot 
as laid out for one will almost always not be right for the other.

According to the help page for dev.copy you cannot by default copy from 
postscript() to pdf().  Is that really what you are doing?  It seems to 
work for me once I enable copying, as those two devices do share fonts.

On Thu, 17 Nov 2005, Florence Combes wrote:

> We are facing this problem for long, and so ask for your help.
>
> We are plotting 2 graphs in a postscript device (left part -layout
> function-), and the common legend for these graphs on the right part.
> The legend in the postscript device looks ok: this is color lines with
> numbers on the right (6 columns) , see the code below:
>
>> nblock<-c(1:48)
>> leg<-paste(c(1:npin)," ",sep=" ")
>> legend(0,19,legend = leg, col=rainbow(nblock), lty=1,
> merge=TRUE,ncol=6,bty="n",cex=0.6)
>
> The problem we are facing is that we dev.copy to a pdf device and then, the
> legend doesn't look the same: numbers overlap a little lines.
>
> Has someone already encountered such a thing ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From charles.raux at let.ish-lyon.cnrs.fr  Thu Nov 17 20:20:30 2005
From: charles.raux at let.ish-lyon.cnrs.fr (Charles Raux)
Date: Thu, 17 Nov 2005 20:20:30 +0100
Subject: [R] transforming table into data frame
Message-ID: <437CE60E.2107.D7B5E7@let.ish-lyon.cnrs.fr>

Hello,
I would like to transform a contingency table (A x B) into a data 
frame while keeping the cross-tabulation structure (i.e. rows = 
levels of A, columns = levels of B). I have tried as.data.frame, 
xtabs, etc... and always got a "flattened" list of 3 variables (A, B, 
Freq).
How to do that?
Thanks in advance
Charles Raux



From deepayan.sarkar at gmail.com  Thu Nov 17 20:22:01 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 17 Nov 2005 13:22:01 -0600
Subject: [R] nlme question
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A0100FD0A@dc1ex3.air.org>
References: <F5ED48890E2ACB468D0F3A64989D335A0100FD0A@dc1ex3.air.org>
Message-ID: <eb555e660511171122k20796537r2fb5592e1b4548a3@mail.gmail.com>

On 11/17/05, Doran, Harold <HDoran at air.org> wrote:
> I think the authors are mistaken. Sigma is random error, and due to its
> randomness it cannot be systematically related to anything. It is this
> ind. assumption that allows for the likelihood to be expressed as
> described in Pinhiero and Bates p.62.

I think not. The issue is dependence between the _estimates_ of sigma,
tao, etc, and that may well be present. Presumably, if one can compute
the likelihood surface as a function of the 3 parameters, the hessian
at the MLE's would give the estimated covariance. However, I don't
think nlme does this.

A different approach you might want to consider is using mcmcsamp in
the lme4 package (or more precisely, the Matrix package) to get
samples from the joint posterior distribution. This is likely to be
better than the asymptotic normal approximation in any case.

Deepayan



From sundar.dorai-raj at pdf.com  Thu Nov 17 20:29:36 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 17 Nov 2005 13:29:36 -0600
Subject: [R] transforming table into data frame
In-Reply-To: <437CE60E.2107.D7B5E7@let.ish-lyon.cnrs.fr>
References: <437CE60E.2107.D7B5E7@let.ish-lyon.cnrs.fr>
Message-ID: <437CDA20.3060002@pdf.com>



Charles Raux wrote:
> Hello,
> I would like to transform a contingency table (A x B) into a data 
> frame while keeping the cross-tabulation structure (i.e. rows = 
> levels of A, columns = levels of B). I have tried as.data.frame, 
> xtabs, etc... and always got a "flattened" list of 3 variables (A, B, 
> Freq).
> How to do that?
> Thanks in advance
> Charles Raux
> 

How about:

x <- table(state.division, state.region)
as.data.frame(array(x, dim(x), dimnames(x)))

HTH,

--sundar

--sundar



From RVARADHAN at JHMI.EDU  Thu Nov 17 20:32:52 2005
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Thu, 17 Nov 2005 14:32:52 -0500
Subject: [R] Converting "numeric" variable to time
Message-ID: <000301c5ebad$b35f3d40$5994100a@win.ad.jhu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/f968911b/attachment.pl

From wpeng at bccrc.ca  Thu Nov 17 21:23:33 2005
From: wpeng at bccrc.ca (Wendy Peng)
Date: Thu, 17 Nov 2005 12:23:33 -0800
Subject: [R] (no subject)
Message-ID: <0BE438149FF2254DB4199E2682C8DFEBF649D9@crcmail1.BCCRC.CA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/eede242e/attachment.pl

From monch1962 at gmail.com  Thu Nov 17 21:31:08 2005
From: monch1962 at gmail.com (David Mitchell)
Date: Fri, 18 Nov 2005 07:31:08 +1100
Subject: [R] Portable R?
In-Reply-To: <17276.40358.394069.146704@basebud.nulle.part>
References: <f6508a860511161916p79490255ke041d0f5719ad691@mail.gmail.com>
	<17275.64366.678545.955642@basebud.nulle.part>
	<Pine.LNX.4.61.0511170732240.19598@gannet.stats>
	<17276.40358.394069.146704@basebud.nulle.part>
Message-ID: <f6508a860511171231r49747c31xf87a6cd047b7895c@mail.gmail.com>

My experiences parallel Dirk's - I also work for several financial
institutions and large corporates, and probably 80% of them allow me
to plug in a USB key and move files around.

Yep, I find it strange too...

Dave M.

On 11/18/05, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> On 17 November 2005 at 07:37, Prof Brian Ripley wrote:
> | I would be surprised to find a PC in a work environment that was bootable
> | from USB or even from DVD/CD.  Even in academia we have such things turned
> | off (and it would be a disciplinary offence to run your own OS on
> | a University-owned machine).
>
> Yes, though it really depends on how well organised and run a workplace is [
> having worked for a few different employers in financial services, I can
> assure you that this varies significantly. ]  But your point is spot-on, and
> I should have mentioned it. E.g. at my current (well-run) workplace, USB
> access is disabled (and we don't get admin rights).



From spencer.graves at pdf.com  Thu Nov 17 21:36:21 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Nov 2005 12:36:21 -0800
Subject: [R] nlminb failed to converge with lmer
In-Reply-To: <BAY103-DAV730DB923CEA2BE78233E5906E0@phx.gbl>
References: <BAY103-DAV730DB923CEA2BE78233E5906E0@phx.gbl>
Message-ID: <437CE9C5.1080705@pdf.com>

	  There are many ways to get this kind of message.  With binary 
outcomes, if complete separation is possible, "nlminb" (or "optim") will 
get tired trying to crank the slope up to Inf;  in general, it suggests 
the model is overparameterized in some way.

	  You say this is only a warning message.  Do you still get answers? 
If yes, can you still get confidence intervals for the paramters?  If 
yes, I might try different sets of starting values;  if they all gave me 
comparable answers, I might decide not to worry about the warning.

	  If I wanted more, I might trace through the code line by line until I 
executed "nlminb" and got the same message.  Then I'd compute the 
eigenvalues and vectors of the hessian.  With luck I might be able to 
figure out from them just how the model is overparameterized.  To do 
this, I first tried typing "lmer" at a command prompt:

 > lmer
standardGeneric for "lmer" defined from package "Matrix"

function (formula, data, family, method = c("REML", "ML", "PQL",
     "Laplace", "AGQ"), control = list(), start, subset, weights,
     na.action, offset, model = TRUE, x = FALSE, y = FALSE, ...)
standardGeneric("lmer")

	  That was not enough.  So I then tried 'showMethods("lmer")'; 
'methods' did not work in this case, because it's S3 and "lmer" is an S4 
generic (or something like that).  I got the following:

 > showMethods("lmer")

Function "lmer":
formula = "formula"
	
	  With this information, I then tried, 'getMethod("lmer", "formula")', 
which gave me the desired source code.  I could then copy it into a 
script file, walk through it line by line, and learn something.

	  Also, I might try to find a much simpler example that produces the 
same message, e.g, using simulated data with as few observations and 
variables as possible.  If you submit a simple, self-contained example, 
others might copy it from your email into R to see if they get the same 
message.  This kind of example makes it much easier for a potential 
respondent to identify the problem, check the code to make sure, and 
compose a reply in a few seconds, and this in turn icreases the chances 
you will receive a quick, useful reply.  (See also the posting guide! 
"www.R-project.org/posting-guide.html".)

	  Good Luck!
	  spencer graves

J??r??me Lema??tre wrote:

> Dear all,
> 
> I'm building binomial mixed-model using lme4 package. 
> I'm able to obtain outputs properly except when I include two particular
> variables: date (from 23 to 34; 1 being to first sampling day) and Latitude
> (UTM/100 000, from 55.42 to 56.53). No "NA" is any of those variables.
> In those cases, I get the warning message: "nlminb failed to converge"
> I tried to modify lmer controls as : msMaxIter; maxIter... but I still get
> the message.
> Should I bother about it?
> If yes, what should I do to not get the message?
> 
> Thank you all in advance for you answers.
> 
> PS: My model writing is for example
> 
> Fm<-lmer(alive~factor(sex)+mass+parasite+Latitude+(1|ID), family=binomial,
> method="AGQ", data=donnee).
> 
> Where "parasite" is presence/absence (0 or 1) and ID is the station identity
> where I captured 0 to 20 specimens, each being alive or dead, having a sex,
> a mass, a presence/absence of parasite. Latitude is given at the station
> level. Date is given at the specimen level because I sampled for 4 days in
> each station.
> 
> 
> J??r??me Lema??tre
> Ph.D. student
> D??partment of biology,
> University Laval
> Quebec, Canada
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From leaflovesun at yahoo.ca  Thu Nov 17 22:13:33 2005
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Thu, 17 Nov 2005 14:13:33 -0700
Subject: [R] Point pattern to grid
Message-ID: <200511172113.jAHLDgga011007@hypatia.math.ethz.ch>

Dear all,

I'd like to change a point pattern to a grid of cells and use one of the variables as the output.

e.g.  The point pattern is of a window of (500*500) and several features such as pH, SoilType etc.  I like to divide it into a grid with cell size 5*5, and use the mean of the point values falling inside the cell as the output.

Is there any package in R working with this? Thanks in advance!

Cheers,

Leaf



From ross at biostat.ucsf.edu  Thu Nov 17 22:50:01 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 17 Nov 2005 13:50:01 -0800
Subject: [R] Makevars
Message-ID: <1132264201.2949.19.camel@iron.psg.net>

Is it safe to make any assumptions about how Makevars is used?

In particular, I want to include some conditional material, and wonder
if the GNU make conditional commands would work (at least for those
building with GNU make).  Does Makevars get included in the Makefile?
Does it get treated as a shell script?  Something else?

Is there any kind of assumption I can make portably?

I realize I could generate the file from a configure script, but I'd
like to avoid doing special purpose code that this case would require.

Thanks.
-- 
Ross Boylan                                      wk:  (415) 514-8146
185 Berry St #5700                               ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 514-8150
University of California, San Francisco
San Francisco, CA 94107-1739                     hm:  (415) 550-1062



From jholtman at gmail.com  Thu Nov 17 23:07:15 2005
From: jholtman at gmail.com (jim holtman)
Date: Thu, 17 Nov 2005 17:07:15 -0500
Subject: [R] Converting "numeric" variable to time
In-Reply-To: <000301c5ebad$b35f3d40$5994100a@win.ad.jhu.edu>
References: <000301c5ebad$b35f3d40$5994100a@win.ad.jhu.edu>
Message-ID: <644e1f320511171407mbca53adl37de52aba594fbb8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/07e88d42/attachment.pl

From michael.conklin at markettools.com  Thu Nov 17 23:28:09 2005
From: michael.conklin at markettools.com (Michael Conklin)
Date: Thu, 17 Nov 2005 16:28:09 -0600
Subject: [R]  Strange parsing behavior
Message-ID: <E51A91AB9FA86C44A929E431E0E2EAB2020A36C6@mnmail01.markettools.com>

I am suddenly having a problem with my R installation (R 2.1.1) in that any linear models that access variables that have a "." in their name fail.

e.g. 
> tempdata<-data.frame(dep=rnorm(1000),pred.test=rnorm(1000))
> lm(dep~pred.test,data=tempdata)
Error in parse(file, n, text, prompt) : parse error
>

But
> tempdata<-data.frame(dep=rnorm(1000),predtest=rnorm(1000))
> lm(dep~predtest,data=tempdata)

Call:
lm(formula = dep ~ predtest, data = tempdata)

Coefficients:
(Intercept)     predtest  
    0.05260      0.04453  

Clearly I have screwed something up but I have no idea what.  (This suddenly started happening today).

Michael Conklin
Chief Methodologist - Advanced Analytics
MarketTools Inc
(952)417-4719?? office
(612)201-8978 mobile
Michael.Conklin at markettools.com
??
??



From efg at stowers-institute.org  Thu Nov 17 23:28:46 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 17 Nov 2005 16:28:46 -0600
Subject: [R] RODBC help
References: <BAY114-F12CBDEDD885260F9BE1AA7D75C0@phx.gbl>
Message-ID: <dlj06v$hpg$1@sea.gmane.org>

I had a similar problem when I posted this recently:

RODBC and Excel: Wrong Data Type Assumed on Import
http://tolstoy.newcastle.edu.au/~rking/R/help/05/11/14938.html

My conclusion was: "Being lucky" shouldn't be part of processing Excel
files, which is the case when RODBC is used.

This reply gave some suggestions:
http://tolstoy.newcastle.edu.au/~rking/R/help/05/11/14990.html

I found this suggestion the most useful:

"You could try using the COM interface rather than the ODBC interface"
http://tolstoy.newcastle.edu.au/~rking/R/help/05/11/15030.html

This approach has problems if you have "holes" in your data, but with some
work I found RDCOMClient the way to go:
http://tolstoy.newcastle.edu.au/~rking/R/help/05/11/15090.html

IMHO, RODBC should only be used if you have an Excel file without holes, and
with very regular numeric data.  I don't understand why the online
documentation is not updated to give a usage note that RODBC will often fail
reading Excel files.

Specifically,

this help:

  library(RODBC)
  ?odbcConnectExce

should be modified to have a warning "RODBC considered harmful with Excel
files"

efg


"Keith Sabol" <sabolk at hotmail.com> wrote in message
news:BAY114-F12CBDEDD885260F9BE1AA7D75C0 at phx.gbl...
> I am using the RODBC package to read data from an Excel file.
> ...
> My problem appears to be related to specification of data types by column.



From j5rodrig at gmail.com  Thu Nov 17 23:50:24 2005
From: j5rodrig at gmail.com (Juan Rodriguez)
Date: Thu, 17 Nov 2005 14:50:24 -0800
Subject: [R] rmysql installed but unable to load
Message-ID: <babd8ba50511171450m3b6e6853w6e9f5e0cdc8d827e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051117/d73e9f4a/attachment.pl

From goran.brostrom at gmail.com  Thu Nov 17 23:52:09 2005
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Thu, 17 Nov 2005 23:52:09 +0100
Subject: [R] Mean survival times
In-Reply-To: <437C84C7.7070908@uni-jena.de>
References: <437C84C7.7070908@uni-jena.de>
Message-ID: <148ed8180511171452v5fc3edcx2aca05b42ef9692b@mail.gmail.com>

On 11/17/05, Christoph Scherber <Christoph.Scherber at uni-jena.de> wrote:
> Dear list,
>
> I have data on insect survival in different cages; these have the
> following structure:
>
>   deathtime status id cage    S      F     G   L     S
>         1.5      1  1 C1      8      2     1   1     1
>         1.5      1  2 C1      8      2     1   1     1
>        11.5      1  3 C1      8      2     1   1     1
>        11.5      1  4 C1      8      2     1   1     1
>
> There are 81 cages and each 20 individuals whose survival was followed
> over time. The columns S,F,G,L and S are experimentally manipulated
> factors thought to have an influence on survival.
>
> Using survfit(Surv(deathtime,status)~cage) gives me the survivorship
> curves for every cage. But what IÂ´d like to have is a mean survivorship
> value for every cage.
>
> Obviously, using tapply (deathtime,cage,mean) gives me mean values, but
> IÂ´d like to have a better estimate of this using a proper statistical
> model. IÂ´ve tried a glm with poisson errors (as suggested in CrawleyÂ´s
> book, page 628), but the back-transformed estimates (using status as the
> response variable and deathtime as an offset) were totally unrealistic.
>
> As IÂ´m new to survival analysis, it would be great if anyone could give
> me some hints on what method would be best.

No method is best, but some methods may be useful ;) One such may be
to fit a parametric model to your data. Check 'survreg'.

GÃ¶ran
>
> Thanks a lot!
> Christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
GÃ¶ran BrostrÃ¶m



From eeide at cs.utah.edu  Fri Nov 18 00:25:13 2005
From: eeide at cs.utah.edu (Eric Eide)
Date: Thu, 17 Nov 2005 16:25:13 -0700
Subject: [R] Histogram over a Large Data Set (DB): How?
Message-ID: <17277.4441.302347.151426@bas.flux.utah.edu>

Hi!  I'm new to R, and I have a question about how R works with large data sets
--- in particular, data sets that come from databases.

I'm using R 2.2.0 with the DBI package (0.1-9) and the RMySQL package (0.5-5).

My get-my-feet-wet-with-R project is to make a histogram from a data set stored
in a MySQL database.  In particular, I have a table that describes some
observed spam emails.  The 'unixtime' column of the table contains the
timestamps of the messages.  My current goal is to plot the number of spams per
day during the recording period.

So far, this is my script (edited for brevity), and it works well:

-----
library(RMySQL)
drv <- dbDriver("MySQL")
con <- dbConnect(drv, group="spam")

# "where" clause to limit data set size, as described below.
spams <- dbGetQuery(con, "select unixtime from email where LENGTH(email_to)=4")

firstspam <- min(spams$unixtime)
lastspam <- max(spams$unixtime)
# spansize == one day
spansize <- 60 * 60 * 24

firstbreak = floor(firstspam / spansize)
lastbreak = celing(lastspam / spansize)
spambreaks = (firstbreak:lastbreak) * spansize

hist(spams$unixtime, br=spambreaks, plot=TRUE, col="red")
-----

The "where" clause serves to limit the number of records, while I figure out
the surrounding parts.  And now I'd like to do that... but I'm not sure how!

The actual "email" table has 2.9 million rows (a lot of spam!), so I presume
that I cannot (or at least, shouldn't) read it all at once.  Reading the
documentation of the RMySQL package, I understand that the syntax I want is
something like this:

-----
rs <- dbSendQuery(con, "select unixtime from email order by unixtime")
out <- dbApply(rs, INDEX = "unixtime",
         FUN = function(x, grp) hist(x$unixtime, ...))
-----

But I can't quite seem to make this work.  When I try the above directly, I get
errors like this:

  Error in mysqlDBApply(res, ...) : unimplemented type 'NULL' in 'length<-'

More generally, I'm pretty sure that I'm misusing `hist' in the code above.  I
don't want a series of histograms; I want just one histogram, made from the
concatenation of all the records fetched from the table.

I didn't find a recipe for doing this sort of thing in the R FAQ or in the
archives for this mailing list, so I'm hoping that someone can set me on the
right track.  How does one code in R with functions that expect "whole"
vectors, and somehow provide those vectors in a "piecemeal" fashion?  Is there
a general recipe for this situation in R (aren't large data sets common?), or
do I need to code the histogram generator myself?

Thanks for any help! ---

Eric.

-- 
-------------------------------------------------------------------------------
Eric Eide <eeide at cs.utah.edu>  .         University of Utah School of Computing
http://www.cs.utah.edu/~eeide/ . +1 (801) 585-5512 voice, +1 (801) 581-5843 FAX



From helprhelp at gmail.com  Fri Nov 18 00:25:45 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 17 Nov 2005 17:25:45 -0600
Subject: [R] a weighted average by group
Message-ID: <cdf817830511171525v2dcd5eedg74d51d248e1cdfb2@mail.gmail.com>

Hi,
I have a data.frame a like this:
> a
   v1 v2 v3 v4
1   1  1  A  X
2   2  2  A  X
3   3  3  A  X
4   4  4  B  X
5   5  5  B  X
6   6  6  C  Y
7   7  7  C  Y
8   8  8  C  Y
9   9  9  D  Y
10 10 10  D  Y

I want to get a weighted average for each combination of v3 and v4,
using v1 as values and v2 as weights i.e.,
for A and X, the final result should be (1*1+2*2+3*3)/(1+2+3)

not sure which function I should use in R ?

thanks,
--
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From sdavis2 at mail.nih.gov  Fri Nov 18 01:03:57 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 17 Nov 2005 19:03:57 -0500
Subject: [R] Histogram over a Large Data Set (DB): How?
References: <17277.4441.302347.151426@bas.flux.utah.edu>
Message-ID: <000701c5ebd3$925ef290$6401a8c0@WATSON>

[R] Histogram over a Large Data Set (DB): How?

Have you tried just grabbing the whole column using dbGetQuery?  Try doing 
this:

spams <- dbGetQuery(con,"select unixtime from email limit 1000000")

Then increase from 1,000,000 to 1.5 million, to 2 million, etc.  until you 
break something (run out of memory), if you do at all.

However, the BETTER way to do this, if you already have the data in the 
database is to allow the database to do the histogram for you.  For example, 
to get a count of spams by day, in MySQL do something like:

spams <- dbGetQuery(con,"select date_format(unixtime,'%m/%d/%y'),count(*) 
from email group by date_format(unixtime,'%m/%d/%y')")

Then, you will have a count of the number of spams by date and can plot as 
you like.

Sean

----- Original Message ----- 
From: Eric Eide
To: r-help at stat.math.ethz.ch
Sent: Thursday, November 17, 2005 6:25 PM
Subject: [R] Histogram over a Large Data Set (DB): How?


Hi!  I'm new to R, and I have a question about how R works with large data 
sets
--- in particular, data sets that come from databases.
I'm using R 2.2.0 with the DBI package (0.1-9) and the RMySQL package 
(0.5-5).
My get-my-feet-wet-with-R project is to make a histogram from a data set 
stored
in a MySQL database.  In particular, I have a table that describes some
observed spam emails.  The 'unixtime' column of the table contains the
timestamps of the messages.  My current goal is to plot the number of spams 
per
day during the recording period.
So far, this is my script (edited for brevity), and it works well:
----- 
library(RMySQL)
drv <- dbDriver("MySQL")
con <- dbConnect(drv, group="spam")
# "where" clause to limit data set size, as described below.
spams <- dbGetQuery(con, "select unixtime from email where 
LENGTH(email_to)=4")
firstspam <- min(spams$unixtime)
lastspam <- max(spams$unixtime)
# spansize == one day
spansize <- 60 * 60 * 24
firstbreak = floor(firstspam / spansize)
lastbreak = celing(lastspam / spansize)
spambreaks = (firstbreak:lastbreak) * spansize
hist(spams$unixtime, br=spambreaks, plot=TRUE, col="red")
----- 
The "where" clause serves to limit the number of records, while I figure out
the surrounding parts.  And now I'd like to do that... but I'm not sure how!
The actual "email" table has 2.9 million rows (a lot of spam!), so I presume
that I cannot (or at least, shouldn't) read it all at once.  Reading the
documentation of the RMySQL package, I understand that the syntax I want is
something like this:
----- 
rs <- dbSendQuery(con, "select unixtime from email order by unixtime")
out <- dbApply(rs, INDEX = "unixtime",
         FUN = function(x, grp) hist(x$unixtime, ...))
----- 
But I can't quite seem to make this work.  When I try the above directly, I 
get
errors like this:
  Error in mysqlDBApply(res, ...) : unimplemented type 'NULL' in 'length<-'
More generally, I'm pretty sure that I'm misusing `hist' in the code above. 
I
don't want a series of histograms; I want just one histogram, made from the
concatenation of all the records fetched from the table.
I didn't find a recipe for doing this sort of thing in the R FAQ or in the
archives for this mailing list, so I'm hoping that someone can set me on the
right track.  How does one code in R with functions that expect "whole"
vectors, and somehow provide those vectors in a "piecemeal" fashion?  Is 
there
a general recipe for this situation in R (aren't large data sets common?), 
or
do I need to code the histogram generator myself?
Thanks for any help! --- 
Eric.
-- 
-------------------------------------------------------------------------------
Eric Eide <eeide at cs.utah.edu>  .         University of Utah School of 
Computing
http://www.cs.utah.edu/~eeide/ . +1 (801) 585-5512 voice, +1 (801) 581-5843 
FAX
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Fri Nov 18 01:47:52 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 17 Nov 2005 19:47:52 -0500
Subject: [R] a weighted average by group
In-Reply-To: <cdf817830511171525v2dcd5eedg74d51d248e1cdfb2@mail.gmail.com>
References: <cdf817830511171525v2dcd5eedg74d51d248e1cdfb2@mail.gmail.com>
Message-ID: <437D24B8.2050107@stats.uwo.ca>

On 11/17/2005 6:25 PM, Weiwei Shi wrote:
> Hi,
> I have a data.frame a like this:
> 
>>a
> 
>    v1 v2 v3 v4
> 1   1  1  A  X
> 2   2  2  A  X
> 3   3  3  A  X
> 4   4  4  B  X
> 5   5  5  B  X
> 6   6  6  C  Y
> 7   7  7  C  Y
> 8   8  8  C  Y
> 9   9  9  D  Y
> 10 10 10  D  Y
> 
> I want to get a weighted average for each combination of v3 and v4,
> using v1 as values and v2 as weights i.e.,
> for A and X, the final result should be (1*1+2*2+3*3)/(1+2+3)
> 
> not sure which function I should use in R ?

Use by().  For example,

by(a, list(a$v3, a$v4), function(subset) weighted.mean(subset$v1, 
subset$v2))

Duncan Murdoch



From spencer.graves at pdf.com  Fri Nov 18 02:06:03 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Nov 2005 17:06:03 -0800
Subject: [R] Linear model mixed with an ARIMA model
In-Reply-To: <1132028079.437960af2bf43@webmail.unb.ca>
References: <1132028079.437960af2bf43@webmail.unb.ca>
Message-ID: <437D28FB.6030306@pdf.com>

	  Have you considered using corARMA with, e.g., lme in library(nlme), 
as described in Pinheiro and Bates (2000) Mixed-Effects Models in S and 
S-PLUS (Springer, esp. table 5.3)?

	  hope this helps.
	  spencer graves
p.s.  You might get quicker and more useful replies if you follow the 
posting guide! "www.R-project.org/posting-guide.html".

Zhu, Zhaoxuan wrote:

> Dear all, 
> 
> 
> I'm looking for how can I input a linear model with an arma model,like
> 
> log(y) = 8.95756 + 0.0346414^t - 0.1*t^2   + ut           
> ut=-0.296ut-1+at-0.68at-1  
> 
> where log(y) is qudratic function ,for the time series trend,
> 
> and get then get the residuals from the first function. 
> 
>  " obersvations value - the fit value = ut"
> 
> and fit an ARIMA(1,1,1) model for ut.
> 
> anyway,how can I combine this two models together as a group ? 
> 
> my purpose is to  to use this mixed model forecast  'y'
> 
> can you help me?  I will very appreciate it.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Fri Nov 18 02:19:32 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Nov 2005 17:19:32 -0800
Subject: [R] nlme questions
In-Reply-To: <OF31D562AB.1B445541-ON842570AE.005E12E8@arauco.cl>
References: <OF31D562AB.1B445541-ON842570AE.005E12E8@arauco.cl>
Message-ID: <437D2C24.3080503@pdf.com>

	  Both your questions seem too vague to me.  You might get more useful 
replies if you provide a simple example in a few lines of R code that a 
reader could copy from your email into R and see the result (as 
suggested in the posting guide! "www.R-project.org/posting-guide.html"). 
  The process of preparing such a simple example might by itself provide 
the insight you desire.  Alternatively, you might work line by line 
through the code for the R function you are using.  Also, if you don't 
have Pinheiro and Bates (2000) Mixed-Effects Models in S and S-PLUS 
(Springer), I suggest you get it;  it is excellent for things like this.

	  I'm sorry I couldn't help more.  	
	  spencer graves	

Christian Mora wrote:

> 
> 
> 
> Dear R users;
> 
> Ive got two questions concerning nlme library 3.1-65 (running on R 2.2.0 /
> Win XP Pro). The first one is related to augPred function. Ive been working
> with a nonlinear mixed model with no problems so far. However, when the
> parameters of the model are specified in terms of some other covariates,
> say treatment (i.e. phi1~trt1+trt2, etc) the augPred function give me the
> following error: "Error in predict.nlme(object,
> value[1:(nrow(value)/nL),,drop=FALSE], : Levels 0,1 not allowed for trt1,
> trt2". The same model specification as well as the augPred function under
> SPlus 2000 run without problems. The second question has to deal with the
> time needed for the model to converge. It really takes a lot of time to fit
> the model on R in relation to the time required to fit the same model on
> SPlus. I can imagine this is related to the optimization algorithm or
> something like that, but I would like to have a different opinion on these
> two issues.
> 
> Thanks in advance
> 
> Christian Mora
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From gerifalte28 at hotmail.com  Fri Nov 18 02:40:24 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 18 Nov 2005 01:40:24 +0000
Subject: [R] discontinuous y-axis (ordinate with a -/ /-)
In-Reply-To: <20051117104001.GA27916@ukb2-09>
Message-ID: <BAY103-F291168402F1A9F99F5CBE8A65E0@phx.gbl>


axis.break() in the plotrix package will do that.  You still have to modify 
the axis scale.

Cheers

Francisco



>From: jobst landgrebe <jlandgr1 at gwdg.de>
>To: r-help at stat.math.ethz.ch
>Subject: [R] discontinuous y-axis (ordinate with a -/ /-)
>Date: Thu, 17 Nov 2005 11:40:01 +0100
>
>Dear List,
>
>can anyone tell me how to plot a discontinuous y-axis (ordinate
>with a -/ /- "break sign") to fit in data with a wide range without the
>need of logarthimic transformation? My data are distributed like this:
>
>(abscissa: 1:10)
>
>1. vector to plot
>
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>   4.030   5.987   6.865  19.520  16.200  88.000
>
>2. vector to plot
>
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>   0.000   2.112   2.620   2.976   4.303   7.030
>
>I have just this one 88 outlier and cannot log-transform the data
>(reviewers want plain data).
>
>I would be glad to get help.
>
>Yours sincerely,
>
>Jobst Landgrebe
>
>--
>Dr. Jobst Landgrebe
>Universität Göttingen
>Abt. Biochemie 2
>Heinrich-Düker-Weg 12
>37073 Göttingen
>Germany
>-------------------------------
>tel: 0551/39-5902 oder -2316
>fax: 0551/39-5979
>mail: jlandgr1 at gwdg.de
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Fri Nov 18 03:07:24 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 17 Nov 2005 21:07:24 -0500
Subject: [R] a weighted average by group
In-Reply-To: <cdf817830511171525v2dcd5eedg74d51d248e1cdfb2@mail.gmail.com>
References: <cdf817830511171525v2dcd5eedg74d51d248e1cdfb2@mail.gmail.com>
Message-ID: <971536df0511171807j5610c353y8510545de4eda097@mail.gmail.com>

On 11/17/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> Hi,
> I have a data.frame a like this:
> > a
>   v1 v2 v3 v4
> 1   1  1  A  X
> 2   2  2  A  X
> 3   3  3  A  X
> 4   4  4  B  X
> 5   5  5  B  X
> 6   6  6  C  Y
> 7   7  7  C  Y
> 8   8  8  C  Y
> 9   9  9  D  Y
> 10 10 10  D  Y
>
> I want to get a weighted average for each combination of v3 and v4,
> using v1 as values and v2 as weights i.e.,
> for A and X, the final result should be (1*1+2*2+3*3)/(1+2+3)
>
> not sure which function I should use in R ?


All our solutions will make use of this function f:

f <- function(i) weighted.mean(a$v1[i], a$v2[i])


# Here are three solutions that only give combos
# that are already present in the data:

aggregate(rownames(a), a[,3:4], f)

aggregate(1:nrow(a), a[,3:4], f)

sapply(split(1:nrow(a), a[,3:4], drop = TRUE), f)

# and here are two solutions that give all combos
# returning results in different ways:

tapply(1:nrow(a), a[,3:4], f)

sapply(split(1:nrow(a), a[,3:4]), f)

Aside:
I found it strange that rownames only works in the
first case.



From doriaba2 at snu.ac.kr  Fri Nov 18 06:37:37 2005
From: doriaba2 at snu.ac.kr (Dong H. Oh)
Date: Fri, 18 Nov 2005 14:37:37 +0900
Subject: [R] Opening help file hangs in emacs
Message-ID: <200511180537.jAI5bmxP004229@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/d5069339/attachment.pl

From joshi_juni at hotmail.com  Fri Nov 18 06:39:51 2005
From: joshi_juni at hotmail.com (Juni Joshi)
Date: Fri, 18 Nov 2005 16:39:51 +1100
Subject: [R] Fitting model with varying number of predictors
Message-ID: <BAY108-F1990135031D45943E57ED8FF5E0@phx.gbl>


   I  need  to fit a number of models with different number of predictors
   in  each  model.  Say for example, I have three predictors: x1, x2, x3
   and I want to fit three models:

   lm(y~x1+x2)
   lm(y~x2+x3)
   lm(y~x1+x2+x3)

   Instead  of  typing  all  models,  what I want is to create a variable
   which  can  take  the right hand side of the models. I tried this with
   paste function.

   xxx <- paste("x1","x2",sep=+) for the first
   xxx <- paste("x2","x3", sep = +) for the second
   xxx  <-  paste("x1","x2","x2",  sep  = +) for the third and then fit a
   single model

   lm(y~xxx)

   It did not work. Please suggest how to do it.

   Thanks.

   Jun


From ggrothendieck at gmail.com  Fri Nov 18 06:59:49 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 18 Nov 2005 00:59:49 -0500
Subject: [R] Fitting model with varying number of predictors
In-Reply-To: <BAY108-F1990135031D45943E57ED8FF5E0@phx.gbl>
References: <BAY108-F1990135031D45943E57ED8FF5E0@phx.gbl>
Message-ID: <971536df0511172159i2db4ba14ld18653a831b9c878@mail.gmail.com>

Try this:

x <- data.frame(x1 = x1, x2 = x2, x3 = x3)
lm(y ~., x[,1:2])
lm(y ~., x[,2:3])
lm(y ~., x[,1:3])



On 11/18/05, Juni Joshi <joshi_juni at hotmail.com> wrote:
>
>   I  need  to fit a number of models with different number of predictors
>   in  each  model.  Say for example, I have three predictors: x1, x2, x3
>   and I want to fit three models:
>
>   lm(y~x1+x2)
>   lm(y~x2+x3)
>   lm(y~x1+x2+x3)
>
>   Instead  of  typing  all  models,  what I want is to create a variable
>   which  can  take  the right hand side of the models. I tried this with
>   paste function.
>
>   xxx <- paste("x1","x2",sep=+) for the first
>   xxx <- paste("x2","x3", sep = +) for the second
>   xxx  <-  paste("x1","x2","x2",  sep  = +) for the third and then fit a
>   single model
>
>   lm(y~xxx)
>
>   It did not work. Please suggest how to do it.
>
>   Thanks.
>
>   Jun
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From perjmi at gmail.com  Fri Nov 18 07:04:41 2005
From: perjmi at gmail.com (Per Jensen)
Date: Fri, 18 Nov 2005 07:04:41 +0100
Subject: [R] Truncated observations in survreg
Message-ID: <8c0ddffa0511172204r49311a9cyd233352805cdaba1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/c7d7345b/attachment.pl

From ggrothendieck at gmail.com  Fri Nov 18 07:42:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 18 Nov 2005 01:42:00 -0500
Subject: [R] Predicting and Plotting "hypothetical" values of factors
In-Reply-To: <971536df0511171035r58a3a8aaud8946b159542d0f8@mail.gmail.com>
References: <437CB41D.9080808@ku.edu> <x23blvfbuq.fsf@viggo.kubism.ku.dk>
	<971536df0511171035r58a3a8aaud8946b159542d0f8@mail.gmail.com>
Message-ID: <971536df0511172242y3fd28689teff2d2da199dadf8@mail.gmail.com>

On 11/17/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 17 Nov 2005 18:00:45 +0100, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > Paul Johnson <pauljohn at ku.edu> writes:
> >
> > > Last Friday, I noticed that it is difficult to work with regression
> > > models in which there are factors.  It is easier to do the old fashioned
> > > thing of coding up "dummy" variables with 0-1 values.  The predict
> > > function's newdata argument is not suited to insertion of hypothetical
> > > values for the factor, whereas it has no trouble with numeric variables.
> > >   For example, if one uses a factor as a predictor in a logistic
> > > regression, then it is tough to plot the S-shaped curve that describes
> > > the probabilities.
> > >
> > > Here is some code with comments describing the difficulties that we have
> > > found.  Are there simpler, less frustrating approaches?
> >
> > I think the point is that if you think a factor can take intermediate
> > values in between the groups (as in sex==1.245), then it is really a
> > numeric variable and should be treated as such in the model, whether
> > or not it has only two distinct values in your data set.
> >
> > It is not obvious to me what the "S shaped curve" means in a model
> > which really only specifies two probabilities.
> >
> >        -p
> >
> >
> > > -----------------------------
> > > # Paul Johnson  <pauljohn at ku.edu> 2005-11-17
> > > # factorFutzing-1.R
> > >
> > >
> > > myfac <- factor(c(1.1, 4.5, 1.1, 1.1, 4.5, 1.1, 4.5, 1.1))
> > >
> > > y <- c(0,1,0,1,0,0,1,0)
> > >
> > > mymod1 <-glm (y~myfac, family=binomial)
> > >
> > > p1 <- predict(mymod1, type="response")
> > >
> > > plot(myfac,p1)
> > >
> > > # Wait, that's not the picture I wanted. I want to see the
> > > # proverbial S-shaped curve!
> > > #
> > >
> > > # The contrast variable that R creates is coded 0 and 1.
> > > # I'd like to have a sequence from 0 to 1 (seq(0,1,length.out=10)) and
> > > # get predicted values for each.  That would give the S-shaped curve.
> > >
> > >
> > > # But the use of predict does not work because the factor has 2
> > > # values and the predict function won't take my new data:
> > >
> > > predict(mymod1, newdata=data.frame(myfac=seq(0,1,length.out=8)))
> > >
> > > # Error: variable 'myfac' was fitted with class "factor" but class
> > > "numeric" was supplied
> > > # In addition: Warning message:
> > > # variable 'myfac' is not a factor in: model.frame.default(Terms,
> > > newdata, na.action = na.action, xlev = object$xlevels)
> > >
> > > # Isn't there an easier way than this?
> > >
> > > c1 <- coef(mymod1)
> > >
> > > myseq <- seq(0,1, length.out=10)
> > >
> > > newdf <- data.frame(1, myseq)
> > >
> > > linearPredictor <- as.matrix(newdf) %*% c1
> > >
> > > p2 <- 1/ (1 + exp(-linearPredictor))
> > > # But there is a big disaster if you just try the obvious thing
> > > # of plotting with
> > > # lines(myseq,p2)
> > > # The line does not show up where you hope in the plot.
> > > # The plot "appears to" have the x-axis on the scale
> > > # 1.1 to 4.5, So in order to see the s-shaped curve, it appears we have
> > > # to re-scale. However, this is a big illusion.  To see what I mean,
> > > # do
> > >
> > > points(2,.4)
> > >
> > > # you expected to see the point appear at (2,.4), but in the plot
> > > # it appears at (4.5,.4).  Huh?
> > >
> > > # The actual values being plotted are the integer-valued levels that
> > > # R uses for factors, the numbers you get from as.numeric(myfac).
> > > # So it is only necessary to re-scale the sequence by adding one.
> > >
> > > myseq2 <- 1 +  myseq
> > >
> > > lines( myseq2, p2, type="l" )
> > >
> > > #Its not all that S-shaped, but you have to take what you can get! :)
>
>
> Perhaps still too complicated but if interp is
> an interpolation function and xx are the two levels and yy
> are the corresponding linear predictors (not the responses)
> then plot xx interpolated vs the inverse link of yy
> interpolated:
>
> # interpolation function
> interp <- function(x, length = 100) seq(x[1], x[2], length = length)
>
> # levels and predictions (on linear predictor scale) from levels
> xx <- c(1.1, 4.5)
> yy <- predict(mymod1, newdata = list(myfac = factor(levs)))
>
> plot(interp(xx), family(mymod1)$linkinv(interp(yy)))
>

Sorry, I just realized that I had changed the variable name levs
to xx before posting but not in all spots so here it is again:

# interpolation function
interp <- function(x, length = 100) seq(x[1], x[2], length = length)

# levels and predictions (on linear predictor scale) from levels
xx <- c(1.1, 4.5)
yy <- predict(mymod1, newdata = list(myfac = factor(xx)))

plot(interp(xx), family(mymod1)$linkinv(interp(yy)))



From wwzhang at gmail.com  Fri Nov 18 08:13:58 2005
From: wwzhang at gmail.com (Wen Zhang)
Date: Fri, 18 Nov 2005 15:13:58 +0800
Subject: [R] help on ks.test and shapiro.test
Message-ID: <B5A7E346-483A-44A1-84EA-06DECF258F59@gmail.com>

I have three files of data which are available at http://zhangw.com/ 
R/, varied at the number of data. I tried to use R to analyze using  
shapiro.test, ks.test, and t.test. t.test ran as expected, however,  
when I run shapiro.test and ks.test commands, error message always  
occurred. Error message for
shapiro.test is
"Error in "[.data.frame"(x, complete.cases(x)) :
	undefined columns selected"

ks.test is
"Error in ks.test(control) : argument "y" is missing, with no default"

As a newcomer to R, I don't know how to deal with this. Can anyone  
help me?
--- ---

Life is a foreign language; all man mispronouce it.
                 ---Christopher Morley

ç»ˆäºŽï¼Œé‚£é‡Žå…½å€’ä¸‹äº†ï¼Œå¼‚æ•™å¾’ä»¬ä¸ºæ­¤è€Œæ¬¢åº†ã€‚ ä½†æ˜¯ 
ä¸€åˆ‡å¹¶æœªç»“æŸï¼Œå› ä¸ºä¸€åªå·¨é¸Ÿå·²ä»Žç°çƒ¬ä¸­è¯žç”Ÿã€‚é‚£é¸Ÿ 
ä»Žç©ºä¸­ä¿¯è§†ç€å¼‚æ•™å¾’ä»¬ï¼Œå‘ä»–ä»¬æ–½æ”¾ç«ä¸Žé›·ç”µã€‚å› ä¸º 
è¿™é‡Žå…½å·²é‡ç”Ÿä¸ºå·¨é¸Ÿï¼Œæ¢å¤äº†åŠ›é‡ã€‚é‡‘é’±çš„è¿½éšè€…ä»¬ 
åœ¨ææƒ§ä¸­åŽé€€ã€‚

                   Mozilla ä¹‹ä¹¦, 7:15

My Blog: http://zhangw.com/wp/

Reclaim Your Inbox!
http://www.mozilla.org/products/thunderbird



From guangxing at ict.ac.cn  Fri Nov 18 08:37:29 2005
From: guangxing at ict.ac.cn (=?gb2312?B?uePQxw==?=)
Date: Fri, 18 Nov 2005 15:37:29 +0800
Subject: [R] How to generate the random numbers with the lognormal
	distribution?
Message-ID: <200511180735.jAI7ZJ4Y028149@hypatia.math.ethz.ch>

Hi,R-list!
I am a newbie to the R and what I want to know mostly now is as follow:
Using "rnorm()",we could get the random numbers with normal distribution,
but how to generate the random numbers with  the lognormal distribution?

Thank you in advance!

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-18



From vito_ricci at yahoo.com  Fri Nov 18 08:39:28 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 18 Nov 2005 08:39:28 +0100 (CET)
Subject: [R] Goodness fit test HELP!
Message-ID: <20051118073928.41141.qmail@web36106.mail.mud.yahoo.com>

Hi David,

you could see my contribute:

"Fitting distributions with R"
http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf

Hoping it could be helpful.

Regards,

Vito




You wrote:

Hi there,

I'm a newbie, plesae bear with me.
I have a dataset with about 10000 ~ 30000 data points.
Would like fit to
both Gamma and Normal distribution to see which one
fits better. How do I do
this in R? Or I could do a normality test of the data,
if it's normal, I
then will do a normal fit, otherwise, a gamma fit. But
again, I don't know
how to do this either.
Please help!

David


Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From ligges at statistik.uni-dortmund.de  Fri Nov 18 08:48:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 18 Nov 2005 08:48:50 +0100
Subject: [R] How to generate the random numbers with the
	lognormal	distribution?
In-Reply-To: <200511180735.jAI7ZJ4Y028149@hypatia.math.ethz.ch>
References: <200511180735.jAI7ZJ4Y028149@hypatia.math.ethz.ch>
Message-ID: <437D8762.3040901@statistik.uni-dortmund.de>

¹ãÐÇ wrote:

> Hi,R-list!
> I am a newbie to the R and what I want to know mostly now is as follow:
> Using "rnorm()",we could get the random numbers with normal distribution,
> but how to generate the random numbers with  the lognormal distribution?

What about searching in the help files?
It's not that far away:
?rlnorm

Uwe Ligges


> Thank you in advance!
> 
> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-18
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Nov 18 08:59:29 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 18 Nov 2005 08:59:29 +0100
Subject: [R] help on ks.test and shapiro.test
In-Reply-To: <B5A7E346-483A-44A1-84EA-06DECF258F59@gmail.com>
References: <B5A7E346-483A-44A1-84EA-06DECF258F59@gmail.com>
Message-ID: <437D89E1.7020504@statistik.uni-dortmund.de>

Wen Zhang wrote:

> I have three files of data which are available at http://zhangw.com/ 
> R/, varied at the number of data. I tried to use R to analyze using  
> shapiro.test, ks.test, and t.test. t.test ran as expected, however,  
> when I run shapiro.test and ks.test commands, error message always  
> occurred. Error message for
> shapiro.test is
> "Error in "[.data.frame"(x, complete.cases(x)) :
> 	undefined columns selected"
> 
> ks.test is
> "Error in ks.test(control) : argument "y" is missing, with no default"
> 
> As a newcomer to R, I don't know how to deal with this. Can anyone  
> help me?

Please start reading the posting guide and the help files of those 
functions.
Without you giving a reproducible example, we cannot help that much.

Uwe Ligges



> --- ---
> 
> Life is a foreign language; all man mispronouce it.
>                  ---Christopher Morley
> 
> ç»ˆäºŽï¼Œé‚£é‡Žå…½å€’ä¸‹äº†ï¼Œå¼‚æ•™å¾’ä»¬ä¸ºæ­¤è€Œæ¬¢åº†ã€‚ ä½†æ˜¯ 
> ä¸€åˆ‡å¹¶æœªç»“æŸï¼Œå› ä¸ºä¸€åªå·¨é¸Ÿå·²ä»Žç°çƒ¬ä¸­è¯žç”Ÿã€‚é‚£é¸Ÿ 
> ä»Žç©ºä¸­ä¿¯è§†ç€å¼‚æ•™å¾’ä»¬ï¼Œå‘ä»–ä»¬æ–½æ”¾ç«ä¸Žé›·ç”µã€‚å› ä¸º 
> è¿™é‡Žå…½å·²é‡ç”Ÿä¸ºå·¨é¸Ÿï¼Œæ¢å¤äº†åŠ›é‡ã€‚é‡‘é’±çš„è¿½éšè€…ä»¬ 
> åœ¨ææƒ§ä¸­åŽé€€ã€‚
> 
>                    Mozilla ä¹‹ä¹¦, 7:15
> 
> My Blog: http://zhangw.com/wp/
> 
> Reclaim Your Inbox!
> http://www.mozilla.org/products/thunderbird
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Nov 18 09:02:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 18 Nov 2005 09:02:42 +0100
Subject: [R] unprecise question on read.table() and the nortest package;
 was: (no subject)
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEBF649D9@crcmail1.BCCRC.CA>
References: <0BE438149FF2254DB4199E2682C8DFEBF649D9@crcmail1.BCCRC.CA>
Message-ID: <437D8AA2.5090908@statistik.uni-dortmund.de>

Wendy Peng wrote:

> Hi there,
> 
>  
> 
> I am trying to perform different normality tests in R. I have no problem
> when I use Shapiro.test( ). However, when I try to use any normality
> tests from the Nortest package, I kept getting this error  " Error in
> read.table(source, header = T, fill = T) : 
> 
>         'file' must be a character string or connection". I have no idea
> why because there was no problem with the read.table () in the
> Shapiro.test(). I had installed Nortest package already.


Please start reading the posting guide and the help files of those 
functions. Note that using a subject line on a high traffic mailing list 
should be mandatory.
Without you giving a reproducible example, we cannot help that much.

Uwe Ligges


>  
> 
> Thank you
> 
> Wendy Peng
> 
> BC Cancer Agency
> 
> Cancer Genetics Department
> 
> Co-p Programmer
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From fcombes at gmail.com  Fri Nov 18 09:16:54 2005
From: fcombes at gmail.com (Florence Combes)
Date: Fri, 18 Nov 2005 09:16:54 +0100
Subject: [R] dev.copy legend problem
In-Reply-To: <1132248015.4391.52.camel@localhost.localdomain>
References: <73dae3060511170803m70e31ad3t1a481e8fe8d21cba@mail.gmail.com>
	<1132248015.4391.52.camel@localhost.localdomain>
Message-ID: <73dae3060511180016x3a2a03abn77a76a12bac0b0eb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/5b5bbc15/attachment.pl

From bhs2 at mevik.net  Fri Nov 18 09:30:48 2005
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 18 Nov 2005 09:30:48 +0100
Subject: [R] changing figure size in Sweave
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BACFB@xchg1.statistik.local>
	(TEMPL Matthias's message of "Thu, 17 Nov 2005 14:36:05 +0100")
References: <83536658864BC243BE3C06D7E936ABD5027BACFB@xchg1.statistik.local>
Message-ID: <m0d5kygxxi.fsf@bar.nemo-project.org>

TEMPL Matthias wrote:

> Use \setkeys{Gin} to modify figure sizes or use explicit
> \includegraphics commands in combination with Sweave option
> include=FALSE.

Or use \documentclass[nogin,...]{...}.  Then the 'Gin' will have no
effect, and the size of the plots in the document will not be changed
from the size given as <<...,height=??,width=??>> (i.e. the size
produced by R).

-- 
Bj??rn-Helge Mevik



From ripley at stats.ox.ac.uk  Fri Nov 18 09:32:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Nov 2005 08:32:34 +0000 (GMT)
Subject: [R] Fitting model with varying number of predictors
In-Reply-To: <BAY108-F1990135031D45943E57ED8FF5E0@phx.gbl>
References: <BAY108-F1990135031D45943E57ED8FF5E0@phx.gbl>
Message-ID: <Pine.LNX.4.61.0511180827520.23185@gannet.stats>

On Fri, 18 Nov 2005, Juni Joshi wrote:

>
>   I  need  to fit a number of models with different number of predictors
>   in  each  model.  Say for example, I have three predictors: x1, x2, x3
>   and I want to fit three models:
>
>   lm(y~x1+x2)
>   lm(y~x2+x3)
>   lm(y~x1+x2+x3)
>
>   Instead  of  typing  all  models,  what I want is to create a variable
>   which  can  take  the right hand side of the models. I tried this with
>   paste function.
>
>   xxx <- paste("x1","x2",sep=+) for the first

This gives a syntax error!

>   xxx <- paste("x2","x3", sep = +) for the second
>   xxx  <-  paste("x1","x2","x2",  sep  = +) for the third and then fit a
>   single model
>
>   lm(y~xxx)
>
>   It did not work. Please suggest how to do it.

You want a formula here, and you also need to use code without syntax 
errors.  For example

> xxx <- paste("x1", "x2", sep="+")
> lm(as.formula(paste("y ~", xxx)))

But a better idea would be to put all your variables into a data frame DF 
and do

use <- c("x1", "x2") # set as appropriate
lm(y ~ ., data=DF[c("y", use)])

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Fri Nov 18 09:39:05 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 18 Nov 2005 09:39:05 +0100 (CET)
Subject: [R] Point pattern to grid
In-Reply-To: <200511172113.jAHLDgga011007@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0511180920410.3750-100000@reclus.nhh.no>

On Thu, 17 Nov 2005, Leaf Sun wrote:

> Dear all,
> 
> I'd like to change a point pattern to a grid of cells and use one of the
> variables as the output.
> 
> e.g.  The point pattern is of a window of (500*500) and several features
> such as pH, SoilType etc.  I like to divide it into a grid with cell
> size 5*5, and use the mean of the point values falling inside the cell
> as the output.
> 
> Is there any package in R working with this? Thanks in advance!

This might have been better posted on R-sig-geo. Try this:

library(sp)
df1 <- data.frame(x=runif(10000,0,500), y=runif(10000,0,500),
  z=rnorm(10000))
coordinates(df1) <- c("x", "y")
summary(df1) # SpatialPointsDataFrame
grd <- GridTopology(c(2.5,2.5), c(5,5), c(100,100))
sgrd <- SpatialGrid(grd) #SpatialGrid
bbox(sgrd)
res <- overlay(sgrd, df1)
# find which grid cells the points are in
str(res)
try0 <- lapply(split(as(df1, "data.frame"), res), mean)
# take means by grid cell - assumes all numeric columns in df1
# (soil type??) - maybe write a custom function to handle non-numeric 
# columns sensibly
try01 <- vector(mode="list", length=prod(slot(slot(sgrd, "grid"),
  "cells.dim")))
nafill <- rep(as.numeric(NA), ncol(as(df1, "data.frame")))
try01 <- lapply(try01, function(x) nafill)
# make a container to put the means in with the right number of columns
try01[as.integer(names(try0))] <- try0
# insert means into correct list elements
try1 <- data.frame(t(data.frame(try01)))
# transpose
summary(try1)
sgrd1 <- SpatialGridDataFrame(slot(sgrd, "grid"), try1)
image(sgrd1, "x")
image(sgrd1, "y")
image(sgrd1, "z")

It goes a bit further than the short description of the sp package in the 
latest R-News, and will most likely be a new method for overlay in sp. If 
these are your 200K points, it may take a little longer ...

> 
> Cheers,
> 
> Leaf
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ym at climpact.com  Fri Nov 18 09:48:26 2005
From: ym at climpact.com (Yves Magliulo)
Date: 18 Nov 2005 09:48:26 +0100
Subject: [R] Fitting model with varying number of predictors
In-Reply-To: <971536df0511172159i2db4ba14ld18653a831b9c878@mail.gmail.com>
References: <BAY108-F1990135031D45943E57ED8FF5E0@phx.gbl>
	<971536df0511172159i2db4ba14ld18653a831b9c878@mail.gmail.com>
Message-ID: <1132303706.4958.12.camel@new-york.climpact.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/7ec0ea7f/attachment.pl

From fcombes at gmail.com  Fri Nov 18 09:53:35 2005
From: fcombes at gmail.com (Florence Combes)
Date: Fri, 18 Nov 2005 09:53:35 +0100
Subject: [R] dev.copy legend problem
In-Reply-To: <Pine.LNX.4.61.0511171859360.26337@gannet.stats>
References: <73dae3060511170803m70e31ad3t1a481e8fe8d21cba@mail.gmail.com>
	<Pine.LNX.4.61.0511171859360.26337@gannet.stats>
Message-ID: <73dae3060511180053h8420337s7d840e01b95530bb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/1120afe5/attachment.pl

From Roger.Bivand at nhh.no  Fri Nov 18 10:09:23 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 18 Nov 2005 10:09:23 +0100 (CET)
Subject: [R] Morans I for Spatial Surveillance
In-Reply-To: <50D7651CF10CA64F9ABD0DE89466AE3B012712DF@exmsp3.perth.ad.curtin.edu.au>
Message-ID: <Pine.LNX.4.44.0511180959410.3750-100000@reclus.nhh.no>

On Thu, 17 Nov 2005, Serryn Eagleson wrote:

> 
> Hello, 
> 
> I am interested in using Morans I for different time intervals to detect
> disease clusters.
> 
> Ultimately I would like to use CUSUM - or similar monitoring statistic
> to monitor the results of Morans I - similar to the work by Rogerson
> (2005) Spatial Surveillance and Cummulative Sum Methods in Spatial and
> Syndromic Surveillance for Public Health.
> 
> Thus far - thanks to the list I have Morans I running in a loop for each
> day (however I found that on some days no data is recorded, this caused
> an error to get around this error I included an elseif statement to skip
> the calculation for days where no disease notifications are recorded.)

Please consider posting on R-sig-geo (see the Spatial Task View for
address) rather than this general list. The book with Rogerson's chapter
isn't yet available everywhere - not reached Norwegian university
libraries yet (journal articles are much more accessible from e-journals
than book chapters), so a very small number of people will know what you
are interested in. 

If you want to follow this up, you'll have to be more specific. The
strucchange package has some interesting functions for these issues, but
you'll have to explain what Rogerson proposes.

> 
> I am just wondering if anyone has code/advice for the best way to
> apprach the next stage of monitoring Morans I over different time
> intervals for the detection of abnormal clusters. My code thus far is
> below - I would also like to assemble the results in a neat table if
> anyone has tips for this I would alo appreciate it as I am rather new to
> R!
> 
> Thanks
> Serryn
> 
> T1<-read.dbf("ob.dbf")
> N <- colnames(T1)
> 
> T2nb <- read.gal("PC.gal",override=TRUE)
> col.W <- nb2listw(T2nb, style="W")
> 
> for(i in seq(N)) {   
>       Vec1 <- spNamedVec(N[i+7], T1)
>       Svec1<-sum(Vec1)
>        ifelse ( Svec1> 0, res[i]<-moran.test(spNamedVec(N[i+7], T1), col.W, zero.policy=TRUE), res[i]<-"NULL") 
> 
>                         }
> 
> print(res)
> 
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From claus.atzenbeck at freenet.de  Fri Nov 18 11:01:03 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Fri, 18 Nov 2005 11:01:03 +0100 (CET)
Subject: [R] Compiled name to object
Message-ID: <Pine.OSX.4.61.0511181046090.5027@cirrus.aue.aau.dk>

Hi,

I have a tricky problem with composing a name that should represent an
object:

I want to pass a data frame and a string that contains a column name to
a function, e.g.:

    check(testFrame, "seconds")

The function should now divide the testFrame into two subsets and call a
t-test on the numbers that are in the given column:

    check <- function(frame, column) {
      sessionNo <- c("s01", "s02", "s03", "s04", "s05", "s06")
      earlySessions <- subset(frame, frame$session %in% sessionNo)
      lateSessions <- subset(frame, !frame$session %in% sessionNo)
      t.test(earlySessions$column, lateSessions$column)
    }

How can I get R to change "earlySessions$column" to whatever was passed
as string, in the above example "earlySessions$seconds"?

Thanks for any hint.
Claus



From r.hankin at noc.soton.ac.uk  Fri Nov 18 11:21:27 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 18 Nov 2005 10:21:27 +0000
Subject: [R] repeated %in%
Message-ID: <BF575670-CABC-4A54-834D-1FD5DD76AC90@soc.soton.ac.uk>

Hi

I have a list of vectors, each one of which should be a subset of the  
previous one.
How do I check that this property actually holds?

Toy problem follows with a list of length 4 but my list can be any  
length


subsets <- list(
                 l1 = 1:10 ,
                 l2 = seq(from=1,to=9,by=2),
                 l3 = c(3,7),
                 l4 = 3
                 )

I need

all(subsets[[4]] %in% subsets[[3]]) &
all(subsets[[3]] %in% subsets[[2]]) &
all(subsets[[2]] %in% subsets[[1]])

I can write a little function to do this:


check.for.inclusion <- function(subsets){
   out <- rep(FALSE,length(subsets)-1)
   for(i in 1:(length(subsets)-1)){
     out[i] <- all(subsets[[i+1]] %in% subsets[[i]])
   }
   return(all(out))
}


how to do it elegantly and/or quickly?




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From claus.atzenbeck at freenet.de  Fri Nov 18 11:20:30 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Fri, 18 Nov 2005 11:20:30 +0100 (CET)
Subject: [R] Compiled name to object
In-Reply-To: <Pine.OSX.4.61.0511181046090.5027@cirrus.aue.aau.dk>
References: <Pine.OSX.4.61.0511181046090.5027@cirrus.aue.aau.dk>
Message-ID: <Pine.OSX.4.61.0511181119170.6041@cirrus.aue.aau.dk>

On Fri, 18 Nov 2005, Claus Atzenbeck wrote:

> How can I get R to change "earlySessions$column" to whatever was passed
> as string, in the above example "earlySessions$seconds"?

In the meanwhile I received a solution: earlySessions[[column]]

Claus



From buser at stat.math.ethz.ch  Fri Nov 18 11:21:59 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Fri, 18 Nov 2005 11:21:59 +0100
Subject: [R] Compiled name to object
In-Reply-To: <Pine.OSX.4.61.0511181046090.5027@cirrus.aue.aau.dk>
References: <Pine.OSX.4.61.0511181046090.5027@cirrus.aue.aau.dk>
Message-ID: <17277.43847.563583.34135@stat.math.ethz.ch>

Dear Claus

You can write

earlySessions[,"seconds"] instead of earlySessions$seconds and
with that syntax you can also use:

col1 <- "seconds"
earlySessions[,col1]

and you will get what you are looking for.

Best regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Claus Atzenbeck writes:
 > Hi,
 > 
 > I have a tricky problem with composing a name that should represent an
 > object:
 > 
 > I want to pass a data frame and a string that contains a column name to
 > a function, e.g.:
 > 
 >     check(testFrame, "seconds")
 > 
 > The function should now divide the testFrame into two subsets and call a
 > t-test on the numbers that are in the given column:
 > 
 >     check <- function(frame, column) {
 >       sessionNo <- c("s01", "s02", "s03", "s04", "s05", "s06")
 >       earlySessions <- subset(frame, frame$session %in% sessionNo)
 >       lateSessions <- subset(frame, !frame$session %in% sessionNo)
 >       t.test(earlySessions$column, lateSessions$column)
 >     }
 > 
 > How can I get R to change "earlySessions$column" to whatever was passed
 > as string, in the above example "earlySessions$seconds"?
 > 
 > Thanks for any hint.
 > Claus
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Nov 18 11:35:17 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Nov 2005 11:35:17 +0100
Subject: [R] repeated %in%
In-Reply-To: <BF575670-CABC-4A54-834D-1FD5DD76AC90@soc.soton.ac.uk>
References: <BF575670-CABC-4A54-834D-1FD5DD76AC90@soc.soton.ac.uk>
Message-ID: <x2zmo2kzve.fsf@viggo.kubism.ku.dk>

Robin Hankin <r.hankin at noc.soton.ac.uk> writes:

> Hi
> 
> I have a list of vectors, each one of which should be a subset of the  
> previous one.
> How do I check that this property actually holds?
> 
> Toy problem follows with a list of length 4 but my list can be any  
> length
> 
> 
> subsets <- list(
>                  l1 = 1:10 ,
>                  l2 = seq(from=1,to=9,by=2),
>                  l3 = c(3,7),
>                  l4 = 3
>                  )
> 
> I need
> 
> all(subsets[[4]] %in% subsets[[3]]) &
> all(subsets[[3]] %in% subsets[[2]]) &
> all(subsets[[2]] %in% subsets[[1]])
> 
> I can write a little function to do this:
> 
> 
> check.for.inclusion <- function(subsets){
>    out <- rep(FALSE,length(subsets)-1)
>    for(i in 1:(length(subsets)-1)){
>      out[i] <- all(subsets[[i+1]] %in% subsets[[i]])
>    }
>    return(all(out))
> }
> 
> 
> how to do it elegantly and/or quickly?

How about

> !any(sapply(mapply(setdiff,subsets[-1],subsets[-4]),length))
[1] TRUE

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From B.Rowlingson at lancaster.ac.uk  Fri Nov 18 11:39:18 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 18 Nov 2005 10:39:18 +0000
Subject: [R] repeated %in%
In-Reply-To: <BF575670-CABC-4A54-834D-1FD5DD76AC90@soc.soton.ac.uk>
References: <BF575670-CABC-4A54-834D-1FD5DD76AC90@soc.soton.ac.uk>
Message-ID: <437DAF56.3030308@lancaster.ac.uk>

Robin Hankin wrote:

> check.for.inclusion <- function(subsets){
>    out <- rep(FALSE,length(subsets)-1)
>    for(i in 1:(length(subsets)-1)){
>      out[i] <- all(subsets[[i+1]] %in% subsets[[i]])
>    }
>    return(all(out))
> }
> 
> 
> how to do it elegantly and/or quickly?
> 

  My first thought was to rewrite that function but drop out if it didnt 
match. But then...

  Assuming the first list element is the longest, then its a one liner:

check2 <- function(l){
   length(unique(unlist(l))) == length(l[[1]])
}

  - basically the set of everything in all the elements is the first 
element, so take unique(everything) and see if its the same size as the 
first element. Unless I've missed something...

  I'd call that elegant, it may also be quicker!

Baz



From B.Rowlingson at lancaster.ac.uk  Fri Nov 18 11:42:28 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 18 Nov 2005 10:42:28 +0000
Subject: [R] repeated %in%
In-Reply-To: <BF575670-CABC-4A54-834D-1FD5DD76AC90@soc.soton.ac.uk>
References: <BF575670-CABC-4A54-834D-1FD5DD76AC90@soc.soton.ac.uk>
Message-ID: <437DB014.4040704@lancaster.ac.uk>

Whoops

  The code I just posted only tested if all the subsequent elements were 
subsets of the first, it didn't check all the sequential subsets!

  Too cold to think straight here today...

Baz



From Matthias.Templ at statistik.gv.at  Fri Nov 18 11:56:10 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 18 Nov 2005 11:56:10 +0100
Subject: [R] Building S4-classes, documents
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAD01@xchg1.statistik.local>

Hi Seth,

Thank you very much! (and thanks to Matthias Kohl and Sean Davis)
My first problems in building a minimal S4-class package are solved with
the help of your hints.
Especially making R CMD INSTALL before trying R CMD check was a very
good idea!

Matthias


> Hi Matthias,
> 
> On 17 Nov 2005, Matthias.Templ at statistik.gv.at wrote:
> > Hello,
> >
> > I have some troubles when building S4-class packages.
> >
> > All my (S4-)code works well (without building a package).
> >
> > When building a package, in the R prompt after checking S3 
> > generic/method consistency Following error occurs: Fehler: 
> Kann R Kode 
> > in Packet 'AddNoise' nicht laden (~Error: Can not load R code from 
> > package 'AddNoise') (and there are some warnings after the
> > error)
> 
> I don't have documentation to recommend other than what you 
> mentioned.  However, a few things to look into:
> 
> 1. If you have your R code in multiple files, you may need to use the
>    DESCRIPTION file's Collate field to control the loading order.
>    Basically, you want: Class defintions, generics, methods, other.
>
> 2. Put methods and any other packages you depend on in Depends.
> 
> 3. Add SaveImage: yes.
> 
> Also, make sure R CMD INSTALL yourPackage/ works before 
> trying R CMD check yourPackage.
> 
> + seth
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From Antti.Pirjeta at hse.fi  Fri Nov 18 12:26:42 2005
From: Antti.Pirjeta at hse.fi (=?iso-8859-1?Q?Pirjet=E4_Antti?=)
Date: Fri, 18 Nov 2005 13:26:42 +0200
Subject: [R] R-News 5/2, Bayesian Model Averaging, a detail
Message-ID: <5EEDC3037D8C0144A46B1FEEFC20DCD0038AA546@e2k3org.hkkk.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/714396d3/attachment.pl

From Antti.Pirjeta at hse.fi  Fri Nov 18 12:54:08 2005
From: Antti.Pirjeta at hse.fi (=?iso-8859-1?Q?Pirjet=E4_Antti?=)
Date: Fri, 18 Nov 2005 13:54:08 +0200
Subject: [R] R-News 5/2, Bayesian Model Averaging, a detail
Message-ID: <5EEDC3037D8C0144A46B1FEEFC20DCD0038AA58A@e2k3org.hkkk.local>

The article on BMA (Bayesian model averaging) presents most valuable tools for model selection, but I find one detail confusing in Example 1. In page 4 of RNews 5/2, second paragraph says that the probability of Time variable not being in the model is 0.445. It seems to me that the figure should be 1 - 0.445 = 0.555, because p!=0.445 is the prob. of Time variable being in the model. The plot in Fig.2 is in line with this, since the height of scaled PDF seems to be 0.445 and the black spike points to 0.555. Have I understood this correctly?
 
Regards, Antti Pirjet??, Helsinki School of Economics. Mail to: pirjeta at hse.fi

Muutoksia s??hk??postiosoitteissamme: @hkkk.fi-p????tteiset osoitteet ovat muuttuneet muotoon @hse.fi. Opiskelijoiden (nyt @ky.hkkk.fi) osoitteen loppuosaksi muuttuu tammikuussa 2006 @student.hse.fi. Vanhoihin osoitteisiin l??hetetyt viestit ohjautuvat muutoksen j??lkeenkin perille.

Changes in our e-mail addresses:  @hkkk.fi has been replaced by @hse.fi. Students' e-mail address (@ky.hkkk.fi) will change in January 2006 to @student.hse.fi. E-mails sent to the old address will be redirected to the new one.



From christian_mora at arauco.cl  Fri Nov 18 13:00:57 2005
From: christian_mora at arauco.cl (Christian Mora)
Date: Fri, 18 Nov 2005 08:00:57 -0400
Subject: [R] nlme questions
Message-ID: <OF86EC5F41.13FB71B8-ON842570BD.00416E82@arauco.cl>






Spencer;

Thanks for your suggestions. I found the problem is in the library nlme. If
you define phi1~factor(trt1)+factor(trt2) instead of phi1~trt1+trt2 the
augPred function works. A bug? I don't know.

Christian






Spencer Graves <spencer.graves at pdf.com> on 17-11-2005 20:19:32

To:    Christian Mora <christian_mora at arauco.cl>
cc:    r-help at stat.math.ethz.ch

Subject:    Re: [R] nlme questions


   Both your questions seem too vague to me.  You might get more useful
replies if you provide a simple example in a few lines of R code that a
reader could copy from your email into R and see the result (as
suggested in the posting guide! "www.R-project.org/posting-guide.html").
  The process of preparing such a simple example might by itself provide
the insight you desire.  Alternatively, you might work line by line
through the code for the R function you are using.  Also, if you don't
have Pinheiro and Bates (2000) Mixed-Effects Models in S and S-PLUS
(Springer), I suggest you get it;  it is excellent for things like this.

   I'm sorry I couldn't help more.
   spencer graves

Christian Mora wrote:

>
>
>
> Dear R users;
>
> Ive got two questions concerning nlme library 3.1-65 (running on R 2.2.0
/
> Win XP Pro). The first one is related to augPred function. Ive been
working
> with a nonlinear mixed model with no problems so far. However, when the
> parameters of the model are specified in terms of some other covariates,
> say treatment (i.e. phi1~trt1+trt2, etc) the augPred function give me the
> following error: "Error in predict.nlme(object,
> value[1:(nrow(value)/nL),,drop=FALSE], : Levels 0,1 not allowed for trt1,
> trt2". The same model specification as well as the augPred function under
> SPlus 2000 run without problems. The second question has to deal with the
> time needed for the model to converge. It really takes a lot of time to
fit
> the model on R in relation to the time required to fit the same model on
> SPlus. I can imagine this is related to the optimization algorithm or
> something like that, but I would like to have a different opinion on
these
> two issues.
>
> Thanks in advance
>
> Christian Mora
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

--
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
 Fax: 408-280-7915



From maustin at amgen.com  Fri Nov 18 13:19:24 2005
From: maustin at amgen.com (Austin, Matt)
Date: Fri, 18 Nov 2005 04:19:24 -0800
Subject: [R] nlme questions
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD48E@teal-exch.amgen.com>

Warning:  non-expert thoughts to follow.

When passing an object to a predict method, the method looks at (a copy) of
the original information from the dataframe that was used in the fit.  Your
original data contains information on trt1 and trt2, but factor(trt1) and
factor(trt2) cannot be found in the original data.  If you did the factor
conversion in the original data 

myDat <- factor(myDat$trt1)
myDat <- factor(myDat$trt2)

then used myDat as the dataframe in the nlme call, all information would be
available for the augPred method.  That's why it works when you use trt1 and
trt2 instead of factor(trt1) and factor(trt2).  There is actually an
implicit factor conversion happening in the nlme call if trt1 and trt2 are
character variables, however if trt1 and trt2 are defined as numeric (ie 0
1) then it will fit as a numeric.

--Matt

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Christian Mora
> Sent: Friday, November 18, 2005 4:01 AM
> To: Spencer Graves
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] nlme questions
> 
> 
> 
> 
> 
> 
> 
> Spencer;
> 
> Thanks for your suggestions. I found the problem is in the 
> library nlme. If
> you define phi1~factor(trt1)+factor(trt2) instead of 
> phi1~trt1+trt2 the
> augPred function works. A bug? I don't know.
> 
> Christian
> 
> 
> 
> 
> 
> 
> Spencer Graves <spencer.graves at pdf.com> on 17-11-2005 20:19:32
> 
> To:    Christian Mora <christian_mora at arauco.cl>
> cc:    r-help at stat.math.ethz.ch
> 
> Subject:    Re: [R] nlme questions
> 
> 
>    Both your questions seem too vague to me.  You might get 
> more useful
> replies if you provide a simple example in a few lines of R 
> code that a
> reader could copy from your email into R and see the result (as
> suggested in the posting guide! 
> "www.R-project.org/posting-guide.html").
>   The process of preparing such a simple example might by 
> itself provide
> the insight you desire.  Alternatively, you might work line by line
> through the code for the R function you are using.  Also, if you don't
> have Pinheiro and Bates (2000) Mixed-Effects Models in S and S-PLUS
> (Springer), I suggest you get it;  it is excellent for things 
> like this.
> 
>    I'm sorry I couldn't help more.
>    spencer graves
> 
> Christian Mora wrote:
> 
> >
> >
> >
> > Dear R users;
> >
> > Ive got two questions concerning nlme library 3.1-65 
> (running on R 2.2.0
> /
> > Win XP Pro). The first one is related to augPred function. Ive been
> working
> > with a nonlinear mixed model with no problems so far. 
> However, when the
> > parameters of the model are specified in terms of some 
> other covariates,
> > say treatment (i.e. phi1~trt1+trt2, etc) the augPred 
> function give me the
> > following error: "Error in predict.nlme(object,
> > value[1:(nrow(value)/nL),,drop=FALSE], : Levels 0,1 not 
> allowed for trt1,
> > trt2". The same model specification as well as the augPred 
> function under
> > SPlus 2000 run without problems. The second question has to 
> deal with the
> > time needed for the model to converge. It really takes a 
> lot of time to
> fit
> > the model on R in relation to the time required to fit the 
> same model on
> > SPlus. I can imagine this is related to the optimization 
> algorithm or
> > something like that, but I would like to have a different opinion on
> these
> > two issues.
> >
> > Thanks in advance
> >
> > Christian Mora
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
>  Fax: 408-280-7915
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From christian_mora at arauco.cl  Fri Nov 18 13:32:33 2005
From: christian_mora at arauco.cl (Christian Mora)
Date: Fri, 18 Nov 2005 08:32:33 -0400
Subject: [R] nlme questions
Message-ID: <OF2429255D.4A360C83-ON842570BD.0043F68E@arauco.cl>






Yes, I agree. But if you define at the beginning of the code:

data$trt1<-as.factor(data$trt1)
data$trt2<-as.factor(data$trt2)

being trt1 and trt2 dummy variables with values 0 or 1, and then run the
model, for instance:

fit_1<-nlme(Y~b0/(1+exp((b1-X)/b2)),fixed=b0+b1+b2~trt1+trt2,
random=b0+b1+b2~1,data=data,start=fixef(fit_0))

the augPred function doesn't work and return the error

"Error in predict.nlme(object, value[1:(nrow(value)/nL),,drop=FALSE], : Levels 0,1 not allowed for trt1,
trt2"

but, if you modify the code as

fit_2<-nlme(Y~b0/(1+exp((b1-X)/b2)),fixed=b0+b1+b2~factor(trt1)+factor(trt2),
random=b0+b1+b2~1,data=data,start=fixef(fit_0))

i.e. indicating again that trt1 and trt2 are factors, even when they were
previouslly defined as factors through the function "as.factors", then
augPred works








"Austin, Matt" <maustin at amgen.com> on 18-11-2005 07:19:24

To:    "'Christian Mora'" <christian_mora at arauco.cl>, Spencer Graves
       <spencer.graves at pdf.com>
cc:    r-help at stat.math.ethz.ch

Subject:    RE: [R] nlme questions


Warning:  non-expert thoughts to follow.

When passing an object to a predict method, the method looks at (a copy) of
the original information from the dataframe that was used in the fit.  Your
original data contains information on trt1 and trt2, but factor(trt1) and
factor(trt2) cannot be found in the original data.  If you did the factor
conversion in the original data

myDat <- factor(myDat$trt1)
myDat <- factor(myDat$trt2)

then used myDat as the dataframe in the nlme call, all information would be
available for the augPred method.  That's why it works when you use trt1
and
trt2 instead of factor(trt1) and factor(trt2).  There is actually an
implicit factor conversion happening in the nlme call if trt1 and trt2 are
character variables, however if trt1 and trt2 are defined as numeric (ie 0
1) then it will fit as a numeric.

--Matt

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Christian Mora
> Sent: Friday, November 18, 2005 4:01 AM
> To: Spencer Graves
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] nlme questions
>
>
>
>
>
>
>
> Spencer;
>
> Thanks for your suggestions. I found the problem is in the
> library nlme. If
> you define phi1~factor(trt1)+factor(trt2) instead of
> phi1~trt1+trt2 the
> augPred function works. A bug? I don't know.
>
> Christian
>
>
>
>
>
>
> Spencer Graves <spencer.graves at pdf.com> on 17-11-2005 20:19:32
>
> To:    Christian Mora <christian_mora at arauco.cl>
> cc:    r-help at stat.math.ethz.ch
>
> Subject:    Re: [R] nlme questions
>
>
>    Both your questions seem too vague to me.  You might get
> more useful
> replies if you provide a simple example in a few lines of R
> code that a
> reader could copy from your email into R and see the result (as
> suggested in the posting guide!
> "www.R-project.org/posting-guide.html").
>   The process of preparing such a simple example might by
> itself provide
> the insight you desire.  Alternatively, you might work line by line
> through the code for the R function you are using.  Also, if you don't
> have Pinheiro and Bates (2000) Mixed-Effects Models in S and S-PLUS
> (Springer), I suggest you get it;  it is excellent for things
> like this.
>
>    I'm sorry I couldn't help more.
>    spencer graves
>
> Christian Mora wrote:
>
> >
> >
> >
> > Dear R users;
> >
> > Ive got two questions concerning nlme library 3.1-65
> (running on R 2.2.0
> /
> > Win XP Pro). The first one is related to augPred function. Ive been
> working
> > with a nonlinear mixed model with no problems so far.
> However, when the
> > parameters of the model are specified in terms of some
> other covariates,
> > say treatment (i.e. phi1~trt1+trt2, etc) the augPred
> function give me the
> > following error: "Error in predict.nlme(object,
> > value[1:(nrow(value)/nL),,drop=FALSE], : Levels 0,1 not
> allowed for trt1,
> > trt2". The same model specification as well as the augPred
> function under
> > SPlus 2000 run without problems. The second question has to
> deal with the
> > time needed for the model to converge. It really takes a
> lot of time to
> fit
> > the model on R in relation to the time required to fit the
> same model on
> > SPlus. I can imagine this is related to the optimization
> algorithm or
> > something like that, but I would like to have a different opinion on
> these
> > two issues.
> >
> > Thanks in advance
> >
> > Christian Mora
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
>
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
>  Fax: 408-280-7915
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
 >



From bhs2 at mevik.net  Fri Nov 18 13:41:57 2005
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 18 Nov 2005 13:41:57 +0100
Subject: [R] changing figure size in Sweave
In-Reply-To: <m0d5kygxxi.fsf@bar.nemo-project.org> 
	=?iso-8859-1?q?=28Bj=F8rn-Helge?= Mevik's message of "Fri,
	18 Nov 2005 09:30:48 +0100")
References: <83536658864BC243BE3C06D7E936ABD5027BACFB@xchg1.statistik.local>
	<m0d5kygxxi.fsf@bar.nemo-project.org>
Message-ID: <m0mzk2f7qi.fsf@bar.nemo-project.org>

Bj??rn-Helge Mevik wrote:

> Or use \documentclass[nogin,...]{...}.  Then the 'Gin' will have no
> effect, and the size of the plots in the document will not be changed
> from the size given as <<...,height=??,width=??>> (i.e. the size
> produced by R).

A small correction:  using the 'nogin' doesn't make LaTeX ignore
settings of 'Gin', but prevents Sweave.sty from setting it.

-- 
Bj??rn-Helge Mevik



From f.harrell at vanderbilt.edu  Fri Nov 18 14:04:51 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 18 Nov 2005 07:04:51 -0600
Subject: [R] Fitting model with varying number of predictors
In-Reply-To: <BAY108-F1990135031D45943E57ED8FF5E0@phx.gbl>
References: <BAY108-F1990135031D45943E57ED8FF5E0@phx.gbl>
Message-ID: <437DD173.6060408@vanderbilt.edu>

Juni Joshi wrote:
>    I  need  to fit a number of models with different number of predictors
>    in  each  model.  Say for example, I have three predictors: x1, x2, x3
>    and I want to fit three models:
> 
>    lm(y~x1+x2)
>    lm(y~x2+x3)
>    lm(y~x1+x2+x3)

Stepwise variable selection has so many problems that it generally ruins 
what you are trying to accomplish.  Trying many models leads to multiple 
coparison problems and bias in coefficient estimates, standard errors, 
R-squared.  With only three models being compared it's not a huge issue 
but in general beware.

Frank Harrell

> 
>    Instead  of  typing  all  models,  what I want is to create a variable
>    which  can  take  the right hand side of the models. I tried this with
>    paste function.
> 
>    xxx <- paste("x1","x2",sep=+) for the first
>    xxx <- paste("x2","x3", sep = +) for the second
>    xxx  <-  paste("x1","x2","x2",  sep  = +) for the third and then fit a
>    single model
> 
>    lm(y~xxx)
> 
>    It did not work. Please suggest how to do it.
> 
>    Thanks.
> 
>    Jun

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From strinz at freenet.de  Fri Nov 18 14:07:55 2005
From: strinz at freenet.de (strinz@freenet.de)
Date: Fri, 18 Nov 2005 14:07:55 +0100
Subject: [R] Surprise when mapping matrix to image
Message-ID: <E1Ed5y7-0006Bz-VD@www12.emo.freenet-rz.de>

Hello,

I wonder if 
image(t(x)[ncol(x):1, ])
can do the job correct!

perhaps this does the job better:

image(t(x)[,nrow(x):1])

Bj??rn



From: Prof Brian Ripley <ripley_at_stats.ox.ac.uk>
Date: Fri 27 Aug 2004 - 06:43:50 EST


On Thu, 26 Aug 2004, Glynn, Earl wrote:

> Start with:
>
> > x <- c(1:7,1)
> > dim(x) <- c(2,4)
> > x
> [,1] [,2] [,3] [,4]
> [1,] 1 3 5 7
> [2,] 2 4 6 1
>
> 2 Rows of 4 Columns. Upper-left and lower-right elements of the matrix
> are the same.
>
> All to this point makes good sense.

It's pure convention: see below.

> > image(x)
>
> However, this image shows 2 columns of 4 rows. The lower-left and
> upper-right elements are the same. This does not make sense to me.
> Did I miss some simple parameter to "fix" all of this naturally? Why
> would the numeric matrix of "x" and the image of "x" have such a
> different geometry?

Did you try reading the help for image? You don't seem to understand it if you actually did. It seems you are looking for

        image(t(x)[ncol(x):1, ])

Easy!

Mathematical conventions are just that, conventions. They differ by field of mathematics. Don't ask us why matrix rows are numbered down but graphs are numbered up the y axis, nor why x comes before y but row before column. But the matrix layout has always seemed illogical to me.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html




"Jetzt Handykosten senken mit klarmobil - 15 Ct./Min.! Hier klicken"
www.klarmobil.de/index.html?pid=73025



From jon.bielby at imperial.ac.uk  Fri Nov 18 15:27:40 2005
From: jon.bielby at imperial.ac.uk (Bielby, Jon)
Date: Fri, 18 Nov 2005 14:27:40 -0000
Subject: [R] Likely cause of error (code=1) in compar.gee/gee
Message-ID: <50B46338D54D214BAEAE2F0EC4C253A801C24726@icex1.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/7ad78097/attachment.pl

From groemping at tfh-berlin.de  Fri Nov 18 15:37:58 2005
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Fri, 18 Nov 2005 15:37:58 +0100
Subject: [R] Method for $
Message-ID: <20051118140018.M46734@tfh-berlin.de>

Dear R experts,

I have defined a class "myclass" and would like the slots to be extractable 
not only by "@" but also by "$". I now try to write a method for "$" that 
simply executes the request object at slotname, whenever someone calls 
object$slotname for any object of class "myclass".
I don't manage to find out how I can provide this function with "slotname", 
so that one and the same function works for any arbitrary slotname a user 
might choose.

I have tried

setMethod("$", signature(x="myclass"), function(x,slotname){ 
 x at slotname 
 } 
)

This produced the error message: 
In method for function "$": expanding the signature to 
include omitted arguments in definition: name = "missing" 
Error in rematchDefinition(definition, fdef, mnames, fnames, signature) : 
        methods can add arguments to the generic only if '...' is an argument 
to the generic

My searches for a solution of this problem have not been successful. Can 
someone help? 

Thanks and regards,
Ulrike Gr??mping, Berlin



From vasu.akkineni at gmail.com  Fri Nov 18 15:46:29 2005
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Fri, 18 Nov 2005 09:46:29 -0500
Subject: [R] Image display in R
Message-ID: <3b67376c0511180646y7ea4985bo9292b74d40e242c1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/4bab37a4/attachment.pl

From 042045003 at fudan.edu.cn  Fri Nov 18 15:57:30 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Fri, 18 Nov 2005 22:57:30 +0800
Subject: [R] about eval and eval.parent
Message-ID: <0IQ5000FANQYP5@mail.fudan.edu.cn>

x<-1
f<-function(){
x<-3
eval(substitute(x+y,list(y=10)))
}
f() #13

x<-1
f<-function(){
x<-3
eval(substitute(x+y,list(y=10)), envir = sys.frame(sys.parent()))
}
f() #11

x<-1
f<-function(){
x<-3
eval.parent(substitute(x+y,list(y=10)))
}
f()#11


the help page says:
    "If 'envir' is
     not specified, then 'sys.frame(sys.parent())', the environment
     where the call to 'eval' was made is used. "

     But it seems that the first one is not equal to the second one.
     
Thank you!


 				


2005-11-18

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From tlumley at u.washington.edu  Fri Nov 18 16:05:10 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 18 Nov 2005 07:05:10 -0800 (PST)
Subject: [R] about eval and eval.parent
In-Reply-To: <0IQ5000FANQYP5@mail.fudan.edu.cn>
References: <0IQ5000FANQYP5@mail.fudan.edu.cn>
Message-ID: <Pine.LNX.4.63a.0511180701140.5669@homer22.u.washington.edu>

On Fri, 18 Nov 2005, ronggui wrote:

> x<-1
> f<-function(){
> x<-3
> eval(substitute(x+y,list(y=10)))
> }
> f() #13
>
> x<-1
> f<-function(){
> x<-3
> eval(substitute(x+y,list(y=10)), envir = sys.frame(sys.parent()))
> }
> f() #11
>
> x<-1
> f<-function(){
> x<-3
> eval.parent(substitute(x+y,list(y=10)))
> }
> f()#11
>
>
> the help page says:
>    "If 'envir' is
>     not specified, then 'sys.frame(sys.parent())', the environment
>     where the call to 'eval' was made is used. "
>
>     But it seems that the first one is not equal to the second one.

No, it isn't.  This is because default arguments are evaluated inside the 
function but explicit argumnets are evaluated in the calling frame

In the first one, sys.frame(sys.parent()) is evaluated in the environment 
inside f(), and so refers to the frame from which f() was called.  That's 
what you would expect from pass-by-value

In the second one, sys.frame(sys.parent()) is evaluated inside 
substitute() and refers to the environment inside f(). That might be 
surprising, which is why it is documented.

 	-thomas



From ramos.beatriz at gmail.com  Fri Nov 18 16:20:30 2005
From: ramos.beatriz at gmail.com (Beatriz)
Date: Fri, 18 Nov 2005 16:20:30 +0100
Subject: [R] How to run R in batch mode
Message-ID: <437DF13E.4090502@gmail.com>

Hello to everybody!

I want to run R in batch mode but it doen't work (Error: syntax error)

I've found this in R help:
R CMD BATCH [options] infile [outfile]

I have tried differents commands:
(I have been working in the same directory I have "test.txt" file and 
"test2.txt" would be the output file)

 > R CMD BATCH "test.txt"

or

 > R CMD BATCH test.txt

or

 > BATCH test.txt

or

 > R CMD BATCH  test.txt  test2.txt

or

 > R CMD BATCH  "test.txt"  "test2.txt"

or

 > R CMD BATCH --no-save <"test.txt">


I don't know what is wrong. Could you help me, please?

Thanks a lot



From roebuck at mdanderson.org  Fri Nov 18 16:27:05 2005
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Fri, 18 Nov 2005 09:27:05 -0600 (CST)
Subject: [R] How to run R in batch mode
In-Reply-To: <437DF13E.4090502@gmail.com>
References: <437DF13E.4090502@gmail.com>
Message-ID: <Pine.OSF.4.58.0511180923570.316673@wotan.mdacc.tmc.edu>

On Fri, 18 Nov 2005, Beatriz wrote:

> I want to run R in batch mode but it doen't work (Error: syntax error)
>
> I've found this in R help:
> R CMD BATCH [options] infile [outfile]
>
> I have tried differents commands:
> (I have been working in the same directory I have "test.txt" file and
> "test2.txt" would be the output file)
>
>  > R CMD BATCH test.txt
>
> [SNIP additional tries]
>
> I don't know what is wrong. Could you help me, please?

Isn't it possible that it is working just fine and you really
do have a syntax error in your R source file (better named
as 'test.R') instead?

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From tom at maladmin.com  Fri Nov 18 11:44:51 2005
From: tom at maladmin.com (tom wright)
Date: Fri, 18 Nov 2005 05:44:51 -0500
Subject: [R] Image display in R
In-Reply-To: <3b67376c0511180646y7ea4985bo9292b74d40e242c1@mail.gmail.com>
References: <3b67376c0511180646y7ea4985bo9292b74d40e242c1@mail.gmail.com>
Message-ID: <1132310692.4819.121.camel@localhost.localdomain>

This peice of very useful getcell() function was posted for me on this
list by Duncan Murdoch (Thanks again Duncan). It allows the
identification of a particular plot in a matrix of plots. All you need
to do then is use this information to identify the original plot which
you can then re-plot.

###### Start Function ######
getcell <- function(pts = locator()) {
   usr <- par("usr")
   plt <- par("plt")
   mfg <- par("mfg")

   x <- pts$x
   y <- pts$y

   # convert to 0-1 plot coordinates
   px <- (x-usr[1])/(usr[2]-usr[1])
   py <- (y-usr[3])/(usr[4]-usr[3])

   # convert to 0-1 frame coordinates
   fx <- plt[1] + px*(plt[2]-plt[1])
   fy <- plt[3] + py*(plt[4]-plt[3])

   # convert to a relative cell position
   cx <- floor(fx)
   cy <- floor(fy)

   # return the absolute cell position
   list(row = -cy+mfg[1],col = cx+mfg[2])
}
####### End Function #######
#For example
Dset1<-1:10
Dset2<-rnorm(10)

oldpar<-par(mfrow=c(2,1))
plot(Dset1)
plot(Dset2)

cell<-getcell(locator(1))

if((cell$row*cell$col)<2){
	par(mfrow=c(1,1))
	plot(Dset1)
} else {
	par(mfrow=c(1,1))
	plot(Dset2)
}

I hope this gives you something to work with.
Tom

On Fri, 2005-18-11 at 09:46 -0500, Vasundhara Akkineni wrote:
> Hi all,
>  I am trying to display a matrix of plots(images), for example a 3*3 matrix
> of 9 image plots, such that when a user clicks on a image i can show the
> enlarged plot. I tried the multiple graphic device(using mfcol=c(3,3) and
> mfg), but it creates multiple plots in a single image file. So, i won't be
> able to highlight a particular plot when the user clicks on it.
>  To be more clear, can i display images in the form of a matrix in R. Are
> there any grids available? Please let me know.
>  Thanks,
> Vasu.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Fri Nov 18 16:40:59 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 18 Nov 2005 10:40:59 -0500
Subject: [R] How to run R in batch mode
In-Reply-To: <437DF13E.4090502@gmail.com>
References: <437DF13E.4090502@gmail.com>
Message-ID: <437DF60B.1040802@stats.uwo.ca>

On 11/18/2005 10:20 AM, Beatriz wrote:
> Hello to everybody!
> 
> I want to run R in batch mode but it doen't work (Error: syntax error)
> 
> I've found this in R help:
> R CMD BATCH [options] infile [outfile]
> 
> I have tried differents commands:
> (I have been working in the same directory I have "test.txt" file and 
> "test2.txt" would be the output file)
> 
>  > R CMD BATCH "test.txt"

Where did you type that?  It looks like you typed it within R.  That's a 
shell command, and needs to be typed in a shell.

However, I can't be sure about this, because you're not showing us what 
happened when you tried it.

Please, please, please:  if you need help with an error, show us the 
exact error message and tell us the context where you received it.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Fri Nov 18 16:58:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Nov 2005 15:58:00 +0000 (GMT)
Subject: [R] about eval and eval.parent
In-Reply-To: <0IQ5000FANQYP5@mail.fudan.edu.cn>
References: <0IQ5000FANQYP5@mail.fudan.edu.cn>
Message-ID: <Pine.LNX.4.61.0511181549210.6471@gannet.stats>

On Fri, 18 Nov 2005, ronggui wrote:

> x<-1
> f<-function(){
> x<-3
> eval(substitute(x+y,list(y=10)))
> }
> f() #13
>
> x<-1
> f<-function(){
> x<-3
> eval(substitute(x+y,list(y=10)), envir = sys.frame(sys.parent()))
> }
> f() #11
>
> x<-1
> f<-function(){
> x<-3
> eval.parent(substitute(x+y,list(y=10)))
> }
> f()#11
>
>
> the help page says:
>    "If 'envir' is
>     not specified, then 'sys.frame(sys.parent())', the environment
>     where the call to 'eval' was made is used. "
>
>     But it seems that the first one is not equal to the second one.

I think you have plucked this out of context.  It is the default value for 
envir which is parent.frame() (and was sys.frame(sys.parent()), the same 
thing).  Default and explicit arguments are not evaluated in the same 
frames, and so giving sys.frame(sys.parent()) as an explicit argument is 
different from taking the default.

Once you remember the last sentence, there is no mystery.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramos.beatriz at gmail.com  Fri Nov 18 17:12:10 2005
From: ramos.beatriz at gmail.com (Beatriz)
Date: Fri, 18 Nov 2005 17:12:10 +0100
Subject: [R] How to run R in batch mode
In-Reply-To: <Pine.OSF.4.58.0511180923570.316673@wotan.mdacc.tmc.edu>
References: <437DF13E.4090502@gmail.com>
	<Pine.OSF.4.58.0511180923570.316673@wotan.mdacc.tmc.edu>
Message-ID: <437DFD5A.6040800@gmail.com>

I've only wirtten
library(limma)
in my "test.txt" file and I've tried with test.R and test.r too

I' don't know the problem

Thank you


Paul Roebuck wrote:

>On Fri, 18 Nov 2005, Beatriz wrote:
>
>  
>
>>I want to run R in batch mode but it doen't work (Error: syntax error)
>>
>>I've found this in R help:
>>R CMD BATCH [options] infile [outfile]
>>
>>I have tried differents commands:
>>(I have been working in the same directory I have "test.txt" file and
>>"test2.txt" would be the output file)
>>
>> > R CMD BATCH test.txt
>>
>>[SNIP additional tries]
>>
>>I don't know what is wrong. Could you help me, please?
>>    
>>
>
>Isn't it possible that it is working just fine and you really
>do have a syntax error in your R source file (better named
>as 'test.R') instead?
>
>----------------------------------------------------------
>SIGSIG -- signature too long (core dumped)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From marcelodamasceno at gmail.com  Fri Nov 18 17:27:32 2005
From: marcelodamasceno at gmail.com (Marcelo Damasceno)
Date: Fri, 18 Nov 2005 14:27:32 -0200
Subject: [R] Problems with tkentry
Message-ID: <a55593730511180827i79cdada2xfc72c218623951d2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/c2bb1d05/attachment.pl

From roebuck at mdanderson.org  Fri Nov 18 17:40:05 2005
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Fri, 18 Nov 2005 10:40:05 -0600 (CST)
Subject: [R] How to run R in batch mode
In-Reply-To: <437DFD5A.6040800@gmail.com>
References: <437DF13E.4090502@gmail.com>
	<Pine.OSF.4.58.0511180923570.316673@wotan.mdacc.tmc.edu>
	<437DFD5A.6040800@gmail.com>
Message-ID: <Pine.OSF.4.58.0511181031080.316673@wotan.mdacc.tmc.edu>

On Fri, 18 Nov 2005, Beatriz wrote:

> Paul Roebuck wrote:
>
> >On Fri, 18 Nov 2005, Beatriz wrote:
> >
> >>I want to run R in batch mode but it doen't work (Error: syntax error)
> >>
> >>[SNIP]
> >>
> >>I don't know what is wrong. Could you help me, please?
> >
> >Isn't it possible that it is working just fine and you really
> >do have a syntax error in your R source file (better named
> >as 'test.R') instead?
>
> I've only wirtten
> library(limma)
> in my "test.txt" file and I've tried with test.R and test.r too

Try the following:

$ cat > hello.R
cat("hello, world", "\n")
<Ctrl-D>
$ R CMD BATCH hello.R hello.Rout
$ cat hello.Rout

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From Christoph.Scherber at uni-jena.de  Fri Nov 18 17:44:22 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 18 Nov 2005 17:44:22 +0100
Subject: [R] Calculation of the median in survfit()
Message-ID: <437E04E6.9090402@uni-jena.de>

Dear R users,

Can anyone tell me how the medians in survfit() are computed? I??ve 
looked it up in the source code (print.survfit.s version 4.19 07/09/00), 
but I??m not a programmer...;-)

Especially, I??d like to know what the pfun() function inside 
print.survfit.s() works.

The help file does describe the calculation of the median, but I??d like 
to know if and how this differs from just directly calculating the 
median of the survival times (e.g. using tapply(time,factor,median)).

I would be most grateful for any help. Thanks a lot!
Christoph

###
[I am using R 2.1.1 on WinXP with the latest version of the survival 
package]



From ligges at statistik.uni-dortmund.de  Fri Nov 18 17:45:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 18 Nov 2005 17:45:40 +0100
Subject: [R] Method for $
In-Reply-To: <20051118140018.M46734@tfh-berlin.de>
References: <20051118140018.M46734@tfh-berlin.de>
Message-ID: <437E0534.6070101@statistik.uni-dortmund.de>

Ulrike Gr??mping wrote:

> Dear R experts,
> 
> I have defined a class "myclass" and would like the slots to be extractable 
> not only by "@" but also by "$". I now try to write a method for "$" that 
> simply executes the request object at slotname, whenever someone calls 
> object$slotname for any object of class "myclass".
> I don't manage to find out how I can provide this function with "slotname", 
> so that one and the same function works for any arbitrary slotname a user 
> might choose.
> 
> I have tried
> 
> setMethod("$", signature(x="myclass"), function(x,slotname){ 
>  x at slotname 
>  } 
> )


Ulrike,

what about (untested!):
slot(x, slotname)

Best wishes from Dortmund and Lena,
Uwe Ligges


> This produced the error message: 
> In method for function "$": expanding the signature to 
> include omitted arguments in definition: name = "missing" 
> Error in rematchDefinition(definition, fdef, mnames, fnames, signature) : 
>         methods can add arguments to the generic only if '...' is an argument 
> to the generic
> 
> My searches for a solution of this problem have not been successful. Can 
> someone help? 
> 
> Thanks and regards,
> Ulrike Gr??mping, Berlin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From apluzhni at bsd.uchicago.edu  Fri Nov 18 18:03:17 2005
From: apluzhni at bsd.uchicago.edu (Anna Pluzhnikov)
Date: Fri, 18 Nov 2005 11:03:17 -0600
Subject: [R] (no subject)
Message-ID: <1132333397.437e0955079b3@netmail.bsd.uchicago.edu>

Hi,
I need to run a Fisher's exact test on thousands of 2x2 contingency tables, and
repeat this process several thousand times (this is a part of the permutation
test for a genome-wide association study).

How can I run this process most efficiently? Is there any way to optimize R code?
 
I have my data in a 2x2xN array (N ~ 5 K; eventually N will be ~ 500 K), and use
apply inside the loop:
> for (iter in 1:1000) {
    apply(data,3,fisherPval)
  }
  fisherPval <- function(x) {
     fisher.test(x)$p.value
  }
Right now, it takes about 30 sec per iteration on an Intel Xeon 3.06GHz processor.

Thanks in advance. 

-- 
Anna Pluzhnikov, PhD
Section of Genetic Medicine
Department of Medicine
The University of Chicago






-------------------------------------------------
This email is intended only for the use of the individual or entity to which
it is addressed and may contain information that is privileged and
confidential.  If the reader of this email message is not the intended
recipient, you are hereby notified that any dissemination, distribution, or
copying of this communication is prohibited.  If you have received this email
in error, please notify the sender and destroy/delete all copies of the
transmittal.  Thank you.



From perjmi at gmail.com  Fri Nov 18 18:06:34 2005
From: perjmi at gmail.com (Per Jensen)
Date: Fri, 18 Nov 2005 18:06:34 +0100
Subject: [R] truncated observations in survreg
Message-ID: <8c0ddffa0511180906o5b1c4df3vf078a03402977a07@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/c844f143/attachment.pl

From murdoch at stats.uwo.ca  Fri Nov 18 18:11:54 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 18 Nov 2005 12:11:54 -0500
Subject: [R] How to run R in batch mode
In-Reply-To: <437E0096.4060700@gmail.com>
References: <437DF13E.4090502@gmail.com> <437DF60B.1040802@stats.uwo.ca>
	<437E0096.4060700@gmail.com>
Message-ID: <437E0B5A.1030003@stats.uwo.ca>

On 11/18/2005 11:25 AM, Beatriz wrote:
> I write
> R CMD BATCH test.R
> in my R console
> 
> I have send you an image (RunBatch.jpg) of my console and the "test.R" file

I don't think the jpg made it to R-help, but I saw it.  You tried to run 
R CMD BATCH test.R from within R.  That's meant to be a system command.

Since you're running in Windows, you should open a command shell.  One 
way to do that is to choose "Run..." from the Start Menu, and enter 
"cmd".  You'll get a black command shell window.

If your path is set properly so that you can run R from there, then
R CMD BATCH test.R should work.

There is a way to do this from within R:  run

 > system("R CMD BATCH test.R",intern=TRUE)

This still depends on your path being set correctly to find R.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Fri Nov 18 18:20:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Nov 2005 17:20:54 +0000 (GMT)
Subject: [R] Millions of calls to fisher.test (was (no subject))
In-Reply-To: <1132333397.437e0955079b3@netmail.bsd.uchicago.edu>
References: <1132333397.437e0955079b3@netmail.bsd.uchicago.edu>
Message-ID: <Pine.LNX.4.61.0511181714340.7408@gannet.stats>

Setting conf.int=FALSE will help.  Looking at the code of fisher.test and 
extracting just the bit you need will help more.

Do you actually need a two-sided test?  Fisher did not, and if not, the 
computations can be reduced to a call to phyper which is vectorized.

On Fri, 18 Nov 2005, Anna Pluzhnikov wrote:

> Hi,
> I need to run a Fisher's exact test on thousands of 2x2 contingency tables, and
> repeat this process several thousand times (this is a part of the permutation
> test for a genome-wide association study).
>
> How can I run this process most efficiently? Is there any way to optimize R code?
>
> I have my data in a 2x2xN array (N ~ 5 K; eventually N will be ~ 500 K), and use
> apply inside the loop:
>> for (iter in 1:1000) {
>    apply(data,3,fisherPval)
>  }

Why are you calling the same thing 1000 times?

>  fisherPval <- function(x) {
>     fisher.test(x)$p.value
>  }
> Right now, it takes about 30 sec per iteration on an Intel Xeon 3.06GHz processor.

[Disclaimer etc removed]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do, and use a meaningful subject line.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Fri Nov 18 18:29:27 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 18 Nov 2005 18:29:27 +0100
Subject: [R] Method for $
In-Reply-To: <20051118140018.M46734@tfh-berlin.de>
References: <20051118140018.M46734@tfh-berlin.de>
Message-ID: <17278.3959.236126.268314@stat.math.ethz.ch>

>>>>> "Ulrike" == Ulrike Gr??mping <groemping at tfh-berlin.de>
>>>>>     on Fri, 18 Nov 2005 15:37:58 +0100 writes:

    Ulrike> Dear R experts,

    Ulrike> I have defined a class "myclass" and would like the
    Ulrike> slots to be extractable not only by "@" but also by "$".

hmm, I know this is not what you've asked for, but
why would you want to do that?  To me, it seems a pretty harmful idea
that only leads to confusion and does not provide sensible
functionality.

When S4 classes where created, using '@' instead of '@' with a
*different* meaning was very much on purpose and I think it's a
bit too ambitious to try being smarter than John Chambers,
the creator of the S4 object system, on this.

Martin Maechler, ETH Zurich

    Ulrike> I now try to write a method for "$" that 
    Ulrike> simply executes the request object at slotname, whenever someone calls 
    Ulrike> object$slotname for any object of class "myclass".
    Ulrike> ..............................
    Ulrike> ..............................



From p.dalgaard at biostat.ku.dk  Fri Nov 18 18:33:24 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Nov 2005 18:33:24 +0100
Subject: [R] (no subject)
In-Reply-To: <1132333397.437e0955079b3@netmail.bsd.uchicago.edu>
References: <1132333397.437e0955079b3@netmail.bsd.uchicago.edu>
Message-ID: <x2psoxlv2z.fsf@viggo.kubism.ku.dk>

Anna Pluzhnikov <apluzhni at bsd.uchicago.edu> writes:

> Hi,
> I need to run a Fisher's exact test on thousands of 2x2 contingency tables, and
> repeat this process several thousand times (this is a part of the permutation
> test for a genome-wide association study).
> 
> How can I run this process most efficiently? Is there any way to optimize R code?
>  
> I have my data in a 2x2xN array (N ~ 5 K; eventually N will be ~ 500 K), and use
> apply inside the loop:
> > for (iter in 1:1000) {
>     apply(data,3,fisherPval)
>   }
>   fisherPval <- function(x) {
>      fisher.test(x)$p.value
>   }
> Right now, it takes about 30 sec per iteration on an Intel Xeon 3.06GHz processor.
> 
> Thanks in advance. 

The appropriate application of phyper() should save you quite a bit,
especially if you're pragmatic and just use the two one-sided tests
rather than the two-sided one which is a bit harder to compute.
(Notice that phyper() is vectorized over all its arguments).

As in:

> M <- array(rpois(2*2*5000,lambda=20),c(2,2,500000))
> x <- M[1,1,]
> m <- M[1,1,]+M[2,1,]
> n <- M[1,2,]+M[2,2,]
> k <- M[1,1,]+M[1,2,]
> system.time(pleft<-phyper(x,m,n,k))
[1] 2.16 0.01 2.16 0.00 0.00
> sum(pleft < 0.05)
[1] 16400
> sum(pleft < 0.05)/500000
[1] 0.0328




-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From groemping at tfh-berlin.de  Fri Nov 18 18:35:23 2005
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Fri, 18 Nov 2005 18:35:23 +0100
Subject: [R] Method for $
In-Reply-To: <437E0534.6070101@statistik.uni-dortmund.de>
References: <20051118140018.M46734@tfh-berlin.de>
	<437E0534.6070101@statistik.uni-dortmund.de>
Message-ID: <20051118172748.M31016@tfh-berlin.de>

Uwe,

thank thank you very much for your suggestion. This does in fact work, if I 
define

"$.myclass"<-function(x,slotname){slot(x,slotname)}

which solves my immediate need.

However, the setMethod version
setMethod("$", signature(x="myclass"), function(x,slotname){ 
  slot(x,slotname) 
  } 
 ) 
still produces the same error message as before (cf. below). If anyone has a 
solution for this, I would appreciate it.

Uwe, my best wishes also to Dortmund and Lena, and by the way, it's your book 
that got me started with understanding (part of) the programming aspects of R 
relatively fast.

Regards,
Ulrike Gr??mping

---------- Original Message ----------- 
From: Uwe Ligges <ligges at statistik.uni-dortmund.de> 
To: Ulrike Gr??mping <groemp at tfh-berlin.de> 
Cc: r-help at stat.math.ethz.ch 
Sent: Fri, 18 Nov 2005 17:45:40 +0100 
Subject: Re: [R] Method for $

> Ulrike Gr??mping wrote: 
> 
> > Dear R experts, 
> > 
> > I have defined a class "myclass" and would like the slots to be 
extractable 
> > not only by "@" but also by "$". I now try to write a method for "$" that 
> > simply executes the request object at slotname, whenever someone calls 
> > object$slotname for any object of class "myclass". 
> > I don't manage to find out how I can provide this function 
with "slotname", 
> > so that one and the same function works for any arbitrary slotname a user 
> > might choose. 
> > 
> > I have tried 
> > 
> > setMethod("$", signature(x="myclass"), function(x,slotname){ 
> > ??x at slotname 
> > ??} 
> > ) 
> 
> Ulrike, 
> 
> what about (untested!): 
> slot(x, slotname) 
> 
> Best wishes from Dortmund and Lena, 
> Uwe Ligges 
> 
> > This produced the error message: 
> > In method for function "$": expanding the signature to 
> > include omitted arguments in definition: name = "missing" 
> > Error in rematchDefinition(definition, fdef, mnames, fnames, signature) : 
> > ?? ?? ?? ?? methods can add arguments to the generic only if '...' is an 
argument 
> > to the generic 
> > 
> > My searches for a solution of this problem have not been successful. Can 
> > someone help? 
> > 
> > Thanks and regards, 
> > Ulrike Gr??mping, Berlin 
> > 
> > ______________________________________________ 
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html 
------- End of Original Message -------



From gunter.berton at gene.com  Fri Nov 18 18:42:12 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 18 Nov 2005 09:42:12 -0800
Subject: [R] Method for $
In-Reply-To: <437E0534.6070101@statistik.uni-dortmund.de>
Message-ID: <200511181742.jAIHgC0R018546@faraday.gene.com>

I believe that a  recommended S4 convention is simply to write methods for
slot extraction with the same name as the slot. Thus, for an object,x of
class 'myclass'

slotname(x) 

would give x at slotname. This is a more natural interface than either @ or $
(and I am not sure that methods for $ can be defined due to S3/S4 class
consistency details). 

Obviously, it is trivial, but a bit tedious, to manually write such methods
for any class you care to. However, I wrote a small generator function that
automatically generates the slot extraction methods for any class if you
wish to do things this way. I just run this right after I've define a new
class,'myclass', to generate slot access methods by: slotget('myclass') 

The code is below. I would appreciate feedback on any errors or suggestions
for improvement.

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA



slotGet<-function(x,where=topenv())
#automatically generates accessor and replacement methods for class x
{
	if(!isClass(x,where=where))stop(paste('Class',x,'not found'))
	slots<-slotNames(getClass(x,where=where))
    for(slt in slots){
    		## slot accessor
        fun<-function(object)NULL
		if(!isGeneric(slt,where=where)){
			if(exists(slt,where=where,mode="function"))
				fun<-get(slt,pos=where,mode="function")
			else body(fun)<-substitute(standardGeneric(slt))
			setGeneric(slt,fun,where=where)
		}
		if(!existsMethod(slt,x,where=where)){
		  body(fun)<-substitute(object at slt)
		  setMethod(slt,x,fun,where=where)
        }
	}
	invisible()
}
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
> Sent: Friday, November 18, 2005 8:46 AM
> To: Ulrike Gr??mping
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Method for $
> 
> Ulrike Gr??mping wrote:
> 
> > Dear R experts,
> > 
> > I have defined a class "myclass" and would like the slots 
> to be extractable 
> > not only by "@" but also by "$". I now try to write a 
> method for "$" that 
> > simply executes the request object at slotname, whenever someone calls 
> > object$slotname for any object of class "myclass".
> > I don't manage to find out how I can provide this function 
> with "slotname", 
> > so that one and the same function works for any arbitrary 
> slotname a user 
> > might choose.
> > 
> > I have tried
> > 
> > setMethod("$", signature(x="myclass"), function(x,slotname){ 
> >  x at slotname 
> >  } 
> > )
> 
> 
> Ulrike,
> 
> what about (untested!):
> slot(x, slotname)
> 
> Best wishes from Dortmund and Lena,
> Uwe Ligges
> 
> 
> > This produced the error message: 
> > In method for function "$": expanding the signature to 
> > include omitted arguments in definition: name = "missing" 
> > Error in rematchDefinition(definition, fdef, mnames, 
> fnames, signature) : 
> >         methods can add arguments to the generic only if 
> '...' is an argument 
> > to the generic
> > 
> > My searches for a solution of this problem have not been 
> successful. Can 
> > someone help? 
> > 
> > Thanks and regards,
> > Ulrike Gr??mping, Berlin
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From groemping at tfh-berlin.de  Fri Nov 18 18:47:52 2005
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Fri, 18 Nov 2005 18:47:52 +0100
Subject: [R] Method for $
In-Reply-To: <17278.3959.236126.268314@stat.math.ethz.ch>
References: <20051118140018.M46734@tfh-berlin.de>
	<17278.3959.236126.268314@stat.math.ethz.ch>
Message-ID: <20051118173744.M50055@tfh-berlin.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/c66756d1/attachment.pl

From lizzylaws at yahoo.com  Fri Nov 18 19:06:15 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Fri, 18 Nov 2005 10:06:15 -0800 (PST)
Subject: [R] Goodness fit test HELP!
In-Reply-To: <4e6115a50511170944g4401e7c1wb401ccd64607fa83@mail.gmail.com>
Message-ID: <20051118180615.8108.qmail@web32112.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/0059bd74/attachment.pl

From duncan at wald.ucdavis.edu  Fri Nov 18 19:26:51 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Fri, 18 Nov 2005 10:26:51 -0800
Subject: [R] Method for $
In-Reply-To: <20051118172748.M31016@tfh-berlin.de>
References: <20051118140018.M46734@tfh-berlin.de>	<437E0534.6070101@statistik.uni-dortmund.de>
	<20051118172748.M31016@tfh-berlin.de>
Message-ID: <437E1CEB.70900@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



Ulrike Gr??mping wrote:
> Uwe,
> 
> thank thank you very much for your suggestion. This does in fact work, if I 
> define
> 
> "$.myclass"<-function(x,slotname){slot(x,slotname)}
> 
> which solves my immediate need.
> 
> However, the setMethod version
> setMethod("$", signature(x="myclass"), function(x,slotname){ 
>   slot(x,slotname) 
>   } 
>  ) 
> still produces the same error message as before (cf. below). If anyone has a 
> solution for this, I would appreciate it.
> 

For the record, the problem you are experiencing is that
you are not using the same argument names in your
method as the function "$" uses. Hence there is a mismatch.
Use
 setMethod("$", signature(x="myclass"), function(x, name){
   slot(x, name)
 } )

i.e. rather than slotname as the second argument, use name
as that is what $ uses.

But, as Martin pointed out, this is not an advisable thing
to do.
And if you want to use an S4 class to house sub-elements,
either explicitly specify their types with
  setClass('myclass', representation( a = "integer",
                                      b = "matrix", ....)

or if you want them to be arbitary objects, then think
about why you are using S4.

> Uwe, my best wishes also to Dortmund and Lena, and by the way, it's your book 
> that got me started with understanding (part of) the programming aspects of R 
> relatively fast.
> 
> Regards,
> Ulrike Gr??mping
> 
> ---------- Original Message ----------- 
> From: Uwe Ligges <ligges at statistik.uni-dortmund.de> 
> To: Ulrike Gr??mping <groemp at tfh-berlin.de> 
> Cc: r-help at stat.math.ethz.ch 
> Sent: Fri, 18 Nov 2005 17:45:40 +0100 
> Subject: Re: [R] Method for $
> 
> 
>>Ulrike Gr??mping wrote: 
>>
>>
>>>Dear R experts, 
>>>
>>>I have defined a class "myclass" and would like the slots to be 
> 
> extractable 
> 
>>>not only by "@" but also by "$". I now try to write a method for "$" that 
>>>simply executes the request object at slotname, whenever someone calls 
>>>object$slotname for any object of class "myclass". 
>>>I don't manage to find out how I can provide this function 
> 
> with "slotname", 
> 
>>>so that one and the same function works for any arbitrary slotname a user 
>>>might choose. 
>>>
>>>I have tried 
>>>
>>>setMethod("$", signature(x="myclass"), function(x,slotname){ 
>>> x at slotname 
>>> } 
>>>) 
>>
>>Ulrike, 
>>
>>what about (untested!): 
>>slot(x, slotname) 
>>
>>Best wishes from Dortmund and Lena, 
>>Uwe Ligges 
>>
>>
>>>This produced the error message: 
>>>In method for function "$": expanding the signature to 
>>>include omitted arguments in definition: name = "missing" 
>>>Error in rematchDefinition(definition, fdef, mnames, fnames, signature) : 
>>>        methods can add arguments to the generic only if '...' is an 
> 
> argument 
> 
>>>to the generic 
>>>
>>>My searches for a solution of this problem have not been successful. Can 
>>>someone help? 
>>>
>>>Thanks and regards, 
>>>Ulrike Gr??mping, Berlin 
>>>
>>>______________________________________________ 
>>>R-help at stat.math.ethz.ch mailing list 
>>>https://stat.ethz.ch/mailman/listinfo/r-help 
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-
> 
> guide.html 
> ------- End of Original Message -------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

- --
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (Darwin)
Comment: Using GnuPG with Thunderbird - http://enigmail.mozdev.org

iD8DBQFDfhzr9p/Jzwa2QP4RAnoPAKCAeNsDpbAC52kfJnmaBdLeGPUTgwCfTqBF
Ffg0eMA//XcfzY3kjBqGJ8c=
=iO7y
-----END PGP SIGNATURE-----



From murdoch at stats.uwo.ca  Fri Nov 18 19:49:36 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 18 Nov 2005 13:49:36 -0500
Subject: [R] SWeave - can I see output in the source?
Message-ID: <437E2240.60304@stats.uwo.ca>

I'm working on a Latex document with lots of R code in it, so naturally 
enough it would be a good idea to use SWeave.  But then I don't get to 
see the output as I'm editing.

Or do I?  Is there a tool to process a .Rnw file and incorporate the 
output from the commands into it (in a form that is not used for 
producing the output .tex file, but which is updated each time I process 
the file)?

Duncan Murdoch



From groemping at tfh-berlin.de  Fri Nov 18 20:05:11 2005
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Fri, 18 Nov 2005 20:05:11 +0100
Subject: [R] Method for $
In-Reply-To: <437E1CEB.70900@wald.ucdavis.edu>
References: <20051118140018.M46734@tfh-berlin.de>	<437E0534.6070101@statistik.uni-dortmund.de>
	<20051118172748.M31016@tfh-berlin.de>
	<437E1CEB.70900@wald.ucdavis.edu>
Message-ID: <20051118185933.M87397@tfh-berlin.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/05a7bbea/attachment.pl

From groemping at tfh-berlin.de  Fri Nov 18 20:07:56 2005
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Fri, 18 Nov 2005 20:07:56 +0100
Subject: [R] Method for $
In-Reply-To: <200511181742.jAIHgC0R018546@faraday.gene.com>
References: <437E0534.6070101@statistik.uni-dortmund.de>
	<200511181742.jAIHgC0R018546@faraday.gene.com>
Message-ID: <20051118190541.M36311@tfh-berlin.de>

Berton, thanks for your code, it does work on my problem. However, I do want 
the $ do work the extraction, and it does if I do it right (see answer from 
Duncan Temple Lang).
Regards, Ulrike

---------- Original Message ----------- 
From: Berton Gunter <gunter.berton at gene.com> 
To: "'Uwe Ligges'" <ligges at statistik.uni-dortmund.de>, "'Ulrike Gr??mping'" 
<groemp at tfh-berlin.de> 
Cc: <r-help at stat.math.ethz.ch> 
Sent: Fri, 18 Nov 2005 09:42:12 -0800 
Subject: RE: [R] Method for $

> I believe that a ??recommended S4 convention is simply to write methods for 
> slot extraction with the same name as the slot. Thus, for an object,x of 
> class 'myclass' 
> 
> slotname(x) 
> 
> would give x at slotname. This is a more natural interface than either @ or $ 
> (and I am not sure that methods for $ can be defined due to S3/S4 class 
> consistency details). 
> 
> Obviously, it is trivial, but a bit tedious, to manually write such methods 
> for any class you care to. However, I wrote a small generator function that 
> automatically generates the slot extraction methods for any class if you 
> wish to do things this way. I just run this right after I've define a new 
> class,'myclass', to generate slot access methods by: slotget('myclass') 
> 
> The code is below. I would appreciate feedback on any errors or suggestions 
> for improvement. 
> 
> Cheers, 
> 
> -- Bert Gunter 
> Genentech Non-Clinical Statistics 
> South San Francisco, CA 
> 
> slotGet<-function(x,where=topenv()) 
> #automatically generates accessor and replacement methods for class x 
> { 
> ????????if(!isClass(x,where=where))stop(paste('Class',x,'not found')) 
> ????????slots<-slotNames(getClass(x,where=where)) 
> ?? ??for(slt in slots){ 
> ?? ?? ???????? ????????## slot accessor 
> ?? ?? ?? ??fun<-function(object)NULL 
> ???????? ???????? if(!isGeneric(slt,where=where)){ 
> ???????? ???????? ???????? if(exists(slt,where=where,mode="function")) 
> ???????? ???????? ???????? ?? ??????fun<-get(slt,pos=where,mode="function") 
> ???????? ???????? ????????else body(fun)<-substitute(standardGeneric(slt)) 
> ???????? ???????? ???????? setGeneric(slt,fun,where=where) 
> ???????? ????????} 
> ???????? ???????? if(!existsMethod(slt,x,where=where)){ 
> ???????? ???????? ?? body(fun)<-substitute(object at slt) 
> ???????? ???????? ?? setMethod(slt,x,fun,where=where) 
> ?? ?? ?? ??} 
> ????????} 
> ????????invisible() 
> } 
> 
> > -----Original Message----- 
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges 
> > Sent: Friday, November 18, 2005 8:46 AM 
> > To: Ulrike Gr??mping 
> > Cc: r-help at stat.math.ethz.ch 
> > Subject: Re: [R] Method for $ 
> > 
> > Ulrike Gr??mping wrote: 
> > 
> > > Dear R experts, 
> > > 
> > > I have defined a class "myclass" and would like the slots 
> > to be extractable 
> > > not only by "@" but also by "$". I now try to write a 
> > method for "$" that 
> > > simply executes the request object at slotname, whenever someone calls 
> > > object$slotname for any object of class "myclass". 
> > > I don't manage to find out how I can provide this function 
> > with "slotname", 
> > > so that one and the same function works for any arbitrary 
> > slotname a user 
> > > might choose. 
> > > 
> > > I have tried 
> > > 
> > > setMethod("$", signature(x="myclass"), function(x,slotname){ 
> > > ??x at slotname 
> > > ??} 
> > > ) 
> > 
> > 
> > Ulrike, 
> > 
> > what about (untested!): 
> > slot(x, slotname) 
> > 
> > Best wishes from Dortmund and Lena, 
> > Uwe Ligges 
> > 
> > 
> > > This produced the error message: 
> > > In method for function "$": expanding the signature to 
> > > include omitted arguments in definition: name = "missing" 
> > > Error in rematchDefinition(definition, fdef, mnames, 
> > fnames, signature) : 
> > > ?? ?? ?? ?? methods can add arguments to the generic only if 
> > '...' is an argument 
> > > to the generic 
> > > 
> > > My searches for a solution of this problem have not been 
> > successful. Can 
> > > someone help? 
> > > 
> > > Thanks and regards, 
> > > Ulrike Gr??mping, Berlin 
> > > 
> > > ______________________________________________ 
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help 
> > > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html 
> > 
> > ______________________________________________ 
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html 
> > 
------- End of Original Message -------



From perjmi at gmail.com  Fri Nov 18 20:13:33 2005
From: perjmi at gmail.com (Per Jensen)
Date: Fri, 18 Nov 2005 20:13:33 +0100
Subject: [R] truncated regression in survreg
Message-ID: <8c0ddffa0511181113l3f3f6acar4443bacc926114d3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051118/642722c1/attachment.pl

From pierre-luc.brunelle at polymtl.ca  Fri Nov 18 22:36:11 2005
From: pierre-luc.brunelle at polymtl.ca (Pierre-Luc Brunelle)
Date: Fri, 18 Nov 2005 16:36:11 -0500
Subject: [R] Adding points to wireframe
Message-ID: <437E494B.9090201@polymtl.ca>

Hi,

I am using function wireframe from package lattice to draw a 3D surface. 
I would like to add a few points on the surface. I read in a post from 
Deepayan Sarkar that "To do this in a wireframe plot you would probably 
use the panel function panel.3dscatter". Does someone have an example? 
When calling panel.3dscatter with only x, y and z arguments I get 
"argument "xlim.scaled" is missing, with no default". I do not know what 
value I should give to xlim.scaled.

Ragards,

Pierre-Luc



From wqiu at nmr.mgh.harvard.edu  Fri Nov 18 22:38:48 2005
From: wqiu at nmr.mgh.harvard.edu (Wei Qiu)
Date: Fri, 18 Nov 2005 16:38:48 -0500 (EST)
Subject: [R] How to plot two dataset in one fig?
In-Reply-To: <mailman.6293.1132265839.27011.r-help@stat.math.ethz.ch>
References: <mailman.6293.1132265839.27011.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.63.0511181628370.23310@gate.nmr.mgh.harvard.edu>

Hi all,

I am new in R tool. I would like to plot two dataset in in fig.

Here is what I did for both a and b data sets

jpeg(file="a.jpeg")
dat<-read.table('a', header=F, sep=',') 
dim(dat)
y<-dat[,1]
y<-y[!is.na(y)]
plot(y);lines(lowess(y, f=0.05), col = 
("red"), lwd=5)
dev.off

Two questions: 
1. How I can save this lowess smooth data?
2. Once I have the smooth a and b data, How I can put two smooth data sets 
in one fig even they have different Y ranger value?

Any input will be greatly appreciated.

Lucy



From leaflovesun at yahoo.ca  Fri Nov 18 23:10:55 2005
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Fri, 18 Nov 2005 15:10:55 -0700
Subject: [R] Point pattern to grid
Message-ID: <200511182211.jAIMB6a8005889@hypatia.math.ethz.ch>

Hi Roger,

Thanks again for your kind help.

Yes, I still use the 200K points data applying this program but the good thing is  I found it finished in no time.

The questions again here are:

1)   >try0 <- lapply(split(as(df1, "data.frame"), res), mean)  

When I tried to replace mean to sum, error looks like this:

Error in x at data[, i, drop = FALSE] : undefined columns selected 

2) If I just need to know the number of points in each cells, how can I modify the codes. The codes still a bit beyond me.

Thanks!

Leaf

======= At 2005-11-18, 01:39:05 you wrote: =======

>On Thu, 17 Nov 2005, Leaf Sun wrote:
>
>> Dear all,
>> 
>> I'd like to change a point pattern to a grid of cells and use one of the
>> variables as the output.
>> 
>> e.g.  The point pattern is of a window of (500*500) and several features
>> such as pH, SoilType etc.  I like to divide it into a grid with cell
>> size 5*5, and use the mean of the point values falling inside the cell
>> as the output.
>> 
>> Is there any package in R working with this? Thanks in advance!
>
>This might have been better posted on R-sig-geo. Try this:
>
>library(sp)
>df1 <- data.frame(x=runif(10000,0,500), y=runif(10000,0,500),
>  z=rnorm(10000))
>coordinates(df1) <- c("x", "y")
>summary(df1) # SpatialPointsDataFrame
>grd <- GridTopology(c(2.5,2.5), c(5,5), c(100,100))
>sgrd <- SpatialGrid(grd) #SpatialGrid
>bbox(sgrd)
>res <- overlay(sgrd, df1)
># find which grid cells the points are in
>str(res)
>try0 <- lapply(split(as(df1, "data.frame"), res), mean)
># take means by grid cell - assumes all numeric columns in df1
># (soil type??) - maybe write a custom function to handle non-numeric 
># columns sensibly
>try01 <- vector(mode="list", length=prod(slot(slot(sgrd, "grid"),
>  "cells.dim")))
>nafill <- rep(as.numeric(NA), ncol(as(df1, "data.frame")))
>try01 <- lapply(try01, function(x) nafill)
># make a container to put the means in with the right number of columns
>try01[as.integer(names(try0))] <- try0
># insert means into correct list elements
>try1 <- data.frame(t(data.frame(try01)))
># transpose
>summary(try1)
>sgrd1 <- SpatialGridDataFrame(slot(sgrd, "grid"), try1)
>image(sgrd1, "x")
>image(sgrd1, "y")
>image(sgrd1, "z")
>
>It goes a bit further than the short description of the sp package in the 
>latest R-News, and will most likely be a new method for overlay in sp. If 
>these are your 200K points, it may take a little longer ...
>
>> 
>> Cheers,
>> 
>> Leaf
>> 
>> 
>
>-- 
>Roger Bivand
>Economic Geography Section, Department of Economics, Norwegian School of
>Economics and Business Administration, Helleveien 30, N-5045 Bergen,
>Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
>e-mail: Roger.Bivand at nhh.no
>

= = = = = = = = = = = = = = = = = = = =



From eeide at cs.utah.edu  Fri Nov 18 23:17:48 2005
From: eeide at cs.utah.edu (Eric Eide)
Date: Fri, 18 Nov 2005 15:17:48 -0700
Subject: [R] Histogram over a Large Data Set (DB): How?
In-Reply-To: Sean Davis's message of Thursday,
	November 17 2005 <000701c5ebd3$925ef290$6401a8c0@WATSON>
References: <17277.4441.302347.151426@bas.flux.utah.edu>
	<000701c5ebd3$925ef290$6401a8c0@WATSON>
Message-ID: <17278.21260.466538.184131@bas.flux.utah.edu>

"Sean" == Sean Davis <sdavis2 at mail.nih.gov> writes:

	Sean> Have you tried just grabbing the whole column using dbGetQuery?
	Sean> Try doing this:
	Sean> 
	Sean> spams <- dbGetQuery(con,"select unixtime from email limit
	Sean> 1000000")
	Sean> 
	Sean> Then increase from 1,000,000 to 1.5 million, to 2 million, etc.
	Sean> until you break something (run out of memory), if you do at all.

Yes, you are right.  For the example problem that I posed, R can indeed process
the entire query result in memory.  (The R process grows to 240MB, though!)

	Sean> However, the BETTER way to do this, if you already have the data
	Sean> in the database is to allow the database to do the histogram for
	Sean> you.  For example, to get a count of spams by day, in MySQL do
	Sean> something like: [...]

Yes, again you are right --- the particular problem that I posed is probably
better handled by formulating a more sophisticated SQL query.

But really, my goal isn't to solve the the example problem that I posed ---
rather, it is to understand how people use R to process very large data sets.
The research project that I'm working on will eventually need to deal with
query results that cannot fit in main memory, and for which the built-in
statistical facilities of most DBMSs will be insufficient.

Some of my colleagues have previously written their analyses "by hand," using
various scripting languages to read and process records from a DB in chunks.
Writing things in this way, however, can be tedious and error-prone.  Instead
of taking this approach, I would like to be able to use existing statistics
packages that have the ability to deal with large datasets in good ways.

So, I seek to understand the ways that people deal with these sorts of
situations in R.  Your advice is very helpful --- one should solve problems in
the simplest ways available! --- but I would still like to understand the
harder cases, and how one can use "general" R functions in combination with
DBI's `dbApply' and `fetch' interfaces, which divide results into chunks.

Thanks!

Eric.

-- 
-------------------------------------------------------------------------------
Eric Eide <eeide at cs.utah.edu>  .         University of Utah School of Computing
http://www.cs.utah.edu/~eeide/ . +1 (801) 585-5512 voice, +1 (801) 581-5843 FAX



From gerifalte28 at hotmail.com  Fri Nov 18 23:28:38 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 18 Nov 2005 22:28:38 +0000
Subject: [R] How to plot two dataset in one fig?
In-Reply-To: <Pine.LNX.4.63.0511181628370.23310@gate.nmr.mgh.harvard.edu>
Message-ID: <BAY103-F3E74B39278811EEFC8EFBA65E0@phx.gbl>

If I understand your question, to superimpose two lines in a same plot, in 
the first call to plot() you want to set the plot(ylim) argument with a 
range that will fit both of your lines.  Then use lines() to add the second 
lowess line on the plot.  Or matplot() will automate the process for you.

Take a look at savePlot() to save your final figure.


Francisco

>From: Wei Qiu <wqiu at nmr.mgh.harvard.edu>
>To: r-help at stat.math.ethz.ch
>Subject: [R] How to plot two dataset in one fig?
>Date: Fri, 18 Nov 2005 16:38:48 -0500 (EST)
>
>Hi all,
>
>I am new in R tool. I would like to plot two dataset in in fig.
>
>Here is what I did for both a and b data sets
>
>jpeg(file="a.jpeg")
>dat<-read.table('a', header=F, sep=',')
>dim(dat)
>y<-dat[,1]
>y<-y[!is.na(y)]
>plot(y);lines(lowess(y, f=0.05), col =
>("red"), lwd=5)
>dev.off
>
>Two questions:
>1. How I can save this lowess smooth data?
>2. Once I have the smooth a and b data, How I can put two smooth data sets
>in one fig even they have different Y ranger value?
>
>Any input will be greatly appreciated.
>
>Lucy
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From Roger.Bivand at nhh.no  Fri Nov 18 23:41:51 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 18 Nov 2005 23:41:51 +0100 (CET)
Subject: [R] Point pattern to grid
In-Reply-To: <200511182211.jAIMB6a8005889@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0511182341160.3807-100000@reclus.nhh.no>

On Fri, 18 Nov 2005, Leaf Sun wrote:

> Hi Roger,
> 
> Thanks again for your kind help.
> 
> Yes, I still use the 200K points data applying this program but the good
> thing is I found it finished in no time.

The reply is on R-sig-geo.

> 
> The questions again here are:
> 
> 1)   >try0 <- lapply(split(as(df1, "data.frame"), res), mean)  
> 
> When I tried to replace mean to sum, error looks like this:
> 
> Error in x at data[, i, drop = FALSE] : undefined columns selected 
> 
> 2) If I just need to know the number of points in each cells, how can I modify the codes. The codes still a bit beyond me.
> 
> Thanks!
> 
> Leaf
> 
> ======= At 2005-11-18, 01:39:05 you wrote: =======
> 
> >On Thu, 17 Nov 2005, Leaf Sun wrote:
> >
> >> Dear all,
> >> 
> >> I'd like to change a point pattern to a grid of cells and use one of the
> >> variables as the output.
> >> 
> >> e.g.  The point pattern is of a window of (500*500) and several features
> >> such as pH, SoilType etc.  I like to divide it into a grid with cell
> >> size 5*5, and use the mean of the point values falling inside the cell
> >> as the output.
> >> 
> >> Is there any package in R working with this? Thanks in advance!
> >
> >This might have been better posted on R-sig-geo. Try this:
> >
> >library(sp)
> >df1 <- data.frame(x=runif(10000,0,500), y=runif(10000,0,500),
> >  z=rnorm(10000))
> >coordinates(df1) <- c("x", "y")
> >summary(df1) # SpatialPointsDataFrame
> >grd <- GridTopology(c(2.5,2.5), c(5,5), c(100,100))
> >sgrd <- SpatialGrid(grd) #SpatialGrid
> >bbox(sgrd)
> >res <- overlay(sgrd, df1)
> ># find which grid cells the points are in
> >str(res)
> >try0 <- lapply(split(as(df1, "data.frame"), res), mean)
> ># take means by grid cell - assumes all numeric columns in df1
> ># (soil type??) - maybe write a custom function to handle non-numeric 
> ># columns sensibly
> >try01 <- vector(mode="list", length=prod(slot(slot(sgrd, "grid"),
> >  "cells.dim")))
> >nafill <- rep(as.numeric(NA), ncol(as(df1, "data.frame")))
> >try01 <- lapply(try01, function(x) nafill)
> ># make a container to put the means in with the right number of columns
> >try01[as.integer(names(try0))] <- try0
> ># insert means into correct list elements
> >try1 <- data.frame(t(data.frame(try01)))
> ># transpose
> >summary(try1)
> >sgrd1 <- SpatialGridDataFrame(slot(sgrd, "grid"), try1)
> >image(sgrd1, "x")
> >image(sgrd1, "y")
> >image(sgrd1, "z")
> >
> >It goes a bit further than the short description of the sp package in the 
> >latest R-News, and will most likely be a new method for overlay in sp. If 
> >these are your 200K points, it may take a little longer ...
> >
> >> 
> >> Cheers,
> >> 
> >> Leaf
> >> 
> >> 
> >
> >-- 
> >Roger Bivand
> >Economic Geography Section, Department of Economics, Norwegian School of
> >Economics and Business Administration, Helleveien 30, N-5045 Bergen,
> >Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
> >e-mail: Roger.Bivand at nhh.no
> >
> 
> = = = = = = = = = = = = = = = = = = = =
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From tchur at optushome.com.au  Fri Nov 18 23:58:32 2005
From: tchur at optushome.com.au (Tim Churches)
Date: Sat, 19 Nov 2005 09:58:32 +1100
Subject: [R] Histogram over a Large Data Set (DB): How?
In-Reply-To: <17278.21260.466538.184131@bas.flux.utah.edu>
References: <17277.4441.302347.151426@bas.flux.utah.edu>	<000701c5ebd3$925ef290$6401a8c0@WATSON>
	<17278.21260.466538.184131@bas.flux.utah.edu>
Message-ID: <437E5C98.9000200@optushome.com.au>

Eric Eide wrote:
> "Sean" == Sean Davis <sdavis2 at mail.nih.gov> writes:
> 
> 	Sean> Have you tried just grabbing the whole column using dbGetQuery?
> 	Sean> Try doing this:
> 	Sean> 
> 	Sean> spams <- dbGetQuery(con,"select unixtime from email limit
> 	Sean> 1000000")
> 	Sean> 
> 	Sean> Then increase from 1,000,000 to 1.5 million, to 2 million, etc.
> 	Sean> until you break something (run out of memory), if you do at all.
> 
> Yes, you are right.  For the example problem that I posed, R can indeed process
> the entire query result in memory.  (The R process grows to 240MB, though!)
> 
> 	Sean> However, the BETTER way to do this, if you already have the data
> 	Sean> in the database is to allow the database to do the histogram for
> 	Sean> you.  For example, to get a count of spams by day, in MySQL do
> 	Sean> something like: [...]
> 
> Yes, again you are right --- the particular problem that I posed is probably
> better handled by formulating a more sophisticated SQL query.
> 
> But really, my goal isn't to solve the the example problem that I posed ---
> rather, it is to understand how people use R to process very large data sets.
> The research project that I'm working on will eventually need to deal with
> query results that cannot fit in main memory, and for which the built-in
> statistical facilities of most DBMSs will be insufficient.
> 
> Some of my colleagues have previously written their analyses "by hand," using
> various scripting languages to read and process records from a DB in chunks.
> Writing things in this way, however, can be tedious and error-prone.  Instead
> of taking this approach, I would like to be able to use existing statistics
> packages that have the ability to deal with large datasets in good ways.
> 
> So, I seek to understand the ways that people deal with these sorts of
> situations in R.  Your advice is very helpful --- one should solve problems in
> the simplest ways available! --- but I would still like to understand the
> harder cases, and how one can use "general" R functions in combination with
> DBI's `dbApply' and `fetch' interfaces, which divide results into chunks.

You might be interested in our project: "NetEpi Analysis", which aims to
provide interactive exploratory data analysis and basic epidemiological
analysis via both a Web front end and a Python programmatic API (forgive
the redundancy in "programmatic API") for datests up to around 30
million rows (and as many columns as you like) on 32 bit platforms -
hundreds of millions of rows should be feasible on 64-bit platforms. It
stores data column-wise in memory-mapped on-disc arrays, and uses set
operations on ordinal indexes to permit rapid subsetting and
cross-tabulation of categorical (factored) data. It is written in
Python, but uses R for graphics and some (but not all) statistical
calculations (and for model fitting when we get round to providing
facilities for same).

See http://www.netepi.org - still in alpha, with an update coming out by
December. Although it is aimed at epidemiological analysis (of large
administrative health datasets), I dare say it might be useful for
exploring large databases of spam too.

Tim C



From izmirlig at mail.nih.gov  Sat Nov 19 01:27:21 2005
From: izmirlig at mail.nih.gov (Izmirlian, Grant (NIH/NCI))
Date: Fri, 18 Nov 2005 19:27:21 -0500
Subject: [R] Batchjob creates small object but large workspace ???
Message-ID: <CE0E73903DB53F43B4B0938747F34F8A01242CC9@nihexchange7.nih.gov>

I ran into an interesting problem that I think I have solved. I ran a batch job
as "--no-save", electing only to save all objects there before the job started and
one reasonably small object created as the result of the job, ( ~ 19K ).  During the
course of the job several large objects are generated, but not among the list of
things which are saved.

The interesting problem is that the .RData file ends up being around 250 MB in size
larger than it was previously. Inside of R, "object.size" returns a reasonably
accurate estimate, 19K, but somehow there is hidden junk.

I am trying, as a write, the solution the problem. Before the "save" command at the
bottome of the batch file I should delete the un-needed large objects and then call
gc() . My thought is that the save operation needs to generate a large temporary file
and then copy only the parts of it that are requested to be saved and this takes
nearly all of my 1GB of system memory so that the poor "gc" program is shoved off the
stack (or some semblence of this reasoning at least).

Oh... (not so) great news... the job just finished and it looks like my idea was
incorrect.

So I am open to suggestions!

.RData before run:    88951444 bytes
.RData after run:    345671147 bytes

size of object saved:   190588 bytes

and by the way, I have good reason to trust the 19K estimate since the information
contained in the object fits on about 3 screens.



From patrick at pdrechsler.de  Sat Nov 19 01:37:37 2005
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Sat, 19 Nov 2005 00:37:37 +0000
Subject: [R] [package concord] seeking maintainer
Message-ID: <87irupxyjy.fsf@pdrechsler.de>


Hi,

can anybody tell me how to contact the maintainer of the
"concord" package? The address given in the help file is not
valid anymore.

,----
| > help(package=concord)
| 
| 		Information f?r Paket 'concord'
| 
| Description:
| 
| Package:       concord
| Version:       1.4-2
| Date:          2005-05-10
| Title:         Concordance and reliability
| Author:        Jim Lemon <jim.lemon at uts.edu.au>
| Maintainer:    Jim Lemon <jim.lemon at uts.edu.au>
`----

,----
| <jim.lemon at uts.edu.au>: host 138.25.243.40[138.25.243.40] said: 550 5.1.6
|     recipient no longer on server: jim.lemon at uts.edu.au (in reply to RCPT TO
|     command)
`----

TIA


Regards

Patrick
-- 
The shortest unit of time in the universe is the New York Second,
defined as the period of time between the traffic lights turning green
and the cab behind you honking. -- Terry Pratchett 'Lords and Ladies'



From MSchwartz at mn.rr.com  Sat Nov 19 02:07:09 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 18 Nov 2005 19:07:09 -0600
Subject: [R] [package concord] seeking maintainer
In-Reply-To: <87irupxyjy.fsf@pdrechsler.de>
References: <87irupxyjy.fsf@pdrechsler.de>
Message-ID: <1132362430.4173.2.camel@localhost.localdomain>

On Sat, 2005-11-19 at 00:37 +0000, Patrick Drechsler wrote:
> Hi,
> 
> can anybody tell me how to contact the maintainer of the
> "concord" package? The address given in the help file is not
> valid anymore.
> 
> ,----
> | > help(package=concord)
> | 
> | 		Information fÃ¼r Paket 'concord'
> | 
> | Description:
> | 
> | Package:       concord
> | Version:       1.4-2
> | Date:          2005-05-10
> | Title:         Concordance and reliability
> | Author:        Jim Lemon <jim.lemon at uts.edu.au>
> | Maintainer:    Jim Lemon <jim.lemon at uts.edu.au>
> `----
> 
> ,----
> | <jim.lemon at uts.edu.au>: host 138.25.243.40[138.25.243.40] said: 550 5.1.6
> |     recipient no longer on server: jim.lemon at uts.edu.au (in reply to RCPT TO
> |     command)
> `----
> 
> TIA
> 
> 
> Regards
> 
> Patrick


A quick search of this month's r-help archive reveals:

  bitwrit at ozemail.com.au

HTH,

Marc Schwartz



From patrick at pdrechsler.de  Sat Nov 19 02:24:53 2005
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Sat, 19 Nov 2005 01:24:53 +0000
Subject: [R] [package concord] seeking maintainer
References: <87irupxyjy.fsf@pdrechsler.de>
	<1132362430.4173.2.camel@localhost.localdomain>
Message-ID: <87y83lwhsq.fsf@pdrechsler.de>


Marc Schwartz wrote on 19 Nov 2005 02:07:09 MET:

> On Sat, 2005-11-19 at 00:37 +0000, Patrick Drechsler wrote:
>> 
>> can anybody tell me how to contact the maintainer of the
>> "concord" package? The address given in the help file is not
>> valid anymore.
[...]
> A quick search of this month's r-help archive reveals:
>
>   bitwrit at ozemail.com.au
>
> HTH,

Thanks Marc!

Regards

Patrick
-- 
Photons have mass!!??
I didn't even know they were Catholic...



From basille at biomserv.univ-lyon1.fr  Sat Nov 19 12:08:56 2005
From: basille at biomserv.univ-lyon1.fr (Mathieu Basille)
Date: Sat, 19 Nov 2005 12:08:56 +0100
Subject: [R] question about R graphics-example plot attached
In-Reply-To: <91ae6e350511080508nb688ac7l647051c35ae29db8@mail.gmail.com>
References: <91ae6e350511020703x34dc80dbj2bb9a92b07efc631@mail.gmail.com>
	<91ae6e350511080508nb688ac7l647051c35ae29db8@mail.gmail.com>
Message-ID: <437F07C8.1050701@biomserv.univ-lyon1.fr>

Does 'rug' help you ?

    example(rug)
    ?rug

Cheers
Mathieu.


jia ding a ??crit :

>---------- Forwarded message ----------
>From: jia ding <dingjia at gmail.com>
>Date: Nov 2, 2005 4:03 PM
>Subject: question about R graphics-example plot attached
>To: r-help at lists.r-project.org
>
>Suppose I have the data set like this:
>A 1
>3
>7
>10
>
>B 5
>9
>13
>The numbers here actually is A or B's occurence positions.So it means,
>position 1,3,7,10 is A;position 5,9,13 is B's occurence.
>
>I want the plot is a line with a little dot (or bar) at the position
>1(A-show red), position 3(A-red),position
>5(B-blue),7(A-red),10(A-red),13(B-blue)
>
>I am not sure, If I explained very clearly about my question. What a
>pity google group not support to attach a file. Otherwise,I can provide
>an example.
>
>Thanks for the help!
>Thanks Marc.
> DJ
>  
>

-- 

           Mathieu Basille - Doctorant

.------------------------------------------------.
| Laboratoire de Biom??trie et Biologie Evolutive |
|    Universit?? Claude Bernard Lyon1 - France    |
`------------------------------------------------'
          basille at biomserv.univ-lyon1.fr



From Friedrich.Leisch at tuwien.ac.at  Sat Nov 19 12:13:42 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Sat, 19 Nov 2005 12:13:42 +0100
Subject: [R] SWeave - can I see output in the source?
In-Reply-To: <437E2240.60304@stats.uwo.ca>
References: <437E2240.60304@stats.uwo.ca>
Message-ID: <17279.2278.290839.316330@celebrian.ci.tuwien.ac.at>

>>>>> On Fri, 18 Nov 2005 13:49:36 -0500,
>>>>> Duncan Murdoch (DM) wrote:

  > I'm working on a Latex document with lots of R code in it, so naturally 
  > enough it would be a good idea to use SWeave.  But then I don't get to 
  > see the output as I'm editing.

  > Or do I?  Is there a tool to process a .Rnw file and incorporate the 
  > output from the commands into it (in a form that is not used for 
  > producing the output .tex file, but which is updated each time I process 
  > the file)?

I'm not sure if I understand the question correctly, but if you edit
Sweave files in Emacs using ESS you can send the code lines to a
running R process, and there you see the output. At least that's how I
write my Sweave files.

When I want to see all at once I typically do a tangle & source.

Best,
Fritz



From adi at roda.ro  Sat Nov 19 14:00:25 2005
From: adi at roda.ro (Adrian DUSA)
Date: Sat, 19 Nov 2005 15:00:25 +0200
Subject: [R] help with apply, please
Message-ID: <200511191500.25863.adi@roda.ro>

Dear list,

I have a problem with a toy example:
mtrx <- matrix(c(1,1,0,1,1,1,0,1,1,0,0,1), nrow=3)
rownames(ma) <- letters[1:3]

I would like to determine which is the minimum combination of rows that 
"covers" all columns with at least a 1.
None of the rows covers all columns; all three rows clearly covers all 
columns, but there are simpler combinations (1st and the 3rd, or 2nd and 3rd) 
which also covers all columns.

I solved this problem by creating a second logical matrix which contains all 
possible combinations of rows:
tt <- matrix(as.logical(c(1,0,0,0,1,0,0,0,1,1,1,0,1,0,1,0,1,1,1,1,1)), nrow=3)

and then subset the first matrix and check if all columns are covered.
This solution, though, is highly inneficient and I am certain that a 
combination of apply or something will do.

###########################

possibles <- NULL
length.possibles <- NULL
## I guess the minimum solution is has half the number of rows
guesstimate <- floor(nrow(tt)/2) + nrow(tt) %% 2
checked <- logical(nrow(tt))
repeat {
    ifelse(checked[guesstimate], break, checked[guesstimate] <- TRUE)
    partials <- as.matrix(tt[, colSums(tt) == guesstimate])
    layer.solution <- logical(ncol(partials))
    
    for (j in 1:ncol(partials)) {
        if (length(which(colSums(mtrx[partials[, j], ]) > 0)) == ncol(mtrx)) {
            layer.solution[j] <- TRUE
        }
    }
    if (sum(layer.solution) == 0) {
        if (!is.null(possibles)) break
        guesstimate <- guesstimate + 1
    } else {
        for (j in which(layer.solution)) {
            possible.solution <- rownames(mtrx)[partials[, j]]
            possibles[[length(possibles) + 1]] <- possible.solution
            length.possibles <- c(length.possibles, length(possible.solution))
        }
        guesstimate <- guesstimate - 1
    }
}
final.solution <- possibles[which(length.possibles == min(length.possibles))]

###########################

More explicitely (if useful) it is about reducing a prime implicants chart in 
a Quine-McCluskey boolean minimisation algorithm. I tried following the 
original algorithm applying row dominance and column dominance, but (as I am 
not a computer scientist), I am unable to apply it.

If you have a better solution for this, I would be gratefull if you'd share 
it.
Thank you in advance,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From fezzi at stat.unibo.it  Sat Nov 19 15:23:24 2005
From: fezzi at stat.unibo.it (Carlo Fezzi)
Date: Sat, 19 Nov 2005 15:23:24 +0100 (CET)
Subject: [R] cointegration rank
Message-ID: <4884860.1132410204440.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Dear R - helpers,

I am using the urca package to estimate cointegration relations, and I
would be really grateful if somebody could help me with this questions:

After estimating the unrestriced VAR with "ca.jo" I would like to impose
the rank restriction (for example rank = 1) and then obtain the
restricted estimate of PI to be utilized to estimate the VECM model.

Is it possible? 

It seems to me that the function "cajools" estimates the VECM without
the restrictions. Did I miss something? How is it possible to impose
them?

Thanks a lot in advance!

Carlo



From murdoch at stats.uwo.ca  Sat Nov 19 15:52:22 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 19 Nov 2005 09:52:22 -0500
Subject: [R] SWeave - can I see output in the source?
In-Reply-To: <17279.2278.290839.316330@celebrian.ci.tuwien.ac.at>
References: <437E2240.60304@stats.uwo.ca>
	<17279.2278.290839.316330@celebrian.ci.tuwien.ac.at>
Message-ID: <437F3C26.1020604@stats.uwo.ca>

On 11/19/2005 6:13 AM, Friedrich.Leisch at tuwien.ac.at wrote:
>>>>>>On Fri, 18 Nov 2005 13:49:36 -0500,
>>>>>>Duncan Murdoch (DM) wrote:
> 
> 
>   > I'm working on a Latex document with lots of R code in it, so naturally 
>   > enough it would be a good idea to use SWeave.  But then I don't get to 
>   > see the output as I'm editing.
> 
>   > Or do I?  Is there a tool to process a .Rnw file and incorporate the 
>   > output from the commands into it (in a form that is not used for 
>   > producing the output .tex file, but which is updated each time I process 
>   > the file)?
> 
> I'm not sure if I understand the question correctly, but if you edit
> Sweave files in Emacs using ESS you can send the code lines to a
> running R process, and there you see the output. At least that's how I
> write my Sweave files.
> 
> When I want to see all at once I typically do a tangle & source.

That's bad news for me, because I'm allergic to Emacs.  What I had in 
mind was this:  In my .Rnw file, I enter:

<<echo=true>>=
1:3
@

Then I pass it to some tool, which modifies the .Rnw file, changing it 
to something like

<<echo=true>>=
1:3
@
% > 1:3
% [1] 1 2 3

(My editor will notice that the tool has changed the file and offer to 
load the new version at this point.  I think that's a reasonably common 
editor option.)

Then I can see what I'm writing about when I describe the results.  If I 
later come along and edit the source to change it to

<<echo=true>>=
1:4
@
% > 1:3
% [1] 1 2 3

then the next time I run the file through the tool it will delete the 
stale output and modify my file to look like this:

<<echo=true>>=
1:4
@
% > 1:4
% [1] 1 2 3 4

Besides the advantage that I had in mind (being able to see the output 
as I'm editing, and being confident that it will match the output in the 
final document), this will mean that I'll have a versionable record of 
what the output looked like (so I'll be alerted to changes in it caused 
by updates to R or some package I'm using).  I could get this by saving 
the .tex output, but to me this seems preferable.  But I don't have a 
lot of experience with SWeave yet, so maybe there's a better workflow.

Duncan Murdoch



From murdoch at stats.uwo.ca  Sat Nov 19 16:06:14 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 19 Nov 2005 10:06:14 -0500
Subject: [R] help with apply, please
In-Reply-To: <200511191500.25863.adi@roda.ro>
References: <200511191500.25863.adi@roda.ro>
Message-ID: <437F3F66.8080109@stats.uwo.ca>

On 11/19/2005 8:00 AM, Adrian DUSA wrote:
> Dear list,
> 
> I have a problem with a toy example:
> mtrx <- matrix(c(1,1,0,1,1,1,0,1,1,0,0,1), nrow=3)
> rownames(ma) <- letters[1:3]
> 
> I would like to determine which is the minimum combination of rows that 
> "covers" all columns with at least a 1.
> None of the rows covers all columns; all three rows clearly covers all 
> columns, but there are simpler combinations (1st and the 3rd, or 2nd and 3rd) 
> which also covers all columns.
> 
> I solved this problem by creating a second logical matrix which contains all 
> possible combinations of rows:
> tt <- matrix(as.logical(c(1,0,0,0,1,0,0,0,1,1,1,0,1,0,1,0,1,1,1,1,1)), nrow=3)
> 
> and then subset the first matrix and check if all columns are covered.
> This solution, though, is highly inneficient and I am certain that a 
> combination of apply or something will do.

First of all, I imagine there isn't a unique solution, i.e. there are 
probably several subsets that can't be reduced but which are not equal. 
  Do you care if you find the smallest one of those?  If so, it looks 
like a reasonably hard problem.  If not, it's a lot easier:  total all 
of the columns, then see if there is any row whose entries all 
correspond to columns with counts bigger than 1.  Remove it, and continue.

This will find a local minimum in one pass through the rows.

You could make it better by sorting the rows into an order so that rows 
that dominate other rows come later, but I think you still wouldn't be 
guaranteed to find the global min.  (By the way, I'm not sure if we have 
a function that can do this:  i.e., given a partial ordering on the 
rows, sort the matrix so that the resulting order is consistent with it.)

Duncan Murdoch

> 
> ###########################
> 
> possibles <- NULL
> length.possibles <- NULL
> ## I guess the minimum solution is has half the number of rows
> guesstimate <- floor(nrow(tt)/2) + nrow(tt) %% 2
> checked <- logical(nrow(tt))
> repeat {
>     ifelse(checked[guesstimate], break, checked[guesstimate] <- TRUE)
>     partials <- as.matrix(tt[, colSums(tt) == guesstimate])
>     layer.solution <- logical(ncol(partials))
>     
>     for (j in 1:ncol(partials)) {
>         if (length(which(colSums(mtrx[partials[, j], ]) > 0)) == ncol(mtrx)) {
>             layer.solution[j] <- TRUE
>         }
>     }
>     if (sum(layer.solution) == 0) {
>         if (!is.null(possibles)) break
>         guesstimate <- guesstimate + 1
>     } else {
>         for (j in which(layer.solution)) {
>             possible.solution <- rownames(mtrx)[partials[, j]]
>             possibles[[length(possibles) + 1]] <- possible.solution
>             length.possibles <- c(length.possibles, length(possible.solution))
>         }
>         guesstimate <- guesstimate - 1
>     }
> }
> final.solution <- possibles[which(length.possibles == min(length.possibles))]
> 
> ###########################
> 
> More explicitely (if useful) it is about reducing a prime implicants chart in 
> a Quine-McCluskey boolean minimisation algorithm. I tried following the 
> original algorithm applying row dominance and column dominance, but (as I am 
> not a computer scientist), I am unable to apply it.
> 
> If you have a better solution for this, I would be gratefull if you'd share 
> it.
> Thank you in advance,
> Adrian
>



From ggrothendieck at gmail.com  Sat Nov 19 16:14:56 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 19 Nov 2005 10:14:56 -0500
Subject: [R] help with apply, please
In-Reply-To: <200511191500.25863.adi@roda.ro>
References: <200511191500.25863.adi@roda.ro>
Message-ID: <971536df0511190714m28933eacw2606585a6f541dc5@mail.gmail.com>

Try minizing 1'x subject to 1 >= x >= 0 and m'x >= 1 where m is your mtrx
and ' means transpose.  It seems to give an integer solution, 1 0 1,
with linear programming even in the absence of explicit integer
constraints:

library(lpSolve)
lp("min", rep(1,3), rbind(t(mtrx), diag(3)), rep(c(">=", "<="), 4:3),
rep(1,7))$solution



On 11/19/05, Adrian DUSA <adi at roda.ro> wrote:
> Dear list,
>
> I have a problem with a toy example:
> mtrx <- matrix(c(1,1,0,1,1,1,0,1,1,0,0,1), nrow=3)
> rownames(ma) <- letters[1:3]
>
> I would like to determine which is the minimum combination of rows that
> "covers" all columns with at least a 1.
> None of the rows covers all columns; all three rows clearly covers all
> columns, but there are simpler combinations (1st and the 3rd, or 2nd and 3rd)
> which also covers all columns.
>
> I solved this problem by creating a second logical matrix which contains all
> possible combinations of rows:
> tt <- matrix(as.logical(c(1,0,0,0,1,0,0,0,1,1,1,0,1,0,1,0,1,1,1,1,1)), nrow=3)
>
> and then subset the first matrix and check if all columns are covered.
> This solution, though, is highly inneficient and I am certain that a
> combination of apply or something will do.
>
> ###########################
>
> possibles <- NULL
> length.possibles <- NULL
> ## I guess the minimum solution is has half the number of rows
> guesstimate <- floor(nrow(tt)/2) + nrow(tt) %% 2
> checked <- logical(nrow(tt))
> repeat {
>    ifelse(checked[guesstimate], break, checked[guesstimate] <- TRUE)
>    partials <- as.matrix(tt[, colSums(tt) == guesstimate])
>    layer.solution <- logical(ncol(partials))
>
>    for (j in 1:ncol(partials)) {
>        if (length(which(colSums(mtrx[partials[, j], ]) > 0)) == ncol(mtrx)) {
>            layer.solution[j] <- TRUE
>        }
>    }
>    if (sum(layer.solution) == 0) {
>        if (!is.null(possibles)) break
>        guesstimate <- guesstimate + 1
>    } else {
>        for (j in which(layer.solution)) {
>            possible.solution <- rownames(mtrx)[partials[, j]]
>            possibles[[length(possibles) + 1]] <- possible.solution
>            length.possibles <- c(length.possibles, length(possible.solution))
>        }
>        guesstimate <- guesstimate - 1
>    }
> }
> final.solution <- possibles[which(length.possibles == min(length.possibles))]
>
> ###########################
>
> More explicitely (if useful) it is about reducing a prime implicants chart in
> a Quine-McCluskey boolean minimisation algorithm. I tried following the
> original algorithm applying row dominance and column dominance, but (as I am
> not a computer scientist), I am unable to apply it.
>
> If you have a better solution for this, I would be gratefull if you'd share
> it.
> Thank you in advance,
> Adrian
>
> --
> Adrian DUSA
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>          +40 21 3120210 / int.101
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Sat Nov 19 16:17:53 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 19 Nov 2005 10:17:53 -0500
Subject: [R] help with apply, please
In-Reply-To: <971536df0511190714m28933eacw2606585a6f541dc5@mail.gmail.com>
References: <200511191500.25863.adi@roda.ro>
	<971536df0511190714m28933eacw2606585a6f541dc5@mail.gmail.com>
Message-ID: <971536df0511190717x5635a606u596ce2958f75ca74@mail.gmail.com>

On 11/19/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try minizing 1'x subject to 1 >= x >= 0 and m'x >= 1 where m is your mtrx
> and ' means transpose.  It seems to give an integer solution, 1 0 1,
> with linear programming even in the absence of explicit integer
> constraints:
>
> library(lpSolve)
> lp("min", rep(1,3), rbind(t(mtrx), diag(3)), rep(c(">=", "<="), 4:3),
> rep(1,7))$solution

Just one after thought.  The constraint x <= 1 will not be active so,
eliminating it, the above can be reduced to:

lp("min", rep(1,3), rbind(t(mtrx)), rep(">=", 4), rep(1,4))$solution


>
>
>
> On 11/19/05, Adrian DUSA <adi at roda.ro> wrote:
> > Dear list,
> >
> > I have a problem with a toy example:
> > mtrx <- matrix(c(1,1,0,1,1,1,0,1,1,0,0,1), nrow=3)
> > rownames(ma) <- letters[1:3]
> >
> > I would like to determine which is the minimum combination of rows that
> > "covers" all columns with at least a 1.
> > None of the rows covers all columns; all three rows clearly covers all
> > columns, but there are simpler combinations (1st and the 3rd, or 2nd and 3rd)
> > which also covers all columns.
> >
> > I solved this problem by creating a second logical matrix which contains all
> > possible combinations of rows:
> > tt <- matrix(as.logical(c(1,0,0,0,1,0,0,0,1,1,1,0,1,0,1,0,1,1,1,1,1)), nrow=3)
> >
> > and then subset the first matrix and check if all columns are covered.
> > This solution, though, is highly inneficient and I am certain that a
> > combination of apply or something will do.
> >
> > ###########################
> >
> > possibles <- NULL
> > length.possibles <- NULL
> > ## I guess the minimum solution is has half the number of rows
> > guesstimate <- floor(nrow(tt)/2) + nrow(tt) %% 2
> > checked <- logical(nrow(tt))
> > repeat {
> >    ifelse(checked[guesstimate], break, checked[guesstimate] <- TRUE)
> >    partials <- as.matrix(tt[, colSums(tt) == guesstimate])
> >    layer.solution <- logical(ncol(partials))
> >
> >    for (j in 1:ncol(partials)) {
> >        if (length(which(colSums(mtrx[partials[, j], ]) > 0)) == ncol(mtrx)) {
> >            layer.solution[j] <- TRUE
> >        }
> >    }
> >    if (sum(layer.solution) == 0) {
> >        if (!is.null(possibles)) break
> >        guesstimate <- guesstimate + 1
> >    } else {
> >        for (j in which(layer.solution)) {
> >            possible.solution <- rownames(mtrx)[partials[, j]]
> >            possibles[[length(possibles) + 1]] <- possible.solution
> >            length.possibles <- c(length.possibles, length(possible.solution))
> >        }
> >        guesstimate <- guesstimate - 1
> >    }
> > }
> > final.solution <- possibles[which(length.possibles == min(length.possibles))]
> >
> > ###########################
> >
> > More explicitely (if useful) it is about reducing a prime implicants chart in
> > a Quine-McCluskey boolean minimisation algorithm. I tried following the
> > original algorithm applying row dominance and column dominance, but (as I am
> > not a computer scientist), I am unable to apply it.
> >
> > If you have a better solution for this, I would be gratefull if you'd share
> > it.
> > Thank you in advance,
> > Adrian
> >
> > --
> > Adrian DUSA
> > Romanian Social Data Archive
> > 1, Schitu Magureanu Bd
> > 050025 Bucharest sector 5
> > Romania
> > Tel./Fax: +40 21 3126618 \
> >          +40 21 3120210 / int.101
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From ggrothendieck at gmail.com  Sat Nov 19 16:24:54 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 19 Nov 2005 10:24:54 -0500
Subject: [R] help with apply, please
In-Reply-To: <971536df0511190717x5635a606u596ce2958f75ca74@mail.gmail.com>
References: <200511191500.25863.adi@roda.ro>
	<971536df0511190714m28933eacw2606585a6f541dc5@mail.gmail.com>
	<971536df0511190717x5635a606u596ce2958f75ca74@mail.gmail.com>
Message-ID: <971536df0511190724k3c32a6b3of5a1b10f567ee8b4@mail.gmail.com>

On 11/19/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 11/19/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Try minizing 1'x subject to 1 >= x >= 0 and m'x >= 1 where m is your mtrx
> > and ' means transpose.  It seems to give an integer solution, 1 0 1,
> > with linear programming even in the absence of explicit integer
> > constraints:
> >
> > library(lpSolve)
> > lp("min", rep(1,3), rbind(t(mtrx), diag(3)), rep(c(">=", "<="), 4:3),
> > rep(1,7))$solution
>
> Just one after thought.  The constraint x <= 1 will not be active so,
> eliminating it, the above can be reduced to:
>
> lp("min", rep(1,3), rbind(t(mtrx)), rep(">=", 4), rep(1,4))$solution

Although the above is not wrong I should have removed the rbind
which is no longer needed and simplifying it further, as it seems
that lp will do the rep for you itself for certain arguments, gives:

lp("min", rep(1,3), t(mtrx), ">=", 1)$solution  # 1 0 1


>
>
> >
> >
> >
> > On 11/19/05, Adrian DUSA <adi at roda.ro> wrote:
> > > Dear list,
> > >
> > > I have a problem with a toy example:
> > > mtrx <- matrix(c(1,1,0,1,1,1,0,1,1,0,0,1), nrow=3)
> > > rownames(ma) <- letters[1:3]
> > >
> > > I would like to determine which is the minimum combination of rows that
> > > "covers" all columns with at least a 1.
> > > None of the rows covers all columns; all three rows clearly covers all
> > > columns, but there are simpler combinations (1st and the 3rd, or 2nd and 3rd)
> > > which also covers all columns.
> > >
> > > I solved this problem by creating a second logical matrix which contains all
> > > possible combinations of rows:
> > > tt <- matrix(as.logical(c(1,0,0,0,1,0,0,0,1,1,1,0,1,0,1,0,1,1,1,1,1)), nrow=3)
> > >
> > > and then subset the first matrix and check if all columns are covered.
> > > This solution, though, is highly inneficient and I am certain that a
> > > combination of apply or something will do.
> > >
> > > ###########################
> > >
> > > possibles <- NULL
> > > length.possibles <- NULL
> > > ## I guess the minimum solution is has half the number of rows
> > > guesstimate <- floor(nrow(tt)/2) + nrow(tt) %% 2
> > > checked <- logical(nrow(tt))
> > > repeat {
> > >    ifelse(checked[guesstimate], break, checked[guesstimate] <- TRUE)
> > >    partials <- as.matrix(tt[, colSums(tt) == guesstimate])
> > >    layer.solution <- logical(ncol(partials))
> > >
> > >    for (j in 1:ncol(partials)) {
> > >        if (length(which(colSums(mtrx[partials[, j], ]) > 0)) == ncol(mtrx)) {
> > >            layer.solution[j] <- TRUE
> > >        }
> > >    }
> > >    if (sum(layer.solution) == 0) {
> > >        if (!is.null(possibles)) break
> > >        guesstimate <- guesstimate + 1
> > >    } else {
> > >        for (j in which(layer.solution)) {
> > >            possible.solution <- rownames(mtrx)[partials[, j]]
> > >            possibles[[length(possibles) + 1]] <- possible.solution
> > >            length.possibles <- c(length.possibles, length(possible.solution))
> > >        }
> > >        guesstimate <- guesstimate - 1
> > >    }
> > > }
> > > final.solution <- possibles[which(length.possibles == min(length.possibles))]
> > >
> > > ###########################
> > >
> > > More explicitely (if useful) it is about reducing a prime implicants chart in
> > > a Quine-McCluskey boolean minimisation algorithm. I tried following the
> > > original algorithm applying row dominance and column dominance, but (as I am
> > > not a computer scientist), I am unable to apply it.
> > >
> > > If you have a better solution for this, I would be gratefull if you'd share
> > > it.
> > > Thank you in advance,
> > > Adrian
> > >
> > > --
> > > Adrian DUSA
> > > Romanian Social Data Archive
> > > 1, Schitu Magureanu Bd
> > > 050025 Bucharest sector 5
> > > Romania
> > > Tel./Fax: +40 21 3126618 \
> > >          +40 21 3120210 / int.101
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
>



From s.molnar at sbcglobal.net  Sat Nov 19 16:35:27 2005
From: s.molnar at sbcglobal.net (Stephen P. Molnar, Ph.D.)
Date: Sat, 19 Nov 2005 10:35:27 -0500
Subject: [R] Autoloading R Commander
Message-ID: <200511191035.27459.s.molnar@sbcglobal.net>

How do I go about autoloading R Commander when I start R?

Thanks in advance.
-- 
Stephen P. Molnar, Ph.D.				        Life is a fuzzy set
Foundation for Chemistry					Stochastic and multivariant
http://www.geocities.com/FoundationForChemistry



From dusa.adrian at gmail.com  Sat Nov 19 15:39:45 2005
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sat, 19 Nov 2005 16:39:45 +0200
Subject: [R] help with apply, please
In-Reply-To: <971536df0511190724k3c32a6b3of5a1b10f567ee8b4@mail.gmail.com>
References: <200511191500.25863.adi@roda.ro>
	<971536df0511190717x5635a606u596ce2958f75ca74@mail.gmail.com>
	<971536df0511190724k3c32a6b3of5a1b10f567ee8b4@mail.gmail.com>
Message-ID: <200511191639.45377.dusa.adrian@gmail.com>

On Saturday 19 November 2005 17:24, Gabor Grothendieck wrote:
> [...snip...]
> Although the above is not wrong I should have removed the rbind
> which is no longer needed and simplifying it further, as it seems
> that lp will do the rep for you itself for certain arguments, gives:
>
> lp("min", rep(1,3), t(mtrx), ">=", 1)$solution  # 1 0 1

Thank you Gabor, this solution is superbe (you never stop amazing me :)
Now... it only finds _one_ of the multiple minimum solutions. In the toy 
example, there are two minimum solutions, hence I reckon the output should 
have been a list with:
[[1]]
[1] 1 0 1

[[2]]
[1] 0 1 1

Also, thanks to Duncan and yes, I do very much care finding the smallest 
possible solutions (if I correctly understand your question).

It seems that lp function is very promising, but can I use it to find _all_ 
minimum solutions?

Adrian



-- 
Adrian DUSA
Arhiva Romana de Date Sociale
Bd. Schitu Magureanu nr.1
050025 Bucuresti sectorul 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From ligges at statistik.uni-dortmund.de  Sat Nov 19 16:41:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 19 Nov 2005 16:41:59 +0100
Subject: [R] Autoloading R Commander
In-Reply-To: <200511191035.27459.s.molnar@sbcglobal.net>
References: <200511191035.27459.s.molnar@sbcglobal.net>
Message-ID: <437F47C7.7020307@statistik.uni-dortmund.de>

Stephen P. Molnar, Ph.D. wrote:

> How do I go about autoloading R Commander when I start R?


See ?Startup

Uwe Ligges


> Thanks in advance.



From kfarnham at mathematicalanalysis.com  Sat Nov 19 16:47:26 2005
From: kfarnham at mathematicalanalysis.com (Kevin Farnham)
Date: Sat, 19 Nov 2005 07:47:26 -0800 (PST)
Subject: [R] new article on R at oreillynet.com
Message-ID: <20051119154726.51816.qmail@web52305.mail.yahoo.com>

An article I wrote that provides a basic introduction to R has
been published on Oreillynet.com. The article is titled
"Analyzing Statistics with GNU/R". Here is the link:

http://www.onlamp.com/pub/a/onlamp/2005/11/17/r_for_statistics.html

Please feel free to post comments or interesting basic R scripts
at the end of the article. 

Kevin Farnham



From ripley at stats.ox.ac.uk  Sat Nov 19 16:50:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Nov 2005 15:50:00 +0000 (GMT)
Subject: [R] Autoloading R Commander
In-Reply-To: <200511191035.27459.s.molnar@sbcglobal.net>
References: <200511191035.27459.s.molnar@sbcglobal.net>
Message-ID: <Pine.LNX.4.61.0511191544510.5929@gannet.stats>

On Sat, 19 Nov 2005, Stephen P. Molnar, Ph.D. wrote:

> How do I go about autoloading R Commander when I start R?

Read ?Startup.  My first idea would be to make use of a ~/.Rprofile file.

`Autoloading' is a technical term in R (see ?autoload), which I presume is 
not what you meant.  My guess is that you want R Commander to be started 
when you start R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Sat Nov 19 17:27:10 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 19 Nov 2005 11:27:10 -0500
Subject: [R] Autoloading R Commander
In-Reply-To: <200511191035.27459.s.molnar@sbcglobal.net>
Message-ID: <20051119162708.NSUM21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Stephen,

I believe that this question has been asked before, though possibly
privately rather than on the r-help list. A solution (kindly provided, as I
recall, by Brian Ripley) is to put the following in an appropriate start-up
file. For example, if you *always* want to start the Rcmdr when R starts,
this could go in Rprofile.site in R's etc subdirectory. For more detail, see
?Startup, as others have suggested.

local({
  old <- getOption("defaultPackages")
  options(defaultPackages = c(old, "Rcmdr"))
})

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Stephen P. Molnar, Ph.D.
> Sent: Saturday, November 19, 2005 10:35 AM
> To: R
> Subject: [R] Autoloading R Commander
> 
> How do I go about autoloading R Commander when I start R?
> 
> Thanks in advance.
> -- 
> Stephen P. Molnar, Ph.D.				        
> Life is a fuzzy set
> Foundation for Chemistry					
> Stochastic and multivariant
> http://www.geocities.com/FoundationForChemistry
>



From jfox at mcmaster.ca  Sat Nov 19 17:43:04 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 19 Nov 2005 11:43:04 -0500
Subject: [R] Autoloading R Commander
In-Reply-To: <20051119162708.NSUM21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <20051119164302.IWOE2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Stephen,

As a brief addendum, this information (and other information) is in the
Rcmdr installation notes, at
http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html.

Sorry I forgot that when I posted my original answer.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Fox
> Sent: Saturday, November 19, 2005 11:27 AM
> To: s.molnar at sbcglobal.net
> Cc: 'R'
> Subject: Re: [R] Autoloading R Commander
> 
> Dear Stephen,
> 
> I believe that this question has been asked before, though 
> possibly privately rather than on the r-help list. A solution 
> (kindly provided, as I recall, by Brian Ripley) is to put the 
> following in an appropriate start-up file. For example, if 
> you *always* want to start the Rcmdr when R starts, this 
> could go in Rprofile.site in R's etc subdirectory. For more 
> detail, see ?Startup, as others have suggested.
> 
> local({
>   old <- getOption("defaultPackages")
>   options(defaultPackages = c(old, "Rcmdr"))
> })
> 
> I hope this helps,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Stephen P. 
> > Molnar, Ph.D.
> > Sent: Saturday, November 19, 2005 10:35 AM
> > To: R
> > Subject: [R] Autoloading R Commander
> > 
> > How do I go about autoloading R Commander when I start R?
> > 
> > Thanks in advance.
> > -- 
> > Stephen P. Molnar, Ph.D.				        
> > Life is a fuzzy set
> > Foundation for Chemistry					
> > Stochastic and multivariant
> > http://www.geocities.com/FoundationForChemistry
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From stratja at auburn.edu  Sat Nov 19 17:44:52 2005
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Sat, 19 Nov 2005 10:44:52 -0600
Subject: [R] predicted values  from cv.glm
Message-ID: <437F0224020000F2000019DF@TMIA1.AUBURN.EDU>

Hi. Is there a way to get the values predicted from (leave-one-out)
cv.glm?  

It seems like a useful diagnostic to plot observed vs. predicted values.

Thanks,

Jeff

****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja



From s.molnar at sbcglobal.net  Sat Nov 19 17:51:13 2005
From: s.molnar at sbcglobal.net (Stephen P. Molnar, Ph.D.)
Date: Sat, 19 Nov 2005 11:51:13 -0500
Subject: [R] Fwd:  Autoloading R Commander
Message-ID: <200511191151.13975.s.molnar@sbcglobal.net>



----------  Forwarded Message  ----------

Subject: [R] Autoloading R Commander
Date: Saturday  November 19, 2005 10:35 am
From: "Stephen P. Molnar, Ph.D." <s.molnar at sbcglobal.net>
To: R <r-help at stat.math.ethz.ch>

How do I go about autoloading R Commander when I start R?

Thanks in advance.
--
Stephen P. Molnar, Ph.D.				        Life is a fuzzy set
Foundation for Chemistry					Stochastic and multivariant
http://www.geocities.com/FoundationForChemistry

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-------------------------------------------------------

My thanks to all that answered.  The solution, thanks to John Fox is to add:

local({
  old <- getOption("defaultPackages")
  options(defaultPackages = c(old, "Rcmdr"))
})

to /usr/local/lib/R/etc/Rprofile.site

-- 
Stephen P. Molnar, Ph.D.				        Life is a fuzzy set
Foundation for Chemistry					Stochastic and multivariant
http://www.geocities.com/FoundationForChemistry



From dieter.menne at menne-biomed.de  Sat Nov 19 18:05:15 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 19 Nov 2005 17:05:15 +0000 (UTC)
Subject: [R] Adding points to wireframe
References: <437E494B.9090201@polymtl.ca>
Message-ID: <loom.20051119T180111-919@post.gmane.org>

Pierre-Luc Brunelle <pierre-luc.brunelle <at> polymtl.ca> writes:

> 
> I am using function wireframe from package lattice to draw a 3D surface. 
> I would like to add a few points on the surface. I read in a post from 
> Deepayan Sarkar that "To do this in a wireframe plot you would probably 
> use the panel function panel.3dscatter". Does someone have an example? 
> When calling panel.3dscatter with only x, y and z arguments I get 
> "argument "xlim.scaled" is missing, with no default". I do not know what 
> value I should give to xlim.scaled.

Maybe your got confused by Deepayan's comment. Normally, you don't call 
panel.3dscatter directly, you will only use it when you want to change the 
default behaviour of cloud with groups.

Try to add the data points to the dataframe you use in the call to wireframe.


Dieter



From pburns at pburns.seanet.com  Sat Nov 19 18:17:15 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 19 Nov 2005 17:17:15 +0000
Subject: [R] help with apply, please
In-Reply-To: <200511191639.45377.dusa.adrian@gmail.com>
References: <200511191500.25863.adi@roda.ro>	<971536df0511190717x5635a606u596ce2958f75ca74@mail.gmail.com>	<971536df0511190724k3c32a6b3of5a1b10f567ee8b4@mail.gmail.com>
	<200511191639.45377.dusa.adrian@gmail.com>
Message-ID: <437F5E1B.4070008@pburns.seanet.com>

I suspect that the answer is that finding all solutions
will be hard.  L1 regression is a special case of LP.
I learned how to move around the corners of the
solution space, and could easily find all of the solutions
in the special case of a two-way table.  However,
sometimes there were a lot of solutions.

I would guess that your problem has a lot of solutions
as well.     One cheat would be to do the LP problem
multiple times with the rows of your matrix randomly
permuted.  Assuming you keep track of the real rows,
you could then get a sense of how many solutions there
might be.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Adrian Dusa wrote:

>On Saturday 19 November 2005 17:24, Gabor Grothendieck wrote:
>  
>
>>[...snip...]
>>Although the above is not wrong I should have removed the rbind
>>which is no longer needed and simplifying it further, as it seems
>>that lp will do the rep for you itself for certain arguments, gives:
>>
>>lp("min", rep(1,3), t(mtrx), ">=", 1)$solution  # 1 0 1
>>    
>>
>
>Thank you Gabor, this solution is superbe (you never stop amazing me :)
>Now... it only finds _one_ of the multiple minimum solutions. In the toy 
>example, there are two minimum solutions, hence I reckon the output should 
>have been a list with:
>[[1]]
>[1] 1 0 1
>
>[[2]]
>[1] 0 1 1
>
>Also, thanks to Duncan and yes, I do very much care finding the smallest 
>possible solutions (if I correctly understand your question).
>
>It seems that lp function is very promising, but can I use it to find _all_ 
>minimum solutions?
>
>Adrian
>
>
>
>  
>



From dusa.adrian at gmail.com  Sat Nov 19 18:32:48 2005
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Sat, 19 Nov 2005 19:32:48 +0200
Subject: [R] help with apply, please
In-Reply-To: <437F5E1B.4070008@pburns.seanet.com>
References: <200511191500.25863.adi@roda.ro>
	<200511191639.45377.dusa.adrian@gmail.com>
	<437F5E1B.4070008@pburns.seanet.com>
Message-ID: <200511191932.49106.dusa.adrian@gmail.com>

On Saturday 19 November 2005 19:17, Patrick Burns wrote:
> [....snip...] One cheat would be to do the LP problem
> multiple times with the rows of your matrix randomly
> permuted.  Assuming you keep track of the real rows,
> you could then get a sense of how many solutions there
> might be.

Thanks for the answer. The trick does work (i.e. it finds all minimum 
solutions) provided that I permute the rows a sufficient number of times. And 
I have to compare each solution to the existing (unique) ones, which takes a 
lot of time...
In your experience, what would be the definiton of "multiple times" for large 
matrices?

My (dumb) solution is guaranteed to find all possible minimums, because it 
checks every possible combination. For large matrices, though, this would be 
really slow. I wonder if that could be vectorized in some way; before the LP 
function, I was thinking there might be a more efficient way to loop over all 
possible columns (using perhaps the apply family).

Thanks again,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From Ted.Harding at nessie.mcc.ac.uk  Sat Nov 19 19:51:03 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 19 Nov 2005 18:51:03 -0000 (GMT)
Subject: [R] help with apply, please
In-Reply-To: <200511191639.45377.dusa.adrian@gmail.com>
Message-ID: <XFMail.051119185103.Ted.Harding@nessie.mcc.ac.uk>

On 19-Nov-05 Adrian Dusa wrote:
> On Saturday 19 November 2005 17:24, Gabor Grothendieck wrote:
>> [...snip...]
>> Although the above is not wrong I should have removed the
>> rbind which is no longer needed and simplifying it further,
>> as it seems that lp will do the rep for you itself for
>> certain arguments, gives:
>>
>> lp("min", rep(1,3), t(mtrx), ">=", 1)$solution  # 1 0 1
> 
> Thank you Gabor, this solution is superbe (you never stop
> amazing me :). Now... it only finds _one_ of the multiple
> minimum solutions. In the toy example, there are two minimum
> solutions, hence I reckon the output should have been a list with:
> [[1]]
> [1] 1 0 1
> 
> [[2]]
> [1] 0 1 1
> 
> Also, thanks to Duncan and yes, I do very much care finding
> the smallest possible solutions (if I correctly understand
> your question).
> 
> It seems that lp function is very promising, but can I use it
> to find _all_ minimum solutions?

Thinking about it, I'm not sure that finding the complete set
of solutions (in general, not just to Adrian's toy example)
can be done without enumeration, complete up to the stage
where it is known that all minimal solutions have been found
(and, having said that, I shall probably provoke an expert
into refuting me; im which case, all the better!).

For example, consider a 9-column matrix with 84 rows, each
with 3 1s and 6 0s (nCm(9,3)=84).

Every minimal solution is the union of 3 rows, e.g.

  1 1 1 0 0 0 0 0 0
  0 0 0 1 1 1 0 0 0
  0 0 0 0 0 0 1 1 1

and there is a 1:1 correspondence between the choices of
3 out of 9 and the 84 rows.

So there are 84 = nCm(9,3) choices of 3 1s for the first row,
leaving 6 0s. There are then nCm(6,3) = 20 choices of 3 1s
for the second row. Having done that, the choice of the 1s
for the third row s determined. But this must be divided by
3! = 6 to give "unordered" rows, so there are 84*20/6 = 280
different minimal solutions.


Without yet having taken this down to the level of R code,
I would envisage an algorithm for an N*k matrix M on the
following lines.

1. Check that it's possible: sum(colSums(matr)==0) = 0

2. For m = 1:N work through all choices of m rows out of
   the N; if, for the current value of m, any one of these
   covers, complete the choices for this value of m,
   noting which choices cover, and then exit.

This will give you all the choices of m out of N rows
which cover the columns, for the smallest value of m
for which this is possible, and you will not have
searched in larger values of m.

The function 'combn' in package combinat may be useful
here: combn(N,m) returns an array with nCm columns and
m rows, where the values in a column are a choice of m
out of (1:N).

There is an opposite version of this algorithm: If there
are not many 1s in matr, then you might guess that a lot
of rows may be needed. So it may be more efficient to
do it the other way round: Starting with all rows (m=N),
leave them out 1 at a time, then 2 at a time, and so on,
until you get a value of m such that no choice of m out
of N covers the columns. Then (m+1) is the minimal order
of a covering set of rows, and you've just found all these.

But this may not be sound either, since if one row has
many 1s then possibly some few others can make up the
covering, so it would be better to do it the original
way round. I can't get a clear view of what sort of
criterion to apply to determine this issue programmatically.

There is bound to be a good algorithm out there somewhere
for finding a "minimal coveriung set" but I don't know it!

Comments?

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 19-Nov-05                                       Time: 18:51:00
------------------------------ XFMail ------------------------------



From spencer.graves at pdf.com  Sat Nov 19 20:04:24 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 19 Nov 2005 11:04:24 -0800
Subject: [R] Specify Z matrix with lmer function
In-Reply-To: <436A514F.8080202@byu.edu>
References: <436A514F.8080202@byu.edu>
Message-ID: <437F7738.5040709@pdf.com>

	  There is probably a way to do what you want, but I don't know how. 
You are to be commended for providing a self-contained example citing an 
interesting article from the Journal of Statistical Software and 
including the modification necessary to make the S-Plus "lme" call work 
in R.

	  I tried several things including the following:

	  fit.r1 <- lmer(y~-1+X+(-1+Z|grp))

	  This seemed to run but gave an answer different from that from lme. 
I also noted that the lmer documentation said the argument "start" was 
"a list of relative precision matrices for the random effects.  This has 
the same form as the slot '"Omega"' in a fitted model.  Only the upper 
triangle of these symmetric matrices should be stored."  This suggested 
I provide "start" with a list consiting of R from the QR decomposition 
of Z:

 > grp.dat.qr <- qr(grp.dat[2:26])
 > grp.dat.R <- qr.R(grp.dat.qr)
 > fit.r <- lmer(y~-1+X+(-1+Z|grp), start=list(grp=grp.dat.R))
Error in lmer(y ~ -1 + X + (-1 + Z | grp), start = list(grp = grp.dat.R)) :
	Leading 1 minor of Omega[[1]] not positive definite

	  Some of the diagonal elements of R were negative, so I just changed 
the sign and tried again:

 > grp.dat.R2 <- (diag(sign(diag(grp.dat.R)))
+                %*% grp.dat.R)
 > fit.r2 <- lmer(y~-1+X+(-1+Z|grp), start=list(grp=grp.dat.R2))
Error in lmer(y ~ -1 + X + (-1 + Z | grp), start = list(grp = 
grp.dat.R2)) :
	Leading 2 minor of Omega[[1]] not positive definite

	  If I were to do more with this, I'd first review all the 
documentation I could find on lmer including Doug Bates, "Fitting linear 
mixed models in R", R News, 5(1):  27-30, May 2005, and "Linear mixed 
model implementation in lmer", July 26, 2005, distributed with lme4 and 
stored on my hard drive under 
"~\R\R-2.2.0\library\lme4\doc\Implementation.pdf".  I might also consult 
Pinheiro and Bates (2000) Mixed-Effects Models in S and S-PLUS 
(Springer), which is my primary source for mixed models generally.  If I 
couldn't figure it out from there, I'd then try to work through the code 
line by line.  Since "lmer" calls "standardGeneric", it's not completely 
obvious how to get the code.  Moreover, "methods" won't get it, because 
"lmer" follows the S4 standard (if I understand correctly). 
Fortunately, 'showMethods("lmer")' produced the following:

Function "lmer":
formula = "formula"

       With this information, I then tried, 'getMethod("lmer", 
"formula")', which gave me the desired source code.  I could then copy 
it into a script file, walk through it line by line, and learn something.

	  hope this helps.
	  spencer graves

Mark Lyman wrote:

> Is there a way to specify a Z matrix using the lmer function, where the 
> model is written as y = X*Beta + Z*u + e?
> 
> I am trying to reproduce smoothing methods illustrated in the paper 
> "Smoothing with Mixed Model Software" my Long Ngo and M.P. Wand. 
> published in the /Journal of Statistical Software/ in 2004 using the 
> lme4 and Matrix packages. The code and data sets used can be found at 
> http://www.jstatsoft.org/v09/i01/.
> 
> There original code did  not work for me without slight modifications 
> here is the code that I used with my modifications noted.
> 
> x <- fossil$age
> y <- 100000*fossil$strontium.ratio
> knots <- seq(94,121,length=25)
> n <- length(x)
> X <- cbind(rep(1,n),x)
> Z <- outer(x,knots,"-")
> Z <- Z*(Z>0)
> # I had to create the groupedData object with one group to fit the model 
> I wanted
> grp <- rep(1,n)
> grp.dat<-groupedData(y~Z|grp)
> fit <- lme(y~-1+X,random=pdIdent(~-1+Z),data=grp.dat)
> 
> I would like to know how I could fit this same model using the lmer 
> function. Specifically can I specify a Z matrix in the same way as I do 
> above in lme?
> 
> Thanks,
> Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From dusa.adrian at gmail.com  Sat Nov 19 19:23:09 2005
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Sat, 19 Nov 2005 20:23:09 +0200
Subject: [R] help with apply, please
In-Reply-To: <XFMail.051119185103.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051119185103.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200511192023.09134.dusa.adrian@gmail.com>

Dear Ted,

On Saturday 19 November 2005 20:51, Ted Harding wrote:
> [...snip...]
> There is bound to be a good algorithm out there somewhere
> for finding a "minimal coveriung set" but I don't know it!
>
> Comments?
>
> Best wishes to all,
> Ted.

My case is probably a subset of your general algorithm.
Peaking in the computer science webpages for Quine-McCluskey algorithm, I 
learned that there are way to simplify a matrix (prime implicants chart) 
before trying to find the minimum solutions. 
For example:
1. Row dominance
   0 0 1 1 0 0
   0 1 1 1 0 0
The second row containes all elements that the first row contains, therefore 
the first row (dominated) can be droped

2. Column dominance
  0 1
  0 1
  1 1
  1 1
  0 0
The second column dominates the first column, therefore we can drop the second 
(dominating) column

In a Quine-McCluskey algorithm, the number of rows will always be much lower 
than the number of columns, and applying the two above principles will make 
the matrix even more simple.
There are algorithms written in other languages (like Java) freely available 
on the Internet, but I have no idea how to adapt them to R.

Best,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From dusa.adrian at gmail.com  Sat Nov 19 19:46:10 2005
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Sat, 19 Nov 2005 20:46:10 +0200
Subject: [R] help with apply, please
In-Reply-To: <XFMail.051119185103.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051119185103.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200511192046.10568.dusa.adrian@gmail.com>

On Saturday 19 November 2005 20:51, Ted Harding wrote:
> [..snip...]
> There is bound to be a good algorithm out there somewhere
> for finding a "minimal coveriung set" but I don't know it!
> Best wishes to all,
> Ted.

I found this presentation very explicit:
http://www.cs.ualberta.ca/~amaral/courses/329/webslides/Topic5-QuineMcCluskey/sld079.htm

Best wishes,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From ggrothendieck at gmail.com  Sat Nov 19 21:09:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 19 Nov 2005 15:09:13 -0500
Subject: [R] help with apply, please
In-Reply-To: <200511191932.49106.dusa.adrian@gmail.com>
References: <200511191500.25863.adi@roda.ro>
	<200511191639.45377.dusa.adrian@gmail.com>
	<437F5E1B.4070008@pburns.seanet.com>
	<200511191932.49106.dusa.adrian@gmail.com>
Message-ID: <971536df0511191209w2d94685dx38990384afbc3236@mail.gmail.com>

On 11/19/05, Adrian DUSA <dusa.adrian at gmail.com> wrote:
> On Saturday 19 November 2005 19:17, Patrick Burns wrote:
> > [....snip...] One cheat would be to do the LP problem
> > multiple times with the rows of your matrix randomly
> > permuted.  Assuming you keep track of the real rows,
> > you could then get a sense of how many solutions there
> > might be.
>
> Thanks for the answer. The trick does work (i.e. it finds all minimum
> solutions) provided that I permute the rows a sufficient number of times. And
> I have to compare each solution to the existing (unique) ones, which takes a
> lot of time...
> In your experience, what would be the definiton of "multiple times" for large
> matrices?
>
> My (dumb) solution is guaranteed to find all possible minimums, because it
> checks every possible combination. For large matrices, though, this would be
> really slow. I wonder if that could be vectorized in some way; before the LP
> function, I was thinking there might be a more efficient way to loop over all
> possible columns (using perhaps the apply family).
>
> Thanks again,
> Adrian
>

Getting back to your original question of using apply, solving the LP
gives us the number of components in any minimal solution and
exhaustive search of all solutions with that many components can
be done using combinations from gtools and apply like this:

library(gtools) # needed for combinations
soln <- lp("min", rep(1,3), rbind(t(mtrx)), rep(">=", 4), rep(1,4))$solution
k <- sum(soln)
m <- nrow(mtrx)
combos <- combinations(m,k)
combos[apply(combos, 1, function(idx) all(colSums(mtrx[idx,]))),]

In the example we get:

     [,1] [,2]
[1,]    1    3
[2,]    2    3

which says that rows 1 and 3 of mtrx form one solution
and rows 2 and 3 of mtrx form another solution.



From claus.atzenbeck at freenet.de  Sat Nov 19 21:11:15 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Sat, 19 Nov 2005 21:11:15 +0100 (CET)
Subject: [R] Crop white border for PDF output
Message-ID: <Pine.OSX.4.61.0511192056590.3976@cirrus.local>

Hi,

I produce a series of diagrams with R in order to include them in my
documents (LaTeX). However, there is a white border around the diagrams.
For some that do not have anything written at the very bottom, the white
border is relatively large. The rather big space between figure and
caption at the final document looks not nice.

It would be best not to have any white border. I played with
oma=c(0,0,0,0) and mar=c(0,0,0,0) for single and multiple figure
environments, however, without success.

Is there an easy way to get rid of the white border for PDF outputs?

Thanks for your input.
Claus



From adi at roda.ro  Sat Nov 19 20:21:25 2005
From: adi at roda.ro (Adrian DUSA)
Date: Sat, 19 Nov 2005 21:21:25 +0200
Subject: [R] help with apply, please
In-Reply-To: <971536df0511191209w2d94685dx38990384afbc3236@mail.gmail.com>
References: <200511191500.25863.adi@roda.ro>
	<200511191932.49106.dusa.adrian@gmail.com>
	<971536df0511191209w2d94685dx38990384afbc3236@mail.gmail.com>
Message-ID: <200511192121.25746.adi@roda.ro>

On Saturday 19 November 2005 22:09, Gabor Grothendieck wrote:
> Getting back to your original question of using apply, solving the LP
> gives us the number of components in any minimal solution and
> exhaustive search of all solutions with that many components can
> be done using combinations from gtools and apply like this:
>
> library(gtools) # needed for combinations
> soln <- lp("min", rep(1,3), rbind(t(mtrx)), rep(">=", 4),
> rep(1,4))$solution k <- sum(soln)
> m <- nrow(mtrx)
> combos <- combinations(m,k)
> combos[apply(combos, 1, function(idx) all(colSums(mtrx[idx,]))),]
>
> In the example we get:
>
>      [,1] [,2]
> [1,]    1    3
> [2,]    2    3
>
> which says that rows 1 and 3 of mtrx form one solution
> and rows 2 and 3 of mtrx form another solution.

I'm speechless.
It is exactly what I needed.
A billion of thanks!
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From jeff.hamann at forestinformatics.com  Sat Nov 19 22:18:26 2005
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Sat, 19 Nov 2005 13:18:26 -0800 (PST)
Subject: [R] directional correlograms?
Message-ID: <3364.128.193.141.175.1132435106.squirrel@www.forestinformatics.com>

I've run out of ideas for a simple solution, so...

Does anyone know of a package that can compute directional correlograms?
The spatial package seems to work for all directions,

Usage:

     correlogram(krig, nint, plotit = TRUE,  ...)

but I don't know how to modify the spatial package (if required) or how I
can arrange/filter the data to be able to compute directional correlograms
so that I may use the spatial::correlogram().

I've used the spatmast package in S+ (but need to be able to do this in R).

Thanks,
Jeff.


-- 
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421
phone 541-754-1428
fax 541-752-0288
jeff.hamann at forestinformatics.com
www.forestinformatics.com



From narguea at ihmc.us  Sun Nov 20 01:35:13 2005
From: narguea at ihmc.us (Nestor Arguea)
Date: Sat, 19 Nov 2005 18:35:13 -0600
Subject: [R] Attach a time series object
Message-ID: <437FC4C1.7090207@ihmc.us>

Is there an attach-like command for time series objects?
Thanks in advance,

Nestor



From ggrothendieck at gmail.com  Sun Nov 20 05:34:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 19 Nov 2005 23:34:38 -0500
Subject: [R] Attach a time series object
In-Reply-To: <437FC4C1.7090207@ihmc.us>
References: <437FC4C1.7090207@ihmc.us>
Message-ID: <971536df0511192034o616a7408t5d21a3a6729d8c3b@mail.gmail.com>

Try this:

   attach(as.list(my.time.series))

On 11/19/05, Nestor Arguea <narguea at ihmc.us> wrote:
> Is there an attach-like command for time series objects?
> Thanks in advance,



From patrick.giraudoux at univ-fcomte.fr  Sun Nov 20 08:22:15 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sun, 20 Nov 2005 08:22:15 +0100
Subject: [R] repeated values, nlme, correlation structures
In-Reply-To: <437FA274.7050607@pdf.com>
References: <4374ED11.6010401@univ-fcomte.fr> <437FA274.7050607@pdf.com>
Message-ID: <43802427.8040604@univ-fcomte.fr>

Spencer Graves a ??crit :

>       You are concerned that, "using the mean of each age category as 
> variable leads to a loss of information regarding the variance on the 
> weight at each age and nestbox."  What information do you think you lose?

The variance  around the mean weight of each age category. This 
variation is a priori not considered in the model when using the mean 
only, and not each value used to compute the mean..

>
>       In particular, have you studied the residuals from your fit?  I 
> would guess that the you probably have heterscedasticity with the 
> variance of the residuals probably increasing with the age.  Plots of 
> the absolute residuals might help identify this.  

Yes, of course. At this stage using a  Continuous AR(1) as Correlation 
Structure, reduces considerably heteroscedasticity up to quasi-normal.

> Also, is the number of blue tits in each age constant, or does it 
> change, e.g., as some of the chicks die?

Yes, unfortunately, it may happen eventually.

>
>       To try to assess how much information I lost (especially if some 
> of the chicks died), I might plot the weights in each nest box and 
> connect the dots manually, attempting to assign chick identity to the 
> individual numbers.  I might do it two different ways, one best fit, 
> and another "worst plausible".  Then I might try to fit models to 
> these two "augmented data sets" as if I had the true chick identity.  
> Then comparing these fits with the one you already have should help 
> you evaluate what information you lost by using the averages AND give 
> you a reasonable shot at recovering that information.  If the results 
> were promising, I might generate more than two sets of assignments, 
> involving other people in that task.

OK, should not be that difficult (actually the data were given with 
pseudo-ID numbers on each chicks and I started with this... until I 
learned they were corresponding to nothing). I suppose one could go as 
far as possible with the "worst possible" with random assignements and 
permutations, and thus comparing the fits.

Many thanks for the hint. I was really wondering what may mean no answer 
on the list... Problem not clear enough, trivial solution or real 
trouble for statisticians with such data? Quite  scaring to a 
biologist...  Now, I am fixed.

> If the results were promising, I might generate more than two sets of 
> assignments, involving other people in that task. 

Of course if some capable mixed-effect models specialist is interested 
in having a look to the data set, I can send it off list.

Many thanks again, Spencer, I can stick on the track, now...

Best regards,

Patrick


>       Bon Chance
>       Spencer Graves
>
> Patrick Giraudoux wrote:
>
>> Dear listers,
>>
>> My request of last week seems not to have drawn someone's attention. 
>> Suppose it was not clear enough.
>>
>> I am coping with an observational study where people's aim was to fit 
>> growth curve for a population of young blue tits. For logistic 
>> reasons, people have not been capable to number each individual, but 
>> they have a method to assess their age. Thus, nestboxes were visited 
>> occasionnally, youngs aged and weighted.
>>
>> This makes a multilevel data set, with two classification factors:
>>
>> - the nestbox (youngs shared the same parents and general feeding 
>> conditions)
>> - age in each nestbox (animals from the same nestbox have been 
>> weighed along time, which likely leads to time correlation)
>>
>> Life would have been heaven if individuals were numbered, and thus 
>> nlme correlation structure implemented in the package be used easy. 
>> As mentioned above, this could not be the case. In a first approach, 
>> I actually used the mean weight of the youngs weighed at each age in 
>> nest boxes for the variable "age", and could get a nice fit with 
>> "nestbox" as random variable and corCAR1(form=~age|nestbox) as 
>> covariation structure.
>>
>> modm0c<-nlme(pds~Asym/(1+exp((xmid-age)/scal)),
>>     fixed=list(Asym~1,xmid~1,scal~1),
>>     random=Asym+xmid~1|nestbox,data=croispulm,
>>     start=list(fixed=c(10,5,2.2)),
>>     method="ML",
>>     corr=corCAR1(form=~age|nestbox)
>>     )
>>
>> Assuming that I did not commited some error in setting model 
>> parameters (?), this way of doing is not fully satisfying, since 
>> using the mean of each age category as variable  leads to a  loss of 
>> information regarding the variance on the weight at each age and 
>> nestbox.
>>
>> My question is: is there a way to handle repeated values per group 
>> (here several youngs in an age category in each nestbox) in such a case?
>>
>> I would really appreciate an answer, even negative...
>>
>> Kind regards,
>>
>> Patrick
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>



From spencer.graves at pdf.com  Sun Nov 20 08:29:05 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 19 Nov 2005 23:29:05 -0800
Subject: [R] Can someone Help in nls() package
In-Reply-To: <a666434f0511080943g434e0dcfg9527bd9736222085@mail.gmail.com>
References: <a666434f0511080943g434e0dcfg9527bd9736222085@mail.gmail.com>
Message-ID: <438025C1.209@pdf.com>

	  When I tried to run your example, I got the following:

Error in func(times[1], y, parms) : object "Cum2" not found

	  While I couldn't replicate your error, I can tell you that the reason 
"print(coef(fit))" gave the error it did was because "nls" refuses to 
return anything when it encounters an error.

	  If it were my problem, I might write a least-squares wrapper and give 
it to "optim(..., hessian=T)", as optim will still return an answer when 
it seems overparameterized, etc.;  in that case, computing the 
eigenvalues and vectors of the hessian can help identify the problem. 
If "optim" also generated an error like "Missing value or an infinity 
produced", I might modify 'f' to print its arguments and output.  From 
that, I can usually figure out what I want to do about that.

	  hope this helps.
	  spencer graves

Raja Jayaraman wrote:

> Hello R-Community,
> we are running aprogram to fit Non-linear differential equations to Aphid
> population Data and to estimate the birth and death parameters,
> here is the code:
> 
> dat<-data.frame(Time=c(0:60),Cur=c(5,6.2,59,39,38,44,20.4,19.4,34.2,35.4,38.2,48.2,55.4,113.2,
> 97,112,115,126,136.6,140.6,147.2,151.6,157.8,170,202,210.4,221.2,224.4,248.2,266,
> 277,291.4,392,461.2,470,418,410.8,395.6,365.2,189.4,43.4,33.2,32,29,26,26,25.6,24.6,
> 24,23.4,18.4,17.6,15.8,13.6,10.6,8.4,6.4,5.4,4.2,3.6,2.7))
> 
> times<-c(0:60)
> f <- function(t,xx,pars){
>  Cur<- xx[1]
>  Cum<- xx[2]
>  a <- pars[1]
>  b <- pars[2]
>  dCur <- a*Cur-b*Cur*Cum^2
>  dCum <- Cur
>  list(c(dCur,dCum))
> }
> 
> 
> require(nls)
> fit <- nls(Cur ~ lsoda(c(Cur=1,Cum=1), times, f, c(a=a, b=b))[,2],
>            data=dat,
>            start=list(a=2.5, b=.01),
>            trace=T
>            )
> print(coef(fit))
> 
> It runs the first iterations and shows the output, after the intial
> eveluation it send the following error message:
> 
> 92 :  2.50 0.01
> Error in numericDeriv(form[[3]], names(ind), env) :
>         Missing value or an infinity produced when evaluating the model
> In addition: There were 50 or more warnings (use warnings() to see the first 50)
> 
>>print(coef(fit))
> 
> Error in coef(fit) : object "fit" not found
> 
> Looking forward to hear soon
> Thanks
> Raj
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From narguea at ihmc.us  Sun Nov 20 08:32:25 2005
From: narguea at ihmc.us (Nestor Arguea)
Date: Sun, 20 Nov 2005 01:32:25 -0600
Subject: [R] Attach a time series object
In-Reply-To: <971536df0511192034o616a7408t5d21a3a6729d8c3b@mail.gmail.com>
References: <437FC4C1.7090207@ihmc.us>
	<971536df0511192034o616a7408t5d21a3a6729d8c3b@mail.gmail.com>
Message-ID: <200511200132.25897.narguea@ihmc.us>

That did it.  Thanks.

Nestor
On Saturday 19 November 2005 10:34 pm, you wrote:
> Try this:
>
>    attach(as.list(my.time.series))
>
> On 11/19/05, Nestor Arguea <narguea at ihmc.us> wrote:
> > Is there an attach-like command for time series objects?
> > Thanks in advance,



From samrobertsmith at yahoo.com  Sun Nov 20 10:24:56 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sun, 20 Nov 2005 01:24:56 -0800 (PST)
Subject: [R] transform
Message-ID: <20051120092457.94591.qmail@web30611.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051120/5eab73b1/attachment.pl

From knoblauch at lyon.inserm.fr  Sun Nov 20 12:12:23 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Sun, 20 Nov 2005 10:12:23 -0100 (CET)
Subject: [R]  transform
Message-ID: <49322.82.231.93.240.1132477943.squirrel@webmail.lyon.inserm.fr>


<I have a=c(1,1,4,3,5);
<  then
<  > a
<[1] 1 1 4 3 5
<
<  is there any function to transform a to b:
<  >b
<  > a
<[1] 1 3 4 5

How  about:

unique(sort(a))

or

sort(unique(a))

>  Thanks!


-- 
Ken Knoblauch
Inserm U371
Cerveau et Vision
Dept. of Cognitive Neuroscience
18 avenue du Doyen L??pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.lyon.inserm.fr/371/



From samrobertsmith at yahoo.com  Sun Nov 20 10:42:59 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sun, 20 Nov 2005 01:42:59 -0800 (PST)
Subject: [R] about demo
Message-ID: <20051120094259.53318.qmail@web30610.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051120/7d59de79/attachment.pl

From samrobertsmith at yahoo.com  Sun Nov 20 10:47:37 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Sun, 20 Nov 2005 01:47:37 -0800 (PST)
Subject: [R] transform
In-Reply-To: <49322.82.231.93.240.1132477943.squirrel@webmail.lyon.inserm.fr>
Message-ID: <20051120094738.52056.qmail@web30603.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051120/57ae99f4/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Sun Nov 20 11:17:30 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 20 Nov 2005 10:17:30 -0000 (GMT)
Subject: [R] about demo
In-Reply-To: <20051120094259.53318.qmail@web30610.mail.mud.yahoo.com>
Message-ID: <XFMail.051120101730.Ted.Harding@nessie.mcc.ac.uk>

On 20-Nov-05 Robert wrote:
> Hi,
>   I might have a silly question:
>   when I type demo()
>   I saw:
>   graphics                A show of some of R's graphics capabilities
>   I wonder how to see the demo of graphics?
>   Thanks!

If you type

  demo(graphics)

you will get it.

Someone needs to write a demo for the analysis of voting statistics,
and call it "cracy".

R used to be a bit kinder to absolute beginners:

  $ R
  R : Copyright 2001, The R Development Core Team
  Version 1.2.3  (2001-04-26)
  ...
  Type `demo()' for some demos, ...
  ...

  > demo()
  Use `demo(topic)' where choices for argument `topic' are:
        topics      
  [1,] "graphics"  
  ...


whereas, at least since 1.6.2  (2003-01-10) (the second oldest
R I have around) it does not tell you to use demo(topic).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Nov-05                                       Time: 10:17:27
------------------------------ XFMail ------------------------------



From spencer.graves at pdf.com  Sun Nov 20 17:42:59 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 20 Nov 2005 08:42:59 -0800
Subject: [R] strategies to obtain convergence using nlme
In-Reply-To: <002101c5e553$afd252d0$9f1ad284@BIO041>
References: <002101c5e553$afd252d0$9f1ad284@BIO041>
Message-ID: <4380A793.9050001@pdf.com>

	  A message like "Singular precision matrix in level -1, block 1" 
usuallly means that the paramters have drifted into a region where the 
log(likelihood) is insensitive to changes in some linear combination of 
the parameters.  In other words, the system is overparameterized, at 
least for that case, and nlme is unable to find unique estimates.

	  If it were my problem, I might try to trace through the code line by 
line.  For many functions, simply typing the function name at a command 
prompt will produce a listing of the function.  In this case, however, 
we get the following:

 > nlme
function (model, data = sys.frame(sys.parent()), fixed, random = fixed,
     groups, start, correlation = NULL, weights = NULL, subset,
     method = c("ML", "REML"), na.action = na.fail, naPattern,
     control = list(), verbose = FALSE)
{
     UseMethod("nlme")
}

	  This is not instantly helpful.  To get to the next step, I requested 
the following:

 > methods("nlme")
[1] nlme.formula nlme.nlsList

	  Then typing "nlme.formula" and "nlme.nlsList" at a command prompt 
gave me listing of those two functions.  If the argument "model" has 
class "formula", it uses nlme.formula;  if the class of "model" is 
"nlsList", it uses nlme.nlsList.

	  Hope this helps.
	  spencer graves

Bill Shipley wrote:

> Hello.  I am working on an analysis involving the nonlinear mixed model
> function (nlme) in R.  The data consist of measures of carbon fixation
> by leaves as a function of light intensity and the parametric function
> (standard in this area because it has a biological interpretation) is a
> non-rectangular hyperbola.  I cannot get the nonlinear mixed model
> (nlme) function to converge cleanly. I am hoping that others have found
> strategies for getting starting values that might solve this problem.  I
> have a request (below), some details of the problem, and then a question
> (at the end of this posting).
> 
>  
> 
> Request 1:  Any documents that you can point to would be welcome,
> especially if they involve a non-rectangular hyperbola.  I have already
> searched the archives and have read the book by Pinheiro & Bates (2000)
> and, although they give many useful suggestions, none seem to work in my
> case.  
> 
>  
> 
> Details: 
> 
>  
> 
> In particular, I can obtain convergence for each group (plant)
> separately using either nls or nlsList, but cannot get convergence using
> nlme.
> 
>  
> 
> My function is a non-rectangular hyperbola with 4 parameters (theta, Am,
> alpha, Rd):
> 
>  
> 
> #Nonrectangular hyperbola for photosynthesis
> 
> # myfunct
> 
> (1/(2*theta))*(
> 
> alpha*Irr + Am -sqrt((alpha*Irr+Am)^2-4*alpha*theta*Am*Irr))-Rd
> 
>  
> 
> I have written a self-starting function (NRhyperbola) to provide
> starting values. This self-starting function works.  When I use this
> self-starting function with nlsList using a data set with the grouping
> variable (and after removing a few groups for which there is an
> insufficient range of the independent variable to provide estimates), I
> get convergence and reasonable estimates.  Similarly if I use nls
> separately for every group I obtain reasonable estimates of the 4
> parameters.
> 
>  
> 
> If I then use the fixed effect values from nlsList (i.e. the average
> value of each parameter over the groups) to do a nonlinear mixed model
> (nlme) using the fixed effects estimates from nlsList, I fail to
> converge after 50 iterations and get two types of  warning:
> 
>  
> 
> (1): "NaNs produced in: sqrt(.expr10)"  - this is because the parameter
> values drift into regions in which the sqrt in the function is
> undefined.
> 
>  (2) "Singular precision matrix in level -1, block 1". - I do not know
> what this warning means.
> 
>  
> 
> I have tried many different changes in the starting values and none
> work.
> 
>  
> 
> Question: what does the second warning mean?
> 
>  
> 
> Thanks for any help.
> 
>  
> 
> Bill Shipley
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Nov 20 18:02:50 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 20 Nov 2005 09:02:50 -0800
Subject: [R] repeated values, nlme, correlation structures
In-Reply-To: <43802427.8040604@univ-fcomte.fr>
References: <4374ED11.6010401@univ-fcomte.fr> <437FA274.7050607@pdf.com>
	<43802427.8040604@univ-fcomte.fr>
Message-ID: <4380AC3A.9090000@pdf.com>

ANOTHER CONCRETE SUGGESTION:

	  Have you considered Monte Carlo?  Take your best model (and perhaps 
some plausible alternatives), and simulate data like what you have, but 
retaining the simulated chick identities.  Then analyze the simulated 
data both with and without the chick identities, averaging over the 
nestboxes, as you've done.  This is easy to do in R.

PHILOSOPHY:

	  At a general, conceptual level, nlme and most other "parametric" 
statistical procedures use maximum likelihood.  The likelihood is the 
probability (density) of what we observe, considered as a function of 
the unknown parameters.  With mixed models, we use a marginal 
likelihood, integrating out the individual parameters for all the 
nestboxes and chicks, leaving the "fixed effect" parameters and the 
(co)variance parameters of the random effects.  This leads to a 
generalized least-squares problem, with the (co)variance parameters 
embedded in some way in the residual covariance matrix.

	  This converts the problem to one of understanding and modeling the 
covariance structure of the residuals.  If I've lost the identity of the 
chicks but I've got a good model for the covariance structure of the 
residuals, I think the answer using nestbox averages should be fairly 
close to the answer I'd get if I thought really hard and developed a 
likelihood more accurately suited to the problem.  This will be less 
true with a nonlinear model, and even less true if the number of chicks 
who die before the end of the experiment.  To answer these questions, 
I'd use Monte Carlo, as I suggested above.

	  Best Wishes,
	  Spencer Graves
	
Patrick Giraudoux wrote:

> Spencer Graves a ??crit :
> 
>>       You are concerned that, "using the mean of each age category as 
>> variable leads to a loss of information regarding the variance on the 
>> weight at each age and nestbox."  What information do you think you lose?
> 
> 
> The variance  around the mean weight of each age category. This 
> variation is a priori not considered in the model when using the mean 
> only, and not each value used to compute the mean..
> 
>>
>>       In particular, have you studied the residuals from your fit?  I 
>> would guess that the you probably have heterscedasticity with the 
>> variance of the residuals probably increasing with the age.  Plots of 
>> the absolute residuals might help identify this.  
> 
> 
> Yes, of course. At this stage using a  Continuous AR(1) as Correlation 
> Structure, reduces considerably heteroscedasticity up to quasi-normal.
> 
>> Also, is the number of blue tits in each age constant, or does it 
>> change, e.g., as some of the chicks die?
> 
> 
> Yes, unfortunately, it may happen eventually.
> 
>>
>>       To try to assess how much information I lost (especially if some 
>> of the chicks died), I might plot the weights in each nest box and 
>> connect the dots manually, attempting to assign chick identity to the 
>> individual numbers.  I might do it two different ways, one best fit, 
>> and another "worst plausible".  Then I might try to fit models to 
>> these two "augmented data sets" as if I had the true chick identity.  
>> Then comparing these fits with the one you already have should help 
>> you evaluate what information you lost by using the averages AND give 
>> you a reasonable shot at recovering that information.  If the results 
>> were promising, I might generate more than two sets of assignments, 
>> involving other people in that task.
> 
> 
> OK, should not be that difficult (actually the data were given with 
> pseudo-ID numbers on each chicks and I started with this... until I 
> learned they were corresponding to nothing). I suppose one could go as 
> far as possible with the "worst possible" with random assignements and 
> permutations, and thus comparing the fits.
> 
> Many thanks for the hint. I was really wondering what may mean no answer 
> on the list... Problem not clear enough, trivial solution or real 
> trouble for statisticians with such data? Quite  scaring to a 
> biologist...  Now, I am fixed.
> 
>> If the results were promising, I might generate more than two sets of 
>> assignments, involving other people in that task. 
> 
> 
> Of course if some capable mixed-effect models specialist is interested 
> in having a look to the data set, I can send it off list.
> 
> Many thanks again, Spencer, I can stick on the track, now...
> 
> Best regards,
> 
> Patrick
> 
> 
>>       Bon Chance
>>       Spencer Graves
>>
>> Patrick Giraudoux wrote:
>>
>>> Dear listers,
>>>
>>> My request of last week seems not to have drawn someone's attention. 
>>> Suppose it was not clear enough.
>>>
>>> I am coping with an observational study where people's aim was to fit 
>>> growth curve for a population of young blue tits. For logistic 
>>> reasons, people have not been capable to number each individual, but 
>>> they have a method to assess their age. Thus, nestboxes were visited 
>>> occasionnally, youngs aged and weighted.
>>>
>>> This makes a multilevel data set, with two classification factors:
>>>
>>> - the nestbox (youngs shared the same parents and general feeding 
>>> conditions)
>>> - age in each nestbox (animals from the same nestbox have been 
>>> weighed along time, which likely leads to time correlation)
>>>
>>> Life would have been heaven if individuals were numbered, and thus 
>>> nlme correlation structure implemented in the package be used easy. 
>>> As mentioned above, this could not be the case. In a first approach, 
>>> I actually used the mean weight of the youngs weighed at each age in 
>>> nest boxes for the variable "age", and could get a nice fit with 
>>> "nestbox" as random variable and corCAR1(form=~age|nestbox) as 
>>> covariation structure.
>>>
>>> modm0c<-nlme(pds~Asym/(1+exp((xmid-age)/scal)),
>>>     fixed=list(Asym~1,xmid~1,scal~1),
>>>     random=Asym+xmid~1|nestbox,data=croispulm,
>>>     start=list(fixed=c(10,5,2.2)),
>>>     method="ML",
>>>     corr=corCAR1(form=~age|nestbox)
>>>     )
>>>
>>> Assuming that I did not commited some error in setting model 
>>> parameters (?), this way of doing is not fully satisfying, since 
>>> using the mean of each age category as variable  leads to a  loss of 
>>> information regarding the variance on the weight at each age and 
>>> nestbox.
>>>
>>> My question is: is there a way to handle repeated values per group 
>>> (here several youngs in an age category in each nestbox) in such a case?
>>>
>>> I would really appreciate an answer, even negative...
>>>
>>> Kind regards,
>>>
>>> Patrick
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From patrick.giraudoux at univ-fcomte.fr  Sun Nov 20 18:45:58 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sun, 20 Nov 2005 18:45:58 +0100
Subject: [R] repeated values, nlme, correlation structures
In-Reply-To: <4380AC3A.9090000@pdf.com>
References: <4374ED11.6010401@univ-fcomte.fr> <437FA274.7050607@pdf.com>
	<43802427.8040604@univ-fcomte.fr> <4380AC3A.9090000@pdf.com>
Message-ID: <4380B656.4090801@univ-fcomte.fr>

Looks fine... and at least accessible to my current understanding and 
capacity.  I wonder if this kind of problem/method would not make a pure 
Bayesian very excited (I know one quite obsessional about it)... and 
propose an alternate approach from there (though beyond my own skill)...

Thanks a lot again. Will do like that ASAP (means w.e.).

Patrick

-- 

Department of Environmental Biology
EA3184 usc INRA
University of Franche-Comte
25030 Besancon Cedex
(France)

tel. +33 381 665 745
fax +33 381 665 797
http://lbe.univ-fcomte.fr




Spencer Graves a ??crit :

> ANOTHER CONCRETE SUGGESTION:
>
>       Have you considered Monte Carlo?  Take your best model (and 
> perhaps some plausible alternatives), and simulate data like what you 
> have, but retaining the simulated chick identities.  Then analyze the 
> simulated data both with and without the chick identities, averaging 
> over the nestboxes, as you've done.  This is easy to do in R.
>
> PHILOSOPHY:
>
>       At a general, conceptual level, nlme and most other "parametric" 
> statistical procedures use maximum likelihood.  The likelihood is the 
> probability (density) of what we observe, considered as a function of 
> the unknown parameters.  With mixed models, we use a marginal 
> likelihood, integrating out the individual parameters for all the 
> nestboxes and chicks, leaving the "fixed effect" parameters and the 
> (co)variance parameters of the random effects.  This leads to a 
> generalized least-squares problem, with the (co)variance parameters 
> embedded in some way in the residual covariance matrix.
>
>       This converts the problem to one of understanding and modeling 
> the covariance structure of the residuals.  If I've lost the identity 
> of the chicks but I've got a good model for the covariance structure 
> of the residuals, I think the answer using nestbox averages should be 
> fairly close to the answer I'd get if I thought really hard and 
> developed a likelihood more accurately suited to the problem.  This 
> will be less true with a nonlinear model, and even less true if the 
> number of chicks who die before the end of the experiment.  To answer 
> these questions, I'd use Monte Carlo, as I suggested above.
>
>       Best Wishes,
>       Spencer Graves
>     
> Patrick Giraudoux wrote:
>
>> Spencer Graves a ??crit :
>>
>>>       You are concerned that, "using the mean of each age category 
>>> as variable leads to a loss of information regarding the variance on 
>>> the weight at each age and nestbox."  What information do you think 
>>> you lose?
>>
>>
>>
>> The variance  around the mean weight of each age category. This 
>> variation is a priori not considered in the model when using the mean 
>> only, and not each value used to compute the mean..
>>
>>>
>>>       In particular, have you studied the residuals from your fit?  
>>> I would guess that the you probably have heterscedasticity with the 
>>> variance of the residuals probably increasing with the age.  Plots 
>>> of the absolute residuals might help identify this.  
>>
>>
>>
>> Yes, of course. At this stage using a  Continuous AR(1) as 
>> Correlation Structure, reduces considerably heteroscedasticity up to 
>> quasi-normal.
>>
>>> Also, is the number of blue tits in each age constant, or does it 
>>> change, e.g., as some of the chicks die?
>>
>>
>>
>> Yes, unfortunately, it may happen eventually.
>>
>>>
>>>       To try to assess how much information I lost (especially if 
>>> some of the chicks died), I might plot the weights in each nest box 
>>> and connect the dots manually, attempting to assign chick identity 
>>> to the individual numbers.  I might do it two different ways, one 
>>> best fit, and another "worst plausible".  Then I might try to fit 
>>> models to these two "augmented data sets" as if I had the true chick 
>>> identity.  Then comparing these fits with the one you already have 
>>> should help you evaluate what information you lost by using the 
>>> averages AND give you a reasonable shot at recovering that 
>>> information.  If the results were promising, I might generate more 
>>> than two sets of assignments, involving other people in that task.
>>
>>
>>
>> OK, should not be that difficult (actually the data were given with 
>> pseudo-ID numbers on each chicks and I started with this... until I 
>> learned they were corresponding to nothing). I suppose one could go 
>> as far as possible with the "worst possible" with random assignements 
>> and permutations, and thus comparing the fits.
>>
>> Many thanks for the hint. I was really wondering what may mean no 
>> answer on the list... Problem not clear enough, trivial solution or 
>> real trouble for statisticians with such data? Quite  scaring to a 
>> biologist...  Now, I am fixed.
>>
>>> If the results were promising, I might generate more than two sets 
>>> of assignments, involving other people in that task. 
>>
>>
>>
>> Of course if some capable mixed-effect models specialist is 
>> interested in having a look to the data set, I can send it off list.
>>
>> Many thanks again, Spencer, I can stick on the track, now...
>>
>> Best regards,
>>
>> Patrick
>>
>>
>>>       Bon Chance
>>>       Spencer Graves
>>>
>>> Patrick Giraudoux wrote:
>>>
>>>> Dear listers,
>>>>
>>>> My request of last week seems not to have drawn someone's 
>>>> attention. Suppose it was not clear enough.
>>>>
>>>> I am coping with an observational study where people's aim was to 
>>>> fit growth curve for a population of young blue tits. For logistic 
>>>> reasons, people have not been capable to number each individual, 
>>>> but they have a method to assess their age. Thus, nestboxes were 
>>>> visited occasionnally, youngs aged and weighted.
>>>>
>>>> This makes a multilevel data set, with two classification factors:
>>>>
>>>> - the nestbox (youngs shared the same parents and general feeding 
>>>> conditions)
>>>> - age in each nestbox (animals from the same nestbox have been 
>>>> weighed along time, which likely leads to time correlation)
>>>>
>>>> Life would have been heaven if individuals were numbered, and thus 
>>>> nlme correlation structure implemented in the package be used easy. 
>>>> As mentioned above, this could not be the case. In a first 
>>>> approach, I actually used the mean weight of the youngs weighed at 
>>>> each age in nest boxes for the variable "age", and could get a nice 
>>>> fit with "nestbox" as random variable and 
>>>> corCAR1(form=~age|nestbox) as covariation structure.
>>>>
>>>> modm0c<-nlme(pds~Asym/(1+exp((xmid-age)/scal)),
>>>>     fixed=list(Asym~1,xmid~1,scal~1),
>>>>     random=Asym+xmid~1|nestbox,data=croispulm,
>>>>     start=list(fixed=c(10,5,2.2)),
>>>>     method="ML",
>>>>     corr=corCAR1(form=~age|nestbox)
>>>>     )
>>>>
>>>> Assuming that I did not commited some error in setting model 
>>>> parameters (?), this way of doing is not fully satisfying, since 
>>>> using the mean of each age category as variable  leads to a  loss 
>>>> of information regarding the variance on the weight at each age and 
>>>> nestbox.
>>>>
>>>> My question is: is there a way to handle repeated values per group 
>>>> (here several youngs in an age category in each nestbox) in such a 
>>>> case?
>>>>
>>>> I would really appreciate an answer, even negative...
>>>>
>>>> Kind regards,
>>>>
>>>> Patrick
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>>
>>
>



From spencer.graves at pdf.com  Sun Nov 20 19:19:26 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 20 Nov 2005 10:19:26 -0800
Subject: [R] repeated values, nlme, correlation structures
In-Reply-To: <4380B656.4090801@univ-fcomte.fr>
References: <4374ED11.6010401@univ-fcomte.fr> <437FA274.7050607@pdf.com>
	<43802427.8040604@univ-fcomte.fr> <4380AC3A.9090000@pdf.com>
	<4380B656.4090801@univ-fcomte.fr>
Message-ID: <4380BE2E.6040802@pdf.com>

	  The pure Bayesian could do two things:

	  (1) Modify the nlme code to accept a prior specification, then add 
log(prior) to the log(marginal likelihood), so the function would then 
estimate the Baysian posterior mode, and then output a posterior 
specification in a format compatible with the input.  I have done 
similar things, but not exactly this, and I don't think it would be too 
hard.  The most difficult part would be just selecting a format to use 
as the primary specification of the prior / posterior.

	  (2) Markov Chain Monte Carlo, which is something I have yet to do.

	  Best Wishes,
	  Spencer Graves

Patrick Giraudoux wrote:

> Looks fine... and at least accessible to my current understanding and 
> capacity.  I wonder if this kind of problem/method would not make a pure 
> Bayesian very excited (I know one quite obsessional about it)... and 
> propose an alternate approach from there (though beyond my own skill)...
> 
> Thanks a lot again. Will do like that ASAP (means w.e.).
> 
> Patrick
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From sebugher at usol.com  Sun Nov 20 19:56:07 2005
From: sebugher at usol.com (Susan Bugher)
Date: Sun, 20 Nov 2005 13:56:07 -0500
Subject: [R] A message from the alt.comp.freeware newsgroup re: R
Message-ID: <4380C6C7.9010203@usol.com>

Hello

Your program is a 2006 Pricelessware selection (The best of the best as 
selected by alt.comp.freeware newsgroup participants). For more 
information about Pricelessware see this web page:

http://www.pricelesswarehome.org/2006/about2006PL.php

Your Pricelessware program R is listed here:
http://www.pricelesswarehome.org/2006/PL2006BUSINESS-HOME.php#3479-PW

Pricelessware link and award buttons are available here (should you care 
to use them):
http://www.pricelesswarehome.org/ftp/LinkButtons/2006LinkBtn.gif
http://www.pricelesswarehome.org/ftp/LinkButtons/2006AwardBtn.gif

alt.comp.freeware participants are creating a Pricelessware 2006 CD. It 
will be free (of course). The goals and method of distribution etc. are 
here:
http://www.pricelesswarehome.org/2006/2006PL-CD-Procedures.php

You may contact me by replying to this email or through the 
Pricelessware Links button: mailto:links at pricelesswarehome.org or by 
posting to the alt.comp.freeware newsgroup if you have any questions.

Sincerely yours
Susan Bugher



From AS-maus at gmx.de  Sun Nov 20 20:35:50 2005
From: AS-maus at gmx.de (Antje)
Date: Sun, 20 Nov 2005 20:35:50 +0100
Subject: [R] Using ComputerModern-Fonts with Matplot
Message-ID: <4380D016.2050200@gmx.de>

Hi There,
I am trying to use ComputerModern Fonts with the matplot-command. Is 
that possible and what command do I have to use?

Thanks,
Antje



From bgreen at dyson.brisnet.org.au  Sun Nov 20 21:44:21 2005
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Mon, 21 Nov 2005 06:44:21 +1000
Subject: [R] use of the 'by' command & converting SPSS ANOVA/GLM syntax into
 R syntax
Message-ID: <5.1.0.14.0.20051121062610.019ebd00@pop3.brisnet.org.au>

I have two questions I would appreciate assistance with:

(1)  I believe "by" is the command used to split a file. In the following 
example, "mydat" is the dataframe , "group" the variable I want to split 
the analysis by & "WK1FREQ,WK2FREQ" the variables

attach(mydat)
by (GROUP)
cor.test (WK1FREQ,WK2FREQ)

I have also tried: (by (GROUP) cor.test (WK1FREQ,WK2FREQ))

(2)

I also wanted to run the following analyses, written in SPSS syntax. I have 
searched the R archives and gather I may need to use lm instead of glm, but 
got lost.

UNIANOVA
avfreq BY sex  WITH blik
/METHOD = SSTYPE(3)
/INTERCEPT = INCLUDE
/EMMEANS = TABLES(OVERALL)
/EMMEANS = TABLES(sex)
/PRINT = DESCRIPTIVE ETASQ OPOWER HOMOGENEITY
/CRITERIA = ALPHA(.05)
/DESIGN = blik sex blik*sex .

  GLM
    avmth3fq avfreq BY SEX
   /WSFACTOR = timex 2 Polynomial
   /METHOD = SSTYPE (3)
   /PRINT = DESCRIPTIVE ETASQ TEST(MMATRIX) HOMOGENEITY
   /CRITERIA = ALPHA (.05)
   /WSDESIGN = timex
   /DESIGN = sex .

Any assistance with either of these enquiries is most appreciated.

Bob



From murdoch at stats.uwo.ca  Mon Nov 21 02:06:38 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 20 Nov 2005 20:06:38 -0500
Subject: [R] use of the 'by' command & converting SPSS ANOVA/GLM syntax
 into R syntax
In-Reply-To: <5.1.0.14.0.20051121062610.019ebd00@pop3.brisnet.org.au>
References: <5.1.0.14.0.20051121062610.019ebd00@pop3.brisnet.org.au>
Message-ID: <43811D9E.5050007@stats.uwo.ca>

On 11/20/2005 3:44 PM, Bob Green wrote:
> I have two questions I would appreciate assistance with:
> 
> (1)  I believe "by" is the command used to split a file. In the following 
> example, "mydat" is the dataframe , "group" the variable I want to split 
> the analysis by & "WK1FREQ,WK2FREQ" the variables
> 
> attach(mydat)
> by (GROUP)
> cor.test (WK1FREQ,WK2FREQ)
> 
> I have also tried: (by (GROUP) cor.test (WK1FREQ,WK2FREQ))

I think you want

by(mydat, mydat$GROUP, function(subset) cor.test(subset$WK1FREQ, 
subset$WK2FREQ))


> 
> (2)
> 
> I also wanted to run the following analyses, written in SPSS syntax. I have 
> searched the R archives and gather I may need to use lm instead of glm, but 
> got lost.
> 
> UNIANOVA
> avfreq BY sex  WITH blik
> /METHOD = SSTYPE(3)
> /INTERCEPT = INCLUDE
> /EMMEANS = TABLES(OVERALL)
> /EMMEANS = TABLES(sex)
> /PRINT = DESCRIPTIVE ETASQ OPOWER HOMOGENEITY
> /CRITERIA = ALPHA(.05)
> /DESIGN = blik sex blik*sex .
> 
>   GLM
>     avmth3fq avfreq BY SEX
>    /WSFACTOR = timex 2 Polynomial
>    /METHOD = SSTYPE (3)
>    /PRINT = DESCRIPTIVE ETASQ TEST(MMATRIX) HOMOGENEITY
>    /CRITERIA = ALPHA (.05)
>    /WSDESIGN = timex
>    /DESIGN = sex .

Sorry, I don't know SPSS syntax, so I can't help you with this one.

Duncan Murdoch
> 
> Any assistance with either of these enquiries is most appreciated.
> 
> Bob
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From zgo10 at student.canterbury.ac.nz  Mon Nov 21 03:47:23 2005
From: zgo10 at student.canterbury.ac.nz (Zhaojing Gong)
Date: Mon, 21 Nov 2005 15:47:23 +1300
Subject: [R] On survival curve scale
Message-ID: <4381353B.4090303@student.canterbury.ac.nz>

Hello, everyone,

I was wondering if you can kindly help me to set the scale in the 
plot.survfit, whose default range is (0,1). In my case, I need to set at 
(0.8,1). I have tried "plot.window" with ylim=(0.8,1) but it did not work.

Many thanks in advance.


-- 
Zhaojing Gong
Ph D. student in Statistics
The Department of Mathematics and Statistics
University of Canterbury
New Zealand



From maustin at amgen.com  Mon Nov 21 04:01:05 2005
From: maustin at amgen.com (Austin, Matt)
Date: Sun, 20 Nov 2005 19:01:05 -0800
Subject: [R] On survival curve scale
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD4A0@teal-exch.amgen.com>

In the helpfile, look at the ymin argument to the plot method for survfit.

--Matt

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Zhaojing Gong
> Sent: Sunday, November 20, 2005 6:47 PM
> To: R-help at stat.math.ethz.ch
> Cc: Zhaojing Gong
> Subject: [R] On survival curve scale
> 
> 
> Hello, everyone,
> 
> I was wondering if you can kindly help me to set the scale in the 
> plot.survfit, whose default range is (0,1). In my case, I 
> need to set at 
> (0.8,1). I have tried "plot.window" with ylim=(0.8,1) but it 
> did not work.
> 
> Many thanks in advance.
> 
> 
> -- 
> Zhaojing Gong
> Ph D. student in Statistics
> The Department of Mathematics and Statistics
> University of Canterbury
> New Zealand
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From vivek.satsangi at gmail.com  Mon Nov 21 05:28:14 2005
From: vivek.satsangi at gmail.com (Vivek Satsangi)
Date: Sun, 20 Nov 2005 23:28:14 -0500
Subject: [R] Cacheing in read.table/ attached data?
Message-ID: <bcb171920511202028j39310fa1t6c2e6f4cd34f5697@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051120/d18049d0/attachment.pl

From jpritikin at pobox.com  Mon Nov 21 05:49:21 2005
From: jpritikin at pobox.com (Joshua N Pritikin)
Date: Mon, 21 Nov 2005 10:19:21 +0530
Subject: [R] feedback on book recommendations
Message-ID: <20051121044921.GG6437@always.joy.eth.net>

I asked this mailing list for book recommendations a few weeks ago.

One of the books I selected was "Introductory Statistics with R" by
Peter Dalgaard.  I am embarrassed to say that this book was too terse
for me. I wasn't able to get very far with it.  However, I was
fortunate enough to stumble upon "Statistical Reasoning" by Bruce King
and Edward Minium.  King & Minium progress step-by-step from basic
concepts to analysis of variance and some non-parametric methods without
any sophisticated math.  The presentation is so simple that the book
could probably be used as a textbook for a high school class.  Anyway,
after finishing "Statistical Reasoning," I found that Dalgaard covers
many of the same topics except more in the style of an R reference
manual.  So now I'm happy with both of the books.

As for the book list, I suggest the addition of some book like
"Statistical Reasoning" even though there is no mention R (even a
calculator is not required!).  This book helped me grok the basics (what
is the central limit theorem? ;-).

-- 
Make April 15 just another day, visit http://fairtax.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051121/1e0985d1/attachment.bin

From Friedrich.Leisch at tuwien.ac.at  Sun Nov 20 15:52:53 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Sun, 20 Nov 2005 15:52:53 +0100
Subject: [R] SWeave - can I see output in the source?
In-Reply-To: <437F3C26.1020604@stats.uwo.ca>
References: <437E2240.60304@stats.uwo.ca>
	<17279.2278.290839.316330@celebrian.ci.tuwien.ac.at>
	<437F3C26.1020604@stats.uwo.ca>
Message-ID: <17280.36293.427937.799971@celebrian.ci.tuwien.ac.at>

>>>>> On Sat, 19 Nov 2005 09:52:22 -0500,
>>>>> Duncan Murdoch (DM) wrote:

  > On 11/19/2005 6:13 AM, Friedrich.Leisch at tuwien.ac.at wrote:
  >>>>>>> On Fri, 18 Nov 2005 13:49:36 -0500,
  >>>>>>> Duncan Murdoch (DM) wrote:
  >> 
  >> 
  >> > I'm working on a Latex document with lots of R code in it, so naturally 
  >> > enough it would be a good idea to use SWeave.  But then I don't get to 
  >> > see the output as I'm editing.
  >> 
  >> > Or do I?  Is there a tool to process a .Rnw file and incorporate the 
  >> > output from the commands into it (in a form that is not used for 
  >> > producing the output .tex file, but which is updated each time I process 
  >> > the file)?
  >> 
  >> I'm not sure if I understand the question correctly, but if you edit
  >> Sweave files in Emacs using ESS you can send the code lines to a
  >> running R process, and there you see the output. At least that's how I
  >> write my Sweave files.
  >> 
  >> When I want to see all at once I typically do a tangle & source.

  > That's bad news for me, because I'm allergic to Emacs.

Well, the origin of Sweave is that I found Tony Rossini's ESS-noweb
mode for Emacs really neat and started to explre what you can do with
it ...


  > What I had in 
  > mind was this:  In my .Rnw file, I enter:

  > <<echo=true>>=
  > 1:3
  > @

  > Then I pass it to some tool, which modifies the .Rnw file, changing it 
  > to something like

  > <<echo=true>>=
  > 1:3
  > @
  > % > 1:3
  > % [1] 1 2 3

  > (My editor will notice that the tool has changed the file and offer to 
  > load the new version at this point.  I think that's a reasonably common 
  > editor option.)

  > Then I can see what I'm writing about when I describe the results.  If I 
  > later come along and edit the source to change it to

  > <<echo=true>>=
  > 1:4
  > @
  > % > 1:3
  > % [1] 1 2 3

  > then the next time I run the file through the tool it will delete the 
  > stale output and modify my file to look like this:

  > <<echo=true>>=
  > 1:4
  > @
  > % > 1:4
  > % [1] 1 2 3 4

Shouldn't be too hard to code (apart from the fact that you cannot
insert figures as easily as text), but I personally am unlikely to do
it because it is completely orthogonal to my style of interacting with
Sweave documents. When I edit them I always want them to be connected
to a running R process to easily try out modifications etc.

Ad emacs vs other editors: well, we "just" would need another editor
that is connected to a running R process, i.e., can easily send code
lines to R, and there are a couple of those. Emacs will know when you
are in a code chunk and when you are in a doc chunk, and lets you
execute code only in those cases (including sending all of the current
chunk to R). It also does syntax highlighting (R vs LaTeX) in
dependence on the 

But if the user knows when to press the corresponding button a simple
solution might be to simply turn of the R connection for LaTeX files
in another editor than Emacs. 



  > Besides the advantage that I had in mind (being able to see the output 
  > as I'm editing, and being confident that it will match the output in the 
  > final document), this will mean that I'll have a versionable record of 
  > what the output looked like (so I'll be alerted to changes in it caused 
  > by updates to R or some package I'm using).  I could get this by saving 
  > the .tex output, but to me this seems preferable.  But I don't have a 
  > lot of experience with SWeave yet, so maybe there's a better workflow.


That is something I have alraqdy thought a little bit about, i.e., to
have mechanisms for easily comparing the processed versions of Sweave
files and see if something has changed. The tricky bit are (as always
with regression tests) the figures, and I have no simple solution for
that yet. Started playing with Paul's GraphicsQC package, but found no
time for serious coding yet.

Without checking figures a simple diff on the .tex files so far worked
fine for me ...


Best,
Fritz



From bbaker at TNC.ORG  Mon Nov 21 07:48:47 2005
From: bbaker at TNC.ORG (Barry Baker)
Date: Mon, 21 Nov 2005 14:48:47 +0800
Subject: [R] Largest allowable matrix
Message-ID: <BFA78ECF.95E%bbaker@tnc.org>

Hello,

I am a new R user and have two datasets that I would like to analyze.  The
first is (2409222 x 17) and the other is (21682998 x 17). Is this possible
in R?  If not then what is the maximum number of rows and columns or number
of elements that R can handle?

Thanks in advance,
Barry
_________________________
Barry Baker, Ph.D.
Global Climate Change Initiative
The Nature Conservancy
2424 Spruce St., Suite 100
Boulder, CO 80302

Tel: (303)-541-0322
Fax: (303)-449-4328

http://nature.org/tncscience/scientists/misc/baker.html



From ligges at statistik.uni-dortmund.de  Mon Nov 21 08:29:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 21 Nov 2005 08:29:47 +0100
Subject: [R] Largest allowable matrix
In-Reply-To: <BFA78ECF.95E%bbaker@tnc.org>
References: <BFA78ECF.95E%bbaker@tnc.org>
Message-ID: <4381776B.6040707@statistik.uni-dortmund.de>

Barry Baker wrote:

> Hello,
> 
> I am a new R user and have two datasets that I would like to analyze.  The
> first is (2409222 x 17) and the other is (21682998 x 17). Is this possible
> in R?  If not then what is the maximum number of rows and columns or number
> of elements that R can handle?


The number of columns and rows is not a problem here, but you will need
21682998 * 17 * 4 bytes to store the latter matrix (assuming floats) in 
memory, that is 1406.139 Mb.
In order to do something sensible with the data, you need *at least* 
twice the amount of RAM, hence at least 3Gb.

Uwe Ligges


> Thanks in advance,
> Barry
> _________________________
> Barry Baker, Ph.D.
> Global Climate Change Initiative
> The Nature Conservancy
> 2424 Spruce St., Suite 100
> Boulder, CO 80302
> 
> Tel: (303)-541-0322
> Fax: (303)-449-4328
> 
> http://nature.org/tncscience/scientists/misc/baker.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Nov 21 08:41:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Nov 2005 07:41:32 +0000 (GMT)
Subject: [R] Cacheing in read.table/ attached data?
In-Reply-To: <bcb171920511202028j39310fa1t6c2e6f4cd34f5697@mail.gmail.com>
References: <bcb171920511202028j39310fa1t6c2e6f4cd34f5697@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0511210730530.26749@gannet.stats>

When you re-create a data frame, you do not change the attached version.
This is not caching, but the documented behaviour of attach().  The help 
page says

      The database is not actually attached.  Rather, a new environment
      is created on the search path and the elements of a list
      (including columns of a dataframe) or objects in a save file are
      _copied_ into the new environment.

However, the code you give does not actually attach SG before using it, so 
I guess you did not run exactly this code.  (You also need 
library(fBasics).)

Try using

with(SG, circlesPlot(A10Holdings,A3Yr, size=NetAssets))

Then if you want to drop points, use some means (e.g. identify() on a 
normal plot) to find which they are, put their indices in variable 'drop' 
and use

with(SG[-drop, ], circlesPlot(A10Holdings,A3Yr, size=NetAssets))


On Sun, 20 Nov 2005, Vivek Satsangi wrote:

> Disclaimer/Apology: I am an R newbie
>
> I am seeing some behaviour that seems to me to be the result of some
> cacheing going on at some level, and perhaps this is expected behaviour. I
> would just like to understand the basic rules.
>
> What I have is a file with some data. I read it in and then do a summary on
> the resulting dataframe. I find the some values are completely outside the
> expected range, these value need to be dropped from further analysis as
> erroneous observations (yes, I apologize to the purists in advance :-) ).
>
> If I do this and read the file again, then circlesPlot (from fBasics) two of
> the columns in the data, then the plot is not updated. The outlier point is
> still there. However, when I detach and reattach the dataframe, it seems to
> work okay. For example,
> # Plot has the outlier point in it.
> # Edit the file, commenting out the outlier line, save, then...
>> SG <- read.table
> ("c:/Vivek/MFC/Data/SG/combinedSG.tdf",header=TRUE,sep="\t")
>> SGm2 <- lm(A3Yr ~ A10Holdings, data=SG)
>> circlesPlot(A10Holdings,A3Yr, size=NetAssets)
>> abline(coef(SGm2)) # Put the regression line on the plot
>> SG <- read.table
> ("c:/Vivek/MFC/Data/SG/combinedSG.tdf",header=TRUE,sep="\t")
>> summary(SG) #Outlier does not show in the summary
>> circlesPlot(A10Holdings,A3Yr, size=NetAssets) # ... But Plot still has the
> outlier
>> detach(SG)
>> attach(SG)
>> circlesPlot(A10Holdings,A3Yr, size=NetAssets) # Outlier is gone from the
> plot
>
> So, here are my questions:
> 1. Is there a simpler / more idiomatic way in R, than commenting out the
> data in the data file to exclude some outliers in the data (i.e. to do data
> trimming). In EViews this is done by setting the sample.
> 2. Is the "flushing" of the cache happening as a result of the
> detach/attach, or some other reason?
>
> Thanks for any help,
>
> Vivek Satsangi
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do: not HTML is requested.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Nov 21 08:46:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Nov 2005 07:46:14 +0000 (GMT)
Subject: [R] Largest allowable matrix
In-Reply-To: <4381776B.6040707@statistik.uni-dortmund.de>
References: <BFA78ECF.95E%bbaker@tnc.org>
	<4381776B.6040707@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0511210743360.26749@gannet.stats>

On Mon, 21 Nov 2005, Uwe Ligges wrote:

> Barry Baker wrote:
>
>> Hello,
>>
>> I am a new R user and have two datasets that I would like to analyze.  The
>> first is (2409222 x 17) and the other is (21682998 x 17). Is this possible
>> in R?  If not then what is the maximum number of rows and columns or number
>> of elements that R can handle?
>
>
> The number of columns and rows is not a problem here, but you will need
> 21682998 * 17 * 4 bytes to store the latter matrix (assuming floats) in
> memory, that is 1406.139 Mb.

R does not use floats internally.  So unless these are integers/logicals 
you are going to need twice that,

> In order to do something sensible with the data, you need *at least*
> twice the amount of RAM, hence at least 3Gb.

Here I think the issue is rather virtual memory and address space.  You 
will need a 64-bit OS to do anything with this object.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Nov 21 08:50:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Nov 2005 07:50:17 +0000 (GMT)
Subject: [R] Using ComputerModern-Fonts with Matplot
In-Reply-To: <4380D016.2050200@gmx.de>
References: <4380D016.2050200@gmx.de>
Message-ID: <Pine.LNX.4.61.0511210746260.26749@gannet.stats>

On Sun, 20 Nov 2005, Antje wrote:

> I am trying to use ComputerModern Fonts with the matplot-command. Is
> that possible and what command do I have to use?

Those are (normally) postscript type1 fonts, so you can do this on the 
postscript() device.  See ?postscript.

If you have them set up in other ways (e.g. for X11) it would be possible 
to use them there -- see the help pages for the appropriate device.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From francoisromain at free.fr  Mon Nov 21 09:12:13 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 21 Nov 2005 09:12:13 +0100
Subject: [R] new article on R at oreillynet.com
In-Reply-To: <20051119154726.51816.qmail@web52305.mail.yahoo.com>
References: <20051119154726.51816.qmail@web52305.mail.yahoo.com>
Message-ID: <4381815D.8010804@free.fr>

Le 19.11.2005 16:47, Kevin Farnham a ??crit :

>An article I wrote that provides a basic introduction to R has
>been published on Oreillynet.com. The article is titled
>"Analyzing Statistics with GNU/R". Here is the link:
>
>http://www.onlamp.com/pub/a/onlamp/2005/11/17/r_for_statistics.html
>
>Please feel free to post comments or interesting basic R scripts
>at the end of the article. 
>
>Kevin Farnham
>  
>
Hi Kevin,

I think you forgot to use dev.off() when producing output graphic files.
For example i don't think the last example before the conclusion works.

Regards,

Romain


-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From r.hankin at noc.soton.ac.uk  Mon Nov 21 09:16:39 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 21 Nov 2005 08:16:39 +0000
Subject: [R] force apply() to return a list
Message-ID: <4CB7CB7A-6750-4943-AB1C-AB6D0A61A39C@soc.soton.ac.uk>


Hi

I have a function f() that I want to apply() to a matrix.  I want the
answer to be a list.

However, sometimes all the vectors returned by f() are the same length
and apply() returns a matrix.  I do not want this.

Hi

How do I force apply() to consistently return a list?
Toy example follows.

R> f <- function(x){1:max(x[1],4)}
R> a1 <- cbind(1:5,5:1)
R> apply(a1,1,f)
[[1]]
[1] 1 2 3 4

[[2]]
[1] 1 2 3 4

[[3]]
[1] 1 2 3 4

[[4]]
[1] 1 2 3 4

[[5]]
[1] 1 2 3 4 5

list returned: desired behaviour.  Now try the
same but with a different matrix from a1:


R> a2 <- cbind(1:3,3:1)
R> apply(a2,1,f)
      [,1] [,2] [,3]
[1,]    1    1    1
[2,]    2    2    2
[3,]    3    3    3
[4,]    4    4    4


matrix returned: this is undesired behaviour (in my application, I
pass the list to do.call()).  How do I force apply() to behave
consistently and return a list, irrespectively of the length of
vectors returned by f()?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From francoisromain at free.fr  Mon Nov 21 09:23:29 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 21 Nov 2005 09:23:29 +0100
Subject: [R] new article on R at oreillynet.com
In-Reply-To: <4381815D.8010804@free.fr>
References: <20051119154726.51816.qmail@web52305.mail.yahoo.com>
	<4381815D.8010804@free.fr>
Message-ID: <43818401.9000606@free.fr>

Le 21.11.2005 09:12, Romain Francois a ??crit :

>Le 19.11.2005 16:47, Kevin Farnham a ??crit :
>
>  
>
>>An article I wrote that provides a basic introduction to R has
>>been published on Oreillynet.com. The article is titled
>>"Analyzing Statistics with GNU/R". Here is the link:
>>
>>http://www.onlamp.com/pub/a/onlamp/2005/11/17/r_for_statistics.html
>>
>>Please feel free to post comments or interesting basic R scripts
>>at the end of the article. 
>>
>>Kevin Farnham
>> 
>>
>>    
>>
>Hi Kevin,
>
>I think you forgot to use dev.off() when producing output graphic files.
>For example i don't think the last example before the conclusion works.
>
>Regards,
>
>Romain
>
>  
>
Hi again,

I shouldn't post anything before the third coffea.
That do work (maybe q() close the device) but I would still advise to 
call dev.off().

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From dimitris.rizopoulos at med.kuleuven.be  Mon Nov 21 09:37:42 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 21 Nov 2005 09:37:42 +0100
Subject: [R] force apply() to return a list
References: <4CB7CB7A-6750-4943-AB1C-AB6D0A61A39C@soc.soton.ac.uk>
Message-ID: <015c01c5ee76$d6f24380$0540210a@www.domain>

a solution is the following

f <- function(x){list(1:max(x[1], 4))}
a2 <- cbind(1:3, 3:1)
out <- lapply(apply(a2, 1, f), "[[", 1)


I hope it helps.

Best,
Dimitris


----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Robin Hankin" <r.hankin at noc.soton.ac.uk>
To: "RHelp" <r-help at stat.math.ethz.ch>
Sent: Monday, November 21, 2005 9:16 AM
Subject: [R] force apply() to return a list


>
> Hi
>
> I have a function f() that I want to apply() to a matrix.  I want 
> the
> answer to be a list.
>
> However, sometimes all the vectors returned by f() are the same 
> length
> and apply() returns a matrix.  I do not want this.
>
> Hi
>
> How do I force apply() to consistently return a list?
> Toy example follows.
>
> R> f <- function(x){1:max(x[1],4)}
> R> a1 <- cbind(1:5,5:1)
> R> apply(a1,1,f)
> [[1]]
> [1] 1 2 3 4
>
> [[2]]
> [1] 1 2 3 4
>
> [[3]]
> [1] 1 2 3 4
>
> [[4]]
> [1] 1 2 3 4
>
> [[5]]
> [1] 1 2 3 4 5
>
> list returned: desired behaviour.  Now try the
> same but with a different matrix from a1:
>
>
> R> a2 <- cbind(1:3,3:1)
> R> apply(a2,1,f)
>      [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    2    2    2
> [3,]    3    3    3
> [4,]    4    4    4
>
>
> matrix returned: this is undesired behaviour (in my application, I
> pass the list to do.call()).  How do I force apply() to behave
> consistently and return a list, irrespectively of the length of
> vectors returned by f()?
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From r.hankin at noc.soton.ac.uk  Mon Nov 21 09:40:19 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 21 Nov 2005 08:40:19 +0000
Subject: [R] force apply() to return a list
In-Reply-To: <015c01c5ee76$d6f24380$0540210a@www.domain>
References: <4CB7CB7A-6750-4943-AB1C-AB6D0A61A39C@soc.soton.ac.uk>
	<015c01c5ee76$d6f24380$0540210a@www.domain>
Message-ID: <E34BC00F-1FB6-40AF-B7FF-A9303E1A2D04@soc.soton.ac.uk>

Hi Dimitris


On 21 Nov 2005, at 08:37, Dimitris Rizopoulos wrote:
> out <- lapply(apply(a2, 1, f), "[[", 1)


thanks for this, but it doesn't quite do what I want:


 > f <- function(x){1:max(x[1],4)}
 > lapply(apply(cbind(1:5,5:1), 1, f), "[[", 1)
[[1]]
[1] 1

[[2]]
[1] 1

[[3]]
[1] 1

[[4]]
[1] 1

[[5]]
[1] 1

 >

I want

> [[1]]
> [1] 1 2 3 4
>
> [[2]]
> [1] 1 2 3 4
>
> [[3]]
> [1] 1 2 3 4
>
> [[4]]
> [1] 1 2 3 4
>
> [[5]]
> [1] 1 2 3 4 5



best wishes

Robin




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From dimitris.rizopoulos at med.kuleuven.be  Mon Nov 21 09:52:54 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 21 Nov 2005 09:52:54 +0100
Subject: [R] force apply() to return a list
References: <4CB7CB7A-6750-4943-AB1C-AB6D0A61A39C@soc.soton.ac.uk>
	<015c01c5ee76$d6f24380$0540210a@www.domain>
	<E34BC00F-1FB6-40AF-B7FF-A9303E1A2D04@soc.soton.ac.uk>
Message-ID: <016b01c5ee78$f679c9b0$0540210a@www.domain>

I've also changed the f() function, i.e.,

f <- function(x){list(1:max(x[1], 4))}


I hope it'll work now.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Robin Hankin" <r.hankin at noc.soton.ac.uk>
To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
Cc: "Robin Hankin" <r.hankin at noc.soton.ac.uk>; 
<r-help at stat.math.ethz.ch>
Sent: Monday, November 21, 2005 9:40 AM
Subject: Re: [R] force apply() to return a list


> Hi Dimitris
>
>
> On 21 Nov 2005, at 08:37, Dimitris Rizopoulos wrote:
>> out <- lapply(apply(a2, 1, f), "[[", 1)
>
>
> thanks for this, but it doesn't quite do what I want:
>
>
> > f <- function(x){1:max(x[1],4)}
> > lapply(apply(cbind(1:5,5:1), 1, f), "[[", 1)
> [[1]]
> [1] 1
>
> [[2]]
> [1] 1
>
> [[3]]
> [1] 1
>
> [[4]]
> [1] 1
>
> [[5]]
> [1] 1
>
> >
>
> I want
>
>> [[1]]
>> [1] 1 2 3 4
>>
>> [[2]]
>> [1] 1 2 3 4
>>
>> [[3]]
>> [1] 1 2 3 4
>>
>> [[4]]
>> [1] 1 2 3 4
>>
>> [[5]]
>> [1] 1 2 3 4 5
>
>
>
> best wishes
>
> Robin
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From spencer.graves at pdf.com  Sat Nov 19 23:08:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 19 Nov 2005 14:08:52 -0800
Subject: [R] repeated values, nlme, correlation structures
In-Reply-To: <4374ED11.6010401@univ-fcomte.fr>
References: <4374ED11.6010401@univ-fcomte.fr>
Message-ID: <437FA274.7050607@pdf.com>

	  You are concerned that, "using the mean of each age category as 
variable leads to a loss of information regarding the variance on the 
weight at each age and nestbox."  What information do you think you lose?

	  In particular, have you studied the residuals from your fit?  I would 
guess that the you probably have heterscedasticity with the variance of 
the residuals probably increasing with the age.  Plots of the absolute 
residuals might help identify this.  Also, is the number of blue tits in 
each age constant, or does it change, e.g., as some of the chicks die?

	  To try to assess how much information I lost (especially if some of 
the chicks died), I might plot the weights in each nest box and connect 
the dots manually, attempting to assign chick identity to the individual 
numbers.  I might do it two different ways, one best fit, and another 
"worst plausible".  Then I might try to fit models to these two 
"augmented data sets" as if I had the true chick identity.  Then 
comparing these fits with the one you already have should help you 
evaluate what information you lost by using the averages AND give you a 
reasonable shot at recovering that information.  If the results were 
promising, I might generate more than two sets of assignments, involving 
other people in that task.

	  Bon Chance
	  Spencer Graves

Patrick Giraudoux wrote:

> Dear listers,
> 
> My request of last week seems not to have drawn someone's attention. 
> Suppose it was not clear enough.
> 
> I am coping with an observational study where people's aim was to fit 
> growth curve for a population of young blue tits. For logistic reasons, 
> people have not been capable to number each individual, but they have a 
> method to assess their age. Thus, nestboxes were visited occasionnally, 
> youngs aged and weighted.
> 
> This makes a multilevel data set, with two classification factors:
> 
> - the nestbox (youngs shared the same parents and general feeding 
> conditions)
> - age in each nestbox (animals from the same nestbox have been weighed 
> along time, which likely leads to time correlation)
> 
> Life would have been heaven if individuals were numbered, and thus nlme 
> correlation structure implemented in the package be used easy. As 
> mentioned above, this could not be the case. In a first approach, I 
> actually used the mean weight of the youngs weighed at each age in nest 
> boxes for the variable "age", and could get a nice fit with "nestbox" as 
> random variable and corCAR1(form=~age|nestbox) as covariation structure.
> 
> modm0c<-nlme(pds~Asym/(1+exp((xmid-age)/scal)),
>     fixed=list(Asym~1,xmid~1,scal~1),
>     random=Asym+xmid~1|nestbox,data=croispulm,
>     start=list(fixed=c(10,5,2.2)),
>     method="ML",
>     corr=corCAR1(form=~age|nestbox)
>     )
> 
> Assuming that I did not commited some error in setting model parameters 
> (?), this way of doing is not fully satisfying, since using the mean of 
> each age category as variable  leads to a  loss of information regarding 
> the variance on the weight at each age and nestbox.
> 
> My question is: is there a way to handle repeated values per group (here 
> several youngs in an age category in each nestbox) in such a case?
> 
> I would really appreciate an answer, even negative...
> 
> Kind regards,
> 
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From r.hankin at noc.soton.ac.uk  Mon Nov 21 10:00:42 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 21 Nov 2005 09:00:42 +0000
Subject: [R] force apply() to return a list
In-Reply-To: <016b01c5ee78$f679c9b0$0540210a@www.domain>
References: <4CB7CB7A-6750-4943-AB1C-AB6D0A61A39C@soc.soton.ac.uk>
	<015c01c5ee76$d6f24380$0540210a@www.domain>
	<E34BC00F-1FB6-40AF-B7FF-A9303E1A2D04@soc.soton.ac.uk>
	<016b01c5ee78$f679c9b0$0540210a@www.domain>
Message-ID: <FB06D3BB-A32A-4462-8773-A4A3D90995E2@soc.soton.ac.uk>

Hi again Dimitris



On 21 Nov 2005, at 08:52, Dimitris Rizopoulos wrote:

> I've also changed the f() function, i.e.,

f <- function(x){list(1:max(x[1], 4))}
lapply(apply(cbind(1:5,5:1), 1, f), "[[", 1)
>
>


oops, I missed that.  Works fine now, with both cases, and I
can use this.

Thanks!

Still, it's a little disconcerting that apply() as generally used
can return a list 99.9% of the time and a matrix the other 0.1%.
It took me a long time to track this down when debugging my
code the other night.

Does anyone else agree?  Would it be possible to add an
argument such as force.list (with default FALSE) to apply()
that makes apply() return a list under all circumstances?

Also, adding Dimitris's excellent workaround to apply()'s manpage
would be helpful.

best wishes

rksh


> I hope it'll work now.
>
> Best,
> Dimitris
>
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ramos.beatriz at gmail.com  Mon Nov 21 11:20:48 2005
From: ramos.beatriz at gmail.com (Beatriz)
Date: Mon, 21 Nov 2005 11:20:48 +0100
Subject: [R] How to run R in batch mode
In-Reply-To: <437E0B5A.1030003@stats.uwo.ca>
References: <437DF13E.4090502@gmail.com> <437DF60B.1040802@stats.uwo.ca>
	<437E0096.4060700@gmail.com> <437E0B5A.1030003@stats.uwo.ca>
Message-ID: <43819F80.9080307@gmail.com>

 "system("R CMD BATCH test.R",intern=TRUE)"   command   works!!!!!

thanks to everybody

thank you kindly



Duncan Murdoch wrote:

> On 11/18/2005 11:25 AM, Beatriz wrote:
>
>> I write
>> R CMD BATCH test.R
>> in my R console
>>
>> I have send you an image (RunBatch.jpg) of my console and the 
>> "test.R" file
>
>
> I don't think the jpg made it to R-help, but I saw it.  You tried to 
> run R CMD BATCH test.R from within R.  That's meant to be a system 
> command.
>
> Since you're running in Windows, you should open a command shell.  One 
> way to do that is to choose "Run..." from the Start Menu, and enter 
> "cmd".  You'll get a black command shell window.
>
> If your path is set properly so that you can run R from there, then
> R CMD BATCH test.R should work.
>
> There is a way to do this from within R:  run
>
> > system("R CMD BATCH test.R",intern=TRUE)
>
> This still depends on your path being set correctly to find R.
>
> Duncan Murdoch
>
>
>
>



From Bernhard_Pfaff at fra.invesco.com  Mon Nov 21 11:21:31 2005
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Mon, 21 Nov 2005 10:21:31 -0000
Subject: [R] cointegration rank
Message-ID: <25D1C2585277D311B9A20000F6CCC71B07510A07@DEFRAEX02>

Dear R - helpers,

I am using the urca package to estimate cointegration relations, and I
would be really grateful if somebody could help me with this questions:

After estimating the unrestriced VAR with "ca.jo" I would like to impose
the rank restriction (for example rank = 1) and then obtain the
restricted estimate of PI to be utilized to estimate the VECM model.

Is it possible? 

It seems to me that the function "cajools" estimates the VECM without
the restrictions. Did I miss something? How is it possible to impose
them?

Thanks a lot in advance!

Carlo


Hello Carlo,

you can achieve this, by calculating your desired PI-matrix by hand, given
the slots 'V' and 'W' of your ca.jo object and then execute a restricted
OLS-estimation, if I understand your goal correctly. 
Please, bear in mind the non-uniqueness of the factorization of the
PI-matrix by doing so.

HTH,
Bernhard    


______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
-------------- next part --------------
*****************************************************************
Confidentiality Note: The information contained in this message,
and any attachments, may contain confidential and/or privileged
material. It is intended solely for the person(s) or entity to
which it is addressed. Any review, retransmission, dissemination,
or taking of any action in reliance upon this information by
persons or entities other than the intended recipient(s) is
prohibited. If you received this in error, please contact the
sender and delete the material from any computer.
*****************************************************************

From Dominik.Sydler at eawag.ch  Mon Nov 21 11:36:24 2005
From: Dominik.Sydler at eawag.ch (Sydler, Dominik)
Date: Mon, 21 Nov 2005 11:36:24 +0100
Subject: [R] PNG-import into R
Message-ID: <59484F5CC089B4499DDAC9503DA0BC6706A7A0@EA-MAIL.eawag.wroot.emp-eaw.ch>

Hi there

I'm looking for a function to read PNG-bitmap-images from a file into R.
I only found:
 - the pixmap-package which cannot import png or similar formats
 - the rimage-package which can only import lossy jpeg-images (the
convertion from png to jpeg modifies the data!)

Is there any possibility to read PNG-files?

Thanks for any help
         Dominic Sydler



From gregor.gorjanc at bfro.uni-lj.si  Mon Nov 21 11:56:48 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 21 Nov 2005 11:56:48 +0100
Subject: [R] Is there anything like a write.fwf() or possibility to print a
 data.frame without rownames?
Message-ID: <4381A7F0.3080201@bfro.uni-lj.si>

Dear R users,

R has read.fwf() function, however I would need write.fwf. I know other
write.* functions, but I need fixed width format of data, which I would
like to export from R. I tried to use:

- write.table, but I can not control alignment of columns

- write.matrix from MASS, but columns are to wide

I came to this option, which is very neat:

# tmp is data.frame

sink(file = file)
print(tmp)
sink()

This works very nice, but I would like to get rid of rownames, which
are always printed.

Another not so important issue is width of printed columns. I presume
this is determined by max(length column name, max(length of "values" in
a column)) but sometimes it would be usefull to control width of columns
also.

Can someone help me with this issue?

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From fezzi at stat.unibo.it  Mon Nov 21 12:14:50 2005
From: fezzi at stat.unibo.it (Carlo Fezzi)
Date: Mon, 21 Nov 2005 12:14:50 +0100 (CET)
Subject: [R] cointegration rank
In-Reply-To: <25D1C2585277D311B9A20000F6CCC71B07510A07@DEFRAEX02>
References: <25D1C2585277D311B9A20000F6CCC71B07510A07@DEFRAEX02>
Message-ID: <1531206.1132571690935.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Thanks a lot!

I have another question on "cointegration", so I will go on this post.

Is it possible to estimate a cointegration with some exogenous
explanatory variables? Since, after testing for exogeneity, I would like
to re-estimate the relation keeping some of the previous endogenous as
exogenous.

Many thanks!

Carlo



On Nov 21, 2005 11:21 AM, "Pfaff, Bernhard Dr."
<Bernhard_Pfaff at fra.invesco.com> wrote:

> Dear R - helpers,
> 
> I am using the urca package to estimate cointegration relations, and I
> would be really grateful if somebody could help me with this
> questions:
> 
> After estimating the unrestriced VAR with "ca.jo" I would like to
> impose
> the rank restriction (for example rank = 1) and then obtain the
> restricted estimate of PI to be utilized to estimate the VECM model.
> 
> Is it possible? 
> 
> It seems to me that the function "cajools" estimates the VECM without
> the restrictions. Did I miss something? How is it possible to impose
> them?
> 
> Thanks a lot in advance!
> 
> Carlo
> 
> 
> Hello Carlo,
> 
> you can achieve this, by calculating your desired PI-matrix by hand,
> given
> the slots 'V' and 'W' of your ca.jo object and then execute a
> restricted
> OLS-estimation, if I understand your goal correctly. 
> Please, bear in mind the non-uniqueness of the factorization of the
> PI-matrix by doing so.
> 
> HTH,
> Bernhard    
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From Jan.Verbesselt at biw.kuleuven.be  Mon Nov 21 12:28:35 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Mon, 21 Nov 2005 12:28:35 +0100
Subject: [R] Howto? plot legend with no line behind the points
Message-ID: <1132572515.4381af6394a80@webmail2.kuleuven.be>

Hi R-help,

We are using R 2.2 on Win XP and have a detail question on how the
legend can be optimised.

We use the following;
-> plot(,type="b",...)
The lines in the plot do not cross the points. How can we obtain the
same effect in the legend? (points without a line through them..)

We tried setting the pt.bg to white but this did not help.
See script below.

thanks,
Jan


 par(mar=c(5,5,2,4) +.1) # dit om extra text in Y-as te plaatsen
   plot(ts.X, type="b", col=1, pch=19,ylim=c(-0.4,0.2),ylab=c("X"))
   legend.txt <- c("X","Y")
   # Define how the legend looks like, place it on the right location
   legend("topright", legend.txt,col=1,lty=1, pch=c(19,1),bty="n", pt.bg=1)
   par(new=T)
   plot(ts.Y, type="b", lty=1, col=1, pch=1, ylab="",
xlab="",yaxt="n",ylim=c(0.1,0.7))
   axis(4)
   mtext(side=4, line=3, "Y", cex=1)

-- 
Ir. Jan Verbesselt
Research Associate
Lab of Geomatics, K.U.Leuven
Vital Decosterstraat 102, 3000 Leuven, Belgium
Tel:+32-16-329750   Fax:+32-16-329760
http://gloveg.kuleuven.ac.be/

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From arildhus at stud.ntnu.no  Mon Nov 21 12:40:06 2005
From: arildhus at stud.ntnu.no (Arild Husby)
Date: Mon, 21 Nov 2005 12:40:06 +0100
Subject: [R] singular convergence with lmer function i lme4
Message-ID: <000901c5ee90$52440d20$e757f181@STUDENTA046822>


Dear R users,


I am trying to fit a GLMM to the following dataset;


tab
   a   b            c
1  1 0.6 199320100313
2  1 0.8 199427100412
3  1 0.8 199427202112
4  1 0.2 199428100611
5  1 1.0 199428101011
6  1 0.8 199428101111
7  0 0.8 199527103011
8  1 0.6 199527200711
9  0 0.8 199527202411
10 0 0.6 199529100412
11 1 0.2 199626201111
12 2 0.8 199627200612
13 1 0.4 199628100111
14 1 0.8 199628101511
15 1 0.4 199726200212
16 1 0.2 199726202111
17 1 0.6 199727101411
18 2 0.6 199727106911
19 2 0.6 199728100212
20 0 0.4 199820100811
21 1 0.8 199826200611
22 2 0.6 199827203811
23 2 1.0 200038109911
24 0 0.6 200126202511
25 0 0.4 200226100311
26 1 0.6 200226100411
27 1 0.4 200226100611
28 1 0.4 200226126011
29 1 0.4 200226203712
30 2 0.6 200227220313


With the following model;

  lmer(a~b + (1|c), family=poisson, data=tab),

What I want to do is to see if number of recruits (a) is dependent on the
brood sex ratio (b) including brood identity (c) as random factor.


However, I get the following error message;

lmer(a~b + (1|c), family=poisson, data=tab)
Error in devAGQ(PQLpars, 1) : Unable to invert singular factor of downdated
X'X
In addition: Warning messages:
1: optim or nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 
2: optim or nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 
3: optim or nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 
4: optim or nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 
5: optim or nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 
6: optim or nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 
7: IRLS iterations for PQL did not converge


I do not understand what causes this error message, all help is highly
appreciated!



I am running R version 2.20 on win XPP.
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.0            
year     2005           
month    10             
day      06             
svn rev  35749          
language R  


lme4 package version: 0.98-1
Matrix version: 0.98-7
lattice version: 0.12-11



Best regards,

Arild



From sundar.dorai-raj at pdf.com  Mon Nov 21 13:02:27 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 21 Nov 2005 06:02:27 -0600
Subject: [R] singular convergence with lmer function i lme4
In-Reply-To: <000901c5ee90$52440d20$e757f181@STUDENTA046822>
References: <000901c5ee90$52440d20$e757f181@STUDENTA046822>
Message-ID: <4381B753.9090404@pdf.com>



Arild Husby wrote:
> Dear R users,
> 
> 
> I am trying to fit a GLMM to the following dataset;
> 
> 
> tab
>    a   b            c
> 1  1 0.6 199320100313
> 2  1 0.8 199427100412
> 3  1 0.8 199427202112
> 4  1 0.2 199428100611
> 5  1 1.0 199428101011
> 6  1 0.8 199428101111
> 7  0 0.8 199527103011
> 8  1 0.6 199527200711
> 9  0 0.8 199527202411
> 10 0 0.6 199529100412
> 11 1 0.2 199626201111
> 12 2 0.8 199627200612
> 13 1 0.4 199628100111
> 14 1 0.8 199628101511
> 15 1 0.4 199726200212
> 16 1 0.2 199726202111
> 17 1 0.6 199727101411
> 18 2 0.6 199727106911
> 19 2 0.6 199728100212
> 20 0 0.4 199820100811
> 21 1 0.8 199826200611
> 22 2 0.6 199827203811
> 23 2 1.0 200038109911
> 24 0 0.6 200126202511
> 25 0 0.4 200226100311
> 26 1 0.6 200226100411
> 27 1 0.4 200226100611
> 28 1 0.4 200226126011
> 29 1 0.4 200226203712
> 30 2 0.6 200227220313
> 
> 
> With the following model;
> 
>   lmer(a~b + (1|c), family=poisson, data=tab),
> 
> What I want to do is to see if number of recruits (a) is dependent on the
> brood sex ratio (b) including brood identity (c) as random factor.
> 
> 
> However, I get the following error message;
> 
> lmer(a~b + (1|c), family=poisson, data=tab)
> Error in devAGQ(PQLpars, 1) : Unable to invert singular factor of downdated
> X'X
> In addition: Warning messages:
> 1: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 2: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 3: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 4: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 5: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 6: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 7: IRLS iterations for PQL did not converge
> 
> 
> I do not understand what causes this error message, all help is highly
> appreciated!
> 
> 
> 
> I am running R version 2.20 on win XPP.
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    2.0            
> year     2005           
> month    10             
> day      06             
> svn rev  35749          
> language R  
> 
> 
> lme4 package version: 0.98-1
> Matrix version: 0.98-7
> lattice version: 0.12-11
> 
> 
> 

Is this the entire dataset or just a portion. If the former, then you 
have thirty groups with one observation per group. This is not 
reasonable for fitting GLMM. I would suggest either making the grouping 
variable more broad or collecting more data. Since tab$c looks like 
dates, maybe try:

tab$c2 <- factor(substr(as.character(tab$c), 1, 4))
table(tab$c2)
fit <- lmer(a ~ b + (1 | c2), tab, poisson)

This works, but you still have only one observation for 1993, 2000, and 
2001.

HTH,

--sundar



From carcappe at unina.it  Mon Nov 21 13:12:22 2005
From: carcappe at unina.it (melina cappelli)
Date: Mon, 21 Nov 2005 13:12:22 +0100
Subject: [R] question on RPART
Message-ID: <000801c5ee94$d735b980$8b7de18f@ibma4zpk0s5hpe>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/7b0154d5/attachment.pl

From zorritillito-secure at yahoo.de  Mon Nov 21 13:14:25 2005
From: zorritillito-secure at yahoo.de (zorritillito-secure@yahoo.de)
Date: Mon, 21 Nov 2005 13:14:25 +0100 (CET)
Subject: [R] =?iso-8859-1?q?SPSS_and_R_=96_do_they_like_each_other=3F?=
Message-ID: <20051121121425.52143.qmail@web86801.mail.ukl.yahoo.com>

Hi,

I wonder how well SPSS and R communicate, because I
need SPSS but would like to do some data manipulations
in R. However I am very afraid of never ending
import-export-complications ? especially with all
those labels and extra information my SPSS files
contain. My data come from SPSS and have to be
exported to SPSS again (because I need to produce
special output tables, which would be hard to generate
in R). 

Have you done that kind of stuff? Thanks for sharing
your experience!

Michael



From Bernhard_Pfaff at fra.invesco.com  Mon Nov 21 13:23:43 2005
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Mon, 21 Nov 2005 12:23:43 -0000
Subject: [R] cointegration rank
Message-ID: <25D1C2585277D311B9A20000F6CCC71B07510AB8@DEFRAEX02>

Thanks a lot!

I have another question on "cointegration", so I will go on this post.

Is it possible to estimate a cointegration with some exogenous
explanatory variables? Since, after testing for exogeneity, I would like
to re-estimate the relation keeping some of the previous endogenous as
exogenous.

Many thanks!

Carlo


Hello Carlo,

you can use the 'dumvar' argument for his purpose, and exclude the relevant
variables from your data matrix 'x'.

HTH,
Bernhard


On Nov 21, 2005 11:21 AM, "Pfaff, Bernhard Dr."
<Bernhard_Pfaff at fra.invesco.com> wrote:

> Dear R - helpers,
> 
> I am using the urca package to estimate cointegration relations, and I
> would be really grateful if somebody could help me with this
> questions:
> 
> After estimating the unrestriced VAR with "ca.jo" I would like to
> impose
> the rank restriction (for example rank = 1) and then obtain the
> restricted estimate of PI to be utilized to estimate the VECM model.
> 
> Is it possible? 
> 
> It seems to me that the function "cajools" estimates the VECM without
> the restrictions. Did I miss something? How is it possible to impose
> them?
> 
> Thanks a lot in advance!
> 
> Carlo
> 
> 
> Hello Carlo,
> 
> you can achieve this, by calculating your desired PI-matrix by hand,
> given
> the slots 'V' and 'W' of your ca.jo object and then execute a
> restricted
> OLS-estimation, if I understand your goal correctly. 
> Please, bear in mind the non-uniqueness of the factorization of the
> PI-matrix by doing so.
> 
> HTH,
> Bernhard    
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
-------------- next part --------------
*****************************************************************
Confidentiality Note: The information contained in this message,
and any attachments, may contain confidential and/or privileged
material. It is intended solely for the person(s) or entity to
which it is addressed. Any review, retransmission, dissemination,
or taking of any action in reliance upon this information by
persons or entities other than the intended recipient(s) is
prohibited. If you received this in error, please contact the
sender and delete the material from any computer.
*****************************************************************

From tschoenhoff at gmail.com  Mon Nov 21 13:33:39 2005
From: tschoenhoff at gmail.com (=?WINDOWS-1252?Q?Thomas_Sch=F6nhoff?=)
Date: Mon, 21 Nov 2005 13:33:39 +0100
Subject: =?WINDOWS-1252?Q?Re:_[R]_SPSS_and_R_=96_do_they_like_each_other=3F?=
In-Reply-To: <20051121121425.52143.qmail@web86801.mail.ukl.yahoo.com>
References: <20051121121425.52143.qmail@web86801.mail.ukl.yahoo.com>
Message-ID: <5ad2dec0511210433w4d8fb97ai@mail.gmail.com>

Hello Micael,

2005/11/21, zorritillito-secure at yahoo.de <zorritillito-secure at yahoo.de>:
> Hi,
>
> I wonder how well SPSS and R communicate, because I
> need SPSS but would like to do some data manipulations
> in R. However I am very afraid of never ending
> import-export-complications ? especially with all
> those labels and extra information my SPSS files
> contain. My data come from SPSS and have to be
> exported to SPSS again (because I need to produce
> special output tables, which would be hard to generate
> in R).

I've experiencing no big problems when choosing a simple text format
for your data files!
Since I don't know what data specifics characterize your files it is
hard to assure everything will go well. But if you do choose the
appropriate file format there shouldn't be too much problem moving
files to and fro R.


regards

Thomas



From petr.pikal at precheza.cz  Mon Nov 21 13:34:41 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 21 Nov 2005 13:34:41 +0100
Subject: [R] Is there anything like a write.fwf() or possibility to
	print a data.frame without rownames?
In-Reply-To: <4381A7F0.3080201@bfro.uni-lj.si>
Message-ID: <4381CCF1.23230.13AA6F6@localhost>

Hi

did you tried something like

write.table( tab, "file.txt", sep="\t", row.names=F)

which writes to tab separated file?

Petr



On 21 Nov 2005 at 11:56, Gregor Gorjanc wrote:

Date sent:      	Mon, 21 Nov 2005 11:56:48 +0100
From:           	Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si>
Organization:   	University of Ljubljana
To:             	r-help at r-project.org
Subject:        	[R] Is there anything like a write.fwf() or possibility to print a
	data.frame without rownames?
Send reply to:  	gregor.gorjanc at bfro.uni-lj.si
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Dear R users,
> 
> R has read.fwf() function, however I would need write.fwf. I know
> other write.* functions, but I need fixed width format of data, which
> I would like to export from R. I tried to use:
> 
> - write.table, but I can not control alignment of columns
> 
> - write.matrix from MASS, but columns are to wide
> 
> I came to this option, which is very neat:
> 
> # tmp is data.frame
> 
> sink(file = file)
> print(tmp)
> sink()
> 
> This works very nice, but I would like to get rid of rownames, which
> are always printed.
> 
> Another not so important issue is width of printed columns. I presume
> this is determined by max(length column name, max(length of "values"
> in a column)) but sometimes it would be usefull to control width of
> columns also.
> 
> Can someone help me with this issue?
> 
> -- 
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> ----------------------------------------------------------------------
> University of Ljubljana     PhD student Biotechnical Faculty
> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
> 
> SI-1230 Domzale             tel: +386 (0)1 72 17 861
> Slovenia, Europe            fax: +386 (0)1 72 17 888
> 
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From stecalza at tiscali.it  Mon Nov 21 13:52:51 2005
From: stecalza at tiscali.it (Stefano Calza)
Date: Mon, 21 Nov 2005 13:52:51 +0100
Subject: [R] SPSS and R ? do they like each other?
In-Reply-To: <5ad2dec0511210433w4d8fb97ai@mail.gmail.com>
References: <20051121121425.52143.qmail@web86801.mail.ukl.yahoo.com>
	<5ad2dec0511210433w4d8fb97ai@mail.gmail.com>
Message-ID: <20051121125251.GF5167@med.unibs.it>

Hi,

from SPSS you can save in export format and then use read.spss in R. But you don't retain the labelling (AFAIK).

Nevertheless, let me ask you what kind of table spss gives you that R can't?

Ciao,
Stefano


On Mon, Nov 21, 2005 at 01:33:39PM +0100, Thomas Sch??nhoff wrote:
<Thomas>Hello Micael,
<Thomas>
<Thomas>2005/11/21, zorritillito-secure at yahoo.de <zorritillito-secure at yahoo.de>:
<Thomas>> Hi,
<Thomas>>
<Thomas>> I wonder how well SPSS and R communicate, because I
<Thomas>> need SPSS but would like to do some data manipulations
<Thomas>> in R. However I am very afraid of never ending
<Thomas>> import-export-complications ? especially with all
<Thomas>> those labels and extra information my SPSS files
<Thomas>> contain. My data come from SPSS and have to be
<Thomas>> exported to SPSS again (because I need to produce
<Thomas>> special output tables, which would be hard to generate
<Thomas>> in R).
<Thomas>
<Thomas>I've experiencing no big problems when choosing a simple text format
<Thomas>for your data files!
<Thomas>Since I don't know what data specifics characterize your files it is
<Thomas>hard to assure everything will go well. But if you do choose the
<Thomas>appropriate file format there shouldn't be too much problem moving
<Thomas>files to and fro R.
<Thomas>
<Thomas>
<Thomas>regards
<Thomas>
<Thomas>Thomas
<Thomas>
<Thomas>______________________________________________
<Thomas>R-help at stat.math.ethz.ch mailing list
<Thomas>https://stat.ethz.ch/mailman/listinfo/r-help
<Thomas>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gregor.gorjanc at bfro.uni-lj.si  Mon Nov 21 13:59:23 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 21 Nov 2005 13:59:23 +0100
Subject: [R] Is there anything like a write.fwf() or possibility to
 print a data.frame without rownames?
In-Reply-To: <4381CCF1.23230.13AA6F6@localhost>
References: <4381CCF1.23230.13AA6F6@localhost>
Message-ID: <4381C4AB.7060103@bfro.uni-lj.si>

Petr Pikal wrote:
> Hi
> 
> did you tried something like
> 
> write.table( tab, "file.txt", sep="\t", row.names=F)
> 
> which writes to tab separated file?
> 

Petr thanks, but I do not want a tab delimited file. I need spaces
between columns.

> 
> On 21 Nov 2005 at 11:56, Gregor Gorjanc wrote:
> 
> Date sent:      	Mon, 21 Nov 2005 11:56:48 +0100
> From:           	Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si>
> Organization:   	University of Ljubljana
> To:             	r-help at r-project.org
> Subject:        	[R] Is there anything like a write.fwf() or possibility to print a
> 	data.frame without rownames?
> Send reply to:  	gregor.gorjanc at bfro.uni-lj.si
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
> 
>>Dear R users,
>>
>>R has read.fwf() function, however I would need write.fwf. I know
>>other write.* functions, but I need fixed width format of data, which
>>I would like to export from R. I tried to use:
>>
>>- write.table, but I can not control alignment of columns
>>
>>- write.matrix from MASS, but columns are to wide
>>
>>I came to this option, which is very neat:
>>
>># tmp is data.frame
>>
>>sink(file = file)
>>print(tmp)
>>sink()
>>
>>This works very nice, but I would like to get rid of rownames, which
>>are always printed.
>>
>>Another not so important issue is width of printed columns. I presume
>>this is determined by max(length column name, max(length of "values"
>>in a column)) but sometimes it would be usefull to control width of
>>columns also.
>>
>>Can someone help me with this issue?
>>

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From sara at gmesintra.com  Mon Nov 21 13:59:44 2005
From: sara at gmesintra.com (Sara Mouro)
Date: Mon, 21 Nov 2005 12:59:44 -0000
Subject: [R] negative x y in ppp.object
Message-ID: <200511211259.jALCxs36005616@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/0423ecd9/attachment.pl

From 042045003 at fudan.edu.cn  Mon Nov 21 14:03:04 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Mon, 21 Nov 2005 21:03:04 +0800
Subject: [R] Fw: Re: Is there anything like a write.fwf() or possibility to
 print adata.frame without rownames?
Message-ID: <0IQB00DX52G6HF@mail.fudan.edu.cn>


>Petr Pikal wrote:
>> Hi
>> 
>> did you tried something like
>> 
>> write.table( tab, "file.txt", sep="\t", row.names=F)
>> 
>> which writes to tab separated file?
>> 
>
>Petr thanks, but I do not want a tab delimited file. I need spaces
>between columns.

write.table( tab, "file.txt", sep="", row.names=F)
Can it do what you want?

>> On 21 Nov 2005 at 11:56, Gregor Gorjanc wrote:
>> 
>> Date sent:      	Mon, 21 Nov 2005 11:56:48 +0100
>> From:           	Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si>
>> Organization:   	University of Ljubljana
>> To:             	r-help at r-project.org
>> Subject:        	[R] Is there anything like a write.fwf() or possibility to print a
>> 	data.frame without rownames?
>> Send reply to:  	gregor.gorjanc at bfro.uni-lj.si
>> 	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
>> 	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
>> 
>>>Dear R users,
>>>
>>>R has read.fwf() function, however I would need write.fwf. I know
>>>other write.* functions, but I need fixed width format of data, which
>>>I would like to export from R. I tried to use:
>>>
>>>- write.table, but I can not control alignment of columns
>>>
>>>- write.matrix from MASS, but columns are to wide
>>>
>>>I came to this option, which is very neat:
>>>
>>># tmp is data.frame
>>>
>>>sink(file = file)
>>>print(tmp)
>>>sink()
>>>
>>>This works very nice, but I would like to get rid of rownames, which
>>>are always printed.
>>>
>>>Another not so important issue is width of printed columns. I presume
>>>this is determined by max(length column name, max(length of "values"
>>>in a column)) but sometimes it would be usefull to control width of
>>>columns also.
>>>
>>>Can someone help me with this issue?
>>>
>
>-- 
>Lep pozdrav / With regards,
>    Gregor Gorjanc
>
>----------------------------------------------------------------------
>University of Ljubljana     PhD student
>Biotechnical Faculty
>Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
>Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
>
>SI-1230 Domzale             tel: +386 (0)1 72 17 861
>Slovenia, Europe            fax: +386 (0)1 72 17 888
>
>----------------------------------------------------------------------
>"One must learn by doing the thing; for though you think you know it,
> you have no certainty until you try." Sophocles ~ 450 B.C.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = = 
2005-11-21

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From arildhus at stud.ntnu.no  Mon Nov 21 14:24:50 2005
From: arildhus at stud.ntnu.no (Arild Husby)
Date: Mon, 21 Nov 2005 14:24:50 +0100
Subject: [R] singular convergence with lmer function i lme4
In-Reply-To: <4381B753.9090404@pdf.com>
Message-ID: <000a01c5ee9e$f3d34580$e757f181@STUDENTA046822>

Dear Sundar,

Thank you for your reply.


The posted dataset is just a subset from the entire dataset.

I found that it is clutch size 5 that causes the model to fail, since
whenever I try to include it the model fails to converge.. Hence, I only
included this subset into the mail.

However, my original dataset is the following (only needed variables
extracted);

tab2 <- cbind(numbrec,sexratio,fclutsize,LNRREIR)
> tab2
      numbrec  sexratio fclutsize      LNRREIR
 [1,]       1 0.6000000         2 199320100313
 [2,]       1 0.7500000         1 199327102511
 [3,]       1 0.2500000         1 199327202911
 [4,]       1 0.6666667         3 199327203011
 [5,]       3 1.0000000         1 199328100411
 [6,]       1 0.2500000         1 199329100511
 [7,]       2 0.2500000         1 199329100711
 [8,]       1 0.7500000         1 199329101211
 [9,]       1 0.5000000         1 199420100411
[10,]       1 0.8000000         2 199427100412
[11,]       1 0.6666667         3 199427100511
[12,]       1 0.8000000         2 199427202112
[13,]       2 0.5000000         1 199427206011
[14,]       1 0.2000000         2 199428100611
[15,]       1 1.0000000         2 199428101011
[16,]       1 0.8000000         2 199428101111
[17,]       2 0.5000000         3 199428101211
[18,]       0 0.5000000         1 199526200811
[19,]       0 1.0000000         1 199526201111
[20,]       0 0.2500000         1 199527100211
[21,]       0 0.8000000         2 199527103011
[22,]       2 0.5000000         1 199527200212
[23,]       1 0.6000000         2 199527200711
[24,]       0 0.2500000         1 199527201111
[25,]       0 0.8000000         2 199527202411
[26,]       0 0.6000000         2 199529100412
[27,]       1 0.5000000         1 199529100712
[28,]       0 0.7500000         1 199626200911
[29,]       1 0.2000000         2 199626201111
[30,]       1 0.2500000         1 199626201211
[31,]       2 0.8000000         2 199627200612
[32,]       1 0.4000000         2 199628100111
[33,]       4 0.7500000         1 199628100911
[34,]       0 0.8333333         3 199628101011
[35,]       1 0.7500000         1 199628101411
[36,]       1 0.8000000         2 199628101511
[37,]       3 0.3333333         3 199628101711
[38,]       1 0.4000000         2 199726200212
[39,]       1 0.2000000         2 199726202111
[40,]       1 0.6000000         2 199727101411
[41,]       2 0.6000000         2 199727106911
[42,]       2 0.7500000         1 199727209811
[43,]       2 0.6000000         2 199728100212
[44,]       0 0.5000000         3 199728100612
[45,]       1 0.5000000         1 199729100611
[46,]       0 0.4000000         2 199820100811
[47,]       3 0.0000000         1 199826102111
[48,]       1 0.8000000         2 199826200611
[49,]       1 0.0000000         1 199826200711
[50,]       3 0.2500000         1 199826203111
[51,]       1 0.2500000         1 199826203412
[52,]       1 0.5000000         1 199827103011
[53,]       2 0.6000000         2 199827203811
[54,]       1 0.5000000         1 199828100512
[55,]       0 0.5000000         1 199828100612
[56,]       0 0.7500000         1 199928101611
[57,]       1 0.3333333         3 199928102511
[58,]       0 0.5000000         1 200028101211
[59,]       1 0.7500000         1 200028101212
[60,]       2 0.2500000         1 200038100111
[61,]       2 1.0000000         2 200038109911
[62,]       3 0.7500000         1 200126101211
[63,]       0 0.6000000         2 200126202511
[64,]       0 0.4000000         2 200226100311
[65,]       1 0.6000000         2 200226100411
[66,]       1 0.4000000         2 200226100611
[67,]       1 0.4000000         2 200226126011
[68,]       0 0.7500000         1 200226202111
[69,]       1 0.4000000         2 200226203712
[70,]       0 0.5000000         3 200227120711
[71,]       1 0.5000000         1 200227120713
[72,]       3 0.7500000         1 200227170311
[73,]       1 0.6666667         3 200227220111
[74,]       0 0.5000000         1 200227220311
[75,]       0 0.5000000         1 200227220312
[76,]       2 0.6000000         2 200227220313
[77,]       0 0.7500000         1 200228101511
[78,]       1 0.2500000         1 200228102511
[79,]       1 0.2500000         1 200238103111


tab2 <- data.frame(tab2) #convert to data frame.

Where numbrec is number of recruits, ant fclutsize is the clutch size
(factor from 4 to 6) and LNRREIR which is a unique number given to a brood
(consisting of year, island number etc..)

So each LNRREIR is a unique number, however we want to include this as a
random factor to account for non independence among nestlings within brood
(Krackow & Tcladek 2001). Furthermore, a mother might have more than one
brood per year or might have broods between years that we want to control
for.

What we want to check is to see if number of recruits is dependent on the
sex ratio or if there is any interaction with clutch size.
And my model is;

obj1 <- lmer(numbrec~sexratio*fclutsize + (1|LNRREIR), family=poisson,
data=tab2)

summary(obj1)


Which gives me the same error message as I posted earlier.

> Error in devAGQ(PQLpars, 1) : Unable to invert singular factor of
downdated
> X'X
> In addition: Warning messages:
> 1: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 2: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 3: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 4: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 5: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 6: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 7: IRLS iterations for PQL did not converge
> 



If I understood your email correctly the problem is the use of LNRREIR as a
random factor as this is unique for each nest... Hence we should perhaps
include the identity of the mother as random factor (although this will
reduce the sample size considerably..).

Best regards, 

Arild



From ggrothendieck at gmail.com  Mon Nov 21 14:26:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Nov 2005 08:26:42 -0500
Subject: [R] force apply() to return a list
In-Reply-To: <E34BC00F-1FB6-40AF-B7FF-A9303E1A2D04@soc.soton.ac.uk>
References: <4CB7CB7A-6750-4943-AB1C-AB6D0A61A39C@soc.soton.ac.uk>
	<015c01c5ee76$d6f24380$0540210a@www.domain>
	<E34BC00F-1FB6-40AF-B7FF-A9303E1A2D04@soc.soton.ac.uk>
Message-ID: <971536df0511210526v27bef200m557e30c5b21e9a9f@mail.gmail.com>

Try this:

tapply(a, row(a), f, simplify = FALSE)

tapply(a, row(a), max, simplify = FALSE) # still returns list


On 11/21/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi Dimitris
>
>
> On 21 Nov 2005, at 08:37, Dimitris Rizopoulos wrote:
> > out <- lapply(apply(a2, 1, f), "[[", 1)
>
>
> thanks for this, but it doesn't quite do what I want:
>
>
>  > f <- function(x){1:max(x[1],4)}
>  > lapply(apply(cbind(1:5,5:1), 1, f), "[[", 1)
> [[1]]
> [1] 1
>
> [[2]]
> [1] 1
>
> [[3]]
> [1] 1
>
> [[4]]
> [1] 1
>
> [[5]]
> [1] 1
>
>  >
>
> I want
>
> > [[1]]
> > [1] 1 2 3 4
> >
> > [[2]]
> > [1] 1 2 3 4
> >
> > [[3]]
> > [1] 1 2 3 4
> >
> > [[4]]
> > [1] 1 2 3 4
> >
> > [[5]]
> > [1] 1 2 3 4 5
>
>
>
> best wishes
>
> Robin
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fezzi at stat.unibo.it  Mon Nov 21 14:51:25 2005
From: fezzi at stat.unibo.it (Carlo Fezzi)
Date: Mon, 21 Nov 2005 14:51:25 +0100 (CET)
Subject: [R] cointegration rank
In-Reply-To: <25D1C2585277D311B9A20000F6CCC71B07510AB8@DEFRAEX02>
References: <25D1C2585277D311B9A20000F6CCC71B07510AB8@DEFRAEX02>
Message-ID: <5662501.1132581085542.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Thanks a million!

Carlo

On Nov 21, 2005 01:23 PM, "Pfaff, Bernhard Dr."
<Bernhard_Pfaff at fra.invesco.com> wrote:

> Thanks a lot!
> 
> I have another question on "cointegration", so I will go on this post.
> 
> Is it possible to estimate a cointegration with some exogenous
> explanatory variables? Since, after testing for exogeneity, I would
> like
> to re-estimate the relation keeping some of the previous endogenous as
> exogenous.
> 
> Many thanks!
> 
> Carlo
> 
> 
> Hello Carlo,
> 
> you can use the 'dumvar' argument for his purpose, and exclude the
> relevant
> variables from your data matrix 'x'.
> 
> HTH,
> Bernhard
> 
> 
> On Nov 21, 2005 11:21 AM, "Pfaff, Bernhard Dr."
> <Bernhard_Pfaff at fra.invesco.com> wrote:
> 
> > Dear R - helpers,
> > 
> > I am using the urca package to estimate cointegration relations, and
> > I
> > would be really grateful if somebody could help me with this
> > questions:
> > 
> > After estimating the unrestriced VAR with "ca.jo" I would like to
> > impose
> > the rank restriction (for example rank = 1) and then obtain the
> > restricted estimate of PI to be utilized to estimate the VECM model.
> > 
> > Is it possible? 
> > 
> > It seems to me that the function "cajools" estimates the VECM
> > without
> > the restrictions. Did I miss something? How is it possible to impose
> > them?
> > 
> > Thanks a lot in advance!
> > 
> > Carlo
> > 
> > 
> > Hello Carlo,
> > 
> > you can achieve this, by calculating your desired PI-matrix by hand,
> > given
> > the slots 'V' and 'W' of your ca.jo object and then execute a
> > restricted
> > OLS-estimation, if I understand your goal correctly. 
> > Please, bear in mind the non-uniqueness of the factorization of the
> > PI-matrix by doing so.
> > 
> > HTH,
> > Bernhard    
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html



From alxmilton at yahoo.it  Mon Nov 21 14:52:35 2005
From: alxmilton at yahoo.it (alessandro carletti)
Date: Mon, 21 Nov 2005 05:52:35 -0800 (PST)
Subject: [R] modify boxplot
Message-ID: <20051121135235.25801.qmail@web26602.mail.ukl.yahoo.com>

Hi everybody,
I'm trying to modify the boxplot just to set the upper
whisker to the 90 percentile value, but I still
couldn't find the solution.
Can anyone help me?
Thanks


Alessandro Carletti



From r.hankin at noc.soton.ac.uk  Mon Nov 21 14:59:53 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 21 Nov 2005 13:59:53 +0000
Subject: [R] force apply() to return a list
In-Reply-To: <971536df0511210526v27bef200m557e30c5b21e9a9f@mail.gmail.com>
References: <4CB7CB7A-6750-4943-AB1C-AB6D0A61A39C@soc.soton.ac.uk>
	<015c01c5ee76$d6f24380$0540210a@www.domain>
	<E34BC00F-1FB6-40AF-B7FF-A9303E1A2D04@soc.soton.ac.uk>
	<971536df0511210526v27bef200m557e30c5b21e9a9f@mail.gmail.com>
Message-ID: <EAD67459-EAC9-4B8D-AE51-C70B469191DA@soc.soton.ac.uk>

Hi Gabor

thanks for this.


On 21 Nov 2005, at 13:26, Gabor Grothendieck wrote:
> Try this:
>
> tapply(a, row(a), f, simplify = FALSE)
>

[
here
a <- cbind(1:5 , 5:1)
f <- function(x){list(1:max(x[1],4))}
]


This also works.  I actually  need unlist(... , recursive=FALSE)
to pass the list to do.call() but that 's fine.

But,  my question here is: how does one arrive at
such a solution from the manpage?  The manpage seems
to indicate that the first argument to tapply() is a ragged
array, or a vector.

I can't reconcile Gabor's  use of  tapply() with the manpage.

How to  understand Gabor's suggestion better?








>>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ggrothendieck at gmail.com  Mon Nov 21 15:11:21 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Nov 2005 09:11:21 -0500
Subject: [R] force apply() to return a list
In-Reply-To: <EAD67459-EAC9-4B8D-AE51-C70B469191DA@soc.soton.ac.uk>
References: <4CB7CB7A-6750-4943-AB1C-AB6D0A61A39C@soc.soton.ac.uk>
	<015c01c5ee76$d6f24380$0540210a@www.domain>
	<E34BC00F-1FB6-40AF-B7FF-A9303E1A2D04@soc.soton.ac.uk>
	<971536df0511210526v27bef200m557e30c5b21e9a9f@mail.gmail.com>
	<EAD67459-EAC9-4B8D-AE51-C70B469191DA@soc.soton.ac.uk>
Message-ID: <971536df0511210611r714d493cm7ae9bfae73f385df@mail.gmail.com>

On 11/21/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi Gabor
>
> thanks for this.
>
>
> On 21 Nov 2005, at 13:26, Gabor Grothendieck wrote:
> > Try this:
> >
> > tapply(a, row(a), f, simplify = FALSE)
> >
>
> [
> here
> a <- cbind(1:5 , 5:1)
> f <- function(x){list(1:max(x[1],4))}
> ]
>
>
> This also works.  I actually  need unlist(... , recursive=FALSE)
> to pass the list to do.call() but that 's fine.
>
> But,  my question here is: how does one arrive at
> such a solution from the manpage?  The manpage seems
> to indicate that the first argument to tapply() is a ragged
> array, or a vector.
>
> I can't reconcile Gabor's  use of  tapply() with the manpage.
>
> How to  understand Gabor's suggestion better?

A matrix is just a vector with a dim attribute.

> x <- 1:4
> attr(x, "dim") <- c(2,2)  # or dim(x) <- c(2,2)
> class(x)
[1] "matrix"



From berwin at maths.uwa.edu.au  Mon Nov 21 15:29:24 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Mon, 21 Nov 2005 22:29:24 +0800
Subject: [R] force apply() to return a list
In-Reply-To: <FB06D3BB-A32A-4462-8773-A4A3D90995E2@soc.soton.ac.uk>
References: <4CB7CB7A-6750-4943-AB1C-AB6D0A61A39C@soc.soton.ac.uk>
	<015c01c5ee76$d6f24380$0540210a@www.domain>
	<E34BC00F-1FB6-40AF-B7FF-A9303E1A2D04@soc.soton.ac.uk>
	<016b01c5ee78$f679c9b0$0540210a@www.domain>
	<FB06D3BB-A32A-4462-8773-A4A3D90995E2@soc.soton.ac.uk>
Message-ID: <17281.55748.496307.662333@bossiaea.maths.uwa.edu.au>

G'day Robin,

>>>>> "RH" == Robin Hankin <r.hankin at noc.soton.ac.uk> writes:

    RH> Still, it's a little disconcerting that apply() as generally
    RH> used can return a list 99.9% of the time and a matrix the
    RH> other 0.1%.
It probably depends on how it is used. :-)

I usually use apply in situations where I know that the lengths of the
result is always the same and want a matrix returned.  Thus, in my
applications in 99.99% of the time a matrix is returned and the other
0.01% a list (and I forget that apply could return a list).

(BTW, 67.8% of all statistics are made up on the spot, just as the
last three presumably. :) )

    RH> It took me a long time to track this down when debugging my
    RH> code the other night.
It is called defensive programming (and it is great if it works). :-)
If your code expects a list, make sure that it gets a list and spits a
dummy if not, then these kind of problems are easy to find.....

    RH> Does anyone else agree?  Would it be possible to add an
    RH> argument such as force.list (with default FALSE) to apply()
    RH> that makes apply() return a list under all circumstances?
    RH> Also, adding Dimitris's excellent workaround to apply()'s
    RH> manpage would be helpful.
If you supply appropriate patches against the SVN source, I am sure
that R core will consider it.

And although Dimitris solution is quite nice, I find that it obfuscate
the code a bit.  What is wrong with checking whether the result
returned by apply is a matrix, and if so turn the matrix into a list.
Probably the easiest way to turn a matrix into a list is to use
as.data.frame(), if you want that the printed version of the object
looks like a list, then use as.list() too:


> a2 <- cbind(1:3,3:1)
> f <- function(x){1:max(x[1],4)}
> apply(a2,1,f)
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    2    2    2
[3,]    3    3    3
[4,]    4    4    4
> res <- apply(a2,1,f)
> if(is.matrix(res)) res <- as.list(as.data.frame(res))
> res
$V1
[1] 1 2 3 4

$V2
[1] 1 2 3 4

$V3
[1] 1 2 3 4

Cheers,

        Berwin



From JAROSLAW.W.TUSZYNSKI at saic.com  Mon Nov 21 15:38:59 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Mon, 21 Nov 2005 09:38:59 -0500
Subject: [R] PNG-import into R
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD504E34C64@us-arlington-0668.mail.saic.com>

I was looking once for all supported image formats, here is what I found:

Format | read file to matrix | write matrix to file | write plot to file
-------|---------------------|----------------------|-------------------
PNG    |                     |                      | png,  bitmap, GDD
JPEG   | read.jpeg           |                      | jpeg, bitmap, GDD
GIF    | read.gif            | write.gif            | GDD
TIFF   | read.picture        | write.picture        | savetiff, bitmap
PDF    |                     |                      | pdf, bitmap
ENVI   | read.ENVI           | write.ENVI           |
?      | read.pnm            | write.pnm            |
	   
I have not played with every one of those functions and I think that every
read/write returns "matrix" in some other internal format. Also, last 2
formats are rather exotic and of limited use. Use CRAN search to look up
package of each function.

So I think that in your case your best bet would be to convert your file to
GIF (if you can live with only 256 colors) or TIFF.

Also does anybody know how hard would it be to tap into C code needed for
'read.jpeg', 'png' and 'jpeg' functions to write 'read.png' , 'write.png',
and 'write.jpeg' functions?

Jarek Tuszynski

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sydler, Dominik
Sent: Monday, November 21, 2005 5:36 AM
To: r-help at stat.math.ethz.ch
Subject: [R] PNG-import into R

Hi there

I'm looking for a function to read PNG-bitmap-images from a file into R.
I only found:
 - the pixmap-package which cannot import png or similar formats
 - the rimage-package which can only import lossy jpeg-images (the
convertion from png to jpeg modifies the data!)

Is there any possibility to read PNG-files?

Thanks for any help
         Dominic Sydler

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jtw2 at CDC.GOV  Mon Nov 21 15:28:42 2005
From: jtw2 at CDC.GOV (Wassell, James T., Ph.D.)
Date: Mon, 21 Nov 2005 09:28:42 -0500
Subject: [R] nlme question
Message-ID: <AF2DCD619279544BA454141F4A45B9E306A3CF@m-niosh-3.niosh.cdc.gov>

Deepayan, 

Yes, thanks for confirming my suspicions.  I know mixed models are
"different" but, I did not think they were so different as to preclude
estimating the var-cov matrix (via the Hessian in Maximum likelihood, as
you point out).  

Thanks for prompting me to think about MCMC.  Your suggestion to
consider MCMC makes me realize that using BUGS, I could directly sample
from the posterior of the linear combination of parameters - to get its
variance and eliminate the extra step using the var-cov matrix.   As you
say, with results better than the asymptotic approximation. (Maybe I can
do the same thing with mcmcsamp?, but I'm not familiar with this and
will have to take a look at it.)
 
-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
Sent: Thursday, November 17, 2005 2:22 PM
To: Doran, Harold
Cc: Wassell, James T., Ph.D.; r-help at stat.math.ethz.ch
Subject: Re: nlme question

On 11/17/05, Doran, Harold <HDoran at air.org> wrote:
> I think the authors are mistaken. Sigma is random error, and due to
its
> randomness it cannot be systematically related to anything. It is this
> ind. assumption that allows for the likelihood to be expressed as
> described in Pinhiero and Bates p.62.

I think not. The issue is dependence between the _estimates_ of sigma,
tao, etc, and that may well be present. Presumably, if one can compute
the likelihood surface as a function of the 3 parameters, the hessian
at the MLE's would give the estimated covariance. However, I don't
think nlme does this.

A different approach you might want to consider is using mcmcsamp in
the lme4 package (or more precisely, the Matrix package) to get
samples from the joint posterior distribution. This is likely to be
better than the asymptotic normal approximation in any case.

Deepayan



From B.Rowlingson at lancaster.ac.uk  Mon Nov 21 15:47:56 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 21 Nov 2005 14:47:56 +0000
Subject: [R] PNG-import into R
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD504E34C64@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD504E34C64@us-arlington-0668.mail.saic.com>
Message-ID: <4381DE1C.70807@lancaster.ac.uk>

Tuszynski, Jaroslaw W. wrote:

> Also does anybody know how hard would it be to tap into C code needed for
> 'read.jpeg', 'png' and 'jpeg' functions to write 'read.png' , 'write.png',
> and 'write.jpeg' functions?

  Much, much harder than using ImageMagick to convert to one of the 
formats that R can read.

  > system("convert foo.png foo.pnm")
  > foo = read.pnm("foo.pnm")

ImageMagick is here:
  http://www.imagemagick.org/script/index.php

And is normally already installed on modern Linux distributions.

But yes, native reading without conversion is sometimes preferable.

Baz



From tom at maladmin.com  Mon Nov 21 10:57:04 2005
From: tom at maladmin.com (tom wright)
Date: Mon, 21 Nov 2005 04:57:04 -0500
Subject: [R] PNG-import into R
In-Reply-To: <59484F5CC089B4499DDAC9503DA0BC6706A7A0@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <59484F5CC089B4499DDAC9503DA0BC6706A7A0@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <1132567025.4819.133.camel@localhost.localdomain>

Hi,
I dont think you can import PNG images into R directly (although I may
be wrong). I use ImageMagick (available for most operating systems) and
the system() function to do my image conversions.

convertImage<-function(dirname,srcname,targetname){
    strSrcName<-shQuote(paste(dirname,srcname,sep='/'))
    strTrgName<-shQuote(paste(dirname,targetname,sep='/'))

    filelist<-system(paste('ls',strSrcName,sep=' '),TRUE,TRUE)
    if(length(filelist)<1){
        stop("Source Image Not Found")
    }

    filelist<-system(paste('ls',strTrgName,sep=' '),TRUE,TRUE)
    if(length(filelist)<1){
        strcmd<-paste('convert',strSrcName,strTrgName,sep=' ')
        system(strcmd)
    }
}

On Mon, 2005-21-11 at 11:36 +0100, Sydler, Dominik wrote:
> Hi there
> 
> I'm looking for a function to read PNG-bitmap-images from a file into R.
> I only found:
>  - the pixmap-package which cannot import png or similar formats
>  - the rimage-package which can only import lossy jpeg-images (the
> convertion from png to jpeg modifies the data!)
> 
> Is there any possibility to read PNG-files?
> 
> Thanks for any help
>          Dominic Sydler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From timo.becker at oeaw.ac.at  Mon Nov 21 15:55:57 2005
From: timo.becker at oeaw.ac.at (Timo Becker)
Date: Mon, 21 Nov 2005 15:55:57 +0100
Subject: [R] binary kmeans tree
In-Reply-To: <mailman.10.1132052401.16708.r-help@stat.math.ethz.ch>
References: <mailman.10.1132052401.16708.r-help@stat.math.ethz.ch>
Message-ID: <4381DFFD.4080508@oeaw.ac.at>

Dear R-Users,

does anyone know if there exists a package for a kmeans variant which 
creates a binary tree by stepwise splitting of the data?

If you do not understand what I mean then have a look at the following 
link (there it is called KD-trees):
http://www.cs.cmu.edu/~dpelleg/kmeans.html

Thanks in advance,
Timo



From maechler at stat.math.ethz.ch  Mon Nov 21 16:33:09 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 21 Nov 2005 16:33:09 +0100
Subject: [R] Howto? plot legend with no line behind the points
In-Reply-To: <1132572515.4381af6394a80@webmail2.kuleuven.be>
References: <1132572515.4381af6394a80@webmail2.kuleuven.be>
Message-ID: <17281.59573.416315.561580@stat.math.ethz.ch>

>>>>> "Jan" == Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be>
>>>>>     on Mon, 21 Nov 2005 12:28:35 +0100 writes:

    Jan> Hi R-help,
    Jan> We are using R 2.2 on Win XP and have a detail question on how the
    Jan> legend can be optimised.

    Jan> We use the following;
    -> plot(,type="b",...)
    Jan> The lines in the plot do not cross the points. How can we obtain the
    Jan> same effect in the legend? (points without a line through them..)

not directly, however you can trick it, see below

    Jan> We tried setting the pt.bg to white but this did not help.

I think it does help some.. read on

    Jan> See script below.

     par(mar=c(5,5,2,4) +.1) # dit om extra text in Y-as te plaatsen
     plot(ts.X, type="b", col=1, pch=19,ylim=c(-0.4,0.2),ylab=c("X"))
     legend.txt <- c("X","Y")
     # Define how the legend looks like, place it on the right location
     legend("topright", legend.txt,col=1,lty=1, pch=c(19,1),bty="n", pt.bg=1)
     par(new=T)
     plot(ts.Y, type="b", lty=1, col=1, pch=1, ylab="",
     xlab="",yaxt="n",ylim=c(0.1,0.7))
     axis(4)
     mtext(side=4, line=3, "Y", cex=1)


Unfortunately you don't give a reproducible example {we don't
have your ts.X and ts.Y}, so I make up a simpler version of
the above:

## A bivariate time-series
set.seed(1)
xy <-  cbind(x = ts(cumsum(rnorm(47))), y = ts(cumsum(rt(47, df=3))))

plot(xy, plot.type = "single", type = "b", pch = 21, col=1:2, bg = "light blue")
legend("topright", c("x","y"), col=1:2, lty="47", 
       pch = 21, bty="n", pt.bg="light blue")


I'm using pch=21 and I've used pt.bg just to show it's effect
here; of course you can play with these as well.

And the real trick was to "play with" 'lty'  using the nice  "on/off"
convention ("47": a dash of length 4; a break of length 7).

Now of course, we also might accept patches for improving
legend (source at 
 https://svn.R-project.org/R/trunk/src/library/graphics/R/legend.R ).

Naturally, I think it's the argument  'merge' that
could be extended to allow something analogous to plot type = "b".

Hoping this helps:
Martin Maechler, ETH Zurich



From Setzer.Woodrow at epamail.epa.gov  Mon Nov 21 16:38:18 2005
From: Setzer.Woodrow at epamail.epa.gov (Setzer.Woodrow@epamail.epa.gov)
Date: Mon, 21 Nov 2005 10:38:18 -0500
Subject: [R] odesolve with banded Jacobian [was "no subject"]
In-Reply-To: <17272.41774.72970.156563@stat.math.ethz.ch>
Message-ID: <OFA1790E80.0E8BF2B0-ON852570C0.0054C968-852570C0.0055E133@epamail.epa.gov>

Dear Karline Soetaert,
I've just returned from a week of travel, so have not had a great deal
of time to look at your request.  From a brief rereading of the original
lsoda documentation, it looks as if all I need to do is set a flag to a
different value (jt to 4), and leave it up to the user to construct the
function that calculates the jacobian  properly.  If you'd contact me
directly, ideally with a test model, I will see if the modification is
really that simple; if so, I'll make the change and release an updated
odesolve to CRAN.
Woody
PS: Thanks, Martin

R. Woodrow Setzer, Jr.
National Center for Computational Toxicology
US Environmental Protection Agency
Mail Drop B205-01/US EPA/RTP, NC 27711
Ph: (919) 541-0128    Fax: (919) 541-1194


                                                                        
             Martin Maechler                                            
             <maechler at stat.m                                           
             ath.ethz.ch>                                            To 
                                      "Soetaert, Karline"               
             11/14/2005 09:46         <K.Soetaert at nioo.knaw.nl>         
             AM                                                      cc 
                                      R-help at stat.math.ethz.ch, Woodrow 
                                      Setzer/RTP/USEPA/US at EPA           
              Please respond                                    Subject 
                    to                Re: [R] odesolve with banded      
             Martin Maechler          Jacobian [was "no subject"]       
             <maechler at stat.m                                           
               ath.ethz.ch>                                             
                                                                        
                                                                        
                                                                        
                                                                        




>>>>> "KSoet" == Soetaert, Karline <K.Soetaert at nioo.knaw.nl>
>>>>>     on Mon, 14 Nov 2005 13:20:24 +0100 writes:

    KSoet> Hi, I am trying to solve a model that consists of
    KSoet> rather stiff ODEs in R.

    KSoet> I use the package ODEsolve (lsoda) to solve these
    KSoet> ODEs.

    KSoet> To speed up the integration, the jacobian is also
    KSoet> specified.

    KSoet> Basically, the model is a one-dimensional
    KSoet> advection-diffusion problem, and thus the jacobian is
    KSoet> a tridiagonal matrix.

    KSoet> The size of this jacobian is 100*100.

    KSoet> In the original package LSODA it is possible to
    KSoet> specify that the jacobian is banded, which makes its
    KSoet> inversion very efficient.

    KSoet> However, this feature seems to have been removed in
    KSoet> the R version.

    KSoet> Is there a way to overcome this limitation?

Yes.  But probably not a very easy one; maybe even a very
cumbersome one... ;-)

Note however that questions like these should typically be
addressed at the package author - which you can always quickly
find out via

  > packageDescription("odesolve")
  Package: odesolve
  Version: 0.5-12
  Date: 2004/10/25
  Title: Solvers for Ordinary Differential Equations
  Author: R. Woodrow Setzer <setzer.woodrow at epa.gov>
  Maintainer: R. Woodrow Setzer <setzer.woodrow at epa.gov>
  Depends: R (>= 1.4.0)
  Description: This package provides an interface for the ODE solver
               lsoda. ODEs are expressed as R functions or as compiled
code.
  .......................


I've CC'ed this e-mail to Woodrow to help you for once


 <..........>

    KSoet>         [[alternative HTML version deleted]]

    KSoet> ______________________________________________
    KSoet> .........
    KSoet> PLEASE do read the posting guide!
    KSoet> http://www.R-project.org/posting-guide.html

if you do read that guide, it will tell you

- why you should always use a 'Subject' for your e-mails
- why HTML-ified e-mails are not much liked and what you can do
   about it.

Regards,
Martin Maechler, ETH Zurich



From close2ceo at yahoo.com  Mon Nov 21 16:44:41 2005
From: close2ceo at yahoo.com (Xiaodong Jin)
Date: Mon, 21 Nov 2005 07:44:41 -0800 (PST)
Subject: [R] garch function in R
Message-ID: <20051121154441.65685.qmail@web31215.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/7e28d59a/attachment.pl

From Cameron.Guenther at MyFWC.com  Mon Nov 21 16:44:58 2005
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Mon, 21 Nov 2005 10:44:58 -0500
Subject: [R] Warning message help
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F304C8658@FWC-TLEX3.fwc.state.fl.us>

I am trying to great a new column of effort data from an existing vector
of gears used.
It is a simple code where 

effort[Gear==300]=(DIST_TOW*7412)
effort[Gear==301]=(DIST_TOW*7412)

The code appears to work for some of the data but fails for others and
inserts a NA value

I also get this warning message

Warning message:
number of items to replace is not a multiple of replacement length 

Can anybody tell me what this means.
thanks

Cameron Guenther 
Associate Research Scientist
FWC/FWRI, Marine Fisheries Research
100 8th Avenue S.E.
St. Petersburg, FL 33701
(727)896-8626 Ext. 4305
cameron.guenther at myfwc.com



From stratja at auburn.edu  Mon Nov 21 16:50:29 2005
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Mon, 21 Nov 2005 09:50:29 -0600
Subject: [R] predicted values from cv.glm
Message-ID: <43819865020000F200001B00@TMIA1.AUBURN.EDU>

Hi folks,

Is there a way to get the predicted values from leave-one-out cross
validation using cv.glm?   More generally, is there a way to see what
output is available with any function that may not show up using the
help() function?

Below is the code that I've been using:

SRCOUNT <- read.table(file.choose(),header=T)  
library(boot)
library(MASS)
q_u2 <- glm.nb(res_est ~ U2 + I(U2^2), SRCOUNT, link = log)
cv.qu2<- cv.glm(SRCOUNT,qu2)

Many thanks,

Jeff 

****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja



From Antje.Doering at komdat.com  Mon Nov 21 16:57:05 2005
From: Antje.Doering at komdat.com (=?iso-8859-1?Q?Antje_D=F6ring?=)
Date: Mon, 21 Nov 2005 16:57:05 +0100
Subject: [R] Comparing rows of matrices with different dimensions
Message-ID: <686C1FDE894539418C5668E5E6DE12DE051033@muc-exch-tmp.komdat.intern>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/80a80968/attachment.pl

From jholtman at gmail.com  Mon Nov 21 16:59:28 2005
From: jholtman at gmail.com (jim holtman)
Date: Mon, 21 Nov 2005 10:59:28 -0500
Subject: [R] modify boxplot
In-Reply-To: <20051121135235.25801.qmail@web26602.mail.ukl.yahoo.com>
References: <20051121135235.25801.qmail@web26602.mail.ukl.yahoo.com>
Message-ID: <644e1f320511210759o15b622b1q48b11588edda8535@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/11f37903/attachment.pl

From murdoch at stats.uwo.ca  Mon Nov 21 17:04:32 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 21 Nov 2005 11:04:32 -0500
Subject: [R] Howto? plot legend with no line behind the points
In-Reply-To: <17281.59573.416315.561580@stat.math.ethz.ch>
References: <1132572515.4381af6394a80@webmail2.kuleuven.be>
	<17281.59573.416315.561580@stat.math.ethz.ch>
Message-ID: <4381F010.5040003@stats.uwo.ca>

On 11/21/2005 10:33 AM, Martin Maechler wrote:
>>>>>> "Jan" == Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be>
>>>>>>     on Mon, 21 Nov 2005 12:28:35 +0100 writes:
> 
>     Jan> Hi R-help,
>     Jan> We are using R 2.2 on Win XP and have a detail question on how the
>     Jan> legend can be optimised.
> 
>     Jan> We use the following;
>     -> plot(,type="b",...)
>     Jan> The lines in the plot do not cross the points. How can we obtain the
>     Jan> same effect in the legend? (points without a line through them..)
> 
> not directly, however you can trick it, see below
> 
>     Jan> We tried setting the pt.bg to white but this did not help.
> 
> I think it does help some.. read on
> 
>     Jan> See script below.
> 
>      par(mar=c(5,5,2,4) +.1) # dit om extra text in Y-as te plaatsen
>      plot(ts.X, type="b", col=1, pch=19,ylim=c(-0.4,0.2),ylab=c("X"))
>      legend.txt <- c("X","Y")
>      # Define how the legend looks like, place it on the right location
>      legend("topright", legend.txt,col=1,lty=1, pch=c(19,1),bty="n", pt.bg=1)
>      par(new=T)
>      plot(ts.Y, type="b", lty=1, col=1, pch=1, ylab="",
>      xlab="",yaxt="n",ylim=c(0.1,0.7))
>      axis(4)
>      mtext(side=4, line=3, "Y", cex=1)
> 
> 
> Unfortunately you don't give a reproducible example {we don't
> have your ts.X and ts.Y}, so I make up a simpler version of
> the above:
> 
> ## A bivariate time-series
> set.seed(1)
> xy <-  cbind(x = ts(cumsum(rnorm(47))), y = ts(cumsum(rt(47, df=3))))
> 
> plot(xy, plot.type = "single", type = "b", pch = 21, col=1:2, bg = "light blue")
> legend("topright", c("x","y"), col=1:2, lty="47", 
>        pch = 21, bty="n", pt.bg="light blue")
> 
> 
> I'm using pch=21 and I've used pt.bg just to show it's effect
> here; of course you can play with these as well.
> 
> And the real trick was to "play with" 'lty'  using the nice  "on/off"
> convention ("47": a dash of length 4; a break of length 7).
> 
> Now of course, we also might accept patches for improving
> legend (source at 
>  https://svn.R-project.org/R/trunk/src/library/graphics/R/legend.R ).
> 
> Naturally, I think it's the argument  'merge' that
> could be extended to allow something analogous to plot type = "b".

Are there any examples with "merge = FALSE" that look good?  It seems to 
me that it would be a cleaner change to add a new parameter "type" that 
did what it does in plot(), and then eventually drop merge.  If there 
really are examples with merge = FALSE that people want to keep, then we 
could keep it, but I still think adding "type" would be better than 
fiddling with merge.

Of course, this suggestion isn't trivial to implement for "type = 'b'", 
where we'd want

    -- * --

in the legend, but I think it could be done with some sort of trickery 
involving 3 points with "type = 'c'" overlaid with one point with "type 
= 'p'".

Duncan Murdoch



From rxg218 at psu.edu  Mon Nov 21 17:08:22 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 21 Nov 2005 11:08:22 -0500
Subject: [R] PNG-import into R
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD504E34C64@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD504E34C64@us-arlington-0668.mail.saic.com>
Message-ID: <1132589302.2936.15.camel@blue.chem.psu.edu>

On Mon, 2005-11-21 at 09:38 -0500, Tuszynski, Jaroslaw W. wrote:

> 
> Also does anybody know how hard would it be to tap into C code needed for
> 'read.jpeg', 'png' and 'jpeg' functions to write 'read.png' , 'write.png',
> and 'write.jpeg' functions?

Not too difficult (at least with libpng).

I was fiddling with adding a read.png method to the rimage package for
my own use, but have not really finished it.

The code is based on the example code from the libpng docs and and an
O'Reilly article

The C code and R wrapper can be found at

http://blue.chem.psu.edu/~rajarshi/code/R/pngio.c
http://blue.chem.psu.edu/~rajarshi/code/R/png.R


Right now it segfaults, so its not really useful yet.

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
All theoretical chemistry is really physics; and all theoretical
chemists 
know it.
-- Richard P. Feynman



From dimitris.rizopoulos at med.kuleuven.be  Mon Nov 21 17:13:22 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 21 Nov 2005 17:13:22 +0100
Subject: [R] Comparing rows of matrices with different dimensions
References: <686C1FDE894539418C5668E5E6DE12DE051033@muc-exch-tmp.komdat.intern>
Message-ID: <007901c5eeb6$7eab9600$0540210a@www.domain>

how about:

a <- cbind(1:4, c(4, 6, 6, 8), 9:12)
b <- cbind(1:3, 4:6)
#####
apply(b, 1, paste, collapse = "") %in% apply(a[, -3], 1, paste, 
collapse = "")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Antje D??ring" <Antje.Doering at komdat.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, November 21, 2005 4:57 PM
Subject: [R] Comparing rows of matrices with different dimensions


>
>
> Hi there,
>
>
>
> I have a question, which I thought is very easy to solve, but 
> somehow I can't find a solution. Probably someone could help me 
> quickly?
>
>
>
> Here it is:
>
>
>
> I have two matrices:
>
>
>
> a
>
>     [,1] [,2] [,3]
>
> [1,]    1    4    9
>
> [2,]    2    6   10
>
> [3,]    3    6   11
>
> [4,]    4    8   12
>
>
>
>
>
> b
>
>     [,1] [,2]
>
> [1,]    1    4
>
> [2,]    2    5
>
> [3,]    3    6
>
>
>
> Now I want to find out which rows of b can also be found in a 
> (without its last column). So the solution must be something like 
> either "TRUE FALSE TRUE" or the rows where their is a match (rows 1 
> and 3)
>
>
>
> Till now I have tried things like b %in% a[,1:2] or so but that 
> doesn't work because I want to compare the WHOLE row of b with the 
> whole row of a without column 3.
>
>
>
> Thank you very much for any help.
>
>
>
> Regards, Antje
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From tom at maladmin.com  Mon Nov 21 12:16:55 2005
From: tom at maladmin.com (tom wright)
Date: Mon, 21 Nov 2005 06:16:55 -0500
Subject: [R] Warning message help
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F304C8658@FWC-TLEX3.fwc.state.fl.us>
References: <BA6FF017E924044A9BF748AFAEEA6F304C8658@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <1132571815.4819.135.camel@localhost.localdomain>

Shouldnt that be
effort[Gear==300]<-(DIST_TOW*7412)
I think we're going to need to see some sample datasets to tell you any
more.

On Mon, 2005-21-11 at 10:44 -0500, Guenther, Cameron wrote:
> I am trying to great a new column of effort data from an existing vector
> of gears used.
> It is a simple code where 
> 
> effort[Gear==300]=(DIST_TOW*7412)
> effort[Gear==301]=(DIST_TOW*7412)
> 
> The code appears to work for some of the data but fails for others and
> inserts a NA value
> 
> I also get this warning message
> 
> Warning message:
> number of items to replace is not a multiple of replacement length 
> 
> Can anybody tell me what this means.
> thanks
> 
> Cameron Guenther 
> Associate Research Scientist
> FWC/FWRI, Marine Fisheries Research
> 100 8th Avenue S.E.
> St. Petersburg, FL 33701
> (727)896-8626 Ext. 4305
> cameron.guenther at myfwc.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From gavin.simpson at ucl.ac.uk  Mon Nov 21 17:11:27 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 21 Nov 2005 16:11:27 +0000
Subject: [R] garch function in R
In-Reply-To: <20051121154441.65685.qmail@web31215.mail.mud.yahoo.com>
References: <20051121154441.65685.qmail@web31215.mail.mud.yahoo.com>
Message-ID: <1132589487.17016.42.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2005-11-21 at 07:44 -0800, Xiaodong Jin wrote:
> I'm using R 2.1.1 and just successfully installed packages tseries, fseries.
>    
>   I try to run example
>   http://www.maths.lth.se/help/R/.R/library/tseries/html/garch.html
>   But it shows
>   > x.arch <- garch(x, order = c(0,2))  # Fit ARCH(2) 
> Error: couldn't find function "garch"
>    
>   Then I run command
>   > help.search("garch")
>   it shows the R information.

Did you load the package before trying to use it? E.g.:

library(tseries)

>From a fresh R session this is what I get:

> ?garch #error as I haven't loaded the package
No documentation for 'garch' in specified packages and libraries:
you could try 'help.search("garch")'
> library(tseries)
Loading required package: quadprog
Loading required package: zoo
> ?garch #displays help file as expected

>    
>   Which package(s) do I really need to run garch models?

Looks like you need quadprog and zoo to be installed and available as
well.

>   Thanks,
>   Shelton

HTH,

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From loncard at who.int  Mon Nov 21 17:21:52 2005
From: loncard at who.int (Loncar, Dejan)
Date: Mon, 21 Nov 2005 17:21:52 +0100
Subject: [R] conversion from RData to R file
Message-ID: <349865A6DFCF704488DCA7BB5CC83BED71EEBE@HQSWKAKI01.hq.intra.who.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/90aee3b3/attachment.pl

From gunter.berton at gene.com  Mon Nov 21 17:24:19 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 21 Nov 2005 08:24:19 -0800
Subject: [R] Comparing rows of matrices with different dimensions
In-Reply-To: <686C1FDE894539418C5668E5E6DE12DE051033@muc-exch-tmp.komdat.intern>
Message-ID: <200511211624.jALGOJTs023570@compton.gene.com>

If I understand you correctly, 

## not tested

apply(cbind(a[,-3],b),1,function(x)isTRUE(all.equal(x[1:2],x[3:4])))

or something similar. The basic idea is just to drop the last column of a
and check to see whether rows are the same (possible within numeric fuzz).
This assumes order counts. If the rows must just contain the same values
(possibly replicated different numbers of times, then add calls to unique
(or maybe use setdiff if numeric fuzz is not an issue).

HTH

Cheers,
Bert

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Antje D??ring
> Sent: Monday, November 21, 2005 7:57 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Comparing rows of matrices with different dimensions
> 
>  
> 
> Hi there,
> 
>  
> 
> I have a question, which I thought is very easy to solve, but 
> somehow I can't find a solution. Probably someone could help 
> me quickly?
> 
>  
> 
> Here it is:
> 
>  
> 
> I have two matrices:
> 
>  
> 
> a
> 
>      [,1] [,2] [,3]
> 
> [1,]    1    4    9
> 
> [2,]    2    6   10
> 
> [3,]    3    6   11
> 
> [4,]    4    8   12
> 
>  
> 
>  
> 
> b
> 
>      [,1] [,2]
> 
> [1,]    1    4
> 
> [2,]    2    5
> 
> [3,]    3    6
> 
>  
> 
> Now I want to find out which rows of b can also be found in a 
> (without its last column). So the solution must be something 
> like either "TRUE FALSE TRUE" or the rows where their is a 
> match (rows 1 and 3)
> 
>  
> 
> Till now I have tried things like b %in% a[,1:2] or so but 
> that doesn't work because I want to compare the WHOLE row of 
> b with the whole row of a without column 3.
> 
>  
> 
> Thank you very much for any help.
> 
>  
> 
> Regards, Antje
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mschwartz at mn.rr.com  Mon Nov 21 17:41:26 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 21 Nov 2005 10:41:26 -0600
Subject: [R] Comparing rows of matrices with different dimensions
In-Reply-To: <686C1FDE894539418C5668E5E6DE12DE051033@muc-exch-tmp.komdat.intern>
References: <686C1FDE894539418C5668E5E6DE12DE051033@muc-exch-tmp.komdat.intern>
Message-ID: <1132591287.5471.19.camel@localhost.localdomain>

On Mon, 2005-11-21 at 16:57 +0100, Antje DÃ¶ring wrote:
>  
> Hi there,
> 
>  
> 
> I have a question, which I thought is very easy to solve, but somehow
> I can't find a solution. Probably someone could help me quickly?
> 
>  
> 
> Here it is:
> 
>  
> 
> I have two matrices:
> 
>  
> 
> a
> 
>      [,1] [,2] [,3]
> 
> [1,]    1    4    9
> 
> [2,]    2    6   10
> 
> [3,]    3    6   11
> 
> [4,]    4    8   12
> 
>  
> 
> 
> 
> b
> 
>      [,1] [,2]
> 
> [1,]    1    4
> 
> [2,]    2    5
> 
> [3,]    3    6
> 
>  
> 
> Now I want to find out which rows of b can also be found in a (without
> its last column). So the solution must be something like either "TRUE
> FALSE TRUE" or the rows where their is a match (rows 1 and 3)
> 
>  
> 
> Till now I have tried things like b %in% a[,1:2] or so but that
> doesn't work because I want to compare the WHOLE row of b with the
> whole row of a without column 3.
> 
>  
> 
> Thank you very much for any help.
> 
>  
> 
> Regards, Antje


Here is one possible approach, though not tested beyond this example:

> which(apply(matrix(b %in% a, dim(b)), 1, all))
[1] 1 3


The steps are:

# which elements of b are in a
> b %in% a
[1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE


# Turn that into a matrix the same shape as b
> matrix(b %in% a, dim(b))
     [,1]  [,2]
[1,] TRUE  TRUE
[2,] TRUE FALSE
[3,] TRUE  TRUE


# get T/F for which rows are all TRUE
> apply(matrix(b %in% a, dim(b)), 1, all)
[1]  TRUE FALSE  TRUE


# Now get the indices
> which(apply(matrix(b %in% a, dim(b)), 1, all))
[1] 1 3


HTH,

Marc Schwartz



From gunter.berton at gene.com  Mon Nov 21 18:01:08 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 21 Nov 2005 09:01:08 -0800
Subject: [R] conversion from RData to R file
In-Reply-To: <349865A6DFCF704488DCA7BB5CC83BED71EEBE@HQSWKAKI01.hq.intra.who.int>
Message-ID: <200511211701.jALH17xP029804@volta.gene.com>

Please read the docs! -- especially "An Introduction to R." There is no need
whatever to have your **data** in text format (other than, perhaps, to read
into into R), which is generally what "R format" (a .R suffix in teh
filename, I presume) indicates. 

See ?attach, perhaps? -- or maybe ?load .

Also please read the posting guide and provide a reproducible example as it
suggests to clearly communicate what you wish to do.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Loncar, Dejan
> Sent: Monday, November 21, 2005 8:22 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] conversion from RData to R file
> 
> 
> I would highly appreciate some hints regarding this issue. 
> 
> Regards,
> Dejan
> > _____________________________________________ 
> > From: 	Loncar, Dejan  
> > Sent:	15 November 2005 10:32
> > To:	'r-help at stat.math.ethz.ch'
> > Subject:	conversion from RData to R file
> > 
> > 
> > Dear all
> > I am beginner in R coding and have a problem to figure out how to
> > convert RData format into R format.
> > After I converted csv file using read.csv  I got RData file 
> but to run
> > some R code need R format
> > 
> > Many Thanks 
> > 
> > Dejan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dhrutiramani at yahoo.com  Mon Nov 21 18:01:39 2005
From: dhrutiramani at yahoo.com (Dhruti Ramani)
Date: Mon, 21 Nov 2005 09:01:39 -0800 (PST)
Subject: [R] ./configure: /bin/sh: bad interpreter: Permission denied
Message-ID: <20051121170139.34048.qmail@web50309.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/ae1f7efc/attachment.pl

From spencer.graves at pdf.com  Mon Nov 21 18:08:55 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Nov 2005 09:08:55 -0800
Subject: [R] Largest allowable matrix
In-Reply-To: <Pine.LNX.4.61.0511210743360.26749@gannet.stats>
References: <BFA78ECF.95E%bbaker@tnc.org>	<4381776B.6040707@statistik.uni-dortmund.de>
	<Pine.LNX.4.61.0511210743360.26749@gannet.stats>
Message-ID: <4381FF27.5080905@pdf.com>

	  What do you want to do with these large matrices?  Both "scan" and 
"read.table" allow you to skip a certain number of lines at the 
beginning of a file and process however many lines you want from that 
point.

	  I recently had large files that were too big for S-Plus 6.  I moved 
to R, and processed them as submatrices without a problem.  I typically 
use "readLines" to check the format of the first few records and 
"count.fields" to determine if all records have the same numbers of 
fields.  In one case recently, I had a file that was almost but not 
quite regular.  I processed the file in pieces, carefully examining 
records right before and after each change in the number of records, and 
recovered basically everything without going back to my client (through 
several layers of bureaucracy) to ask for their help in parsing that file.

	  I frequently use a construct like the following:

File. <- ".....<filename>"
readLines(File., 9)
# to check the format including the "sep" character
quantile(nFlds <- count.fields(File., sep="\t")) #or sep="," for csv

# If the file honestly has a fixed number of fields,
# this will show that.
# If not, either the "sep" character is wrong or the file has problems.
# In either case, this helps me plan what to do next.

	  hope this helps.
	  spencer graves

Prof Brian Ripley wrote:

> On Mon, 21 Nov 2005, Uwe Ligges wrote:
> 
> 
>>Barry Baker wrote:
>>
>>
>>>Hello,
>>>
>>>I am a new R user and have two datasets that I would like to analyze.  The
>>>first is (2409222 x 17) and the other is (21682998 x 17). Is this possible
>>>in R?  If not then what is the maximum number of rows and columns or number
>>>of elements that R can handle?
>>
>>
>>The number of columns and rows is not a problem here, but you will need
>>21682998 * 17 * 4 bytes to store the latter matrix (assuming floats) in
>>memory, that is 1406.139 Mb.
> 
> 
> R does not use floats internally.  So unless these are integers/logicals 
> you are going to need twice that,
> 
> 
>>In order to do something sensible with the data, you need *at least*
>>twice the amount of RAM, hence at least 3Gb.
> 
> 
> Here I think the issue is rather virtual memory and address space.  You 
> will need a 64-bit OS to do anything with this object.
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From gunter.berton at gene.com  Mon Nov 21 18:11:16 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 21 Nov 2005 09:11:16 -0800
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <200511211711.jALHBGOw013426@ohm.gene.com>

 

Newbies (and others!) may find useful the R Reference Card made available by
Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
the "Contributed" link on CRAN (where some other reference cards are also
linked). It categorizes and organizes a bunch of R's basic, most used
functions so that they can be easily found. For example, paste() is under
the "Strings" heading and expand.grid() is under "Data Creation." For
newbies struggling to find the right R function as well as veterans who
can't quite remember the function name, it's very handy.
 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From matthieu.cornec at gmail.com  Mon Nov 21 18:16:20 2005
From: matthieu.cornec at gmail.com (Matthieu Cornec)
Date: Mon, 21 Nov 2005 18:16:20 +0100
Subject: [R] Help with xtable
Message-ID: <8a83e5000511210916n76292305r1737861759118081@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/cf942bba/attachment.pl

From djames at frontierassoc.com  Mon Nov 21 18:17:41 2005
From: djames at frontierassoc.com (David C. James)
Date: Mon, 21 Nov 2005 11:17:41 -0600
Subject: [R] Multinomial Nested Logit package in R?
Message-ID: <5330C869-C095-43BA-BCEE-8F6DECA8C848@frontierassoc.com>

Dear R-Help,

I'm hoping to find a Multinomial Nested Logit package in R.  It would  
be great to find something analogous to "PROC MDC" in SAS:
> The MDC (Multinomial Discrete Choice) procedure analyzes models  
> where the
> choice set consists of multiple alternatives. This procedure  
> supports conditional logit,
> mixed logit, heteroscedastic extreme value, nested logit, and  
> multinomial probit mod-
> els. The MDC procedure uses the maximum likelihood (ML) or  
> simulated maximum
> likelihood method for model estimation. Since the term multinomial  
> logit is often
> used instead of conditional logit in econometrics literature, the  
> term simple multino-
> mial logit is used here to denote the model used by Schmidt and  
> Strauss (1975), while
> multinomial logit is used as a synonym of conditional logit.

I found this web site useful in comparing various categorical  
dependent variable models, but it only addresses SAS, STATA, LIMDEP,  
and SPSS:
http://www.indiana.edu/~statmath/stat/all/cdvm/cdvm1.html

I have searched using:
1. Google> site:r-project multinomial nested logit
2. Google> site:r-project.org multinomial discrete choice
3. R> help.search("multinomial nested logit")
No help files found matching ?multinomial nested logit? using fuzzy  
matching
4. R> help.search("multinomial discrete choice")
No help files found matching ?multinomial discrete choice? using  
fuzzy matching

Possibilities that seem unclear and/or not very promising:
1. It is my understanding that the multinom function in the nnet  
package does NOT do nested models.
2. The MNP (multinomial probit) package is close, but I want a logit,  
not probit, model.
3. http://www.r-project.org/nocvs/mail/r-help/2002/4394.html
On Wed, 29 May 2002, Vumani Dlamini wrote:
> > Has anyone implemented the conditional logit and/or the nested  
> logit model
> > in R?
>
> Conditional logistic regression is clogit() in the survival package.
But the survival package didn't seem to have what I needed.  Am I  
overlooking anything?

Are there any synonyms that I should be using for the search?  I  
would appreciate any pointers or suggestions.

Thanks,
-David



From redelico_f at yahoo.com.ar  Mon Nov 21 18:20:11 2005
From: redelico_f at yahoo.com.ar (Francisco Redelico)
Date: Mon, 21 Nov 2005 14:20:11 -0300 (ART)
Subject: [R] Monte Carlo EM for GLMM
Message-ID: <20051121172011.69631.qmail@web60711.mail.yahoo.com>

Dear All,

I have to programme a Monte Carlo EM for an
Generalized Linear Mixed Model, Binomial Response and
Normal Random Effect, Could anyone give me a hand
sending some R code?

TIA 

Francisco 


	


	
		
___________________________________________________________ 
1GB gratis, Antivirus y Antispam 
Correo Yahoo!, el mejor correo web del mundo 
http://correo.yahoo.com.ar



From gianpaolo.romeo at nectarine.it  Mon Nov 21 11:12:23 2005
From: gianpaolo.romeo at nectarine.it (gianpaolo.romeo@nectarine.it)
Date: Mon, 21 Nov 2005 11:12:23 +0100
Subject: [R] Plotting one or more series on the same graphs
Message-ID: <1132567943.43819d8729822@mail.nectarine.it>

Hi, I'm from Italy (sorry for my english...). I've two questions about
the plot function.
I've to create a simple graph for the data set "n_species":


species=sqlQuery(dati, "select count(distinct species), season from
captures_complete_r group by species, season")
n_species=tapply(species$count, species$season, sum)
n_species=data.frame(n_species)
n_species
       n_species
autumn        10
spring         7
summer         7
winter         7


The problem is that, if I use "plot(n_species)", I can't put the seasons
on the x-axis, and so the values on that axes appear as numbers.
I've tried to remove the axis and then I've add with the script:

plot(n_species, type="l", main="Species trend", xlab="Season",  
ylab="Number of species",ylim=c(3,10), col=1, axes=FALSE)
legend("topright", c("Species captured"), col = c(1), lty=c(1))
axis(1,  1:4, c("autumn", "spring", "summer", "winter") )
axis(2, 1:11)


and it seems to work, but when I have decimal value on the y-axis, the
script


plot(k$simp_seas, ylim=c(min(k), max(k)), type="l",main="Index trend", 
xlab="Season", ylab="Index value", axes=FALSE)
points(k$shan_seas, col="red", type="l")
leg.txt=c("Simpson's index", "Shannon's index")
legend("topleft", leg.txt, col=c(1,2), lty=c(1,1))
axis(1,  1:4, c("autumn", "spring", "summer", "winter") )
axis(2, 1:11)

doesn't put decimal value, but jut a "1" in the middle of the vertical
axis.
How can make a correct graphs?

Thanks, Gianpaolo.



From Iyue.Sung at lm.mmc.com  Mon Nov 21 19:01:05 2005
From: Iyue.Sung at lm.mmc.com (Sung, Iyue)
Date: Mon, 21 Nov 2005 13:01:05 -0500
Subject: [R] Multinomial Nested Logit package in R?
Message-ID: <B01A505EA0E3124998DF4661D313B206012EF3B2@mmci-bos03fs01.mmci.ad.root>

Peter Rossi has a package "bayesm" for various flavors of choice models.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David C. James
> Sent: Monday, November 21, 2005 12:18 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Multinomial Nested Logit package in R?
> 
> Dear R-Help,
> 
> I'm hoping to find a Multinomial Nested Logit package in R.  
> It would be great to find something analogous to "PROC MDC" in SAS:
> > The MDC (Multinomial Discrete Choice) procedure analyzes 
> models where 
> > the choice set consists of multiple alternatives. This procedure 
> > supports conditional logit, mixed logit, heteroscedastic extreme 
> > value, nested logit, and multinomial probit mod- els. The MDC 
> > procedure uses the maximum likelihood (ML) or simulated maximum 
> > likelihood method for model estimation. Since the term multinomial 
> > logit is often used instead of conditional logit in econometrics 
> > literature, the term simple multino- mial logit is used 
> here to denote 
> > the model used by Schmidt and Strauss (1975), while 
> multinomial logit 
> > is used as a synonym of conditional logit.
> 
> I found this web site useful in comparing various categorical 
> dependent variable models, but it only addresses SAS, STATA, 
> LIMDEP, and SPSS:
> http://www.indiana.edu/~statmath/stat/all/cdvm/cdvm1.html
> 
> I have searched using:
> 1. Google> site:r-project multinomial nested logit 2. Google> 
> site:r-project.org multinomial discrete choice 3. R> 
> help.search("multinomial nested logit") No help files found 
> matching 'multinomial nested logit' using fuzzy matching 4. 
> R> help.search("multinomial discrete choice") No help files 
> found matching 'multinomial discrete choice' using fuzzy matching
> 
> Possibilities that seem unclear and/or not very promising:
> 1. It is my understanding that the multinom function in the 
> nnet package does NOT do nested models.
> 2. The MNP (multinomial probit) package is close, but I want 
> a logit, not probit, model.
> 3. http://www.r-project.org/nocvs/mail/r-help/2002/4394.html
> On Wed, 29 May 2002, Vumani Dlamini wrote:
> > > Has anyone implemented the conditional logit and/or the nested
> > logit model
> > > in R?
> >
> > Conditional logistic regression is clogit() in the survival package.
> But the survival package didn't seem to have what I needed.  
> Am I overlooking anything?
> 
> Are there any synonyms that I should be using for the search? 
>  I would appreciate any pointers or suggestions.
> 
> Thanks,
> -David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> --------------------------------------------------------------
> --------------
> This e-mail and any attachments may be confidential or\ > ...{{dropped}}



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov 21 19:14:29 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 21 Nov 2005 18:14:29 -0000 (GMT)
Subject: [R] conversion from RData to R file
In-Reply-To: <200511211701.jALH17xP029804@volta.gene.com>
Message-ID: <XFMail.051121181429.Ted.Harding@nessie.mcc.ac.uk>

On 21-Nov-05 Berton Gunter wrote:
> Please read the docs! -- especially "An Introduction to R."
> There is no need whatever to have your **data** in text format
> (other than, perhaps, to read into into R), which is generally
> what "R format" (a .R suffix in the filename, I presume) indicates.

I would qualify this (very slightly).

One of the good reasons for having data in a text file is that
it is easy to edit it, along with the fact that a frequent
format for "primary" data is textual -- e.g. CSV.

In my experience, one can often encounter errors in data that
need correction quite some time after they have first been
used in R. (I'm talking mainly about non-obvious errors:
plausible values that are in fact wrong, missing values in
the wrong place or extra ones introduced, etc.)

Once one has saved out data as R-data, there is a psychological
prejudice that they don't need looking at again (this is one
of those psycho-logical "inferences", which "follows" from
the practical fact that data in R-Data format are "for" just
loading).

Just a thought ...
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 21-Nov-05                                       Time: 18:14:28
------------------------------ XFMail ------------------------------



From Mike.Prager at noaa.gov  Mon Nov 21 19:19:35 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Mon, 21 Nov 2005 13:19:35 -0500
Subject: [R] Warning message help
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F304C8658@FWC-TLEX3.fwc.state.fl.us>
References: <BA6FF017E924044A9BF748AFAEEA6F304C8658@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <43820FB7.5030501@noaa.gov>

Cameron

Assuming DIST_TOW is a vector of the same length as effort and Gear, is 
this what you mean?

effort[Gear==300] = DIST_TOW[Gear==300]*7412


MHP


on 11/21/2005 10:44 AM Guenther, Cameron said the following:

>I am trying to great a new column of effort data from an existing vector
>of gears used.
>It is a simple code where 
>
>effort[Gear==300]=(DIST_TOW*7412)
>effort[Gear==301]=(DIST_TOW*7412)
>
>[...]
>  
>

-- 

Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
Opinions expressed are personal, not official.  No
government endorsement of any product is made or implied.



From mschwartz at mn.rr.com  Mon Nov 21 19:38:17 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 21 Nov 2005 12:38:17 -0600
Subject: [R] ./configure: /bin/sh: bad interpreter: Permission denied
In-Reply-To: <20051121170139.34048.qmail@web50309.mail.yahoo.com>
References: <20051121170139.34048.qmail@web50309.mail.yahoo.com>
Message-ID: <1132598297.5471.56.camel@localhost.localdomain>

On Mon, 2005-11-21 at 09:01 -0800, Dhruti Ramani wrote:
> I am trying to install R1.8. When I use "make" to build R, it gives me following error,
> 
> * Installing *source* package 'foreign' ...
> /usr/local/bin/R-1.8.1/bin/INSTALL: ./configure: /bin/sh: bad interpreter: Permission denied
> ERROR: configuration failed for package 'foreign'
> ** Removing '/usr/local/bin/R-1.8.1/library/foreign'
> make[2]: *** [foreign.ts] Error 1
> make[2]: Leaving directory `/usr/local/bin/R-1.8.1/src/library/Recommended'
> make[1]: *** [recommended-packages] Error 2
> make[1]: Leaving directory `/usr/local/bin/R-1.8.1/src/library/Recommended'
> make: *** [stamp-recommended] Error 2
> 
> 
> Am I doing something wrong here? I am installing as root.
> 
> Thanks,
> Denna

First, you are trying to install a version of R (1.8.1) that is two
years old today.

I would suggest getting either the current release version tarball for
2.2.0, or better would be the R 2.2.0 patched version.

The former is available from your local CRAN mirror, the latter is
available at:

  ftp://ftp.stat.math.ethz.ch/Software/R/R-patched.tar.gz

I would try to install using a current version of R first to see if the
error is still present.

If it is, then you may have an access/permission issue with /tmp. Review
sections 2.1 and 5.1 in the R Admin Manual regarding /tmp and the TMPDIR
environment variable for more information.

HTH,

Marc Schwartz



From deepayan.sarkar at gmail.com  Mon Nov 21 20:07:00 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 21 Nov 2005 13:07:00 -0600
Subject: [R] nlme question
In-Reply-To: <AF2DCD619279544BA454141F4A45B9E306A3CF@m-niosh-3.niosh.cdc.gov>
References: <AF2DCD619279544BA454141F4A45B9E306A3CF@m-niosh-3.niosh.cdc.gov>
Message-ID: <eb555e660511211107v140a27cak3ba43dab1df4fc1f@mail.gmail.com>

On 11/21/05, Wassell, James T., Ph.D. <jtw2 at cdc.gov> wrote:
> Deepayan,
>
> Yes, thanks for confirming my suspicions.  I know mixed models are
> "different" but, I did not think they were so different as to preclude
> estimating the var-cov matrix (via the Hessian in Maximum likelihood, as
> you point out).
>
> Thanks for prompting me to think about MCMC.  Your suggestion to
> consider MCMC makes me realize that using BUGS, I could directly sample
> from the posterior of the linear combination of parameters - to get its
> variance and eliminate the extra step using the var-cov matrix.   As you
> say, with results better than the asymptotic approximation. (Maybe I can
> do the same thing with mcmcsamp?, but I'm not familiar with this and
> will have to take a look at it.)

That should be easy. mcmcsamp produces "mcmc" objects, which are
essentially matrices, with each row containing one set of parameter
values. Getting a sample of a linear combination is one call to %*%
away.

Deepayan



From pedroestrela at yahoo.com  Mon Nov 21 20:18:16 2005
From: pedroestrela at yahoo.com (Pedro Cordeiro Estrela)
Date: Mon, 21 Nov 2005 11:18:16 -0800 (PST)
Subject: [R] Problem in compilation from source in./configure R.2.2
Message-ID: <20051121191816.14751.qmail@web54207.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/bd65ebd3/attachment.pl

From mschwartz at mn.rr.com  Mon Nov 21 20:22:40 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 21 Nov 2005 13:22:40 -0600
Subject: [R] Plotting one or more series on the same graphs
In-Reply-To: <1132567943.43819d8729822@mail.nectarine.it>
References: <1132567943.43819d8729822@mail.nectarine.it>
Message-ID: <1132600961.5471.86.camel@localhost.localdomain>

On Mon, 2005-11-21 at 11:12 +0100, gianpaolo.romeo at nectarine.it wrote:
> Hi, I'm from Italy (sorry for my english...). I've two questions about
> the plot function.
> I've to create a simple graph for the data set "n_species":
> 
> 
> species=sqlQuery(dati, "select count(distinct species), season from
> captures_complete_r group by species, season")
> n_species=tapply(species$count, species$season, sum)
> n_species=data.frame(n_species)
> n_species
>        n_species
> autumn        10
> spring         7
> summer         7
> winter         7
> 
> 
> The problem is that, if I use "plot(n_species)", I can't put the seasons
> on the x-axis, and so the values on that axes appear as numbers.

The problem here is that by default, plot() is using the result of:

  axTicks(1)

as the x labels and tick mark locations, which yields:

> axTicks(1)
[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0

with four data points.  Actually, R is using a C coded equivalent of
axTicks() internally, but the result is the same. See ?axTicks for more
information.

> I've tried to remove the axis and then I've add with the script:
> 
> plot(n_species, type="l", main="Species trend", xlab="Season",  
> ylab="Number of species",ylim=c(3,10), col=1, axes=FALSE)
> legend("topright", c("Species captured"), col = c(1), lty=c(1))
> axis(1,  1:4, c("autumn", "spring", "summer", "winter") )
> axis(2, 1:11)

This is better and gives you finer control over the axis labels and tick
mark locations. However, using axis(2, 1:11) will cause problems as the
'y' ranges of your values change, as you see below.

> and it seems to work, but when I have decimal value on the y-axis, the
> script
> 
> 
> plot(k$simp_seas, ylim=c(min(k), max(k)), type="l",main="Index trend", 
> xlab="Season", ylab="Index value", axes=FALSE)
> points(k$shan_seas, col="red", type="l")
> leg.txt=c("Simpson's index", "Shannon's index")
> legend("topleft", leg.txt, col=c(1,2), lty=c(1,1))
> axis(1,  1:4, c("autumn", "spring", "summer", "winter") )
> axis(2, 1:11)
> 
> doesn't put decimal value, but jut a "1" in the middle of the vertical
> axis.
> How can make a correct graphs?
> 
> Thanks, Gianpaolo.

Since we don't have your actual data, I can provide some guidance, but
not a specific solution.

In your examples above, you need to exclude the x axis from being drawn,
but not the y axis.  Using 'axes = FALSE' will preclude both axes from
being drawn.

Thus:

Instead of using 'axes = FALSE', use 'xaxt = "n"', which will preclude
the x axis only from being drawn. You will then end up with the default
y axis tick marks and labels.

See ?par for more information.

If you want more control over the y axis values as well, you can
continue to use 'axes = FALSE', but will need to consider how you want
the tick marks and labels to look and then use those values as the
arguments to axis(2, ...) as appropriate, rather than hard coding the
values and expecting them to work in all cases.

In addition, you might want to take a look at ?matplot, which will
enable you to plot multiple series in a single plot.

HTH,

Marc Schwartz



From mschwartz at mn.rr.com  Mon Nov 21 20:35:46 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 21 Nov 2005 13:35:46 -0600
Subject: [R] Help with xtable
In-Reply-To: <8a83e5000511210916n76292305r1737861759118081@mail.gmail.com>
References: <8a83e5000511210916n76292305r1737861759118081@mail.gmail.com>
Message-ID: <1132601746.5471.97.camel@localhost.localdomain>

On Mon, 2005-11-21 at 18:16 +0100, Matthieu Cornec wrote:
> Hello,
>  How do you change the size of the caracters (tiny, small) using xtable?
> It works out for print.xtable when typing
> print.xtable(xtable(mydata),size="small")
> but I do not see any results when doing
> xtable(mydata,size="small")
>  Anyone could help ?
> Thanks
>  Matthieu


As documented, xtable() ignores any "..." arguments specified. Thus
passing 'size' as part of the call here has no effect, since 'size' is
not an enumerated argument for xtable().

print.xtable() has a 'size' argument enumerated, so it is respected
there.

BTW, print.xtable(..) is not required.

   print(xtable(mydata), size = "small") 

will work and is preferred over calling the method directly.

HTH,

Marc Schwartz



From eric_wzl at yahoo.com  Mon Nov 21 20:54:03 2005
From: eric_wzl at yahoo.com (peter eric)
Date: Mon, 21 Nov 2005 11:54:03 -0800 (PST)
Subject: [R] how to plot a list in graphs
Message-ID: <20051121195403.16115.qmail@web36409.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/31d78cf9/attachment.pl

From dusa.adrian at gmail.com  Mon Nov 21 20:18:54 2005
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Mon, 21 Nov 2005 21:18:54 +0200
Subject: [R] attributes of a data.frame
Message-ID: <200511212118.54937.dusa.adrian@gmail.com>


Dear all,

I noticed that a data.frame has four attributes:
- names
- row.names
- class
- variable.labels

While one can use the first three (i.e. names(foo) or class(foo)), the fourth 
one can only be used via:
attributes(foo)$variable.labels
(which is kind of a tedious thing to type)

Is it or would be possible to simply use:
variable.labels(foo)
like the first three attributes?

I tried:
varlab <- function(x) attributes(x)$variable.labels

but then I cannot use this to assign a specific label:
> varlab(foo)[1] <- "some string"
Error: couldn't find function "varlab<-"

Thank you,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From gunter.berton at gene.com  Mon Nov 21 21:24:33 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 21 Nov 2005 12:24:33 -0800
Subject: [R] attributes of a data.frame
In-Reply-To: <200511212118.54937.dusa.adrian@gmail.com>
Message-ID: <200511212024.jALKOXk0016687@hertz.gene.com>


?assign



-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adrian DUSA
> Sent: Monday, November 21, 2005 11:19 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] attributes of a data.frame
> 
> 
> Dear all,
> 
> I noticed that a data.frame has four attributes:
> - names
> - row.names
> - class
> - variable.labels
> 
> While one can use the first three (i.e. names(foo) or 
> class(foo)), the fourth 
> one can only be used via:
> attributes(foo)$variable.labels
> (which is kind of a tedious thing to type)
> 
> Is it or would be possible to simply use:
> variable.labels(foo)
> like the first three attributes?
> 
> I tried:
> varlab <- function(x) attributes(x)$variable.labels
> 
> but then I cannot use this to assign a specific label:
> > varlab(foo)[1] <- "some string"
> Error: couldn't find function "varlab<-"
> 
> Thank you,
> Adrian
> 
> -- 
> Adrian DUSA
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>           +40 21 3120210 / int.101
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From deepayan.sarkar at gmail.com  Mon Nov 21 21:24:56 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 21 Nov 2005 14:24:56 -0600
Subject: [R] Adding points to wireframe
In-Reply-To: <437E494B.9090201@polymtl.ca>
References: <437E494B.9090201@polymtl.ca>
Message-ID: <eb555e660511211224t5e86b162kfe2cbb7d9b926ed7@mail.gmail.com>

On 11/18/05, Pierre-Luc Brunelle <pierre-luc.brunelle at polymtl.ca> wrote:
> Hi,
>
> I am using function wireframe from package lattice to draw a 3D surface.
> I would like to add a few points on the surface. I read in a post from
> Deepayan Sarkar that "To do this in a wireframe plot you would probably
> use the panel function panel.3dscatter". Does someone have an example?

Here goes. Let's say the surface you want is:

surf <-
    expand.grid(x = seq(-pi, pi, length = 50),
                y = seq(-pi, pi, length = 50))

surf$z <-
    with(surf, {
        d <- 3 * sqrt(x^2 + y^2)
        exp(-0.02 * d^2) * sin(d)
    })


To draw just the surface, you can of course do

library(lattice)
wireframe(z ~ x * y, g, aspect = c(1, .5),
          scales = list(arrows = FALSE))

To modify the display, you will want to write your own
panel.3d.wireframe function (as suggested in ?panel.cloud), which
defaults to 'panel.3dwire'.  The first thing to realise is that
lots of funny things go on inside panel functions that you don't
really want to know about.  The trick is to make liberal use of the
... argument, only naming arguments that you need to work with or
override.  So our first try might be to write an explicit but minimal
panel.3d.wireframe function that does nothing new:


wireframe(z ~ x * y, g, aspect = c(1, .5),
          scales = list(arrows = FALSE),
          panel.3d.wireframe = function(...) {
              panel.3dwire(...)
          })

Now, you want to add points using 3dscatter, and that's obviously
going to use different data.  However, most of the rest of the
arguments will be the same for the panel.3dwire and panel.3dscatter
calls (giving details of the 3-D to 2-D projection, mostly), so you need to
capture the data passed in to panel.3d.wireframe, without worrying
about the exact form of the rest of the arguments.  This can be done by:


wireframe(z ~ x * y, g, aspect = c(1, .5),
          scales = list(arrows = FALSE),
          panel.3d.wireframe = function(x, y, z, ...) {
              panel.3dwire(x = x, y = y, z = z, ...)
          })

which is of course the same as before.  Now let's add a few points
using panel.3dscatter:


wireframe(z ~ x * y, g, aspect = c(1, .5),
          scales = list(arrows = FALSE),
          panel.3d.wireframe = function(x, y, z, ...) {
              panel.3dwire(x = x, y = y, z = z, ...)
              panel.3dscatter(x = runif(10, -0.5, 0.5),
                              y = runif(10, -0.5, 0.5),
                              z = runif(10, -0.25, 0.25),
                              ...)
          })

Note that the ... arguments have been passed on to panel.3dscatter.
Without it, you would have the error that you reported.

Now, presumably the points that you want to add are in the original
data scale, whereas panel.3dwire wants data in a different
(linearly shifted and scaled) scale suitable for 3-D
transformations.  I happened to know what that transformed scale is
(usually [-0.5, 0.5]), and the call above makes use of that
knowledge.  In practice, you would have to make the conversion from
data scale to transformed scale yourself, and that's where the *lim
and *lim.scaled arguments come in.  They contain the range of the
data cube in the original and transformed scales respectively.  So
let's say the points you want to add (in the original scale) are:


pts <-
    data.frame(x = runif(10, -pi, pi),
               y = runif(10, -pi, pi),
               z = runif(10, -1, 1))


Then the suitable transformation can be done as follows:

wireframe(z ~ x * y, g, aspect = c(1, .5),
          scales = list(arrows = FALSE),
          pts = pts,
          panel.3d.wireframe =
          function(x, y, z,
                   xlim, ylim, zlim,
                   xlim.scaled, ylim.scaled, zlim.scaled,
                   pts,
                   ...) {
              panel.3dwire(x = x, y = y, z = z,
                           xlim = xlim,
                           ylim = ylim,
                           zlim = zlim,
                           xlim.scaled = xlim.scaled,
                           ylim.scaled = ylim.scaled,
                           zlim.scaled = zlim.scaled,
                           ...)
              xx <-
                  xlim.scaled[1] + diff(xlim.scaled) *
                      (pts$x - xlim[1]) / diff(xlim)
              yy <-
                  ylim.scaled[1] + diff(ylim.scaled) *
                      (pts$y - ylim[1]) / diff(ylim)
              zz <-
                  zlim.scaled[1] + diff(zlim.scaled) *
                      (pts$z - zlim[1]) / diff(zlim)
              panel.3dscatter(x = xx,
                              y = yy,
                              z = zz,
                              xlim = xlim,
                              ylim = ylim,
                              zlim = zlim,
                              xlim.scaled = xlim.scaled,
                              ylim.scaled = ylim.scaled,
                              zlim.scaled = zlim.scaled,
                              ...)
          })

Hope this helps,

-Deepayan



From deepayan.sarkar at gmail.com  Mon Nov 21 21:28:59 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 21 Nov 2005 14:28:59 -0600
Subject: [R] Adding points to wireframe
In-Reply-To: <eb555e660511211224t5e86b162kfe2cbb7d9b926ed7@mail.gmail.com>
References: <437E494B.9090201@polymtl.ca>
	<eb555e660511211224t5e86b162kfe2cbb7d9b926ed7@mail.gmail.com>
Message-ID: <eb555e660511211228h745320f0v80f1fb8697cc232b@mail.gmail.com>

On 11/21/05, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> On 11/18/05, Pierre-Luc Brunelle <pierre-luc.brunelle at polymtl.ca> wrote:
> > Hi,
> >
> > I am using function wireframe from package lattice to draw a 3D surface.
> > I would like to add a few points on the surface. I read in a post from
> > Deepayan Sarkar that "To do this in a wireframe plot you would probably
> > use the panel function panel.3dscatter". Does someone have an example?
>
> Here goes. Let's say the surface you want is:
>
> surf <-
>     expand.grid(x = seq(-pi, pi, length = 50),
>                 y = seq(-pi, pi, length = 50))
>
> surf$z <-
>     with(surf, {
>         d <- 3 * sqrt(x^2 + y^2)
>         exp(-0.02 * d^2) * sin(d)
>     })

Add

g <- surf

here if you want the rest to work. Sorry about that.
-Deepayan



From gunter.berton at gene.com  Mon Nov 21 21:33:23 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 21 Nov 2005 12:33:23 -0800
Subject: [R] attributes of a data.frame
In-Reply-To: <200511212118.54937.dusa.adrian@gmail.com>
Message-ID: <200511212033.jALKXMtF020853@volta.gene.com>

No! Ignore my previous "advice" .

To assign attrributes of anything, see ?attributes.

Shame on me!

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adrian DUSA
> Sent: Monday, November 21, 2005 11:19 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] attributes of a data.frame
> 
> 
> Dear all,
> 
> I noticed that a data.frame has four attributes:
> - names
> - row.names
> - class
> - variable.labels
> 
> While one can use the first three (i.e. names(foo) or 
> class(foo)), the fourth 
> one can only be used via:
> attributes(foo)$variable.labels
> (which is kind of a tedious thing to type)
> 
> Is it or would be possible to simply use:
> variable.labels(foo)
> like the first three attributes?
> 
> I tried:
> varlab <- function(x) attributes(x)$variable.labels
> 
> but then I cannot use this to assign a specific label:
> > varlab(foo)[1] <- "some string"
> Error: couldn't find function "varlab<-"
> 
> Thank you,
> Adrian
> 
> -- 
> Adrian DUSA
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>           +40 21 3120210 / int.101
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mark.salsburg at gmail.com  Mon Nov 21 21:37:28 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Mon, 21 Nov 2005 15:37:28 -0500
Subject: [R] Removing Rows
Message-ID: <dd48e20f0511211237k13d63732ue1625abe1cd3ecf0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/aafc695f/attachment.pl

From mark.salsburg at gmail.com  Mon Nov 21 21:37:28 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Mon, 21 Nov 2005 15:37:28 -0500
Subject: [R] Removing Rows
Message-ID: <dd48e20f0511211237k13d63732ue1625abe1cd3ecf0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/aafc695f/attachment-0001.pl

From close2ceo at yahoo.com  Mon Nov 21 21:39:20 2005
From: close2ceo at yahoo.com (Xiaodong Jin)
Date: Mon, 21 Nov 2005 12:39:20 -0800 (PST)
Subject: [R] arima prediction
Message-ID: <20051121203920.49108.qmail@web31208.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/adeed3fc/attachment.pl

From statsfay at hotmail.com  Mon Nov 21 21:42:22 2005
From: statsfay at hotmail.com (Gao Fay)
Date: Mon, 21 Nov 2005 15:42:22 -0500
Subject: [R] How to assign the p value from >anova to a variable ?
Message-ID: <BAY104-F157102F019A2C0D0C61DA5D0530@phx.gbl>

Hi , 

If I need to assign the p-value from >anova(lm(y~x1*x2)) to a variable 
temp. How can I do that?
In other words, I need to let temp<-1.228e-07. I ever tried to use 
temp<-anova(lm(y~x1*x2))$Respone["x1:x2","Pr(>F)"], but it doesn't work. 
The result is like:
>anova(lm(y~x1*x2))
Analysis of Variance Table

Response: y
           Df Sum Sq Mean Sq  F value    Pr(>F)    
x1      1 0.1009  0.1009   8.7183   0.01835 *  
x2       1 0.5983  0.5983  51.6845 9.343e-05 ***
x1:x2  1 3.4940  3.4940 301.8363 1.228e-07 ***
Residuals   8 0.0926  0.0116   

Any idea is appreiciated, thanks a lot!
Fay



From murdoch at stats.uwo.ca  Mon Nov 21 21:41:06 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 21 Nov 2005 15:41:06 -0500
Subject: [R] attributes of a data.frame
In-Reply-To: <200511212118.54937.dusa.adrian@gmail.com>
References: <200511212118.54937.dusa.adrian@gmail.com>
Message-ID: <438230E2.2070007@stats.uwo.ca>

On 11/21/2005 2:18 PM, Adrian DUSA wrote:
> Dear all,
> 
> I noticed that a data.frame has four attributes:
> - names
> - row.names
> - class
> - variable.labels
> 
> While one can use the first three (i.e. names(foo) or class(foo)), the fourth 
> one can only be used via:
> attributes(foo)$variable.labels
> (which is kind of a tedious thing to type)
> 
> Is it or would be possible to simply use:
> variable.labels(foo)
> like the first three attributes?
> 
> I tried:
> varlab <- function(x) attributes(x)$variable.labels
> 
> but then I cannot use this to assign a specific label:
>> varlab(foo)[1] <- "some string"
> Error: couldn't find function "varlab<-"


Not all dataframes have the variable.labels attribute.  I'm guessing 
you've installed some contributed package to add them, or are importing 
an SPSS datafile using read.spss.  So don't expect varlab() or 
variable.labels() function to be a standard R function.

If you want to define it, definitions like this should work (but I can't 
test them):

varlab <- function(foo) attr(foo, "variable.labels")

"varlab<-" <- function(foo, label, value) {
   attr(foo, "variable.labels")[label] <- value
   foo
}

Use them like this:

varlab(x)  # to see the labels

varlab(x, "varname") <- "label"  # to set one

Duncan Murdoch



From deepayan.sarkar at gmail.com  Mon Nov 21 21:42:51 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 21 Nov 2005 14:42:51 -0600
Subject: [R] how to plot a list in graphs
In-Reply-To: <20051121195403.16115.qmail@web36409.mail.mud.yahoo.com>
References: <20051121195403.16115.qmail@web36409.mail.mud.yahoo.com>
Message-ID: <eb555e660511211242g2f6d1964g5a5fd45e0da428dc@mail.gmail.com>

On 11/21/05, peter eric <eric_wzl at yahoo.com> wrote:
> hi all,
>   I have a matrix and named each row and column as like below...
>
>    a<-matrix(c(seq(3,45,3),seq(10,6,-1)),4,5,byrow=F)
> > col<-c("peter","david","richrd","vincent","selva")
> > rows<-c("julius","caeser","anja","maya")
> > dimnames(a)<-list(rows,col)
> > a
>          peter david richrd vincent selva
> julius     3    15     27      39     9
> caeser   6    18     30      42     8
> anja       9    21     33      45     7
> maya   12    24     36      10     6
>
>   How I can plot this in graphs like julius Vs peter and so on..
>
>   whether it could ne done with image function?..

Sure, how about

image(a)

If you want the axes to be labelled appropriately, it might be easier with

library(lattice)
levelplot(a)

or perhaps

levelplot(a, scales = list(x = list(rot = 90)))

-Deepayan



From timh at insightful.com  Mon Nov 21 21:49:31 2005
From: timh at insightful.com (Tim Hesterberg)
Date: 21 Nov 2005 12:49:31 -0800
Subject: [R] Method for $
Message-ID: <SE2KEXCH01ktLxHbHuc000002ac@se2kexch01.insightful.com>

Ulrike Groemping <groemping at tfh-berlin.de> wrote:

>I have defined a class "myclass" and would like the slots to be extractable 
>not only by "@" but also by "$". I now try to write a method for "$" that 
>simply executes the request object at slotname, whenever someone calls 
>object$slotname for any object of class "myclass".
>I don't manage to find out how I can provide this function with "slotname", 
>so that one and the same function works for any arbitrary slotname a user 
>might choose.
>...

I would caution against defining methods for $.

In addition to Martin Maechler and Duncan Temple Lange's warnings
about the danger of this, I would note that it could make R run much
slower.  I once tried defining a method for it in S-PLUS; that
converted $ into a generic function, which slowed down every call to $,
of which there are many.

Even if it wouldn't slow down R, as we work to make R and S-PLUS more
compatible you or someone else might try your code in S-PLUS, and
cause a big speed hit.

>Maybe I could (and should?) have defined the class with just one slot
>that contains the list, which would make it behave like I want it
>immediately.

Why not make it a list with an S3 class, rather than an S4 class?

Tim Hesterberg

========================================================
| Tim Hesterberg       Research Scientist              |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3012, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Download the S+Resample library from www.insightful.com/downloads/libraries

Two Research Scientist positions:
	data mining
	frailty/mixed effects
    http://www.insightful.com/company/jobs.asp

Speak out about biased science in Washington D.C.
    http://home.comcast.net/~timhesterberg/ScientificIntegrity.html



From adi at roda.ro  Mon Nov 21 20:51:31 2005
From: adi at roda.ro (Adrian DUSA)
Date: Mon, 21 Nov 2005 21:51:31 +0200
Subject: [R] attributes of a data.frame
In-Reply-To: <438230E2.2070007@stats.uwo.ca>
References: <200511212118.54937.dusa.adrian@gmail.com>
	<438230E2.2070007@stats.uwo.ca>
Message-ID: <200511212151.31488.adi@roda.ro>

On Monday 21 November 2005 22:41, Duncan Murdoch wrote:
> [...snip...]
> Not all dataframes have the variable.labels attribute.  I'm guessing
> you've installed some contributed package to add them, or are importing
> an SPSS datafile using read.spss.  So don't expect varlab() or
> variable.labels() function to be a standard R function.

Aa-haa... of course you are right: I read them via read.spss. I understand. 
Now, just to the sake of it, would it be wrong to make it standard? 
Is there a special reason not to?


> If you want to define it, definitions like this should work (but I can't
> test them):
>
> varlab <- function(foo) attr(foo, "variable.labels")
>
> "varlab<-" <- function(foo, label, value) {
>    attr(foo, "variable.labels")[label] <- value
>    foo
> }
>
> Use them like this:
>
> varlab(x)  # to see the labels
>
> varlab(x, "varname") <- "label"  # to set one
>
> Duncan Murdoch

Thank you for the tip; I'll certainly use it.
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From sourceforge at metrak.com  Mon Nov 21 21:53:29 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Tue, 22 Nov 2005 07:53:29 +1100
Subject: [R] PNG-import into R
In-Reply-To: <1132589302.2936.15.camel@blue.chem.psu.edu>
References: <CA0BCF3BED56294AB91E3AD74B849FD504E34C64@us-arlington-0668.mail.saic.com>
	<1132589302.2936.15.camel@blue.chem.psu.edu>
Message-ID: <438233C9.90001@metrak.com>

Rajarshi Guha wrote:
> Right now it segfaults, so its not really useful yet.

Can I use that as a tag line?



From Roger.Bivand at nhh.no  Mon Nov 21 21:53:36 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 21 Nov 2005 21:53:36 +0100 (CET)
Subject: [R] PNG-import into R
In-Reply-To: <59484F5CC089B4499DDAC9503DA0BC6706A7A0@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <Pine.LNX.4.44.0511212141220.16434-100000@reclus.nhh.no>

On Mon, 21 Nov 2005, Sydler, Dominik wrote:

> Hi there
> 
> I'm looking for a function to read PNG-bitmap-images from a file into R.
> I only found:
>  - the pixmap-package which cannot import png or similar formats
>  - the rimage-package which can only import lossy jpeg-images (the
> convertion from png to jpeg modifies the data!)
> 
> Is there any possibility to read PNG-files?

Under Linux, install GDAL directly then package rgdal from source, I guess 
some variant of this under OSX. Under Windows, install the rgdal binary 
package from Prof. Ripley's repository - for me:

> options("repos")
$repos
                                CRAN                            CRANextra 
                            "@CRAN@" "http://www.stats.ox.ac.uk/pub/RWin" 

or download from:
http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.2/
directly. Then:

> x.png <- GDAL.open(file.choose())
> getDriverLongName(getDriver(x.png))
[1] "Portable Network Graphics"

>From there you use getRasterData() on the bands and subscenes you want, 
GDAL.open() just returns a file handle, letting you choose the parts you 
need. That is probably the least resistance path.

> 
> Thanks for any help
>          Dominic Sydler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From murdoch at stats.uwo.ca  Mon Nov 21 22:00:03 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 21 Nov 2005 16:00:03 -0500
Subject: [R] attributes of a data.frame
In-Reply-To: <200511212151.31488.adi@roda.ro>
References: <200511212118.54937.dusa.adrian@gmail.com>
	<438230E2.2070007@stats.uwo.ca> <200511212151.31488.adi@roda.ro>
Message-ID: <43823553.5090408@stats.uwo.ca>

On 11/21/2005 2:51 PM, Adrian DUSA wrote:
> On Monday 21 November 2005 22:41, Duncan Murdoch wrote:
>> [...snip...]
>> Not all dataframes have the variable.labels attribute.  I'm guessing
>> you've installed some contributed package to add them, or are importing
>> an SPSS datafile using read.spss.  So don't expect varlab() or
>> variable.labels() function to be a standard R function.
> 
> Aa-haa... of course you are right: I read them via read.spss. I understand. 
> Now, just to the sake of it, would it be wrong to make it standard? 
> Is there a special reason not to?

I think it's just that the R core developers don't see the need for 
them.  If something is worth documenting, then you should write an .Rd 
file or a vignette about it, and that gives you more flexibility than a 
one line label.

I think there are definitely developers out there who disagree with this 
point of view, and I'm pretty sure I've seen a contributed package that 
offered support for this, but I can't remember which one right now.  So 
that's another reason why it's not in the base:  it doesn't need to be, 
you can just go find and install that contributed package!

Duncan Murdoch

> 
> 
>> If you want to define it, definitions like this should work (but I can't
>> test them):
>>
>> varlab <- function(foo) attr(foo, "variable.labels")
>>
>> "varlab<-" <- function(foo, label, value) {
>>    attr(foo, "variable.labels")[label] <- value
>>    foo
>> }
>>
>> Use them like this:
>>
>> varlab(x)  # to see the labels
>>
>> varlab(x, "varname") <- "label"  # to set one
>>
>> Duncan Murdoch
> 
> Thank you for the tip; I'll certainly use it.
> Adrian
>



From adi at roda.ro  Mon Nov 21 21:08:10 2005
From: adi at roda.ro (Adrian DUSA)
Date: Mon, 21 Nov 2005 22:08:10 +0200
Subject: [R] attributes of a data.frame
In-Reply-To: <43823553.5090408@stats.uwo.ca>
References: <200511212118.54937.dusa.adrian@gmail.com>
	<200511212151.31488.adi@roda.ro> <43823553.5090408@stats.uwo.ca>
Message-ID: <200511212208.10110.adi@roda.ro>

On Monday 21 November 2005 23:00, Duncan Murdoch wrote:
> [...snip...]
> I think it's just that the R core developers don't see the need for
> them.  If something is worth documenting, then you should write an .Rd
> file or a vignette about it, and that gives you more flexibility than a
> one line label.
>
> I think there are definitely developers out there who disagree with this
> point of view, and I'm pretty sure I've seen a contributed package that
> offered support for this, but I can't remember which one right now.  So
> that's another reason why it's not in the base:  it doesn't need to be,
> you can just go find and install that contributed package!
>
> Duncan Murdoch

I got it, it's logic. Well, one could always use Hmisc which does very well 
these things.
Thank you again,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From eric_wzl at yahoo.com  Mon Nov 21 22:40:39 2005
From: eric_wzl at yahoo.com (peter eric)
Date: Mon, 21 Nov 2005 13:40:39 -0800 (PST)
Subject: [R] how to plot a list in graphs
In-Reply-To: <eb555e660511211242g2f6d1964g5a5fd45e0da428dc@mail.gmail.com>
Message-ID: <20051121214039.41238.qmail@web36401.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/c98440d5/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Nov 21 23:26:25 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Nov 2005 23:26:25 +0100
Subject: [R] Problem in compilation from source in./configure R.2.2
In-Reply-To: <20051121191816.14751.qmail@web54207.mail.yahoo.com>
References: <20051121191816.14751.qmail@web54207.mail.yahoo.com>
Message-ID: <x2hda5k57y.fsf@turmalin.kubism.ku.dk>

Pedro Cordeiro Estrela <pedroestrela at yahoo.com> writes:

> Hello useRs!
>  
>  I'm REALLY having trouble with readline when compiling R. 2.2 from source   during ./configure.
>  Here are the last lines of the configure log:
>  
>  checking readline/history.h usability... yes
>  checking readline/history.h presence... yes
>  checking for readline/history.h... yes
>  checking readline/readline.h usability... yes
>  checking readline/readline.h presence... yes
>  checking for readline/readline.h... yes
>  checking for rl_callback_read_char in -lreadline... no


>  checking for main in -lncurses... no
>  checking for main in -ltermcap... no
>  checking for main in -ltermlib... no
>  checking for rl_callback_read_char in -lreadline... no

<twice?>

This check is in there to check for some really old readline versions
that didn't allow the style of readline interface that R used. Do you
happen to have such an older version that could get picked up by the
linker? 

What's inside config.log corresponding to the messages? These tests
generally work by compiling and linking a small test program, and if
that fails, you get the "no" part. However, sometimes the program
fails for some other reason and the message becomes misleading.


>  checking for history_truncate_file... no
>  configure: error: --with-readline=yes (default) and headers/libs are not available
>  
>  I'm on a ASUS W2V laptop (Chipset 915PM), on Mandriva 2005LE distro, libreadline5  and readline5-devel installed.
>  
>  I managed to get it compiled and installed  using option --with-readline=no. However, as I use R dayly, I'd like to get a functional terminal again....
>  
>  Can I do something to fix this, is it a configure script error or should I just change distro? :))
>  
>  The R.2.0 rpm for mandrake installs ok.
>  
>  here the location of my libreadline files
>  /usr/lib/libreadline.a
>  /usr/lib/libreadline.so
>  /lib/libreadline.so.5
>  /lib/libreadline.so.5.0
>  /lib/libreadline.so
>  
>   CheeRs!
>  
>  Pedro
> 
> 
> ____________________________________________________________
> 
> Pedro Cordeiro Estrela
> PhD. student
> 
> UMR 2695: Origine structure et ????volution de la biodiversit????
> D????partement de syst????matique et ????volution
> Mus????um National d'Histoire Naturelle 
> 
> 55, rue Buffon
> 75005
> Paris - FRANCE
> 
> tel: (33) [0]1 40 79 30 86
> fax: (33) [0]1 40 79 30 63
> [0] : from france only
> _____________________________________________________________
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From claus.atzenbeck at freenet.de  Mon Nov 21 23:44:04 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Mon, 21 Nov 2005 23:44:04 +0100 (CET)
Subject: [R] How to assign the p value from >anova to a variable ?
In-Reply-To: <BAY104-F157102F019A2C0D0C61DA5D0530@phx.gbl>
References: <BAY104-F157102F019A2C0D0C61DA5D0530@phx.gbl>
Message-ID: <Pine.OSX.4.61.0511212340180.4295@cirrus.local>

On Mon, 21 Nov 2005, Gao Fay wrote:

> If I need to assign the p-value from >anova(lm(y~x1*x2)) to a variable temp.
> How can I do that?

Maybe this is what you want:
anova(...)$Pr[3]

Claus



From liuwensui at gmail.com  Mon Nov 21 23:54:07 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 21 Nov 2005 17:54:07 -0500
Subject: [R] arima prediction
In-Reply-To: <20051121203920.49108.qmail@web31208.mail.mud.yahoo.com>
References: <20051121203920.49108.qmail@web31208.mail.mud.yahoo.com>
Message-ID: <1115a2b00511211454m32bed374ja7fff3cc578bf66f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/7f5c7dc0/attachment.pl

From RVARADHAN at JHMI.EDU  Tue Nov 22 00:12:17 2005
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Mon, 21 Nov 2005 18:12:17 -0500
Subject: [R] (no subject)
Message-ID: <000001c5eef1$046e8c90$5994100a@win.ad.jhu.edu>

Hi,

I have written the following function to check whether a vector has elements
satisfying monotonicity.

is.monotone <- function(vec, increase=T){
# check for monotonicity in time-stamp data for cortisol collection
ans <- TRUE
vec.nomis <- vec[!is.na(vec)]
if (increase & any(diff(vec.nomis,1) < 0, na.rm=T)) ans <- FALSE
if (!increase & any(diff(vec.nomis,1) > 0, na.rm=T)) ans <- FALSE
ans
}

This works correctly, but I get this error message as below.

> x <- 2:10
> is.monotone(x)
[1] TRUE
Warning messages:
1: the condition has length > 1 and only the first element will be used in:
if (increase & any(diff(vec.nomis, 1) < 0, na.rm = T)) ans <- FALSE 
2: the condition has length > 1 and only the first element will be used in:
if (!increase & any(diff(vec.nomis, 1) > 0, na.rm = T)) ans <- FALSE 
>

I am unable to see why the condition should have a length greater than 1,
since "any" should give me a single logical value.  

Can any one tell me what is going on here?  ( I am using version 2.1.1 on
Windows).

Thanks very much,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,?? The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:?? rvaradhan at jhmi.edu



From RVARADHAN at JHMI.EDU  Tue Nov 22 00:16:43 2005
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Mon, 21 Nov 2005 18:16:43 -0500
Subject: [R] Can't figure out warning message
Message-ID: <000101c5eef1$a2954df0$5994100a@win.ad.jhu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051121/46936099/attachment.pl

From gunter.berton at gene.com  Tue Nov 22 00:24:24 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 21 Nov 2005 15:24:24 -0800
Subject: [R] Can't figure out warning message
In-Reply-To: <000101c5eef1$a2954df0$5994100a@win.ad.jhu.edu>
Message-ID: <200511212324.jALNOOUT010552@faraday.gene.com>

Works fine for me with no warning messages. R for Windows 2.2.0. Try
upgrading (but I don't see why you get the warning either).

Try changing & to && though, as you don't need to vectorize the conjunction.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ravi Varadhan
> Sent: Monday, November 21, 2005 3:17 PM
> To: 'RHelp'
> Subject: [R] Can't figure out warning message
> 
> Hi,
> 
>  
> 
> I apologize for the previous posting, where the message was 
> not formatted
> properly.  Here is a better version:
> 
>  
> 
> I have written the following function to check whether a 
> vector has elements
> satisfying monotonicity.
> 
>  
> 
> is.monotone <- function(vec, increase=T){
> 
> ans <- TRUE
> 
> vec.nomis <- vec[!is.na(vec)]
> 
> if (increase & any(diff(vec.nomis,1) < 0, na.rm=T)) ans <- FALSE
> 
> if (!increase & any(diff(vec.nomis,1) > 0, na.rm=T)) ans <- FALSE
> 
> ans
> 
> }
> 
>  
> 
> This works correctly, but I get this error message as below.
> 
>  
> 
> > x <- 2:10
> 
> > is.monotone(x)
> 
> [1] TRUE
> 
> Warning messages:
> 
> 1: the condition has length > 1 and only the first element 
> will be used in:
> if (increase & any(diff(vec.nomis, 1) < 0, na.rm = T)) ans <- FALSE
> 
> 2: the condition has length > 1 and only the first element 
> will be used in:
> if (!increase & any(diff(vec.nomis, 1) > 0, na.rm = T)) ans <- FALSE 
> 
> > 
> 
>  
> 
> I am unable to see why the condition should have a length 
> greater than 1,
> since "any" should give me a single logical value.  
> 
>  
> 
> Can any one tell me what is going on here?  (I am using 
> version 2.1.1 on
> Windows).
> 
>  
> 
> Thanks very much,
> 
> Ravi.
> 
>  
> 
> --------------------------------------------------------------
> ------------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor,  The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu
> 
> --------------------------------------------------------------
> ------------
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From kjetilbrinchmannhalvorsen at gmail.com  Tue Nov 22 00:24:32 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 21 Nov 2005 19:24:32 -0400
Subject: [R] Can't figure out warning message
In-Reply-To: <000101c5eef1$a2954df0$5994100a@win.ad.jhu.edu>
References: <000101c5eef1$a2954df0$5994100a@win.ad.jhu.edu>
Message-ID: <43825730.7020105@gmail.com>

Ravi Varadhan wrote:
> Hi,
> 
>  
> 
> I apologize for the previous posting, where the message was not formatted
> properly.  Here is a better version:
> 
>  
> 
> I have written the following function to check whether a vector has elements
> satisfying monotonicity.
> 
>  
> 
> is.monotone <- function(vec, increase=T){
> 
> ans <- TRUE
> 
> vec.nomis <- vec[!is.na(vec)]
> 
> if (increase & any(diff(vec.nomis,1) < 0, na.rm=T)) ans <- FALSE
> 
> if (!increase & any(diff(vec.nomis,1) > 0, na.rm=T)) ans <- FALSE
> 
> ans
> 
> }
> 
>  
> 
> This works correctly, but I get this error message as below.
> 
>  
> 
>> x <- 2:10
> 
>> is.monotone(x)
> 
> [1] TRUE
> 
> Warning messages:
> 
> 1: the condition has length > 1 and only the first element will be used in:
> if (increase & any(diff(vec.nomis, 1) < 0, na.rm = T)) ans <- FALSE
> 
> 2: the condition has length > 1 and only the first element will be used in:
> if (!increase & any(diff(vec.nomis, 1) > 0, na.rm = T)) ans <- FALSE 
> 
> 

Try to double the &: && in place of &

Kjetil


>  
> 
> I am unable to see why the condition should have a length greater than 1,
> since "any" should give me a single logical value.  
> 
>  
> 
> Can any one tell me what is going on here?  (I am using version 2.1.1 on
> Windows).
> 
>  
> 
> Thanks very much,
> 
> Ravi.
> 
>  
> 
> --------------------------------------------------------------------------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor,  The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu
> 
> --------------------------------------------------------------------------
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rvaradha at jhsph.edu  Tue Nov 22 00:29:31 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Mon, 21 Nov 2005 18:29:31 -0500
Subject: [R] Can't figure out warning message
In-Reply-To: <000101c5eef1$a2954df0$5994100a@win.ad.jhu.edu>
Message-ID: <000001c5eef3$6ccc0220$5994100a@win.ad.jhu.edu>

Hi,

I apologize for my hasty posting.  The function works perfectly, after I
realized that I had a matrix named "T" sitting in my workspace, so when I
set my default option for "increase" to T, it created a problem.  Changing
my default to "TRUE" solved my problem.

Thanks,
Ravi.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Ravi Varadhan
> Sent: Monday, November 21, 2005 6:17 PM
> To: 'RHelp'
> Subject: [R] Can't figure out warning message
> 
> Hi,
> 
> 
> 
> I apologize for the previous posting, where the message was not formatted
> properly.  Here is a better version:
> 
> 
> 
> I have written the following function to check whether a vector has
> elements
> satisfying monotonicity.
> 
> 
> 
> is.monotone <- function(vec, increase=T){
> 
> ans <- TRUE
> 
> vec.nomis <- vec[!is.na(vec)]
> 
> if (increase & any(diff(vec.nomis,1) < 0, na.rm=T)) ans <- FALSE
> 
> if (!increase & any(diff(vec.nomis,1) > 0, na.rm=T)) ans <- FALSE
> 
> ans
> 
> }
> 
> 
> 
> This works correctly, but I get this error message as below.
> 
> 
> 
> > x <- 2:10
> 
> > is.monotone(x)
> 
> [1] TRUE
> 
> Warning messages:
> 
> 1: the condition has length > 1 and only the first element will be used
> in:
> if (increase & any(diff(vec.nomis, 1) < 0, na.rm = T)) ans <- FALSE
> 
> 2: the condition has length > 1 and only the first element will be used
> in:
> if (!increase & any(diff(vec.nomis, 1) > 0, na.rm = T)) ans <- FALSE
> 
> >
> 
> 
> 
> I am unable to see why the condition should have a length greater than 1,
> since "any" should give me a single logical value.
> 
> 
> 
> Can any one tell me what is going on here?  (I am using version 2.1.1 on
> Windows).
> 
> 
> 
> Thanks very much,
> 
> Ravi.
> 
> 
> 
> --------------------------------------------------------------------------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor,  The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu
> 
> --------------------------------------------------------------------------
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From pearce.rachel at gmail.com  Tue Nov 22 00:46:40 2005
From: pearce.rachel at gmail.com (Rachel Pearce)
Date: Mon, 21 Nov 2005 23:46:40 +0000
Subject: [R] Function comparable to cutpt.coxph from "Survival Analysis
	using S"
Message-ID: <595414120511211546l21a43fe4h@mail.gmail.com>

The title says it all really; I am looking for a function along the lines of
cutpt.coxph as described in "Survival Analysis Using S" (Tableman and
Kim), Chapter 6. As may be guessed, the function optimises the
cutpoint of a continuous variable for cox proportional hazard
modelling. I can't find it, or any similar function, on CRAN.

Alternatively, perhaps there is a way of extracting the likelihoods
from the output of coxph.

Rachel



From deepayan.sarkar at gmail.com  Tue Nov 22 01:05:57 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 21 Nov 2005 18:05:57 -0600
Subject: [R] how to plot a list in graphs
In-Reply-To: <20051121214039.41238.qmail@web36401.mail.mud.yahoo.com>
References: <eb555e660511211242g2f6d1964g5a5fd45e0da428dc@mail.gmail.com>
	<20051121214039.41238.qmail@web36401.mail.mud.yahoo.com>
Message-ID: <eb555e660511211605p3b9442d0y84462c772abf699f@mail.gmail.com>

On 11/21/05, peter eric <eric_wzl at yahoo.com> wrote:
>
> Really your (Mr.deepayan??s)answer is what I expected..thanks a lot...and I
> like to ask you one more thing..
>   Is it possible to do boolean operations in this matrix and plot that also
> in graphs?
>
>   for example
>   1.  a[anja,maya]   Vs   a[vincent,selva]
>   means things common between the rows anja&maya  and columns vincent &
> selva

You mean

levelplot(a[c("anja", "maya"), c("vincent", "selva")])

? This is standard matrix indexing in R.

>   2.a[anja,maya]   Vs   a[peter,david]
>   and so on...
>
>   So in that case whether it is possible to plot in the same graph(since I??m
> doing different...more than one.. comparisions) or I??ve to plot in a
> multiple graph..

It depends on what you want to do. If the different comparisons are
based on some systematic rule then levelplot may take you further.

[Note that for this you may need to work with a data.frame instead of
a matrix. E.g.,

mat2df <- function(x) {
    rx <- row(x)
    cx <- col(x)
    data.frame(row = rownames(x)[rx],
               column = colnames(x)[cx],
               z = as.vector(x))
}

b <- mat2df(a)

b is now a data frame:

> b
      row  column  z
1  julius   peter  3
2  caeser   peter  6
3    anja   peter  9
...

]



From p.dalgaard at biostat.ku.dk  Tue Nov 22 01:10:51 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Nov 2005 01:10:51 +0100
Subject: [R] (no subject)
In-Reply-To: <000001c5eef1$046e8c90$5994100a@win.ad.jhu.edu>
References: <000001c5eef1$046e8c90$5994100a@win.ad.jhu.edu>
Message-ID: <x27jb14k50.fsf@turmalin.kubism.ku.dk>

Ravi Varadhan <RVARADHAN at jhmi.edu> writes:

> Hi,
> 
> I have written the following function to check whether a vector has elements
> satisfying monotonicity.
> 
> is.monotone <- function(vec, increase=T){
> # check for monotonicity in time-stamp data for cortisol collection
> ans <- TRUE
> vec.nomis <- vec[!is.na(vec)]
> if (increase & any(diff(vec.nomis,1) < 0, na.rm=T)) ans <- FALSE
> if (!increase & any(diff(vec.nomis,1) > 0, na.rm=T)) ans <- FALSE
> ans
> }
> 
> This works correctly, but I get this error message as below.
> 
> > x <- 2:10
> > is.monotone(x)
> [1] TRUE
> Warning messages:
> 1: the condition has length > 1 and only the first element will be used in:
> if (increase & any(diff(vec.nomis, 1) < 0, na.rm = T)) ans <- FALSE 
> 2: the condition has length > 1 and only the first element will be used in:
> if (!increase & any(diff(vec.nomis, 1) > 0, na.rm = T)) ans <- FALSE 
> >
> 
> I am unable to see why the condition should have a length greater than 1,
> since "any" should give me a single logical value.  
> 
> Can any one tell me what is going on here?  ( I am using version 2.1.1 on
> Windows).

Would you happen to have a variable called "T" around?

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ehlers at math.ucalgary.ca  Tue Nov 22 01:12:02 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Mon, 21 Nov 2005 17:12:02 -0700
Subject: [R] Can't figure out warning message
In-Reply-To: <000101c5eef1$a2954df0$5994100a@win.ad.jhu.edu>
References: <000101c5eef1$a2954df0$5994100a@win.ad.jhu.edu>
Message-ID: <43826252.4000102@math.ucalgary.ca>

My guess is that you have an object 'T' hanging around.
Like Berton, I get no error until I define:

T <- 0:1

and then run your test case, resulting in the warnings.

Peter


Ravi Varadhan wrote:

> Hi,
> 
>  
> 
> I apologize for the previous posting, where the message was not formatted
> properly.  Here is a better version:
> 
>  
> 
> I have written the following function to check whether a vector has elements
> satisfying monotonicity.
> 
>  
> 
> is.monotone <- function(vec, increase=T){
> 
> ans <- TRUE
> 
> vec.nomis <- vec[!is.na(vec)]
> 
> if (increase & any(diff(vec.nomis,1) < 0, na.rm=T)) ans <- FALSE
> 
> if (!increase & any(diff(vec.nomis,1) > 0, na.rm=T)) ans <- FALSE
> 
> ans
> 
> }
> 
>  
> 
> This works correctly, but I get this error message as below.
> 
>  
> 
> 
>>x <- 2:10
> 
> 
>>is.monotone(x)
> 
> 
> [1] TRUE
> 
> Warning messages:
> 
> 1: the condition has length > 1 and only the first element will be used in:
> if (increase & any(diff(vec.nomis, 1) < 0, na.rm = T)) ans <- FALSE
> 
> 2: the condition has length > 1 and only the first element will be used in:
> if (!increase & any(diff(vec.nomis, 1) > 0, na.rm = T)) ans <- FALSE 
> 
> 
> 
>  
> 
> I am unable to see why the condition should have a length greater than 1,
> since "any" should give me a single logical value.  
> 
>  
> 
> Can any one tell me what is going on here?  (I am using version 2.1.1 on
> Windows).
> 
>  
> 
> Thanks very much,
> 
> Ravi.
> 
>  
> 
> --------------------------------------------------------------------------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor,  The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu
> 
> --------------------------------------------------------------------------
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Peter Ehlers
Department of Mathematics and Statistics
University of Calgary, 2500 University Dr. NW       ph: 403-220-3936



From ehlers at math.ucalgary.ca  Tue Nov 22 01:31:16 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Mon, 21 Nov 2005 17:31:16 -0700
Subject: [R] Can't figure out warning message
In-Reply-To: <43826252.4000102@math.ucalgary.ca>
References: <000101c5eef1$a2954df0$5994100a@win.ad.jhu.edu>
	<43826252.4000102@math.ucalgary.ca>
Message-ID: <438266D4.6050306@math.ucalgary.ca>


P Ehlers (that's me) wrote:

> My guess is that you have an object 'T' hanging around.
> Like Berton, I get no error until I define:
> 
> T <- 0:1
> 
> and then run your test case, resulting in the warnings.
> 
> Peter
> 
> 
> Ravi Varadhan wrote:
> 
> 
>>Hi,
>>
>> 
>>
>>I apologize for the previous posting, where the message was not formatted
>>properly.  Here is a better version:
>>
>>
[snip]

As the posting guide says:

When responding to a very simple question, use the
following algorithm:
2. type 4*runif(1) at the R prompt, and wait this many hours

 > 4*runif(1)
[1] 3.141592

Obviously, I acted much too hastily. My apologies.

Peter

-- 
Peter Ehlers
Department of Mathematics and Statistics
University of Calgary, 2500 University Dr. NW       ph: 403-220-3936



From jpablo.romero at gmail.com  Tue Nov 22 01:42:25 2005
From: jpablo.romero at gmail.com (Juan Pablo Romero)
Date: Mon, 21 Nov 2005 18:42:25 -0600
Subject: [R] Removing Rows
In-Reply-To: <dd48e20f0511211237k13d63732ue1625abe1cd3ecf0@mail.gmail.com>
References: <dd48e20f0511211237k13d63732ue1625abe1cd3ecf0@mail.gmail.com>
Message-ID: <e6507ac70511211642p75a274aaw@mail.gmail.com>

try this: (if d is the data.frame)

1)

d[ (1:217)[1:217 %% 10 == 0],  ]

2)

d[ (1:217)[1:217 %% 10 != 0],  ]



2005/11/21, mark salsburg <mark.salsburg at gmail.com>:
> I have a data frame with the following dimensions 217 x 5
>
> I want to create two data frames from the original.
>
> 1) One containing every tenth row of the original data frame
> 2) Other containing the rest of the rows.
>
> How do I do this? I've tried subset() and calling the index.
>
> thank you in advance,
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jpablo.romero at gmail.com  Tue Nov 22 01:42:25 2005
From: jpablo.romero at gmail.com (Juan Pablo Romero)
Date: Mon, 21 Nov 2005 18:42:25 -0600
Subject: [R] Removing Rows
In-Reply-To: <dd48e20f0511211237k13d63732ue1625abe1cd3ecf0@mail.gmail.com>
References: <dd48e20f0511211237k13d63732ue1625abe1cd3ecf0@mail.gmail.com>
Message-ID: <e6507ac70511211642p75a274aaw@mail.gmail.com>

try this: (if d is the data.frame)

1)

d[ (1:217)[1:217 %% 10 == 0],  ]

2)

d[ (1:217)[1:217 %% 10 != 0],  ]



2005/11/21, mark salsburg <mark.salsburg at gmail.com>:
> I have a data frame with the following dimensions 217 x 5
>
> I want to create two data frames from the original.
>
> 1) One containing every tenth row of the original data frame
> 2) Other containing the rest of the rows.
>
> How do I do this? I've tried subset() and calling the index.
>
> thank you in advance,
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jpablo.romero at gmail.com  Tue Nov 22 01:54:17 2005
From: jpablo.romero at gmail.com (Juan Pablo Romero)
Date: Mon, 21 Nov 2005 18:54:17 -0600
Subject: [R] read.csv in R 1.7.1 for MacOS
Message-ID: <e6507ac70511211654t2feace5fu@mail.gmail.com>

Hello

I'm stuck with some notebooks having MacOS 10.2.8.

After trying R-2.2.0 (which won't run), I had to settle with R 1.7.1,
which runs fine.

The problem is this:

I have a csv file in a web server, which I'd wish to be able to load with

d <- read.csv("http://server/path/cuestionario.csv")

Unfortunately it doesn't work in R 1.7.1 / Mac, although the help page
says it should.

??There is some other way to load remote files?

Thanks in advance.

  Juan Pablo



From hb at maths.lth.se  Tue Nov 22 01:56:11 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 22 Nov 2005 11:56:11 +1100
Subject: [R] Removing Rows
In-Reply-To: <e6507ac70511211642p75a274aaw@mail.gmail.com>
References: <dd48e20f0511211237k13d63732ue1625abe1cd3ecf0@mail.gmail.com>
	<e6507ac70511211642p75a274aaw@mail.gmail.com>
Message-ID: <43826CAB.7050200@maths.lth.se>

seq() is good at these kind of things:

keep <- seq(from=1, to=nrow(d), by=10)
d1 <- d[ keep,]
d2 <- d[-keep,]

/Henrik

Juan Pablo Romero wrote:
> try this: (if d is the data.frame)
> 
> 1)
> 
> d[ (1:217)[1:217 %% 10 == 0],  ]
> 
> 2)
> 
> d[ (1:217)[1:217 %% 10 != 0],  ]
> 
> 
> 
> 2005/11/21, mark salsburg <mark.salsburg at gmail.com>:
> 
>>I have a data frame with the following dimensions 217 x 5
>>
>>I want to create two data frames from the original.
>>
>>1) One containing every tenth row of the original data frame
>>2) Other containing the rest of the rows.
>>
>>How do I do this? I've tried subset() and calling the index.
>>
>>thank you in advance,
>>
>>        [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Tue Nov 22 03:43:22 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Nov 2005 18:43:22 -0800
Subject: [R] Newton-Raphson
In-Reply-To: <C4A33886378A9447B5C05B3010868146FFA415@UM-EMAIL10.um.umsystem.edu>
References: <C4A33886378A9447B5C05B3010868146FFA415@UM-EMAIL10.um.umsystem.edu>
Message-ID: <438285CA.3040401@pdf.com>

	  I just got 37 hits from 'RSiteSearch("solve nonlinear equation")',
many of which seem relevant to your question.  If none of these seem to
answer your question, submit another question.  (Before you submit
another question, however, I suggest you read the posting guide!
"www.R-project.org/posting-guide.html".  It may increase the speed and
utility of responses.)

	  hope this helps.
	  spencer graves
p.s.  See also uniroot.  One common meaning of the word "score" in
statistics in the first derivative of a log(likelihood).  If you also
have that log(likelihood), then "optim", "nlminb", and "nlm" might
interest you.  With a vector function to set to zero over vector inputs,
I typically write a simple function to compute the sum of squares and
then give that to "optim".  If none of these suggestions meet your
needs, please be more explicity.

Zhu, Chao (UMC-Student) wrote:

> Dear all,
>  
> I want to solve a score function by using Newton-Raphson algorithm. 
Is there such a fucntion in R? I know there's one called optim, but
it seems only doing minimizing or maximizing.
>  
> Thanks,
>  
> Jimmy
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From shuguang at gmail.com  Tue Nov 22 04:31:32 2005
From: shuguang at gmail.com (S. Sun)
Date: Tue, 22 Nov 2005 11:31:32 +0800
Subject: [R] what does the it when there is a zero events in the Logistic
 Regression with glm?
Message-ID: <43829114.9000106@gmail.com>

Dear all,

I have a question about the glm. When the events of an observation is 0, 
the logit function on it is Inf. I wonder how the glm solve it.

An example:
Treat Events Trials
A     0      50
B     7      50
C     10     50
D     15     50
E     17     50

Program:

treat <- factor(c("A", "B", "C", "D", "E"))
events <- c(0, 7, 10, 15, 17)
trials <- rep(50, 5)
glm(cbind(events, trials-events)~treat, family=binomial)

What's wrong with it? And are there better ideas?

--
Sh. Sun



From szlevine at nana.co.il  Tue Nov 22 07:40:15 2005
From: szlevine at nana.co.il (Stephen)
Date: Tue, 22 Nov 2005 08:40:15 +0200
Subject: [R] Weibull  and survival
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD661@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/cf9cb6fd/attachment.pl

From ripley at stats.ox.ac.uk  Tue Nov 22 08:29:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Nov 2005 07:29:25 +0000 (GMT)
Subject: [R] read.csv in R 1.7.1 for MacOS
In-Reply-To: <e6507ac70511211654t2feace5fu@mail.gmail.com>
References: <e6507ac70511211654t2feace5fu@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0511220721250.19590@gannet.stats>

On Mon, 21 Nov 2005, Juan Pablo Romero wrote:

> I'm stuck with some notebooks having MacOS 10.2.8.
>
> After trying R-2.2.0 (which won't run), I had to settle with R 1.7.1,
> which runs fine.

Which version of R 1.7.1 is this?  The Classic MacOS version, I guess.
If you go to the link labelled `MacOS X (10.2.x and above)' you will see
R 2.0.1 there for MacOS X 10.2+.

> The problem is this:
>
> I have a csv file in a web server, which I'd wish to be able to load with
>
> d <- read.csv("http://server/path/cuestionario.csv")
>
> Unfortunately it doesn't work in R 1.7.1 / Mac, although the help page
> says it should.

What does `it doesn't work' mean?

> ?There is some other way to load remote files?

Yes, e.g. loadURL().  I think you mean to read in a remote CSV file,
which you may be able to do via download.file.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do, and give us the information it asks for which we need to be 
able to help you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From petr.pikal at precheza.cz  Tue Nov 22 08:53:51 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 22 Nov 2005 08:53:51 +0100
Subject: [R] Warning message help
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F304C8658@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <4382DC9F.4034.3840B2@localhost>

Hi



On 21 Nov 2005 at 10:44, Guenther, Cameron wrote:

Date sent:      	Mon, 21 Nov 2005 10:44:58 -0500
From:           	"Guenther, Cameron" <Cameron.Guenther at myfwc.com>
To:             	<R-help at r-project.org>
Subject:        	[R] Warning message help

> I am trying to great a new column of effort data from an existing
> vector of gears used. It is a simple code where 
> 
> effort[Gear==300]=(DIST_TOW*7412)
> effort[Gear==301]=(DIST_TOW*7412)
> 
> The code appears to work for some of the data but fails for others and
> inserts a NA value
> 
> I also get this warning message
> 
> Warning message:
> number of items to replace is not a multiple of replacement length 

basically it says

number of items to replace is not a multiple of replacement length

what means that length

effort[.....] is different from length DIST_TOW*7412 and

that the lengths are not divisible without a reminder so the 
replacement cannot be fully recycled.

HTH
Petr

> 
> Can anybody tell me what this means.
> thanks
> 
> Cameron Guenther 
> Associate Research Scientist
> FWC/FWRI, Marine Fisheries Research
> 100 8th Avenue S.E.
> St. Petersburg, FL 33701
> (727)896-8626 Ext. 4305
> cameron.guenther at myfwc.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Tue Nov 22 08:58:55 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 22 Nov 2005 08:58:55 +0100
Subject: [R] Removing Rows
In-Reply-To: <dd48e20f0511211237k13d63732ue1625abe1cd3ecf0@mail.gmail.com>
Message-ID: <4382DDCF.7498.3CE55B@localhost>

Hi

use
selection <- seq(10, final length, 10) 
or
selection <- (1:n)*10

and

mydf[selection,]
mydf[-selection,]

HTH
Petr


On 21 Nov 2005 at 15:37, mark salsburg wrote:

Date sent:      	Mon, 21 Nov 2005 15:37:28 -0500
From:           	mark salsburg <mark.salsburg at gmail.com>
To:             	R-help at stat.math.ethz.ch, r-help at stat.math.ethz.ch
Subject:        	[R] Removing Rows

> I have a data frame with the following dimensions 217 x 5
> 
> I want to create two data frames from the original.
> 
> 1) One containing every tenth row of the original data frame
> 2) Other containing the rest of the rows.
> 
> How do I do this? I've tried subset() and calling the index.
> 
> thank you in advance,
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Nov 22 09:07:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Nov 2005 08:07:23 +0000 (GMT)
Subject: [R] what does the it when there is a zero events in the
 Logistic Regression with glm?
In-Reply-To: <43829114.9000106@gmail.com>
References: <43829114.9000106@gmail.com>
Message-ID: <Pine.LNX.4.61.0511220745370.19870@gannet.stats>

On Tue, 22 Nov 2005, S. Sun wrote:

> I have a question about the glm.

Not really: your question is about understanding logistic regressions.

> When the events of an observation is 0,
> the logit function on it is Inf. I wonder how the glm solve it.

Note that logit(0)  = -Inf whereas logit(1) = Inf.

It is the fitted probabilities which are passed to logit, not the 
empirical proportions.  Logistic regression is often applied to Bernouilli 
trials with 0/1 proportions, with nothing to `solve'.

So the issue only arises if the MLE would give 0 (or 1) fitted values, and 
it cannot in a logistic regression.  You have here an example in which the 
MLE does not exist and the log-likelihood does not attain its maximum. 
Such situations are known as `separation' and it is well-known that there 
are better algorithms for such problems.

> An example:
> Treat Events Trials
> A     0      50
> B     7      50
> C     10     50
> D     15     50
> E     17     50
>
> Program:
>
> treat <- factor(c("A", "B", "C", "D", "E"))
> events <- c(0, 7, 10, 15, 17)
> trials <- rep(50, 5)
> glm(cbind(events, trials-events)~treat, family=binomial)
>
> What's wrong with it? And are there better ideas?

Nothing is `wrong with it'.  It finds fitted values which are very close 
to the observed values.  You have chosen an inappropriate model and an 
inappropriate parametrization (see ?relevel).

I presume you did think something is wrong, but you did not tell us what.
Please do read the posting guide and try to provide us with enough 
information to help you.  Also, please do sign your messages indicating 
who you are and what your background is.  In cases like this the best 
advice is to suggest asking your supervisor (if you have one) or to read 
the literature (but what specifically depends on your background).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov 22 09:15:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Nov 2005 08:15:43 +0000 (GMT)
Subject: [R] Weibull  and survival
In-Reply-To: <E76EB96029DCAE4A9CB967D7F6712D1DBFD661@NANAMAILBACK1.nanamail.co.il>
References: <E76EB96029DCAE4A9CB967D7F6712D1DBFD661@NANAMAILBACK1.nanamail.co.il>
Message-ID: <Pine.LNX.4.61.0511220808000.19870@gannet.stats>

On Tue, 22 Nov 2005, Stephen wrote:

> I have been asked to provide Weibull parameters from a paper using
> Kaplan Meir survival analysis.
>
> This is something I am not familiar with.
>
> The survival analysis in R works nicely and is the same as commercial
> software (only the graphs are superior in R).
>
> The Weibull does not and produces an error (see below).
>
> Any ideas why this error should occur?

Do you have zero survival times?  They cannot occur for a Weibull, but
might as a result of rounding.

For a worked example of how to deal with these, see MASS4 p. 380.

The other possibility is that you have infinite survival times, but that 
is unlikely to be correct ....


> My approach may be spurious.
>
> Code follows
>
> #The following works fine
>
>> surv.mod1 <- survfit( Surv(SURALL2, relall6==1)~randgrpc,
> type=c("kaplan-meier"),data=Dataset)
>
> #The following works produces the error below
>
>> surv.mod2 <- survreg( Surv(SURALL2, relall6 == 1)~randgrpc,
> data=Dataset, dist="weibull")
>
> Error in survreg(Surv(SURALL2, relall6 == 1) ~ randgrpc, data = Dataset,
> :
>
>        Invalid survival times for this distribution

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ericlecoutre at gmail.com  Tue Nov 22 09:29:35 2005
From: ericlecoutre at gmail.com (Eric Lecoutre)
Date: Tue, 22 Nov 2005 09:29:35 +0100
Subject: [R] Any volonteer to maintain package R2HTML ?
Message-ID: <5d897a2f0511220029k38bb850g@mail.gmail.com>

Hi useRs!

I am in search of any R programer / user to go on with the package R2HTML.
I did change of job 3 months ago and I could only realize I have
absolutely no time anymore for that (even so little) task.

R2HTML package consists in a set of HTML.* functions that could be
used to export R objects to raw HTML. It could also be used in a
litterate programing way thanks to Sweave tool.

Basically, the task should not be too hard, as the whole stuff works
as it ; it consists mainly in adapting little parts as R new versions
do require such or such additional quality check. Currently, .Rd help
files require some minors modifs to pass checks (break returns). I was
intending to do that this week end but did fail installing all
necessary stuff to compile R packages (I do not have all admin rights
on my new computer).

If anyone is interested, please contact me directly at: ericlecoutre at gmail.com

Best wishes, and all go happyR and happyR!

Eric



From ripley at stats.ox.ac.uk  Tue Nov 22 09:39:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Nov 2005 08:39:32 +0000 (GMT)
Subject: [R] Method for $
In-Reply-To: <20051118185933.M87397@tfh-berlin.de>
References: <20051118140018.M46734@tfh-berlin.de>
	<437E0534.6070101@statistik.uni-dortmund.de>
	<20051118172748.M31016@tfh-berlin.de> <437E1CEB.70900@wald.ucdavis.edu>
	<20051118185933.M87397@tfh-berlin.de>
Message-ID: <Pine.LNX.4.61.0511181959060.24187@gannet.stats>

On Fri, 18 Nov 2005, Ulrike Gr?mping wrote:

> From: Duncan Temple Lang <duncan at wald.ucdavis.edu>

[...]
>> For the record, the problem you are experiencing is that
>> you are not using the same argument names in your
>> method as the function "$" uses. Hence there is a mismatch.
>> Use
>> setMethod("$", signature(x="myclass"), function(x, name){
>> ? slot(x, name)
>> } )
>>
>> i.e. rather than slotname as the second argument, use name
>> as that is what $ uses.
>
> Thanks very much for clarifying that. I just wasn't aware of the 
> argument names of $. It's obvious from the help file, once you know what 
> to look for, but ...

`For the record', this is rather misleading.  $ is a primitive function in 
R, and does _not_ have argument names (it uses only positional matching 
like all primitive functions).  To check this, try

xx <- list(a=1, b=2)
`$`(y=xx, zz="b")

which is perfectly valid R and works just like xx$b.  Hence there is no 
reason for the help page to specify argument names (and it does in fact 
explain that they are not used).

However, the S4 model needs argument names, and so when setting S4 methods 
for primitive functions one needs to use the argument names which it 
_assumes_.  Sometimes the only way to find those out is to read the code. 
They are in src/library/methods/R/BasicFunsList.R which contains

"$" = function(x, name)
{
     name <- as.character(substitute(name))
     standardGeneric("$")
}

S3 methods for primitives do not need to match argument names, fortunately 
as Duncan TL has just added an S3 method with the (mis-matching) arguments

> args(`$.DLLInfo`)
function (x, i, ...)

Since there are no argument names for the generic, the QC tools do not 
check such cases.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From bady at univ-lyon1.fr  Tue Nov 22 09:41:20 2005
From: bady at univ-lyon1.fr (bady@univ-lyon1.fr)
Date: Tue, 22 Nov 2005 09:41:20 +0100
Subject: [R] Multinomial Nested Logit package in R?
In-Reply-To: <5330C869-C095-43BA-BCEE-8F6DECA8C848@frontierassoc.com>
References: <5330C869-C095-43BA-BCEE-8F6DECA8C848@frontierassoc.com>
Message-ID: <1132648880.4382d9b0823a8@webmail.univ-lyon1.fr>

hi, hi all,

you can consult this link :
http://tolstoy.newcastle.edu.au/R/help/05/11/15455.html

hope this help,

cheers,


P.BADY












Selon "David C. James" <djames at frontierassoc.com>:

> Dear R-Help,
>
> I'm hoping to find a Multinomial Nested Logit package in R.  It would
> be great to find something analogous to "PROC MDC" in SAS:
> > The MDC (Multinomial Discrete Choice) procedure analyzes models
> > where the
> > choice set consists of multiple alternatives. This procedure
> > supports conditional logit,
> > mixed logit, heteroscedastic extreme value, nested logit, and
> > multinomial probit mod-
> > els. The MDC procedure uses the maximum likelihood (ML) or
> > simulated maximum
> > likelihood method for model estimation. Since the term multinomial
> > logit is often
> > used instead of conditional logit in econometrics literature, the
> > term simple multino-
> > mial logit is used here to denote the model used by Schmidt and
> > Strauss (1975), while
> > multinomial logit is used as a synonym of conditional logit.
>
> I found this web site useful in comparing various categorical
> dependent variable models, but it only addresses SAS, STATA, LIMDEP,
> and SPSS:
> http://www.indiana.edu/~statmath/stat/all/cdvm/cdvm1.html
>
> I have searched using:
> 1. Google> site:r-project multinomial nested logit
> 2. Google> site:r-project.org multinomial discrete choice
> 3. R> help.search("multinomial nested logit")
> No help files found matching ?multinomial nested logit? using fuzzy
> matching
> 4. R> help.search("multinomial discrete choice")
> No help files found matching ?multinomial discrete choice? using
> fuzzy matching
>
> Possibilities that seem unclear and/or not very promising:
> 1. It is my understanding that the multinom function in the nnet
> package does NOT do nested models.
> 2. The MNP (multinomial probit) package is close, but I want a logit,
> not probit, model.
> 3. http://www.r-project.org/nocvs/mail/r-help/2002/4394.html
> On Wed, 29 May 2002, Vumani Dlamini wrote:
> > > Has anyone implemented the conditional logit and/or the nested
> > logit model
> > > in R?
> >
> > Conditional logistic regression is clogit() in the survival package.
> But the survival package didn't seem to have what I needed.  Am I
> overlooking anything?
>
> Are there any synonyms that I should be using for the search?  I
> would appreciate any pointers or suggestions.
>
> Thanks,
> -David
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From fcombes at gmail.com  Tue Nov 22 10:03:43 2005
From: fcombes at gmail.com (Florence Combes)
Date: Tue, 22 Nov 2005 10:03:43 +0100
Subject: [R] Crop white border for PDF output
In-Reply-To: <Pine.OSX.4.61.0511192056590.3976@cirrus.local>
References: <Pine.OSX.4.61.0511192056590.3976@cirrus.local>
Message-ID: <73dae3060511220103g7355b021x246e9c4be8aa363a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/92864234/attachment.pl

From parrinel at med.unibs.it  Tue Nov 22 10:21:38 2005
From: parrinel at med.unibs.it (giovanni parrinello)
Date: Tue, 22 Nov 2005 10:21:38 +0100
Subject: [R] residuals.coxph
Message-ID: <003e01c5ef46$241c22a0$2b18a7c0@alice>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/9f6a6568/attachment.pl

From szlevine at nana.co.il  Tue Nov 22 10:31:17 2005
From: szlevine at nana.co.il (Stephen)
Date: Tue, 22 Nov 2005 11:31:17 +0200
Subject: [R] Weibull  and survival
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD663@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/818919ee/attachment.pl

From Antje.Doering at komdat.com  Tue Nov 22 10:41:07 2005
From: Antje.Doering at komdat.com (=?iso-8859-1?Q?Antje_D=F6ring?=)
Date: Tue, 22 Nov 2005 10:41:07 +0100
Subject: [R] problem with "parse"
Message-ID: <686C1FDE894539418C5668E5E6DE12DE05106F@muc-exch-tmp.komdat.intern>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/e0a6e093/attachment.pl

From claus.atzenbeck at freenet.de  Tue Nov 22 10:48:38 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Tue, 22 Nov 2005 10:48:38 +0100 (CET)
Subject: [R] Crop white border for PDF output
In-Reply-To: <73dae3060511220103g7355b021x246e9c4be8aa363a@mail.gmail.com>
References: <Pine.OSX.4.61.0511192056590.3976@cirrus.local>
	<73dae3060511220103g7355b021x246e9c4be8aa363a@mail.gmail.com>
Message-ID: <Pine.OSX.4.61.0511221047270.1635@cirrus.aue.aau.dk>

On Tue, 22 Nov 2005, Florence Combes wrote:

> I had not exactly the same pb, but close enough to propose you to take a
> look at the options "fin" and "fig" in the par() function.

Thanks for your answer. In the meanwhile I also found another solution.
I produce diagrams as usual and use pdfcrop to crop them afterwards. It
takes two steps, but that's also OK.

Claus



From lisadowson at gmail.com  Tue Nov 22 11:04:20 2005
From: lisadowson at gmail.com (Lisa Dowson)
Date: Tue, 22 Nov 2005 12:04:20 +0200
Subject: [R] Kolmogorov-Smirnov test help
Message-ID: <30a2cdc20511220204r5f281c64rfe3fa4d1857db6b2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/8f23ab45/attachment.pl

From fezzi at stat.unibo.it  Tue Nov 22 11:15:10 2005
From: fezzi at stat.unibo.it (Carlo Fezzi)
Date: Tue, 22 Nov 2005 11:15:10 +0100 (CET)
Subject: [R] cointegration rank
In-Reply-To: <25D1C2585277D311B9A20000F6CCC71B07510AB8@DEFRAEX02>
References: <25D1C2585277D311B9A20000F6CCC71B07510AB8@DEFRAEX02>
Message-ID: <8039059.1132654510268.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Another question on cointegration...

Is it possible to insert in the model dummy variables restricted in the
cointegration space?

Many thanks,

Carlo


On Nov 21, 2005 01:23 PM, "Pfaff, Bernhard Dr."
<Bernhard_Pfaff at fra.invesco.com> wrote:

> Thanks a lot!
> 
> I have another question on "cointegration", so I will go on this post.
> 
> Is it possible to estimate a cointegration with some exogenous
> explanatory variables? Since, after testing for exogeneity, I would
> like
> to re-estimate the relation keeping some of the previous endogenous as
> exogenous.
> 
> Many thanks!
> 
> Carlo
> 
> 
> Hello Carlo,
> 
> you can use the 'dumvar' argument for his purpose, and exclude the
> relevant
> variables from your data matrix 'x'.
> 
> HTH,
> Bernhard
> 
> 
> On Nov 21, 2005 11:21 AM, "Pfaff, Bernhard Dr."
> <Bernhard_Pfaff at fra.invesco.com> wrote:
> 
> > Dear R - helpers,
> > 
> > I am using the urca package to estimate cointegration relations, and
> > I
> > would be really grateful if somebody could help me with this
> > questions:
> > 
> > After estimating the unrestriced VAR with "ca.jo" I would like to
> > impose
> > the rank restriction (for example rank = 1) and then obtain the
> > restricted estimate of PI to be utilized to estimate the VECM model.
> > 
> > Is it possible? 
> > 
> > It seems to me that the function "cajools" estimates the VECM
> > without
> > the restrictions. Did I miss something? How is it possible to impose
> > them?
> > 
> > Thanks a lot in advance!
> > 
> > Carlo
> > 
> > 
> > Hello Carlo,
> > 
> > you can achieve this, by calculating your desired PI-matrix by hand,
> > given
> > the slots 'V' and 'W' of your ca.jo object and then execute a
> > restricted
> > OLS-estimation, if I understand your goal correctly. 
> > Please, bear in mind the non-uniqueness of the factorization of the
> > PI-matrix by doing so.
> > 
> > HTH,
> > Bernhard    
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html



From marc.kirchner at iwr.uni-heidelberg.de  Tue Nov 22 11:19:23 2005
From: marc.kirchner at iwr.uni-heidelberg.de (Marc Kirchner)
Date: Tue, 22 Nov 2005 10:19:23 +0000
Subject: [R] problem with "parse"
In-Reply-To: <686C1FDE894539418C5668E5E6DE12DE05106F@muc-exch-tmp.komdat.intern>
References: <686C1FDE894539418C5668E5E6DE12DE05106F@muc-exch-tmp.komdat.intern>
Message-ID: <20051122101914.GA6194@iwr.uni-heidelberg.de>

Hello Antje,

> Is there anything I have overlooked? Can't I put special characters
> like "$" or "," into this test-character?

If I understand "?parse" correctly, you are supposed to pass a valid R
expression to it and 
	"u.g$par1, u.g$par2"
is not as the comma is not a valid delimiter here.
However,

> parse(text="u.g$par1; u.g$par2")
expression(u.g$par1, u.g$par2)

does work (note the semicolon instead of the comma) and 
might be what you want.

Regards,
Marc

-- 
========================================================
Dipl. Inform. Med. Marc Kirchner
Interdisciplinary Centre for Scientific Computing (IWR)
Multidimensional Image Processing
INF 368
University of Heidelberg
D-69120 Heidelberg
Tel: ++49-6221-54 87 97
Fax: ++49-6221-54 88 50
marc.kirchner at iwr.uni-heidelberg.de

-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051122/06546549/attachment.bin

From ligges at statistik.uni-dortmund.de  Tue Nov 22 11:28:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 22 Nov 2005 11:28:24 +0100
Subject: [R] Kolmogorov-Smirnov test help
In-Reply-To: <30a2cdc20511220204r5f281c64rfe3fa4d1857db6b2@mail.gmail.com>
References: <30a2cdc20511220204r5f281c64rfe3fa4d1857db6b2@mail.gmail.com>
Message-ID: <4382F2C8.2060303@statistik.uni-dortmund.de>

Lisa Dowson wrote:

> Hi
> 
> I am conducting 2-sample Kolmogorov Smirnov tests for my Masters project to
> determine if two independant tree populations have the same size-class
> distribution or not. The trees have been placed into size-class categories
> based on their basal diameters. Once I started running the stats on my data,
> I got confused with the results. Just to show an example of what I was
> testing I ran stats comparing population1 to population 2. and then
> comparing population 3 to population 2.
>   Popn1 Popn2 Popn3  880 769 0  34 40 19  10 24 19  2 2 8  2 2 36  0 0 0

If I interpret your data correctly, we can look at the step functions by:

  plot.ecdf(x[,1], xlim=c(-10, 900), verticals=TRUE)
  plot.ecdf(x[,2], xlim=c(-10, 900), col.hor="blue", col.vert="blue", 
add=TRUE, verticals=TRUE)
  plot.ecdf(x[,3], xlim=c(-10, 900), col.hor="red", col.vert="red", 
add=TRUE, verticals=TRUE)

and see that there is no good reason why the test should reject the Null 
  for so few (6!) observations.

Note that you need at least 4 observations in each group to be able to 
reject anything at alpha=0.05 even for completely different distributions:

  ks.test(1:3, 101:103)


> Common sense tells me that P1 and P2 are similar and that P3 and P2 are
> dissimilar. However, for the P1 versus P2 test I am getting p-value of 1
> (saying they are strongly similar -what I expected) and for the P3 versus P2
> test, a p-value of 0.9307 (saying they are strongly similar, but slightly
> less similar than the other test- not what I expected at all). However,
> common sense tells me that P3 and P2 are not similar at all and that I
> should be expecting a far lower p-value for the test comparing P3 and P2.

Sometimes common sense is dangerous. Sort your data, look again, look 
into the plot, and rethink...

Uwe Ligges




> Any help would be greatly appreciated
> 
> Thanks
> 
> Lisa
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Florian.Koller at gfk.de  Tue Nov 22 12:04:49 2005
From: Florian.Koller at gfk.de (Florian.Koller@gfk.de)
Date: Tue, 22 Nov 2005 12:04:49 +0100
Subject: [R] Output of row and column names
Message-ID: <OF7E99A944.B0369AD3-ONC12570C1.003B2AD2-C12570C1.003CDEA4@gfk.de>





Dear all,

I am looking for a way to identify the row and column names of all elements
within a matrix which fulfill some specified condition.

Example: I have a correlation matrix of the following form:

      Y1    Y2
X1    0.4   0.3
X2    0.6   0.1

Suppose, I want to know which elements are smaller than 0.2, so the desired
output should be something like:

"X2 Y2"


Thank you,
Florian Koller



______________________
GfK Fernsehforschung GmbH
Research Consulting & Development
Nordwestring 101
D-90319 N??rnberg
Fon  +49 (0)911 395-3554
Fax  +49 (0)911 395-4130
www.gfk.de / www.gfk.com



_________________________

Diese E-Mail (ggf. nebst Anhang) enth??lt vertrauliche und/oder rechtlich
gesch??tzte Informationen. Wenn Sie nicht der richtige Adressat sind, oder
diese E-Mail irrt??mlich erhalten haben, informieren Sie bitte sofort den
Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren sowie die
unbefugte Weitergabe dieser Mail ist nicht gestattet.

This e-mail (and any attachment/s) contains confidential and/or privileged
information. If you are not the intended recipient (or have received this
e-mail in error) please notify the sender immediately and destroy this
e-mail. Any unauthorised copying, disclosure or distribution of the
material in this e-mail is strictly forbidden.



From vincent at 7d4.com  Tue Nov 22 12:27:09 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Tue, 22 Nov 2005 12:27:09 +0100
Subject: [R] Output of row and column names
In-Reply-To: <OF7E99A944.B0369AD3-ONC12570C1.003B2AD2-C12570C1.003CDEA4@gfk.de>
References: <OF7E99A944.B0369AD3-ONC12570C1.003B2AD2-C12570C1.003CDEA4@gfk.de>
Message-ID: <4383008D.9080908@7d4.com>

?which



From Gregor.Gorjanc at bfro.uni-lj.si  Tue Nov 22 12:26:43 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Tue, 22 Nov 2005 12:26:43 +0100
Subject: [R] Fw: Re: Is there anything like a write.fwf() or possibility to
	print adata.frame without rownames?
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31AC5@pollux.bfro.uni-lj.si>

>>Petr Pikal wrote:
>>> Hi
>>>
>>> did you tried something like
>>>
>>> write.table( tab, "file.txt", sep="\t", row.names=F)
>>>
>>> which writes to tab separated file?
>>>
>>
>>Petr thanks, but I do not want a tab delimited file. I need spaces
>>between columns.
>
> write.table( tab, "file.txt", sep="", row.names=F)
> Can it do what you want?

Ronggui thanks,

but this does not work also. For example I get something like
this bellow

"26" "1" 42 "DA" "DA" "lipa" "Monika"
"26" "1" 42 "DA" "DA" "lipa" "Monika"
"27" "1" 41 "DA" "DA" "smreka" "Monika"
"27" "1" 41 "DA" "DA" "smreka" "Monika"

and you can see, that there is a problem, when all "values"
in a column do not have the same length. I need to get

"26" "1" 42 "DA" "DA" "lipa"   "Monika"
"26" "1" 42 "DA" "DA" "lipa"   "Monika"
"27" "1" 41 "DA" "DA" "smreka" "Monika"
"27" "1" 41 "DA" "DA" "smreka" "Monika"

i.e. columns should be properly aligned.

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From ripley at stats.ox.ac.uk  Tue Nov 22 13:01:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Nov 2005 12:01:02 +0000 (GMT)
Subject: [R] residuals.coxph
In-Reply-To: <003e01c5ef46$241c22a0$2b18a7c0@alice>
References: <003e01c5ef46$241c22a0$2b18a7c0@alice>
Message-ID: <Pine.LNX.4.61.0511221156370.27058@gannet.stats>

On Tue, 22 Nov 2005, giovanni parrinello wrote:

> I am trying to apply the function 'cox.zph' of the library survival, but 
> I receive this error message: not found the object 'residuals.coxph'.

Please show us exactly what you did and use traceback() to show where the 
message came from.  example(cox.zph) works on my systems, and

> getAnywhere("residuals.coxph")
A single object matching 'residuals.coxph' was found
It was found in the following places
   registered S3 method for residuals from namespace survival
   namespace:survival
...

Do you perhaps have a local copy of cox.zph?  Try running R with 
--vanilla to be sure.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Tue Nov 22 13:03:47 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 22 Nov 2005 12:03:47 -0000 (GMT)
Subject: [R] Output of row and column names
In-Reply-To: <OF7E99A944.B0369AD3-ONC12570C1.003B2AD2-C12570C1.003CDEA4@gfk.de>
Message-ID: <XFMail.051122120347.Ted.Harding@nessie.mcc.ac.uk>

On 22-Nov-05 Florian.Koller at gfk.de wrote:
> Dear all,
> 
> I am looking for a way to identify the row and column names of all
> elements
> within a matrix which fulfill some specified condition.
> 
> Example: I have a correlation matrix of the following form:
> 
>       Y1    Y2
> X1    0.4   0.3
> X2    0.6   0.1
> 
> Suppose, I want to know which elements are smaller than 0.2, so the
> desired
> output should be something like:
> 
> "X2 Y2"

The clue is the "arr.ind" parameter in 'which' (default=FALSE):

  > A
       [,1] [,2]
  [1,]  0.4  0.3
  [2,]  0.6  0.1

  > which(A<0.2, arr.ind=TRUE)
       row col
  [1,]   2   2

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Nov-05                                       Time: 12:03:45
------------------------------ XFMail ------------------------------



From Allan at STATS.uct.ac.za  Tue Nov 22 13:07:32 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Tue, 22 Nov 2005 14:07:32 +0200
Subject: [R] R: pp plot
Message-ID: <43830A04.4F581D22@STATS.uct.ac.za>

hi all

i would like to know if anyone has a reference on how one would place
the "bands" on the pp plot.

i want to test whether or not a certain data set comes from a particular
distribution (not normal).

i've already plotted F(X(j)) vs j/(n+1)	where F(x) is the cum dist
function, X(j) is the j'th order statistic and n is the sample size. 

a goole search gave arb references and thought some one one the list
should definitely know how to solve this problem.

thanking you in advance
allan

From petr.pikal at precheza.cz  Tue Nov 22 13:08:52 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 22 Nov 2005 13:08:52 +0100
Subject: [R] Fw: Re: Is there anything like a write.fwf() or possibility
	to	print adata.frame without rownames?
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730F31AC5@pollux.bfro.uni-lj.si>
Message-ID: <43831864.21219.121C8A1@localhost>


   Hi

   can you please explain why do you need it? What do you want to do with
   the exported file?

   I wonder what type of software can not accept any reasonable delimiter
   and requires fwf files.

   The  only  workaround  I  can  imagine  is  to transfer all columns to
   character  and  add  leading spaces to each item which is shorter than
   longest  item  in  specified column to equalize length of all items in
   column and then use

   write.table( tab, "file.txt", sep=" ", row.names=F)

   as suggested.

   But I still wonder why?

   If you used e.g.

   write.table( tab, "file.xls", sep="\t", row.names=F)

   you can open it directly by spreadsheet program just by clickung on it
   and everything shall be properly aligned.

   HTH

   Petr

   On 22 Nov 2005 at 12:26, Gorjanc Gregor wrote:

   Date sent:                Tue, 22 Nov 2005 12:26:43 +0100

   From:                       "Gorjanc Gregor"
   <Gregor.Gorjanc at bfro.uni-lj.si>

   To:                           <r-help at stat.math.ethz.ch>

   Copies to:                042045003 at fudan.edu.cn

   Subject:                     [R]  Fw:  Re:  Is  there  anything like a
   write.fwf() or possibility to

                                    print adata.frame without rownames?

   > >>Petr Pikal wrote:

   > >>> Hi

   > >>>

   > >>> did you tried something like

   > >>>

   > >>> write.table( tab, "file.txt", sep="\t", row.names=F)

   > >>>

   > >>> which writes to tab separated file?

   > >>>

   > >>

   > >>Petr thanks, but I do not want a tab delimited file. I need spaces

   > >>between columns.

   > >

   > > write.table( tab, "file.txt", sep="", row.names=F)

   > > Can it do what you want?

   >

   > Ronggui thanks,

   >

   > but this does not work also. For example I get something like

   > this bellow

   >

   > "26" "1" 42 "DA" "DA" "lipa" "Monika"

   > "26" "1" 42 "DA" "DA" "lipa" "Monika"

   > "27" "1" 41 "DA" "DA" "smreka" "Monika"

   > "27" "1" 41 "DA" "DA" "smreka" "Monika"

   >

   > and you can see, that there is a problem, when all "values"

   > in a column do not have the same length. I need to get

   >

   > "26" "1" 42 "DA" "DA" "lipa"   "Monika"

   > "26" "1" 42 "DA" "DA" "lipa"   "Monika"

   > "27" "1" 41 "DA" "DA" "smreka" "Monika"

   > "27" "1" 41 "DA" "DA" "smreka" "Monika"

   >

   > i.e. columns should be properly aligned.

   >

   > Lep pozdrav / With regards,

   >     Gregor Gorjanc

   >

   >
   ----------------------------------------------------------------------

   > University of Ljubljana Biotechnical Faculty        URI:

   >    http://www.bfro.uni-lj.si/MR/ggorjan    Zootechnical   Department
   mail:

   > gregor.gorjanc <at> bfro.uni-lj.si Groblje 3                   tel:

   > +386 (0)1 72 17 861 SI-1230 Domzale             fax: +386 (0)1 72 17

   > 888 Slovenia, Europe

   >
   ----------------------------------------------------------------------

   >  "One  must  learn by doing the thing; for though you think you know
   it,

   >  you have no certainty until you try." Sophocles ~ 450 B.C.

   >

   > ______________________________________________

   > R-help at stat.math.ethz.ch mailing list

   > https://stat.ethz.ch/mailman/listinfo/r-help

   > PLEASE do read the posting guide!

   > http://www.R-project.org/posting-guide.html

   Petr Pikal

   petr.pikal at precheza.cz


From sara at gmesintra.com  Tue Nov 22 13:30:51 2005
From: sara at gmesintra.com (Sara Mouro)
Date: Tue, 22 Nov 2005 12:30:51 -0000
Subject: [R] Plot L-function (standardized Ripley's K)
Message-ID: <200511221230.jAMCUsxb020932@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/57874863/attachment.pl

From ehlers at math.ucalgary.ca  Tue Nov 22 13:48:05 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Tue, 22 Nov 2005 05:48:05 -0700
Subject: [R] R: pp plot
In-Reply-To: <43830A04.4F581D22@STATS.uct.ac.za>
References: <43830A04.4F581D22@STATS.uct.ac.za>
Message-ID: <43831385.4030406@math.ucalgary.ca>

Is qq.plot in package 'car' of use to you? I think that it
requires your distribution to be one of those available in R.

Peter

Clark Allan wrote:
> hi all
> 
> i would like to know if anyone has a reference on how one would place
> the "bands" on the pp plot.
> 
> i want to test whether or not a certain data set comes from a particular
> distribution (not normal).
> 
> i've already plotted F(X(j)) vs j/(n+1)	where F(x) is the cum dist
> function, X(j) is the j'th order statistic and n is the sample size. 
> 
> a goole search gave arb references and thought some one one the list
> should definitely know how to solve this problem.
> 
> thanking you in advance
> allan
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

--
Peter Ehlers
University of Calgary



From Gregor.Gorjanc at bfro.uni-lj.si  Tue Nov 22 13:48:59 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Tue, 22 Nov 2005 13:48:59 +0100
Subject: [R] Fw: Re: Is there anything like a write.fwf() or possibility
	to	print adata.frame without rownames?
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31AC6@pollux.bfro.uni-lj.si>

> Hi
>
> can you please explain why do you need it? What do you want to do with the exported file?
> 
> I wonder what type of software can not accept any reasonable delimiter and requires fwf files.
>
> The only workaround I can imagine is to transfer all columns to character and add leading spaces to each item which is > shorter than longest item in specified column to equalize length of all items in column and then use  
> 
> write.table( tab, "file.txt", sep=" ", row.names=F)
> 
> as suggested.
>
> But I still wonder why?

OK, I did not want to be to specific, but here it goes. I am using some 
"special" software for variance component estimation and prediction in
genetics. Programs are VCE and PEST (http://w3.tzv.fal.de/%7Eeg/) and
they both read data in FW (fixed width) format. For those programs
you can only give data in such format and it is really tedious to do so,
but that is the way it is. 

>
> If you used e.g.
> write.table( tab, "file.xls", sep="\t", row.names=F)
>
> you can open it directly by spreadsheet program just by clickung on it and everything shall be properly aligned.
>

I am fully aware of this, but I do need FW format. I will try with sprintf(), but
this looks very though for me.

> >>Petr Pikal wrote:
> >>> Hi
> >>>
> >>> did you tried something like
> >>>
> >>> write.table( tab, "file.txt", sep="\t", row.names=F)
> >>>
> >>> which writes to tab separated file?
> >>>
> >>
> >>Petr thanks, but I do not want a tab delimited file. I need spaces
> >>between columns.
> >
> > write.table( tab, "file.txt", sep="", row.names=F)
> > Can it do what you want?
> 
> Ronggui thanks,
> 
> but this does not work also. For example I get something like
> this bellow
> 
> "26" "1" 42 "DA" "DA" "lipa" "Monika"
> "26" "1" 42 "DA" "DA" "lipa" "Monika"
> "27" "1" 41 "DA" "DA" "smreka" "Monika"
> "27" "1" 41 "DA" "DA" "smreka" "Monika"
> 
> and you can see, that there is a problem, when all "values"
> in a column do not have the same length. I need to get
> 
> "26" "1" 42 "DA" "DA" "lipa"   "Monika"
> "26" "1" 42 "DA" "DA" "lipa"   "Monika"
> "27" "1" 41 "DA" "DA" "smreka" "Monika"
> "27" "1" 41 "DA" "DA" "smreka" "Monika"
> 
> i.e. columns should be properly aligned.
> 
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> ----------------------------------------------------------------------
> University of Ljubljana Biotechnical Faculty        URI:
> http://www.bfro.uni-lj.si/MR/ggorjan Zootechnical Department     mail:
> gregor.gorjanc <at> bfro.uni-lj.si Groblje 3                   tel:
> +386 (0)1 72 17 861 SI-1230 Domzale             fax: +386 (0)1 72 17
> 888 Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


Petr Pikal
petr.pikal at precheza.cz



From stratja at auburn.edu  Tue Nov 22 13:51:48 2005
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Tue, 22 Nov 2005 06:51:48 -0600
Subject: [R] cv.glm help: no responses?
Message-ID: <4382C004020000F200001C54@TMIA1.AUBURN.EDU>

I sent out a question a few days ago asking help with cv.glm and I
didn't get any responses.  Is it my question?  I'd appreciate any
response just to see my post makes it out there. 

What I'm looking to do is to get a column of predicted responses from
cv.glm (leave-one-out).  This produces a single predicted value for each
case, correct?  How can I get those values?  I don't want predicted
values using a full model but just the values that come from the cross
validation.  

Thanks,

Jeff

Here's the code I used for the model and to get delta: 

q_uk.nb <- glm.nb(nat_est ~ UK + I(UK^2), SRCOUNT, link=log)
cv.err1 <- cv.glm(SRCOUNT,q_uk.nb)


****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja



From rlittle at ula.ve  Tue Nov 22 13:53:20 2005
From: rlittle at ula.ve (Roy Little)
Date: Tue, 22 Nov 2005 08:53:20 -0400
Subject: [R] loadings matrices in plsr vs pcr in pls pacakage
Message-ID: <438314C0.2040902@ula.ve>

Dear list,
I have a question concerning the above mentioned methods in the pls 
package with respect to the loadings matrix produced by the call. In 
some work I am doing I have found that the values produced are nearly of 
the same magnitude but of opposite sign. When I use the example data 
(sensory) I find this result reproduced. I am prepared to work this 
through but I have a feeling that there could be a possible error in the 
code. (?!)



 > sens.pcr$loadings
            Comp 1      Comp 2     Comp 3    Comp 4
yellow  75.186621  -0.4780473   3.212149  1.750123
green  -90.490256   8.5880530   1.634961  1.042239
brown   -2.861241 -11.3600509 -15.920789 -1.105799
glossy  13.347090  19.3103902  -3.121693  2.781282
transp  20.126987  24.0653312  -6.656764 -1.842907
syrup   -7.199972  -5.3436196  -5.073675  5.620454
 > sens.pls$loadings

Loadings:
        Comp 1  Comp 2  Comp 3  Comp 4
yellow -74.448 -10.519   3.169  -1.056
green   88.299  21.627  -0.521  -0.976
brown    4.959 -14.253 -12.761  -0.371
glossy -15.798  15.914  -7.574   3.504
transp -23.049  18.673 -12.214  -2.068
syrup    8.045  -5.313  -3.698   2.181

Thank you for your help.

Roy Little
Dept. Chem.
Facultad de Ciencias
Universidad de los Andes
M??rida, Venezuela



From Allan at STATS.uct.ac.za  Tue Nov 22 13:56:41 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Tue, 22 Nov 2005 14:56:41 +0200
Subject: [R] R: pp plot
References: <43830A04.4F581D22@STATS.uct.ac.za>
	<43831385.4030406@math.ucalgary.ca>
Message-ID: <43831589.D1EC0E44@STATS.uct.ac.za>

the distribution is not a standard one

P Ehlers wrote:
> 
> Is qq.plot in package 'car' of use to you? I think that it
> requires your distribution to be one of those available in R.
> 
> Peter
> 
> Clark Allan wrote:
> > hi all
> >
> > i would like to know if anyone has a reference on how one would place
> > the "bands" on the pp plot.
> >
> > i want to test whether or not a certain data set comes from a particular
> > distribution (not normal).
> >
> > i've already plotted F(X(j)) vs j/(n+1)       where F(x) is the cum dist
> > function, X(j) is the j'th order statistic and n is the sample size.
> >
> > a goole search gave arb references and thought some one one the list
> > should definitely know how to solve this problem.
> >
> > thanking you in advance
> > allan
> >
> >
> > ------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> --
> Peter Ehlers
> University of Calgary
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

From ggrothendieck at gmail.com  Tue Nov 22 14:09:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 22 Nov 2005 08:09:38 -0500
Subject: [R] Fw: Re: Is there anything like a write.fwf() or possibility
	to print adata.frame without rownames?
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730F31AC6@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730F31AC6@pollux.bfro.uni-lj.si>
Message-ID: <971536df0511220509sb08a615h9bb3746ab6b211e5@mail.gmail.com>

On 11/22/05, Gorjanc Gregor <Gregor.Gorjanc at bfro.uni-lj.si> wrote:
> > Hi
> >
> > can you please explain why do you need it? What do you want to do with the exported file?
> >
> > I wonder what type of software can not accept any reasonable delimiter and requires fwf files.
> >
> > The only workaround I can imagine is to transfer all columns to character and add leading spaces to each item which is > shorter than longest item in specified column to equalize length of all items in column and then use
> >
> > write.table( tab, "file.txt", sep=" ", row.names=F)
> >
> > as suggested.
> >
> > But I still wonder why?
>
> OK, I did not want to be to specific, but here it goes. I am using some
> "special" software for variance component estimation and prediction in
> genetics. Programs are VCE and PEST (http://w3.tzv.fal.de/%7Eeg/) and
> they both read data in FW (fixed width) format. For those programs
> you can only give data in such format and it is really tedious to do so,
> but that is the way it is.
>
> >
> > If you used e.g.
> > write.table( tab, "file.xls", sep="\t", row.names=F)
> >
> > you can open it directly by spreadsheet program just by clickung on it and everything shall be properly aligned.
> >
>
> I am fully aware of this, but I do need FW format. I will try with sprintf(), but
> this looks very though for me.
>
> > >>Petr Pikal wrote:
> > >>> Hi
> > >>>
> > >>> did you tried something like
> > >>>
> > >>> write.table( tab, "file.txt", sep="\t", row.names=F)
> > >>>
> > >>> which writes to tab separated file?
> > >>>
> > >>
> > >>Petr thanks, but I do not want a tab delimited file. I need spaces
> > >>between columns.
> > >
> > > write.table( tab, "file.txt", sep="", row.names=F)
> > > Can it do what you want?
> >
> > Ronggui thanks,
> >
> > but this does not work also. For example I get something like
> > this bellow
> >
> > "26" "1" 42 "DA" "DA" "lipa" "Monika"
> > "26" "1" 42 "DA" "DA" "lipa" "Monika"
> > "27" "1" 41 "DA" "DA" "smreka" "Monika"
> > "27" "1" 41 "DA" "DA" "smreka" "Monika"
> >
> > and you can see, that there is a problem, when all "values"
> > in a column do not have the same length. I need to get
> >
> > "26" "1" 42 "DA" "DA" "lipa"   "Monika"
> > "26" "1" 42 "DA" "DA" "lipa"   "Monika"
> > "27" "1" 41 "DA" "DA" "smreka" "Monika"
> > "27" "1" 41 "DA" "DA" "smreka" "Monika"
> >
> > i.e. columns should be properly aligned.
> >

Try this:

> irish <- head(iris)
> write(t(apply(irish, 2, format)), file = "", ncol = ncol(irish))
5.1 3.5 1.4 0.2 setosa
4.9 3.0 1.4 0.2 setosa
4.7 3.2 1.3 0.2 setosa
4.6 3.1 1.5 0.2 setosa
5.0 3.6 1.4 0.2 setosa
5.4 3.9 1.7 0.4 setosa



From Bernhard_Pfaff at fra.invesco.com  Tue Nov 22 14:28:36 2005
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 22 Nov 2005 13:28:36 -0000
Subject: [R] cointegration rank
Message-ID: <25D1C2585277D311B9A20000F6CCC71B07510EC6@DEFRAEX02>

Hello Carlo,

no, this is not possible, per se. In case of a structural break in terms of
a one-time level shift, you might want to consider the function cajolst().
Another possibility would be to run a regression with the dummy-variables
and use the fitted values for your data matrix x. That is, the data is
'pre-filtered' by the impact of the dummy variables.

HTH,
Bernhard


Another question on cointegration...

Is it possible to insert in the model dummy variables restricted in the
cointegration space?

Many thanks,

Carlo


On Nov 21, 2005 01:23 PM, "Pfaff, Bernhard Dr."
<Bernhard_Pfaff at fra.invesco.com> wrote:

> Thanks a lot!
> 
> I have another question on "cointegration", so I will go on this post.
> 
> Is it possible to estimate a cointegration with some exogenous
> explanatory variables? Since, after testing for exogeneity, I would
> like
> to re-estimate the relation keeping some of the previous endogenous as
> exogenous.
> 
> Many thanks!
> 
> Carlo
> 
> 
> Hello Carlo,
> 
> you can use the 'dumvar' argument for his purpose, and exclude the
> relevant
> variables from your data matrix 'x'.
> 
> HTH,
> Bernhard
> 
> 
> On Nov 21, 2005 11:21 AM, "Pfaff, Bernhard Dr."
> <Bernhard_Pfaff at fra.invesco.com> wrote:
> 
> > Dear R - helpers,
> > 
> > I am using the urca package to estimate cointegration relations, and
> > I
> > would be really grateful if somebody could help me with this
> > questions:
> > 
> > After estimating the unrestriced VAR with "ca.jo" I would like to
> > impose
> > the rank restriction (for example rank = 1) and then obtain the
> > restricted estimate of PI to be utilized to estimate the VECM model.
> > 
> > Is it possible? 
> > 
> > It seems to me that the function "cajools" estimates the VECM
> > without
> > the restrictions. Did I miss something? How is it possible to impose
> > them?
> > 
> > Thanks a lot in advance!
> > 
> > Carlo
> > 
> > 
> > Hello Carlo,
> > 
> > you can achieve this, by calculating your desired PI-matrix by hand,
> > given
> > the slots 'V' and 'W' of your ca.jo object and then execute a
> > restricted
> > OLS-estimation, if I understand your goal correctly. 
> > Please, bear in mind the non-uniqueness of the factorization of the
> > PI-matrix by doing so.
> > 
> > HTH,
> > Bernhard    
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
-------------- next part --------------
*****************************************************************
Confidentiality Note: The information contained in this message,
and any attachments, may contain confidential and/or privileged
material. It is intended solely for the person(s) or entity to
which it is addressed. Any review, retransmission, dissemination,
or taking of any action in reliance upon this information by
persons or entities other than the intended recipient(s) is
prohibited. If you received this in error, please contact the
sender and delete the material from any computer.
*****************************************************************

From MSchwartz at mn.rr.com  Tue Nov 22 14:42:23 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 22 Nov 2005 07:42:23 -0600
Subject: [R] Crop white border for PDF output
In-Reply-To: <Pine.OSX.4.61.0511221047270.1635@cirrus.aue.aau.dk>
References: <Pine.OSX.4.61.0511192056590.3976@cirrus.local>
	<73dae3060511220103g7355b021x246e9c4be8aa363a@mail.gmail.com>
	<Pine.OSX.4.61.0511221047270.1635@cirrus.aue.aau.dk>
Message-ID: <1132666943.19636.1.camel@localhost.localdomain>

On Tue, 2005-11-22 at 10:48 +0100, Claus Atzenbeck wrote:
> On Tue, 22 Nov 2005, Florence Combes wrote:
> 
> > I had not exactly the same pb, but close enough to propose you to take a
> > look at the options "fin" and "fig" in the par() function.
> 
> Thanks for your answer. In the meanwhile I also found another solution.
> I produce diagrams as usual and use pdfcrop to crop them afterwards. It
> takes two steps, but that's also OK.
> 
> Claus


Without your code, it is hard to know exactly what you need to resolve
the problem or why your manipulation of the margins did not work as you
want, but I am guessing that the following might be helpful.

Here is a default PDF plot:

 pdf("PDF1.pdf", height = 4, width = 4)
 barplot(1:5)
 dev.off()

 
Here is the same plot, but with the margins reduced:

 pdf("PDF2.pdf", height = 4, width = 4)
 par(mar = c(1, 3, 1, 1))
 barplot(1:5)
 dev.off()


Here is a default 2 x 2 plot:

 pdf("PDF3.pdf", height = 4, width = 4)
 par(mfrow = c(2, 2))
 barplot(1:5)
 barplot(1:5)
 barplot(1:5)
 barplot(1:5)
 dev.off()


Here is the same with reduced margins:

 pdf("PDF4.pdf", height = 4, width = 4)
 par(mfrow = c(2, 2))
 par(mar = c(1, 3, 1, 1))
 barplot(1:5)
 barplot(1:5)
 barplot(1:5)
 barplot(1:5)
 dev.off()



HTH,

Marc Schwartz



From claus.atzenbeck at freenet.de  Tue Nov 22 14:59:16 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Tue, 22 Nov 2005 14:59:16 +0100 (CET)
Subject: [R] Crop white border for PDF output
In-Reply-To: <1132666943.19636.1.camel@localhost.localdomain>
References: <Pine.OSX.4.61.0511192056590.3976@cirrus.local> 
	<73dae3060511220103g7355b021x246e9c4be8aa363a@mail.gmail.com> 
	<Pine.OSX.4.61.0511221047270.1635@cirrus.aue.aau.dk>
	<1132666943.19636.1.camel@localhost.localdomain>
Message-ID: <Pine.OSX.4.61.0511221449430.26628@cirrus.aue.aau.dk>

On Tue, 22 Nov 2005, Marc Schwartz wrote:

> Without your code, it is hard to know exactly what you need to resolve
> the problem or why your manipulation of the margins did not work as you
> want, but I am guessing that the following might be helpful.

The reason why the manipulation of margins does not make me happy is
that I have to name concrete numbers for every diagram.

Basically, it would be nice to tell R to produce PDF output without
*any* white border. It looks ugly when viewed directly, but it is
perfect when included in documents, where, e.g., TeX takes care of
spaces between diagrams an captions, etc.

Even your examples with the reduced borders still show a white border
around the diagram. It would take rather long to get rid of that by
using trial and error.

The disadvantage with pdfcrop is that it takes an additional step:
First, I have to produce the diagrams, then, I have to call pdfcrop on
them to crop the white space. pdfcrop does a perfect job here: It crops
the complete white space around a diagram automatically without having
me to name some concrete numbers. It would be very nice if R would have
an option that would allow to do me to do this for graphics that will be
included in documents later.

Claus



From pedroestrela at yahoo.com  Tue Nov 22 15:03:29 2005
From: pedroestrela at yahoo.com (Pedro Cordeiro Estrela)
Date: Tue, 22 Nov 2005 06:03:29 -0800 (PST)
Subject: [R] Problem in compilation from source in./configure R.2.2
In-Reply-To: <x2hda5k57y.fsf@turmalin.kubism.ku.dk>
Message-ID: <20051122140330.69659.qmail@web54202.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/7e427252/attachment.pl

From jholtman at gmail.com  Tue Nov 22 15:04:25 2005
From: jholtman at gmail.com (jim holtman)
Date: Tue, 22 Nov 2005 09:04:25 -0500
Subject: [R] Fw: Re: Is there anything like a write.fwf() or possibility
	to print adata.frame without rownames?
In-Reply-To: <971536df0511220509sb08a615h9bb3746ab6b211e5@mail.gmail.com>
References: <7FFEE688B57D7346BC6241C55900E730F31AC6@pollux.bfro.uni-lj.si>
	<971536df0511220509sb08a615h9bb3746ab6b211e5@mail.gmail.com>
Message-ID: <644e1f320511220604r714a8ee9n90b4105c0f364041@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/9e9baf7d/attachment.pl

From p.dalgaard at biostat.ku.dk  Tue Nov 22 15:05:01 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Nov 2005 15:05:01 +0100
Subject: [R] R: pp plot
In-Reply-To: <43831589.D1EC0E44@STATS.uct.ac.za>
References: <43830A04.4F581D22@STATS.uct.ac.za>
	<43831385.4030406@math.ucalgary.ca>
	<43831589.D1EC0E44@STATS.uct.ac.za>
Message-ID: <x23blodbhu.fsf@viggo.kubism.ku.dk>

Clark Allan <Allan at STATS.uct.ac.za> writes:

> the distribution is not a standard one

As long as it has a CDF and a density it doesn't seem to matter if you
just follow the naming conventions:

 qfoo <- qnorm
 dfoo <- dnorm
 z <- rnorm(50)
 qq.plot(z,"foo")

The code for qq.plot.default appears quite readable and should also
provide you an answer to the theoretical side of your question. Notice
however, that the technique relies on the approximation of the
binomial distribution by a normal one and on the delta method, so is
not going to be very accurate in the tails. I suspect that you can do
somewhat better by using exact confidence intervals for the binomial
distribution, or (likely better) the beta distribution of the order
statistics in the uniform distribution.

(BTW, P-P plots are often useless because they cannot resolve
discrepancies in small p-values. At the very least you want to use
log="xy". Q-Q plots are much better in this respect.)

 
> P Ehlers wrote:
> > 
> > Is qq.plot in package 'car' of use to you? I think that it
> > requires your distribution to be one of those available in R.
> > 
> > Peter
> > 
> > Clark Allan wrote:
> > > hi all
> > >
> > > i would like to know if anyone has a reference on how one would place
> > > the "bands" on the pp plot.
> > >
> > > i want to test whether or not a certain data set comes from a particular
> > > distribution (not normal).
> > >
> > > i've already plotted F(X(j)) vs j/(n+1)       where F(x) is the cum dist
> > > function, X(j) is the j'th order statistic and n is the sample size.
> > >
> > > a goole search gave arb references and thought some one one the list
> > > should definitely know how to solve this problem.
> > >
> > > thanking you in advance
> > > allan


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From szlevine at nana.co.il  Tue Nov 22 15:09:24 2005
From: szlevine at nana.co.il (Stephen)
Date: Tue, 22 Nov 2005 16:09:24 +0200
Subject: [R] Weibull  and survival
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD668@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/8e3d625b/attachment.pl

From p.dalgaard at biostat.ku.dk  Tue Nov 22 15:18:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Nov 2005 15:18:08 +0100
Subject: [R] Problem in compilation from source in./configure R.2.2
In-Reply-To: <20051122140330.69659.qmail@web54202.mail.yahoo.com>
References: <20051122140330.69659.qmail@web54202.mail.yahoo.com>
Message-ID: <x2y83gbwbj.fsf@viggo.kubism.ku.dk>

Pedro Cordeiro Estrela <pedroestrela at yahoo.com> writes:

> I tried to get old libraries (libreadline4) installed without success.
>  If the configure script checks for old versions of the readline library can't we just comment the corresponding lines?

No, because the code in question still needs to work. If it doesn't,
then you cannot compile against libreadline, you'll just get the error
further down the line.
 
> 

[You need to work on your quoting mechanisms: The next two paragraphs
were from my mail to you, but it doesn't show] 

> This check is in there to check for some really old readline versions
> that didn't allow the style of readline interface that R used. Do you
> happen to have such an older version that could get picked up by the
> linker? 
> 
>    
>    
>  
> What's inside config.log corresponding to the messages? These tests
> generally work by compiling and linking a small test program, and if
> that fails, you get the "no" part. However, sometimes the program
> fails for some other reason and the message becomes misleading.



[and now it's you talking again]
> 
> 
> Here what's in the config.log file for the readline part.
>  Thanks a lot!
>  
>  configure:21592: result: yes
>  configure:21627: checking for readline/readline.h
>  configure:21634: result: yes
>  configure:21652: checking for rl_callback_read_char in -lreadline
>  configure:21682: gcc -o conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -lreadline   >&5
>  /usr/lib/gcc/i586-mandrake-linux-gnu/3.4.3/../../../libreadline.so: undefined reference to `tgetnum'
>  /usr/lib/gcc/i586-mandrake-linux-gnu/3.4.3/../../../libreadline.so: undefined reference to `tgoto'
>  /usr/lib/gcc/i586-mandrake-linux-gnu/3.4.3/../../../libreadline.so: undefined reference to `tgetflag'
>  /usr/lib/gcc/i586-mandrake-linux-gnu/3.4.3/../../../libreadline.so: undefined reference to `BC'
>  /usr/lib/gcc/i586-mandrake-linux-gnu/3.4.3/../../../libreadline.so: undefined reference to `tputs'
>  /usr/lib/gcc/i586-mandrake-linux-gnu/3.4.3/../../../libreadline.so: undefined reference to `PC'
>  /usr/lib/gcc/i586-mandrake-linux-gnu/3.4.3/../../../libreadline.so: undefined reference to `tgetent'
>  /usr/lib/gcc/i586-mandrake-linux-gnu/3.4.3/../../../libreadline.so: undefined reference to `UP'
>  /usr/lib/gcc/i586-mandrake-linux-gnu/3.4.3/../../../libreadline.so: undefined reference to `tgetstr'


Oho, now those are ncurses routines, which suggests that you need to
install ncurses or ncurses-devel (or so, I'm not aware of how 
Mandrake/-driva organizes things). Either that, or you need to include
-lncurses in the --with-readline stuff.

>  collect2: ld returned 1 exit status
>  configure:21688: $? = 1
>  configure: failed program was:
>  | /* confdefs.h.  */
>  | 
>  | #define PACKAGE_NAME "R"
>  | #define PACKAGE_TARNAME "R"
>  | #define PACKAGE_VERSION "2.2.0"
>  | #define PACKAGE_STRING "R 2.2.0"
>  | #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
>  | #define PACKAGE "R"
>  | #define VERSION "2.2.0"
>  | #define R_PLATFORM "i686-pc-linux-gnu"
>  | #define R_CPU "i686"
>  | #define R_VENDOR "pc"
>  | #define R_OS "linux-gnu"
>  | #define Unix 1
>  | #ifdef __cplusplus
>  | extern "C" void std::exit (int) throw (); using std::exit;
>  | #endif
>  | #define STDC_HEADERS 1
>  | #define HAVE_SYS_TYPES_H 1
>  | #define HAVE_SYS_STAT_H 1
>  | #define HAVE_STDLIB_H 1
>  | #define HAVE_STRING_H 1
>  | #define HAVE_MEMORY_H 1
>  | #define HAVE_STRINGS_H 1
>  | #define HAVE_INTTYPES_H 1
>  | #define HAVE_STDINT_H 1
>  | #define HAVE_UNISTD_H 1
>  | #define HAVE_DLFCN_H 1
>  | #define HAVE_LIBM 1
>  | #define HAVE_LIBDL 1
>  | #define HAVE_READLINE_HISTORY_H 1
>  | #define HAVE_READLINE_READLINE_H 1
>  | /* end confdefs.h.  */
>  | 
>  | /* Override any gcc2 internal prototype to avoid an error.  */
>  | #ifdef __cplusplus
>  | extern "C"
>  | #endif
>  | /* We use char because int might match the return type of a gcc2
>  |    builtin and then its argument prototype would still apply.  */
>  | char rl_callback_read_char ();
>  | int
>  | main ()
>  | {
>  | rl_callback_read_char ();
>  |   ;
>  |   return 0;
>  | }
>  configure:21714: result: no
>  configure:21730: checking for main in -lncurses
>  configure:21754: gcc -o conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -lncurses   >&5
>  /usr/bin/ld: cannot find -lncurses
>  collect2: ld returned 1 exit status
>  configure:21760: $? = 1
>  configure: failed program was:
>  | /* confdefs.h.  */
>  | 
>  | #define PACKAGE_NAME "R"
>  | #define PACKAGE_TARNAME "R"
>  | #define PACKAGE_VERSION "2.2.0"
>  | #define PACKAGE_STRING "R 2.2.0"
>  | #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
>  | #define PACKAGE "R"
>  | #define VERSION "2.2.0"
>  | #define R_PLATFORM "i686-pc-linux-gnu"
>  | #define R_CPU "i686"
>  | #define R_VENDOR "pc"
>  | #define R_OS "linux-gnu"
>  | #define Unix 1
>  | #ifdef __cplusplus
>  | extern "C" void std::exit (int) throw (); using std::exit;
>  | #endif
>  | #define STDC_HEADERS 1
>  | #define HAVE_SYS_TYPES_H 1
>  | #define HAVE_SYS_STAT_H 1
>  | #define HAVE_STDLIB_H 1
>  | #define HAVE_STRING_H 1
>  | #define HAVE_MEMORY_H 1
>  | #define HAVE_STRINGS_H 1
>  | #define HAVE_INTTYPES_H 1
>  | #define HAVE_STDINT_H 1
>  | #define HAVE_UNISTD_H 1
>  | #define HAVE_DLFCN_H 1
>  | #define HAVE_LIBM 1
>  | #define HAVE_LIBDL 1
>  | #define HAVE_READLINE_HISTORY_H 1
>  | #define HAVE_READLINE_READLINE_H 1
>  | /* end confdefs.h.  */
>  | 
>  | 
>  | int
>  | main ()
>  | {
>  | main ();
>  |   ;
>  |   return 0;
>  | }
>  
> 
> 
> 
> ____________________________________________________________
> 
> Pedro Cordeiro Estrela
> PhD. student
> 
> UMR 2695: Origine structure et ????volution de la biodiversit????
> D????partement de syst????matique et ????volution
> Mus????um National d'Histoire Naturelle 
> 
> 55, rue Buffon
> 75005
> Paris - FRANCE
> 
> tel: (33) [0]1 40 79 30 86
> fax: (33) [0]1 40 79 30 63
> [0] : from france only
> _____________________________________________________________
> 		
> ---------------------------------
>  Yahoo! FareChase - Search multiple travel sites in one click.  

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From brian_cade at usgs.gov  Tue Nov 22 16:35:40 2005
From: brian_cade at usgs.gov (Brian S Cade)
Date: Tue, 22 Nov 2005 08:35:40 -0700
Subject: [R] modifying code in contributed libraries - changes from versions
	1.* to 2.*
Message-ID: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/9a7c4157/attachment.pl

From tlumley at u.washington.edu  Tue Nov 22 17:22:27 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 22 Nov 2005 08:22:27 -0800 (PST)
Subject: [R] residuals.coxph
In-Reply-To: <003e01c5ef46$241c22a0$2b18a7c0@alice>
References: <003e01c5ef46$241c22a0$2b18a7c0@alice>
Message-ID: <Pine.LNX.4.63a.0511220821171.29597@homer22.u.washington.edu>

On Tue, 22 Nov 2005, giovanni parrinello wrote:

> Dear All,
> I am trying to apply the function 'cox.zph' of the library survival, but I receive this error message:
> not found the object 'residuals.coxph'.
> I have re-installed the library 'survival' without any change and also a search with RSiteSearch was unsuccessful..
> Any suggestion?

Does
   library(survival)
   example(cox.zph)
work?

If it doesn't, something is wrong with your R setup. If it does, the 
problem is specific to your data and code (there might be an error on line 
42).


 	-thomas



From gunter.berton at gene.com  Tue Nov 22 17:24:09 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 22 Nov 2005 08:24:09 -0800
Subject: [R] Kolmogorov-Smirnov test help
In-Reply-To: <4382F2C8.2060303@statistik.uni-dortmund.de>
Message-ID: <200511221624.jAMGO8af028711@ohm.gene.com>

Re: Uwe's advice.

A quote from the late Ellis Ott (a U.S. quality control statistician of the
1970's and 80's):

"Look at your data -- and think!"

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
> Sent: Tuesday, November 22, 2005 2:28 AM
> To: lisadowson at gmail.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Kolmogorov-Smirnov test help
> 
> Lisa Dowson wrote:
> 
> > Hi
> > 
> > I am conducting 2-sample Kolmogorov Smirnov tests for my 
> Masters project to
> > determine if two independant tree populations have the same 
> size-class
> > distribution or not. The trees have been placed into 
> size-class categories
> > based on their basal diameters. Once I started running the 
> stats on my data,
> > I got confused with the results. Just to show an example of 
> what I was
> > testing I ran stats comparing population1 to population 2. and then
> > comparing population 3 to population 2.
> >   Popn1 Popn2 Popn3  880 769 0  34 40 19  10 24 19  2 2 8  
> 2 2 36  0 0 0
> 
> If I interpret your data correctly, we can look at the step 
> functions by:
> 
>   plot.ecdf(x[,1], xlim=c(-10, 900), verticals=TRUE)
>   plot.ecdf(x[,2], xlim=c(-10, 900), col.hor="blue", col.vert="blue", 
> add=TRUE, verticals=TRUE)
>   plot.ecdf(x[,3], xlim=c(-10, 900), col.hor="red", col.vert="red", 
> add=TRUE, verticals=TRUE)
> 
> and see that there is no good reason why the test should 
> reject the Null 
>   for so few (6!) observations.
> 
> Note that you need at least 4 observations in each group to 
> be able to 
> reject anything at alpha=0.05 even for completely different 
> distributions:
> 
>   ks.test(1:3, 101:103)
> 
> 
> > Common sense tells me that P1 and P2 are similar and that 
> P3 and P2 are
> > dissimilar. However, for the P1 versus P2 test I am getting 
> p-value of 1
> > (saying they are strongly similar -what I expected) and for 
> the P3 versus P2
> > test, a p-value of 0.9307 (saying they are strongly 
> similar, but slightly
> > less similar than the other test- not what I expected at 
> all). However,
> > common sense tells me that P3 and P2 are not similar at all 
> and that I
> > should be expecting a far lower p-value for the test 
> comparing P3 and P2.
> 
> Sometimes common sense is dangerous. Sort your data, look again, look 
> into the plot, and rethink...
> 
> Uwe Ligges
> 
> 
> 
> 
> > Any help would be greatly appreciated
> > 
> > Thanks
> > 
> > Lisa
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From MiraKuester at web.de  Tue Nov 22 17:32:30 2005
From: MiraKuester at web.de (=?iso-8859-1?Q?Mira=20K=FCster?=)
Date: Tue, 22 Nov 2005 17:32:30 +0100
Subject: [R] searching for the 'right' cluster analysis solution
Message-ID: <473185165@web.de>


Hello,

I need some help in finding the 'right'(maybe better 'the best') solution for my cluster analysis calculations. There are so many ways to do cluster analysis, I'm a litte bit puzzled. I've read one should choose the solution after what makes the best sense in regard to the content of the clusters.
Is there a function in R that can help to evaluate the cluster analysis solutions?

Another question:
I've done a ward cluster analysis and I've read in a book about cluster analysis, that one could make a structogram with the error variances, that looks almost like the scree plot of the PCA, to define the number of clusters, but unfortunately I haven't found any function for a structogram in R.
Is there some one who has written the function for that on his own, or who could give me a hint how to do this?


Best wishes,

Mira



From jari.oksanen at oulu.fi  Tue Nov 22 17:35:21 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Tue, 22 Nov 2005 18:35:21 +0200
Subject: [R] modifying code in contributed libraries - changes from
 versions 1.* to 2.*
In-Reply-To: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>
References: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>
Message-ID: <7ffe38a0c53d14e462b76e0d9bc07977@oulu.fi>


On 22 Nov 2005, at 17:35, Brian S Cade wrote:

> Having finally updated from R 1.91 to R 2.2.0 with my installation of a
> new computer, I discovered that something has changed drastically about
> the way code for contributed packages is stored when installed in a 
> local
> version of R.  In the 1.* versions it was easy for me to go in and 
> modify
> some of the code for a contributed package by using a text editor to
> change the script files (these were the files usually stored under the
> package name\R folder).  Now under R 2.2 when I look at the same 
> location
> (package name\R folder) there are 3 files, one with no extension, one 
> with
> .rdb extension, and one with a .rdx extension and none of these seem 
> to be
> text files that can be easily edited (at least not as I did before).  
> Now
> I know I can edit a specific function after I've loaded a package into 
> a
> specific workspace, but I would like to edit the contributed package so
> that anytime I load it the changes I've made are implemented.  While 
> I'm
> not generally in the habit of modifying other peoples code, when I 
> find a
> problem that I know how to fix, I'ld like to fix it - once.

I think the best way of fixing problems is to send your edited code to 
the package maintainer so that she (or he) can fix it now and in the 
future versions. If you want to fix the problem only privately, you may 
consider making your private package from your private fixes of 
functions from public CRAN package(s), and load that when you need.

>  I suspect I'm
> missing something pretty obvious about how to edit these but my search 
> of
> the online documentation, etc. didn't suggest any simple approach like 
> I
> use to do.  Any help would be welcome.
>
> Brian
>
> Brian S. Cade
>
> U. S. Geological Survey
> Fort Collins Science Center
> 2150 Centre Ave., Bldg. C
> Fort Collins, CO  80526-8818
>
> email:  brian_cade at usgs.gov
> tel:  970 226-9326
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
--
Jari Oksanen, Oulu, Finland



From levyr at umd.edu  Tue Nov 22 17:38:47 2005
From: levyr at umd.edu (levyr@umd.edu)
Date: Tue, 22 Nov 2005 11:38:47 -0500
Subject: [R] change axis format for different panels in xyplot in lattice
Message-ID: <430f06c.5f489b43.81ab200@po1.mail.umd.edu>

Dear R users,

My apologies for a simple question for which I suspect there
is a simple answer that I have yet to find.  I'd like to plot
panels in lattice with different graphical parameters for the
axes.  For example, the code 

x<-rnorm(100)
y<-rnorm(100)
z<-c(rep(1,50), rep(2,50))
library(lattice)
xyplot(y~x|z)

plots two panels with the default black axes.  Running the
following code

trellis.par.set(list(axis.line = list(col = "transparent")))
xyplot(y~x|z)

plots the same data without the axes.  Is it possible (in one
plot) to plot the first panel with black axes and the second
panel with tranparent axes?  

Thank you for your time and your attention,
Roy



From ligges at statistik.uni-dortmund.de  Tue Nov 22 17:44:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 22 Nov 2005 17:44:16 +0100
Subject: [R] modifying code in contributed libraries - changes from
 versions 1.* to 2.*
In-Reply-To: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>
References: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>
Message-ID: <43834AE0.2050304@statistik.uni-dortmund.de>

Brian S Cade wrote:
> Having finally updated from R 1.91 to R 2.2.0 with my installation of a 
> new computer, I discovered that something has changed drastically about 
> the way code for contributed packages is stored when installed in a local 
> version of R.  In the 1.* versions it was easy for me to go in and modify 
> some of the code for a contributed package by using a text editor to 
> change the script files (these were the files usually stored under the 
> package name\R folder).  Now under R 2.2 when I look at the same location 
> (package name\R folder) there are 3 files, one with no extension, one with 
> .rdb extension, and one with a .rdx extension and none of these seem to be 
> text files that can be easily edited (at least not as I did before).  Now 
> I know I can edit a specific function after I've loaded a package into a 
> specific workspace, but I would like to edit the contributed package so 
> that anytime I load it the changes I've made are implemented.  While I'm 
> not generally in the habit of modifying other peoples code, when I find a 
> problem that I know how to fix, I'ld like to fix it - once.  I suspect I'm 
> missing something pretty obvious about how to edit these but my search of 
> the online documentation, etc. didn't suggest any simple approach like I 
> use to do.  Any help would be welcome.


Due to lazy loading (see a corresponding R News article) code is stored 
in databases for some time now, if you are looking into binary packages.

In order to modify/fix something, download the source version of a 
package, change the code, and INSTALL the source package.

Uwe Ligges


> Brian
> 
> Brian S. Cade
> 
> U. S. Geological Survey
> Fort Collins Science Center
> 2150 Centre Ave., Bldg. C
> Fort Collins, CO  80526-8818
> 
> email:  brian_cade at usgs.gov
> tel:  970 226-9326
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From plummer at iarc.fr  Tue Nov 22 17:48:04 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Tue, 22 Nov 2005 17:48:04 +0100
Subject: [R] modifying code in contributed libraries - changes
	from	versions 1.* to 2.*
In-Reply-To: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>
References: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>
Message-ID: <1132678084.5502.31.camel@seurat>

On Tue, 2005-11-22 at 08:35 -0700, Brian S Cade wrote:
> Having finally updated from R 1.91 to R 2.2.0 with my installation of a 
> new computer, I discovered that something has changed drastically about 
> the way code for contributed packages is stored when installed in a local 
> version of R.  In the 1.* versions it was easy for me to go in and modify 
> some of the code for a contributed package by using a text editor to 
> change the script files (these were the files usually stored under the 
> package name\R folder).  Now under R 2.2 when I look at the same location 
> (package name\R folder) there are 3 files, one with no extension, one with 
> .rdb extension, and one with a .rdx extension and none of these seem to be 
> text files that can be easily edited (at least not as I did before).  Now 
> I know I can edit a specific function after I've loaded a package into a 
> specific workspace, but I would like to edit the contributed package so 
> that anytime I load it the changes I've made are implemented.  While I'm 
> not generally in the habit of modifying other peoples code, when I find a 
> problem that I know how to fix, I'ld like to fix it - once.  I suspect I'm 
> missing something pretty obvious about how to edit these but my search of 
> the online documentation, etc. didn't suggest any simple approach like I 
> use to do.  Any help would be welcome.
> 
> Brian
> 
> Brian S. Cade

These changes were introduced in R 2.0.0 to enable lazy loading of
objects in packages.  There is an article by Brian Ripley explaining the
changes in the September 2004 issue of R news (Vol 4/Issue 2), which you
can find here:

http://cran.r-project.org/doc/Rnews/

You can still modify source packages, but for this to be useful you must
have the capability to build binary packages yourself. If you are using
Windows, this might be hard.  If your change is a bug fix, rather than a
personal modification, then your best bet is to send an email to the
package maintainer. 

Martyn


-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}



From eric_wzl at yahoo.com  Tue Nov 22 18:04:38 2005
From: eric_wzl at yahoo.com (peter eric)
Date: Tue, 22 Nov 2005 09:04:38 -0800 (PST)
Subject: [R] how to plot a clustered matrix in graphs
In-Reply-To: <eb555e660511211605p3b9442d0y84462c772abf699f@mail.gmail.com>
Message-ID: <20051122170438.77958.qmail@web36404.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/e1202777/attachment.pl

From jpritikin at pobox.com  Tue Nov 22 18:13:29 2005
From: jpritikin at pobox.com (Joshua N Pritikin)
Date: Tue, 22 Nov 2005 22:43:29 +0530
Subject: [R] _simple_ introduction to bayesian methods?
Message-ID: <20051122171329.GR6437@always.joy.eth.net>

I made it through the first chapter ("Background") of Bayesian Data
Analysis by Gelman, et al.  Conceptually, the Bayesian approach
appeals to me and my curiosity is piqued.  However, the next chapter
was much too terse. The math is daunting.  Where can I find a gentle
introduction?  Or which math book(s) do I need to read first?

On page 27, there is mention of introductory books including Bayesian
Methods by Jeff Gill (2002).  Just for fun, I took a look at this book
to see whether I could get through it.  Chapter 1 was inviting, but
again, the math got too sophisticated starting from chapter 2.

-- 
Make April 15 just another day, visit http://fairtax.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051122/ceea49cc/attachment.bin

From sfalcon at fhcrc.org  Tue Nov 22 18:09:41 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 22 Nov 2005 09:09:41 -0800
Subject: [R] modifying code in contributed libraries - changes from
	versions 1.* to 2.*
In-Reply-To: <43834AE0.2050304@statistik.uni-dortmund.de> (Uwe Ligges's
	message of "Tue, 22 Nov 2005 17:44:16 +0100")
References: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>
	<43834AE0.2050304@statistik.uni-dortmund.de>
Message-ID: <m2lkzgy5gq.fsf@ziti.local>

On 22 Nov 2005, ligges at statistik.uni-dortmund.de wrote:
> Due to lazy loading (see a corresponding R News article) code is
> stored in databases for some time now, if you are looking into
> binary packages.
>
> In order to modify/fix something, download the source version of a 
> package, change the code, and INSTALL the source package.

Actually, R source packages are also mangled.  While the source is
readable, it is not in the form used to develop the package.  Changes
one makes by editing code in the source package will be difficult to
communicate to the package maintainer.

IMHO, the best option is to get a copy of the package source directory
tarball that the maintainer uses for package development.  This way,
you can easily send patches to the maintainer.

I suspect this will become more important as more things are computed
up front at R CMD build time.

Said another way: R source packages are special R-specific beasts.
They are not tarballs of the sources used to build the package.

+ seth



From ripley at stats.ox.ac.uk  Tue Nov 22 18:39:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Nov 2005 17:39:03 +0000 (GMT)
Subject: [R] Problem in compilation from source in./configure R.2.2
In-Reply-To: <20051122140330.69659.qmail@web54202.mail.yahoo.com>
References: <20051122140330.69659.qmail@web54202.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0511221718150.30466@gannet.stats>

On Tue, 22 Nov 2005, Pedro Cordeiro Estrela wrote:

> I tried to get old libraries (libreadline4) installed without success.

Why do you need that?  The configure test works correctly with the current 
readline 5.0, which also needs ncurses.

What is `R.2.2'?  There is no such thing, and your line numbers do not 
correspond to R 2.2.0.  But it is clear you are missing ncurses, on which 
readline depends.  My guess is that you have not installed the 
ncurses-devel rpm (on which the readline-devel one really should depend).

[...]

> What's inside config.log corresponding to the messages? These tests
> generally work by compiling and linking a small test program, and if
> that fails, you get the "no" part. However, sometimes the program
> fails for some other reason and the message becomes misleading.
>
> Here what's in the config.log file for the readline part.
> Thanks a lot!

[...]

> configure:21730: checking for main in -lncurses
> configure:21754: gcc -o conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -lncurses   >&5
> /usr/bin/ld: cannot find -lncurses
                ^^^^^^^^^^^^^^^^^^^^^

seems clear enough.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov 22 18:41:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Nov 2005 17:41:21 +0000 (GMT)
Subject: [R] loadings matrices in plsr vs pcr in pls pacakage
In-Reply-To: <438314C0.2040902@ula.ve>
References: <438314C0.2040902@ula.ve>
Message-ID: <Pine.LNX.4.61.0511221740240.30466@gannet.stats>

Please read ?princomp to understand more about loadings and their 
arbitrary signs.

On Tue, 22 Nov 2005, Roy Little wrote:

> Dear list,
> I have a question concerning the above mentioned methods in the pls
> package with respect to the loadings matrix produced by the call. In
> some work I am doing I have found that the values produced are nearly of
> the same magnitude but of opposite sign. When I use the example data
> (sensory) I find this result reproduced. I am prepared to work this
> through but I have a feeling that there could be a possible error in the
> code. (?!)
>
>
>
> > sens.pcr$loadings
>            Comp 1      Comp 2     Comp 3    Comp 4
> yellow  75.186621  -0.4780473   3.212149  1.750123
> green  -90.490256   8.5880530   1.634961  1.042239
> brown   -2.861241 -11.3600509 -15.920789 -1.105799
> glossy  13.347090  19.3103902  -3.121693  2.781282
> transp  20.126987  24.0653312  -6.656764 -1.842907
> syrup   -7.199972  -5.3436196  -5.073675  5.620454
> > sens.pls$loadings
>
> Loadings:
>        Comp 1  Comp 2  Comp 3  Comp 4
> yellow -74.448 -10.519   3.169  -1.056
> green   88.299  21.627  -0.521  -0.976
> brown    4.959 -14.253 -12.761  -0.371
> glossy -15.798  15.914  -7.574   3.504
> transp -23.049  18.673 -12.214  -2.068
> syrup    8.045  -5.313  -3.698   2.181
>
> Thank you for your help.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From elvis at xlsolutions-corp.com  Tue Nov 22 19:15:04 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 22 Nov 2005 11:15:04 -0700
Subject: [R] December Course In New York***R/Splus Fundamentals and
	Programming Techniques
Message-ID: <20051122111504.a108dc04937c07ba67766dad37185406.db733cd246.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" in New York: www.xlsolutions-corp.com/Rfund.htm

**** New York,   December 19-20, 2005

Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From friendly at yorku.ca  Tue Nov 22 19:57:55 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Tue, 22 Nov 2005 13:57:55 -0500
Subject: [R] attributes of a data.frame
Message-ID: <43836A33.4050405@yorku.ca>

It's hard for me to resist dipping my oar into this...

Variable labels are so generally useful, both in documenting a
dataset (what was 'pctx723' again?) and in producing readable
output and graphs that it is a shame they are not provided in
base R.  If they were (and were used in print and plot methods,
when available) it would avoid a lot of the necessity to specify
xlab= and ylab= in graphs, or, perhaps worse, ending up with
pctx723 as the label in your presentation.

-Michael

> On 11/21/2005 2:51 PM, Adrian DUSA wrote:
> 
>>> On Monday 21 November 2005 22:41, Duncan Murdoch wrote:
>>
>>>>> [...snip...]
>>>>> Not all dataframes have the variable.labels attribute.  I'm guessing
>>>>> you've installed some contributed package to add them, or are importing
>>>>> an SPSS datafile using read.spss.  So don't expect varlab() or
>>>>> variable.labels() function to be a standard R function.
>>
>>> 
>>> Aa-haa... of course you are right: I read them via read.spss. I understand. 
>>> Now, just to the sake of it, would it be wrong to make it standard? 
>>> Is there a special reason not to?
> 
> 
> I think it's just that the R core developers don't see the need for 
> them.  If something is worth documenting, then you should write an .Rd 
> file or a vignette about it, and that gives you more flexibility than a 
> one line label.
> 
> I think there are definitely developers out there who disagree with this 
> point of view, and I'm pretty sure I've seen a contributed package that 
> offered support for this, but I can't remember which one right now.  So 
> that's another reason why it's not in the base:  it doesn't need to be, 
> you can just go find and install that contributed package!
> 

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From groemping at tfh-berlin.de  Tue Nov 22 20:08:12 2005
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Tue, 22 Nov 2005 20:08:12 +0100
Subject: [R] Method for $
In-Reply-To: <Pine.LNX.4.61.0511181959060.24187@gannet.stats>
References: <20051118140018.M46734@tfh-berlin.de>
	<437E0534.6070101@statistik.uni-dortmund.de>
	<20051118172748.M31016@tfh-berlin.de>
	<437E1CEB.70900@wald.ucdavis.edu>
	<20051118185933.M87397@tfh-berlin.de>
	<Pine.LNX.4.61.0511181959060.24187@gannet.stats>
Message-ID: <20051122190533.M47306@tfh-berlin.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/3874d7e1/attachment.pl

From gavin.simpson at ucl.ac.uk  Tue Nov 22 20:17:08 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 22 Nov 2005 19:17:08 +0000
Subject: [R] loadings matrices in plsr vs pcr in pls pacakage
In-Reply-To: <438314C0.2040902@ula.ve>
References: <438314C0.2040902@ula.ve>
Message-ID: <1132687028.23462.22.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2005-11-22 at 08:53 -0400, Roy Little wrote:
> Dear list,
> I have a question concerning the above mentioned methods in the pls 
> package with respect to the loadings matrix produced by the call. In 
> some work I am doing I have found that the values produced are nearly of 
> the same magnitude but of opposite sign. When I use the example data 
> (sensory) I find this result reproduced. I am prepared to work this 
> through but I have a feeling that there could be a possible error in the 
> code. (?!)

But only for the first axis! I am not an expert, but as I understand it,
PCR and PLS are related but different methods. PCR components maximise
variance within the predictor variables, whilst PLS components maximise
the covariance with the response variable. Both methods appear to
extract first components that are similar - perhaps indicative of strong
structure within the data set. That the subsequent components are
different reflects the different criteria maximised by the two methods.

Prof. Ripley has indicated where to look for discussions about the signs
of eigenvectors.

Maybe I misunderstood your query? Were you expecting the results to be
the same on all components? Why do you suspect an error in the code?

HTH,

Gav

> 
> 
>  > sens.pcr$loadings
>             Comp 1      Comp 2     Comp 3    Comp 4
> yellow  75.186621  -0.4780473   3.212149  1.750123
> green  -90.490256   8.5880530   1.634961  1.042239
> brown   -2.861241 -11.3600509 -15.920789 -1.105799
> glossy  13.347090  19.3103902  -3.121693  2.781282
> transp  20.126987  24.0653312  -6.656764 -1.842907
> syrup   -7.199972  -5.3436196  -5.073675  5.620454
>  > sens.pls$loadings
> 
> Loadings:
>         Comp 1  Comp 2  Comp 3  Comp 4
> yellow -74.448 -10.519   3.169  -1.056
> green   88.299  21.627  -0.521  -0.976
> brown    4.959 -14.253 -12.761  -0.371
> glossy -15.798  15.914  -7.574   3.504
> transp -23.049  18.673 -12.214  -2.068
> syrup    8.045  -5.313  -3.698   2.181
> 
> Thank you for your help.
> 
> Roy Little
> Dept. Chem.
> Facultad de Ciencias
> Universidad de los Andes
> MÃ©rida, Venezuela
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From deepayan.sarkar at gmail.com  Tue Nov 22 20:47:20 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 22 Nov 2005 13:47:20 -0600
Subject: [R] change axis format for different panels in xyplot in lattice
In-Reply-To: <430f06c.5f489b43.81ab200@po1.mail.umd.edu>
References: <430f06c.5f489b43.81ab200@po1.mail.umd.edu>
Message-ID: <eb555e660511221147v100afb92o2d4fd9d84b2c76fb@mail.gmail.com>

On 11/22/05, levyr at umd.edu <levyr at umd.edu> wrote:
> Dear R users,
>
> My apologies for a simple question for which I suspect there
> is a simple answer that I have yet to find.  I'd like to plot
> panels in lattice with different graphical parameters for the
> axes.  For example, the code
>
> x<-rnorm(100)
> y<-rnorm(100)
> z<-c(rep(1,50), rep(2,50))
> library(lattice)
> xyplot(y~x|z)
>
> plots two panels with the default black axes.  Running the
> following code
>
> trellis.par.set(list(axis.line = list(col = "transparent")))
> xyplot(y~x|z)
>
> plots the same data without the axes.  Is it possible (in one
> plot) to plot the first panel with black axes and the second
> panel with tranparent axes?

Not systematically, but consider this approach:

library(grid)
trellis.par.set(list(axis.line = list(col = "transparent")))
xyplot(y~x|z,
       panel = function(..., panel.number) {
           if (panel.number == 1) grid.rect(gp = gpar(col = 'black'))
           panel.xyplot(...)
       } )

This is probably not exactly what you want, since the tick marks are
transparent in both panels, but you get the idea. You can gain finer
control of the axis annotation using panel.axis inside your panel
function (but you will need to disable clipping, controlled by
trellis.par.get("clip")). Alternatively, check out ?trellis.focus.

Deepayan



From apitts at burstmedia.com  Tue Nov 22 21:03:50 2005
From: apitts at burstmedia.com (Anthony Pitts)
Date: Tue, 22 Nov 2005 15:03:50 -0500
Subject: [R] graphing help
Message-ID: <3B4E3BFCA04B46499CEF35EA9996A751019F6EA3@exchsvr1.BurstHQ.burstmedia.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/bfbc3628/attachment.pl

From deepayan.sarkar at gmail.com  Tue Nov 22 21:07:00 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 22 Nov 2005 14:07:00 -0600
Subject: [R] how to plot a clustered matrix in graphs
In-Reply-To: <20051122170438.77958.qmail@web36404.mail.mud.yahoo.com>
References: <eb555e660511211605p3b9442d0y84462c772abf699f@mail.gmail.com>
	<20051122170438.77958.qmail@web36404.mail.mud.yahoo.com>
Message-ID: <eb555e660511221207h4c6ccb62k3df53e8196b69890@mail.gmail.com>

On 11/22/05, peter eric <eric_wzl at yahoo.com> wrote:
> dear deepayan & all...
>   let me explain you my full problem clearly..so that you could clearly
> suggest me in right direction....
>   my matrix (a) looks like as below...I??ve done clusutering in  this matrix
>   ----------------------------------------------------------------
>    a<-matrix(c(seq(3,45,3),seq(10,6,-1)),4,5,byrow=F)
>  col<-c("ra","rb","rc","rd","re")
>  rows<-c("ca","cb","cc","cd")
>  dimnames(a)<-list(rows,col)
> a
>      ra rb rc rd re
> ca  3 15 27 39  9
> cb  6 18 30 42  8
> cc  9 21 33 45  7
> cd 12 24 36 10  6

Your notation below (what you call rows and columns) is inconsistent
with what you have above, so I'm going to pretend that

a <- t(a)

>   -----------------------------------------------------------
>   clustering based on row and columns..suppose after clustering my row & col
> clustes looks like this..
>
>   row cluster 1: ra,rb
>                    2: rc,rd
>                    3:re
>   column cluster 1: ca,cb
>                          2: cc,cd

Let's code this as

rowc <- list(c("ra", "rb"), c("rc", "rd"), "re")
colc <- list(c("ca", "cb"), c("cc", "cd"))


>   So my clustered matrix (should) looks like
>
>                                                         ROWS
>                                         1                   2
>   3
>                                  I  ra    rb       I      rc   rd       I
>    re
>
> ----------------I------------------I----------------------I------------------
>                         ca     I   3    15      I      27  39       I
> 9
>                 1             I                  I                      I
>                         cb     I   6    18      I      30  42       I
> 8
>                                  I                  I
> I
>
> COLUMNS---------------I-----------------------------------------I-----------------
>                       cc     I   9     21     I      33  45       I      7
>               2               I                  I                      I
>                         cd     I 12    24     I      36  10       I      6
>
> ----------------I-----------------I-----------------------I--------------------
>   The above is the output matrix..It has to be plotted in graphs showing the
> partitions clearly as it is shown in the above matrix...
>   .
>   what kind of approach will be useful in gettign this..?particularly in
> colored graphs...

Here's what I would do:

levelplot(a[unlist(rowc), unlist(colc)],
          colc = colc,
          rowc  = rowc,
          aspect = "iso",
          panel = function(..., rowc, colc) {
              panel.levelplot(...)
              h = 0.5 + cumsum(sapply(colc, length))
              v = 0.5 + cumsum(sapply(rowc, length))
              panel.abline(h = h[-length(h)], v = v[-length(v)])
          })

-Deepayan



From tom at maladmin.com  Tue Nov 22 16:15:36 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 22 Nov 2005 10:15:36 -0500
Subject: [R] graphing help
In-Reply-To: <3B4E3BFCA04B46499CEF35EA9996A751019F6EA3@exchsvr1.BurstHQ.burstmedia.local>
References: <3B4E3BFCA04B46499CEF35EA9996A751019F6EA3@exchsvr1.BurstHQ.burstmedia.local>
Message-ID: <1132672536.4819.154.camel@localhost.localdomain>

a<-c(1:10)
plot(a,ylim=c(10,1))

On Tue, 2005-22-11 at 15:03 -0500, Anthony Pitts wrote:
> Hello all,
> 
> Does anyone know how to make R graph the x-axis in descending order? I am all
> set in creating a vertical bar chart using either the plot or the xychart (in
> lattice), but I need the x-axis in descending order. I have tried resorting
> the data but apparently R automatically graphs the x-axis in ascending order.
> I have multiplied the data used for the x-axis by -1 and this works but now
> the tick-mark labels are negative. So does anyone know how to 1) make R graph
> the x-axis in descending order or 2) change the tick labels so I can get rid
> of the negative signs?
> 
> Thanks in advance for your help
> 
> Tony
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From deepayan.sarkar at gmail.com  Tue Nov 22 21:19:24 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 22 Nov 2005 14:19:24 -0600
Subject: [R] graphing help
In-Reply-To: <3B4E3BFCA04B46499CEF35EA9996A751019F6EA3@exchsvr1.BurstHQ.burstmedia.local>
References: <3B4E3BFCA04B46499CEF35EA9996A751019F6EA3@exchsvr1.BurstHQ.burstmedia.local>
Message-ID: <eb555e660511221219o2f8ba3e4wee0223eb33bd8946@mail.gmail.com>

On 11/22/05, Anthony Pitts <apitts at burstmedia.com> wrote:
> Hello all,
>
> Does anyone know how to make R graph the x-axis in descending order? I am
> all set in creating a vertical bar chart using either the plot or the xychart
> (in lattice),

There's no such function, I presume you mean barchart.

> but I need the x-axis in descending order.

I'm a bit confused, because a vertical barchart would normally have a
categorical variable on the x-axis, yet you talk of numeric labels for
them. Please provide a reproducible example. For numeric axes
specifying a reversed xlim (e.g. xlim = c(10, 0) ) should work.

-Deepayan

> I have tried resorting
> the data but apparently R automatically graphs the x-axis in ascending
> order.
> I have multiplied the data used for the x-axis by -1 and this works but now
> the tick-mark labels are negative. So does anyone know how to 1) make R
> graph
> the x-axis in descending order or 2) change the tick labels so I can get
> rid
> of the negative signs?
>
> Thanks in advance for your help
>
> Tony



From buddhahead at ranpura.com  Tue Nov 22 21:23:14 2005
From: buddhahead at ranpura.com (Ashish Ranpura)
Date: Tue, 22 Nov 2005 20:23:14 +0000
Subject: [R] SPSS-like factor analysis procedure
Message-ID: <3CB93289-655C-4C0D-BB86-B5D224F94796@ranpura.com>


I've read through many postings about principle component analysis in  
the R-help archives, but haven't been able to piece together the  
information I need. I'd like to recreate an SPSS-like experience of  
factor analysis using R. Here's what SPSS produces:

1. Scatterplots of all possible variable pairs, with regression lines.
xyplot(my.dataframe) is perfect but for the lack of regression lines.

2. Frequency histograms overlaid with normal curves for each variable.
I can do this one at a time; I'd love R to do it in a big layout for  
all the variables in the data frame.

3. Descriptive statistics of each variable.
Jim Lemon's excellent dstats() function does this. Solved.

4. A large correlation matrix for the data frame.
The built-in function cov() does this. Solved.

5. KMO (Kaiser-Meyer-Olkin Measure of Sampling Adequacy) and Bartlett  
test of sphericity on the data frame as a whole.
I can't find ways to recreate these tests -- bartlett.test() doesn't  
produce the type of response that makes sense.

6. Anti-image matricies, including MSA (sampling adequacy) scores for  
each variable
I can't find a way to generate this, maybe because I'm unsure how its  
calculated. The MSA scores would tell me how strongly each variable  
measures the data set as a whole, which I could use to guide  
subsequent factor analysis.

7. Total Variance Explained -- a table listing eigenvalues for each  
eigenvector, along with the % variance for each eigenvector.
This is the best part of the SPSS output. I feel like I'm close to  
finding the right function in R , but I don't know how to look at the  
eigenvalues of each component in R. princomp() seems a step in the  
right direction.

8. Scree plot.
No problem, princomp() and screeplot() seem to produce about the  
right result.

9. Component matrix (lists the variable loading on each factor)
factanal() seems to do this, but again the results don't jive with  
SPSS and I'm unsure why.

10. Factor rotation
No problem, factanal(rotation="varimax") does this.


If anyone can suggest how to fill in the missing pieces (particularly  
steps 6 and 7), please do let me know. Thanks!

--Ashish.


-----
Ashish Ranpura
Institute of Cognitive Neuroscience
University College London



From kaniovsk at wsr.ac.at  Tue Nov 22 21:55:28 2005
From: kaniovsk at wsr.ac.at (Serguei Kaniovski)
Date: Tue, 22 Nov 2005 21:55:28 +0100 (CET)
Subject: [R] the matrix of rows with specific row sums
Message-ID: <1269167.1132692928476.SLOX.WebMail.wwwrun@billa.wsr.ac.at>

I am just starting with R and have the following problem: given a matrix
of ones and zeroes, say

mdim=4
m<-matrix(round(runif(mdim^mdim)),mdim,mdim)

how to construct the matrix of those rows of m, whose elements sum to 2.
Contrary to the random matrix above, the actual matrix always has at
least one such row.

Serguei Kaniovski



From quantpm at yahoo.com  Tue Nov 22 21:56:37 2005
From: quantpm at yahoo.com (t c)
Date: Tue, 22 Nov 2005 12:56:37 -0800 (PST)
Subject: [R] running out of memory while running a VERY LARGE regression
Message-ID: <20051122205637.3177.qmail@web35007.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/e9c43e03/attachment.pl

From rxg218 at psu.edu  Tue Nov 22 22:00:19 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Tue, 22 Nov 2005 16:00:19 -0500
Subject: [R] the matrix of rows with specific row sums
In-Reply-To: <1269167.1132692928476.SLOX.WebMail.wwwrun@billa.wsr.ac.at>
References: <1269167.1132692928476.SLOX.WebMail.wwwrun@billa.wsr.ac.at>
Message-ID: <1132693220.15904.56.camel@blue.chem.psu.edu>

On Tue, 2005-11-22 at 21:55 +0100, Serguei Kaniovski wrote:
> mdim=4
> m<-matrix(round(runif(mdim^mdim)),mdim,mdim)

One approach:

> m
     [,1] [,2] [,3] [,4]
[1,]    0    0    1    0
[2,]    0    1    1    0
[3,]    0    0    1    1
[4,]    0    0    0    0
> idx <- apply(m, 1, sum)
> m[idx,]
     [,1] [,2] [,3] [,4]
[1,]    0    0    1    0
[2,]    0    1    1    0
[3,]    0    1    1    0
>

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Cogito cogito ergo cogito sum 
(I think that I think, therefore I think that I am.)
-- Ambrose Bierce



From ross at biostat.ucsf.edu  Tue Nov 22 22:13:52 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 22 Nov 2005 13:13:52 -0800
Subject: [R] Customizing the package build process
Message-ID: <1132694032.2951.139.camel@iron.psg.net>

I've made a package for which R CMD build isn't producing very
satisfactory results.  I'll get to the details in a moment.

I wonder if it would make sense to have my own makefiles (which already
exist and are doing quite a lot) produce the .tar.gz file ordinarily
produced by R CMD build.  As far as I can tell, R CMD build basically
tars up of the project directory after running some checks.  I could run
R CMD check separately.

There are two main problems with the results of R CMD build.  First, it
has lots of files that I don't want included (the input files used to
generate configure, miscellaneous garbage, other stuff not suitable for
distribution).  Second, I have data files as both "data.gz" and "data".
R puts "data" into the .tar.gz file and sensibly ignores the .gz file.
Unfortunately, my makefiles assume the existence of the "data.gz" files,
and so may have trouble after the .tar.gz is unpacked and there are no
"data.gz" files.

My bias would ordinarily be to piggy back on the R build system as much
as possible.  In principle, this could get me extra features (binary
builds, MS Windows builds) and it would track the things R build does
beyond tarring files.  But in this case using the R build system seems
quite ugly.  I could in principle use .Rbuildignore, probably generated
dynamically, to exclude files.  That doesn't solve the 2nd problem
(data.gz becomes data).

So does the alternative of doing the tar'ing myself make sense?

Is there another option that could hook into the R CMD build process
more deeply than the use of .Rbuildignore?

I suppose another option would be to do a clean checkout of the sources
for my package, run a special makefile target that would create the
necessary files and delete all unwanted files, and then do a regular R
CMD build.  This might still have trouble with "data.gz".

P.S. Previous list postings advised that R CMD install was a better way
to produce binaries than R CMD build --binary.  The former command
doesn't seem to have any options for making binaries; has that facility
been removed?

Second question: my reading is that .Rbuildignore is only read in the
package root directory, and will have no effect below that.  Is that
correct?  Per directory .Rbuildignore's would be convenient..

-- 
Ross Boylan                                      wk:  (415) 514-8146
185 Berry St #5700                               ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 514-8150
University of California, San Francisco
San Francisco, CA 94107-1739                     hm:  (415) 550-1062



From rxg218 at psu.edu  Tue Nov 22 22:21:27 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Tue, 22 Nov 2005 16:21:27 -0500
Subject: [R] the matrix of rows with specific row sums
In-Reply-To: <1132693220.15904.56.camel@blue.chem.psu.edu>
References: <1269167.1132692928476.SLOX.WebMail.wwwrun@billa.wsr.ac.at>
	<1132693220.15904.56.camel@blue.chem.psu.edu>
Message-ID: <1132694487.15904.58.camel@blue.chem.psu.edu>

On Tue, 2005-11-22 at 16:00 -0500, Rajarshi Guha wrote:
> On Tue, 2005-11-22 at 21:55 +0100, Serguei Kaniovski wrote:
> > mdim=4
> > m<-matrix(round(runif(mdim^mdim)),mdim,mdim)
> 
> One approach:
> 
> > m
>      [,1] [,2] [,3] [,4]
> [1,]    0    0    1    0
> [2,]    0    1    1    0
> [3,]    0    0    1    1
> [4,]    0    0    0    0

Actually there was a mistake. It should read as

> > idx <- which(apply(m, 1, sum) == 2)

 
-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
He who is in love with himself has at least this advantage -- he won't
encounter many rivals.
-- Georg Lichtenberg, "Aphorisms"



From sfalcon at fhcrc.org  Tue Nov 22 22:37:34 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 22 Nov 2005 13:37:34 -0800
Subject: [R] the matrix of rows with specific row sums
In-Reply-To: <1269167.1132692928476.SLOX.WebMail.wwwrun@billa.wsr.ac.at>
	(Serguei Kaniovski's message of "Tue,
	22 Nov 2005 21:55:28 +0100 (CET)")
References: <1269167.1132692928476.SLOX.WebMail.wwwrun@billa.wsr.ac.at>
Message-ID: <m2zmnwuzxd.fsf@fhcrc.org>

On 22 Nov 2005, kaniovsk at wsr.ac.at wrote:

> I am just starting with R and have the following problem: given a
> matrix of ones and zeroes, say
>
> mdim=4
> m<-matrix(round(runif(mdim^mdim)),mdim,mdim)
>
> how to construct the matrix of those rows of m, whose elements sum
> to 2.  Contrary to the random matrix above, the actual matrix always
> has at least one such row.

Untested, but I think you want something like:

idx <- apply(m, 1, sum) == 2
ans <- m[idx, ]


+ seth



From gunter.berton at gene.com  Tue Nov 22 23:20:34 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 22 Nov 2005 14:20:34 -0800
Subject: [R] the matrix of rows with specific row sums
In-Reply-To: <m2zmnwuzxd.fsf@fhcrc.org>
Message-ID: <200511222220.jAMMKXbL028136@volta.gene.com>

Standard caution: == should not be used for comparison of floating point
numbers. See ?all.equal  and ?identical. See also the Green Book (Chambers:
PROGRAMMING WITH DATA). Sometimes what you think are integers may be floats,
too, as numerous postings to this list have shown.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Seth Falcon
> Sent: Tuesday, November 22, 2005 1:38 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] the matrix of rows with specific row sums
> 
> On 22 Nov 2005, kaniovsk at wsr.ac.at wrote:
> 
> > I am just starting with R and have the following problem: given a
> > matrix of ones and zeroes, say
> >
> > mdim=4
> > m<-matrix(round(runif(mdim^mdim)),mdim,mdim)
> >
> > how to construct the matrix of those rows of m, whose elements sum
> > to 2.  Contrary to the random matrix above, the actual matrix always
> > has at least one such row.
> 
> Untested, but I think you want something like:
> 
> idx <- apply(m, 1, sum) == 2
> ans <- m[idx, ]
> 
> 
> + seth
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Hong.Ooi at iag.com.au  Wed Nov 23 00:14:51 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 23 Nov 2005 10:14:51 +1100
Subject: [R] the matrix of rows with specific row sums
Message-ID: <200511222314.jAMNEtdt028122@hypatia.math.ethz.ch>


_______________________________________________________________________________________


There is also rowSums (and colSums) which does the same thing without
needing apply:

m[rowSums(m) == 2, ]

Subject to Bert Gunter's caveat about comparing floating point numbers
for equality, of course.

-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
(02) 9292 1566

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Seth Falcon
Sent: Wednesday, 23 November 2005 8:38 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] the matrix of rows with specific row sums

On 22 Nov 2005, kaniovsk at wsr.ac.at wrote:

> I am just starting with R and have the following problem: given a
> matrix of ones and zeroes, say
>
> mdim=4
> m<-matrix(round(runif(mdim^mdim)),mdim,mdim)
>
> how to construct the matrix of those rows of m, whose elements sum
> to 2.  Contrary to the random matrix above, the actual matrix always
> has at least one such row.

Untested, but I think you want something like:

idx <- apply(m, 1, sum) == 2
ans <- m[idx, ]


+ seth

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From kartik at pdx.edu  Wed Nov 23 01:28:05 2005
From: kartik at pdx.edu (kartik@pdx.edu)
Date: Tue, 22 Nov 2005 16:28:05 -0800
Subject: [R] negative binomial overdispersion question
Message-ID: <1132705685.4383b7954f89e@webmail.pdx.edu>

Hello,

I'm a grad student in the Intelligent Transportation Systems lab at Portland
State Univ. in Portland, OR, USA.  I'm trying to learn the basics of R to run a
negative binomial in the near future, and so I ran a test regression on roadway
crash data obtained from "Statistical and Econometric Methods for
Transportation Data Analysis" by Washington et al (p. 250).  I ran the test
(glm.nb from library MASS) and got the same output as in the text for all the
parameters except the overdispersion parameter; the text lists 0.516, but R
gave me an overdispersion parameter output of 1.9365.  (I've attached the
raw dataset, in Excel .csv format, if anyone wants to try the test.  If the
attachment does not go through, please email me and I'll forward it on to you.)

Has anyone had similar problems with the overdispersion parameter output from R?
 Any thoughts on this would be much appreciated.

Thanks!
Kartik


From Hong.Ooi at iag.com.au  Wed Nov 23 01:42:59 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 23 Nov 2005 11:42:59 +1100
Subject: [R] negative binomial overdispersion question
Message-ID: <200511230043.jAN0hE2t023537@hypatia.math.ethz.ch>


_______________________________________________________________________________________


I don't know the text in question, but it looks like a difference in
parametrisation -- the R output is just the inverse of that from the
book.

-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
(02) 9292 1566

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of kartik at pdx.edu
Sent: Wednesday, 23 November 2005 11:28 AM
To: r-help at stat.math.ethz.ch
Subject: [R] negative binomial overdispersion question

Hello,

I'm a grad student in the Intelligent Transportation Systems lab at
Portland
State Univ. in Portland, OR, USA.  I'm trying to learn the basics of R
to run a
negative binomial in the near future, and so I ran a test regression on
roadway
crash data obtained from "Statistical and Econometric Methods for
Transportation Data Analysis" by Washington et al (p. 250).  I ran the
test
(glm.nb from library MASS) and got the same output as in the text for
all the
parameters except the overdispersion parameter; the text lists 0.516,
but R
gave me an overdispersion parameter output of 1.9365.  (I've attached
the
raw dataset, in Excel .csv format, if anyone wants to try the test.  If
the
attachment does not go through, please email me and I'll forward it on
to you.)

Has anyone had similar problems with the overdispersion parameter output
from R?
 Any thoughts on this would be much appreciated.

Thanks!
Kartik




_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From jfox at mcmaster.ca  Wed Nov 23 02:01:21 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 22 Nov 2005 20:01:21 -0500
Subject: [R] _simple_ introduction to bayesian methods?
In-Reply-To: <20051122171329.GR6437@always.joy.eth.net>
Message-ID: <20051123010119.BSWQ26102.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Joshua,

You might take a look at Lancaster, An Introduction to Modern Bayesian
Econometrics.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Joshua 
> N Pritikin
> Sent: Tuesday, November 22, 2005 12:13 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] _simple_ introduction to bayesian methods?
> 
> I made it through the first chapter ("Background") of 
> Bayesian Data Analysis by Gelman, et al.  Conceptually, the 
> Bayesian approach appeals to me and my curiosity is piqued.  
> However, the next chapter was much too terse. The math is 
> daunting.  Where can I find a gentle introduction?  Or which 
> math book(s) do I need to read first?
> 
> On page 27, there is mention of introductory books including 
> Bayesian Methods by Jeff Gill (2002).  Just for fun, I took a 
> look at this book to see whether I could get through it.  
> Chapter 1 was inviting, but again, the math got too 
> sophisticated starting from chapter 2.
> 
> --
> Make April 15 just another day, visit http://fairtax.org
>



From jfox at mcmaster.ca  Wed Nov 23 02:14:28 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 22 Nov 2005 20:14:28 -0500
Subject: [R] graphing help
In-Reply-To: <3B4E3BFCA04B46499CEF35EA9996A751019F6EA3@exchsvr1.BurstHQ.burstmedia.local>
Message-ID: <20051123011425.JVGW2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Anthony,

You can use xlim in plot() to reverse the x-axis. For example,

x <- 1:10
y <- x
plot(x, y, xlim=rev(range(x)))

(I'm pretty sure, by the way, that this question has been asked before.)

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anthony Pitts
> Sent: Tuesday, November 22, 2005 3:04 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] graphing help
> 
> Hello all,
> 
> Does anyone know how to make R graph the x-axis in descending 
> order? I am all set in creating a vertical bar chart using 
> either the plot or the xychart (in lattice), but I need the 
> x-axis in descending order. I have tried resorting the data 
> but apparently R automatically graphs the x-axis in ascending order.
> I have multiplied the data used for the x-axis by -1 and this 
> works but now the tick-mark labels are negative. So does 
> anyone know how to 1) make R graph the x-axis in descending 
> order or 2) change the tick labels so I can get rid of the 
> negative signs?
> 
> Thanks in advance for your help
> 
> Tony
>



From r.babigumira at gmail.com  Wed Nov 23 02:38:15 2005
From: r.babigumira at gmail.com (Ronnie Babigumira)
Date: Wed, 23 Nov 2005 02:38:15 +0100
Subject: [R] getting started, reading listing and saving data
Message-ID: <7d2f562c0511221738l725b2a2exf388a38af0ad7cab@mail.gmail.com>

Dear List
I am new to R and to the list and will try best as I can be clear and
concise. My apologies if anything I write contravenes the posting code
on this list. I would also like to say I have run through most of the
material on the R website before writing this email however, I am
stuck.

Here is what I want to do and what I have done

1. Read a comma seperated text file into R
I have used read.csv and it seems to have worked

2. List the a few observations to make sure the right stuff came in
I have failed to find a command that allows me to list a few
observations and would appreciate some help on this (I have used edit
which pops up an spreadsheet however, I would prefer a command that
allows me to list a few observations for inspection)

3. Save this data as an r dataset
I cant seem to figure this out (I tried  save(mytextfile, file =
"myrdata") but when I try to load what I saved, I get an error message
"Error: bad restore file magic number (file may be corrupted) -- no data loaded"

4. Load this r dataset and proceed to work on it

I would appreciate some help.



From Murraypu at aimnsw.com.au  Wed Nov 23 03:03:17 2005
From: Murraypu at aimnsw.com.au (Murray Pung)
Date: Wed, 23 Nov 2005 13:03:17 +1100
Subject: [R] getting started, reading listing and saving data
Message-ID: <3028F4C4647C9043B870276E28C69FD6013438E0@syd05.aimnsw.com.au>

Not sure about reading the first few lines, but summary(x) might be what you're after.

Try using the menu to save the workspace. File > Save Workspace. You can then load the workspace at the start of your next session and continue working.

Murray

-----Original Message-----
From: Ronnie Babigumira [mailto:r.babigumira at gmail.com]
Sent: Wednesday, 23 November 2005 12:38 PM
To: r-help at stat.math.ethz.ch
Subject: [R] getting started, reading listing and saving data


Dear List
I am new to R and to the list and will try best as I can be clear and
concise. My apologies if anything I write contravenes the posting code
on this list. I would also like to say I have run through most of the
material on the R website before writing this email however, I am
stuck.

Here is what I want to do and what I have done

1. Read a comma seperated text file into R
I have used read.csv and it seems to have worked

2. List the a few observations to make sure the right stuff came in
I have failed to find a command that allows me to list a few
observations and would appreciate some help on this (I have used edit
which pops up an spreadsheet however, I would prefer a command that
allows me to list a few observations for inspection)

3. Save this data as an r dataset
I cant seem to figure this out (I tried  save(mytextfile, file =
"myrdata") but when I try to load what I saved, I get an error message
"Error: bad restore file magic number (file may be corrupted) -- no data loaded"

4. Load this r dataset and proceed to work on it

I would appreciate some help.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From samuele_carcagno at yahoo.co.uk  Wed Nov 23 04:33:39 2005
From: samuele_carcagno at yahoo.co.uk (Samuele Carcagno)
Date: Wed, 23 Nov 2005 04:33:39 +0100
Subject: [R] Greenhouse-Geisser epsilon for interaction term
Message-ID: <MFBBKIIHBOBHHOLKMFMDCEKMCAAA.samuele_carcagno@yahoo.co.uk>

Hi,

I'm running a repeated measure ANOVA to test the effects of the
within-subjects
factors "congr", "isi" and their interaction, on the variable "latencies"

aov(latencies ~ congr*isi + Error(subj/(congr*isi)),data=dats)

"congr" has 4 levels, while "isi" has "3" levels.

I would like to check for possible violations of the sphericity assumption,
and
following the example given by Baron and Li(2004), I was able to compute
Greenhouse-Geisser epsilon for the effect of "congr" and for the effect of
"isi". I calculated the variance/covariance matrix for each of these terms
by
averaging the "latencies" across the levels of the other factor,  but I
can't
figure out how to calculate the variance/covariance matrix for the
interaction
term. I've looked up several books and tutorials over the internet to find a
solution, but either they skip entirely over the actual calculations and
present
instead the output given by SPSS, or they are at a too advanced level for
me.

Any help would be really appreciated,


Samuele


References:

- Baron,J. & Li,Y.(2004).Notes on the use of R for psychology experiments
and
questionnaires. Available:
http://www.psych.upenn.edu/~baron/rpsych/rpsych.html



From lparsons at asu.edu  Wed Nov 23 05:29:55 2005
From: lparsons at asu.edu (Lance Parsons)
Date: Tue, 22 Nov 2005 21:29:55 -0700
Subject: [R] Survival Analysis Questions
Message-ID: <dm0r80$2bv$1@sea.gmane.org>

I am currently using the coxph function from R on a microarray dataset. 
  The curve fit is performed using each gene's expression measurements 
separately.  I then get a p-value for each gene, from the Coxph.summary 
method.  Can someone please explain how this is computed and the 
significance of the p-value?

However, the bigger question I have is how to perform permutation tests 
to get a more robust estimate of significance.  After permuting the 
survival times and running Cox on each gene, I would like to save the 
most extreme values (representing the gene most correlated with 
survival).  After ~1000 runs, I then run Cox on each gene with the 
correct survival numbers.  My question is, which statistic(s) should I 
look at?  I was thinking that the coefficient might be appropriate, but 
I'm new to survival analysis and am not certain.  Can anyone offer any 
advice?

Finally, currently, I permute the survival times and maintain the proper 
association with the status column.  Should I permute the status separately?

Thanks for your time.



From tomas.r at gmail.com  Wed Nov 23 05:43:07 2005
From: tomas.r at gmail.com (Tomas)
Date: Tue, 22 Nov 2005 20:43:07 -0800
Subject: [R] 1st derivative {mgcv} gam smooth
Message-ID: <c08359f90511222043we8f695bgec6eeff94aa340e6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/37765ee3/attachment.pl

From jporzak at gmail.com  Wed Nov 23 07:07:02 2005
From: jporzak at gmail.com (Jim Porzak)
Date: Tue, 22 Nov 2005 22:07:02 -0800
Subject: [R] getting started, reading listing and saving data
In-Reply-To: <7d2f562c0511221738l725b2a2exf388a38af0ad7cab@mail.gmail.com>
References: <7d2f562c0511221738l725b2a2exf388a38af0ad7cab@mail.gmail.com>
Message-ID: <2a9c000c0511222207m26245dddm2f79c172eec8edd4@mail.gmail.com>

Ronnie, try

head?
tail?

and, I also fine useful after loading
str?


On 11/22/05, Ronnie Babigumira <r.babigumira at gmail.com> wrote:
> Dear List
> I am new to R and to the list and will try best as I can be clear and
> concise. My apologies if anything I write contravenes the posting code
> on this list. I would also like to say I have run through most of the
> material on the R website before writing this email however, I am
> stuck.
>
> Here is what I want to do and what I have done
>
> 1. Read a comma seperated text file into R
> I have used read.csv and it seems to have worked
>
> 2. List the a few observations to make sure the right stuff came in
> I have failed to find a command that allows me to list a few
> observations and would appreciate some help on this (I have used edit
> which pops up an spreadsheet however, I would prefer a command that
> allows me to list a few observations for inspection)
>
> 3. Save this data as an r dataset
> I cant seem to figure this out (I tried  save(mytextfile, file =
> "myrdata") but when I try to load what I saved, I get an error message
> "Error: bad restore file magic number (file may be corrupted) -- no data loaded"
>
> 4. Load this r dataset and proceed to work on it
>
> I would appreciate some help.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
HTH,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA



From sourceforge at metrak.com  Wed Nov 23 07:13:10 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Wed, 23 Nov 2005 17:13:10 +1100
Subject: [R] Fw: Re: Is there anything like a write.fwf() or possibility
 to	print adata.frame without rownames?
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730F31AC5@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730F31AC5@pollux.bfro.uni-lj.si>
Message-ID: <43840876.4020500@metrak.com>

If you are desperate, save the file as tab delimited and then use vi or 
some stream editor to convert the tabs to spaces.  If you set the tab 
stop wide enough you should be able to guarantee uniform columns.

Gorjanc Gregor wrote:
>>>Petr Pikal wrote:
>>>
>>>>Hi
>>>>
>>>>did you tried something like
>>>>
>>>>write.table( tab, "file.txt", sep="\t", row.names=F)
>>>>
>>>>which writes to tab separated file?
>>>>
>>>
>>>Petr thanks, but I do not want a tab delimited file. I need spaces
>>>between columns.
>>
>>write.table( tab, "file.txt", sep="", row.names=F)
>>Can it do what you want?
> 
> 
> Ronggui thanks,
> 
> but this does not work also. For example I get something like
> this bellow
> 
> "26" "1" 42 "DA" "DA" "lipa" "Monika"
> "26" "1" 42 "DA" "DA" "lipa" "Monika"
> "27" "1" 41 "DA" "DA" "smreka" "Monika"
> "27" "1" 41 "DA" "DA" "smreka" "Monika"
> 
> and you can see, that there is a problem, when all "values"
> in a column do not have the same length. I need to get
> 
> "26" "1" 42 "DA" "DA" "lipa"   "Monika"
> "26" "1" 42 "DA" "DA" "lipa"   "Monika"
> "27" "1" 41 "DA" "DA" "smreka" "Monika"
> "27" "1" 41 "DA" "DA" "smreka" "Monika"
> 
> i.e. columns should be properly aligned.
> 
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> ----------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rlist.10.phftt at xoxy.net  Wed Nov 23 09:42:02 2005
From: rlist.10.phftt at xoxy.net (rlist.10.phftt@xoxy.net)
Date: Wed, 23 Nov 2005 03:42:02 -0500
Subject: [R] Dancing lissajous
Message-ID: <43842B5A.2000506@fatkat.com>

We can't post images here so I've put up a web page to show off a neat 
little animation in R:

http://www.geocities.com/robsteele/

Adios!



From ripley at stats.ox.ac.uk  Wed Nov 23 09:46:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Nov 2005 08:46:23 +0000 (GMT)
Subject: [R] negative binomial overdispersion question
In-Reply-To: <1132705685.4383b7954f89e@webmail.pdx.edu>
References: <1132705685.4383b7954f89e@webmail.pdx.edu>
Message-ID: <Pine.LNX.4.61.0511230835360.8302@gannet.stats>

On Tue, 22 Nov 2005 kartik at pdx.edu wrote:

> Hello,
>
> I'm a grad student in the Intelligent Transportation Systems lab at Portland
> State Univ. in Portland, OR, USA.  I'm trying to learn the basics of R to run a
> negative binomial in the near future, and so I ran a test regression on roadway
> crash data obtained from "Statistical and Econometric Methods for
> Transportation Data Analysis" by Washington et al (p. 250).  I ran the test
> (glm.nb from library MASS) and got the same output as in the text for all the
> parameters except the overdispersion parameter; the text lists 0.516, but R
> gave me an overdispersion parameter output of 1.9365.  (I've attached the
> raw dataset, in Excel .csv format, if anyone wants to try the test.  If the
> attachment does not go through, please email me and I'll forward it on to you.)
>
> Has anyone had similar problems with the overdispersion parameter output from R?

MASS documents exactly what it does on p.206, but in no parametrization 
does the negative binomial have an `overdispersion' parameter in the sense 
used for glms (e.g. in McCullagh and Nelder).  glm.nb certainly does not 
report an `overdispersion parameter output', so you are not reading it 
carefully enough.

Did you consult the book whose support software you are using?  That is 
where the detailed documentation is.  My guess is that your reference has 
a parameter 1/theta, as 1/1.9365 is very close to 0.516.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From guangxing at ict.ac.cn  Wed Nov 23 09:59:38 2005
From: guangxing at ict.ac.cn (=?gb2312?B?uePQxw==?=)
Date: Wed, 23 Nov 2005 16:59:38 +0800
Subject: [R] How to plot two different lines with different color with the
	same "plot" function?
Message-ID: <200511230857.jAN8vTaw032297@hypatia.math.ethz.ch>

Hi,R_Help!
I have done something like this:
>x<-seq(412,612,1)
>plot(x,dpois(x,512),col="blue",type="l")
>plot(x,dnorm(x,512,sd=sqrt(512)),col="red",type="l")
And now,I want to plot the two lines in the same picture or with the same "plot" action?
What should I do? 
And any advises? 	

Thank you in advance!
 				

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¹ãÐÇ
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-23



From Jordi.Molins at drkw.com  Wed Nov 23 10:00:05 2005
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Wed, 23 Nov 2005 10:00:05 +0100
Subject: [R] How to open a text file in my screen (not in the R session)?
Message-ID: <C5A76BA0CA4D734CA725124C4D6397AC03219432@ibfftce502.de.ad.drkw.net>


In the middle of my code, I have the following:

write.table(PredictionTest, file = "predictions.txt")

what I would like is that inmediately after this line is executed, the file
predictions.txt is opened in the middle of my screen (not in my R session,
where I continue running my code). Is there an easy way to do that?

Thank you



--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Nov 23 10:06:31 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Nov 2005 09:06:31 +0000 (GMT)
Subject: [R] running out of memory while running a VERY LARGE regression
In-Reply-To: <20051122205637.3177.qmail@web35007.mail.mud.yahoo.com>
References: <20051122205637.3177.qmail@web35007.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0511230856020.8302@gannet.stats>

On Tue, 22 Nov 2005, t c wrote:

> I am running a VERY LARGE regression (many factors, many rows of data) 
> using LM.
>
>  I think I have my memory set as high as possible. ( I ran 
> "memory.limit(size = 4000)")
>
>  Is there anything I can do?  ( FYI, I "think" I have removed all data I 
> am not using, and I "think" I have only the data needed for the 
> regression loaded.) Thanks.

Snce you mention memory.limit, I guess you are using Windows without 
telling us.  If so, have you set up the /3GB switch (see the rw-FAQ Q2.9) 
and modified the R executables?  (The modification is not necessary if you 
use the current R-patched available from CRAN.)

You will be able to save memory by using lm.fit rather than lm, perhaps 
running a session containing just the model matrix and the response.
(Unless of course you run out of memory forming the model matrix.)

The best answer is to use a 64-bit OS and a 64-bit build of R.

> 	[[alternative HTML version deleted]]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please do as it asks and tell us your OS and do not send HTML mail and 
report the exact problem with the error messages.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Wed Nov 23 10:26:16 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 23 Nov 2005 10:26:16 +0100
Subject: [R] paste argument of a function as a file name
In-Reply-To: <43735521.8040900@statistik.uni-dortmund.de>
References: <s3734afe.043@ffdata.setur.fo>
	<43735521.8040900@statistik.uni-dortmund.de>
Message-ID: <17284.13752.239303.276299@stat.math.ethz.ch>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Thu, 10 Nov 2005 15:11:45 +0100 writes:

    UweL> Luis Ridao Cruz wrote:
    >> R-help,
    >> 
    >> I have a function which is exporting the output to a file via
    >> write.table(df, file =  "file name.xls" )
    >> 
    >> What I want is to paste the file name (above) by taking the argument to
    >> the function as a file name 
    >> 
    >> something like this:
    >> 
    >> MY.function<- function(df)
    >> {
    >> ...
    >> ...
    >> write.table(df,"argument.xls")
    >> }
    >> MY.function(argument)

    UweL> Has been asked hundreds of times on this list. 

indeed! ...  and, it seems, has been answered 200 times with
hints that I would consider "suboptimal" ..

    UweL> check the archives as the posting guide asks to do ...

    UweL> Hint: paste()

well, we know that that works;
however, since at least R version 1.0.0, there's the function 
file.path()  which is a simple wrapper for paste()  allowing a
slightly nicer syntax more readable and portable *) code.
So, we do recommend using   file.path()  as in

> list.files( file.path("C:", "WINDOWS", "system") )
or
> list.files( file.path("usr", "local", "lib"))

---

*) Though since nowadays the file separator "/" works on
   all current versions of R (I think),
   the trick of automatically using the correct file name separator
   for the given platform (namely  .Platform$file.sep )
   seems less important now.

Martin Maechler, ETH Zurich



From bhs2 at mevik.net  Wed Nov 23 10:32:43 2005
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Wed, 23 Nov 2005 10:32:43 +0100
Subject: [R] modifying code in contributed libraries - changes from
 versions 1.* to 2.*
In-Reply-To: <m2lkzgy5gq.fsf@ziti.local> (Seth Falcon's message of "Tue, 22
	Nov 2005 09:09:41 -0800")
References: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>
	<43834AE0.2050304@statistik.uni-dortmund.de>
	<m2lkzgy5gq.fsf@ziti.local>
Message-ID: <m03bln675w.fsf@bar.nemo-project.org>

Seth Falcon wrote:

> Actually, R source packages are also mangled.  While the source is
> readable, it is not in the form used to develop the package.

I haven't seen this behaviour.  At least for the simple package I'm
maintaining (pls), the only file in the source package that is changed
by R CMD build, is DESCRIPTION.  All .R and .Rd files are untouched
(even the modification dates are unchanged).  (This is on a Linux
system, I don't know how it works on MS/Mac.)

-- 
Bj??rn-Helge Mevik



From andrea.toreti at apat.it  Tue Nov 22 13:52:26 2005
From: andrea.toreti at apat.it (Andrea Toreti)
Date: Tue, 22 Nov 2005 13:52:26 +0100
Subject: [R] garch function in R
Message-ID: <003601c5ef63$9795b9e0$e0019e0a@Toreti>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051122/609f7e73/attachment.pl

From asaguiar at spsconsultoria.com  Wed Nov 23 01:52:18 2005
From: asaguiar at spsconsultoria.com (Alexandre Santos Aguiar)
Date: Tue, 22 Nov 2005 22:52:18 -0200
Subject: [R] More than one hidden layer with Neural package
Message-ID: <200511222252.19016.asaguiar@spsconsultoria.com>

Hi,

After a minor difficulty with the package, I have managed to trais some 
networks.
However, I have failed to create nets with more than one hidden layer. Can 
someone, please, send me a simple (yet not real) example?

Thanx.

-- 

        Alexandre Santos Aguiar
- consultoria para pesquisa em sa??de -
         R Botucatu, 591 cj 81
           tel 11-9320-2046
           fax 11-5549-8760
        www.spsconsultoria.com



From ligges at statistik.uni-dortmund.de  Wed Nov 23 10:37:26 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Nov 2005 10:37:26 +0100
Subject: [R] How to plot two different lines with different color with
 the	same "plot" function?
In-Reply-To: <200511230857.jAN8vTaw032297@hypatia.math.ethz.ch>
References: <200511230857.jAN8vTaw032297@hypatia.math.ethz.ch>
Message-ID: <43843856.1070401@statistik.uni-dortmund.de>

¹ãÐÇ wrote:

> Hi,R_Help!
> I have done something like this:
> 
>>x<-seq(412,612,1)
>>plot(x,dpois(x,512),col="blue",type="l")
>>plot(x,dnorm(x,512,sd=sqrt(512)),col="red",type="l")
> 
> And now,I want to plot the two lines in the same picture or with the same "plot" action?
> What should I do? 
> And any advises? 	


Please read the psoting guide. It points you to the mailing list
archives where this question has been answered hundreds of times.

Uwe Ligges


> Thank you in advance!
>  				
> 
> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¹ãÐÇ
> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-23
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Nov 23 10:43:08 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Nov 2005 10:43:08 +0100
Subject: [R] How to open a text file in my screen (not in the R session)?
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC03219432@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC03219432@ibfftce502.de.ad.drkw.net>
Message-ID: <438439AC.7080003@statistik.uni-dortmund.de>

Molins, Jordi wrote:

> In the middle of my code, I have the following:
> 
> write.table(PredictionTest, file = "predictions.txt")
> 
> what I would like is that inmediately after this line is executed, the file
> predictions.txt is opened in the middle of my screen (not in my R session,
> where I continue running my code). Is there an easy way to do that?
> 
> Thank you

If the pager is set up appropriately use file.show() or start another 
program by a system() call.

Uwe Ligges


> 
> 
> --------------------------------------------------------------------------------
> The information contained herein is confidential and is inte...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Nov 23 10:44:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Nov 2005 10:44:11 +0100
Subject: [R] More than one hidden layer with Neural package
In-Reply-To: <200511222252.19016.asaguiar@spsconsultoria.com>
References: <200511222252.19016.asaguiar@spsconsultoria.com>
Message-ID: <438439EB.7030603@statistik.uni-dortmund.de>

Alexandre Santos Aguiar wrote:

> Hi,
> 
> After a minor difficulty with the package, I have managed to trais some 
> networks.
> However, I have failed to create nets with more than one hidden layer. Can 
> someone, please, send me a simple (yet not real) example?

I cannot find a package called "Neural" on CRAN. Please tell us which 
package you are referring to.

Uwe Ligges


> Thanx.
>



From massimiliano.tripoli at ieo.it  Wed Nov 23 19:39:09 2005
From: massimiliano.tripoli at ieo.it (Massimiliano Tripoli)
Date: Wed, 23 Nov 2005 10:39:09 -0800
Subject: [R] How to open a text file in my screen (not in the R session)?
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC03219432@ibfftce502.de.ad.
	drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC03219432@ibfftce502.de.ad.drkw.net>
Message-ID: <6.2.3.4.0.20051123103822.03680ed8@mail.ieo.it>

You could use the following istruction:

system("notepad predictions.txt",wait=FALSE)

I hope this will be helpful.

Regards,

Massimiliano

At 01:00 AM 11/23/2005, Molins, Jordi wrote:

>In the middle of my code, I have the following:
>
>write.table(PredictionTest, file = "predictions.txt")
>
>what I would like is that inmediately after this line is executed, the file
>predictions.txt is opened in the middle of my screen (not in my R session,
>where I continue running my code). Is there an easy way to do that?
>
>Thank you
>
>
>
>--------------------------------------------------------------------------------
>The information contained herein is confidential and is inte...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Nov 23 10:49:53 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Nov 2005 10:49:53 +0100
Subject: [R] Customizing the package build process
In-Reply-To: <1132694032.2951.139.camel@iron.psg.net>
References: <1132694032.2951.139.camel@iron.psg.net>
Message-ID: <43843B41.3020805@statistik.uni-dortmund.de>

Ross Boylan wrote:

> I've made a package for which R CMD build isn't producing very
> satisfactory results.  I'll get to the details in a moment.
> 
> I wonder if it would make sense to have my own makefiles (which already
> exist and are doing quite a lot) produce the .tar.gz file ordinarily
> produced by R CMD build.  As far as I can tell, R CMD build basically
> tars up of the project directory after running some checks.  I could run
> R CMD check separately.
> 
> There are two main problems with the results of R CMD build.  First, it
> has lots of files that I don't want included (the input files used to
> generate configure, miscellaneous garbage, other stuff not suitable for
> distribution).  Second, I have data files as both "data.gz" and "data".
> R puts "data" into the .tar.gz file and sensibly ignores the .gz file.
> Unfortunately, my makefiles assume the existence of the "data.gz" files,
> and so may have trouble after the .tar.gz is unpacked and there are no
> "data.gz" files.
> 
> My bias would ordinarily be to piggy back on the R build system as much
> as possible.  In principle, this could get me extra features (binary
> builds, MS Windows builds) and it would track the things R build does
> beyond tarring files.  But in this case using the R build system seems
> quite ugly.  I could in principle use .Rbuildignore, probably generated
> dynamically, to exclude files.  That doesn't solve the 2nd problem
> (data.gz becomes data).
> 
> So does the alternative of doing the tar'ing myself make sense?
> 
> Is there another option that could hook into the R CMD build process
> more deeply than the use of .Rbuildignore?
> 
> I suppose another option would be to do a clean checkout of the sources
> for my package, run a special makefile target that would create the
> necessary files and delete all unwanted files, and then do a regular R
> CMD build.  This might still have trouble with "data.gz".
> 
> P.S. Previous list postings advised that R CMD install was a better way
> to produce binaries than R CMD build --binary.  The former command
> doesn't seem to have any options for making binaries; has that facility
> been removed?

No, you can use:

R CMD INSTALL --build


Uwe Ligges


> 
> Second question: my reading is that .Rbuildignore is only read in the
> package root directory, and will have no effect below that.  Is that
> correct?  Per directory .Rbuildignore's would be convenient..
>



From Jordi.Molins at drkw.com  Wed Nov 23 11:13:38 2005
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Wed, 23 Nov 2005 11:13:38 +0100
Subject: [R] How to open a text file in my screen (not in the R sessio
	n)?
Message-ID: <C5A76BA0CA4D734CA725124C4D6397AC03219435@ibfftce502.de.ad.drkw.net>


Thank you for all the answers.

I have used

system("notepad predictions.txt",wait=FALSE)

and it works. Just a final question: is there a way to force the file to
open on top of all the other applications in my desktop?


> -----Original Message-----
> From: Massimiliano Tripoli [mailto:massimiliano.tripoli at ieo.it]
> Sent: 23 November 2005 19:39
> To: Molins, Jordi; 'r-help at stat.math.ethz.ch'
> Cc: Jordi Molins
> Subject: Re: [R] How to open a text file in my screen (not in the R
> session)?
> 
> 
> You could use the following istruction:
> 
> system("notepad predictions.txt",wait=FALSE)
> 
> I hope this will be helpful.
> 
> Regards,
> 
> Massimiliano
> 
> At 01:00 AM 11/23/2005, Molins, Jordi wrote:
> 
> >In the middle of my code, I have the following:
> >
> >write.table(PredictionTest, file = "predictions.txt")
> >
> >what I would like is that inmediately after this line is 
> executed, the file
> >predictions.txt is opened in the middle of my screen (not in 
> my R session,
> >where I continue running my code). Is there an easy way to do that?
> >
> >Thank you
> >
>


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From r.babigumira at gmail.com  Wed Nov 23 11:27:03 2005
From: r.babigumira at gmail.com (Ronnie Babigumira)
Date: Wed, 23 Nov 2005 11:27:03 +0100
Subject: [R] getting started, reading listing and saving data
In-Reply-To: <2a9c000c0511222207m26245dddm2f79c172eec8edd4@mail.gmail.com>
References: <7d2f562c0511221738l725b2a2exf388a38af0ad7cab@mail.gmail.com>
	<2a9c000c0511222207m26245dddm2f79c172eec8edd4@mail.gmail.com>
Message-ID: <7d2f562c0511230227y1f8b6581tfc7a338bf6da569e@mail.gmail.com>

Many thanks to Peter Alspach, Jim Porzak and Murray Pung for the help.
Peter and Jim, head? and tail? was just what I needed to list a few
observations. Peter, thanks for pointing out str? to me. I totally
agree with you on its usefulness.

Murray thanks for file > save workspace (and Peters save.image)
addresses the third of my concerns

One last question related to head and tails, this works best if you
have a few variables (columns). Given more, how can I use the
information on the variable names given after str to list the first
few few observations for a set of variable.

To make it clear. Say I load a dataset with n variables named v1 to
vn. I use str(mydata) and I get a list of variable names..

str(x)
v1 ......
.  ......
.  ......
.  ......
.  ......
Vn ......

How do i list the first n observations of say v5 to v9

Many thanks

Ronnie



From petr.pikal at precheza.cz  Wed Nov 23 11:26:58 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 23 Nov 2005 11:26:58 +0100
Subject: [R] How to plot two different lines with different color with
	the	same "plot" function?
In-Reply-To: <200511230857.jAN8vTaw032297@hypatia.math.ethz.ch>
Message-ID: <43845202.19585.C4167C@localhost>



On 23 Nov 2005 at 16:59, ???????? wrote:

Date sent:      	Wed, 23 Nov 2005 16:59:38 +0800
From:           	"????????" <guangxing at ict.ac.cn>
To:             	"R-help at stat.math.ethz.ch" <R-help at stat.math.ethz.ch>
Subject:        	[R] How to plot two different lines with different color with the
	same "plot" function?

> Hi,R_Help!
> I have done something like this:
> >x<-seq(412,612,1)
> >plot(x,dpois(x,512),col="blue",type="l")
> >plot(x,dnorm(x,512,sd=sqrt(512)),col="red",type="l")
> And now,I want to plot the two lines in the same picture or with the
> same "plot" action? 

What should I do? And any advises? 	

Read some intro manuals.
Read help page for plot where you can find section see 
also.
Read help page for line.

If you still do not know how to plot line in existing 
plot, search help archives, read posting guide and as a 
last resort post your question again.

HTH
Petr


> 
> Thank you in advance!
> 
> 
> ????????????????????????????????????????
> ????????????????????????????????guangxing at ict.ac.cn
> ????????????????????????????????????????2005-11-23
> 
> 

Petr Pikal
petr.pikal at precheza.cz



From pburns at pburns.seanet.com  Wed Nov 23 11:48:08 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 23 Nov 2005 10:48:08 +0000
Subject: [R] getting started, reading listing and saving data
In-Reply-To: <7d2f562c0511230227y1f8b6581tfc7a338bf6da569e@mail.gmail.com>
References: <7d2f562c0511221738l725b2a2exf388a38af0ad7cab@mail.gmail.com>	<2a9c000c0511222207m26245dddm2f79c172eec8edd4@mail.gmail.com>
	<7d2f562c0511230227y1f8b6581tfc7a338bf6da569e@mail.gmail.com>
Message-ID: <438448E8.6080806@pburns.seanet.com>

The 'corner' function is in the spirit (but not the letter)
of what you ask for.  It is a generalization of 'head' and
'tail' which gives the first or last few items in each dimension
of a matrix or higher dimensional array.

It is available in the public domain code on the Burns
Statistics website.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ronnie Babigumira wrote:

>Many thanks to Peter Alspach, Jim Porzak and Murray Pung for the help.
>Peter and Jim, head? and tail? was just what I needed to list a few
>observations. Peter, thanks for pointing out str? to me. I totally
>agree with you on its usefulness.
>
>Murray thanks for file > save workspace (and Peters save.image)
>addresses the third of my concerns
>
>One last question related to head and tails, this works best if you
>have a few variables (columns). Given more, how can I use the
>information on the variable names given after str to list the first
>few few observations for a set of variable.
>
>To make it clear. Say I load a dataset with n variables named v1 to
>vn. I use str(mydata) and I get a list of variable names..
>
>str(x)
>v1 ......
>.  ......
>.  ......
>.  ......
>.  ......
>Vn ......
>
>How do i list the first n observations of say v5 to v9
>
>Many thanks
>
>Ronnie
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From Rau at demogr.mpg.de  Wed Nov 23 11:50:52 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Wed, 23 Nov 2005 11:50:52 +0100
Subject: [R] Dancing lissajous
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E69FE07C@HERMES.demogr.mpg.de>

Hi,
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> rlist.10.phftt at xoxy.net
> Sent: Wednesday, November 23, 2005 9:42 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Dancing lissajous
> 
> http://www.geocities.com/robsteele/
> 


this is what pops up when I try to access your site:
"The web site you are trying to access has exceeded its allocated data
transfer."

Maybe you can just post the code and we can run it ourselves?

Best,
Roland

+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From tschoenhoff at gmail.com  Wed Nov 23 11:52:14 2005
From: tschoenhoff at gmail.com (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Wed, 23 Nov 2005 11:52:14 +0100
Subject: [R] getting started, reading listing and saving data
In-Reply-To: <7d2f562c0511230227y1f8b6581tfc7a338bf6da569e@mail.gmail.com>
References: <7d2f562c0511221738l725b2a2exf388a38af0ad7cab@mail.gmail.com>
	<2a9c000c0511222207m26245dddm2f79c172eec8edd4@mail.gmail.com>
	<7d2f562c0511230227y1f8b6581tfc7a338bf6da569e@mail.gmail.com>
Message-ID: <5ad2dec0511230252n3e710aedm@mail.gmail.com>

Hello,

2005/11/23, Ronnie Babigumira <r.babigumira at gmail.com>:
> Many thanks to Peter Alspach, Jim Porzak and Murray Pung for the help.
> Peter and Jim, head? and tail? was just what I needed to list a few
> observations. Peter, thanks for pointing out str? to me. I totally
> agree with you on its usefulness.
>
> Murray thanks for file > save workspace (and Peters save.image)
> addresses the third of my concerns
>
> One last question related to head and tails, this works best if you
> have a few variables (columns). Given more, how can I use the
> information on the variable names given after str to list the first
> few few observations for a set of variable.
>
> To make it clear. Say I load a dataset with n variables named v1 to
> vn. I use str(mydata) and I get a list of variable names..
>
> str(x)
> v1 ......
> .  ......
> .  ......
> .  ......
> .  ......
> Vn ......
>
> How do i list the first n observations of say v5 to v9

Not sure if this is what you are looking for: str(myobject[ 5:9])

regards



From p.dalgaard at biostat.ku.dk  Wed Nov 23 12:05:21 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Nov 2005 12:05:21 +0100
Subject: [R] Dancing lissajous
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E69FE07C@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E69FE07C@HERMES.demogr.mpg.de>
Message-ID: <x24q63sjym.fsf@viggo.kubism.ku.dk>

"Rau, Roland" <Rau at demogr.mpg.de> writes:

> Hi,
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > rlist.10.phftt at xoxy.net
> > Sent: Wednesday, November 23, 2005 9:42 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Dancing lissajous
> > 
> > http://www.geocities.com/robsteele/
> > 
> 
> 
> this is what pops up when I try to access your site:
> "The web site you are trying to access has exceeded its allocated data
> transfer."

So R-help now rivals slashdot.org in its ability to generate de facto
distributed denial of service attacks? ;-)
 
> Maybe you can just post the code and we can run it ourselves?

He already did, I believe.

From: rlist.10.phftt at xoxy.net
Subject: [R] Animated lissajous
To: r-help at stat.math.ethz.ch
Date: Sat, 15 Oct 2005 23:48:46 -0400


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From rlittle at ula.ve  Wed Nov 23 13:03:26 2005
From: rlittle at ula.ve (Roy Little)
Date: Wed, 23 Nov 2005 08:03:26 -0400
Subject: [R] loadings matrices in plsr vs pcr in pls pacakage
In-Reply-To: <1132687028.23462.22.camel@gsimpson.geog.ucl.ac.uk>
References: <438314C0.2040902@ula.ve>
	<1132687028.23462.22.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <43845A8E.8050101@ula.ve>

Many thanks to Prof. Ripley, whose concise and pointed references to the 
R documentation are always well received. I indeed have had experience 
with the princomp() call, but it had slipped my mind.



From koen.hufkens at telenet.be  Wed Nov 23 13:37:04 2005
From: koen.hufkens at telenet.be (Koen Hufkens)
Date: Wed, 23 Nov 2005 13:37:04 +0100
Subject: [R] assign() problem
Message-ID: <43846270.9010904@telenet.be>

I've written a piece of code (see below) to do a wavelet image 
decomposition, during  the evaluation of this code I would like to write 
the results of some calculations back to the R root directory. I used 
assign() to do so because the names should vary when going thrue a 
while() loop. For some unknown reason I get an error that says:

Error in assign(varname[i], imwrImage) :
         invalid first argument

what could be wrong, when I do it on the commandline everything works 
out just fine. But within the function it doesn't. When I disable the 
assign statement, everything runs fine so the rest of the code should be 
clean.

My code:

# Wavelet multiresolution decomposition

wmra <- function(image_file){
#require(rimage)
require(wavethresh)

#reading image file and converting it to grayscale
#dimage <- read.jpeg(image_file)
#dimage <- imagematrix(dimage)

# discrete wavelet decomposition
imwdImage <- imwd(image_file)

# get a value for the number of decomposition levels
#nlevels <- imwdImage$nlevels

i <- 0
varname <- paste("level",0:(imwdImage$nlevels-1),sep="")
print(varname)
while ( i < imwdImage$nlevels)
	 {
	
	 # set the threshold to 0 on all levels except the one in evaluation
	 # thresholdLevels is the list of levels to set to zero
	 thresholdLevels <- 1:(imwdImage$nlevels-1)
	 thresholdLevels[i] <- 0
	 thresholdedCoeff <- threshold.imwd(imwdImage,levels=thresholdLevels, 
policy=c("manual"), type="hard", value=10000)
	
          # calculate the inverse wavelet transform
	 imwrImage <- imwr(thresholdedCoeff)

	 # assign the various decomposition level data a name
	 # starting at i+1 because i = 0.
          assign(varname[i+1],imwrImage) #  --> gives an error
	    	
	 # export the multiresolution decomposition for level i as graph
	 jpeg(paste("level",i,".jpg",sep=""),quality=100)	
	 image(imwrImage,xlab=c("MRA image of level: ",i),col=gray(255:0/256))
	 dev.off()
    	 	 	
	 i <- i + 1
	  }

}

Best regards, Koen



From r.babigumira at gmail.com  Wed Nov 23 13:44:38 2005
From: r.babigumira at gmail.com (Ronnie Babigumira)
Date: Wed, 23 Nov 2005 13:44:38 +0100
Subject: [R] getting started, reading listing and saving data
In-Reply-To: <5ad2dec0511230252n3e710aedm@mail.gmail.com>
References: <7d2f562c0511221738l725b2a2exf388a38af0ad7cab@mail.gmail.com>
	<2a9c000c0511222207m26245dddm2f79c172eec8edd4@mail.gmail.com>
	<7d2f562c0511230227y1f8b6581tfc7a338bf6da569e@mail.gmail.com>
	<5ad2dec0511230252n3e710aedm@mail.gmail.com>
Message-ID: <7d2f562c0511230444p2742319aj4358dbb92f93bacd@mail.gmail.com>

Thomas many thanks,  though str(myobject[ 5:9]) is not what I want, it
prompted me to try head(myobject [3:4]) and this gets me really close
to what I want, I get the first few observations of variables 3 and 4
in my data. What I would like to do however is to do the same thing
but with the variable names explicitly (so instead of 3:4 I use
something like varname3 and varname4)

Any ideas

Again, many thanks to all on the list who have helped (I promise I am
frantically trying to read up everything I have been given and bring
myself upto speed so I will not be consuming bandwidth with a lot of
read the manual sort of questions)



On 11/23/05, Thomas Sch??nhoff <tschoenhoff at gmail.com> wrote:
> Hello,
>
> 2005/11/23, Ronnie Babigumira <r.babigumira at gmail.com>:
> > Many thanks to Peter Alspach, Jim Porzak and Murray Pung for the help.
> > Peter and Jim, head? and tail? was just what I needed to list a few
> > observations. Peter, thanks for pointing out str? to me. I totally
> > agree with you on its usefulness.
> >
> > Murray thanks for file > save workspace (and Peters save.image)
> > addresses the third of my concerns
> >
> > One last question related to head and tails, this works best if you
> > have a few variables (columns). Given more, how can I use the
> > information on the variable names given after str to list the first
> > few few observations for a set of variable.
> >
> > To make it clear. Say I load a dataset with n variables named v1 to
> > vn. I use str(mydata) and I get a list of variable names..
> >
> > str(x)
> > v1 ......
> > .  ......
> > .  ......
> > .  ......
> > .  ......
> > Vn ......
> >
> > How do i list the first n observations of say v5 to v9
>
> Not sure if this is what you are looking for: str(myobject[ 5:9])
>
> regards
>



From kaniovsk at wifo.ac.at  Wed Nov 23 14:01:32 2005
From: kaniovsk at wifo.ac.at (Serguei Kaniovski)
Date: Wed, 23 Nov 2005 14:01:32 +0100
Subject: [R] vector of permutated products
Message-ID: <4384682C.8030805@wifo.ac.at>

Given an x-vector with, say, 3 elements, I would like to compute the 
following vector of permutated products

(1-x1)*(1-x2)*(1-x3)
(1-x1)*(1-x2)*x3
(1-x1)*x2*(1-x3)
x1*(1-x2)*(1-x3)
(1-x1)*x2*x3
x1*(1-x2)*x3
x1*x2*(1-x3)
x1*x2*x3

Now, I already have the correctly sorted matrix of permutations! So, the 
input looks something like:

#input
x<-c(0.3,0.1,0.2)
Nx<-length(x)
Ncomb<-2^Nx
permat<-matrix(c(1,1,1,1,1,0,1,0,1,1,0,0,0,1,1,0,1,0,0,0,1,0,0,0),Ncomb,Nx)

I code the rest as follows:

#correct but clumsy code
temp1<-t(matrix(rep(x,2^3),3,2^3))
temp2<-t(matrix(rep(1-x,2^3),3,2^3))
result<-apply(permat*temp1-(permat-1)*temp2,1,prod)

But I would like to do without temp1 and temp2. To have something like

result<-apply(permat*x-(permat-1)*x,1,prod)

My problem is that permat*x does not produce permat*temp1 due to the way 
the element-by-element multiplication works in R. Can you help to 
simplify the above code?

Thank you in advance,
Serguei Kaniovski
-- 
___________________________________________________________________

Austrian Institute of Economic Research (WIFO)

Name: Serguei Kaniovski			P.O.Box 91
Tel.: +43-1-7982601-243			Arsenal Objekt 20
Fax:  +43-1-7989386			1103 Vienna, Austria
Mail: Serguei.Kaniovski at wifo.ac.at

http://www.wifo.ac.at/Serguei.Kaniovski



From charles.hebert.rml at gmail.com  Wed Nov 23 14:01:40 2005
From: charles.hebert.rml at gmail.com (Charles Hebert)
Date: Wed, 23 Nov 2005 14:01:40 +0100
Subject: [R] Hmisc compilation error
Message-ID: <5a2682400511230501s5624a454y@mail.gmail.com>

Dear all,

I'm trying to install the Hmisc package (3.0-7) on macOSX (10.4.3) and
R (2.2.0).
There's an error in the library linkage step (see log).

How can i solve this problem ?

Perhaps a solution would be to substitute gcc3 for gcc4 or remove the
invalid option... but i didn't find the "makefile" (where is it ?).

Thanks, C.

/------- Compilation log ---------/

* Installing *source* package 'Hmisc' ...
** libs
g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
gcc-3.3 -no-cpp-precomp
-I/Library/Frameworks/R.framework/Resources/include 
-I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
g77   -fno-common  -g -O2 -c rcorr.f -o rcorr.o
g77   -fno-common  -g -O2 -c wclosest.f -o wclosest.o
gcc-3.3 -bundle -flat_namespace -undefined suppress -L/usr/local/lib
-o Hmisc.so cidxcn.o cidxcp.o hoeffd.o jacklins.o largrec.o ranksort.o
rcorr.o wclosest.o  -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2
-lg2c -lSystem -lcc_dynamic -F/Library/Frameworks/R.framework/..
-framework R
** Removing '/Library/Frameworks/R.framework/Versions/2.2/Resources/library/Hmisc'
ld: can't locate file for: -lcc_dynamic
make: *** [Hmisc.so] Error 1
ERROR: compilation failed for package 'Hmisc'



From krcabrer at unalmed.edu.co  Wed Nov 23 14:14:53 2005
From: krcabrer at unalmed.edu.co (Kenneth Roy Cabrera Torres)
Date: Wed, 23 Nov 2005 08:14:53 -0500
Subject: [R] Proxy config in R for windows
In-Reply-To: <200511191151.13975.s.molnar@sbcglobal.net>
References: <200511191151.13975.s.molnar@sbcglobal.net>
Message-ID: <43846B4D.20104@unalmed.edu.co>

Hi R users:

I know that the --internet2 options enable me in Windows to download 
packages in R,
but now, in my university the put a usename and password configuration 
for the proxy.

How can I configure R to take the proxy password and username.

I linux I solve the problem when I configre the
export http_proxy="username:pass at server:port"

But I don??t know how to make it in windows.

Kenneth



From tschoenhoff at gmail.com  Wed Nov 23 14:14:36 2005
From: tschoenhoff at gmail.com (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Wed, 23 Nov 2005 14:14:36 +0100
Subject: [R] getting started, reading listing and saving data
In-Reply-To: <7d2f562c0511230444p2742319aj4358dbb92f93bacd@mail.gmail.com>
References: <7d2f562c0511221738l725b2a2exf388a38af0ad7cab@mail.gmail.com>
	<2a9c000c0511222207m26245dddm2f79c172eec8edd4@mail.gmail.com>
	<7d2f562c0511230227y1f8b6581tfc7a338bf6da569e@mail.gmail.com>
	<5ad2dec0511230252n3e710aedm@mail.gmail.com>
	<7d2f562c0511230444p2742319aj4358dbb92f93bacd@mail.gmail.com>
Message-ID: <5ad2dec0511230514t67cfea50t@mail.gmail.com>

Hi,

2005/11/23, Ronnie Babigumira <r.babigumira at gmail.com>:
> Thomas many thanks,  though str(myobject[ 5:9]) is not what I want, it
> prompted me to try head(myobject [3:4]) and this gets me really close
> to what I want, I get the first few observations of variables 3 and 4
> in my data. What I would like to do however is to do the same thing
> but with the variable names explicitly (so instead of 3:4 I use
> something like varname3 and varname4)
>
> Any ideas
>
> Again, many thanks to all on the list who have helped (I promise I am
> frantically trying to read up everything I have been given and bring
> myself upto speed so I will not be consuming bandwidth with a lot of
> read the manual sort of questions)
>
>
>
> On 11/23/05, Thomas Sch??nhoff <tschoenhoff at gmail.com> wrote:
> > Hello,
> >
> > 2005/11/23, Ronnie Babigumira <r.babigumira at gmail.com>:
> > > Many thanks to Peter Alspach, Jim Porzak and Murray Pung for the help.
> > > Peter and Jim, head? and tail? was just what I needed to list a few
> > > observations. Peter, thanks for pointing out str? to me. I totally
> > > agree with you on its usefulness.
> > >
> > > Murray thanks for file > save workspace (and Peters save.image)
> > > addresses the third of my concerns
> > >
> > > One last question related to head and tails, this works best if you
> > > have a few variables (columns). Given more, how can I use the
> > > information on the variable names given after str to list the first
> > > few few observations for a set of variable.
> > >
> > > To make it clear. Say I load a dataset with n variables named v1 to
> > > vn. I use str(mydata) and I get a list of variable names..
> > >
> > > str(x)
> > > v1 ......
> > > .  ......
> > > .  ......
> > > .  ......
> > > .  ......
> > > Vn ......
> > >
> > > How do i list the first n observations of say v5 to v9
> >
> > Not sure if this is what you are looking for: str(myobject[ 5:9])

myobservs  <-str(myobject, myobject$v5[1:n],...,...,... myobject$9[1:n])

where n = number of observations to be selected.

Hmm, not sure if this works for your object (data.frame, matrix,
multi-dimensional array?). Would be much easier to know more about
your object!

regards

Thomas



From tschoenhoff at gmail.com  Wed Nov 23 14:16:33 2005
From: tschoenhoff at gmail.com (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Wed, 23 Nov 2005 14:16:33 +0100
Subject: [R] getting started, reading listing and saving data
In-Reply-To: <5ad2dec0511230514t67cfea50t@mail.gmail.com>
References: <7d2f562c0511221738l725b2a2exf388a38af0ad7cab@mail.gmail.com>
	<2a9c000c0511222207m26245dddm2f79c172eec8edd4@mail.gmail.com>
	<7d2f562c0511230227y1f8b6581tfc7a338bf6da569e@mail.gmail.com>
	<5ad2dec0511230252n3e710aedm@mail.gmail.com>
	<7d2f562c0511230444p2742319aj4358dbb92f93bacd@mail.gmail.com>
	<5ad2dec0511230514t67cfea50t@mail.gmail.com>
Message-ID: <5ad2dec0511230516g32c14d26g@mail.gmail.com>

Again,

2005/11/23, Thomas Sch??nhoff <tschoenhoff at gmail.com>:
> Hi,
>
> 2005/11/23, Ronnie Babigumira <r.babigumira at gmail.com>:
> > Thomas many thanks,  though str(myobject[ 5:9]) is not what I want, it
> > prompted me to try head(myobject [3:4]) and this gets me really close
> > to what I want, I get the first few observations of variables 3 and 4
> > in my data. What I would like to do however is to do the same thing
> > but with the variable names explicitly (so instead of 3:4 I use
> > something like varname3 and varname4)
> >
> > Any ideas
> >
> > Again, many thanks to all on the list who have helped (I promise I am
> > frantically trying to read up everything I have been given and bring
> > myself upto speed so I will not be consuming bandwidth with a lot of
> > read the manual sort of questions)
> >
> >
> >
> > On 11/23/05, Thomas Sch??nhoff <tschoenhoff at gmail.com> wrote:
> > > Hello,
> > >
> > > 2005/11/23, Ronnie Babigumira <r.babigumira at gmail.com>:
> > > > Many thanks to Peter Alspach, Jim Porzak and Murray Pung for the help.
> > > > Peter and Jim, head? and tail? was just what I needed to list a few
> > > > observations. Peter, thanks for pointing out str? to me. I totally
> > > > agree with you on its usefulness.
> > > >
> > > > Murray thanks for file > save workspace (and Peters save.image)
> > > > addresses the third of my concerns
> > > >
> > > > One last question related to head and tails, this works best if you
> > > > have a few variables (columns). Given more, how can I use the
> > > > information on the variable names given after str to list the first
> > > > few few observations for a set of variable.
> > > >
> > > > To make it clear. Say I load a dataset with n variables named v1 to
> > > > vn. I use str(mydata) and I get a list of variable names..
> > > >
> > > > str(x)
> > > > v1 ......
> > > > .  ......
> > > > .  ......
> > > > .  ......
> > > > .  ......
> > > > Vn ......
> > > >
> > > > How do i list the first n observations of say v5 to v9

Sorry for the typo, once again:
>
> myobservs  <-str(myobject, myobject$v5[1:n],...,...,... myobject$v9[1:n])
>
> where n = number of observations to be selected.


Thomas



From maechler at stat.math.ethz.ch  Wed Nov 23 14:35:14 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 23 Nov 2005 14:35:14 +0100
Subject: [R] finding peaks in a simple dataset with R
Message-ID: <17284.28690.71104.813153@stat.math.ethz.ch>

I've been asked in private,
(and am replying BCC to the asker),

>> I saw your post on the R-help archives page about the possibility of 
>> porting a function from S-Plus called peaks() to R. I am looking for 
>> some way to locate peaks in a simple x,y data set, and thought that R 
>> might be the way to go.

"of course" it is the way to go, don't get lost be going
somewhere else  :-)
and try

    install.packages("fortune")
    fortune("go with R")

>> Any ideas would be a great help,

Using      RSiteSearch("peaks") gives too many hits, among
which those you can get by the more advanced (regular expression) call

    RSiteSearch("/peaks\\b.*\\bfunction/")

where in the 2nd hit,
    http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33097.html
Petr Pikal gives a simple peaks() function, originally by Brian Ripley
which is using embed() and max.col() smartly.

I wonder if we shouldn't polish that a bit and add to R's
standard 'utils' package.

Martin Maechler, ETH Zurich



From dimitris.rizopoulos at med.kuleuven.be  Wed Nov 23 14:39:42 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 23 Nov 2005 14:39:42 +0100
Subject: [R] vector of permutated products
References: <4384682C.8030805@wifo.ac.at>
Message-ID: <00a301c5f033$5be6d520$0540210a@www.domain>

maybe something along these lines could be helpful:

x <- c(0.3, 0.1, 0.2)
###########
nx <- length(x)
perms <- as.matrix(expand.grid(lapply(1:nx, function(i) c(FALSE, 
TRUE))))
apply(perms, 1, function(ind) prod(c(x[ind], (1 - x[!ind]))))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Serguei Kaniovski" <kaniovsk at wifo.ac.at>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, November 23, 2005 2:01 PM
Subject: [R] vector of permutated products


> Given an x-vector with, say, 3 elements, I would like to compute the
> following vector of permutated products
>
> (1-x1)*(1-x2)*(1-x3)
> (1-x1)*(1-x2)*x3
> (1-x1)*x2*(1-x3)
> x1*(1-x2)*(1-x3)
> (1-x1)*x2*x3
> x1*(1-x2)*x3
> x1*x2*(1-x3)
> x1*x2*x3
>
> Now, I already have the correctly sorted matrix of permutations! So, 
> the
> input looks something like:
>
> #input
> x<-c(0.3,0.1,0.2)
> Nx<-length(x)
> Ncomb<-2^Nx
> permat<-matrix(c(1,1,1,1,1,0,1,0,1,1,0,0,0,1,1,0,1,0,0,0,1,0,0,0),Ncomb,Nx)
>
> I code the rest as follows:
>
> #correct but clumsy code
> temp1<-t(matrix(rep(x,2^3),3,2^3))
> temp2<-t(matrix(rep(1-x,2^3),3,2^3))
> result<-apply(permat*temp1-(permat-1)*temp2,1,prod)
>
> But I would like to do without temp1 and temp2. To have something 
> like
>
> result<-apply(permat*x-(permat-1)*x,1,prod)
>
> My problem is that permat*x does not produce permat*temp1 due to the 
> way
> the element-by-element multiplication works in R. Can you help to
> simplify the above code?
>
> Thank you in advance,
> Serguei Kaniovski
> -- 
> ___________________________________________________________________
>
> Austrian Institute of Economic Research (WIFO)
>
> Name: Serguei Kaniovski P.O.Box 91
> Tel.: +43-1-7982601-243 Arsenal Objekt 20
> Fax:  +43-1-7989386 1103 Vienna, Austria
> Mail: Serguei.Kaniovski at wifo.ac.at
>
> http://www.wifo.ac.at/Serguei.Kaniovski
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From andy_liaw at merck.com  Wed Nov 23 14:40:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Nov 2005 08:40:12 -0500
Subject: [R] vector of permutated products
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED5EC@usctmx1106.merck.com>

Here's one possibility:

> x <- c(0.3, 0.1, 0.2)
> ## Create the permutation matrix:
> m <- do.call("expand.grid", lapply(x, function(x) c(x, 1-x)))
> ## get the row product:
> apply(m, 1, prod)
    1     2     3     4     5     6     7     8 
0.006 0.014 0.054 0.126 0.024 0.056 0.216 0.504 
> ## Alternatively:
> exp(rowSums(log(m)))
    1     2     3     4     5     6     7     8 
0.006 0.014 0.054 0.126 0.024 0.056 0.216 0.504 

HTH,
Andy

> From: Serguei Kaniovski
> 
> Given an x-vector with, say, 3 elements, I would like to compute the 
> following vector of permutated products
> 
> (1-x1)*(1-x2)*(1-x3)
> (1-x1)*(1-x2)*x3
> (1-x1)*x2*(1-x3)
> x1*(1-x2)*(1-x3)
> (1-x1)*x2*x3
> x1*(1-x2)*x3
> x1*x2*(1-x3)
> x1*x2*x3
> 
> Now, I already have the correctly sorted matrix of 
> permutations! So, the 
> input looks something like:
> 
> #input
> x<-c(0.3,0.1,0.2)
> Nx<-length(x)
> Ncomb<-2^Nx
> permat<-matrix(c(1,1,1,1,1,0,1,0,1,1,0,0,0,1,1,0,1,0,0,0,1,0,0
> ,0),Ncomb,Nx)
> 
> I code the rest as follows:
> 
> #correct but clumsy code
> temp1<-t(matrix(rep(x,2^3),3,2^3))
> temp2<-t(matrix(rep(1-x,2^3),3,2^3))
> result<-apply(permat*temp1-(permat-1)*temp2,1,prod)
> 
> But I would like to do without temp1 and temp2. To have something like
> 
> result<-apply(permat*x-(permat-1)*x,1,prod)
> 
> My problem is that permat*x does not produce permat*temp1 due 
> to the way 
> the element-by-element multiplication works in R. Can you help to 
> simplify the above code?
> 
> Thank you in advance,
> Serguei Kaniovski
> -- 
> ___________________________________________________________________
> 
> Austrian Institute of Economic Research (WIFO)
> 
> Name: Serguei Kaniovski			P.O.Box 91
> Tel.: +43-1-7982601-243			Arsenal Objekt 20
> Fax:  +43-1-7989386			1103 Vienna, Austria
> Mail: Serguei.Kaniovski at wifo.ac.at
> 
http://www.wifo.ac.at/Serguei.Kaniovski

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Nov 23 15:10:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Nov 2005 14:10:44 +0000 (GMT)
Subject: [R] modifying code in contributed libraries - changes from
 versions 1.* to 2.*
In-Reply-To: <m03bln675w.fsf@bar.nemo-project.org>
References: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>
	<43834AE0.2050304@statistik.uni-dortmund.de>
	<m2lkzgy5gq.fsf@ziti.local> <m03bln675w.fsf@bar.nemo-project.org>
Message-ID: <Pine.LNX.4.61.0511231358330.7559@gannet.stats>

I didn't understand this either.  Perhaps Seth could explain it.  [I've 
added Seth back to the recipients.]

In particular, GPL requires the sources to be made available, and if the 
source package is not the sources, where are they made available?  So for 
a GPL-ed package it seems completely reasonable to contribute patches to 
the source package.

The only thing I can guess is that a very small number of packages are 
kept in some other form, e.g. to be pre-processed into R and S-PLUS 
variants or in some web-like form (in the sense of 'web' used for TeX, 
mixing code and documentation).  But the unequivocal statement

> it is not in the form used to develop the package

is false for e.g. the stats package and the VR bundle.


On Wed, 23 Nov 2005, Bj?rn-Helge Mevik wrote:

> Seth Falcon wrote:
>
>> Actually, R source packages are also mangled.  While the source is
>> readable, it is not in the form used to develop the package.
>
> I haven't seen this behaviour.  At least for the simple package I'm
> maintaining (pls), the only file in the source package that is changed
> by R CMD build, is DESCRIPTION.  All .R and .Rd files are untouched
> (even the modification dates are unchanged).  (This is on a Linux
> system, I don't know how it works on MS/Mac.)

It's the same Perl code.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From jean-pierre.both at cea.fr  Wed Nov 23 16:11:37 2005
From: jean-pierre.both at cea.fr (both jean-pierre)
Date: Wed, 23 Nov 2005 16:11:37 +0100
Subject: [R] help on list
Message-ID: <438486A9.7020205@cea.fr>

Hi,
New to R,

having done a wavelet analysis i got a result as list.
The problem I have is : how can I acces to a given element of the list.

here is what I get from my variable scale_d28

 >typeof(scale_d28)
[1] "list"
 > length(scale_d28)
[1] 1

and scale_d28 is a list of 100000 double values?

Thanks



-- 
Jean Pierre BOTH

phone	(33) 01.69.08.84.78    (work)
email	jean-pierre.both at cea point fr (work)

Complete address:
-----------------
Commissariat a l Energie Atomique 
Centre Etudes Nucleaires de Saclay
DRT/LIST/DETECS/SSTM/L2MA  bat.516
91191 GIF/YVETTE CEDEX
France



From Sebastian.Leuzinger at unibas.ch  Wed Nov 23 15:17:36 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Wed, 23 Nov 2005 15:17:36 +0100
Subject: [R] axis fontsize suse 9.3
Message-ID: <200511231517.37609.Sebastian.Leuzinger@unibas.ch>

hello, does anybody have an idea why the fontsize is impossible to be changed 
in R version 2.1.0 under linux suse 9.3?
par(cex.axis=1.5) does not change anything and 
par(cex.axis=2) triggers the error message

X11 font at size 16 could not be loaded

any hints appreciated

-- 
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From ripley at stats.ox.ac.uk  Wed Nov 23 15:20:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Nov 2005 14:20:23 +0000 (GMT)
Subject: [R] attributes of a data.frame
In-Reply-To: <43836A33.4050405@yorku.ca>
References: <43836A33.4050405@yorku.ca>
Message-ID: <Pine.LNX.4.61.0511231415200.7559@gannet.stats>

Why do you have names like 'pctx723' in the first place?

I have never had a difficulty with using informative column names whereas 
you seem to require the extra complication of `variable labels'.
Now we have `` and allow _ in syntactic names it is even easier than it 
was.

On Tue, 22 Nov 2005, Michael Friendly wrote:

> It's hard for me to resist dipping my oar into this...
>
> Variable labels are so generally useful, both in documenting a
> dataset (what was 'pctx723' again?) and in producing readable
> output and graphs that it is a shame they are not provided in
> base R.  If they were (and were used in print and plot methods,
> when available) it would avoid a lot of the necessity to specify
> xlab= and ylab= in graphs, or, perhaps worse, ending up with
> pctx723 as the label in your presentation.
>
> -Michael
>
>> On 11/21/2005 2:51 PM, Adrian DUSA wrote:
>>
>>>> On Monday 21 November 2005 22:41, Duncan Murdoch wrote:
>>>
>>>>>> [...snip...]
>>>>>> Not all dataframes have the variable.labels attribute.  I'm guessing
>>>>>> you've installed some contributed package to add them, or are importing
>>>>>> an SPSS datafile using read.spss.  So don't expect varlab() or
>>>>>> variable.labels() function to be a standard R function.
>>>
>>>>
>>>> Aa-haa... of course you are right: I read them via read.spss. I understand.
>>>> Now, just to the sake of it, would it be wrong to make it standard?
>>>> Is there a special reason not to?
>>
>>
>> I think it's just that the R core developers don't see the need for
>> them.  If something is worth documenting, then you should write an .Rd
>> file or a vignette about it, and that gives you more flexibility than a
>> one line label.
>>
>> I think there are definitely developers out there who disagree with this
>> point of view, and I'm pretty sure I've seen a contributed package that
>> offered support for this, but I can't remember which one right now.  So
>> that's another reason why it's not in the base:  it doesn't need to be,
>> you can just go find and install that contributed package!


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Nov 23 15:24:16 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Nov 2005 14:24:16 +0000 (GMT)
Subject: [R] Proxy config in R for windows
In-Reply-To: <43846B4D.20104@unalmed.edu.co>
References: <200511191151.13975.s.molnar@sbcglobal.net>
	<43846B4D.20104@unalmed.edu.co>
Message-ID: <Pine.LNX.4.61.0511231423390.7559@gannet.stats>

On Wed, 23 Nov 2005, Kenneth Roy Cabrera Torres wrote:

> Hi R users:
>
> I know that the --internet2 options enable me in Windows to download
> packages in R,
> but now, in my university the put a usename and password configuration
> for the proxy.
>
> How can I configure R to take the proxy password and username.
>
> I linux I solve the problem when I configre the
> export http_proxy="username:pass at server:port"
>
> But I don?t know how to make it in windows.

The same way (without --internet2).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From marc.kirchner at iwr.uni-heidelberg.de  Wed Nov 23 15:33:28 2005
From: marc.kirchner at iwr.uni-heidelberg.de (Marc Kirchner)
Date: Wed, 23 Nov 2005 14:33:28 +0000
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <17284.28690.71104.813153@stat.math.ethz.ch>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
Message-ID: <20051123143323.GB6368@iwr.uni-heidelberg.de>

> 
> I wonder if we shouldn't polish that a bit and add to R's
> standard 'utils' package.
> 

Hm, I figured out there are (at least) two versions out there, one being
the "original" idea and a modification. 

=== Petr Pikal in 2001 (based on Brian Ripley's idea)==
peaks <- function(series, span=3) {
	z <- embed(series, span)
	result <- max.col(z) == 1 + span %/% 2
	result
}

versus

=== Petr Pikal in 2004 ==
peaks2<-function(series,span=3) {
	z <- embed(series, span)
	s <- span%/%2
	v<- max.col(z) == 1 + s
	result <- c(rep(FALSE,s),v)
	result <- result[1:(length(result)-s)]
	result
} 

Comparison shows
> peaks(c(1,4,1,1,6,1,5,1,1),3)
[1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE
which is a logical vector for elements 2:N-1 and

> peaks2(c(1,4,1,1,6,1,5,1,1),3)
[1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
which is a logical vector for elements 1:N-2.

As I would expect to "lose" (span-1)/2 elements on each side 
of the vector, to me the 2001 version feels more natural.

Also, both "suffer" from being non-deterministic in the 
multiple-maxima-case (the two 4s here)

> peaks(c(1,4,4,1,6,1,5,1,1),3)
[1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE
> peaks(c(1,4,4,1,6,1,5,1,1),3)
[1]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE
> peaks(c(1,4,4,1,6,1,5,1,1),3)
[1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE
> peaks(c(1,4,4,1,6,1,5,1,1),3)
[1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE

which also persits for span > 3 (without the 6 then, of course):

> peaks(c(1,4,4,1,1,1,5,1,1),5)
[1]  TRUE FALSE FALSE FALSE  TRUE
> peaks(c(1,4,4,1,1,1,5,1,1),5)
[1] FALSE FALSE FALSE FALSE  TRUE
> peaks(c(1,4,4,1,1,1,5,1,1),5)
[1]  TRUE FALSE FALSE FALSE  TRUE

This could (should?) be fixed by modifying the call to max.col()
	result <- max.col(z, "first") == 1 + span %/% 2;

Just my two cents,
Marc

-- 
========================================================
Dipl. Inform. Med. Marc Kirchner
Interdisciplinary Centre for Scientific Computing (IWR)
Multidimensional Image Processing
INF 368
University of Heidelberg
D-69120 Heidelberg
Tel: ++49-6221-54 87 97
Fax: ++49-6221-54 88 50
marc.kirchner at iwr.uni-heidelberg.de

-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051123/bba13290/attachment.bin

From br44114 at gmail.com  Wed Nov 23 15:52:17 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 23 Nov 2005 09:52:17 -0500
Subject: [R] assign() problem
Message-ID: <8d5a36350511230652v2b85a310g185cefaa06b934e0@mail.gmail.com>

Don't use assign(), named lists are much better (check the stuff on
indexing lists). Here's an example:
a <- list()
a[["one"]] <- c(1,2,3)
a[["two"]] <- c(4,5,6)
a[["two"]]
do.call("rbind",a)
do.call("cbind",a)
lapply(a,sum)

With regards to your question, did you try printing varname[i] in your
loop to see which value causes the error message?


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Koen Hufkens
> Sent: Wednesday, November 23, 2005 7:37 AM
> To: R help mailinglist
> Subject: [R] assign() problem
>
>
> I've written a piece of code (see below) to do a wavelet image
> decomposition, during  the evaluation of this code I would
> like to write
> the results of some calculations back to the R root directory. I used
> assign() to do so because the names should vary when going thrue a
> while() loop. For some unknown reason I get an error that says:
>
> Error in assign(varname[i], imwrImage) :
>          invalid first argument
>
> what could be wrong, when I do it on the commandline everything works
> out just fine. But within the function it doesn't. When I disable the
> assign statement, everything runs fine so the rest of the
> code should be
> clean.
>
> My code:
>
> # Wavelet multiresolution decomposition
>
> wmra <- function(image_file){
> #require(rimage)
> require(wavethresh)
>
> #reading image file and converting it to grayscale
> #dimage <- read.jpeg(image_file)
> #dimage <- imagematrix(dimage)
>
> # discrete wavelet decomposition
> imwdImage <- imwd(image_file)
>
> # get a value for the number of decomposition levels
> #nlevels <- imwdImage$nlevels
>
> i <- 0
> varname <- paste("level",0:(imwdImage$nlevels-1),sep="")
> print(varname)
> while ( i < imwdImage$nlevels)
> 	 {
> 	
> 	 # set the threshold to 0 on all levels except the one
> in evaluation
> 	 # thresholdLevels is the list of levels to set to zero
> 	 thresholdLevels <- 1:(imwdImage$nlevels-1)
> 	 thresholdLevels[i] <- 0
> 	 thresholdedCoeff <-
> threshold.imwd(imwdImage,levels=thresholdLevels,
> policy=c("manual"), type="hard", value=10000)
> 	
>           # calculate the inverse wavelet transform
> 	 imwrImage <- imwr(thresholdedCoeff)
>
> 	 # assign the various decomposition level data a name
> 	 # starting at i+1 because i = 0.
>           assign(varname[i+1],imwrImage) #  --> gives an error
> 	    	
> 	 # export the multiresolution decomposition for level i as graph
> 	 jpeg(paste("level",i,".jpg",sep=""),quality=100)	
> 	 image(imwrImage,xlab=c("MRA image of level:
> ",i),col=gray(255:0/256))
> 	 dev.off()
>     	 	 	
> 	 i <- i + 1
> 	  }
>
> }
>
> Best regards, Koen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed Nov 23 15:57:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Nov 2005 14:57:56 +0000 (GMT)
Subject: [R] axis fontsize suse 9.3
In-Reply-To: <200511231517.37609.Sebastian.Leuzinger@unibas.ch>
References: <200511231517.37609.Sebastian.Leuzinger@unibas.ch>
Message-ID: <Pine.LNX.4.61.0511231454320.7620@gannet.stats>

Please search the archives.  The issue is that you need the X11 fonts 
installed, both the 75dpi and 100dpi sets.

RSiteSearch("Suse font size") gave me lots of relevant hits.

On Wed, 23 Nov 2005, Sebastian Leuzinger wrote:

> hello, does anybody have an idea why the fontsize is impossible to be changed
> in R version 2.1.0 under linux suse 9.3?
> par(cex.axis=1.5) does not change anything and
> par(cex.axis=2) triggers the error message
>
> X11 font at size 16 could not be loaded
>
> any hints appreciated

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rhelpforsun at gmail.com  Wed Nov 23 16:31:21 2005
From: rhelpforsun at gmail.com (Sh.G. Sun)
Date: Wed, 23 Nov 2005 23:31:21 +0800
Subject: [R] what does the it when there is a zero events in the
 Logistic Regression with glm?
In-Reply-To: <Pine.LNX.4.61.0511220745370.19870@gannet.stats>
References: <43829114.9000106@gmail.com>
	<Pine.LNX.4.61.0511220745370.19870@gannet.stats>
Message-ID: <43848B49.6070409@gmail.com>

Sorry for my stupid mistakes and thanks for your reply.

I just have a study on the occurrence of rare events. Although I 
collected thousands of observations, there are some groups with 0 
events. I think it is too crude to drop those 0-events groups.

I have read some books about logistic regression searched the r-help 
maillist. But I donot find enough information about "separation". Would 
you be so kind to give me some suggestions on "separation" and the 
"better algorithms"?

Thanks!

Sh.G. Sun


Prof Brian Ripley wrote:
> On Tue, 22 Nov 2005, S. Sun wrote:
> 
>> I have a question about the glm.
> 
> Not really: your question is about understanding logistic regressions.
> 
>> When the events of an observation is 0,
>> the logit function on it is Inf. I wonder how the glm solve it.
> 
> Note that logit(0)  = -Inf whereas logit(1) = Inf.
> 
> It is the fitted probabilities which are passed to logit, not the 
> empirical proportions.  Logistic regression is often applied to 
> Bernouilli trials with 0/1 proportions, with nothing to `solve'.
> 
> So the issue only arises if the MLE would give 0 (or 1) fitted values, 
> and it cannot in a logistic regression.  You have here an example in 
> which the MLE does not exist and the log-likelihood does not attain its 
> maximum. Such situations are known as `separation' and it is well-known 
> that there are better algorithms for such problems.
> 
>> An example:
>> Treat Events Trials
>> A     0      50
>> B     7      50
>> C     10     50
>> D     15     50
>> E     17     50
>>
>> Program:
>>
>> treat <- factor(c("A", "B", "C", "D", "E"))
>> events <- c(0, 7, 10, 15, 17)
>> trials <- rep(50, 5)
>> glm(cbind(events, trials-events)~treat, family=binomial)
>>
>> What's wrong with it? And are there better ideas?
> 
> Nothing is `wrong with it'.  It finds fitted values which are very close 
> to the observed values.  You have chosen an inappropriate model and an 
> inappropriate parametrization (see ?relevel).
> 
> I presume you did think something is wrong, but you did not tell us what.
> Please do read the posting guide and try to provide us with enough 
> information to help you.  Also, please do sign your messages indicating 
> who you are and what your background is.  In cases like this the best 
> advice is to suggest asking your supervisor (if you have one) or to read 
> the literature (but what specifically depends on your background).
>



From singyee at ma.hw.ac.uk  Wed Nov 23 16:37:27 2005
From: singyee at ma.hw.ac.uk (Sing-Yee Ling)
Date: Wed, 23 Nov 2005 15:37:27 +0000
Subject: [R] survdiff for Left-truncated and right-censored data
Message-ID: <43848CB7.8070303@ma.hw.ac.uk>

dear all,

I would like to know whether survdiff and survReg function in the 
survival package work for left-truncated and right-censored data.

If not, what other functions can i use to make comparison between two 
survival curves with LTRC data.

thanks for any help given

sing yee



From ripley at stats.ox.ac.uk  Wed Nov 23 16:38:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Nov 2005 15:38:22 +0000 (GMT)
Subject: [R] what does the it when there is a zero events in the
 Logistic Regression with glm?
In-Reply-To: <43848B49.6070409@gmail.com>
References: <43829114.9000106@gmail.com>
	<Pine.LNX.4.61.0511220745370.19870@gannet.stats>
	<43848B49.6070409@gmail.com>
Message-ID: <Pine.LNX.4.61.0511231533270.8024@gannet.stats>

Let me repeat what I said:

> Also, please do sign your messages indicating who you are and what your 
> background is.  In cases like this the best advice is to suggest asking 
> your supervisor (if you have one) or to read the literature (but what 
> specifically depends on your background).

You have still not signed your message so I have no idea of your 
background, nor does `I have read some books about logistic regression' 
help me.  Two accounts are

@Book{Santner.Duffy.89,
   author       = "T. J. Santner and D. E. Duffy",
   title        = "The Statistical Analysis of Discrete Data",
   publisher    = "Springer-Verlag",
   address      = "New York",
   year         = "1989",
   ISBN         = "0-387-97018-5",
   comment      = "Reference from MASS",
}


@Book{Ripley.96,
   author       = "B. D. Ripley",
   title        = "Pattern Recognition and Neural Networks",
   publisher    = "Cambridge University Press",
   address      = "Cambridge",
   year         = "1996",
   ISBN         = "0-521-46086-7",
   comment      = "Reference from MASS",
}


On Wed, 23 Nov 2005, Sh.G. Sun wrote:

> Sorry for my stupid mistakes and thanks for your reply.
>
> I just have a study on the occurrence of rare events. Although I collected 
> thousands of observations, there are some groups with 0 events. I think it is 
> too crude to drop those 0-events groups.
>
> I have read some books about logistic regression searched the r-help 
> maillist. But I donot find enough information about "separation". Would you 
> be so kind to give me some suggestions on "separation" and the "better 
> algorithms"?
>
> Thanks!
>
> Sh.G. Sun
>
>
> Prof Brian Ripley wrote:
>> On Tue, 22 Nov 2005, S. Sun wrote:
>> 
>>> I have a question about the glm.
>> 
>> Not really: your question is about understanding logistic regressions.
>> 
>>> When the events of an observation is 0,
>>> the logit function on it is Inf. I wonder how the glm solve it.
>> 
>> Note that logit(0)  = -Inf whereas logit(1) = Inf.
>> 
>> It is the fitted probabilities which are passed to logit, not the empirical 
>> proportions.  Logistic regression is often applied to Bernouilli trials 
>> with 0/1 proportions, with nothing to `solve'.
>> 
>> So the issue only arises if the MLE would give 0 (or 1) fitted values, and 
>> it cannot in a logistic regression.  You have here an example in which the 
>> MLE does not exist and the log-likelihood does not attain its maximum. Such 
>> situations are known as `separation' and it is well-known that there are 
>> better algorithms for such problems.
>> 
>>> An example:
>>> Treat Events Trials
>>> A     0      50
>>> B     7      50
>>> C     10     50
>>> D     15     50
>>> E     17     50
>>> 
>>> Program:
>>> 
>>> treat <- factor(c("A", "B", "C", "D", "E"))
>>> events <- c(0, 7, 10, 15, 17)
>>> trials <- rep(50, 5)
>>> glm(cbind(events, trials-events)~treat, family=binomial)
>>> 
>>> What's wrong with it? And are there better ideas?
>> 
>> Nothing is `wrong with it'.  It finds fitted values which are very close to 
>> the observed values.  You have chosen an inappropriate model and an 
>> inappropriate parametrization (see ?relevel).
>> 
>> I presume you did think something is wrong, but you did not tell us what.
>> Please do read the posting guide and try to provide us with enough 
>> information to help you.  Also, please do sign your messages indicating who 
>> you are and what your background is.  In cases like this the best advice is 
>> to suggest asking your supervisor (if you have one) or to read the 
>> literature (but what specifically depends on your background).
>> 
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christos at nuverabio.com  Wed Nov 23 16:40:12 2005
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 23 Nov 2005 10:40:12 -0500
Subject: [R] Time-varying coefficients in Cox regression model
Message-ID: <200511231540.jANFejc13718@plus46.host4u.net>

Dear All,

I was wondering whether there is an R/S implementation of Gray's method
(JASA 1992) for B-spline based time-varying coefficients in Cox regression
models.  I have searched the R help archive on the subject and although
there were several matches none seem to have address this point.

Thanks for any help.

-Christos Hatzis



From maechler at stat.math.ethz.ch  Wed Nov 23 17:10:44 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 23 Nov 2005 17:10:44 +0100
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <20051123143323.GB6368@iwr.uni-heidelberg.de>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
	<20051123143323.GB6368@iwr.uni-heidelberg.de>
Message-ID: <17284.38020.14279.158148@stat.math.ethz.ch>

>>>>> "Marc" == Marc Kirchner <marc.kirchner at iwr.uni-heidelberg.de>
>>>>>     on Wed, 23 Nov 2005 14:33:28 +0000 writes:

    >> 
    >> I wonder if we shouldn't polish that a bit and add to R's
    >> standard 'utils' package.
    >> 

    Marc> Hm, I figured out there are (at least) two versions out there, one being
    Marc> the "original" idea and a modification. 

    Marc> === Petr Pikal in 2001 (based on Brian Ripley's idea)==
    Marc> peaks <- function(series, span=3) {
    Marc> z <- embed(series, span)
    Marc> result <- max.col(z) == 1 + span %/% 2
    Marc> result
    Marc> }

    Marc> versus

    Marc> === Petr Pikal in 2004 ==
    Marc> peaks2<-function(series,span=3) {
    Marc> z <- embed(series, span)
    Marc> s <- span%/%2
    Marc> v<- max.col(z) == 1 + s
    Marc> result <- c(rep(FALSE,s),v)
    Marc> result <- result[1:(length(result)-s)]
    Marc> result
    Marc> } 

Thank you, Marc,

    Marc> Comparison shows
    >> peaks(c(1,4,1,1,6,1,5,1,1),3)
    Marc> [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE
    Marc> which is a logical vector for elements 2:N-1 and

    >> peaks2(c(1,4,1,1,6,1,5,1,1),3)
    Marc> [1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
    Marc> which is a logical vector for elements 1:N-2.

    Marc> As I would expect to "lose" (span-1)/2 elements on each side 
    Marc> of the vector, to me the 2001 version feels more natural.

I think for the function to be more useful it the result should
have the original vector length and hence I'd propose to also
pad with FALSE at the upper end.

    Marc> Also, both "suffer" from being non-deterministic in the 
    Marc> multiple-maxima-case (the two 4s here)

yes, of course, because of max.col().  
Note that Venables & Ripley would consider this to be rather a feature.

    Marc> This could (should?) be fixed by modifying the call to max.col()
    Marc> result <- max.col(z, "first") == 1 + span %/% 2;

Actually I think it should become an option, but I'd use "first"
as default.

    Marc> Just my two cents,

Thank you.

Here is my current proposal which also demonstrates why it's
useful to pad with FALSE :

peaks <-function(series, span = 3, ties.method = "first") {
    if((span <- as.integer(span)) %% 2 != 1) stop("'span' must be odd")
    z <- embed(series, span)
    s <- span %/% 2
    v <- max.col(z, ties.method = ties.method) == s + 1:1
    pad <- rep(FALSE, s)
    c(pad, v, pad)
}

y <- c(1,4,1,1,6,1,5,1,1) ; (ii <- which(peaks(y))); y[ii]
##- [1] 2 5 7
##- [1] 4 6 5

set.seed(7)
y <- rpois(100, lambda = 7)
py <- peaks(y)
plot(y, type="o", cex = 1/4, main = "y and peaks(y,3)")
points(seq(y)[py], y[py], col = 2, cex = 1.5)

p7 <- peaks(y,7)
points(seq(y)[p7], y[p7], col = 3, cex = 2)
mtext("peaks(y,7)", col=3)



From maechler at stat.math.ethz.ch  Wed Nov 23 17:18:50 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 23 Nov 2005 17:18:50 +0100
Subject: [R] go with R :-) {Re: finding peaks ..}
In-Reply-To: <17284.28690.71104.813153@stat.math.ethz.ch>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
Message-ID: <17284.38506.925743.36050@stat.math.ethz.ch>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Wed, 23 Nov 2005 14:35:14 +0100 writes:

    MM> I've been asked in private,
    MM> (and am replying BCC to the asker),

    >>> I saw your post on the R-help archives page about the possibility of 
    >>> porting a function from S-Plus called peaks() to R. I am looking for 
    >>> some way to locate peaks in a simple x,y data set, and thought that R 
    >>> might be the way to go.

    MM> "of course" it is the way to go, don't get lost be going
    MM> somewhere else  :-)
    MM> and try

    MM> install.packages("fortune")
    MM> fortune("go with R")

auch!  Two mistakes in such short section.. (thanks, Andy!)
Instead, it should have been


    >>> ..... and thought that R might be the way to go.

  "of course" it is the way to go, don't get lost by going somewhere else  :-)

and try

   install.packages("fortune")
   library(fortune)
   fortune("go with R")

Martin



From dargosch at gmail.com  Wed Nov 23 17:27:22 2005
From: dargosch at gmail.com (Fredrik Karlsson)
Date: Wed, 23 Nov 2005 17:27:22 +0100
Subject: [R] Really supress output from Sweave
Message-ID: <376e97ec0511230827r48941204g3d2ec79e857f4f45@mail.gmail.com>

Hi,

I am using Sweave for chapters in my thesis that contain results.
In the beginning of each chapter, I use this to load libraries I need.

<<init,echo=FALSE,quiet=TRUE>>=
library(gplots)
library(Hmisc)
library(e1071)

@


What I want is, of course, to supress messages written by this code,
but what I get in the end is X-init.tex with this the contents below.
How do I really supress it?

/Fredrik

Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
to see overall documentation.

NOTE:Hmisc no longer redefines [.factor to drop unused levels when
subsetting.  To get the old behavior of Hmisc type dropUnusedLevels().

Attaching package: 'Hmisc'


        The following object(s) are masked from package:gdata :

         reorder.factor


        The following object(s) are masked from package:car :

         recode


        The following object(s) are masked from package:stats :

         ecdf
\end{Soutput}
\begin{Soutput}
Loading required package: class

Attaching package: 'e1071'


        The following object(s) are masked from package:Hmisc :

         impute


        The following object(s) are masked from package:gtools :

         permutations


        The following object(s) are masked from package:foreign :

         read.octave
\end{Soutput}
\begin{Soutput}
Attaching package: 'xtable'


        The following object(s) are masked _by_ .GlobalEnv :

         digits


        The following object(s) are masked from package:Hmisc :

         label label<-
\end{Soutput}
\end{Schunk}



From macq at llnl.gov  Wed Nov 23 17:23:56 2005
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 23 Nov 2005 08:23:56 -0800
Subject: [R] getting started, reading listing and saving data
In-Reply-To: <7d2f562c0511230227y1f8b6581tfc7a338bf6da569e@mail.gmail.com>
References: <7d2f562c0511221738l725b2a2exf388a38af0ad7cab@mail.gmail.com>
	<2a9c000c0511222207m26245dddm2f79c172eec8edd4@mail.gmail.com>
	<7d2f562c0511230227y1f8b6581tfc7a338bf6da569e@mail.gmail.com>
Message-ID: <p06210201bfaa46adf2e4@[128.115.153.6]>

To use head() but restrict it to, say, the 5th through 9th variables, use

     head( x[ , 5:9] )

or variants of that idea. Further examples
    head( x[  , c(5,7,9,13)] )
or
    head( x[  , c('A','vname','var2')])
where 'A', 'vname', and 'var2' are column names.

Try also
     x[1:12, c(1, 4:6)]

I'm sure that methods of extracting portions of an object are in the 
intro documents, but you may not have recognized them for what they 
are. Try

   help('[')

at the prompt for more information.

-Don

At 11:27 AM +0100 11/23/05, Ronnie Babigumira wrote:
>Many thanks to Peter Alspach, Jim Porzak and Murray Pung for the help.
>Peter and Jim, head? and tail? was just what I needed to list a few
>observations. Peter, thanks for pointing out str? to me. I totally
>agree with you on its usefulness.
>
>Murray thanks for file > save workspace (and Peters save.image)
>addresses the third of my concerns
>
>One last question related to head and tails, this works best if you
>have a few variables (columns). Given more, how can I use the
>information on the variable names given after str to list the first
>few few observations for a set of variable.
>
>To make it clear. Say I load a dataset with n variables named v1 to
>vn. I use str(mydata) and I get a list of variable names..
>
>str(x)
>v1 ......
>.  ......
>.  ......
>.  ......
>.  ......
>Vn ......
>
>How do i list the first n observations of say v5 to v9
>
>Many thanks
>
>Ronnie
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From Achim.Zeileis at wu-wien.ac.at  Wed Nov 23 17:32:41 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 23 Nov 2005 17:32:41 +0100 (CET)
Subject: [R] Really supress output from Sweave
In-Reply-To: <376e97ec0511230827r48941204g3d2ec79e857f4f45@mail.gmail.com>
References: <376e97ec0511230827r48941204g3d2ec79e857f4f45@mail.gmail.com>
Message-ID: <Pine.LNX.4.58.0511231732170.10694@thorin.ci.tuwien.ac.at>

On Wed, 23 Nov 2005, Fredrik Karlsson wrote:

> Hi,
>
> I am using Sweave for chapters in my thesis that contain results.
> In the beginning of each chapter, I use this to load libraries I need.

You want results=hide in your options for this chunk.
hth,
Z

> <<init,echo=FALSE,quiet=TRUE>>=
> library(gplots)
> library(Hmisc)
> library(e1071)
>
> @
>
>
> What I want is, of course, to supress messages written by this code,
> but what I get in the end is X-init.tex with this the contents below.
> How do I really supress it?
>
> /Fredrik
>
> Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
> to see overall documentation.
>
> NOTE:Hmisc no longer redefines [.factor to drop unused levels when
> subsetting.  To get the old behavior of Hmisc type dropUnusedLevels().
>
> Attaching package: 'Hmisc'
>
>
>         The following object(s) are masked from package:gdata :
>
>          reorder.factor
>
>
>         The following object(s) are masked from package:car :
>
>          recode
>
>
>         The following object(s) are masked from package:stats :
>
>          ecdf
> \end{Soutput}
> \begin{Soutput}
> Loading required package: class
>
> Attaching package: 'e1071'
>
>
>         The following object(s) are masked from package:Hmisc :
>
>          impute
>
>
>         The following object(s) are masked from package:gtools :
>
>          permutations
>
>
>         The following object(s) are masked from package:foreign :
>
>          read.octave
> \end{Soutput}
> \begin{Soutput}
> Attaching package: 'xtable'
>
>
>         The following object(s) are masked _by_ .GlobalEnv :
>
>          digits
>
>
>         The following object(s) are masked from package:Hmisc :
>
>          label label<-
> \end{Soutput}
> \end{Schunk}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kevin.thorpe at utoronto.ca  Wed Nov 23 17:39:44 2005
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Wed, 23 Nov 2005 11:39:44 -0500
Subject: [R] Really supress output from Sweave
In-Reply-To: <376e97ec0511230827r48941204g3d2ec79e857f4f45@mail.gmail.com>
References: <376e97ec0511230827r48941204g3d2ec79e857f4f45@mail.gmail.com>
Message-ID: <43849B50.6090206@utoronto.ca>

Fredrik Karlsson wrote:
> Hi,
> 
> I am using Sweave for chapters in my thesis that contain results.
> In the beginning of each chapter, I use this to load libraries I need.
> 
> <<init,echo=FALSE,quiet=TRUE>>=

Have you tried <<init,echo=FALSE,quiet=TRUE,results=hide>>=

> library(gplots)
> library(Hmisc)
> library(e1071)
> 
> @
> 
> 
> What I want is, of course, to supress messages written by this code,
> but what I get in the end is X-init.tex with this the contents below.
> How do I really supress it?
> 
> /Fredrik
> 
> Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
> to see overall documentation.
> 
> NOTE:Hmisc no longer redefines [.factor to drop unused levels when
> subsetting.  To get the old behavior of Hmisc type dropUnusedLevels().
> 
> Attaching package: 'Hmisc'
> 
> 
>         The following object(s) are masked from package:gdata :
> 
>          reorder.factor
> 
> 
>         The following object(s) are masked from package:car :
> 
>          recode
> 
> 
>         The following object(s) are masked from package:stats :
> 
>          ecdf
> \end{Soutput}
> \begin{Soutput}
> Loading required package: class
> 
> Attaching package: 'e1071'
> 
> 
>         The following object(s) are masked from package:Hmisc :
> 
>          impute
> 
> 
>         The following object(s) are masked from package:gtools :
> 
>          permutations
> 
> 
>         The following object(s) are masked from package:foreign :
> 
>          read.octave
> \end{Soutput}
> \begin{Soutput}
> Attaching package: 'xtable'
> 
> 
>         The following object(s) are masked _by_ .GlobalEnv :
> 
>          digits
> 
> 
>         The following object(s) are masked from package:Hmisc :
> 
>          label label<-
> \end{Soutput}
> \end{Schunk}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.946.3297



From dlvanbrunt at gmail.com  Wed Nov 23 17:57:15 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Wed, 23 Nov 2005 11:57:15 -0500
Subject: [R] TryCatch() with read.csv("http://...")
Message-ID: <d332d3e10511230857v7d01c173gb4f7a19d61a9b891@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051123/164fd278/attachment.pl

From p.dalgaard at biostat.ku.dk  Wed Nov 23 18:00:59 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Nov 2005 18:00:59 +0100
Subject: [R] go with R :-) {Re: finding peaks ..}
In-Reply-To: <17284.38506.925743.36050@stat.math.ethz.ch>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
	<17284.38506.925743.36050@stat.math.ethz.ch>
Message-ID: <x2k6ezqoxg.fsf@viggo.kubism.ku.dk>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

>    install.packages("fortune")
>    library(fortune)
>    fortune("go with R")

Ackchewly....

> library(fortune)
Error in library(fortune) : there is no package called 'fortune'
> library(fortunes)
> fortune("go with R")

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dylan.beaudette at gmail.com  Wed Nov 23 18:01:08 2005
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Wed, 23 Nov 2005 09:01:08 -0800
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <17284.38020.14279.158148@stat.math.ethz.ch>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
	<20051123143323.GB6368@iwr.uni-heidelberg.de>
	<17284.38020.14279.158148@stat.math.ethz.ch>
Message-ID: <952f46a2de1daac26e698251907db057@gmail.com>

On Nov 23, 2005, at 8:10 AM, Martin Maechler wrote:

>>>>>> "Marc" == Marc Kirchner <marc.kirchner at iwr.uni-heidelberg.de>
>>>>>>     on Wed, 23 Nov 2005 14:33:28 +0000 writes:
>
>>>
>>> I wonder if we shouldn't polish that a bit and add to R's
>>> standard 'utils' package.
>>>
>
>     Marc> Hm, I figured out there are (at least) two versions out 
> there, one being
>     Marc> the "original" idea and a modification.
>
>     Marc> === Petr Pikal in 2001 (based on Brian Ripley's idea)==
>     Marc> peaks <- function(series, span=3) {
>     Marc> z <- embed(series, span)
>     Marc> result <- max.col(z) == 1 + span %/% 2
>     Marc> result
>     Marc> }
>
>     Marc> versus
>
>     Marc> === Petr Pikal in 2004 ==
>     Marc> peaks2<-function(series,span=3) {
>     Marc> z <- embed(series, span)
>     Marc> s <- span%/%2
>     Marc> v<- max.col(z) == 1 + s
>     Marc> result <- c(rep(FALSE,s),v)
>     Marc> result <- result[1:(length(result)-s)]
>     Marc> result
>     Marc> }
>
> Thank you, Marc,
>
>     Marc> Comparison shows
>>> peaks(c(1,4,1,1,6,1,5,1,1),3)
>     Marc> [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE
>     Marc> which is a logical vector for elements 2:N-1 and
>
>>> peaks2(c(1,4,1,1,6,1,5,1,1),3)
>     Marc> [1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
>     Marc> which is a logical vector for elements 1:N-2.
>
>     Marc> As I would expect to "lose" (span-1)/2 elements on each side
>     Marc> of the vector, to me the 2001 version feels more natural.
>
> I think for the function to be more useful it the result should
> have the original vector length and hence I'd propose to also
> pad with FALSE at the upper end.
>

Hi, been lurking for a while, and thought that I would chime in.

I have similar need: x,y data with many "peaks" - finding them with R 
would be a nice feature. However, as mentioned above the ability to 
find more than one peak would be especially helpful. In addition 
preserving the length of the input vector would help with linking peak 
locations to their real index.

In my case the data is generated by an X-ray diffraction machine: 
x-axis in Degrees, y-axis in relative intensity. The data looks like 
this:
http://casoilresource.lawr.ucdavis.edu/drupal/node/71


>     Marc> Also, both "suffer" from being non-deterministic in the
>     Marc> multiple-maxima-case (the two 4s here)
>
> yes, of course, because of max.col().
> Note that Venables & Ripley would consider this to be rather a feature.
>
>     Marc> This could (should?) be fixed by modifying the call to 
> max.col()
>     Marc> result <- max.col(z, "first") == 1 + span %/% 2;
>
> Actually I think it should become an option, but I'd use "first"
> as default.
>

I would second that.

>     Marc> Just my two cents,
>
> Thank you.
>
> Here is my current proposal which also demonstrates why it's
> useful to pad with FALSE :
>
> peaks <-function(series, span = 3, ties.method = "first") {
>     if((span <- as.integer(span)) %% 2 != 1) stop("'span' must be odd")
>     z <- embed(series, span)
>     s <- span %/% 2
>     v <- max.col(z, ties.method = ties.method) == s + 1:1
>     pad <- rep(FALSE, s)
>     c(pad, v, pad)
> }
>
> y <- c(1,4,1,1,6,1,5,1,1) ; (ii <- which(peaks(y))); y[ii]
> ##- [1] 2 5 7
> ##- [1] 4 6 5
>
> set.seed(7)
> y <- rpois(100, lambda = 7)
> py <- peaks(y)
> plot(y, type="o", cex = 1/4, main = "y and peaks(y,3)")
> points(seq(y)[py], y[py], col = 2, cex = 1.5)
>
> p7 <- peaks(y,7)
> points(seq(y)[p7], y[p7], col = 3, cex = 2)
> mtext("peaks(y,7)", col=3)

Thanks for working on this, as I would imagine there are other lurkers 
out there who are waiting for a solution to this problem.

--
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341



From lisawang at uhnres.utoronto.ca  Wed Nov 23 18:03:01 2005
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Wed, 23 Nov 2005 12:03:01 -0500
Subject: [R] wilcoxon.test?
Message-ID: <4384A0C5.8001A011@uhnres.utoronto.ca>

Hello there,

I would like to do a Wilcoxon matched pairs signed rank sum test in R
and tried the function wilcoxon.test. Is it in the "base" library? If
not, please let me know which library it is in. 

Thank you in advance for your help

Lisa Wang
Biostatistician 
Princess Margaret Hospital
Toronto, Ca



From henric.nilsson at statisticon.se  Wed Nov 23 18:12:07 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 23 Nov 2005 18:12:07 +0100 (CET)
Subject: [R] 1st derivative {mgcv} gam smooth
In-Reply-To: <c08359f90511222043we8f695bgec6eeff94aa340e6@mail.gmail.com>
References: <c08359f90511222043we8f695bgec6eeff94aa340e6@mail.gmail.com>
Message-ID: <4330.83.253.15.61.1132765927.squirrel@poisson.statisticon.se>


On On, 2005-11-23, 05:43, Tomas skrev:
> Dear R-hep,
>           I'm trying to get the first derivative of a smooth from a gam
> model like:
> model<-gam(y~s(x,bs="cr", k=5)+z) and need the derivative: ds(x)/dx. Since
> coef(model) give me all the parameters, including the parameters of the
> basis, I just need the 1st derivative of the basis s(x).1, s(x).2, s(x).3,
> s(x).4. If the basis were generated with the function spline.des() from
> the
> spline library I would be able to get the derivative of the basis and
> would
> be pretty much done. (spline.des() has the option of computing 1st
> derivative)
>
> The difficulty I'm having is that I can't get how the basis were
> generated.
> If I define an "s" object like: object<-s(x,bs="cr",k=5) and use
> smooth.construct() to generate the basis, it gives 5 basis but the gam
> object "model" gives only 4 s(x).i elements. It generalizes for different

Try

> model2 <- gam(y ~ s(x, bs = "cr", k = 5) + z, control =
gam.control(absorb.cons = FALSE))
> coef(model2)
(Intercept)         z     s(x).1     s(x).2     s(x).3    s(x).4    s(x).5
  43.675698 -2.685739 -43.305376 -34.325047 -14.015483 20.155349 71.490557

See ?gam.control

HTH,
Henric


> values of k. See the example:
>
> library(mgcv)
> y<-c(1,5,6,12,13,14,45,42,56,68,89,120)
> z<-c(rep(0,5),rep(1,7))
> x<-c(seq(1:12))
> data<-data.frame(y,x,z)
>
> model<-gam(y~s(x,bs="cr", k=5)+z)
>
> object<-s(x,bs="cr", k=5)
> basis<-smooth.construct(object,data,knots=NULL)
>
> summary(model)
> .
> .
> .
> Parametric coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)   43.676      5.492   7.953 6.66e-05 ***
> z             -2.686      9.041  -0.297    0.775
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Approximate significance of smooth terms:
>       edf Est.rank     F  p-value
> s(x) 2.53     4.00 35.18 6.48e-05 ***
>
> coef(model)
> (Intercept)           z      s(x).1      s(x).2      s(x).3      s(x).4
>  43.6756983  -2.6857394 -20.9429503  -0.6333857  33.5374463  84.8726537
>
> dim(basis$X)
> [1] 12  5
>
> #Here are the basis
> basis$X
>
>               [,1]        [,2]        [,3]        [,4]         [,5]
>  [1,]  1.000000000  0.00000000  0.00000000  0.00000000  0.000000000
>  [2,]  0.551840721  0.55522164 -0.13523666  0.03380917 -0.005634861
>  [3,]  0.180959536  0.93527960 -0.14682838  0.03670709 -0.006117849
>  [4,] -0.035821616  0.96624450  0.08768917 -0.02173446  0.003622411
>  [5,] -0.076875604  0.62353762  0.54792315 -0.11350220  0.018917033
>  [6,] -0.027771815  0.17264141  0.93603091 -0.09708061  0.016180101
>  [7,]  0.016180101 -0.09708061  0.93603091  0.17264141 -0.027771815
>  [8,]  0.018917033 -0.11350220  0.54792315  0.62353762 -0.076875604
>  [9,]  0.003622411 -0.02173446  0.08768917  0.96624450 -0.035821616
> [10,] -0.006117849  0.03670709 -0.14682838  0.93527960  0.180959536
> [11,] -0.005634861  0.03380917 -0.13523666  0.55522164  0.551840721
> [12,]  0.000000000  0.00000000  0.00000000  0.00000000  1.000000000
>
> #and their knots
>
> basis$xp
> [1]  1.00  3.75  6.50  9.25 12.00
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Nov 23 18:17:12 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Nov 2005 18:17:12 +0100
Subject: [R] attributes of a data.frame
In-Reply-To: <Pine.LNX.4.61.0511231415200.7559@gannet.stats>
References: <43836A33.4050405@yorku.ca>
	<Pine.LNX.4.61.0511231415200.7559@gannet.stats>
Message-ID: <x2fypnqo6f.fsf@viggo.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> Why do you have names like 'pctx723' in the first place?
> 
> I have never had a difficulty with using informative column names whereas 
> you seem to require the extra complication of `variable labels'.
> Now we have `` and allow _ in syntactic names it is even easier than it 
> was.

But do we want to have `Maternal height (inches)` as a variable name?
Imagine a multiple regression with a few dozen such terms as
predictors.  

We do have the comment attribute (as in help(comment)), but we're not
using it much. There could be some reason in using a such a comment,
if present, and possibly optionally, to name dimnames in tables and
for x/y labels in plots. I'm not sure it is easy to do without
disrupting existing code, though. I notice in particular that the
comment attribute is retained according to rules that might not be
what users expect.

 
> On Tue, 22 Nov 2005, Michael Friendly wrote:
> 
> > It's hard for me to resist dipping my oar into this...
> >
> > Variable labels are so generally useful, both in documenting a
> > dataset (what was 'pctx723' again?) and in producing readable
> > output and graphs that it is a shame they are not provided in
> > base R.  If they were (and were used in print and plot methods,
> > when available) it would avoid a lot of the necessity to specify
> > xlab= and ylab= in graphs, or, perhaps worse, ending up with
> > pctx723 as the label in your presentation.
> >
> > -Michael
> >
> >> On 11/21/2005 2:51 PM, Adrian DUSA wrote:
> >>
> >>>> On Monday 21 November 2005 22:41, Duncan Murdoch wrote:
> >>>
> >>>>>> [...snip...]
> >>>>>> Not all dataframes have the variable.labels attribute.  I'm guessing
> >>>>>> you've installed some contributed package to add them, or are importing
> >>>>>> an SPSS datafile using read.spss.  So don't expect varlab() or
> >>>>>> variable.labels() function to be a standard R function.
> >>>
> >>>>
> >>>> Aa-haa... of course you are right: I read them via read.spss. I understand.
> >>>> Now, just to the sake of it, would it be wrong to make it standard?
> >>>> Is there a special reason not to?
> >>
> >>
> >> I think it's just that the R core developers don't see the need for
> >> them.  If something is worth documenting, then you should write an .Rd
> >> file or a vignette about it, and that gives you more flexibility than a
> >> one line label.
> >>
> >> I think there are definitely developers out there who disagree with this
> >> point of view, and I'm pretty sure I've seen a contributed package that
> >> offered support for this, but I can't remember which one right now.  So
> >> that's another reason why it's not in the base:  it doesn't need to be,
> >> you can just go find and install that contributed package!
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ccleland at optonline.net  Wed Nov 23 18:20:39 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 23 Nov 2005 12:20:39 -0500
Subject: [R] wilcoxon.test?
In-Reply-To: <4384A0C5.8001A011@uhnres.utoronto.ca>
References: <4384A0C5.8001A011@uhnres.utoronto.ca>
Message-ID: <4384A4E7.4060609@optonline.net>

help.search("wilcoxon") shows you wilcox.test() in the stats package.

Lisa Wang wrote:
> Hello there,
> 
> I would like to do a Wilcoxon matched pairs signed rank sum test in R
> and tried the function wilcoxon.test. Is it in the "base" library? If
> not, please let me know which library it is in. 
> 
> Thank you in advance for your help
> 
> Lisa Wang
> Biostatistician 
> Princess Margaret Hospital
> Toronto, Ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ggrothendieck at gmail.com  Wed Nov 23 18:29:03 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 23 Nov 2005 12:29:03 -0500
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <17284.38020.14279.158148@stat.math.ethz.ch>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
	<20051123143323.GB6368@iwr.uni-heidelberg.de>
	<17284.38020.14279.158148@stat.math.ethz.ch>
Message-ID: <971536df0511230929q74c48caax3892feec51fdcafe@mail.gmail.com>

On 11/23/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "Marc" == Marc Kirchner <marc.kirchner at iwr.uni-heidelberg.de>
> >>>>>     on Wed, 23 Nov 2005 14:33:28 +0000 writes:
>
>    >>
>    >> I wonder if we shouldn't polish that a bit and add to R's
>    >> standard 'utils' package.
>    >>
>
>    Marc> Hm, I figured out there are (at least) two versions out there, one being
>    Marc> the "original" idea and a modification.
>
>    Marc> === Petr Pikal in 2001 (based on Brian Ripley's idea)==
>    Marc> peaks <- function(series, span=3) {
>    Marc> z <- embed(series, span)
>    Marc> result <- max.col(z) == 1 + span %/% 2
>    Marc> result
>    Marc> }
>
>    Marc> versus
>
>    Marc> === Petr Pikal in 2004 ==
>    Marc> peaks2<-function(series,span=3) {
>    Marc> z <- embed(series, span)
>    Marc> s <- span%/%2
>    Marc> v<- max.col(z) == 1 + s
>    Marc> result <- c(rep(FALSE,s),v)
>    Marc> result <- result[1:(length(result)-s)]
>    Marc> result
>    Marc> }
>
> Thank you, Marc,
>
>    Marc> Comparison shows
>    >> peaks(c(1,4,1,1,6,1,5,1,1),3)
>    Marc> [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE
>    Marc> which is a logical vector for elements 2:N-1 and
>
>    >> peaks2(c(1,4,1,1,6,1,5,1,1),3)
>    Marc> [1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
>    Marc> which is a logical vector for elements 1:N-2.
>
>    Marc> As I would expect to "lose" (span-1)/2 elements on each side
>    Marc> of the vector, to me the 2001 version feels more natural.
>
> I think for the function to be more useful it the result should
> have the original vector length and hence I'd propose to also
> pad with FALSE at the upper end.
>
>    Marc> Also, both "suffer" from being non-deterministic in the
>    Marc> multiple-maxima-case (the two 4s here)
>
> yes, of course, because of max.col().
> Note that Venables & Ripley would consider this to be rather a feature.
>
>    Marc> This could (should?) be fixed by modifying the call to max.col()
>    Marc> result <- max.col(z, "first") == 1 + span %/% 2;
>
> Actually I think it should become an option, but I'd use "first"
> as default.
>
>    Marc> Just my two cents,
>
> Thank you.
>
> Here is my current proposal which also demonstrates why it's
> useful to pad with FALSE :
>
> peaks <-function(series, span = 3, ties.method = "first") {
>    if((span <- as.integer(span)) %% 2 != 1) stop("'span' must be odd")
>    z <- embed(series, span)
>    s <- span %/% 2
>    v <- max.col(z, ties.method = ties.method) == s + 1:1
>    pad <- rep(FALSE, s)
>    c(pad, v, pad)
> }
>
> y <- c(1,4,1,1,6,1,5,1,1) ; (ii <- which(peaks(y))); y[ii]
> ##- [1] 2 5 7
> ##- [1] 4 6 5
>
> set.seed(7)
> y <- rpois(100, lambda = 7)
> py <- peaks(y)
> plot(y, type="o", cex = 1/4, main = "y and peaks(y,3)")
> points(seq(y)[py], y[py], col = 2, cex = 1.5)
>
> p7 <- peaks(y,7)
> points(seq(y)[p7], y[p7], col = 3, cex = 2)
> mtext("peaks(y,7)", col=3)

I think ties are still problems with this approach
as:

set.seed(1)
peaks( c(1,2,2,2,3), 3 )

gives a peak in the 2,2,2 stretch.

Also NA would seem to be a better pad than false or maybe
it should be specifiable including whether there is padding at
all.   The zoo rapply and rollmax functions which can also be used
to specify a similar naive peak function have na.pad=
and align= arguments.  Also any peaks function should
be generic so that various time series classes can implement
their own methods and in the case of irregularly spaced series
note that there are two possibilities for span, the time distance
and the number of points, and they are not the same.

It might also be nice to be able to get back peaks, troughs or
both via  1,0,-1 in the output.



From ripley at stats.ox.ac.uk  Wed Nov 23 18:29:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Nov 2005 17:29:01 +0000 (GMT)
Subject: [R] attributes of a data.frame
In-Reply-To: <x2fypnqo6f.fsf@viggo.kubism.ku.dk>
References: <43836A33.4050405@yorku.ca>
	<Pine.LNX.4.61.0511231415200.7559@gannet.stats>
	<x2fypnqo6f.fsf@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0511231724530.13601@gannet.stats>

On Wed, 23 Nov 2005, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> Why do you have names like 'pctx723' in the first place?
>>
>> I have never had a difficulty with using informative column names whereas
>> you seem to require the extra complication of `variable labels'.
>> Now we have `` and allow _ in syntactic names it is even easier than it
>> was.
>
> But do we want to have `Maternal height (inches)` as a variable name?
> Imagine a multiple regression with a few dozen such terms as
> predictors.

I have actually done something very like this today (it was (m), (km) 
and (ha), but you get the idea).  If you are going to have them in the 
printout of the results you may as well have them in the formula.

> We do have the comment attribute (as in help(comment)), but we're not
> using it much. There could be some reason in using a such a comment,
> if present, and possibly optionally, to name dimnames in tables and
> for x/y labels in plots. I'm not sure it is easy to do without
> disrupting existing code, though. I notice in particular that the
> comment attribute is retained according to rules that might not be
> what users expect.
>
>
>> On Tue, 22 Nov 2005, Michael Friendly wrote:
>>
>>> It's hard for me to resist dipping my oar into this...
>>>
>>> Variable labels are so generally useful, both in documenting a
>>> dataset (what was 'pctx723' again?) and in producing readable
>>> output and graphs that it is a shame they are not provided in
>>> base R.  If they were (and were used in print and plot methods,
>>> when available) it would avoid a lot of the necessity to specify
>>> xlab= and ylab= in graphs, or, perhaps worse, ending up with
>>> pctx723 as the label in your presentation.
>>>
>>> -Michael
>>>
>>>> On 11/21/2005 2:51 PM, Adrian DUSA wrote:
>>>>
>>>>>> On Monday 21 November 2005 22:41, Duncan Murdoch wrote:
>>>>>
>>>>>>>> [...snip...]
>>>>>>>> Not all dataframes have the variable.labels attribute.  I'm guessing
>>>>>>>> you've installed some contributed package to add them, or are importing
>>>>>>>> an SPSS datafile using read.spss.  So don't expect varlab() or
>>>>>>>> variable.labels() function to be a standard R function.
>>>>>
>>>>>>
>>>>>> Aa-haa... of course you are right: I read them via read.spss. I understand.
>>>>>> Now, just to the sake of it, would it be wrong to make it standard?
>>>>>> Is there a special reason not to?
>>>>
>>>>
>>>> I think it's just that the R core developers don't see the need for
>>>> them.  If something is worth documenting, then you should write an .Rd
>>>> file or a vignette about it, and that gives you more flexibility than a
>>>> one line label.
>>>>
>>>> I think there are definitely developers out there who disagree with this
>>>> point of view, and I'm pretty sure I've seen a contributed package that
>>>> offered support for this, but I can't remember which one right now.  So
>>>> that's another reason why it's not in the base:  it doesn't need to be,
>>>> you can just go find and install that contributed package!
>>
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
> -- 
>   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From tschoenhoff at gmail.com  Wed Nov 23 18:39:43 2005
From: tschoenhoff at gmail.com (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Wed, 23 Nov 2005 18:39:43 +0100
Subject: [R] wilcoxon.test?
In-Reply-To: <4384A0C5.8001A011@uhnres.utoronto.ca>
References: <4384A0C5.8001A011@uhnres.utoronto.ca>
Message-ID: <5ad2dec0511230939n3a9edd81t@mail.gmail.com>

Hello,


2005/11/23, Lisa Wang <lisawang at uhnres.utoronto.ca>:
> Hello there,
>
> I would like to do a Wilcoxon matched pairs signed rank sum test in R
> and tried the function wilcoxon.test. Is it in the "base" library? If
> not, please let me know which library it is in.
>
> Thank you in advance for your help

help.search("wilcoxon")

reveals that it's part of stats package



regards



From rlist.10.phftt at xoxy.net  Wed Nov 23 18:45:35 2005
From: rlist.10.phftt at xoxy.net (rlist.10.phftt@xoxy.net)
Date: Wed, 23 Nov 2005 12:45:35 -0500
Subject: [R] Dancing lissajous
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E69FE07C@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E69FE07C@HERMES.demogr.mpg.de>
Message-ID: <4384AABF.3000603@yahoo.com>

I should have thought about that--sorry.  Try the Coral Cache: 
http://geocities.com.nyud.net:8090/robsteele/

Rob

Rau Roland - Rau at demogr.mpg.de wrote:

>>http://www.geocities.com/robsteele/
>>    
>>
>
>
>this is what pops up when I try to access your site:
>"The web site you are trying to access has exceeded its allocated data
>transfer."
>
>Maybe you can just post the code and we can run it ourselves?
>
>Best,
>Roland
>  
>



From sfalcon at fhcrc.org  Wed Nov 23 19:00:43 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 23 Nov 2005 10:00:43 -0800
Subject: [R] TryCatch() with read.csv("http://...")
In-Reply-To: <d332d3e10511230857v7d01c173gb4f7a19d61a9b891@mail.gmail.com>
	(David L. Van Brunt's message of "Wed,
	23 Nov 2005 11:57:15 -0500")
References: <d332d3e10511230857v7d01c173gb4f7a19d61a9b891@mail.gmail.com>
Message-ID: <m2fypnp7lg.fsf@fhcrc.org>

Hi David,

On 23 Nov 2005, dlvanbrunt at gmail.com wrote:
> Since the read is inside a "FOR" loop, (pulling in and analyzing
> different data each time) I've been trying to use TryCatch() to pull
> in the file if it's OK, but to jump out to the next iteration of the
> loop otherwise (using "next"). 

I think this is a common approach.  Here's an example using try (not
tryCatch):

doRead <- function() {
    x <- runif(1)
    if (x < 0.2)
      stop("failure in doRead")
    x
}

iters <- 20
vals <- NULL
for (i in seq(length=iters)) {
    val <- try(doRead(), silent=TRUE)
    if (inherits(val, "try-error"))
      next
    else
      vals <- c(vals, val)
}


length(vals)
[1] 16   ## there were 4 errors


> This seems like it should be obvious, but I've read through the ?
> files and tried everything my smooth little brain can come up
> with... no luck. I still end up jumping out of the entire script if
> I hit an error page.

tryCatch and the associated tools are many things (very powerful, very
cool), but I think they are not obvious :-)

I'm not sure if you can use tryCatch directly... probably so, but it
isn't obvious to me ;-)  However, here is an example using
withCallingHandlers and withRestarts.

vals <- NULL
iters <- 20
withCallingHandlers({
    for (i in seq(length=iters)) {
        val <- withRestarts(doRead(),
                            skipError=function() return(NULL))
        if (!is.null(val))
          vals <- c(vals, val)
    }},
                    error=function(e) invokeRestart("skipError"))


What's happening here as I understand it:

1. Establish a restart with

      val <- withRestarts(doRead(),
                          skipError=function() return(NULL))

   This says, "mark this spot with a recovery function called
   'skipError'".  This code does not handle the error.  But marks a
   spot where recovery occur.

2. Establish a handler for errors.  This is the withCallingHandlers
   call.  When an error occurs the function specified by the error arg
   gets called.  In this case, we invoke the restart function created
   above.  The restart function is evaluated "in the right place".

Why this is good: it decouples the handling of errors from the
recovery from errors.  With try(), you have to catch and handle the
error all at once.  With the withCallingHandlers/withRestarts
approach, you can use the same inner code (the same restarts) and
build wrappers that do different things.

HTH,

+ seth


PS: I found the following chapter to be helpful:
http://www.gigamonkeys.com/book/beyond-exception-handling-conditions-and-restarts.html



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Nov 23 19:15:38 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 23 Nov 2005 13:15:38 -0500
Subject: [R] finding peaks in a simple dataset with R
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD504F413EA@us-arlington-0668.mail.saic.com>



>> I am looking for some way to locate peaks in a simple x,y data set.

See my 'msc.peaks.find' function in 'caMassClass', it has a simple peak
finding algorithm.

Jarek Tuszynski



From tlumley at u.washington.edu  Wed Nov 23 19:21:09 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 23 Nov 2005 10:21:09 -0800 (PST)
Subject: [R] survdiff for Left-truncated and right-censored data
In-Reply-To: <43848CB7.8070303@ma.hw.ac.uk>
References: <43848CB7.8070303@ma.hw.ac.uk>
Message-ID: <Pine.LNX.4.63a.0511231019540.7612@homer24.u.washington.edu>

On Wed, 23 Nov 2005, Sing-Yee Ling wrote:

> dear all,
>
> I would like to know whether survdiff and survReg function in the
> survival package work for left-truncated and right-censored data.

The survdiff and survfit functions do, as does coxph.  There is no survReg 
in the survival package, but there is survreg, and it does not handle left 
truncation.

 	-thomas

> If not, what other functions can i use to make comparison between two
> survival curves with LTRC data.
>
> thanks for any help given
>
> sing yee
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From sfalcon at fhcrc.org  Wed Nov 23 19:29:59 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 23 Nov 2005 10:29:59 -0800
Subject: [R] modifying code in contributed libraries - changes from
	versions 1.* to 2.*
In-Reply-To: <Pine.LNX.4.61.0511231358330.7559@gannet.stats> (Brian Ripley's
	message of "Wed, 23 Nov 2005 14:10:44 +0000 (GMT)")
References: <OF5F187570.28A70F75-ON872570C1.005432AE-872570C1.0056274A@usgs.gov>
	<43834AE0.2050304@statistik.uni-dortmund.de>
	<m2lkzgy5gq.fsf@ziti.local> <m03bln675w.fsf@bar.nemo-project.org>
	<Pine.LNX.4.61.0511231358330.7559@gannet.stats>
Message-ID: <m27jazp68o.fsf@fhcrc.org>

On 23 Nov 2005, ripley at stats.ox.ac.uk wrote:

> I didn't understand this either.  Perhaps Seth could explain it.
> [I've added Seth back to the recipients.]

Easily, I was confused and wrong about this.  Somehow I must have been
thinking about installed packages, but I don't know why.

+ seth



From ross at biostat.ucsf.edu  Wed Nov 23 20:46:16 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 23 Nov 2005 11:46:16 -0800
Subject: [R] Customizing the package build process
In-Reply-To: <43843B41.3020805@statistik.uni-dortmund.de>
References: <1132694032.2951.139.camel@iron.psg.net>
	<43843B41.3020805@statistik.uni-dortmund.de>
Message-ID: <1132775176.2949.170.camel@iron.psg.net>

On Wed, 2005-11-23 at 10:49 +0100, Uwe Ligges wrote:
> Ross Boylan wrote:
....
> > P.S. Previous list postings advised that R CMD install was a better way
> > to produce binaries than R CMD build --binary.  The former command
> > doesn't seem to have any options for making binaries; has that facility
> > been removed?
> 
> No, you can use:
> 
> R CMD INSTALL --build
> 
> 
> Uwe Ligges
I was looking at R CMD install --help.  I didn't realize the upper lower
case versions of install differed.  That's a little confusing.



From fparlamis at mac.com  Wed Nov 23 21:02:19 2005
From: fparlamis at mac.com (Parlamis Franklin)
Date: Wed, 23 Nov 2005 10:02:19 -1000
Subject: [R] date/time arithmetic
Message-ID: <5C840B2C-24A1-4062-B717-1ECB863FC9E5@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051123/06d5fc9f/attachment.pl

From deepayan.sarkar at gmail.com  Wed Nov 23 21:28:06 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 23 Nov 2005 14:28:06 -0600
Subject: [R] Customizing the package build process
In-Reply-To: <1132775176.2949.170.camel@iron.psg.net>
References: <1132694032.2951.139.camel@iron.psg.net>
	<43843B41.3020805@statistik.uni-dortmund.de>
	<1132775176.2949.170.camel@iron.psg.net>
Message-ID: <eb555e660511231228od39d86cs3a8b2364b3c4b6bb@mail.gmail.com>

On 11/23/05, Ross Boylan <ross at biostat.ucsf.edu> wrote:
> On Wed, 2005-11-23 at 10:49 +0100, Uwe Ligges wrote:
> > Ross Boylan wrote:
> ....
> > > P.S. Previous list postings advised that R CMD install was a better way
> > > to produce binaries than R CMD build --binary.  The former command
> > > doesn't seem to have any options for making binaries; has that facility
> > > been removed?
> >
> > No, you can use:
> >
> > R CMD INSTALL --build
> >
> >
> > Uwe Ligges
> I was looking at R CMD install --help.  I didn't realize the upper lower
> case versions of install differed.  That's a little confusing.

R --help does not list 'install' as a valid command, and on my system

R CMD install --help

basically just runs

install --help

-Deepayan



From murdoch at stats.uwo.ca  Wed Nov 23 21:57:32 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 23 Nov 2005 15:57:32 -0500
Subject: [R] Customizing the package build process
In-Reply-To: <1132775176.2949.170.camel@iron.psg.net>
References: <1132694032.2951.139.camel@iron.psg.net>	<43843B41.3020805@statistik.uni-dortmund.de>
	<1132775176.2949.170.camel@iron.psg.net>
Message-ID: <4384D7BC.9040008@stats.uwo.ca>

Ross Boylan wrote:
> On Wed, 2005-11-23 at 10:49 +0100, Uwe Ligges wrote:
> 
>>Ross Boylan wrote:
> 
> ....
> 
>>>P.S. Previous list postings advised that R CMD install was a better way
>>>to produce binaries than R CMD build --binary.  The former command
>>>doesn't seem to have any options for making binaries; has that facility
>>>been removed?
>>
>>No, you can use:
>>
>>R CMD INSTALL --build
>>
>>
>>Uwe Ligges
> 
> I was looking at R CMD install --help.  I didn't realize the upper lower
> case versions of install differed.  That's a little confusing.

That's Unix for you.

What R CMD does on Unix is to set up several environment variables, then 
runs the command. So something like

R CMD ls

will run the ls command.

On Windows, the search is limited to the R bin directory, which seems 
more reasonable to me.

Duncan Murdoch



From ggrothendieck at gmail.com  Wed Nov 23 22:51:17 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 23 Nov 2005 16:51:17 -0500
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <971536df0511230929q74c48caax3892feec51fdcafe@mail.gmail.com>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
	<20051123143323.GB6368@iwr.uni-heidelberg.de>
	<17284.38020.14279.158148@stat.math.ethz.ch>
	<971536df0511230929q74c48caax3892feec51fdcafe@mail.gmail.com>
Message-ID: <971536df0511231351k52e1cd89o8c790b6ad6a1d1c5@mail.gmail.com>

One idea might be, rather than have a peaks function, enhance
rle so that it optionally produces a third component with the
peak information, perhaps 1, 0, -1 for peak, neither and trough.
This would avoid any problems with ties since the output of rle is
based on runs.

On 11/23/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 11/23/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> > >>>>> "Marc" == Marc Kirchner <marc.kirchner at iwr.uni-heidelberg.de>
> > >>>>>     on Wed, 23 Nov 2005 14:33:28 +0000 writes:
> >
> >    >>
> >    >> I wonder if we shouldn't polish that a bit and add to R's
> >    >> standard 'utils' package.
> >    >>
> >
> >    Marc> Hm, I figured out there are (at least) two versions out there, one being
> >    Marc> the "original" idea and a modification.
> >
> >    Marc> === Petr Pikal in 2001 (based on Brian Ripley's idea)==
> >    Marc> peaks <- function(series, span=3) {
> >    Marc> z <- embed(series, span)
> >    Marc> result <- max.col(z) == 1 + span %/% 2
> >    Marc> result
> >    Marc> }
> >
> >    Marc> versus
> >
> >    Marc> === Petr Pikal in 2004 ==
> >    Marc> peaks2<-function(series,span=3) {
> >    Marc> z <- embed(series, span)
> >    Marc> s <- span%/%2
> >    Marc> v<- max.col(z) == 1 + s
> >    Marc> result <- c(rep(FALSE,s),v)
> >    Marc> result <- result[1:(length(result)-s)]
> >    Marc> result
> >    Marc> }
> >
> > Thank you, Marc,
> >
> >    Marc> Comparison shows
> >    >> peaks(c(1,4,1,1,6,1,5,1,1),3)
> >    Marc> [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE
> >    Marc> which is a logical vector for elements 2:N-1 and
> >
> >    >> peaks2(c(1,4,1,1,6,1,5,1,1),3)
> >    Marc> [1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
> >    Marc> which is a logical vector for elements 1:N-2.
> >
> >    Marc> As I would expect to "lose" (span-1)/2 elements on each side
> >    Marc> of the vector, to me the 2001 version feels more natural.
> >
> > I think for the function to be more useful it the result should
> > have the original vector length and hence I'd propose to also
> > pad with FALSE at the upper end.
> >
> >    Marc> Also, both "suffer" from being non-deterministic in the
> >    Marc> multiple-maxima-case (the two 4s here)
> >
> > yes, of course, because of max.col().
> > Note that Venables & Ripley would consider this to be rather a feature.
> >
> >    Marc> This could (should?) be fixed by modifying the call to max.col()
> >    Marc> result <- max.col(z, "first") == 1 + span %/% 2;
> >
> > Actually I think it should become an option, but I'd use "first"
> > as default.
> >
> >    Marc> Just my two cents,
> >
> > Thank you.
> >
> > Here is my current proposal which also demonstrates why it's
> > useful to pad with FALSE :
> >
> > peaks <-function(series, span = 3, ties.method = "first") {
> >    if((span <- as.integer(span)) %% 2 != 1) stop("'span' must be odd")
> >    z <- embed(series, span)
> >    s <- span %/% 2
> >    v <- max.col(z, ties.method = ties.method) == s + 1:1
> >    pad <- rep(FALSE, s)
> >    c(pad, v, pad)
> > }
> >
> > y <- c(1,4,1,1,6,1,5,1,1) ; (ii <- which(peaks(y))); y[ii]
> > ##- [1] 2 5 7
> > ##- [1] 4 6 5
> >
> > set.seed(7)
> > y <- rpois(100, lambda = 7)
> > py <- peaks(y)
> > plot(y, type="o", cex = 1/4, main = "y and peaks(y,3)")
> > points(seq(y)[py], y[py], col = 2, cex = 1.5)
> >
> > p7 <- peaks(y,7)
> > points(seq(y)[p7], y[p7], col = 3, cex = 2)
> > mtext("peaks(y,7)", col=3)
>
> I think ties are still problems with this approach
> as:
>
> set.seed(1)
> peaks( c(1,2,2,2,3), 3 )
>
> gives a peak in the 2,2,2 stretch.
>
> Also NA would seem to be a better pad than false or maybe
> it should be specifiable including whether there is padding at
> all.   The zoo rapply and rollmax functions which can also be used
> to specify a similar naive peak function have na.pad=
> and align= arguments.  Also any peaks function should
> be generic so that various time series classes can implement
> their own methods and in the case of irregularly spaced series
> note that there are two possibilities for span, the time distance
> and the number of points, and they are not the same.
>
> It might also be nice to be able to get back peaks, troughs or
> both via  1,0,-1 in the output.
>



From tom at maladmin.com  Wed Nov 23 17:59:21 2005
From: tom at maladmin.com (tom wright)
Date: Wed, 23 Nov 2005 11:59:21 -0500
Subject: [R] TryCatch() with read.csv("http://...")
In-Reply-To: <d332d3e10511230857v7d01c173gb4f7a19d61a9b891@mail.gmail.com>
References: <d332d3e10511230857v7d01c173gb4f7a19d61a9b891@mail.gmail.com>
Message-ID: <1132765161.4819.156.camel@localhost.localdomain>

How are you trying to break the loop?
The next statement should move to the next in the sequence (from the
Introduction To R manual)

On Wed, 2005-23-11 at 11:57 -0500, David L. Van Brunt, Ph.D. wrote:
> Hi, folks!
> 
> I'm trying to pull in data using read.csv("my URL goes here"), and it really
> works fantastically. Amazing to pull in live data right off the internet,
> into RAM, and get busy...
> 
> however...
> 
> occasionally there is a server problem, or the data are not up  yet, and
> instead of pushing through a nice CSV file, the server sends a 404 "Not
> Found" page...
> 
> Since the read is inside a "FOR" loop, (pulling in and analyzing different
> data each time) I've been trying to use TryCatch() to pull in the file if
> it's OK, but to jump out to the next iteration of the loop otherwise (using
> "next"). I don't want to exit the script, I just want to skip this iteration
> and move on. This seems like it should be obvious, but I've read through the
> ? files and tried everything my smooth little brain can come up with... no
> luck. I still end up jumping out of the entire script if I hit an error
> page.
> 
> Does anyone have some example syntax that I might examine or try out?
> 
> I'm feeling pretty thick here...
> 
> --
> ---------------------------------------
> David L. Van Brunt, Ph.D.
> mailto:dlvanbrunt at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tom at maladmin.com  Wed Nov 23 18:03:27 2005
From: tom at maladmin.com (tom wright)
Date: Wed, 23 Nov 2005 12:03:27 -0500
Subject: [R] help on list
In-Reply-To: <438486A9.7020205@cea.fr>
References: <438486A9.7020205@cea.fr>
Message-ID: <1132765407.4819.160.camel@localhost.localdomain>

the dwt function returns an object with 'slots'
dwt$d1
dwt$d2
...
which contain the partial wave coefficients
and dwt$s1 which contains the leftover factors

Please excuse my terminology here, I know what I'm talking about..



On Wed, 2005-23-11 at 16:11 +0100, both jean-pierre wrote:
> Hi,
> New to R,
> 
> having done a wavelet analysis i got a result as list.
> The problem I have is : how can I acces to a given element of the list.
> 
> here is what I get from my variable scale_d28
> 
>  >typeof(scale_d28)
> [1] "list"
>  > length(scale_d28)
> [1] 1
> 
> and scale_d28 is a list of 100000 double values?
> 
> Thanks
> 
> 
>



From dylan.beaudette at gmail.com  Wed Nov 23 23:47:17 2005
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Wed, 23 Nov 2005 14:47:17 -0800
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD504F413EA@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD504F413EA@us-arlington-0668.mail.saic.com>
Message-ID: <200511231447.17551.dylan.beaudette@gmail.com>

On Wednesday 23 November 2005 10:15 am, Tuszynski, Jaroslaw W. wrote:
> >> I am looking for some way to locate peaks in a simple x,y data set.
>
> See my 'msc.peaks.find' function in 'caMassClass', it has a simple peak
> finding algorithm.
>
> Jarek Tuszynski
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Jarek,

Thanks for the tip. I was able to install the caMassClass package and all of 
its dependancies. In addition, I was able to run the examples on the manual 
pages.

However, The format of the input data to the 'msc.peaks.find' function is not 
apparent to me. In its simplest form, my data looks something like this:

2.00 233
2.04 220
...
11.60 540
12.00 600   <-- a peak!
12.04 450
...

Here is an example R session, trying out the function you suggested:

#importing my data like this:
X <- read.table("input.dat", header=TRUE)

#from the example:
Peaks = msc.peaks.find(X)

#errors with:
Error in sort(x, partial = unique(c(lo, hi))) :
        'x' must be atomic


Also: I have tried one of the functions ( 'getPeaks' ) listed on the 
'msc.peaks.find' manual page, however I am still having a problem with the 
format of my data vs. what the function is expecting.

#importing my data like this:
X <- read.table("input.dat", header=TRUE)

#setup an output file for peak information
peakfile <- paste("peakinfo.csv", sep="/")

#run the analysis:
getPeaks(X,peakfile)

#errors with:
Error in area/max(area) : non-numeric argument to binary operator
In addition: Warning message:
no finite arguments to max; returning -Inf

any ideas would be greatly appreciated!

-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341



From amla20 at hotmail.com  Thu Nov 24 00:12:38 2005
From: amla20 at hotmail.com (=?iso-8859-1?B?QWxtYSBDcmlzdGluYSBO+vFleiBkZWwgVG9ybw==?=)
Date: Wed, 23 Nov 2005 23:12:38 +0000
Subject: [R] Linear and cuadratic effects
Message-ID: <BAY109-F1547A5AC06B6C31C748DFFCC550@phx.gbl>


   I'd  like  to know how to obtain a table of anova of a 3^2 desing with
   linear  and  cuadratic  sum  of  squares  because if I use "aov", this
   comand only bring me the linear sum of squares:

   > A<-rep(c(rep(-1,3),rep(0,3),rep(1,3)),2)
   > B<-rep(rep(c(-1,0,1),3),2)
   >
   y<-c(1.5,1.4,0.8,3.5,2.9,1.8,4,3.8,2.7,1.2,1.3,1.2,3.2,2.5,2,4.2,3.4,3
   )
   > anova(aov(y~(A*B)))


   Analysis of Variance Table
   Response: y
                 Df  Sum Sq Mean Sq  F value    Pr(>F)
   A             1 15.6408 15.6408 204.0109  9.711e-10 ***
   B             1   3.1008  3.1008   40.4457   1.769e-05 ***
   A:B          1   0.4050  0.4050     5.2826      0.03746 *
   Residuals 14  1.0733  0.0767
   ---
   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



   Thanks a lot ,
   Cristina


From maj at stats.waikato.ac.nz  Thu Nov 24 00:12:48 2005
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 24 Nov 2005 12:12:48 +1300
Subject: [R] Simple introduction to Bayesian Statistics
Message-ID: <4384F770.3000809@stats.waikato.ac.nz>

Joshua N Pritikin wrote:

> I made it through the first chapter ("Background") of Bayesian Data
> Analysis by Gelman, et al.  Conceptually, the Bayesian approach
> appeals to me and my curiosity is piqued.  However, the next chapter
> was much too terse. The math is daunting.  Where can I find a gentle
> introduction?  Or which math book(s) do I need to read first?
> 
> On page 27, there is mention of introductory books including Bayesian
> Methods by Jeff Gill (2002).  Just for fun, I took a look at this book
> to see whether I could get through it.  Chapter 1 was inviting, but
> again, the math got too sophisticated starting from chapter 2.

Try
Introduction to Bayesian Statistics
William M. Bolstad
ISBN: 0-471-27020-2

by my colleague Bill Bolstad. This is written for a course targeting 
bright first or second year students and assumes very little background.

Cheers,  Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862



From nhepburn at ualberta.ca  Thu Nov 24 00:13:25 2005
From: nhepburn at ualberta.ca (nhepburn)
Date: Wed, 23 Nov 2005 16:13:25 -0700
Subject: [R] adding variables to a data set/combining two data sets
Message-ID: <000401c5f083$82fa4930$7000a8c0@Natasha>

I have a couple of data sets that I want to combine into one data frame.
One set contains a number of records on individual observations and includes
a geographic descriptor called dacode.  The dacode is not unique in that
table.  The other table contains a number of socio-economic variables for
each of the geographic areas identified in the other table.  This second
table also includes a variable called dacode and I have also used dacode for
the rownames.  I want to pull a number of the variables from the
socioeconomic status data into the first table but I'm having no luck.
Here's what I have tried so far:
 
#first attempt
 
grade6DA$ses1 <- sesdata$ses1[as.character(grade6DA$dacode)]
 
All that this does is create a new column in the grade6DA data set and calls
it ses1 but then fills it with NA.
 
 
#second attempt
grade6DA$ses1 <- sesdata$ses1[grade6DA$dacode]
 
 
Neither approach has worked so I've ended up combining the tables in a MySQL
database and then importing back into R.  However, it would be much easier
if I could just manipulate the variables in R rather than going through
MySQL everytime I want to try something new.  I've looked in the R-manuals
but did not see anything about this - but I could have been looking in the
wrong places.  Any ideas on how to accomplish what I am trying to do or
advice on where to find the info would be greatly appreciated.
 
Cheers,
Neil
 
=================================================
Neil Hepburn, PhD Candidate
Department of Economics
University of Alberta
 
email nhepburn at ualberta.ca
URL http://www.ualberta.ca/~nhepburn



From statsfay at hotmail.com  Thu Nov 24 00:39:22 2005
From: statsfay at hotmail.com (Gao Fay)
Date: Wed, 23 Nov 2005 18:39:22 -0500
Subject: [R] Find main effect in 2-way ANOVA
Message-ID: <BAY104-F37C04F59B3F62A09E0D7ECD0550@phx.gbl>

Hi,

I use anova() to find interaction effect and then I need to find data with 
main effect from those data with interaction effect. How to do that?

I used : anova(lm(data~factor1*factor2)), then select data with interaction 
effect. Then I need to select those data also with main effect of factor1 
or factor2, from previous selected data. How to do that? Many thanks for 
your time on my quesiton!

Fay



From petr.pikal at precheza.cz  Thu Nov 24 07:36:38 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 24 Nov 2005 07:36:38 +0100
Subject: [R] getting started, reading listing and saving data
In-Reply-To: <7d2f562c0511230444p2742319aj4358dbb92f93bacd@mail.gmail.com>
References: <5ad2dec0511230252n3e710aedm@mail.gmail.com>
Message-ID: <43856D86.7148.53152A@localhost>


   On 23 Nov 2005 at 13:44, Ronnie Babigumira wrote:
   Date sent:         &#16
   From:         &#16   <r.babigumira at gmail.com>

   To:          
   Subject:             and saving data

   [DEL:  > Thomas many thanks,  though str(myo   I want, it :DEL]

   [DEL:  >  prompted  me  to try head(myobject [3:4])    really close :DEL]

   [DEL:  >  to  what  I  want,  I  get  the  first few obse   variables 3 and 4 :DEL]

   [DEL:  >  in  my  data. What I would like to do howe   same thing :DEL]

   [DEL:  > but with the variable names explicitly (   use :DEL]

   [DEL: > something like varname3 and varname4) :DEL]

   [DEL: > :DEL]

   [DEL: > Any ideas :DEL]

   Hi

   qouote names

   head(myobject [, c("some.name", "some.other.name
   HTH

   Petr

   [DEL: > :DEL]

   [DEL:  >  Again,  many  thanks to all on the list wh   promise I am :DEL]

   [DEL:  >  frantically  trying to read up everything   and bring :DEL]

   [DEL:  > myself upto speed so I will not be consu   a lot of :DEL]

   [DEL: > read the manual sort of questions) :DEL]

   [DEL: > :DEL]

   [DEL: > :DEL]

   [DEL: > :DEL]

   [DEL:  >  On  11/23/05,  Thomas  Sch?nhoff <tsch   wrote: :DEL]

   [DEL: > > Hello, :DEL]

   [DEL: > > :DEL]

   [DEL:  >  >  2005/11/23, Ronnie Babigumira <r   :DEL]

   [DEL: > > > Many thanks to Peter Alspach,    for the :DEL]

   [DEL:  >  >  >  help.  Peter and Jim, head? and   needed to :DEL]

   [DEL:  > > > list a few observations. Peter   str? to :DEL]

   [DEL: > > > me. I totally agree with you o
   [DEL: > > > :DEL]

   [DEL:  >  >  >  Murray  thanks for file > sa   save.image) :DEL]

   [DEL: > > > addresses the third of my conc
   [DEL: > > > :DEL]

   [DEL:  > > > One last question related to h   best if :DEL]

   [DEL:  > > > you have a few variables (colu   I use the :DEL]

   [DEL:  >  >  >  information on the variable na   list the :DEL]

   [DEL: > > > first few few observations for
   [DEL: > > > :DEL]

   [DEL:  >  > > To make it clear. Say I load a   named v1 :DEL]

   [DEL:  >  >  >  to vn. I use str(mydata) and I   names.. :DEL]

   [DEL: > > > :DEL]

   [DEL: > > > str(x) :DEL]

   [DEL: > > > v1 ...... :DEL]

   [DEL: > > > .  ...... :DEL]

   [DEL: > > > .  ...... :DEL]

   [DEL: > > > .  ...... :DEL]

   [DEL: > > > .  ...... :DEL]

   [DEL: > > > Vn ...... :DEL]

   [DEL: > > > :DEL]

   [DEL:  >  > > How do i list the first n obse   :DEL]

   [DEL: > > :DEL]

   [DEL:  >  >  Not  sure  if  this  is  what  you  are  lo   str(myobject[ 5:9]) :DEL]

   [DEL: > > :DEL]

   [DEL: > > regards :DEL]

   [DEL: > > :DEL]

   [DEL: > :DEL]

   [DEL: > ________________________________________
   [DEL: > R-help at stat.math.ethz.ch mailing list :DEL]

   [DEL: > https://stat.ethz.ch/mailman/listinfo/r-
   [DEL: > PLEASE do read the posting guide! :DEL] 
   [DEL: > http://www.R-project.org/posting-guide.h
   Petr Pikal

   petr.pikal at precheza.cz


From xmeng at capitalbio.com  Thu Nov 24 08:31:28 2005
From: xmeng at capitalbio.com (=?gb2312?B?w8/QwA==?=)
Date: Thu, 24 Nov 2005 15:31:28 +0800
Subject: [R] a question
Message-ID: <332817488.03032@capitalbio.com>

Hello sir:
Here's a question:
x<-1:5
y<-c(2,1,4)

I wanna get the different elements between x & y,i.e. 3,5.

How can I get the result(3,5) via R function?

Thanks a lot for your help.

My best!







------------------------------
*******************************************
Xin Meng 
Capitalbio Corporation
National Engineering Research Center 
for Beijing Biochip Technology 
BioPharma-informatics & Software Dept. 
Research Engineer
Tel: +86-10-80715888/80726868-6444 
Fax: +86-10-80726790
Email£ºxmeng at capitalbio.com 
Address:18 Life Science Parkway, 
Changping District, Beijing 102206, China



From ripley at stats.ox.ac.uk  Thu Nov 24 08:27:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Nov 2005 07:27:23 +0000 (GMT)
Subject: [R] Linear and cuadratic effects
In-Reply-To: <BAY109-F1547A5AC06B6C31C748DFFCC550@phx.gbl>
References: <BAY109-F1547A5AC06B6C31C748DFFCC550@phx.gbl>
Message-ID: <Pine.LNX.4.61.0511240719090.31387@gannet.stats>

1) You need to declare ordered factors for your terms:

A <- ordered(A)
B <- ordered(B)

2) You want summary() not anova():

> summary(aov(y~(A*B)))
             Df  Sum Sq Mean Sq  F value    Pr(>F)
A            2 15.9433  7.9717 170.8214 6.953e-08
B            2  3.1633  1.5817  33.8929 6.461e-05
A:B          4  0.6933  0.1733   3.7143    0.0473
Residuals    9  0.4200  0.0467

and (if I understand you correctly) to use its optional argument 'split'

summary(aov(y~(A*B)), split = list(A = list(L = 1, Q = 2),
                                    B = list(L = 1, Q = 2)))

(L for 'linear', Q for 'quadratic')

or possibly with 'expand.split = FALSE'. See ?summary.aov.


On Wed, 23 Nov 2005, Alma Cristina N??ez del Toro wrote:

>
>   I'd  like  to know how to obtain a table of anova of a 3^2 desing with
>   linear  and  cuadratic  sum  of  squares  because if I use "aov", this
>   comand only bring me the linear sum of squares:
>
>   > A<-rep(c(rep(-1,3),rep(0,3),rep(1,3)),2)
>   > B<-rep(rep(c(-1,0,1),3),2)
>   >
>   y<-c(1.5,1.4,0.8,3.5,2.9,1.8,4,3.8,2.7,1.2,1.3,1.2,3.2,2.5,2,4.2,3.4,3
>   )
>   > anova(aov(y~(A*B)))
>
>
>   Analysis of Variance Table
>   Response: y
>                 Df  Sum Sq Mean Sq  F value    Pr(>F)
>   A             1 15.6408 15.6408 204.0109  9.711e-10 ***
>   B             1   3.1008  3.1008   40.4457   1.769e-05 ***
>   A:B          1   0.4050  0.4050     5.2826      0.03746 *
>   Residuals 14  1.0733  0.0767
>   ---
>   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ggrothendieck at gmail.com  Thu Nov 24 08:29:02 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Nov 2005 02:29:02 -0500
Subject: [R] a question
In-Reply-To: <332817488.03032@capitalbio.com>
References: <332817488.03032@capitalbio.com>
Message-ID: <971536df0511232329v3d3ef909xb56d835b412056e2@mail.gmail.com>

Look at:

?setdiff

On 11/24/05, å­Ÿæ¬£ <xmeng at capitalbio.com> wrote:
> Hello sir:
> Here's a question:
> x<-1:5
> y<-c(2,1,4)
>
> I wanna get the different elements between x & y,i.e. 3,5.
>
> How can I get the result(3,5) via R function?
>
> Thanks a lot for your help.
>
> My best!
>
>
>
>
>
>
>
> ------------------------------
> *******************************************
> Xin Meng
> Capitalbio Corporation
> National Engineering Research Center
> for Beijing Biochip Technology
> BioPharma-informatics & Software Dept.
> Research Engineer
> Tel: +86-10-80715888/80726868-6444
> Fax: +86-10-80726790
> EmailÂ£Âºxmeng at capitalbio.com
> Address:18 Life Science Parkway,
> Changping District, Beijing 102206, China
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From chenh at seas.upenn.edu  Thu Nov 24 08:51:18 2005
From: chenh at seas.upenn.edu (Huang-Wen Chen)
Date: Thu, 24 Nov 2005 02:51:18 -0500
Subject: [R] Inversion function of dnorm ?
Message-ID: <200511240749.jAO7nAMB017717@lion.seas.upenn.edu>

Hi,

In R, qnorm is the inversion function of pnorm. (c.d.f)
But there is no inversion function for dnorm. (p.d.f).
Is there any easy (and quick) way to compute the inversion function of p.d.f
in R ?
Thanks ...



From maechler at stat.math.ethz.ch  Thu Nov 24 09:31:33 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 24 Nov 2005 09:31:33 +0100
Subject: [R] Inversion function of dnorm ?
In-Reply-To: <200511240749.jAO7nAMB017717@lion.seas.upenn.edu>
References: <200511240749.jAO7nAMB017717@lion.seas.upenn.edu>
Message-ID: <17285.31333.255677.370595@stat.math.ethz.ch>

>>>>> "Huang-Wen" == Huang-Wen Chen <chenh at seas.upenn.edu>
>>>>>     on Thu, 24 Nov 2005 02:51:18 -0500 writes:

    Huang-Wen> Hi,
    Huang-Wen> In R, qnorm is the inversion function of pnorm. (c.d.f)
    Huang-Wen> But there is no inversion function for dnorm. (p.d.f).
    Huang-Wen> Is there any easy (and quick) way to compute the inversion function of p.d.f
    Huang-Wen> in R ?

Hmm,  please do think again!
About what definition of "inversion" have you been thinking?  The usual
one is surely undefined for a non-monotone function like dnorm() !



From Elizabeth.Boakes at ioz.ac.uk  Thu Nov 24 09:53:29 2005
From: Elizabeth.Boakes at ioz.ac.uk (Elizabeth Boakes)
Date: Thu, 24 Nov 2005 08:53:29 -0000
Subject: [R] AIC in lmer when using PQL
Message-ID: <41E1ED29E5E8E34BBDD8B82CFA1A9D04083DB3@ZSL26.zsl.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/59a14551/attachment.pl

From kalar1 at wp.pl  Thu Nov 24 01:32:55 2005
From: kalar1 at wp.pl (Kuba)
Date: Thu, 24 Nov 2005 01:32:55 +0100
Subject: [R] source code
Message-ID: <43850a376bd4d@wp.pl>

Hi,
  I cannot find source code of package "stats". Do you know where 
I can find it?

Regards
Jakub Kalarus

----------------------------------------------------
Andrzejki - wr????by tylko dla pa??!
Katarzynki - wr????by tylko dla kawaler??w!
Sprawd?? - zanim b??dzie za p????no ;)
http://klik.wp.pl/?adr=http%3A%2F%2Fadv.reklama.wp.pl%2Fas%2Fhandrzejki.html&sid=581



From Antje.Doering at komdat.com  Thu Nov 24 09:37:20 2005
From: Antje.Doering at komdat.com (=?iso-8859-1?Q?Antje_D=F6ring?=)
Date: Thu, 24 Nov 2005 09:37:20 +0100
Subject: [R] How to save an object list as txt?
Message-ID: <686C1FDE894539418C5668E5E6DE12DE0511C9@muc-exch-tmp.komdat.intern>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/7e71ed94/attachment.pl

From szlevine at nana.co.il  Thu Nov 24 10:13:48 2005
From: szlevine at nana.co.il (Stephen)
Date: Thu, 24 Nov 2005 11:13:48 +0200
Subject: [R] Survreg Weibull lambda and p
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD66B@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/17cd120f/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Thu Nov 24 10:33:40 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 24 Nov 2005 10:33:40 +0100
Subject: [R] AIC in lmer when using PQL
References: <41E1ED29E5E8E34BBDD8B82CFA1A9D04083DB3@ZSL26.zsl.org>
Message-ID: <00ee01c5f0da$275b2a70$0540210a@www.domain>

note that although PQL is the default method in lmer() for GLMMs, the 
recent version of the function allow also for Laplace or adaptive 
Gauss-Hermite approximations. In these cases it might be reasonable to 
compute AIC values depending on how good the approximation to the 
likelihood is; however, the use of AIC in mixed models can be tricky 
depending on the focus of your analysis, check e.g.,

Vaida, F. and Blanchard, S. (2005). Conditional Akaike information for 
mixed-effects models, Biometrika, 92, 351-370.

Regarding inference, I'd rely mainly on LRTs instead of Wald type 
p-values.


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Elizabeth Boakes" <Elizabeth.Boakes at ioz.ac.uk>
To: <r-help-request at stat.math.ethz.ch>; <r-help at stat.math.ethz.ch>
Sent: Thursday, November 24, 2005 9:53 AM
Subject: [R] AIC in lmer when using PQL


>I am analysing binomial data using a generalised mixed effects model. 
>I
> understand that if I use glmmPQL it is not appropriate to compare 
> AIC
> values to obtain a minimum adequate model.
>
>
>
> I am assuming that this means it is also inappropriate to use AIC 
> values
> from lmer since, when analysing binomial data, lmer also uses PQL
> methods.  However, I wasn't sure so please could somebody clarify 
> this
> for me.
>
>
>
> I was also wondering how best to assess your minimum adequate model
> without AIC values?  Do you simply have to rely on the p values
> associated with the t-values/z-values?
>
>
>
> Thanks very much.
>
> Elizabeth Boakes
>
>
>
> Elizabeth Boakes
> PhD Student
> Institute of Zoology
> Regent's Park
> London NW1 4RY
> tel: 020 7449 6621
>
>
>
>
>
> _________________________________________________________________________
> This e-mail has been sent in confidence to the named\ > ad...{{dropped}}



From ccleland at optonline.net  Thu Nov 24 11:27:19 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 24 Nov 2005 05:27:19 -0500
Subject: [R] source code
In-Reply-To: <43850a376bd4d@wp.pl>
References: <43850a376bd4d@wp.pl>
Message-ID: <43859587.4010302@optonline.net>

https://svn.r-project.org/R/trunk/src/library/stats/

Kuba wrote:
> Hi,
>   I cannot find source code of package "stats". Do you know where 
> I can find it?
> 
> Regards
> Jakub Kalarus
> 
> ----------------------------------------------------
> Andrzejki - wr????by tylko dla pa??!
> Katarzynki - wr????by tylko dla kawaler??w!
> Sprawd?? - zanim b??dzie za p????no ;)
> http://klik.wp.pl/?adr=http%3A%2F%2Fadv.reklama.wp.pl%2Fas%2Fhandrzejki.html&sid=581
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From aconesa at ivia.es  Thu Nov 24 11:33:45 2005
From: aconesa at ivia.es (Ana Conesa)
Date: Thu, 24 Nov 2005 11:33:45 +0100
Subject: [R] hamming distance
Message-ID: <6.1.0.6.0.20051124113250.01a90f68@master.ivia.es>


   Hi,
   Does anyone know an R function to impute hamming distance?
   Thanks
   Ana

      O@@@@@  &nb     @@@O@@O@   Centro de Gen?mica
     @O@@@@O@   Instituto Valenciano de Investigaciones Agrarias (IVIA)
     @@@O@@@@   Carretera Moncada - Naquera, Km. 4,5
      @@@@O@    46113 Moncada (Valencia) SPAIN
        ||  &        ||  &

From bianca.vieru at free.fr  Thu Nov 24 12:13:24 2005
From: bianca.vieru at free.fr (Bianca Vieru- Dimulescu)
Date: Thu, 24 Nov 2005 12:13:24 +0100
Subject: [R] Chi-squared test
Message-ID: <4385A054.1000305@free.fr>

Hello,

I'm trying to calculate a chi-squared test to see if my data are 
different from the theoretical distribution or not:

chisq.test(rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 
60),c(80,80,80, 80, 80, 80, 80, 80, 80, 80, 80, 80)))

      Pearson's Chi-squared test

data:  rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60), c(80, 
80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80))
X-squared = 17.6, df = 11, p-value = 0.09142

Is this correct? If I'm doing the same thing using Excel I obtained a 
different value of p.. (1.65778E-14)

Thanks a lot,
Bianca



From S.J.Eglen at damtp.cam.ac.uk  Thu Nov 24 12:15:40 2005
From: S.J.Eglen at damtp.cam.ac.uk (Stephen Eglen)
Date: Thu, 24 Nov 2005 11:15:40 +0000
Subject: [R] Crop white border for PDF output
In-Reply-To: <Pine.OSX.4.61.0511221449430.26628@cirrus.aue.aau.dk>
References: <Pine.OSX.4.61.0511192056590.3976@cirrus.local>
	<73dae3060511220103g7355b021x246e9c4be8aa363a@mail.gmail.com>
	<Pine.OSX.4.61.0511221047270.1635@cirrus.aue.aau.dk>
	<1132666943.19636.1.camel@localhost.localdomain>
	<Pine.OSX.4.61.0511221449430.26628@cirrus.aue.aau.dk>
Message-ID: <17285.41180.274466.386251@notch.damtp.cam.ac.uk>

Claus Atzenbeck writes:
 > On Tue, 22 Nov 2005, Marc Schwartz wrote:

 > The disadvantage with pdfcrop is that it takes an additional step:
 > First, I have to produce the diagrams, then, I have to call pdfcrop on
 > them to crop the white space. pdfcrop does a perfect job here: It crops
 > the complete white space around a diagram automatically without having
 > me to name some concrete numbers. It would be very nice if R would have
 > an option that would allow to do me to do this for graphics that will be
 > included in documents later.

Does pdfcrop take input from stdin?  I normally have a similar problem
when making postscript files.  Rather than worry about the excess size
of the border, I do the following:

  postscript(file="|psfbb> w81s1_m623.ps",
             width=7, height=4, onefile=FALSE, horiz=F)
  plot ...
  dev.off()

Where my script, psfbb, crops to a tight bounding box.  Maybe
you could try 

  pdf(file='|pdfcrop > yourfile.pdf')
  ...
  dev.off()


My script, psfbb, is available from:
  http://www.damtp.cam.ac.uk/user/sje30/postscript/psfbb

Stephen



From vivek.satsangi at gmail.com  Thu Nov 24 12:27:18 2005
From: vivek.satsangi at gmail.com (Vivek Satsangi)
Date: Thu, 24 Nov 2005 06:27:18 -0500
Subject: [R] Suggested add to the documentation for the identify() function
Message-ID: <bcb171920511240327l8932a30t630a688759cb350@mail.gmail.com>

Folks,

1. Is there a more appropriate list (r-devel?) for posting such
suggestions? I am a newbie to R, and doubtless will have some
suggestions for the documentation -- some good, others not quite so. I
would actually like to help give back to the community (I was
motivated by Prof. Ripley's 2001 talk in which he had commented that
open source software users rarely give back anything.) -- but I know
very little right now, so I might make things worse in some cases.

2. I would like to suggest adding the following to the examples
section of the help on the identify function:

Suppose you want to be able to remove some points from your analysis.

In its simplest form, Identify() will give you the row number of the
points that you  mark. Try running the following 3 commands:

>plot.new()
>plot(1:10, 1:10)
>identify(x=1:10, y=1:10, n=10)

What  you will observe is that when you click on the points of the
plot , it will show  the row number of those points.

If you  are using some other function to produce your plot, identify
can work with that  as well....Just use the same vectors in the
arguments to plot and identify.

Next,  you can remove those outlier points from your data using -
>  x1 <- x[-c(3,5,7), ]

In  this case x is your orignal matrix and 3,5,7 are the row numbers
shown by  identify() for your outlier data points.

See also: Negative subscripts


3. My most sincere apologies for sending HTML in my email to the
distribution list the last time.

-- Vivek Satsangi



From 042045003 at fudan.edu.cn  Thu Nov 24 13:01:53 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Thu, 24 Nov 2005 20:01:53 +0800
Subject: [R] what's the meaning of these in R-lang?
Message-ID: <0IQG0020QJM4XT@mail.fudan.edu.cn>

In this case the environment contains the variables local to the function, and its enclosure is the environment of the enclosing function.(R-lang:p11)

I want to know if the "enclosing function" means the closure of the function? for example ,if I call function mean(),and the create an environment,say e1,then the enclosure of e1 is the namespace of base package? Right?

Thank you!



From ehlers at math.ucalgary.ca  Thu Nov 24 13:07:52 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Thu, 24 Nov 2005 05:07:52 -0700
Subject: [R] Chi-squared test
In-Reply-To: <4385A054.1000305@free.fr>
References: <4385A054.1000305@free.fr>
Message-ID: <4385AD18.6010507@math.ucalgary.ca>


Bianca Vieru- Dimulescu wrote:

> Hello,
> 
> I'm trying to calculate a chi-squared test to see if my data are 
> different from the theoretical distribution or not:
> 
> chisq.test(rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 
> 60),c(80,80,80, 80, 80, 80, 80, 80, 80, 80, 80, 80)))
> 
>       Pearson's Chi-squared test
> 
> data:  rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60), c(80, 
> 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80))
> X-squared = 17.6, df = 11, p-value = 0.09142
> 
> Is this correct? If I'm doing the same thing using Excel I obtained a 
> different value of p.. (1.65778E-14)
> 
> Thanks a lot,
> Bianca

It would be unusual to have 12 observed frequencies all equal to 80.
So I'm guessing that you have a 12-category variable and want to
test its fit to a discrete uniform distribution. I assume that your
frequencies are

x <- c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60)

Then just use

chisq.test(x)

(see the help page).

(If those 80's are expected cell frequencies, they should sum to
sum(x) = 851.)

I don't know what Excel does.

Peter

Peter Ehlers
University of Calgary

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Arne.Muller at sanofi-aventis.com  Thu Nov 24 13:11:45 2005
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Thu, 24 Nov 2005 13:11:45 +0100
Subject: [R] data frames and factors
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF473@CRBSMXSUSR04>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/dd866f89/attachment.pl

From aleszib at gmail.com  Thu Nov 24 13:12:43 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Thu, 24 Nov 2005 13:12:43 +0100
Subject: [R] Survreg Weibull lambda and p
References: <E76EB96029DCAE4A9CB967D7F6712D1DBFD66B@NANAMAILBACK1.nanamail.co.il>
Message-ID: <065601c5f0f0$61143a20$0100a8c0@ALES>

Firstly, I assume that your variable is a numeric one. For seperat values p 
and lambda for diferent categories, you should convert it to factor.

However, this has no effect in your case, since you have only 2 categories. 
You can have only one p and lambda for a variable with only 2 values. The 
model can only evaluate the diference, which is what you got, assuming that 
your groups are coded in such a way, that the difference between the codes 
is 1.

Best,
Ales Ziberna



----- Original Message ----- 
From: "Stephen" <szlevine at nana.co.il>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, November 24, 2005 10:13 AM
Subject: [R] Survreg Weibull lambda and p


> Hi All,
>
>
>
> I have conducted the following survival analysis which appears to be OK
>
> (thanks BRipley for solving my earlier problem).
>
>
>
>> surv.mod1 <- survreg( Surv(timep1, relall6)~randgrpc, data=Dataset,
> dist="weibull", scale = 1)
>
>> summary(surv.mod1)
>
>
>
> Call:
>
> survreg(formula = Surv(timep1, relall6) ~ randgrpc, data = Dataset,
>
>    dist = "weibull", scale = 1)
>
>            Value Std. Error     z         p
>
> (Intercept)  7.36      0.259 28.42 1.27e-177
>
> randgrpc    -0.59      0.156 -3.80  1.47e-04
>
>
>
> Scale fixed at 1
>
>
>
> Weibull distribution
>
> Loglik(model)= -1268.6   Loglik(intercept only)= -1276
>
>        Chisq= 14.72 on 1 degrees of freedom, p= 0.00012
>
> Number of Newton-Raphson Iterations: 5
>
> n= 400
>
>
>
>> version
>
>         _
>
> platform i386-pc-mingw32
>
> arch     i386
>
> os       mingw32
>
> system   i386, mingw32
>
> status
>
> major    2
>
> minor    1.1
>
> year     2005
>
> month    06
>
> day      20
>
> language R
>
>>
>
> I emailed this output to a colleague and received an email requesting
> for the 2 groups (randgrpc in the code)
>
> the 'Weibull lambda and p values' in the analysis. I checked the
> mailings for direction but to no avail.
>
> Could someone please provide direction as how to extract the Weibull
> "lambda" and "p" for both randgrpc values?
>
> Many thanks
>
> S.
>
>
> ???? ?"? ???? ????
> http://mail.nana.co.il
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From aimi.andrea at gmail.com  Thu Nov 24 13:19:45 2005
From: aimi.andrea at gmail.com (Andrea Aimi)
Date: Thu, 24 Nov 2005 13:19:45 +0100
Subject: [R] spatial-time smoothing
In-Reply-To: <29ba3d720511240252k21d99851ga83ad84163ba89ba@mail.gmail.com>
References: <29ba3d720511240252k21d99851ga83ad84163ba89ba@mail.gmail.com>
Message-ID: <29ba3d720511240419k53495a09u28d652204b4fd95a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/92b87136/attachment.pl

From petr.pikal at precheza.cz  Thu Nov 24 13:31:31 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 24 Nov 2005 13:31:31 +0100
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <20051123143323.GB6368@iwr.uni-heidelberg.de>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
Message-ID: <4385C0B3.32077.1980ABD@localhost>

Hi Marc

I use this function for finding maxima in some spectral 
data (eg. from Xray diffraction) and it satisfied my 
needs. The function itself was modified probably due to 
some reasons for ploting my data so it dropped values 
from the end rather than from both sides.

Peaks in those cases are different than just occasional 
spikes from noise so therefore I did not notice this bug.
Thanks for your suggestion.

Best regards.

Petr



On 23 Nov 2005 at 14:33, Marc Kirchner wrote:

Date sent:      	Wed, 23 Nov 2005 14:33:28 +0000
From:           	Marc Kirchner <marc.kirchner at iwr.uni-heidelberg.de>
To:             	Martin Maechler <maechler at stat.math.ethz.ch>
Copies to:      	R-help at r-project.org
Subject:        	Re: [R] finding peaks in a simple dataset with R

> > 
> > I wonder if we shouldn't polish that a bit and add to R's
> > standard 'utils' package.
> > 
> 
> Hm, I figured out there are (at least) two versions out there, one
> being the "original" idea and a modification. 
> 
> === Petr Pikal in 2001 (based on Brian Ripley's idea)==
> peaks <- function(series, span=3) {
>  z <- embed(series, span)
>  result <- max.col(z) == 1 + span %/% 2
>  result
> }
> 
> versus
> 
> === Petr Pikal in 2004 ==
> peaks2<-function(series,span=3) {
>  z <- embed(series, span)
>  s <- span%/%2
>  v<- max.col(z) == 1 + s
>  result <- c(rep(FALSE,s),v)
>  result <- result[1:(length(result)-s)]
>  result
> } 
> 
> Comparison shows
> > peaks(c(1,4,1,1,6,1,5,1,1),3)
> [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE
> which is a logical vector for elements 2:N-1 and
> 
> > peaks2(c(1,4,1,1,6,1,5,1,1),3)
> [1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
> which is a logical vector for elements 1:N-2.
> 
> As I would expect to "lose" (span-1)/2 elements on each side 
> of the vector, to me the 2001 version feels more natural.
> 
> Also, both "suffer" from being non-deterministic in the 
> multiple-maxima-case (the two 4s here)
> 
> > peaks(c(1,4,4,1,6,1,5,1,1),3)
> [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE
> > peaks(c(1,4,4,1,6,1,5,1,1),3)
> [1]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE
> > peaks(c(1,4,4,1,6,1,5,1,1),3)
> [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE
> > peaks(c(1,4,4,1,6,1,5,1,1),3)
> [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE
> 
> which also persits for span > 3 (without the 6 then, of course):
> 
> > peaks(c(1,4,4,1,1,1,5,1,1),5)
> [1]  TRUE FALSE FALSE FALSE  TRUE
> > peaks(c(1,4,4,1,1,1,5,1,1),5)
> [1] FALSE FALSE FALSE FALSE  TRUE
> > peaks(c(1,4,4,1,1,1,5,1,1),5)
> [1]  TRUE FALSE FALSE FALSE  TRUE
> 
> This could (should?) be fixed by modifying the call to max.col()
>  result <- max.col(z, "first") == 1 + span %/% 2;
> 
> Just my two cents,
> Marc
> 
> -- 
> ========================================================
> Dipl. Inform. Med. Marc Kirchner
> Interdisciplinary Centre for Scientific Computing (IWR)
> Multidimensional Image Processing
> INF 368
> University of Heidelberg
> D-69120 Heidelberg
> Tel: ++49-6221-54 87 97
> Fax: ++49-6221-54 88 50
> marc.kirchner at iwr.uni-heidelberg.de
> 
> 

Petr Pikal
petr.pikal at precheza.cz



From p.dalgaard at biostat.ku.dk  Thu Nov 24 13:57:25 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Nov 2005 13:57:25 +0100
Subject: [R] what's the meaning of these in R-lang?
In-Reply-To: <0IQG0020QJM4XT@mail.fudan.edu.cn>
References: <0IQG0020QJM4XT@mail.fudan.edu.cn>
Message-ID: <x2acfuxky2.fsf@viggo.kubism.ku.dk>

ronggui <042045003 at fudan.edu.cn> writes:

> In this case the environment contains the variables local to the
> function, and its enclosure is the environment of the enclosing
> function.(R-lang:p11)

Page number are of dubious value for documents in multiple formats. It
is page 5 in the PDF version on CRAN!

> 
> I want to know if the "enclosing function" means the closure of the
> function? for example ,if I call function mean(),and the create an
> environment,say e1,then the enclosure of e1 is the namespace of base
> package? Right?

No. A closure of a function is the function plus its environment. 

For "enclosing function", consider

f <- function() {
        print(environment())
        hello <- "Hi, babe!"
        g <- function() print(hello)
        g
}
myfun <- f() 
myfun()
environment(myfun)


The enclosing function of g, and hence of myfun, is f. The evaluation
environment of g has the evaluation environment of f as its enclosure. 

(The text is arguably imprecise: "evaluation environment of the
enclosing function call" would be better, especially since
environment(f) is something different.)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From HerwigMeschke at t-online.de  Thu Nov 24 14:35:57 2005
From: HerwigMeschke at t-online.de (Dr. Herwig Meschke)
Date: Thu, 24 Nov 2005 14:35:57 +0100
Subject: [R] R: pp plot
In-Reply-To: <mailman.8.1132743600.9825.r-help@stat.math.ethz.ch>
Message-ID: <4385CFCD.3008.CA00B5@localhost>

To construct a nonparametric (1-alpha) confidence set for an arbitrary 
CDF F, you can use the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality 
(e.g., see Wasserman, L. (2005). All of Statistics. 2nd, corr. printing. 
NY: Springer, p. 99).
With n=sample size and eps=sqrt(log(2/alpha)/(2*n)),
the lower and upper limits are pmax(F-eps ,0) and pmin(F+eps, 1).
Disadvantage: the sample size must be large.

Herwig

-- 
Dr. Herwig Meschke
Wissenschaftliche Beratung
Hagsbucher Weg 27
D-89150 Laichingen

phone +49 7333 210 417 / fax +49 7333 210 418
email HerwigMeschke at t-online.de
> hi all
> 
> i would like to know if anyone has a reference on how one would place
> the "bands" on the pp plot.
> 
> i want to test whether or not a certain data set comes from a particular
> distribution (not normal).
> 
> i've already plotted F(X(j)) vs j/(n+1)	where F(x) is the cum dist
> function, X(j) is the j'th order statistic and n is the sample size. 
> 
> a goole search gave arb references and thought some one one the list
> should definitely know how to solve this problem.
> 
> thanking you in advance
> allan



From vito_ricci at yahoo.com  Thu Nov 24 14:39:35 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 24 Nov 2005 14:39:35 +0100 (CET)
Subject: [R] Chi-squared test
Message-ID: <20051124133935.1342.qmail@web36106.mail.mud.yahoo.com>

Hi Bianca,

you could see my contribute "Fitting distribution with
R", pagg. 16-18:

http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf


Hoping it could help you.

Regards,

Vito


From: Bianca Vieru- Dimulescu <bianca.vieru <at>
free.fr>
Subject: [R] Chi-squared test
Date: 2005-11-24 11:13:24 GMT (2 hours and 23 minutes
ago)

Hello,

I'm trying to calculate a chi-squared test to see if
my data are 
different from the theoretical distribution or not:

chisq.test(rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55,
78, 49, 
60),c(80,80,80, 80, 80, 80, 80, 80, 80, 80, 80, 80)))

      Pearson's Chi-squared test

data:  rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78,
49, 60), c(80, 
80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80))
X-squared = 17.6, df = 11, p-value = 0.09142

Is this correct? If I'm doing the same thing using
Excel I obtained a 
different value of p.. (1.65778E-14)

Thanks a lot,
Bianca



Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From szlevine at nana.co.il  Thu Nov 24 15:44:29 2005
From: szlevine at nana.co.il (Stephen)
Date: Thu, 24 Nov 2005 16:44:29 +0200
Subject: [R] Survreg Weibull lambda and p
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD66C@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/315349e7/attachment.pl

From maechler at stat.math.ethz.ch  Thu Nov 24 16:06:52 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 24 Nov 2005 16:06:52 +0100
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <971536df0511231351k52e1cd89o8c790b6ad6a1d1c5@mail.gmail.com>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
	<20051123143323.GB6368@iwr.uni-heidelberg.de>
	<17284.38020.14279.158148@stat.math.ethz.ch>
	<971536df0511230929q74c48caax3892feec51fdcafe@mail.gmail.com>
	<971536df0511231351k52e1cd89o8c790b6ad6a1d1c5@mail.gmail.com>
Message-ID: <17285.55052.354968.489470@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Wed, 23 Nov 2005 16:51:17 -0500 writes:

    Gabor> One idea might be, rather than have a peaks function, enhance
    Gabor> rle so that it optionally produces a third component with the
    Gabor> peak information, perhaps 1, 0, -1 for peak, neither and trough.
    Gabor> This would avoid any problems with ties since the output of rle is
    Gabor> based on runs.

that's an interesting idea.   Contributions?

{see also my comments further below}

    Gabor> On 11/23/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
    >> On 11/23/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
    >> > >>>>> "Marc" == Marc Kirchner <marc.kirchner at iwr.uni-heidelberg.de>
    >> > >>>>>     on Wed, 23 Nov 2005 14:33:28 +0000 writes:
    >> >
    >> >    >>
    >> >    >> I wonder if we shouldn't polish that a bit and add to R's
    >> >    >> standard 'utils' package.
    >> >    >>
    >> >
    >> >    Marc> Hm, I figured out there are (at least) two versions out there, one being
    >> >    Marc> the "original" idea and a modification.
    >> >
    >> >    Marc> === Petr Pikal in 2001 (based on Brian Ripley's idea)==
    >> >    Marc> peaks <- function(series, span=3) {
    >> >    Marc> z <- embed(series, span)
    >> >    Marc> result <- max.col(z) == 1 + span %/% 2
    >> >    Marc> result
    >> >    Marc> }
    >> >
    >> >    Marc> versus
    >> >
    >> >    Marc> === Petr Pikal in 2004 ==
    >> >    Marc> peaks2<-function(series,span=3) {
    >> >    Marc> z <- embed(series, span)
    >> >    Marc> s <- span%/%2
    >> >    Marc> v<- max.col(z) == 1 + s
    >> >    Marc> result <- c(rep(FALSE,s),v)
    >> >    Marc> result <- result[1:(length(result)-s)]
    >> >    Marc> result
    >> >    Marc> }
    >> >
    >> > Thank you, Marc,
    >> >
    >> >    Marc> Comparison shows
    >> >    >> peaks(c(1,4,1,1,6,1,5,1,1),3)
    >> >    Marc> [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE
    >> >    Marc> which is a logical vector for elements 2:N-1 and
    >> >
    >> >    >> peaks2(c(1,4,1,1,6,1,5,1,1),3)
    >> >    Marc> [1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
    >> >    Marc> which is a logical vector for elements 1:N-2.
    >> >
    >> >    Marc> As I would expect to "lose" (span-1)/2 elements on each side
    >> >    Marc> of the vector, to me the 2001 version feels more natural.
    >> >
    >> > I think for the function to be more useful it the result should
    >> > have the original vector length and hence I'd propose to also
    >> > pad with FALSE at the upper end.
    >> >
    >> >    Marc> Also, both "suffer" from being non-deterministic in the
    >> >    Marc> multiple-maxima-case (the two 4s here)
    >> >
    >> > yes, of course, because of max.col().
    >> > Note that Venables & Ripley would consider this to be rather a feature.
    >> >
    >> >    Marc> This could (should?) be fixed by modifying the call to max.col()
    >> >    Marc> result <- max.col(z, "first") == 1 + span %/% 2;
    >> >
    >> > Actually I think it should become an option, but I'd use "first"
    >> > as default.
    >> >
    >> >    Marc> Just my two cents,
    >> >
    >> > Thank you.
    >> >
    >> > Here is my current proposal which also demonstrates why it's
    >> > useful to pad with FALSE :
    >> >
    >> > peaks <-function(series, span = 3, ties.method = "first") {
    >> >    if((span <- as.integer(span)) %% 2 != 1) stop("'span' must be odd")
    >> >    z <- embed(series, span)
    >> >    s <- span %/% 2
    >> >    v <- max.col(z, ties.method = ties.method) == s + 1:1
    >> >    pad <- rep(FALSE, s)
    >> >    c(pad, v, pad)
    >> > }
    >> >
    >> > y <- c(1,4,1,1,6,1,5,1,1) ; (ii <- which(peaks(y))); y[ii]
    >> > ##- [1] 2 5 7
    >> > ##- [1] 4 6 5
    >> >
    >> > set.seed(7)
    >> > y <- rpois(100, lambda = 7)
    >> > py <- peaks(y)
    >> > plot(y, type="o", cex = 1/4, main = "y and peaks(y,3)")
    >> > points(seq(y)[py], y[py], col = 2, cex = 1.5)
    >> >
    >> > p7 <- peaks(y,7)
    >> > points(seq(y)[p7], y[p7], col = 3, cex = 2)
    >> > mtext("peaks(y,7)", col=3)
    >> 
    >> I think ties are still problems with this approach
    >> as:
    >> 
    >> set.seed(1)

[that's not needed for the above peaks()!]


    >> peaks( c(1,2,2,2,3), 3 )
    >> 
    >> gives a peak in the 2,2,2 stretch.

Yes, thank you for the example, I've noticed similar behavior in
the plots I gave above.


    >> Also NA would seem to be a better pad than false or maybe
    >> it should be specifiable including whether there is
    >> padding at all.

NA's have the big drawback of producing a logical vector that
can NOT be used for subsetting -- and subsetting was exactly a main
reason for the padding...

I agree however that an option to "not pad" is sensible.

    >> The zoo rapply and rollmax functions which can also be used
    >> to specify a similar naive peak function have na.pad=
    >> and align= arguments.  Also any peaks function should
    >> be generic so that various time series classes can implement
    >> their own methods and in the case of irregularly spaced series
    >> note that there are two possibilities for span, the time distance
    >> and the number of points, and they are not the same.
    >> 
    >> It might also be nice to be able to get back peaks, troughs or
    >> both via  1,0,-1 in the output.
    >> 

    Gabor> ______________________________________________
    Gabor> R-help at stat.math.ethz.ch mailing list
    Gabor> https://stat.ethz.ch/mailman/listinfo/r-help
    Gabor> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From szlevine at nana.co.il  Thu Nov 24 16:17:28 2005
From: szlevine at nana.co.il (Stephen)
Date: Thu, 24 Nov 2005 17:17:28 +0200
Subject: [R] Survreg Weibull lambda and p
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD66E@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/3a107bbb/attachment.pl

From 042045003 at fudan.edu.cn  Thu Nov 24 16:30:14 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Thu, 24 Nov 2005 23:30:14 +0800
Subject: [R] what's the meaning of these in R-lang?
Message-ID: <0IQG003ATT9EAU@mail.fudan.edu.cn>

Thanks very much.

>ronggui <042045003 at fudan.edu.cn> writes:
>
>> In this case the environment contains the variables local to the
>> function, and its enclosure is the environment of the enclosing
>> function.(R-lang:p11)
>
>Page number are of dubious value for documents in multiple formats. It
>is page 5 in the PDF version on CRAN!
>
>> 
>> I want to know if the "enclosing function" means the closure of the
>> function? for example ,if I call function mean(),and the create an
>> environment,say e1,then the enclosure of e1 is the namespace of base
>> package? Right?
>
>No. A closure of a function is the function plus its environment. 
>
>For "enclosing function", consider
>
>f <- function() {
>        print(environment())
>        hello <- "Hi, babe!"
>        g <- function() print(hello)
>        g
>}
>myfun <- f() 
>myfun()
>environment(myfun)
>
>
>The enclosing function of g, and hence of myfun, is f. The evaluation
>environment of g has the evaluation environment of f as its enclosure. 
I think I get your point.

I would like to express what I think using the folloing code:
> debug(mean)
> mean(1:10)
debugging in: mean(1:10)
debug: UseMethod("mean")
Browse[1]> environment()
<environment: 0236EDA0>  # the evaluation environment.
Browse[1]> parent.env(environment()) 
<environment: namespace:base> # the enclosure of the evaluation environment.
Browse[1]> 
exiting from: mean(1:10)
[1] 5.5
> environment(mean)
<environment: namespace:base> # the environment of mean.

So what "# the enclosure of the evaluation environment." is always the same as "# the environment of function (here is mean)" .

Am I wrong again?


Regards!

>(The text is arguably imprecise: "evaluation environment of the
>enclosing function call" would be better, especially since
>environment(f) is something different.)

Yes,It's easier to understand by "evaluation environment of the
enclosing function call" 


>
>-- 
>   O__  ---- Peter Dalgaard             Øster Farimagsgade 5, Entr.B
>  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

= = = = = = = = = = = = = = = = = = = =
			


 

2005-11-24

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From JeeBee at troefpunt.nl  Thu Nov 24 16:44:05 2005
From: JeeBee at troefpunt.nl (JeeBee)
Date: Thu, 24 Nov 2005 16:44:05 +0100
Subject: [R] hamming distance
References: <6.1.0.6.0.20051124113250.01a90f68@master.ivia.es>
Message-ID: <pan.2005.11.24.15.44.01.959898@troefpunt.nl>

You could install.packages("e1071")
and see help("hamming.distance")

JeeBee.


hamming.distance            package:e1071            R Documentation
Hamming Distances of Vectors
Description:
     If both 'x' and 'y' are vectors, 'hamming.distance' returns the
     Hamming distance (number of different bytes) between this two
     vectors. If 'x' is a matrix, the Hamming distances between the
     rows of 'x' are computed and 'y' is ignored.
Usage:
      hamming.distance(x, y)
Arguments:
       x: a vector or matrix.
       y: an optional vector.
Examples:
     x <- c(1, 0, 0)
     y <- c(1, 0, 1)
     hamming.distance(x, y)
     z <- rbind(x,y)
     rownames(z) <- c("Fred", "Tom")
     hamming.distance(z)


On Thu, 24 Nov 2005 11:33:45 +0100, Ana Conesa wrote:

> 
>    Hi,
>    Does anyone know an R function to impute hamming distance?
>    Thanks
>    Ana
> 
>       O@@@@@  &nb     @@@O@@O@   Centro de Gen??mica
>      @O@@@@O@   Instituto Valenciano de Investigaciones Agrarias (IVIA)
>      @@@O@@@@   Carretera Moncada - Naquera, Km. 4,5
>       @@@@O@    46113 Moncada (Valencia) SPAIN
>         ||  &        ||  &
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From herodote at oreka.com  Thu Nov 24 16:57:37 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Thu, 24 Nov 2005 16:57:37 +0100
Subject: [R] =?iso-8859-1?q?font_size_in_legend?=
Message-ID: <IQGV01$5CC9836DDC060F13965A724F1B4D060E@oreka.com>

Hy all

I use barplot to draw frequencies by dates.

On the x axis it shows only 1 date for 2 bars, i've understand why, because R cant put so much date on the same axis, it will get out of the graph.

that's why i tought about reducing the size of the font used to stamp the x axis.

anyone knows the right way? i've tryed par(cin) and barplot(cin=3) but it says i cannot do it at this level...

thks you all.

guillaume.



From blh at cpom.ucl.ac.uk  Thu Nov 24 17:15:58 2005
From: blh at cpom.ucl.ac.uk (Benjamin Lloyd-Hughes)
Date: Thu, 24 Nov 2005 16:15:58 +0000
Subject: [R] Matrix rotation
Message-ID: <4e91be745ed00c0c9a2d5cac42dd5ee7@cpom.ucl.ac.uk>

Dearest All,

Ok so I've had a couple of glasses of wine over lunch today... This is 
likely to be trivial but I'm struggling to find a more elegant way to 
obtain the following matrix rotations:

 > M <- matrix(c(1,0,0,0), ncol=2)
 > M
      [,1] [,2]
[1,]    1    0
[2,]    0    0
 > N <- abind(M[2,],M[1,],along=2)
 > N
      [,1] [,2]
[1,]    0    1
[2,]    0    0
 > P <- abind(N[2,],N[1,],along=2)
 > P
      [,1] [,2]
[1,]    0    0
[2,]    0    1
 > Q <- abind(P[,2],P[,1],along=2)
 > Q
      [,1] [,2]
[1,]    0    0
[2,]    1    0

And, more generally wish to rotate a n-dimensional data cube about some 
specified axis.

Cheers, Ben



From sfelten at uwinst.unizh.ch  Thu Nov 24 17:27:33 2005
From: sfelten at uwinst.unizh.ch (Stefanie von Felten, IPW&IfU)
Date: Thu, 24 Nov 2005 17:27:33 +0100
Subject: [R] type III sums of squares in R
Message-ID: <4385E9F5.9090000@uwinst.unizh.ch>

Hi everyone,

Can someone explain me how to calculate SAS type III sums of squares in 
R? Not that I would like to use them, I know they are problematic. I 
would like to know how to calculate them in order to demonstrate that 
strange things happen when you use them (for a course for example). I 
know you can use drop1(lm(), test="F") but for an lm(y~A+B+A:B), type 
III SSQs are only calculated for the A:B interaction. If I specify 
lm(y~A+B), it calculates type II SSQ for the main effects (as type III 
only differs from type II if interactions are included). Thus, this 
approach doesn't help.

Another approach is the Anova(, type="III") function within the 
library(car). But somehow it produces strange results. Somebody told me 
I have to change the contrast settings using
options(contrasts=c("contr.helmert", "contr.poly"))
 But I had the impression that my results are still impossible.
Are the calculations dependent from the version of R I use? I am 
currently using R2.1.1

The only thing that seems to work is a trick: Specify a separate column 
AB that codes a new variable for the interaction of A:B. Now you can fit 
A,B, and AB (as 3 main effects) in 3 different sequential models with 
each one of them in the end once. For the term in the end you then get 
type III SSQ which seem to be correct.

Cheers
Steffi

-- 
---------------------------------
Stefanie von Felten
Doktorandin

ETH Z??rich
Institut f??r Pflanzenwissenschaften
Universit??tstrasse 2, LFW A2
8092 Z??rich
Telefon: 044 632 85 97
Telefax: 044 632 11 53
e-mail: stefanie.vonfelten at ipw.agrl.ethz.ch

Universit??t Z??rich
Institut f??r Umweltwissenschaften
Winterthurerstrasse 190
8057 Z??rich
Telefon: 044 635 61 23
Telefax: 044 635 57 11
e-mail:  sfelten at uwinst.unizh.ch



From blh at cpom.ucl.ac.uk  Thu Nov 24 17:32:32 2005
From: blh at cpom.ucl.ac.uk (Benjamin Lloyd-Hughes)
Date: Thu, 24 Nov 2005 16:32:32 +0000
Subject: [R] Fwd: Matrix rotation
Message-ID: <a670d84c003d14886b4976242ceb7541@cpom.ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/e6f7be02/attachment.pl

From ggrothendieck at gmail.com  Thu Nov 24 17:33:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Nov 2005 11:33:13 -0500
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <17285.55052.354968.489470@stat.math.ethz.ch>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
	<20051123143323.GB6368@iwr.uni-heidelberg.de>
	<17284.38020.14279.158148@stat.math.ethz.ch>
	<971536df0511230929q74c48caax3892feec51fdcafe@mail.gmail.com>
	<971536df0511231351k52e1cd89o8c790b6ad6a1d1c5@mail.gmail.com>
	<17285.55052.354968.489470@stat.math.ethz.ch>
Message-ID: <971536df0511240833l4e0d49a0x6ee4b4d3675ec776@mail.gmail.com>

On 11/24/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

> NA's have the big drawback of producing a logical vector that
> can NOT be used for subsetting -- and subsetting was exactly a main
> reason for the padding...
>

Since 'which' throws away NAs, one extra 'which' can solve
that.  In the following, runmax from caTools (which is quite
fast being written in C) uses NA padding by default:

x <- c(1:3, 3, 3:5, 5:1) # test data.  peak is at 5.

library(caTools)
x.rv <- rle(x)$values
x.rv[which(runmax(x.rv, 3) == x.rv)]



From cgb at datanalytics.com  Thu Nov 24 17:25:29 2005
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Thu, 24 Nov 2005 17:25:29 +0100
Subject: [R] hamming distance
In-Reply-To: <pan.2005.11.24.15.44.01.959898@troefpunt.nl>
References: <6.1.0.6.0.20051124113250.01a90f68@master.ivia.es>
	<pan.2005.11.24.15.44.01.959898@troefpunt.nl>
Message-ID: <20051124172529.fo6o5sdi974848og@webmail.datanalytics.com>

True, but if

>     x <- c(1, 0, 0)
>     y <- c(1, 0, 1),

then you can just define

hamming.distance <- function(x,y){
  sum(x != y)
}

The problem is just a bit harder when x and y are, for instance, strings.

Carlos J. Gil Bellosta
http://www.datanalytics.com

Quoting JeeBee <JeeBee at troefpunt.nl>:

> You could install.packages("e1071")
> and see help("hamming.distance")
>
> JeeBee.
>
>
> hamming.distance            package:e1071            R Documentation
> Hamming Distances of Vectors
> Description:
>     If both 'x' and 'y' are vectors, 'hamming.distance' returns the
>     Hamming distance (number of different bytes) between this two
>     vectors. If 'x' is a matrix, the Hamming distances between the
>     rows of 'x' are computed and 'y' is ignored.
> Usage:
>      hamming.distance(x, y)
> Arguments:
>       x: a vector or matrix.
>       y: an optional vector.
> Examples:
>     x <- c(1, 0, 0)
>     y <- c(1, 0, 1)
>     hamming.distance(x, y)
>     z <- rbind(x,y)
>     rownames(z) <- c("Fred", "Tom")
>     hamming.distance(z)
>
>
> On Thu, 24 Nov 2005 11:33:45 +0100, Ana Conesa wrote:
>
>>
>>    Hi,
>>    Does anyone know an R function to impute hamming distance?
>>    Thanks
>>    Ana
>>
>>       O@@@@@  &nb     @@@O@@O@   Centro de Gen??mica
>>      @O@@@@O@   Instituto Valenciano de Investigaciones Agrarias (IVIA)
>>      @@@O@@@@   Carretera Moncada - Naquera, Km. 4,5
>>       @@@@O@    46113 Moncada (Valencia) SPAIN
>>         ||  &        ||  &
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Thu Nov 24 17:47:00 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 24 Nov 2005 17:47:00 +0100
Subject: [R] font size in legend
In-Reply-To: <IQGV01$5CC9836DDC060F13965A724F1B4D060E@oreka.com>
References: <IQGV01$5CC9836DDC060F13965A724F1B4D060E@oreka.com>
Message-ID: <4385EE84.5080006@statistik.uni-dortmund.de>

herodote at oreka.com wrote:

> Hy all
> 
> I use barplot to draw frequencies by dates.
> 
> On the x axis it shows only 1 date for 2 bars, i've understand why, because R cant put so much date on the same axis, it will get out of the graph.
> 
> that's why i tought about reducing the size of the font used to stamp the x axis.
> 
> anyone knows the right way? i've tryed par(cin) and barplot(cin=3) but it says i cannot do it at this level...

So are we talking about a legend or the axis annotation of a barplot or 
what? For the latter, use the argument "cex.axis".

Uwe Ligges




> thks you all.
> 
> guillaume.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Thu Nov 24 17:50:25 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Nov 2005 11:50:25 -0500
Subject: [R] Suggested add to the documentation for the identify()
	function
In-Reply-To: <bcb171920511240327l8932a30t630a688759cb350@mail.gmail.com>
References: <bcb171920511240327l8932a30t630a688759cb350@mail.gmail.com>
Message-ID: <971536df0511240850t161baf17h5383a920e4b48453@mail.gmail.com>

On 11/24/05, Vivek Satsangi <vivek.satsangi at gmail.com> wrote:
> motivated by Prof. Ripley's 2001 talk in which he had commented that
> open source software users rarely give back anything.) -- but I know

With 600+ addon packages this does not seem to be the case for R.



From p.dalgaard at biostat.ku.dk  Thu Nov 24 17:53:12 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Nov 2005 17:53:12 +0100
Subject: [R] type III sums of squares in R
In-Reply-To: <4385E9F5.9090000@uwinst.unizh.ch>
References: <4385E9F5.9090000@uwinst.unizh.ch>
Message-ID: <x2acfuf0nb.fsf@viggo.kubism.ku.dk>

"Stefanie von Felten, IPW&IfU" <sfelten at uwinst.unizh.ch> writes:

> Hi everyone,
> 
> Can someone explain me how to calculate SAS type III sums of squares in 
> R? Not that I would like to use them, I know they are problematic. I 
> would like to know how to calculate them in order to demonstrate that 
> strange things happen when you use them (for a course for example). I 
> know you can use drop1(lm(), test="F") but for an lm(y~A+B+A:B), type 
> III SSQs are only calculated for the A:B interaction. If I specify 
> lm(y~A+B), it calculates type II SSQ for the main effects (as type III 
> only differs from type II if interactions are included). Thus, this 
> approach doesn't help.
> 
> Another approach is the Anova(, type="III") function within the 
> library(car). But somehow it produces strange results. Somebody told me 
> I have to change the contrast settings using
> options(contrasts=c("contr.helmert", "contr.poly"))
>  But I had the impression that my results are still impossible.
> Are the calculations dependent from the version of R I use? I am 
> currently using R2.1.1
> 
> The only thing that seems to work is a trick: Specify a separate column 
> AB that codes a new variable for the interaction of A:B. Now you can fit 
> A,B, and AB (as 3 main effects) in 3 different sequential models with 
> each one of them in the end once. For the term in the end you then get 
> type III SSQ which seem to be correct.

I think you need to qualify "correct", "strange", and "impossible"...

In particular, since you're hinting at a bug in "car", I'm sure its
author would like to know what it is you think is wrong.

Also, in the "trick" you don't specify *how* you code the three terms.
I don't think it is immaterial, and I'm not sure which of the possible
versions (if any) matches up with e.g. SAS's version of type III for
main effects in the presence of interaction.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From msears at unr.edu  Thu Nov 24 19:53:54 2005
From: msears at unr.edu (Mike Sears)
Date: Thu, 24 Nov 2005 10:53:54 -0800
Subject: [R] type III sums of squares in R
In-Reply-To: <4385E9F5.9090000@uwinst.unizh.ch>
References: <4385E9F5.9090000@uwinst.unizh.ch>
Message-ID: <200511241053.54555.msears@unr.edu>

If you use the Anova function in the car package and specify contr.sum or 
contr.SAS for the contrasts for your categorical factors, you will get the 
same results as outputted by SAS. I've tried this with a variety of data sets 
and it works.

On Thu November 24 2005 08:27, Stefanie von Felten, IPW&IfU wrote:
> Hi everyone,
>
> Can someone explain me how to calculate SAS type III sums of squares in
> R? Not that I would like to use them, I know they are problematic. I
> would like to know how to calculate them in order to demonstrate that
> strange things happen when you use them (for a course for example). I
> know you can use drop1(lm(), test="F") but for an lm(y~A+B+A:B), type
> III SSQs are only calculated for the A:B interaction. If I specify
> lm(y~A+B), it calculates type II SSQ for the main effects (as type III
> only differs from type II if interactions are included). Thus, this
> approach doesn't help.
>
> Another approach is the Anova(, type="III") function within the
> library(car). But somehow it produces strange results. Somebody told me
> I have to change the contrast settings using
> options(contrasts=c("contr.helmert", "contr.poly"))
>  But I had the impression that my results are still impossible.
> Are the calculations dependent from the version of R I use? I am
> currently using R2.1.1
>
> The only thing that seems to work is a trick: Specify a separate column
> AB that codes a new variable for the interaction of A:B. Now you can fit
> A,B, and AB (as 3 main effects) in 3 different sequential models with
> each one of them in the end once. For the term in the end you then get
> type III SSQ which seem to be correct.
>
> Cheers
> Steffi

-- 
Michael W. Sears, Ph.D.
Postdoctoral Fellow
Department of Biology/MS 314
University of Nevada, Reno
Reno, NV 89557

Assistant Professor
Department of Zoology
Southern Illinois University
Carbondale, IL 62901

phone: 775.784.8008
cell: ??775.232.3520
web:????????http://www.unr.edu/homepage/msears
????????????????http://www.science.siu.edu/zoology/sears

"Natural selection is a mechanism for generating 
??an exceedingly high degree of improbability."
????????????????????????????????????????????????-Sir Ronald Fisher (1890-1962)



From buddhahead at ranpura.com  Thu Nov 24 18:22:49 2005
From: buddhahead at ranpura.com (Ashish Ranpura)
Date: Thu, 24 Nov 2005 17:22:49 +0000
Subject: [R] SPSS-like factor analysis procedure
Message-ID: <E5F2A4EF-5621-4745-A525-28B1081B0EB9@ranpura.com>


[Apologies if this posting is appears twice -- I think I was  
unsuccessful in posting it previously.]


I've read through many postings about principle component analysis in  
the R-help archives, but haven't been able to piece together the  
information I need. I'd like to recreate an SPSS-like experience of  
factor analysis using R. Here's what SPSS produces:

1. Scatterplots of all possible variable pairs, with regression lines.
xyplot(my.dataframe) is perfect but for the lack of regression lines.

2. Frequency histograms overlaid with normal curves for each variable.
I can do this one at a time; I'd love R to do it in a big layout for  
all the variables in the data frame.

3. Descriptive statistics of each variable.
Jim Lemon's excellent dstats() function does this. Solved.

4. A large correlation matrix for the data frame.
The built-in function cov() does this. Solved.

5. KMO (Kaiser-Meyer-Olkin Measure of Sampling Adequacy) and Bartlett  
test of sphericity on the data frame as a whole.
I can't find ways to recreate these tests -- bartlett.test() doesn't  
produce the type of response that makes sense.

6. Anti-image matricies, including MSA (sampling adequacy) scores for  
each variable
I can't find a way to generate this, maybe because I'm unsure how its  
calculated. The MSA scores would tell me how strongly each variable  
measures the data set as a whole, which I could use to guide  
subsequent factor analysis.

7. Total Variance Explained -- a table listing eigenvalues for each  
eigenvector, along with the % variance for each eigenvector.
This is the best part of the SPSS output. I feel like I'm close to  
finding the right function in R , but I don't know how to look at the  
eigenvalues of each component in R. princomp() seems a step in the  
right direction.

8. Scree plot.
No problem, princomp() and screeplot() seem to produce about the  
right result.

9. Component matrix (lists the variable loading on each factor)
factanal() seems to do this, but again the results don't jive with  
SPSS and I'm unsure why.

10. Factor rotation
No problem, factanal(rotation="varimax") does this.


If anyone can suggest how to fill in the missing pieces (particularly  
steps 6 and 7), please do let me know. Thanks!

--Ashish.


-----
Ashish Ranpura
Institute of Cognitive Neuroscience
University College London



From jfox at mcmaster.ca  Thu Nov 24 18:57:23 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 24 Nov 2005 12:57:23 -0500
Subject: [R] type III sums of squares in R
In-Reply-To: <200511241053.54555.msears@unr.edu>
Message-ID: <20051124175723.TDAT26550.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Stefanie and Mike,

To elaborate slightly on Mike's points, the Anova() function in car
calculates "Type-III" (and "Type-II") tests differently from SAS. (The
difference originates in the fact that SAS uses a deficient-rank
parametrization of the model while R uses a full-rank parametrization; it
would be possible to mimic SAS's behaviour more closely, but I think that
there are problems with it.) As a consequence, you have to use a
contrast-generating function, such as contr.helmert or contr.sum (but not
contr.SAS), that provides contrasts that are orthogonal in the row-basis of
the model matrix. 

I should probably elaborate the warning about "Type-III" tests in the help
page for Anova(), but perhaps it would help to know that the issue is
discussed at greater length in the book with which the car package is
associated.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Sears
> Sent: Thursday, November 24, 2005 1:54 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] type III sums of squares in R
> 
> If you use the Anova function in the car package and specify 
> contr.sum or contr.SAS for the contrasts for your categorical 
> factors, you will get the same results as outputted by SAS. 
> I've tried this with a variety of data sets and it works.
> 
> On Thu November 24 2005 08:27, Stefanie von Felten, IPW&IfU wrote:
> > Hi everyone,
> >
> > Can someone explain me how to calculate SAS type III sums 
> of squares 
> > in R? Not that I would like to use them, I know they are 
> problematic. 
> > I would like to know how to calculate them in order to demonstrate 
> > that strange things happen when you use them (for a course for 
> > example). I know you can use drop1(lm(), test="F") but for an 
> > lm(y~A+B+A:B), type III SSQs are only calculated for the A:B 
> > interaction. If I specify lm(y~A+B), it calculates type II 
> SSQ for the 
> > main effects (as type III only differs from type II if interactions 
> > are included). Thus, this approach doesn't help.
> >
> > Another approach is the Anova(, type="III") function within the 
> > library(car). But somehow it produces strange results. 
> Somebody told 
> > me I have to change the contrast settings using 
> > options(contrasts=c("contr.helmert", "contr.poly"))  But I had the 
> > impression that my results are still impossible.
> > Are the calculations dependent from the version of R I use? I am 
> > currently using R2.1.1
> >
> > The only thing that seems to work is a trick: Specify a separate 
> > column AB that codes a new variable for the interaction of A:B. Now 
> > you can fit A,B, and AB (as 3 main effects) in 3 different 
> sequential 
> > models with each one of them in the end once. For the term 
> in the end 
> > you then get type III SSQ which seem to be correct.
> >
> > Cheers
> > Steffi
> 
> --
> Michael W. Sears, Ph.D.
> Postdoctoral Fellow
> Department of Biology/MS 314
> University of Nevada, Reno
> Reno, NV 89557
> 
> Assistant Professor
> Department of Zoology
> Southern Illinois University
> Carbondale, IL 62901
> 
> phone: 775.784.8008
> cell: ??775.232.3520
> web:????????http://www.unr.edu/homepage/msears
> ????????????????http://www.science.siu.edu/zoology/sears
> 
> "Natural selection is a mechanism for generating
> ??an exceedingly high degree of improbability."
> ????????????????????????????????????????????????-Sir Ronald Fisher (1890-1962)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From leaflovesun at yahoo.ca  Thu Nov 24 18:16:21 2005
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Fri, 25 Nov 2005 01:16:21 +0800
Subject: [R] read.list()
Message-ID: <200511241802.jAOI2NxW015040@hypatia.math.ethz.ch>

  Hi all,

I need to write and read a list in R. I did r.site.search, found there is a package   "rmutil" doing this, unfortunately it is not on the list of package. In another words, I can't install it from any CRAN mirror. 

Anybody has idea about this? or any suggestion about the list? Thanks!

Best!

Leaf



From ggrothendieck at gmail.com  Thu Nov 24 19:17:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Nov 2005 13:17:00 -0500
Subject: [R] Fwd: Matrix rotation
In-Reply-To: <a670d84c003d14886b4976242ceb7541@cpom.ucl.ac.uk>
References: <a670d84c003d14886b4976242ceb7541@cpom.ucl.ac.uk>
Message-ID: <971536df0511241017h5f80ae07o6c876d62cae3f803@mail.gmail.com>

Try this:

# cyclically rotate a vector to the right
rot <- function(x) c(x[length(x)], x[-length(x)])

# create a vector from a 2x2 matrix moving left to right along
# first row and then right to left along second row
m2v <- function(m) m[c(1,3,4,2)]

# inverse of m2v.  Note that c(1,4,2,3) equals order(c(1,3,4,2))
v2m <- function(v) matrix(v[c(1,4,2,3)],2)

m <- matrix(1:4, 2, byrow = TRUE)
print(mm <- m)
for(i in 1:4) print(mm <- v2m(rot(m2v(mm))))


On 11/24/05, Benjamin Lloyd-Hughes <blh at cpom.ucl.ac.uk> wrote:
> Ok I warned you that I'd been drinking! What I really meant was
> something to go from:
>
>      [,1] [,2]
> [1,]    1    2
> [2,]    4    3
>
> to
>
>      [,1] [,2]
> [1,]    4    1
> [2,]    3    2
>
> to
>
>      [,1] [,2]
> [1,]    3    4
> [2,]    2    1
>
> to
>
>      [,1] [,2]
> [1,]    2    3
> [2,]    1    4
>
> Sorry for being a muppet, B
>
>
> Begin forwarded message:
>
> > From: Benjamin Lloyd-Hughes <blh at cpom.ucl.ac.uk>
> > Date: 24 November 2005 16:15:58 GMT
> > To: r-help at stat.math.ethz.ch
> > Subject: Matrix rotation
> >
> > Dearest All,
> >
> > Ok so I've had a couple of glasses of wine over lunch today... This is
> > likely to be trivial but I'm struggling to find a more elegant way to
> > obtain the following matrix rotations:
> >
> > > M <- matrix(c(1,0,0,0), ncol=2)
> > > M
> >      [,1] [,2]
> > [1,]    1    0
> > [2,]    0    0
> > > N <- abind(M[2,],M[1,],along=2)
> > > N
> >      [,1] [,2]
> > [1,]    0    1
> > [2,]    0    0
> > > P <- abind(N[2,],N[1,],along=2)
> > > P
> >      [,1] [,2]
> > [1,]    0    0
> > [2,]    0    1
> > > Q <- abind(P[,2],P[,1],along=2)
> > > Q
> >      [,1] [,2]
> > [1,]    0    0
> > [2,]    1    0
> >
> > And, more generally wish to rotate a n-dimensional data cube about
> > some specified axis.
> >
> > Cheers, Ben
> >
>
>        [[alternative text/enriched version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From suncertain at gmail.com  Thu Nov 24 19:35:54 2005
From: suncertain at gmail.com (Urania Sun)
Date: Thu, 24 Nov 2005 12:35:54 -0600
Subject: [R] residuals in logistic regression model
Message-ID: <4ab0fb470511241035t58ea985amff856aa4649d0127@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/1f6bb03e/attachment.pl

From kjetilbrinchmannhalvorsen at gmail.com  Thu Nov 24 20:26:43 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 24 Nov 2005 15:26:43 -0400
Subject: [R] R: pp plot
In-Reply-To: <4385CFCD.3008.CA00B5@localhost>
References: <4385CFCD.3008.CA00B5@localhost>
Message-ID: <438613F3.3080502@gmail.com>

Dr. Herwig Meschke wrote:
> To construct a nonparametric (1-alpha) confidence set for an arbitrary 
> CDF F, you can use the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality 
> (e.g., see Wasserman, L. (2005). All of Statistics. 2nd, corr. printing. 
> NY: Springer, p. 99).
> With n=sample size and eps=sqrt(log(2/alpha)/(2*n)),
> the lower and upper limits are pmax(F-eps ,0) and pmin(F+eps, 1).
> Disadvantage: the sample size must be large.
> 
> Herwig
> 
There is also ecdf.ksCI in CRAN package sfsmisc.


Kjetil



From Rich at mango-solutions.com  Thu Nov 24 21:52:21 2005
From: Rich at mango-solutions.com (Rich@mango-solutions.com)
Date: Thu, 24 Nov 2005 20:52:21 -0000
Subject: [R] JOB: R/S Consultants, Reading (UK)
Message-ID: <200511242052.jAOKqFcn027001@hypatia.math.ethz.ch>

Mango Solutions, providers of R and S-PLUS consulting, development and
training Services, are looking for 2 consultants to join their UK-based
technical team.  We are looking for highly motivated individuals to work in
a customer-focused environment.

> R/S Pharmaceutical Consultant (Reading, UK)
> R/S Finance Consultant (Reading, UK)

More information at http://www.mango-solutions.com/careers.htm

(Apologies for cross posting.  Many thanks for Martin for the guidelines on
posting)



From jfox at mcmaster.ca  Thu Nov 24 22:10:02 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 24 Nov 2005 16:10:02 -0500
Subject: [R] residuals in logistic regression model
In-Reply-To: <4ab0fb470511241035t58ea985amff856aa4649d0127@mail.gmail.com>
Message-ID: <20051124211002.NQUW21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Urania,

The residuals method for glm objects can compute several kinds of residuals;
the default is deviance residuals. See ?residuals.glm for details and
references.

I hope this helps.
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Urania Sun
> Sent: Thursday, November 24, 2005 1:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] residuals in logistic regression model
> 
> In the logistic regression model, there is no residual
> 
> log (pi/(1-pi)) = beta_0 + beta_1*X_1 + .....
> 
> But glm model will return
> 
> residuals
> 
> What is that?
> 
> How to understand this? Can we put some residual in the 
> logistic regression model by replacing pi with pi' (the estimated pi)?
> 
>  log (pi'/(1-pi')) = beta_0 + beta_1*X_1 + .....+ ei
> 
> Thanks!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From aolinto_r at bignet.com.br  Thu Nov 24 22:33:53 2005
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Thu, 24 Nov 2005 19:33:53 -0200
Subject: [R] model selection with step function
Message-ID: <1132868033.438631c1a2cc5@webmail2.bignet.com.br>

Hello,

I have a doubt in using the function step (step wise) to select glm models.

Usually I apply the gamma distribution to analyze fishery data. To select the
terms I use a routine where I first compare single term models to the null model
(eg. U~1 vs. U~depth; U~1 vs. U~latitude; etc. ? where U= abundance) and, by
means of the result given by a likelihook function applied for each comparison,
I select the ?strongest? effect, let?s say depth. Then I run a new step
comparing the U~depth vs. U~depth+latitude; U~depth vs. U~depth+... etc. Making
this way I put the terms in ?magnitude? order.

I tried to make a gaussian model using the step(glm.model) function to select
the terms but I saw that in the output table given by anova(glm.model) the
selected terms kept the original order.

Is it possible to have the terms in the model rearranged, as in my example?

Thanks for any help. I read Chambers and Hastie?s ?Statistical Models in S?,
Venables and Ripley ?Modern Applied Statistics? and, of course, R help but I
couldn?t get the trick.

Antonio

--
Biologist
Sao Paulo Fisheries Institute



-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 24 22:55:17 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 24 Nov 2005 21:55:17 -0000 (GMT)
Subject: [R] Chi-squared test
In-Reply-To: <4385AD18.6010507@math.ucalgary.ca>
Message-ID: <XFMail.051124215517.Ted.Harding@nessie.mcc.ac.uk>

On 24-Nov-05 P Ehlers wrote:
> Bianca Vieru- Dimulescu wrote:
>> Hello,
>> I'm trying to calculate a chi-squared test to see if my data are 
>> different from the theoretical distribution or not:
>> 
>> chisq.test(rbind(c(79,52,69,71,82,87,95,74,55,78,49,60),
                    c(80,80,80,80,80,80,80,80,80,80,80,80)))
>> 
>>       Pearson's Chi-squared test
>> 
>> data:  rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60),
>>              c(80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80))
>> X-squared = 17.6, df = 11, p-value = 0.09142
>> 
>> Is this correct? If I'm doing the same thing using Excel I obtained
>> a different value of p.. (1.65778E-14)
>> 
>> Thanks a lot,
>> Bianca
> 
> It would be unusual to have 12 observed frequencies all equal to 80.
> So I'm guessing that you have a 12-category variable and want to
> test its fit to a discrete uniform distribution. I assume that your
> frequencies are
> 
> x <- c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60)
> 
> Then just use
> 
> chisq.test(x)
> 
> (see the help page).
> 
> (If those 80's are expected cell frequencies, they should sum to
> sum(x) = 851.)
> 
> I don't know what Excel does.
> 
> Peter
> 
> Peter Ehlers
> University of Calgary

I'm rather with Peter on this question! I've tried to infer what
you're really trying to do.

My a-priori plausible hypothesis was that you have

  k<-12

independent observations which have equal expected values

  m<-rep(80,k)

and are observed as

  x<-c(79,52,69,71,82,87,95,74,55,78,49,60)

On this basis, a chi-squared test Sum((O-E)^2/E) gives

  C2<-sum(((x-m)^2)/m)

so C2 = 41.1375, and on this hypothesis the chi-squared would
have k=12 degrees of freedom. Then:

  1-pchisq(C2,k)
## [1] 4.647553e-05

which is nowhere near the 1.65778E-14 you report from Excel.
Also, the result from Peter's chisq.test(x) is p = 0.0006468,
even further away.

So this makes me really wonder what you are doing.

The nearest I can get to your Excel result 1.65778E-14 is

  ix<-x<m
  prod(2*ppois(x[ix],m[ix]))*prod(2*(1-ppois(x[!ix],m[!ix])))
## 2.831963e-14

which is based on the guess that independent 2-sided Poisson
tests of agreement between O and E have been carried out on each
component, and the final P-value is the product of these P-values.

But this doesn't make a lot of sense from a statistical point
of view, so it's time to stop guessing!

Please tell us what hypothesis you are testing, what sort of
distribution the x-values are supposed to have, what the
repeated "80" values represent, and also please tell us
in detail what you asked Excel to do!

Then, perhaps, a useful reply can be made.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 24-Nov-05                                       Time: 21:55:14
------------------------------ XFMail ------------------------------



From ggrothendieck at gmail.com  Fri Nov 25 01:24:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Nov 2005 19:24:47 -0500
Subject: [R] Fwd: Matrix rotation
In-Reply-To: <971536df0511241017h5f80ae07o6c876d62cae3f803@mail.gmail.com>
References: <a670d84c003d14886b4976242ceb7541@cpom.ucl.ac.uk>
	<971536df0511241017h5f80ae07o6c876d62cae3f803@mail.gmail.com>
Message-ID: <971536df0511241624ife83819vb61ee29cf60653bd@mail.gmail.com>

In thinking about this a bit more it can be simplified to
the following where f takes a 2x2 matrix as its first
argument and permutes it according to the second
argument returning the permuted 2x2 matrix:

f <- function(m = c(1,4,2,3), idx = c(2,4,1,3)) matrix(m[idx],2)
print(m <- f(,1:4))
for(i in 1:4) print(m <- f(m))


On 11/24/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> # cyclically rotate a vector to the right
> rot <- function(x) c(x[length(x)], x[-length(x)])
>
> # create a vector from a 2x2 matrix moving left to right along
> # first row and then right to left along second row
> m2v <- function(m) m[c(1,3,4,2)]
>
> # inverse of m2v.  Note that c(1,4,2,3) equals order(c(1,3,4,2))
> v2m <- function(v) matrix(v[c(1,4,2,3)],2)
>
> m <- matrix(1:4, 2, byrow = TRUE)
> print(mm <- m)
> for(i in 1:4) print(mm <- v2m(rot(m2v(mm))))
>
>
> On 11/24/05, Benjamin Lloyd-Hughes <blh at cpom.ucl.ac.uk> wrote:
> > Ok I warned you that I'd been drinking! What I really meant was
> > something to go from:
> >
> >      [,1] [,2]
> > [1,]    1    2
> > [2,]    4    3
> >
> > to
> >
> >      [,1] [,2]
> > [1,]    4    1
> > [2,]    3    2
> >
> > to
> >
> >      [,1] [,2]
> > [1,]    3    4
> > [2,]    2    1
> >
> > to
> >
> >      [,1] [,2]
> > [1,]    2    3
> > [2,]    1    4
> >
> > Sorry for being a muppet, B
> >
> >
> > Begin forwarded message:
> >
> > > From: Benjamin Lloyd-Hughes <blh at cpom.ucl.ac.uk>
> > > Date: 24 November 2005 16:15:58 GMT
> > > To: r-help at stat.math.ethz.ch
> > > Subject: Matrix rotation
> > >
> > > Dearest All,
> > >
> > > Ok so I've had a couple of glasses of wine over lunch today... This is
> > > likely to be trivial but I'm struggling to find a more elegant way to
> > > obtain the following matrix rotations:
> > >
> > > > M <- matrix(c(1,0,0,0), ncol=2)
> > > > M
> > >      [,1] [,2]
> > > [1,]    1    0
> > > [2,]    0    0
> > > > N <- abind(M[2,],M[1,],along=2)
> > > > N
> > >      [,1] [,2]
> > > [1,]    0    1
> > > [2,]    0    0
> > > > P <- abind(N[2,],N[1,],along=2)
> > > > P
> > >      [,1] [,2]
> > > [1,]    0    0
> > > [2,]    0    1
> > > > Q <- abind(P[,2],P[,1],along=2)
> > > > Q
> > >      [,1] [,2]
> > > [1,]    0    0
> > > [2,]    1    0
> > >
> > > And, more generally wish to rotate a n-dimensional data cube about
> > > some specified axis.
> > >
> > > Cheers, Ben
> > >
> >
> >        [[alternative text/enriched version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From ehlers at math.ucalgary.ca  Fri Nov 25 01:51:56 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Thu, 24 Nov 2005 17:51:56 -0700
Subject: [R] model selection with step function
In-Reply-To: <1132868033.438631c1a2cc5@webmail2.bignet.com.br>
References: <1132868033.438631c1a2cc5@webmail2.bignet.com.br>
Message-ID: <4386602C.4050106@math.ucalgary.ca>


Antonio Olinto wrote:

> Hello,
> 
> I have a doubt in using the function step (step wise) to select glm models.
> 
> Usually I apply the gamma distribution to analyze fishery data. To select the
> terms I use a routine where I first compare single term models to the null model
> (eg. U~1 vs. U~depth; U~1 vs. U~latitude; etc. ? where U= abundance) and, by
> means of the result given by a likelihook function applied for each comparison,
> I select the ?strongest? effect, let?s say depth. Then I run a new step
> comparing the U~depth vs. U~depth+latitude; U~depth vs. U~depth+... etc. Making
> this way I put the terms in ?magnitude? order.
> 
> I tried to make a gaussian model using the step(glm.model) function to select
> the terms but I saw that in the output table given by anova(glm.model) the
> selected terms kept the original order.
> 
> Is it possible to have the terms in the model rearranged, as in my example?
> 
> Thanks for any help. I read Chambers and Hastie?s ?Statistical Models in S?,
> Venables and Ripley ?Modern Applied Statistics? and, of course, R help but I
> couldn?t get the trick.
> 

I don't know if this will get you there, but

1. I would use stepAIC in package MASS;
2. set argument trace = TRUE;
3. think very hard about the interpretation of the model;
4. read also Frank Harrell's "Regression Modeling Strategies".

Peter


> Antonio
> 
> --
> Biologist
> Sao Paulo Fisheries Institute
> 
> 
> 
> -------------------------------------------------
> WebMail Bignet - O seu provedor do litoral
> www.bignet.com.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Peter Ehlers
Department of Mathematics and Statistics
University of Calgary, 2500 University Dr. NW       ph: 403-220-3936
Calgary, Alberta  T2N 1N4, CANADA                  fax: 403-282-5150



From guangxing at ict.ac.cn  Fri Nov 25 02:03:39 2005
From: guangxing at ict.ac.cn (=?gb2312?B?uePQxw==?=)
Date: Fri, 25 Nov 2005 09:03:39 +0800
Subject: [R] How to test a time series fit the Poisson or other process?
Message-ID: <200511250101.jAP11TuG028852@hypatia.math.ethz.ch>

Hi, R-Help,
I am a newbie.
what I concern most recently is the analysis of the time series,
But there are a lot of package in my eyes.
All I want to try is as follow:
How to test whether a time series fit the Poisson or other process in R? 

Thank you very much in advance.

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-25



From MSchwartz at mn.rr.com  Fri Nov 25 02:03:17 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 24 Nov 2005 19:03:17 -0600
Subject: [R] Chi-squared test
In-Reply-To: <XFMail.051124215517.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051124215517.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1132880598.4061.23.camel@localhost.localdomain>

On Thu, 2005-11-24 at 21:55 +0000, Ted Harding wrote:
> On 24-Nov-05 P Ehlers wrote:
> > Bianca Vieru- Dimulescu wrote:
> >> Hello,
> >> I'm trying to calculate a chi-squared test to see if my data are 
> >> different from the theoretical distribution or not:
> >> 
> >> chisq.test(rbind(c(79,52,69,71,82,87,95,74,55,78,49,60),
>                     c(80,80,80,80,80,80,80,80,80,80,80,80)))
> >> 
> >>       Pearson's Chi-squared test
> >> 
> >> data:  rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60),
> >>              c(80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80))
> >> X-squared = 17.6, df = 11, p-value = 0.09142
> >> 
> >> Is this correct? If I'm doing the same thing using Excel I obtained
> >> a different value of p.. (1.65778E-14)
> >> 
> >> Thanks a lot,
> >> Bianca
> > 
> > It would be unusual to have 12 observed frequencies all equal to 80.
> > So I'm guessing that you have a 12-category variable and want to
> > test its fit to a discrete uniform distribution. I assume that your
> > frequencies are
> > 
> > x <- c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60)
> > 
> > Then just use
> > 
> > chisq.test(x)
> > 
> > (see the help page).
> > 
> > (If those 80's are expected cell frequencies, they should sum to
> > sum(x) = 851.)
> > 
> > I don't know what Excel does.
> > 
> > Peter
> > 
> > Peter Ehlers
> > University of Calgary
> 
> I'm rather with Peter on this question! I've tried to infer what
> you're really trying to do.
> 
> My a-priori plausible hypothesis was that you have
> 
>   k<-12
> 
> independent observations which have equal expected values
> 
>   m<-rep(80,k)
> 
> and are observed as
> 
>   x<-c(79,52,69,71,82,87,95,74,55,78,49,60)
> 
> On this basis, a chi-squared test Sum((O-E)^2/E) gives
> 
>   C2<-sum(((x-m)^2)/m)
> 
> so C2 = 41.1375, and on this hypothesis the chi-squared would
> have k=12 degrees of freedom. Then:
> 
>   1-pchisq(C2,k)
> ## [1] 4.647553e-05
>
> which is nowhere near the 1.65778E-14 you report from Excel.
> Also, the result from Peter's chisq.test(x) is p = 0.0006468,
> even further away.

It's late on Turkey Day here, but shouldn't that be:

> 1 - pchisq(C2, k - 1)  # 11 df
[1] 2.282202e-05

which is what I get using OO.org's Calc 2.0 with the CHITEST function
using the two vectors as the observed (x) and expected (m) values. I
also get this result from Gnumeric 1.4.3 using the same CHITEST
function.

Using the CHIDIST function in OO.org's Calc:

=CHIDIST(41.1375;11)

I also get the same p value.

Lastly, I get the same results on my wife's computer using Excel 2002
and the CHITEST function, so Bianca may want to check for typos in the
Excel sheet, or the possibility that the wrong syntax was used in the
CHITEST function formula (ie. wrong cell range, etc.).

Lacking that, I too am confuzzled as to where the 1.65778E-14 comes
from.

HTH,

Marc Schwartz

<snip>



From p.dalgaard at biostat.ku.dk  Fri Nov 25 02:14:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Nov 2005 02:14:22 +0100
Subject: [R] Chi-squared test
In-Reply-To: <XFMail.051124215517.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051124215517.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2iruhwmtt.fsf@turmalin.kubism.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 24-Nov-05 P Ehlers wrote:
> > Bianca Vieru- Dimulescu wrote:
> >> Hello,
> >> I'm trying to calculate a chi-squared test to see if my data are 
> >> different from the theoretical distribution or not:
> >> 
> >> chisq.test(rbind(c(79,52,69,71,82,87,95,74,55,78,49,60),
>                     c(80,80,80,80,80,80,80,80,80,80,80,80)))
> >> 
> >>       Pearson's Chi-squared test
> >> 
> >> data:  rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60),
> >>              c(80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80))
> >> X-squared = 17.6, df = 11, p-value = 0.09142
> >> 
> >> Is this correct? If I'm doing the same thing using Excel I obtained
> >> a different value of p.. (1.65778E-14)
> >> 
> >> Thanks a lot,
> >> Bianca
> > 
> > It would be unusual to have 12 observed frequencies all equal to 80.
> > So I'm guessing that you have a 12-category variable and want to
> > test its fit to a discrete uniform distribution. I assume that your
> > frequencies are
> > 
> > x <- c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60)
> > 
> > Then just use
> > 
> > chisq.test(x)
> > 
> > (see the help page).
> > 
> > (If those 80's are expected cell frequencies, they should sum to
> > sum(x) = 851.)
> > 
> > I don't know what Excel does.
> > 
> > Peter
> > 
> > Peter Ehlers
> > University of Calgary
> 
> I'm rather with Peter on this question! I've tried to infer what
> you're really trying to do.
> 
> My a-priori plausible hypothesis was that you have
> 
>   k<-12
> 
> independent observations which have equal expected values
> 
>   m<-rep(80,k)
> 
> and are observed as
> 
>   x<-c(79,52,69,71,82,87,95,74,55,78,49,60)
> 
> On this basis, a chi-squared test Sum((O-E)^2/E) gives
> 
>   C2<-sum(((x-m)^2)/m)
> 
> so C2 = 41.1375, and on this hypothesis the chi-squared would
> have k=12 degrees of freedom. Then:
> 
>   1-pchisq(C2,k)
> ## [1] 4.647553e-05
> 
> which is nowhere near the 1.65778E-14 you report from Excel.
> Also, the result from Peter's chisq.test(x) is p = 0.0006468,
> even further away.
> 
> So this makes me really wonder what you are doing.
> 
> The nearest I can get to your Excel result 1.65778E-14 is
> 
>   ix<-x<m
>   prod(2*ppois(x[ix],m[ix]))*prod(2*(1-ppois(x[!ix],m[!ix])))
> ## 2.831963e-14
> 
> which is based on the guess that independent 2-sided Poisson
> tests of agreement between O and E have been carried out on each
> component, and the final P-value is the product of these P-values.
> 
> But this doesn't make a lot of sense from a statistical point
> of view, so it's time to stop guessing!
> 
> Please tell us what hypothesis you are testing, what sort of
> distribution the x-values are supposed to have, what the
> repeated "80" values represent, and also please tell us
> in detail what you asked Excel to do!
> 
> Then, perhaps, a useful reply can be made.

I think what Excel does is outlined here:

http://www.gifted.uconn.edu/siegle/research/ChiSquare/chiexcel.htm

(Notice the helpful wizard which in step 2 claims that you are doing a
test for independence, not for a given distribution.)

This would seem to coincide with Peter E's guess. The example on that
page matches chisq.test(c(10,3,2))

I believe that the expected values are expected (!) to sum to the
total counts. If they do not, I guess that Excel is numb-skulled
enough to compute sum((O-E)^2/E) anyway and look it up its p value
with k-1 DF. Still gets you nowhere near 1.6e-14 though.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ehlers at math.ucalgary.ca  Fri Nov 25 02:50:59 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Thu, 24 Nov 2005 18:50:59 -0700
Subject: [R] Chi-squared test
In-Reply-To: <1132880598.4061.23.camel@localhost.localdomain>
References: <XFMail.051124215517.Ted.Harding@nessie.mcc.ac.uk>
	<1132880598.4061.23.camel@localhost.localdomain>
Message-ID: <43866E03.60108@math.ucalgary.ca>


Marc Schwartz wrote:
> On Thu, 2005-11-24 at 21:55 +0000, Ted Harding wrote:
> 
>>On 24-Nov-05 P Ehlers wrote:
>>
>>>Bianca Vieru- Dimulescu wrote:
>>>
>>>>Hello,
>>>>I'm trying to calculate a chi-squared test to see if my data are 
>>>>different from the theoretical distribution or not:
>>>>
>>>>chisq.test(rbind(c(79,52,69,71,82,87,95,74,55,78,49,60),
>>
>>                    c(80,80,80,80,80,80,80,80,80,80,80,80)))
>>
>>>>      Pearson's Chi-squared test
>>>>
>>>>data:  rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60),
>>>>             c(80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80))
>>>>X-squared = 17.6, df = 11, p-value = 0.09142
>>>>
>>>>Is this correct? If I'm doing the same thing using Excel I obtained
>>>>a different value of p.. (1.65778E-14)
>>>>
>>>>Thanks a lot,
>>>>Bianca
>>>
>>>It would be unusual to have 12 observed frequencies all equal to 80.
>>>So I'm guessing that you have a 12-category variable and want to
>>>test its fit to a discrete uniform distribution. I assume that your
>>>frequencies are
>>>
>>>x <- c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60)
>>>
>>>Then just use
>>>
>>>chisq.test(x)
>>>
>>>(see the help page).
>>>
>>>(If those 80's are expected cell frequencies, they should sum to
>>>sum(x) = 851.)
>>>
>>>I don't know what Excel does.
>>>
>>>Peter
>>>
>>>Peter Ehlers
>>>University of Calgary
>>
>>I'm rather with Peter on this question! I've tried to infer what
>>you're really trying to do.
>>
>>My a-priori plausible hypothesis was that you have
>>
>>  k<-12
>>
>>independent observations which have equal expected values
>>
>>  m<-rep(80,k)
>>
>>and are observed as
>>
>>  x<-c(79,52,69,71,82,87,95,74,55,78,49,60)
>>
>>On this basis, a chi-squared test Sum((O-E)^2/E) gives
>>
>>  C2<-sum(((x-m)^2)/m)
>>
>>so C2 = 41.1375, and on this hypothesis the chi-squared would
>>have k=12 degrees of freedom. Then:
>>
>>  1-pchisq(C2,k)
>>## [1] 4.647553e-05
>>
>>which is nowhere near the 1.65778E-14 you report from Excel.
>>Also, the result from Peter's chisq.test(x) is p = 0.0006468,
>>even further away.
> 
> 
> It's late on Turkey Day here, but shouldn't that be:
> 
> 
>>1 - pchisq(C2, k - 1)  # 11 df
> 
> [1] 2.282202e-05
> 
> which is what I get using OO.org's Calc 2.0 with the CHITEST function
> using the two vectors as the observed (x) and expected (m) values. I
> also get this result from Gnumeric 1.4.3 using the same CHITEST
> function.
> 
[snip]

Marc, it's a bit sad to see that OO.org copies Excel's behaviour
to a _fault_. As Peter D. points out, we would expect the expected
frequencies and the observed frequencies to sum to the same value.
Excel (and Calc) blithely ignores that. R, OTH, gives an error
message when the probabilities don't sum to 1.

Turkey soup for a few days now?

Peter Ehlers



From suncertain at gmail.com  Fri Nov 25 02:52:19 2005
From: suncertain at gmail.com (Urania Sun)
Date: Thu, 24 Nov 2005 19:52:19 -0600
Subject: [R] residuals in logistic regression model
In-Reply-To: <20051124211002.NQUW21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>
References: <4ab0fb470511241035t58ea985amff856aa4649d0127@mail.gmail.com>
	<20051124211002.NQUW21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <4ab0fb470511241752j2da9e4d1oed3a1ae1f1baacf0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/78738df0/attachment.pl

From MSchwartz at mn.rr.com  Fri Nov 25 03:06:42 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 24 Nov 2005 20:06:42 -0600
Subject: [R] adding variables to a data set/combining two data sets
In-Reply-To: <000401c5f083$82fa4930$7000a8c0@Natasha>
References: <000401c5f083$82fa4930$7000a8c0@Natasha>
Message-ID: <1132884403.4061.27.camel@localhost.localdomain>

On Wed, 2005-11-23 at 16:13 -0700, nhepburn wrote:
> I have a couple of data sets that I want to combine into one data frame.
> One set contains a number of records on individual observations and includes
> a geographic descriptor called dacode.  The dacode is not unique in that
> table.  The other table contains a number of socio-economic variables for
> each of the geographic areas identified in the other table.  This second
> table also includes a variable called dacode and I have also used dacode for
> the rownames.  I want to pull a number of the variables from the
> socioeconomic status data into the first table but I'm having no luck.
> Here's what I have tried so far:
>  
> #first attempt
>  
> grade6DA$ses1 <- sesdata$ses1[as.character(grade6DA$dacode)]
>  
> All that this does is create a new column in the grade6DA data set and calls
> it ses1 but then fills it with NA.
>  
> 
> #second attempt
> grade6DA$ses1 <- sesdata$ses1[grade6DA$dacode]
>  
> 
> Neither approach has worked so I've ended up combining the tables in a MySQL
> database and then importing back into R.  However, it would be much easier
> if I could just manipulate the variables in R rather than going through
> MySQL everytime I want to try something new.  I've looked in the R-manuals
> but did not see anything about this - but I could have been looking in the
> wrong places.  Any ideas on how to accomplish what I am trying to do or
> advice on where to find the info would be greatly appreciated.
>  
> Cheers,
> Neil

Neil,

See ?merge which performs a SQL join type operation.

HTH,

Marc Schwartz

P.S. To R Core: help.search("join") does not seem to return merge(),
which would likely be helpful here.



From jfox at mcmaster.ca  Fri Nov 25 03:11:08 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 24 Nov 2005 21:11:08 -0500
Subject: [R] residuals in logistic regression model
In-Reply-To: <4ab0fb470511241752j2da9e4d1oed3a1ae1f1baacf0@mail.gmail.com>
Message-ID: <20051125021108.UPWA28424.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Urania,

> -----Original Message-----
> From: Urania Sun [mailto:suncertain at gmail.com] 
> Sent: Thursday, November 24, 2005 8:52 PM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] residuals in logistic regression model
> 
> Thanks, Professor. 
>  
> But is it ok to write residuals in the right hand side of the 
> logistic regression formula? Some people said I cannot since 
> the generalized linear model is to use a function to link the 
> expectation to a linear model. So there should not be 
> residuals in the right hand side. 
>  
> My question is that If residuals do exist (as in the glm 
> model output), why not put them in the formula (for example, 
> if I write the left-hand side as the estimated odds-ratio)?
> 

There are several kinds of residuals for generalized linear models, as I
mentioned (see ?residuals.glm). The residuals in the glm output are deviance
residuals, which are the casewise (signed) components of the residual
deviance; differences between y and fitted-y are called response residuals
(and aren't generally as useful). The left-hand side of a logit model
transformed with the logit-link is the log-odds, not the odds or odds-ratio.
The form of the model to which the response residuals applies has the
proportion, not the logit, on the left-hand side.

These matters are discussed in the references given in ?residuals.glm, and
in many other places, such as Sec. 6.6 of my R and S-PLUS Companion to
Applied Regression.

> Many thanks!
>  
> Happy Thanksgiving!

Unfortunately we celebrate Thanksgiving in Canada in October, probably
because the weather here in late November leaves little to be thankful for.

Regards,
 John

>  
> On 11/24/05, John Fox <jfox at mcmaster.ca> wrote: 
> 
> 	Dear Urania,
> 	
> 	The residuals method for glm objects can compute 
> several kinds of residuals;
> 	the default is deviance residuals. See ?residuals.glm 
> for details and 
> 	references.
> 	
> 	I hope this helps.
> 	John
> 	
> 	--------------------------------
> 	John Fox
> 	Department of Sociology
> 	McMaster University
> 	Hamilton, Ontario
> 	Canada L8S 4M4
> 	905-525-9140x23604
> 	http://socserv.mcmaster.ca/jfox 
> <http://socserv.mcmaster.ca/jfox> 
> 	--------------------------------
> 	
> 	> -----Original Message-----
> 	> From: r-help-bounces at stat.math.ethz.ch
> 	> [mailto: r-help-bounces at stat.math.ethz.ch] On Behalf 
> Of Urania Sun
> 	> Sent: Thursday, November 24, 2005 1:36 PM
> 	> To: r-help at stat.math.ethz.ch 
> 	> Subject: [R] residuals in logistic regression model
> 	>
> 	> In the logistic regression model, there is no residual
> 	>
> 	> log (pi/(1-pi)) = beta_0 + beta_1*X_1 + .....
> 	>
> 	> But glm model will return 
> 	>
> 	> residuals
> 	>
> 	> What is that?
> 	>
> 	> How to understand this? Can we put some residual in the
> 	> logistic regression model by replacing pi with pi' 
> (the estimated pi)?
> 	>
> 	>  log (pi'/(1-pi')) = beta_0 + beta_1*X_1 + .....+ ei 
> 	>
> 	> Thanks!
> 	>
> 	>       [[alternative HTML version deleted]]
> 	>
> 	> ______________________________________________
> 	> R-help at stat.math.ethz.ch mailing list
> 	> https://stat.ethz.ch/mailman/listinfo/r-help
> 	> PLEASE do read the posting guide!
> 	> http://www.R-project.org/posting-guide.html 
> <http://www.R-project.org/posting-guide.html> 
> 	
> 	
> 
> 
>



From suncertain at gmail.com  Fri Nov 25 03:32:43 2005
From: suncertain at gmail.com (Urania Sun)
Date: Thu, 24 Nov 2005 20:32:43 -0600
Subject: [R] residuals in logistic regression model
In-Reply-To: <20051125021108.UPWA28424.tomts25-srv.bellnexxia.net@JohnDesktop8300>
References: <4ab0fb470511241752j2da9e4d1oed3a1ae1f1baacf0@mail.gmail.com>
	<20051125021108.UPWA28424.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <4ab0fb470511241832h1445626ak54f8c338714284fd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051124/42cce62e/attachment.pl

From MSchwartz at mn.rr.com  Fri Nov 25 03:52:24 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 24 Nov 2005 20:52:24 -0600
Subject: [R] Chi-squared test
In-Reply-To: <43866E03.60108@math.ucalgary.ca>
References: <XFMail.051124215517.Ted.Harding@nessie.mcc.ac.uk>
	<1132880598.4061.23.camel@localhost.localdomain>
	<43866E03.60108@math.ucalgary.ca>
Message-ID: <1132887144.4061.46.camel@localhost.localdomain>

On Thu, 2005-11-24 at 18:50 -0700, P Ehlers wrote:
> Marc Schwartz wrote:
> > On Thu, 2005-11-24 at 21:55 +0000, Ted Harding wrote:
> > 
> >>On 24-Nov-05 P Ehlers wrote:
> >>
> >>>Bianca Vieru- Dimulescu wrote:
> >>>
> >>>>Hello,
> >>>>I'm trying to calculate a chi-squared test to see if my data are 
> >>>>different from the theoretical distribution or not:
> >>>>
> >>>>chisq.test(rbind(c(79,52,69,71,82,87,95,74,55,78,49,60),
> >>
> >>                    c(80,80,80,80,80,80,80,80,80,80,80,80)))
> >>
> >>>>      Pearson's Chi-squared test
> >>>>
> >>>>data:  rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60),
> >>>>             c(80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80))
> >>>>X-squared = 17.6, df = 11, p-value = 0.09142
> >>>>
> >>>>Is this correct? If I'm doing the same thing using Excel I obtained
> >>>>a different value of p.. (1.65778E-14)
> >>>>
> >>>>Thanks a lot,
> >>>>Bianca
> >>>
> >>>It would be unusual to have 12 observed frequencies all equal to 80.
> >>>So I'm guessing that you have a 12-category variable and want to
> >>>test its fit to a discrete uniform distribution. I assume that your
> >>>frequencies are
> >>>
> >>>x <- c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60)
> >>>
> >>>Then just use
> >>>
> >>>chisq.test(x)
> >>>
> >>>(see the help page).
> >>>
> >>>(If those 80's are expected cell frequencies, they should sum to
> >>>sum(x) = 851.)
> >>>
> >>>I don't know what Excel does.
> >>>
> >>>Peter
> >>>
> >>>Peter Ehlers
> >>>University of Calgary
> >>
> >>I'm rather with Peter on this question! I've tried to infer what
> >>you're really trying to do.
> >>
> >>My a-priori plausible hypothesis was that you have
> >>
> >>  k<-12
> >>
> >>independent observations which have equal expected values
> >>
> >>  m<-rep(80,k)
> >>
> >>and are observed as
> >>
> >>  x<-c(79,52,69,71,82,87,95,74,55,78,49,60)
> >>
> >>On this basis, a chi-squared test Sum((O-E)^2/E) gives
> >>
> >>  C2<-sum(((x-m)^2)/m)
> >>
> >>so C2 = 41.1375, and on this hypothesis the chi-squared would
> >>have k=12 degrees of freedom. Then:
> >>
> >>  1-pchisq(C2,k)
> >>## [1] 4.647553e-05
> >>
> >>which is nowhere near the 1.65778E-14 you report from Excel.
> >>Also, the result from Peter's chisq.test(x) is p = 0.0006468,
> >>even further away.
> > 
> > 
> > It's late on Turkey Day here, but shouldn't that be:
> > 
> > 
> >>1 - pchisq(C2, k - 1)  # 11 df
> > 
> > [1] 2.282202e-05
> > 
> > which is what I get using OO.org's Calc 2.0 with the CHITEST function
> > using the two vectors as the observed (x) and expected (m) values. I
> > also get this result from Gnumeric 1.4.3 using the same CHITEST
> > function.
> > 
> [snip]
> 
> Marc, it's a bit sad to see that OO.org copies Excel's behaviour
> to a _fault_. As Peter D. points out, we would expect the expected
> frequencies and the observed frequencies to sum to the same value.
> Excel (and Calc) blithely ignores that. R, OTH, gives an error
> message when the probabilities don't sum to 1.

Peter, yes indeed. If you search the archives, you see a thread here:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/18179.html

and

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/18474.html

where some discussion on this occurred within the context of rounding
issues and IEEE 754 compliance. Calc has truly copied Excel's behavior
to a fault, since the intention is to be a "drop-in" replacement for the
latter. At least Gnumeric has not done so in all cases, though it has
here.

Calc and Gnumeric indicate that CHITEST is a test for independence, not
for goodness of fit. I did not pay attention to Excel's description, but
presumably it is similar. Clearly no checks on O vs E sums though in any
of these apps.

Further data to reinforce the notion of not using spreadsheets for this.

> Turkey soup for a few days now?

Yes, indeed, along with turkey salad, turkey sandwiches...  :-)

My son is home from McGill in Montreal for the weekend, so he gets to
celebrate Thanksgiving a second time. He can help to reduce the turkey
inventory before flying back on Sunday...  ;-)

Best regards,

Marc



From hlk23 at cornell.edu  Fri Nov 25 05:23:42 2005
From: hlk23 at cornell.edu (Holger Lutz Kern)
Date: Thu, 24 Nov 2005 23:23:42 -0500
Subject: [R] (no subject)
Message-ID: <438691CE.10700@cornell.edu>

Hi all,

does anyone know if there exists some library that implements the 
sensitivity tests for matched pairs and unmatched groups proposed in 
Rosenbaum's Observational Studies (2002: ch.4)?

Cheers,
Holger



From itsme_410 at yahoo.com  Fri Nov 25 06:22:47 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Thu, 24 Nov 2005 21:22:47 -0800 (PST)
Subject: [R] OT: algorithm for generating all possible combinations with
	replacement
Message-ID: <20051125052247.55386.qmail@web54504.mail.yahoo.com>

Dear all,

I have n objects and I want to select k of these with replacement. Do you know
of code which would generate all the possible arrangements? Note that this is
different from the selection of k of n objects without replacement and wanting
to generate all the possible permutations.

Any suggestions? Existing C code would be fantastic btw, but I would be happy
with an algorithm.

Best wishes!



From kristel.joossens at econ.kuleuven.be  Fri Nov 25 08:56:11 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Fri, 25 Nov 2005 08:56:11 +0100
Subject: [R] OT: algorithm for generating all possible combinations with
 replacement
In-Reply-To: <20051125052247.55386.qmail@web54504.mail.yahoo.com>
References: <20051125052247.55386.qmail@web54504.mail.yahoo.com>
Message-ID: <4386C39B.3020300@econ.kuleuven.be>

I think you just need a sample with replacement ....

sample(1:n,k,replace=TRUE)

Best regards,
Kristel


Globe Trotter wrote:
> Dear all,
> 
> I have n objects and I want to select k of these with replacement. Do you know
> of code which would generate all the possible arrangements? Note that this is
> different from the selection of k of n objects without replacement and wanting
> to generate all the possible permutations.
> 
> Any suggestions? Existing C code would be fantastic btw, but I would be happy
> with an algorithm.
> 
> Best wishes!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From aleszib at gmail.com  Fri Nov 25 08:57:17 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Fri, 25 Nov 2005 08:57:17 +0100
Subject: [R] OT: algorithm for generating all possible combinations
	withreplacement
References: <20051125052247.55386.qmail@web54504.mail.yahoo.com>
Message-ID: <076201c5f195$dfa43930$0100a8c0@ALES>

Does this do what you want?
n<-10
k<-5
expand.grid(rep(list(1:n),k))

However be carful, even this small example prouces matrix with100000 rows.

Best,
Ales Ziberna
----- Original Message ----- 
From: "Globe Trotter" <itsme_410 at yahoo.com>
To: <fedora-list at redhat.com>; <r-help at stat.math.ethz.ch>
Sent: Friday, November 25, 2005 6:22 AM
Subject: [R] OT: algorithm for generating all possible combinations 
withreplacement


> Dear all,
>
> I have n objects and I want to select k of these with replacement. Do you 
> know
> of code which would generate all the possible arrangements? Note that this 
> is
> different from the selection of k of n objects without replacement and 
> wanting
> to generate all the possible permutations.
>
> Any suggestions? Existing C code would be fantastic btw, but I would be 
> happy
> with an algorithm.
>
> Best wishes!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From aleszib at gmail.com  Fri Nov 25 09:05:28 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Fri, 25 Nov 2005 09:05:28 +0100
Subject: [R] Survreg Weibull lambda and p
References: <E76EB96029DCAE4A9CB967D7F6712D1DBFD66E@NANAMAILBACK1.nanamail.co.il>
Message-ID: <079d01c5f197$0374d030$0100a8c0@ALES>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051125/151e7d82/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Nov 25 09:36:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Nov 2005 09:36:00 +0100
Subject: [R] OT: algorithm for generating all possible combinations
	withreplacement
In-Reply-To: <076201c5f195$dfa43930$0100a8c0@ALES>
References: <20051125052247.55386.qmail@web54504.mail.yahoo.com>
	<076201c5f195$dfa43930$0100a8c0@ALES>
Message-ID: <x21x155dlb.fsf@turmalin.kubism.ku.dk>

"Ales Ziberna" <aleszib at gmail.com> writes:

> Does this do what you want?
> n<-10
> k<-5
> expand.grid(rep(list(1:n),k))
> 
> However be carful, even this small example prouces matrix with100000 rows.

I'm not sure this is what was wanted, since it is generating the same
samples multiple times. Consider

m <- expand.grid(rep(list(1:3),3))

table(apply(apply(m,1,sort),2,paste,collapse=" "))

1 1 1 1 1 2 1 1 3 1 2 2 1 2 3 1 3 3 2 2 2 2 2 3 2 3 3 3 3 3
    1     3     3     3     6     3     1     3     3     1

It is possible that "Globe Trotter" wanted essentially the names of
the table, or maybe unique(t(apply(m,1,sort))). There's probably a
faster way of generating those. However, to use them for anything
probabilistic, you also need to keep track of the counts (using
multinomial coefficients).
 

> Best,
> Ales Ziberna
> ----- Original Message ----- 
> From: "Globe Trotter" <itsme_410 at yahoo.com>
> To: <fedora-list at redhat.com>; <r-help at stat.math.ethz.ch>
> Sent: Friday, November 25, 2005 6:22 AM
> Subject: [R] OT: algorithm for generating all possible combinations 
> withreplacement
> 
> 
> > Dear all,
> >
> > I have n objects and I want to select k of these with replacement. Do you 
> > know
> > of code which would generate all the possible arrangements? Note that this 
> > is
> > different from the selection of k of n objects without replacement and 
> > wanting
> > to generate all the possible permutations.
> >
> > Any suggestions? Existing C code would be fantastic btw, but I would be 
> > happy
> > with an algorithm.
> >
> > Best wishes!
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From glsmah001 at mail.uct.ac.za  Fri Nov 25 10:25:22 2005
From: glsmah001 at mail.uct.ac.za (Maha Golestaneh)
Date: Fri, 25 Nov 2005 11:25:22 +0200
Subject: [R] Tiger Mac stalls running Rcmdr program
Message-ID: <BFACA522.BEF%glsmah001@mail.uct.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051125/5c34a652/attachment.pl

From hacker-24 at versanet.de  Fri Nov 25 10:58:11 2005
From: hacker-24 at versanet.de (hacker-24@versanet.de)
Date: Fri, 25 Nov 2005 10:58:11 +0100
Subject: [R] Problems compiling R under AIX 4.3
Message-ID: <20051125105811.rhiamoxcay8sswo4@webmail.versatel.de>

Hi,

I have problems compiling R 2.2.0 under AIX 4.3 with GCC and xlf95 (FORTRAN)
Compilers.


here the error message I got:

make[1]: Entering directory `/home/mau/sy/R-2.2.0/src/modules/X11'
make[1]: `Makedeps' is up to date.
make[1]: Leaving directory `/home/mau/sy/R-2.2.0/src/modules/X11'
make[1]: Entering directory `/home/mau/sy/R-2.2.0/src/modules/X11'
/usr/local/bin/gcc -Wl,-bM:SRE -Wl,-H512 -Wl,-T512 -Wl,-bnoentry -Wl,-bexpall
-Wl,-bI:../../../etc/R.exp -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo
rotated.lo rbitmap.lo  -lSM -lICE -lX11  -ljpeg -lpng -lz
/usr/local/lib/gcc/powerpc-ibm-aix4.3.2.0/3.4.3/../../../../powerpc-ibm-aix4.3.2.0/bin/ld:
-static and -shared may not be used together
collect2: ld returned 1 exit status
make[1]: *** [R_X11.so] Error 1
make[1]: Leaving directory `/home/mau/sy/R-2.2.0/src/modules/X11'
make: *** [R] Error 2


I do not see where I use -dynamic and/or -static.

If I use the IMB linker I got:

with IBM linker (ld):
$ ld -b32 -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo rotated.lo
rbitmap.lo  -lSM -lICE -lX11  -ljpeg -lpng -lz
ld: 0711-327 WARNING: Entry point not found: __start
ld: 0711-244 ERROR: No csects or exported symbols have been saved.



Any help are welcome - Is there anybody with the same configuration (AIX
4.3,gcc,xlf95) and has build a working R ?

Thanks
Reinhard



From leog at anicca-vijja.de  Fri Nov 25 11:00:50 2005
From: leog at anicca-vijja.de (=?ISO-8859-1?Q?Leo_G=FCrtler?=)
Date: Fri, 25 Nov 2005 11:00:50 +0100
Subject: [R] multiple imputation of anova tables
Message-ID: <4386E0D2.2080503@anicca-vijja.de>

Dear list members,

how can multiple imputation realized for anova tables in R? Concretely, 
how to combine

F-values and R^2, R^2_adjusted from multiple imputations in R?

Of course, the point estimates can be averaged, but how to get 
standarderrors for F-values/R^2 etc. in R?
For linear models, lm.mids() works well, but according to Rubins rules, 
standard errors have to be used together with the estimates to get 
unbiased estimates. The same is needed for lme models. For the 
regression coefficients of lme, it is no problem, because s.e.'s are 
present. But how to combine AIC/ BIC,loglik and especially how to 
proceed with the random effects in lme's? I assume there is a general 
rule which can be applied to all these cases, but I do not get it right.

e.g.

 > anova(limo1)
Analysis of Variance Table

Response: lverb.ona
           Df Sum Sq Mean Sq F value  Pr(>F)
klasse      6  301.6    50.3  2.0985 0.05514 .
Residuals 193 4623.3    24.0
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

or (from the manpage of lme)

 >      summary(fm2)
Linear mixed-effects model fit by REML
 Data: Orthodont
       AIC      BIC    logLik
  447.5125 460.7823 -218.7563

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    1.807425 1.431592

Fixed effects: distance ~ age + Sex
                Value Std.Error DF   t-value p-value
(Intercept) 17.706713 0.8339225 80 21.233044  0.0000
age          0.660185 0.0616059 80 10.716263  0.0000
SexFemale   -2.321023 0.7614168 25 -3.048294  0.0054
 Correlation:
          (Intr) age  
age       -0.813      
SexFemale -0.372  0.000

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max
-3.74889609 -0.55034466 -0.02516628  0.45341781  3.65746539

Number of Observations: 108
Number of Groups: 27
 >

and the ANOVA of the lme:

 > anova(fm2)
            numDF denDF  F-value p-value
(Intercept)     1    80 4123.156  <.0001
age             1    80  114.838  <.0001
Sex             1    25    9.292  0.0054

I am confused about that and I did not find any hint in norm, 
mice/pan/mix or Hmisc.

Any help and hints are appreciated,

best regards

Leo G??rtler / Germany



From Ted.Harding at nessie.mcc.ac.uk  Fri Nov 25 11:06:08 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 25 Nov 2005 10:06:08 -0000 (GMT)
Subject: [R] Chi-squared test
In-Reply-To: <1132880598.4061.23.camel@localhost.localdomain>
Message-ID: <XFMail.051125100608.Ted.Harding@nessie.mcc.ac.uk>

On 25-Nov-05 Marc Schwartz wrote:
> On Thu, 2005-11-24 at 21:55 +0000, Ted Harding wrote:
>> I'm rather with Peter on this question! I've tried to infer what
>> you're really trying to do.
>> 
>> My a-priori plausible hypothesis was that you have
>> 
>>   k<-12
>> 
>> independent observations which have equal expected values
>> 
>>   m<-rep(80,k)
>> 
>> and are observed as
>> 
>>   x<-c(79,52,69,71,82,87,95,74,55,78,49,60)
>> 
>> On this basis, a chi-squared test Sum((O-E)^2/E) gives
>> 
>>   C2<-sum(((x-m)^2)/m)
>> 
>> so C2 = 41.1375, and on this hypothesis the chi-squared would
>> have k=12 degrees of freedom. Then:
>> 
>>   1-pchisq(C2,k)
>> ## [1] 4.647553e-05
>>[...]
> 
> It's late on Turkey Day here, but shouldn't that be:
> 
>> 1 - pchisq(C2, k - 1)  # 11 df
> [1] 2.282202e-05

I don't think so ... I was trying to guess at a "chi=squareed"
scenario which included, in particular, the feature that the
observed do not sum to the same value as the expected. So I tried
the case in the 12 expected values have been given (in this case
all "80"), and the 12 independent observed values are to be compared
with them. This is 12 independent comparisons, and there is nothing
to be fitted, and no overall constraint on the observed values.
Hence 12 d.f.

> [...]
> Lastly, I get the same results on my wife's computer using Excel 2002
> and the CHITEST function, so Bianca may want to check for typos in the
> Excel sheet, or the possibility that the wrong syntax was used in the
> CHITEST function formula (ie. wrong cell range, etc.).
> 
> Lacking that, I too am confuzzled as to where the 1.65778E-14 comes
> from.

Confuzzled indeed ... ! Not gallopavated, though :)

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 25-Nov-05                                       Time: 10:06:06
------------------------------ XFMail ------------------------------



From aleszib at gmail.com  Fri Nov 25 11:13:07 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Fri, 25 Nov 2005 11:13:07 +0100
Subject: [R] Generating all possible partitions
Message-ID: <081201c5f1a8$d67886f0$0100a8c0@ALES>

Hellp!



I would like to generate all possible partitions of length n with k 
clusters. It should be noted that the labels of the classes are irrelevant.



Thanks for any suggestions!


Best regards,

Ales Ziberna



From bianca.vieru at free.fr  Fri Nov 25 11:33:37 2005
From: bianca.vieru at free.fr (Bianca Vieru-Dimulescu)
Date: Fri, 25 Nov 2005 11:33:37 +0100
Subject: [R] Chi-squared test
In-Reply-To: <43866E03.60108@math.ucalgary.ca>
References: <XFMail.051124215517.Ted.Harding@nessie.mcc.ac.uk>
	<1132880598.4061.23.camel@localhost.localdomain>
	<43866E03.60108@math.ucalgary.ca>
Message-ID: <4386E881.3020109@free.fr>

P Ehlers wrote:

>
> Marc Schwartz wrote:
>
>> On Thu, 2005-11-24 at 21:55 +0000, Ted Harding wrote:
>>
>>> On 24-Nov-05 P Ehlers wrote:
>>>
>>>> Bianca Vieru- Dimulescu wrote:
>>>>
>>>>> Hello,
>>>>> I'm trying to calculate a chi-squared test to see if my data are 
>>>>> different from the theoretical distribution or not:
>>>>>
>>>>> chisq.test(rbind(c(79,52,69,71,82,87,95,74,55,78,49,60),
>>>>
>>>
>>>                    c(80,80,80,80,80,80,80,80,80,80,80,80)))
>>>
>>>>>      Pearson's Chi-squared test
>>>>>
>>>>> data:  rbind(c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60),
>>>>>             c(80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80))
>>>>> X-squared = 17.6, df = 11, p-value = 0.09142
>>>>>
>>>>> Is this correct? If I'm doing the same thing using Excel I obtained
>>>>> a different value of p.. (1.65778E-14)
>>>>>
>>>>> Thanks a lot,
>>>>> Bianca
>>>>
>>>>
>>>> It would be unusual to have 12 observed frequencies all equal to 80.
>>>> So I'm guessing that you have a 12-category variable and want to
>>>> test its fit to a discrete uniform distribution. I assume that your
>>>> frequencies are
>>>>
>>>> x <- c(79, 52, 69, 71, 82, 87, 95, 74, 55, 78, 49, 60)
>>>>
>>>> Then just use
>>>>
>>>> chisq.test(x)
>>>>
>>>> (see the help page).
>>>>
>>>> (If those 80's are expected cell frequencies, they should sum to
>>>> sum(x) = 851.)
>>>>
>>>> I don't know what Excel does.
>>>>
>>>> Peter
>>>>
>>>> Peter Ehlers
>>>> University of Calgary
>>>
>>>
>>> I'm rather with Peter on this question! I've tried to infer what
>>> you're really trying to do.
>>>
>>> My a-priori plausible hypothesis was that you have
>>>
>>>  k<-12
>>>
>>> independent observations which have equal expected values
>>>
>>>  m<-rep(80,k)
>>>
>>> and are observed as
>>>
>>>  x<-c(79,52,69,71,82,87,95,74,55,78,49,60)
>>>
>>> On this basis, a chi-squared test Sum((O-E)^2/E) gives
>>>
>>>  C2<-sum(((x-m)^2)/m)
>>>
>>> so C2 = 41.1375, and on this hypothesis the chi-squared would
>>> have k=12 degrees of freedom. Then:
>>>
>>>  1-pchisq(C2,k)
>>> ## [1] 4.647553e-05
>>>
>>> which is nowhere near the 1.65778E-14 you report from Excel.
>>> Also, the result from Peter's chisq.test(x) is p = 0.0006468,
>>> even further away.
>>
>>
>>
>> It's late on Turkey Day here, but shouldn't that be:
>>
>>
>>> 1 - pchisq(C2, k - 1)  # 11 df
>>
>>
>> [1] 2.282202e-05
>>
>> which is what I get using OO.org's Calc 2.0 with the CHITEST function
>> using the two vectors as the observed (x) and expected (m) values. I
>> also get this result from Gnumeric 1.4.3 using the same CHITEST
>> function.
>>
> [snip]
>
> Marc, it's a bit sad to see that OO.org copies Excel's behaviour
> to a _fault_. As Peter D. points out, we would expect the expected
> frequencies and the observed frequencies to sum to the same value.
> Excel (and Calc) blithely ignores that. R, OTH, gives an error
> message when the probabilities don't sum to 1.
>
> Turkey soup for a few days now? 

Thanks a lot for your answers! I have a fault in my Excel sheet:(,
sorry. I corrected it and indeed I obtained 2.282202e-05

As  I want to make a comparaison between independent observations which
have equal expected values, I will do as Marc suggested and give up at
the idea of using excel:)

Bianca



From a.menicacci at fr.fournierpharma.com  Fri Nov 25 11:32:24 2005
From: a.menicacci at fr.fournierpharma.com (a.menicacci@fr.fournierpharma.com)
Date: Fri, 25 Nov 2005 11:32:24 +0100
Subject: [R] covariance analysis by using R
Message-ID: <OF430988D7.47F14CBC-ONC12570C4.0034D4EF-C12570C4.0039E635@fr.fournierpharma.com>





Hi,

Is anyone has solved MR Xin Meng problem (see below) ?

We have the same analysis configuration : 10 groups (including control one)
with 2 mesures for each (ref at t0 and response at t1).

We expect to compare each group response with control response (group 1)
using a multiple comparison procedure (Dunnett test).

In order to perform this test, we have to normalize our data (as you) to
correct response values by the base line normalized.

Covariance analysis seems to represent the best way to do this. But how to
perform this by using R ?

So, if  someone is able to deal with this problem, could you please share
with us your precious knowledge ?

Thanks in advance,

Best Regards.




Alexandre MENICACCI
Bioinformatics - FOURNIER PHARMA
50, rue de Dijon - 21121 Daix - FRANCE
a.menicacci at fr.fournierpharma.com
t??l : 03.80.44.76.17



Original message :

Hello sir:
Here's a question on covariance analysis which needs your help. There're 3
experiments,and x refers to control while y refers to experimental result.
The purpose is to compare the "y" values across the 3 experiments.


experiment_1:
x:0.1 0.2 0.3 0.4 0.5
y:0.5 0.6 0.6 0.7 0.9


experiment_2:
x:1 2 3 4 5
y:3 4 6.5 7.5 11


experiment_3:
x:10 20 30 40 50
y:18 35 75 90 98


Apparently,the control("x") isn't at the similar level so that we can't
compare the "y" directly through ANOVA. We must normalize "y" via "x" in
order to eliminate the influence of different level of "x". The method of
normalize I can get is "covariance analysis",since "x" is the covariant of
y.


My question is:
How to perform "covariance analysis" by using R? After this
normalization,we can get the according "normalized y" of every "original
y".


All in all,the "normalized y" of every "original y" is what I want indeed.


Thanks a lot!


My best regards!



From j.logsdon at quantex-research.com  Fri Nov 25 11:38:55 2005
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Fri, 25 Nov 2005 10:38:55 +0000 (GMT)
Subject: [R] Ordering problem
Message-ID: <Pine.LNX.4.10.10511250932040.24834-100000@quantex-research.co.uk>

I have an ordering and factor problem to which there must be a simple
solution!  The version is R 2.0.1  (2004-11-15) on A Linux platform.

A data frame H is read in from a .csv file using read.csv with as.is=TRUE.  

Another data frame HN is constructed from data and I want to compare two
columns both named ss of the (sorted) data frames that are the same
length.

The problem is that HN$ss is always treated as a factor whatever I do
while H$ss is treated as an integer, which is what I want.  Somewhere R is
making an implicit transformation but I can't see how to correct it.

The data are all integers in the range 1:13 - in fact with no gaps.  If I
tabulate from H:

> table(H$ss)

   1    2    3    4    5    6    7    8    9   10   11   12   13 
 176  176  176  176  176  176  341 8726 8784 8777 8773 8749 8747 

and for HN:

> table(HN$ss)

   1   10   11   12   13    2    3    4    5    6    7    8    9 
 176 8777 8773 8749 8747  176  176  176  176  176  341 8726 8784 

At some time while constructing HN, I have to make it a character matrix -
otherwise gsub doesn't work when removing surplus blanks for example - but
I have turned it back into a data frame in the end.

If I check the modes, both data frames are lists and both columns are
numeric - HN is not reported as a factor.  Yet it appears to be treated as
a factor, for example:

> table(formatC(H$ss,dig=0,width=2,format="f",flag="0"))

  01   02   03   04   05   06   07   08   09   10   11   12   13 
 176  176  176  176  176  176  341 8726 8784 8777 8773 8749 8747 
> table(formatC(HN$ss,dig=0,width=2,format="f",flag="0"))

yet:

   1   10   11   12   13    2    3    4    5    6    7    8    9 
 176 8777 8773 8749 8747  176  176  176  176  176  341 8726 8784 
Warning messages: 
1: "+" not meaningful for factors in: Ops.factor(x, ifelse(x == 0, 1, 0)) 
2: "<" not meaningful for factors in: Ops.factor(x, 0) 

I have tried as.numeric but then I get the factor level rather than name
returned:

> table(formatC(as.numeric(HN$ss),dig=0,width=2,format="f",flag="0"))

  01   02   03   04   05   06   07   08   09   10   11   12   13 
 176 8777 8773 8749 8747  176  176  176  176  176  341 8726 8784 

which obviously is a tabulation of the internal levels rather than the
data.

TIA

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com



From fcombes at gmail.com  Fri Nov 25 12:00:03 2005
From: fcombes at gmail.com (Florence Combes)
Date: Fri, 25 Nov 2005 12:00:03 +0100
Subject: [R] Ordering problem
In-Reply-To: <Pine.LNX.4.10.10511250932040.24834-100000@quantex-research.co.uk>
References: <Pine.LNX.4.10.10511250932040.24834-100000@quantex-research.co.uk>
Message-ID: <73dae3060511250300h733d985fw29bcd53dd95f1950@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051125/9de5479e/attachment.pl

From fcombes at gmail.com  Fri Nov 25 12:06:43 2005
From: fcombes at gmail.com (Florence Combes)
Date: Fri, 25 Nov 2005 12:06:43 +0100
Subject: [R] Ordering problem
In-Reply-To: <Pine.LNX.4.10.10511250932040.24834-100000@quantex-research.co.uk>
References: <Pine.LNX.4.10.10511250932040.24834-100000@quantex-research.co.uk>
Message-ID: <73dae3060511250306ob8bc025gdda980f1589f396d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051125/b734ed08/attachment.pl

From aolinto_r at bignet.com.br  Fri Nov 25 12:47:38 2005
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Fri, 25 Nov 2005 09:47:38 -0200
Subject: [R] model selection with step function
In-Reply-To: <4386602C.4050106@math.ucalgary.ca>
References: <1132868033.438631c1a2cc5@webmail2.bignet.com.br>
	<4386602C.4050106@math.ucalgary.ca>
Message-ID: <1132919258.4386f9daf301f@webmail2.bignet.com.br>

Dear Ehlers, thanks for your message.

Following the example on stepAIC and Venebles & Ripley?s book, it seems that
update rearranges the terms. I didn?t understand how to indicate the formula in
the function.

I have the initial model U ~ var1+var2+var3+var4 (family Gaussian). I want first
to select the terms, putting main effects first. If I just write step(model) it
will take out no significant variables (lets say var2) and will give U ~
var1+var3+var4. Supposing the var4 have the ?strongest? effect upon U, followed
by var1 and var3, I would like to have the out put U ~ var4+var1+var2.

Is it possible to do so?

Thanks again.

Antonio Olinto
Biologist
Sao Paulo Fisheries Institute

PS. Unfortunately I don?t have Regression Modeling Strategies around here


Citando P Ehlers <ehlers at math.ucalgary.ca>:

> 
> Antonio Olinto wrote:
> 
> > Hello,
> > 
> > I have a doubt in using the function step (step wise) to select glm
> models.
> > 
> > Usually I apply the gamma distribution to analyze fishery data. To select
> the
> > terms I use a routine where I first compare single term models to the null
> model
> > (eg. U~1 vs. U~depth; U~1 vs. U~latitude; etc. ? where U= abundance) and,
> by
> > means of the result given by a likelihook function applied for each
> comparison,
> > I select the ?strongest? effect, let?s say depth. Then I run a new step
> > comparing the U~depth vs. U~depth+latitude; U~depth vs. U~depth+... etc.
> Making
> > this way I put the terms in ?magnitude? order.
> > 
> > I tried to make a gaussian model using the step(glm.model) function to
> select
> > the terms but I saw that in the output table given by anova(glm.model) the
> > selected terms kept the original order.
> > 
> > Is it possible to have the terms in the model rearranged, as in my
> example?
> > 
> > Thanks for any help. I read Chambers and Hastie?s ?Statistical Models in
> S?,
> > Venables and Ripley ?Modern Applied Statistics? and, of course, R help but
> I
> > couldn?t get the trick.
> > 
> 
> I don't know if this will get you there, but
> 
> 1. I would use stepAIC in package MASS;
> 2. set argument trace = TRUE;
> 3. think very hard about the interpretation of the model;
> 4. read also Frank Harrell's "Regression Modeling Strategies".
> 
> Peter
> 
> 
> > Antonio
> > 
> > --
> > Biologist
> > Sao Paulo Fisheries Institute
> > 
> > 
> > 
> > -------------------------------------------------
> > WebMail Bignet - O seu provedor do litoral
> > www.bignet.com.br
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> -- 
> Peter Ehlers
> Department of Mathematics and Statistics
> University of Calgary, 2500 University Dr. NW       ph: 403-220-3936
> Calgary, Alberta  T2N 1N4, CANADA                  fax: 403-282-5150
> 
> 
> 



-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br



From szlevine at nana.co.il  Fri Nov 25 12:52:05 2005
From: szlevine at nana.co.il (Stephen)
Date: Fri, 25 Nov 2005 13:52:05 +0200
Subject: [R] Survreg Weibull lambda and p
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD670@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051125/f4b23cb4/attachment.pl

From koen.hufkens at telenet.be  Fri Nov 25 12:54:32 2005
From: koen.hufkens at telenet.be (Koen Hufkens)
Date: Fri, 25 Nov 2005 12:54:32 +0100
Subject: [R] wavethresh addon package LS2W
Message-ID: <4386FB78.40609@telenet.be>

Hi list,

I've been trying to install the LS2W package in R. It's originally a 
S+plus package so things don't work out as planned. Has anyone 
succesfully installed this package in R (2.2.0)?

Cheers,
Koen



From j.logsdon at quantex-research.com  Fri Nov 25 13:25:11 2005
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Fri, 25 Nov 2005 12:25:11 +0000 (GMT)
Subject: [R] Ordering problem
In-Reply-To: <73dae3060511250300h733d985fw29bcd53dd95f1950@mail.gmail.com>
Message-ID: <Pine.LNX.4.10.10511251135490.32264-100000@quantex-research.co.uk>

Thanks to Florence but it needs a little modification.  However as I have
now discovered the str() command, things are looking up.:))

I have a character matrix so I() just leaves it as characters whereas I
want the various columns to be integers or whatever they contain.

To take Florence's example slightly extended:

> v1<-c(1,2,3);v2<-c("a","b","c");v3<-c("1","2","3")

Note that the third vector is a character with numerical contents.

> data.frame(v1,v2,v3)
  v1 v2 v3
1  1  a  1
2  2  b  2
3  3  c  3

so it looks OK, but

> str(data.frame(v1,v2,v3))
`data.frame':   3 obs. of  3 variables:
 $ v1: num  1 2 3
 $ v2: Factor w/ 3 levels "a","b","c": 1 2 3
 $ v3: Factor w/ 3 levels "1","2","3": 1 2 3

reveals the nasty truth!

whereas

> str(data.frame(v1,v2,I(v3)))
`data.frame':   3 obs. of  3 variables:
 $ v1: num  1 2 3
 $ v2: Factor w/ 3 levels "a","b","c": 1 2 3
 $ v3:Class 'AsIs'  chr [1:3] "1" "2" "3"

just keeps the character v3 as characters.  I want it to be interpreted as
numeric so:

> str(data.frame(v1,v2,as.numeric(v3)))
`data.frame':   3 obs. of  3 variables:
 $ v1            : num  1 2 3
 $ v2            : Factor w/ 3 levels "a","b","c": 1 2 3
 $ as.numeric.v3.: num  1 2 3

actually gives me what I need.  

The only problem is that I have to do everything column by column and
there are 15 cols all.  So it makes particularly ugly coding to reproduce
an as.is read from a .csv file. 

The other solutions from Baz and Carlos would also work of course - but
they are still pretty horrible.  Perhaps another way to do this is to
write it out using cat then read it in again using as.is=TRUE!! ;)

Thanks to one and all

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


On Fri, 25 Nov 2005, Florence Combes wrote:

> John,
> 
> at ?factor, you can see :
> 
> " Be careful only to compare factors with the
>   same set of levels (in the same order).  In particular,
>   'as.numeric' applied to a factor is meaningless, and may happen by
>   implicit coercion.  To "revert" a factor 'f' to its original
>   numeric values, 'as.numeric(levels(f))[f]' is recommended and
>   slightly more efficient than 'as.numeric(as.character(f))'. "
> 
> 'as.numeric(levels(f))[f]'  worked well for me in the similar situation i.e.
> to get back numeric values from a factor type.
> But see also the I() "option" of the data.frame() function, which allows you
> not to obtain a factor (from a character vector only) if it is not what you
> want.
> 
> from ?data.frame :
> 
> "Objects passed to 'data.frame' should have the same number of
>      rows, but atomic vectors, factors and character vectors protected
>      by 'I' will be recycled a whole number of times if necessary."
> 
> 
> see this example:
> --------------------------------------------------
> > v1<-c(1,2,3)
> > v2<-c("a","b","c")
> > df.A<-data.frame(v1,v2)
> > str(df.A)
> `data.frame':   3 obs. of  2 variables:
>  $ v1: num  1 2 3
>  $ v2: Factor w/ 3 levels "a","b","c": 1 2 3
> > df.B<-data.frame(v1,I(v2))
> > str(df.B)
> `data.frame':   3 obs. of  2 variables:
>  $ v1: num  1 2 3
>  $ v2:Class 'AsIs'  chr [1:3] "a" "b" "c"
> -------------------------------------------------
> 
> hope this helps,
> 
> Florence.
> 
> 
> 
> 
> 
> On 11/25/05, John Logsdon <j.logsdon at quantex-research.com> wrote:
> >
> > I have an ordering and factor problem to which there must be a simple
> > solution!  The version is R 2.0.1  (2004-11-15) on A Linux platform.
> >
> > A data frame H is read in from a .csv file using read.csv with as.is=TRUE.
> >
> > Another data frame HN is constructed from data and I want to compare two
> > columns both named ss of the (sorted) data frames that are the same
> > length.
> >
> > The problem is that HN$ss is always treated as a factor whatever I do
> > while H$ss is treated as an integer, which is what I want.  Somewhere R is
> > making an implicit transformation but I can't see how to correct it.
> >
> > The data are all integers in the range 1:13 - in fact with no gaps.  If I
> > tabulate from H:
> >
> > > table(H$ss)
> >
> >    1    2    3    4    5    6    7    8    9   10   11   12   13
> > 176  176  176  176  176  176  341 8726 8784 8777 8773 8749 8747
> >
> > and for HN:
> >
> > > table(HN$ss)
> >
> >    1   10   11   12   13    2    3    4    5    6    7    8    9
> > 176 8777 8773 8749 8747  176  176  176  176  176  341 8726 8784
> >
> > At some time while constructing HN, I have to make it a character matrix -
> > otherwise gsub doesn't work when removing surplus blanks for example - but
> > I have turned it back into a data frame in the end.
> >
> > If I check the modes, both data frames are lists and both columns are
> > numeric - HN is not reported as a factor.  Yet it appears to be treated as
> > a factor, for example:
> >
> > > table(formatC(H$ss,dig=0,width=2,format="f",flag="0"))
> >
> >   01   02   03   04   05   06   07   08   09   10   11   12   13
> > 176  176  176  176  176  176  341 8726 8784 8777 8773 8749 8747
> > > table(formatC(HN$ss,dig=0,width=2,format="f",flag="0"))
> >
> > yet:
> >
> >    1   10   11   12   13    2    3    4    5    6    7    8    9
> > 176 8777 8773 8749 8747  176  176  176  176  176  341 8726 8784
> > Warning messages:
> > 1: "+" not meaningful for factors in: Ops.factor(x, ifelse(x == 0, 1, 0))
> > 2: "<" not meaningful for factors in: Ops.factor(x, 0)
> >
> > I have tried as.numeric but then I get the factor level rather than name
> > returned:
> >
> > > table(formatC(as.numeric(HN$ss),dig=0,width=2,format="f",flag="0"))
> >
> >   01   02   03   04   05   06   07   08   09   10   11   12   13
> > 176 8777 8773 8749 8747  176  176  176  176  176  341 8726 8784
> >
> > which obviously is a tabulation of the internal levels rather than the
> > data.
> >
> > TIA
> >
> > John
> >
> > John Logsdon                               "Try to make things as simple
> > Quantex Research Ltd, Manchester UK         as possible but not simpler"
> > j.logsdon at quantex-research.com              a.einstein at relativity.org
> > +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>



From jfox at mcmaster.ca  Fri Nov 25 13:43:08 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 25 Nov 2005 07:43:08 -0500
Subject: [R] Tiger Mac stalls running Rcmdr program
In-Reply-To: <BFACA522.BEF%glsmah001@mail.uct.ac.za>
Message-ID: <20051125124308.MVYN26550.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Maha,

I'm not a Mac user, so can't offer much help, since you appear to have
followed the Mac installation notes. (Rick: I'm copying this reply to you in
case you didn't see Maha's message on r-help.)

I'm writing primarily to make sure that you know that the Rcmdr doesn't
support either structural-equation modeling or meta-analysis.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Maha Golestaneh
> Sent: Friday, November 25, 2005 4:25 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Tiger Mac stalls running Rcmdr program
> 
> I am a Macintosh computer (MAC OS X Version 10.4.3) user.  I 
> would like to run R for structural equation modeling and 
> meta-analysis but am having difficulty using the Rcmdr interface.
> 
> According to the R commander installation notes for Tiger 
> Macs - I need to
> 1) Install X11.app from Apple Install disks - which I have done
> 2) Install R.app - which I have done
> 3) Install binary package rgl from CRAN - which I have done
> 4) Install binary Rcmdr from CRAN - which I have done
> 
> I then need to start R and X11 and type library (Rcmdr) in 
> the R console and return... 
> 
> My computer now is stalling (a colored CD spins) meaning 
> possibly it can9t run the Rcmdr (R commander) program.  
> Please help me install this successfully. Thank you.
> 
> Maha
> 
> Maha Golestaneh
> PhD Candidate, Graduate School of Business The Woolsack, 
> Pavilion 2, Room 2.20 University of Cape Town, Woolsack Drive 
> Rondebosch, Cape Town South Africa glsmah001 at mail.uct.ac.za 
> glsmah001 at gsb.uct.ac.za H 011-27-21-685-4050 x222 M 011-27-72-713-0649
> 
> 
> 	[[alternative HTML version deleted]]
> 
>



From dimitris.rizopoulos at med.kuleuven.be  Fri Nov 25 14:01:41 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 25 Nov 2005 14:01:41 +0100
Subject: [R] Ordering problem
References: <Pine.LNX.4.10.10511251135490.32264-100000@quantex-research.co.uk>
Message-ID: <007901c5f1c0$615c2210$0540210a@www.domain>

another posibility would be to use something like:

v1 <- c(1, 2, 3); v2 <- c("a", "b", "c"); v3 <- c("1", "2", "3")
dat <- data.frame(v1, v2, v3)
############3
dat <- lapply(dat, as.character)
dat <- as.data.frame(lapply(dat, type.convert))

dat
sapply(dat, data.class)



I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "John Logsdon" <j.logsdon at quantex-research.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, November 25, 2005 1:25 PM
Subject: Re: [R] Ordering problem


> Thanks to Florence but it needs a little modification.  However as I 
> have
> now discovered the str() command, things are looking up.:))
>
> I have a character matrix so I() just leaves it as characters 
> whereas I
> want the various columns to be integers or whatever they contain.
>
> To take Florence's example slightly extended:
>
>> v1<-c(1,2,3);v2<-c("a","b","c");v3<-c("1","2","3")
>
> Note that the third vector is a character with numerical contents.
>
>> data.frame(v1,v2,v3)
>  v1 v2 v3
> 1  1  a  1
> 2  2  b  2
> 3  3  c  3
>
> so it looks OK, but
>
>> str(data.frame(v1,v2,v3))
> `data.frame':   3 obs. of  3 variables:
> $ v1: num  1 2 3
> $ v2: Factor w/ 3 levels "a","b","c": 1 2 3
> $ v3: Factor w/ 3 levels "1","2","3": 1 2 3
>
> reveals the nasty truth!
>
> whereas
>
>> str(data.frame(v1,v2,I(v3)))
> `data.frame':   3 obs. of  3 variables:
> $ v1: num  1 2 3
> $ v2: Factor w/ 3 levels "a","b","c": 1 2 3
> $ v3:Class 'AsIs'  chr [1:3] "1" "2" "3"
>
> just keeps the character v3 as characters.  I want it to be 
> interpreted as
> numeric so:
>
>> str(data.frame(v1,v2,as.numeric(v3)))
> `data.frame':   3 obs. of  3 variables:
> $ v1            : num  1 2 3
> $ v2            : Factor w/ 3 levels "a","b","c": 1 2 3
> $ as.numeric.v3.: num  1 2 3
>
> actually gives me what I need.
>
> The only problem is that I have to do everything column by column 
> and
> there are 15 cols all.  So it makes particularly ugly coding to 
> reproduce
> an as.is read from a .csv file.
>
> The other solutions from Baz and Carlos would also work of course - 
> but
> they are still pretty horrible.  Perhaps another way to do this is 
> to
> write it out using cat then read it in again using as.is=TRUE!! ;)
>
> Thanks to one and all
>
> Best wishes
>
> John
>
> John Logsdon                               "Try to make things as 
> simple
> Quantex Research Ltd, Manchester UK         as possible but not 
> simpler"
> j.logsdon at quantex-research.com 
> a.einstein at relativity.org
> +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
>
>
> On Fri, 25 Nov 2005, Florence Combes wrote:
>
>> John,
>>
>> at ?factor, you can see :
>>
>> " Be careful only to compare factors with the
>>   same set of levels (in the same order).  In particular,
>>   'as.numeric' applied to a factor is meaningless, and may happen 
>> by
>>   implicit coercion.  To "revert" a factor 'f' to its original
>>   numeric values, 'as.numeric(levels(f))[f]' is recommended and
>>   slightly more efficient than 'as.numeric(as.character(f))'. "
>>
>> 'as.numeric(levels(f))[f]'  worked well for me in the similar 
>> situation i.e.
>> to get back numeric values from a factor type.
>> But see also the I() "option" of the data.frame() function, which 
>> allows you
>> not to obtain a factor (from a character vector only) if it is not 
>> what you
>> want.
>>
>> from ?data.frame :
>>
>> "Objects passed to 'data.frame' should have the same number of
>>      rows, but atomic vectors, factors and character vectors 
>> protected
>>      by 'I' will be recycled a whole number of times if necessary."
>>
>>
>> see this example:
>> --------------------------------------------------
>> > v1<-c(1,2,3)
>> > v2<-c("a","b","c")
>> > df.A<-data.frame(v1,v2)
>> > str(df.A)
>> `data.frame':   3 obs. of  2 variables:
>>  $ v1: num  1 2 3
>>  $ v2: Factor w/ 3 levels "a","b","c": 1 2 3
>> > df.B<-data.frame(v1,I(v2))
>> > str(df.B)
>> `data.frame':   3 obs. of  2 variables:
>>  $ v1: num  1 2 3
>>  $ v2:Class 'AsIs'  chr [1:3] "a" "b" "c"
>> -------------------------------------------------
>>
>> hope this helps,
>>
>> Florence.
>>
>>
>>
>>
>>
>> On 11/25/05, John Logsdon <j.logsdon at quantex-research.com> wrote:
>> >
>> > I have an ordering and factor problem to which there must be a 
>> > simple
>> > solution!  The version is R 2.0.1  (2004-11-15) on A Linux 
>> > platform.
>> >
>> > A data frame H is read in from a .csv file using read.csv with 
>> > as.is=TRUE.
>> >
>> > Another data frame HN is constructed from data and I want to 
>> > compare two
>> > columns both named ss of the (sorted) data frames that are the 
>> > same
>> > length.
>> >
>> > The problem is that HN$ss is always treated as a factor whatever 
>> > I do
>> > while H$ss is treated as an integer, which is what I want. 
>> > Somewhere R is
>> > making an implicit transformation but I can't see how to correct 
>> > it.
>> >
>> > The data are all integers in the range 1:13 - in fact with no 
>> > gaps.  If I
>> > tabulate from H:
>> >
>> > > table(H$ss)
>> >
>> >    1    2    3    4    5    6    7    8    9   10   11   12   13
>> > 176  176  176  176  176  176  341 8726 8784 8777 8773 8749 8747
>> >
>> > and for HN:
>> >
>> > > table(HN$ss)
>> >
>> >    1   10   11   12   13    2    3    4    5    6    7    8    9
>> > 176 8777 8773 8749 8747  176  176  176  176  176  341 8726 8784
>> >
>> > At some time while constructing HN, I have to make it a character 
>> > matrix -
>> > otherwise gsub doesn't work when removing surplus blanks for 
>> > example - but
>> > I have turned it back into a data frame in the end.
>> >
>> > If I check the modes, both data frames are lists and both columns 
>> > are
>> > numeric - HN is not reported as a factor.  Yet it appears to be 
>> > treated as
>> > a factor, for example:
>> >
>> > > table(formatC(H$ss,dig=0,width=2,format="f",flag="0"))
>> >
>> >   01   02   03   04   05   06   07   08   09   10   11   12   13
>> > 176  176  176  176  176  176  341 8726 8784 8777 8773 8749 8747
>> > > table(formatC(HN$ss,dig=0,width=2,format="f",flag="0"))
>> >
>> > yet:
>> >
>> >    1   10   11   12   13    2    3    4    5    6    7    8    9
>> > 176 8777 8773 8749 8747  176  176  176  176  176  341 8726 8784
>> > Warning messages:
>> > 1: "+" not meaningful for factors in: Ops.factor(x, ifelse(x == 
>> > 0, 1, 0))
>> > 2: "<" not meaningful for factors in: Ops.factor(x, 0)
>> >
>> > I have tried as.numeric but then I get the factor level rather 
>> > than name
>> > returned:
>> >
>> > > table(formatC(as.numeric(HN$ss),dig=0,width=2,format="f",flag="0"))
>> >
>> >   01   02   03   04   05   06   07   08   09   10   11   12   13
>> > 176 8777 8773 8749 8747  176  176  176  176  176  341 8726 8784
>> >
>> > which obviously is a tabulation of the internal levels rather 
>> > than the
>> > data.
>> >
>> > TIA
>> >
>> > John
>> >
>> > John Logsdon                               "Try to make things as 
>> > simple
>> > Quantex Research Ltd, Manchester UK         as possible but not 
>> > simpler"
>> > j.logsdon at quantex-research.com 
>> > a.einstein at relativity.org
>> > +44(0)161 445 4951/G:+44(0)7717758675 
>> > www.quantex-research.com
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> > http://www.R-project.org/posting-guide.html
>> >
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From gret at nsl.ethz.ch  Fri Nov 25 14:06:59 2005
From: gret at nsl.ethz.ch (Adrienne Gret-Regamey)
Date: Fri, 25 Nov 2005 14:06:59 +0100
Subject: [R] optim
Message-ID: <43870C73.6080500@nsl.ethz.ch>

Hello:

I am trying to use optim() to estimate the maximum likelihood of a 
function a*x^b = y.
Unfortunately, I always get the error, that there is no default value for b.

Could you give me an example, on how to correctly optimize this function 
with input data x<-c(1,3,11,14).

Thanks a lot,

Adrienne

-- 
***********************************************************************
Adrienne Gr??t-Regamey          			
LEP, Landscape and Environmental Planning,
HIL H 31.2, ETH H??nggerberg
CH - 8093 Z??rich

Phone: +41/1/633'30'02 or +41/31/971'53'74
Fax:   +41/1/633'10'84 	
http://lep.ethz.ch/



From ripley at stats.ox.ac.uk  Fri Nov 25 14:08:48 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Nov 2005 13:08:48 +0000 (GMT)
Subject: [R] Problems compiling R under AIX 4.3
In-Reply-To: <20051125105811.rhiamoxcay8sswo4@webmail.versatel.de>
References: <20051125105811.rhiamoxcay8sswo4@webmail.versatel.de>
Message-ID: <Pine.LNX.4.61.0511251302410.26499@gannet.stats>

On Fri, 25 Nov 2005 hacker-24 at versanet.de wrote:

> Hi,
>
> I have problems compiling R 2.2.0 under AIX 4.3 with GCC and xlf95 (FORTRAN)
> Compilers.

Did you check out the R-admin manual?  That suggests you need

SHLIB_LDFLAGS=-Wl,-G

which you do not seem to have.  Note also the report there that you cannot 
successfully build 2.2.0 under AIX 4.3, and therefore you are advised to 
try R-patched.

Probably the R-devel list would be more appropriate for this topic, and it 
is essential that you do tell us exactly what you did.

> here the error message I got:
>
> make[1]: Entering directory `/home/mau/sy/R-2.2.0/src/modules/X11'
> make[1]: `Makedeps' is up to date.
> make[1]: Leaving directory `/home/mau/sy/R-2.2.0/src/modules/X11'
> make[1]: Entering directory `/home/mau/sy/R-2.2.0/src/modules/X11'
> /usr/local/bin/gcc -Wl,-bM:SRE -Wl,-H512 -Wl,-T512 -Wl,-bnoentry -Wl,-bexpall
> -Wl,-bI:../../../etc/R.exp -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo
> rotated.lo rbitmap.lo  -lSM -lICE -lX11  -ljpeg -lpng -lz
> /usr/local/lib/gcc/powerpc-ibm-aix4.3.2.0/3.4.3/../../../../powerpc-ibm-aix4.3.2.0/bin/ld:
> -static and -shared may not be used together
> collect2: ld returned 1 exit status
> make[1]: *** [R_X11.so] Error 1
> make[1]: Leaving directory `/home/mau/sy/R-2.2.0/src/modules/X11'
> make: *** [R] Error 2
>
> I do not see where I use -dynamic and/or -static.
>
> If I use the IMB linker I got:
>
> with IBM linker (ld):
> $ ld -b32 -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo rotated.lo
> rbitmap.lo  -lSM -lICE -lX11  -ljpeg -lpng -lz
> ld: 0711-327 WARNING: Entry point not found: __start
> ld: 0711-244 ERROR: No csects or exported symbols have been saved.
>
> Any help are welcome - Is there anybody with the same configuration (AIX
> 4.3,gcc,xlf95) and has build a working R ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at pdf.com  Fri Nov 25 14:37:42 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 25 Nov 2005 07:37:42 -0600
Subject: [R] optim
In-Reply-To: <43870C73.6080500@nsl.ethz.ch>
References: <43870C73.6080500@nsl.ethz.ch>
Message-ID: <438713A6.7010005@pdf.com>



Adrienne Gret-Regamey wrote:
> Hello:
> 
> I am trying to use optim() to estimate the maximum likelihood of a 
> function a*x^b = y.
> Unfortunately, I always get the error, that there is no default value for b.
> 
> Could you give me an example, on how to correctly optimize this function 
> with input data x<-c(1,3,11,14).
> 
> Thanks a lot,
> 
> Adrienne
> 

Problems such as these will have more meaningful responses if you post 
an example (see the posting guide). What's being parameterized here? a 
or b or both? If just 'a', then fix 'b' at the desired value. If both, 
then give optim a starting value. And what is 'y'?

fn <- function(par, y, x) {
   a <- par[1]
   b <- par[2]
   sum((y - a * x^b)^2)
}

x <- c(1, 3, 11, 14)
y <- exp(rnorm(length(x)))
## y ~ a * x^b
## log(y) ~ log(a) + b * log(x)
v <- coef(lm(log(y) ~ log(x)))
optim(c(exp(v[1]), v[2]), fn, y = y, x = x)

Again, I may be way off. Please provide an example if this doesn't cover 
what you need. And define all the variables need to run your script.

--sundar



From arildhus at stud.ntnu.no  Fri Nov 25 15:11:29 2005
From: arildhus at stud.ntnu.no (Arild Husby)
Date: Fri, 25 Nov 2005 15:11:29 +0100
Subject: [R] Use of nesting in lmer- error in numerical expression
Message-ID: <001601c5f1ca$21794290$e757f181@STUDENTA046822>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051125/389314c4/attachment.pl

From ehlers at math.ucalgary.ca  Fri Nov 25 15:43:47 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 25 Nov 2005 07:43:47 -0700
Subject: [R] model selection with step function
In-Reply-To: <1132919258.4386f9daf301f@webmail2.bignet.com.br>
References: <1132868033.438631c1a2cc5@webmail2.bignet.com.br>	<4386602C.4050106@math.ucalgary.ca>
	<1132919258.4386f9daf301f@webmail2.bignet.com.br>
Message-ID: <43872323.9070301@math.ucalgary.ca>

Antonio,

Antonio Olinto wrote:
> Dear Ehlers, thanks for your message.
> 
> Following the example on stepAIC and Venebles & Ripley?s book, it seems that
> update rearranges the terms. I didn?t understand how to indicate the formula in
> the function.
> 
> I have the initial model U ~ var1+var2+var3+var4 (family Gaussian). I want first
> to select the terms, putting main effects first. If I just write step(model) it
> will take out no significant variables (lets say var2) and will give U ~
> var1+var3+var4. Supposing the var4 have the ?strongest? effect upon U, followed
> by var1 and var3, I would like to have the out put U ~ var4+var1+var2.
> 
> Is it possible to do so?

Isn't that what stepAIC gives (with trace=TRUE) for its last model?
It orders predictors in terms of their effect on the AIC.

But why are you using glm() with the Gaussian family? See the comment
in MASS 4, page 190.

> 
> Thanks again.
> 
> Antonio Olinto
> Biologist
> Sao Paulo Fisheries Institute
> 
> PS. Unfortunately I don?t have Regression Modeling Strategies around here

That's a shame. Here's a quote from Frank's book:

"Stepwise variable selection has been a very popular
technique for many years, but if this procedure had just
been proposed as a statistical method, it would most likely
be rejected because it violates every principle of
statistical estimation and hypothesis testing."

I would use the technique only in an exploratory setting, i.e. one that
might help me to refine further experimentation.

Peter

> 
> 
> Citando P Ehlers <ehlers at math.ucalgary.ca>:
> 
> 
>>Antonio Olinto wrote:
>>
>>
>>>Hello,
>>>
>>>I have a doubt in using the function step (step wise) to select glm
>>
>>models.
>>
>>>Usually I apply the gamma distribution to analyze fishery data. To select
>>
>>the
>>
>>>terms I use a routine where I first compare single term models to the null
>>
>>model
>>
>>>(eg. U~1 vs. U~depth; U~1 vs. U~latitude; etc. ? where U= abundance) and,
>>
>>by
>>
>>>means of the result given by a likelihook function applied for each
>>
>>comparison,
>>
>>>I select the ?strongest? effect, let?s say depth. Then I run a new step
>>>comparing the U~depth vs. U~depth+latitude; U~depth vs. U~depth+... etc.
>>
>>Making
>>
>>>this way I put the terms in ?magnitude? order.
>>>
>>>I tried to make a gaussian model using the step(glm.model) function to
>>
>>select
>>
>>>the terms but I saw that in the output table given by anova(glm.model) the
>>>selected terms kept the original order.
>>>
>>>Is it possible to have the terms in the model rearranged, as in my
>>
>>example?
>>
>>>Thanks for any help. I read Chambers and Hastie?s ?Statistical Models in
>>
>>S?,
>>
>>>Venables and Ripley ?Modern Applied Statistics? and, of course, R help but
>>
>>I
>>
>>>couldn?t get the trick.
>>>
>>
>>I don't know if this will get you there, but
>>
>>1. I would use stepAIC in package MASS;
>>2. set argument trace = TRUE;
>>3. think very hard about the interpretation of the model;
>>4. read also Frank Harrell's "Regression Modeling Strategies".
>>
>>Peter
>>
>>
>>
>>>Antonio
>>>
>>>--
>>>Biologist
>>>Sao Paulo Fisheries Institute
>>>
>>>
>>>
>>>-------------------------------------------------
>>>WebMail Bignet - O seu provedor do litoral
>>>www.bignet.com.br
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>>-- 
>>Peter Ehlers
>>Department of Mathematics and Statistics
>>University of Calgary, 2500 University Dr. NW       ph: 403-220-3936
>>Calgary, Alberta  T2N 1N4, CANADA                  fax: 403-282-5150
>>
>>
>>
> 
> 
> 
> 
> -------------------------------------------------
> WebMail Bignet - O seu provedor do litoral
> www.bignet.com.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ehlers at math.ucalgary.ca  Fri Nov 25 16:07:44 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 25 Nov 2005 08:07:44 -0700
Subject: [R] residuals in logistic regression model
In-Reply-To: <4ab0fb470511241832h1445626ak54f8c338714284fd@mail.gmail.com>
References: <4ab0fb470511241752j2da9e4d1oed3a1ae1f1baacf0@mail.gmail.com>	<20051125021108.UPWA28424.tomts25-srv.bellnexxia.net@JohnDesktop8300>
	<4ab0fb470511241832h1445626ak54f8c338714284fd@mail.gmail.com>
Message-ID: <438728C0.4020301@math.ucalgary.ca>

Urania,

I'm not very fond of "putting additive residuals on the righthand side".
This practice tends to obscure the fact that we're fitting a conditional
mean function:

  E(Y|x) = function(x; parameters)

We then need to assess the model fit and uncertainties of parameter
estimates. We may want to consider an additive error structure
as part of the assessment (e.g. LS regression). In linear regression
this is often included as an error term on the RHS of a model equation,
probably because it is efficient to do so.

For logistic regression, model fit may be assessed by the deviance,
which can be considered to be a sum of deviance residuals. But the
model also assumes a dispersion factor of 1.0. This assumption is
assessed (in R) with the Pearson residuals. Further, the fitting
method is iterative, so R gives us the "working" residuals of the
final fit.

Peter Ehlers

Urania Sun wrote:

>  Thanks a lot, Professor.
> 
> Now I know if I put some additive residuals in the right handside of the
> logistic regression equation, they are different with any glm returned
> residuals.
> 
> But is it ever ok or legal to put some additive residuals in the right-hand
> side of the logistic equation regardless whatever they are when I write the
> left-hand side as the log odds-ratio of proportion instead of probability?
> 
> I have a big confusion on this. Thanks a lot. Your book in the library is
> currently checked out by someone else. I may try to get one of my own.
> 
> 
> On 11/24/05, John Fox <jfox at mcmaster.ca> wrote:
> 
>>Dear Urania,
>>
>>
>>>-----Original Message-----
>>>From: Urania Sun [mailto: suncertain at gmail.com]
>>>Sent: Thursday, November 24, 2005 8:52 PM
>>>To: John Fox
>>>Cc: r-help at stat.math.ethz.ch
>>>Subject: Re: [R] residuals in logistic regression model
>>>
>>>Thanks, Professor.
>>>
>>>But is it ok to write residuals in the right hand side of the
>>>logistic regression formula? Some people said I cannot since
>>>the generalized linear model is to use a function to link the
>>>expectation to a linear model. So there should not be
>>>residuals in the right hand side.
>>>
>>>My question is that If residuals do exist (as in the glm
>>>model output), why not put them in the formula (for example,
>>>if I write the left-hand side as the estimated odds-ratio)?
>>>
>>
>>There are several kinds of residuals for generalized linear models, as I
>>mentioned (see ?residuals.glm). The residuals in the glm output are
>>deviance
>>residuals, which are the casewise (signed) components of the residual
>>deviance; differences between y and fitted-y are called response residuals
>>(and aren't generally as useful). The left-hand side of a logit model
>>transformed with the logit-link is the log-odds, not the odds or
>>odds-ratio.
>>The form of the model to which the response residuals applies has the
>>proportion, not the logit, on the left-hand side.
>>
>>These matters are discussed in the references given in ?residuals.glm, and
>>
>>in many other places, such as Sec. 6.6 of my R and S-PLUS Companion to
>>Applied Regression.
>>
>>
>>>Many thanks!
>>>
>>>Happy Thanksgiving!
>>
>>Unfortunately we celebrate Thanksgiving in Canada in October, probably
>>because the weather here in late November leaves little to be thankful
>>for.
>>
>>Regards,
>>John
>>
>>
>>>On 11/24/05, John Fox <jfox at mcmaster.ca> wrote:
>>>
>>>      Dear Urania,
>>>
>>>      The residuals method for glm objects can compute
>>>several kinds of residuals;
>>>      the default is deviance residuals. See ?residuals.glm
>>>for details and
>>>      references.
>>>
>>>      I hope this helps.
>>>      John
>>>
>>>      --------------------------------
>>>      John Fox
>>>      Department of Sociology
>>>      McMaster University
>>>      Hamilton, Ontario
>>>      Canada L8S 4M4
>>>      905-525-9140x23604
>>>      http://socserv.mcmaster.ca/jfox
>>>< http://socserv.mcmaster.ca/jfox>
>>>      --------------------------------
>>>
>>>      > -----Original Message-----
>>>      > From: r-help-bounces at stat.math.ethz.ch
>>>      > [mailto: r-help-bounces at stat.math.ethz.ch] On Behalf
>>>Of Urania Sun
>>>      > Sent: Thursday, November 24, 2005 1:36 PM
>>>      > To: r-help at stat.math.ethz.ch
>>>      > Subject: [R] residuals in logistic regression model
>>>      >
>>>      > In the logistic regression model, there is no residual
>>>      >
>>>      > log (pi/(1-pi)) = beta_0 + beta_1*X_1 + .....
>>>      >
>>>      > But glm model will return
>>>      >
>>>      > residuals
>>>      >
>>>      > What is that?
>>>      >
>>>      > How to understand this? Can we put some residual in the
>>>      > logistic regression model by replacing pi with pi'
>>>(the estimated pi)?
>>>      >
>>>      >  log (pi'/(1-pi')) = beta_0 + beta_1*X_1 + .....+ ei
>>>      >
>>>      > Thanks!
>>>      >
>>>      >       [[alternative HTML version deleted]]
>>>      >
>>>      > ______________________________________________
>>>      > R-help at stat.math.ethz.ch mailing list
>>>      > https://stat.ethz.ch/mailman/listinfo/r-help
>>>      > PLEASE do read the posting guide!
>>>      > http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>>><http://www.R-project.org/posting-guide.html
>>
>><http://www.r-project.org/posting-guide.html>>
>>
>>>
>>>
>>>
>>>
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From goedman at mac.com  Fri Nov 25 17:17:01 2005
From: goedman at mac.com (Rob J Goedman)
Date: Fri, 25 Nov 2005 08:17:01 -0800
Subject: [R] Tiger Mac stalls running Rcmdr program
In-Reply-To: <BFACA522.BEF%glsmah001@mail.uct.ac.za>
References: <BFACA522.BEF%glsmah001@mail.uct.ac.za>
Message-ID: <5116EB1E-6DBF-404C-8B48-5E3DFB7E5F22@mac.com>

Hi Maha,

Can you provide me with a bit more info? No need to copy the R Help  
Mailing List for now.

Can you let me know:

1) When you start X11, does the X11 window popup (which I usually  
close immediately)?
2) Can you load library(car) from the R console?
3) Does the Rcmdr window pops up?
4) Any further output on the R console?

Rob


On Nov 25, 2005, at 1:25 AM, Maha Golestaneh wrote:

> I am a Macintosh computer (MAC OS X Version 10.4.3) user.  I would  
> like to
> run R for structural equation modeling and meta-analysis but am having
> difficulty using the Rcmdr interface.
>
> According to the R commander installation notes for Tiger Macs ? I  
> need to
> 1) Install X11.app from Apple Install disks ? which I have done
> 2) Install R.app ? which I have done
> 3) Install binary package rgl from CRAN ? which I have done
> 4) Install binary Rcmdr from CRAN ? which I have done
>
> I then need to start R and X11 and type library (Rcmdr) in the R  
> console and
> return...
>
> My computer now is stalling (a colored CD spins) meaning possibly  
> it can?t
> run the Rcmdr (R commander) program.  Please help me install this
> successfully. Thank you.
>
> Maha
>
> Maha Golestaneh
> PhD Candidate, Graduate School of Business
> The Woolsack, Pavilion 2, Room 2.20
> University of Cape Town, Woolsack Drive
> Rondebosch, Cape Town South Africa
> glsmah001 at mail.uct.ac.za
> glsmah001 at gsb.uct.ac.za
> H 011-27-21-685-4050 x222
> M 011-27-72-713-0649
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From aleszib at gmail.com  Fri Nov 25 18:08:02 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Fri, 25 Nov 2005 18:08:02 +0100
Subject: [R] Generating all possible partitions
Message-ID: <08c001c5f1e2$d7eeedf0$0100a8c0@ALES>

I have posed this question earlier, however it has probably not been clear 
enough.



My problem is such. I would like to find all possible partitions of a set of 
n objects into k groups. The ordering of the groups does not matter, only 
which objects are together matters.



For example, there are two possible partitions of 3 objects into 2 groups:

1 1 2

1 2 2

By "the labels are not important" I meant that a partition 1 1 2 is 
identical to the partition 2 2 1.


Best regards,

Ales Ziberna



From otoomet at econ.dk  Fri Nov 25 18:40:14 2005
From: otoomet at econ.dk (Ott Toomet)
Date: Fri, 25 Nov 2005 18:40:14 +0100
Subject: [R] problems with dynamic objects on solaris
Message-ID: <E1EfhYU-0001CH-Lq@localhost.localdomain>

Dear R people,

I am working on a solaris 9 workstation with very restrictive access
policy.  It means I have still to use R 1.7.1 and gcc 2.95.

The problems is following: I have written a small function in c++ using
boost library and I want to dyn.load the resulting .so file into R.
Compilation works fine:

/akf/705769/zpu5769/proge/R$ R CMD SHLIB pcw.cc
g++ -I/opt2/R/lib/R/include  -I/usr/local/include   -fPIC  -g -O2 -c pcw.cc -o pcw.o
g++ -G -L/usr/local/lib -o pcw.so pcw.o   
/akf/705769/zpu5769/proge/R$ 

Note the resulting .so file is huge (it is only about 25kB on my
modern linux box):

/akf/705769/zpu5769/proge/R$ ll
total 6134
-rw-r-----+  1 zpu5769  705769      3151 nov 25 17:24 pcw.cc
-rw-r-----+  1 zpu5769  705769      3054 nov 18 15:12 pcw.cc~
-rw----rw-+  1 zpu5769  705769    810492 nov 25 18:00 pcw.o
-rwxrwx---+  1 zpu5769  705769   1637112 nov 25 18:00 pcw.so

However, I cannot load the object:

> dyn.load("/akf/705769/zpu5769/proge/R/pcw.so")
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
	unable to load shared library "/akf/705769/zpu5769/proge/R/pcw.so":
  ld.so.1: /opt2/R/lib/R/bin/R.bin: fatal: relocation error: file /akf/705769/zpu5769/proge/R/pcw.so: symbol Rf_isReal__FP7SEXPREC: referenced symbol not found

Any ideas?  It works on my linux box (R 2.2.0, gcc 3.3.5).  Several
functions I have written before, in C (and without boost), are
working.

Thanks in advance,
Ott



From ripley at stats.ox.ac.uk  Fri Nov 25 18:54:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Nov 2005 17:54:21 +0000 (GMT)
Subject: [R] problems with dynamic objects on solaris
In-Reply-To: <E1EfhYU-0001CH-Lq@localhost.localdomain>
References: <E1EfhYU-0001CH-Lq@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0511251751490.16528@gannet.stats>

On Fri, 25 Nov 2005, Ott Toomet wrote:

> Dear R people,
>
> I am working on a solaris 9 workstation with very restrictive access
> policy.  It means I have still to use R 1.7.1 and gcc 2.95.

Headers that long ago were not AFAIR written for C++, so ensure you used
extern "C" {} when including them.

> The problems is following: I have written a small function in c++ using
> boost library and I want to dyn.load the resulting .so file into R.
> Compilation works fine:
>
> /akf/705769/zpu5769/proge/R$ R CMD SHLIB pcw.cc
> g++ -I/opt2/R/lib/R/include  -I/usr/local/include   -fPIC  -g -O2 -c pcw.cc -o pcw.o
> g++ -G -L/usr/local/lib -o pcw.so pcw.o
> /akf/705769/zpu5769/proge/R$
>
> Note the resulting .so file is huge (it is only about 25kB on my
> modern linux box):

Statically linked libraries?

> /akf/705769/zpu5769/proge/R$ ll
> total 6134
> -rw-r-----+  1 zpu5769  705769      3151 nov 25 17:24 pcw.cc
> -rw-r-----+  1 zpu5769  705769      3054 nov 18 15:12 pcw.cc~
> -rw----rw-+  1 zpu5769  705769    810492 nov 25 18:00 pcw.o
> -rwxrwx---+  1 zpu5769  705769   1637112 nov 25 18:00 pcw.so
>
> However, I cannot load the object:
>
>> dyn.load("/akf/705769/zpu5769/proge/R/pcw.so")
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
> 	unable to load shared library "/akf/705769/zpu5769/proge/R/pcw.so":
>  ld.so.1: /opt2/R/lib/R/bin/R.bin: fatal: relocation error: file /akf/705769/zpu5769/proge/R/pcw.so: symbol Rf_isReal__FP7SEXPREC: referenced symbol not found

Looks like name mangling of R entry points.

> Any ideas?  It works on my linux box (R 2.2.0, gcc 3.3.5).  Several
> functions I have written before, in C (and without boost), are
> working.
>
> Thanks in advance,
> Ott
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From quantpm at yahoo.com  Fri Nov 25 18:17:34 2005
From: quantpm at yahoo.com (t c)
Date: Fri, 25 Nov 2005 09:17:34 -0800 (PST)
Subject: [R] vector memory exhausted (limit reached?)" error message while
	loading saved workspace
Message-ID: <20051125171735.48177.qmail@web35003.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051125/25be8170/attachment.pl

From ggrothendieck at gmail.com  Fri Nov 25 19:09:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 25 Nov 2005 13:09:30 -0500
Subject: [R] Generating all possible partitions
In-Reply-To: <08c001c5f1e2$d7eeedf0$0100a8c0@ALES>
References: <08c001c5f1e2$d7eeedf0$0100a8c0@ALES>
Message-ID: <971536df0511251009x5197c100s123c2b593e89ec3a@mail.gmail.com>

Probably not very fast but the number of partitions of a number,
also known as the Bell number, grows pretty dramatically so you
won't be able to use it for large numbers even with an efficient
implementation (though you could use it for larger numbers than
the solution here).  The main attribute of this approach is its
simplicity.   It generates the cartesian product
{ 0, 1, 2, ..., n } ^ n and then picks off the elements that are
non-increasing and sum to n.

n <- 3
g <- do.call("expand.grid", rep(list(0:n), n)) # cartesian product
f <- function(x) all(diff(x) <= 0) && sum(x) == length(x)
g[apply(g, 1, f), ]


On 11/25/05, Ales Ziberna <aleszib at gmail.com> wrote:
> I have posed this question earlier, however it has probably not been clear
> enough.
>
>
>
> My problem is such. I would like to find all possible partitions of a set of
> n objects into k groups. The ordering of the groups does not matter, only
> which objects are together matters.
>
>
>
> For example, there are two possible partitions of 3 objects into 2 groups:
>
> 1 1 2
>
> 1 2 2
>
> By "the labels are not important" I meant that a partition 1 1 2 is
> identical to the partition 2 2 1.
>
>
> Best regards,
>
> Ales Ziberna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From singyee.ling at googlemail.com  Fri Nov 25 19:25:48 2005
From: singyee.ling at googlemail.com (singyee ling)
Date: Fri, 25 Nov 2005 18:25:48 +0000
Subject: [R] rescale x-axis
Message-ID: <ca33a9890511251025m6c4d1f49u@mail.gmail.com>

Dear all,

I am trying to draw a survival curve with probability of surviving as
the y-axis and days (0- 500 days )as the x-axis. however, i do not
want the days to be equally spaced on the x-axis as i am more
interested in looking at the behaviour of the curve in the first 50
days. I am reluctant to use  xlim=c(0,1000) as i want to see the whole
picture. Hence, what I am interested in is a scale in which the days
are not equally spaced. By that , I mean the length of the interval
between the days get smaller and smaller, which gives greater emphasis
to the intial period. (i.e the length of the interval betwen 0-1 days
is longer then the interval between 1-2 days and so on) .Hope what i
say above make sense. any advise?

thanks!

sing yee



From maechler at stat.math.ethz.ch  Fri Nov 25 19:27:58 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 25 Nov 2005 19:27:58 +0100
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <17285.55052.354968.489470@stat.math.ethz.ch>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
	<20051123143323.GB6368@iwr.uni-heidelberg.de>
	<17284.38020.14279.158148@stat.math.ethz.ch>
	<971536df0511230929q74c48caax3892feec51fdcafe@mail.gmail.com>
	<971536df0511231351k52e1cd89o8c790b6ad6a1d1c5@mail.gmail.com>
	<17285.55052.354968.489470@stat.math.ethz.ch>
Message-ID: <17287.22446.567343.472410@stat.math.ethz.ch>

Let me try to summarize my view on this:

- I still it would make sense to have a *simple* peaks() function
  in R which provides the same (or more) functionality as the
  corresponding S-plus one.From
  For a proper data analysis situation, I think one would have to
  do something more sophisticated, based on a model (with a random
  component), such as nonparametric regression, time-series,....
  Hence peaks() should be kept as simple as reasonable.

- Of course I know that  which() or %in% can be used to deal
  with logicals containing NAs {As a matter of fact, I've had
  my fingers in both implementations for R!}.
  Still, the main use of logical vectors in S often is for
  situations where NAs only appear because of missing data:

  Indexing ([]), all(), any(), sum()  are all very nice and
  useful for logical vectors particularly when there are no NAs.

- I agree that a different more flexible function returning
  values from {-1,0,1} would be desirable, "for symmetry reasons".
  ===> added a peaksign() function

Here's code that implements the above {and other concerns
mentioned in this thread}, including some ``consistency
checking'' :

peaks <- function(series, span = 3, do.pad = TRUE) {
    if((span <- as.integer(span)) %% 2 != 1) stop("'span' must be odd")
    s1 <- 1:1 + (s <- span %/% 2)
    if(span == 1) return(rep.int(TRUE, length(series)))
    z <- embed(series, span)
    v <- apply(z[,s1] > z[, -s1, drop=FALSE], 1, all)
    if(do.pad) {
        pad <- rep.int(FALSE, s)
        c(pad, v, pad)
    } else v
}

peaksign <- function(series, span = 3, do.pad = TRUE)
{
    ## Purpose: return (-1 / 0 / 1) if series[i] is ( trough / "normal" / peak )
    ## ----------------------------------------------------------------------
    ## Author: Martin Maechler, Date: 25 Nov 2005

    if((span <- as.integer(span)) %% 2 != 1 || span == 1)
        stop("'span' must be odd and >= 3")
    s1 <- 1:1 + (s <- span %/% 2)
    z <- embed(series, span)
    d <- z[,s1] - z[, -s1, drop=FALSE]
    ans <- rep.int(0:0, nrow(d))
    ans[apply(d > 0, 1, all)] <- as.integer(1)
    ans[apply(d < 0, 1, all)] <- as.integer(-1)
    if(do.pad) {
        pad <- rep.int(0:0, s)
        c(pad, ans, pad)
    } else ans
}


check.pks <- function(y, span = 3)
    stopifnot(identical(peaks( y, span), peaksign(y, span) ==  1),
              identical(peaks(-y, span), peaksign(y, span) == -1))

for(y in list(1:10, rep(1,10), c(11,2,2,3,4,4,6,6,6))) {
    for(sp in c(3,5,7))
        check.pks(y, span = sp)
    stopifnot(peaksign(y) == 0)
}

y <- c(1,4,1,1,6,1,5,1,1) ; (ii <- which(peaks(y))); y[ii]
##- [1] 2 5 7
##- [1] 4 6 5
check.pks(y)

set.seed(7)
y <- rpois(100, lambda = 7)
check.pks(y)
py <- peaks(y)
plot(y, type="o", cex = 1/4, main = "y and peaks(y,3)")
points(seq(y)[py], y[py], col = 2, cex = 1.5)

p7 <- peaks(y,7)
points(seq(y)[p7], y[p7], col = 3, cex = 2)
mtext("peaks(y,7)", col=3)

set.seed(2)
x <- round(rnorm(500), 2)
y <- cumsum(x)
check.pks(y)

plot(y, type="o", cex = 1/4)
p15 <- peaks(y,15)
points(seq(y)[p15], y[p15], col = 3, cex = 2)
mtext("peaks(y,15)", col=3)



From vasu.akkineni at gmail.com  Fri Nov 25 19:31:40 2005
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Fri, 25 Nov 2005 13:31:40 -0500
Subject: [R] read.table without sep
Message-ID: <3b67376c0511251031j7a1c2515r47468b1ed7019e31@mail.gmail.com>

Hello all,

I have a data file table.txt  which i have attached. I am trying to pass the
columns as arguments to a function "totnorm" where i am displaying a total
normalization plot. The function is given below:

totnorm<-function(x,y){scale<-sum(x)/sum(y);xlab<-colnames(x);ylab<-colnames(y);x1<-x[[1]];y1<-scale*y[[1]];plot(x1,y1,xlab=xlab,ylab=ylab,col=6,
col.lab=4);}

i tried doing this:

data<-read.table("alldata.txt",header=TRUE,sep="\t")
a<-data[1]
b<-data[2]
totnorm(a,b)

The problem i am facing is- xlab and ylab contain the column names of
data[1] and data[2], but data[1][[1]] which is assigned to x1 has different
data which does not correspond to the colname(data[1]). Stating more
clearly, the colnames and the coldata don't match. I tried usind
read.tablewithout sep attribute, as given below:

data1<-read.table("alldata.txt",header=TRUE)

But this statement is not getting executed using Rserve when i make a
connection to R and try to execute it from a java servlet. I don't know why
it was doing so, so thought it would be better to fix this on R side, i.e,
try to use the "sep" attribue in read.table and still make the colnames and
coldata point to the same col#.

Please suggest a solution.
Thanks,
Vasu.
-------------- next part --------------
14A_U133A_StatPairs	14A_U133A_Detection	14B_U133A_Signal	88A_U133A_Signal	88B_U133A_Signal	183A_U133A_Signal	183B_U133A_Signal
AFFX-BioB-5_at	403.0	409.3	611.5	569.2	536.6	580.2	
AFFX-BioB-M_at	757.3	574.4	826.7	595.3	755.2	956.0	
AFFX-BioB-3_at	284.4	327.3	421.6	336.6	391.3	412.6	
AFFX-BioC-5_at	2314.2	1685.3	2264.7	2204.1	2233.1	2458.4	
AFFX-BioC-3_at	1574.5	1273.0	1484.6	1321.2	1474.7	1774.1	
AFFX-BioDn-5_at	2333.7	1796.8	2464.5	2372.5	2095.9	2735.7	
AFFX-BioDn-3_at	13673.9	11463.9	13624.7	14513.9	12934.1	16293.1	
AFFX-CreX-5_at	17778.8	15248.8	19977.2	19613.4	18609.1	18988.2	
AFFX-CreX-3_at	31056.6	24869.9	30773.4	32918.6	34412.1	33954.6	
AFFX-DapX-5_at	36.3	69.8	92.0	52.0	57.3	64.9	
AFFX-DapX-M_at	133.4	75.1	76.2	108.9	74.0	100.2	
AFFX-DapX-3_at	10.0	11.1	84.0	9.6	9.3	9.6	
AFFX-LysX-5_at	40.4	31.1	8.3	6.6	8.6	50.0	
AFFX-LysX-M_at	12.8	16.5	65.2	67.8	13.7	39.1	
AFFX-LysX-3_at	66.1	8.6	83.5	9.4	43.9	28.7	
AFFX-PheX-5_at	14.8	17.6	9.7	14.7	15.2	19.3	
AFFX-PheX-M_at	70.6	12.4	22.8	88.0	8.0	18.5	
AFFX-PheX-3_at	33.2	97.4	31.6	31.7	129.5	11.1	
AFFX-ThrX-5_at	26.4	31.3	14.5	23.4	28.1	24.2	
AFFX-ThrX-M_at	87.4	43.9	89.4	33.0	52.4	52.8	
AFFX-ThrX-3_at	19.9	18.9	13.9	26.1	24.0	17.0	
AFFX-TrpnX-5_at	32.6	13.5	26.5	11.4	60.3	18.4	
AFFX-TrpnX-M_at	14.9	7.5	12.1	10.1	11.3	12.8	
AFFX-TrpnX-3_at	17.3	4.3	7.0	26.0	2.3	8.6	










From ggrothendieck at gmail.com  Fri Nov 25 20:05:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 25 Nov 2005 14:05:35 -0500
Subject: [R] Generating all possible partitions
In-Reply-To: <971536df0511251009x5197c100s123c2b593e89ec3a@mail.gmail.com>
References: <08c001c5f1e2$d7eeedf0$0100a8c0@ALES>
	<971536df0511251009x5197c100s123c2b593e89ec3a@mail.gmail.com>
Message-ID: <971536df0511251105x27dc906ay3a8deac9fa6a065e@mail.gmail.com>

Just some clarification.  The answer I gave before is interpreted like this:
3 0 0 means the partition of 3 into 3+0+0, 2 1 0 means
the partition of 3 into 2+1+0 and 1 1 1 means the partition
of 3 into 1+1+1.

As pointed out to me privately you asked for only those partitions
that correspond to exactly k groups so it should be modified
like this:

n <- 3; k <- 2
g <- do.call("expand.grid", rep(list(1:n), k)) # cartesian product
f <- function(x) all(diff(x) <= 0) && sum(x) == n
g[apply(g, 1, f), ]

The output in the above case, i.e. n=3, k=2, has only one element which
is 2 1 which means the only partition of 3 into 2 groups is 2+1.

On 11/25/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Probably not very fast but the number of partitions of a number,
> also known as the Bell number, grows pretty dramatically so you
> won't be able to use it for large numbers even with an efficient
> implementation (though you could use it for larger numbers than
> the solution here).  The main attribute of this approach is its
> simplicity.   It generates the cartesian product
> { 0, 1, 2, ..., n } ^ n and then picks off the elements that are
> non-increasing and sum to n.
>
> n <- 3
> g <- do.call("expand.grid", rep(list(0:n), n)) # cartesian product
> f <- function(x) all(diff(x) <= 0) && sum(x) == length(x)
> g[apply(g, 1, f), ]
>
>
> On 11/25/05, Ales Ziberna <aleszib at gmail.com> wrote:
> > I have posed this question earlier, however it has probably not been clear
> > enough.
> >
> >
> >
> > My problem is such. I would like to find all possible partitions of a set of
> > n objects into k groups. The ordering of the groups does not matter, only
> > which objects are together matters.
> >
> >
> >
> > For example, there are two possible partitions of 3 objects into 2 groups:
> >
> > 1 1 2
> >
> > 1 2 2
> >
> > By "the labels are not important" I meant that a partition 1 1 2 is
> > identical to the partition 2 2 1.
> >
> >
> > Best regards,
> >
> > Ales Ziberna
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From rvaradha at jhsph.edu  Fri Nov 25 20:09:38 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 25 Nov 2005 14:09:38 -0500
Subject: [R] Generating all possible partitions
In-Reply-To: <971536df0511251009x5197c100s123c2b593e89ec3a@mail.gmail.com>
Message-ID: <000001c5f1f3$c858d8e0$5994100a@win.ad.jhu.edu>

Isn't Bell number different from the number of partitions, P_n, of a number,
n?

Bell number, B_n, is the number of subsets into which a set with "n"
elements can be divided.  So, B_3 = 5, and B_4 = 15, whereas P_3 = 3, and
P_4 = 5.  Bell numbers grow much more rapidly than the number of partitions.

Ravi.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: Friday, November 25, 2005 1:10 PM
> To: Ales Ziberna
> Cc: R-help
> Subject: Re: [R] Generating all possible partitions
> 
> Probably not very fast but the number of partitions of a number,
> also known as the Bell number, grows pretty dramatically so you
> won't be able to use it for large numbers even with an efficient
> implementation (though you could use it for larger numbers than
> the solution here).  The main attribute of this approach is its
> simplicity.   It generates the cartesian product
> { 0, 1, 2, ..., n } ^ n and then picks off the elements that are
> non-increasing and sum to n.
> 
> n <- 3
> g <- do.call("expand.grid", rep(list(0:n), n)) # cartesian product
> f <- function(x) all(diff(x) <= 0) && sum(x) == length(x)
> g[apply(g, 1, f), ]
> 
> 
> On 11/25/05, Ales Ziberna <aleszib at gmail.com> wrote:
> > I have posed this question earlier, however it has probably not been
> clear
> > enough.
> >
> >
> >
> > My problem is such. I would like to find all possible partitions of a
> set of
> > n objects into k groups. The ordering of the groups does not matter,
> only
> > which objects are together matters.
> >
> >
> >
> > For example, there are two possible partitions of 3 objects into 2
> groups:
> >
> > 1 1 2
> >
> > 1 2 2
> >
> > By "the labels are not important" I meant that a partition 1 1 2 is
> > identical to the partition 2 2 1.
> >
> >
> > Best regards,
> >
> > Ales Ziberna
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From ggrothendieck at gmail.com  Fri Nov 25 20:16:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 25 Nov 2005 14:16:07 -0500
Subject: [R] Generating all possible partitions
In-Reply-To: <000001c5f1f3$c858d8e0$5994100a@win.ad.jhu.edu>
References: <971536df0511251009x5197c100s123c2b593e89ec3a@mail.gmail.com>
	<000001c5f1f3$c858d8e0$5994100a@win.ad.jhu.edu>
Message-ID: <971536df0511251116p1b11e8c9n5101ff2b736e9c48@mail.gmail.com>

Yes, I just checked on Wikipedia and its as you say.

On 11/25/05, Ravi Varadhan <rvaradha at jhsph.edu> wrote:
> Isn't Bell number different from the number of partitions, P_n, of a number,
> n?
>
> Bell number, B_n, is the number of subsets into which a set with "n"
> elements can be divided.  So, B_3 = 5, and B_4 = 15, whereas P_3 = 3, and
> P_4 = 5.  Bell numbers grow much more rapidly than the number of partitions.
>
> Ravi.
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> > Sent: Friday, November 25, 2005 1:10 PM
> > To: Ales Ziberna
> > Cc: R-help
> > Subject: Re: [R] Generating all possible partitions
> >
> > Probably not very fast but the number of partitions of a number,
> > also known as the Bell number, grows pretty dramatically so you
> > won't be able to use it for large numbers even with an efficient
> > implementation (though you could use it for larger numbers than
> > the solution here).  The main attribute of this approach is its
> > simplicity.   It generates the cartesian product
> > { 0, 1, 2, ..., n } ^ n and then picks off the elements that are
> > non-increasing and sum to n.
> >
> > n <- 3
> > g <- do.call("expand.grid", rep(list(0:n), n)) # cartesian product
> > f <- function(x) all(diff(x) <= 0) && sum(x) == length(x)
> > g[apply(g, 1, f), ]
> >
> >
> > On 11/25/05, Ales Ziberna <aleszib at gmail.com> wrote:
> > > I have posed this question earlier, however it has probably not been
> > clear
> > > enough.
> > >
> > >
> > >
> > > My problem is such. I would like to find all possible partitions of a
> > set of
> > > n objects into k groups. The ordering of the groups does not matter,
> > only
> > > which objects are together matters.
> > >
> > >
> > >
> > > For example, there are two possible partitions of 3 objects into 2
> > groups:
> > >
> > > 1 1 2
> > >
> > > 1 2 2
> > >
> > > By "the labels are not important" I meant that a partition 1 1 2 is
> > > identical to the partition 2 2 1.
> > >
> > >
> > > Best regards,
> > >
> > > Ales Ziberna
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
>



From ehlers at math.ucalgary.ca  Fri Nov 25 20:29:29 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 25 Nov 2005 12:29:29 -0700
Subject: [R] read.table without sep
In-Reply-To: <3b67376c0511251031j7a1c2515r47468b1ed7019e31@mail.gmail.com>
References: <3b67376c0511251031j7a1c2515r47468b1ed7019e31@mail.gmail.com>
Message-ID: <43876619.2010806@math.ucalgary.ca>

Vasu,

You have a lot of problems here.

1. How was your file generated? Excel? You have trailing tabs on
all but row 1 which is why your read.table call with sep="\t"
gives you columns that don't seem to agree with what you expect.
See the argument row.names in ?read.table.

2. It's never a good idea to use colnames that begin with a digit.

3. read.table creates a _data frame_ for which data[i] would be
a data frame; your function does not accept data frames as input.
You probably want data[,i].

4. One-line 158-character function definitions with no spaces
are not exactly easy to read.

5. It's not clear whether you want the first column to be rownames
or data. R can handle both, but you need to tell it what to do.

6. ***** Best not to send such questions to R-devel. They have
nothing to do with the development of R.

Peter

Vasundhara Akkineni wrote:

> Hello all,
> 
> I have a data file table.txt  which i have attached. I am trying to pass the
> columns as arguments to a function "totnorm" where i am displaying a total
> normalization plot. The function is given below:
> 
> totnorm<-function(x,y){scale<-sum(x)/sum(y);xlab<-colnames(x);ylab<-colnames(y);x1<-x[[1]];y1<-scale*y[[1]];plot(x1,y1,xlab=xlab,ylab=ylab,col=6,
> col.lab=4);}
> 
> i tried doing this:
> 
> data<-read.table("alldata.txt",header=TRUE,sep="\t")
> a<-data[1]
> b<-data[2]
> totnorm(a,b)

> 
> The problem i am facing is- xlab and ylab contain the column names of
> data[1] and data[2], but data[1][[1]] which is assigned to x1 has different
> data which does not correspond to the colname(data[1]). Stating more
> clearly, the colnames and the coldata don't match. I tried usind
> read.tablewithout sep attribute, as given below:
> 
> data1<-read.table("alldata.txt",header=TRUE)
> 
> But this statement is not getting executed using Rserve when i make a
> connection to R and try to execute it from a java servlet. I don't know why
> it was doing so, so thought it would be better to fix this on R side, i.e,
> try to use the "sep" attribue in read.table and still make the colnames and
> coldata point to the same col#.
> 
> Please suggest a solution.
> Thanks,
> Vasu.
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Peter Ehlers
Department of Mathematics and Statistics
University of Calgary, 2500 University Dr. NW       ph: 403-220-3936
Calgary, Alberta  T2N 1N4, CANADA                  fax: 403-282-5150



From Mike.Prager at noaa.gov  Fri Nov 25 21:41:24 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Fri, 25 Nov 2005 15:41:24 -0500
Subject: [R] read.list()
In-Reply-To: <200511241802.jAOI2NxW015040@hypatia.math.ethz.ch>
References: <200511241802.jAOI2NxW015040@hypatia.math.ethz.ch>
Message-ID: <438776F4.8050907@noaa.gov>

Your question is not quite clear. Do you mean write and read an R list
object to a file?

If so, one method is provided by

dget() and dput()



on 11/24/2005 12:16 PM Leaf Sun said the following:

>
> I need to write and read a list in R. I did r.site.search, found there
> is a package "rmutil" doing this, unfortunately it is not on the list
> of package. In another words, I can't install it from any CRAN mirror.
>
-- 

Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina 28516
Opinions expressed are personal, not official. No
government endorsement of any product is made or implied.



From jmgreyes at ugr.es  Fri Nov 25 21:54:16 2005
From: jmgreyes at ugr.es (jmgreyes@ugr.es)
Date: Fri, 25 Nov 2005 21:54:16 +0100 (MET)
Subject: [R] glmmPQL
Message-ID: <4019.83.52.58.131.1132952056.squirrel@goliat1.ugr.es>

Hi,

My name is Jos?? Mar??a G??mez, and I am pretty new in R. Thus, I apologize
deeply if my questions are extremmely na??ve.I have checked several
available books and URL's, without finding any answer.

I'm trying to fit Generalized Linear Mixed Models via PQL. Below I provide
the structure of my data set. Year and Plot are random variables. Fate is
the binomial dependent. I have severe problems after calling glmmPQL:

1) I would like to nest Plot within Year, but I don't know how to argument
it.

2) I'm not sure the analysis I fit (see below) is actually considering
Year and Plot as random.

3) No way to obtain an analysis of deviance for the GLMM fit.

4) Year appears without degrees of freedom.

As you see, too many problems. Thank you very much for any help.
JM
_____________________________________________

> summary (Ifate)
 Year    Plot        Fate         Hab           Peso             Dist
 A:664   A:359   cache :  91   Oak  :621   Min.   :0.1903   Min.   :1.362
 B:574   B:427   comida:1147   Open : 94   1st Qu.:0.5515   1st Qu.:1.847
         C:135                 Pine :159   Median :0.6670   Median :2.137
         D: 97                 Rock : 23   Mean   :0.6674   Mean   :2.200
         E:220                 Shrub:341   3rd Qu.:0.7597   3rd Qu.:2.476
                                           Max.   :1.1386   Max.   :3.872
> library (MASS)
> library (nlme)
> f1<-glmmPQL(Fate~Year+Plot+Hab+Peso+Dist, random=~1|Year/Plot,
family=binomial, data=Ifate)

> anova.glm(f1)
Error in eval(expr, envir, enclos) : Object "Fate" not found

> anova(f1)
            numDF denDF   F-value p-value
(Intercept)     1  1223 124.10634  <.0001
Year            1     0   0.00271     NaN
Plot            4     3   0.96230  0.5343
Hab             4  1223   6.03162  0.0001
Peso            1  1223   4.34225  0.0374
Dist            1  1223   5.97248  0.0147
Warning messages:
1: NaNs produced in: pf(q, df1, df2, lower.tail, log.p)
2: NAs introduced by coercion



From prasannaprakash at gmail.com  Sat Nov 26 00:34:59 2005
From: prasannaprakash at gmail.com (Prasanna)
Date: Sat, 26 Nov 2005 00:34:59 +0100
Subject: [R] Plotting the diff. between two curves
Message-ID: <fc5b8ae70511251534u743fbb64yaa28229ce638ce30@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051126/88824d91/attachment.pl

From ggrothendieck at gmail.com  Sat Nov 26 01:04:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 25 Nov 2005 19:04:26 -0500
Subject: [R] Plotting the diff. between two curves
In-Reply-To: <fc5b8ae70511251534u743fbb64yaa28229ce638ce30@mail.gmail.com>
References: <fc5b8ae70511251534u743fbb64yaa28229ce638ce30@mail.gmail.com>
Message-ID: <971536df0511251604h76289859n69f892e282eff9a7@mail.gmail.com>

Using the zoo package you can merge the curves together
and use na.approx to fill in the blanks.  Assuming that
the first column in each instance is the x variable:

library(zoo)
Az <- zoo(A[,2], A[,1])
Bz <- zoo(B[,2], B[,1])
Cz <- na.approx(merge(Az, Bz))
plot(Cz[,1] - Cz[,2])


On 11/25/05, Prasanna <prasannaprakash at gmail.com> wrote:
> Dear Rs
>
> I have two vectors A and B
> where
>
> A is
>
>
>            V1        d
> 1  0.000100000 1.123278
> 2  0.002186431 1.120448
> 3  0.004351214 1.106661
> 4  0.006515998 1.107713
> 5  0.008680781 1.107667
> 6  0.013010348 1.106353
> 7  0.019504698 1.104077
> 8  0.034658181 1.103202
> 9  0.051976447 1.103200
> 10 0.073624280 1.094825
> 11 0.093085682 1.085123
> 12 0.095250465 1.087325
> 13 0.132051782 1.086158
> 14 0.168853098 1.084814
> 15 0.233774949 1.077453
> 16 0.296553665 1.076972
> 17 0.298718448 1.077258
> 18 0.354981166 1.073860
> 19 0.383123349 1.068163
> 20 0.398276832 1.068830
> 21 0.415595099 1.068267
> 22 0.426419015 1.066104
> 23 0.478373815 1.064580
> 24 0.534636533 1.062743
> 25 0.694808850 1.060799
> 26 0.816015067 1.037726
> 27 0.818179850 1.038340
> 28 0.820344634 1.027501
> 29 0.826838983 1.022188
> 30 0.829003767 1.017346
> 31 0.839827683 1.016244
> 32 0.841992467 1.007883
> 33 0.846322033 1.007745
> 34 0.852816383 1.006360
> 35 1.000000000 1.000000
>
> and  B is
>
>           V1        d
> 1 0.000100000 1.123278
> 2 0.002186431 1.112518
> 3 0.004351214 1.104332
> 4 0.006515998 1.091071
> 5 0.008680781 1.003591
> 6 0.010845564 0.948444
> 7 1.000000000 0.948444
>
>
> If I plot A and B, I will get two curves. However, is there any method to
> find the signed difference between these two curves so that I will have only
> one curve which shows the difference between these two curves?
>
> Thanks
> Prasanna
>
>
>
>
>
> --
> Prasanna BALAPRAKASH
> IRIDIA, Universit?? Libre de Bruxelles
> 50, Av. F. Roosevelt, CP 194/6
> 1050  Brussels
> Belgium.
> http://iridia.ulb.ac.be/~prasanna
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From prasannaprakash at gmail.com  Sat Nov 26 01:24:28 2005
From: prasannaprakash at gmail.com (Prasanna)
Date: Sat, 26 Nov 2005 01:24:28 +0100
Subject: [R] Plotting the diff. between two curves
In-Reply-To: <971536df0511251604h76289859n69f892e282eff9a7@mail.gmail.com>
References: <fc5b8ae70511251534u743fbb64yaa28229ce638ce30@mail.gmail.com>
	<971536df0511251604h76289859n69f892e282eff9a7@mail.gmail.com>
Message-ID: <fc5b8ae70511251624ybac1f80n74265a0f572086f6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051126/bb6d6af5/attachment.pl

From ggrothendieck at gmail.com  Sat Nov 26 01:32:53 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 25 Nov 2005 19:32:53 -0500
Subject: [R] Plotting the diff. between two curves
In-Reply-To: <fc5b8ae70511251624ybac1f80n74265a0f572086f6@mail.gmail.com>
References: <fc5b8ae70511251534u743fbb64yaa28229ce638ce30@mail.gmail.com>
	<971536df0511251604h76289859n69f892e282eff9a7@mail.gmail.com>
	<fc5b8ae70511251624ybac1f80n74265a0f572086f6@mail.gmail.com>
Message-ID: <971536df0511251632lcaa2ba6u4f15113476339470@mail.gmail.com>

In that case you will need to modify the last line to be:

plot(time(Cz), coredata(Cz[,1] - Cz[,2]), log = "x", type = "l")

On 11/25/05, Prasanna <prasannaprakash at gmail.com> wrote:
> Thanks for your replies, Jim and Gabor:
>
> Jim:
>
> The x-axis is going to be in log scale. Basically I am comparing two
> algorithms. One is very quick and another one is slow. X axis denotes time
> and y axis is a normalized solution quality.
> I hope it is clear now if not then let me know
>
> Gabor:
>
> I am looking into this Zoo package.
>
> Thanks for your replies
>
>
>
>
>
> On 11/26/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> > Using the zoo package you can merge the curves together
> > and use na.approx to fill in the blanks.  Assuming that
> > the first column in each instance is the x variable:
> >
> > library(zoo)
> > Az <- zoo(A[,2], A[,1])
> > Bz <- zoo(B[,2], B[,1])
> > Cz <- na.approx(merge(Az, Bz))
> > plot(Cz[,1] - Cz[,2])
> >
> >
> > On 11/25/05, Prasanna <prasannaprakash at gmail.com> wrote:
> > > Dear Rs
> > >
> > > I have two vectors A and B
> > > where
> > >
> > > A is
> > >
> > >
> > >            V1        d
> > > 1  0.000100000 1.123278
> > > 2  0.002186431 1.120448
> > > 3  0.004351214 1.106661
> > > 4  0.006515998 1.107713
> > > 5  0.008680781 1.107667
> > > 6  0.013010348 1.106353
> > > 7  0.019504698 1.104077
> > > 8  0.034658181 1.103202
> > > 9  0.051976447 1.103200
> > > 10 0.073624280 1.094825
> > > 11 0.093085682 1.085123
> > > 12 0.095250465 1.087325
> > > 13 0.132051782 1.086158
> > > 14 0.168853098 1.084814
> > > 15 0.233774949 1.077453
> > > 16 0.296553665 1.076972
> > > 17 0.298718448 1.077258
> > > 18 0.354981166 1.073860
> > > 19 0.383123349 1.068163
> > > 20 0.398276832 1.068830
> > > 21 0.415595099 1.068267
> > > 22 0.426419015 1.066104
> > > 23 0.478373815 1.064580
> > > 24 0.534636533 1.062743
> > > 25 0.694808850 1.060799
> > > 26 0.816015067 1.037726
> > > 27 0.818179850 1.038340
> > > 28 0.820344634 1.027501
> > > 29 0.826838983 1.022188
> > > 30 0.829003767 1.017346
> > > 31 0.839827683 1.016244
> > > 32 0.841992467 1.007883
> > > 33 0.846322033 1.007745
> > > 34 0.852816383 1.006360
> > > 35 1.000000000 1.000000
> > >
> > > and  B is
> > >
> > >           V1        d
> > > 1 0.000100000 1.123278
> > > 2 0.002186431 1.112518
> > > 3 0.004351214 1.104332
> > > 4 0.006515998 1.091071
> > > 5 0.008680781 1.003591
> > > 6 0.010845564 0.948444
> > > 7 1.000000000 0.948444
> > >
> > >
> > > If I plot A and B, I will get two curves. However, is there any method
> > to
> > > find the signed difference between these two curves so that I will have
> > only
> > > one curve which shows the difference between these two curves?
> > >
> > > Thanks
> > > Prasanna
> > >
> > >
> > >
> > >
> > >
> > > --
> > > Prasanna BALAPRAKASH
> > > IRIDIA, Universit?? Libre de Bruxelles
> > > 50, Av. F. Roosevelt, CP 194/6
> > > 1050  Brussels
> > > Belgium.
> > > http://iridia.ulb.ac.be/~prasanna
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
>
>
>
> --
> Prasanna BALAPRAKASH
> IRIDIA, Universit?? Libre de Bruxelles
> 50, Av. F. Roosevelt, CP 194/6
> 1050  Brussels
> Belgium.
> http://iridia.ulb.ac.be/~prasanna
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From prasannaprakash at gmail.com  Sat Nov 26 02:03:58 2005
From: prasannaprakash at gmail.com (Prasanna)
Date: Sat, 26 Nov 2005 02:03:58 +0100
Subject: [R] Plotting the diff. between two curves
In-Reply-To: <971536df0511251632lcaa2ba6u4f15113476339470@mail.gmail.com>
References: <fc5b8ae70511251534u743fbb64yaa28229ce638ce30@mail.gmail.com>
	<971536df0511251604h76289859n69f892e282eff9a7@mail.gmail.com>
	<fc5b8ae70511251624ybac1f80n74265a0f572086f6@mail.gmail.com>
	<971536df0511251632lcaa2ba6u4f15113476339470@mail.gmail.com>
Message-ID: <fc5b8ae70511251703ha6ad28dm38bace4972f349c5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051126/540403c2/attachment.pl

From guangxing at ict.ac.cn  Sat Nov 26 05:08:32 2005
From: guangxing at ict.ac.cn (=?gb2312?B?uePQxw==?=)
Date: Sat, 26 Nov 2005 12:08:32 +0800
Subject: [R] How can I get the difference seq directly?
Message-ID: <200511260406.jAQ46Mb8003992@hypatia.math.ethz.ch>

Hi, R-help,
let x<-rnorm(1000),
now I want to get a sequence y,which is satisfied with as follow:
 y[1]<-x[2]-x[1]
 y[2]<-x[3]-x[2]
 ....
 y[999]<-x[1000]-x[999]

Is there a function in R could achieve this requirement directly?

Thank you in advance!



	

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ÖÂ
Àñ£¡
 				

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¹ãÐÇ
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-26



From sundar.dorai-raj at pdf.com  Sat Nov 26 05:19:32 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 25 Nov 2005 22:19:32 -0600
Subject: [R] How can I get the difference seq directly?
In-Reply-To: <200511260406.jAQ46Mb8003992@hypatia.math.ethz.ch>
References: <200511260406.jAQ46Mb8003992@hypatia.math.ethz.ch>
Message-ID: <4387E254.5060104@pdf.com>

?diff

y <- diff(x)

HTH,

--sundar

å¹¿æ˜Ÿ wrote:
> Hi, R-help,
> let x<-rnorm(1000),
> now I want to get a sequence y,which is satisfied with as follow:
>  y[1]<-x[2]-x[1]
>  y[2]<-x[3]-x[2]
>  ....
>  y[999]<-x[1000]-x[999]
> 
> Is there a function in R could achieve this requirement directly?
> 
> Thank you in advance!
> 
> 
> 
> 	
> 
> ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€è‡´
> ç¤¼ï¼
>  				
> 
> ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€å¹¿æ˜Ÿ
> ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€guangxing at ict.ac.cn
> ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€2005-11-26
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From guangxing at ict.ac.cn  Sat Nov 26 05:28:14 2005
From: guangxing at ict.ac.cn (=?gb2312?B?uePQxw==?=)
Date: Sat, 26 Nov 2005 12:28:14 +0800
Subject: [R] How can I get the difference seq directly?
Message-ID: <200511260426.jAQ4Q2A6009449@hypatia.math.ethz.ch>

Thank you very much.

I have tried as follow:
y<-x[2:length[x]]-x[1:(length[x]-1)]

apparently, it is not better than the "diff".

Thank you again.
======= 2005-11-26 12:19:32 ÄúÔÚÀ´ÐÅÖÐÐ´µÀ£º=======

>?diff
>
>y <- diff(x)
>
>HTH,
>
>--sundar
>
>¹ãÐÇ wrote:
>> Hi, R-help,
>> let x<-rnorm(1000),
>> now I want to get a sequence y,which is satisfied with as follow:
>>  y[1]<-x[2]-x[1]
>>  y[2]<-x[3]-x[2]
>>  ....
>>  y[999]<-x[1000]-x[999]
>> 
>> Is there a function in R could achieve this requirement directly?
>> 
>> Thank you in advance!
>> 
>> 
>> 
>> 	
>> 
>> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ÖÂ
>> Àñ£¡
>>  				
>> 
>> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¹ãÐÇ
>> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
>> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-26
>> 
>> 
>> 
>> ------------------------------------------------------------------------
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

= = = = = = = = = = = = = = = = = = = =
			

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ÖÂ
Àñ£¡
 
				 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¹ãÐÇ
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-26



From deleeuw at cuddyvalley.org  Sat Nov 26 05:31:05 2005
From: deleeuw at cuddyvalley.org (Jan de Leeuw)
Date: Fri, 25 Nov 2005 20:31:05 -0800
Subject: [R] correspondence analysis and canonical correspondence analysis
	in R
Message-ID: <FB51C729-3DEA-467D-BF0D-928D231D3927@cuddyvalley.org>


http://www.cuddyvalley.org/psychoR/code

now contains version 1.4 of ca.R

There are already many versions of CA and CCA in various R packages,
but this one has some unique features, so maybe it is useful.

The function ca() can do simple CA, but it also
allows for linear restrictions on the row and column scores (they
can be restricted to be in the span of a number of covariates). Thus
the program can also do CCA (and more, because column scores can
be restricted as well).

I adapted the canonical partition of chi-square to restricted
CA, and generalized the notion of Benzecri distances
that are approximated from below. Row-wise and column-wise
partitionings of the inertias are also printed.

The CA program can make row plots, column plots, joint plots,
regression plots, transformation plots, and Benzecri plots.

It allows for four different scalings: x in the centroid of y,
y in the centroid of x, Goodman scaling, and Benzecri scaling.

This still needs some work to make sure everything makes sense
for linearly restricted scores -- and I need to add some plots
specific to CCA.

Updates will be posted to members of

http://www.stat.ucla.edu/mailman/listinfo/albertgifi

The directory

http://www.cuddyvalley.org/psychoR/

also has updated code for logistic unfolding, for
distance association models, for multidimensional
scaling using the smacof algorithm and for multidimensional
scaling using alscal.

==========================================================
Jan de Leeuw, 11667 Steinhoff Rd, Frazier Park, CA 93225, 661-245-1725
.mac: jdeleeuw ++++++  aim: deleeuwjan ++++++ skype: j_deleeuw
homepages: http://www.cuddyvalley.org and http://gifi.stat.ucla.edu
==========================================================
The Good -- and this is surely true --
  is just the Bad that we don't do !          (Wilhelm Busch)



From Charles.Annis at StatisticalEngineering.com  Sat Nov 26 06:59:39 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sat, 26 Nov 2005 00:59:39 -0500
Subject: [R] covariance analysis by using R
In-Reply-To: <OF430988D7.47F14CBC-ONC12570C4.0034D4EF-C12570C4.0039E635@fr.fournierpharma.com>
Message-ID: <200511260559.jAQ5xcHW031073@hypatia.math.ethz.ch>


An informal assessment may be useful: PLOT THE DATA.

x	y	experiment
0.1	0.5	A
0.2	0.6	A
0.3	0.6	A
0.4	0.7	A
0.5	0.9	A
1	3	B
2	4	B
3	6.5	B
4	7.5	B
5	11	B
10	18	C
20	35	C
30	75	C
40	90	C
50	98	C

Save the data as a csv file and read it into an R session:

data.df <- read.csv(file.choose())
data.df

Plot it:

plot(data.df$x, data.df$y)

Not surprisingly, the Cartesian axes obscure the behavior of the Experiment
A so we try logs:

plot(data.df$x, data.df$y, log="xy")

This plot suggests that while Experiments B and C might have a similar
relationship between x and y, Experiment A differs.


Since I know nothing of the physical meaning of these observations I am
unqualified to comment further.

Best wishes.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
a.menicacci at fr.fournierpharma.com
Sent: Friday, November 25, 2005 5:32 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] covariance analysis by using R





Hi,

Is anyone has solved MR Xin Meng problem (see below) ?

We have the same analysis configuration : 10 groups (including control one)
with 2 mesures for each (ref at t0 and response at t1).

We expect to compare each group response with control response (group 1)
using a multiple comparison procedure (Dunnett test).

In order to perform this test, we have to normalize our data (as you) to
correct response values by the base line normalized.

Covariance analysis seems to represent the best way to do this. But how to
perform this by using R ?

So, if  someone is able to deal with this problem, could you please share
with us your precious knowledge ?

Thanks in advance,

Best Regards.




Alexandre MENICACCI
Bioinformatics - FOURNIER PHARMA
50, rue de Dijon - 21121 Daix - FRANCE
a.menicacci at fr.fournierpharma.com
t??l : 03.80.44.76.17



Original message :

Hello sir:
Here's a question on covariance analysis which needs your help. There're 3
experiments,and x refers to control while y refers to experimental result.
The purpose is to compare the "y" values across the 3 experiments.


experiment_1:
x:0.1 0.2 0.3 0.4 0.5
y:0.5 0.6 0.6 0.7 0.9


experiment_2:
x:1 2 3 4 5
y:3 4 6.5 7.5 11


experiment_3:
x:10 20 30 40 50
y:18 35 75 90 98


Apparently,the control("x") isn't at the similar level so that we can't
compare the "y" directly through ANOVA. We must normalize "y" via "x" in
order to eliminate the influence of different level of "x". The method of
normalize I can get is "covariance analysis",since "x" is the covariant of
y.


My question is:
How to perform "covariance analysis" by using R? After this
normalization,we can get the according "normalized y" of every "original
y".


All in all,the "normalized y" of every "original y" is what I want indeed.


Thanks a lot!


My best regards!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From aleszib at gmail.com  Sat Nov 26 09:13:35 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Sat, 26 Nov 2005 09:13:35 +0100
Subject: [R] Generating all possible partitions
References: <971536df0511251009x5197c100s123c2b593e89ec3a@mail.gmail.com>
	<000001c5f1f3$c858d8e0$5994100a@win.ad.jhu.edu>
	<971536df0511251116p1b11e8c9n5101ff2b736e9c48@mail.gmail.com>
Message-ID: <091501c5f261$557270c0$0100a8c0@ALES>

Thank you all for your answers. Unfortunately, I have realized that my 
example was flawed. There are obviously 3 partitions of 3 objects into 2 
classes:



1 1 2

1 2 1

2 1 1



I would like to emphasize that the numbers are not important, for example, 
we could exchage 1s and 2s or even replace them with a and b.



What did until now is to use the following two functions (the functions are 
at the bottom of the mail):



All.k3.n5.par<- find.all.par(n=5,k=3)

All.k3.n5.par<-remove.double(All.k3.n5.par)



However, this is very time consuming and only suitable for very small n and 
k.



Thank you again for all the answers,



Best refgards,

Ales Ziberna



find.all.par<-function( #suitable for only very small networks and ks 
(complexity is k^n)

            n,         #number of units

            k,         #number of clusters

            only.k.groups=TRUE,  #do we damand that a partition has exactly 
k groups, or are less groups also allowed

            switch.names=TRUE   #should partitions that only differ in group 
names be considert equal (is c(1,1,2)==c(2,2,1))

){

            groups<-rep(list(1:k),n)

            comb<-as.matrix(expand.grid(groups))

            comb<-comb[,dim(comb)[2]:1]

            dimnames(comb)<-NULL

            if(switch.names) comb<-comb[1:(dim(comb)[1]/2),]

            comb<-comb[apply(comb,1,function(x)length(table(x)))>=ifelse(only.k.groups,k,2),]

            return(comb)

}



remove.double<-function(M) #removes duplicated partitios (when rand = 1) 
from matrix M

{

            new.M<-M[1,, drop = FALSE]

            for(i in 2:dim(M)[1]){

                        new<-TRUE

                        for(i2 in 1:dim(new.M)[1])

                        {

                                   if(rand(table(as.numeric(M[i,]),as.numeric(new.M[i2,])))==1)new<-FALSE

                        }

                        if(new) new.M<-rbind(new.M,M[i,])

            }

            return(new.M)

}



#a function used by function "remove.double"

rand<-function (tab) #extracted from function classAgreement from packcage 
'e1071'

{

    n <- sum(tab)

    ni <- apply(tab, 1, sum)

    nj <- apply(tab, 2, sum)

    n2 <- choose(n, 2)

    1 + (sum(tab^2) - (sum(ni^2) + sum(nj^2))/2)/n2

}



----- Original Message ----- 
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
To: "Ravi Varadhan" <rvaradha at jhsph.edu>
Cc: "Ales Ziberna" <aleszib at gmail.com>; "R-help" <r-help at stat.math.ethz.ch>
Sent: Friday, November 25, 2005 8:16 PM
Subject: Re: [R] Generating all possible partitions


Yes, I just checked on Wikipedia and its as you say.

On 11/25/05, Ravi Varadhan <rvaradha at jhsph.edu> wrote:
> Isn't Bell number different from the number of partitions, P_n, of a 
> number,
> n?
>
> Bell number, B_n, is the number of subsets into which a set with "n"
> elements can be divided.  So, B_3 = 5, and B_4 = 15, whereas P_3 = 3, and
> P_4 = 5.  Bell numbers grow much more rapidly than the number of 
> partitions.
>
> Ravi.
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> > Sent: Friday, November 25, 2005 1:10 PM
> > To: Ales Ziberna
> > Cc: R-help
> > Subject: Re: [R] Generating all possible partitions
> >
> > Probably not very fast but the number of partitions of a number,
> > also known as the Bell number, grows pretty dramatically so you
> > won't be able to use it for large numbers even with an efficient
> > implementation (though you could use it for larger numbers than
> > the solution here).  The main attribute of this approach is its
> > simplicity.   It generates the cartesian product
> > { 0, 1, 2, ..., n } ^ n and then picks off the elements that are
> > non-increasing and sum to n.
> >
> > n <- 3
> > g <- do.call("expand.grid", rep(list(0:n), n)) # cartesian product
> > f <- function(x) all(diff(x) <= 0) && sum(x) == length(x)
> > g[apply(g, 1, f), ]
> >
> >
> > On 11/25/05, Ales Ziberna <aleszib at gmail.com> wrote:
> > > I have posed this question earlier, however it has probably not been
> > clear
> > > enough.
> > >
> > >
> > >
> > > My problem is such. I would like to find all possible partitions of a
> > set of
> > > n objects into k groups. The ordering of the groups does not matter,
> > only
> > > which objects are together matters.
> > >
> > >
> > >
> > > For example, there are two possible partitions of 3 objects into 2
> > groups:
> > >
> > > 1 1 2
> > >
> > > 1 2 2
> > >
> > > By "the labels are not important" I meant that a partition 1 1 2 is
> > > identical to the partition 2 2 1.
> > >
> > >
> > > Best regards,
> > >
> > > Ales Ziberna
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
>



From anjali_karve at yahoo.com  Fri Nov 25 22:23:12 2005
From: anjali_karve at yahoo.com (Anjali Karve)
Date: Fri, 25 Nov 2005 13:23:12 -0800 (PST)
Subject: [R] obtaining a ROC curve
Message-ID: <20051125212312.53287.qmail@web35505.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051125/ce75076d/attachment.pl

From c.ginestet at imperial.ac.uk  Sat Nov 26 16:09:14 2005
From: c.ginestet at imperial.ac.uk (Ginestet, Cedric)
Date: Sat, 26 Nov 2005 15:09:14 -0000
Subject: [R] Double FOR
Message-ID: <3C2082E50F32E1459503A8E675E24EE47AD371@icex3.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051126/81133e3b/attachment.pl

From maechler at stat.math.ethz.ch  Sat Nov 26 17:13:52 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 26 Nov 2005 17:13:52 +0100
Subject: [R] optim
In-Reply-To: <438713A6.7010005@pdf.com>
References: <43870C73.6080500@nsl.ethz.ch>
	<438713A6.7010005@pdf.com>
Message-ID: <17288.35264.748480.655430@stat.math.ethz.ch>

Further note, that the standard package  'stats4'  has a
function  mle()  which was written exactly for problems like
yours {and mle() internally calls optim() !}.

But are we sure we are not solving your homework assignment here?

>>>>> "Sundar" == Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>
>>>>>     on Fri, 25 Nov 2005 07:37:42 -0600 writes:

    Sundar> Adrienne Gret-Regamey wrote:
    >> Hello:
    >> 
    >> I am trying to use optim() to estimate the maximum
    >> likelihood of a function a*x^b = y.  Unfortunately, I
    >> always get the error, that there is no default value for
    >> b.
    >> 
    >> Could you give me an example, on how to correctly
    >> optimize this function with input data x<-c(1,3,11,14).
    >> 
    >> Thanks a lot,
    >> 
    >> Adrienne
>>>>> "Sundar" == Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>
>>>>>     on Fri, 25 Nov 2005 07:37:42 -0600 writes:

    Sundar> Adrienne Gret-Regamey wrote:
    >> Hello:
    >> 
    >> I am trying to use optim() to estimate the maximum likelihood of a 
    >> function a*x^b = y.
    >> Unfortunately, I always get the error, that there is no default value for b.
    >> 
    >> Could you give me an example, on how to correctly optimize this function 
    >> with input data x<-c(1,3,11,14).
    >> 
    >> Thanks a lot,
    >> 
    >> Adrienne

    Sundar> Problems such as these will have more meaningful responses if you post 
    Sundar> an example (see the posting guide). What's being parameterized here? a 
    Sundar> or b or both? If just 'a', then fix 'b' at the desired value. If both, 
    Sundar> then give optim a starting value. And what is 'y'?

    Sundar> fn <- function(par, y, x) {
    Sundar> a <- par[1]
    Sundar> b <- par[2]
    Sundar> sum((y - a * x^b)^2)
    Sundar> }

    Sundar> x <- c(1, 3, 11, 14)
    Sundar> y <- exp(rnorm(length(x)))
    Sundar> ## y ~ a * x^b
    Sundar> ## log(y) ~ log(a) + b * log(x)
    Sundar> v <- coef(lm(log(y) ~ log(x)))
    Sundar> optim(c(exp(v[1]), v[2]), fn, y = y, x = x)

    Sundar> Again, I may be way off. Please provide an example if this doesn't cover 
    Sundar> what you need. And define all the variables need to run your script.

    Sundar> --sundar



From aleszib at gmail.com  Sat Nov 26 17:34:22 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Sat, 26 Nov 2005 17:34:22 +0100
Subject: [R] Double FOR
References: <3C2082E50F32E1459503A8E675E24EE47AD371@icex3.ic.ac.uk>
Message-ID: <098a01c5f2a7$6e97f5c0$0100a8c0@ALES>

Maybe you whould try:

One of your problems is that you are rewriting the resoults so that the 
resoult you get is only for the final combination of m, s, y.

Secondly, I am not sure if you want to use in the equation elements of m, s, 
y, or the whole vectors.

function (m,s,y)

{
DIC.hat<-NULL
for (j in m){

for (k in s){

for (i in y){



DIC.hat<-cbind(DIC.hat,c(m=j,s=k,y=i,DIC.hat=sum(-2*((log(1/sqrt(2*pi*k^2))*exp((((i-j)/k)^2)/-2)))))

}

}

}

DIC.hat

}


Best,
Ales Ziberna
----- Original Message ----- 
From: "Ginestet, Cedric" <c.ginestet at imperial.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Saturday, November 26, 2005 4:09 PM
Subject: [R] Double FOR


> Hi,
>
>
>
> I want to run through a formula several times with several different
> variables (which are defined by independent vectors of equal length 10
> elements). It looks like this:
>
>
>
>
>
> function (m,s,y)
>
> {
>
> for (j in m){
>
> for (k in s){
>
> for (i in y){
>
>
>
> DIC.hat<-sum(-2*((log(1/sqrt(2*pi*s^2))*exp((((y-m[j])/s)^2)/-2))))
>
> }
>
> }
>
> }
>
> DIC.hat
>
> }
>
>
>
>
>
> My problem is that R runs the three variables at the same time providing
> me with 10 new elements for DIC.hat, when I would like to have 20 times
> more.
>
>
>
> Can you help?
>
>
>
>
>
> ----------------------------------------------------
>
> Cedric Ginestet
>
> Department of Epidemiology and Public Health
>
> Faculty of Medicine
>
> Imperial College
>
> Norfolk Place
>
> London
>
> W2 1PG
>
> UK
>
> Tel:  +44 (0)77 8688 4313
>
> Fax: +44 (0)20 7402 2150
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From h.wickham at gmail.com  Sat Nov 26 18:21:08 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 26 Nov 2005 09:21:08 -0800
Subject: [R] Double FOR
In-Reply-To: <3C2082E50F32E1459503A8E675E24EE47AD371@icex3.ic.ac.uk>
References: <3C2082E50F32E1459503A8E675E24EE47AD371@icex3.ic.ac.uk>
Message-ID: <f8e6ff050511260921j5eebf41et3412b41f195eff1d@mail.gmail.com>

> I want to run through a formula several times with several different
> variables (which are defined by independent vectors of equal length 10
> elements). It looks like this:

One way would be explicitly create your data first:
df <- expand.grid(a = 1:5, b= 1:5, c=1:5)

And then take advantage of vectorisation to compute DIC.hat:
with(df, sum(-2*((log(1/sqrt(2*pi*a^2))*exp((((b-c)/a)^2)/-2))))

which looks awfully like differences of two normal densities, so you
might be able to use dnorm (which might be faster as it is written
purely in C (but not noticeably so unless you have a lot of data), but
will make your algorithm more clear).

By breaking it down into multiple steps, hopefully you can get a
better idea of what's going on.  Putting browser() in the middle of
your loop would be another way for you to check out what is really
happening.

Hadley

On 11/26/05, Ginestet, Cedric <c.ginestet at imperial.ac.uk> wrote:
> Hi,
>
>
>
>
>
>
>
>
> function (m,s,y)
>
> {
>
> for (j in m){
>
> for (k in s){
>
> for (i in y){
>
>
>
> DIC.hat<-sum(-2*((log(1/sqrt(2*pi*s^2))*exp((((y-m[j])/s)^2)/-2))))
>
> }
>
> }
>
> }
>
> DIC.hat
>
> }
>
>
>
>
>
> My problem is that R runs the three variables at the same time providing
> me with 10 new elements for DIC.hat, when I would like to have 20 times
> more.
>
>
>
> Can you help?
>
>
>
>
>
> ----------------------------------------------------
>
> Cedric Ginestet
>
> Department of Epidemiology and Public Health
>
> Faculty of Medicine
>
> Imperial College
>
> Norfolk Place
>
> London
>
> W2 1PG
>
> UK
>
> Tel:  +44 (0)77 8688 4313
>
> Fax: +44 (0)20 7402 2150
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmbates at gmail.com  Sat Nov 26 18:38:33 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sat, 26 Nov 2005 11:38:33 -0600
Subject: [R] Use of nesting in lmer- error in numerical expression
In-Reply-To: <001601c5f1ca$21794290$e757f181@STUDENTA046822>
References: <001601c5f1ca$21794290$e757f181@STUDENTA046822>
Message-ID: <40e66e0b0511260938v3f3001canb4682ee3f7fac616@mail.gmail.com>

On 11/25/05, Arild Husby <arildhus at stud.ntnu.no> wrote:
>
>
> Dear R users,
>
>
>
> I am trying to fit a GLMM using lmer to a dataset where the brood identity
> (LNRREIR) is nested within mothers identity. The reason for this is that
> each mother can have several nests within each year and also between years.
>
>
>
> I am running the following script (actually I have tried all different
> combinations with LNRREIR and mother):
>
>
>
> mod <- lmer(sr~z.hatchday  + (1|LNRREIR:mother) + (1|mother),
> family=binomial, data=aggrsexfil)
>
> Error in LNRREIR:mother : result would be too long a vector

Are both  LNRREIR and mother stored as factors?

If the brood identity is unique for each brood then there is no need
to create the grouping factor as an interaction (although the practice
is encouraged as a precautionary measure).

> In addition: Warning messages:
>
> 1: numerical expression has 64 elements: only the first used in:
> LNRREIR:mother
>
> 2: numerical expression has 64 elements: only the first used in:
> LNRREIR:mother
>
>
>
> Is this the wrong way to specify it or what is happening here?
>
>
>
> I am also wondering if only including mother as a random factor, do R
> understand that its nested? That is, when there are e.g. 60 different nests
> and 40 mothers. Do R then take into account that each mother can have
> multiple broods? In that way the nesting seems redundant..?

If you only include mother as a random factor it can't be either
nested or not.  The property of nesting is a relationship between two
grouping factors.

>
>
>
>
>
> Thanks for all help!
>
>
>
> Best regards,
>
>
>
> Arild
>
>
>
>
>
> --------------------------------------------
>
> Arild Husby (M.Sc.),
>
> Research Technician,
>
> Department of Biology,
>
> Realfagbygget,
>
> Norwegian University of Science and Technology,
>
> N-7491 Trondheim
>
> NORWAY
>
> e-mail:  <mailto:arildhus at stud.ntnu.no> arildhus at stud.ntnu.no
>
> mobile: +47 92294412
>
> office : +47 73596266
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wqiu at nmr.mgh.harvard.edu  Sat Nov 26 20:39:31 2005
From: wqiu at nmr.mgh.harvard.edu (Wei Qiu)
Date: Sat, 26 Nov 2005 14:39:31 -0500 (EST)
Subject: [R] How to do the multiple plots?
In-Reply-To: <200511182211.jAIMB6a8005889@hypatia.math.ethz.ch>
References: <200511182211.jAIMB6a8005889@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.62.0511261431240.27951@entry.nmr.mgh.harvard.edu>

Hi all,

I am new in R tool. I have a basic question. I would like to do a 
combination of two multiple lines plots for one X-variable and two 
different sets (lists) of Y-variables. Any suggestion will be greatly 
appreciated.

Thanks!

Wei



From ggrothendieck at gmail.com  Sat Nov 26 20:47:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 26 Nov 2005 14:47:59 -0500
Subject: [R] How to do the multiple plots?
In-Reply-To: <Pine.LNX.4.62.0511261431240.27951@entry.nmr.mgh.harvard.edu>
References: <200511182211.jAIMB6a8005889@hypatia.math.ethz.ch>
	<Pine.LNX.4.62.0511261431240.27951@entry.nmr.mgh.harvard.edu>
Message-ID: <971536df0511261147sd2eaf96sfd718acb22fe01fb@mail.gmail.com>

?lines
?matplot
?ts.plot
library(zoo); ?plot.zoo


On 11/26/05, Wei Qiu <wqiu at nmr.mgh.harvard.edu> wrote:
> Hi all,
>
> I am new in R tool. I have a basic question. I would like to do a
> combination of two multiple lines plots for one X-variable and two
> different sets (lists) of Y-variables. Any suggestion will be greatly
> appreciated.
>
> Thanks!
>
> Wei
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wqiu at nmr.mgh.harvard.edu  Sat Nov 26 21:40:52 2005
From: wqiu at nmr.mgh.harvard.edu (Wei Qiu)
Date: Sat, 26 Nov 2005 15:40:52 -0500 (EST)
Subject: [R] How to do the multiple plots?
In-Reply-To: <971536df0511261147sd2eaf96sfd718acb22fe01fb@mail.gmail.com>
References: <200511182211.jAIMB6a8005889@hypatia.math.ethz.ch> 
	<Pine.LNX.4.62.0511261431240.27951@entry.nmr.mgh.harvard.edu>
	<971536df0511261147sd2eaf96sfd718acb22fe01fb@mail.gmail.com>
Message-ID: <Pine.LNX.4.62.0511261534050.28175@entry.nmr.mgh.harvard.edu>

Dear Babor and all,

Thanks for your quick response. I tried the following. We can plot 
y1 and Y2 on one figs. It just shows one Y axis on the left side. I 
would like to show the Y1 axis on left side and Y2 Axis on right side.

Any others input,

Wei

On Sat, 26 Nov 2005, Gabor Grothendieck wrote:

> ?lines
> ?matplot
> ?ts.plot
> library(zoo); ?plot.zoo
>
>
> On 11/26/05, Wei Qiu <wqiu at nmr.mgh.harvard.edu> wrote:
>> Hi all,
>>
>> I am new in R tool. I have a basic question. I would like to do a
>> combination of two multiple lines plots for one X-variable and two
>> different sets (lists) of Y-variables. Any suggestion will be greatly
>> appreciated.
>>
>> Thanks!
>>
>> Wei
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>
>



From wqiu at nmr.mgh.harvard.edu  Sat Nov 26 21:46:20 2005
From: wqiu at nmr.mgh.harvard.edu (Wei Qiu)
Date: Sat, 26 Nov 2005 15:46:20 -0500 (EST)
Subject: [R] How to do the multiple plots?
In-Reply-To: <Pine.LNX.4.62.0511261534050.28175@entry.nmr.mgh.harvard.edu>
References: <200511182211.jAIMB6a8005889@hypatia.math.ethz.ch> 
	<Pine.LNX.4.62.0511261431240.27951@entry.nmr.mgh.harvard.edu>
	<971536df0511261147sd2eaf96sfd718acb22fe01fb@mail.gmail.com>
	<Pine.LNX.4.62.0511261534050.28175@entry.nmr.mgh.harvard.edu>
Message-ID: <Pine.LNX.4.62.0511261546030.28175@entry.nmr.mgh.harvard.edu>

On Sat, 26 Nov 2005, Wei Qiu wrote:

> Dear Gabor and all,
>
> Thanks for your quick response. I tried the following. We can plot
> y1 and Y2 on one figs. It just shows one Y axis on the left side. I
> would like to show the Y1 axis on left side and Y2 Axis on right side.
>
> Any others input,
>
> Wei
>
> On Sat, 26 Nov 2005, Gabor Grothendieck wrote:
>
>> ?lines
>> ?matplot
>> ?ts.plot
>> library(zoo); ?plot.zoo
>>
>>
>> On 11/26/05, Wei Qiu <wqiu at nmr.mgh.harvard.edu> wrote:
>>> Hi all,
>>>
>>> I am new in R tool. I have a basic question. I would like to do a
>>> combination of two multiple lines plots for one X-variable and two
>>> different sets (lists) of Y-variables. Any suggestion will be greatly
>>> appreciated.
>>>
>>> Thanks!
>>>
>>> Wei
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>



From Bpol at poste.it  Sat Nov 26 15:27:29 2005
From: Bpol at poste.it (Bpol@poste.it)
Date: 26 Nov 2005 23:27:29 +0900
Subject: [R] Misure di sicurezza di cliente
Message-ID: <20051126142729.2591.qmail@na1.newseoul.com>


   Caro R-help at lists.R-project.org ,
   Recentemente  abbiamo  notato uno o più tentativi di entrare al vostro
   conto di BancoPostaonline da un IP indirizzo differente.
   Se  recentemente  accedeste  al  vostro  conto  mentre  viaggiavate, i
   tentativi  insoliti  di  accedere  a  vostro  Conto BancoPosta possono
   essere iniziati da voi.
   Tuttavia,   visiti   prego   appena   possibile  BancoPostaonline  per
   controllare le vostre informazioni di conto:
   [1]https://bancopostaonline.poste.it/bpol/bancoposta/formslogin.asp
   Ringraziamenti per vostra pazienza.
   BancoPostaonline.
   ----------------------------------------------------------
   Non  risponda  prego  a  questo  E-mail.  Il E-mail trasmesso a questo
   indirizzo non può essere risposto a.

References

   1. http://www.booktree.co.kr/AsaMall/poste.php


From leavestonebodt at yahoo.com  Sat Nov 26 22:15:09 2005
From: leavestonebodt at yahoo.com (yuying shi)
Date: Sat, 26 Nov 2005 13:15:09 -0800 (PST)
Subject: [R] Newton iteration questions
Message-ID: <20051126211509.40438.qmail@web52504.mail.yahoo.com>

Dear Sir/Madam,
     If I have a sample of observations that come from
an extreme value distribution, the density function
for the extreme value distribution is: 

f(x)=(1/b)exp[-(x-a)/b]exp{-exp[-(x-a)/b]}, b>0, x can
be any value,
 
my question is how to implement the Newton iteration
and estimate the parameters for this distribution and
the accuracy of epsilon=0.0001?

 The n= 100 observations are given as follows:
x<- c(8.8, 9.4, 8.7, 9.3, 9.6, 9.4, 9.1, 9.4, 8.4,
6.8, 8.4,
?.2, 9.4, 7.4, 8.7, 9.4, 9.2, 9.3, 8.0, 8.5, 8.7, 9.7,
9.8,
?.5, 7.1, 7.8, 9.0, 8.6, 9.4, 6.9, 9.1, 9.9, 7.3, 8.5,
8.8,
?.4, 9.0, 8.6, 8.5, 9.2, 9.7, 9.2, 9.2, 8.4, 8.7, 9.6,
9.2,
?.8, 8.5, 9.0, 8.9, 9.6, 8.0, 9.7, 8.4, 7.5, 9.1, 9.2,
8.9,
?.2, 9.8, 9.4, 8.5, 9.3, 9.8, 9.6, 9.7, 8.9, 9.7, 8.7,
8.6,
?.7, 8.6, 9.7, 7.7, 8.6, 9.7, 8.5, 9.4, 9.4, 9.7, 8.1,
9.5,
?.3, 8.0, 9.8, 8.9, 9.5, 9.0, 8.7, 9.1, 8.5, 8.7, 8.4,
9.3,
?.5, 8.9, 9.3, 9.0, 9.9)?

thanks in advance!
xingyu



From c_naber at yahoo.com.br  Sat Nov 26 22:50:55 2005
From: c_naber at yahoo.com.br (Caio Lucidius Naberezny Azevedo)
Date: Sat, 26 Nov 2005 21:50:55 +0000 (GMT)
Subject: [R] IRT Package
Message-ID: <20051126215056.3051.qmail@web34003.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051126/436b9732/attachment.pl

From dataanalytics at earthlink.net  Sat Nov 26 22:53:00 2005
From: dataanalytics at earthlink.net (Walter R. Paczkowski)
Date: Sat, 26 Nov 2005 21:53:00 +0000
Subject: [R] Using an editor with R
Message-ID: <E1Eg7zV-0000FT-3o@smtpauth02.mail.atl.earthlink.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051126/32b2250b/attachment.pl

From charles_loboz at yahoo.com  Sat Nov 26 23:24:58 2005
From: charles_loboz at yahoo.com (charles loboz)
Date: Sat, 26 Nov 2005 14:24:58 -0800 (PST)
Subject: [R] What does KalmanRun$states really return?
Message-ID: <20051126222459.32445.qmail@web60818.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051126/6bfa4984/attachment.pl

From ggrothendieck at gmail.com  Sat Nov 26 23:40:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 26 Nov 2005 17:40:40 -0500
Subject: [R] How to do the multiple plots?
In-Reply-To: <Pine.LNX.4.62.0511261546030.28175@entry.nmr.mgh.harvard.edu>
References: <200511182211.jAIMB6a8005889@hypatia.math.ethz.ch>
	<Pine.LNX.4.62.0511261431240.27951@entry.nmr.mgh.harvard.edu>
	<971536df0511261147sd2eaf96sfd718acb22fe01fb@mail.gmail.com>
	<Pine.LNX.4.62.0511261534050.28175@entry.nmr.mgh.harvard.edu>
	<Pine.LNX.4.62.0511261546030.28175@entry.nmr.mgh.harvard.edu>
Message-ID: <971536df0511261440q5d3495g27b96a621537c040@mail.gmail.com>

axis(4, .,..) does it.  Try

     RSiteSearch("axis(4,")

to locate examples.


On 11/26/05, Wei Qiu <wqiu at nmr.mgh.harvard.edu> wrote:
> On Sat, 26 Nov 2005, Wei Qiu wrote:
>
> > Dear Gabor and all,
> >
> > Thanks for your quick response. I tried the following. We can plot
> > y1 and Y2 on one figs. It just shows one Y axis on the left side. I
> > would like to show the Y1 axis on left side and Y2 Axis on right side.
> >
> > Any others input,
> >
> > Wei
> >
> > On Sat, 26 Nov 2005, Gabor Grothendieck wrote:
> >
> >> ?lines
> >> ?matplot
> >> ?ts.plot
> >> library(zoo); ?plot.zoo
> >>
> >>
> >> On 11/26/05, Wei Qiu <wqiu at nmr.mgh.harvard.edu> wrote:
> >>> Hi all,
> >>>
> >>> I am new in R tool. I have a basic question. I would like to do a
> >>> combination of two multiple lines plots for one X-variable and two
> >>> different sets (lists) of Y-variables. Any suggestion will be greatly
> >>> appreciated.
> >>>
> >>> Thanks!
> >>>
> >>> Wei
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >>
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> >
>



From murdoch at stats.uwo.ca  Sun Nov 27 00:09:04 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 26 Nov 2005 18:09:04 -0500
Subject: [R] Using an editor with R
In-Reply-To: <E1Eg7zV-0000FT-3o@smtpauth02.mail.atl.earthlink.net>
References: <E1Eg7zV-0000FT-3o@smtpauth02.mail.atl.earthlink.net>
Message-ID: <4388EB10.6050105@stats.uwo.ca>

On 11/26/2005 4:53 PM, Walter R. Paczkowski wrote:
> Hello,
> 
> I changed the setting in options$editor to allow me to use my favorite editor.  In R 2.1.1 on Windows XP, I entered at the command line:
> 
> options(editor="c:\\program files\\winedit\\winedit.exe")
> 
> When I edited a function, say test, using fix(test), the editor opened perfectly.  But, when I saved the file and closed the editor, the R gui screen was white, blank, and completely unresponsive.  The only thing I could do was close R by clicking on the "X" in the upper right corner of the window.  How can I use my editor but be able to continue using R after I close the editor?  What extra setting am I missing?

This sounds like you didn't really close your editor.  R isn't smart 
enough to know that the editor closed a file, it can only see when the 
process finishes.

I'd recommend using the RWinEdt package instead for a different way to 
integrate winedit with R.

Duncan Murdoch



From ggrothendieck at gmail.com  Sun Nov 27 00:21:52 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 26 Nov 2005 18:21:52 -0500
Subject: [R] Using an editor with R
In-Reply-To: <4388EB10.6050105@stats.uwo.ca>
References: <E1Eg7zV-0000FT-3o@smtpauth02.mail.atl.earthlink.net>
	<4388EB10.6050105@stats.uwo.ca>
Message-ID: <971536df0511261521x7391ad67ia4102477e03c8cda@mail.gmail.com>

But beware of those untrustworthy ESS users :)

>From www.vim.org:

The spammers have found us
[2005-11-18] I suppose I should be surprised that it took this long
but it appears as if the spammers have found us. There are two things
that I want to do to fix this: (1) Implement moderators, (2) Require
login to post a tip. I would like to get a quick and dirty version of
#1 up quickly to deal with the problem. I am looking for volunteers to
moderate. A moderator must: (a) visit and read tips regularly, (b) be
willing to dedicate time to moderating, (c) be a good person. OK, so I
admit (c) might be a bit much but if you are a vim user there is a
good chance you are a good person (it is the emacs users I never know
if I can trust :). If you are interested please send me an email at
scott dot m dot johnston at g mail dot com. Thanks for your patience.
(Scott Johnston)



On 11/26/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 11/26/2005 4:53 PM, Walter R. Paczkowski wrote:
> > Hello,
> >
> > I changed the setting in options$editor to allow me to use my favorite editor.  In R 2.1.1 on Windows XP, I entered at the command line:
> >
> > options(editor="c:\\program files\\winedit\\winedit.exe")
> >
> > When I edited a function, say test, using fix(test), the editor opened perfectly.  But, when I saved the file and closed the editor, the R gui screen was white, blank, and completely unresponsive.  The only thing I could do was close R by clicking on the "X" in the upper right corner of the window.  How can I use my editor but be able to continue using R after I close the editor?  What extra setting am I missing?
>
> This sounds like you didn't really close your editor.  R isn't smart
> enough to know that the editor closed a file, it can only see when the
> process finishes.
>
> I'd recommend using the RWinEdt package instead for a different way to
> integrate winedit with R.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From svenier at aueb.gr  Sun Nov 27 01:05:59 2005
From: svenier at aueb.gr (svenier@aueb.gr)
Date: Sun, 27 Nov 2005 02:05:59 +0200
Subject: [R] coherency-Time Series
Message-ID: <6.0.2.0.1.20051127020530.01b45278@pop.aueb.gr>

hello!
My name is Stefanos, from Athens.
I'm a new user of R and I'm studying multivariate time series. I can't find 
in the help menu how to calculate the cross spectrum and the coherency of 2 
Time Series. Would you like to help me?
Thanks



From dimitrijoe at ipea.gov.br  Sun Nov 27 02:54:55 2005
From: dimitrijoe at ipea.gov.br (dimitrijoe@ipea.gov.br)
Date: Sat, 26 Nov 2005 23:54:55 -0200
Subject: [R] creating a factor from other factors and ifelse
Message-ID: <1133056495.438911ef2e42b@webmail.ipea.gov.br>



Hi,

Given

> sec98 <- factor(rep(1:2,3), labels=c("A", "B"))
> sec99 <- factor(rep(2:1,3), labels=c("A", "B"))
>    sec99[c(2,5)] <- NA
> sec00 <- factor( c( rep(1,3), rep(2,3) ), labels=c("A", "B"))
>    sec00[c(2,4)] <- NA
> sec1 <- ifelse(!is.na(sec99), sec99,
        ifelse(!is.na(sec00), sec00, NA ))

We get

> sec1; class(sec1)
[1]  2 NA  2  1  2  1
[1] "integer"

I wonder why sec1 as above defined  in not a factor, since it has been 
created from (logical operations and) factors. Of course, one could do

> sec1 <- factor(sec1, labels=levels(sec99))

but this would be a problem if I had (as I actually do) sec99 and sec00 
instead defined as

> sec99 <- factor(c(1,2,3,2,3,3), labels=c("A", "B", "C"))
>   sec99[c(2,5)] <- NA
> sec00 <- factor(c(4,1,1,2,4,2), labels=c("A", "B", "D"))
>    sec00[c(2,4)] <- NA

    # because
> sec1 <- ifelse(!is.na(sec99), sec99,
>        ifelse(!is.na(sec00), sec00, NA ))

    # gives us
> sec1; class(sec1)
[1]  1 NA  3  2  3  3
[1] "integer"

now it's hard to tell where each "3" in sec1 means "C" or "D". What I 
actually wanted was

> sec1; class(sec1)
[1]  A <NA>  C  B  D  C
[1] "factor"

Any suggestions on how to do it in a simple way will be welcome.
Thanks,
Dimitri



From dimitrijoe at ipea.gov.br  Sun Nov 27 02:56:12 2005
From: dimitrijoe at ipea.gov.br (dimitrijoe@ipea.gov.br)
Date: Sat, 26 Nov 2005 23:56:12 -0200
Subject: [R] creating a factor from other factors and ifelse
Message-ID: <1133056572.4389123c336d9@webmail.ipea.gov.br>



Hi,

Given

> sec98 <- factor(rep(1:2,3), labels=c("A", "B"))
> sec99 <- factor(rep(2:1,3), labels=c("A", "B"))
>    sec99[c(2,5)] <- NA
> sec00 <- factor( c( rep(1,3), rep(2,3) ), labels=c("A", "B"))
>    sec00[c(2,4)] <- NA
> sec1 <- ifelse(!is.na(sec99), sec99,
        ifelse(!is.na(sec00), sec00, NA ))

We get

> sec1; class(sec1)
[1]  2 NA  2  1  2  1
[1] "integer"

I wonder why sec1 as above defined  in not a factor, since it has been 
created from (logical operations and) factors. Of course, one could do

> sec1 <- factor(sec1, labels=levels(sec99))

but this would be a problem if I had (as I actually do) sec99 and sec00 
instead defined as

> sec99 <- factor(c(1,2,3,2,3,3), labels=c("A", "B", "C"))
>   sec99[c(2,5)] <- NA
> sec00 <- factor(c(4,1,1,2,4,2), labels=c("A", "B", "D"))
>    sec00[c(2,4)] <- NA

    # because
> sec1 <- ifelse(!is.na(sec99), sec99,
>        ifelse(!is.na(sec00), sec00, NA ))

    # gives us
> sec1; class(sec1)
[1]  1 NA  3  2  3  3
[1] "integer"

now it's hard to tell where each "3" in sec1 means "C" or "D". What I 
actually wanted was

> sec1; class(sec1)
[1]  A <NA>  C  B  D  C
[1] "factor"

Any suggestions on how to do it in a simple way will be welcome.
Thanks,
Dimitri



From dimitrijoe at ipea.gov.br  Sun Nov 27 02:57:28 2005
From: dimitrijoe at ipea.gov.br (dimitrijoe@ipea.gov.br)
Date: Sat, 26 Nov 2005 23:57:28 -0200
Subject: [R] creating a factor from other factors and ifelse
Message-ID: <1133056648.438912884e920@webmail.ipea.gov.br>



Hi,

Given

> sec98 <- factor(rep(1:2,3), labels=c("A", "B"))
> sec99 <- factor(rep(2:1,3), labels=c("A", "B"))
>    sec99[c(2,5)] <- NA
> sec00 <- factor( c( rep(1,3), rep(2,3) ), labels=c("A", "B"))
>    sec00[c(2,4)] <- NA
> sec1 <- ifelse(!is.na(sec99), sec99,
        ifelse(!is.na(sec00), sec00, NA ))

We get

> sec1; class(sec1)
[1]  2 NA  2  1  2  1
[1] "integer"

I wonder why sec1 as above defined  in not a factor, since it has been 
created from (logical operations and) factors. Of course, one could do

> sec1 <- factor(sec1, labels=levels(sec99))

but this would be a problem if I had (as I actually do) sec99 and sec00 
instead defined as

> sec99 <- factor(c(1,2,3,2,3,3), labels=c("A", "B", "C"))
>   sec99[c(2,5)] <- NA
> sec00 <- factor(c(4,1,1,2,4,2), labels=c("A", "B", "D"))
>    sec00[c(2,4)] <- NA

    # because
> sec1 <- ifelse(!is.na(sec99), sec99,
>        ifelse(!is.na(sec00), sec00, NA ))

    # gives us
> sec1; class(sec1)
[1]  1 NA  3  2  3  3
[1] "integer"

now it's hard to tell where each "3" in sec1 means "C" or "D". What I 
actually wanted was

> sec1; class(sec1)
[1]  A <NA>  C  B  D  C
[1] "factor"

Any suggestions on how to do it in a simple way will be welcome.
Thanks,
Dimitri



From dimitrijoe at ipea.gov.br  Sun Nov 27 02:58:44 2005
From: dimitrijoe at ipea.gov.br (dimitrijoe@ipea.gov.br)
Date: Sat, 26 Nov 2005 23:58:44 -0200
Subject: [R] creating a factor from other factors and ifelse
Message-ID: <1133056724.438912d47acaa@webmail.ipea.gov.br>



Hi,

Given

> sec98 <- factor(rep(1:2,3), labels=c("A", "B"))
> sec99 <- factor(rep(2:1,3), labels=c("A", "B"))
>    sec99[c(2,5)] <- NA
> sec00 <- factor( c( rep(1,3), rep(2,3) ), labels=c("A", "B"))
>    sec00[c(2,4)] <- NA
> sec1 <- ifelse(!is.na(sec99), sec99,
        ifelse(!is.na(sec00), sec00, NA ))

We get

> sec1; class(sec1)
[1]  2 NA  2  1  2  1
[1] "integer"

I wonder why sec1 as above defined  in not a factor, since it has been 
created from (logical operations and) factors. Of course, one could do

> sec1 <- factor(sec1, labels=levels(sec99))

but this would be a problem if I had (as I actually do) sec99 and sec00 
instead defined as

> sec99 <- factor(c(1,2,3,2,3,3), labels=c("A", "B", "C"))
>   sec99[c(2,5)] <- NA
> sec00 <- factor(c(4,1,1,2,4,2), labels=c("A", "B", "D"))
>    sec00[c(2,4)] <- NA

    # because
> sec1 <- ifelse(!is.na(sec99), sec99,
>        ifelse(!is.na(sec00), sec00, NA ))

    # gives us
> sec1; class(sec1)
[1]  1 NA  3  2  3  3
[1] "integer"

now it's hard to tell where each "3" in sec1 means "C" or "D". What I 
actually wanted was

> sec1; class(sec1)
[1]  A <NA>  C  B  D  C
[1] "factor"

Any suggestions on how to do it in a simple way will be welcome.
Thanks,
Dimitri



From dimitrijoe at ipea.gov.br  Sun Nov 27 03:00:00 2005
From: dimitrijoe at ipea.gov.br (dimitrijoe@ipea.gov.br)
Date: Sun, 27 Nov 2005 00:00:00 -0200
Subject: [R] creating a factor from other factors and ifelse
Message-ID: <1133056800.43891320910ce@webmail.ipea.gov.br>



Hi,

Given

> sec98 <- factor(rep(1:2,3), labels=c("A", "B"))
> sec99 <- factor(rep(2:1,3), labels=c("A", "B"))
>    sec99[c(2,5)] <- NA
> sec00 <- factor( c( rep(1,3), rep(2,3) ), labels=c("A", "B"))
>    sec00[c(2,4)] <- NA
> sec1 <- ifelse(!is.na(sec99), sec99,
        ifelse(!is.na(sec00), sec00, NA ))

We get

> sec1; class(sec1)
[1]  2 NA  2  1  2  1
[1] "integer"

I wonder why sec1 as above defined  in not a factor, since it has been 
created from (logical operations and) factors. Of course, one could do

> sec1 <- factor(sec1, labels=levels(sec99))

but this would be a problem if I had (as I actually do) sec99 and sec00 
instead defined as

> sec99 <- factor(c(1,2,3,2,3,3), labels=c("A", "B", "C"))
>   sec99[c(2,5)] <- NA
> sec00 <- factor(c(4,1,1,2,4,2), labels=c("A", "B", "D"))
>    sec00[c(2,4)] <- NA

    # because
> sec1 <- ifelse(!is.na(sec99), sec99,
>        ifelse(!is.na(sec00), sec00, NA ))

    # gives us
> sec1; class(sec1)
[1]  1 NA  3  2  3  3
[1] "integer"

now it's hard to tell where each "3" in sec1 means "C" or "D". What I 
actually wanted was

> sec1; class(sec1)
[1]  A <NA>  C  B  D  C
[1] "factor"

Any suggestions on how to do it in a simple way will be welcome.
Thanks,
Dimitri



From leavestonebodt at yahoo.com  Sun Nov 27 05:29:12 2005
From: leavestonebodt at yahoo.com (yuying shi)
Date: Sat, 26 Nov 2005 20:29:12 -0800 (PST)
Subject: [R] r question
Message-ID: <20051127042913.46995.qmail@web52508.mail.yahoo.com>

If there are two random variable X1 and X2 which have
a bivariate normal distribution with mean vector (10,
10)and variance covariance matrix 
[2    1.95
 1.95    3]

How to calculate the mean and variance of the function
Y=X1/X2? 

Thanks a lot!
xingyu



From dylan.beaudette at gmail.com  Sun Nov 27 05:48:11 2005
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Sat, 26 Nov 2005 20:48:11 -0800
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <17287.22446.567343.472410@stat.math.ethz.ch>
References: <17284.28690.71104.813153@stat.math.ethz.ch>
	<20051123143323.GB6368@iwr.uni-heidelberg.de>
	<17284.38020.14279.158148@stat.math.ethz.ch>
	<971536df0511230929q74c48caax3892feec51fdcafe@mail.gmail.com>
	<971536df0511231351k52e1cd89o8c790b6ad6a1d1c5@mail.gmail.com>
	<17285.55052.354968.489470@stat.math.ethz.ch>
	<17287.22446.567343.472410@stat.math.ethz.ch>
Message-ID: <4012629d428b1d0c5f241be84df72374@gmail.com>

On Nov 25, 2005, at 10:27 AM, Martin Maechler wrote:

> Let me try to summarize my view on this:
>
> - I still it would make sense to have a *simple* peaks() function
>   in R which provides the same (or more) functionality as the
>   corresponding S-plus one.From
>   For a proper data analysis situation, I think one would have to
>   do something more sophisticated, based on a model (with a random
>   component), such as nonparametric regression, time-series,....
>   Hence peaks() should be kept as simple as reasonable.
>
> - Of course I know that  which() or %in% can be used to deal
>   with logicals containing NAs {As a matter of fact, I've had
>   my fingers in both implementations for R!}.
>   Still, the main use of logical vectors in S often is for
>   situations where NAs only appear because of missing data:
>
>   Indexing ([]), all(), any(), sum()  are all very nice and
>   useful for logical vectors particularly when there are no NAs.
>
> - I agree that a different more flexible function returning
>   values from {-1,0,1} would be desirable, "for symmetry reasons".
>   ===> added a peaksign() function
>
> Here's code that implements the above {and other concerns
> mentioned in this thread}, including some ``consistency
> checking'' :
>
> peaks <- function(series, span = 3, do.pad = TRUE) {
>     if((span <- as.integer(span)) %% 2 != 1) stop("'span' must be odd")
>     s1 <- 1:1 + (s <- span %/% 2)
>     if(span == 1) return(rep.int(TRUE, length(series)))
>     z <- embed(series, span)
>     v <- apply(z[,s1] > z[, -s1, drop=FALSE], 1, all)
>     if(do.pad) {
>         pad <- rep.int(FALSE, s)
>         c(pad, v, pad)
>     } else v
> }
>
> peaksign <- function(series, span = 3, do.pad = TRUE)
> {
>     ## Purpose: return (-1 / 0 / 1) if series[i] is ( trough / 
> "normal" / peak )
>     ## 
> ----------------------------------------------------------------------
>     ## Author: Martin Maechler, Date: 25 Nov 2005
>
>     if((span <- as.integer(span)) %% 2 != 1 || span == 1)
>         stop("'span' must be odd and >= 3")
>     s1 <- 1:1 + (s <- span %/% 2)
>     z <- embed(series, span)
>     d <- z[,s1] - z[, -s1, drop=FALSE]
>     ans <- rep.int(0:0, nrow(d))
>     ans[apply(d > 0, 1, all)] <- as.integer(1)
>     ans[apply(d < 0, 1, all)] <- as.integer(-1)
>     if(do.pad) {
>         pad <- rep.int(0:0, s)
>         c(pad, ans, pad)
>     } else ans
> }
>
>
> check.pks <- function(y, span = 3)
>     stopifnot(identical(peaks( y, span), peaksign(y, span) ==  1),
>               identical(peaks(-y, span), peaksign(y, span) == -1))
>
> for(y in list(1:10, rep(1,10), c(11,2,2,3,4,4,6,6,6))) {
>     for(sp in c(3,5,7))
>         check.pks(y, span = sp)
>     stopifnot(peaksign(y) == 0)
> }
>
> y <- c(1,4,1,1,6,1,5,1,1) ; (ii <- which(peaks(y))); y[ii]
> ##- [1] 2 5 7
> ##- [1] 4 6 5
> check.pks(y)
>
> set.seed(7)
> y <- rpois(100, lambda = 7)
> check.pks(y)
> py <- peaks(y)
> plot(y, type="o", cex = 1/4, main = "y and peaks(y,3)")
> points(seq(y)[py], y[py], col = 2, cex = 1.5)
>
> p7 <- peaks(y,7)
> points(seq(y)[p7], y[p7], col = 3, cex = 2)
> mtext("peaks(y,7)", col=3)
>
> set.seed(2)
> x <- round(rnorm(500), 2)
> y <- cumsum(x)
> check.pks(y)
>
> plot(y, type="o", cex = 1/4)
> p15 <- peaks(y,15)
> points(seq(y)[p15], y[p15], col = 3, cex = 2)
> mtext("peaks(y,15)", col=3)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>

Wow! This was the exact sort of simple peak finding algorithm I was 
looking for. Would it be ok for me to post this to our dept. webpage so 
that others may use it?

Cheers,

--
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341



From ggrothendieck at gmail.com  Sun Nov 27 06:39:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 27 Nov 2005 00:39:01 -0500
Subject: [R] creating a factor from other factors and ifelse
In-Reply-To: <1133056495.438911ef2e42b@webmail.ipea.gov.br>
References: <1133056495.438911ef2e42b@webmail.ipea.gov.br>
Message-ID: <971536df0511262139scd888d4uc497313bbddd988b@mail.gmail.com>

We don't need the inner if, ifelse does not work with factors as
I think you expect and in any case we really want to create a new
factor with a new set of levels consisting of those in the new variable

sec.char <- ifelse(is.na(sec99), as.character(sec00), as.character(sec99))
factor(sec.char)

or maybe the union of the original levels of sec99 and sec00 regardless
of whether they are in the resulting factor variable or not:

factor(sec.char, levels = union(levels(sec99), levels(sec00)))

(In this case the two factor calls above give the same result but in
general they could differ in the levels.)


On 11/26/05, dimitrijoe at ipea.gov.br <dimitrijoe at ipea.gov.br> wrote:
>
>
> Hi,
>
> Given
>
> > sec98 <- factor(rep(1:2,3), labels=c("A", "B"))
> > sec99 <- factor(rep(2:1,3), labels=c("A", "B"))
> >    sec99[c(2,5)] <- NA
> > sec00 <- factor( c( rep(1,3), rep(2,3) ), labels=c("A", "B"))
> >    sec00[c(2,4)] <- NA
> > sec1 <- ifelse(!is.na(sec99), sec99,
>        ifelse(!is.na(sec00), sec00, NA ))
>
> We get
>
> > sec1; class(sec1)
> [1]  2 NA  2  1  2  1
> [1] "integer"
>
> I wonder why sec1 as above defined  in not a factor, since it has been
> created from (logical operations and) factors. Of course, one could do
>
> > sec1 <- factor(sec1, labels=levels(sec99))
>
> but this would be a problem if I had (as I actually do) sec99 and sec00
> instead defined as
>
> > sec99 <- factor(c(1,2,3,2,3,3), labels=c("A", "B", "C"))
> >   sec99[c(2,5)] <- NA
> > sec00 <- factor(c(4,1,1,2,4,2), labels=c("A", "B", "D"))
> >    sec00[c(2,4)] <- NA
>
>    # because
> > sec1 <- ifelse(!is.na(sec99), sec99,
> >        ifelse(!is.na(sec00), sec00, NA ))
>
>    # gives us
> > sec1; class(sec1)
> [1]  1 NA  3  2  3  3
> [1] "integer"
>
> now it's hard to tell where each "3" in sec1 means "C" or "D". What I
> actually wanted was
>
> > sec1; class(sec1)
> [1]  A <NA>  C  B  D  C
> [1] "factor"
>
> Any suggestions on how to do it in a simple way will be welcome.
> Thanks,
> Dimitri
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bgreen at dyson.brisnet.org.au  Sun Nov 27 07:18:55 2005
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Sun, 27 Nov 2005 16:18:55 +1000
Subject: [R] obtaining p values & CI for a non-parametric correlation matrix
Message-ID: <5.1.0.14.0.20051127161310.02209b38@pop3.brisnet.org.au>

Hello,

I tried to obtain a nonparametric correlation matrix with the following syntax:

by(mydat, mydat$GROUP, function(subset) { matrix <- cbind(subset$WK1FREQ, 
subset$WK2FREQ, subset$WK3FREQ, subset$WK4FREQ, subset$BESTDAY) cor(matrix, 
method="kendall",use="pairwise.complete.obs") })
Error: syntax error

In addition to the matrix, I was hoping to obtain p-value and CI (rounded 
to 3 decimal points).

Any suggestions are much appreciated,

Bob Green



From phgrosjean at sciviews.org  Sun Nov 27 07:51:55 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sun, 27 Nov 2005 07:51:55 +0100
Subject: [R] finding peaks in a simple dataset with R
In-Reply-To: <4012629d428b1d0c5f241be84df72374@gmail.com>
References: <17284.28690.71104.813153@stat.math.ethz.ch>	<20051123143323.GB6368@iwr.uni-heidelberg.de>	<17284.38020.14279.158148@stat.math.ethz.ch>	<971536df0511230929q74c48caax3892feec51fdcafe@mail.gmail.com>	<971536df0511231351k52e1cd89o8c790b6ad6a1d1c5@mail.gmail.com>	<17285.55052.354968.489470@stat.math.ethz.ch>	<17287.22446.567343.472410@stat.math.ethz.ch>
	<4012629d428b1d0c5f241be84df72374@gmail.com>
Message-ID: <4389578B.4080805@sciviews.org>

Hello,

This is interesting! Note that you have the turnpoints() function in 
pastecs library. The function calculates all peaks and pits in a series. 
It also calculates information of each turning point (peak or pit), 
according to Kendall's information theory. It is then possible to filter 
these peaks or pits at a certain level of significance. The later 
function is not implemented in the pastecs package, but it can be done 
easily (that one retreives ony peaks):

Peaks <- function(x, level = 0.05) {
	if (!inherits(x, "turnpoints"))
		stop("x must be a 'turnpoints' object!")
	# Extract position and probability
	tp.pos <- x$tppos
	tp.proba <- x$proba
	# We have both peaks and pits. Keep only peaks
	keep <- 1:(x$nturns / 2) * 2
	if (x$firstispeak) keep <- keep - 1
	tp.pos <- tp.pos[keep]
	tp.proba <- tp.proba[keep]
	# Keep only peaks whose probability is lower than level
	return(tp.pos[tp.proba < level])
}

# Now, your examples using turnpoints()
library(pastecs)

set.seed(7)
y <- rpois(100, lambda = 7)
tp <- turnpoints(y)
summary(tp)
# Take all peaks
p.all <- Peaks(tp, level = 1)
plot(y, type="o", cex = 1/4, main = "y and all peaks")
points(seq(y)[p.all], y[p.all], col = 2, cex = 1.5)

# Take only most significative ones
p.50 <- Peaks(tp, level = 0.5)
points(seq(y)[p.50], y[p.50], col = 3, cex = 2)
mtext("peaks at level = 50%", col = 3)
# Note that the result is very different than yours. Here, it is not
# the elevation of the peak that is a criterion, but whether you have
# several observations lower than the peak around it.

set.seed(2)
x <- round(rnorm(500), 2)
y <- cumsum(x)
plot(y, type="o", cex = 1/4)
p.1 <- Peaks(turnpoints(y), level = 0.01)
points(seq(y)[p.1], y[p.1], col = 3, cex = 2)
mtext("peaks al level = 1%", col=3)


You could read more about Kendall's information theory in the pastecs 
manual located in /library/pastecs/doc/pastecs.pdf (pp 106-110, in 
French) and cited references (quite old). Note that you do not 
necessarily get the highest peaks in the series, because the criterion 
here is whether you have several lower observations around a given peak 
in the series, or not... not the absolute height of the peaks in a 
moving window, as you do. Kendall's test makes sense for a time series 
where you can have white noise on top of your signal: some of the 
highest peaks my simply be positive random variations on top of values 
that are not peaks in the general trend.

Now, if you want to retrieve the peaks the way you do from the 
turnpoints object, it is relatively straightforward to do so from the 
turnpoints object because *all* peaks and pits are returned in it (just 
look whether each peak is the largest one in a given window.

A quick check for speed shows that your function is slightly faster than 
turnpoints, but this is not considering that turnpoints also calculates 
probability and quantity of information related to all peaks and pits. 
On my machine (PIV 3Ghz), the normal distribution example with 1.000.000 
data points took 32 seconds with your function and 44 seconds with 
turnpoints... not a major difference, isn't it?

Best,

Philippe Grosjean

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Dylan Beaudette wrote:
> On Nov 25, 2005, at 10:27 AM, Martin Maechler wrote:
> 
> 
>>Let me try to summarize my view on this:
>>
>>- I still it would make sense to have a *simple* peaks() function
>>  in R which provides the same (or more) functionality as the
>>  corresponding S-plus one.From
>>  For a proper data analysis situation, I think one would have to
>>  do something more sophisticated, based on a model (with a random
>>  component), such as nonparametric regression, time-series,....
>>  Hence peaks() should be kept as simple as reasonable.
>>
>>- Of course I know that  which() or %in% can be used to deal
>>  with logicals containing NAs {As a matter of fact, I've had
>>  my fingers in both implementations for R!}.
>>  Still, the main use of logical vectors in S often is for
>>  situations where NAs only appear because of missing data:
>>
>>  Indexing ([]), all(), any(), sum()  are all very nice and
>>  useful for logical vectors particularly when there are no NAs.
>>
>>- I agree that a different more flexible function returning
>>  values from {-1,0,1} would be desirable, "for symmetry reasons".
>>  ===> added a peaksign() function
>>
>>Here's code that implements the above {and other concerns
>>mentioned in this thread}, including some ``consistency
>>checking'' :
>>
>>peaks <- function(series, span = 3, do.pad = TRUE) {
>>    if((span <- as.integer(span)) %% 2 != 1) stop("'span' must be odd")
>>    s1 <- 1:1 + (s <- span %/% 2)
>>    if(span == 1) return(rep.int(TRUE, length(series)))
>>    z <- embed(series, span)
>>    v <- apply(z[,s1] > z[, -s1, drop=FALSE], 1, all)
>>    if(do.pad) {
>>        pad <- rep.int(FALSE, s)
>>        c(pad, v, pad)
>>    } else v
>>}
>>
>>peaksign <- function(series, span = 3, do.pad = TRUE)
>>{
>>    ## Purpose: return (-1 / 0 / 1) if series[i] is ( trough / 
>>"normal" / peak )
>>    ## 
>>----------------------------------------------------------------------
>>    ## Author: Martin Maechler, Date: 25 Nov 2005
>>
>>    if((span <- as.integer(span)) %% 2 != 1 || span == 1)
>>        stop("'span' must be odd and >= 3")
>>    s1 <- 1:1 + (s <- span %/% 2)
>>    z <- embed(series, span)
>>    d <- z[,s1] - z[, -s1, drop=FALSE]
>>    ans <- rep.int(0:0, nrow(d))
>>    ans[apply(d > 0, 1, all)] <- as.integer(1)
>>    ans[apply(d < 0, 1, all)] <- as.integer(-1)
>>    if(do.pad) {
>>        pad <- rep.int(0:0, s)
>>        c(pad, ans, pad)
>>    } else ans
>>}
>>
>>
>>check.pks <- function(y, span = 3)
>>    stopifnot(identical(peaks( y, span), peaksign(y, span) ==  1),
>>              identical(peaks(-y, span), peaksign(y, span) == -1))
>>
>>for(y in list(1:10, rep(1,10), c(11,2,2,3,4,4,6,6,6))) {
>>    for(sp in c(3,5,7))
>>        check.pks(y, span = sp)
>>    stopifnot(peaksign(y) == 0)
>>}
>>
>>y <- c(1,4,1,1,6,1,5,1,1) ; (ii <- which(peaks(y))); y[ii]
>>##- [1] 2 5 7
>>##- [1] 4 6 5
>>check.pks(y)
>>
>>set.seed(7)
>>y <- rpois(100, lambda = 7)
>>check.pks(y)
>>py <- peaks(y)
>>plot(y, type="o", cex = 1/4, main = "y and peaks(y,3)")
>>points(seq(y)[py], y[py], col = 2, cex = 1.5)
>>
>>p7 <- peaks(y,7)
>>points(seq(y)[p7], y[p7], col = 3, cex = 2)
>>mtext("peaks(y,7)", col=3)
>>
>>set.seed(2)
>>x <- round(rnorm(500), 2)
>>y <- cumsum(x)
>>check.pks(y)
>>
>>plot(y, type="o", cex = 1/4)
>>p15 <- peaks(y,15)
>>points(seq(y)[p15], y[p15], col = 3, cex = 2)
>>mtext("peaks(y,15)", col=3)
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> Wow! This was the exact sort of simple peak finding algorithm I was 
> looking for. Would it be ok for me to post this to our dept. webpage so 
> that others may use it?
> 
> Cheers,
> 
> --
> Dylan Beaudette
> Soils and Biogeochemistry Graduate Group
> University of California at Davis
> 530.754.7341
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From j.logsdon at quantex-research.com  Sun Nov 27 11:04:41 2005
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Sun, 27 Nov 2005 10:04:41 +0000 (GMT)
Subject: [R] gsub syntax
Message-ID: <Pine.LNX.4.10.10511270937580.32264-100000@quantex-research.co.uk>

Hello

I know that R's string functions are not as extensive as those of Unix but
I need to do some text handling totally within an R environment because
the target is a Windows system which will not have the corresponding shell
utilities, sed, awk etc.

Can anyone explain the following gsub phenomenon to me:

> dates<-c("73","74","02","1973","1974","2002")

I want to take just the last two digits where it is a 4-digit year and
both digits when it is a 2-digit year.  I should be able to use substr but
measurement from the string end (with a negative counter or something) is
not implemented:

> substr(dates,3,4)
[1] ""   ""   ""   "73" "74" "02"
> substr(dates,-2,4)
[1] "73"   "74"   "02"   "1973" "1974" "2002"
> substr(dates,4,-2)
[1] "" "" "" "" "" ""

So I tried gsub:

> gsub("[19|20]([0-9][0-9])","\\1",dates)
[1] "73"  "74"  "02"  "973" "974" "002"

As I understand it (and comparing with sed), the \\1 should take the first
bracketed string but clearly this doesn't work.  If I try what should also
work:

> gsub("[19|20]([0-9])([0-9])","\\1\\2",dates)
[1] "73"  "74"  "02"  "973" "974" "002"

On the other hand the following does work:

> gsub("[19|20]([0-9])([0-9])","\\2",dates) 
[1] "73" "74" "02" "73" "74" "02"

So it appears that the substitution takes one character extra to the left
but the following indicates that the lower limit of the selected range is
also at fault:

> s<-c("1","12","123","1234","12345","123456")
> gsub("[12]([4-6]*)","",s)
[1] ""     ""     "3"    "34"   "345"  "3456"

Probably more elegant examples could be constructed that could home in on
the issue.

The version is R 2.0.1 on Linux so perhaps it is a little old now.

Questions:

1) Am I misunderstanding the gsub use?

2) Was it a bug that has since been corrected?

3) Is it still a bug in the latest version?

TIA

JOhn

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com



From dimitris.rizopoulos at med.kuleuven.be  Sun Nov 27 11:20:34 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Sun, 27 Nov 2005 11:20:34 +0100
Subject: [R] gsub syntax
References: <Pine.LNX.4.10.10511270937580.32264-100000@quantex-research.co.uk>
Message-ID: <006501c5f33c$33d52080$0540210a@www.domain>

you could use something like:

dates <- c("73", "74", "02", "1973", "1974", "2002")
###############
nd <- nchar(dates)
substr(dates, ifelse(nd == 2, 1, 3), nd)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "John Logsdon" <j.logsdon at quantex-research.com>
To: <r-help at stat.math.ethz.ch>
Sent: Sunday, November 27, 2005 11:04 AM
Subject: [R] gsub syntax


> Hello
>
> I know that R's string functions are not as extensive as those of 
> Unix but
> I need to do some text handling totally within an R environment 
> because
> the target is a Windows system which will not have the corresponding 
> shell
> utilities, sed, awk etc.
>
> Can anyone explain the following gsub phenomenon to me:
>
>> dates<-c("73","74","02","1973","1974","2002")
>
> I want to take just the last two digits where it is a 4-digit year 
> and
> both digits when it is a 2-digit year.  I should be able to use 
> substr but
> measurement from the string end (with a negative counter or 
> something) is
> not implemented:
>
>> substr(dates,3,4)
> [1] ""   ""   ""   "73" "74" "02"
>> substr(dates,-2,4)
> [1] "73"   "74"   "02"   "1973" "1974" "2002"
>> substr(dates,4,-2)
> [1] "" "" "" "" "" ""
>
> So I tried gsub:
>
>> gsub("[19|20]([0-9][0-9])","\\1",dates)
> [1] "73"  "74"  "02"  "973" "974" "002"
>
> As I understand it (and comparing with sed), the \\1 should take the 
> first
> bracketed string but clearly this doesn't work.  If I try what 
> should also
> work:
>
>> gsub("[19|20]([0-9])([0-9])","\\1\\2",dates)
> [1] "73"  "74"  "02"  "973" "974" "002"
>
> On the other hand the following does work:
>
>> gsub("[19|20]([0-9])([0-9])","\\2",dates)
> [1] "73" "74" "02" "73" "74" "02"
>
> So it appears that the substitution takes one character extra to the 
> left
> but the following indicates that the lower limit of the selected 
> range is
> also at fault:
>
>> s<-c("1","12","123","1234","12345","123456")
>> gsub("[12]([4-6]*)","",s)
> [1] ""     ""     "3"    "34"   "345"  "3456"
>
> Probably more elegant examples could be constructed that could home 
> in on
> the issue.
>
> The version is R 2.0.1 on Linux so perhaps it is a little old now.
>
> Questions:
>
> 1) Am I misunderstanding the gsub use?
>
> 2) Was it a bug that has since been corrected?
>
> 3) Is it still a bug in the latest version?
>
> TIA
>
> JOhn
>
> John Logsdon                               "Try to make things as 
> simple
> Quantex Research Ltd, Manchester UK         as possible but not 
> simpler"
> j.logsdon at quantex-research.com 
> a.einstein at relativity.org
> +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From sundar.dorai-raj at pdf.com  Sun Nov 27 11:37:41 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sun, 27 Nov 2005 04:37:41 -0600
Subject: [R] gsub syntax
In-Reply-To: <Pine.LNX.4.10.10511270937580.32264-100000@quantex-research.co.uk>
References: <Pine.LNX.4.10.10511270937580.32264-100000@quantex-research.co.uk>
Message-ID: <43898C75.8090202@pdf.com>



John Logsdon wrote:
> Hello
> 
> I know that R's string functions are not as extensive as those of Unix but
> I need to do some text handling totally within an R environment because
> the target is a Windows system which will not have the corresponding shell
> utilities, sed, awk etc.
> 
> Can anyone explain the following gsub phenomenon to me:
> 
> 
>>dates<-c("73","74","02","1973","1974","2002")
> 
> 
> I want to take just the last two digits where it is a 4-digit year and
> both digits when it is a 2-digit year.  I should be able to use substr but
> measurement from the string end (with a negative counter or something) is
> not implemented:
> 
> 
>>substr(dates,3,4)
> 
> [1] ""   ""   ""   "73" "74" "02"
> 
>>substr(dates,-2,4)
> 
> [1] "73"   "74"   "02"   "1973" "1974" "2002"
> 
>>substr(dates,4,-2)
> 
> [1] "" "" "" "" "" ""
> 
> So I tried gsub:
> 
> 
>>gsub("[19|20]([0-9][0-9])","\\1",dates)
> 
> [1] "73"  "74"  "02"  "973" "974" "002"
> 
> As I understand it (and comparing with sed), the \\1 should take the first
> bracketed string but clearly this doesn't work.  If I try what should also
> work:
> 
> 
>>gsub("[19|20]([0-9])([0-9])","\\1\\2",dates)
> 
> [1] "73"  "74"  "02"  "973" "974" "002"
> 
> On the other hand the following does work:
> 
> 
>>gsub("[19|20]([0-9])([0-9])","\\2",dates) 
> 
> [1] "73" "74" "02" "73" "74" "02"
> 
> So it appears that the substitution takes one character extra to the left
> but the following indicates that the lower limit of the selected range is
> also at fault:
> 
> 
>>s<-c("1","12","123","1234","12345","123456")
>>gsub("[12]([4-6]*)","",s)
> 
> [1] ""     ""     "3"    "34"   "345"  "3456"
> 
> Probably more elegant examples could be constructed that could home in on
> the issue.
> 
> The version is R 2.0.1 on Linux so perhaps it is a little old now.
> 
> Questions:
> 
> 1) Am I misunderstanding the gsub use?
> 
> 2) Was it a bug that has since been corrected?
> 
> 3) Is it still a bug in the latest version?
> 
> TIA
> 
> JOhn
>

Hi, John,

I cannot comment on your questions since I'm no regexpr guru. However, 
it seems to me you can do the following instead:

gsub(".*([0-9][0-9])", "\\1", dates)

This works fine on Linux & Windows, R-2.2.0.

HTH,

--sundar



From alanzhao at gmail.com  Sun Nov 27 11:39:36 2005
From: alanzhao at gmail.com (Zheng Zhao)
Date: Sun, 27 Nov 2005 03:39:36 -0700
Subject: [R] the output of coxph
Message-ID: <43898CE8.3070802@gmail.com>

Dear All:

I have some questions about the output of coxph.

Below is the input and output:

----------------------------------------
 > coxph(formula = Surv(futime, fustat) ~ age + rx + ecog.ps, data =
+  ovarian, x = TRUE)

Call:
coxph(formula = Surv(futime, fustat) ~ age + rx + ecog.ps, data =
    ovarian, x = TRUE)


           coef exp(coef) se(coef)     z      p
age      0.147     1.158   0.0463  3.17 0.0015
rx      -0.815     0.443   0.6342 -1.28 0.2000
ecog.ps  0.103     1.109   0.6064  0.17 0.8600

Likelihood ratio test=15.9  on 3 df, p=0.00118  n= 26
---------------------------------------
Question One:
As I know, the p-value of "age" is the significance level. However what 
is the exact meaning of the parameter, and how do we calculate the 
parameter? If the sample size is small (20~40), is this estimation still 
reliable?

Question Two:
the p-value in the last line (Likelihood ratio test=15.9 on 3 df, 
p=0.00118) is asymptotically equivalent tests of the omnibus null 
hypothesis that all of the Î²â€™s are zero, according to John Fox's "Cox 
Proportional-Hazards Regression for Survival Data" 
(http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf)
Can anybody explain that why this true? (As I know, the p-value is 
obtained by 1-pchisq(2*log Likelihood ratio), and this is because 
2*log(likelihood ratio) is approximately chi-square for nested models.)

Thank you very much.

Sincerely,
Alan
2005-11-27



From alanzhao at gmail.com  Sun Nov 27 11:40:10 2005
From: alanzhao at gmail.com (Zheng Zhao)
Date: Sun, 27 Nov 2005 03:40:10 -0700
Subject: [R] the output of coxph
Message-ID: <43898D0A.8070001@gmail.com>

Dear All:

I have some questions about the output of coxph.

Below is the input and output:

----------------------------------------
 > coxph(formula = Surv(futime, fustat) ~ age + rx + ecog.ps, data =
+  ovarian, x = TRUE)

Call:
coxph(formula = Surv(futime, fustat) ~ age + rx + ecog.ps, data =
    ovarian, x = TRUE)


           coef exp(coef) se(coef)     z      p
age      0.147     1.158   0.0463  3.17 0.0015
rx      -0.815     0.443   0.6342 -1.28 0.2000
ecog.ps  0.103     1.109   0.6064  0.17 0.8600

Likelihood ratio test=15.9  on 3 df, p=0.00118  n= 26
---------------------------------------
Question One:
As I know, the p-value of "age" is the significance level. However what 
is the exact meaning of the parameter, and how do we calculate the 
parameter? If the sample size is small (20~40), is this estimation still 
reliable?

Question Two:
the p-value in the last line (Likelihood ratio test=15.9 on 3 df, 
p=0.00118) is asymptotically equivalent tests of the omnibus null 
hypothesis that all of the Î²â€™s are zero, according to John Fox's "Cox 
Proportional-Hazards Regression for Survival Data" 
(http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf)
Can anybody explain that why this true? (As I know, the p-value is 
obtained by 1-pchisq(2*log Likelihood ratio), and this is because 
2*log(likelihood ratio) is approximately chi-square for nested models.)

Thank you very much.

Sincerely,
Alan
2005-11-27



From ligges at statistik.uni-dortmund.de  Sun Nov 27 12:01:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 27 Nov 2005 12:01:59 +0100
Subject: [R] Using an editor with R
In-Reply-To: <4388EB10.6050105@stats.uwo.ca>
References: <E1Eg7zV-0000FT-3o@smtpauth02.mail.atl.earthlink.net>
	<4388EB10.6050105@stats.uwo.ca>
Message-ID: <43899227.9060105@statistik.uni-dortmund.de>

Duncan Murdoch wrote:

> On 11/26/2005 4:53 PM, Walter R. Paczkowski wrote:
> 
>> Hello,
>> 
>> I changed the setting in options$editor to allow me to use my
>> favorite editor.  In R 2.1.1 on Windows XP, I entered at the
>> command line:
>> 
>> options(editor="c:\\program files\\winedit\\winedit.exe")
>> 
>> When I edited a function, say test, using fix(test), the editor
>> opened perfectly.  But, when I saved the file and closed the
>> editor, the R gui screen was white, blank, and completely
>> unresponsive.  The only thing I could do was close R by clicking on
>> the "X" in the upper right corner of the window.  How can I use my
>> editor but be able to continue using R after I close the editor?
>> What extra setting am I missing?


1. Probably you do not want to use fix() (or only under very rare 
circumstances), but use the code in your editor and source the file into 
R, so you do not need to close the editor.

2. Are you talking about "winedit" or the editor "WinEdt" (just one "i" 
in it ...).?


> 
> This sounds like you didn't really close your editor.  R isn't smart
>  enough to know that the editor closed a file, it can only see when
> the process finishes.
> 
> I'd recommend using the RWinEdt package instead for a different way
> to integrate winedit with R.

Well, at least to integrate "WinEdt". ;-)

Best,
Uwe Ligges



> Duncan Murdoch
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun Nov 27 12:09:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 27 Nov 2005 12:09:25 +0100
Subject: [R] r question
In-Reply-To: <20051127042913.46995.qmail@web52508.mail.yahoo.com>
References: <20051127042913.46995.qmail@web52508.mail.yahoo.com>
Message-ID: <438993E5.7070907@statistik.uni-dortmund.de>

Please,

1. read the posting guide
2. use a sensible subject line
3. this is NOT an "r question"
4. ask your teacher to explain your homeworks, but not this list

Uwe Ligges



yuying shi wrote:

> If there are two random variable X1 and X2 which have
> a bivariate normal distribution with mean vector (10,
> 10)and variance covariance matrix 
> [2    1.95
>  1.95    3]
> 
> How to calculate the mean and variance of the function
> Y=X1/X2? 
> 
> Thanks a lot!
> xingyu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun Nov 27 12:12:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 27 Nov 2005 12:12:55 +0100
Subject: [R] coherency-Time Series
In-Reply-To: <6.0.2.0.1.20051127020530.01b45278@pop.aueb.gr>
References: <6.0.2.0.1.20051127020530.01b45278@pop.aueb.gr>
Message-ID: <438994B7.1080205@statistik.uni-dortmund.de>

svenier at aueb.gr wrote:

> hello!
> My name is Stefanos, from Athens.
> I'm a new user of R and I'm studying multivariate time series. I can't find 
> in the help menu how to calculate the cross spectrum and the coherency of 2 
> Time Series. Would you like to help me?

See ?spectrum and ?cor

Uwe Ligges


> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kjetilbrinchmannhalvorsen at gmail.com  Sun Nov 27 12:13:33 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 27 Nov 2005 07:13:33 -0400
Subject: [R] coherency-Time Series
In-Reply-To: <6.0.2.0.1.20051127020530.01b45278@pop.aueb.gr>
References: <6.0.2.0.1.20051127020530.01b45278@pop.aueb.gr>
Message-ID: <438994DD.7050000@gmail.com>

svenier at aueb.gr wrote:
> hello!
> My name is Stefanos, from Athens.
> I'm a new user of R and I'm studying multivariate time series. I can't find 
> in the help menu how to calculate the cross spectrum and the coherency of 2 
> Time Series. Would you like to help me?
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
See
?spectrum,and especially component $coh  of output.

Kjetil



From ligges at statistik.uni-dortmund.de  Sun Nov 27 12:15:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 27 Nov 2005 12:15:50 +0100
Subject: [R] Newton iteration questions
In-Reply-To: <20051126211509.40438.qmail@web52504.mail.yahoo.com>
References: <20051126211509.40438.qmail@web52504.mail.yahoo.com>
Message-ID: <43899566.2010703@statistik.uni-dortmund.de>

Yet another time we shall solve your homeworks?
Please stop sending your homework questions to R-help!

Uwe Ligges


yuying shi wrote:

> Dear Sir/Madam,
>      If I have a sample of observations that come from
> an extreme value distribution, the density function
> for the extreme value distribution is: 
> 
> f(x)=(1/b)exp[-(x-a)/b]exp{-exp[-(x-a)/b]}, b>0, x can
> be any value,
>  
> my question is how to implement the Newton iteration
> and estimate the parameters for this distribution and
> the accuracy of epsilon=0.0001?
> 
>  The n= 100 observations are given as follows:
> x<- c(8.8, 9.4, 8.7, 9.3, 9.6, 9.4, 9.1, 9.4, 8.4,
> 6.8, 8.4,
> ?.2, 9.4, 7.4, 8.7, 9.4, 9.2, 9.3, 8.0, 8.5, 8.7, 9.7,
> 9.8,
> ?.5, 7.1, 7.8, 9.0, 8.6, 9.4, 6.9, 9.1, 9.9, 7.3, 8.5,
> 8.8,
> ?.4, 9.0, 8.6, 8.5, 9.2, 9.7, 9.2, 9.2, 8.4, 8.7, 9.6,
> 9.2,
> ?.8, 8.5, 9.0, 8.9, 9.6, 8.0, 9.7, 8.4, 7.5, 9.1, 9.2,
> 8.9,
> ?.2, 9.8, 9.4, 8.5, 9.3, 9.8, 9.6, 9.7, 8.9, 9.7, 8.7,
> 8.6,
> ?.7, 8.6, 9.7, 7.7, 8.6, 9.7, 8.5, 9.4, 9.4, 9.7, 8.1,
> 9.5,
> ?.3, 8.0, 9.8, 8.9, 9.5, 9.0, 8.7, 9.1, 8.5, 8.7, 8.4,
> 9.3,
> ?.5, 8.9, 9.3, 9.0, 9.9)?
> 
> thanks in advance!
> xingyu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun Nov 27 12:17:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 27 Nov 2005 12:17:49 +0100
Subject: [R] rescale x-axis
In-Reply-To: <ca33a9890511251025m6c4d1f49u@mail.gmail.com>
References: <ca33a9890511251025m6c4d1f49u@mail.gmail.com>
Message-ID: <438995DD.6080907@statistik.uni-dortmund.de>

singyee ling wrote:

> Dear all,
> 
> I am trying to draw a survival curve with probability of surviving as
> the y-axis and days (0- 500 days )as the x-axis. however, i do not
> want the days to be equally spaced on the x-axis as i am more
> interested in looking at the behaviour of the curve in the first 50
> days. I am reluctant to use  xlim=c(0,1000) as i want to see the whole
> picture. Hence, what I am interested in is a scale in which the days
> are not equally spaced. By that , I mean the length of the interval
> between the days get smaller and smaller, which gives greater emphasis
> to the intial period. (i.e the length of the interval betwen 0-1 days
> is longer then the interval between 1-2 days and so on) .Hope what i
> say above make sense. any advise?


What about applying a logarithm such as in

plot(1:10, log="x")

Uwe Ligges


> thanks!
> 
> sing yee
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Sun Nov 27 13:26:53 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Nov 2005 13:26:53 +0100
Subject: [R] creating a factor from other factors and ifelse
In-Reply-To: <1133056800.43891320910ce@webmail.ipea.gov.br>
References: <1133056800.43891320910ce@webmail.ipea.gov.br>
Message-ID: <x2r792mg36.fsf@turmalin.kubism.ku.dk>

dimitrijoe at ipea.gov.br writes:

> Hi,
> 
> Given 

<nevermind...>

Five identical messages in five minutes and five seconds! Perhaps a
little more patience next time?


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Sun Nov 27 13:31:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Nov 2005 13:31:41 +0100
Subject: [R] r question
In-Reply-To: <438993E5.7070907@statistik.uni-dortmund.de>
References: <20051127042913.46995.qmail@web52508.mail.yahoo.com>
	<438993E5.7070907@statistik.uni-dortmund.de>
Message-ID: <x2mzjqmfv6.fsf@turmalin.kubism.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Please,
> 
> 1. read the posting guide
> 2. use a sensible subject line
> 3. this is NOT an "r question"
> 4. ask your teacher to explain your homeworks, but not this list
> 
> Uwe Ligges

And, btw, neither the mean nor the variance exists, so the question is
incomplete, and any answer approximate.
 
> 
> 
> yuying shi wrote:
> 
> > If there are two random variable X1 and X2 which have
> > a bivariate normal distribution with mean vector (10,
> > 10)and variance covariance matrix 
> > [2    1.95
> >  1.95    3]
> > 
> > How to calculate the mean and variance of the function
> > Y=X1/X2? 
> > 
> > Thanks a lot!
> > xingyu
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From HDoran at air.org  Sun Nov 27 14:52:03 2005
From: HDoran at air.org (Doran, Harold)
Date: Sun, 27 Nov 2005 08:52:03 -0500
Subject: [R] IRT Package
Message-ID: <F5ED48890E2ACB468D0F3A64989D335AC990F6@dc1ex3.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051127/60b389e3/attachment.pl

From ggrothendieck at gmail.com  Sun Nov 27 15:50:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 27 Nov 2005 09:50:30 -0500
Subject: [R] gsub syntax
In-Reply-To: <Pine.LNX.4.10.10511270937580.32264-100000@quantex-research.co.uk>
References: <Pine.LNX.4.10.10511270937580.32264-100000@quantex-research.co.uk>
Message-ID: <971536df0511270650hd826f61j3a5e8690624e1813@mail.gmail.com>

On 11/27/05, John Logsdon <j.logsdon at quantex-research.com> wrote:
> Hello
>
> I know that R's string functions are not as extensive as those of Unix but

I don't think this statement is true although I have seen it repeated.

> I need to do some text handling totally within an R environment because
> the target is a Windows system which will not have the corresponding shell
> utilities, sed, awk etc.

Free versions of these utilities are available for Windows although they
don't come with Windows.  e.g. Google for gawk.

>
> Can anyone explain the following gsub phenomenon to me:
>
> > dates<-c("73","74","02","1973","1974","2002")
>
> I want to take just the last two digits where it is a 4-digit year and
> both digits when it is a 2-digit year.  I should be able to use substr but
> measurement from the string end (with a negative counter or something) is
> not implemented:
>
> > substr(dates,3,4)
> [1] ""   ""   ""   "73" "74" "02"
> > substr(dates,-2,4)
> [1] "73"   "74"   "02"   "1973" "1974" "2002"
> > substr(dates,4,-2)
> [1] "" "" "" "" "" ""
>
> So I tried gsub:
>
> > gsub("[19|20]([0-9][0-9])","\\1",dates)
> [1] "73"  "74"  "02"  "973" "974" "002"
>
> As I understand it (and comparing with sed), the \\1 should take the first
> bracketed string but clearly this doesn't work.  If I try what should also
> work:
>
> > gsub("[19|20]([0-9])([0-9])","\\1\\2",dates)
> [1] "73"  "74"  "02"  "973" "974" "002"
>
> On the other hand the following does work:
>
> > gsub("[19|20]([0-9])([0-9])","\\2",dates)
> [1] "73" "74" "02" "73" "74" "02"
>
> So it appears that the substitution takes one character extra to the left
> but the following indicates that the lower limit of the selected range is
> also at fault:
>
> > s<-c("1","12","123","1234","12345","123456")
> > gsub("[12]([4-6]*)","",s)
> [1] ""     ""     "3"    "34"   "345"  "3456"
>
> Probably more elegant examples could be constructed that could home in on
> the issue.
>
> The version is R 2.0.1 on Linux so perhaps it is a little old now.
>
> Questions:
>
> 1) Am I misunderstanding the gsub use?
>
> 2) Was it a bug that has since been corrected?
>
> 3) Is it still a bug in the latest version?
>

It works the same on my system which is 2.2.0 Windows patched
(2005-10-24). At first I too thought it was a bug but I noticed it
works the same in perl so now I am not sure. The following perl
program under Windows using perl 5.8.6 on Windows
gives 002 as the answer as the answer too:

   $_ = "2002";
   s/[19|20]([0-9])([0-9])/\1\2/g;
   print;

In any any case, it could be done like this:

   sub(".*(..)$", "\\1", dates)

or

   substring(dates, nchar(dates)-1)

or the following which appends -01-01 to the year, converts it to Date
class, implicitly converts it back to character and then extracts
the 3rd to 4th character of the result:

   substring(as.Date(sprintf("%s-01-01", dates)), 3, 4)

or



From ripley at stats.ox.ac.uk  Sun Nov 27 17:41:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 27 Nov 2005 16:41:40 +0000 (GMT)
Subject: [R] gsub syntax
In-Reply-To: <Pine.LNX.4.10.10511270937580.32264-100000@quantex-research.co.uk>
References: <Pine.LNX.4.10.10511270937580.32264-100000@quantex-research.co.uk>
Message-ID: <Pine.LNX.4.61.0511271121560.14253@gannet.stats>

R is blameless here: it works as documented and in the same way as 
POSIX tools.  It agrees with 'sed' using the same syntax (modulo the 
shell-specific quoting rules) e.g. in csh

    % echo 1973 | sed 's/[19|20]\([0-9][0-9]\)/\1/g'
    973
    % echo 1973 | sed 's/\([19|20]\)\([0-9][0-9]\)/-\1-\2-/g'
    -1-97-3
    % echo "73 74 02 1973 1974 2002" | sed 's/[19|20]\([0-9][0-9]\)/\1/g'
    73 74 02 973 974 002

so what happened when you were 'comparing with sed'?

"[19|20]" is a character class (containing five characters) matching one 
character, not a match for two characters as you seem to imagine.  It does 
not mean the same as "19|20", which is what you seem to have intended (and 
you seem only to want to do the substitution once on each string, so why 
use gsub?):

> sub("19|20([0-9][0-9])", "\\1", dates)
[1] "73" "74" "02" "73" "74" "02"

A more direct way which would work e.g. for 1837 would be

sub(".*([0-9]{2}$)", "\\1", dates)

or even better (locale-independent)

sub(".*([[:digit:]]{2}$)", "\\1", dates)

Current versions of R have a help page ?regexp explaining what regexps 
are.  Even 2.0.1 did, although you were asked to update *before* posting 
(see the posting guide).  It was unambiguous:

    A _character class_ is a list of characters enclosed by '[' and
    ']' matches any single character in that list ...
                    ^^^^^^
    ...  Note that alternation does not work inside character classes,
    where \code{|} has its literal meaning.


On Sun, 27 Nov 2005, John Logsdon wrote:

> Hello
>
> I know that R's string functions are not as extensive as those of Unix but
> I need to do some text handling totally within an R environment because
> the target is a Windows system which will not have the corresponding shell
> utilities, sed, awk etc.
> Can anyone explain the following gsub phenomenon to me:
>
>> dates<-c("73","74","02","1973","1974","2002")
>
> I want to take just the last two digits where it is a 4-digit year and
> both digits when it is a 2-digit year.  I should be able to use substr but
> measurement from the string end (with a negative counter or something) is
> not implemented:

Why 'should' it work in a different way to that documented?

>> substr(dates,3,4)
> [1] ""   ""   ""   "73" "74" "02"
>> substr(dates,-2,4)
> [1] "73"   "74"   "02"   "1973" "1974" "2002"
>> substr(dates,4,-2)
> [1] "" "" "" "" "" ""
>
> So I tried gsub:
>
>> gsub("[19|20]([0-9][0-9])","\\1",dates)
> [1] "73"  "74"  "02"  "973" "974" "002"
>
> As I understand it (and comparing with sed), the \\1 should take the first
> bracketed string but clearly this doesn't work.
> If I try what should also work:
>
>> gsub("[19|20]([0-9])([0-9])","\\1\\2",dates)
> [1] "73"  "74"  "02"  "973" "974" "002"

> On the other hand the following does work:
>
>> gsub("[19|20]([0-9])([0-9])","\\2",dates)
> [1] "73" "74" "02" "73" "74" "02"
>
> So it appears that the substitution takes one character extra to the left
> but the following indicates that the lower limit of the selected range is
> also at fault:
>> s<-c("1","12","123","1234","12345","123456")
>> gsub("[12]([4-6]*)","",s)
> [1] ""     ""     "3"    "34"   "345"  "3456"
>
> Probably more elegant examples could be constructed that could home in on
> the issue.
> The version is R 2.0.1 on Linux so perhaps it is a little old now.
>
> Questions:
>
> 1) Am I misunderstanding the gsub use?

Yes.

> 2) Was it a bug that has since been corrected?

Unfortunately the bug reported two years ago in

> library(fortunes); fortune("WTFM")

still seems extant.  See the posting guide for advice on how to correct 
it.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From 042045003 at fudan.edu.cn  Sun Nov 27 18:33:38 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Mon, 28 Nov 2005 01:33:38 +0800
Subject: [R] multilevel models and sample size
Message-ID: <0IQM00388IYVVY@mail.fudan.edu.cn>

It is not a pure  R question,but I hope some one can give me advices.

I want to use analysis my data with the multilevel model.The data has 2 levels---- the second level has 52 units and each second level unit has 19-23 units.I think the sample size is quite small,but just now I can't make the sample size much bigger.So I want to ask if I use the multilevel model to analysis the data set,will it be acceptable?  or  unacceptable because of the small sample size?

Thank you very much!

ronggui 

2005-11-28

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From visser_md at yahoo.com  Sun Nov 27 18:49:19 2005
From: visser_md at yahoo.com (Marco Visser)
Date: Sun, 27 Nov 2005 09:49:19 -0800 (PST)
Subject: [R] Counting the occurence of each unique "charecter string"
Message-ID: <20051127174920.48183.qmail@web51914.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051127/a4b27a2d/attachment.pl

From 042045003 at fudan.edu.cn  Sun Nov 27 19:02:56 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Mon, 28 Nov 2005 02:02:56 +0800
Subject: [R] Counting the occurence of each unique "charecter string"
Message-ID: <0IQM003LWKBPVY@mail.fudan.edu.cn>

use table() to get what you want.
see ?table

======= 2005-11-28 01:49:19 ÄúÔÚÀ´ÐÅÖÐÐ´µÀ£º=======

>LS,
>  
>  I would really like to know how to count the frequency/occurrence of  chachters inside a dataset. I am working with extreemly large datasets  of forest inventory data with a large variety of different species  inside it. 
>  Each row inside the dataframe represents one individual tree and the simplified dataframe looks something like this:
>  
>  num species dbh   
>  1        sp1           30
>  2        sp1          20
>  3        sp2          30
>  4        sp1          40
>  
>  I need to be able to count the number of individuals per species, so I  need a command that will return for each unique species its occurence  inside the dataframe; 
>  
>  [sp1]     3
>  [sp2]     1
>  
>  After a long search through help.search() and the web I found very  little and any alternative like exporting the dataset to another  program(excel) is not really an option because the dataset is far to  large.
>  
>  I am using R 2.2.0 in Windows and if anyone knows a solution please help!
>  
>  Many sincere thanks in advance,
>  
>  Marco 
>  
>  
>  
>		
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-11-28

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From ccleland at optonline.net  Sun Nov 27 19:06:28 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 27 Nov 2005 13:06:28 -0500
Subject: [R] Counting the occurence of each unique "charecter string"
In-Reply-To: <20051127174920.48183.qmail@web51914.mail.yahoo.com>
References: <20051127174920.48183.qmail@web51914.mail.yahoo.com>
Message-ID: <4389F5A4.7050600@optonline.net>

?table

table(mydata$species)

Marco Visser wrote:
> LS,
>   
>   I would really like to know how to count the frequency/occurrence of  chachters inside a dataset. I am working with extreemly large datasets  of forest inventory data with a large variety of different species  inside it. 
>   Each row inside the dataframe represents one individual tree and the simplified dataframe looks something like this:
>   
>   num species dbh   
>   1        sp1           30
>   2        sp1          20
>   3        sp2          30
>   4        sp1          40
>   
>   I need to be able to count the number of individuals per species, so I  need a command that will return for each unique species its occurence  inside the dataframe; 
>   
>   [sp1]     3
>   [sp2]     1
>   
>   After a long search through help.search() and the web I found very  little and any alternative like exporting the dataset to another  program(excel) is not really an option because the dataset is far to  large.
>   
>   I am using R 2.2.0 in Windows and if anyone knows a solution please help!
>   
>   Many sincere thanks in advance,
>   
>   Marco 
>   
>   
>   
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From gunter.berton at gene.com  Sun Nov 27 19:16:13 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Sun, 27 Nov 2005 10:16:13 -0800
Subject: [R] multilevel models and sample size
In-Reply-To: <0IQM00388IYVVY@mail.fudan.edu.cn>
Message-ID: <200511271816.jARIGCrG007077@hertz.gene.com>

"All models are wrong, but some are useful."  --George Box 

I do not understand what you mean by "acceptable, nor "levels" nor "units".
Specifying your model would help clarify things, I think. If by "levels" you
mean number of different values of a random factor, than 2 levels is
unlikely to tell you much useful about the variability of that factor. On
the other hand, 50 values might be. Depends on the model,the data, and the
scientific objectives, none of which you have stated clearly enough for me
to understand, anyway.

-- Bert

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ronggui
Sent: Sunday, November 27, 2005 9:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] multilevel models and sample size

It is not a pure  R question,but I hope some one can give me advices.

I want to use analysis my data with the multilevel model.The data has 2
levels---- the second level has 52 units and each second level unit has
19-23 units.I think the sample size is quite small,but just now I can't make
the sample size much bigger.So I want to ask if I use the multilevel model
to analysis the data set,will it be acceptable?  or  unacceptable because of
the small sample size?

Thank you very much!

ronggui 

2005-11-28

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From Ted.Harding at nessie.mcc.ac.uk  Sun Nov 27 19:27:12 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 27 Nov 2005 18:27:12 -0000 (GMT)
Subject: [R] Counting the occurence of each unique "charecter string"
In-Reply-To: <20051127174920.48183.qmail@web51914.mail.yahoo.com>
Message-ID: <XFMail.051127182712.Ted.Harding@nessie.mcc.ac.uk>

On 27-Nov-05 Marco Visser wrote:
> LS,
>   
>   I would really like to know how to count the frequency/occurrence of 
> chachters inside a dataset. I am working with extreemly large datasets 
> of forest inventory data with a large variety of different species 
> inside it. 
>   Each row inside the dataframe represents one individual tree and the
> simplified dataframe looks something like this:
>   
>   num species dbh   
>   1        sp1           30
>   2        sp1          20
>   3        sp2          30
>   4        sp1          40
>   
>   I need to be able to count the number of individuals per species, so
> I  need a command that will return for each unique species its
> occurence  inside the dataframe; 
>   
>   [sp1]     3
>   [sp2]     1

Does the following help? (Using an artificial example a bit more
complicated than yours). The dataframe "trees" consists of a list
of species names under "Species", and values of a numeric variable
under "X".


  > trees
              Species   X
  1     Larix decidua 203
  2  Pinus sylvestris 303
  3     Larix decidua 202
  4  Pinus sylvestris 301
  5       Picea abies 102
  6       Picea abies 103
  7  Pinus sylvestris 302
  8       Picea abies 101
  9     Larix decidua 201
  10      Picea abies 104
  11      Picea abies 105
  12 Pinus sylvestris 304


  > freqs<-as.data.frame(table(trees$Species))
  > colnames(freqs)<-c("Species","Counts")
  > freqs
             Species Counts
  1    Larix decidua      3
  2      Picea abies      5
  3 Pinus sylvestris      4


  > mean(freqs$Counts)
  [1] 4
  > sd(freqs$Counts)
  [1] 1


Just using table() would give you the same information, but
converting it to a dataframe makes that information more
readily accessible by familiar methods.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 27-Nov-05                                       Time: 18:27:10
------------------------------ XFMail ------------------------------



From kjetilbrinchmannhalvorsen at gmail.com  Sun Nov 27 19:27:47 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 27 Nov 2005 14:27:47 -0400
Subject: [R] multilevel models and sample size
In-Reply-To: <0IQM00388IYVVY@mail.fudan.edu.cn>
References: <0IQM00388IYVVY@mail.fudan.edu.cn>
Message-ID: <4389FAA3.3020408@gmail.com>

ronggui wrote:
> It is not a pure  R question,but I hope some one can give me advices.
> 
> I want to use analysis my data with the multilevel model.The data has 2 levels---- the second level has 52 units and each second level unit has 19-23 units.I think the sample size is quite small,but just now I can't make the sample size much bigger.So I want to ask if I use the multilevel model to analysis the data set,will it be acceptable?  or  unacceptable because of the small sample size?
> 

This kind of question I usually try to answer by
simulation, which is very easy in R.

Kjetil


> Thank you very much!
> 
> ronggui 
> 
> 2005-11-28
> 
> ------
> Deparment of Sociology
> Fudan University
> 
> My new mail addres is ronggui.huang at gmail.com
> Blog:http://sociology.yculblog.com
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sun Nov 27 19:38:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 27 Nov 2005 18:38:33 +0000 (GMT)
Subject: [R] multilevel models and sample size
In-Reply-To: <200511271816.jARIGCrG007077@hertz.gene.com>
References: <200511271816.jARIGCrG007077@hertz.gene.com>
Message-ID: <Pine.LNX.4.61.0511271834580.19594@gannet.stats>

On Sun, 27 Nov 2005, Berton Gunter wrote:

> "All models are wrong, but some are useful."  --George Box
>
> I do not understand what you mean by "acceptable, nor "levels" nor "units".
> Specifying your model would help clarify things, I think. If by "levels" you
> mean number of different values of a random factor, than 2 levels is
> unlikely to tell you much useful about the variability of that factor. On
> the other hand, 50 values might be. Depends on the model,the data, and the
> scientific objectives, none of which you have stated clearly enough for me
> to understand, anyway.

My guess is that he means this is a tested design with e.g. 52 classes
containing 19-23 pupils each.  (It always helps to state the real 
problem!)

If so, this is quite a large problem for multilevel models.  The classical 
nested designs for measurement errors typically have two replications at 
the lowest level - you get an idea of the variability from the many 
differences between matched pairs.  Of course the homogeneity assumptions 
have to be approximately true.

> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ronggui
> Sent: Sunday, November 27, 2005 9:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] multilevel models and sample size
>
> It is not a pure  R question,but I hope some one can give me advices.
>
> I want to use analysis my data with the multilevel model.The data has 2
> levels---- the second level has 52 units and each second level unit has
> 19-23 units.I think the sample size is quite small,but just now I can't make
> the sample size much bigger.So I want to ask if I use the multilevel model
> to analysis the data set,will it be acceptable?  or  unacceptable because of
> the small sample size?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.harrell at vanderbilt.edu  Sun Nov 27 21:16:30 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 27 Nov 2005 15:16:30 -0500
Subject: [R] obtaining a ROC curve
In-Reply-To: <20051125212312.53287.qmail@web35505.mail.mud.yahoo.com>
References: <20051125212312.53287.qmail@web35505.mail.mud.yahoo.com>
Message-ID: <438A141E.1090501@vanderbilt.edu>

Anjali Karve wrote:
> Hello,
>   
>   I have a classification tree. I want to obtain a ROC curve for this test. What is the easiest way to obtain one?
>   
>   -Anjali

ROC curves have a number of problems, chief among them leading to the 
temptation of dichotomizing test results.  ROC areas are useful 
statistics though.  In the Hmisc package see somers2 and rcorr.cens for 
getting the ROC area nonparametrically.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ggrothendieck at gmail.com  Sun Nov 27 20:16:34 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 27 Nov 2005 14:16:34 -0500
Subject: [R] Counting the occurence of each unique "charecter string"
In-Reply-To: <XFMail.051127182712.Ted.Harding@nessie.mcc.ac.uk>
References: <20051127174920.48183.qmail@web51914.mail.yahoo.com>
	<XFMail.051127182712.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <971536df0511271116g29b2b469i2d87abc53addc173@mail.gmail.com>

On 11/27/05, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
> On 27-Nov-05 Marco Visser wrote:
> > LS,
> >
> >   I would really like to know how to count the frequency/occurrence of
> > chachters inside a dataset. I am working with extreemly large datasets
> > of forest inventory data with a large variety of different species
> > inside it.
> >   Each row inside the dataframe represents one individual tree and the
> > simplified dataframe looks something like this:
> >
> >   num species dbh
> >   1        sp1           30
> >   2        sp1          20
> >   3        sp2          30
> >   4        sp1          40
> >
> >   I need to be able to count the number of individuals per species, so
> > I  need a command that will return for each unique species its
> > occurence  inside the dataframe;
> >
> >   [sp1]     3
> >   [sp2]     1
>
> Does the following help? (Using an artificial example a bit more
> complicated than yours). The dataframe "trees" consists of a list
> of species names under "Species", and values of a numeric variable
> under "X".
>
>
>  > trees
>              Species   X
>  1     Larix decidua 203
>  2  Pinus sylvestris 303
>  3     Larix decidua 202
>  4  Pinus sylvestris 301
>  5       Picea abies 102
>  6       Picea abies 103
>  7  Pinus sylvestris 302
>  8       Picea abies 101
>  9     Larix decidua 201
>  10      Picea abies 104
>  11      Picea abies 105
>  12 Pinus sylvestris 304
>
>
>  > freqs<-as.data.frame(table(trees$Species))
>  > colnames(freqs)<-c("Species","Counts")
>  > freqs
>             Species Counts
>  1    Larix decidua      3
>  2      Picea abies      5
>  3 Pinus sylvestris      4
>
>
>  > mean(freqs$Counts)
>  [1] 4
>  > sd(freqs$Counts)
>  [1] 1
>
>
> Just using table() would give you the same information, but
> converting it to a dataframe makes that information more
> readily accessible by familiar methods.
>
> Hoping this helps,
> Ted.
>
>

or using the iris dataset that comes with R and making use
of as.data.frame.table we can shorten that slightly to just:

as.data.frame.table(table(Species = iris$Species), responseName = "Count")

Incidently, I just noticed that there is an inconsistency between as.data.frame
and as.data.frame.table making it impossible to shorten as.data.frame.table
to as.data.frame in the above due to the responseName= argument
which is not referenced in the generic.

> args(as.data.frame)
function (x, row.names = NULL, optional = FALSE)
NULL
> args(as.data.frame.table)
function (x, row.names = NULL, optional = FALSE, responseName = "Freq")
NULL
> R.version.string # Windows
[1] "R version 2.2.0, 2005-10-24"



From blindglobe at gmail.com  Sun Nov 27 21:12:13 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Sun, 27 Nov 2005 21:12:13 +0100
Subject: [R] R-help Digest, Vol 33, Issue 27
In-Reply-To: <mailman.6.1133089201.18367.r-help@stat.math.ethz.ch>
References: <mailman.6.1133089201.18367.r-help@stat.math.ethz.ch>
Message-ID: <1abe3fa90511271212s4e8a0649lacaed08cd44f5ad9@mail.gmail.com>

> From: Duncan Murdoch <murdoch at stats.uwo.ca>

> I'd recommend using the RWinEdt package instead for a different way to
> integrate winedit with R.

winedit and winedt are two different editors, last I checked.

best,
-tony

blindglobe at gmail.com
Muttenz, Switzerland.
"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).



From schoenle at Princeton.EDU  Sun Nov 27 21:31:31 2005
From: schoenle at Princeton.EDU (Raphael Schoenle)
Date: Sun, 27 Nov 2005 15:31:31 -0500
Subject: [R] fixed, random effects with variable weights
Message-ID: <4ff61dee1abd251972c2d0a857073f61@princeton.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051127/770f65a2/attachment.pl

From ronpicci at yahoo.fr  Sun Nov 27 21:51:55 2005
From: ronpicci at yahoo.fr (Ron Piccinini)
Date: Sun, 27 Nov 2005 21:51:55 +0100 (CET)
Subject: [R] 'For each file in folder F do....'
Message-ID: <20051127205156.93370.qmail@web26406.mail.ukl.yahoo.com>

Hello,

I have 2700 text files in a folder and need to apply
the same program/procedure to each individually. I'm
trying to find how to code something like:

For each file in <Folder> do {<Procedure>}

is there an easy way to do this? other suggestions? 

I have tried to list all the files names in a vector
e.g.

>listfiles[1:10,1] 

1   H:/Rtest/AXP.txt
2    H:/Rtest/BA.txt
3     H:/Rtest/C.txt
4   H:/Rtest/CAT.txt
5    H:/Rtest/DD.txt
6   H:/Rtest/DIS.txt
7    H:/Rtest/EK.txt
8    H:/Rtest/GE.txt
9    H:/Rtest/GM.txt
10   H:/Rtest/HD.txt

but R doesn't like statements of type

>read.table(file=listfiles[1,1])

since 'file' must be a character string or
connection...

Any thoughts?

Many thanks in advance,

Ron Piccinini.



From kjetilbrinchmannhalvorsen at gmail.com  Sun Nov 27 22:19:35 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 27 Nov 2005 17:19:35 -0400
Subject: [R] 'For each file in folder F do....'
In-Reply-To: <20051127205156.93370.qmail@web26406.mail.ukl.yahoo.com>
References: <20051127205156.93370.qmail@web26406.mail.ukl.yahoo.com>
Message-ID: <438A22E7.6010801@gmail.com>

Ron Piccinini wrote:
> Hello,
> 
> I have 2700 text files in a folder and need to apply
> the same program/procedure to each individually. I'm
> trying to find how to code something like:
> 
> For each file in <Folder> do {<Procedure>}
> 
> is there an easy way to do this? other suggestions? 

files <- listfiles()
results <- lapply(files, yourprocessing())

where yourprocessing is a function taking as argument a file name and 
returning whatever you want.

Kjetil


> 
> I have tried to list all the files names in a vector
> e.g.
> 
>> listfiles[1:10,1] 
> 
> 1   H:/Rtest/AXP.txt
> 2    H:/Rtest/BA.txt
> 3     H:/Rtest/C.txt
> 4   H:/Rtest/CAT.txt
> 5    H:/Rtest/DD.txt
> 6   H:/Rtest/DIS.txt
> 7    H:/Rtest/EK.txt
> 8    H:/Rtest/GE.txt
> 9    H:/Rtest/GM.txt
> 10   H:/Rtest/HD.txt
> 
> but R doesn't like statements of type
> 
>> read.table(file=listfiles[1,1])
> 
> since 'file' must be a character string or
> connection...
> 
> Any thoughts?
> 
> Many thanks in advance,
> 
> Ron Piccinini.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Sun Nov 27 22:25:05 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 27 Nov 2005 16:25:05 -0500
Subject: [R] 'For each file in folder F do....'
In-Reply-To: <20051127205156.93370.qmail@web26406.mail.ukl.yahoo.com>
References: <20051127205156.93370.qmail@web26406.mail.ukl.yahoo.com>
Message-ID: <438A2431.8020409@stats.uwo.ca>

On 11/27/2005 3:51 PM, Ron Piccinini wrote:
> Hello,
> 
> I have 2700 text files in a folder and need to apply
> the same program/procedure to each individually. I'm
> trying to find how to code something like:
> 
> For each file in <Folder> do {<Procedure>}
> 
> is there an easy way to do this? other suggestions? 
> 
> I have tried to list all the files names in a vector
> e.g.
> 
> 
>>listfiles[1:10,1] 
> 
> 
> 1   H:/Rtest/AXP.txt
> 2    H:/Rtest/BA.txt
> 3     H:/Rtest/C.txt
> 4   H:/Rtest/CAT.txt
> 5    H:/Rtest/DD.txt
> 6   H:/Rtest/DIS.txt
> 7    H:/Rtest/EK.txt
> 8    H:/Rtest/GE.txt
> 9    H:/Rtest/GM.txt
> 10   H:/Rtest/HD.txt
> 
> but R doesn't like statements of type
> 
> 
>>read.table(file=listfiles[1,1])
> 
> 
> since 'file' must be a character string or
> connection...
> 
> Any thoughts?

 From the look of it, the listfiles column that you created has been 
converted to a factor.  You can convert back to character using 
as.character(); the as.is=TRUE parameter in the file reading functions 
will prevent the conversion in the first place, if that's how it happened.

Then something like

results <- list()
for (f in as.character(listfiles[,1])) results[[f]] <- read.table(file=f)

will read all the files and put them in a list.

Duncan Murdoch



From qysopjasmwq at hotmail.com  Sun Nov 27 22:35:23 2005
From: qysopjasmwq at hotmail.com (tenchi2641880)
Date: Sun, 27 Nov 2005 22:35:23 +0100
Subject: [R] You can see you are getting older: gray hair,
	wrinkles on your forehead, glasses for reading, 
Message-ID: <200511272135.jARLZNDW005213@hypatia.math.ethz.ch>


   You  seem  n0t to have any problems with erections but you still dream
   of fuller and harder ones. Dont say NO to our Soft Viagra tabs.
   Do you think your marriage is getting ruined because of bad sex?
   Viagra Soft Tabs will help you save your marriage.
   If you have always dreamed of being called a sex machine,
   our Viagra Soft Tabs is specially for you.
   [1]http://drnowmed.info/vt/ ?z5HNCT

References

   1. http://adeptbar.info/vt/


From dimitrijoe at yahoo.com.br  Sun Nov 27 01:52:45 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Sat, 26 Nov 2005 22:52:45 -0200
Subject: [R] creating a factor from other factors and ifelse
Message-ID: <001401c5f2ec$e3c9bd20$1600a8c0@thesahajamach>

Hi,

Given

> sec98 <- factor(rep(1:2,3), labels=c("A", "B"))
> sec99 <- factor(rep(2:1,3), labels=c("A", "B"))
>    sec99[c(2,5)] <- NA
> sec00 <- factor( c( rep(1,3), rep(2,3) ), labels=c("A", "B"))
>    sec00[c(2,4)] <- NA
> sec1 <- ifelse(!is.na(sec99), sec99,
        ifelse(!is.na(sec00), sec00, NA ))

We get

> sec1; class(sec1)
[1]  2 NA  2  1  2  1
[1] "integer"

I wonder why sec1 as above defined  in not a factor, since it has been 
created from (logical operations and) factors. Of course, one could do

> sec1 <- factor(sec1, labels=levels(sec99))

but this would be a problem if I had (as I actually do) sec99 and sec00 
instead defined as

> sec99 <- factor(c(1,2,3,2,3,3), labels=c("A", "B", "C"))
>   sec99[c(2,5)] <- NA
> sec00 <- factor(c(4,1,1,2,4,2), labels=c("A", "B", "D"))
>    sec00[c(2,4)] <- NA

    # because
> sec1 <- ifelse(!is.na(sec99), sec99,
>        ifelse(!is.na(sec00), sec00, NA ))

    # gives us
> sec1; class(sec1)
[1]  1 NA  3  2  3  3
[1] "integer"

now it's hard to tell where each "3" in sec1 means "C" or "D". What I 
actually wanted was

> sec1; class(sec1)
[1]  A <NA>  C  B  D  C
[1] "factor"

Any suggestions on how to do it in a simple way will be welcome.
Thanks,
Dimitri



From dimitris.rizopoulos at med.kuleuven.be  Mon Nov 28 09:09:32 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 28 Nov 2005 09:09:32 +0100
Subject: [R] IRT Package
References: <F5ED48890E2ACB468D0F3A64989D335AC990F6@dc1ex3.air.org>
Message-ID: <007201c5f3f3$1020e180$0540210a@www.domain>

In fact, there some common IRT models that ltm cannot currently 
handle, e.g., the three-parameter logistic model or the graded 
response model; however, I do plan to incorporate these models in the 
near future.


Best,
Dimitris


----- Original Message ----- 
From: "Doran, Harold" <HDoran at air.org>
To: "Caio Lucidius Naberezny Azevedo" <c_naber at yahoo.com.br>; 
<r-help at stat.math.ethz.ch>
Sent: Sunday, November 27, 2005 2:52 PM
Subject: Re: [R] IRT Package


>I do not believe another IRT package exists. However, I have recently 
>used the rasch() function in ltm for a study I am doing and have 
>found it very useful. I'm curious (as I'm sure the ltm developer is) 
>as to what are you doing that ltm cannot handle.
>
> Harold
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch on behalf of Caio Lucidius 
> Naberezny Azevedo
> Sent: Sat 11/26/2005 4:50 PM
> To: r-help at stat.math.ethz.ch
> Cc:
> Subject: [R] IRT Package
>
> Hi all,
>
>  Could anyone tell me if there is some package that fits any Item 
> Response Model (further the ltm package)?
>
>  Regards,
>
> Caio
>
>
> ---------------------------------
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From buser at stat.math.ethz.ch  Mon Nov 28 09:14:43 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 28 Nov 2005 09:14:43 +0100
Subject: [R] Find main effect in 2-way ANOVA
In-Reply-To: <BAY104-F37C04F59B3F62A09E0D7ECD0550@phx.gbl>
References: <BAY104-F37C04F59B3F62A09E0D7ECD0550@phx.gbl>
Message-ID: <17290.48243.7338.469717@stat.math.ethz.ch>

Hi

It is not so clear to me what your intention is. Could you
provide a reproducible example to show what you want to do. 

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Gao Fay writes:
 > Hi,
 > 
 > I use anova() to find interaction effect and then I need to find data with 
 > main effect from those data with interaction effect. How to do that?
 > 
 > I used : anova(lm(data~factor1*factor2)), then select data with interaction 
 > effect. Then I need to select those data also with main effect of factor1 
 > or factor2, from previous selected data. How to do that? Many thanks for 
 > your time on my quesiton!
 > 
 > Fay
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Mon Nov 28 08:43:12 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 28 Nov 2005 08:43:12 +0100
Subject: [R] How to do the multiple plots?
In-Reply-To: <Pine.LNX.4.62.0511261534050.28175@entry.nmr.mgh.harvard.edu>
References: <971536df0511261147sd2eaf96sfd718acb22fe01fb@mail.gmail.com>
Message-ID: <438AC320.10568.5F4B33@localhost>



On 26 Nov 2005 at 15:40, Wei Qiu wrote:

Date sent:      	Sat, 26 Nov 2005 15:40:52 -0500 (EST)
From:           	Wei Qiu <wqiu at nmr.mgh.harvard.edu>
To:             	Gabor Grothendieck <ggrothendieck at gmail.com>
Copies to:      	r-help <r-help at stat.math.ethz.ch>
Subject:        	Re: [R] How to do the multiple plots?

> Dear Babor and all,
> 
> Thanks for your quick response. I tried the following. We can plot y1
> and Y2 on one figs. It just shows one Y axis on the left side. I would
> like to show the Y1 axis on left side and Y2 Axis on right side.
> 
> Any others input,

Hi

so add to your evening reading also

?axis and other functions from See also chapters in man 
pages.

HTH
Petr



> 
> Wei
> 
> On Sat, 26 Nov 2005, Gabor Grothendieck wrote:
> 
> > ?lines
> > ?matplot
> > ?ts.plot
> > library(zoo); ?plot.zoo
> >
> >
> > On 11/26/05, Wei Qiu <wqiu at nmr.mgh.harvard.edu> wrote:
> >> Hi all,
> >>
> >> I am new in R tool. I have a basic question. I would like to do a
> >> combination of two multiple lines plots for one X-variable and two
> >> different sets (lists) of Y-variables. Any suggestion will be
> >> greatly appreciated.
> >>
> >> Thanks!
> >>
> >> Wei
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Mon Nov 28 08:38:07 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 28 Nov 2005 08:38:07 +0100
Subject: [R] str and structable error
Message-ID: <438AC1EF.7105.5AA39D@localhost>

Hallo

I encountered a behaviour which puzzles me (but 
finally I did get what I wanted).

I used structable and strucplot but I wanted to change 
names of variables in structable object. I tried to subset 
it, use names but to no avail. So I tried str and 
expected to get a structure of an object but:

> sss<-structable(Titanic)
> str(sss)
Error in "[.structable"(x, args[[1]], ) : subscript out of 
bounds

Finally I learned, that I need to change attributes of 
structable object.

Is this error message OK and I did not read 
documentation properly? Or is it normal that str gives 
an error on some objects but I just was not so lucky to 
meet one?.

W2000, R2.2.0, vcd package Built: R 2.2.0; ; 2005-11-
22 14:23:44; windows, 

Best regards.

Petr

Petr Pikal
petr.pikal at precheza.cz



From guangxing at ict.ac.cn  Mon Nov 28 07:44:25 2005
From: guangxing at ict.ac.cn (=?gb2312?B?uePQxw==?=)
Date: Mon, 28 Nov 2005 14:44:25 +0800
Subject: [R] How Can I change the acf's plot type?
Message-ID: <E1Egck4-0002Xd-00@bernie.ethz.ch>

In the R Document, the usage of the acf() is as  follow:
  acf(x, lag.max = NULL,
    type = c("correlation", "covariance", "partial"),
    plot = TRUE, na.action = na.fail, demean = TRUE, ...)
But now I want to get the result picture like:
 plot(x,type="l")
 or
 plot(x,type="p")
How can I do this with acf function?

	

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ÖÂ
Àñ£¡
 				

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¹ãÐÇ
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-28



From suncertain at gmail.com  Mon Nov 28 07:23:49 2005
From: suncertain at gmail.com (Urania Sun)
Date: Mon, 28 Nov 2005 00:23:49 -0600
Subject: [R] any package to compare two scoring systems
Message-ID: <4ab0fb470511272223s7f818d25xc19eb943a7fee06c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051128/5866a461/attachment.pl

From Hong.Ooi at iag.com.au  Mon Nov 28 08:01:52 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Mon, 28 Nov 2005 18:01:52 +1100
Subject: [R] Looking for constrained optimisation code
Message-ID: <E1Egd6D-0003M3-00@bernie.ethz.ch>


_______________________________________________________________________________________


Hi,

I was just wondering if there was any available R code that could handle
general constrained optimisation problems. At the moment I'm using
nlminb and optim, both of which allow box constraints on the parameters,
but ideally I'd like to be able to specify more general constraints on
the solution space.

Even if code isn't readily available, any tips on how to persuade
optim/nlminb to cope with general constraints would also be much
appreciated.

Thanks!


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
(02) 9292 1566


_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From hvermei1 at vrcbe.jnj.com  Mon Nov 28 09:48:13 2005
From: hvermei1 at vrcbe.jnj.com (Vermeiren, Hans [VRCBE])
Date: Mon, 28 Nov 2005 09:48:13 +0100
Subject: [R] obtaining a ROC curve
Message-ID: <9AC105024CEA64458BF66D1DE13CA50D070FB3E3@tibbemeexs1.eu.jnj.com>

	Hello,
  
 	 I have a classification tree. I want to obtain a ROC curve for this
test. What is the easiest way to obtain one?
  
 	 -Anjali
  



did you try the ROCR package ?
regards,
Hans Vermeiren



From spencer.graves at pdf.com  Sun Nov 27 23:44:43 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 27 Nov 2005 14:44:43 -0800
Subject: [R] anova.gls from nlme on multiple arguments within a function
 fails
In-Reply-To: <1132226230.16749.11.camel@pallas.abo.fi>
References: <1132226230.16749.11.camel@pallas.abo.fi>
Message-ID: <438A36DB.80301@pdf.com>

	  You've posed an excellent question with simple and elegant, 
reproducible example.  I've seen no replies, so I will attempt a partial 
response.  RSiteSearch("lexical scoping") produced some potentially 
useful comments (e.g., 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/37769.html), but nothing 
that allowed me to work around the problem.

	  The following modification of your example makes it clear that 
"anova.lme" (called by "anova.gls") choked on the second argument not 
the first:

 > dummy2 <- function(obj)
+   {
+     obj2 <- obj[[2]]
+     anova(obj[[1]], obj2)
+   }
 > dummy2(list(fm1, fm2))
Error in anova.lme(object = obj[[1]], obj2) :
	object "obj2" not found

	  The following helped isolate this further to "dots <- list(...)", the 
second line in "anova.lme":

debug(anova.lme)
dummy2(list(fm1, fm2))

	  I don't know why your example fails, especially "anova.lm" worked. 
Also, there should be a way  to use something like "assign" to work 
around this problem, but nothing I tried worked.

	  I know this is not a complete reply, but I hope it helps.
	  spencer graves

Markus Jantti wrote:

> Dear All -- 
> 
> I am trying to use within a little table producing code an anova
> comparison of two gls fitted objects, contained in a list of such
> object, obtained using nlme function gls.
> The anova procedure fails to locate the second of the objects.
> 
> The following code, borrowed from the help page of anova.gls,
> exemplifies:
> --------------- start example code ---------------
> library(nlme)
> 
> ## stolen from example(anova.gls)
> # AR(1) errors within each Mare
> fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
>            correlation = corAR1(form = ~ 1 | Mare))
> anova(fm1)
> # variance changes with a power of the absolute fitted values?
> fm2 <- update(fm1, weights = varPower())
> anova(fm1, fm2)
> 
> ## now define a little function
> dummy <- function(obj)
>   {
>     anova(obj[[1]], obj[[2]])
>   }
> dummy(list(fm1, fm2))
> 
> ## compare with what happens in anova.lm:
> 
> lm1 <- lm(follicles ~ sin(2*pi*Time), Ovary)
> lm2 <- lm(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary)
> dummy(list(lm1, lm2))
> ------------- end example code ------------------
> 
> It is not the end of the world: I can easily work around this. 
> But it would be nice to know why this does not work.
> 
> Digging around using options(error=recover) did not help my much, I'm
> afraid.  
> 
> Best,
> 
> Markus 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From kjetilbrinchmannhalvorsen at gmail.com  Mon Nov 28 00:21:01 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 27 Nov 2005 19:21:01 -0400
Subject: [R] Question on KalmanSmooth
Message-ID: <438A3F5D.4020706@gmail.com>

I am trying to use KalmanSmooth to smooth a time series
fitted by arima (and with missing values), but the $smooth component
of the output baffles me.  Look at the following example:

testts <- arima.sim(list(ar=0.9),n=100)
testts[6:14] <- NA
testmod <- arima(testts, c(1,0,0))
testsmooth <- KalmanSmooth(testts, testmod$model)
par(mfrow=c(2,1))
plot(testsmooth$smooth, type="l")
plot(testsmooth$var, type="l")

Look at the lower panel plot, how the uncertainty of the
smoothed values first is lowered, then being the highest
at the end ( of the smoothed part, indexes 6:14).
Anybody can explain this,or is this an error?


Kjetil Halvorsen



From ligges at statistik.uni-dortmund.de  Mon Nov 28 10:19:13 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 28 Nov 2005 10:19:13 +0100
Subject: [R] str and structable error
In-Reply-To: <438AC1EF.7105.5AA39D@localhost>
References: <438AC1EF.7105.5AA39D@localhost>
Message-ID: <438ACB91.2040007@statistik.uni-dortmund.de>

Petr Pikal wrote:

> Hallo
> 
> I encountered a behaviour which puzzles me (but 
> finally I did get what I wanted).
> 
> I used structable and strucplot but I wanted to change 
> names of variables in structable object. I tried to subset 
> it, use names but to no avail. So I tried str and 
> expected to get a structure of an object but:
> 
> 
>>sss<-structable(Titanic)
>>str(sss)
> 
> Error in "[.structable"(x, args[[1]], ) : subscript out of 
> bounds

Looks like package vcd needs a separate structable method for the str() 
generic.

Uwe Ligges



> Finally I learned, that I need to change attributes of 
> structable object.
> 
> Is this error message OK and I did not read 
> documentation properly? Or is it normal that str gives 
> an error on some objects but I just was not so lucky to 
> meet one?.
> 
> W2000, R2.2.0, vcd package Built: R 2.2.0; ; 2005-11-
> 22 14:23:44; windows, 
> 
> Best regards.
> 
> Petr
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From hastie at stanford.edu  Mon Nov 28 02:28:46 2005
From: hastie at stanford.edu (Trevor Hastie)
Date: Sun, 27 Nov 2005 17:28:46 -0800
Subject: [R] [R-pkgs] glmpath: L1 regularization path for glms
Message-ID: <2E1DE912-9603-4733-B00E-7450335F65E9@stanford.edu>

We have uploaded to CRAN the first version of glmpath,
which fits the L1 regularization path for generalized linear models.

The lars package fits the entire piecewise-linear L1 regularization  
path for
the lasso. The coefficient paths for L1 regularized glms, however,   
are not piecewise linear.
glmpath uses convex optimization - in particular predictor-corrector  
methods-
to fit the coefficient path at important junctions. These junctions  
are at the "knots" in |beta|
where variables enter/leave the active set; i.e.  nonzero/zero values.
Users can request greater resolution at a cost of more computation,  
and compute values
on a fine grid between the knots.

The code is fast, and can handle largish problems efficiently.
it took just over 4 sec system cpu time to fit the logistic  
regression path for
the "spam" data from UCI with 3065 training obs and 57 predictors.
For a microarray example with 5000 variables and 100 observations, 11  
seconds cpu time.

Currently glmpath implements binomial, poisson and gaussian families.

Mee Young Park and Trevor Hastie




-------------------------------------------------------------------
   Trevor Hastie                                   hastie at stanford.edu
   Professor, Department of Statistics, Stanford University
   Phone: (650) 725-2231 (Statistics)          Fax: (650) 725-8977
   (650) 498-5233 (Biostatistics)   Fax: (650) 725-6951
   URL: http://www-stat.stanford.edu/~hastie
    address: room 104, Department of Statistics, Sequoia Hall
            390 Serra Mall, Stanford University, CA 94305-4065
  --------------------------------------------------------------------

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From maechler at stat.math.ethz.ch  Mon Nov 28 10:22:21 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 28 Nov 2005 10:22:21 +0100
Subject: [R] str and structable error
In-Reply-To: <438AC1EF.7105.5AA39D@localhost>
References: <438AC1EF.7105.5AA39D@localhost>
Message-ID: <17290.52301.709338.154991@stat.math.ethz.ch>

>>>>> "Petr" == Petr Pikal <petr.pikal at precheza.cz>
>>>>>     on Mon, 28 Nov 2005 08:38:07 +0100 writes:

    Petr> Hallo I encountered a behaviour which puzzles me (but
    Petr> finally I did get what I wanted).


    Petr> I used structable and strucplot but I wanted to change
    Petr> names of variables in structable object. 

structable is not "part of R" :
You should tell us that you are using an extra package where
structable is from : namely -- I did your homework -- "vcd"

    Petr> I tried to subset it, use names but to no avail. So I
    Petr> tried str and expected to get a structure of an object
    Petr> but:

    >> sss<-structable(Titanic)
    >> str(sss)
    Petr> Error in "[.structable"(x, args[[1]], ) : subscript
    Petr> out of bounds

yes, and similar problems from

    library(vcd)
    example(structable)
    str(hec)

--> see below

    Petr> Finally I learned, that I need to change attributes of
    Petr> structable object.

    Petr> Is this error message OK and I did not read
    Petr> documentation properly? Or is it normal that str gives
    Petr> an error on some objects but I just was not so lucky
    Petr> to meet one?.

No, it is not normal (and is a bug -- in vcd code IMO):

The reason is that  structable ``objects'' (S3)
do not fulfill a fundamental property that all  S (and hence R)
objects should fulfill IMO  {but read on before protesting}

      >>>>> length() and "[" should be compatible
      >>>>> -------------------------------------

namely, for an object 'x', if
   n <- length(x)
and assume n > 0 for the moment, 
then
   x[j]
should return something reasonable for all numeric vectors 'j'
which have values in {1,2,...,n}  (and  also for {-n,...,-1})

This is unfortunately not at all true for structable objects.
I'd say the authors of "structable" made a bit a peculiar
decision when they designed the "[.structable" method since that
invalidates the above basic principle.
This is particularly unfortunate, since "structable" also
inherits from "ftable" {a ``standard R'' S3 class} which does
not have that bad property

If the current "[" (non-S-like IMO) behavior of structable
objects really will be maintained in the future, 
one solution / workaround would be to define a simple 
str.structable  method -- which would also help you for the
moment :

str.structable <- function(object, ...) { 
   cat("structable ")
   class(object) <- class(object)[-1]
   str(object, ...) 
}

> str(structable(Titanic))
structable  ftable [1:8, 1:4] 0 118 0 154 35 387 0 670 5 57 ...
 - attr(*, "dnames")=List of 4
  ..$ Class   : chr [1:4] "1st" "2nd" "3rd" "Crew"
  ..$ Sex     : chr [1:2] "Male" "Female"
  ..$ Age     : chr [1:2] "Child" "Adult"
  ..$ Survived: chr [1:2] "No" "Yes"
 - attr(*, "split_vertical")= logi [1:4] FALSE  TRUE FALSE  TRUE
 - attr(*, "col.vars")=List of 2
  ..$ Sex     : chr [1:2] "Male" "Female"
  ..$ Survived: chr [1:2] "No" "Yes"
 - attr(*, "row.vars")=List of 2
  ..$ Class: chr [1:4] "1st" "2nd" "3rd" "Crew"
  ..$ Age  : chr [1:2] "Child" "Adult"
 - attr(*, "class")= chr "ftable"
> 

Regards,
Martin Maechler, ETH Zurich


    Petr> W2000, R2.2.0, vcd package Built: R 2.2.0; ; 2005-11-
    Petr> 22 14:23:44; windows,

    Petr> Best regards.

    Petr> Petr

    Petr> Petr Pikal petr.pikal at precheza.cz



From anil_rohilla at rediffmail.com  Mon Nov 28 10:47:30 2005
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 28 Nov 2005 09:47:30 -0000
Subject: [R] possible Probabilstic models in R
Message-ID: <20051128094730.10189.qmail@webmail36.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051128/9222ccc0/attachment.pl

From david.meyer at wu-wien.ac.at  Mon Nov 28 11:08:38 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Mon, 28 Nov 2005 11:08:38 +0100
Subject: [R] str and structable error
In-Reply-To: <438ACB91.2040007@statistik.uni-dortmund.de>
References: <438AC1EF.7105.5AA39D@localhost>
	<438ACB91.2040007@statistik.uni-dortmund.de>
Message-ID: <20051128110838.527bc901.david.meyer@wu-wien.ac.at>

> > 
> > I encountered a behaviour which puzzles me (but 
> > finally I did get what I wanted).
> > 
> > I used structable and strucplot but I wanted to change 
> > names of variables in structable object. 
I tried to subset 
> > it, use names but to no avail. So I tried str and 
> > expected to get a structure of an object but:
> > 
> > 
> >>sss<-structable(Titanic)
> >>str(sss)
> > 
> > Error in "[.structable"(x, args[[1]], ) : subscript out of 
> > bounds
> 
> Looks like package vcd needs a separate structable method for the str() 
> generic.

yes! Thanks for pointing this out. It's because "[.structable" has a
non-standard behavior. Using:

"[.structable" = function(object, ...) NextMethod()

at the command line, str() would work as expected.

David

> 
> Uwe Ligges
> 
> 
> 
> > Finally I learned, that I need to change attributes of 
> > structable object.
> > 
> > Is this error message OK and I did not read 
> > documentation properly? Or is it normal that str gives 
> > an error on some objects but I just was not so lucky to 
> > meet one?.
> > 
> > W2000, R2.2.0, vcd package Built: R 2.2.0; ; 2005-11-
> > 22 14:23:44; windows, 
> > 
> > Best regards.
> > 
> > Petr
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 
Dr. David Meyer
Department of Information Systems and Operations

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From ripley at stats.ox.ac.uk  Mon Nov 28 11:09:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Nov 2005 10:09:03 +0000 (GMT)
Subject: [R] How Can I change the acf's plot type?
In-Reply-To: <E1Egck4-0002Xd-00@bernie.ethz.ch>
References: <E1Egck4-0002Xd-00@bernie.ethz.ch>
Message-ID: <Pine.LNX.4.61.0511281006580.3364@gannet.stats>

On Mon, 28 Nov 2005, [gb2312] ???? wrote:

> In the R Document, the usage of the acf() is as  follow:
>  acf(x, lag.max = NULL,
>    type = c("correlation", "covariance", "partial"),
>    plot = TRUE, na.action = na.fail, demean = TRUE, ...)
> But now I want to get the result picture like:
> plot(x,type="l")
> or
> plot(x,type="p")
> How can I do this with acf function?

From the very same help page

      The generic function 'plot' has a method for objects of class
      '"acf"'.

so use that directly: it has a 'type' parameter.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From allan_sta_staff_sci_main_uct at mail.uct.ac.za  Mon Nov 28 11:29:55 2005
From: allan_sta_staff_sci_main_uct at mail.uct.ac.za (allan_sta_staff_sci_main_uct@mail.uct.ac.za)
Date: Mon, 28 Nov 2005 12:29:55 +0200
Subject: [R] mathematical expressions ...
Message-ID: <1133173795.438adc2358602@webmail.uct.ac.za>



hi all

i have a few questions about formatting plots:

1. i would like to add a symbol such as chi squared with 3 degrees of freedom
onto a plot.    expression(chi^2 == jnfdjb), can be used in conjuction with
"legend" to solve this but how does one include an under score for the degrees
of freedom

2. on the y axis: how does one change the orientation of the label from vertical
to horisontal

3. can one add dots (not lines) to a barplot (ie : inside each of the bars)


sorry if the questions are too simple



From roebuck at mdanderson.org  Mon Nov 28 11:35:59 2005
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Mon, 28 Nov 2005 04:35:59 -0600 (CST)
Subject: [R] Fwd: Matrix rotation
In-Reply-To: <a670d84c003d14886b4976242ceb7541@cpom.ucl.ac.uk>
References: <a670d84c003d14886b4976242ceb7541@cpom.ucl.ac.uk>
Message-ID: <Pine.OSF.4.58.0511280425210.387400@wotan.mdacc.tmc.edu>

On Thu, 24 Nov 2005, Benjamin Lloyd-Hughes wrote:

> Ok I warned you that I'd been drinking! What I really meant was
> something to go from:
>
>       [,1] [,2]
> [1,]    1    2
> [2,]    4    3
>
> to
>
>       [,1] [,2]
> [1,]    4    1
> [2,]    3    2
>
> to
>
>       [,1] [,2]
> [1,]    3    4
> [2,]    2    1
>
> to
>
>       [,1] [,2]
> [1,]    2    3
> [2,]    1    4
>


Another possible solution...

> library(matlab)
> x <- matrix(c(1,2,4,3), nrow=2, byrow=TRUE)
> x
     [,1] [,2]
[1,]    1    2
[2,]    4    3
> matlab::rot90(x, 3)
     [,1] [,2]
[1,]    4    1
[2,]    3    2
> matlab::rot90(x, 2)
     [,1] [,2]
[1,]    3    4
[2,]    2    1
> matlab::rot90(x, 1)
     [,1] [,2]
[1,]    2    3
[2,]    1    4

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From ripley at stats.ox.ac.uk  Mon Nov 28 11:44:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Nov 2005 10:44:27 +0000 (GMT)
Subject: [R] anova.gls from nlme on multiple arguments within a function
 fails
In-Reply-To: <438A36DB.80301@pdf.com>
References: <1132226230.16749.11.camel@pallas.abo.fi> <438A36DB.80301@pdf.com>
Message-ID: <Pine.LNX.4.61.0511281016270.3364@gannet.stats>

The error is in anova.gls(): traceback() would have told you that was 
involved.  It ends

do.call("anova.lme", as.list(match.call()[-1]))

and that call needs to be evaluated in the parent, not in the body of 
anova.lme.  Several similar errors (e.g. in the update methods) in package 
nlme have been corrected over the years.

Replacing anova() by anova.lme() in dummy() works.

If you want to do this sort of thing more generally (e.g. messing with 
the contents of '...'), the elegant way is

Call <- match.call()
Call[[1]] <- as.name("anova.lme")
eval.parent(Call)

and that works here.

Since at some later point substitute() is used to find the arguments, here 
you don't want to use do.call() with the evaluated arguments, which is the 
way it is intended to be used.  Similarly, anova.lme(object, ...) is not 
what you want.


On Sun, 27 Nov 2005, Spencer Graves wrote:

> 	  You've posed an excellent question with simple and elegant,
> reproducible example.  I've seen no replies, so I will attempt a partial
> response.  RSiteSearch("lexical scoping") produced some potentially
> useful comments (e.g.,
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/37769.html), but nothing
> that allowed me to work around the problem.
>
> 	  The following modification of your example makes it clear that
> "anova.lme" (called by "anova.gls") choked on the second argument not
> the first:

It actually chokes on both.

> > dummy2 <- function(obj)
> +   {
> +     obj2 <- obj[[2]]
> +     anova(obj[[1]], obj2)
> +   }
> > dummy2(list(fm1, fm2))
> Error in anova.lme(object = obj[[1]], obj2) :
> 	object "obj2" not found
>
> 	  The following helped isolate this further to "dots <- list(...)", the
> second line in "anova.lme":
>
> debug(anova.lme)
> dummy2(list(fm1, fm2))
>
> 	  I don't know why your example fails, especially "anova.lm" worked.
> Also, there should be a way  to use something like "assign" to work
> around this problem, but nothing I tried worked.
>
> 	  I know this is not a complete reply, but I hope it helps.
> 	  spencer graves
>
> Markus Jantti wrote:
>
>> Dear All --
>>
>> I am trying to use within a little table producing code an anova
>> comparison of two gls fitted objects, contained in a list of such
>> object, obtained using nlme function gls.
>> The anova procedure fails to locate the second of the objects.
>>
>> The following code, borrowed from the help page of anova.gls,
>> exemplifies:
>> --------------- start example code ---------------
>> library(nlme)
>>
>> ## stolen from example(anova.gls)
>> # AR(1) errors within each Mare
>> fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
>>            correlation = corAR1(form = ~ 1 | Mare))
>> anova(fm1)
>> # variance changes with a power of the absolute fitted values?
>> fm2 <- update(fm1, weights = varPower())
>> anova(fm1, fm2)
>>
>> ## now define a little function
>> dummy <- function(obj)
>>   {
>>     anova(obj[[1]], obj[[2]])
>>   }
>> dummy(list(fm1, fm2))
>>
>> ## compare with what happens in anova.lm:
>>
>> lm1 <- lm(follicles ~ sin(2*pi*Time), Ovary)
>> lm2 <- lm(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary)
>> dummy(list(lm1, lm2))
>> ------------- end example code ------------------
>>
>> It is not the end of the world: I can easily work around this.
>> But it would be nice to know why this does not work.
>>
>> Digging around using options(error=recover) did not help my much, I'm
>> afraid.
>>
>> Best,
>>
>> Markus
>
> -- 
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
>
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Nov 28 11:58:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 28 Nov 2005 11:58:12 +0100
Subject: [R] mathematical expressions ...
In-Reply-To: <1133173795.438adc2358602@webmail.uct.ac.za>
References: <1133173795.438adc2358602@webmail.uct.ac.za>
Message-ID: <438AE2C4.1090207@statistik.uni-dortmund.de>

allan_sta_staff_sci_main_uct at mail.uct.ac.za wrote:
> 
> hi all
> 
> i have a few questions about formatting plots:
> 
> 1. i would like to add a symbol such as chi squared with 3 degrees of freedom
> onto a plot.    expression(chi^2 == jnfdjb), can be used in conjuction with
> "legend" to solve this but how does one include an under score for the degrees
> of freedom


See ?plotmath and use the index brackets.


> 2. on the y axis: how does one change the orientation of the label from vertical
> to horisontal

See ?par, in particular its argument "las".


> 3. can one add dots (not lines) to a barplot (ie : inside each of the bars)


Yes, use points() to add dots, and the invisibly returned value of 
barplot() contains information on the postition of the bars.

Uwe Ligges


> 
> sorry if the questions are too simple
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From guangxing at ict.ac.cn  Mon Nov 28 12:16:31 2005
From: guangxing at ict.ac.cn (=?gb2312?B?uePQxw==?=)
Date: Mon, 28 Nov 2005 19:16:31 +0800
Subject: [R] How Can I change the acf's plot type?
Message-ID: <200511281114.jASBEJHs010577@hypatia.math.ethz.ch>

Oops,I got it!

>plot(acf(x,plot=false),type="l",col="red")
will be OK!

Thank you very much!	

======= 2005-11-28 18:09:03 ÄúÔÚÀ´ÐÅÖÐÐ´µÀ£º=======

>On Mon, 28 Nov 2005, [gb2312] ¹ãÐÇ wrote:
>
>> In the R Document, the usage of the acf() is as  follow:
>>  acf(x, lag.max = NULL,
>>    type = c("correlation", "covariance", "partial"),
>>    plot = TRUE, na.action = na.fail, demean = TRUE, ...)
>> But now I want to get the result picture like:
>> plot(x,type="l")
>> or
>> plot(x,type="p")
>> How can I do this with acf function?
>
>From the very same help page
>
>      The generic function 'plot' has a method for objects of class
>      '"acf"'.
>
>so use that directly: it has a 'type' parameter.
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595

= = = = = = = = = = = = = = = = = = = =
			

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ÖÂ
Àñ£¡
 
				 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¹ãÐÇ
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-28



From bjoern.stollenwerk at uk-koeln.de  Mon Nov 28 12:18:00 2005
From: bjoern.stollenwerk at uk-koeln.de (=?ISO-8859-1?Q?Bj=F6rn_Stollenwerk?=)
Date: Mon, 28 Nov 2005 12:18:00 +0100
Subject: [R] glm: quasi models with logit link function and binary data
Message-ID: <438AE768.3010201@uk-koeln.de>


# Hello R Users,
#
# I would like to fit a glm model with quasi family and
# logistical link function, but this does not seam to work
# with binary data.
#
# Please don't suggest to use the quasibinomial family. This
# works out, but when applied to the true data, the
# variance function does not seams to be
# appropriate.
#
# I couldn't see in the
# theory why this does not work.
# Is this a bug, or are there theoretical reasons?
# One problem might be, that logit(0)=-Inf and logit(1)=Inf.
# But I can't see how this disturbes the calculation of quasi-Likelihood.
#
# Thank you very much,
# best,
#
# Bj??rn

set.seed(0)
y <- sample(c(0,1), size=100, replace=T)

# the following models work:
glm(y ~ 1)
glm(y ~ 1, family=binomial(link=logit))
glm(y ~ 1, family=quasibinomial(link=logit))

# the next model doesn't work:
glm(y ~ 1, family=quasi(link=logit))



From m_osm at gmx.net  Mon Nov 28 12:49:23 2005
From: m_osm at gmx.net (Mahdi Osman)
Date: Mon, 28 Nov 2005 12:49:23 +0100 (MET)
Subject: [R] Xemacs
Message-ID: <539.1133178563@www82.gmx.net>

Hi all,

How can I stop my xemacs from lauching R whenever it starts? I want to use
the usual M-x R instead. 

This is something I have to edit in the init file. 


Thanks

Mahdi

-- 
-----------------------------------
Mahdi Osman (PhD)
E-mail: m_osm at gmx.net



From vietnguyen at fastmail.fm  Mon Nov 28 13:14:45 2005
From: vietnguyen at fastmail.fm (Viet Nguyen)
Date: Mon, 28 Nov 2005 23:14:45 +1100
Subject: [R] using minor tickmarks with xYplot
Message-ID: <438AF4B5.1000301@fastmail.fm>

Hi all,

I'm trying to make a plot with the function xYplot from package Hmisc in 
R.  I would like to have minor tick-marks on the axis.  This should be a 
common simple feature to have but I don't seem to find any discussion on 
the topic.

Following is one of the things I tried and the error returned:

xYplot(y~x,data.frame(x=seq(1,10),y=runif(10)),minor.ticks=c(3.5,5.5))
Error in panel(x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), y = 
c(0.88172451662831,  :
    object "gfun" not found

It's important that I use "xYplot" and not "plot" so function 
minor.tick() is not useful.

Anything I can try?  Thanks in advance for your help.

vn



From juansan at dca.upv.es  Mon Nov 28 13:16:33 2005
From: juansan at dca.upv.es (=?iso-8859-1?Q?Juan_Pablo_S=E1nchez?=)
Date: Mon, 28 Nov 2005 13:16:33 +0100
Subject: [R] About the error of the expentancy of life
Message-ID: <003701c5f415$929b0290$13662a9e@portatilJP>

Dear R users:
I am looking for information on how to compute the error in an estimation of the expentancy of life using survival analysis methods. I am interesting either in the theoretical calculations or in the way to carry out the analysis using R. I know that the mean life can be computed in R using the function cph from the Design package, but what about the error of this estimation.

Cheers,
Juan Pablo.



From sundar.dorai-raj at pdf.com  Mon Nov 28 13:20:16 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 28 Nov 2005 06:20:16 -0600
Subject: [R] glm: quasi models with logit link function and binary data
In-Reply-To: <438AE768.3010201@uk-koeln.de>
References: <438AE768.3010201@uk-koeln.de>
Message-ID: <438AF600.3020805@pdf.com>



Bj??rn Stollenwerk wrote:
> # Hello R Users,
> #
> # I would like to fit a glm model with quasi family and
> # logistical link function, but this does not seam to work
> # with binary data.
> #
> # Please don't suggest to use the quasibinomial family. This
> # works out, but when applied to the true data, the
> # variance function does not seams to be
> # appropriate.
> #
> # I couldn't see in the
> # theory why this does not work.
> # Is this a bug, or are there theoretical reasons?
> # One problem might be, that logit(0)=-Inf and logit(1)=Inf.
> # But I can't see how this disturbes the calculation of quasi-Likelihood.
> #
> # Thank you very much,
> # best,
> #
> # Bj??rn
> 
> set.seed(0)
> y <- sample(c(0,1), size=100, replace=T)
> 
> # the following models work:
> glm(y ~ 1)
> glm(y ~ 1, family=binomial(link=logit))
> glm(y ~ 1, family=quasibinomial(link=logit))
> 
> # the next model doesn't work:
> glm(y ~ 1, family=quasi(link=logit))
> 

This is an issue with the starting values provided to glm. Take a look 
at the difference between:

quasibinomial()$initialize

and

quasi("logit")$initialize

and where this is used in glm.fit and you should see the why the error 
occurs. To avoid this, you can supply your own starting values from a 
call to glm

mustart <- predict(glm(y ~ 1, binomial), type = "response")
glm(y ~ 1, quasi("logit"), mustart = mustart)

or just use:

glm(y ~ 1, quasi("logit"), mustart = rep(0.5, length(y)))

HTH,

--sundar



From koen.hufkens at telenet.be  Mon Nov 28 13:23:32 2005
From: koen.hufkens at telenet.be (Koen Hufkens)
Date: Mon, 28 Nov 2005 13:23:32 +0100
Subject: [R] background computation, &
Message-ID: <438AF6C4.3050304@telenet.be>

Hi list,

Is there a way to process R commands in the background other then using
R in batch mode? I'm looking for the equivalent of the & operator on the 
*nix commandline.

Cheers,
Koen



From maechler at stat.math.ethz.ch  Mon Nov 28 14:19:16 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 28 Nov 2005 14:19:16 +0100
Subject: [R] How Can I change the acf's plot type?
In-Reply-To: <200511281114.jASBEJHs010577@hypatia.math.ethz.ch>
References: <200511281114.jASBEJHs010577@hypatia.math.ethz.ch>
Message-ID: <17291.980.422264.141114@stat.math.ethz.ch>

>>>>> "$A9cPG(B" == $A9cPG(B  <guangxing at ict.ac.cn>
>>>>>     on Mon, 28 Nov 2005 19:16:31 +0800 writes:

    $A9cPG(B> Oops,I got it!
    >> plot(acf(x,plot=false),type="l",col="red")

almost, 
but not if you falsely write 'false' instead of 'FALSE' !

    $A9cPG(B> will be OK!

    $A9cPG(B> Thank you very much!


    $A9cPG(B> ======= 2005-11-28 18:09:03 $ADzTZ at 4PEVPP45@#:(B=======

    >> On Mon, 28 Nov 2005, [gb2312] $A9cPG(B wrote:

[ pretty neat how I see chinese letters in my mail reader
  {Emacs+VM "of course"} and even named citation automagically
  works... ]

Martin Maechler, ETH Zurich


    >>> In the R Document, the usage of the acf() is as follow:
    >>> acf(x, lag.max = NULL, type = c("correlation",
    >>> "covariance", "partial"), plot = TRUE, na.action =
    >>> na.fail, demean = TRUE, ...)  But now I want to get the
    >>> result picture like: plot(x,type="l") or
    >>> plot(x,type="p") How can I do this with acf function?
    >>  From the very same help page
    >> 
    >> The generic function 'plot' has a method for objects of
    >> class '"acf"'.
    >> 
    >> so use that directly: it has a 'type' parameter.
    >> 
    >> -- 
    >> Brian D. Ripley, ripley at stats.ox.ac.uk Professor of
    >> Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
    >> University of Oxford, Tel: +44 1865 272861 (self) 1 South
    >> Parks Road, +44 1865 272866 (PA) Oxford OX1 3TG, UK Fax:
    >> +44 1865 272595

    $A9cPG(B> = = = = = = = = = = = = = = = = = = = =
			

    $A9cPG(B> $A!!!!!!!!!!!!!!!!VB at q#!(B
 
				 
    $A9cPG(B> $A!!!!!!!!!!!!!!!!9cPG!!!!!!!!!!!!!!!!(B
    $A9cPG(B> guangxing at ict.ac.cn $A!!!!!!!!!!!!!!!!!!!!(B2005-11-28

    $A9cPG(B> ______________________________________________
    $A9cPG(B> R-help at stat.math.ethz.ch mailing list
    $A9cPG(B> https://stat.ethz.ch/mailman/listinfo/r-help
    $A9cPG(B> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From leog at anicca-vijja.de  Mon Nov 28 14:24:48 2005
From: leog at anicca-vijja.de (=?ISO-8859-1?Q?Leo_G=FCrtler?=)
Date: Mon, 28 Nov 2005 14:24:48 +0100
Subject: [R] analzying multiple variables (dv) in a sequence by using
 fit.mult.impute together with a MICE.object
Message-ID: <438B0520.5070104@anicca-vijja.de>

Dear list members,

my problem is to analyze multiple variables by using a simple loop.
Without a loop, no problem:

fit1 <- fit.mult.impute(variable_from_MIC_datset ~ group, fitter=ols, 
xtrans=imp.dmat)

Without a multiple imputation data set, that works:

vars <- c(
"lverb.ona", "l2", "lalles.ona", "vl", "notdmer.ona", "pisalern.ona", 
"lern",
"pswa", "pswn", "pska", "pskd", "pskt", "pkoo", "pkom", "isall", "vorwisse",
"im", "ke", "sb", "an", "akog", "aemo", "amb", "stru")

for(i in vars)
{  lm(dmat[,i]~ group)  }

However, I do not know how to specifiy the following correctly with the 
MICE object:

for(i in vars)
{
  fit1 <- fit.mult.impute(?????? ~ group, fitter=ols, xtrans=imp.dmat)
}

Looking into the structure of MICE objects, each of the x multiple 
imputed datasets is stored in a data.frame within a list. That list is 
named after the respective variable.
If I specify just the variable like

imp.dmat$imp$var_name

that does not work, because it is just the name of the list element. 
Otherwise by specifying one of the columns with the list element, the 
multiple datasets would be missed.
I thought about using eval() or expr() but without real succes.

Every hint is appreciated,

best regards

leo g??rtler



From marc.kirchner at iwr.uni-heidelberg.de  Mon Nov 28 14:38:42 2005
From: marc.kirchner at iwr.uni-heidelberg.de (Marc Kirchner)
Date: Mon, 28 Nov 2005 13:38:42 +0000
Subject: [R] background computation, &
In-Reply-To: <438AF6C4.3050304@telenet.be>
References: <438AF6C4.3050304@telenet.be>
Message-ID: <20051128133838.GG6153@iwr.uni-heidelberg.de>

Hey Koen,

> Is there a way to process R commands in the background other then using
> R in batch mode? I'm looking for the equivalent of the & operator on the 
> *nix commandline.

Well, that's not R-specific, but what I do is to use "screen", which is
a handy little program that emulates multiple screens in one console.

So you start "screen", fire up R, work with it as usual; then with
Ctrl-a-d you can detach from the screen and do whatever you want
before reattaching with "screen -r".

More info with "man screen". :)

Best,
Marc

-- 
========================================================
Dipl. Inform. Med. Marc Kirchner
Interdisciplinary Centre for Scientific Computing (IWR)
Multidimensional Image Processing
INF 368
University of Heidelberg
D-69120 Heidelberg
Tel: ++49-6221-54 87 97
Fax: ++49-6221-54 88 50
marc.kirchner at iwr.uni-heidelberg.de

-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051128/4bb3e6b2/attachment.bin

From claus.atzenbeck at freenet.de  Mon Nov 28 14:57:28 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Mon, 28 Nov 2005 14:57:28 +0100 (CET)
Subject: [R] Games-Howell, Gabriel, Hochberg
Message-ID: <Pine.OSX.4.61.0511281451470.4752@rgnb-d9b8743f.pool.mediaways.net>

Hello,

I read a book about statistics in psychology. The authors use SPSS. They
talk about post hoc tests after ANOVA finds significant effects:

    - Gabriel's procedure (for equal or slightly different sample sizes)
    - Hochberg's GT2 (for different sample sizes)
    - Games-Howell procedure (for populations with unequal variances)

I could not find them in R. Do they not exist in R or are there any
equivalents?

I know that I can use Tukey HSD if the sample sizes are equal and the
variances are homogeneous.

Thanks!
Claus



From dieter.menne at menne-biomed.de  Mon Nov 28 15:02:25 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 28 Nov 2005 14:02:25 +0000 (UTC)
Subject: [R] Xemacs
References: <539.1133178563@www82.gmx.net>
Message-ID: <loom.20051128T150145-69@post.gmane.org>

Mahdi Osman <m_osm <at> gmx.net> writes:

> How can I stop my xemacs from lauching R whenever it starts? I want to use
> the usual M-x R instead. 
> 
> This is something I have to edit in the init file. 

Remove 

(start-R) 

at the end of the init file (assuming you use the John-Fox Version).

Dieter



From svenknueppel at reilich.net  Mon Nov 28 15:26:49 2005
From: svenknueppel at reilich.net (svenknueppel@reilich.net)
Date: Mon, 28 Nov 2005 15:26:49 +0100 (CET)
Subject: [R] How define global Variable?
Message-ID: <20051128142649.C0CAC718794@basicbox6.server-home.net>

Hello,

I try to define a global variable.

My example:

R> a <- "old"
R> test <- function () { a <- "new" }
R> test()
R> a # shoud be "new"

This doesn't work. I would like to modify the variable "a" in a
procedure. How can I do that.

Thank you for helping.

Sven Knüppel (Germany-Berlin)



From guangxing at ict.ac.cn  Mon Nov 28 15:32:46 2005
From: guangxing at ict.ac.cn (GuangXing)
Date: Mon, 28 Nov 2005 22:32:46 +0800
Subject: [R] How Can I change the acf's plot type?
Message-ID: <200511281430.jASEUX7w004466@hypatia.math.ethz.ch>

I am sososo sorry for my inattention to my EMail's format£¡
I have change that.
Thank you very much again!
======= 2005-11-28 21:19:16 Write£º=======

>>>>>> "$A9cPG(B" == $A9cPG(B  <guangxing at ict.ac.cn>
>>>>>>     on Mon, 28 Nov 2005 19:16:31 +0800 writes:
>
>    $A9cPG(B> Oops,I got it!
>    >> plot(acf(x,plot=false),type="l",col="red")
>
>almost, 
>but not if you falsely write 'false' instead of 'FALSE' !
>
>    $A9cPG(B> will be OK!
>
>    $A9cPG(B> Thank you very much!
>
>
>    $A9cPG(B> ======= 2005-11-28 18:09:03 $ADzTZ at 4PEVPP45@#:(B=======
>
>    >> On Mon, 28 Nov 2005, [gb2312] $A9cPG(B wrote:
>
>[ pretty neat how I see chinese letters in my mail reader
>  {Emacs+VM "of course"} and even named citation automagically
>  works... ]
>
>Martin Maechler, ETH Zurich
>
>
>    >>> In the R Document, the usage of the acf() is as follow:
>    >>> acf(x, lag.max = NULL, type = c("correlation",
>    >>> "covariance", "partial"), plot = TRUE, na.action =
>    >>> na.fail, demean = TRUE, ...)  But now I want to get the
>    >>> result picture like: plot(x,type="l") or
>    >>> plot(x,type="p") How can I do this with acf function?
>    >>  From the very same help page
>    >> 
>    >> The generic function 'plot' has a method for objects of
>    >> class '"acf"'.
>    >> 
>    >> so use that directly: it has a 'type' parameter.
>    >> 
>    >> -- 
>    >> Brian D. Ripley, ripley at stats.ox.ac.uk Professor of
>    >> Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
>    >> University of Oxford, Tel: +44 1865 272861 (self) 1 South
>    >> Parks Road, +44 1865 272866 (PA) Oxford OX1 3TG, UK Fax:
>    >> +44 1865 272595
>
>    $A9cPG(B> = = = = = = = = = = = = = = = = = = = =
>			
>
>    $A9cPG(B> $A!!!!!!!!!!!!!!!!VB at q#!(B
> 
>				 
>    $A9cPG(B> $A!!!!!!!!!!!!!!!!9cPG!!!!!!!!!!!!!!!!(B
>    $A9cPG(B> guangxing at ict.ac.cn $A!!!!!!!!!!!!!!!!!!!!(B2005-11-28
>
>    $A9cPG(B> ______________________________________________
>    $A9cPG(B> R-help at stat.math.ethz.ch mailing list
>    $A9cPG(B> https://stat.ethz.ch/mailman/listinfo/r-help
>    $A9cPG(B> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

= = = = = = = = = = = = = = = = = = = =
				 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡GuangXing
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-28



From kristel.joossens at econ.kuleuven.be  Mon Nov 28 15:34:47 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Mon, 28 Nov 2005 15:34:47 +0100
Subject: [R] How define global Variable?
In-Reply-To: <20051128142649.C0CAC718794@basicbox6.server-home.net>
References: <20051128142649.C0CAC718794@basicbox6.server-home.net>
Message-ID: <438B1587.7090007@econ.kuleuven.be>

The problem is that the a is within the function
You can easily solve this by

test <- function () { a <- "new"; return(a) }
a=test()

Best regards,
Kristel


(or test <- function () { return(a <- "new")})

svenknueppel at reilich.net wrote:
> Hello,
> 
> I try to define a global variable.
> 
> My example:
> 
> R> a <- "old"
> R> test <- function () { a <- "new" }
> R> test()
> R> a # shoud be "new"
> 
> This doesn't work. I would like to modify the variable "a" in a
> procedure. How can I do that.
> 
> Thank you for helping.
> 
> Sven Kn??ppel (Germany-Berlin)
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From dimitris.rizopoulos at med.kuleuven.be  Mon Nov 28 15:39:26 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 28 Nov 2005 15:39:26 +0100
Subject: [R] How define global Variable?
References: <20051128142649.C0CAC718794@basicbox6.server-home.net>
Message-ID: <010e01c5f429$8828f830$0540210a@www.domain>

try

a <- "old"
test <- function () { assign("a", "new", envir = .GlobalEnv) }
test()
a


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: <svenknueppel at reilich.net>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, November 28, 2005 3:26 PM
Subject: [R] How define global Variable?


> Hello,
>
> I try to define a global variable.
>
> My example:
>
> R> a <- "old"
> R> test <- function () { a <- "new" }
> R> test()
> R> a # shoud be "new"
>
> This doesn't work. I would like to modify the variable "a" in a
> procedure. How can I do that.
>
> Thank you for helping.
>
> Sven Kn??ppel (Germany-Berlin)
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From B.Rowlingson at lancaster.ac.uk  Mon Nov 28 15:41:19 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 28 Nov 2005 14:41:19 +0000
Subject: [R] How define global Variable?
In-Reply-To: <20051128142649.C0CAC718794@basicbox6.server-home.net>
References: <20051128142649.C0CAC718794@basicbox6.server-home.net>
Message-ID: <438B170F.2040903@lancaster.ac.uk>

svenknueppel at reilich.net wrote:

> R> a <- "old"
> R> test <- function () { a <- "new" }
> R> test()
> R> a # shoud be "new"
> 
> This doesn't work. I would like to modify the variable "a" in a
> procedure. How can I do that.

  You may like to modify the variable, but who else wants you to?

Functions should have zero side effects whenever possible. Wanting to 
muck with global variables is a big red flag that something is wrong 
with your program. It will become hard to debug or follow what is going 
on. Imagine, in six weeks time you look at:

  a = "old"
  test()
  if (a == "new"){
    doSomething()
  }

  - well, its not obvious that 'a' could possibly have changed to "new". 
Sure you could look at test() and see, but then test() could call 
something else that calls something else and then somewhere else 'a' is 
set. It can make for very very messy code.

  The solution is to return anything that changes. Example:

  a = "old"

  test=function(){return(list(a="new"))}

  ttt = test()
  a = ttt$a

  That's probably the recommended way of returning multiple things from 
a function too - wrap them in a list and get them. Modifying global 
variables is very rarely the Right Thing.

  I'm sure someone will come up with a solution but it'll probably 
involve frames and environments and other messy magic language stuff you 
really dont want to get into. Keep It Simple, Sunshine.

Barry



From JAROSLAW.W.TUSZYNSKI at saic.com  Mon Nov 28 15:41:28 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Mon, 28 Nov 2005 09:41:28 -0500
Subject: [R] finding peaks in a simple dataset with R
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4166@us-arlington-0668.mail.saic.com>

Try,

  # work directly with data from the input files
  directory  = system.file("Test", package = "caMassClass")
  X = msc.rawMS.read.csv(directory, "IMAC_normal_.*csv")
  Peaks = msc.peaks.find(X) # Find Peaks
  cat(nrow(Peaks), "peaks were found in", Peaks[nrow(Peaks),2], "files.\n")
  stopifnot( nrow(Peaks)==424 )

On my data to see that every thing works OK. Than I would convert your
"input.dat" to CSV format:

2.00, 233
2.04, 220
...
11.60, 540
12.00, 600   <-- a peak!
12.04, 450
...

On Windows machine, you can do it by opening your file in excel, and saving
it as CSV. Or possibly using test editor to replace ' ' with ', '. Than the
script

  X = msc.rawMS.read.csv('.', "Input.csv")
  Peaks = msc.peaks.find(X)
  cat(nrow(Peaks), "peaks were found in", Peaks  [nrow(Peaks),2],
"files.\n")

 should work.

Other way, is to try:

  X = read.table("input.dat", header=TRUE)
  Y = X[,2]
  rownames(Y) = signif(X[,1], 6)
  Peaks = msc.peaks.find(Y)

Which casts your data in correct format, described in documentation as:
"Spectrum data either in matrix format [nFeatures x nSamples] or in 3D array
format [nFeatures x nSamples x nCopies]. Row names (rownames(X)) store M/Z
mass of each row."

I hope one of those solutions works for you.

Good Luck.

Jarek Tuszynski

-----Original Message-----
From: dylan.beaudette at gmail.com [mailto:dylan.beaudette at gmail.com] 
Sent: Wednesday, November 23, 2005 5:47 PM
To: r-help at stat.math.ethz.ch
Cc: Tuszynski, Jaroslaw W.
Subject: Re: [R] finding peaks in a simple dataset with R


On Wednesday 23 November 2005 10:15 am, Tuszynski, Jaroslaw W. wrote:
> >> I am looking for some way to locate peaks in a simple x,y data set.
>
> See my 'msc.peaks.find' function in 'caMassClass', it has a simple 
> peak finding algorithm.
>
> Jarek Tuszynski
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

Jarek,

Thanks for the tip. I was able to install the caMassClass package and all of

its dependancies. In addition, I was able to run the examples on the manual 
pages.

However, The format of the input data to the 'msc.peaks.find' function is
not 
apparent to me. In its simplest form, my data looks something like this:

2.00 233
2.04 220
...
11.60 540
12.00 600   <-- a peak!
12.04 450
...

Here is an example R session, trying out the function you suggested:

#importing my data like this:
X <- read.table("input.dat", header=TRUE)

#from the example:
Peaks = msc.peaks.find(X)

#errors with:
Error in sort(x, partial = unique(c(lo, hi))) :
        'x' must be atomic


Also: I have tried one of the functions ( 'getPeaks' ) listed on the 
'msc.peaks.find' manual page, however I am still having a problem with the 
format of my data vs. what the function is expecting.

#importing my data like this:
X <- read.table("input.dat", header=TRUE)

#setup an output file for peak information
peakfile <- paste("peakinfo.csv", sep="/")

#run the analysis:
getPeaks(X,peakfile)

#errors with:
Error in area/max(area) : non-numeric argument to binary operator In
addition: Warning message: no finite arguments to max; returning -Inf

any ideas would be greatly appreciated!

-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341



From anthony at stat.sdu.dk  Mon Nov 28 15:40:03 2005
From: anthony at stat.sdu.dk (Gichangi, Anthony)
Date: Mon, 28 Nov 2005 15:40:03 +0100
Subject: [R] How define global Variable?
References: <20051128142649.C0CAC718794@basicbox6.server-home.net>
Message-ID: <001e01c5f429$9f4b5d00$cb83e182@yatesvmware>

In your current definitions a can not change value to
"new" unless you type  a<-test(). If you want the
results of the test to be global then you add something like this

test <-function()a<<-"new"

This will always replace the existing value of a once
you type test()



regards
Anthony----- Original Message ----- 
From: <svenknueppel at reilich.net>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, November 28, 2005 3:26 PM
Subject: [R] How define global Variable?


> Hello,
>
> I try to define a global variable.
>
> My example:
>
> R> a <- "old"
> R> test <- function () { a <- "new" }
> R> test()
> R> a # shoud be "new"
>
> This doesn't work. I would like to modify the variable "a" in a
> procedure. How can I do that.
>
> Thank you for helping.
>
> Sven Kn??ppel (Germany-Berlin)
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Nov 28 15:46:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 28 Nov 2005 15:46:03 +0100
Subject: [R] How define global Variable?
In-Reply-To: <20051128142649.C0CAC718794@basicbox6.server-home.net>
References: <20051128142649.C0CAC718794@basicbox6.server-home.net>
Message-ID: <438B182B.1070800@statistik.uni-dortmund.de>

svenknueppel at reilich.net wrote:

> Hello,
> 
> I try to define a global variable.

You should not do that unless you really know why you want it this way. 
You probably want to read the R Language Definition manual.

Anyway, read ?assign if you want to proceed doing dangerous things.

Uwe Ligges



> My example:
> 
> R> a <- "old"
> R> test <- function () { a <- "new" }
> R> test()
> R> a # shoud be "new"
> 
> This doesn't work. I would like to modify the variable "a" in a
> procedure. How can I do that.
> 
> Thank you for helping.
> 
> Sven Kn??ppel (Germany-Berlin)
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f_bresson at yahoo.fr  Mon Nov 28 16:22:25 2005
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Mon, 28 Nov 2005 16:22:25 +0100 (CET)
Subject: [R] optimization with inequalities
Message-ID: <20051128152226.9786.qmail@web26801.mail.ukl.yahoo.com>

I have to estimate the following model for several
group of observations :

 y(1-y) = p[1]*(x^2-y) + p[2]*y*(x-1) + p[3]*(x-y)

with constraints :
 p[1]+p[3] >= 1
 p[1]+p[2]+p[3]+1 >= 0
 p[3] >= 0

I use the following code :
 func <- sum((y(1-y) - p[1]*(x^2-y) + p[2]*y*(x-1) +
p[3]*(x-y))^2)
 estim <- optim( c(1,0,0),func, method="L-BFGS-B" ,
lower=c(1-p[3], -p[1]-p[3]-1, 0) )

and for some group of observations, I observe that the
estimated parameters don't respect the constraints,
espacially the first. Where's the problem please ?



From ggrothendieck at gmail.com  Mon Nov 28 16:24:27 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Nov 2005 10:24:27 -0500
Subject: [R] How define global Variable?
In-Reply-To: <20051128142649.C0CAC718794@basicbox6.server-home.net>
References: <20051128142649.C0CAC718794@basicbox6.server-home.net>
Message-ID: <971536df0511280724s1d25d7b3jc6c4637b4382ab3f@mail.gmail.com>

See:

http://tolstoy.newcastle.edu.au/~rking/R/help/05/11/15737.html

and the responses.

On 11/28/05, svenknueppel at reilich.net <svenknueppel at reilich.net> wrote:
> Hello,
>
> I try to define a global variable.
>
> My example:
>
> R> a <- "old"
> R> test <- function () { a <- "new" }
> R> test()
> R> a # shoud be "new"
>
> This doesn't work. I would like to modify the variable "a" in a
> procedure. How can I do that.
>
> Thank you for helping.
>
> Sven Kn??ppel (Germany-Berlin)
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Mon Nov 28 16:27:38 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Mon, 28 Nov 2005 10:27:38 -0500
Subject: [R] obtaining a ROC curve
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4167@us-arlington-0668.mail.saic.com>


See, my old post at 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/61879.html
 to see a list of ROC related packages and functions. One of them should
work well for your application. All of them take data in the form:
 - x - real number value returned by the classifier
 - y - true labels / classes (only 2 levels allowed)

In Case of classification trees it might be hard to get your hands on the
"x" since your function might only return binary labels (classes), what
gives you only 3 points on your ROC ( (0,0), (1,1) and one point calculated
from returned labels). But you might find function that can return
probabilities of each sample. 

For example if you use 'rpart' than:
    model = rpart( ytrain~., data = data.frame(cbind(ytrain,xtrain)), ...)
    Prob  = predict(model, newdata=xtest, type="prob")
    x = Prob[,1]
    y = ytest

 will give you needed probabilities instead of classes.

Jarek Tuszynski

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:anjali_karve at yahoo.com] 
Sent: Friday, November 25, 2005 4:23 PM
To: r-help at stat.math.ethz.ch
Subject: [R] obtaining a ROC curve


Hello,
  
  I have a classification tree. I want to obtain a ROC curve for this test.
What is the easiest way to obtain one?
  
  -Anjali
  

		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From David.Ecotiere at equipement.gouv.fr  Mon Nov 28 13:26:15 2005
From: David.Ecotiere at equipement.gouv.fr (David.Ecotiere@equipement.gouv.fr)
Date: Mon, 28 Nov 2005 13:26:15 +0100
Subject: [R] window in a "waitbar" style ?
Message-ID: <AFA36FE36ADDBF48B1E9E802E6060FE33C4C9F@ct57-mel-lrs.cete-est.i2>

Hello,

Is there any function to display a "waitbar" window (using tclTk ?)

Thanks for any help !

D. Ecoti??re



From p.dalgaard at biostat.ku.dk  Mon Nov 28 17:10:19 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Nov 2005 17:10:19 +0100
Subject: [R] optimization with inequalities
In-Reply-To: <20051128152226.9786.qmail@web26801.mail.ukl.yahoo.com>
References: <20051128152226.9786.qmail@web26801.mail.ukl.yahoo.com>
Message-ID: <x2r790ois4.fsf@viggo.kubism.ku.dk>

Florent Bresson <f_bresson at yahoo.fr> writes:

> I have to estimate the following model for several
> group of observations :
> 
>  y(1-y) = p[1]*(x^2-y) + p[2]*y*(x-1) + p[3]*(x-y)
> 
> with constraints :
>  p[1]+p[3] >= 1
>  p[1]+p[2]+p[3]+1 >= 0
>  p[3] >= 0
> 
> I use the following code :
>  func <- sum((y(1-y) - p[1]*(x^2-y) + p[2]*y*(x-1) +
> p[3]*(x-y))^2)
>  estim <- optim( c(1,0,0),func, method="L-BFGS-B" ,
> lower=c(1-p[3], -p[1]-p[3]-1, 0) )
> 
> and for some group of observations, I observe that the
> estimated parameters don't respect the constraints,
> espacially the first. Where's the problem please ?

If you think the boundaries in lower=c(....) are recomputed as the
iteration progresses, you're wrong. L-BGFS-B does box constraints
only. Instead parametrize using

q1=p1+p3
q2=p1+p2+p3
q3=p3

which is easily inverted to get the p's from the q's. Then optimize as
a function of q1..q3, substituting the inversion in the expression for
func (which btw needs to be a _function_), using the relevant box
constraints. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Mon Nov 28 17:12:31 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Nov 2005 16:12:31 +0000 (GMT)
Subject: [R] optimization with inequalities
In-Reply-To: <20051128152226.9786.qmail@web26801.mail.ukl.yahoo.com>
References: <20051128152226.9786.qmail@web26801.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0511281608180.5239@gannet.stats>

On Mon, 28 Nov 2005, Florent Bresson wrote:

> I have to estimate the following model for several
> group of observations :
>
> y(1-y) = p[1]*(x^2-y) + p[2]*y*(x-1) + p[3]*(x-y)
>
> with constraints :
> p[1]+p[3] >= 1
> p[1]+p[2]+p[3]+1 >= 0
> p[3] >= 0
>
> I use the following code :
> func <- sum((y(1-y) - p[1]*(x^2-y) + p[2]*y*(x-1) +
> p[3]*(x-y))^2)
> estim <- optim( c(1,0,0),func, method="L-BFGS-B" ,
> lower=c(1-p[3], -p[1]-p[3]-1, 0) )
>
> and for some group of observations, I observe that the
> estimated parameters don't respect the constraints,
> espacially the first. Where's the problem please ?

User mis-reading the help page!

L-BFGS-B handles `box constraints', not linear inequality constraints.
You can reparametrize to make these box constraints: use p[3], p[1]+p[3] 
and p[1]+p[2]+p[3] are variables.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Mon Nov 28 17:30:43 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Nov 2005 11:30:43 -0500
Subject: [R] optimization with inequalities
In-Reply-To: <20051128152226.9786.qmail@web26801.mail.ukl.yahoo.com>
References: <20051128152226.9786.qmail@web26801.mail.ukl.yahoo.com>
Message-ID: <971536df0511280830o48497052m2c59c963c0f0a411@mail.gmail.com>

If I understand this correctly the variables over which
you are optimizing are p[1], p[2] and p[3] whereas x and y
are fixed and known during the optimization.  In that case its
a linear programming problem and you could use the lpSolve
library which would allow the explicit specification of the
constraints.

On 11/28/05, Florent Bresson <f_bresson at yahoo.fr> wrote:
> I have to estimate the following model for several
> group of observations :
>
>  y(1-y) = p[1]*(x^2-y) + p[2]*y*(x-1) + p[3]*(x-y)
>
> with constraints :
>  p[1]+p[3] >= 1
>  p[1]+p[2]+p[3]+1 >= 0
>  p[3] >= 0
>
> I use the following code :
>  func <- sum((y(1-y) - p[1]*(x^2-y) + p[2]*y*(x-1) +
> p[3]*(x-y))^2)
>  estim <- optim( c(1,0,0),func, method="L-BFGS-B" ,
> lower=c(1-p[3], -p[1]-p[3]-1, 0) )
>
> and for some group of observations, I observe that the
> estimated parameters don't respect the constraints,
> espacially the first. Where's the problem please ?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From nina.klar at ufz.de  Mon Nov 28 17:36:53 2005
From: nina.klar at ufz.de (nina klar)
Date: Mon, 28 Nov 2005 17:36:53 +0100
Subject: [R] GLMM: measure for significance of random variable?
Message-ID: <004601c5f439$f0cb5da0$8b33418d@NinaLap>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051128/04971b9f/attachment.pl

From spencer.graves at pdf.com  Mon Nov 28 18:01:03 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 28 Nov 2005 09:01:03 -0800
Subject: [R] anova.gls from nlme on multiple arguments within a function
 fails
In-Reply-To: <Pine.LNX.4.61.0511281016270.3364@gannet.stats>
References: <1132226230.16749.11.camel@pallas.abo.fi> <438A36DB.80301@pdf.com>
	<Pine.LNX.4.61.0511281016270.3364@gannet.stats>
Message-ID: <438B37CF.2050204@pdf.com>

Dear Prof. Ripley:

	  Thanks very much.  I tried several superficially similar things but 
not either of the solutions you suggest.

	  Best Wishes,
	  spencer graves

Prof Brian Ripley wrote:
> The error is in anova.gls(): traceback() would have told you that was 
> involved.  It ends
> 
> do.call("anova.lme", as.list(match.call()[-1]))
> 
> and that call needs to be evaluated in the parent, not in the body of 
> anova.lme.  Several similar errors (e.g. in the update methods) in 
> package nlme have been corrected over the years.
> 
> Replacing anova() by anova.lme() in dummy() works.
> 
> If you want to do this sort of thing more generally (e.g. messing with 
> the contents of '...'), the elegant way is
> 
> Call <- match.call()
> Call[[1]] <- as.name("anova.lme")
> eval.parent(Call)
> 
> and that works here.
> 
> Since at some later point substitute() is used to find the arguments, 
> here you don't want to use do.call() with the evaluated arguments, which 
> is the way it is intended to be used.  Similarly, anova.lme(object, ...) 
> is not what you want.
> 
> 
> On Sun, 27 Nov 2005, Spencer Graves wrote:
> 
>>       You've posed an excellent question with simple and elegant,
>> reproducible example.  I've seen no replies, so I will attempt a partial
>> response.  RSiteSearch("lexical scoping") produced some potentially
>> useful comments (e.g.,
>> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/37769.html), but nothing
>> that allowed me to work around the problem.
>>
>>       The following modification of your example makes it clear that
>> "anova.lme" (called by "anova.gls") choked on the second argument not
>> the first:
> 
> 
> It actually chokes on both.
> 
>> > dummy2 <- function(obj)
>> +   {
>> +     obj2 <- obj[[2]]
>> +     anova(obj[[1]], obj2)
>> +   }
>> > dummy2(list(fm1, fm2))
>> Error in anova.lme(object = obj[[1]], obj2) :
>>     object "obj2" not found
>>
>>       The following helped isolate this further to "dots <- 
>> list(...)", the
>> second line in "anova.lme":
>>
>> debug(anova.lme)
>> dummy2(list(fm1, fm2))
>>
>>       I don't know why your example fails, especially "anova.lm" worked.
>> Also, there should be a way  to use something like "assign" to work
>> around this problem, but nothing I tried worked.
>>
>>       I know this is not a complete reply, but I hope it helps.
>>       spencer graves
>>
>> Markus Jantti wrote:
>>
>>> Dear All --
>>>
>>> I am trying to use within a little table producing code an anova
>>> comparison of two gls fitted objects, contained in a list of such
>>> object, obtained using nlme function gls.
>>> The anova procedure fails to locate the second of the objects.
>>>
>>> The following code, borrowed from the help page of anova.gls,
>>> exemplifies:
>>> --------------- start example code ---------------
>>> library(nlme)
>>>
>>> ## stolen from example(anova.gls)
>>> # AR(1) errors within each Mare
>>> fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
>>>            correlation = corAR1(form = ~ 1 | Mare))
>>> anova(fm1)
>>> # variance changes with a power of the absolute fitted values?
>>> fm2 <- update(fm1, weights = varPower())
>>> anova(fm1, fm2)
>>>
>>> ## now define a little function
>>> dummy <- function(obj)
>>>   {
>>>     anova(obj[[1]], obj[[2]])
>>>   }
>>> dummy(list(fm1, fm2))
>>>
>>> ## compare with what happens in anova.lm:
>>>
>>> lm1 <- lm(follicles ~ sin(2*pi*Time), Ovary)
>>> lm2 <- lm(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary)
>>> dummy(list(lm1, lm2))
>>> ------------- end example code ------------------
>>>
>>> It is not the end of the world: I can easily work around this.
>>> But it would be nice to know why this does not work.
>>>
>>> Digging around using options(error=recover) did not help my much, I'm
>>> afraid.
>>>
>>> Best,
>>>
>>> Markus
>>
>>
>> -- 
>> Spencer Graves, PhD
>> Senior Development Engineer
>> PDF Solutions, Inc.
>> 333 West San Carlos Street Suite 700
>> San Jose, CA 95110, USA
>>
>> spencer.graves at pdf.com
>> www.pdf.com <http://www.pdf.com>
>> Tel:  408-938-4420
>> Fax: 408-280-7915
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From costas.magnuse at gmail.com  Mon Nov 28 18:16:36 2005
From: costas.magnuse at gmail.com (Constantine Tsardounis)
Date: Mon, 28 Nov 2005 19:16:36 +0200
Subject: [R] Durbin-Watson Critical Values Tables
Message-ID: <30ddfdae0511280916y6ff349f0j56ddde244f40d33a@mail.gmail.com>

Hello to everyone!...

I would like to ask you if there is a way to extract the known
Durbin-Watson critical values (D_L and D_U - lower and upper limits)
within R, as we do in the DW statistic tables at the end of
Econometrics Textbooks, because packages "car" and "lmtest" seem to
provide only the p-value (it is useful, but I  would like to have the
exact critical vaues).

Thank you very much in advance,

Tsardounis Constantine



From georgia.chan at gmail.com  Mon Nov 28 19:27:32 2005
From: georgia.chan at gmail.com (Georgia Chan)
Date: Mon, 28 Nov 2005 18:27:32 +0000
Subject: [R] Graph plots
Message-ID: <c30d61d30511281027q23ed4aaemcd238b48cc57a23b@mail.gmail.com>

Dear R  people,
I am  currently experimenting with graph plotting using libraries
Rgraphviz, graph, etc. as needed by GeneTS (bioconductor project).

The plots I get use huge fonts to label the nodes and the edges
and that often makes the plot unreadable (overlapping edge labels etc).

The question is perhaps more related to graph attribute values of  Graphviz,
but I was hoping that there is a more straightforward way of making
graphs more readable within R.

Any advice/examples will be much appreciated.

Thanks in advance,
georgia



From tlumley at u.washington.edu  Mon Nov 28 19:30:44 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 28 Nov 2005 10:30:44 -0800 (PST)
Subject: [R] Looking for constrained optimisation code
In-Reply-To: <E1Egd6D-0003M3-00@bernie.ethz.ch>
References: <E1Egd6D-0003M3-00@bernie.ethz.ch>
Message-ID: <Pine.LNX.4.63a.0511281030170.9346@homer21.u.washington.edu>

On Mon, 28 Nov 2005, Hong Ooi wrote:
>
> Hi,
>
> I was just wondering if there was any available R code that could handle
> general constrained optimisation problems. At the moment I'm using
> nlminb and optim, both of which allow box constraints on the parameters,
> but ideally I'd like to be able to specify more general constraints on
> the solution space.
>

constrOptim() allows linear inequality constraints.

 	-thomas



From coxdavia at msu.edu  Mon Nov 28 19:36:21 2005
From: coxdavia at msu.edu (Davia Cox)
Date: Mon, 28 Nov 2005 13:36:21 -0500
Subject: [R] read.spss problem
Message-ID: <4F2C5827-A526-472C-8256-9CE2F9B83F05@msu.edu>

Hello,
I am having trouble reading an spss file into R. I have reset my  
working directory to the folder where this file is stored. This is  
what I've typed into R and the error message I received:

+ getwd()
[1] "/Users/daviacox/Graduate School/PLS 801"
 > read.spss("norwil.spss")
Error in read.spss("norwil.spss") : error reading portable-file  
dictionary
In addition: Warning message:
Expected variable count record

Please help, I don't have SPSS on my computer (I am re-analyzing some  
data for a statistics project) so I can't alter the file that way.

Thanks again for your help.

Davia

Davia S. Cox
Global Urban Studies Program
Graduate Assistant/Doctoral Student
305 Berkey Hall
East Lansing, MI 48825
517-353-5987 Office
517-353-6680 Fax
coxdavia at msu.edu

"One of the penalties for refusing to participate in politics is that  
you
end up being governed by your inferiors."
  -Plato



From martac21 at libero.it  Mon Nov 28 19:38:12 2005
From: martac21 at libero.it (Marta Colombo)
Date: Mon, 28 Nov 2005 19:38:12 +0100
Subject: [R] Robust fitting
Message-ID: <IQOH3O$7167E5D1DFB02C59B108538C94CE73D8@libero.it>

Good evening,I am Marta Colombo, student of "Politecnico di Milano". I'm studying Local Regression Techniques such as loess, smoothing splines and kernel smoothers. Choosing "symmetric" for the argument "family" in loess function it is possible to produce a robust estimate , in function smooth.spline and ksmooth I didn't find this possibility. Well, is there a way to produce a robust estimate using smoothing splines or kernel smoothers? And if the answer is no, why? I'm asking these questions because I need to know loess' advantages and disadvantages compared to other techniques. Thank you very much for attention,

Marta Colombo



From f.harrell at vanderbilt.edu  Mon Nov 28 20:57:53 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 28 Nov 2005 14:57:53 -0500
Subject: [R] using minor tickmarks with xYplot
In-Reply-To: <438AF4B5.1000301@fastmail.fm>
References: <438AF4B5.1000301@fastmail.fm>
Message-ID: <438B6141.20702@vanderbilt.edu>

Viet Nguyen wrote:
> Hi all,
> 
> I'm trying to make a plot with the function xYplot from package Hmisc in 
> R.  I would like to have minor tick-marks on the axis.  This should be a 
> common simple feature to have but I don't seem to find any discussion on 
> the topic.
> 
> Following is one of the things I tried and the error returned:
> 
> xYplot(y~x,data.frame(x=seq(1,10),y=runif(10)),minor.ticks=c(3.5,5.5))
> Error in panel(x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), y = 
> c(0.88172451662831,  :
>     object "gfun" not found

This is a bug in panel.xYplot.  Charles Thomas Dupont will fix it and 
give you the URL to our CVS area containing the new version, then you 
can do source('http://....') to override panel.xYplot until Thomas puts 
out a new version of Hmisc. -Frank

> 
> It's important that I use "xYplot" and not "plot" so function 
> minor.tick() is not useful.
> 
> Anything I can try?  Thanks in advance for your help.
> 
> vn
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From kristel.joossens at econ.kuleuven.ac.be  Mon Nov 28 20:29:10 2005
From: kristel.joossens at econ.kuleuven.ac.be (Kristel Joossens)
Date: Mon, 28 Nov 2005 20:29:10 +0100
Subject: [R] Robust fitting
In-Reply-To: <IQOH3O$7167E5D1DFB02C59B108538C94CE73D8@libero.it>
References: <IQOH3O$7167E5D1DFB02C59B108538C94CE73D8@libero.it>
Message-ID: <438B5A86.9010105@econ.kuleuven.ac.be>

http://www.maths.lth.se/help/R/.R/library/R.basic/html/robust.smooth.spline.html

Best regards,
Kristel

Marta Colombo wrote:
> Good evening,I am Marta Colombo, student of "Politecnico di Milano". I'm studying Local Regression Techniques such as loess, smoothing splines and kernel smoothers. Choosing "symmetric" for the argument "family" in loess function it is possible to produce a robust estimate , in function smooth.spline and ksmooth I didn't find this possibility. Well, is there a way to produce a robust estimate using smoothing splines or kernel smoothers? And if the answer is no, why? I'm asking these questions because I need to know loess' advantages and disadvantages compared to other techniques. Thank you very much for attention,
> 
> Marta Colombo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From jfontain at free.fr  Mon Nov 28 20:37:50 2005
From: jfontain at free.fr (Jean-Luc Fontaine)
Date: Mon, 28 Nov 2005 20:37:50 +0100
Subject: [R] AIC and BIC from arima()
Message-ID: <438B5C8E.3030509@free.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

My ultimate goal is to best fit time series by comparing AICs and BICs
(as in Bayesian) from arima() and nnet().
I looked at the arima.R source code, but I am afraid I do not
understand it.
What I only miss really is the number of parameters p, where: AIC =
n*log(S/n) + 2*p
with S the squared residuals and n the number of observations.

Can I get p from arima() (for both non and seasonal cases) result?
I am obviously not an expert in the matter, so please accept my
apologies if this is a stupid question...

Many thanks in advance,

- --
Jean-Luc Fontaine  http://jfontain.free.fr/
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.1 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFDi1yOkG/MMvcT1qQRAgLkAJ49RWVGG0h2plx3OOA8x1pIQdimXgCfbJ2w
kRog3Jj6q5uUHygyLn6Rbuo=
=iSXZ
-----END PGP SIGNATURE-----



From gunter.berton at gene.com  Mon Nov 28 20:42:45 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 28 Nov 2005 11:42:45 -0800
Subject: [R] Robust fitting
In-Reply-To: <IQOH3O$7167E5D1DFB02C59B108538C94CE73D8@libero.it>
Message-ID: <200511281942.jASJgjEo004369@ohm.gene.com>

Note:

As I believe Brian Ripley pointed out in his MASS book, loess may not be as
resistant to outliers (which is one aspect of robustness; robustness of
efficiency is another) as you think. The problem is that it starts off with
LS estimates and these can be so distorted by unusual values that the
reweighting cannot properly recover; i.e. convergence is to a local minimum
far from the desired global one. You might wish to read the documentation
for rlm() (in MASS, the package) and the appropriate sections of MASS, the
book. 

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marta Colombo
> Sent: Monday, November 28, 2005 10:38 AM
> To: R help
> Subject: [R] Robust fitting
> 
> Good evening,I am Marta Colombo, student of "Politecnico di 
> Milano". I'm studying Local Regression Techniques such as 
> loess, smoothing splines and kernel smoothers. Choosing 
> "symmetric" for the argument "family" in loess function it is 
> possible to produce a robust estimate , in function 
> smooth.spline and ksmooth I didn't find this possibility. 
> Well, is there a way to produce a robust estimate using 
> smoothing splines or kernel smoothers? And if the answer is 
> no, why? I'm asking these questions because I need to know 
> loess' advantages and disadvantages compared to other 
> techniques. Thank you very much for attention,
> 
> Marta Colombo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From fisher at plessthan.com  Mon Nov 28 22:18:14 2005
From: fisher at plessthan.com (Dennis Fisher)
Date: Mon, 28 Nov 2005 13:18:14 -0800
Subject: [R] Use of axis() in conjunction with plot(..., axes=F)
In-Reply-To: <200310071009.h97A54qM000048@stat.math.ethz.ch>
References: <200310071009.h97A54qM000048@stat.math.ethz.ch>
Message-ID: <7887D7BF-2F4C-4656-9967-E1EE985CEDA9@plessthan.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051128/e642831d/attachment.pl

From ripley at stats.ox.ac.uk  Mon Nov 28 22:19:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Nov 2005 21:19:14 +0000 (GMT)
Subject: [R] AIC and BIC from arima()
In-Reply-To: <438B5C8E.3030509@free.fr>
References: <438B5C8E.3030509@free.fr>
Message-ID: <Pine.LNX.4.61.0511282100570.8810@gannet.stats>

On Mon, 28 Nov 2005, Jean-Luc Fontaine wrote:

> My ultimate goal is to best fit time series by comparing AICs and BICs
> (as in Bayesian) from arima() and nnet().

Whoa!  nnet() does not do maximum likelihood fitting so AIC and BIC are 
not even defined.

On the other hand, ?WWWusage has an example of choosing an ARIMA fit by 
AIC.

> I looked at the arima.R source code, but I am afraid I do not
> understand it.
> What I only miss really is the number of parameters p, where: AIC =
> n*log(S/n) + 2*p
> with S the squared residuals and n the number of observations.
>
> Can I get p from arima() (for both non and seasonal cases) result?

By reading the help page:

     coef: a vector of AR, MA and regression coefficients, which can be
           extracted by the 'coef' method.

so length(fit$coef) will tell you how many parameters you have fitted,
and if you read on

      aic: the AIC value corresponding to the log-likelihood. Only valid
           for 'method = "ML"' fits.


You give us no idea where you got the formula for 'AIC' from, but it is 
not that introduced by Akaike (1973, 4) and commonly used in time-series 
(and by arima()).  I think you are applying a formula applicable to linear 
regression for independent observations, incorrectly.  There are really 
are a lot of subtleties here, and although p is well-defined, n is not.
Thus applying Schwarz's criterion (aka BIC in one of its senses) is not at 
all clearcut, a not uncommon situation with non-iid sampling.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Nov 28 22:33:57 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Nov 2005 22:33:57 +0100
Subject: [R] read.spss problem
In-Reply-To: <4F2C5827-A526-472C-8256-9CE2F9B83F05@msu.edu>
References: <4F2C5827-A526-472C-8256-9CE2F9B83F05@msu.edu>
Message-ID: <x2br04e9tm.fsf@turmalin.kubism.ku.dk>

Davia Cox <coxdavia at msu.edu> writes:

> Hello,
> I am having trouble reading an spss file into R. I have reset my  
> working directory to the folder where this file is stored. This is  
> what I've typed into R and the error message I received:
> 
> + getwd()
> [1] "/Users/daviacox/Graduate School/PLS 801"
>  > read.spss("norwil.spss")
> Error in read.spss("norwil.spss") : error reading portable-file  
> dictionary
> In addition: Warning message:
> Expected variable count record
> 
> Please help, I don't have SPSS on my computer (I am re-analyzing some  
> data for a statistics project) so I can't alter the file that way.
> 
> Thanks again for your help.

Hmm, what kind of file is this? SPSS data files are generally called
.sav or .por. I wonder if you've got a "syntax file" on your hand,
i.e. a file containing program code. Is it readable as a text file?
 
> Davia
> 
> Davia S. Cox
> Global Urban Studies Program
> Graduate Assistant/Doctoral Student
> 305 Berkey Hall
> East Lansing, MI 48825
> 517-353-5987 Office
> 517-353-6680 Fax
> coxdavia at msu.edu
> 
> "One of the penalties for refusing to participate in politics is that  
> you
> end up being governed by your inferiors."
>   -Plato
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dylan.beaudette at gmail.com  Mon Nov 28 22:56:27 2005
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Mon, 28 Nov 2005 13:56:27 -0800
Subject: [R] overlay additional axes
Message-ID: <200511281356.28162.dylan.beaudette@gmail.com>

Greetings,

I am trying to add an extra labled axis in position 3 (top x-axis), with 
numbers that do not match up with the existing axes.

Surely this must be possible, and I am just doing it incorectly.

So far I have tried the following:
#make a plot
plot(TIK, type="l", cex=.25, xlim=c(2,32), ylim=c(0,1600))

#try and add a new axis with different numbers in position 3
axis(3,0.154/(2*sin(TIK[,1]/2*pi/180)))

...obviously the nature of the numbers in both axes is quite different. 

is it possible to have the bottom axis (degrees 2Theta) line up with a 
corosponding top axis of 0.154/(2*sin(TIK[,1]/2*pi/180)) ?

thanks in advance!

-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341



From mschwartz at mn.rr.com  Mon Nov 28 23:38:23 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 28 Nov 2005 16:38:23 -0600
Subject: [R] Use of axis() in conjunction with plot(..., axes=F)
In-Reply-To: <7887D7BF-2F4C-4656-9967-E1EE985CEDA9@plessthan.com>
References: <200310071009.h97A54qM000048@stat.math.ethz.ch>
	<7887D7BF-2F4C-4656-9967-E1EE985CEDA9@plessthan.com>
Message-ID: <1133217503.2495.58.camel@localhost.localdomain>

On Mon, 2005-11-28 at 13:18 -0800, Dennis Fisher wrote:
> Colleagues
> 
> On occasion, I want to control either tick marks or labels in axes  
> different from the defaults created with "axes=T" in the plot  
> command.  If I invoke "axes=F" and axis(n), I can do so.  However,  
> the axes produced by axis() differ slightly from those produced  
> within plot.  I have "bty" in par set to "l" (i.e., left and bottom  
> axes only).  Differences include:
> 
> 1.  when an axis contains a factor, plot() produces axes showing the  
> actual factors whereas axis() replaces these factors with integers  
> representing the level of the factor (e.g., if the factor is  
> countries, plot() yields axes labeled "Argentina", "Brazil", etc.  
> whereas axes() yields 1, 2, ...

Without your actual code here, I may be wrong, but I am going to guess
that the difference is that when you are using a factor, plot.factor()
gets used, which ends up using barplot() in the case where 'x' is a
factor.

plot.factor() by default uses table(x) on the 'x' argument from the
initial plot() call and passes this to barplot().

barplot() by default uses the names attribute of the table object
created above as the 'names.arg' argument and thus, the resultant labels
on the x axis. Hence, there is an implicit coercion of the factor levels
to character by using table().

If you specify axes = FALSE, there is no implicit coercion that takes
place. You can do that yourself of course by using as.character(FACTOR)
for the labels argument.

See ?plot.factor and ?barplot for more information here.

> 2.  axes produced by plot() are "full-length" (i.e., the axes connect  
> at the corner of the plot) whereas axes produced by axis() are not  
> full-length (i.e., they run between the smallest and largest label).   
> It appears that this can be overcome by using the "at" option within  
> axes().  However, I cannot figure out how to use the "at" option when  
> the axis is a factor.
> 
> Thoughts?

For number 2, plotting functions such as plot.default use box() to place
a full frame around the plot region. In plot.default(), this is done
using the 'frame.plot' argument, which is set by default to the value of
the  'axes' argument. So, if you set 'axes = FALSE', box() is not used.
So just call it explicitly after using axis(). See ?box().

HTH,

Marc Schwartz



From Achim.Zeileis at wu-wien.ac.at  Mon Nov 28 23:40:55 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 28 Nov 2005 23:40:55 +0100 (CET)
Subject: [R] Durbin-Watson Critical Values Tables
In-Reply-To: <30ddfdae0511280916y6ff349f0j56ddde244f40d33a@mail.gmail.com>
References: <30ddfdae0511280916y6ff349f0j56ddde244f40d33a@mail.gmail.com>
Message-ID: <Pine.LNX.4.58.0511282330080.28403@thorin.ci.tuwien.ac.at>

On Mon, 28 Nov 2005, Constantine Tsardounis wrote:

> Hello to everyone!...
>
> I would like to ask you if there is a way to extract the known
> Durbin-Watson critical values (D_L and D_U - lower and upper limits)
> within R, as we do in the DW statistic tables at the end of
> Econometrics Textbooks, because packages "car" and "lmtest" seem to
> provide only the p-value (it is useful, but I  would like to have the
> exact critical vaues).

lmtest and car do not *only* provide p values, they are able to provide a
p-value whereas using the upper and lower bounds you might obtain
inconclusive results. Hence, the p values yield more precise results!

It is not possible to provide a single table of critical values because
the null distribution depends on the regressor matrix. If you really want
to have a single critical value for a particular problem, look at the
implementations of dwtest() or durbin.watson() how to compute the full
(approximate) null distribution.

Best,
Z

> Thank you very much in advance,
>
> Tsardounis Constantine
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Tue Nov 29 00:52:29 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 28 Nov 2005 18:52:29 -0500
Subject: [R] What made us so popular Nov 16-20?
Message-ID: <438B983D.1080807@stats.uwo.ca>

Our main US mirror is cran.mirrors.pair.com, AKA cran.us.r-project.org. 
  Pair.com keeps statistics on traffic on the mirror sites, and I got 
all excited when I looked at this page:

http://mirrors.pair.com/pair/stats.html

and saw that CRAN was 5th most popular over the last month, getting more 
visitors than Apache, MySQL, OpenOffice, etc.  Then I looked at this graph:

http://mirrors.pair.com/freebsd/stats/cran-ip.png

and saw that this is likely due to a huge spike in traffic between Nov 
16 and 20.  Our "visitors" (not sure of the exact definition) went from 
the usual  <10K/day up to 50-150K/day during that week.

Did we get mentioned somewhere (e.g. Slashdot), or was someone just 
experimenting with some automated downloading?

Duncan Murdoch



From totavi at utu.fi  Tue Nov 29 01:08:57 2005
From: totavi at utu.fi (Tommi Viitanen)
Date: Tue, 29 Nov 2005 02:08:57 +0200
Subject: [R] qcc
Message-ID: <438B9C19.1040909@utu.fi>

violating.runs

I read from the news

cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf

that the criteria for the violating is 5 but
1)I cannot find "5" in the code of the function. Where is the "5" ?
2)What is the easiest way to change it ?
3)Is there any more criterias made somewhere ?

Yours sincerelly, Tommi Viitanen



From Hong.Ooi at iag.com.au  Tue Nov 29 01:12:08 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Tue, 29 Nov 2005 11:12:08 +1100
Subject: [R] Looking for constrained optimisation code
Message-ID: <200511290012.jAT0CNcF018084@hypatia.math.ethz.ch>


_______________________________________________________________________________________


You know, this is the first time I've heard of constrOptim.
 
I actually have a rather complicated, nonlinear boundary expression in
mind, so this function by itself isn't quite what I'm after. Still, I
should be able to hack up a barrier function in my own code and feed
that into optim/nlminb/constrOptim.

Thanks!


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
(02) 9292 1566

-----Original Message-----
From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
Sent: Tuesday, 29 November 2005 5:31 AM
To: Hong Ooi
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Looking for constrained optimisation code

On Mon, 28 Nov 2005, Hong Ooi wrote:
>
> Hi,
>
> I was just wondering if there was any available R code that could
handle
> general constrained optimisation problems. At the moment I'm using
> nlminb and optim, both of which allow box constraints on the
parameters,
> but ideally I'd like to be able to specify more general constraints on
> the solution space.
>

constrOptim() allows linear inequality constraints.

 	-thomas



_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From Seeliger.Curt at epamail.epa.gov  Tue Nov 29 01:14:07 2005
From: Seeliger.Curt at epamail.epa.gov (Seeliger.Curt@epamail.epa.gov)
Date: Mon, 28 Nov 2005 16:14:07 -0800
Subject: [R] What made us so popular Nov 16-20?
In-Reply-To: <438B983D.1080807@stats.uwo.ca>
Message-ID: <OF2A60E141.7643B758-ON882570C8.0000C835-882570C8.00014B40@epamail.epa.gov>

Duncan asks:
> Did we get mentioned somewhere (e.g. Slashdot), or was someone just
> experimenting with some automated downloading?

R was mentioned in last week's (I think) O'Reilly newsletter, which
included a link to a short article showing how easy it is to get R to
graph stuff like stock price histories.  That's the publisher, not the
talking head.

For what it's worth, the article isn't worth chasing down.  It left a
beginner like me disappointed that R's capabilities weren't better
shown, and that he relied on Perl to do data manipulation.

cur

--
Curt Seeliger, Data Ranger
CSC, EPA/WED contractor
541/754-4638
seeliger.curt at epa.gov



From Hong.Ooi at iag.com.au  Tue Nov 29 01:23:52 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Tue, 29 Nov 2005 11:23:52 +1100
Subject: [R] glm: quasi models with logit link function and binary data
Message-ID: <200511290024.jAT0O9Mj021411@hypatia.math.ethz.ch>


_______________________________________________________________________________________


This would be because quasi(link=logit) doesn't actually fit a logistic regression. The default variance function for quasi is the identity, not binomial variance. To emulate a logistic regression, use var="mu(1-mu)" in addition to link=logit.


> y <- runif(100)
> glm(y ~ 1, family=binomial)

Call:  glm(formula = y ~ 1, family = binomial) 

Coefficients:
(Intercept)  
   -0.01208  

Degrees of Freedom: 99 Total (i.e. Null);  99 Residual
Null Deviance:      37.15 
Residual Deviance: 37.15        AIC: 140.6 
Warning message:
non-integer #successes in a binomial glm! in: eval(expr, envir, enclos)


> glm(y ~ 1, family=quasi(var="mu(1-mu)", link=logit))

Call:  glm(formula = y ~ 1, family = quasi(var = "mu(1-mu)", link = logit)) 

Coefficients:
(Intercept)  
   -0.01208  

Degrees of Freedom: 99 Total (i.e. Null);  99 Residual
Null Deviance:      37.15 
Residual Deviance: 37.15        AIC: NA


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
(02) 9292 1566

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bj??rn Stollenwerk
Sent: Monday, 28 November 2005 10:18 PM
To: R-help at stat.math.ethz.ch
Subject: [R] glm: quasi models with logit link function and binary data


# Hello R Users,
#
# I would like to fit a glm model with quasi family and
# logistical link function, but this does not seam to work
# with binary data.
#
# Please don't suggest to use the quasibinomial family. This
# works out, but when applied to the true data, the
# variance function does not seams to be
# appropriate.
#
# I couldn't see in the
# theory why this does not work.
# Is this a bug, or are there theoretical reasons?
# One problem might be, that logit(0)=-Inf and logit(1)=Inf.
# But I can't see how this disturbes the calculation of quasi-Likelihood.
#
# Thank you very much,
# best,
#
# Bj??rn

set.seed(0)
y <- sample(c(0,1), size=100, replace=T)

# the following models work:
glm(y ~ 1)
glm(y ~ 1, family=binomial(link=logit))
glm(y ~ 1, family=quasibinomial(link=logit))

# the next model doesn't work:
glm(y ~ 1, family=quasi(link=logit))

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From p.dalgaard at biostat.ku.dk  Tue Nov 29 01:32:09 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Nov 2005 01:32:09 +0100
Subject: [R] What made us so popular Nov 16-20?
In-Reply-To: <438B983D.1080807@stats.uwo.ca>
References: <438B983D.1080807@stats.uwo.ca>
Message-ID: <x2lkz8qoom.fsf@turmalin.kubism.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> Our main US mirror is cran.mirrors.pair.com, AKA cran.us.r-project.org. 
>   Pair.com keeps statistics on traffic on the mirror sites, and I got 
> all excited when I looked at this page:
> 
> http://mirrors.pair.com/pair/stats.html
> 
> and saw that CRAN was 5th most popular over the last month, getting more 
> visitors than Apache, MySQL, OpenOffice, etc.  Then I looked at this graph:
> 
> http://mirrors.pair.com/freebsd/stats/cran-ip.png
> 
> and saw that this is likely due to a huge spike in traffic between Nov 
> 16 and 20.  Our "visitors" (not sure of the exact definition) went from 
> the usual  <10K/day up to 50-150K/day during that week.
> 
> Did we get mentioned somewhere (e.g. Slashdot), or was someone just 
> experimenting with some automated downloading?
> 

Kevin Farnham's article on O'Reilly might have something to do with it...

http://www.onlamp.com/pub/a/onlamp/2005/11/17/r_for_statistics.html

As the link suggests, it was published on the 17th.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Hong.Ooi at iag.com.au  Tue Nov 29 01:40:16 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Tue, 29 Nov 2005 11:40:16 +1100
Subject: [R] glm: quasi models with logit link function and binary data
Message-ID: <200511290040.jAT0edL8027342@hypatia.math.ethz.ch>


_______________________________________________________________________________________


Hm, I should have checked what would happen with binary data and not just continuous. Using glm with quasi(var="mu(1-mu)", link=logit) indeed fails with NAs/NaNs when y is binary.

-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
(02) 9292 1566

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sundar Dorai-Raj
Sent: Monday, 28 November 2005 11:20 PM
To: Bj??rn Stollenwerk
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] glm: quasi models with logit link function and binary data


This is an issue with the starting values provided to glm. Take a look 
at the difference between:

quasibinomial()$initialize

and

quasi("logit")$initialize

and where this is used in glm.fit and you should see the why the error 
occurs. To avoid this, you can supply your own starting values from a 
call to glm

mustart <- predict(glm(y ~ 1, binomial), type = "response")
glm(y ~ 1, quasi("logit"), mustart = mustart)

or just use:

glm(y ~ 1, quasi("logit"), mustart = rep(0.5, length(y)))

HTH,

--sundar

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From p.dalgaard at biostat.ku.dk  Tue Nov 29 01:45:34 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Nov 2005 01:45:34 +0100
Subject: [R] What made us so popular Nov 16-20?
In-Reply-To: <OF2A60E141.7643B758-ON882570C8.0000C835-882570C8.00014B40@epamail.epa.gov>
References: <OF2A60E141.7643B758-ON882570C8.0000C835-882570C8.00014B40@epamail.epa.gov>
Message-ID: <x2hd9wqo29.fsf@turmalin.kubism.ku.dk>

Seeliger.Curt at epamail.epa.gov writes:

> Duncan asks:
> > Did we get mentioned somewhere (e.g. Slashdot), or was someone just
> > experimenting with some automated downloading?
> 
> R was mentioned in last week's (I think) O'Reilly newsletter, which
> included a link to a short article showing how easy it is to get R to
> graph stuff like stock price histories.  That's the publisher, not the
> talking head.

[See other mail for the link]

> For what it's worth, the article isn't worth chasing down.  It left a
> beginner like me disappointed that R's capabilities weren't better
> shown, and that he relied on Perl to do data manipulation.


Er, where did you see Perl being used? The only thing that irked me
(admittedly, I only skimmed the article) was that he was using
regression models to test for correlation (why not cor.test()?) and
speculates a bit wildly about the sign of a clearly nonsignificant
relation. It's a bit superficial, but I suspect that this sort of
paper has to be.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From levyr at umd.edu  Tue Nov 29 01:50:27 2005
From: levyr at umd.edu (levyr@umd.edu)
Date: Mon, 28 Nov 2005 19:50:27 -0500
Subject: [R] change axis format for different panels in xyplot in lattice
Message-ID: <945a994c.628c9ab0.81b4600@po1.mail.umd.edu>

Utilizing panel.number and grid allowed me to do all my
desired manipulations - thanks Deepayan!

Roy

---- Original message ----
>Date: Tue, 22 Nov 2005 13:47:20 -0600
>From: Deepayan Sarkar <deepayan.sarkar at gmail.com>  
>Subject: Re: change axis format for different panels in
xyplot in lattice  
>To: levyr at umd.edu
>Cc: r-help at stat.math.ethz.ch
>
>On 11/22/05, levyr at umd.edu <levyr at umd.edu> wrote:
>> Dear R users,
>>
>> My apologies for a simple question for which I suspect there
>> is a simple answer that I have yet to find.  I'd like to plot
>> panels in lattice with different graphical parameters for the
>> axes.  For example, the code
>>
>> x<-rnorm(100)
>> y<-rnorm(100)
>> z<-c(rep(1,50), rep(2,50))
>> library(lattice)
>> xyplot(y~x|z)
>>
>> plots two panels with the default black axes.  Running the
>> following code
>>
>> trellis.par.set(list(axis.line = list(col = "transparent")))
>> xyplot(y~x|z)
>>
>> plots the same data without the axes.  Is it possible (in one
>> plot) to plot the first panel with black axes and the second
>> panel with tranparent axes?
>
>Not systematically, but consider this approach:
>
>library(grid)
>trellis.par.set(list(axis.line = list(col = "transparent")))
>xyplot(y~x|z,
>       panel = function(..., panel.number) {
>           if (panel.number == 1) grid.rect(gp = gpar(col =
'black'))
>           panel.xyplot(...)
>       } )
>
>This is probably not exactly what you want, since the tick
marks are
>transparent in both panels, but you get the idea. You can
gain finer
>control of the axis annotation using panel.axis inside your panel
>function (but you will need to disable clipping, controlled by
>trellis.par.get("clip")). Alternatively, check out
?trellis.focus.
>
>Deepayan



From spencer.graves at pdf.com  Tue Nov 29 02:58:06 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 28 Nov 2005 17:58:06 -0800
Subject: [R] Looking for constrained optimisation code
In-Reply-To: <200511290012.jAT0CNcF018084@hypatia.math.ethz.ch>
References: <200511290012.jAT0CNcF018084@hypatia.math.ethz.ch>
Message-ID: <438BB5AE.7030304@pdf.com>

	  Have you considered migrating the constraints into the objective 
function, then cranking up the penalty for constraint violation once you 
have a more or less feasible solution?

	  spencer graves

Hong Ooi wrote:

> _______________________________________________________________________________________
> 
> 
> You know, this is the first time I've heard of constrOptim.
>  
> I actually have a rather complicated, nonlinear boundary expression in
> mind, so this function by itself isn't quite what I'm after. Still, I
> should be able to hack up a barrier function in my own code and feed
> that into optim/nlminb/constrOptim.
> 
> Thanks!
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From jhainm at fas.harvard.edu  Tue Nov 29 03:00:44 2005
From: jhainm at fas.harvard.edu (Jens Hainmueller)
Date: Mon, 28 Nov 2005 21:00:44 -0500
Subject: [R] sensitivity tests fo causal inference
Message-ID: <200511290200.jAT20msB028964@us17.unix.fas.harvard.edu>

Hi all,

Following up on Holger's email last week: 

Does anyone know if there exists a library that implements the sensitivity
tests for hidden bias for matched pairs and unmatched groups as proposed in
Rosenbaum's Observational Studies (2002: ch.4)?

Thanks.

Best,
jens



From ggrothendieck at gmail.com  Tue Nov 29 03:04:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Nov 2005 21:04:07 -0500
Subject: [R] What made us so popular Nov 16-20?
In-Reply-To: <x2hd9wqo29.fsf@turmalin.kubism.ku.dk>
References: <OF2A60E141.7643B758-ON882570C8.0000C835-882570C8.00014B40@epamail.epa.gov>
	<x2hd9wqo29.fsf@turmalin.kubism.ku.dk>
Message-ID: <971536df0511281804i440723d0g24db73d192115d4e@mail.gmail.com>

On 29 Nov 2005 01:45:34 +0100, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Seeliger.Curt at epamail.epa.gov writes:
>
> > Duncan asks:
> > > Did we get mentioned somewhere (e.g. Slashdot), or was someone just
> > > experimenting with some automated downloading?
> >
> > R was mentioned in last week's (I think) O'Reilly newsletter, which
> > included a link to a short article showing how easy it is to get R to
> > graph stuff like stock price histories.  That's the publisher, not the
> > talking head.
>
> [See other mail for the link]
>
> > For what it's worth, the article isn't worth chasing down.  It left a
> > beginner like me disappointed that R's capabilities weren't better
> > shown, and that he relied on Perl to do data manipulation.
>
>
> Er, where did you see Perl being used? The only thing that irked me
> (admittedly, I only skimmed the article) was that he was using
> regression models to test for correlation (why not cor.test()?) and
> speculates a bit wildly about the sign of a clearly nonsignificant
> relation. It's a bit superficial, but I suspect that this sort of
> paper has to be.

That article not only gave the (false) impression that one
needed something like perl to do data analysis but it
failed to use time series objects to represent time series
making the examples poor (e.g. x axis was labelled
1, 2, ... instead of using time) and unnecessarily lengthy
and complex.

By the way, during November there was also a vote on
alt.comp.freeware for the pricelessware collection

     http://www.pricelesswarehome.org

(which R failed to get on in 2005 but did manage to make it on
for 2006) and this may have been the reason, in part, for
the spike.



From vietnguyen at fastmail.fm  Tue Nov 29 05:32:49 2005
From: vietnguyen at fastmail.fm (Viet Nguyen)
Date: Tue, 29 Nov 2005 15:32:49 +1100
Subject: [R] rotated ylab with xyplot
Message-ID: <438BD9F1.4070106@fastmail.fm>

hi all,

in R, what's the best way to have a rotated ylab in a graph plotted with 
either xyplot or xYplot?  I tried this but it didn't work.

xyplot(y~x,data.frame(x=1:10,y=runif(10)),ylab=list(srt=90,crt=90,rot=90,label="my label"))


more generally, how do you output a text at an angle in a lattice graph?

what would be a good reference for R lattice graphics?  I need more help 
than the help pages provide.

thank you in advance.

vn



From maustin at amgen.com  Tue Nov 29 06:00:45 2005
From: maustin at amgen.com (Austin, Matt)
Date: Mon, 28 Nov 2005 21:00:45 -0800
Subject: [R] rotated ylab with xyplot
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD4EA@teal-exch.amgen.com>

Try

library(grid)
xyplot(y~x,data.frame(x=1:10,y=runif(10)), ylab=textGrob(label="my label",
rot=0))

Note in the ?xyplot the xlab/ylab section points to the 'main' parameter
where it defines that a list can be used, however string rotation parameters
are not in the list.  The helpfile notes that a grob object can be used.

The reference for lattice and grid is R Graphics by Paul Murrell.

--Matt

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Viet Nguyen
> Sent: Monday, November 28, 2005 8:33 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] rotated ylab with xyplot
> 
> 
> hi all,
> 
> in R, what's the best way to have a rotated ylab in a graph 
> plotted with 
> either xyplot or xYplot?  I tried this but it didn't work.
> 
> xyplot(y~x,data.frame(x=1:10,y=runif(10)),ylab=list(srt=90,crt
> =90,rot=90,label="my label"))
> 
> 
> more generally, how do you output a text at an angle in a 
> lattice graph?
> 
> what would be a good reference for R lattice graphics?  I 
> need more help 
> than the help pages provide.
> 
> thank you in advance.
> 
> vn
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From samrobertsmith at yahoo.com  Tue Nov 29 06:33:24 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Mon, 28 Nov 2005 21:33:24 -0800 (PST)
Subject: [R] label point
Message-ID: <20051129053324.34875.qmail@web30615.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051128/f1e82a44/attachment.pl

From petr.pikal at precheza.cz  Tue Nov 29 08:05:10 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 29 Nov 2005 08:05:10 +0100
Subject: [R] label point
In-Reply-To: <20051129053324.34875.qmail@web30615.mail.mud.yahoo.com>
Message-ID: <438C0BB6.12460.317C84@localhost>

Hallo

On 28 Nov 2005 at 21:33, Robert wrote:

Date sent:      	Mon, 28 Nov 2005 21:33:24 -0800 (PST)
From:           	Robert <samrobertsmith at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] label point

>   Hi, I have a matrix:
>   [,1] [,2]
>   [1,] 11 31
>   [2,] 44 50
>   [3,] 23 100
>   [4,] 90 31
>   I use plot to draw the four points. Is there any way to label the
>   point? for insatnce, for (11,31), it is "1", for (44,50), it is "4".
>   Thanks!
> 


label.vec<-c(1, 4, l2, l3.....)
plot(matrix)
text(matrix, label.vec)

Should do the trick but it overplots your points, so you need to 
shift positions e.g. by

text(matrix+c(some offset,0), label.vec)

HTH
Petr




> 
> 
> 
> ---------------------------------
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ggrothendieck at gmail.com  Tue Nov 29 08:17:48 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Nov 2005 02:17:48 -0500
Subject: [R] label point
In-Reply-To: <20051129053324.34875.qmail@web30615.mail.mud.yahoo.com>
References: <20051129053324.34875.qmail@web30615.mail.mud.yahoo.com>
Message-ID: <971536df0511282317j362020c0o578147f7ee6df0cd@mail.gmail.com>

See ?text
e.g.

plot(m[,1], m[,2]) # or plot(m)
text(m[,1], m[,2], pos = 3, cex = 0.7) # 3 means above, 0.7 is 70% size



On 11/29/05, Robert <samrobertsmith at yahoo.com> wrote:
>  Hi, I have a matrix:
>  [,1] [,2]
>  [1,] 11 31
>  [2,] 44 50
>  [3,] 23 100
>  [4,] 90 31
>  I use plot to draw the four points. Is there any way to label the point? for insatnce, for (11,31), it is "1", for (44,50), it is "4".
>  Thanks!
>
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Tue Nov 29 08:58:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 29 Nov 2005 08:58:24 +0100
Subject: [R] qcc
In-Reply-To: <438B9C19.1040909@utu.fi>
References: <438B9C19.1040909@utu.fi>
Message-ID: <438C0A20.30403@statistik.uni-dortmund.de>

Tommi Viitanen wrote:

> violating.runs
> 
> I read from the news
> 
> cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf
> 
> that the criteria for the violating is 5 but
> 1)I cannot find "5" in the code of the function. Where is the "5" ?

See ?violating.runs:
violating.runs(object, run.length = qcc.options("run.length"))
                                     ^^^^^ ^^^ ^^^ ^^ ^^^ ^^^

And try

 > qcc.options("run.length")
[1] 5

Ah!


> 2)What is the easiest way to change it ?

I'd try, e.g.,
  qcc.options(run.length = 6)

or directly
  violating.runs(object, run.length = 6)


> 3)Is there any more criterias made somewhere ?

No, looking 10 seconds into the code, but why do you not read it 
yourself? It is easily understandable and you will also see at once that 
you can will get a performance boost by optimizing that code....

Uwe Ligges


> Yours sincerelly, Tommi Viitanen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Tue Nov 29 09:18:16 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 29 Nov 2005 09:18:16 +0100
Subject: [R] Robust smoothing
In-Reply-To: <200511281942.jASJgjEo004369@ohm.gene.com>
References: <IQOH3O$7167E5D1DFB02C59B108538C94CE73D8@libero.it>
	<200511281942.jASJgjEo004369@ohm.gene.com>
Message-ID: <17292.3784.195519.15503@stat.math.ethz.ch>

  [Cross-posted to R-SIG-robust,
   the Special Interest Group (mailing list) on "Robustness and R"]

>>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
>>>>>     on Mon, 28 Nov 2005 11:42:45 -0800 writes:

    BertG> Note: As I believe Brian Ripley pointed out in his
    BertG> MASS book, loess may not be as resistant to outliers
    BertG> (which is one aspect of robustness; robustness of
    BertG> efficiency is another) as you think. The problem is
    BertG> that it starts off with LS estimates and these can be
    BertG> so distorted by unusual values that the reweighting
    BertG> cannot properly recover; i.e. convergence is to a
    BertG> local minimum far from the desired global one. 

indeed  {I've researched on that about 15 years ago as part of
my Ph.D.}.
I'm convinced that robust smoothing should be done quite
analogously to how (many agree) it should happen for parametric
regression:

1) initialized by a ``high breakdown'' (that's not a trivial notion when you do
   non-parametric curve estimation!) smoother;  
2) From that compute residuals r_i  and compute  weights w_i := psi(r_i)/r_i
   typically for a redescending psi.
3) Now use these weights for the ``high efficiency'' smoother,
   e.g., smooth.spline(), 
   maybe even without iterating {``1-step M-estimator'' idea}
   or then with iterating, i.e. reweighting.

For that reason, i.e. for being able to do "1)", 
I had collected algorithms for fast running medians {quite some time ago}
and added the R function  runmed()  {running medians}
which should be very fast, particularly for large data where
it's of optimal complexity see  help(runmed).

Martin Maechler, ETH Zurich


    BertG> You might wish to read the documentation for rlm() (in
    BertG> MASS, the package) and the appropriate sections of
    BertG> MASS, the book.

    BertG> Cheers,

    BertG> -- Bert Gunter Genentech Non-Clinical Statistics
    BertG> South San Francisco, CA
 
    BertG> "The business of the statistician is to catalyze the
    BertG> scientific learning process."  - George E. P. Box
 
 

    >> -----Original Message----- From:
    >> r-help-bounces at stat.math.ethz.ch
    >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
    >> Marta Colombo Sent: Monday, November 28, 2005 10:38 AM
    >> To: R help Subject: [R] Robust fitting
    >> 
    >> Good evening,I am Marta Colombo, student of "Politecnico
    >> di Milano". I'm studying Local Regression Techniques such
    >> as loess, smoothing splines and kernel
    >> smoothers. Choosing "symmetric" for the argument "family"
    >> in loess function it is possible to produce a robust
    >> estimate , in function smooth.spline and ksmooth I didn't
    >> find this possibility.  Well, is there a way to produce a
    >> robust estimate using smoothing splines or kernel
    >> smoothers? And if the answer is no, why? I'm asking these
    >> questions because I need to know loess' advantages and
    >> disadvantages compared to other techniques. Thank you
    >> very much for attention,
    >> 
    >> Marta Colombo



From svenknueppel at reilich.net  Tue Nov 29 10:02:03 2005
From: svenknueppel at reilich.net (svenknueppel@reilich.net)
Date: Tue, 29 Nov 2005 10:02:03 +0100 (CET)
Subject: [R] package kinship - %*%
Message-ID: <20051129090203.7A3F77187DE@basicbox6.server-home.net>

Hello,
I like to use the package "kinship" (R version 2.2.0). After loading
this package the operator %*% doesn't work.

Example:
R> library(kinship)
R> a <- cbind(1:2,rnorm(2))
R> a%*%a
Error message:
Fehler in a %*% a : keine anwendbare Methode für "%*%"

The Message in English: Error in a %*% a: no applicable message for
"%*%".

What can I do?

Thank you.
Sven Knüppel (Germany - Berlin)



From jordi_molins at hotmail.com  Tue Nov 29 10:07:56 2005
From: jordi_molins at hotmail.com (Jordi)
Date: Tue, 29 Nov 2005 10:07:56 +0100
Subject: [R] Compiling R in C / C++
Message-ID: <BAY111-DAV1095D4AD306D3A1AFA1ECAF04B0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/3044145d/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Nov 29 10:11:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 29 Nov 2005 10:11:47 +0100
Subject: [R] package kinship - %*%
In-Reply-To: <20051129090203.7A3F77187DE@basicbox6.server-home.net>
References: <20051129090203.7A3F77187DE@basicbox6.server-home.net>
Message-ID: <438C1B53.4050102@statistik.uni-dortmund.de>

svenknueppel at reilich.net wrote:

> Hello,
> I like to use the package "kinship" (R version 2.2.0). After loading
> this package the operator %*% doesn't work.
> 
> Example:
> R> library(kinship)
> R> a <- cbind(1:2,rnorm(2))
> R> a%*%a
> Error message:
> Fehler in a %*% a : keine anwendbare Methode f??r "%*%"
> 
> The Message in English: Error in a %*% a: no applicable message for
> "%*%".
> 
> What can I do?

1. Ask the package maintainer of kinship for a bugfix (CCing).
2. you can still use the "%*%" operator from the base namespace:
  base::"%*%"(a,a)

Uwe Ligges



> Thank you.
> Sven Kn??ppel (Germany - Berlin)
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From R.A.Sanderson at newcastle.ac.uk  Tue Nov 29 10:42:12 2005
From: R.A.Sanderson at newcastle.ac.uk (Roy Sanderson)
Date: Tue, 29 Nov 2005 09:42:12 +0000
Subject: [R] Reclassifying values within a vector to several other values
Message-ID: <3.0.3.32.20051129094212.00959c30@popin.ncl.ac.uk>

Dear List

Apologies for such a simple question:

I have a vector of 738 elements, coded with values between 1 and 16 (but
not containing 7, 10, 11 or 13) and wish to recode value 14 to 1, 4 to 2, 1
to 3, 2 to 4 and all other values to 5.  I've been trying to use the
replace function (in base) and %in%, but without success.

Many thanks
Roy
----------------------------------------------------------------------------
-------
Roy Sanderson
Institute for Research on Environment and Sustainability
Devonshire Building
University of Newcastle
Newcastle upon Tyne
NE1 7RU
United Kingdom

Tel: +44 191 246 4835
Fax: +44 191 246 4998

http://www.ncl.ac.uk/environment/
r.a.sanderson at newcastle.ac.uk



From ezhil02 at yahoo.com  Tue Nov 29 11:11:38 2005
From: ezhil02 at yahoo.com (A Ezhil)
Date: Tue, 29 Nov 2005 02:11:38 -0800 (PST)
Subject: [R] R implementation of logit - t algorithm?
Message-ID: <20051129101138.37144.qmail@web32114.mail.mud.yahoo.com>

Hi,

Is there any R implementation of logit-t algorithm
described in "A high performance test of differential
gene expression for oligonucleotide arrays", (Genome
Biology 2003, 4:R67) ? If not, is it worth
implementing it in R?

Thanks in Advance.

Regards,
Ezhil



From petr.pikal at precheza.cz  Tue Nov 29 11:28:28 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 29 Nov 2005 11:28:28 +0100
Subject: [R] Reclassifying values within a vector to several other values
In-Reply-To: <3.0.3.32.20051129094212.00959c30@popin.ncl.ac.uk>
Message-ID: <438C3B5C.4440.EBAA81@localhost>

Hi

On 29 Nov 2005 at 9:42, Roy Sanderson wrote:

Date sent:      	Tue, 29 Nov 2005 09:42:12 +0000
To:             	r-help at stat.math.ethz.ch
From:           	Roy Sanderson <R.A.Sanderson at newcastle.ac.uk>
Subject:        	[R] Reclassifying values within a vector to several other values

> Dear List
> 
> Apologies for such a simple question:
> 
> I have a vector of 738 elements, coded with values between 1 and 16
> (but not containing 7, 10, 11 or 13) and wish to recode value 14 to 1,
> 4 to 2, 1 to 3, 2 to 4 and all other values to 5.  I've been trying to
> use the replace function (in base) and %in%, but without success.

Try to use factor.

test<-sample(1:16, 1000, replace=T)
test<-test[!test%in%c(7,10,11,13)]
test.f<-as.factor(test)
levels(test.f)
 [1] "1"  "2"  "3"  "4"  "5"  "6"  "8"  "9"  "12" "14" "15" "16"
levels(test.f)<-c(3,4,5,2,5,5,5,5,5,1,5,5)
 levels(test.f)
[1] "3" "4" "5" "2" "1"
tapply(test,test.f,mean)
        3         4         5         2         1 
 1.000000  2.000000  9.281496  4.000000 14.000000

If you want it numeric use as.numeric(as.character()) conversion.

HTH
Petr



> 
> Many thanks
> Roy
> ----------------------------------------------------------------------
> ------ ------- Roy Sanderson Institute for Research on Environment and
> Sustainability Devonshire Building University of Newcastle Newcastle
> upon Tyne NE1 7RU United Kingdom
> 
> Tel: +44 191 246 4835
> Fax: +44 191 246 4998
> 
> http://www.ncl.ac.uk/environment/
> r.a.sanderson at newcastle.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Nov 29 11:29:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Nov 2005 10:29:53 +0000 (GMT)
Subject: [R] Reclassifying values within a vector to several other values
In-Reply-To: <3.0.3.32.20051129094212.00959c30@popin.ncl.ac.uk>
References: <3.0.3.32.20051129094212.00959c30@popin.ncl.ac.uk>
Message-ID: <Pine.LNX.4.61.0511291025450.26339@gannet.stats>

On Tue, 29 Nov 2005, Roy Sanderson wrote:

> Dear List
>
> Apologies for such a simple question:
>
> I have a vector of 738 elements, coded with values between 1 and 16 (but
> not containing 7, 10, 11 or 13) and wish to recode value 14 to 1, 4 to 2, 1
> to 3, 2 to 4 and all other values to 5.  I've been trying to use the
> replace function (in base) and %in%, but without success.

newvector <- c(3,4,5,2,5,5,5,5,5,5,5,5,5,1,5,5)[oldvector]

Check:

> oldvector <- 1:16
> c(3,4,5,2,5,5,5,5,5,5,5,5,5,1,5,5)[oldvector]
  [1] 3 4 5 2 5 5 5 5 5 5 5 5 5 1 5 5

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From samrobertsmith at yahoo.com  Tue Nov 29 12:04:36 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Tue, 29 Nov 2005 03:04:36 -0800 (PST)
Subject: [R] symmetric matrix
Message-ID: <20051129110436.85158.qmail@web30608.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/9ecfa786/attachment.pl

From samrobertsmith at yahoo.com  Tue Nov 29 12:46:05 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Tue, 29 Nov 2005 03:46:05 -0800 (PST)
Subject: [R] transform matrix
Message-ID: <20051129114605.27532.qmail@web30605.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/2e9e7bde/attachment.pl

From sdavis2 at mail.nih.gov  Tue Nov 29 12:52:16 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 29 Nov 2005 06:52:16 -0500
Subject: [R] transform matrix
In-Reply-To: <20051129114605.27532.qmail@web30605.mail.mud.yahoo.com>
Message-ID: <BFB1AB20.13AB5%sdavis2@mail.nih.gov>

On 11/29/05 6:46 AM, "Robert" <samrobertsmith at yahoo.com> wrote:

> I run the following code and got a wrong message. Anyway to transform test1 to
> a 6 by 6 matrix? many thanks!
>> test1
>         1         2        3         4         5         6
> 1 0.0000000 0.7760856 2.022222 0.6148687 3.0227028 3.2104434
> 2 0.7760856 0.0000000 1.690790 0.2424415 2.3636083 2.5334957
> 3 2.0222216 1.6907899 0.000000 1.5939158 1.5126344 1.7304217
> 4 0.6148687 0.2424415 1.593916 0.0000000 2.4265906 2.6085845
> 5 3.0227028 2.3636083 1.512634 2.4265906 0.0000000 0.2184739
> 6 3.2104434 2.5334957 1.730422 2.6085845 0.2184739 0.0000000
>> tryD=matrix(data = test1, nrow = 6, ncol = 6)
> Warning message: 
> Replacement length not a multiple of the elements to replace in matrix(...)

What is test1?  Try:

tryD <- as.matrix(test1)

Does that do it?  

Sean



From ramasamy at cancer.org.uk  Tue Nov 29 13:02:10 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 29 Nov 2005 12:02:10 +0000
Subject: [R] symmetric matrix
In-Reply-To: <20051129110436.85158.qmail@web30608.mail.mud.yahoo.com>
References: <20051129110436.85158.qmail@web30608.mail.mud.yahoo.com>
Message-ID: <1133265730.9219.32.camel@dhcp-82.wolf.ox.ac.uk>

Use as.matrix() :

 m <- round( as.dist( cor( matrix( rnorm(600), nc=6 ) ) ), 2 )
 m
      1     2     3     4     5
2 -0.05
3  0.01  0.03
4  0.00  0.05  0.00
5  0.20  0.07  0.09 -0.07
6  0.03  0.02  0.11 -0.15 -0.11

 as.matrix( m )
      1     2    3     4     5     6
1  0.00 -0.05 0.01  0.00  0.20  0.03
2 -0.05  0.00 0.03  0.05  0.07  0.02
3  0.01  0.03 0.00  0.00  0.09  0.11
4  0.00  0.05 0.00  0.00 -0.07 -0.15
5  0.20  0.07 0.09 -0.07  0.00 -0.11
6  0.03  0.02 0.11 -0.15 -0.11  0.00




On Tue, 2005-11-29 at 03:04 -0800, Robert wrote:
> I have the following matrix:
>             1         2        3        4         5
> 2 0.7760856                                      
> 3 2.0222216 1.6907899                            
> 4 0.6148687 0.2424415 1.593916                   
> 5 3.0227028 2.3636083 1.512634 2.426591          
> 6 3.2104434 2.5334957 1.730422 2.608584 0.2184739
>   the diagonal is 0 and it is a symmetric matrix.
>   Is there any function to return to the normal one?
>   That is, the 6 by 6 one?
>    
> 
> 		
> ---------------------------------
> 
>  Single? There's someone we'd like you to meet.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From christian.bieli at unibas.ch  Tue Nov 29 13:06:20 2005
From: christian.bieli at unibas.ch (Christian Bieli)
Date: Tue, 29 Nov 2005 13:06:20 +0100
Subject: [R] Indexing variables within lapply?
Message-ID: <438C443C.8000608@unibas.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/b674b46b/attachment.pl

From ccleland at optonline.net  Tue Nov 29 13:23:39 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 29 Nov 2005 07:23:39 -0500
Subject: [R] Reclassifying values within a vector to several other values
In-Reply-To: <3.0.3.32.20051129094212.00959c30@popin.ncl.ac.uk>
References: <3.0.3.32.20051129094212.00959c30@popin.ncl.ac.uk>
Message-ID: <438C484B.3080506@optonline.net>

Here is another approach using recode() in the car package:

 > library(car)
 > x <- 1:16
 > recode(x, "14=1; 4=2; 1=3; 2=4; else=5")
  [1] 3 4 5 2 5 5 5 5 5 5 5 5 5 1 5 5

Roy Sanderson wrote:
> Dear List
> 
> Apologies for such a simple question:
> 
> I have a vector of 738 elements, coded with values between 1 and 16 (but
> not containing 7, 10, 11 or 13) and wish to recode value 14 to 1, 4 to 2, 1
> to 3, 2 to 4 and all other values to 5.  I've been trying to use the
> replace function (in base) and %in%, but without success.
> 
> Many thanks
> Roy
> ----------------------------------------------------------------------------
> -------
> Roy Sanderson
> Institute for Research on Environment and Sustainability
> Devonshire Building
> University of Newcastle
> Newcastle upon Tyne
> NE1 7RU
> United Kingdom
> 
> Tel: +44 191 246 4835
> Fax: +44 191 246 4998
> 
> http://www.ncl.ac.uk/environment/
> r.a.sanderson at newcastle.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From SmithSJ at mar.dfo-mpo.gc.ca  Tue Nov 29 13:30:34 2005
From: SmithSJ at mar.dfo-mpo.gc.ca (Smith, Stephen)
Date: Tue, 29 Nov 2005 08:30:34 -0400
Subject: [R] label point
Message-ID: <2AA35FD5B398FF4FBD14B80C81D53B3D07FF56F8@msgmarbio03.bio.dfo.ca>

You could also use plot(matrix,type="n") to avoid overplot. 

-----Original Message-----
From: Petr Pikal [mailto:petr.pikal at precheza.cz] 
Sent: November 29, 2005 3:05 AM
To: Robert; r-help at stat.math.ethz.ch
Subject: Re: [R] label point

Hallo

On 28 Nov 2005 at 21:33, Robert wrote:

Date sent:      	Mon, 28 Nov 2005 21:33:24 -0800 (PST)
From:           	Robert <samrobertsmith at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] label point

>   Hi, I have a matrix:
>   [,1] [,2]
>   [1,] 11 31
>   [2,] 44 50
>   [3,] 23 100
>   [4,] 90 31
>   I use plot to draw the four points. Is there any way to label the
>   point? for insatnce, for (11,31), it is "1", for (44,50), it is "4".
>   Thanks!
> 


label.vec<-c(1, 4, l2, l3.....)
plot(matrix)
text(matrix, label.vec)

Should do the trick but it overplots your points, so you need to 
shift positions e.g. by

text(matrix+c(some offset,0), label.vec)

HTH
Petr




> 
> 
> 
> ---------------------------------
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue Nov 29 13:36:42 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 29 Nov 2005 13:36:42 +0100 (CET)
Subject: [R] Cochran-Armitage-trend-test
In-Reply-To: <038b01c59386$95d37090$6e8b6882@didacdom.stat.ucl.ac.be>
References: <038b01c59386$95d37090$6e8b6882@didacdom.stat.ucl.ac.be>
Message-ID: <Pine.LNX.4.51.0511291329070.7430@artemis.imbe.med.uni-erlangen.de>


On Thu, 28 Jul 2005, Eric Lecoutre wrote:

> Hi there,
>
> I often do receive some mails about this piece of code regarding
> Cochran-Armitage or Mantel Chi square.

This is a very late reply but maybe still interesting. The
conditional version of the Cochran-Armitage test for trend
for proportions is part of the framework implemented by `coin'
and can be computed as follows:

R> library("coin")
R>
R> ### Cochran-Armitage trend test for proportions
R> ### Lung tumors in female mice exposed to 1,2-dichloroethane
R> ### Encyclopedia of Biostatistics (Armitage & Colton, 1998),
R> ### Chapter Trend Test for Counts and Proportions, page 4578, Table 2
R> lungtumor <- data.frame(dose = rep(c(0, 1, 2), c(40, 50, 48)),
+                         tumor = c(rep(c(0, 1), c(38, 2)),
+                                   rep(c(0, 1), c(43, 7)),
+                                   rep(c(0, 1), c(33, 15))))
R> table(lungtumor$dose, lungtumor$tumor)

     0  1
  0 38  2
  1 43  7
  2 33 15
R>
R> ### Cochran-Armitage test (permutation equivalent to correlation
R> ### between dose and tumor), cf. Table 2 for results
R> independence_test(tumor ~ dose, data = lungtumor, teststat = "quad")

        Asymptotic General Independence Test

data:  tumor by dose
T = 10.6381, df = 1, p-value = 0.001108

R>
R> ### approximate distribution by Monte-Carlo
R> independence_test(tumor ~ dose, data = lungtumor, teststat = "quad",
+     distribution = approximate(B = 50000))

        Approximative General Independence Test

data:  tumor by dose
T = 10.6381, p-value = 0.00092

R>

Moreover, we can use a linear-by-linear association test with the
appropriate scores which, of course, could be changed as well

R> ### linear-by-linear association test with scores 0, 1, 2
R> ### is identical with Cochran-Armitage test
R> lungtumor$dose <- ordered(lungtumor$dose)
R> independence_test(tumor ~ dose, data = lungtumor, teststat = "quad",
+                   scores = list(dose = c(0, 1, 2)))

        Asymptotic General Independence Test

data:  tumor by groups 0 < 1 < 2
T = 10.6381, df = 1, p-value = 0.001108

R>

Just for the record ...

Best,

Torsten

> The archived mail does unfortunately lack some pieces of code (function
> "scores").
> I copy there all my raw code that I did implement to mimic SAS PROC FREQ
> statistics regarding the analysis of contingency tables. Whoever is
> interested to take it and rework it a little bit (for example redefining
> outputs so that they suits a htest object) is welcome.
>
> Best wishes,
>
> Eric
>
>
> -----
>
>
>
> # R functions to provides statistics on contingency tables
> # Mimics SAS PROC FREQ outputs
> # Implementation is the one described in SAS PROC FREQ manual
>
> # Eric Lecoutre <ericlecoutre at gmail.com
>
> # Feel free to use / modify / document / add to a package
>
>
>
> #------------------------------------ UTILITARY FUNCTIONS
> ------------------------------------#
>
>
> print.ordtest=function(l,...)
> {
>  tmp=matrix(c(l$estimate,l$ASE),nrow=1)
>  dimnames(tmp)=list(l$name,c("Estimate","ASE"))
>  print(round(tmp,4),...)
> }
>
>
> compADPQ=function(x)
> {
> 	nr=nrow(x)
> 	nc=ncol(x)
> 	Aij=matrix(0,nrow=nr,ncol=nc)
> 	Dij=matrix(0,nrow=nr,ncol=nc)
> 	for (i in 1:nr)	{
> 		for (j in 1:nc)	{
>
> Aij[i,j]=sum(x[1:i,1:j])+sum(x[i:nr,j:nc])-sum(x[i,])-sum(x[,j])
>
> Dij[i,j]=sum(x[i:nr,1:j])+sum(x[1:i,j:nc])-sum(x[i,])-sum(x[,j])
> 	}}
> 	P=sum(x*Aij)
> 	Q=sum(x*Dij)
> 	return(list(Aij=Aij,Dij=Dij,P=P,Q=Q))
> }
>
>
> scores=function(x,MARGIN=1,method="table",...)
> {
> 	# MARGIN
> 	#	1 - row
> 	# 	2 - columns
>
> 	# Methods for ranks are
> 	#
> 	# x - default
> 	# rank
> 	# ridit
> 	# modridit
>
> 	if (method=="table")
> 	{
> 		if (is.null(dimnames(x))) return(1:(dim(x)[MARGIN]))
> 		else {
> 			options(warn=-1)
> 			if
> (sum(is.na(as.numeric(dimnames(x)[[MARGIN]])))>0)
> 			{
> 				out=(1:(dim(x)[MARGIN]))
> 			}
> 			else
> 			{
> 			out=(as.numeric(dimnames(x)[[MARGIN]]))
> 			}
> 			options(warn=0)
> 		}
> 	}
> 	else	{
> 	### method is a rank one
> 	Ndim=dim(x)[MARGIN]
> 	OTHERMARGIN=3-MARGIN
>
> ranks=c(0,(cumsum(apply(x,MARGIN,sum))))[1:Ndim]+(apply(x,MARGIN,sum)+1)
> /2
> 	if (method=="ranks") out=ranks
> 	if (method=="ridit") out=ranks/(sum(x))
> 	if (method=="modridit") out=ranks/(sum(x)+1)
> 	}
>
> 	return(out)
> }
>
>
> #------------------------------------ FUNCTIONS
> ------------------------------------#
>
> tablegamma=function(x)
> {
> # Statistic
> 	tmp=compADPQ(x)
> 	P=tmp$P
> 	Q=tmp$Q
> 	gamma=(P-Q)/(P+Q)
> # ASE
> 	Aij=tmp$Aij
> 	Dij=tmp$Dij
> 	tmp1=4/(P+Q)^2
> 	tmp2=sqrt(sum((Q*Aij - P*Dij)^2 * x))
> 	gamma.ASE=tmp1*tmp2
> # Test
> 	var0=(4/(P+Q)^2) * (sum(x*(Aij-Dij)^2) - ((P-Q)^2)/sum(x))
> 	tb=gamma/sqrt(var0)
> 	p.value=2*(1-pnorm(tb))
> # Output
>
> out=list(estimate=gamma,ASE=gamma.ASE,statistic=tb,p.value=p.value,name=
> "Gamma",bornes=c(-1,1))
> 	class(out)="ordtest"
> 	return(out)
> }
>
>
> tabletauc=function(x)
> {
> 	tmp=compADPQ(x)
> 	P=tmp$P
> 	Q=tmp$Q
> 	m=min(dim(x))
> 	n=sum(x)
> # statistic
>
> 	tauc=(m*(P-Q))/(n^2*(m-1))
> # ASE
> 	Aij=tmp$Aij
> 	Dij=tmp$Dij
> 	dij=Aij-Dij
> 	tmp1=2*m/((m-1)*n^2)
> 	tmp2= sum(x * dij^2) - (P-Q)^2/n
> 	ASE=tmp1*sqrt(tmp2)
>
> # Test
> 	tb=tauc/ASE
> 	p.value=2*(1-pnorm(tb))
> # Output
>
> out=list(estimate=tauc,ASE=ASE,statistic=tb,p.value=p.value,name="Kendal
> l's tau-c",bornes=c(-1,1))
> 	class(out)="ordtest"
> 	return(out)
> }
>
> tabletaub=function(x)
> {
> # Statistic
> 	tmp=compADPQ(x)
> 	P=tmp$P
> 	Q=tmp$Q
> 	n=sum(x)
> 	wr=n^2 - sum(apply(x,1,sum)^2)
> 	wc=n^2 - sum(apply(x,2,sum)^2)
> 	taub=(P-Q)/sqrt(wr*wc)
> # ASE
> 	Aij=tmp$Aij
> 	Dij=tmp$Dij
> 	w=sqrt(wr*wc)
> 	dij=Aij-Dij
> 	nidot=apply(x,1,sum)
> 	ndotj=apply(x,2,sum)
> 	n=sum(x)
> 	vij=outer(nidot,ndotj, FUN=function(a,b) return(a*wc+b*wr))
> 	tmp1=1/(w^2)
> 	tmp2= sum(x*(2*w*dij + taub*vij)^2)
> 	tmp3=n^3*taub^2*(wr+wc)^2
> 	tmp4=sqrt(tmp2-tmp3)
> 	taub.ASE=tmp1*tmp4
> # Test
> 	var0=4/(wr*wc) * (sum(x*(Aij-Dij)^2) - (P-Q)^2/n)
> 	tb=taub/sqrt(var0)
> 	p.value=2*(1-pnorm(tb))
> # Output
>
> out=list(estimate=taub,ASE=taub.ASE,statistic=tb,p.value=p.value,name="K
> endall's tau-b",bornes=c(-1,1))
> 	class(out="ordtest")
> 	return(out)
> }
>
> tablesomersD=function(x,dep=2)
> {
> 	# dep: which dimension stands for the dependant variable
> 	# 1 - ROWS
> 	# 2 - COLS
> # Statistic
> 	if (dep==1) x=t(x)
> 	tmp=compADPQ(x)
> 	P=tmp$P
> 	Q=tmp$Q
> 	n=sum(x)
> 	wr=n^2 - sum(apply(x,1,sum)^2)
> 	somers=(P-Q)/wr
> # ASE
> 	Aij=tmp$Aij
> 	Dij=tmp$Dij
> 	dij=Aij-Dij
> 	tmp1=2/wr^2
> 	tmp2=sum(x*(wr*dij - (P-Q)*(n-apply(x,1,sum)))^2)
> 	ASE=tmp1*sqrt(tmp2)
> # Test
> 	var0=4/(wr^2) * (sum(x*(Aij-Dij)^2) - (P-Q)^2/n)
> 	tb=somers/sqrt(var0)
> 	p.value=2*(1-pnorm(tb))
> # Output
> 	if (dep==1) dir="R|C" else dir= "C|R"
> 	name=paste("Somer's D",dir)
>
> out=list(estimate=somers,ASE=ASE,statistic=tb,p.value=p.value,name=name,
> bornes=c(-1,1))
> 	class(out)="ordtest"
> 	return(out)
>
>
> }
>
> #out=table.somersD(data)
>
>
>
> tablepearson=function(x,scores.type="table")
> {
>
> # Statistic
> 	sR=scores(x,1,scores.type)
> 	sC=scores(x,2,scores.type)
> 	n=sum(x)
> 	Rbar=sum(apply(x,1,sum)*sR)/n
> 	Cbar=sum(apply(x,2,sum)*sC)/n
> 	ssr=sum(x*(sR-Rbar)^2)
> 	ssc=sum(t(x)* (sC-Cbar)^2)
> 	tmpij=outer(sR,sC,FUN=function(a,b) return((a-Rbar)*(b-Cbar)))
> 	ssrc= sum(x*tmpij)
> 	v=ssrc
> 	w=sqrt(ssr*ssc)
> 	r=v/w
> # ASE
> 	bij=outer(sR,sC, FUN=function(a,b)return((a-Rbar)^2*ssc +
> (b-Cbar)^2*ssr))
> 	tmp1=1/w^2
> 	tmp2=x*(w*tmpij - (bij*v)/(2*w))^2
> 	tmp3=sum(tmp2)
> 	ASE=tmp1*sqrt(tmp3)
> # Test
> 	var0= (sum(x*tmpij) - (ssrc^2/n))/ (ssr*ssc)
> 	tb=r/sqrt(var0)
> 	p.value=2*(1-pnorm(tb))
> # Output
>
> out=list(estimate=r,ASE=ASE,statistic=tb,p.value=p.value,name="Pearson
> Correlation",bornes=c(-1,1))
> 	class(out)="ordtest"
> 	return(out)
> }
>
> # table.pearson(data)
>
>
> tablespearman=function(x)
> {
> 	# Details algorithme manuel SAS PROC FREQ page 540
> # Statistic
> 	n=sum(x)
> 	nr=nrow(x)
> 	nc=ncol(x)
> 	tmpd=cbind(expand.grid(1:nr,1:nc))
> 	ind=rep(1:(nr*nc),as.vector(x))
> 	tmp=tmpd[ind,]
> 	rhos=cor(apply(tmp,2,rank))[1,2]
> # ASE
> 	Ri=scores(x,1,"ranks")- n/2
> 	Ci=scores(x,2,"ranks")- n/2
> 	sr=apply(x,1,sum)
> 	sc=apply(x,2,sum)
> 	F=n^3 - sum(sr^3)
> 	G=n^3 - sum(sc^3)
> 	w=(1/12)*sqrt(F*G)
> 	vij=data
> 	for (i in 1:nrow(x))
> 	{
> 		qi=0
> 		if (i<nrow(x))
> 		{
> 		for (k in i:nrow(x)) qi=qi+sum(x[k,]*Ci)
> 		}
> 	}
> 	for (j in 1:ncol(x))
> 		{
> 			qj=0
> 			if (j<ncol(x))
> 			{
> 			for (k in j:ncol(x)) qj=qj+sum(x[,k]*Ri)
> 			}
> 		vij[i,j]=n*(Ri[i]*Ci[j] +
> 0.5*sum(x[i,]*Ci)+0.5*sum(data[,j]*Ri) +qi+qj)
> 		}
>
>
> 	v=sum(data*outer(Ri,Ci))
> 	wij=-n/(96*w)*outer(sr,sc,FUN=function(a,b) return(a^2*G+b^2*F))
> 	zij=w*vij-v*wij
> 	zbar=sum(data*zij)/n
> 	vard=(1/(n^2*w^4))*sum(x*(zij-zbar)^2)
> 	ASE=sqrt(vard)
> # Test
> 	vbar=sum(x*vij)/n
> 	p1=sum(x*(vij-vbar)^2)
> 	p2=n^2*w^2
> 	var0=p1/p2
> 	stat=rhos/sqrt(var0)
>
> # Output
> 	out=list(estimate=rhos,ASE=ASE,name="Spearman
> correlation",bornes=c(-1,1))
> 	class(out)="ordtest"
> 	return(out)
> }
>
> #tablespearman(data)
>
>
>
>
> tablelambdasym=function(x)
> {
> # Statistic
> 	ri = apply(x,1,max)
> 	r=max(apply(x,2,sum))
> 	n=sum(x)
> 	cj=apply(x,2,max)
> 	c=max(apply(x,1,sum))
> 	sri=sum(ri)
> 	w=2*n - r -c
> 	v=2*n - sri - sum(cj)
> 	lambda=(w-v)/w
> # ASE ...
>
> 	tmpSi=0
> 	l=min(which(apply(x,2,sum)==r))
> 	for (i in 1:length(ri))
> 	{
> 		li=min(which(x[i,]==ri[i]))
> 		if (li==l) tmpSi=tmpSi+x[i,li]
> 	}
>
> 	tmpSj=0
> 	k=min(which(apply(x,1,sum)==c))
> 	for (j in 1:length(cj))
> 	{
> 		kj=min(which(x[,j]==cj[j]))
> 		if (kj==k) tmpSj=tmpSj+x[kj,j]
> 	}
>
> 	rk=max(x[k,])
> 	cl=max(x[,l])
> 	tmpx=tmpSi+tmpSj+rk+cl
> 	y=8*n-w-v-2*tmpx
>
> 	nkl=x[k,l]
> 	tmpSij=0
> 	for (i in 1:nrow(x))
> 	{
> 		for (j in 1:ncol(x))
> 		{
> 		li=min(which(x[i,]==ri[i]))
> 		kj=min(which(x[,j]==cj[j]))
> 		tmpSij=tmpSij+x[kj,li]
> 		}
> 	}
>
>
> 	ASE=(1/(w^2))*sqrt(w*v*y-(2*(w^2)*(n-tmpSij))-2*(v^2)*(n-nkl))
> # Output
>
> 	out=list(estimate=lambda,ASE=ASE,name="Lambda
> Symetric",bornes=c(0,1))
> 	class(out)="ordtest"
> 	return(out)
>
> }
> #tablelambdasym(data)
>
> tablelambdaasym=function(x,transpose=FALSE)
> {
> # Statistic
> 	if (transpose==TRUE) x=t(x)
> 	ri = apply(x,1,max)
> 	r=max(apply(x,2,sum))
> 	sri=sum(ri)
> 	n=sum(x)
> 	lambda=(sum(ri)-r)/(n-r)
> # ASE
> 	l=min(which(apply(x,2,sum)==r))
> 	tmp=0
> 	for (i in 1:length(ri))
> 	{
> 		li=min(which(x[i,]==ri[i]))
> 		if (li==l) tmp=tmp+x[i,li]
> 	}
> 	ASE=sqrt(((n-sri)/(n-r)^3) *(sri+r-2*tmp))
> # Output
> 	if (transpose) dir="R|C" else dir= "C|R"
> 	name=paste("Lambda asymetric",dir)
> 	out=list(estimate=lambda,ASE=ASE,name=name,bornes=c(0,1))
> 	class(out)="ordtest"
> 	return(out)
> }
>
>
>
>
> tableUCA=function(x,transpose=TRUE)
> {
> 	if (transpose==TRUE) x=t(x)
> 	# Statistic
> 		n=sum(x)
> 		ni=apply(x,1,sum)
> 		nj=apply(x,2,sum)
> 		Hx=-sum((ni/n)*log(ni/n))
> 		Hy=-sum((nj/n)*log(nj/n))
> 		Hxy=-sum((x/n)*log(x/n))
> 		v=Hx+Hy- Hxy
> 		w=Hy
> 		U=v/w
> 	# ASE
> 		tmp1=1/((n)*(w^2))
> 		tmpij=0
> 		for (i in 1:nrow(x))
> 		{
> 			for (j in 1:ncol(x))
> 			{
> 			tmpij=tmpij+(  x[i,j]*  (
> Hy*log(x[i,j]/ni[i])+(Hx-Hxy)*    log(nj[j]/n)               )^2     )
> 			}
> 		}
> 		ASE=tmp1*sqrt(tmpij)
> 	# Output
> 		if (transpose) dir="R|C" else dir= "C|R"
> 		name=paste("Uncertainty Coefficient",dir)
> 		out=list(estimate=U,ASE=ASE,name=name,bornes=c(0,1))
> 		class(out)="ordtest"
> 		return(out)
> }
>
> #tableUCA(data)
>
> tableUCS=function(x)
> {
>
> 	# Statistic
> 		n=sum(x)
> 		ni=apply(x,1,sum)
> 		nj=apply(x,2,sum)
> 		Hx=-sum((ni/n)*log(ni/n))
> 		Hy=-sum((nj/n)*log(nj/n))
> 		Hxy=-sum((x/n)*log(x/n))
> 		U=(2*(Hx+Hy-Hxy))/(Hx+Hy)
> 	# ASE
> 		tmpij=0
> 		for (i in 1:nrow(x))
> 		{
> 			for (j in 1:ncol(x))
> 			{
> 			tmpij=tmpij+(  x[i,j]*
> (Hxy*log(ni[i]*nj[j]/n^2) - (Hx+Hy)*log(x[i,j]/n))^2   /(n^2*(Hx+Hy)^4)
> )
> 			}
> 		}
> 		ASE=2*sqrt(tmpij)
> 	# Output
> 		name="Uncertainty Coefficient Symetric"
> 		out=list(estimate=U,ASE=ASE,name=name,bornes=c(0,1))
> 		class(out)="ordtest"
> 		return(out)
> }
>
>
>
>
>
> tablelinear=function(x,scores.type="table")
> {
> 	r=tablepearson(x,scores.type)$estimate
> 	n=sum(x)
> 	ll=r^2*(n-1)
> 	out=list(estimate=ll)
> 	return(out)
> }
>
> tablephi=function(x)
> {
> 	if (all.equal(dim(x),c(2,2))==TRUE)
> 	{
> 	rtot=apply(x,1,sum)
> 	ctot=apply(x,2,sum)
> 	phi= det(x)/sqrt(prod(rtot)*prod(ctot))
> 	}
> 	else {
> 	Qp=chisq.test(x)$statistic
> 	phi=sqrt(Qp/sum(x))
> 	}
> 	names(phi)="phi"
> 	return(phi=phi)
> }
>
>
> tableCramerV=function(x)
> {
> 	if (all.equal(dim(x),c(2,2))==TRUE)
> 	{
> 	cramerV=tablephi(x)
> 	}
> 	else
> 	{
> 	Qp=tableChisq(x)$estimate
> 	cramerV=sqrt((Qp/n)/min(dim(x)-1))
> 	}
> 	names(cramerV)="Cramer's V"
> 	return(cramerV)
> }
>
>
> tableChisq=function(x)
> {
> 	nidot=apply(x,1,sum)
> 	ndotj=apply(x,2,sum)
> 	n=sum(nidot)
> 	eij=outer(nidot,ndotj,"*")/n
> 	R=length(nidot)
> 	C=length(ndotj)
> 	dll=(R-1)*(C-1)
> 	Qp=sum((x-eij)^2/eij)
> 	p.value=1-pchisq(Qp,dll)
>
> out=list(estimate=Qp,dll=dll,p.value=p.value,dim=c(R,C),name="Pearson's
> Chi-square")
> 	return(out)
> }
>
>
> tableChisqLR=function(x)
> {
> # Likelihood ratio Chi-squared test
> 	nidot=apply(x,1,sum)
> 	ndotj=apply(x,2,sum)
> 	n=sum(nidot)
> 	eij=outer(nidot,ndotj,"*")/n
> 	R=length(nidot)
> 	C=length(ndotj)
> 	dll=(R-1)*(C-1)
> 	G2=2*sum(x*log(x/eij))
> 	p.value=1-pchisq(G2,dll)
>
> out=list(estimate=G2,dll=dll,p.value=p.value,dim=c(R,C),name="Likelihood
> ratio Chi-square")
> 	return(out)
> }
>
> tableChisqCA=function(x)
> {
> 	if (all.equal(dim(x),c(2,2))==TRUE)
> 	{
> 		nidot=apply(x,1,sum)
> 		ndotj=apply(x,2,sum)
> 		n=sum(nidot)
> 		eij=outer(nidot,ndotj,"*")/n
> 		R=length(nidot)
> 		C=length(ndotj)
> 		dll=(R-1)*(C-1)
> 		tmp=as.vector(abs(x-eij))
> 		tmp=pmax(tmp-0.5,0)
> 		tmp=matrix(tmp,byrow=TRUE,ncol=C)
> 		Qc=sum(tmp^2/eij)
> 		p.value=1-pchisq(Qc,dll)
>
> out=list(estimate=Qc,dll=dll,p.value=p.value,dim=c(R,C),name="Continuity
> adjusted Chi-square")
> 		return(out)
> 	}
> 	else
> 	{ stop("Continuity-adjusted chi-square must be used with
> (2,2)-tables",call.=FALSE) }
> }
>
> tableChisqMH=function(x)
> {
> 	n=sum(x)
> 	G2=(n-1)*(tablepearson(x)$estimate^2)
> 	dll=1
> 	p.value=1-pchisq(G2,dll)
>
> out=list(estimate=G2,dll=dll,p.value=p.value,dim=dim(x),name="Mantel-Hae
> nszel Chi-square")
> 	return(out)
>
> }
>
> tableCC=function(x)
> {
> 	Qp=tableChisq(x)$estimate
> 	n=sum(x)
> 	P=sqrt(Qp/(Qp+n))
> 	m=min(dim(x))
>
> out=list(estimate=P,dim=dim(x),bornes=c(0,sqrt((m-1)/m)),name="Contingen
> cy coefficient")
> 	return(out)
>
> }
>
> tabletrend=function(x,transpose=FALSE)
> {
> 	if (any(dim(x)==2))
> 	{
> 	if (transpose==TRUE) {
> 	x=t(x)
> 	}
>
> 	if (dim(x)[2]!=2){stop("Cochran-Armitage test for trend must be
> used with a (R,2) table. Use transpose argument",call.=FALSE) }
>
> 	nidot=apply(x,1,sum)
> 	n=sum(nidot)
>
> 	Ri=scores(x,1,"table")
> 	Rbar=sum(nidot*Ri)/n
>
> 	s2=sum(nidot*(Ri-Rbar)^2)
> 	pdot1=sum(x[,1])/n
> 	T=sum(x[,1]*(Ri-Rbar))/sqrt(pdot1*(1-pdot1)*s2)
> 	p.value.uni=1-pnorm(abs(T))
> 	p.value.bi=2*p.value.uni
>
> out=list(estimate=T,dim=dim(x),p.value.uni=p.value.uni,p.value.bi=p.valu
> e.bi,name="Cochran-Armitage test for trend")
> 	return(out)
>
> 	}
> 	else {stop("Cochran-Armitage test for trend must be used with a
> (2,C) or a (R,2) table",call.=FALSE) }
> }
>
>
>
>
>
>
>
>
>
>
>
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
>
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
>
> If the statistics are boring, then you've got the wrong numbers. -Edward
> Tufte
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vito Ricci
> > Sent: jeudi 28 juillet 2005 16:30
> > To: r-help at stat.math.ethz.ch
> > Cc: amelie2000 at gmx.de
> > Subject: Re: [R] Cochran-Armitage-trend-test
> >
> >
> > Hi,
> > see:
> > http://finzi.psych.upenn.edu/R/Rhelp02a/archive/20396.html
> >
> > Regards,
> > Vito
> >
> >
> >
> > amelie2000 at gmx.de wrote:
> >
> > Hi!
> >
> > I am searching for the Cochran-Armitage-trend-test. Is
> > it included in an
> > R-package?
> >
> > Thank you!
> >
> >
> >
> > Diventare costruttori di soluzioni
> > Became solutions' constructors
> >
> > "The business of the statistician is to catalyze
> > the scientific learning process."
> > George E. P. Box
> >
> > "Statistical thinking will one day be as necessary for
> > efficient citizenship as the ability to read and write" H. G. Wells
> >
> > Top 10 reasons to become a Statistician
> >
> >      1. Deviation is considered normal
> >      2. We feel complete and sufficient
> >      3. We are 'mean' lovers
> >      4. Statisticians do it discretely and continuously
> >      5. We are right 95% of the time
> >      6. We can legally comment on someone's posterior distribution
> >      7. We may not be normal, but we are transformable
> >      8. We never have to say we are certain
> >      9. We are honestly significantly different
> >     10. No one wants our jobs
> >
> >
> > Visitate il portale http://www.modugno.it/
> > e in particolare la sezione su Palese
> > http://www.modugno.it/archivio/palesesanto_spirito/
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From ligges at statistik.uni-dortmund.de  Tue Nov 29 13:41:54 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 29 Nov 2005 13:41:54 +0100
Subject: [R] Indexing variables within lapply?
In-Reply-To: <438C443C.8000608@unibas.ch>
References: <438C443C.8000608@unibas.ch>
Message-ID: <438C4C92.2040607@statistik.uni-dortmund.de>

Christian Bieli wrote:

> Hello
> I am using R 2.2.0 with Windows XP.
> I've got a five element list object, each element containing two 
> dataframes of equivalent size.
> 
>  > str(mylist)
> List of 1
>  $ data1:List of 2
>   ..$ data1a       :`data.frame':    77 obs. of  63 variables:
>   .. ..$ var1    : num [1:77] 0.41375 0.00056 1.43040 1.43528 0.61730 ...
>   .. ..$ var2    : num [1:77] 1.154 1.686 0.673 0.800 0.760 ...
>   .. ..$ var3    : num [1:77] 1.245 0.575 0.934 0.461 0.757 ...
>     .
>     .
>     .
>   ..$ data1b      :`data.frame':    77 obs. of  63 variables:
>   .. ..$ var1    : num [1:77] 1284.59    7.59    1.16    8.66 2646.38 ...
>   .. ..$ var2    : num [1:77] 1.231 1.129 0.427 1.132 0.692 ...
>   .. ..$ var3    : num [1:77] 1.36 0.68 1.39 2.54 1.39 ...
>     .
>     .
>     .
>  $ data2:List of 2
>   ..$ data2a       :`data.frame':    77 obs. of  63 variables:
>   .. ..$ var1    : num [1:77] 0.41375 0.00056 1.43040 1.43528 0.61730 ...
>   .. ..$ var2    : num [1:77] 1.154 1.686 0.673 0.800 0.760 ...
>   .. ..$ var3    : num [1:77] 1.245 0.575 0.934 0.461 0.757 ...
>     .
>     .
>     .
>   ..$ data2b       :`data.frame':    77 obs. of  63 variables:
>   .. ..$ var1    : num [1:77] 1284.59    7.59    1.16    8.66 2646.38 ...
>   .. ..$ var2    : num [1:77] 1.231 1.129 0.427 1.132 0.692 ...
>   .. ..$ var3    : num [1:77] 1.36 0.68 1.39 2.54 1.39 ...
>      .
>     .
>     .
> 
> My goal is to do calculations with corresponding variables (e.g. 
> correlation between var1 of data1a and var1 of data1b) within each list 
> element.
> I tried to do it with lapply(data, myfunction,...), but I dont know if 

Without having tested:

myfunction <- function(x, var){
	cor(x[[1]][[var]], x[[2]][[var]])
}
lapply(data, myfunction, var="var1")

Uwe Ligges


> theres a way to index the two vars within lapply.
> There is certainly a way to do it by looping, but lapply is much more 
> elegant.
> 
> Thank you for your help.
> Christian
>



From samrobertsmith at yahoo.com  Tue Nov 29 13:42:33 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Tue, 29 Nov 2005 04:42:33 -0800 (PST)
Subject: [R] transform matrix
In-Reply-To: <BFB1AB20.13AB5%sdavis2@mail.nih.gov>
Message-ID: <20051129124233.51182.qmail@web30611.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/4468e841/attachment.pl

From sdavis2 at mail.nih.gov  Tue Nov 29 13:45:28 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 29 Nov 2005 07:45:28 -0500
Subject: [R] transform matrix
In-Reply-To: <20051129124233.51182.qmail@web30611.mail.mud.yahoo.com>
Message-ID: <BFB1B798.13AC9%sdavis2@mail.nih.gov>

On 11/29/05 7:42 AM, "Robert" <samrobertsmith at yahoo.com> wrote:

> 
> 
> Sean Davis <sdavis2 at mail.nih.gov> wrote:
> 
> On 11/29/05 6:46 AM, "Robert" wrote:
> 
>> I run the following code and got a wrong message. Anyway to transform test1
>> to
>> a 6 by 6 matrix? many thanks!
>>> test1
>> 1 2 3 4 5 6
>> 1 0.0000000 0.7760856 2.022222 0.6148687 3.0227028 3.2104434
>> 2 0.7760856 0.0000000 1.690790 0.2424415 2.3636083 2.5334957
>> 3 2.0222216 1.6907899 0.000000 1.5939158 1.5126344 1.7304217
>> 4 0.6148687 0.2424415 1.593916 0.0000000 2.4265906 2.6085845
>> 5 3.0227028 2.3636083 1.512634 2.4265906 0.0000000 0.2184739
>> 6 3.2104434 2.5334957 1.730422 2.6085845 0.2184739 0.0000000
>>> tryD=matrix(data = test1, nrow = 6, ncol = 6)
>> Warning message:
>> Replacement length not a multiple of the elements to replace in matrix(...)
> 
> What is test1? Try:
> 
> tryD <- as.matrix(test1)
> 
> Does that do it? 
> 
> Sean
> 
> 
> 
> 
> It works if I use tryD <- as.matrix(test1).
> But why tryD=matrix(data = test1, nrow = 6, ncol = 6) wrong?

What is test1?  What are the results of:

class(test1)
dim(test1)

Sean



From samrobertsmith at yahoo.com  Tue Nov 29 13:47:21 2005
From: samrobertsmith at yahoo.com (Robert)
Date: Tue, 29 Nov 2005 04:47:21 -0800 (PST)
Subject: [R] transform matrix
In-Reply-To: <BFB1B798.13AC9%sdavis2@mail.nih.gov>
Message-ID: <20051129124721.44637.qmail@web30610.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/162139ef/attachment.pl

From georg.otto at tuebingen.mpg.de  Mon Nov 28 18:47:38 2005
From: georg.otto at tuebingen.mpg.de (Georg Otto)
Date: Mon, 28 Nov 2005 18:47:38 +0100
Subject: [R] combine two columns
Message-ID: <ly7jas8y11.fsf@tuebingen.mpg.de>

Hi,

I have an R programming problem and I havent found anything in the
documentation yet:

I have a data matrix, in which two neighbouring columns represent
replicates of the same experiment, e.g. something like this:

       A A B B C C
row1   1 1 1 2 2 2
row2   1 1 1 1 1 2

I would like to test, if the values for the two replicates in a row
are the same or if they differ and generate a new matrix with the
results of the tests, something like this:

        A B C
row1    T F T
row2    T T F

Any hint will be appreciated!

Georg



From sdavis2 at mail.nih.gov  Tue Nov 29 14:00:38 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 29 Nov 2005 08:00:38 -0500
Subject: [R] transform matrix
In-Reply-To: <20051129124721.44637.qmail@web30610.mail.mud.yahoo.com>
Message-ID: <BFB1BB26.13AD0%sdavis2@mail.nih.gov>

On 11/29/05 7:47 AM, "Robert" <samrobertsmith at yahoo.com> wrote:

> 
> 
> Sean Davis <sdavis2 at mail.nih.gov> wrote:
> 
> On 11/29/05 7:42 AM, "Robert" wrote:
> 
>> 
>> 
>> Sean Davis wrote:
>> 
>> On 11/29/05 6:46 AM, "Robert" wrote:
>> 
>>> I run the following code and got a wrong message. Anyway to transform test1
>>> to
>>> a 6 by 6 matrix? many thanks!
>>>> test1
>>> 1 2 3 4 5 6
>>> 1 0.0000000 0.7760856 2.022222 0.6148687 3.0227028 3.2104434
>>> 2 0.7760856 0.0000000 1.690790 0.2424415 2.3636083 2.5334957
>>> 3 2.0222216 1.6907899 0.000000 1.5939158 1.5126344 1.7304217
>>> 4 0.6148687 0.2424415 1.593916 0.0000000 2.4265906 2.6085845
>>> 5 3.0227028 2.3636083 1.512634 2.4265906 0.0000000 0.2184739
>>> 6 3.2104434 2.5334957 1.730422 2.6085845 0.2184739 0.0000000
>>>> tryD=matrix(data = test1, nrow = 6, ncol = 6)
>>> Warning message:
>>> Replacement length not a multiple of the elements to replace in matrix(...)
>> 
>> What is test1? Try:
>> 
>> tryD <- as.matrix(test1)
>> 
>> Does that do it?
>> 
>> Sean
>> 
>> 
>> 
>> 
>> It works if I use tryD <- as.matrix(test1).
>> But why tryD=matrix(data = test1, nrow = 6, ncol = 6) wrong?
> 
> What is test1? What are the results of:
> 
> class(test1)
> dim(test1)
> 
> Sean
> 
> 
> 
> 
> 
>> class(test1)
> [1] "dist"
>> dim(test1)
> NULL

If you read the help for "dist", you will see that objects of class "dist"
have some other information besides the matrix itself (which is actually
stored in upper triangular form only, I believe.  So, while the "show"
method for "dist" objects shows the matrix, it isn't really a matrix in
memory, but a "dist" object.  There happens to be a coercion function to go
from "dist" to "matrix" which is why the as.matrix method works.

Sean



From zorritillito-secure at yahoo.de  Tue Nov 29 14:13:14 2005
From: zorritillito-secure at yahoo.de (zorritillito-secure@yahoo.de)
Date: Tue, 29 Nov 2005 14:13:14 +0100 (CET)
Subject: [R] SPSS and R ? do they like each other?
Message-ID: <20051129131314.67228.qmail@web86802.mail.ukl.yahoo.com>

Well, it ??s not the output table that I need to share
with SPSS, but the working data file itself. It
contains labeled variables and values and the labels
are important to me. That seems to be a problem since
? as you say ? at least read.spss ignores the labels.
If I should ever have very very much time, I might try
to write something that retains the labels.

Thanks for your answers!

Michael



From 042045003 at fudan.edu.cn  Tue Nov 29 14:32:11 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Tue, 29 Nov 2005 21:32:11 +0800
Subject: [R] SPSS and R ? do they like each other?
Message-ID: <0IQP00JNEX4G0H@mail.fudan.edu.cn>


======= 2005-11-29 21:13:14 ÄúÔÚÀ´ÐÅÖÐÐ´µÀ£º=======

>Well, it ´s not the output table that I need to share
>with SPSS, but the working data file itself. It
>contains labeled variables and values and the labels
>are important to me. That seems to be a problem since
>?as you say ?at least read.spss ignores the labels.
>If I should ever have very very much time, I might try
>to write something that retains the labels.

In fact,when using read.spss to read the spss data file into R,the labels are retained (keeped) rather than be dropped.
for example WVS is the data file be readed in,it has label.table and variable.labels.
> names(attributes(WVS.CHINA))
[1] "label.table"     "variable.labels" "names"  
and label.table is something like "value: in spss file and variable.labels are "label" in spss data file.
hope this helps.

>Thanks for your answers!
>
>Michael
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-11-29

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From sdavis2 at mail.nih.gov  Tue Nov 29 14:34:39 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 29 Nov 2005 08:34:39 -0500
Subject: [R] combine two columns
In-Reply-To: <ly7jas8y11.fsf@tuebingen.mpg.de>
Message-ID: <BFB1C31F.13ADA%sdavis2@mail.nih.gov>

On 11/28/05 12:47 PM, "Georg Otto" <georg.otto at tuebingen.mpg.de> wrote:

> Hi,
> 
> I have an R programming problem and I havent found anything in the
> documentation yet:
> 
> I have a data matrix, in which two neighbouring columns represent
> replicates of the same experiment, e.g. something like this:
> 
>      A A B B C C
> row1   1 1 1 2 2 2
> row2   1 1 1 1 1 2
> 
> I would like to test, if the values for the two replicates in a row
> are the same or if they differ and generate a new matrix with the
> results of the tests, something like this:
> 
>       A B C
> row1    T F T
> row2    T T F
> 
> Any hint will be appreciated!

 x <- matrix(c(1,1,1,2,2,2,1,1,1,1,1,2),nr=2,byrow=TRUE)
 colnames(x) <- c('A','A','B','B','C','C')
 aggregate(t(x),by=list(colnames(x)),function(y) {length(unique(y))==1})

This should be close, I think.

Sean



From jmacdon at med.umich.edu  Tue Nov 29 14:43:29 2005
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 29 Nov 2005 08:43:29 -0500
Subject: [R] What made us so popular Nov 16-20?
In-Reply-To: <x2hd9wqo29.fsf@turmalin.kubism.ku.dk>
References: <OF2A60E141.7643B758-ON882570C8.0000C835-882570C8.00014B40@epamail.epa.gov>
	<x2hd9wqo29.fsf@turmalin.kubism.ku.dk>
Message-ID: <438C5B01.8040702@med.umich.edu>

Peter Dalgaard wrote:
> Seeliger.Curt at epamail.epa.gov writes:
> 
> 
>>Duncan asks:
>>
>>>Did we get mentioned somewhere (e.g. Slashdot), or was someone just
>>>experimenting with some automated downloading?
>>
>>R was mentioned in last week's (I think) O'Reilly newsletter, which
>>included a link to a short article showing how easy it is to get R to
>>graph stuff like stock price histories.  That's the publisher, not the
>>talking head.
> 
> 
> [See other mail for the link]
> 
> 
>>For what it's worth, the article isn't worth chasing down.  It left a
>>beginner like me disappointed that R's capabilities weren't better
>>shown, and that he relied on Perl to do data manipulation.
> 
> 
> 
> Er, where did you see Perl being used? The only thing that irked me
> (admittedly, I only skimmed the article) was that he was using
> regression models to test for correlation (why not cor.test()?) and
> speculates a bit wildly about the sign of a clearly nonsignificant
> relation. It's a bit superficial, but I suspect that this sort of
> paper has to be.

I think Curt is referring to this part:

A simple starting point is a vector or array of numbers. I downloaded 
historical Standard and Poor's (S&P) 500 stock index data and wrote a 
Perl program to convert the data into a simple two-column table. Figure 
2 shows a snippet of the start of the file.

Best,

Jim


> 
> 


-- 
James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623



From Matthias.Templ at statistik.gv.at  Tue Nov 29 14:45:14 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Tue, 29 Nov 2005 14:45:14 +0100
Subject: [R] combine two columns
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAD25@xchg1.statistik.local>

Hello,

Following should work.

m <- matrix(round(runif(16,0,2)),nrow=2)
colnames(m) <- c("A","A","B","B","C","C","D","D")
m2 <- m #matrix(, nrow=dim(m)[1], ncol=dim(m)[2]/2)
z <- 1
ss <- seq(1,dim(m)[2],2)

for(j in ss){
  for(i in 1:dim(m)[1]){
    m2[i,j] <- substring(m[i,ss[z]] == m[i,ss[z]+1],1,1)
  }
  z <- z + 1
}
m2 <- m2[,ss]
colnames(m2) <- colnames(m)[ss]

When your data set is large, then you should try to code this using
apply.

Best,
Matthias

> 
> Hi,
> 
> I have an R programming problem and I havent found anything 
> in the documentation yet:
> 
> I have a data matrix, in which two neighbouring columns 
> represent replicates of the same experiment, e.g. something like this:
> 
>        A A B B C C
> row1   1 1 1 2 2 2
> row2   1 1 1 1 1 2
> 
> I would like to test, if the values for the two replicates in 
> a row are the same or if they differ and generate a new 
> matrix with the results of the tests, something like this:
> 
>         A B C
> row1    T F T
> row2    T T F
> 
> Any hint will be appreciated!
> 
> Georg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Tue Nov 29 15:00:40 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 29 Nov 2005 15:00:40 +0100
Subject: [R] combine two columns
In-Reply-To: <ly7jas8y11.fsf@tuebingen.mpg.de>
Message-ID: <438C6D18.28041.886404@localhost>

Hallo

On 28 Nov 2005 at 18:47, Georg Otto wrote:

To:             	r-help at stat.math.ethz.ch
From:           	Georg Otto <georg.otto at tuebingen.mpg.de>
Date sent:      	Mon, 28 Nov 2005 18:47:38 +0100
Subject:        	[R] combine two columns

> Hi,
> 
> I have an R programming problem and I havent found anything in the
> documentation yet:
> 
> I have a data matrix, in which two neighbouring columns represent
> replicates of the same experiment, e.g. something like this:
> 
>        A A B B C C
> row1   1 1 1 2 2 2
> row2   1 1 1 1 1 2

Not very neat but

> tabul
      A A.1 B B.1 C C.1 
 row1 1   1 1   2 2   2
 row2 1   1 1   1 1   2

 > logmat<-as.logical(apply(tabul,1,diff))
 > dim(logmat)<-c(2,5)
 > logmat
       [,1]  [,2]  [,3]  [,4]  [,5]
 [1,] FALSE  TRUE FALSE FALSE FALSE
 [2,] FALSE FALSE FALSE FALSE  TRUE


 > liche
 function (x) 
 {
     indices <- seq(along = x)
     x[indices%%2 == 1]
 }

 > !logmat[,liche(1:5)]
     [,1] [,2]  [,3]
[1,] TRUE TRUE  TRUE
[2,] TRUE TRUE FALSE
>

Finally gives the result you want.

HTH
Petr

> 
> I would like to test, if the values for the two replicates in a row
> are the same or if they differ and generate a new matrix with the
> results of the tests, something like this:
> 
>         A B C
> row1    T F T
> row2    T T F
> 
> Any hint will be appreciated!
> 
> Georg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From andy_liaw at merck.com  Tue Nov 29 15:20:53 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 29 Nov 2005 09:20:53 -0500
Subject: [R] Compiling R in C / C++
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED606@usctmx1106.merck.com>

Here are just some (probably not very coherent) rambling:

It's not very clear to me what you are looking for.  Are you looking for a
tool that compile R code into a stand-alone application?  That's what the
Matlab compiler does for Matlab code, but AFAIK is _not_ what C++/Connect or
Java/Connect for S-PLUS does.  The latter two just provide means of calling
S code from those two languages (and perhaps the other way around, too, I
don't remember), if I'm not mistaken.

There has been some effort at creating a tool for compiling R code into C.
See http://hipersoft.cs.rice.edu/rcc/index.html.

If I may say so, the tone in your message seems to suggest that you're
simply asking people to do work for free.  (Are you willing to do that
yourself?)  Open Source projects like R need efforts to go both ways.  If
everyone just ask for new features and no one contribute, the project isn't
going to get very far.  If you can not contribute code that you're asking
for, perhaps at the very least you can contribute to the R Foundation.  It's
the least you could do for not paying whatever it was to get
Matlab/Mathematica/S-PLUS but get to use R instead.

Best,
Andy

From: Jordi
> 
> I am interested in being able to use R in my own libraries, 
> written in C++.
> I have seen that in the past several people have asked about this
> possibility. In the book Programming in S it is stated that 
> it is possible
> to call R functions from a C++ called from R. Also, it seems 
> it is possible
> to do something with the R D-COM port, but at least for 
> somebody with my
> limited capabilities (and time) this is not a feasible possibility.
> 
>  
> 
> I have heard in the past days that both Matlab and 
> Mathematica have modules
> that compile the whole code into C++. I think that S-PLUS 
> does something
> similar with C++/Connect. Matlab is going to do something 
> equivalent with C#
> and Java next year.
> 
>  
> 
> These modules are pretty expensive (around 3,000 euros). A 
> question to the R
> popes: wouldn't it be a good idea to do something like this for R?
> Advantages:
> 
> - Many people like the idea to use R as an extended library, 
> and not being
> tied to a given framework, GUI, . even, some things cannot be 
> done with R
> because they need to be implemented in a stand-alone basis;
> 
> - "You have to run to stay in the same place you were" 
> (Alice): if other
> frameworks like Matlab, Mathematica and S-PLUS do it, and if 
> R does not do
> it, "we" are staying behind;
> 
> - If this were implemented in R, the cost would be 0, adding 
> an advantage
> over the other computing frameworks, where the cost of C++ 
> compiling is much
> higher.
> 
>  
> 
> I guess that what I am proposing is easier said that done. I 
> imagine that
> there are many technical problems. There are many other 
> interesting things
> to develop. Time is limited. I know. I am being egoist: I 
> probably would not
> contribute, but I would reap the advantages. I know. But I think the
> proposal makes sense.
> 
>  
> 
> What do people think about it?
> 
>  
> 
> I was just thinking in posting this email in devel, but I thought the
> general forum was better.
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From wjwest at CLEMSON.EDU  Tue Nov 29 15:28:11 2005
From: wjwest at CLEMSON.EDU (Bill West)
Date: Tue, 29 Nov 2005 09:28:11 -0500
Subject: [R] combine two columns
In-Reply-To: <ly7jas8y11.fsf@tuebingen.mpg.de>
Message-ID: <200511291428.jATES5CG008936@CLEMSON.EDU>

Yet another way...
 
a<-matrix(c(1,1,1,1,1,1,2,1,2,1,2,2),nr=2)
x<-c("A","B","C")
colnames(a)<-sort(rep(x,2))
compare<-function(x,a){all(a[1,colnames(a)==x]==a[1,x][1])}
v<-function(y,x,compare,a){sapply(x,compare,a=a[y,,drop=F])}
t(sapply(1:nrow(a),v,x,compare,a))

--Bill

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Georg Otto
Sent: Monday, November 28, 2005 12:48 PM
To: r-help at stat.math.ethz.ch
Subject: [R] combine two columns

Hi,

I have an R programming problem and I havent found anything in the
documentation yet:

I have a data matrix, in which two neighbouring columns represent replicates
of the same experiment, e.g. something like this:

       A A B B C C
row1   1 1 1 2 2 2
row2   1 1 1 1 1 2

I would like to test, if the values for the two replicates in a row are the
same or if they differ and generate a new matrix with the results of the
tests, something like this:

        A B C
row1    T F T
row2    T T F

Any hint will be appreciated!

Georg

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From levin001 at 123mail.cl  Tue Nov 29 15:57:17 2005
From: levin001 at 123mail.cl (Peyuco Porras Porras .)
Date: Tue, 29 Nov 2005 11:57:17 -0300
Subject: [R] Newbie: Translating GLM SAS code into R
Message-ID: <72928a1729608c.729608c72928a1@123mail.cl>

Dear all;

I'm looking for some help in translating the following SAS code to R. The code represents a 

factorial design plus 1 control plot (2 x 2 + 1). The data is the following
	
BLOCK	FA	FB	FC	Y
1	0	0	0	15.33
1	1	0	0	14.40
1	1	1	0	15.49
1	1	0	1	16.87
1	1	1	1	18.84
2	0	0	0	15.97
2	1	0	0	16.18
2	1	1	0	14.52
2	1	0	1	18.04
2	1	1	1	19.81
3	0	0	0	15.60
3	1	0	0	14.79
3	1	1	0	14.30
3	1	0	1	17.18
3	1	1	1	18.37
4	0	0	0	15.22
4	1	0	0	16.24
4	1	1	0	15.97
4	1	0	1	16.51
4	1	1	1	19.05

The SAS code is:

proc glm data=test;
class BLOCK FA FB FC;
model Y = BLOCK FA FB(FA) FC(FA) FB*FC(FA)/alpha=0.05;
random BLOCK/test;
lsmeans FB*FC(FA)/pdiff stderr;
quit; run;

I've tried:

library(nlme)
options(contrasts=c("contr.SAS","contr.SAS"))

data.gr<-groupedData(Y~1|BLocK,data=data)

data.gr$BLocK<-as.factor(data.gr$BLocK);data.gr$FA<-as.factor(data.gr$FA)
data.gr$FB<-as.factor(data.gr$FB);data.gr$FC<-as.factor(data.gr$FC)

mod1<-lme(Y~FA+(FB*FC)/FA,random=~1|BLOCK,data=data.gr)

but I can't get the model works and I can't figure out how can I do it.

I'll appreciate any ideas

Best regards
P. Porras



From ggrothendieck at gmail.com  Tue Nov 29 16:03:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Nov 2005 10:03:09 -0500
Subject: [R] combine two columns
In-Reply-To: <ly7jas8y11.fsf@tuebingen.mpg.de>
References: <ly7jas8y11.fsf@tuebingen.mpg.de>
Message-ID: <971536df0511290703uc34f02djb26b5db347ac893c@mail.gmail.com>

Try this:

> is.odd <- !array(0:1, ncol(mat))
> mat[,is.odd] == mat[,!is.odd]
        A     B     C
row1 TRUE FALSE  TRUE
row2 TRUE  TRUE FALSE

On 11/28/05, Georg Otto <georg.otto at tuebingen.mpg.de> wrote:
> Hi,
>
> I have an R programming problem and I havent found anything in the
> documentation yet:
>
> I have a data matrix, in which two neighbouring columns represent
> replicates of the same experiment, e.g. something like this:
>
>       A A B B C C
> row1   1 1 1 2 2 2
> row2   1 1 1 1 1 2
>
> I would like to test, if the values for the two replicates in a row
> are the same or if they differ and generate a new matrix with the
> results of the tests, something like this:
>
>        A B C
> row1    T F T
> row2    T T F
>
> Any hint will be appreciated!
>
> Georg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mmiller3 at iupui.edu  Tue Nov 29 16:14:59 2005
From: mmiller3 at iupui.edu (Michael A. Miller)
Date: Tue, 29 Nov 2005 10:14:59 -0500
Subject: [R] What made us so popular Nov 16-20?
In-Reply-To: <438B983D.1080807@stats.uwo.ca> (Duncan Murdoch's message of
	"Mon, 28 Nov 2005 18:52:29 -0500")
References: <438B983D.1080807@stats.uwo.ca>
Message-ID: <87sltfzdsc.fsf@lumen.indyrad.iupui.edu>

>>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca> writes:

    > Then I looked at this graph:

    > http://mirrors.pair.com/freebsd/stats/cran-ip.png

    > and saw that this is likely due to a huge spike in traffic
    > between Nov 16 and 20.  

FWIW, apache shows a similar spike:
http://mirrors.pair.com/freebsd/stats/apache-xfer.pgn

Mike



From kaniovsk at wifo.ac.at  Tue Nov 29 16:24:01 2005
From: kaniovsk at wifo.ac.at (Serguei Kaniovski)
Date: Tue, 29 Nov 2005 16:24:01 +0100
Subject: [R] Constraints in Quadprog
Message-ID: <438C7291.5020004@wifo.ac.at>

I'm having difficulty figuring out how to implement the
following set of constraints in Quadprog:

1). x1+x2+x3+x4=a1
2). x1+x2+x5+x6=a2
3). x1+x3+x5+x7=a3
4). x1+x2=b1
5). x1+x3=b2
6). x1+x5=b3

for the problem: MIN (x1-c1)2+(x2-c2)2+...+(x8-c8)2.

As far a I understand, "solve.QP(Dmat, dvec, Amat, bvec, meq=0,
factorized=FALSE)" reads contraints using an element-by-element
multiplication, i.e. Amat'*x, not using the matrix-product, i.e. 
Amat'%*%x, required for the sums on the left-hand-side of 1-6).

I would very much appreciate a suggestion on this problem.

Thank you,
Serguei Kaniovski



From amsa36060 at yahoo.com  Tue Nov 29 17:13:19 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Tue, 29 Nov 2005 08:13:19 -0800 (PST)
Subject: [R] Calculating the 2th power of a vector
Message-ID: <20051129161319.8141.qmail@web60411.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/affe4208/attachment.pl

From B.Rowlingson at lancaster.ac.uk  Tue Nov 29 17:30:11 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 29 Nov 2005 16:30:11 +0000
Subject: [R] Calculating the 2th power of a vector
In-Reply-To: <20051129161319.8141.qmail@web60411.mail.yahoo.com>
References: <20051129161319.8141.qmail@web60411.mail.yahoo.com>
Message-ID: <438C8213.6010404@lancaster.ac.uk>

Amir Safari wrote:

>   I simply want to calculate the 2th power of a vector without changing the sign of values. How it is possible in R ?


I'm not quite sure what you mean, but maybe:

  > x
  [1] -4 -3 -2 -1  0  1  2  3  4
  > x^2
  [1] 16  9  4  1  0  1  4  9 16

  - that obviously makes everything positive (unless any of x are complex!)

  So do:

  > x^2 * sign(x)
  [1] -16  -9  -4  -1   0   1   4   9  16

  to keep the sign. Is that what you want?

Baz



From ccleland at optonline.net  Tue Nov 29 17:30:12 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 29 Nov 2005 11:30:12 -0500
Subject: [R] Calculating the 2th power of a vector
In-Reply-To: <20051129161319.8141.qmail@web60411.mail.yahoo.com>
References: <20051129161319.8141.qmail@web60411.mail.yahoo.com>
Message-ID: <438C8214.2020108@optonline.net>

?sign

 > x <- rnorm(20)

 > x
  [1]  0.42775501  0.77370847 -2.39860006  1.75882148
  [5] -0.08231117 -0.11121029  0.93786747 -1.73588115
  [9]  0.92659445 -0.59913052  0.16399931 -1.08586327
[13]  0.30092859 -0.30683075 -0.87974642  1.65483615
[17] -0.05118100  0.70005335  1.86429650  1.12937824

 > x^2
  [1] 0.182974345 0.598624790 5.753282243 3.093453001
  [5] 0.006775129 0.012367728 0.879595393 3.013283374
  [9] 0.858577271 0.358957383 0.026895773 1.179099031
[13] 0.090558015 0.094145111 0.773953771 2.738482695
[17] 0.002619495 0.490074687 3.475601451 1.275495200

 > x^2*sign(x)
  [1]  0.182974345  0.598624790 -5.753282243
  [4]  3.093453001 -0.006775129 -0.012367728
  [7]  0.879595393 -3.013283374  0.858577271
[10] -0.358957383  0.026895773 -1.179099031
[13]  0.090558015 -0.094145111 -0.773953771
[16]  2.738482695 -0.002619495  0.490074687
[19]  3.475601451  1.275495200

Amir Safari wrote:
>  
>   
>   Hi every body ,
>   
>   I simply want to calculate the 2th power of a vector without changing the sign of values. How it is possible in R ?
>   
>   Thanks a lot for any idea.
>   Amir
>   
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From marcelodamasceno at gmail.com  Tue Nov 29 17:31:54 2005
From: marcelodamasceno at gmail.com (Marcelo Damasceno)
Date: Tue, 29 Nov 2005 14:31:54 -0200
Subject: [R] Error in plclust
Message-ID: <a55593730511290831x1f2982d5h6e34590bebbc6541@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/11e71985/attachment.pl

From maechler at stat.math.ethz.ch  Tue Nov 29 17:38:06 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 29 Nov 2005 17:38:06 +0100
Subject: [R] Calculating the 2th power of a vector
In-Reply-To: <438C8213.6010404@lancaster.ac.uk>
References: <20051129161319.8141.qmail@web60411.mail.yahoo.com>
	<438C8213.6010404@lancaster.ac.uk>
Message-ID: <17292.33774.493359.110343@stat.math.ethz.ch>

>>>>> "BaRow" == Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>
>>>>>     on Tue, 29 Nov 2005 16:30:11 +0000 writes:

    BaRow> Amir Safari wrote:
    >> I simply want to calculate the 2th power of a vector
    >> without changing the sign of values. How it is possible
    >> in R ?


    BaRow> I'm not quite sure what you mean, but maybe:

    >> x
    BaRow>   [1] -4 -3 -2 -1 0 1 2 3 4
    >> x^2
    BaRow>   [1] 16 9 4 1 0 1 4 9 16

    BaRow>   - that obviously makes everything positive (unless
    BaRow> any of x are complex!)

    BaRow>   So do:

    >> x^2 * sign(x)
    BaRow>   [1] -16 -9 -4 -1 0 1 4 9 16


    BaRow>   to keep the sign. Is that what you want?

which is
           x * abs(x)

{of course, the are more possibilities -- since the OP asked "how" .. ;-)
 but I'd guess that  x*abs(x) is close to speed-optimal}

Martin Maechler, ETH Zurich



From jordi_molins at hotmail.com  Tue Nov 29 18:05:26 2005
From: jordi_molins at hotmail.com (Jordi)
Date: Tue, 29 Nov 2005 18:05:26 +0100
Subject: [R] Compiling R in C / C++
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED606@usctmx1106.merck.com>
Message-ID: <BAY111-DAV4D4E4EE730F70A63B5025F04B0@phx.gbl>

Andy,

What I tried with my previous email was to give an idea for future
development of R. I would love to be able to do the C++ / R compiler by
myself, but unfortunately for me, I do not have the technological knowledge
to put it into practice. 

Of course, I am really grateful to all R developers that give their code for
free. I should have stated that before, because I really feel it. I am a
humble R user. If you want to call the R users (as opposed to developers)
people who ask the developers to work for free, let be it. Definitely, it is
not the way I feel.

What I had in my mind is a suggestion for the R architects. I see that there
are several R GUIs floating around, that are very good ideas in themselves.
But extrapolating this to the limit, why not making R even GUI independent?
Why not extending the FREEDOM, not only in the code license, but also
escaping from the slavery of GUIs? Just have the functions and the syntax,
everything else is accessory.

It was only my intention to present these ideas. For sure more intelligent
people than me have already thought about this, and much more deeply. I just
wanted to recall that there are some simple users out there that would be
deeply grateful if they could use R as an extended library.

Again: thank you to all R developers for making such a great computing
framework.

> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com]
> Sent: Tuesday, November 29, 2005 3:21 PM
> To: 'Jordi'; r-help at stat.math.ethz.ch
> Subject: RE: [R] Compiling R in C / C++
> 
> Here are just some (probably not very coherent) rambling:
> 
> It's not very clear to me what you are looking for.  Are you looking for a
> tool that compile R code into a stand-alone application?  That's what the
> Matlab compiler does for Matlab code, but AFAIK is _not_ what C++/Connect
> or
> Java/Connect for S-PLUS does.  The latter two just provide means of
> calling
> S code from those two languages (and perhaps the other way around, too, I
> don't remember), if I'm not mistaken.
> 
> There has been some effort at creating a tool for compiling R code into C.
> See http://hipersoft.cs.rice.edu/rcc/index.html.
> 
> If I may say so, the tone in your message seems to suggest that you're
> simply asking people to do work for free.  (Are you willing to do that
> yourself?)  Open Source projects like R need efforts to go both ways.  If
> everyone just ask for new features and no one contribute, the project
> isn't
> going to get very far.  If you can not contribute code that you're asking
> for, perhaps at the very least you can contribute to the R Foundation.
> It's
> the least you could do for not paying whatever it was to get
> Matlab/Mathematica/S-PLUS but get to use R instead.
> 
> Best,
> Andy
> 
> From: Jordi
> >
> > I am interested in being able to use R in my own libraries,
> > written in C++.
> > I have seen that in the past several people have asked about this
> > possibility. In the book Programming in S it is stated that
> > it is possible
> > to call R functions from a C++ called from R. Also, it seems
> > it is possible
> > to do something with the R D-COM port, but at least for
> > somebody with my
> > limited capabilities (and time) this is not a feasible possibility.
> >
> >
> >
> > I have heard in the past days that both Matlab and
> > Mathematica have modules
> > that compile the whole code into C++. I think that S-PLUS
> > does something
> > similar with C++/Connect. Matlab is going to do something
> > equivalent with C#
> > and Java next year.
> >
> >
> >
> > These modules are pretty expensive (around 3,000 euros). A
> > question to the R
> > popes: wouldn't it be a good idea to do something like this for R?
> > Advantages:
> >
> > - Many people like the idea to use R as an extended library,
> > and not being
> > tied to a given framework, GUI, . even, some things cannot be
> > done with R
> > because they need to be implemented in a stand-alone basis;
> >
> > - "You have to run to stay in the same place you were"
> > (Alice): if other
> > frameworks like Matlab, Mathematica and S-PLUS do it, and if
> > R does not do
> > it, "we" are staying behind;
> >
> > - If this were implemented in R, the cost would be 0, adding
> > an advantage
> > over the other computing frameworks, where the cost of C++
> > compiling is much
> > higher.
> >
> >
> >
> > I guess that what I am proposing is easier said that done. I
> > imagine that
> > there are many technical problems. There are many other
> > interesting things
> > to develop. Time is limited. I know. I am being egoist: I
> > probably would not
> > contribute, but I would reap the advantages. I know. But I think the
> > proposal makes sense.
> >
> >
> >
> > What do people think about it?
> >
> >
> >
> > I was just thinking in posting this email in devel, but I thought the
> > general forum was better.
> >
> >
> >
> >
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> 
> --------------------------------------------------------------------------
> ----
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From elvis at xlsolutions-corp.com  Tue Nov 29 18:06:04 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 29 Nov 2005 10:06:04 -0700
Subject: [R] Course***Adanced R/Splus Programming***at 3 USA locations,
	January 2006
Message-ID: <20051129100604.a108dc04937c07ba67766dad37185406.3fc46c77f7.wbe@email.secureserver.net>

Happy New Year
XSolutions Corp (www.xlsolutions-corp.com) is proud to announce
a 2-day "Advanced R/Splus programming" taught by R Development
Core Team Guru.

*********San Francisco ------------- January 09-10,2006
*********Seattle ------------------------ January 12-13,2006
*********New York -------------------- January 26-27,2006
*********Washington DC ----------  TBD

            Reserve your seat Now  (payment due after the class)

www.xlsolutions-corp.com/Radv.htm

Email Sue Turner: sue at xlsolutions-corp.com

Course outline:
- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 (S-Plus 6)
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently 

This course will also deal with lots of S-Plus efficiency issues and
any special topics from participants is welcome.
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. 

Register now to secure your seat in this course!
 
Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From eymw at hotmail.com  Tue Nov 29 18:50:37 2005
From: eymw at hotmail.com (Ed Wang)
Date: Tue, 29 Nov 2005 11:50:37 -0600
Subject: [R] help with R
Message-ID: <BAY103-F15D93E123332A67E324962C94B0@phx.gbl>

Hi,

New to R on Windows and also someone trying to learn how
to use R in batch.  My apologies if this posting is a little long
but users may better understand the problems I'm having
if I explain what I'm doing.

Goal: use R to look at seasonality on a daily level, where I have
15 years of daily data for a 246 day year (proprietary reasons
for using this number of days).

Data is a large vector of integer data, 3690 elements long.
Have found R interactive mode crashes at times dealing with
this much data, and it is painful to interactively build this
vector over and over.

Question 1: is there a way to expand the memory limits of R?
On a LINUX box I'm sure there must be a way to specifiy at
build how large arrays can be used in R, but this is Windows
so I've downloaded a precompiled binary for Windows XP.

I also want to use 245 dummy variables with a linear model
to identify non-trivial seasonality occuring on certain days.
All this makes for quite a bit to type in, which is why I've
resorted to writing a batch script.

I've tried loading the batch script via the command

>source(C:\\Program Files\\R\\rw2011\\HO_Rscript.txt)

in interactive mode but I get an error.

Question 2: can someone point out the syntax flaw in trying
to upload this batch script text file?  If I can get R to upload
the script I can atleast begin to debug it.  I am a UNIX 3.0
person by training.

If you like you can email me your comments directly

eymw at hotmail.com

I still can't easily find my way around the R-help mailing list.
Sorry, still new to this R-webpage, but enjoy using the
package so far!

Thanks.

Ed Wang



From german.lopez at ua.es  Tue Nov 29 19:01:39 2005
From: german.lopez at ua.es (german.lopez@ua.es)
Date: Tue, 29 Nov 2005 19:01:39 +0100
Subject: [R] saving AIC of intermediate models in step
Message-ID: <200511291801.jATI1d2T027463@aitana.cpd.ua.es>

Hi all,
  I'm fitting GLM's using the step or stepAIC procedures and I would 
like to save the AIC of the intermediate models. I would appreciate 
very much information about how todo this.
  Best wishes
  Germ??n L??pez



From jfontain at free.fr  Tue Nov 29 19:05:52 2005
From: jfontain at free.fr (Jean-Luc Fontaine)
Date: Tue, 29 Nov 2005 19:05:52 +0100
Subject: [R] AIC and BIC from arima()
In-Reply-To: <Pine.LNX.4.61.0511282100570.8810@gannet.stats>
References: <438B5C8E.3030509@free.fr>
	<Pine.LNX.4.61.0511282100570.8810@gannet.stats>
Message-ID: <438C9880.305@free.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Prof Brian Ripley wrote:

>> My ultimate goal is to best fit time series by comparing AICs and
>> BICs (as in Bayesian) from arima() and nnet().
> Whoa! nnet() does not do maximum likelihood fitting so AIC and BIC
> are not even defined. On the other hand, ?WWWusage has an example
> of choosing an ARIMA fit by AIC.
Thanks: I looked at it.

>> I looked at the arima.R source code, but I am afraid I do not
>> understand it. What I only miss really is the number of
>> parameters p, where: AIC = n*log(S/n) + 2*p with S the squared
>> residuals and n the number of observations. Can I get p from
>> arima() (for both non and seasonal cases) result?

> By reading the help page: coef: a vector of AR, MA and regression
> coefficients, which can be extracted by the 'coef' method. so
> length(fit$coef) will tell you how many parameters you have fitted,
> and if you read on aic: the AIC value corresponding to the
> log-likelihood. Only valid for 'method = "ML"' fits.
I saw that but that did not match my formula so I thought it was another
AIC...

> You give us no idea where you got the formula for 'AIC' from, but
> it is not that introduced by Akaike (1973, 4) and commonly used in
> time-series (and by arima()). I think you are applying a formula
> applicable to linear regression for independent observations,
> incorrectly. There are really are a lot of subtleties here, and
> although p is well-defined, n is not. Thus applying Schwarz's
> criterion (aka BIC in one of its senses) is not at all clearcut, a
> not uncommon situation with non-iid sampling.

I got the formula from the nnts package, where p is the number of weights
and n the number of fitted values in the nnet case. I naively thought I
could use such formula for both arima and nnet.

I guess that at my level of (in)competence, I could just stick to the
squared residuals for comparing arima and nnet results?
I was also thinking of, in my application, letting the user choose a
test period for fitting. For example, for a 12 month data span, the test
period would be the last month, and fitting would be done by using the
first 11 months, predicting the 12th month and comparing with the actual
data.
Or even use as criterion a weighted combination of residuals for the 11
first months and the last month?

Thank you very much for all the pointers and your patience.

- --

Jean-Luc Fontaine  http://jfontain.free.fr/
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.1 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFDjJiAkG/MMvcT1qQRAjKuAKCKDJTxVCzDZpBspHg6KTY5ZoKBMACdFTAf
26/brCmF5UcRO78pWOqb7jI=
=fHLw
-----END PGP SIGNATURE-----



From srini_iyyer_bio at yahoo.com  Tue Nov 29 19:16:00 2005
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Tue, 29 Nov 2005 10:16:00 -0800 (PST)
Subject: [R] R software on 64bit - Intel Xeon processor
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BAD25@xchg1.statistik.local>
Message-ID: <20051129181600.63685.qmail@web31615.mail.mud.yahoo.com>

Dear Group, 
 I have a machine which has a 64bit Intel?? Xeon?
Processor 3.00GHz, 2MB L2 Cache 6T302N - [ 221-7984 ]
processor. 
(Dell Precision Workstation 670n Intel?? Xeon?
Processor)

The OS is RedHat Enterprise Linux version 4 (for
64bit). 

I went to /bin/linux/redhat/el4/i386 on CRAN FTP site.
 I have no clue if any of these RPMs are suitable for
this machines configuration. 

Could any one point me to an appropriate RPM that I
can download and install it on this machine. 

Thank you. 

Sr



From jjmichael at cc.usu.edu  Tue Nov 29 12:30:58 2005
From: jjmichael at cc.usu.edu (Jake Michaelson)
Date: Tue, 29 Nov 2005 11:30:58 +0000
Subject: [R] help combining mtext and strwrap?
Message-ID: <200511291130.58086.jjmichael@cc.usu.edu>

Hi all,

I've got some image plots on which I'd like to include some gene information 
(in the margins using mtext).  Unfortunately, the description is rather long 
and will need to be wrapped to fit on several lines.  From what I know about 
mtext, it's really only meant for single-line labels, not paragraphs.

Here's some sample code of the idea I'm trying to accomplish:

notes=c("Repressible alkaline phosphatase, a glycoprotein localized to the 
vacuole; regulated by levels of inorganic phosphate and by a system 
consisting of Pho4p, Pho9p, Pho80p, Pho81p and Pho85p; dephosphorylates 
phosphotyrosyl peptides")

par(mar=c(10,3,10,3))

image(as.matrix(c(1,2,3,4,5)))

mtext(strwrap(notes, width=60), line=5, side=3)


..as you can see, the long text plots over itself and doesn't wrap.  Does 
anyone know how to include a paragraph in the margins?

Thanks in advance!

--Jake



From tring at gvdnet.dk  Tue Nov 29 19:37:58 2005
From: tring at gvdnet.dk (Troels Ring)
Date: Tue, 29 Nov 2005 19:37:58 +0100
Subject: [R] drawing a circle using symbols
Message-ID: <6.2.3.4.0.20051129192044.04c6a9e0@home.gvdnet.dk>

Dear friends, I was drawing a circle with centrum in (1,-1) and 
radius 5 to show my girl that the line y=3*x+1 goes through (1,4) and 
(-2,-5) of the circle, but on Windows XP, R 2.20 the drawing was not 
good at all, and the known solutions were not shown in the graph. I 
guess I got it wrong? Is this use not intended ?

Best wishes
Troels Ring, MD
Aalborg, Denmark

symbols(x=1,y=-1,circles=5,inches=FALSE, xlim=c(-10,10),ylim=c(-10,10))
curve(3*x+1,-10,10,1000,add=T)
abline(v=c(-2,1))
abline(h=c(-1,-5,4))



From Eric.Archer at noaa.gov  Tue Nov 29 19:41:53 2005
From: Eric.Archer at noaa.gov (Eric Archer)
Date: Tue, 29 Nov 2005 10:41:53 -0800
Subject: [R] plotting filled.contour without color key
Message-ID: <438CA0F1.1080305@noaa.gov>

List,

I'd like to plot a filled.contour() plot, but exclude the color key that 
is printed on the left side.  I understand that the key.axis argument 
allows for control of the axes around the key, but there doesn't seem to 
be an argument that eliminates it all together.  I have looked in the 
help files for the function and the archives, but have not come across 
an answer or clue to the answer.  Is this possible?

Thanks in advance.

-- 

Eric Archer, Ph.D.
NOAA-SWFSC
8604 La Jolla Shores Dr.
La Jolla, CA 92037
858-546-7121,7003(FAX)
eric.archer at noaa.gov


"Lighthouses are more helpful than churches."
    - Benjamin Franklin

"Cogita tute" - Think for yourself



From gunter.berton at gene.com  Tue Nov 29 20:05:02 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 29 Nov 2005 11:05:02 -0800
Subject: [R] help with R
In-Reply-To: <BAY103-F15D93E123332A67E324962C94B0@phx.gbl>
Message-ID: <200511291905.jATJ51eS024074@meitner.gene.com>

You're not telling us something or there's a problem with your R build: a
3960 element vectors of integer is tiny and will not cause R to crash. 

Regarding your regression model. You do **not** need dummy variables in R.
Please read the docs (e.g. AN INTRODUCTION TO R) and help files on lm() and
factor() to see how to do linear modeling in R. lag() and diff() may also be
relevant. OTOH, R has many better ways to model time series and seasonality,
both in base R and numerous add-on packages. Try help.search('time series')
and RSiteSearch('time series')

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ed Wang
> Sent: Tuesday, November 29, 2005 9:51 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] help with R
> 
> Hi,
> 
> New to R on Windows and also someone trying to learn how
> to use R in batch.  My apologies if this posting is a little long
> but users may better understand the problems I'm having
> if I explain what I'm doing.
> 
> Goal: use R to look at seasonality on a daily level, where I have
> 15 years of daily data for a 246 day year (proprietary reasons
> for using this number of days).
> 
> Data is a large vector of integer data, 3690 elements long.
> Have found R interactive mode crashes at times dealing with
> this much data, and it is painful to interactively build this
> vector over and over.
> 
> Question 1: is there a way to expand the memory limits of R?
> On a LINUX box I'm sure there must be a way to specifiy at
> build how large arrays can be used in R, but this is Windows
> so I've downloaded a precompiled binary for Windows XP.
> 
> I also want to use 245 dummy variables with a linear model
> to identify non-trivial seasonality occuring on certain days.
> All this makes for quite a bit to type in, which is why I've
> resorted to writing a batch script.
> 
> I've tried loading the batch script via the command
> 
> >source(C:\\Program Files\\R\\rw2011\\HO_Rscript.txt)
> 
> in interactive mode but I get an error.
> 
> Question 2: can someone point out the syntax flaw in trying
> to upload this batch script text file?  If I can get R to upload
> the script I can atleast begin to debug it.  I am a UNIX 3.0
> person by training.
> 
> If you like you can email me your comments directly
> 
> eymw at hotmail.com
> 
> I still can't easily find my way around the R-help mailing list.
> Sorry, still new to this R-webpage, but enjoy using the
> package so far!
> 
> Thanks.
> 
> Ed Wang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.murrell at auckland.ac.nz  Tue Nov 29 20:05:29 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 30 Nov 2005 08:05:29 +1300
Subject: [R] drawing a circle using symbols
In-Reply-To: <6.2.3.4.0.20051129192044.04c6a9e0@home.gvdnet.dk>
References: <6.2.3.4.0.20051129192044.04c6a9e0@home.gvdnet.dk>
Message-ID: <438CA679.4060505@stat.auckland.ac.nz>

Hi


Troels Ring wrote:
> Dear friends, I was drawing a circle with centrum in (1,-1) and 
> radius 5 to show my girl that the line y=3*x+1 goes through (1,4) and 
> (-2,-5) of the circle, but on Windows XP, R 2.20 the drawing was not 
> good at all, and the known solutions were not shown in the graph. I 
> guess I got it wrong? Is this use not intended ?
> 
> Best wishes
> Troels Ring, MD
> Aalborg, Denmark
> 
> symbols(x=1,y=-1,circles=5,inches=FALSE, xlim=c(-10,10),ylim=c(-10,10))
> curve(3*x+1,-10,10,1000,add=T)
> abline(v=c(-2,1))
> abline(h=c(-1,-5,4))


The important part may be that your plot is not square.   In the help 
file for symbols() it says (NOTE the "x axis") ...

   inches: If 'inches' is 'FALSE', the units are taken to be those of
           the x axis.

If you precede your code with ...

   par(pty="s")

... does the result look better?

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From ripley at stats.ox.ac.uk  Tue Nov 29 20:05:41 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Nov 2005 19:05:41 +0000 (GMT)
Subject: [R] help combining mtext and strwrap?
In-Reply-To: <200511291130.58086.jjmichael@cc.usu.edu>
References: <200511291130.58086.jjmichael@cc.usu.edu>
Message-ID: <Pine.LNX.4.61.0511291856220.17035@gannet.stats>

On Tue, 29 Nov 2005, Jake Michaelson wrote:

> I've got some image plots on which I'd like to include some gene information
> (in the margins using mtext).  Unfortunately, the description is rather long
> and will need to be wrapped to fit on several lines.  From what I know about
> mtext, it's really only meant for single-line labels, not paragraphs.
>
> Here's some sample code of the idea I'm trying to accomplish:
>
> notes=c("Repressible alkaline phosphatase, a glycoprotein localized to the
> vacuole; regulated by levels of inorganic phosphate and by a system
> consisting of Pho4p, Pho9p, Pho80p, Pho81p and Pho85p; dephosphorylates
> phosphotyrosyl peptides")
>
> par(mar=c(10,3,10,3))
>
> image(as.matrix(c(1,2,3,4,5)))
>
> mtext(strwrap(notes, width=60), line=5, side=3)
>
> ..as you can see, the long text plots over itself and doesn't wrap.  Does
> anyone know how to include a paragraph in the margins?

The clue is 'line = 5'.  You asked for this to be plotted on one line.
You can strsplit the result and plot the lines of text separately. E.g.

par(mar=c(10,3,10,3))
image(as.matrix(c(1,2,3,4,5)))
res <- strsplit( strwrap(notes, width=60), "\n")
for(i in seq(along=res)) mtext(res[[i]], line = 10-i, side=3)

Or just use title(), which seems to be what you are trying to emulate.

title(strwrap(notes, width=60), cex.main=1, font.main=1, line=5)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov 29 20:11:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Nov 2005 19:11:42 +0000 (GMT)
Subject: [R] drawing a circle using symbols
In-Reply-To: <6.2.3.4.0.20051129192044.04c6a9e0@home.gvdnet.dk>
References: <6.2.3.4.0.20051129192044.04c6a9e0@home.gvdnet.dk>
Message-ID: <Pine.LNX.4.61.0511291907000.17035@gannet.stats>

?symbols says

   inches: If 'inches' is 'FALSE', the units are taken to be those of
           the x axis.

Note, 'the x axis' and you have not ensured the x and y axes have the same 
scale.  Try

plot(c(-10,10), c(-10,10), asp=1, type="n")
symbols(x=1,y=-1,circles=5,inches=FALSE, add=TRUE
curve(3*x+1,-10,10,1000,add=TRUE)
abline(v=c(-2,1))
abline(h=c(-1,-5,4))

which works for me.  (eqscplot in MASS is another way to do this.)

On Tue, 29 Nov 2005, Troels Ring wrote:

> Dear friends, I was drawing a circle with centrum in (1,-1) and
> radius 5 to show my girl that the line y=3*x+1 goes through (1,4) and
> (-2,-5) of the circle, but on Windows XP, R 2.20 the drawing was not
> good at all, and the known solutions were not shown in the graph. I
> guess I got it wrong? Is this use not intended ?
>
> Best wishes
> Troels Ring, MD
> Aalborg, Denmark
>
> symbols(x=1,y=-1,circles=5,inches=FALSE, xlim=c(-10,10),ylim=c(-10,10))
> curve(3*x+1,-10,10,1000,add=T)
> abline(v=c(-2,1))
> abline(h=c(-1,-5,4))
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at pdf.com  Tue Nov 29 20:10:34 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 29 Nov 2005 13:10:34 -0600
Subject: [R] drawing a circle using symbols
In-Reply-To: <6.2.3.4.0.20051129192044.04c6a9e0@home.gvdnet.dk>
References: <6.2.3.4.0.20051129192044.04c6a9e0@home.gvdnet.dk>
Message-ID: <438CA7AA.8040506@pdf.com>



Troels Ring wrote:
> Dear friends, I was drawing a circle with centrum in (1,-1) and 
> radius 5 to show my girl that the line y=3*x+1 goes through (1,4) and 
> (-2,-5) of the circle, but on Windows XP, R 2.20 the drawing was not 
> good at all, and the known solutions were not shown in the graph. I 
> guess I got it wrong? Is this use not intended ?
> 
> Best wishes
> Troels Ring, MD
> Aalborg, Denmark
> 
> symbols(x=1,y=-1,circles=5,inches=FALSE, xlim=c(-10,10),ylim=c(-10,10))
> curve(3*x+1,-10,10,1000,add=T)
> abline(v=c(-2,1))
> abline(h=c(-1,-5,4))
> 

Hi, Troels,

The aspect ratio of y to x is not 1. So, the symbol is visually a circle 
but in reality an ellipse. Try

symbols(x = 1, y = -1, circles = 5, inches = FALSE,
         asp = 1, xlim = c(-10, 10), ylim = c(-10, 10))
curve(3 * x + 1, -10, 10, 1000, add = TRUE)
abline(v = c(-2, 1))
abline(h = c(-1, -5, 4))

HTH,

--sundar



From tring at gvdnet.dk  Tue Nov 29 20:45:47 2005
From: tring at gvdnet.dk (Troels Ring)
Date: Tue, 29 Nov 2005 20:45:47 +0100
Subject: [R] drawing a circle using symbols
In-Reply-To: <438CA679.4060505@stat.auckland.ac.nz>
References: <6.2.3.4.0.20051129192044.04c6a9e0@home.gvdnet.dk>
	<438CA679.4060505@stat.auckland.ac.nz>
Message-ID: <6.2.3.4.0.20051129204258.04fb20d0@home.gvdnet.dk>

Thanks a lot, Paul!
Best wishes
Troels

At 20:05 29-11-2005, you wrote:
>Hi
>
>
>Troels Ring wrote:
>>Dear friends, I was drawing a circle with centrum in (1,-1) and 
>>radius 5 to show my girl that the line y=3*x+1 goes through (1,4) 
>>and (-2,-5) of the circle, but on Windows XP, R 2.20 the drawing 
>>was not good at all, and the known solutions were not shown in the 
>>graph. I guess I got it wrong? Is this use not intended ?
>>Best wishes
>>Troels Ring, MD
>>Aalborg, Denmark
>>symbols(x=1,y=-1,circles=5,inches=FALSE, xlim=c(-10,10),ylim=c(-10,10))
>>curve(3*x+1,-10,10,1000,add=T)
>>abline(v=c(-2,1))
>>abline(h=c(-1,-5,4))
>
>
>The important part may be that your plot is not square.   In the 
>help file for symbols() it says (NOTE the "x axis") ...
>
>   inches: If 'inches' is 'FALSE', the units are taken to be those of
>           the x axis.
>
>If you precede your code with ...
>
>   par(pty="s")
>
>... does the result look better?
>
>Paul
>--
>Dr Paul Murrell
>Department of Statistics
>The University of Auckland
>Private Bag 92019
>Auckland
>New Zealand
>64 9 3737599 x85392
>paul at stat.auckland.ac.nz
>http://www.stat.auckland.ac.nz/~paul/



From ripley at stats.ox.ac.uk  Tue Nov 29 21:43:48 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Nov 2005 20:43:48 +0000 (GMT)
Subject: [R] R software on 64bit - Intel Xeon processor
In-Reply-To: <20051129181600.63685.qmail@web31615.mail.mud.yahoo.com>
References: <20051129181600.63685.qmail@web31615.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0511292041090.18489@gannet.stats>

You don't seem to know your OS spec, so how can we guess?
You chip can run various different OSes.  RH claim to have
RHEL4 for AMD64/EM64T, but not `for 64bit'.

Use uname -a.  If it mentions ix86 (for x=3,4,5,6 or perhaps 7) use that 
RPM.  I expect it will mention x86_64.  In that case you may need to 
install from the sources.  One way to do so is to install the SRPM, 
rpmbuild that and then install it.  But building from the source tarball 
is also a cinch, and will avoid RH's broken blas library (if you have that 
installed).  The other advantage is that you can install the current 
R-patched rather than R-2.2.0 and benefit from all the patches.

Of course, it is possible that RedHat has an RPM (they do for FC3 and
FC4), so have you checked their repositories?


On Tue, 29 Nov 2005, Srinivas Iyyer wrote:

> Dear Group,
> I have a machine which has a 64bit Intel? Xeon?
> Processor 3.00GHz, 2MB L2 Cache 6T302N - [ 221-7984 ]
> processor.
> (Dell Precision Workstation 670n Intel? Xeon?
> Processor)
>
> The OS is RedHat Enterprise Linux version 4 (for
> 64bit).
>
> I went to /bin/linux/redhat/el4/i386 on CRAN FTP site.
> I have no clue if any of these RPMs are suitable for
> this machines configuration.
>
> Could any one point me to an appropriate RPM that I
> can download and install it on this machine.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From HDoran at air.org  Tue Nov 29 21:52:45 2005
From: HDoran at air.org (Doran, Harold)
Date: Tue, 29 Nov 2005 15:52:45 -0500
Subject: [R] rlogis() in simulation
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A0101032B@dc1ex3.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/ff4661b0/attachment.pl

From claus.atzenbeck at freenet.de  Tue Nov 29 21:53:39 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Tue, 29 Nov 2005 21:53:39 +0100 (CET)
Subject: [R] Games-Howell, Gabriel, Hochberg
In-Reply-To: <Pine.OSX.4.61.0511281451470.4752@rgnb-d9b8743f.pool.mediaways.net>
References: <Pine.OSX.4.61.0511281451470.4752@rgnb-d9b8743f.pool.mediaways.net>
Message-ID: <Pine.OSX.4.61.0511292131020.13751@rgnb-d9b86cf6.pool.mediaways.net>

On Mon, 28 Nov 2005, Claus Atzenbeck wrote:

> I read a book about statistics in psychology. The authors use SPSS. They
> talk about post hoc tests after ANOVA finds significant effects:
>
>     - Gabriel's procedure (for equal or slightly different sample sizes)
>     - Hochberg's GT2 (for different sample sizes)
>     - Games-Howell procedure (for populations with unequal variances)
>
> I could not find them in R. Do they not exist in R or are there any
> equivalents?

I saw a message today by Brian Ripley at
<http://www.r-project.org/nocvs/mail/r-help/2002/0565.html> stating that
there are only a few multiple comparison tests in R.

How do you calculate post hoc multiple comparisons tests with R for
normal distributed samples with different variances?

Would you claim that I savely can use TukeyHSD even for unequal
variances?

Claus



From wilm at biophys.uni-duesseldorf.de  Tue Nov 29 23:10:36 2005
From: wilm at biophys.uni-duesseldorf.de (Andreas Wilm)
Date: Tue, 29 Nov 2005 23:10:36 +0100
Subject: [R] Superimpose Histograms
Message-ID: <438CD1DC.3050905@biophys.uni-duesseldorf.de>

Hi all,

I have data which is represented as a histogram and want to add more
data / another histogram to this plot using another color. That is I
need to superimpose multiple histograms.
But have no idea how to do this.

Can anybody please give me a hint?

Thanks,
Andreas

-- 

Andreas Wilm

Institut fuer Physikalische Biologie
Heinrich-Heine-Universitaet Duesseldorf
http://www.biophys.uni-duesseldorf.de

E-Mail: wilm at biophys.uni-duesseldorf.de / 0x7C68FBCC
Phone:  +49 211 8115966



From ggrothendieck at gmail.com  Tue Nov 29 23:49:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Nov 2005 17:49:35 -0500
Subject: [R] combine two columns
In-Reply-To: <971536df0511290703uc34f02djb26b5db347ac893c@mail.gmail.com>
References: <ly7jas8y11.fsf@tuebingen.mpg.de>
	<971536df0511290703uc34f02djb26b5db347ac893c@mail.gmail.com>
Message-ID: <971536df0511291449p35dfbd00i29028bf27628b3b8@mail.gmail.com>

And here is one minor variation of this:

odd <- seq(1, ncol(mat), 2)
mat[,odd] == mat[,odd+1]

On 11/29/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> > is.odd <- !array(0:1, ncol(mat))
> > mat[,is.odd] == mat[,!is.odd]
>        A     B     C
> row1 TRUE FALSE  TRUE
> row2 TRUE  TRUE FALSE
>
> On 11/28/05, Georg Otto <georg.otto at tuebingen.mpg.de> wrote:
> > Hi,
> >
> > I have an R programming problem and I havent found anything in the
> > documentation yet:
> >
> > I have a data matrix, in which two neighbouring columns represent
> > replicates of the same experiment, e.g. something like this:
> >
> >       A A B B C C
> > row1   1 1 1 2 2 2
> > row2   1 1 1 1 1 2
> >
> > I would like to test, if the values for the two replicates in a row
> > are the same or if they differ and generate a new matrix with the
> > results of the tests, something like this:
> >
> >        A B C
> > row1    T F T
> > row2    T T F
> >
> > Any hint will be appreciated!
> >
> > Georg
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From hb at maths.lth.se  Wed Nov 30 00:08:55 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 30 Nov 2005 10:08:55 +1100
Subject: [R] Superimpose Histograms
In-Reply-To: <438CD1DC.3050905@biophys.uni-duesseldorf.de>
References: <438CD1DC.3050905@biophys.uni-duesseldorf.de>
Message-ID: <438CDF87.1060407@maths.lth.se>

See

library(R.basic)
example(plot.histogram)

  x1 <- rnorm(1000,  0.4, 0.8)
  x2 <- rnorm(1000,  0.0, 1.0)
  x3 <- rnorm(1000, -1.0, 1.0)
  hist(x1, width=0.33, offset=0.00, col="blue", xlim=c(-4,4),
       main="Histogram of x1, x2 & x3",
       xlab="x1 - blue, x2 - red, x3 - green")
  hist(x2, width=0.33, offset=0.33, col="red", add=TRUE)
  hist(x3, width=0.33, offset=0.66, col="green", add=TRUE)

It overloads the default plot.histogram() (called when you do hist()) so 
it takes arguments 'offset' and 'width' too.

You can the package following the instructions at http://www.braju.com/R/.

BTW, I'm sure some would add, so I do it here instead, that it is better 
to plot density curves, cf. ?density.

Cheers

Henrik

Andreas Wilm wrote:
> Hi all,
> 
> I have data which is represented as a histogram and want to add more
> data / another histogram to this plot using another color. That is I
> need to superimpose multiple histograms.
> But have no idea how to do this.
> 
> Can anybody please give me a hint?
> 
> Thanks,
> Andreas
>



From aliscla at yahoo.com  Wed Nov 30 00:35:26 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Tue, 29 Nov 2005 15:35:26 -0800 (PST)
Subject: [R] floor()
Message-ID: <20051129233526.7861.qmail@web61219.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/0dc411e1/attachment.pl

From berwin at maths.uwa.edu.au  Wed Nov 30 00:47:04 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Wed, 30 Nov 2005 07:47:04 +0800
Subject: [R] floor()
In-Reply-To: <20051129233526.7861.qmail@web61219.mail.yahoo.com>
References: <20051129233526.7861.qmail@web61219.mail.yahoo.com>
Message-ID: <17292.59512.874859.723715@bossiaea.maths.uwa.edu.au>

G'day Werner,

>>>>> "WB" == Werner Bier <aliscla at yahoo.com> writes:

    >> floor((5.05-floor(5))*100)
    WB> [1] 4

    WB> I would expect 5, or am I wrong?
You are wrong. :)

Consider:

> (5.05-floor(5))*100
[1] 5
> (5.05-floor(5))*100 - 5
[1] -1.776357e-14

and read FAQ 7.31

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From maustin at amgen.com  Wed Nov 30 00:48:31 2005
From: maustin at amgen.com (Austin, Matt)
Date: Tue, 29 Nov 2005 15:48:31 -0800
Subject: [R] floor()
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD4FB@teal-exch.amgen.com>

I believe this is a FAQ.

Examine:

> format((5.05-floor(5))*100, nsmall=16)
[1] "4.9999999999999822"

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Werner Bier
> Sent: Tuesday, November 29, 2005 3:35 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] floor()
> 
> 
> Dear All,
>    
>   Is this right? 
>    
>   >  floor((5.05-floor(5))*100)
> [1] 4
> 
>   I would expect 5, or am I wrong?
>    
>   Thanks and regards,
>   W
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From aliscla at yahoo.com  Wed Nov 30 01:02:41 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Tue, 29 Nov 2005 16:02:41 -0800 (PST)
Subject: [R] floor()
In-Reply-To: <E7D5AB4811D20B489622AABA9C53859109DAD4FB@teal-exch.amgen.com>
Message-ID: <20051130000241.65658.qmail@web61217.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/32ed06cf/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 30 01:09:47 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 30 Nov 2005 00:09:47 -0000 (GMT)
Subject: [R] floor()
In-Reply-To: <20051129233526.7861.qmail@web61219.mail.yahoo.com>
Message-ID: <XFMail.051130000947.Ted.Harding@nessie.mcc.ac.uk>

On 29-Nov-05 Werner Bier wrote:
> Dear All,
>    
>   Is this right? 
>    
>   >  floor((5.05-floor(5))*100)
> [1] 4
> 
>   I would expect 5, or am I wrong?
>    
>   Thanks and regards,
>   W

It may seem reasonable to expect it, but in the case of R
(and most other computer languages) you would be wrong.

The reason:

  print((5.05-floor(5))*100,digits=20)
  [1] 4.9999999999999822

whose floor() is 4.

The underlying reason for this and all similar phenomena is
the slight imprecision of floating-point arithmetic when the
fractional part is not a multiple of 1/2^k for some k. Since
0.05 = 1/20 and 20 = 4*5, you have a factor 1/5 in there and
the imprecision will occur.

If you really *know* what you are doing in a particular context,
you can guard against it by a deliberate tiny mistake, such as

  dtm <- 1e-13
  floor((5.05-floor(5))*100 + dtm)
  [1] 5

but you have to be careful that you don't let this happen when
it should not happen. And you have to choose your dtm with
care: 1e-14 is not good enough!

Though, since the trouble really arises at the (5.05-floor(5))
level, you could use, more judiciously,

  dtm <- 1e-15
  floor((5.05-floor(5)+dtm)*100)
  [1] 5

and here 1e-16 won't work.

Check:

  print((5.05-floor(5)),digits=20)
  [1] 0.049999999999999822

where the last "9" is the 15th digit after the ".".

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 30-Nov-05                                       Time: 00:09:44
------------------------------ XFMail ------------------------------



From Hong.Ooi at iag.com.au  Wed Nov 30 01:34:41 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 30 Nov 2005 11:34:41 +1100
Subject: [R] rlogis() in simulation
Message-ID: <200511300034.jAU0Y7CS009077@hypatia.math.ethz.ch>


_______________________________________________________________________________________


Well, first the variance of a logistic distribution with scale parameter
s is not s^2, but (pi^2 * s^2)/3 (see ?rlogis). So if you want the
distribution to have a variance of .25, this implies s should be about
sqrt(3*0.25)/pi ~ 0.27567.



-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
(02) 9292 1566

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
Sent: Wednesday, 30 November 2005 7:53 AM
To: r-help at stat.math.ethz.ch
Subject: [R] rlogis() in simulation

Dear List:

We are generating data such that students are clustered in schools for
some item response data for a simulation study. One component of our
simulation is to generate measurement error from a logistic distribution
with a mean of 0 and standard deviation of 1.7 to match the logistic
curve of the Rasch model.

We are generating an error term for each of the 40 hypothetical test
items a student would respond to. So, we create 40 error terms for each
of the N students. This error term is a composite of a "school effect"
and a student-specific effect. The school effect is the variance term
common to all students in the school whereas the student effect varies
over N.

We want the variance of the school effect, e_gk, to be .25 and we want
the variance of the student-specific effect, e_gik, to be 1.7 - .25 =
1.45 so that the total variance sums to about 1.7.

Below is a self contained example of a small portion of the simulation.
However, I seem to misunderstand the scale and location parameters in
the rlogis function. Can anyone suggest how I might specify the variance
of the school effect to be .25 and for the student effect to be 1.45 as
I describe above? You can see my erroneous code below.

Thanks,
Harold


## Note, the subscripts are a little messy as I am changing some
notation. But it should be transparent (I hope)

N   <- 5000 # Number of students
J   <- 50   # Number of schools
N_j <- N/J  # Number of students in each school
rps  <- rep(N_j, J)

error <- matrix(numeric(N * 40), ncol = 40)
for(i in 1:40){
e_gik   <- rlogis(N, 0, sqrt(1.7-.25) ) 
e_gk    <- rep(rlogis(J, 0, sqrt(.25) ), rps) 
error[,i] <- e_gk + e_gik
}


Windows XP

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.0            
year     2005           
month    10             
day      06             
svn rev  35749          
language R          

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From wilm at biophys.uni-duesseldorf.de  Wed Nov 30 01:39:20 2005
From: wilm at biophys.uni-duesseldorf.de (Andreas Wilm)
Date: Wed, 30 Nov 2005 01:39:20 +0100
Subject: [R] Superimpose Histograms
In-Reply-To: <438CDF87.1060407@maths.lth.se>
References: <438CD1DC.3050905@biophys.uni-duesseldorf.de>
	<438CDF87.1060407@maths.lth.se>
Message-ID: <438CF4B8.2000204@biophys.uni-duesseldorf.de>

> library(R.basic)
> example(plot.histogram)
> 
>  x1 <- rnorm(1000,  0.4, 0.8)
>  x2 <- rnorm(1000,  0.0, 1.0)
>  x3 <- rnorm(1000, -1.0, 1.0)
>  hist(x1, width=0.33, offset=0.00, col="blue", xlim=c(-4,4),
>       main="Histogram of x1, x2 & x3",
>       xlab="x1 - blue, x2 - red, x3 - green")
>  hist(x2, width=0.33, offset=0.33, col="red", add=TRUE)
>  hist(x3, width=0.33, offset=0.66, col="green", add=TRUE)
> 
> It overloads the default plot.histogram() (called when you do hist()) so
> it takes arguments 'offset' and 'width' too.


Great! That's what I was lookin for.
Thanks Henrik.

Andreas


-- 

Andreas Wilm

Institut fuer Physikalische Biologie
Heinrich-Heine-Universitaet Duesseldorf
http://www.biophys.uni-duesseldorf.de

E-Mail: wilm at biophys.uni-duesseldorf.de / 0x7C68FBCC
Phone:  +49 211 8115966



From vasu.akkineni at gmail.com  Wed Nov 30 04:10:59 2005
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Tue, 29 Nov 2005 22:10:59 -0500
Subject: [R] Row-wise data retrieval
Message-ID: <3b67376c0511291910k667d9401x638992a104b75914@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051129/de41d552/attachment.pl

From hb at maths.lth.se  Wed Nov 30 04:29:01 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 30 Nov 2005 14:29:01 +1100
Subject: [R] Row-wise data retrieval
In-Reply-To: <3b67376c0511291910k667d9401x638992a104b75914@mail.gmail.com>
References: <3b67376c0511291910k667d9401x638992a104b75914@mail.gmail.com>
Message-ID: <438D1C7D.9030408@maths.lth.se>

Vasundhara Akkineni wrote:
> I want to retrieve data row wise from a data frame. My data frame is as
> below:
> 
> data<-read.table("table.txt", header=TRUE)
 >
 > how can i get row-wise data?

Examples:

data[1,]
data[2,]

for (rr in 1:nrow(data))
   data[rr,]

rows <- c(1, 5:8, 3)
data[rows,]

/Henrik

> how can i get row-wise data?
> 
> Thanks,
> Vasu.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Wed Nov 30 05:33:16 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 29 Nov 2005 20:33:16 -0800
Subject: [R] spatial-time smoothing
In-Reply-To: <29ba3d720511240419k53495a09u28d652204b4fd95a@mail.gmail.com>
References: <29ba3d720511240252k21d99851ga83ad84163ba89ba@mail.gmail.com>
	<29ba3d720511240419k53495a09u28d652204b4fd95a@mail.gmail.com>
Message-ID: <438D2B8C.5070806@pdf.com>

	  Have you considered the "pastecs" package 
(http://finzi.psych.upenn.edu/R/library/pastecs/html/00Index.html)? 
This was the first of 2 hits for RSiteSearch("space-time 
interpolation").  Alternatively, have you considered the "locfit" 
package, mentioned in 2 of the 5 hits to RSiteSearch("multidimensional 
interpolation").

	  hope this helps.
	  spencer graves

Andrea Aimi wrote:

> Hi all,
> I'm looking for to interpolate hourly temperature date collected from more
> than 140 automatic weather station (irregularly spaced) using 4 independent
> variable:
> 
> 1-2) geografic coordinates (x,y) (from DEM - 40m)
> 3) altitude (z) (from DEM - 40m)
> 4) solar radiation (from a model calculated with grass: r.sun)
> 
> In addition, I would like to use also "time" variable (e.g.: hours). So this
> will be not only a spatial model, but rather a time-space interpolation in
> order to calculate the estimated temperature for each hour in each x,y
> location placed in the study region. The method that I'd like to use is
> thin-plate spline because I think that this will be the better one for my
> survey (but I'm not sure; what do you think about it?)
> In any case the "heart" of the issue is how I can do it. I readen some
> papers and looked the reference manual for several R packages, but I don't
> understand wich is the better method to use for a spatial-time spline
> smoothing.
> 
> Has anybody an idea how to solve a problem like this or that know some paper
> that can be useful for this pourpose?
> 
> Thank you in advance
> 
> Andrea
> 
> --------------------------------------------
> Andrea Aimi
> Department of Agriculture and Forestry
> Institute of Entomology
> Padua - Italy
> e-mail: aimi.andrea at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Wed Nov 30 05:51:32 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 29 Nov 2005 20:51:32 -0800
Subject: [R] How to test a time series fit the Poisson or other process?
In-Reply-To: <200511250101.jAP11TuG028852@hypatia.math.ethz.ch>
References: <200511250101.jAP11TuG028852@hypatia.math.ethz.ch>
Message-ID: <438D2FD4.8040900@pdf.com>

	  I just did  RSiteSearch("poisson time series").  The second and third
of 75 hits seemed relevant to your question.  (e.g.,
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/58054.html)  Some of the
other responses did not seem relevant, but I didn't look at all of them.
 This response menitoned Jim Lindsey, whose "R code" web site is
"http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html".

	  Hope this helps.  If I had a long time series of Poisson counts, I'd
be tempted to try a standard time series model on the square root of the
counts.  If the results were dramatically different from what I got from
some more sophisticated modeling strategy, I'd look very carefully at
both to make sure I hadn't made a mistake some place.  If it were not
that important, I might just apply standard time series techniques to
the square roots of the counts and go on to the next task.

	  hope this helps.
	  spencer graves
p.s.  If you'd like more information from this listserve, PLEASE do read
the posting guide! "www.R-project.org/posting-guide.html".  I believe
that people who follow that guide generally get quicker, more useful
replies.  This is especially true for those who supply a simple, toy
example in a few lines of R code that someone else can copy from an
email into R, test a few ideas, and craft a reply in a very few minutes.

¹ãÐÇ wrote:

> Hi, R-Help,
> I am a newbie.
> what I concern most recently is the analysis of the time series,
> But there are a lot of package in my eyes.
> All I want to try is as follow:
> How to test whether a time series fit the Poisson or other process in R? 
> 
> Thank you very much in advance.
> 
> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡guangxing at ict.ac.cn
> ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-11-25
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From vietnguyen at fastmail.fm  Wed Nov 30 07:07:41 2005
From: vietnguyen at fastmail.fm (Viet Nguyen)
Date: Wed, 30 Nov 2005 17:07:41 +1100
Subject: [R] RNetCDF seg fault
Message-ID: <438D41AD.2020403@fastmail.fm>

Dear RNetCDF developers,

I haven't been able to load RNetCDF in R for a while.  I wonder if this 
is a bug or a problem with my installation.

I'm using Debian testing.

 > library(RNetCDF)
Segmentation fault
5:01pm(dongda)~>R --version
R 2.2.0 (2005-10-06).
Copyright (C) 2005 R Development Core Team

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under the terms of the GNU
General Public License.  For more information about these matters,
see http://www.gnu.org/copyleft/gpl.html.
5:01pm(dongda)~>uname -a
Linux dongda 2.6.11 #8 SMP Sun Mar 20 21:09:51 CET 2005 i686 GNU/Linux



From LJin at lbl.gov  Wed Nov 30 07:36:17 2005
From: LJin at lbl.gov (Ling Jin)
Date: Tue, 29 Nov 2005 22:36:17 -0800
Subject: [R] cospectrum
Message-ID: <3d06da3d277e.3d277e3d06da@lbl.gov>

Hi, Does anyone know which function in R can compute cross-spectrum of 
two time series, and what lib is needed? I found ccf, which only gives 
cross-correlation. I need to carry that further by doing a Fourier 
transform.

Thanks,

Ling



From xl252 at cam.ac.uk  Wed Nov 30 08:02:18 2005
From: xl252 at cam.ac.uk (Xiaofan Li)
Date: Wed, 30 Nov 2005 07:02:18 -0000
Subject: [R] String values as data marks on X/Y axes?
Message-ID: <000501c5f57c$01af6970$050e3c50$@ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/79a473cf/attachment.pl

From xl252 at cam.ac.uk  Wed Nov 30 08:03:13 2005
From: xl252 at cam.ac.uk (Xiaofan Li)
Date: Wed, 30 Nov 2005 07:03:13 -0000
Subject: [R] Recall: String values as data marks on X/Y axes?
Message-ID: <!&!GAAAAAAAAAAh6TtT+mDZSaP23LQW6VOiwoAAABgAAAAAAAAAIek7U/pg2Umj9ty0FulTosQyIwAAAAAAEAAAAOAPyMWG/z5Khe7px9+w0ogpAAAAU3RyaW5nIHZhbHVlcyBhcyBkYXRhIG1hcmtzIG9uIFgvWSBheGVzPwA=@cam.ac.uk>

Xiaofan Li would like to recall the message, "String values as data marks on
X/Y axes?".

From xiaofan.mlist at gmail.com  Wed Nov 30 08:03:36 2005
From: xiaofan.mlist at gmail.com (Xiaofan Li)
Date: Wed, 30 Nov 2005 07:03:36 -0000
Subject: [R] String values as data marks on X/Y axes?
Message-ID: <!&!GAAAAAAAAAAh6TtT+mDZSaP23LQW6VOiwoAAABgAAAAAAAAAIek7U/pg2Umj9ty0FulTosQyIwAAAAAAEAAAAOAPyMWG/z5Khe7px9+w0ogpAAAAU3RyaW5nIHZhbHVlcyBhcyBkYXRhIG1hcmtzIG9uIFgvWSBheGVzPwA=@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/263354bb/attachment.pl

From ripley at stats.ox.ac.uk  Wed Nov 30 08:33:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Nov 2005 07:33:27 +0000 (GMT)
Subject: [R] cospectrum
In-Reply-To: <3d06da3d277e.3d277e3d06da@lbl.gov>
References: <3d06da3d277e.3d277e3d06da@lbl.gov>
Message-ID: <Pine.LNX.4.61.0511300730550.26367@gannet.stats>

On Tue, 29 Nov 2005, Ling Jin wrote:

> Hi, Does anyone know which function in R can compute cross-spectrum of
> two time series, and what lib is needed? I found ccf, which only gives
> cross-correlation. I need to carry that further by doing a Fourier
> transform.

?spectrum: Remmeber R has multivariate time series.

By `lib' did you mean `package'?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Nov 30 08:40:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Nov 2005 07:40:00 +0000 (GMT)
Subject: [R] RNetCDF seg fault
In-Reply-To: <438D41AD.2020403@fastmail.fm>
References: <438D41AD.2020403@fastmail.fm>
Message-ID: <Pine.LNX.4.61.0511300733350.26367@gannet.stats>

This is not the address for the `RNetCDF developers': see the DESCRIPTION 
file for the maintainer's address.

Note that this package does pass the CRAN dailiy testing (on Debian Linux) 
so it looks like your own problem rather than a general one:

 	http://cran.r-project.org/src/contrib/checkSummary.html

Can you not at least use gdb to identify where the segfault is?

On Wed, 30 Nov 2005, Viet Nguyen wrote:

> Dear RNetCDF developers,
>
> I haven't been able to load RNetCDF in R for a while.  I wonder if this
> is a bug or a problem with my installation.
>
> I'm using Debian testing.
>
> > library(RNetCDF)
> Segmentation fault
> 5:01pm(dongda)~>R --version
> R 2.2.0 (2005-10-06).
> Copyright (C) 2005 R Development Core Team
>
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under the terms of the GNU
> General Public License.  For more information about these matters,
> see http://www.gnu.org/copyleft/gpl.html.
> 5:01pm(dongda)~>uname -a
> Linux dongda 2.6.11 #8 SMP Sun Mar 20 21:09:51 CET 2005 i686 GNU/Linux
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pmilin at ff.ns.ac.yu  Wed Nov 30 08:53:39 2005
From: pmilin at ff.ns.ac.yu (Petar Milin)
Date: Wed, 30 Nov 2005 08:53:39 +0100
Subject: [R] How to solve allocation problem in lme() analysis?
Message-ID: <1133337219.9360.15.camel@localhost.localdomain>

Hello!
I am running analysis on the data from 4 experiments, with approximately
4600 rows (cases). My working model is:
fitA1 = lme(RT~F1+F2+L,random=~1|Experiment/Subject,data=data)

Model works very fine, but if I try to check whether the effect of L
depends on Experiments/Subjects with:
fitA2 = lme(RT~F1+F2+L,random=~1+L|Experiment/Subject,data=data)
[with the idea to make: anova(fitA1,fitA2)]

> analysis crashes with the message:
> Error: cannot allocate vector of size 2481574 Kb
> In addition: Warning message:
> Fewer observations than random effects in all level 2 groups in:
> lme.formula(RT ~ F1 + F2 + L, random = ~1 + L | Experiment/Subject,

Can anyone help me with this issue? Thanks in advance.

Sincerely,
Petar Milin
Assistant Professor
Department of Psychology
University of Novi Sad
Serbia and Montenegro



From Elizabeth.Boakes at ioz.ac.uk  Wed Nov 30 09:10:35 2005
From: Elizabeth.Boakes at ioz.ac.uk (Elizabeth Boakes)
Date: Wed, 30 Nov 2005 08:10:35 -0000
Subject: [R] likelihood ratio tests using glmmPQL
Message-ID: <41E1ED29E5E8E34BBDD8B82CFA1A9D04083DBD@ZSL26.zsl.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/014a18ec/attachment.pl

From berg.august at gmail.com  Wed Nov 30 10:25:31 2005
From: berg.august at gmail.com (August Berg)
Date: Wed, 30 Nov 2005 01:25:31 -0800
Subject: [R] about kidpack package
Message-ID: <71cea5aa0511300125g7e06ce4awf581353ff3646ebf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/6286ecc1/attachment.pl

From maechler at stat.math.ethz.ch  Wed Nov 30 10:27:35 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 30 Nov 2005 10:27:35 +0100
Subject: [R] nicer matrix construction: rbind()
In-Reply-To: <BFB1C31F.13ADA%sdavis2@mail.nih.gov>
References: <ly7jas8y11.fsf@tuebingen.mpg.de>
	<BFB1C31F.13ADA%sdavis2@mail.nih.gov>
Message-ID: <17293.28807.117410.14661@stat.math.ethz.ch>

Just a small remark on "R coding style" :

>>>>> "Sean" == Sean Davis <sdavis2 at mail.nih.gov>
>>>>>     on Tue, 29 Nov 2005 08:34:39 -0500 writes:

      Sean> <............>

      Sean> x <- matrix(c(1,1,1,2,2,2,1,1,1,1,1,2),nr=2,byrow=TRUE)

      Sean>  <.....>
            (a very helpful answer to Georg's question; thanks, Sean!)

I've seen example code like this in many places,
and I'd like advocate a more readable alternative

   x <- rbind(c(1,1,1, 2,2,2)
              c(1,1,1, 1,1,2))

{efficiency really not being of any concern;
 we are talking about didactical examples}.

I believe  rbind() -- and indenting ("white space"
in general!) should be used much more for matrix construction --
because of quick "visual validation"
at least for small examples.

{and if you'd use ESS (http://ESS.r-project.org/), 
 a simple <Tab> key press automatically indents the 2nd line correctly!}

Martin Maechler, ETH Zurich



From ripley at stats.ox.ac.uk  Wed Nov 30 10:55:48 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Nov 2005 09:55:48 +0000 (GMT)
Subject: [R] nicer matrix construction: rbind()
In-Reply-To: <17293.28807.117410.14661@stat.math.ethz.ch>
References: <ly7jas8y11.fsf@tuebingen.mpg.de>
	<BFB1C31F.13ADA%sdavis2@mail.nih.gov>
	<17293.28807.117410.14661@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0511300945060.794@gannet.stats>

On Wed, 30 Nov 2005, Martin Maechler wrote:

> Just a small remark on "R coding style" :
>
>>>>>> "Sean" == Sean Davis <sdavis2 at mail.nih.gov>
>>>>>>     on Tue, 29 Nov 2005 08:34:39 -0500 writes:
>
>      Sean> <............>
>
>      Sean> x <- matrix(c(1,1,1,2,2,2,1,1,1,1,1,2),nr=2,byrow=TRUE)
>
>      Sean>  <.....>
>            (a very helpful answer to Georg's question; thanks, Sean!)
>
> I've seen example code like this in many places,
> and I'd like advocate a more readable alternative
>
>   x <- rbind(c(1,1,1, 2,2,2)
>              c(1,1,1, 1,1,2))
>
> {efficiency really not being of any concern;
> we are talking about didactical examples}.

But introducing unnecessary concepts is a concern, and I think

x <- matrix(c(1, 1, 1, 2, 2, 2,
               1, 1, 1, 1, 1, 2),
             nrow = 2, byrow = TRUE)

is clearer (and does not leave me wondering about the non-obvious choice 
of spaces).

Abbreviating argument names in didactical examples (or in R code) is a bad 
practice and sometimes clobbers you.  For example whoever used 'lab' in 
the examples of axis() invited confusion between the argument 
'labels' and the graphical par 'lab', confusion which duly got propagated 
to plot.default having a 'lab' argument and passing it nowhere.

> I believe  rbind() -- and indenting ("white space"
> in general!) should be used much more for matrix construction --
> because of quick "visual validation"
> at least for small examples.
>
> {and if you'd use ESS (http://ESS.r-project.org/),
> a simple <Tab> key press automatically indents the 2nd line correctly!}
>
> Martin Maechler, ETH Zurich

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Giuseppe.Palermo at bo.infn.it  Wed Nov 30 11:19:10 2005
From: Giuseppe.Palermo at bo.infn.it (Giuseppe.Palermo@bo.infn.it)
Date: Wed, 30 Nov 2005 11:19:10 +0100
Subject: [R] Random Effects for One-Way Anova
Message-ID: <20051130111910.0golkebc5cwg8ook@lnxm.bo.infn.it>

Hello to All.
I'd want to use a one-way ANOVA. This means that I have only one factor, with,
lets say, 5 levels.
I made a dataframe, called "DATA", with two Columns:
A, that is my response, and it is "class numerical".
B, that defines the different levels of my factor, and it is "class factor".

If I want to use a fixed effect model,
I know that the formula I have to use is:
lm.1 <- lm(A~B,data=DATA)
anova(lm.1)

My questions is:
which formula should I use if I want to use a random effects model?
I think I should use "lme", but I don't know how.

I hope to get a reply
Best wishes to all
Giuseppe



From henrik.parn at bio.ntnu.no  Wed Nov 30 11:37:15 2005
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Wed, 30 Nov 2005 11:37:15 +0100
Subject: [R] SciViews-R_0.8-9 Console problem
Message-ID: <438D80DB.8040006@bio.ntnu.no>

Dear R users,

I successfully installed SciViews the other day. However, when I try to 
run it now,  the command/script window does not appear. Strange. Well, 
actually I see a tendency to a script window (in the lower part of the 
sciview window where it is suppose to be) during start up, but when the 
program is entirely open, the script window is gone.

I have tried to uninstall and reinstall latest versions available of R, 
Sciviews and Tinn-R several times, but it does not solve the problem. I 
have tried both the 'Detailed installation of Sciviews-R' and 'Manually 
installing additional R packages'.

Another 'new' problem I did not have before is the default size of the 
Sciviews window - both the upper and lower part 'disappears' outside my 
screen when starting up. And in the upper 'R-window' two prompts appear 
with maybe 6-7 lines in between. However, as soon as I hit a key the 
lower of these to prompt disappears. But still no script editor.

I suspect I have missed something fundamental...but what?

Any suggestions that can help me is appreciated!

Thanks!

Henrik

PS I have tried to mail to support at sciviews.org, but the mail just bounces.

I have WinXP, PIII, 512 RAM

R.Version()
$platform
[1] "i386-pc-mingw32"
$arch
[1] "i386"
$os
[1] "mingw32"
$system
[1] "i386, mingw32"
$status
[1] ""
$major
[1] "2"
$minor
[1] "2.0"
$year
[1] "2005"
$month
[1] "10"
$day
[1] "06"
$"svn rev"
[1] "35749"
$language
[1] "R"

<>capabilities("tcltk")
tcltk TRUE

SciViews version: SciViews-R_0.8-9Setup.exe

 From the SciViews R console:
search()

[1] ".GlobalEnv"        "package:Rcmdr"     "package:car"     [4] 
"package:svGUI"     "package:svViews"   "package:svIO"    [7] 
"package:svMisc"    "package:R2HTML"    "package:tcltk"   [10] 
"package:methods"   "package:stats"     "package:graphics"
[13] "package:grDevices" "package:utils"     "package:datasets"
[16] "TempEnv"           "RcmdrEnv"          "Autoloads"       [19] 
"package:base"

-- 
************************
Henrik P??rn
Department of Biology
NTNU
7491 Trondheim
Norway

+47 735 96282 (office)
+47 909 89 255 (mobile)
+47 735 96100 (fax)



From fcombes at gmail.com  Wed Nov 30 12:00:45 2005
From: fcombes at gmail.com (Florence Combes)
Date: Wed, 30 Nov 2005 12:00:45 +0100
Subject: [R] help with R
In-Reply-To: <BAY103-F15D93E123332A67E324962C94B0@phx.gbl>
References: <BAY103-F15D93E123332A67E324962C94B0@phx.gbl>
Message-ID: <73dae3060511300300q6eb2afakda6023d41de77306@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/827dc2b0/attachment.pl

From spencer.graves at pdf.com  Wed Nov 30 12:03:22 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 30 Nov 2005 03:03:22 -0800
Subject: [R] glmmPQL
In-Reply-To: <4019.83.52.58.131.1132952056.squirrel@goliat1.ugr.es>
References: <4019.83.52.58.131.1132952056.squirrel@goliat1.ugr.es>
Message-ID: <438D86FA.2010702@pdf.com>

	  1.  Did you try "anova(f1)"?  The documentation for "glmmPQL" says it 
produces an object of class lme, and "anova.glm" might not work properly 
for it.  Also, did you try "summary(f1)", as suggested in the example in 
"?glmmPQL"?

	  2.  Have you considered using "lmer" in the "lme4" package?  As far 
as I know, this is the most up-to-date and state-of-the art function for 
this kind of analysis.  For documentation on "lmer", I'd start with Doug 
Bates, "Fitting linear mixed models in R", R News, 5(1):  27-30, May 
2005, and "Linear mixed model implementation in lmer", July 26, 2005, 
distributed with lme4 and stored on my hard drive under 
"~\R\R-2.2.0\library\lme4\doc\Implementation.pdf".  I might also consult 
Pinheiro and Bates (2000) Mixed-Effects Models in S and S-PLUS 
(Springer), which is my primary source for mixed models generally.

	  3.  If you'd like more help with this, I suggest you PLEASE do read 
the posting guide! "www.R-project.org/posting-guide.html", especially 
the bit about supplying a simple, reproducible example that someone like 
me can copy from your email into R to see if they get the same error 
message.  I and others are much more likely to have the time to test a 
simple, self-contained example than to try to replicate your problem 
from scratch.

	  hope this helps.
	  spencer graves

jmgreyes at ugr.es wrote:

> Hi,
> 
> My name is Jos?? Mar??a G??mez, and I am pretty new in R. Thus, I apologize
> deeply if my questions are extremmely na??ve.I have checked several
> available books and URL's, without finding any answer.
> 
> I'm trying to fit Generalized Linear Mixed Models via PQL. Below I provide
> the structure of my data set. Year and Plot are random variables. Fate is
> the binomial dependent. I have severe problems after calling glmmPQL:
> 
> 1) I would like to nest Plot within Year, but I don't know how to argument
> it.
> 
> 2) I'm not sure the analysis I fit (see below) is actually considering
> Year and Plot as random.
> 
> 3) No way to obtain an analysis of deviance for the GLMM fit.
> 
> 4) Year appears without degrees of freedom.
> 
> As you see, too many problems. Thank you very much for any help.
> JM
> _____________________________________________
> 
> 
>>summary (Ifate)
> 
>  Year    Plot        Fate         Hab           Peso             Dist
>  A:664   A:359   cache :  91   Oak  :621   Min.   :0.1903   Min.   :1.362
>  B:574   B:427   comida:1147   Open : 94   1st Qu.:0.5515   1st Qu.:1.847
>          C:135                 Pine :159   Median :0.6670   Median :2.137
>          D: 97                 Rock : 23   Mean   :0.6674   Mean   :2.200
>          E:220                 Shrub:341   3rd Qu.:0.7597   3rd Qu.:2.476
>                                            Max.   :1.1386   Max.   :3.872
> 
>>library (MASS)
>>library (nlme)
>>f1<-glmmPQL(Fate~Year+Plot+Hab+Peso+Dist, random=~1|Year/Plot,
> 
> family=binomial, data=Ifate)
> 
> 
>>anova.glm(f1)
> 
> Error in eval(expr, envir, enclos) : Object "Fate" not found
> 
> 
>>anova(f1)
> 
>             numDF denDF   F-value p-value
> (Intercept)     1  1223 124.10634  <.0001
> Year            1     0   0.00271     NaN
> Plot            4     3   0.96230  0.5343
> Hab             4  1223   6.03162  0.0001
> Peso            1  1223   4.34225  0.0374
> Dist            1  1223   5.97248  0.0147
> Warning messages:
> 1: NaNs produced in: pf(q, df1, df2, lower.tail, log.p)
> 2: NAs introduced by coercion
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Giuseppe.Palermo at bo.infn.it  Wed Nov 30 12:11:02 2005
From: Giuseppe.Palermo at bo.infn.it (Giuseppe.Palermo@bo.infn.it)
Date: Wed, 30 Nov 2005 12:11:02 +0100
Subject: [R] Random Effects for One-Way Anova
In-Reply-To: <018901c5f59a$2bd97f10$2b18a7c0@alice>
References: <20051130111910.0golkebc5cwg8ook@lnxm.bo.infn.it>
	<018901c5f59a$2bd97f10$2b18a7c0@alice>
Message-ID: <20051130121102.kaqd01sytcwgg4oo@lnxm.bo.infn.it>

I'm sorry if I wasn't clear....
There are repeated measures for each level of my factor.
For instance, the dataframe could look like that:

A B
12.0 1
12.3 1
15 1
12.9 2
16.7 2
15.4 2
23.5 3
9.6 3
7.8 3

In order to fit a random effects, is right to apply:
lme(A~1,data=DATA,random=~1|B)    ?

Giuseppe




Quoting giovanni parrinello <parrinel at med.unibs.it>:

> La domanda ?? poco chiara! Ci sono misure ripetute?
> Senza queste informazioni non so pu?? rispondere..
> Giovanni Parrinello
> ----- Original Message -----
> From: <Giuseppe.Palermo at bo.infn.it>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, November 30, 2005 11:19 AM
> Subject: [R] Random Effects for One-Way Anova
>
>
>> Hello to All.
>> I'd want to use a one-way ANOVA. This means that I have only one factor,
> with,
>> lets say, 5 levels.
>> I made a dataframe, called "DATA", with two Columns:
>> A, that is my response, and it is "class numerical".
>> B, that defines the different levels of my factor, and it is "class
> factor".
>>
>> If I want to use a fixed effect model,
>> I know that the formula I have to use is:
>> lm.1 <- lm(A~B,data=DATA)
>> anova(lm.1)
>>
>> My questions is:
>> which formula should I use if I want to use a random effects model?
>> I think I should use "lme", but I don't know how.
>>
>> I hope to get a reply
>> Best wishes to all
>> Giuseppe
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>>
>
>



From andersdetermigigen at hotmail.com  Tue Nov 29 20:51:13 2005
From: andersdetermigigen at hotmail.com (anders superanders)
Date: Tue, 29 Nov 2005 19:51:13 +0000
Subject: [R] permutation test for linear models with continuous covariates
Message-ID: <BAY110-F30ED0C3D41C56644E5EAF6C44B0@phx.gbl>

Hi I was wondering if there is a permutation test available in R for linear 
models with continuous dependent covariates. I want to do a test like the 
one shown here.

bmi<-rnorm(100,25)
x<-c(rep(0,75),rep(1,25))
y<-rnorm(100)+bmi^(1/2)+rnorm(100,2)*x+bmi*x

H0<-lm(y~1+x+bmi)
H1<-lm(y~1+x+bmi+x*bmi)
anova(H0,H1)
summary(lm(y~1+x+bmi))


But I want to use permutation testing to avoid an inflated p-value due to a 
y that is not totally normal distributed and I do not want to log transform 
y.

Thanks

Anders



From ripley at stats.ox.ac.uk  Wed Nov 30 12:06:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Nov 2005 11:06:35 +0000 (GMT)
Subject: [R] Random Effects for One-Way Anova
In-Reply-To: <20051130111910.0golkebc5cwg8ook@lnxm.bo.infn.it>
References: <20051130111910.0golkebc5cwg8ook@lnxm.bo.infn.it>
Message-ID: <Pine.LNX.4.61.0511301101430.1692@gannet.stats>

On Wed, 30 Nov 2005 Giuseppe.Palermo at bo.infn.it wrote:

> Hello to All.
> I'd want to use a one-way ANOVA. This means that I have only one factor, with,
> lets say, 5 levels.
> I made a dataframe, called "DATA", with two Columns:
> A, that is my response, and it is "class numerical".

There is no class 'numerical': I presume you mean 'numeric'.

> B, that defines the different levels of my factor, and it is "class factor".
>
> If I want to use a fixed effect model,
> I know that the formula I have to use is:
> lm.1 <- lm(A~B,data=DATA)
> anova(lm.1)
>
> My questions is:
> which formula should I use if I want to use a random effects model?
> I think I should use "lme", but I don't know how.

aov() is the function for anova modelling. so you want

aov(A ~ B, data = DATA)         # fixed effects
aov(A ~ Error(B), data = DATA)  # random effects

but a 1-way ANOVA with fixed or random effects is the same analysis, and 
only the interpretation differs.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From HDoran at air.org  Wed Nov 30 12:11:08 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 30 Nov 2005 06:11:08 -0500
Subject: [R] Random Effects for One-Way Anova
Message-ID: <F5ED48890E2ACB468D0F3A64989D335AC990FC@dc1ex3.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/250b7c8c/attachment.pl

From ked at nilu.no  Wed Nov 30 13:07:58 2005
From: ked at nilu.no (=?ISO-8859-1?Q?K=E5re?= Edvardsen)
Date: Wed, 30 Nov 2005 13:07:58 +0100
Subject: [R] Addon packages
Message-ID: <1133352478.9667.27.camel@localhost.localdomain>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/96ed1074/attachment.pl

From ramasamy at cancer.org.uk  Wed Nov 30 13:17:02 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 30 Nov 2005 12:17:02 +0000
Subject: [R] saving AIC of intermediate models in step
In-Reply-To: <200511291801.jATI1d2T027463@aitana.cpd.ua.es>
References: <200511291801.jATI1d2T027463@aitana.cpd.ua.es>
Message-ID: <1133353022.22754.32.camel@dhcp-82.wolf.ox.ac.uk>

df           <- data.frame( matrix( rnorm(1000), nc=10 ) )
colnames(df) <- c("y", paste("x", 1:9, sep=""))
ifit         <- glm( y ~ ., data=df ) # initial fit

a <- stepAIC( ifit, keep=extractAIC )
a$keep
        [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]
[1,]  10.000   9.0000   8.0000   7.0000   6.0000   5.0000   4.0000
[2,] 319.356 317.3819 315.4327 314.3526 313.2192 312.3311 311.1450
         [,8]     [,9]    [,10]
[1,]   3.0000   2.0000   1.0000
[2,] 310.2517 309.1266 308.1171


On Tue, 2005-11-29 at 19:01 +0100, german.lopez at ua.es wrote:
> Hi all,
>   I'm fitting GLM's using the step or stepAIC procedures and I would 
> like to save the AIC of the intermediate models. I would appreciate 
> very much information about how todo this.
>   Best wishes
>   GermÃ¡n LÃ³pez
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Allan at STATS.uct.ac.za  Wed Nov 30 13:20:19 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Wed, 30 Nov 2005 14:20:19 +0200
Subject: [R] R: scale and location - "t distr"
Message-ID: <438D9903.A0AFE163@STATS.uct.ac.za>

hi all

HOPE SOMEONE CAN HELP!

i 've been searching r and some of the archives in order to find out how
one can consistently estimate the degrees of freedom of the following
random variable:

	Y = a*T(v)+b

	a and b = constants
	T(v) is a Students t distribution with v degrees of freedom

i found one posting in 2001 but no answer!




i know that this problem can easily be solved using MLE (numerically)
but i am interested in the method of moment estimates. 

i derived the estimators and then simulated 10 000 times from the
distribution in order to evaluate the efficiency of the estimators.

the parameters used are:

	a=0.1
	b=0
	v=9.5

and the simulation results are:

	mean(a)=0.10017
	mean(b)=6.4202e-06
	mean(v)=9.780

	sd(a)=0.0016
	sd(b)=1.1334e-03
	sd(v)=1.1681

the sample size used is also 10 000!

from the above results it seems as if "a" and "b" is being estimated
consistently but "v" is not (i.e. the mean of the different paameter
estimates is not equal to 9.5)! i know that a confidence interval about
the estimate does contain 9.5 but the sd of the "v" estimate seems to
big!  

	QUESTION:
	#########################################################
	is this because the number of simulations is to small and that the
asymptotic results for "v" does not yet hold?



i will run the simulations 100 000 times and report the results later.
hopefully someone can shed some insight on this problem.

/
allan

From Roger.Bivand at nhh.no  Wed Nov 30 13:27:28 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 30 Nov 2005 13:27:28 +0100 (CET)
Subject: [R] Addon packages
In-Reply-To: <1133352478.9667.27.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0511301321000.1080-100000@reclus.nhh.no>

On Wed, 30 Nov 2005, K??re Edvardsen wrote:

> I'm trying to add 'gplots' from install.packages("gplots", lib =
> "/usr/lib/R/library", dependencies = TRUE) on a linux host, but R does
> not seem to figure out there's a new package installed (yes, I've
> restarted the R-session). I've tried the default, and different library
> folders, without success. Installation does not report any warnings or
> bad exit status, so this one beats me...
> 
> Anyone know what I'm doing wrong here?

FAQ 7.30, I think:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#I-installed-a-package-but-the-functions-are-not-there

"gplots" %in% installed.packages()[,1]

library(gplots)

require(gplots)

come to mind. Looking at search() can also be instructive.

> 
> K.Edv
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From j.van_den_Hoff at fz-rossendorf.de  Wed Nov 30 13:41:28 2005
From: j.van_den_Hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Wed, 30 Nov 2005 13:41:28 +0100
Subject: [R] nls and weighting
Message-ID: <438D9DF8.605@fz-rossendorf.de>

I posted this a week ago on r-devel but to no avail and hope this not 
considered cross-posting:


===============================cut===============================
hi everybody,

which each release I hope that the section

"weights: an optional numeric vector of (fixed) weights.  When present,
            the objective function is weighted least squares. _not yet
            implemented_"

in the help page of 'nls' is missing the last sentence.

are their any plans to allow/include weighting in the upcoming releases?

modifying the cost function to include the weights is probably not the
problem, I presume. what is the reason for not including the weighting?
are they related to the 'statistical' output (estimation of parameter
uncertainties and significances?).

I know of the "y ~ M"   vs. "~ sqrt(W)*(y-M)" work around suggestion in
MASS to include weighting. (I understand that resulting error estimates
und confidence intervals from 'nls' are wrong in this case. right?)
would'nt it be sensible to inlcude weighting in 'nls' at least on this
level, i.e. weighted parameters provided now, correct error estimates
and the like coming later?


regards,
joerg
===============================cut===============================

I post this here again hoping to learn about possible work arounds 
(apart from the MASS proposal) for the current situation with 'nls' (no 
weighting allowed).

thanks in advance

joerg



From berg.august at gmail.com  Wed Nov 30 13:51:30 2005
From: berg.august at gmail.com (August Berg)
Date: Wed, 30 Nov 2005 04:51:30 -0800
Subject: [R] about kidpack package
In-Reply-To: <71cea5aa0511300125g7e06ce4awf581353ff3646ebf@mail.gmail.com>
References: <71cea5aa0511300125g7e06ce4awf581353ff3646ebf@mail.gmail.com>
Message-ID: <71cea5aa0511300451o288a6f7dv5f6469d92dbf2712@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/6da2cc1f/attachment.pl

From andylehnert at gmx.de  Wed Nov 30 14:11:24 2005
From: andylehnert at gmx.de (Andreas Lehnert)
Date: Wed, 30 Nov 2005 14:11:24 +0100 (MET)
Subject: [R] perp() and axes
Message-ID: <3607.1133356284@www42.gmx.net>


Hello R-sters

Is there a way to force e.g. the y-axis of
a persp()-plot to show not equaly spaced labels but the ones
you can give in the "y="-command?

I tried 

persp(....,y=c(0.1,0.5,0.7,1),...)

but the labels were 0.0:1 by 0.2

I'm looking forward to your help

Andy

--



From ked at nilu.no  Wed Nov 30 14:29:44 2005
From: ked at nilu.no (=?ISO-8859-1?Q?K=E5re?= Edvardsen)
Date: Wed, 30 Nov 2005 14:29:44 +0100
Subject: [R] Addon packages
In-Reply-To: <Pine.LNX.4.44.0511301321000.1080-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0511301321000.1080-100000@reclus.nhh.no>
Message-ID: <1133357384.9667.34.camel@localhost.localdomain>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/56ae7c5a/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed Nov 30 14:35:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 30 Nov 2005 14:35:25 +0100
Subject: [R] about kidpack package
In-Reply-To: <71cea5aa0511300451o288a6f7dv5f6469d92dbf2712@mail.gmail.com>
References: <71cea5aa0511300125g7e06ce4awf581353ff3646ebf@mail.gmail.com>
	<71cea5aa0511300451o288a6f7dv5f6469d92dbf2712@mail.gmail.com>
Message-ID: <438DAA9D.7010807@statistik.uni-dortmund.de>

August Berg wrote:

>>library(kidpack)
> 
> Error in library(kidpack) : 'kidpack' is not a valid package -- installed <
> 2.0.0?
> 
> On 11/30/05, August Berg <berg.august at gmail.com> wrote:
> 
>>* I have both R and Biobase and also download kidpack_1.1.1.tar.gz.
>>After I unzip the gz file, I got the folders and files. Then i got
>>lost: how to install the package of kidpack(because there is no zipped
>>file after I uninstall kidpack_1.1.1.tar.gz)? If I choose install from
>>local zipped file, kidpack_1.1.1.tar.gz does not show on the selection
>>list.
>>Thanks a lot! *
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Looks like you are on Windows.
Apparently (from clicking into the R menu to install a Bioconductor 
package: "Select repository...", then "Install package(s)..." ), for 
this *Bioconductor* package, no Windows binary is available. Hence (if 
possible at all) you have to install the package from sources. You need 
the required development tools (see the Istallation and Administration 
manual) and call R CMD INSTALL from the commandline.

For questions on the availability of BioC Windows binary packages please 
contact the Bioconductor folks and use their mailing list (after looking 
for documentation why kidpack is not available for Windows, of course).

Uwe Ligges



From p.dalgaard at biostat.ku.dk  Wed Nov 30 14:52:48 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2005 14:52:48 +0100
Subject: [R] Random Effects for One-Way Anova
In-Reply-To: <Pine.LNX.4.61.0511301101430.1692@gannet.stats>
References: <20051130111910.0golkebc5cwg8ook@lnxm.bo.infn.it>
	<Pine.LNX.4.61.0511301101430.1692@gannet.stats>
Message-ID: <x2lkz6xmxb.fsf@viggo.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> > My questions is:
> > which formula should I use if I want to use a random effects model?
> > I think I should use "lme", but I don't know how.
> 
> aov() is the function for anova modelling. so you want
> 
> aov(A ~ B, data = DATA)         # fixed effects
> aov(A ~ Error(B), data = DATA)  # random effects
> 
> but a 1-way ANOVA with fixed or random effects is the same analysis, and 
> only the interpretation differs.

Not quite. Only if the design is balanced and you're not looking to
compute things like the variance of the estimated overall mean. 

In the unbalanced case, I don't believe aov() is doing the right thing
(it might not be very wrong, though, if the imbalance is slight) and
lme/lmer is closer on the mark.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Nov 30 14:57:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Nov 2005 13:57:03 +0000 (GMT)
Subject: [R] Random Effects for One-Way Anova
In-Reply-To: <x2lkz6xmxb.fsf@viggo.kubism.ku.dk>
References: <20051130111910.0golkebc5cwg8ook@lnxm.bo.infn.it>
	<Pine.LNX.4.61.0511301101430.1692@gannet.stats>
	<x2lkz6xmxb.fsf@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0511301353400.6141@gannet.stats>

On Wed, 30 Nov 2005, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>>> My questions is:
>>> which formula should I use if I want to use a random effects model?
>>> I think I should use "lme", but I don't know how.
>>
>> aov() is the function for anova modelling. so you want
>>
>> aov(A ~ B, data = DATA)         # fixed effects
>> aov(A ~ Error(B), data = DATA)  # random effects
>>
>> but a 1-way ANOVA with fixed or random effects is the same analysis, and
>> only the interpretation differs.
>
> Not quite. Only if the design is balanced and you're not looking to
> compute things like the variance of the estimated overall mean.

(That's part of the interpretation.  An ANOVA does not give you that.)

> In the unbalanced case, I don't believe aov() is doing the right thing
> (it might not be very wrong, though, if the imbalance is slight) and
> lme/lmer is closer on the mark.

The example given in his follow-up posting was balanced, though.

I think it is pretty conventional to consider such ANOVAs to be balanced, 
and if they are not, to use the term to refer to the classical analyses 
even if they are not according to some definition `the right thing'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From claus.atzenbeck at freenet.de  Wed Nov 30 15:02:12 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Wed, 30 Nov 2005 15:02:12 +0100 (CET)
Subject: [R] Games-Howell, Gabriel, Hochberg
In-Reply-To: <Pine.OSX.4.61.0511292131020.13751@rgnb-d9b86cf6.pool.mediaways.net>
References: <Pine.OSX.4.61.0511281451470.4752@rgnb-d9b8743f.pool.mediaways.net>
	<Pine.OSX.4.61.0511292131020.13751@rgnb-d9b86cf6.pool.mediaways.net>
Message-ID: <Pine.OSX.4.61.0511301500010.25986@rgnb-d9b87605.pool.mediaways.net>

On Tue, 29 Nov 2005, Claus Atzenbeck wrote:

> How do you calculate post hoc multiple comparisons tests with R for
> normal distributed samples with different variances?

In order to make it more visible, I have created an overview that shows
my decision about what test I use. It is available at
<http://cs.aaue.dk/~claus/temp/TestOverview.pdf>. What can I change that
I still can use R to perform my test? (At the very bottom of the
overview you see note about the missing tests in R.)

Thanks for any comment.
Claus



From vasu.akkineni at gmail.com  Wed Nov 30 15:07:44 2005
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Wed, 30 Nov 2005 09:07:44 -0500
Subject: [R] Row-wise data retrieval
In-Reply-To: <438D1C7D.9030408@maths.lth.se>
References: <3b67376c0511291910k667d9401x638992a104b75914@mail.gmail.com>
	<438D1C7D.9030408@maths.lth.se>
Message-ID: <3b67376c0511300607n7d80a715y9a9eca373516f681@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/a21da3f3/attachment.pl

From chrish at stats.ucl.ac.uk  Wed Nov 30 15:10:38 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Wed, 30 Nov 2005 14:10:38 +0000 (GMT)
Subject: [R] Random Effects for One-Way Anova
In-Reply-To: <20051130121102.kaqd01sytcwgg4oo@lnxm.bo.infn.it>
References: <20051130111910.0golkebc5cwg8ook@lnxm.bo.infn.it>
	<018901c5f59a$2bd97f10$2b18a7c0@alice>
	<20051130121102.kaqd01sytcwgg4oo@lnxm.bo.infn.it>
Message-ID: <Pine.LNX.4.64.0511301409590.9914@egon.stats.ucl.ac.uk>

On Wed, 30 Nov 2005, Giuseppe.Palermo at bo.infn.it wrote:

> I'm sorry if I wasn't clear....
> There are repeated measures for each level of my factor.
> For instance, the dataframe could look like that:
>
> A B
> 12.0 1
> 12.3 1
> 15 1
> 12.9 2
> 16.7 2
> 15.4 2
> 23.5 3
> 9.6 3
> 7.8 3
>
> In order to fit a random effects, is right to apply:
> lme(A~1,data=DATA,random=~1|B)    ?

Yes.

Christian



*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From p.dalgaard at biostat.ku.dk  Wed Nov 30 15:11:45 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2005 15:11:45 +0100
Subject: [R] permutation test for linear models with continuous
	covariates
In-Reply-To: <BAY110-F30ED0C3D41C56644E5EAF6C44B0@phx.gbl>
References: <BAY110-F30ED0C3D41C56644E5EAF6C44B0@phx.gbl>
Message-ID: <x2hd9uxm1q.fsf@viggo.kubism.ku.dk>

"anders superanders" <andersdetermigigen at hotmail.com> writes:

> Hi I was wondering if there is a permutation test available in R for linear 
> models with continuous dependent covariates. I want to do a test like the 
> one shown here.
> 
> bmi<-rnorm(100,25)
> x<-c(rep(0,75),rep(1,25))
> y<-rnorm(100)+bmi^(1/2)+rnorm(100,2)*x+bmi*x
> 
> H0<-lm(y~1+x+bmi)
> H1<-lm(y~1+x+bmi+x*bmi)
> anova(H0,H1)
> summary(lm(y~1+x+bmi))
> 
> 
> But I want to use permutation testing to avoid an inflated p-value due to a 
> y that is not totally normal distributed and I do not want to log transform 
> y.

Er, what would you permute? For an interaction test like this (notice
by the way that "*" in your model formula does not mean what you think
it does) I do not think a permutation test exists.  You could try
bootstrapping to get an improved approximation the distribution of the
interaction term.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Hong.Ooi at iag.com.au  Wed Nov 30 15:15:10 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Thu, 1 Dec 2005 01:15:10 +1100
Subject: [R] Looking for constrained optimisation code
Message-ID: <200511301414.jAUEEa8D025206@hypatia.math.ethz.ch>


_______________________________________________________________________________________


Ah! I didn't notice this until now.

Yes, following Nocedal & Wright (excellent book, that) I've defined a
quadratic penalty function to enforce an equality constraint, and added
it to the objective. (Originally I had an inequality constraint in mind,
but the equality one doesn't run into problems when you're on the
boundary -- as is likely to happen in this case -- and gets the job
done.) Getting the gradient right was a bit tedious, but it seems to be
working now.

I did find an interesting thing in working with both Splus and R,
though. In Splus, both optim and nlminb run fairly slowly with a pure-S
objective function supplied, even when it was as vectorized as I could
make it. So I thought to compile the objective function in C, and this
sped it up by a factor of 2-4x. On the other hand, the pure-S version
actually ran quite fast in R, and the compiled version actually _slowed_
it down by a small but noticeable amount.

Is this a common experience? Is the R interpreter so efficient that
using compiled code is unlikely to improve speed further? (I'm using R
2.2 and Splus 7.04, on a Windows XP workstation with 3GB of memory.)


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566
-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Tuesday, 29 November 2005 12:58 PM
To: Hong Ooi
Cc: Thomas Lumley; r-help at stat.math.ethz.ch
Subject: Re: [R] Looking for constrained optimisation code

	  Have you considered migrating the constraints into the
objective 
function, then cranking up the penalty for constraint violation once you

have a more or less feasible solution?

	  spencer graves

Hong Ooi wrote:

>
________________________________________________________________________
_______________
> 
> 
> You know, this is the first time I've heard of constrOptim.
>  
> I actually have a rather complicated, nonlinear boundary expression in
> mind, so this function by itself isn't quite what I'm after. Still, I
> should be able to hack up a barrier function in my own code and feed
> that into optim/nlminb/constrOptim.
> 
> Thanks!
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Nov 30 15:16:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Nov 2005 14:16:42 +0000 (GMT)
Subject: [R] about kidpack package
In-Reply-To: <438DAA9D.7010807@statistik.uni-dortmund.de>
References: <71cea5aa0511300125g7e06ce4awf581353ff3646ebf@mail.gmail.com>
	<71cea5aa0511300451o288a6f7dv5f6469d92dbf2712@mail.gmail.com>
	<438DAA9D.7010807@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0511301405470.6141@gannet.stats>

On Wed, 30 Nov 2005, Uwe Ligges wrote:

> August Berg wrote:
>
>>> library(kidpack)
>>
>> Error in library(kidpack) : 'kidpack' is not a valid package -- installed <
>> 2.0.0?
>>
>> On 11/30/05, August Berg <berg.august at gmail.com> wrote:
>>
>>> * I have both R and Biobase and also download kidpack_1.1.1.tar.gz.
>>> After I unzip the gz file, I got the folders and files. Then i got
>>> lost: how to install the package of kidpack(because there is no zipped
>>> file after I uninstall kidpack_1.1.1.tar.gz)? If I choose install from
>>> local zipped file, kidpack_1.1.1.tar.gz does not show on the selection
>>> list.
>>> Thanks a lot! *
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
> Looks like you are on Windows.
> Apparently (from clicking into the R menu to install a Bioconductor
> package: "Select repository...", then "Install package(s)..." ), for
> this *Bioconductor* package, no Windows binary is available. Hence (if
> possible at all) you have to install the package from sources. You need
> the required development tools (see the Istallation and Administration
> manual) and call R CMD INSTALL from the commandline.

install.packages("mypkg", type="source") will also work, if the package 
does install on Windows.  But kidpack is not a listed BioC package in the 
current release, and a search there produced nothing.

> For questions on the availability of BioC Windows binary packages please
> contact the Bioconductor folks and use their mailing list (after looking
> for documentation why kidpack is not available for Windows, of course).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Wed Nov 30 15:19:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 09:19:00 -0500
Subject: [R] Games-Howell, Gabriel, Hochberg
In-Reply-To: <Pine.OSX.4.61.0511301500010.25986@rgnb-d9b87605.pool.mediaways.net>
References: <Pine.OSX.4.61.0511281451470.4752@rgnb-d9b8743f.pool.mediaways.net>
	<Pine.OSX.4.61.0511292131020.13751@rgnb-d9b86cf6.pool.mediaways.net>
	<Pine.OSX.4.61.0511301500010.25986@rgnb-d9b87605.pool.mediaways.net>
Message-ID: <971536df0511300619i4e46a00bl7a3264d6a95a72b9@mail.gmail.com>

What would be nice would be an R routine that automatically
implements this flowchart.

On 11/30/05, Claus Atzenbeck <claus.atzenbeck at freenet.de> wrote:
> On Tue, 29 Nov 2005, Claus Atzenbeck wrote:
>
> > How do you calculate post hoc multiple comparisons tests with R for
> > normal distributed samples with different variances?
>
> In order to make it more visible, I have created an overview that shows
> my decision about what test I use. It is available at
> <http://cs.aaue.dk/~claus/temp/TestOverview.pdf>. What can I change that
> I still can use R to perform my test? (At the very bottom of the
> overview you see note about the missing tests in R.)
>
> Thanks for any comment.
> Claus
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sstory at montana.edu  Wed Nov 30 16:14:23 2005
From: sstory at montana.edu (Scott Story)
Date: Wed, 30 Nov 2005 08:14:23 -0700
Subject: [R] Solving Systems of Non-linear equations
Message-ID: <438DC1CF.8080904@montana.edu>

I am trying to write a function that will solve a simple system of 
nonlinear equations for the parameters that describe the beta 
distribution (a,b) given the mean and variance.


mean = a/(a+b)
variance = (a*b)/(((a+b)^2) * (a+b+1))

Any help as to where to start would be welcome.



-- 
Scott Story
Graduate Student
MSU Ecology Department
319 Lewis Hall
Bozeman, Mt 59717
406.994.2670
sstory at montana.edu



From ggrothendieck at gmail.com  Wed Nov 30 16:17:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 10:17:12 -0500
Subject: [R] nicer matrix construction: rbind()
In-Reply-To: <Pine.LNX.4.61.0511300945060.794@gannet.stats>
References: <ly7jas8y11.fsf@tuebingen.mpg.de>
	<BFB1C31F.13ADA%sdavis2@mail.nih.gov>
	<17293.28807.117410.14661@stat.math.ethz.ch>
	<Pine.LNX.4.61.0511300945060.794@gannet.stats>
Message-ID: <971536df0511300717n53982ea0hdd1affce80794942@mail.gmail.com>

On 11/30/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Wed, 30 Nov 2005, Martin Maechler wrote:
>
> > Just a small remark on "R coding style" :
> >
> >>>>>> "Sean" == Sean Davis <sdavis2 at mail.nih.gov>
> >>>>>>     on Tue, 29 Nov 2005 08:34:39 -0500 writes:
> >
> >      Sean> <............>
> >
> >      Sean> x <- matrix(c(1,1,1,2,2,2,1,1,1,1,1,2),nr=2,byrow=TRUE)
> >
> >      Sean>  <.....>
> >            (a very helpful answer to Georg's question; thanks, Sean!)
> >
> > I've seen example code like this in many places,
> > and I'd like advocate a more readable alternative
> >
> >   x <- rbind(c(1,1,1, 2,2,2)
> >              c(1,1,1, 1,1,2))
> >
> > {efficiency really not being of any concern;
> > we are talking about didactical examples}.
>
> But introducing unnecessary concepts is a concern, and I think
>
> x <- matrix(c(1, 1, 1, 2, 2, 2,
>               1, 1, 1, 1, 1, 2),
>             nrow = 2, byrow = TRUE)
>
> is clearer (and does not leave me wondering about the non-obvious choice
> of spaces).

This is harder to maintain since changing the matrix may also
require that one changes nrow thus introducing a potential source
of error not present in the rbind solution.



From sstory at montana.edu  Wed Nov 30 16:25:02 2005
From: sstory at montana.edu (Scott Story)
Date: Wed, 30 Nov 2005 08:25:02 -0700
Subject: [R] Loop within nlme
Message-ID: <438DC44E.5070109@montana.edu>

	I am trying to mimic the SAS code below in R. The trick is that each 
row in the dataset has variable "t" which controls how many times the 
do-loop below will be iterated (that is, the model is fit to the 
response, ifate, 0 to t-1 times for each row of data). Is it possible to 
incorporate a loop like this into nlme by writing a function? Can 
anybody provide some hints to get me on my way? The code below is for a 
very simple model, an intercept only model, but more complex models will 
be evaluated (some potentially including random effects). The code is 
used to model daily nest survival.


Proc Nlmixed data=Mall tech=quanew method=gauss maxiter=1000;
parms B0=0;
	p=1;
	   do i=0 TO t-1;
	   	   logit=B0;
	      p=p*(exp(logit)/(1+exp(logit)));
	   end;
model ifate~binomial(1,p);


-- 
Scott Story
Graduate Student
MSU Ecology Department
319 Lewis Hall
Bozeman, Mt 59717
406.994.2670
sstory at montana.edu



From sharonanandhi at gmail.com  Wed Nov 30 16:36:41 2005
From: sharonanandhi at gmail.com (Sharon Anbu)
Date: Wed, 30 Nov 2005 16:36:41 +0100
Subject: [R] Coefficient of Variance (log values) !
Message-ID: <3709c2370511300736w2c150609v84ad627562be91f2@mail.gmail.com>

Hi,

I am calculating coefficient of variance (CV) for my ELISA's data. For
normal values, I use the standard formula CV = SD/Mean * 100.  Now, I
would like to calculate CV  for log values.  Can any one please
suggest me, how I can do this in R?

Thanks in Advance.

Regards,
Sharon



From p.dalgaard at biostat.ku.dk  Wed Nov 30 16:36:59 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2005 16:36:59 +0100
Subject: [R] Permutation tests for correlations
Message-ID: <x24q5uxi3o.fsf@viggo.kubism.ku.dk>

Apropos the question about permutation tests in multiple regression:

We do have perm.test in package ExactRankTests, but it does one- and
two-sample tests, as in t.test, wilcox.test, etc. There doesn't seem
to be an exact version of the permutation test for correlations, i.e.,
the one that could be estimated using

replicate(10000, cor(x,sample(y))) # or other values of 10000

or, of course, computed exactly by enumeration of all the
permutations, which is feasible up to length(x) == 10 or so.

So I'm wondering: Is this due to lack of theory/algorithm or just lack of
implementation? 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From claus.atzenbeck at freenet.de  Wed Nov 30 16:35:21 2005
From: claus.atzenbeck at freenet.de (Claus Atzenbeck)
Date: Wed, 30 Nov 2005 16:35:21 +0100 (CET)
Subject: [R] Games-Howell, Gabriel, Hochberg
In-Reply-To: <971536df0511300619i4e46a00bl7a3264d6a95a72b9@mail.gmail.com>
References: <Pine.OSX.4.61.0511281451470.4752@rgnb-d9b8743f.pool.mediaways.net>
	<Pine.OSX.4.61.0511292131020.13751@rgnb-d9b86cf6.pool.mediaways.net> 
	<Pine.OSX.4.61.0511301500010.25986@rgnb-d9b87605.pool.mediaways.net>
	<971536df0511300619i4e46a00bl7a3264d6a95a72b9@mail.gmail.com>
Message-ID: <Pine.OSX.4.61.0511301631330.2007@rgnb-d9b87605.pool.mediaways.net>

On Wed, 30 Nov 2005, Gabor Grothendieck wrote:

> What would be nice would be an R routine that automatically
> implements this flowchart.

This overview is just for my personal usage. I am not a statistician,
but some others told me that there is a lot of experience behind
choosing the right test. Therefore, I don't know if those
"yes-no-decisions" depicted in this overview would be appropriate for
all cases.

Claus



From ggrothendieck at gmail.com  Wed Nov 30 16:49:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 10:49:30 -0500
Subject: [R] Games-Howell, Gabriel, Hochberg
In-Reply-To: <Pine.OSX.4.61.0511301631330.2007@rgnb-d9b87605.pool.mediaways.net>
References: <Pine.OSX.4.61.0511281451470.4752@rgnb-d9b8743f.pool.mediaways.net>
	<Pine.OSX.4.61.0511292131020.13751@rgnb-d9b86cf6.pool.mediaways.net>
	<Pine.OSX.4.61.0511301500010.25986@rgnb-d9b87605.pool.mediaways.net>
	<971536df0511300619i4e46a00bl7a3264d6a95a72b9@mail.gmail.com>
	<Pine.OSX.4.61.0511301631330.2007@rgnb-d9b87605.pool.mediaways.net>
Message-ID: <971536df0511300749q56144357p5f1c066d33a7e575@mail.gmail.com>

One could have a method= argument with
the default chosen by using the flowchart and the output including
information on which method was used.  A good help page
and/or vignette (that included the flowchart) could help ameliorate
difficulties.  That would still allow the user to specify a particular method
and provide for sufficient understanding.

On 11/30/05, Claus Atzenbeck <claus.atzenbeck at freenet.de> wrote:
> On Wed, 30 Nov 2005, Gabor Grothendieck wrote:
>
> > What would be nice would be an R routine that automatically
> > implements this flowchart.
>
> This overview is just for my personal usage. I am not a statistician,
> but some others told me that there is a lot of experience behind
> choosing the right test. Therefore, I don't know if those
> "yes-no-decisions" depicted in this overview would be appropriate for
> all cases.
>
> Claus
>



From p.dalgaard at biostat.ku.dk  Wed Nov 30 16:55:13 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2005 16:55:13 +0100
Subject: [R] Games-Howell, Gabriel, Hochberg
In-Reply-To: <971536df0511300619i4e46a00bl7a3264d6a95a72b9@mail.gmail.com>
References: <Pine.OSX.4.61.0511281451470.4752@rgnb-d9b8743f.pool.mediaways.net>
	<Pine.OSX.4.61.0511292131020.13751@rgnb-d9b86cf6.pool.mediaways.net>
	<Pine.OSX.4.61.0511301500010.25986@rgnb-d9b87605.pool.mediaways.net>
	<971536df0511300619i4e46a00bl7a3264d6a95a72b9@mail.gmail.com>
Message-ID: <x2zmnmw2ou.fsf@viggo.kubism.ku.dk>

Gabor Grothendieck <ggrothendieck at gmail.com> writes:

> What would be nice would be an R routine that automatically
> implements this flowchart.


I'd recommend learning about p.adjust and the multcomp package
*instead* of following the flowchart. (Ignoring heteroscedasticity
seems a bit silly too, given the availability of oneway.test(), and
I'm not too happy about mechanistic dependency upon tests for
normality either.) 
 
> On 11/30/05, Claus Atzenbeck <claus.atzenbeck at freenet.de> wrote:
> > On Tue, 29 Nov 2005, Claus Atzenbeck wrote:
> >
> > > How do you calculate post hoc multiple comparisons tests with R for
> > > normal distributed samples with different variances?
> >
> > In order to make it more visible, I have created an overview that shows
> > my decision about what test I use. It is available at
> > <http://cs.aaue.dk/~claus/temp/TestOverview.pdf>. What can I change that
> > I still can use R to perform my test? (At the very bottom of the
> > overview you see note about the missing tests in R.)
> >
> > Thanks for any comment.
> > Claus
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tom at maladmin.com  Wed Nov 30 12:05:25 2005
From: tom at maladmin.com (tom wright)
Date: Wed, 30 Nov 2005 06:05:25 -0500
Subject: [R] OT: Statistics question
Message-ID: <1133348726.4389.9.camel@localhost.localdomain>

I apologise for asking this question here but I am hoping that someone
can either give me direct guidance and/or point me to a better group for
this type of disucssion.

I have what I feel should be a fairly simple problem but which my
limited stats knowledge can't answer. I have two overlapping
distributions (both normal) and I want to answer the question how do I
calculate the cut-off value so I can be 95% sure that samples => than
the cut off fall in the right hand distribution?

A while ago I did a bayesian statistics course that I think answered
this very question but in the absence of any course notes or recent
practice Ihave forgotten how to go about this.

Many thanks for your time
Tom



From ggrothendieck at gmail.com  Wed Nov 30 17:00:05 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 11:00:05 -0500
Subject: [R] Games-Howell, Gabriel, Hochberg
In-Reply-To: <x2zmnmw2ou.fsf@viggo.kubism.ku.dk>
References: <Pine.OSX.4.61.0511281451470.4752@rgnb-d9b8743f.pool.mediaways.net>
	<Pine.OSX.4.61.0511292131020.13751@rgnb-d9b86cf6.pool.mediaways.net>
	<Pine.OSX.4.61.0511301500010.25986@rgnb-d9b87605.pool.mediaways.net>
	<971536df0511300619i4e46a00bl7a3264d6a95a72b9@mail.gmail.com>
	<x2zmnmw2ou.fsf@viggo.kubism.ku.dk>
Message-ID: <971536df0511300800u2fcf617cl1e753322997241e7@mail.gmail.com>

Another possibility might be to have a CRAN Task View devoted
to tests.  The whole area is quite confusing and it would be nice
to have a central point for guidance.

On 30 Nov 2005 16:55:13 +0100, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Gabor Grothendieck <ggrothendieck at gmail.com> writes:
>
> > What would be nice would be an R routine that automatically
> > implements this flowchart.
>
>
> I'd recommend learning about p.adjust and the multcomp package
> *instead* of following the flowchart. (Ignoring heteroscedasticity
> seems a bit silly too, given the availability of oneway.test(), and
> I'm not too happy about mechanistic dependency upon tests for
> normality either.)
>
> > On 11/30/05, Claus Atzenbeck <claus.atzenbeck at freenet.de> wrote:
> > > On Tue, 29 Nov 2005, Claus Atzenbeck wrote:
> > >
> > > > How do you calculate post hoc multiple comparisons tests with R for
> > > > normal distributed samples with different variances?
> > >
> > > In order to make it more visible, I have created an overview that shows
> > > my decision about what test I use. It is available at
> > > <http://cs.aaue.dk/~claus/temp/TestOverview.pdf>. What can I change that
> > > I still can use R to perform my test? (At the very bottom of the
> > > overview you see note about the missing tests in R.)
> > >
> > > Thanks for any comment.
> > > Claus
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> --
>   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>



From p.dalgaard at biostat.ku.dk  Wed Nov 30 17:02:12 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2005 17:02:12 +0100
Subject: [R] Solving Systems of Non-linear equations
In-Reply-To: <438DC1CF.8080904@montana.edu>
References: <438DC1CF.8080904@montana.edu>
Message-ID: <x2veyaw2d7.fsf@viggo.kubism.ku.dk>

Scott Story <sstory at montana.edu> writes:

> I am trying to write a function that will solve a simple system of 
> nonlinear equations for the parameters that describe the beta 
> distribution (a,b) given the mean and variance.
> 
> 
> mean = a/(a+b)
> variance = (a*b)/(((a+b)^2) * (a+b+1))
> 
> Any help as to where to start would be welcome.

On a pad of paper...

First look at mean*(1-mean)/variance, and the rest should follow. (Hint
minimized in case this was homework...)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Wed Nov 30 17:05:14 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 30 Nov 2005 11:05:14 -0500
Subject: [R] Solving Systems of Non-linear equations
In-Reply-To: <438DC1CF.8080904@montana.edu>
References: <438DC1CF.8080904@montana.edu>
Message-ID: <438DCDBA.70009@stats.uwo.ca>

On 11/30/2005 10:14 AM, Scott Story wrote:
> I am trying to write a function that will solve a simple system of 
> nonlinear equations for the parameters that describe the beta 
> distribution (a,b) given the mean and variance.
> 
> 
> mean = a/(a+b)
> variance = (a*b)/(((a+b)^2) * (a+b+1))
> 
> Any help as to where to start would be welcome.

You should use a package like Maple or Mathematica (or just some pencil 
and paper work) to determine the solution.  Then the function is really 
easy to write.

Maple gives

 > solve({mean = a/(a+b),variance = (a*b)/(((a+b)^2) * (a+b+1))},{a,b});

{a = -mean*(variance+mean^2-mean)/variance,
  b = (variance+mean^2-mean)*(mean-1)/variance}

from which you can write your own function pretty easily.

Duncan Murdoch



From dlavecchia at tiscali.it  Wed Nov 30 17:08:47 2005
From: dlavecchia at tiscali.it (dlavecchia@tiscali.it)
Date: Wed, 30 Nov 2005 17:08:47 +0100
Subject: [R] newton-raphson
Message-ID: <43855F7800019949@mail-7.mail.tiscali.sys>

Hi everybody,
I have to solve a score function by using Newton-Raphson algorithm. Is there
such a fucntion in R? I have built this algoritm

newton<-function(tgt,drva,th0,err) {
iter=0
repeat  {iter = iter+1
th1=th0-tgt(th0)/drva(th0)
if (abs(th0-th1)<err||abs(tgt(th1))<.1e-10)
break
th0=th1}
th1
}

but it does not work for my function because the ratio "tgt(th0)/drva(th0)"
is very high and the algoritm does not converge. On the contrary it works
very well for some simple functions (like X^2, X^3..and so on)
Please, can you help me?

Thanks in advance,
Davide   



__________________________________________________________________
TISCALI ADSL
Solo con Tiscali Adsl navighi senza limiti e telefoni senza canone 
Telecom a partire da 19,95 Euro/mese.
Attivala subito, I PRIMI DUE MESI SONO GRATIS! CLICCA QUI:
http://abbonati.tiscali.it/adsl/sa/1e25flat_tc/



From mathematician4 at hotmail.com  Wed Nov 30 17:33:18 2005
From: mathematician4 at hotmail.com (Emanuele Mazzola)
Date: Wed, 30 Nov 2005 16:33:18 +0000
Subject: [R] help with ks.test
Message-ID: <BAY107-F38AEFDE7DB642169215C29A4A0@phx.gbl>

Hello everybody,

after consulting V.Ricci's paper ("Rappresentazione analitica delle 
distribuzioni statistiche con R" / Analytical representation of statistical 
distributions in R) i would like to ask you further advice and a 
confirmation about ks.test.
My willing is to perform ks.test to evaluate if my dataset comes from a non 
standard distribution, namely hypoexponential distribution.
I have built its cdf as a function of two numerical parameters, in a 
separated .r script. (namely "hypo.cdf<-function(a1,a2,x){...}" ).
Is it correct running then ks.test with
                 ks.test(datafile, hypo.cdf(a1,a2,x),...)          ???

I really hope so...
Please, let me know asap !!!
Thank you in advance for your kind answers,
Regards!

Emanuele Mazzola



From ggrothendieck at gmail.com  Wed Nov 30 17:37:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 11:37:42 -0500
Subject: [R] Solving Systems of Non-linear equations
In-Reply-To: <438DC1CF.8080904@montana.edu>
References: <438DC1CF.8080904@montana.edu>
Message-ID: <971536df0511300837g5f885200wfbf7991934fe86e3@mail.gmail.com>

Go to http://mathomatic.orgserve.de/math/ and install mathomatic
(its free) or just connect to the online server and do this.

The C output, i.e the result of the two code c commands,
can be used verbatim in R.

Note that mathomatic does not support logs but for simply
problems like this its very useful.

Note that 1-> and 2-> are the mathomatic prompts and what
comes after them are what I typed in.  The entry goes into
the corresponding equation space, i.e. equation 1 or equation 2.

1-> mean = a/(a+b)

              a
#1: mean = -------
           (a + b)

1-> variance = (a*b)/(((a+b)^2) * (a+b+1))

                          a*b
#2: variance = -------------------------
               (((a + b)^2)*(a + b + 1))

2-> eliminate b
Solving equation #1 for (b)...

                                        1
                                (a^2)*(---- - 1)
                                       mean
#2: variance = ---------------------------------------------------
                           1                       1
               (((a + (a*(---- - 1)))^2)*(a + (a*(---- - 1)) + 1))
                          mean                    mean

2-> a

              mean*(1 - mean)
#2: a = mean*(--------------- - 1)
                 variance

2-> simplify

        ((mean^2) - (mean^3))
#2: a = --------------------- - mean
              variance

2-> eliminate a
Solving equation #1 for (a)...

      b*mean     ((mean^2) - (mean^3))
#2: ---------- = --------------------- - mean
    (1 - mean)         variance

2-> b

         mean*(1 - mean)
#2: b = (--------------- - 1)*(1 - mean)
            variance
2-> simplify

             ((mean^2) - mean)
#2: b = (1 + -----------------)*(mean - 1)
                 variance


2-> code c
b = ((1.0 + (((mean * mean) - mean) / variance)) * (mean - 1.0));

2-> #1

          b*mean
#1: a = ----------
        (1 - mean)

1-> code c
a = (b * mean / (1.0 - mean));



On 11/30/05, Scott Story <sstory at montana.edu> wrote:
> I am trying to write a function that will solve a simple system of
> nonlinear equations for the parameters that describe the beta
> distribution (a,b) given the mean and variance.
>
>
> mean = a/(a+b)
> variance = (a*b)/(((a+b)^2) * (a+b+1))
>
> Any help as to where to start would be welcome.
>
>
>
> --
> Scott Story
> Graduate Student
> MSU Ecology Department
> 319 Lewis Hall
> Bozeman, Mt 59717
> 406.994.2670
> sstory at montana.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Wed Nov 30 17:38:09 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2005 17:38:09 +0100
Subject: [R] Coefficient of Variance (log values) !
In-Reply-To: <3709c2370511300736w2c150609v84ad627562be91f2@mail.gmail.com>
References: <3709c2370511300736w2c150609v84ad627562be91f2@mail.gmail.com>
Message-ID: <x2r78yw0pa.fsf@viggo.kubism.ku.dk>

Sharon Anbu <sharonanandhi at gmail.com> writes:

> Hi,
> 
> I am calculating coefficient of variance (CV) for my ELISA's data. For
> normal values, I use the standard formula CV = SD/Mean * 100.  Now, I
> would like to calculate CV  for log values.  Can any one please
> suggest me, how I can do this in R?

It usually makes best sense just to quote the SD of the natural-log
transformed values as a coefficient of variation (sic). The precise
formula is that in the lognormal distribution,

CV = sqrt(exp(SD^2)-1)

where CV is the coefficient of X and SD is the standard deviation of
log(x). If you look at

curve(sqrt(exp(x^2)-1))
abline(0,1)

you'll see that the deviation is quite small for x < 0.3 or so.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Achim.Zeileis at wu-wien.ac.at  Wed Nov 30 17:44:19 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 30 Nov 2005 17:44:19 +0100 (CET)
Subject: [R] Permutation tests for correlations
In-Reply-To: <x24q5uxi3o.fsf@viggo.kubism.ku.dk>
References: <x24q5uxi3o.fsf@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.58.0511301728360.2503@thorin.ci.tuwien.ac.at>

On Wed, 30 Nov 2005, Peter Dalgaard wrote:

> Apropos the question about permutation tests in multiple regression:
>
> We do have perm.test in package ExactRankTests, but it does one- and

just for the record: exactRankTests.

> two-sample tests, as in t.test, wilcox.test, etc. There doesn't seem
> to be an exact version of the permutation test for correlations, i.e.,
> the one that could be estimated using
>
> replicate(10000, cor(x,sample(y))) # or other values of 10000
>
> or, of course, computed exactly by enumeration of all the
> permutations, which is feasible up to length(x) == 10 or so.
>
> So I'm wondering: Is this due to lack of theory/algorithm or just lack of
> implementation?

Lack of looking at the right package ;-)

The package `coin' provides conditional inference for the independence
problem in a rather flexible setup (in particular for arbitrary scales of
x  and y). It provides asymptotic p values as well as approximations (as
you suggest above) to the exact distribution for all types of data and
also the exact distribution if feasible. See
  vignette("coin", package = "coin")
for more details.

Best,
Z



From ggrothendieck at gmail.com  Wed Nov 30 17:50:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 11:50:09 -0500
Subject: [R] Solving Systems of Non-linear equations
In-Reply-To: <971536df0511300837g5f885200wfbf7991934fe86e3@mail.gmail.com>
References: <438DC1CF.8080904@montana.edu>
	<971536df0511300837g5f885200wfbf7991934fe86e3@mail.gmail.com>
Message-ID: <971536df0511300850y6552adaflbedfcff54a1da19c@mail.gmail.com>

Sorry I seemed to have messed up the copying and pasting.
Here it is again.

---

Go to http://mathomatic.orgserve.de/math/ and install mathomatic
(its free) or just connect to the online server and do this.

The C output, i.e the result of the two code c commands,
can be used verbatim in R.

Note that mathomatic does not support logs but for simple
problems like this its very useful.

Note that 1-> and 2-> are the mathomatic prompts and what
comes after them are what I typed in.  The entry goes into
the corresponding equation space, i.e. equation 1 or equation 2.

This is what you enter:

mean = a/(a+b)
variance = (a*b)/(((a+b)^2) * (a+b+1))

eliminate b
a
simplify
code c

eliminate a
b
simplify
code c

and this is the entire session:


1-> mean = a/(a+b)

              a
#1: mean = -------
           (a + b)

1-> variance = (a*b)/(((a+b)^2) * (a+b+1))

                          a*b
#2: variance = -------------------------
               (((a + b)^2)*(a + b + 1))

2-> eliminate b
Solving equation #1 for (b)...

                                        1
                                (a^2)*(---- - 1)
                                       mean
#2: variance = ---------------------------------------------------
                           1                       1
               (((a + (a*(---- - 1)))^2)*(a + (a*(---- - 1)) + 1))
                          mean                    mean

2-> a

              mean*(1 - mean)
#2: a = mean*(--------------- - 1)
                 variance

2-> simplify

        ((mean^2) - (mean^3))
#2: a = --------------------- - mean
              variance

2-> code c
a = ((((mean * mean) - pow(mean, 3.0)) / variance) - mean);

2-> eliminate a
Solving equation #1 for (a)...

      b*mean     ((mean^2) - (mean^3))
#2: ---------- = --------------------- - mean
    (1 - mean)         variance

2-> b

         mean*(1 - mean)
#2: b = (--------------- - 1)*(1 - mean)
            variance

2-> simplify

             ((mean^2) - mean)
#2: b = (1 + -----------------)*(mean - 1)
                 variance

2-> code c
b = ((1.0 + (((mean * mean) - mean) / variance)) * (mean - 1.0));




On 11/30/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Go to http://mathomatic.orgserve.de/math/ and install mathomatic
> (its free) or just connect to the online server and do this.
>
> The C output, i.e the result of the two code c commands,
> can be used verbatim in R.
>
> Note that mathomatic does not support logs but for simply
> problems like this its very useful.
>
> Note that 1-> and 2-> are the mathomatic prompts and what
> comes after them are what I typed in.  The entry goes into
> the corresponding equation space, i.e. equation 1 or equation 2.
>
> 1-> mean = a/(a+b)
>
>              a
> #1: mean = -------
>           (a + b)
>
> 1-> variance = (a*b)/(((a+b)^2) * (a+b+1))
>
>                          a*b
> #2: variance = -------------------------
>               (((a + b)^2)*(a + b + 1))
>
> 2-> eliminate b
> Solving equation #1 for (b)...
>
>                                        1
>                                (a^2)*(---- - 1)
>                                       mean
> #2: variance = ---------------------------------------------------
>                           1                       1
>               (((a + (a*(---- - 1)))^2)*(a + (a*(---- - 1)) + 1))
>                          mean                    mean
>
> 2-> a
>
>              mean*(1 - mean)
> #2: a = mean*(--------------- - 1)
>                 variance
>
> 2-> simplify
>
>        ((mean^2) - (mean^3))
> #2: a = --------------------- - mean
>              variance
>
> 2-> eliminate a
> Solving equation #1 for (a)...
>
>      b*mean     ((mean^2) - (mean^3))
> #2: ---------- = --------------------- - mean
>    (1 - mean)         variance
>
> 2-> b
>
>         mean*(1 - mean)
> #2: b = (--------------- - 1)*(1 - mean)
>            variance
> 2-> simplify
>
>             ((mean^2) - mean)
> #2: b = (1 + -----------------)*(mean - 1)
>                 variance
>
>
> 2-> code c
> b = ((1.0 + (((mean * mean) - mean) / variance)) * (mean - 1.0));
>
> 2-> #1
>
>          b*mean
> #1: a = ----------
>        (1 - mean)
>
> 1-> code c
> a = (b * mean / (1.0 - mean));
>
>
>
> On 11/30/05, Scott Story <sstory at montana.edu> wrote:
> > I am trying to write a function that will solve a simple system of
> > nonlinear equations for the parameters that describe the beta
> > distribution (a,b) given the mean and variance.
> >
> >
> > mean = a/(a+b)
> > variance = (a*b)/(((a+b)^2) * (a+b+1))
> >
> > Any help as to where to start would be welcome.
> >
> >
> >
> > --
> > Scott Story
> > Graduate Student
> > MSU Ecology Department
> > 319 Lewis Hall
> > Bozeman, Mt 59717
> > 406.994.2670
> > sstory at montana.edu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From eymw at hotmail.com  Wed Nov 30 17:53:48 2005
From: eymw at hotmail.com (Ed Wang)
Date: Wed, 30 Nov 2005 10:53:48 -0600
Subject: [R] help with R
In-Reply-To: <200511291905.jATJ51eS024074@meitner.gene.com>
Message-ID: <BAY103-F231FDB730585577843A9F9C94A0@phx.gbl>

Berton,

Firstly, thanks for your comments.

To address the what you first said, plotting the 3690-element vector is what
is causing R to hang.  Rather than lose everything I've entered by hand each
interactive run I've switched to using a batch script, which I can now load
and run at prompt.  Using sink("filename.txt") I'm able to save the output
to study.

My usage of dummy variables is to identify seasonality on the daily level 
over
15 years with 246 days per year.  I need to identify the day each month
when an (expected) event is occuring.  The date the event occurs does
not occur on necessarily the same day each month.  I don't know of
another method that could identify these statistically significant seasonal
events using R.  Dummy variables with a LM is the only method I have
experience with using R.  If you or anyone has suggestions on what other
methods to use I would appreciate some suggestions.  Using 245 dummy
variables is quite awkward.

I see lag() can be used to build a first- or multi-order differenced time
series to extract any underlying trend in a time series.

Using STL() might be promising.  It appears to be similar to other methods
I've used with MINITAB but called something different.

Nor an ARIMA nor a BSM is really what I need as I'm not focused on
performing predictions or modeling of the (possibly non-normal) properties
of the residuals.

Thanks.  All your advice is greatly appreciated.

Ed

       "A man is not old until regrets take the place of dreams."
                     Actor John Barrymore






From: Berton Gunter <gunter.berton at gene.com>
To: "'Ed Wang'" <eymw at hotmail.com>, <r-help at stat.math.ethz.ch>
Subject: RE: [R] help with R
Date: Tue, 29 Nov 2005 11:05:02 -0800

You're not telling us something or there's a problem with your R build: a
3960 element vectors of integer is tiny and will not cause R to crash.

Regarding your regression model. You do **not** need dummy variables in R.
Please read the docs (e.g. AN INTRODUCTION TO R) and help files on lm() and
factor() to see how to do linear modeling in R. lag() and diff() may also be
relevant. OTOH, R has many better ways to model time series and seasonality,
both in base R and numerous add-on packages. Try help.search('time series')
and RSiteSearch('time series')

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA

"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From giovanna.jonalasinio at uniroma1.it  Wed Nov 30 18:00:45 2005
From: giovanna.jonalasinio at uniroma1.it (giovanna jona lasinio)
Date: Wed, 30 Nov 2005 18:00:45 +0100
Subject: [R] Corrupted workspace(?)
In-Reply-To: <mailman.9.1132398002.32610.r-help@stat.math.ethz.ch>
Message-ID: <005801c5f5cf$9b10c4d0$65036497@sta.uniroma1.it>

Dear R helpers,
I'm using R2.2 under windows XP professional on a Dell double processor
workstation.
I have a large (29Mb) workspace that I'm trying to load both, by double
clicking and by direct load. The message I get is "Bad restore  file
magic number (file may be corrupted)...". I believe that this is a
consequence of a huge spike in the electrical system that my voltage
regulator couldn't manage.
I already searched the R mail archives and it seems I'm in big
troubles... Does any one have a new idea on what I can do to recover it?

Best
Giovanna Jona Lasinio



From plummer at iarc.fr  Wed Nov 30 18:03:58 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 30 Nov 2005 18:03:58 +0100
Subject: [R] R software on 64bit - Intel Xeon processor
In-Reply-To: <Pine.LNX.4.61.0511292041090.18489@gannet.stats>
References: <20051129181600.63685.qmail@web31615.mail.mud.yahoo.com>
	<Pine.LNX.4.61.0511292041090.18489@gannet.stats>
Message-ID: <1133370238.3553.5.camel@seurat>

Many packages in Fedora Extras, including R, have been recompiled for
CentOS (a RHEL clone). You can find them here:

http://centos.karan.org/

They should be compatible with RHEL, but of course your mileage may
vary.

Martyn

On Tue, 2005-11-29 at 20:43 +0000, Prof Brian Ripley wrote:
> You don't seem to know your OS spec, so how can we guess?
> You chip can run various different OSes.  RH claim to have
> RHEL4 for AMD64/EM64T, but not `for 64bit'.
> 
> Use uname -a.  If it mentions ix86 (for x=3,4,5,6 or perhaps 7) use that 
> RPM.  I expect it will mention x86_64.  In that case you may need to 
> install from the sources.  One way to do so is to install the SRPM, 
> rpmbuild that and then install it.  But building from the source tarball 
> is also a cinch, and will avoid RH's broken blas library (if you have that 
> installed).  The other advantage is that you can install the current 
> R-patched rather than R-2.2.0 and benefit from all the patches.
> 
> Of course, it is possible that RedHat has an RPM (they do for FC3 and
> FC4), so have you checked their repositories?
> 
> 
> On Tue, 29 Nov 2005, Srinivas Iyyer wrote:
> 
> > Dear Group,
> > I have a machine which has a 64bit Intel?? Xeon?
> > Processor 3.00GHz, 2MB L2 Cache 6T302N - [ 221-7984 ]
> > processor.
> > (Dell Precision Workstation 670n Intel?? Xeon?
> > Processor)
> >
> > The OS is RedHat Enterprise Linux version 4 (for
> > 64bit).
> >
> > I went to /bin/linux/redhat/el4/i386 on CRAN FTP site.
> > I have no clue if any of these RPMs are suitable for
> > this machines configuration.
> >
> > Could any one point me to an appropriate RPM that I
> > can download and install it on this machine.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> ______________________________________________ R-help at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}



From sstory at montana.edu  Wed Nov 30 18:05:28 2005
From: sstory at montana.edu (Scott Story)
Date: Wed, 30 Nov 2005 10:05:28 -0700
Subject: [R] Solution to non-linear equation problem
Message-ID: <438DDBD8.6020408@montana.edu>

	Thanks to Gabor, Duncan, and Peter. I knew the answer had something to 
do with solving for a and b in terms of mean and variance. I will build 
a function using the equations you provided Duncan and will  look into 
using Mathomatic in the future Gabor. Appreciate the help. Peter, this 
was not homework but I understand your concern. I don't use listserves 
that often but they do open a whole arena for students to get free 
information (hadn't even crossed my mind). Just as an aside, I take it 
that R cannot solve non-linear equations (since you all gave me 
solutions outside of it).


-- 
Scott Story
Graduate Student
MSU Ecology Department
319 Lewis Hall
Bozeman, Mt 59717
406.994.2670
sstory at montana.edu



From wowen at richmond.edu  Wed Nov 30 18:14:35 2005
From: wowen at richmond.edu (Owen, Jason)
Date: Wed, 30 Nov 2005 12:14:35 -0500
Subject: [R] strange plots with type = "h" option
Message-ID: <0F98C8BA43C00C42AFFBE000DA9DDB230AD3809D@pollux.richmond.edu>

Hello,

With the new version 2.2.0, I get strange plots when using the 
histogram-like option in plot().  For example, a plot of binomial
probabilities:

> plot(0:10,dbinom(0:10,10,.1), type = "h", lwd = 30)

gives me weird fat cirular bars, with mass out at values with low
probability.  What is the issue here?  This never happened with earlier 
versions.

R on Windows XP SP 2
Intel processor

Jason



From maechler at stat.math.ethz.ch  Wed Nov 30 18:22:51 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 30 Nov 2005 18:22:51 +0100
Subject: [R] overlay additional axes
In-Reply-To: <200511281356.28162.dylan.beaudette@gmail.com>
References: <200511281356.28162.dylan.beaudette@gmail.com>
Message-ID: <17293.57323.349921.366629@stat.math.ethz.ch>

>>>>> "Dylan" == Dylan Beaudette <dylan.beaudette at gmail.com>
>>>>>     on Mon, 28 Nov 2005 13:56:27 -0800 writes:

    Dylan> Greetings,
    Dylan> I am trying to add an extra labled axis in position 3 (top x-axis), with 
    Dylan> numbers that do not match up with the existing axes.

    Dylan> Surely this must be possible, and I am just doing it incorectly.

yes and yes.

But the real problem is that you don't follow the posting guide
which asks for a reproducible example
(hint: we don't have 'TIK').

    Dylan> So far I have tried the following:
    Dylan> #make a plot
    Dylan> plot(TIK, type="l", cex=.25, xlim=c(2,32), ylim=c(0,1600))

    Dylan> #try and add a new axis with different numbers in position 3
    Dylan> axis(3,0.154/(2*sin(TIK[,1]/2*pi/180)))

    ....................

    Dylan> ______________________________________________
    Dylan> R-help at stat.math.ethz.ch mailing list
    Dylan> https://stat.ethz.ch/mailman/listinfo/r-help

 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
   ^^^^^^



From p.dalgaard at biostat.ku.dk  Wed Nov 30 18:26:29 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2005 18:26:29 +0100
Subject: [R] OT: Statistics question
In-Reply-To: <1133348726.4389.9.camel@localhost.localdomain>
References: <1133348726.4389.9.camel@localhost.localdomain>
Message-ID: <x2ek4yvygq.fsf@viggo.kubism.ku.dk>

tom wright <tom at maladmin.com> writes:

> I apologise for asking this question here but I am hoping that someone
> can either give me direct guidance and/or point me to a better group for
> this type of disucssion.
> 
> I have what I feel should be a fairly simple problem but which my
> limited stats knowledge can't answer. I have two overlapping
> distributions (both normal) and I want to answer the question how do I
> calculate the cut-off value so I can be 95% sure that samples => than
> the cut off fall in the right hand distribution?
> 
> A while ago I did a bayesian statistics course that I think answered
> this very question but in the absence of any course notes or recent
> practice Ihave forgotten how to go about this.

The ratio of the tail probabilities should be bigger than 19
(.95/.05), *if* there is a 50/50 chance of belonging to either group.
Otherwise, you have to weight the tails according to the group
probability. Beware that this ratio is not necessarily a monotone
function of the cutoff if the variances differ.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From David.Ruau at rwth-aachen.de  Wed Nov 30 18:27:39 2005
From: David.Ruau at rwth-aachen.de (David Ruau)
Date: Wed, 30 Nov 2005 18:27:39 +0100
Subject: [R] Where ca I find the hcass2 Fortran code
Message-ID: <664cc4abd7fd9736d81081f5a22667cc@rwth-aachen.de>

Hi everybody,

I am using R 2.2.0 under OS X 10.3.9.
And I am working on the "hclust" function from the package STATS. 
Inside I found a call to a Fortran program hclust.f that I was able to 
find into the source distribution of R, but there is a also a call to a 
Fortran program hcass2. This very last I was enable to found it in the 
sources...
Does anyone can tell me where can I find it?

Thanks,
David



From herodote at oreka.com  Wed Nov 30 18:32:39 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Wed, 30 Nov 2005 18:32:39 +0100
Subject: [R] =?iso-8859-1?q?about_sorting_table?=
Message-ID: <IQS3EF$E9EE818DDB470875CF259FC884ACB168@oreka.com>

hi all,

I load a table with headers that enable me to acces it by the column names:

tab<-read.table("blob/data.dat",h=T)
attach(tab)

everythings are OK, but i try to sort this table against one of his column like this:

tab<-tab[order(tab$IndexUI),];

It is still ok, the table is sorted, if i type "tab" i see a sorted table.

but, when i call the column by their names, it appears that the column isn't sorted...

I believe that, is there a solution to attach column names another time, to reflect the effect of sorting this table?

thks all for your answers

guillaume.



From maechler at stat.math.ethz.ch  Wed Nov 30 18:35:08 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 30 Nov 2005 18:35:08 +0100
Subject: [R] newton-raphson
In-Reply-To: <43855F7800019949@mail-7.mail.tiscali.sys>
References: <43855F7800019949@mail-7.mail.tiscali.sys>
Message-ID: <17293.58060.181637.519831@stat.math.ethz.ch>

>>>>> "dlavecchia" == dlavecchia  <dlavecchia at tiscali.it>
>>>>>     on Wed, 30 Nov 2005 17:08:47 +0100 writes:

    dlavecchia> I have to solve a score function by using
    dlavecchia> Newton-Raphson algorithm. Is there such a
    dlavecchia> fucntion in R? I have built this algoritm

    dlavecchia> newton<-function(tgt,drva,th0,err) {
    dlavecchia> iter=0
    dlavecchia> repeat  {iter = iter+1
    dlavecchia> th1=th0-tgt(th0)/drva(th0)
    dlavecchia> if (abs(th0-th1)<err||abs(tgt(th1))<.1e-10)
    dlavecchia> break
    dlavecchia> th0=th1}
    dlavecchia> th1
    dlavecchia> }

    dlavecchia> but it does not work for my function because the
    dlavecchia> ratio "tgt(th0)/drva(th0)" is very high and the
    dlavecchia> algoritm does not converge. On the contrary it
    dlavecchia> works very well for some simple functions (like
    dlavecchia> X^2, X^3..and so on) 

    dlavecchia> Please, can you help me?

yes, instead of Newton-Raphson  which is fine for teaching the
basics, do use a much smarter and more "robust" algorithm like
the one built into

    uniroot(drva, ......)  ## read  ?uniroot and look at the 'Examples'

Martin



From spencer.graves at pdf.com  Wed Nov 30 19:11:25 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 30 Nov 2005 10:11:25 -0800
Subject: [R] Solution to non-linear equation problem
In-Reply-To: <438DDBD8.6020408@montana.edu>
References: <438DDBD8.6020408@montana.edu>
Message-ID: <438DEB4D.4040703@pdf.com>

	  RSiteSearch("solving nonlinear equations") just returned 42 hits for 
me, many of them potentially relevant to your question.  And there are 
more capabilities than what I found listed in the first few responses, 
including nlme in addition to optim, nls, etc.  A more focused question 
as suggested in the posting guide! 
http://www.R-project.org/posting-guide.html might elicit more 
informative responses.

	  spencer graves

Scott Story wrote:

> 	Thanks to Gabor, Duncan, and Peter. I knew the answer had something to 
> do with solving for a and b in terms of mean and variance. I will build 
> a function using the equations you provided Duncan and will  look into 
> using Mathomatic in the future Gabor. Appreciate the help. Peter, this 
> was not homework but I understand your concern. I don't use listserves 
> that often but they do open a whole arena for students to get free 
> information (hadn't even crossed my mind). Just as an aside, I take it 
> that R cannot solve non-linear equations (since you all gave me 
> solutions outside of it).
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From gunter.berton at gene.com  Wed Nov 30 19:13:31 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 30 Nov 2005 10:13:31 -0800
Subject: [R] strange plots with type = "h" option
In-Reply-To: <0F98C8BA43C00C42AFFBE000DA9DDB230AD3809D@pollux.richmond.edu>
Message-ID: <200511301813.jAUIDT0I011928@compton.gene.com>

?par "lend"

Before plotting:

par(lend='square')


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Owen, Jason
> Sent: Wednesday, November 30, 2005 9:15 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] strange plots with type = "h" option
> 
> Hello,
> 
> With the new version 2.2.0, I get strange plots when using the 
> histogram-like option in plot().  For example, a plot of binomial
> probabilities:
> 
> > plot(0:10,dbinom(0:10,10,.1), type = "h", lwd = 30)
> 
> gives me weird fat cirular bars, with mass out at values with low
> probability.  What is the issue here?  This never happened 
> with earlier 
> versions.
> 
> R on Windows XP SP 2
> Intel processor
> 
> Jason
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Greg.Snow at intermountainmail.org  Wed Nov 30 19:22:47 2005
From: Greg.Snow at intermountainmail.org (Gregory Snow)
Date: Wed, 30 Nov 2005 11:22:47 -0700
Subject: [R] strange plots with type = "h" option
Message-ID: <07E228A5BE53C24CAD490193A7381BBB0C5146@LP-EXCHVS07.CO.IHC.COM>

Look at ?par specifically at the 'lend' argument.

You probably want to do:

par(lend=1)

Before doing your plot.

-- 
Gregory L. Snow Ph.D.
Statistical Data Center, IHC
greg.snow at ihc.com
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Owen, Jason
> Sent: Wednesday, November 30, 2005 10:15 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] strange plots with type = "h" option
> 
> Hello,
> 
> With the new version 2.2.0, I get strange plots when using 
> the histogram-like option in plot().  For example, a plot of binomial
> probabilities:
> 
> > plot(0:10,dbinom(0:10,10,.1), type = "h", lwd = 30)
> 
> gives me weird fat cirular bars, with mass out at values with 
> low probability.  What is the issue here?  This never 
> happened with earlier versions.
> 
> R on Windows XP SP 2
> Intel processor
> 
> Jason
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tom at maladmin.com  Wed Nov 30 14:33:58 2005
From: tom at maladmin.com (tom wright)
Date: Wed, 30 Nov 2005 08:33:58 -0500
Subject: [R] cant get colAUC to plot
Message-ID: <1133357638.4389.13.camel@localhost.localdomain>

Can someone please explain why this wont plot. The cats example given
for the colAUC function will plot.

Many thanks again
tom

#########################
library(caTools)
a<-rnorm(100)
b<-rbinom(100,1,0.7)
colAUC(a,b,plotROC=TRUE)

> colAUC(a,b,plotROC=TRUE)
Error in strwidth(legend, units = "user", cex = cex) :
        argument "legend" is missing, with no default


> R.Version()
$platform
[1] "x86_64-pc-linux-gnu"

$arch
[1] "x86_64"

$os
[1] "linux-gnu"

$system
[1] "x86_64, linux-gnu"

$status
[1] ""

$major
[1] "2"

$minor
[1] "1.0"

$year
[1] "2005"

$month
[1] "04"

$day
[1] "18"

$language
[1] "R"



From leif at reflectivity.com  Wed Nov 30 19:37:41 2005
From: leif at reflectivity.com (Leif Kirschenbaum)
Date: Wed, 30 Nov 2005 10:37:41 -0800
Subject: [R] qcc
Message-ID: <200511301837.jAUIbjkU021226@hypatia.math.ethz.ch>

	If you examine the code for the function "violating.runs" in the qcc package (try "violating.runs") you can deconstruct it to find that the code flags runs where there are .qcc.options$run.length or more points in a row on one side of the center (process mean).
	However, the classic Shewhart rules dictate that a run of monotonically increasing or decreasing points of <run length> is also a run violation. I include here my modified version of the function "violating.runs" which also checks for these violations:

##
## Correct some typos in violating.runs from qcc package
## Added test for run.length of points monotonically increasing or decreasing
## The simplest way is to re-run the code but with "diffs"
## representing the sign of the difference from one point to the next
##
violating.runs<-function (object, run.length = qcc.options("run.length")) 
{
    center <- object$center
    statistics <- c(object$statistics, object$new.statistics)
    cl <- object$limits
    violators <- numeric()

   for(i in 1:2){
    diffs <- statistics - center
    if(i==2) {
     diffs <- c(0,diff(statistics))
	## need to decrement run.length since we're looking at differences between points
     run.length<-run.length-1
    }
    diffs[diffs > 0] <- 1
    diffs[diffs < 0] <- -1
    runs <- rle(diffs)
    index.lengths <- (1:length(runs$lengths))[runs$lengths >= run.length]
    index.stats <- 1:length(statistics)
    vruns <- rep(runs$lengths >= run.length, runs$lengths)
    vruns.above <- (vruns & (diffs > 0))
    vruns.below <- (vruns & (diffs < 0))
    rvruns.above <- rle(vruns.above)
    rvruns.below <- rle(vruns.below)
    vbeg.above <- cumsum(rvruns.above$lengths)[rvruns.above$values]-(rvruns.above$lengths - run.length)[rvruns.above$values]
    vend.above <- cumsum(rvruns.above$lengths)[rvruns.above$values]
    vbeg.below <- cumsum(rvruns.below$lengths)[rvruns.below$values]-(rvruns.below$lengths - run.length)[rvruns.below$values]
    vend.below <- cumsum(rvruns.below$lengths)[rvruns.below$values]
    if (length(vbeg.above)) {
        for (i in 1:length(vbeg.above)) violators <- c(violators, vbeg.above[i]:vend.above[i])
    }
    if (length(vbeg.below)) {
        for (i in 1:length(vbeg.below)) violators <- c(violators, vbeg.below[i]:vend.below[i])
    }
   } ## ENDOF for i in 1:2
    return(violators)
}


> 3)Is there any more criterias made somewhere ?
	The other criteria is found by the function "beyond.limits" which checks to see if any points are beyond the upper or lower control limits.

	I believe that Prof. Scrucca wrote the qcc package as a tool to demonstrate process control to his students taking his statistics classes. Hence the statistics are excellent. For example note the use of log-gamma and exponential functions in the function "sd.xbar" to calculate the constant "c4", which avoids the divison of extremely large [~ 1E20] numbers:
	c4 <- function(n) sqrt(2/(n-1)) * exp(lgamma(n/2) - lgamma((n-1)/2)))

where a textbook might write it as:
	c4 <- function(n) sqrt(2/(n-1)) * (gamma(n/2) / gamma((n-1)/2)))
	Thus I suggest that although qcc provides an excellent basis for constructing statistical control reports, that you should be prepared to modify it for your purposes (as I am heavily doing).

P.S. I change .qcc.options using syntax such as ".qcc.options$violating.runs<-7".

Leif S. Kirschenbaum, Ph.D.
Senior Yield Engineer
Reflectivity, Inc.
408-737-8100 x307
408-737-8153 (Fax)


> Date: Tue, 29 Nov 2005 02:08:57 +0200
> From: Tommi Viitanen <totavi at utu.fi>
> Subject: [R] qcc
> 
> violating.runs
[deleted]
> 
> that the criteria for the violating is 5 but
> 1)I cannot find "5" in the code of the function. Where is the "5" ?
> 2)What is the easiest way to change it ?
> 3)Is there any more criterias made somewhere ?
> 
> Yours sincerelly, Tommi Viitanen
> 
[deleted]
> ------------------------------
> 
> Date: Tue, 29 Nov 2005 08:58:24 +0100
> From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
> Subject: Re: [R] qcc
> To: Tommi Viitanen <totavi at utu.fi>
> 
> 
> See ?violating.runs:
> violating.runs(object, run.length = qcc.options("run.length"))
>                                      ^^^^^ ^^^ ^^^ ^^ ^^^ ^^^
> 
[deleted]
> 
> Uwe Ligges



From wowen at richmond.edu  Wed Nov 30 19:39:23 2005
From: wowen at richmond.edu (Owen, Jason)
Date: Wed, 30 Nov 2005 13:39:23 -0500
Subject: [R] strange plots with type = "h" option
Message-ID: <0F98C8BA43C00C42AFFBE000DA9DDB230C459763@pollux.richmond.edu>

Thanks a lot!  I wonder why that isn't the default....

 

> -----Original Message-----
> From: Berton Gunter [mailto:gunter.berton at gene.com] 
> Sent: Wednesday, November 30, 2005 1:14 PM
> To: Owen, Jason; r-help at stat.math.ethz.ch
> Subject: RE: [R] strange plots with type = "h" option
> 
> ?par "lend"
> 
> Before plotting:
> 
> par(lend='square')
> 
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the 
> scientific learning
> process."  - George E. P. Box
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Owen, Jason
> > Sent: Wednesday, November 30, 2005 9:15 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] strange plots with type = "h" option
> > 
> > Hello,
> > 
> > With the new version 2.2.0, I get strange plots when using the 
> > histogram-like option in plot().  For example, a plot of binomial
> > probabilities:
> > 
> > > plot(0:10,dbinom(0:10,10,.1), type = "h", lwd = 30)
> > 
> > gives me weird fat cirular bars, with mass out at values with low
> > probability.  What is the issue here?  This never happened 
> > with earlier 
> > versions.
> > 
> > R on Windows XP SP 2
> > Intel processor
> > 
> > Jason
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
>



From murdoch at stats.uwo.ca  Wed Nov 30 19:51:42 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 30 Nov 2005 13:51:42 -0500
Subject: [R] Corrupted workspace(?)
In-Reply-To: <005801c5f5cf$9b10c4d0$65036497@sta.uniroma1.it>
References: <005801c5f5cf$9b10c4d0$65036497@sta.uniroma1.it>
Message-ID: <438DF4BE.6070004@stats.uwo.ca>

On 11/30/2005 12:00 PM, giovanna jona lasinio wrote:
> Dear R helpers,
> I'm using R2.2 under windows XP professional on a Dell double processor
> workstation.
> I have a large (29Mb) workspace that I'm trying to load both, by double
> clicking and by direct load. The message I get is "Bad restore  file
> magic number (file may be corrupted)...". I believe that this is a
> consequence of a huge spike in the electrical system that my voltage
> regulator couldn't manage.
> I already searched the R mail archives and it seems I'm in big
> troubles... Does any one have a new idea on what I can do to recover it?

We don't have any tools for recovering a partially corrupted database. 
This isn't much help to you now, but my usual advice is not to use 
workspaces as the main storage for data or code.  Keep both in text 
files, and back them up.

The storage format is documented in the src/main/serialize.c source code 
file; if you feel like some hacking, you might be able to write a 
recovery tool based on that.

Duncan Murdoch



From murdoch at stats.uwo.ca  Wed Nov 30 19:55:53 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 30 Nov 2005 13:55:53 -0500
Subject: [R] Where ca I find the hcass2 Fortran code
In-Reply-To: <664cc4abd7fd9736d81081f5a22667cc@rwth-aachen.de>
References: <664cc4abd7fd9736d81081f5a22667cc@rwth-aachen.de>
Message-ID: <438DF5B9.1000203@stats.uwo.ca>

On 11/30/2005 12:27 PM, David Ruau wrote:
> Hi everybody,
> 
> I am using R 2.2.0 under OS X 10.3.9.
> And I am working on the "hclust" function from the package STATS. 
> Inside I found a call to a Fortran program hclust.f that I was able to 
> find into the source distribution of R, but there is a also a call to a 
> Fortran program hcass2. This very last I was enable to found it in the 
> sources...
> Does anyone can tell me where can I find it?

Fortran is not case sensitive.  This is HCASS2, in 
src/library/stats/src/hclust.f

Duncan Murdoch



From fparlamis at mac.com  Wed Nov 30 19:56:57 2005
From: fparlamis at mac.com (Parlamis Franklin)
Date: Wed, 30 Nov 2005 08:56:57 -1000
Subject: [R] date/time arithmetic
In-Reply-To: <5C840B2C-24A1-4062-B717-1ECB863FC9E5@mac.com>
References: <5C840B2C-24A1-4062-B717-1ECB863FC9E5@mac.com>
Message-ID: <7CF4A3E0-66E3-4512-AA97-919782666AC4@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/881aa781/attachment.pl

From br44114 at gmail.com  Wed Nov 30 20:42:12 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 30 Nov 2005 14:42:12 -0500
Subject: [R] date/time arithmetic
Message-ID: <8d5a36350511301142r31d2af2bg5a8b595ec2963b97@mail.gmail.com>

What do you need a bunch of functions for? I'm not familiar with the
details of difftime objects, however an easy way out of here is to get
the time difference in seconds, which you can then add or subtract as
you please from date-times.

x<-Sys.time(); y<-Sys.time()+3600
diff <- as.numeric(difftime(y,x,units="secs"))
x; y; diff
y-diff


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> Parlamis Franklin
> Sent: Wednesday, November 30, 2005 1:57 PM
> To: Parlamis Franklin
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] date/time arithmetic
>
>
> Hi.  Sorry to pester, but I didn't get a reply to this
> (perhaps owing
> to Thanksgiving break).  This seems to be a case where the R
> language
> simply doesn't do what it says it does (i.e., allow addition or
> subtraction of a difftime object and a date-time object).
>
> Am I wrong?  I am about to write a bunch of functions (easy, but
> perhaps redundant) to do this, and I just want to know whether there
> is a bug here, or whether I am missing something obvious.
>
> Thanks.
>
> FP
>
> On Nov 23, 2005, at 10:02 AM, Parlamis Franklin wrote:
>
> > On the help page "DateTimeClasses {base}" it says:
> >
> > "One can add or subtract a number of seconds or a difftime object
> > from a date-time object, but not add two date-time objects."
> >
> > However,
> >
> > > x<-Sys.time(); y<-Sys.time()+3600
> > > diff<-y-x
> > > x; y; diff
> > [1] "2005-11-23 19:58:20 GMT"
> > [1] "2005-11-23 20:58:20 GMT"
> > Time difference of 1 hours
> > > y-diff
> > [1] "2005-11-23 20:58:19 GMT"
> > Warning message:
> > Incompatible methods ("-.POSIXt", "Ops.difftime") for "-"
> >
> > Do I have the syntax wrong?  Or is some conversion of the difftime
> > object to raw seconds necessary prior to performing arithmetic?
> > And is the help page wrong?
> >
> > Thanks.
> >
> > Franklin
> >
> >
> >
> >
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed Nov 30 20:59:49 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 14:59:49 -0500
Subject: [R] date/time arithmetic
In-Reply-To: <5C840B2C-24A1-4062-B717-1ECB863FC9E5@mac.com>
References: <5C840B2C-24A1-4062-B717-1ECB863FC9E5@mac.com>
Message-ID: <971536df0511301159y36265e25uaa7681439347f576@mail.gmail.com>

I don't know what is wrong but you could use the numeric class
instead of the difftime class as a workaround:

x <- Sys.time()
y <- x + 3600
diffyx <- as.numeric(y) - as.numeric(x)
identical(y - diffyx, x) # TRUE

On 11/23/05, Parlamis Franklin <fparlamis at mac.com> wrote:
> On the help page "DateTimeClasses {base}" it says:
>
> "One can add or subtract a number of seconds or a difftime object
> from a date-time object, but not add two date-time objects."
>
> However,
>
>  > x<-Sys.time(); y<-Sys.time()+3600
>  > diff<-y-x
>  > x; y; diff
> [1] "2005-11-23 19:58:20 GMT"
> [1] "2005-11-23 20:58:20 GMT"
> Time difference of 1 hours
>  > y-diff
> [1] "2005-11-23 20:58:19 GMT"
> Warning message:
> Incompatible methods ("-.POSIXt", "Ops.difftime") for "-"
>
> Do I have the syntax wrong?  Or is some conversion of the difftime
> object to raw seconds necessary prior to performing arithmetic?  And
> is the help page wrong?
>
> Thanks.
>
> Franklin
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sourceforge at metrak.com  Wed Nov 30 21:50:23 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Thu, 01 Dec 2005 07:50:23 +1100
Subject: [R] calculating IRR (accounting) in R
Message-ID: <438E108F.7020003@metrak.com>

I am trying to replace a spreadsheet model of a project justification 
with an R script.

I can't seem to track down R functions to calculate Internal Rate of 
Return and NPV?  Am I missing something?  NPV doesn't seem so difficult 
to calculate (at least for a regular series) but I am struggling to 
identify how to solve for IRR in R.

It would be sufficient if it worked for a regular series but really 
useful if there was something that worked with irregular time series.

cheers



From fparlamis at mac.com  Wed Nov 30 21:58:55 2005
From: fparlamis at mac.com (Parlamis Franklin)
Date: Wed, 30 Nov 2005 10:58:55 -1000
Subject: [R] date/time arithmetic
In-Reply-To: <8d5a36350511301142r31d2af2bg5a8b595ec2963b97@mail.gmail.com>
References: <8d5a36350511301142r31d2af2bg5a8b595ec2963b97@mail.gmail.com>
Message-ID: <28EEF87B-BDCF-4581-B53E-7C329F2B84E6@mac.com>

I agree there are workarounds (although reduction to secs is not  
necessarily graceful when you want to add, say, "1 month" to a date,  
or any other increment for which the total number of seconds is not  
fixed--and is also complicated by the existence of leap seconds).   
But upon inspection, they are never as clean as they seem.

The representation in the man pages that difftime / date-time  
arithmetic was possible made me hopeful that some of these tasks had  
been handled already, in a manner consistent with the internal date- 
time functions.


On Nov 30, 2005, at 9:42 AM, bogdan romocea wrote:

> What do you need a bunch of functions for? I'm not familiar with the
> details of difftime objects, however an easy way out of here is to get
> the time difference in seconds, which you can then add or subtract as
> you please from date-times.
>
> x<-Sys.time(); y<-Sys.time()+3600
> diff <- as.numeric(difftime(y,x,units="secs"))
> x; y; diff
> y-diff
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
>> Parlamis Franklin
>> Sent: Wednesday, November 30, 2005 1:57 PM
>> To: Parlamis Franklin
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] date/time arithmetic
>>
>>
>> Hi.  Sorry to pester, but I didn't get a reply to this
>> (perhaps owing
>> to Thanksgiving break).  This seems to be a case where the R
>> language
>> simply doesn't do what it says it does (i.e., allow addition or
>> subtraction of a difftime object and a date-time object).
>>
>> Am I wrong?  I am about to write a bunch of functions (easy, but
>> perhaps redundant) to do this, and I just want to know whether there
>> is a bug here, or whether I am missing something obvious.
>>
>> Thanks.
>>
>> FP
>>
>> On Nov 23, 2005, at 10:02 AM, Parlamis Franklin wrote:
>>
>>> On the help page "DateTimeClasses {base}" it says:
>>>
>>> "One can add or subtract a number of seconds or a difftime object
>>> from a date-time object, but not add two date-time objects."
>>>
>>> However,
>>>
>>>> x<-Sys.time(); y<-Sys.time()+3600
>>>> diff<-y-x
>>>> x; y; diff
>>> [1] "2005-11-23 19:58:20 GMT"
>>> [1] "2005-11-23 20:58:20 GMT"
>>> Time difference of 1 hours
>>>> y-diff
>>> [1] "2005-11-23 20:58:19 GMT"
>>> Warning message:
>>> Incompatible methods ("-.POSIXt", "Ops.difftime") for "-"
>>>
>>> Do I have the syntax wrong?  Or is some conversion of the difftime
>>> object to raw seconds necessary prior to performing arithmetic?
>>> And is the help page wrong?
>>>
>>> Thanks.
>>>
>>> Franklin
>>>
>>>
>>>
>>>
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>



From ggrothendieck at gmail.com  Wed Nov 30 22:13:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 16:13:41 -0500
Subject: [R] date/time arithmetic
In-Reply-To: <28EEF87B-BDCF-4581-B53E-7C329F2B84E6@mac.com>
References: <8d5a36350511301142r31d2af2bg5a8b595ec2963b97@mail.gmail.com>
	<28EEF87B-BDCF-4581-B53E-7C329F2B84E6@mac.com>
Message-ID: <971536df0511301313h3c6af209sd3459dfa7068b538@mail.gmail.com>

On 11/30/05, Parlamis Franklin <fparlamis at mac.com> wrote:
> I agree there are workarounds (although reduction to secs is not
> necessarily graceful when you want to add, say, "1 month" to a date,

You can do that without difftime via:

  seq(x, len = 2, by = "month")[2]

> or any other increment for which the total number of seconds is not
> fixed--and is also complicated by the existence of leap seconds).
> But upon inspection, they are never as clean as they seem.
>
> The representation in the man pages that difftime / date-time
> arithmetic was possible made me hopeful that some of these tasks had
> been handled already, in a manner consistent with the internal date-
> time functions.
>
>
> On Nov 30, 2005, at 9:42 AM, bogdan romocea wrote:
>
> > What do you need a bunch of functions for? I'm not familiar with the
> > details of difftime objects, however an easy way out of here is to get
> > the time difference in seconds, which you can then add or subtract as
> > you please from date-times.
> >
> > x<-Sys.time(); y<-Sys.time()+3600
> > diff <- as.numeric(difftime(y,x,units="secs"))
> > x; y; diff
> > y-diff
> >
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch
> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> >> Parlamis Franklin
> >> Sent: Wednesday, November 30, 2005 1:57 PM
> >> To: Parlamis Franklin
> >> Cc: r-help at stat.math.ethz.ch
> >> Subject: Re: [R] date/time arithmetic
> >>
> >>
> >> Hi.  Sorry to pester, but I didn't get a reply to this
> >> (perhaps owing
> >> to Thanksgiving break).  This seems to be a case where the R
> >> language
> >> simply doesn't do what it says it does (i.e., allow addition or
> >> subtraction of a difftime object and a date-time object).
> >>
> >> Am I wrong?  I am about to write a bunch of functions (easy, but
> >> perhaps redundant) to do this, and I just want to know whether there
> >> is a bug here, or whether I am missing something obvious.
> >>
> >> Thanks.
> >>
> >> FP
> >>
> >> On Nov 23, 2005, at 10:02 AM, Parlamis Franklin wrote:
> >>
> >>> On the help page "DateTimeClasses {base}" it says:
> >>>
> >>> "One can add or subtract a number of seconds or a difftime object
> >>> from a date-time object, but not add two date-time objects."
> >>>
> >>> However,
> >>>
> >>>> x<-Sys.time(); y<-Sys.time()+3600
> >>>> diff<-y-x
> >>>> x; y; diff
> >>> [1] "2005-11-23 19:58:20 GMT"
> >>> [1] "2005-11-23 20:58:20 GMT"
> >>> Time difference of 1 hours
> >>>> y-diff
> >>> [1] "2005-11-23 20:58:19 GMT"
> >>> Warning message:
> >>> Incompatible methods ("-.POSIXt", "Ops.difftime") for "-"
> >>>
> >>> Do I have the syntax wrong?  Or is some conversion of the difftime
> >>> object to raw seconds necessary prior to performing arithmetic?
> >>> And is the help page wrong?
> >>>
> >>> Thanks.
> >>>
> >>> Franklin
> >>>
> >>>
> >>>
> >>>
> >>
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From br44114 at gmail.com  Wed Nov 30 22:40:15 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 30 Nov 2005 16:40:15 -0500
Subject: [R] OT: Statistics question
Message-ID: <8d5a36350511301340m16fb7ccbxe233692556480ea0@mail.gmail.com>

What if the distributions are not normal etc? You might want to try a
simulation to get an answer. Draw random samples from each
distribution (without assuming normality etc - one way to do this is
to get the quantiles, then draw a sample of quantiles, then draw a
value from each quantile), throw them into a grid and get the % of
As/Bs in each bin. After several iterations it should become
reasonably clear where the 95% B bins start.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Wednesday, November 30, 2005 12:26 PM
> To: tom wright
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] OT: Statistics question
>
>
> tom wright <tom at maladmin.com> writes:
>
> > I apologise for asking this question here but I am hoping
> that someone
> > can either give me direct guidance and/or point me to a
> better group for
> > this type of disucssion.
> >
> > I have what I feel should be a fairly simple problem but which my
> > limited stats knowledge can't answer. I have two overlapping
> > distributions (both normal) and I want to answer the
> question how do I
> > calculate the cut-off value so I can be 95% sure that
> samples => than
> > the cut off fall in the right hand distribution?
> >
> > A while ago I did a bayesian statistics course that I think answered
> > this very question but in the absence of any course notes or recent
> > practice Ihave forgotten how to go about this.
>
> The ratio of the tail probabilities should be bigger than 19
> (.95/.05), *if* there is a 50/50 chance of belonging to either group.
> Otherwise, you have to weight the tails according to the group
> probability. Beware that this ratio is not necessarily a monotone
> function of the cutoff if the variances differ.
>
> --
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX:
> (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Robert.McGehee at geodecapital.com  Wed Nov 30 22:44:36 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 30 Nov 2005 16:44:36 -0500
Subject: [R] calculating IRR (accounting) in R
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946720@MSGBOSCLB2WIN.DMN1.FMR.COM>

Hello,
If we define IRR implicitly such that NPV(C, IRR) = 0, then we can just
write an IRR function that finds the zeros of the NPV function. Here are
two such functions:

NPV <- function(C, r) {
    sum(C / (1 + r) ^ (seq(along = C) - 1))
}

IRR <- function(C) {
    uniroot(NPV, c(0, 1), C = C)$root
}

Hope this helps,
Robert

-----Original Message-----
From: paul sorenson [mailto:sourceforge at metrak.com] 
Sent: Wednesday, November 30, 2005 3:50 PM
To: r-help
Subject: [R] calculating IRR (accounting) in R


I am trying to replace a spreadsheet model of a project justification 
with an R script.

I can't seem to track down R functions to calculate Internal Rate of 
Return and NPV?  Am I missing something?  NPV doesn't seem so difficult 
to calculate (at least for a regular series) but I am struggling to 
identify how to solve for IRR in R.

It would be sufficient if it worked for a regular series but really 
useful if there was something that worked with irregular time series.

cheers

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Wed Nov 30 22:46:03 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 30 Nov 2005 15:46:03 -0600
Subject: [R] likelihood ratio tests using glmmPQL
In-Reply-To: <41E1ED29E5E8E34BBDD8B82CFA1A9D04083DBD@ZSL26.zsl.org>
References: <41E1ED29E5E8E34BBDD8B82CFA1A9D04083DBD@ZSL26.zsl.org>
Message-ID: <40e66e0b0511301346v256a45f7n5d8bcbcdfed825ac@mail.gmail.com>

If you use lmer from the lme4 package (actually it is in the Matrix
package but "logically" it is in the lme4 package) to fit a
Generalized Linear Mixed Model you have the option of using PQL, or
the Laplace approximation or Adaptive Gauss-Hermite Quadrature (AGQ). 
The log-likelihood for any of these methods, including PQL, is an
approximation to the actual log-likelihood of the GLMM model and can
be used for likelihood ratio tests.

On 11/30/05, Elizabeth Boakes <Elizabeth.Boakes at ioz.ac.uk> wrote:
>
>
> I am analysing some binary data with a mixed effects model using
> glmmPQL.
>
> I am aware that I cannot use the AIC values to help me find the minimum
> adequate model so how do I perform likelihood ratio tests?  I need to
> fix on the minimum adequate model but I'm not sure of the proper way to
> do this.
>
>
>
> Thank you very much,
>
> Elizabeth Boakes
>
> Elizabeth Boakes
> PhD Student
> Institute of Zoology
> Regent's Park
> London NW1 4RY
> tel: 020 7449 6621
>
>
>
>
>
> _________________________________________________________________________
> This e-mail has been sent in confidence to the named address...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed Nov 30 23:01:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 17:01:09 -0500
Subject: [R] calculating IRR (accounting) in R
In-Reply-To: <438E108F.7020003@metrak.com>
References: <438E108F.7020003@metrak.com>
Message-ID: <971536df0511301401n37f7188dr84132f0ed5e4558a@mail.gmail.com>

You can use uniroot:

npv <- function(i, cf, tt = seq(along = cf)) sum(cf / (1-i)^tt)
npv0 <- npv(0.1, 1:10)
npv0
f <- function(i, cf, npv0) npv0 - npv(i, cf)
uniroot(f, c(0.01, .99), cf = 1:10, npv0 = npv0)


On 11/30/05, paul sorenson <sourceforge at metrak.com> wrote:
> I am trying to replace a spreadsheet model of a project justification
> with an R script.
>
> I can't seem to track down R functions to calculate Internal Rate of
> Return and NPV?  Am I missing something?  NPV doesn't seem so difficult
> to calculate (at least for a regular series) but I am struggling to
> identify how to solve for IRR in R.
>
> It would be sufficient if it worked for a regular series but really
> useful if there was something that worked with irregular time series.
>
> cheers
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed Nov 30 23:02:56 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 17:02:56 -0500
Subject: [R] calculating IRR (accounting) in R
In-Reply-To: <971536df0511301401n37f7188dr84132f0ed5e4558a@mail.gmail.com>
References: <438E108F.7020003@metrak.com>
	<971536df0511301401n37f7188dr84132f0ed5e4558a@mail.gmail.com>
Message-ID: <971536df0511301402y767140a1leba7a52776948b8f@mail.gmail.com>

On 11/30/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> You can use uniroot:
>
> npv <- function(i, cf, tt = seq(along = cf)) sum(cf / (1-i)^tt)

Sorry that should 1+i

> npv0 <- npv(0.1, 1:10)
> npv0
> f <- function(i, cf, npv0) npv0 - npv(i, cf)
> uniroot(f, c(0.01, .99), cf = 1:10, npv0 = npv0)
>
>
> On 11/30/05, paul sorenson <sourceforge at metrak.com> wrote:
> > I am trying to replace a spreadsheet model of a project justification
> > with an R script.
> >
> > I can't seem to track down R functions to calculate Internal Rate of
> > Return and NPV?  Am I missing something?  NPV doesn't seem so difficult
> > to calculate (at least for a regular series) but I am struggling to
> > identify how to solve for IRR in R.
> >
> > It would be sufficient if it worked for a regular series but really
> > useful if there was something that worked with irregular time series.
> >
> > cheers
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From charles_loboz at yahoo.com  Wed Nov 30 23:12:05 2005
From: charles_loboz at yahoo.com (charles loboz)
Date: Wed, 30 Nov 2005 14:12:05 -0800 (PST)
Subject: [R] unexpected result from KalmanRun (KalmanLike, StructTS)
Message-ID: <20051130221205.35990.qmail@web60814.mail.yahoo.com>

(re-formulate, re-send, without html) 

  for vector y = c(1,2,3,4,5), H = 0.66 manual
calculations
  using the equations below give a =
c(1,1.66,2.55,3.51,4.50).
  KalmanRun with these parameters gives res$states =
(1,1,1,1,1)!


for Kalman Filter Durbin/Koopman give at p67 eqs 
4.13:
  
     v = y - Z a,  F = Z P Z' + H,  K = T P Z' / F +
H,
     a[t+1] = T a + K v,  P[t+1] = T P L' + R Q R'
  
  for P1 = 0, Q=0,  T=Z=R=1 that reduces to:
  
     v = y - a,  F = H,  K = H,  a[t+1] = a + K v, 
P[t+1] = 0
     (also equivalent to exponential moving average,
Durbin/Koopman p49)
  
  So I am getting a serious discrepancy between manual
and KalmanRun computations. To make things more
interesting, looking into the code of arima.c we have
at line 109 an equivalent of
      a[t+1] = anew + Pnew * resid0 / gain
  where gain = mod$h = H (by line 97), resid0 = y-a =
v (by lines 94-96)
  Since Pnew = 0, then a[t+1] = a, which explains why
the computation
  returns res$states = c(1,1,1,1,1).
  
  The help file says "'states', the contemporaneous
state estimates",
  which I assumed to mean 'a' in the equations above.
But that
  assumption does not agree with the numerical
results. It also
  does not agree with the coding(?) as  a[t+1] = a + K
v  differs
  substantially from   a[t+1] = anew + Pnew * resid0 /
gain. (all the previous lines of coding follow the
kalman filter equations, but this one does not seem to
- do we have some strong reformulation of the
equations in the basic form?).
  
  So, what does 'states' contain?



From u08adh at hotmail.com  Wed Nov 30 19:50:11 2005
From: u08adh at hotmail.com (Andreas Hary)
Date: Wed, 30 Nov 2005 18:50:11 -0000
Subject: [R] SciViews-R_0.8-9 Console problem
References: <438D80DB.8040006@bio.ntnu.no>
Message-ID: <BAY114-DAV2B2713443B49ED569E684DF4A0@phx.gbl>

Hi,

> [...] the command/script window does not appear. [...]

Have you tried

Misc => Toolbars => Command

on the menu? Don't click until you get to Command, otherwise the menu closes 
again for some reason.
Regards,

Andreas






----- Original Message ----- 
From: "Henrik Parn" <henrik.parn at bio.ntnu.no>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, November 30, 2005 10:37 AM
Subject: [R] SciViews-R_0.8-9 Console problem


Dear R users,

I successfully installed SciViews the other day. However, when I try to
run it now,  the command/script window does not appear. Strange. Well,
actually I see a tendency to a script window (in the lower part of the
sciview window where it is suppose to be) during start up, but when the
program is entirely open, the script window is gone.

I have tried to uninstall and reinstall latest versions available of R,
Sciviews and Tinn-R several times, but it does not solve the problem. I
have tried both the 'Detailed installation of Sciviews-R' and 'Manually
installing additional R packages'.

Another 'new' problem I did not have before is the default size of the
Sciviews window - both the upper and lower part 'disappears' outside my
screen when starting up. And in the upper 'R-window' two prompts appear
with maybe 6-7 lines in between. However, as soon as I hit a key the
lower of these to prompt disappears. But still no script editor.

I suspect I have missed something fundamental...but what?

Any suggestions that can help me is appreciated!

Thanks!

Henrik

PS I have tried to mail to support at sciviews.org, but the mail just bounces.

I have WinXP, PIII, 512 RAM

R.Version()
$platform
[1] "i386-pc-mingw32"
$arch
[1] "i386"
$os
[1] "mingw32"
$system
[1] "i386, mingw32"
$status
[1] ""
$major
[1] "2"
$minor
[1] "2.0"
$year
[1] "2005"
$month
[1] "10"
$day
[1] "06"
$"svn rev"
[1] "35749"
$language
[1] "R"

<>capabilities("tcltk")
tcltk TRUE

SciViews version: SciViews-R_0.8-9Setup.exe

 From the SciViews R console:
search()

[1] ".GlobalEnv"        "package:Rcmdr"     "package:car"     [4]
"package:svGUI"     "package:svViews"   "package:svIO"    [7]
"package:svMisc"    "package:R2HTML"    "package:tcltk"   [10]
"package:methods"   "package:stats"     "package:graphics"
[13] "package:grDevices" "package:utils"     "package:datasets"
[16] "TempEnv"           "RcmdrEnv"          "Autoloads"       [19]
"package:base"

-- 
************************
Henrik P??rn
Department of Biology
NTNU
7491 Trondheim
Norway

+47 735 96282 (office)
+47 909 89 255 (mobile)
+47 735 96100 (fax)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



