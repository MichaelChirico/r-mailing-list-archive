From jpk2005 at columbia.edu  Sun May  1 04:19:30 2005
From: jpk2005 at columbia.edu (Jouni Kerman)
Date: Sat, 30 Apr 2005 22:19:30 -0400
Subject: [R] eigen() may fail for some symmetric matrices, affects mvrnorm()
Message-ID: <7348D12D-B9E7-11D9-901D-000393C4E756@columbia.edu>


Hi all,

Recently our statistics students noticed that their Gibbs samplers were 
crashing due to some NaNs in some parameters. The NaNs came from 
mvrnorm (Ripley & Venables' MASS package multivariate normal sampling 
function) and with some more investigation it turned out that they were 
generated by function eigen, the eigenvalue computing function. The 
problem did not seem to happen when the option "symmetric" was set to 
FALSE; at first I thought that only matrices that were not completely 
symmetric (such as ones with small errors of the magnitude 1e-17 or so) 
were affected but then we encountered matrices that were perfectly 
symmetric but eigen still generated NaNs when symmetric=TRUE.

I wrote a trivial patch to mvrnorm to detect this problem and to 
recompute the eigenvalues with symmetric=TRUE and EISPACK=TRUE. This 
seems to work. symmetric=FALSE also seems to work, but is somewhat 
slower.

If you use mvrnorm then you may want to try this. Details and the patch 
are on my web page, along with one example of a problematic symmetric 
matrix (to be loaded with the 'load' function). The NaNs should appear 
in the eigenvectors. This particular matrix may not produce an error on 
all platforms.

      http://www.stat.columbia.edu/~kerman/Software/rprog.html

Thanks to Radford Neal who suggested on our research web log (link 
below) that setting EISPACK=TRUE may help.

     http://www.stat.columbia.edu/~cook/movabletype/mlm/


Jouni Kerman
Department of Statistics
Columbia University
New York



From ripley at stats.ox.ac.uk  Sun May  1 08:42:09 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 1 May 2005 07:42:09 +0100 (BST)
Subject: [R] eigen() may fail for some symmetric matrices, affects
	mvrnorm()
In-Reply-To: <7348D12D-B9E7-11D9-901D-000393C4E756@columbia.edu>
References: <7348D12D-B9E7-11D9-901D-000393C4E756@columbia.edu>
Message-ID: <Pine.LNX.4.61.0505010712060.20811@gannet.stats>

I do not begin to understand this.

mvrnorm() uses EISPACK=TRUE and always has done, explicitly since there
was such an argument to eigen() (since 15 Jan 2003: the argument was
introduced in R 1.7.0 in April 2003).  Just take a look at the code.

The claimed `patch' (which is not a patch at all but a replacement) is not 
based on any remotely current version of MASS.

The puported example against eigen(EISPACK=FALSE) (the default) works 
correctly (no NaNs) on all my systems, including the distributed version 
of rw2010.  It seems that students (and `Jouni Kerman' is listed as a 
student) at the Department of Statistics, Columbia University are provided 
with a deficient computing environment (unstated despite the posting 
guide): my surmise is that a broken external BLAS or LAPACK is in use.


On Sat, 30 Apr 2005, Jouni Kerman wrote:

>
> Hi all,
>
> Recently our statistics students noticed that their Gibbs samplers were 
> crashing due to some NaNs in some parameters. The NaNs came from mvrnorm 
> (Ripley & Venables' MASS package multivariate normal sampling function) and 
> with some more investigation it turned out that they were generated by 
> function eigen, the eigenvalue computing function. The problem did not seem 
> to happen when the option "symmetric" was set to FALSE; at first I thought 
> that only matrices that were not completely symmetric (such as ones with 
> small errors of the magnitude 1e-17 or so) were affected but then we 
> encountered matrices that were perfectly symmetric but eigen still generated 
> NaNs when symmetric=TRUE.
>
> I wrote a trivial patch to mvrnorm to detect this problem and to recompute 
> the eigenvalues with symmetric=TRUE and EISPACK=TRUE. This seems to work. 
> symmetric=FALSE also seems to work, but is somewhat slower.
>
> If you use mvrnorm then you may want to try this. Details and the patch are 
> on my web page, along with one example of a problematic symmetric matrix (to 
> be loaded with the 'load' function). The NaNs should appear in the 
> eigenvectors. This particular matrix may not produce an error on all 
> platforms.
>
>     http://www.stat.columbia.edu/~kerman/Software/rprog.html
>
> Thanks to Radford Neal who suggested on our research web log (link below) 
> that setting EISPACK=TRUE may help.
>
>    http://www.stat.columbia.edu/~cook/movabletype/mlm/
>
>
> Jouni Kerman
> Department of Statistics
> Columbia University
> New York
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun May  1 09:25:14 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 May 2005 09:25:14 +0200
Subject: [R] legend(): how to put variable in subscript?
In-Reply-To: <200504302028.24736.naumov@buffalo.edu>
References: <200504302028.24736.naumov@buffalo.edu>
Message-ID: <x2u0lnflwl.fsf@turmalin.kubism.ku.dk>

Aleksey Naumov <naumov at buffalo.edu> writes:

> Dear List,
> 
> I would like to plot a simple legend with two math expressions, e.g.
> 
> plot(0)
> legend(1, 0.5, expression(sigma[i], sigma[j]))
> 
> The difficulty is that i and j should be variables rather than strings "i" and 
> "j". In other words I'd like to do something like:
> 
> i = "A"
> j = "B"
> legend(1, 0.5, expression(sigma[i], sigma[j]))
> 
> and have "A" and "B" as the actual subscripts. I can substitute the variable 
> in the expression e.g.:
> 
> legend(1, 0.5, substitute(sigma[i], list(i='A', j='B')))
> legend(1, 0.5, bquote(sigma[.(i)]))
> 
> however, this gives me just one of the two entries in the legend. I cannot 
> figure out how to include both sigmas in the legend.
> 
> What would be the best way to do something like this? Thank you for your ideas 
> or suggestions.

Ick. One of those cases that suggests that our current substitute
mechanisms don't quite cut it... However, try

 i <- "A"; j <- "B"
 e <- bquote(expression(sigma[.(i)],sigma[.(j)]))
 plot(0)
 legend(1,.5,eval(e))
 legend(1,-.5,e) # for comparison

Thing is, substitute(expression(....),...) returns a call to the
"expression" constructor, rather than the expression itself, so you
need the eval().


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Matthias.Kohl at uni-bayreuth.de  Sun May  1 10:45:31 2005
From: Matthias.Kohl at uni-bayreuth.de (Matthias Kohl)
Date: Sun, 01 May 2005 10:45:31 +0200
Subject: [R] User-defined random variable
In-Reply-To: <Pine.LNX.4.58.0504301724230.12788@thorin.ci.tuwien.ac.at>
References: <6ade6f6c05043008027c698dcd@mail.gmail.com>	<x27jikthsy.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.58.0504301724230.12788@thorin.ci.tuwien.ac.at>
Message-ID: <4274972B.9080000@uni-bayreuth.de>

Achim Zeileis wrote:

>On Sat, 30 Apr 2005, Peter Dalgaard wrote:
>
>  
>
>>Paul Smith <phhs80 at gmail.com> writes:
>>
>>    
>>
>>>Dear All
>>>
>>>I would like to know whether it is possible with R to define a
>>>discrete random variable different from the ones already defined
>>>inside R and generate random numbers from that user-defined
>>>distribution.
>>>      
>>>
>>Yes. One generic way is to specify the quantile function (as in
>>qpois() etc.) and do qfun(runif(N)).
>>    
>>
>
>If the support discrete but also finite, you can also use sample(), e.g.
>
>  sample(myset, N, replace = TRUE, prob = myprob)
>
>Z
>
>  
>
>>--
>>   O__  ---- Peter Dalgaard             Blegdamsvej 3
>>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>
Hi,

one can also use our R package "distr" to generate discrete random 
variables. The subsequent code provides a function which generates an 
object of class "DiscreteDistribution" based on a finite support "supp". 
If "prob" is missing all elements in "supp" are equally weighted.

Matthias


# generating function
DiscreteDistribution <- function(supp, prob){
    if(!is.numeric(supp))
        stop("'supp' is no numeric vector")
    if(any(!is.finite(supp)))   # admit +/- Inf?
        stop("inifinite or missing values in supp")
    len <- length(supp)
    if(missing(prob)){
        prob <- rep(1/len, len)
    }else{
        if(len != length(prob))
            stop("'supp' and 'prob' must have equal lengths")
        if(any(!is.finite(prob)))
            stop("inifinite or missing values in prob")
        if(!identical(all.equal(sum(prob), 1,
                            tolerance = 2*distr::TruncQuantile), TRUE))
            stop("sum of 'prob' has to be (approximately) 1")
        if(!all(prob >= 0))
            stop("'prob' contains values < 0")
    }
    if(length(usupp <- unique(supp)) < len){
        warning("collapsing to unique support values")
        prob <- as.vector(tapply(prob, supp, sum))
        supp <- sort(usupp)
        len <- length(supp)
    }else{
        o <- order(supp)
        supp <- supp[o]
        prob <- prob[o]
    }
   
    if(len > 1){
      if(min(supp[2:len] - supp[1:(len - 1)]) < distr::DistrResolution)
        stop("grid too narrow --> change DistrResolution")
    }
    rfun <- function(n){
        sample(x = supp, size = n, replace = TRUE, prob = prob)
    }
    intervall <- distr::DistrResolution / 2 
 
    supp.grid <- as.numeric(matrix(
                      rbind(supp - intervall, supp + intervall), nrow = 1))
    prob.grid <- c(as.numeric(matrix(rbind(0, prob), nrow = 1)), 0)
    dfun <- function(x){ stepfun(x = supp.grid, y = prob.grid)(x) }
 
    cumprob <- cumsum(prob)
    pfun <- function(x){ stepfun(x = supp, y = c(0, cumprob))(x) }

    qfun <- function(x){ supp[sum(cumprob<x)+1] }

    return(new("DiscreteDistribution", r = rfun, d = dfun, p = pfun,
        q = qfun, support = supp))
}

# some examples
supp <- rpois(20, lambda=7) # some support vector
D1 <- DiscreteDistribution(supp = supp) # prob is missing
r(D1)(10) # 10 random numbers of Distribution D1
support(D1) # support
d(D1)(support(D1)) # pdf
p(D1)(5) # cdf
q(D1)(0.5) # quantile (here: median)
plot(D1) # plot of pdf, cdf and quantile
D2 <- lgamma(D1) # apply member of group generic "Math"
plot(D2)
D3 <- D1 + Binom(size=10) # convolution with object of class 
"DiscreteDistribution"
plot(D3)
D4 <- D1 + Norm() # convolution with object of class "AbscontDistribution"
plot(D4)



From machud at intellektik.informatik.tu-darmstadt.de  Sun May  1 12:54:44 2005
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Sun, 1 May 2005 12:54:44 +0200 (CEST)
Subject: [R] Problems using html and dvips of Hmisc 
In-Reply-To: <200505011012.j41A6kU7017193@hypatia.math.ethz.ch>
References: <200505011012.j41A6kU7017193@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0505011250030.7825@kika.intellektik.informatik.tu-darmstadt.de>

Dear all,

I am experiencing the two following problems trying to produce a
comfortable output using the package Hmisc.

If I use:

html(latex(D,file="here.tex"),file="here.html")

where D is a data.frame the function html does not write in "here.html" but
only in a temporary file somewhere else in my PC (latex instead
writes correctly in the file "here.tex".

If I use

dvips(latex(D),file="here.ps")

everything runs smooth but if the
data frame is quite large I am not able to center it properly in
the output file .ps. The table always stays high on the page
going out from the upper side of the paper.

I tested this two unwanted behaviours both under linux and under MacOsx
and the packages are all up-to-date.


Thank you for the consideration.

Regards,

Marco



From jpk2005 at columbia.edu  Sun May  1 16:27:45 2005
From: jpk2005 at columbia.edu (Jouni Kerman)
Date: Sun, 1 May 2005 10:27:45 -0400
Subject: [R] eigen() may fail for some symmetric matrices,
	affects mvrnorm()
In-Reply-To: <200505011008.j41A6kTE017193@hypatia.math.ethz.ch>
References: <200505011008.j41A6kTE017193@hypatia.math.ethz.ch>
Message-ID: <2FB38F45-BA4D-11D9-AC20-000393C4E756@columbia.edu>


Hi all,

Professor Ripley is right. This problem does not affect newer versions 
of mvrnorm. We had an older version of mvrnorm which did not set 
EISPACK=TRUE as it should. Sorry for the confusion. Thank you for the 
correction.

Jouni Kerman

On May 1, 2005, at 6:08 AM, Prof Brian Ripley <ripley at stats.ox.ac.uk> 
wrote:
> mvrnorm() uses EISPACK=TRUE and always has done, explicitly since there
> was such an argument to eigen() (since 15 Jan 2003: the argument was
> introduced in R 1.7.0 in April 2003).  Just take a look at the code.



From s.o.nyangoma at rug.nl  Sun May  1 16:34:18 2005
From: s.o.nyangoma at rug.nl (Stephen Nyangoma)
Date: Sun, 1 May 2005 16:34:18 +0200
Subject: [R] International Association for Statistical Computing Conference
	(session on Diagnostics)
References: <200505011012.j41A6kUB017193@hypatia.math.ethz.ch>
Message-ID: <021b01c54e5b$4ce31c70$ee0c7d81@NIJENBOR13TN3U>



Dear Everyone,
I am organizing a session on Diagnostics (Regression etc.) for the  IASC on 
Computational Statistics and Data Analysis which will take place at the 
Amathus Beach Hotel in Limassol Cyprus, October 28-31, 2005. Those of you 
who would like to attend and present a paper may submit their 
abstracts/papers online at
    http://www.csdassn.org/europe/csda2005/
The papers will be considered for publication in the CSDA Journal.

Best regards.


Dr. Stephen Nyangoma
Groningen Bioinformatics Institute
The University of Groningen
The Netherlands.



From john.janmaat at acadiau.ca  Sun May  1 16:47:43 2005
From: john.janmaat at acadiau.ca (John Janmaat)
Date: Sun,  1 May 2005 11:47:43 -0300
Subject: [R] Roots of quadratic system.
Message-ID: <1114958863.4274ec0f3108f@webmail.acadiau.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050501/76406421/attachment.pl

From GVG at Stowers-Institute.org  Sun May  1 17:14:40 2005
From: GVG at Stowers-Institute.org (Glazko, Galina)
Date: Sun, 1 May 2005 10:14:40 -0500
Subject: [R] simulate zero-truncated Poisson distribution
Message-ID: <200505011514.j41FEh8o010269@hypatia.math.ethz.ch>


Dear All

I would like to know whether it is possible with R to 
generate random numbers from zero-truncated Poisson 
distribution.

Thanks in advance,
Galina



From tvqtb at ibest.com.br  Sun May  1 18:21:38 2005
From: tvqtb at ibest.com.br (Programas de email)
Date: Sun, 1 May 2005 13:21:38 -0300
Subject: [R] Programa de email e contatos
Message-ID: <20050501162130.92617134C5B@manwe.ibest.com.br>


Convidamos vocÅÍ a experimentar o Magic Seven,
o melhor programa de envio de emails para listas
de contatos. 

Com o Magic Seven vocÅÍ pode enviar emails personalizados
e com recursos multimÅÌdia, alÅÈm de ser o programa mais
rÅ·pido e confiÅ·vel do mercado.

O Magic Seven estÅ· no mercado hÅ· mais de 5 anos,
jÅ· estando na sua versÅ„o 5. Å… um produto em
constante evoluÅÁÅ„o, com suporte permanente ao cliente.

Alguns dos recursos do Magic Seven:

- OpÅÁÅ„o de envio de emails 100% personalizados. Ex: "Caro JosÅÈ",
utilizando atÅÈ 100 campos diferentes por email.
- Envio de emails com sons, mÅ˙sicas, animaÅÁÅıes e
qualquer recurso multimÅÌdia.
- OpÅÁÅ„o de envio de emails sem a utilizaÅÁÅ„o do servidor
  SMTP do seu provedor.
-Å†Envio de milhares de emails por hora.
- IdentificaÅÁÅ„o de emails inexistentes e incorretos.
- RelatÅÛrio(LOG) de envio dos emails.
- Os emails sÅ„o enviados um a um. Por isso sua lista de emails
fica protegida, nÅ„o aparecendo nas mensagens enviadas.
- OpÅÁÅ„o de attachments(arquivos) nas mensagens enviadas.
-Å†Leitura e deleÅÁÅ„o rÅ·pida de emails e retornos de erro.
-Å†Gerenciamento completo das listas de emails, com filtros
para emails duplicados e diversas outras opÅÁÅıes.
-Å†VisualizaÅÁÅ„o dos emails antes de enviar. Assim vocÅÍ pode
saber como as mensagens serÅ„o vistas pelos destinatÅ·ros.
- O envio das mensagens pode ser continuado do mesmo ponto,
se houver qualquer interrupÅÁÅ„o durante o envio.
- OpÅÁÅ„o de nÅ„o aparecer o nome de sua mÅ·quina(Local name)
e o identificador interno de sua mensagem(Message-ID) nos emails.
- Diversos outros recursos Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å†Å† 

--------------------------------------------------------
Para receber gratuitamente a versÅ„o demo do Magic Seven,
responda esse email com os dados a seguir:

Nome:
Email:
Telefone:
--------------------------------------------------------

Qualquer dÅ˙vida que vocÅÍ tenha, basta nos escrever.

Um grande abraÅÁo,

Equipe de divulgaÅÁÅ„o do Magic Seven



From Ted.Harding at nessie.mcc.ac.uk  Sun May  1 18:24:39 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 01 May 2005 17:24:39 +0100 (BST)
Subject: [R] simulate zero-truncated Poisson distribution
In-Reply-To: <200505011514.j41FEh8o010269@hypatia.math.ethz.ch>
Message-ID: <XFMail.050501172439.Ted.Harding@nessie.mcc.ac.uk>

On 01-May-05 Glazko, Galina wrote:
> 
> Dear All
> 
> I would like to know whether it is possible with R to 
> generate random numbers from zero-truncated Poisson 
> distribution.
> 
> Thanks in advance,
> Galina

Maybe someone has written an efficient function for this.

If not, you anyway have at least two options with basic R.

(A) "Rejection sampling": OK if the probability of 0 is small.
In that case, suppose you want n zero-truncated Poisson values.
Sample n values from rpois. Reject any which are 0, leaving
you with say r to find. Sample r from rpois, and continue
in the same way until you have all n.

This could be done by something on the following lines:

  n<-1000; T<-3.5
  Y<-rpois(n,T); Y0<-Y[Y>0]; r<-(n - length(Y0))
  while(r>0){
    Y<-rpois(r,T); Y0<-c(Y0,Y[Y>0]); r<-(n - length(Y0))
  }

and Y is then the required sample of n=1000 from a Poisson
distribution with mean T=3.5, after rejecting all zeros.

(B) This has a deeper theoretical base.

Suppose the mean of the original Poisson distribution (i.e.
before the 0's are cut out) is T (notation chosen for intuitive
convenience).

The number of events in a Poisson process of rate 1 in the
interval (0,T) has a Poisson distribution with mean T.
The time to the first event of a Poisson process of rate 1
has the negative exponential distribution with density exp(-t).

Conditional on the first event lying in (0,T), the time
to it has the conditional distribution with density

  exp(-t)/(1 - exp(-T)) (0 <= t <= T)

and the PDF (cumulative distribution) is

  F(t) = (1 - exp(-t))/(1 - exp(-T))

If t is randomly sampled from this distribution, then
U = F(t) has a uniform distribution on (0,1). So, if
you sample U from runif(1), and then

  t = -log(1 - U*(1 - exp(-T)))

you will have a random variable which is the time to
the first event, conditional on it being in (0,T).

Next, the number of Poisson-process events in (t,T),
conditional on t, simply has a Poisson distribution
with mean (T-t).

So sample from rpois(1,(T-t)), and add 1 (corresponding to
the "first" event whose time you sampled as above) to this
value.

The result is a single value from a zero-truncated Poisson
distribution with (pre-truncation) mean T.

Something like the following code will do the job vectorially:

  n<-1000       # desired size of sample
  T<-3.5        # pre-truncation mean of Poisson
  U<-runif(n)   # the uniform sample
  t = -log(1 - U*(1 - exp(-T))) # the "first" event-times
  T1<-(T - t)   # the set of (T-t)

  X <- rpois(n,T1)+1 # the final truncated Poisson sample

The expected value of your truncated distribution is of course
related to the mean of the pre-truncated Poisson by

  E(X) = T/(1 - exp(-T))

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 01-May-05                                       Time: 17:20:09
------------------------------ XFMail ------------------------------



From gottfried.gruber at terminal.at  Sun May  1 19:21:44 2005
From: gottfried.gruber at terminal.at (Gottfried Gruber)
Date: Sun, 1 May 2005 19:21:44 +0200
Subject: [R] opimization problem
Message-ID: <200505011921.44723.gottfried.gruber@terminal.at>

hi,

i want to execute the following opimization problem:
max r*w
s.t.:   w*z=1	# sum of w is 1
r, w are [nx1] vectors, z is a [nx1] vector consisting of 1
so far so good, works fine with lp

the problem arises with the additional restriction
w' * V * w
where V is a [nxn] matrix
how can i include this restriction since w arises twice?

thanks,
gg

-- 
---------------------------------------------------
Gottfried Gruber
mailto:gottfried.gruber at terminal.at
www: http://gogo.sehrsupa.net



From p.dalgaard at biostat.ku.dk  Sun May  1 21:35:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 May 2005 21:35:22 +0200
Subject: [R] simulate zero-truncated Poisson distribution
In-Reply-To: <XFMail.050501172439.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050501172439.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2y8ay3fk5.fsf@turmalin.kubism.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 01-May-05 Glazko, Galina wrote:
> > 
> > Dear All
> > 
> > I would like to know whether it is possible with R to 
> > generate random numbers from zero-truncated Poisson 
> > distribution.
> > 
> > Thanks in advance,
> > Galina
.....
> (B) This has a deeper theoretical base.
> 
> Suppose the mean of the original Poisson distribution (i.e.
> before the 0's are cut out) is T (notation chosen for intuitive
> convenience).
> 
> The number of events in a Poisson process of rate 1 in the
> interval (0,T) has a Poisson distribution with mean T.
> The time to the first event of a Poisson process of rate 1
> has the negative exponential distribution with density exp(-t).
> 
> Conditional on the first event lying in (0,T), the time
> to it has the conditional distribution with density
> 
>   exp(-t)/(1 - exp(-T)) (0 <= t <= T)
> 
> and the PDF (cumulative distribution) is
> 
>   F(t) = (1 - exp(-t))/(1 - exp(-T))
> 
> If t is randomly sampled from this distribution, then
> U = F(t) has a uniform distribution on (0,1). So, if
> you sample U from runif(1), and then
> 
>   t = -log(1 - U*(1 - exp(-T)))
> 
> you will have a random variable which is the time to
> the first event, conditional on it being in (0,T).
> 
> Next, the number of Poisson-process events in (t,T),
> conditional on t, simply has a Poisson distribution
> with mean (T-t).
> 
> So sample from rpois(1,(T-t)), and add 1 (corresponding to
> the "first" event whose time you sampled as above) to this
> value.
> 
> The result is a single value from a zero-truncated Poisson
> distribution with (pre-truncation) mean T.
> 
> Something like the following code will do the job vectorially:
> 
>   n<-1000       # desired size of sample
>   T<-3.5        # pre-truncation mean of Poisson
>   U<-runif(n)   # the uniform sample
>   t = -log(1 - U*(1 - exp(-T))) # the "first" event-times
>   T1<-(T - t)   # the set of (T-t)
> 
>   X <- rpois(n,T1)+1 # the final truncated Poisson sample
> 
> The expected value of your truncated distribution is of course
> related to the mean of the pre-truncated Poisson by
> 
>   E(X) = T/(1 - exp(-T))

There must be an easier way... Anything wrong with

 rtpois <- function(N, lambda)
     qpois(runif(N, dpois(0, lambda), 1), lambda)

 rtpois(100,5)

?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jrclmilks at joimail.com  Sun May  1 21:42:41 2005
From: jrclmilks at joimail.com (Jim Milks)
Date: Sun, 1 May 2005 15:42:41 -0400
Subject: [R] Partial F-test comparing full and reduced regression models
Message-ID: <b95217ffe90b7e2bcfa0be1d06834ed7@joimail.com>

Dear all:

I have a regression model that has collinearity problems (between three 
regressor variables).  I need a F-test that will allow me to compare 
between full (with all variables) and partial models (minus 1=< 
variables).  The general F-test formula I'm using is:

F = {[SS(full model) - SS(reduced model)] / (#variables taken out)} / 
MSS(full model)

Unfortunately, the ANOVA table parses the SS and MSS between the 
variables and does not give the statistics for the regression model as 
a whole, otherwise I'd do this by hand.

So, really, I have two questions: 1) Can I just add up all the SS and 
MSS for all the variables to get the model SS and MSS and 2)  Are there 
any functions or packages I can use to calculate the F-statistic?

Thank you for any help you can provide.

Sincerely,
Jim Milks

Graduate Student
Environmental Sciences Ph.D. Program
Wright State University
3640 Colonel Glenn Hwy
Dayton, OH 45435



From p.dalgaard at biostat.ku.dk  Sun May  1 21:52:32 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 May 2005 21:52:32 +0200
Subject: [R] Partial F-test comparing full and reduced regression models
In-Reply-To: <b95217ffe90b7e2bcfa0be1d06834ed7@joimail.com>
References: <b95217ffe90b7e2bcfa0be1d06834ed7@joimail.com>
Message-ID: <x2u0lm3erj.fsf@turmalin.kubism.ku.dk>

Jim Milks <jrclmilks at joimail.com> writes:

> Dear all:
> 
> I have a regression model that has collinearity problems (between
> three regressor variables).  I need a F-test that will allow me to
> compare between full (with all variables) and partial models (minus
> 1=< variables).  The general F-test formula I'm using is:
> 
> F = {[SS(full model) - SS(reduced model)] / (#variables taken out)} /
> MSS(full model)
> 
> Unfortunately, the ANOVA table parses the SS and MSS between the
> variables and does not give the statistics for the regression model as
> a whole, otherwise I'd do this by hand.
> 
> So, really, I have two questions: 1) Can I just add up all the SS and
> MSS for all the variables to get the model SS and MSS and 2)  Are
> there any functions or packages I can use to calculate the F-statistic?

Just use anova(model1, model2).

(One potential catch: Make sure that both models are fitted to the
same data set. Missing values in predictors may interfere.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Sun May  1 22:39:20 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 01 May 2005 13:39:20 -0700
Subject: [R] simulate zero-truncated Poisson distribution
In-Reply-To: <x2y8ay3fk5.fsf@turmalin.kubism.ku.dk>
References: <XFMail.050501172439.Ted.Harding@nessie.mcc.ac.uk>
	<x2y8ay3fk5.fsf@turmalin.kubism.ku.dk>
Message-ID: <42753E78.7040308@pdf.com>

Hi, Peter:

Your solution is simple, elegant, very general yet sufficiently subtle 
that I (and I suspect many others) might not have thought of it until 
you mentioned it.  It is worth remembering.

Thanks.  spencer graves

Peter Dalgaard wrote:

> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
> 
> 
>>On 01-May-05 Glazko, Galina wrote:
>>
>>>Dear All
>>>
>>>I would like to know whether it is possible with R to 
>>>generate random numbers from zero-truncated Poisson 
>>>distribution.
>>>
>>>Thanks in advance,
>>>Galina
> 
> .....
> 
>>(B) This has a deeper theoretical base.
>>
>>Suppose the mean of the original Poisson distribution (i.e.
>>before the 0's are cut out) is T (notation chosen for intuitive
>>convenience).
>>
>>The number of events in a Poisson process of rate 1 in the
>>interval (0,T) has a Poisson distribution with mean T.
>>The time to the first event of a Poisson process of rate 1
>>has the negative exponential distribution with density exp(-t).
>>
>>Conditional on the first event lying in (0,T), the time
>>to it has the conditional distribution with density
>>
>>  exp(-t)/(1 - exp(-T)) (0 <= t <= T)
>>
>>and the PDF (cumulative distribution) is
>>
>>  F(t) = (1 - exp(-t))/(1 - exp(-T))
>>
>>If t is randomly sampled from this distribution, then
>>U = F(t) has a uniform distribution on (0,1). So, if
>>you sample U from runif(1), and then
>>
>>  t = -log(1 - U*(1 - exp(-T)))
>>
>>you will have a random variable which is the time to
>>the first event, conditional on it being in (0,T).
>>
>>Next, the number of Poisson-process events in (t,T),
>>conditional on t, simply has a Poisson distribution
>>with mean (T-t).
>>
>>So sample from rpois(1,(T-t)), and add 1 (corresponding to
>>the "first" event whose time you sampled as above) to this
>>value.
>>
>>The result is a single value from a zero-truncated Poisson
>>distribution with (pre-truncation) mean T.
>>
>>Something like the following code will do the job vectorially:
>>
>>  n<-1000       # desired size of sample
>>  T<-3.5        # pre-truncation mean of Poisson
>>  U<-runif(n)   # the uniform sample
>>  t = -log(1 - U*(1 - exp(-T))) # the "first" event-times
>>  T1<-(T - t)   # the set of (T-t)
>>
>>  X <- rpois(n,T1)+1 # the final truncated Poisson sample
>>
>>The expected value of your truncated distribution is of course
>>related to the mean of the pre-truncated Poisson by
>>
>>  E(X) = T/(1 - exp(-T))
> 
> 
> There must be an easier way... Anything wrong with
> 
>  rtpois <- function(N, lambda)
>      qpois(runif(N, dpois(0, lambda), 1), lambda)
> 
>  rtpois(100,5)
> 
> ?
>



From Ted.Harding at nessie.mcc.ac.uk  Sun May  1 22:33:23 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 01 May 2005 21:33:23 +0100 (BST)
Subject: [R] simulate zero-truncated Poisson distribution
In-Reply-To: <x2y8ay3fk5.fsf@turmalin.kubism.ku.dk>
Message-ID: <XFMail.050501213323.Ted.Harding@nessie.mcc.ac.uk>

On 01-May-05 Peter Dalgaard wrote:
> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
>> Something like the following code will do the job vectorially:
>> 
>>   n<-1000       # desired size of sample
>>   T<-3.5        # pre-truncation mean of Poisson
>>   U<-runif(n)   # the uniform sample
>>   t = -log(1 - U*(1 - exp(-T))) # the "first" event-times
>>   T1<-(T - t)   # the set of (T-t)
>> 
>>   X <- rpois(n,T1)+1 # the final truncated Poisson sample
>> 
>> The expected value of your truncated distribution is of course
>> related to the mean of the pre-truncated Poisson by
>> 
>>   E(X) = T/(1 - exp(-T))
> 
> There must be an easier way... Anything wrong with
> 
>  rtpois <- function(N, lambda)
>      qpois(runif(N, dpois(0, lambda), 1), lambda)
> 
>  rtpois(100,5)
> 
> ?

Well, that's neat! (When I saw that, I had to switch in
backup-brain to think clearly about qpois, but there you are ... )

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 01-May-05                                       Time: 21:20:43
------------------------------ XFMail ------------------------------



From kjetil at acelerate.com  Sun May  1 22:51:10 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 01 May 2005 16:51:10 -0400
Subject: [R] Roots of quadratic system.
In-Reply-To: <1114958863.4274ec0f3108f@webmail.acadiau.ca>
References: <1114958863.4274ec0f3108f@webmail.acadiau.ca>
Message-ID: <4275413E.4070804@acelerate.com>

John Janmaat wrote:

>Hello,
>
>I have a system of quadratic equations (results of a Hamiltonian optimization)
>which I need to find the roots for.  Is there a package and/or function which
>will find the roots for a quadratic system? 
>
Certainly you cxould use solve, see
?solve
Alternatively you could go for a computer algebra system with an 
implemantation
of groebner basis, and use an symbolic method.

Kjetil

> Note that I am not opimizing, but
>rather solving the first order conditions which come from a Hamiltonian.  I am
>basically looking for something in R that will do the same thing as fsolve in
>Matlab.
>
>Thanks,
>
>John.
>
>==============================================
>Dr. John Janmaat
>Department of Economics
>Acadia University
>Tel: 902-585-1461
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From Bill.Venables at csiro.au  Mon May  2 02:20:11 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Mon, 2 May 2005 10:20:11 +1000
Subject: [R] Roots of quadratic system.
Message-ID: <B998A44C8986644EA8029CFE6396A9241B32F6@exqld2-bne.qld.csiro.au>

Are you looking for a unique solution or families of solutions?

Can't you turn a root-finding problem for a system of equations 
with a unique solution into an optimisation problem, anyway?

E.g.  You want to solve

f1(x) = g1
f2(x) = g2
...

Why not optimise L(x) = (f1(x) - g1)^2 + (f2(x) - g2)^2 + ... 
with respect to x?  If the minimum value is zero, then you are
done; if it is greater than zero your original system does not
have a solution.

If you are in the complex domain the changes needed are obvious.

V.

: -----Original Message-----
: From: r-help-bounces at stat.math.ethz.ch 
: [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Janmaat
: Sent: Monday, 2 May 2005 12:48 AM
: To: r-help at stat.math.ethz.ch
: Subject: [R] Roots of quadratic system.
: 
: 
: Hello,
: 
: I have a system of quadratic equations (results of a 
: Hamiltonian optimization)
: which I need to find the roots for.  Is there a package 
: and/or function which
: will find the roots for a quadratic system?  Note that I am 
: not opimizing, but
: rather solving the first order conditions which come from a 
: Hamiltonian.  I am
: basically looking for something in R that will do the same 
: thing as fsolve in
: Matlab.
: 
: Thanks,
: 
: John.
: 
: ==============================================
: Dr. John Janmaat
: Department of Economics
: Acadia University
: Tel: 902-585-1461
: 
: ______________________________________________
: R-help at stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! 
: http://www.R-project.org/posting-guide.html
:



From sdavis2 at mail.nih.gov  Mon May  2 02:44:01 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sun, 1 May 2005 20:44:01 -0400
Subject: [R] how to replace text...
References: <e206273d0504290311460cd671@mail.gmail.com>
	<604c2cd37be8d7392f3c65830bcb7c9e@mail.nih.gov>
	<e206273d05050111235e4d718c@mail.gmail.com>
Message-ID: <000801c54eb0$09ffd920$5179f345@WATSON>

Jonathan,

Just for future reference, you can send replies to the list--everyone 
benefits then.

I'm not sure how pricelts works, but you can type ?pricelts to see.  It is 
possible that pricelts takes a vector of stock names.  You need to look at 
the "Value" part of the help to see what pricelts returns.  Depending on how 
pricelts works, much of the work may be done for you.

However, if you have a vector of stock names like:

stocks <- c('IBM','QQQ','RRR')

You could do something like:

stocklist <- list()
sapply(stocks,pricelts,quote="Close")

Then, stocklist will be a list that can be accessed like:

plot(stocklist[["IBM"]])

Or, to plot all of them, you could do:

par(ask=T)  #this will ask you to hit return before making a plot
lapply(stocklist,plot)  #this will go through each stock in stocklist one at 
a time, plotting each one, asking you to press return between each plot

Remember, anything command above you don't understand (actually, every 
command at LEAST once) deserves a FULL read of the documentation.  If it 
doesn't make sense, that is OK, but then read it again after a bit.  Also, 
remember that almost every command has an example (or several examples) that 
you can see if you type:

example(command)

Sean

----- Original Message ----- 
From: "Jonathan Q." <jqm475 at gmail.com>
To: "Sean Davis" <sdavis2 at mail.nih.gov>
Sent: Sunday, May 01, 2005 2:23 PM
Subject: Re: [R] how to replace text...


sorry for so long in getting back. It does work (only error was
priceIts vs priceits), however the faq 7.21 deals with "7.21 How can I
turn a string into a variable?" - not sure what this has to do with
lists.

While I have more reading to do, my goal is to do what you mentioned
but instead of having to retype

pricelist[['STOCK']] <- pricelts('STOCK',quote="Close")

for each stock, if I could somehow have it reference a file with all
the tickers and loop through them.  I am sure as I get through all the
R material I will figure this out, but if you know in advance that
would be great.  Regards/Jonathan


On 4/29/05, Sean Davis <sdavis2 at mail.nih.gov> wrote:
>
> On Apr 29, 2005, at 6:11 AM, Jonathan Q. wrote:
>
> > if I have....
> >
> > QQQQ<-priceIts("QQQQ",quote="Close")
> > QQQQ<-priceIts("QQQQ",quote="Close");plot(QQQQ)
> >
> > and then i want to do the same thing but say with IBM instead of QQQQ
> > is there an easy way like replace qqqq/ibm
> >
>
> You should probably use a list instead.  This is a FAQ (see 7.21):
>
> http://cran.r-project.org/doc/FAQ/R-FAQ.html
>
> Here is about how you might do it (untested):
>
> pricelist <- list()
>
> pricelist[['IBM']] <- pricelts('IBM',quote="Close")
> pricelist[['QQQQ']] <- pricelts('QQQQ',quote="Close")
>
> par(ask=T)
> lapply(pricelist,plot)
>
> Does this do what you want?
>
> Sean
>
>


-- 
Jonathan
jqm475 at gmail.com



From itsme_410 at yahoo.com  Mon May  2 05:10:55 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Sun, 1 May 2005 20:10:55 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
Message-ID: <20050502031055.9314.qmail@web54510.mail.yahoo.com>

Hi,

It is my understanding that the eigenvectors of a circulant matrix are given as
follows:

1,omega,omega^2,....,omega^{p-1}

where the matrix has dimension given by p x p and omega is one of p complex
roots of unity. (See Bellman for an excellent discussion on this). 

The matrix created  by the attached row and obtained using the following
commands
indicates no imaginary parts for the eigenvectors. It appears that the real
values are close, but not exactly so, and there is no imaginary part
whatsoever. 

x<-scan("kinv.dat")       #length(x) = 216
y<-x[c(109:216,1:108)]
X<-toeplitz(y)
eigen(X)$vectors

Note that the eigenvectors are correct, and they are indeed real, because X is
symmetric.

Is this a bug in R? Any insight if not, please!

Many thanks and best wishes!


This is unrelated, but can the R-help archive maintainers please not put e-mail
addresses in the archive? This would really help people like me who would like
to post using their professional e-mail addresses. Just stripping the e-mail
address from everything else would be great, or make it non-spammable by adding
some random number or something which would be obvious to anyone reading it
without the help of a machine. After all, why give spider programs more fodder?

Best wishes!

__________________________________________________




__________________________________________________




__________________________________________________



From mingan at unm.edu  Mon May  2 07:20:18 2005
From: mingan at unm.edu (mingan)
Date: Sun, 01 May 2005 23:20:18 -0600
Subject: [R] help : Bisect( )
Message-ID: <4275B892.8060607@unm.edu>


 I  am using the package of  Icens
 but for the function  Bisect

  I do not know  what  ndir is

  it just says the direction of pvec, but what kind of values should I 
use ?

 thanks for reply

  pls give me an example..

 


Bisect(tA, pvec, ndir, Meps, tolbis=1e-07)

Arguments:

      tA: The transpose of the clique matrix.

    pvec: The current estimate of the probability vector.

    ndir: The direction to explore.

    Meps: Machine epsilon, elements of 'pvec' that are less than this
          are assumed to be zero.

  tolbis: The tolerance used to determine if the algorithm has
          converged.



From ripley at stats.ox.ac.uk  Mon May  2 08:05:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 May 2005 07:05:30 +0100 (BST)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <20050502031055.9314.qmail@web54510.mail.yahoo.com>
References: <20050502031055.9314.qmail@web54510.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0505020644500.2667@gannet.stats>

On Sun, 1 May 2005, someone who didn't give his name wrote:

> It is my understanding that the eigenvectors of a circulant matrix are 
> given as follows:
>
> 1,omega,omega^2,....,omega^{p-1}
>
> where the matrix has dimension given by p x p and omega is one of p complex
> roots of unity. (See Bellman for an excellent discussion on this).

What is the relevance of this?  Also, your reference is useless to us, 
which is important as this all hinges on your definitions.

> The matrix created by the attached row and obtained using the following 
> commands indicates no imaginary parts for the eigenvectors. It appears 
> that the real values are close, but not exactly so, and there is no 
> imaginary part whatsoever.
>
> x<-scan("kinv.dat")       #length(x) = 216
> y<-x[c(109:216,1:108)]
> X<-toeplitz(y)
> eigen(X)$vectors

We don't have "kinv.dat", but X is not circulant as usually defined.

> Note that the eigenvectors are correct, and they are indeed real, 
> because X is symmetric.
>
> Is this a bug in R? Any insight if not, please!

Well, first R calls LAPACK or EISPACK, so it would be a bug in one of 
those.  But in so far as I understand you, X is a real symmetric matrix, 
and those have real eigenvalues and eigenvectors.

I think you are confused about the meaning of Toeplitz and circulant.
Compare

http://mathworld.wolfram.com/CirculantMatrix.html
http://mathworld.wolfram.com/ToeplitzMatrix.html

and note that ?toeplitz says it computes the *symmetric* Toeplitz matrix.

There is a very regretable tendency here for people to assume their 
lack of understanding is `a bug in R'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ahenningsen at email.uni-kiel.de  Mon May  2 09:55:20 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 2 May 2005 09:55:20 +0200
Subject: [R] opimization problem
In-Reply-To: <200505011921.44723.gottfried.gruber@terminal.at>
References: <200505011921.44723.gottfried.gruber@terminal.at>
Message-ID: <200505020955.20279.ahenningsen@email.uni-kiel.de>

Hi Gottfried,

w' * V * w is not a restriction, because there is no equal sign. 
Do you mean w' * V * w = 1?

Arne
 
On Sunday 01 May 2005 19:21, Gottfried Gruber wrote:
> hi,
>
> i want to execute the following opimization problem:
> max r*w
> s.t.:   w*z=1	# sum of w is 1
> r, w are [nx1] vectors, z is a [nx1] vector consisting of 1
> so far so good, works fine with lp
>
> the problem arises with the additional restriction
> w' * V * w
> where V is a [nxn] matrix
> how can i include this restriction since w arises twice?
>
> thanks,
> gg

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From Jan.Verbesselt at biw.kuleuven.be  Mon May  2 10:06:55 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Mon, 2 May 2005 10:06:55 +0200
Subject: [R] Restricted cubic spline function ERROR?: glm(Y~rcs(x,5)) 
Message-ID: <001701c54eed$e7dd68b0$1145210a@agr.ad10.intern.kuleuven.ac.be>

Dear all,

Is the restricted cubic spline function working properly in the glm model?

We used glm(y~rcs(x,5), family=binomial) but it seems that for some
theoretical reasons the rcs, restricted cubic spline function can not be
fitted by a glm function. Is this correct?


Regards,
Jan

((Originally, we used lrm(y~ rcs(x,5)) but we couldn't find how to derive
the AIC value of the fitted model. Is there a solution?))



From ripley at stats.ox.ac.uk  Mon May  2 11:30:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 May 2005 10:30:08 +0100 (BST)
Subject: [R] Restricted cubic spline function ERROR?: glm(Y~rcs(x,5)) 
In-Reply-To: <001701c54eed$e7dd68b0$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <001701c54eed$e7dd68b0$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <Pine.LNX.4.61.0505021014290.4872@gannet.stats>

R itself does not have a function rcs(), and help.search("rcs") does not 
find one.  My guess is that you are using package Design and failing to 
tell us.

[Aside here: this is not the first time I have seen help.search() results 
that give no indication why the result was found.  To wit

Design.trans(Design)    Design Special Transformation Functions
rcspline.eval(Hmisc)    Restricted Cubic Spline Design Matrix
rcspline.plot(Hmisc)    Plot Restricted Cubic Spline Function
rcspline.restate(Hmisc)
                         Re-state Restricted Cubic Spline Function
GtkVisibility(RGtk)     Automatically generated S functions for
                         bindings to the RGtk library
mysqlInitDriver(RMySQL)
                         Support Functions
sqliteInitDriver(RSQLite)
                         Support Functions
prob.frcs.dat(verification)
                         Probablisitic Forecast Dataset.

is not much help and suggested to me to look at the second to fourth 
entries.]

If you read the help page ?Design.trans you will see that rcs is part of 
the Design system built on top of S/R.  What makes you think it is 
intended to work with glm()?


On Mon, 2 May 2005, Jan Verbesselt wrote:

> Is the restricted cubic spline function working properly in the glm model?
>
> We used glm(y~rcs(x,5), family=binomial) but it seems that for some
> theoretical reasons the rcs, restricted cubic spline function can not be
> fitted by a glm function. Is this correct?
>
> ((Originally, we used lrm(y~ rcs(x,5)) but we couldn't find how to derive
> the AIC value of the fitted model. Is there a solution?))

Yes.  Make use of your theory to write an AIC() method.

Note that

lrm> fit <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,
     4)), x = TRUE, y = TRUE)
> class(fit)
[1] "lrm"    "Design" "glm"

is apparently incorrectly asserting inheritance and so the current logLik 
and hence AIC methods selected do not work.  (An "lrm" object is missing 
the "family" component that is documented for a "glm" object, and in any 
case I believe lrm also fits non-GLM models.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From alxmilton at yahoo.it  Mon May  2 11:59:00 2005
From: alxmilton at yahoo.it (alessandro carletti)
Date: Mon, 2 May 2005 11:59:00 +0200 (CEST)
Subject: [R] Multiple regression
Message-ID: <20050502095900.72493.qmail@web26609.mail.ukl.yahoo.com>

Hi,
what package could I install to best perform a
Multiple Linear Regression (and what for PCA)?
Thanks

Alessandro Carletti



From ripley at stats.ox.ac.uk  Mon May  2 13:00:41 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 May 2005 12:00:41 +0100 (BST)
Subject: [R] Multiple regression
In-Reply-To: <20050502095900.72493.qmail@web26609.mail.ukl.yahoo.com>
References: <20050502095900.72493.qmail@web26609.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0505021158050.5987@gannet.stats>

None: basic R comes with a package `stats' that does both (very well!).
It is loaded by default.

You do need to read `An Introduction to R', as the posting guide asks.

On Mon, 2 May 2005, alessandro carletti wrote:

> what package could I install to best perform a
> Multiple Linear Regression (and what for PCA)?

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Uwe.Ligges at R-project.org  Mon May  2 13:03:26 2005
From: Uwe.Ligges at R-project.org (Uwe Ligges)
Date: Mon, 02 May 2005 13:03:26 +0200
Subject: [R] [R-pkgs] RWinEdt_1.7-1 for R>=2.1.0
Message-ID: <427608FE.3080201@R-project.org>

Dear RWinEdt users,

unfortunately, it seems to be impossible to support localized (i.e. 
translated) versions of RGui in MDI mode. The support of this feature 
has been removed now. All the workarounds I tried caused bugs (as many 
of you told me re. RWinEdt_1.7-0). Thanks for all the bug reports!

It is recommended to use the new version of RWinEdt (1.7-1) together 
with RGui in SDI mode (which should work with arbitrary translations).
Further on, only english versions of RGui in MDI mode are supported by 
RWinEdt.

Uwe Ligges

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From chrysopa at gmail.com  Mon May  2 13:28:13 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Mon, 2 May 2005 08:28:13 -0300
Subject: [R] Keep R-mirror updated
Message-ID: <200505020828.13577.chrysopa@gmail.com>

Hi,

I have a R mirror on www.termix.ufv.br/CRAN

I try to keep it updated using rsync and cron.

rsync -rzuav --progress --delete 
cran.r-project.org::CRAN /var/www/termix/mirrors/linux/CRAN/ &

But all times the rsync abort with error:

95.99kB/s    0:00:02      548960  77%  110.50kB/s    0:00:01rsync: read error: 
Connection reset by peer (104)
rsync error: error in rsync protocol data stream (code 12) at io.c(515)
rsync: connection unexpectedly closed (391811 bytes received so far) 
[generator]
rsync error: error in rsync protocol data stream (code 12) at io.c(359)

And it dont made my mirror updated.

Exist any method more efficient to made it updated?

Thanks
Ronaldo
-- 
O cachorro abana o rabo porque o rabo, enquanto segmento discriminado do
corpo da sociedade, ainda n??o conquistou o direito de abanar o cachorro 
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From john.janmaat at acadiau.ca  Mon May  2 00:17:45 2005
From: john.janmaat at acadiau.ca (John Janmaat)
Date: Sun, 01 May 2005 19:17:45 -0300
Subject: [R] Roots of quadratic system.
In-Reply-To: <4275413E.4070804@acelerate.com>
References: <1114958863.4274ec0f3108f@webmail.acadiau.ca>
	<4275413E.4070804@acelerate.com>
Message-ID: <42755589.6050507@acadiau.ca>

Kjetil Brinchmann Halvorsen wrote:
> John Janmaat wrote:
> 
>> Hello,
>>
>> I have a system of quadratic equations (results of a Hamiltonian 
>> optimization)
>> which I need to find the roots for.  Is there a package and/or 
>> function which
>> will find the roots for a quadratic system?
> 
> Certainly you cxould use solve, see
> ?solve
> Alternatively you could go for a computer algebra system with an 
> implemantation
> of groebner basis, and use an symbolic method.

I have looked into using solve.  However, solve works on a system of 
linear equations (at least that is how I read it).  I have a system of 
quadratic equations.  They can be written to equate to zero, so that a 
non-linear system solver should do the trick.

John.

> Kjetil
> 
>> Note that I am not opimizing, but
>> rather solving the first order conditions which come from a 
>> Hamiltonian.  I am
>> basically looking for something in R that will do the same thing as 
>> fsolve in
>> Matlab.
>>
>> Thanks,
>>
>> John.
>>
>> ==============================================
>> Dr. John Janmaat
>> Department of Economics
>> Acadia University
>> Tel: 902-585-1461
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>>  
>>
> 
> 


-- 
==========================================
Dr. John Janmaat
Department of Economics
Acadia University
Wolfville, Nova Scotia, Canada.



From john.janmaat at acadiau.ca  Mon May  2 02:51:56 2005
From: john.janmaat at acadiau.ca (John Janmaat)
Date: Sun, 01 May 2005 21:51:56 -0300
Subject: [R] Roots of quadratic system.
In-Reply-To: <B998A44C8986644EA8029CFE6396A9241B32F6@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A9241B32F6@exqld2-bne.qld.csiro.au>
Message-ID: <427579AC.20000@acadiau.ca>

Hello Bill,

I have used the optimization approach you suggest in past.  I was hoping 
that someone had written something specifically for solving a system of 
nonlinear equations, as the fsolve function does in MatLab.  The Octave 
version is somewhat limited compared to the MatLab version, and I like 
working in R.

Thanks,

John.

ps: I would like the system to have a unique solution, but there is 
nothing about the system that precludes multiple equilibria.  Of course, 
the L(x) = ... approach can search for multiple equilibria if I try 
enough different starting points.

Bill.Venables at csiro.au wrote:
> Are you looking for a unique solution or families of solutions?
> 
> Can't you turn a root-finding problem for a system of equations 
> with a unique solution into an optimisation problem, anyway?
> 
> E.g.  You want to solve
> 
> f1(x) = g1
> f2(x) = g2
> ...
> 
> Why not optimise L(x) = (f1(x) - g1)^2 + (f2(x) - g2)^2 + ... 
> with respect to x?  If the minimum value is zero, then you are
> done; if it is greater than zero your original system does not
> have a solution.
> 
> If you are in the complex domain the changes needed are obvious.
> 
> V.
> 
> : -----Original Message-----
> : From: r-help-bounces at stat.math.ethz.ch 
> : [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Janmaat
> : Sent: Monday, 2 May 2005 12:48 AM
> : To: r-help at stat.math.ethz.ch
> : Subject: [R] Roots of quadratic system.
> : 
> : 
> : Hello,
> : 
> : I have a system of quadratic equations (results of a 
> : Hamiltonian optimization)
> : which I need to find the roots for.  Is there a package 
> : and/or function which
> : will find the roots for a quadratic system?  Note that I am 
> : not opimizing, but
> : rather solving the first order conditions which come from a 
> : Hamiltonian.  I am
> : basically looking for something in R that will do the same 
> : thing as fsolve in
> : Matlab.
> : 
> : Thanks,
> : 
> : John.
> : 
> : ==============================================
> : Dr. John Janmaat
> : Department of Economics
> : Acadia University
> : Tel: 902-585-1461
> : 
> : ______________________________________________
> : R-help at stat.math.ethz.ch mailing list
> : https://stat.ethz.ch/mailman/listinfo/r-help
> : PLEASE do read the posting guide! 
> : http://www.R-project.org/posting-guide.html
> : 
> 


-- 
==========================================
Dr. John Janmaat
Department of Economics
Acadia University
Wolfville, Nova Scotia, Canada.



From jan.sabee at gmail.com  Mon May  2 14:23:12 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Mon, 2 May 2005 14:23:12 +0200
Subject: [R] Take each one cell
Message-ID: <96507a8e05050205231e3ffce5@mail.gmail.com>

Are there any way to take
> x <- c("0", "large", "medium", "small")
>   x
[1] "0"      "large"  "medium" "small" 
> 

like
  
  x="0"
  x="large"
  x="medium"
  x="small"


Best regards
Jan Sabee



From andorxor at gmx.de  Mon May  2 14:28:40 2005
From: andorxor at gmx.de (Stephan Tolksdorf)
Date: Mon, 02 May 2005 13:28:40 +0100
Subject: [R] Multivariate kernel density estimation
Message-ID: <42761CF8.6060908@gmx.de>

Hi,

I need to estimate the density at the mean of a sample of a few 
thousands data points with a dimesion up to 5. The data is uni-modal and 
  regularly shaped.

I couldn't find any kernel density package for R which supports more
than 3 dimensions. Have I overlooked a package or does somebody have 
code for this purpose? Any other advice?

Regards,
   Stephan



From sdavis2 at mail.nih.gov  Mon May  2 14:39:10 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 2 May 2005 08:39:10 -0400
Subject: [R] Take each one cell
References: <96507a8e05050205231e3ffce5@mail.gmail.com>
Message-ID: <004d01c54f13$f0a14180$5179f345@WATSON>

see ?sapply and ?for

Sean

----- Original Message ----- 
From: "Jan Sabee" <jan.sabee at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, May 02, 2005 8:23 AM
Subject: [R] Take each one cell


> Are there any way to take
>> x <- c("0", "large", "medium", "small")
>>   x
> [1] "0"      "large"  "medium" "small"
>>
>
> like
>
>  x="0"
>  x="large"
>  x="medium"
>  x="small"
>
>
> Best regards
> Jan Sabee
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From itsme_410 at yahoo.com  Mon May  2 14:57:56 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Mon, 2 May 2005 05:57:56 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: 6667
Message-ID: <20050502125756.97473.qmail@web54502.mail.yahoo.com>

Dear Professor Ripley:

Lets do this professionally, shall we?

--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Sun, 1 May 2005, someone who didn't give his name wrote:
> 
> > It is my understanding that the eigenvectors of a circulant matrix are 
> > given as follows:
> >
> > 1,omega,omega^2,....,omega^{p-1}
> >
> > where the matrix has dimension given by p x p and omega is one of p complex
> > roots of unity. (See Bellman for an excellent discussion on this).
> 
> What is the relevance of this?  Also, your reference is useless to us, 
> which is important as this all hinges on your definitions.

Bellman is an excellent book on the topic and that is what I was alluding to,
that you can calculate the eigendecomposition by hand without any costly
computations, actually.

> 
> > The matrix created by the attached row and obtained using the following 
> > commands indicates no imaginary parts for the eigenvectors. It appears 
> > that the real values are close, but not exactly so, and there is no 
> > imaginary part whatsoever.
> >
> > x<-scan("kinv.dat")       #length(x) = 216
> > y<-x[c(109:216,1:108)]
> > X<-toeplitz(y)
> > eigen(X)$vectors
> 
> We don't have "kinv.dat", but X is not circulant as usually defined.

Sorry about kinv.dat -- in the e-mail that came back to me, it read "kinv",
btw.

I am unclear why you say that "X is not circulant as usually defined" -- do you
think you could clarify? It is true I use a Toeplitz matrix to set this up, but
how does that matter? The end result in this case is still a circulant matrix
that is symmetric, is it not? I would like to know why my result is not
circulant here.

> > Note that the eigenvectors are correct, and they are indeed real, 
> > because X is symmetric.
> >
> > Is this a bug in R? Any insight if not, please!
> 
> Well, first R calls LAPACK or EISPACK, so it would be a bug in one of 
> those.  But in so far as I understand you, X is a real symmetric matrix, 
> and those have real eigenvalues and eigenvectors.

Yes, I know that R calls LAPACK (which now contains EISPACK, btw). But I also
know that LAPACK contains complex eigendecomposition routines in addition to
double precision ones and it would need to be used if there is reason to
believe that the result is complex valued. (In particular ZGESDD would do it.)

The eigendecomposition of a matrix is unique. Whatever you think of Bellman,
the book does show how the eigenvectors of a circulant matrix are given by the
complex roots of unity as given above. We have therefore exhibited an
eigendecomposition without actually going through major computations (which is
good, because statistical computing is best when you use it sparingly). 
Why then does the result differ from that in R, and why by so much? (After all,
the eigendecomposition is unique, or is that only fpr real matrices?)

> I think you are confused about the meaning of Toeplitz and circulant.

Unclear, but would like to hear about your views on the actual differences in
this specific example.

> Compare
> 
> http://mathworld.wolfram.com/CirculantMatrix.html
> http://mathworld.wolfram.com/ToeplitzMatrix.html
> 
> and note that ?toeplitz says it computes the *symmetric* Toeplitz matrix.

In my case, my matrix is symmetric and the result is a circulant matrix.

> There is a very regretable tendency here for people to assume their 
> lack of understanding is `a bug in R'.

True, but bugs in software are not exactly rare. Though R does have very few
bugs and which is why I recommend the software to every Tom.

Besides I asked a question here because I was confused....

Many thanks and best wishes!

> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From andy_liaw at merck.com  Mon May  2 15:34:47 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 2 May 2005 09:34:47 -0400
Subject: [R] Multivariate kernel density estimation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076EA5@usctmx1106.merck.com>

locfit() in the `locfit' package should be able to handle at least 5
dimensions, but you need tons of data and either lots of time or a very fast
computer.  ssden() in the `gss' package will handle up to 4 dimensions.
Those are the only two I know about.  Others may well know better.

HTH,
Andy

> From: Stephan Tolksdorf
> 
> Hi,
> 
> I need to estimate the density at the mean of a sample of a few 
> thousands data points with a dimesion up to 5. The data is 
> uni-modal and 
>   regularly shaped.
> 
> I couldn't find any kernel density package for R which supports more
> than 3 dimensions. Have I overlooked a package or does somebody have 
> code for this purpose? Any other advice?
> 
> Regards,
>    Stephan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From itsme_410 at yahoo.com  Mon May  2 15:35:55 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Mon, 2 May 2005 06:35:55 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: 6667
Message-ID: <20050502133555.57317.qmail@web54510.mail.yahoo.com>

OK, lets redo this again, and ensure that we start with a row that will indeed
lead to a symmetric matrix for the circulant matrix:

x<-scan("kinv")
y<-x[c(109:1,2:108)]

X=toeplitz(y)
Z=y
for (i in 2:216) Z=rbind(Z,y[c((216-i+2):216,1:(216-i+1))])

range(X-Z)
[1] 0 0

eigen(X) is the same as eigen(Z), but we know that Z is a circulant matrix so
the eigenvectors are complex....

Any thoughts/screams?


--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Sun, 1 May 2005, someone who didn't give his name wrote:
> 
> > It is my understanding that the eigenvectors of a circulant matrix are 
> > given as follows:
> >
> > 1,omega,omega^2,....,omega^{p-1}
> >
> > where the matrix has dimension given by p x p and omega is one of p complex
> > roots of unity. (See Bellman for an excellent discussion on this).
> 
> What is the relevance of this?  Also, your reference is useless to us, 
> which is important as this all hinges on your definitions.
> 
> > The matrix created by the attached row and obtained using the following 
> > commands indicates no imaginary parts for the eigenvectors. It appears 
> > that the real values are close, but not exactly so, and there is no 
> > imaginary part whatsoever.
> >
> > x<-scan("kinv.dat")       #length(x) = 216
> > y<-x[c(109:216,1:108)]
> > X<-toeplitz(y)
> > eigen(X)$vectors
> 
> We don't have "kinv.dat", but X is not circulant as usually defined.
> 
> > Note that the eigenvectors are correct, and they are indeed real, 
> > because X is symmetric.
> >
> > Is this a bug in R? Any insight if not, please!
> 
> Well, first R calls LAPACK or EISPACK, so it would be a bug in one of 
> those.  But in so far as I understand you, X is a real symmetric matrix, 
> and those have real eigenvalues and eigenvectors.
> 
> I think you are confused about the meaning of Toeplitz and circulant.
> Compare
> 
> http://mathworld.wolfram.com/CirculantMatrix.html
> http://mathworld.wolfram.com/ToeplitzMatrix.html
> 
> and note that ?toeplitz says it computes the *symmetric* Toeplitz matrix.
> 
> There is a very regretable tendency here for people to assume their 
> lack of understanding is `a bug in R'.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From jkawczak at uncc.edu  Mon May  2 15:46:26 2005
From: jkawczak at uncc.edu (Janusz Kawczak)
Date: Mon, 2 May 2005 09:46:26 -0400 (EDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <20050502133555.57317.qmail@web54510.mail.yahoo.com>
References: <20050502133555.57317.qmail@web54510.mail.yahoo.com>
Message-ID: <Pine.GSO.4.55.0505020945510.4539@is-sm1.uncc.edu>

Again, what's your "kinv"?

On Mon, 2 May 2005, Globe Trotter wrote:

> OK, lets redo this again, and ensure that we start with a row that will indeed
> lead to a symmetric matrix for the circulant matrix:
>
> x<-scan("kinv")
> y<-x[c(109:1,2:108)]
>
> X=toeplitz(y)
> Z=y
> for (i in 2:216) Z=rbind(Z,y[c((216-i+2):216,1:(216-i+1))])
>
> range(X-Z)
> [1] 0 0
>
> eigen(X) is the same as eigen(Z), but we know that Z is a circulant matrix so
> the eigenvectors are complex....
>
> Any thoughts/screams?
>
>
> --- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > On Sun, 1 May 2005, someone who didn't give his name wrote:
> >
> > > It is my understanding that the eigenvectors of a circulant matrix are
> > > given as follows:
> > >
> > > 1,omega,omega^2,....,omega^{p-1}
> > >
> > > where the matrix has dimension given by p x p and omega is one of p complex
> > > roots of unity. (See Bellman for an excellent discussion on this).
> >
> > What is the relevance of this?  Also, your reference is useless to us,
> > which is important as this all hinges on your definitions.
> >
> > > The matrix created by the attached row and obtained using the following
> > > commands indicates no imaginary parts for the eigenvectors. It appears
> > > that the real values are close, but not exactly so, and there is no
> > > imaginary part whatsoever.
> > >
> > > x<-scan("kinv.dat")       #length(x) = 216
> > > y<-x[c(109:216,1:108)]
> > > X<-toeplitz(y)
> > > eigen(X)$vectors
> >
> > We don't have "kinv.dat", but X is not circulant as usually defined.
> >
> > > Note that the eigenvectors are correct, and they are indeed real,
> > > because X is symmetric.
> > >
> > > Is this a bug in R? Any insight if not, please!
> >
> > Well, first R calls LAPACK or EISPACK, so it would be a bug in one of
> > those.  But in so far as I understand you, X is a real symmetric matrix,
> > and those have real eigenvalues and eigenvectors.
> >
> > I think you are confused about the meaning of Toeplitz and circulant.
> > Compare
> >
> > http://mathworld.wolfram.com/CirculantMatrix.html
> > http://mathworld.wolfram.com/ToeplitzMatrix.html
> >
> > and note that ?toeplitz says it computes the *symmetric* Toeplitz matrix.
> >
> > There is a very regretable tendency here for people to assume their
> > lack of understanding is `a bug in R'.
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rolf at math.unb.ca  Mon May  2 16:07:26 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 2 May 2005 11:07:26 -0300 (ADT)
Subject: [R] eigenvalues of a circulant matrix
Message-ID: <200505021407.j42E7QBO013253@erdos.math.unb.ca>

I just Googled around a bit and found definitions of Toeplitz and
circulant matrices as follows:

A Toeplitz matrix is any n x n matrix with values constant along each
(top-left to lower-right) diagonal.  matrix has the form

	a_0 a_1 .   .  .   .  ... a_{n-1}
	a_{-1} a_0 a_1        ... a_{n-2}
	a_{-2} a_{-1} a_0 a_1 ...    .
	   .      .    .   .   .     .
	   .      .    .   .   .     .
	   .      .    .   .   .     .
	a_{-(n-1)} a_{-(n-2)} ... a_1 a_0

(A Toeplitz matrix ***may*** be symmetric.)

A circulant matrix is an n x n matrix whose rows are composed of
cyclically shifted versions of a length-n vector. For example, the
circulant matrix on the vector (1, 2, 3, 4)  is

	4 1 2 3
	3 4 1 2
	2 3 4 1
	1 2 3 4

So circulant matrices are a special case of Toeplitz matrices.
However a circulant matrix cannot be symmetric.

The eigenvalues of the forgoing circulant matrix are 10, 2 + 2i,
2 - 2i, and 2 --- certainly not roots of unity.  Bellman may have
been talking about the particular (important) case of a circulant
matrix where the vector from which it is constructed is a canonical
basis vector e_i with a 1 in the i-th slot and zeroes elsewhere.

Such a matrix is in fact a unitary matrix (operator), whence its
spectrum is contained in the unit circle; its eigenvalues are indeed
n-th roots of unity.

Such matrices are related to the unilateral shift operator on
Hilbert space (which is the ``primordial'' Toeplitz operator).
It arises as multiplication by z on H^2 --- the ``analytic''
elements of L^2 of the unit circle.

On (infinite dimensional) Hilbert space the unilateral shift
looks like

	0 0 0 0 0 ...
	1 0 0 0 0 ...
	0 1 0 0 0 ...
	0 0 1 0 0 ...
        . . . . . ...
        . . . . . ...

which maps e_0 to e_1, e_1 to e_2, e_2 to e_3, ...  on and on
forever.  On (say) 4 dimensional space we can have a unilateral
shift operator/matrix

	0 0 0 0
	1 0 0 0
	0 1 0 0
	0 0 1 0

but its range is a 3 dimensional subspace (e_4 gets ``killed'').

The ``corresponding'' circulant matrix is

	0 0 0 1
	1 0 0 0
	0 1 0 0
	0 0 1 0

which is an onto mapping --- e_4 gets sent back to e_1.

I hope this clears up some of the confusion.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From itsme_410 at yahoo.com  Mon May  2 16:22:35 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Mon, 2 May 2005 07:22:35 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: 6667
Message-ID: <20050502142235.44157.qmail@web54501.mail.yahoo.com>


--- Rolf Turner <rolf at math.unb.ca> wrote:
> I just Googled around a bit and found definitions of Toeplitz and
> circulant matrices as follows:
> 
> A Toeplitz matrix is any n x n matrix with values constant along each
> (top-left to lower-right) diagonal.  matrix has the form
> 
> 	a_0 a_1 .   .  .   .  ... a_{n-1}
> 	a_{-1} a_0 a_1        ... a_{n-2}
> 	a_{-2} a_{-1} a_0 a_1 ...    .
> 	   .      .    .   .   .     .
> 	   .      .    .   .   .     .
> 	   .      .    .   .   .     .
> 	a_{-(n-1)} a_{-(n-2)} ... a_1 a_0
> 
> (A Toeplitz matrix ***may*** be symmetric.)

Agreed. As may a circulant matrix if a_i = a_{p-i+2}

> 
> A circulant matrix is an n x n matrix whose rows are composed of
> cyclically shifted versions of a length-n vector. For example, the
> circulant matrix on the vector (1, 2, 3, 4)  is
> 
> 	4 1 2 3
> 	3 4 1 2
> 	2 3 4 1
> 	1 2 3 4
> 
> So circulant matrices are a special case of Toeplitz matrices.
> However a circulant matrix cannot be symmetric.
> 
> The eigenvalues of the forgoing circulant matrix are 10, 2 + 2i,
> 2 - 2i, and 2 --- certainly not roots of unity. 

The eigenvalues are 4+1*omega+2*omega^2+3*omega^3.
omega=cos(2*pi*k/4)+isin(2*pi*k/4) as k ranges over 1, 2, 3, 4, so the above
holds.

 Bellman may have
> been talking about the particular (important) case of a circulant
> matrix where the vector from which it is constructed is a canonical
> basis vector e_i with a 1 in the i-th slot and zeroes elsewhere.

No, that is not true: his result can be verified for any circulant matrix,
directly.

> Such a matrix is in fact a unitary matrix (operator), whence its
> spectrum is contained in the unit circle; its eigenvalues are indeed
> n-th roots of unity.
> 
> Such matrices are related to the unilateral shift operator on
> Hilbert space (which is the ``primordial'' Toeplitz operator).
> It arises as multiplication by z on H^2 --- the ``analytic''
> elements of L^2 of the unit circle.
> 
> On (infinite dimensional) Hilbert space the unilateral shift
> looks like
> 
> 	0 0 0 0 0 ...
> 	1 0 0 0 0 ...
> 	0 1 0 0 0 ...
> 	0 0 1 0 0 ...
>         . . . . . ...
>         . . . . . ...
> 
> which maps e_0 to e_1, e_1 to e_2, e_2 to e_3, ...  on and on
> forever.  On (say) 4 dimensional space we can have a unilateral
> shift operator/matrix
> 
> 	0 0 0 0
> 	1 0 0 0
> 	0 1 0 0
> 	0 0 1 0
> 
> but its range is a 3 dimensional subspace (e_4 gets ``killed'').
> 
> The ``corresponding'' circulant matrix is
> 
> 	0 0 0 1
> 	1 0 0 0
> 	0 1 0 0
> 	0 0 1 0
> 
> which is an onto mapping --- e_4 gets sent back to e_1.
> 
> I hope this clears up some of the confusion.
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca

Many thanks and best wishes!



From abunn at whrc.org  Mon May  2 16:25:59 2005
From: abunn at whrc.org (Andy Bunn)
Date: Mon, 2 May 2005 10:25:59 -0400
Subject: [R] congratulations to the JGR developers
In-Reply-To: <MJEMKNGDPMLPFPPKGPAMOEKDCAAA.gfiske@whrc.org>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEOCDEAA.abunn@whrc.org>

> Just want to offer my congratulations to the JGR developers as
> the recepient
> of the 2005 Chambers Award.  Great job, guys!!
> http://stats.math.uni-augsburg.de/JGR/

This feels like the future of R to me. It's simple, powerful, and elegant
just like R. As soon as the binary that works with 2.1 is released I'll use
it exclusively on Linux and Windows. I'm deeply impressed.

-Andy



From reid_huntsinger at merck.com  Mon May  2 16:43:27 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 2 May 2005 10:43:27 -0400
Subject: [R] eigenvalues of a circulant matrix
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9401@uswpmx00.merck.com>

It's hard to argue against the fact that a real symmetric matrix has real
eigenvalues. The eigenvalues of the circulant matrix with first row v are
*polynomials* (not the roots of 1 themselves, unless as Rolf suggested you
start with a vector with all zeros except one 1) in the roots of 1, with
coefficients equal to the entries in v. This is the finite Fourier transform
of v, by the way, and takes real values when the coefficients are real and
symmetric, ie when the matrix is symmetric.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Globe Trotter
Sent: Monday, May 02, 2005 10:23 AM
To: Rolf Turner
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] eigenvalues of a circulant matrix



--- Rolf Turner <rolf at math.unb.ca> wrote:
> I just Googled around a bit and found definitions of Toeplitz and
> circulant matrices as follows:
> 
> A Toeplitz matrix is any n x n matrix with values constant along each
> (top-left to lower-right) diagonal.  matrix has the form
> 
> 	a_0 a_1 .   .  .   .  ... a_{n-1}
> 	a_{-1} a_0 a_1        ... a_{n-2}
> 	a_{-2} a_{-1} a_0 a_1 ...    .
> 	   .      .    .   .   .     .
> 	   .      .    .   .   .     .
> 	   .      .    .   .   .     .
> 	a_{-(n-1)} a_{-(n-2)} ... a_1 a_0
> 
> (A Toeplitz matrix ***may*** be symmetric.)

Agreed. As may a circulant matrix if a_i = a_{p-i+2}

> 
> A circulant matrix is an n x n matrix whose rows are composed of
> cyclically shifted versions of a length-n vector. For example, the
> circulant matrix on the vector (1, 2, 3, 4)  is
> 
> 	4 1 2 3
> 	3 4 1 2
> 	2 3 4 1
> 	1 2 3 4
> 
> So circulant matrices are a special case of Toeplitz matrices.
> However a circulant matrix cannot be symmetric.
> 
> The eigenvalues of the forgoing circulant matrix are 10, 2 + 2i,
> 2 - 2i, and 2 --- certainly not roots of unity. 

The eigenvalues are 4+1*omega+2*omega^2+3*omega^3.
omega=cos(2*pi*k/4)+isin(2*pi*k/4) as k ranges over 1, 2, 3, 4, so the above
holds.

 Bellman may have
> been talking about the particular (important) case of a circulant
> matrix where the vector from which it is constructed is a canonical
> basis vector e_i with a 1 in the i-th slot and zeroes elsewhere.

No, that is not true: his result can be verified for any circulant matrix,
directly.

> Such a matrix is in fact a unitary matrix (operator), whence its
> spectrum is contained in the unit circle; its eigenvalues are indeed
> n-th roots of unity.
> 
> Such matrices are related to the unilateral shift operator on
> Hilbert space (which is the ``primordial'' Toeplitz operator).
> It arises as multiplication by z on H^2 --- the ``analytic''
> elements of L^2 of the unit circle.
> 
> On (infinite dimensional) Hilbert space the unilateral shift
> looks like
> 
> 	0 0 0 0 0 ...
> 	1 0 0 0 0 ...
> 	0 1 0 0 0 ...
> 	0 0 1 0 0 ...
>         . . . . . ...
>         . . . . . ...
> 
> which maps e_0 to e_1, e_1 to e_2, e_2 to e_3, ...  on and on
> forever.  On (say) 4 dimensional space we can have a unilateral
> shift operator/matrix
> 
> 	0 0 0 0
> 	1 0 0 0
> 	0 1 0 0
> 	0 0 1 0
> 
> but its range is a 3 dimensional subspace (e_4 gets ``killed'').
> 
> The ``corresponding'' circulant matrix is
> 
> 	0 0 0 1
> 	1 0 0 0
> 	0 1 0 0
> 	0 0 1 0
> 
> which is an onto mapping --- e_4 gets sent back to e_1.
> 
> I hope this clears up some of the confusion.
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca

Many thanks and best wishes!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Christoph.Scherber at uni-jena.de  Mon May  2 16:52:48 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Mon, 02 May 2005 16:52:48 +0200
Subject: [R] "apply" question
Message-ID: <42763EC0.3070304@uni-jena.de>

Dear R users,

I??ve got a simple question but somehow I can??t find the solution:

I have a data frame with columns 1-5 containing one set of integer 
values, and columns 6-10 containing another set of integer values. 
Columns 6-10 contain NA??s at some places.

I now want to calculate
(1) the number of values in each row of columns 6-10 that were NA??s
(2) the sum of all values on columns 1-5 for which there were no missing 
values in the corresponding cells of columns 6-10.


Example: (let??s call the data frame "data")

Col1   Col2   Col3   Col4   Col5   Col6   Col7   Col8   Col9   Col10
1      2      5      2      3      NA      5      NA    1      4
3      1      4      5      2      6      NA      4     NA     1

The result would then be (for the first row)
(1) "There were 2 NA??s in columns 6-10."
(2) The mean of Columns 1-5 was 2+2+3=7" (because there were NA??s in the 
1st and 3rd position in rows 6-10)

So far, I know how to calculate the rowSums for the data.frame, but I 
don??t know how to condition these on the values of columns 6-10

rowSums(data[,1:5]) #that??s straightforward
apply(data[,6:19],1,function(x)sum(is.na(x))) #this also works fine

But I don??t know how to select just the desired values of columns 1-5 
(as described above)


Can anyone help me? Thanks a lot in advance!

Best regards
Christoph



From sdavis2 at mail.nih.gov  Mon May  2 16:58:39 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 2 May 2005 10:58:39 -0400
Subject: [R] "apply" question
References: <42763EC0.3070304@uni-jena.de>
Message-ID: <000e01c54f27$6cea2410$5179f345@WATSON>


----- Original Message ----- 
From: "Christoph Scherber" <Christoph.Scherber at uni-jena.de>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, May 02, 2005 10:52 AM
Subject: [R] "apply" question


> Dear R users,
>
> I??ve got a simple question but somehow I can??t find the solution:
>
> I have a data frame with columns 1-5 containing one set of integer values, 
> and columns 6-10 containing another set of integer values. Columns 6-10 
> contain NA??s at some places.
>
> I now want to calculate
> (1) the number of values in each row of columns 6-10 that were NA??s
> (2) the sum of all values on columns 1-5 for which there were no missing 
> values in the corresponding cells of columns 6-10.
>
>
> Example: (let??s call the data frame "data")
>
> Col1   Col2   Col3   Col4   Col5   Col6   Col7   Col8   Col9   Col10
> 1      2      5      2      3      NA      5      NA    1      4
> 3      1      4      5      2      6      NA      4     NA     1
>
> The result would then be (for the first row)
> (1) "There were 2 NA??s in columns 6-10."
> (2) The mean of Columns 1-5 was 2+2+3=7" (because there were NA??s in the 
> 1st and 3rd position in rows 6-10)
>
> So far, I know how to calculate the rowSums for the data.frame, but I 
> don??t know how to condition these on the values of columns 6-10
>
> rowSums(data[,1:5]) #that??s straightforward
> apply(data[,6:19],1,function(x)sum(is.na(x))) #this also works fine
>
> But I don??t know how to select just the desired values of columns 1-5 (as 
> described above)

tmp <- rowSums(data[apply(data[,6:19],1,function(x) sum(is.na(x)))==0,1:5])

Now, tmp contains only the rowsums for the rows with no NAs in the other 
columns.

Sean



From christoph.lehmann at gmx.ch  Mon May  2 18:13:16 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 02 May 2005 18:13:16 +0200
Subject: [R] RMySQL query: why result takes so much memory in R ?
Message-ID: <4276519C.7@gmx.ch>

Hi
I just started with RMySQL. I have a database with roughly 12 millions 
rows/records and 8 columns/fields.

 From all 12 millions of records I want to import 3 fields only.
The fields are specified as:id int(11), group char(15), measurement 
float(4,2).
Why does this take > 1G RAM? I run R on suse linux, with 1G RAM and with 
the code below it even fills the whole 1G of swap. I just don't 
understand how 12e6 * 3 can fill such a huge range of RAM? Thanks for 
clarification and potential solutions.


## my code
library(RMySQL)
drv <- dbDriver("MySQL")
ch <- dbConnect(drv,dbname="testdb",
                 user="root",password="mysql")
testdb <- dbGetQuery(ch,
        "select id, group, measurement from mydata")
dbDisconnect(ch)
dbUnloadDriver(drv)

## end of my code

Cheers
Christoph



From ggrothendieck at gmail.com  Mon May  2 17:06:03 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 May 2005 11:06:03 -0400
Subject: [R] congratulations to the JGR developers
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIKEOCDEAA.abunn@whrc.org>
References: <MJEMKNGDPMLPFPPKGPAMOEKDCAAA.gfiske@whrc.org>
	<NEBBIPHDAMMOKDKPOFFIKEOCDEAA.abunn@whrc.org>
Message-ID: <971536df05050208063cb824f0@mail.gmail.com>

On 5/2/05, Andy Bunn <abunn at whrc.org> wrote:
> > Just want to offer my congratulations to the JGR developers as
> > the recepient
> > of the 2005 Chambers Award.  Great job, guys!!
> > http://stats.math.uni-augsburg.de/JGR/
> 
> This feels like the future of R to me. It's simple, powerful, and elegant
> just like R. As soon as the binary that works with 2.1 is released I'll use
> it exclusively on Linux and Windows. I'm deeply impressed.

I have not tried JGR but regarding your three adjective describing R,
R is very powerful but I am not sure I would characterize it as simple
and elegant -- complex and practical seem nearer to the mark to me.
Some parts of R may be simple and elegant but when I think of simple
and elegant languages I think of ones that are organized around a
single concept like APL (arrays), Smalltalk (objects), etc.   The underlying
Lisp roots of R may have a certain simplicity to them and S3 (though not
S4) is relatively simple but not R as a whole.  On the other hand, the
fact that it is practical, powerful and free with a broad set of builtin
and addon functionality and has become a de facto standard for statistical 
research have been sufficient reason for me to do all my computing using R.



From jose_fcunam at hotmail.com  Mon May  2 17:06:08 2005
From: jose_fcunam at hotmail.com (=?iso-8859-1?B?Sm9zZSBIZXJyZXJhIEJheuFu?=)
Date: Mon, 02 May 2005 10:06:08 -0500
Subject: [R] Variance Homogenity
Message-ID: <BAY2-F24FF1301DFAEB82E576FEF91270@phx.gbl>

Working with living models sometimes the response is reflected in the 
variability more than the middle parameters, i.e., the variability can 
reflected the system response, p.ej., in toxicology models the variability 
in the response means something about the selective pressure, buy how to 
evaluated this variability?. The ANOVA Test evaluated difference between the 
medians using the variance between groups and behind groups, but i can??t use 
ANOVA if the two suposes don??t exists, normalitiy and variance homogeniety, 
then how to evaluated the differences in variance?, Barttlet Test evaluated 
the homoscedascity using the square sum and not the variance as a response 
variable. How could I evaluated the difference in groups with differents 
variances?. I don??t want use Kruskall-Wallis Test because my variable is 
continue and numerical. Do someone know what do must do it?

Jos?? Herrera Baz??n
Science Faculty
UNAM


http://search.t1msn.com.mx/



From itsme_410 at yahoo.com  Mon May  2 17:06:26 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Mon, 2 May 2005 08:06:26 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A9401@uswpmx00.merck.com>
Message-ID: <20050502150627.67639.qmail@web54507.mail.yahoo.com>

Please, please, we are talking about eigenvectors in the original post, not
eigenvalues. They are indeed all real when they are symmetric (even for
circulant matrices because the imaginary parts sum up to zero).

Many thanks!

--- "Huntsinger, Reid" <reid_huntsinger at merck.com> wrote:

> It's hard to argue against the fact that a real symmetric matrix has real
> eigenvalues. The eigenvalues of the circulant matrix with first row v are
> *polynomials* (not the roots of 1 themselves, unless as Rolf suggested you
> start with a vector with all zeros except one 1) in the roots of 1, with
> coefficients equal to the entries in v. This is the finite Fourier transform
> of v, by the way, and takes real values when the coefficients are real and
> symmetric, ie when the matrix is symmetric.
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Globe Trotter
> Sent: Monday, May 02, 2005 10:23 AM
> To: Rolf Turner
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] eigenvalues of a circulant matrix
> 
> 
> 
> --- Rolf Turner <rolf at math.unb.ca> wrote:
> > I just Googled around a bit and found definitions of Toeplitz and
> > circulant matrices as follows:
> > 
> > A Toeplitz matrix is any n x n matrix with values constant along each
> > (top-left to lower-right) diagonal.  matrix has the form
> > 
> > 	a_0 a_1 .   .  .   .  ... a_{n-1}
> > 	a_{-1} a_0 a_1        ... a_{n-2}
> > 	a_{-2} a_{-1} a_0 a_1 ...    .
> > 	   .      .    .   .   .     .
> > 	   .      .    .   .   .     .
> > 	   .      .    .   .   .     .
> > 	a_{-(n-1)} a_{-(n-2)} ... a_1 a_0
> > 
> > (A Toeplitz matrix ***may*** be symmetric.)
> 
> Agreed. As may a circulant matrix if a_i = a_{p-i+2}
> 
> > 
> > A circulant matrix is an n x n matrix whose rows are composed of
> > cyclically shifted versions of a length-n vector. For example, the
> > circulant matrix on the vector (1, 2, 3, 4)  is
> > 
> > 	4 1 2 3
> > 	3 4 1 2
> > 	2 3 4 1
> > 	1 2 3 4
> > 
> > So circulant matrices are a special case of Toeplitz matrices.
> > However a circulant matrix cannot be symmetric.
> > 
> > The eigenvalues of the forgoing circulant matrix are 10, 2 + 2i,
> > 2 - 2i, and 2 --- certainly not roots of unity. 
> 
> The eigenvalues are 4+1*omega+2*omega^2+3*omega^3.
> omega=cos(2*pi*k/4)+isin(2*pi*k/4) as k ranges over 1, 2, 3, 4, so the above
> holds.
> 
>  Bellman may have
> > been talking about the particular (important) case of a circulant
> > matrix where the vector from which it is constructed is a canonical
> > basis vector e_i with a 1 in the i-th slot and zeroes elsewhere.
> 
> No, that is not true: his result can be verified for any circulant matrix,
> directly.
> 
> > Such a matrix is in fact a unitary matrix (operator), whence its
> > spectrum is contained in the unit circle; its eigenvalues are indeed
> > n-th roots of unity.
> > 
> > Such matrices are related to the unilateral shift operator on
> > Hilbert space (which is the ``primordial'' Toeplitz operator).
> > It arises as multiplication by z on H^2 --- the ``analytic''
> > elements of L^2 of the unit circle.
> > 
> > On (infinite dimensional) Hilbert space the unilateral shift
> > looks like
> > 
> > 	0 0 0 0 0 ...
> > 	1 0 0 0 0 ...
> > 	0 1 0 0 0 ...
> > 	0 0 1 0 0 ...
> >         . . . . . ...
> >         . . . . . ...
> > 
> > which maps e_0 to e_1, e_1 to e_2, e_2 to e_3, ...  on and on
> > forever.  On (say) 4 dimensional space we can have a unilateral
> > shift operator/matrix
> > 
> > 	0 0 0 0
> > 	1 0 0 0
> > 	0 1 0 0
> > 	0 0 1 0
> > 
> > but its range is a 3 dimensional subspace (e_4 gets ``killed'').
> > 
> > The ``corresponding'' circulant matrix is
> > 
> > 	0 0 0 1
> > 	1 0 0 0
> > 	0 1 0 0
> > 	0 0 1 0
> > 
> > which is an onto mapping --- e_4 gets sent back to e_1.
> > 
> > I hope this clears up some of the confusion.
> > 
> > 				cheers,
> > 
> > 					Rolf Turner
> > 					rolf at math.unb.ca
> 
> Many thanks and best wishes!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From andy_liaw at merck.com  Mon May  2 17:06:47 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 2 May 2005 11:06:47 -0400
Subject: [R] "apply" question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076EA8@usctmx1106.merck.com>

Try:

> ## Number of NAs in columns 6-10.
> colSums(is.na(data[6:10]))
 Col6  Col7  Col8  Col9 Col10 
    1     1     1     1     0 
> 
> ## Number of NAs in each row of columns 6-10.
> rowSums(is.na(data[6:10]))
1 2 
2 2 
> 
> ## Sums of rows 1-5 omitting corresponding NAs in cols 6-10.
> rowSums(data[,1:5] * !is.na(data[,6:10]))
1 2 
7 9 

If all entries are numeric, it'd be easier to use matrices instead of data
frames.

HTH,
Andy

> From: Christoph Scherber
> 
> Dear R users,
> 
> I??ve got a simple question but somehow I can??t find the solution:
> 
> I have a data frame with columns 1-5 containing one set of integer 
> values, and columns 6-10 containing another set of integer values. 
> Columns 6-10 contain NA??s at some places.
> 
> I now want to calculate
> (1) the number of values in each row of columns 6-10 that were NA??s
> (2) the sum of all values on columns 1-5 for which there were 
> no missing 
> values in the corresponding cells of columns 6-10.
> 
> 
> Example: (let??s call the data frame "data")
> 
> Col1   Col2   Col3   Col4   Col5   Col6   Col7   Col8   Col9   Col10
> 1      2      5      2      3      NA      5      NA    1      4
> 3      1      4      5      2      6      NA      4     NA     1
> 
> The result would then be (for the first row)
> (1) "There were 2 NA??s in columns 6-10."
> (2) The mean of Columns 1-5 was 2+2+3=7" (because there were 
> NA??s in the 
> 1st and 3rd position in rows 6-10)
> 
> So far, I know how to calculate the rowSums for the data.frame, but I 
> don??t know how to condition these on the values of columns 6-10
> 
> rowSums(data[,1:5]) #that??s straightforward
> apply(data[,6:19],1,function(x)sum(is.na(x))) #this also works fine
> 
> But I don??t know how to select just the desired values of columns 1-5 
> (as described above)
> 
> 
> Can anyone help me? Thanks a lot in advance!
> 
> Best regards
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From GVG at Stowers-Institute.org  Mon May  2 17:07:34 2005
From: GVG at Stowers-Institute.org (Glazko, Galina)
Date: Mon, 2 May 2005 10:07:34 -0500
Subject: [R] Re: simulate zero-truncated Poisson distribution 
Message-ID: <200505021507.j42F7mkq016807@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050502/d9aecb9d/attachment.pl

From dimitris.rizopoulos at med.kuleuven.ac.be  Mon May  2 17:13:32 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 2 May 2005 17:13:32 +0200
Subject: [R] "apply" question
References: <42763EC0.3070304@uni-jena.de>
Message-ID: <009601c54f29$81389670$0540210a@www.domain>

you could try something like this:

dat <- rbind(c(1, 2, 5, 2, 3, NA, 5, NA, 1, 4),
             c(3, 1, 4, 5, 2, 6, NA, 4, NA, 1))
##########
# (1)
rowSums(is.na(dat[, 6:10]))

## (2)
dat. <- dat[, 1:5]
dat.[is.na(dat[, 6:10])] <- NA
rowSums(dat., na.rm=TRUE)
rowMeans(dat., na.rm=TRUE)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christoph Scherber" <Christoph.Scherber at uni-jena.de>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, May 02, 2005 4:52 PM
Subject: [R] "apply" question


> Dear R users,
>
> I??ve got a simple question but somehow I can??t find the solution:
>
> I have a data frame with columns 1-5 containing one set of integer 
> values, and columns 6-10 containing another set of integer values. 
> Columns 6-10 contain NA??s at some places.
>
> I now want to calculate
> (1) the number of values in each row of columns 6-10 that were NA??s
> (2) the sum of all values on columns 1-5 for which there were no 
> missing values in the corresponding cells of columns 6-10.
>
>
> Example: (let??s call the data frame "data")
>
> Col1   Col2   Col3   Col4   Col5   Col6   Col7   Col8   Col9   Col10
> 1      2      5      2      3      NA      5      NA    1      4
> 3      1      4      5      2      6      NA      4     NA     1
>
> The result would then be (for the first row)
> (1) "There were 2 NA??s in columns 6-10."
> (2) The mean of Columns 1-5 was 2+2+3=7" (because there were NA??s in 
> the 1st and 3rd position in rows 6-10)
>
> So far, I know how to calculate the rowSums for the data.frame, but 
> I don??t know how to condition these on the values of columns 6-10
>
> rowSums(data[,1:5]) #that??s straightforward
> apply(data[,6:19],1,function(x)sum(is.na(x))) #this also works fine
>
> But I don??t know how to select just the desired values of columns 
> 1-5 (as described above)
>
>
> Can anyone help me? Thanks a lot in advance!
>
> Best regards
> Christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Sebastian.Leuzinger at unibas.ch  Mon May  2 17:11:49 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Mon, 2 May 2005 17:11:49 +0200
Subject: [R] Roots of quadratic system.
In-Reply-To: <427579AC.20000@acadiau.ca>
References: <B998A44C8986644EA8029CFE6396A9241B32F6@exqld2-bne.qld.csiro.au>
	<427579AC.20000@acadiau.ca>
Message-ID: <200505021711.49805.Sebastian.Leuzinger@unibas.ch>

Hi, 
I used polyroot() and it works fine.
Sebastian


On Monday 02 May 2005 02:51, John Janmaat wrote:
> Hello Bill,
>
> I have used the optimization approach you suggest in past.  I was hoping
> that someone had written something specifically for solving a system of
> nonlinear equations, as the fsolve function does in MatLab.  The Octave
> version is somewhat limited compared to the MatLab version, and I like
> working in R.
>
> Thanks,
>
> John.
>
> ps: I would like the system to have a unique solution, but there is
> nothing about the system that precludes multiple equilibria.  Of course,
> the L(x) = ... approach can search for multiple equilibria if I try
> enough different starting points.
>
> Bill.Venables at csiro.au wrote:
> > Are you looking for a unique solution or families of solutions?
> >
> > Can't you turn a root-finding problem for a system of equations
> > with a unique solution into an optimisation problem, anyway?
> >
> > E.g.  You want to solve
> >
> > f1(x) = g1
> > f2(x) = g2
> > ...
> >
> > Why not optimise L(x) = (f1(x) - g1)^2 + (f2(x) - g2)^2 + ...
> > with respect to x?  If the minimum value is zero, then you are
> > done; if it is greater than zero your original system does not
> > have a solution.
> >
> > If you are in the complex domain the changes needed are obvious.
> >
> > V.
> >
> > : -----Original Message-----
> > : From: r-help-bounces at stat.math.ethz.ch
> > : [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Janmaat
> > : Sent: Monday, 2 May 2005 12:48 AM
> > : To: r-help at stat.math.ethz.ch
> > : Subject: [R] Roots of quadratic system.
> > :
> > :
> > : Hello,
> > :
> > : I have a system of quadratic equations (results of a
> > : Hamiltonian optimization)
> > : which I need to find the roots for.  Is there a package
> > : and/or function which
> > : will find the roots for a quadratic system?  Note that I am
> > : not opimizing, but
> > : rather solving the first order conditions which come from a
> > : Hamiltonian.  I am
> > : basically looking for something in R that will do the same
> > : thing as fsolve in
> > : Matlab.
> > :
> > : Thanks,
> > :
> > : John.
> > :
> > : ==============================================
> > : Dr. John Janmaat
> > : Department of Economics
> > : Acadia University
> > : Tel: 902-585-1461
> > :
> > : ______________________________________________
> > : R-help at stat.math.ethz.ch mailing list
> > : https://stat.ethz.ch/mailman/listinfo/r-help
> > : PLEASE do read the posting guide!
> > : http://www.R-project.org/posting-guide.html

-- 
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From kriskyc at ohsu.edu  Mon May  2 17:13:55 2005
From: kriskyc at ohsu.edu (Christine Krisky)
Date: Mon, 02 May 2005 08:13:55 -0700
Subject: [R] newbie ifelse matrix question
Message-ID: <s275e156.000@ohsu.edu>

Hi all,
 
I have time series data in a matrix format 20 rows x 205 columns and have been trying to replace outliers with NA.  My first column contains the outliers threshold (3 Standard deviations) for each row  - here's a bit of the first row
 
   sd3      V1 V2 V3 V4         V5        V6        V7        V8        V9
1  13.03267 1797157 75 84 58  -1.958649  0.048775  2.056198  8.063622  3.071045
 
What I want is a statment that says if the absolute value of [,5:205]  <= column 1 keep the value, otherwise replace with NA
 
I've tried this without success - I do know this line works with s-plus
data1[,6:205]<-ifelse(abs(data1[,6:205])<=data1[,1],data1[,6:205], NA)
 
But what I get with R is only partially correct.  I get NA in places where there shouldn't be.  About every 7 columns or so I end up with 3 columns of NA and no replacement with NA where there should be (V11) in other places.  The majority of the matrix though is correct.
 
   sd3      V1     V2 V3 V4     V5        V6        V7        V8        V9         V10      V11	    V12  V13	V14	V15
1  13.03267 1797157 75 84 58  -1.958649  0.048775  2.056198  8.063622  3.071045  0.078468   -21.9141    NA   NA   NA  3.115585
 
 
I've tried searching the manual and user list and countless changes to the syntax over the last week, but no luck so far.
 
Here is my syntax and error
 
> data1<-read.table("testR_data",na.string="0.000000")
> dim(data1) 
[1]  20 204
# Find sd of rows, then multiply * 3
> sd_data3<-apply(data1[,5:204],1,sd,na.rm=TRUE)*3
# Attach sd_data3 to data1
> sd_and_data1<-cbind(sd_data3,data1)
# Replace values >= 3 SDs with NA
> sd_and_data1[,6:205]<-ifelse(abs(sd_and_data1[,6:205])<=sd_and_data1[,1],sd_and_data1[,6:205], NA)
Warning message: 
provided 4000 variables to replace 200 variables in: "[<-.data.frame"(`*tmp*`, , 6:205, value = list(c(-1.958649,  
 
I can't figure this out and would be very grateful for your help.
 
-chris



From murdoch at stats.uwo.ca  Mon May  2 17:18:35 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 02 May 2005 16:18:35 +0100
Subject: [R] RMySQL query: why result takes so much memory in R ?
In-Reply-To: <4276519C.7@gmx.ch>
References: <4276519C.7@gmx.ch>
Message-ID: <427644CB.1030000@stats.uwo.ca>

Christoph Lehmann wrote:
> Hi
> I just started with RMySQL. I have a database with roughly 12 millions 
> rows/records and 8 columns/fields.
> 
>  From all 12 millions of records I want to import 3 fields only.
> The fields are specified as:id int(11), group char(15), measurement 
> float(4,2).
> Why does this take > 1G RAM? I run R on suse linux, with 1G RAM and with 
> the code below it even fills the whole 1G of swap. I just don't 
> understand how 12e6 * 3 can fill such a huge range of RAM? Thanks for 
> clarification and potential solutions.

Those fields are each 8 or 20 bytes in size, so you're talking 12e6 
times 36 or about nearly half a Gig for each copy.  Presumably the code 
is storing more than one or two copies of the data.

Why don't you use fetch() to get your records in more manageable chunks?

Duncan Murdoch

> 
> 
> ## my code
> library(RMySQL)
> drv <- dbDriver("MySQL")
> ch <- dbConnect(drv,dbname="testdb",
>                 user="root",password="mysql")
> testdb <- dbGetQuery(ch,
>        "select id, group, measurement from mydata")
> dbDisconnect(ch)
> dbUnloadDriver(drv)
> 
> ## end of my code
> 
> Cheers
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Mon May  2 17:19:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 May 2005 11:19:59 -0400
Subject: [R] "apply" question
In-Reply-To: <42763EC0.3070304@uni-jena.de>
References: <42763EC0.3070304@uni-jena.de>
Message-ID: <971536df050502081971a0977e@mail.gmail.com>

On 5/2/05, Christoph Scherber <Christoph.Scherber at uni-jena.de> wrote:
> Dear R users,
> 
> I??ve got a simple question but somehow I can??t find the solution:
> 
> I have a data frame with columns 1-5 containing one set of integer
> values, and columns 6-10 containing another set of integer values.
> Columns 6-10 contain NA??s at some places.
> 
> I now want to calculate
> (1) the number of values in each row of columns 6-10 that were NA??s

Supposing our data is called DF,

rowSums(!is.na(DF[,6:10]))

> (2) the sum of all values on columns 1-5 for which there were no missing
> values in the corresponding cells of columns 6-10.

In the expression below 1 + 0 *DF[,6:10] is like DF[,6:10] except
all non-NAs are replaced by 1.  Multiplying DF[,1:5] by that
effectively replaces each element in DF[,1:5] with an NA if
the corresponding DF[,6:10] contained an NA.

rowSums( DF[,1:5] * (1 + 0 * DF[,6:10]), na.rm = TRUE )

> 
> Example: (let??s call the data frame "data")
> 
> Col1   Col2   Col3   Col4   Col5   Col6   Col7   Col8   Col9   Col10
> 1      2      5      2      3      NA      5      NA    1      4
> 3      1      4      5      2      6      NA      4     NA     1
> 
> The result would then be (for the first row)
> (1) "There were 2 NA??s in columns 6-10."
> (2) The mean of Columns 1-5 was 2+2+3=7" (because there were NA??s in the
> 1st and 3rd position in rows 6-10)

I guess you meant sum when you referred to mean in (2).  If you really
do want the mean replace rowSums with rowMeans in the expression
given above in the answer to (2).



From abunn at whrc.org  Mon May  2 17:23:21 2005
From: abunn at whrc.org (Andy Bunn)
Date: Mon, 2 May 2005 11:23:21 -0400
Subject: [R] congratulations to the JGR developers
In-Reply-To: <971536df05050208063cb824f0@mail.gmail.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIEEOFDEAA.abunn@whrc.org>

> I have not tried JGR but regarding your three adjective describing R,
> R is very powerful but I am not sure I would characterize it as simple
> and elegant -- complex and practical seem nearer to the mark to me.

I take umbrage (and not in the sense of affording shade).

Take 'simple' to mean plain, or having few ornamentations. Then it sounds
like R.  As for elegance, R is refined, tasteful, and beautiful. When I grow
up, I want to marry R.



From bates at stat.wisc.edu  Mon May  2 17:24:50 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 02 May 2005 10:24:50 -0500
Subject: [R] formula in fixed-effects part of GLMM
In-Reply-To: <42743416@webmail.ualberta.ca>
References: <42743416@webmail.ualberta.ca>
Message-ID: <42764642.6060705@stat.wisc.edu>

weihong wrote:
> Can GLMM take formula derived from another object?
> 
> foo <- glm (OVEN ~ h + h2, poisson, dataset)
> 
> # ok
> bar <- GLMM (OVEN ~ h + h2, poisson, dataset, random = list (yr = ~1))
> 
> #error
> bar <- GLMM (foo$formula, poisson, dataset, random = list (yr = ~1))
> #Error in foo$("formula" + yr + 1) : invalid subscript type

That won't work without some tweaking of the GLMM function.  In the
0.95-1 and later versions of the lme4 package the capabilities of GLMM
have been folded in to the lmer function and you would need to fit that
model as

 lmer(OVEN ~ h + h2 + (1|yr), dataset, poisson)

anyway.



From ggrothendieck at gmail.com  Mon May  2 17:39:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 May 2005 11:39:28 -0400
Subject: [R] congratulations to the JGR developers
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIEEOFDEAA.abunn@whrc.org>
References: <971536df05050208063cb824f0@mail.gmail.com>
	<NEBBIPHDAMMOKDKPOFFIEEOFDEAA.abunn@whrc.org>
Message-ID: <971536df0505020839260ca5@mail.gmail.com>

On 5/2/05, Andy Bunn <abunn at whrc.org> wrote:
> > I have not tried JGR but regarding your three adjective describing R,
> > R is very powerful but I am not sure I would characterize it as simple
> > and elegant -- complex and practical seem nearer to the mark to me.
> 
> I take umbrage (and not in the sense of affording shade).
> 
> Take 'simple' to mean plain, or having few ornamentations. Then it sounds
> like R.  As for elegance, R is refined, tasteful, and beautiful. When I grow
> up, I want to marry R.

This seems to me to be like the Oscar Wilde sketch where they 
are criticizing the King behind his back and every time they get
caught redefine their words to turn the insult into praise in order
to please him.  They keep coming up with more and more
outrageous insults making it harder and harder to figure out how
to redefine them as praise.



From reid_huntsinger at merck.com  Mon May  2 17:43:35 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 2 May 2005 11:43:35 -0400
Subject: [R] eigenvalues of a circulant matrix
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9402@uswpmx00.merck.com>

When the matrix is symmetric and omega is not real, omega and its conjugate
(= inverse) give the same eigenvalue, so you have a 2-dimensional
eigenspace. R chooses a real basis of this, which is perfectly fine since
it's not looking for circulant structure.

For example,

> m
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    2    3    3    2
[2,]    2    1    2    3    3
[3,]    3    2    1    2    3
[4,]    3    3    2    1    2
[5,]    2    3    3    2    1

> eigen(m)
$values
[1] 11.000000 -0.381966 -0.381966 -2.618034 -2.618034

$vectors
          [,1]      [,2]       [,3]       [,4]      [,5]
[1,] 0.4472136  0.000000 -0.6324555  0.6324555  0.000000
[2,] 0.4472136  0.371748  0.5116673  0.1954395  0.601501
[3,] 0.4472136 -0.601501 -0.1954395 -0.5116673  0.371748
[4,] 0.4472136  0.601501 -0.1954395 -0.5116673 -0.371748
[5,] 0.4472136 -0.371748  0.5116673  0.1954395 -0.601501

and you can match these columns up with the "canonical" eigenvectors
exp(2*pi*1i*(0:4)*j/5) for j = 0,1,2,3,4. E.g.,

> Im(exp(2*pi*1i*(0:4)*3/5))
[1]  0.0000000 -0.5877853  0.9510565 -0.9510565  0.5877853

which can be seen to be a scalar multiple of column 2. 

Reid Huntsinger

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Huntsinger, Reid
Sent: Monday, May 02, 2005 10:43 AM
To: 'Globe Trotter'; Rolf Turner
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] eigenvalues of a circulant matrix


It's hard to argue against the fact that a real symmetric matrix has real
eigenvalues. The eigenvalues of the circulant matrix with first row v are
*polynomials* (not the roots of 1 themselves, unless as Rolf suggested you
start with a vector with all zeros except one 1) in the roots of 1, with
coefficients equal to the entries in v. This is the finite Fourier transform
of v, by the way, and takes real values when the coefficients are real and
symmetric, ie when the matrix is symmetric.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Globe Trotter
Sent: Monday, May 02, 2005 10:23 AM
To: Rolf Turner
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] eigenvalues of a circulant matrix



--- Rolf Turner <rolf at math.unb.ca> wrote:
> I just Googled around a bit and found definitions of Toeplitz and
> circulant matrices as follows:
> 
> A Toeplitz matrix is any n x n matrix with values constant along each
> (top-left to lower-right) diagonal.  matrix has the form
> 
> 	a_0 a_1 .   .  .   .  ... a_{n-1}
> 	a_{-1} a_0 a_1        ... a_{n-2}
> 	a_{-2} a_{-1} a_0 a_1 ...    .
> 	   .      .    .   .   .     .
> 	   .      .    .   .   .     .
> 	   .      .    .   .   .     .
> 	a_{-(n-1)} a_{-(n-2)} ... a_1 a_0
> 
> (A Toeplitz matrix ***may*** be symmetric.)

Agreed. As may a circulant matrix if a_i = a_{p-i+2}

> 
> A circulant matrix is an n x n matrix whose rows are composed of
> cyclically shifted versions of a length-n vector. For example, the
> circulant matrix on the vector (1, 2, 3, 4)  is
> 
> 	4 1 2 3
> 	3 4 1 2
> 	2 3 4 1
> 	1 2 3 4
> 
> So circulant matrices are a special case of Toeplitz matrices.
> However a circulant matrix cannot be symmetric.
> 
> The eigenvalues of the forgoing circulant matrix are 10, 2 + 2i,
> 2 - 2i, and 2 --- certainly not roots of unity. 

The eigenvalues are 4+1*omega+2*omega^2+3*omega^3.
omega=cos(2*pi*k/4)+isin(2*pi*k/4) as k ranges over 1, 2, 3, 4, so the above
holds.

 Bellman may have
> been talking about the particular (important) case of a circulant
> matrix where the vector from which it is constructed is a canonical
> basis vector e_i with a 1 in the i-th slot and zeroes elsewhere.

No, that is not true: his result can be verified for any circulant matrix,
directly.

> Such a matrix is in fact a unitary matrix (operator), whence its
> spectrum is contained in the unit circle; its eigenvalues are indeed
> n-th roots of unity.
> 
> Such matrices are related to the unilateral shift operator on
> Hilbert space (which is the ``primordial'' Toeplitz operator).
> It arises as multiplication by z on H^2 --- the ``analytic''
> elements of L^2 of the unit circle.
> 
> On (infinite dimensional) Hilbert space the unilateral shift
> looks like
> 
> 	0 0 0 0 0 ...
> 	1 0 0 0 0 ...
> 	0 1 0 0 0 ...
> 	0 0 1 0 0 ...
>         . . . . . ...
>         . . . . . ...
> 
> which maps e_0 to e_1, e_1 to e_2, e_2 to e_3, ...  on and on
> forever.  On (say) 4 dimensional space we can have a unilateral
> shift operator/matrix
> 
> 	0 0 0 0
> 	1 0 0 0
> 	0 1 0 0
> 	0 0 1 0
> 
> but its range is a 3 dimensional subspace (e_4 gets ``killed'').
> 
> The ``corresponding'' circulant matrix is
> 
> 	0 0 0 1
> 	1 0 0 0
> 	0 1 0 0
> 	0 0 1 0
> 
> which is an onto mapping --- e_4 gets sent back to e_1.
> 
> I hope this clears up some of the confusion.
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca

Many thanks and best wishes!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Ted.Harding at nessie.mcc.ac.uk  Mon May  2 17:37:00 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 02 May 2005 16:37:00 +0100 (BST)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <200505021407.j42E7QBO013253@erdos.math.unb.ca>
Message-ID: <XFMail.050502163700.Ted.Harding@nessie.mcc.ac.uk>

On 02-May-05 Rolf Turner wrote:
> I just Googled around a bit and found definitions of Toeplitz and
> circulant matrices as follows:
> [...]
> A circulant matrix is an n x n matrix whose rows are composed of
> cyclically shifted versions of a length-n vector. For example, the
> circulant matrix on the vector (1, 2, 3, 4)  is
> 
>       4 1 2 3
>       3 4 1 2
>       2 3 4 1
>       1 2 3 4
> 
> So circulant matrices are a special case of Toeplitz matrices.
> However a circulant matrix cannot be symmetric.

I suspect the confusion may lie in what's meant by "cyclically
shifted". In Rolf's example above, each row is shifted right by 1
and the one that falls off the end is put at the beginning. This
cannot be symmetric for general values in the fist row.

However, if you shift left instead, then you get

        4 1 2 3
        1 2 3 4
        2 3 4 1
        3 4 1 2

and this *is* symmetric (and indeed will always be so, for
general values in the first row).

All the formal definitions of "circulant" which I have seen
use Rolf's definition ("shift right"). Suppose we call the
left-shifted one "anti-circulant" ("AC").

The vector (which *is* and eigenvector of a circulant matrix):

  c(1, w, w^2, ... , w^(p-1))

where w is *any* complex p-th root of unity (including 1), is
not in general an eigenvector of a pxp AC matrix.

Example:

M<-matrix(c(1,2,3,2,3,1,3,1,2),nrow=3)
M
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    2    3    1
[3,]    3    1    2

p1 <- 1 ; p2 <- (-1-sqrt(3))/2 ; p3 <- (-1+sqrt(3))/2
e1 <- c(1,p1,p1^2)
e2 <- c(1,p2,p2^2)
e3 <- c(1,p3,p3^2)

v1<-(M%*%e1)[1] ; v1
[1] 6
cbind(round(M%*%e1/v1,15), e1)
       e1
[1,] 1  1
[2,] 1  1
[3,] 1  1

v2<-(M%*%e2)[1] ; v2
[1] -0.2320508
cbind(round(M%*%e2/v2,15), e2)
                                  e2
[1,]  1.0+0.0000000i  1.0+0.0000000i
[2,] -0.5+0.8660254i -0.5-0.8660254i
[3,] -0.5-0.8660254i -0.5+0.8660254i

v3<-(M%*%e3)[1] ; v3
[1] 2.133975
cbind(round(M%*%e3/v3,15), e3)
                                  e3
[1,]  1.0+0.0000000i  1.0+0.0000000i
[2,] -0.5-0.8660254i -0.5+0.8660254i
[3,] -0.5+0.8660254i -0.5-0.8660254i

(I've out in rounding because of nasty little specks of "i"
 that keep dropping out, as in

  p2^3
  [1] 1 - 6.432571e-16i
)

So (except for e1) e2 and e3 are not eigenvectors of M
(note the switching of signs in the imaginary parts of
row 2 and in row 3).

The AC matrix, being symmetric, heas real eigenvalues and
real eigevectors, as can be easily verified using 'eigen'.

Therefore I suspect that "Globe Trotter"'s "circulant matrix"
was in fact an "anti-circulant" ("AC") matrix. However, from
the information he gave I'm not clear how to verify that
this is the case.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 02-May-05                                       Time: 16:32:27
------------------------------ XFMail ------------------------------



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon May  2 17:54:54 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 2 May 2005 17:54:54 +0200
Subject: [R] newbie ifelse matrix question
References: <s275e156.000@ohsu.edu>
Message-ID: <00eb01c54f2f$4883edb0$0540210a@www.domain>

try something like this:

dat <- matrix(rnorm(20*205), 20, 205)
sds <- sd(t(dat))
######
dat[, 6:205][abs(dat[, 6:205]) > sds] <- NA


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christine Krisky" <kriskyc at ohsu.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, May 02, 2005 5:13 PM
Subject: [R] newbie ifelse matrix question


> Hi all,
>
> I have time series data in a matrix format 20 rows x 205 columns and 
> have been trying to replace outliers with NA.  My first column 
> contains the outliers threshold (3 Standard deviations) for each 
> ow  - here's a bit of the first row
>
>   sd3      V1 V2 V3 V4         V5        V6        V7        V8 
> V9
> 1  13.03267 1797157 75 84 58  -1.958649  0.048775  2.056198 
> 8.063622  3.071045
>
> What I want is a statment that says if the absolute value of 
> [,5:205]  <= column 1 keep the value, otherwise replace with NA
>
> I've tried this without success - I do know this line works with 
> s-plus
> data1[,6:205]<-ifelse(abs(data1[,6:205])<=data1[,1],data1[,6:205], 
> NA)
>
> But what I get with R is only partially correct.  I get NA in places 
> where there shouldn't be.  About every 7 columns or so I end up with 
> 3 columns of NA and no replacement with NA where there should be 
> (V11) in other places.  The majority of the matrix though is 
> correct.
>
>   sd3      V1     V2 V3 V4     V5        V6        V7        V8 
> V9         V10      V11     V12  V13 V14 V15
> 1  13.03267 1797157 75 84 58  -1.958649  0.048775  2.056198 
> 8.063622  3.071045  0.078468   -21.9141    NA   NA   NA  3.115585
>
>
> I've tried searching the manual and user list and countless changes 
> to the syntax over the last week, but no luck so far.
>
> Here is my syntax and error
>
>> data1<-read.table("testR_data",na.string="0.000000")
>> dim(data1)
> [1]  20 204
> # Find sd of rows, then multiply * 3
>> sd_data3<-apply(data1[,5:204],1,sd,na.rm=TRUE)*3
> # Attach sd_data3 to data1
>> sd_and_data1<-cbind(sd_data3,data1)
> # Replace values >= 3 SDs with NA
>> sd_and_data1[,6:205]<-ifelse(abs(sd_and_data1[,6:205])<=sd_and_data1[,1],sd_and_data1[,6:205], 
>> NA)
> Warning message:
> provided 4000 variables to replace 200 variables in: 
> "[<-.data.frame"(`*tmp*`, , 6:205, value = list(c(-1.958649,
>
> I can't figure this out and would be very grateful for your help.
>
> -chris
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ksr5g4 at mizzou.edu  Mon May  2 18:08:01 2005
From: ksr5g4 at mizzou.edu (Reeves, Kerry Scott (UMC-Student))
Date: Mon, 2 May 2005 11:08:01 -0500
Subject: [R] ANOSIM
Message-ID: <4D72877564AABB478AA1C5C77A593EFE43D87A@UM-EMAIL07.um.umsystem.edu>

Hello,

I am interested in using ANOSIM (analysis of similarities) which is part
of the vegan package.  When I look at the programmers notes they mention
that "They don't quite trust this method, and someone should study it
closely."  I was just wondering if this had been done, and whether it
had been found reliable or not?

Thanks,
Kerry Reeves



From ozric at web.de  Mon May  2 18:19:39 2005
From: ozric at web.de (christian schulz)
Date: Mon, 02 May 2005 18:19:39 +0200
Subject: [R] RMySQL query: why result takes so much memory in R ?
In-Reply-To: <4276519C.7@gmx.ch>
References: <4276519C.7@gmx.ch>
Message-ID: <4276531B.3060007@web.de>

Hi,

IMHO  you need only when your columns are numeric
X rows /100/100/8  MB.
 >>(12000000*3)/100/100/8
[1] 450

But one of your columns  is  group char.
I'm suffering in the past in lot of things with massive data and R and
recognize doing how many as possible in the database, or you have to
upgrade your computer to 2-4GB like a database machine!?

regards, christian


Christoph Lehmann schrieb:

> Hi
> I just started with RMySQL. I have a database with roughly 12 millions 
> rows/records and 8 columns/fields.
>
> From all 12 millions of records I want to import 3 fields only.
> The fields are specified as:id int(11), group char(15), measurement 
> float(4,2).
> Why does this take > 1G RAM? I run R on suse linux, with 1G RAM and 
> with the code below it even fills the whole 1G of swap. I just don't 
> understand how 12e6 * 3 can fill such a huge range of RAM? Thanks for 
> clarification and potential solutions.
>
>
> ## my code
> library(RMySQL)
> drv <- dbDriver("MySQL")
> ch <- dbConnect(drv,dbname="testdb",
>                 user="root",password="mysql")
> testdb <- dbGetQuery(ch,
>        "select id, group, measurement from mydata")
> dbDisconnect(ch)
> dbUnloadDriver(drv)
>
> ## end of my code
>
> Cheers
> Christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From xl2134 at columbia.edu  Mon May  2 18:43:27 2005
From: xl2134 at columbia.edu (Xiang-Jun Lu)
Date: Mon, 2 May 2005 12:43:27 -0400 (EDT)
Subject: [R] Re fixed -- R2.1.0: X11 font at size 14 could not be loaded
In-Reply-To: <Pine.GSO.4.62.0504291823340.15402@mango.cc.columbia.edu>
References: <Pine.GSO.4.62.0504281709001.23791@banana.cc.columbia.edu>
	<20050428214948.GA10940@hortresearch.co.nz>
	<Pine.LNX.4.61.0504290709280.4155@gannet.stats>
	<Pine.GSO.4.62.0504291823340.15402@mango.cc.columbia.edu>
Message-ID: <Pine.GSO.4.62.0505021237000.11170@mango.cc.columbia.edu>


Thanks again to Prof. Ripley, Peter Dalgaard, and Seth Falcon for their 
prompt responses to my X11 font problem. After restarting the font server, 
the problem has been fixed.

Xiang-Jun



From f.harrell at vanderbilt.edu  Mon May  2 18:05:26 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 02 May 2005 11:05:26 -0500
Subject: [R] Restricted cubic spline function ERROR?: glm(Y~rcs(x,5))
In-Reply-To: <001701c54eed$e7dd68b0$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <001701c54eed$e7dd68b0$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <42764FC6.3010806@vanderbilt.edu>

Jan Verbesselt wrote:
> Dear all,
> 
> Is the restricted cubic spline function working properly in the glm model?
> 
> We used glm(y~rcs(x,5), family=binomial) but it seems that for some
> theoretical reasons the rcs, restricted cubic spline function can not be
> fitted by a glm function. Is this correct?
> 
> 
> Regards,
> Jan
> 
> ((Originally, we used lrm(y~ rcs(x,5)) but we couldn't find how to derive
> the AIC value of the fitted model. Is there a solution?))

The fit object from lrm has all the information you need to easily 
compute AIC.  Type names(fit object) to see what's there.

If you really needed glm (which you don't), Design wants you to use glmD.



-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Steven.Murdoch at cl.cam.ac.uk  Mon May  2 18:53:43 2005
From: Steven.Murdoch at cl.cam.ac.uk (Steven J. Murdoch)
Date: Mon, 2 May 2005 17:53:43 +0100
Subject: [R] xinch/yinch equivalent for log axis
Message-ID: <20050502165343.GA26558@cl.cam.ac.uk>

I would like to draw a vertical line from a given point, in user
coordinates, to x inches before from another point, also in user
coordinates. This is easy enough to do for linear scales, using code
based on xinch/yinch, but I do not know how to do this for logarithmic
scales.

This code shows an example of what I mean[1]:

split.screen(c(2,1))
screen(1)
# Linear scale, works fine
plot(1:100, cex=0.5, pch=19)
plotheight <- diff(par("usr")[3:4])
igap = 0.5 # intended gap, in inches
gap <- igap/par("pin")[2]*plotheight # gap in user units
lines(c(20,20),c(100,60+gap))
lines(c(20,20),c(60-gap,1))
points(20,60, pch=18)

screen(2)
# Logarithmic scale, point no longer centered
plot(1:100, cex=0.5, pch=19, log="y")
plotheight <- diff(par("usr")[3:4])
igap = 10
gap <- igap/par("pin")[2]*plotheight
lines(c(20,20),c(100,10+gap))
lines(c(20,20),c(10-gap,1))
points(20,10, pch=18)
close.screen(all=TRUE)

The top graph is as I want. The diamond is centered in the gap, and
the gap is 1 inch high (2*igap).

In bottom graph, using a log scale, the diamond is no longer centered
in the gap and there is a non-linear relationship between the gap
height in inches and igap.

I understand why this is happening, and this is why the xinch and
yinch functions raise a warning, but is there a way to handle it? For
example, is there a function which will return the user coordinate of
a point, x inches above a given point p (in user coordinates), for a
logarithmic y axis?

Thanks in advance,
Steven Murdoch. 

[1] I actually want to use this in a larger script, where I leave a
 gap in the axis where the median in. This needs to be small but
 legible to the eye, so that is why I am defining it in inches.
 The source code is at:
  http://www.cl.cam.ac.uk/users/sjm217/projects/graphics/fancyaxis.R

-- 
w: http://www.cl.cam.ac.uk/users/sjm217/



From gottfried.gruber at terminal.at  Mon May  2 19:06:21 2005
From: gottfried.gruber at terminal.at (Gottfried Gruber)
Date: Mon, 2 May 2005 19:06:21 +0200
Subject: [R] opimization problem
In-Reply-To: <200505020955.20279.ahenningsen@email.uni-kiel.de>
References: <200505011921.44723.gottfried.gruber@terminal.at>
	<200505020955.20279.ahenningsen@email.uni-kiel.de>
Message-ID: <200505021906.21994.gottfried.gruber@terminal.at>

Hi Arne,

i'm sorry - the additional restriction is 
w' * V * w <= j
where j is a [1x1] matrix.
i don't know how to incorporate this restriction in the lp-object.

would be grateful on any response,
thanks,
gg

On Monday 02 May 2005 09:55, Arne Henningsen wrote:
> Hi Gottfried,
>
> w' * V * w is not a restriction, because there is no equal sign.
> Do you mean w' * V * w = 1?
>
> Arne
>
> On Sunday 01 May 2005 19:21, Gottfried Gruber wrote:
> > hi,
> >
> > i want to execute the following opimization problem:
> > max r*w
> > s.t.:   w*z=1	# sum of w is 1
> > r, w are [nx1] vectors, z is a [nx1] vector consisting of 1
> > so far so good, works fine with lp
> >
> > the problem arises with the additional restriction
> > w' * V * w
> > where V is a [nxn] matrix
> > how can i include this restriction since w arises twice?
> >
> > thanks,
> > gg

-- 
---------------------------------------------------
Gottfried Gruber
mailto:gottfried.gruber at terminal.at
www: http://gogo.sehrsupa.net



From henric.nilsson at statisticon.se  Mon May  2 19:27:25 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Mon, 2 May 2005 19:27:25 +0200 (CEST)
Subject: [R] formula in fixed-effects part of GLMM
In-Reply-To: <42764642.6060705@stat.wisc.edu>
References: <42743416@webmail.ualberta.ca> <42764642.6060705@stat.wisc.edu>
Message-ID: <4993.10.0.10.126.1115054845.squirrel@poisson.statisticon.se>


On Mon, 2005-05-02, 17:24, Douglas Bates wrote:

> weihong wrote:
>> Can GLMM take formula derived from another object?
>>
>> foo <- glm (OVEN ~ h + h2, poisson, dataset)
>>
>> # ok
>> bar <- GLMM (OVEN ~ h + h2, poisson, dataset, random = list (yr = ~1))
>>
>> #error
>> bar <- GLMM (foo$formula, poisson, dataset, random = list (yr = ~1))
>> #Error in foo$("formula" + yr + 1) : invalid subscript type
>
> That won't work without some tweaking of the GLMM function.  In the
> 0.95-1 and later versions of the lme4 package the capabilities of GLMM
> have been folded in to the lmer function and you would need to fit that
> model as
>
>  lmer(OVEN ~ h + h2 + (1|yr), dataset, poisson)
>
> anyway.

I don't have access to the "White Book" right now (or R, for that matter),
but doesn't it say that something like the following works?

fit.lm <- lm(y ~ x)
fit.glm <- update(fit.lm, class = "glm")

But this isn't implemented in R, right? If one can make a wish, it would
be really nice being able to (using weihong's example):

bar <- update(foo, . ~ . + (1|yr), class = "lmer")


//Henric



From phhs80 at gmail.com  Mon May  2 20:02:06 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Mon, 2 May 2005 19:02:06 +0100
Subject: [R] User-defined random variable
In-Reply-To: <4274972B.9080000@uni-bayreuth.de>
References: <6ade6f6c05043008027c698dcd@mail.gmail.com>
	<x27jikthsy.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.58.0504301724230.12788@thorin.ci.tuwien.ac.at>
	<4274972B.9080000@uni-bayreuth.de>
Message-ID: <6ade6f6c05050211026ffcc3ac@mail.gmail.com>

On 5/1/05, Matthias Kohl <Matthias.Kohl at uni-bayreuth.de> wrote:
> >>>I would like to know whether it is possible with R to define a
> >>>discrete random variable different from the ones already defined
> >>>inside R and generate random numbers from that user-defined
> >>>distribution.
> >>>
> >>Yes. One generic way is to specify the quantile function (as in
> >>qpois() etc.) and do qfun(runif(N)).
> >
> >If the support discrete but also finite, you can also use sample(), e.g.
> >
> >  sample(myset, N, replace = TRUE, prob = myprob)
> 
> one can also use our R package "distr" to generate discrete random
> variables. The subsequent code provides a function which generates an
> object of class "DiscreteDistribution" based on a finite support "supp".
> If "prob" is missing all elements in "supp" are equally weighted.

Thanks you for all your helpful replies to my question.

Paul



From vograno at evafunds.com  Mon May  2 20:22:58 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 2 May 2005 11:22:58 -0700
Subject: [R] congratulations to the JGR developers
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A59E90E3@phost015.EVAFUNDS.intermedia.net>

Well put, Gabor!

P.S. Sorry for wasting the bandwidth. Just couldn't resist, so much true
it is. 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> Grothendieck
> Sent: Monday, May 02, 2005 8:06 AM
> To: Andy Bunn
> Cc: R-Help
> Subject: Re: [R] congratulations to the JGR developers
> 
> On 5/2/05, Andy Bunn <abunn at whrc.org> wrote:
> > > Just want to offer my congratulations to the JGR 
> developers as the 
> > > recepient of the 2005 Chambers Award.  Great job, guys!!
> > > http://stats.math.uni-augsburg.de/JGR/
> > 
> > This feels like the future of R to me. It's simple, powerful, and 
> > elegant just like R. As soon as the binary that works with 2.1 is 
> > released I'll use it exclusively on Linux and Windows. I'm 
> deeply impressed.
> 
> I have not tried JGR but regarding your three adjective 
> describing R, R is very powerful but I am not sure I would 
> characterize it as simple and elegant -- complex and 
> practical seem nearer to the mark to me.
> Some parts of R may be simple and elegant but when I think of 
> simple and elegant languages I think of ones that are 
> organized around a
> single concept like APL (arrays), Smalltalk (objects), etc.   
> The underlying
> Lisp roots of R may have a certain simplicity to them and S3 
> (though not
> S4) is relatively simple but not R as a whole.  On the other 
> hand, the fact that it is practical, powerful and free with a 
> broad set of builtin and addon functionality and has become a 
> de facto standard for statistical research have been 
> sufficient reason for me to do all my computing using R.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From itsme_410 at yahoo.com  Mon May  2 20:33:54 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Mon, 2 May 2005 11:33:54 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A9402@uswpmx00.merck.com>
Message-ID: <20050502183355.45605.qmail@web54507.mail.yahoo.com>

By the way, I just noticed that eigen(X) returns eigenvectors, at least two of
which are NaN's. 

Best wishes!

--- "Huntsinger, Reid" <reid_huntsinger at merck.com> wrote:

> When the matrix is symmetric and omega is not real, omega and its conjugate
> (= inverse) give the same eigenvalue, so you have a 2-dimensional
> eigenspace. R chooses a real basis of this, which is perfectly fine since
> it's not looking for circulant structure.
> 
> For example,
> 
> > m
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    2    3    3    2
> [2,]    2    1    2    3    3
> [3,]    3    2    1    2    3
> [4,]    3    3    2    1    2
> [5,]    2    3    3    2    1
> 
> > eigen(m)
> $values
> [1] 11.000000 -0.381966 -0.381966 -2.618034 -2.618034
> 
> $vectors
>           [,1]      [,2]       [,3]       [,4]      [,5]
> [1,] 0.4472136  0.000000 -0.6324555  0.6324555  0.000000
> [2,] 0.4472136  0.371748  0.5116673  0.1954395  0.601501
> [3,] 0.4472136 -0.601501 -0.1954395 -0.5116673  0.371748
> [4,] 0.4472136  0.601501 -0.1954395 -0.5116673 -0.371748
> [5,] 0.4472136 -0.371748  0.5116673  0.1954395 -0.601501
> 
> and you can match these columns up with the "canonical" eigenvectors
> exp(2*pi*1i*(0:4)*j/5) for j = 0,1,2,3,4. E.g.,
> 
> > Im(exp(2*pi*1i*(0:4)*3/5))
> [1]  0.0000000 -0.5877853  0.9510565 -0.9510565  0.5877853
> 
> which can be seen to be a scalar multiple of column 2. 
> 
> Reid Huntsinger
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Huntsinger, Reid
> Sent: Monday, May 02, 2005 10:43 AM
> To: 'Globe Trotter'; Rolf Turner
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] eigenvalues of a circulant matrix
> 
> 
> It's hard to argue against the fact that a real symmetric matrix has real
> eigenvalues. The eigenvalues of the circulant matrix with first row v are
> *polynomials* (not the roots of 1 themselves, unless as Rolf suggested you
> start with a vector with all zeros except one 1) in the roots of 1, with
> coefficients equal to the entries in v. This is the finite Fourier transform
> of v, by the way, and takes real values when the coefficients are real and
> symmetric, ie when the matrix is symmetric.
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Globe Trotter
> Sent: Monday, May 02, 2005 10:23 AM
> To: Rolf Turner
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] eigenvalues of a circulant matrix
> 
> 
> 
> --- Rolf Turner <rolf at math.unb.ca> wrote:
> > I just Googled around a bit and found definitions of Toeplitz and
> > circulant matrices as follows:
> > 
> > A Toeplitz matrix is any n x n matrix with values constant along each
> > (top-left to lower-right) diagonal.  matrix has the form
> > 
> > 	a_0 a_1 .   .  .   .  ... a_{n-1}
> > 	a_{-1} a_0 a_1        ... a_{n-2}
> > 	a_{-2} a_{-1} a_0 a_1 ...    .
> > 	   .      .    .   .   .     .
> > 	   .      .    .   .   .     .
> > 	   .      .    .   .   .     .
> > 	a_{-(n-1)} a_{-(n-2)} ... a_1 a_0
> > 
> > (A Toeplitz matrix ***may*** be symmetric.)
> 
> Agreed. As may a circulant matrix if a_i = a_{p-i+2}
> 
> > 
> > A circulant matrix is an n x n matrix whose rows are composed of
> > cyclically shifted versions of a length-n vector. For example, the
> > circulant matrix on the vector (1, 2, 3, 4)  is
> > 
> > 	4 1 2 3
> > 	3 4 1 2
> > 	2 3 4 1
> > 	1 2 3 4
> > 
> > So circulant matrices are a special case of Toeplitz matrices.
> > However a circulant matrix cannot be symmetric.
> > 
> > The eigenvalues of the forgoing circulant matrix are 10, 2 + 2i,
> > 2 - 2i, and 2 --- certainly not roots of unity. 
> 
> The eigenvalues are 4+1*omega+2*omega^2+3*omega^3.
> omega=cos(2*pi*k/4)+isin(2*pi*k/4) as k ranges over 1, 2, 3, 4, so the above
> holds.
> 
>  Bellman may have
> > been talking about the particular (important) case of a circulant
> > matrix where the vector from which it is constructed is a canonical
> > basis vector e_i with a 1 in the i-th slot and zeroes elsewhere.
> 
> No, that is not true: his result can be verified for any circulant matrix,
> directly.
> 
> > Such a matrix is in fact a unitary matrix (operator), whence its
> > spectrum is contained in the unit circle; its eigenvalues are indeed
> > n-th roots of unity.
> > 
> > Such matrices are related to the unilateral shift operator on
> > Hilbert space (which is the ``primordial'' Toeplitz operator).
> > It arises as multiplication by z on H^2 --- the ``analytic''
> > elements of L^2 of the unit circle.
> > 
> > On (infinite dimensional) Hilbert space the unilateral shift
> > looks like
> > 
> > 	0 0 0 0 0 ...
> > 	1 0 0 0 0 ...
> > 	0 1 0 0 0 ...
> > 	0 0 1 0 0 ...
> >         . . . . . ...
> >         . . . . . ...
> > 
> > which maps e_0 to e_1, e_1 to e_2, e_2 to e_3, ...  on and on
> > forever.  On (say) 4 dimensional space we can have a unilateral
> > shift operator/matrix
> > 
> > 	0 0 0 0
> > 	1 0 0 0
> > 	0 1 0 0
> > 	0 0 1 0
> > 
> > but its range is a 3 dimensional subspace (e_4 gets ``killed'').
> > 
> > The ``corresponding'' circulant matrix is
> > 
> > 	0 0 0 1
> > 	1 0 0 0
> > 	0 1 0 0
> > 	0 0 1 0
> > 
> > which is an onto mapping --- e_4 gets sent back to e_1.
> > 
> > I hope this clears up some of the confusion.
> > 
> > 				cheers,
> > 
> > 					Rolf Turner
> > 					rolf at math.unb.ca
> 
> Many thanks and best wishes!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ----------------------------------------------------------------------------
> --
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
> Banyu) that may be confidential, proprietary copyrighted and/or legally
> privileged. It is intended solely for the use of the individual or entity
> named on this message.  If you are not the intended recipient, and have
> received this message in error, please notify us immediately by reply e-mail
> and then delete it from your system.
> ----------------------------------------------------------------------------
> --
> 
> 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
> Banyu) that may be confidential, proprietary copyrighted and/or legally
> privileged. It is intended solely for the use of the individual or entity
> named on this message.  If you are not the intended recipient, and have
> received this message in error, please notify us immediately by reply e-mail
> and then delete it from your system.
>
------------------------------------------------------------------------------
>



From bates at stat.wisc.edu  Mon May  2 20:57:19 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 02 May 2005 13:57:19 -0500
Subject: [R] formula in fixed-effects part of GLMM
In-Reply-To: <4993.10.0.10.126.1115054845.squirrel@poisson.statisticon.se>
References: <42743416@webmail.ualberta.ca> <42764642.6060705@stat.wisc.edu>
	<4993.10.0.10.126.1115054845.squirrel@poisson.statisticon.se>
Message-ID: <4276780F.6010606@stat.wisc.edu>

Henric Nilsson wrote:
> On Mon, 2005-05-02, 17:24, Douglas Bates wrote:
> 
> 
>>weihong wrote:
>>
>>>Can GLMM take formula derived from another object?
>>>
>>>foo <- glm (OVEN ~ h + h2, poisson, dataset)
>>>
>>># ok
>>>bar <- GLMM (OVEN ~ h + h2, poisson, dataset, random = list (yr = ~1))
>>>
>>>#error
>>>bar <- GLMM (foo$formula, poisson, dataset, random = list (yr = ~1))
>>>#Error in foo$("formula" + yr + 1) : invalid subscript type
>>
>>That won't work without some tweaking of the GLMM function.  In the
>>0.95-1 and later versions of the lme4 package the capabilities of GLMM
>>have been folded in to the lmer function and you would need to fit that
>>model as
>>
>> lmer(OVEN ~ h + h2 + (1|yr), dataset, poisson)
>>
>>anyway.
> 
> 
> I don't have access to the "White Book" right now (or R, for that matter),
> but doesn't it say that something like the following works?
> 
> fit.lm <- lm(y ~ x)
> fit.glm <- update(fit.lm, class = "glm")
> 
> But this isn't implemented in R, right? If one can make a wish, it would
> be really nice being able to (using weihong's example):
> 
> bar <- update(foo, . ~ . + (1|yr), class = "lmer")
> 
> 
> //Henric

It's not very convenient from the point of view of dispatch.  The
dispatch rules are such that the update method for the lm or glm class
would need to be aware of the lmer class to be able to do this.  The lm
and glm classes predate lmer by a long time.



From andy_liaw at merck.com  Mon May  2 21:06:21 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 2 May 2005 15:06:21 -0400
Subject: [R] formula in fixed-effects part of GLMM
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076EAF@usctmx1106.merck.com>

> From: Douglas Bates
> 
> Henric Nilsson wrote:
> > On Mon, 2005-05-02, 17:24, Douglas Bates wrote:
> > 
> > 
> >>weihong wrote:
> >>
> >>>Can GLMM take formula derived from another object?
> >>>
> >>>foo <- glm (OVEN ~ h + h2, poisson, dataset)
> >>>
> >>># ok
> >>>bar <- GLMM (OVEN ~ h + h2, poisson, dataset, random = 
> list (yr = ~1))
> >>>
> >>>#error
> >>>bar <- GLMM (foo$formula, poisson, dataset, random = list 
> (yr = ~1))
> >>>#Error in foo$("formula" + yr + 1) : invalid subscript type
> >>
> >>That won't work without some tweaking of the GLMM function.  In the
> >>0.95-1 and later versions of the lme4 package the 
> capabilities of GLMM
> >>have been folded in to the lmer function and you would need 
> to fit that
> >>model as
> >>
> >> lmer(OVEN ~ h + h2 + (1|yr), dataset, poisson)
> >>
> >>anyway.
> > 
> > 
> > I don't have access to the "White Book" right now (or R, 
> for that matter),
> > but doesn't it say that something like the following works?
> > 
> > fit.lm <- lm(y ~ x)
> > fit.glm <- update(fit.lm, class = "glm")
> > 
> > But this isn't implemented in R, right? If one can make a 
> wish, it would
> > be really nice being able to (using weihong's example):
> > 
> > bar <- update(foo, . ~ . + (1|yr), class = "lmer")
> > 
> > 
> > //Henric
> 
> It's not very convenient from the point of view of dispatch.  The
> dispatch rules are such that the update method for the lm or glm class
> would need to be aware of the lmer class to be able to do 
> this.  The lm
> and glm classes predate lmer by a long time.

In that case, wouldn't lmer also need to be in "stats" (or somewhere "stats"
can import from)?

Best,
Andy



From jimmcloughlin at earthlink.net  Mon May  2 22:06:28 2005
From: jimmcloughlin at earthlink.net (Jim McLoughlin)
Date: Mon, 2 May 2005 13:06:28 -0700
Subject: [R] opimization problem
In-Reply-To: <200505021906.21994.gottfried.gruber@terminal.at>
References: <200505011921.44723.gottfried.gruber@terminal.at>
	<200505020955.20279.ahenningsen@email.uni-kiel.de>
	<200505021906.21994.gottfried.gruber@terminal.at>
Message-ID: <d8c1a35441a9b59bae191967008da5e1@earthlink.net>

The form of the problem looks like you are trying to do a mean-variance 
portfolio optimization.  If that is the case, you should not be dealing 
with variance as a restriction, but as part of the objective function:

max (r'*w - rho*w'*V*w)
s.t. sum(w) == 1

where rho is a risk aversion parameter.

You can solve this as a quadratic programming problem using either 1) 
solve.QP from the quadprog package; 2) portfolio.optim in package 
tseries

see http://tolstoy.newcastle.edu.au/R/help/05/01/10505.html for details 
on how to use the two.


>> On Sunday 01 May 2005 19:21, Gottfried Gruber wrote:
>>> hi,
>>>
>>> i want to execute the following opimization problem:
>>> max r*w
>>> s.t.:   w*z=1	# sum of w is 1
>>> r, w are [nx1] vectors, z is a [nx1] vector consisting of 1
>>> so far so good, works fine with lp
>>>
>>> the problem arises with the additional restriction
>>> w' * V * w
>>> where V is a [nxn] matrix
>>> how can i include this restriction since w arises twice?



From wowen at richmond.edu  Mon May  2 22:20:45 2005
From: wowen at richmond.edu (Owen, Jason)
Date: Mon, 2 May 2005 16:20:45 -0400
Subject: [R] Reading in a dataset with uneven variable lengths
Message-ID: <0F98C8BA43C00C42AFFBE000DA9DDB23015D1A0C@exchange.richmond.edu>

Suppose I have a text file that I want to read into R like the
following:

X	Y
649	699
657	891
714	632
849	727
721	597
791	868
874	652
405	978
	733
	549
	790

This is a simple example -- I could have a huge file with many
columns of unequal lengths.

What is the best way to do it?  I can't see how a data frame
can be used.  I checked the FAQ and did a web search on the topic
but I came up empty.

Jason



From andy_liaw at merck.com  Mon May  2 22:24:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 2 May 2005 16:24:29 -0400
Subject: [R] Reading in a dataset with uneven variable lengths
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076EB4@usctmx1106.merck.com>

One possibility is to use read.fwf().

Andy

> From: Owen, Jason
> 
> Suppose I have a text file that I want to read into R like the
> following:
> 
> X	Y
> 649	699
> 657	891
> 714	632
> 849	727
> 721	597
> 791	868
> 874	652
> 405	978
> 	733
> 	549
> 	790
> 
> This is a simple example -- I could have a huge file with many
> columns of unequal lengths.
> 
> What is the best way to do it?  I can't see how a data frame
> can be used.  I checked the FAQ and did a web search on the topic
> but I came up empty.
> 
> Jason
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From cahn at email.unc.edu  Mon May  2 22:31:42 2005
From: cahn at email.unc.edu (Chaehyung Ahn)
Date: Mon, 2 May 2005 16:31:42 -0400 (EDT)
Subject: [R] making a log file with error messages?
Message-ID: <Pine.LNX.4.44+UNC.0505021607050.22176-100000@bc05-n14.isis.unc.edu>

Dear all:

Is there anyone who would help me to generate a log file reporting error
messages?

I can see error messages on the screen, but I would like to report them
as a file.

thanks

Sincerely,

cahn

-----------------------------------------------
ChaeHyung Ahn (cahn) Ph.D.
Department of Biostatistics, School of Public Health
CB #7420, University of North Carolina
Chapel Hill, NC  27599-7420
phone: 919-966-7284
home: http://cyworld.nate.com/cahn88



From itsme_410 at yahoo.com  Mon May  2 22:50:23 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Mon, 2 May 2005 13:50:23 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A9403@uswpmx00.merck.com>
Message-ID: <20050502205024.87590.qmail@web54509.mail.yahoo.com>

The example that I submitted earlier in the day. Would you like me to send
again?

Thanks!


--- "Huntsinger, Reid" <reid_huntsinger at merck.com> wrote:

> For which X?
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: Globe Trotter [mailto:itsme_410 at yahoo.com] 
> Sent: Monday, May 02, 2005 2:34 PM
> To: Huntsinger, Reid; Rolf Turner
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] eigenvalues of a circulant matrix
> 
> 
> By the way, I just noticed that eigen(X) returns eigenvectors, at least two
> of
> which are NaN's. 
> 
> Best wishes!
> 
> --- "Huntsinger, Reid" <reid_huntsinger at merck.com> wrote:
> 
> > When the matrix is symmetric and omega is not real, omega and its
> conjugate
> > (= inverse) give the same eigenvalue, so you have a 2-dimensional
> > eigenspace. R chooses a real basis of this, which is perfectly fine since
> > it's not looking for circulant structure.
> > 
> > For example,
> > 
> > > m
> >      [,1] [,2] [,3] [,4] [,5]
> > [1,]    1    2    3    3    2
> > [2,]    2    1    2    3    3
> > [3,]    3    2    1    2    3
> > [4,]    3    3    2    1    2
> > [5,]    2    3    3    2    1
> > 
> > > eigen(m)
> > $values
> > [1] 11.000000 -0.381966 -0.381966 -2.618034 -2.618034
> > 
> > $vectors
> >           [,1]      [,2]       [,3]       [,4]      [,5]
> > [1,] 0.4472136  0.000000 -0.6324555  0.6324555  0.000000
> > [2,] 0.4472136  0.371748  0.5116673  0.1954395  0.601501
> > [3,] 0.4472136 -0.601501 -0.1954395 -0.5116673  0.371748
> > [4,] 0.4472136  0.601501 -0.1954395 -0.5116673 -0.371748
> > [5,] 0.4472136 -0.371748  0.5116673  0.1954395 -0.601501
> > 
> > and you can match these columns up with the "canonical" eigenvectors
> > exp(2*pi*1i*(0:4)*j/5) for j = 0,1,2,3,4. E.g.,
> > 
> > > Im(exp(2*pi*1i*(0:4)*3/5))
> > [1]  0.0000000 -0.5877853  0.9510565 -0.9510565  0.5877853
> > 
> > which can be seen to be a scalar multiple of column 2. 
> > 
> > Reid Huntsinger
> > 
> > Reid Huntsinger
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Huntsinger, Reid
> > Sent: Monday, May 02, 2005 10:43 AM
> > To: 'Globe Trotter'; Rolf Turner
> > Cc: r-help at stat.math.ethz.ch
> > Subject: RE: [R] eigenvalues of a circulant matrix
> > 
> > 
> > It's hard to argue against the fact that a real symmetric matrix has real
> > eigenvalues. The eigenvalues of the circulant matrix with first row v are
> > *polynomials* (not the roots of 1 themselves, unless as Rolf suggested you
> > start with a vector with all zeros except one 1) in the roots of 1, with
> > coefficients equal to the entries in v. This is the finite Fourier
> transform
> > of v, by the way, and takes real values when the coefficients are real and
> > symmetric, ie when the matrix is symmetric.
> > 
> > Reid Huntsinger
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Globe Trotter
> > Sent: Monday, May 02, 2005 10:23 AM
> > To: Rolf Turner
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] eigenvalues of a circulant matrix
> > 
> > 
> > 
> > --- Rolf Turner <rolf at math.unb.ca> wrote:
> > > I just Googled around a bit and found definitions of Toeplitz and
> > > circulant matrices as follows:
> > > 
> > > A Toeplitz matrix is any n x n matrix with values constant along each
> > > (top-left to lower-right) diagonal.  matrix has the form
> > > 
> > > 	a_0 a_1 .   .  .   .  ... a_{n-1}
> > > 	a_{-1} a_0 a_1        ... a_{n-2}
> > > 	a_{-2} a_{-1} a_0 a_1 ...    .
> > > 	   .      .    .   .   .     .
> > > 	   .      .    .   .   .     .
> > > 	   .      .    .   .   .     .
> > > 	a_{-(n-1)} a_{-(n-2)} ... a_1 a_0
> > > 
> > > (A Toeplitz matrix ***may*** be symmetric.)
> > 
> > Agreed. As may a circulant matrix if a_i = a_{p-i+2}
> > 
> > > 
> > > A circulant matrix is an n x n matrix whose rows are composed of
> > > cyclically shifted versions of a length-n vector. For example, the
> > > circulant matrix on the vector (1, 2, 3, 4)  is
> > > 
> > > 	4 1 2 3
> > > 	3 4 1 2
> > > 	2 3 4 1
> > > 	1 2 3 4
> > > 
> > > So circulant matrices are a special case of Toeplitz matrices.
> > > However a circulant matrix cannot be symmetric.
> > > 
> > > The eigenvalues of the forgoing circulant matrix are 10, 2 + 2i,
> > > 2 - 2i, and 2 --- certainly not roots of unity. 
> > 
> > The eigenvalues are 4+1*omega+2*omega^2+3*omega^3.
> > omega=cos(2*pi*k/4)+isin(2*pi*k/4) as k ranges over 1, 2, 3, 4, so the
> above
> > holds.
> > 
> >  Bellman may have
> > > been talking about the particular (important) case of a circulant
> > > matrix where the vector from which it is constructed is a canonical
> > > basis vector e_i with a 1 in the i-th slot and zeroes elsewhere.
> > 
> > No, that is not true: his result can be verified for any circulant matrix,
> > directly.
> > 
> > > Such a matrix is in fact a unitary matrix (operator), whence its
> > > spectrum is contained in the unit circle; its eigenvalues are indeed
> > > n-th roots of unity.
> > > 
> > > Such matrices are related to the unilateral shift operator on
> > > Hilbert space (which is the ``primordial'' Toeplitz operator).
> > > It arises as multiplication by z on H^2 --- the ``analytic''
> > > elements of L^2 of the unit circle.
> > > 
> > > On (infinite dimensional) Hilbert space the unilateral shift
> > > looks like
> > > 
> > > 	0 0 0 0 0 ...
> > > 	1 0 0 0 0 ...
> > > 	0 1 0 0 0 ...
> > > 	0 0 1 0 0 ...
> > >         . . . . . ...
> > >         . . . . . ...
> > > 
> > > which maps e_0 to e_1, e_1 to e_2, e_2 to e_3, ...  on and on
> > > forever.  On (say) 4 dimensional space we can have a unilateral
> > > shift operator/matrix
> > > 
> > > 	0 0 0 0
> > > 	1 0 0 0
> > > 	0 1 0 0
> > > 	0 0 1 0
> > > 
> > > but its range is a 3 dimensional subspace (e_4 gets ``killed'').
> > > 
> > > The ``corresponding'' circulant matrix is
> > > 
> > > 	0 0 0 1
> > > 	1 0 0 0
> > > 	0 1 0 0
> > > 	0 0 1 0
> > > 
> > > which is an onto mapping --- e_4 gets sent back to e_1.
> > > 
> > > I hope this clears up some of the confusion.
> > > 
> > > 				cheers,
> > > 
> > > 					Rolf Turner
> > > 					rolf at math.unb.ca
> > 
> > Many thanks and best wishes!
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> >
> ----------------------------------------------------------------------------
> > --
> > Notice:  This e-mail message, together with any attachments, contains
> > information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> > Jersey, USA 08889), and/or its affiliates (which may be known outside the
> > United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
> > Banyu) that may be confidential, proprietary copyrighted and/or legally
> > privileged. It is intended solely for the use of the individual or entity
> > named on this message.  If you are not the intended recipient, and have
> > received this message in error, please notify us immediately by reply
> e-mail
> > and then delete it from your system.
> >
> ----------------------------------------------------------------------------
> > --
> > 
> > 
> > 
> > 
> >
> ----------------------------------------------------------------------------
> --
> > Notice:  This e-mail message, together with any attachments, contains
> > information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> > Jersey, USA 08889), and/or its affiliates (which may be known outside the
> > United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
> > Banyu) that may be confidential, proprietary copyrighted and/or legally
> > privileged. It is intended solely for the use of the individual or entity
> > named on this message.  If you are not the intended recipient, and have
> > received this message in error, please notify us immediately by reply
> e-mail
> > and then delete it from your system.
> >
> ----------------------------------------------------------------------------
> --
> > 
> 
> 
> __________________________________________________
> Do You Yahoo!?

> http://mail.yahoo.com 
> 
> 
> 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From christoph.lehmann at gmx.ch  Tue May  3 00:29:32 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 03 May 2005 00:29:32 +0200
Subject: [R] summary(as.factor(x) - force to not sort the result according
 factor levels
Message-ID: <4276A9CC.6030502@gmx.ch>

Hi
The result of a summary(as.factor(x)) (see example below) call is sorted 
according to the factor level. How can I get the result not sorted but 
in the original order of the levels in x?


 > test <- c(120402, 120402, 120402, 1323, 1323,200393, 200393, 200393, 
200393, 200393)
 > summary(as.factor(test))
   1323 120402 200393
      2      3      5

I need:
120402 1323 200393
      3    2      5

thanks for a hint

christoph



From Ted.Harding at nessie.mcc.ac.uk  Mon May  2 23:27:43 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 02 May 2005 22:27:43 +0100 (BST)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <XFMail.050502163700.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050502222743.Ted.Harding@nessie.mcc.ac.uk>

On 02-May-05 Ted Harding wrote:
> On 02-May-05 Rolf Turner wrote:
>> I just Googled around a bit and found definitions of Toeplitz and
>> circulant matrices as follows:
>> [...]
>> A circulant matrix is an n x n matrix whose rows are composed of
>> cyclically shifted versions of a length-n vector. For example, the
>> circulant matrix on the vector (1, 2, 3, 4)  is
>> 
>>       4 1 2 3
>>       3 4 1 2
>>       2 3 4 1
>>       1 2 3 4
>> 
>> So circulant matrices are a special case of Toeplitz matrices.
>> However a circulant matrix cannot be symmetric.
> 
> I suspect the confusion may lie in what's meant by "cyclically
> shifted". In Rolf's example above, each row is shifted right by 1
> and the one that falls off the end is put at the beginning. This
> cannot be symmetric for general values in the fist row.
> 
> However, if you shift left instead, then you get
> 
>         4 1 2 3
>         1 2 3 4
>         2 3 4 1
>         3 4 1 2
> 
> and this *is* symmetric (and indeed will always be so, for
> general values in the first row).

I just had a look at ?toeplitz

(We should have done that earlier!)

toeplitz                package:stats                R Documentation
Form Symmetric Toeplitz Matrix
     *********

Description:
     Forms a symmetric Toeplitz matrix given its first row.
             *********
[...]
Examples:

     x <- 1:5
     toeplitz (x)

> x <- 1:5
>      toeplitz (x)
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    2    3    4    5
[2,]    2    1    2    3    4
[3,]    3    2    1    2    3
[4,]    4    3    2    1    2
[5,]    5    4    3    2    1

Since "Globe Trotter's" construction was

  Y<-toeplitz(x)

it's not surprising what he got (and it *certainly* wasn't
a circulant!!!).

Everybody barking up the wring tree here!

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 02-May-05                                       Time: 22:27:32
------------------------------ XFMail ------------------------------



From tplate at acm.org  Mon May  2 23:38:17 2005
From: tplate at acm.org (Tony Plate)
Date: Mon, 02 May 2005 15:38:17 -0600
Subject: [R] summary(as.factor(x) - force to not sort the result according
	factor levels
In-Reply-To: <4276A9CC.6030502@gmx.ch>
References: <4276A9CC.6030502@gmx.ch>
Message-ID: <42769DC9.5040507@acm.org>

Christoph Lehmann wrote:
> Hi
> The result of a summary(as.factor(x)) (see example below) call is sorted 
> according to the factor level. How can I get the result not sorted but 
> in the original order of the levels in x?

by creating the factor with the levels in the order you want:

 > test <- c(120402, 120402, 120402, 1323, 1323,200393, 200393, 200393, 
200393, 200393)
 > summary(factor(test, levels=unique(test)))
120402   1323 200393
      3      2      5



From OlsenN at pac.dfo-mpo.gc.ca  Mon May  2 23:40:19 2005
From: OlsenN at pac.dfo-mpo.gc.ca (OlsenN@pac.dfo-mpo.gc.ca)
Date: Mon, 2 May 2005 14:40:19 -0700 
Subject: [R] making a log file with error messages?
Message-ID: <7CBBD627E4E688499349A5D11D07831602ECB6AA@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>

One way is to define your own function to use as the "error" option value.
E.g.

my.error.fun <- function() {
   cat(geterrmessage(), file="rerr.txt", append=T)
}

then,

options("error"=my.error.fun)

Norm

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chaehyung Ahn
Sent: Monday, May 02, 2005 1:32 PM
To: r-help at stat.math.ethz.ch
Subject: [R] making a log file with error messages?

Dear all:

Is there anyone who would help me to generate a log file reporting error
messages?

I can see error messages on the screen, but I would like to report them as a
file.

thanks

Sincerely,

cahn

-----------------------------------------------
ChaeHyung Ahn (cahn) Ph.D.
Department of Biostatistics, School of Public Health CB #7420, University of
North Carolina Chapel Hill, NC  27599-7420
phone: 919-966-7284
home: http://cyworld.nate.com/cahn88

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Scott.Waichler at pnl.gov  Mon May  2 23:43:02 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Mon, 02 May 2005 14:43:02 -0700
Subject: [R] Trying to understand kpss.test() in tseries package
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7E7A@pnlmse35.pnl.gov>


I'm trying to understand how to use kpss.test() properly.  If I have a
level stationary series like rnorm() in the help page, shouldn't I get a
small p-value with the null hypothesis set to "Trend"?  The (condensed)
output from kpss.test() for the two possible null hypotheses is given
below.  I don't see any significant difference between these results.  

> x <- rnorm(1000)  # is level stationary
> kpss.test(x, null="Level")
        KPSS Test for Level Stationarity
KPSS Level = 0.0638, Truncation lag parameter = 7, p-value = 0.1
Warning:  p-value greater than printed p-value

> kpss.test(x, null="Trend")
        KPSS Test for Trend Stationarity
KPSS Trend = 0.0275, Truncation lag parameter = 7, p-value = 0.1
Warning:  p-value greater than printed p-value

I can't get the original reference easily.

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From reid_huntsinger at merck.com  Tue May  3 00:24:15 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 2 May 2005 18:24:15 -0400
Subject: [R] eigenvalues of a circulant matrix
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9405@uswpmx00.merck.com>

The construction was

y<-x[c(109:1,2:108)]

so y is symmetric in the sense of the usual way of writing a function on
integers mod n as a vector with 1-based indexing. I.e., y[i+1] = y[n-(i+1)]
for i=0,1,...,n-1. So the assignment

Z <- toeplitz(y)

*does* create a symmetric circulant matrix. It is diagonalizable but does
not have distinct eigenvalues, hence the eigenspaces may be more than
one-dimensional, so you can't just pick a unit vector and call it "the"
eigenvector for that eigenvalue. You choose a basis for each eigenspace. R
detects the symmetry:

...
symmetric: if `TRUE', the matrix is assumed to be symmetric (or
          Hermitian if complex) and only its lower triangle is used. If
          `symmetric' is not specified, the matrix is inspected for
          symmetry.

(from help(eigen))
and knows that computations can be done with real arithmetic.

As for why you get NaN, you should submit --along with your example--
details of your platform (machine, R version, how R was built and installed,
etc).

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Ted.Harding at nessie.mcc.ac.uk
Sent: Monday, May 02, 2005 5:28 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] eigenvalues of a circulant matrix


On 02-May-05 Ted Harding wrote:
> On 02-May-05 Rolf Turner wrote:
>> I just Googled around a bit and found definitions of Toeplitz and
>> circulant matrices as follows:
>> [...]
>> A circulant matrix is an n x n matrix whose rows are composed of
>> cyclically shifted versions of a length-n vector. For example, the
>> circulant matrix on the vector (1, 2, 3, 4)  is
>> 
>>       4 1 2 3
>>       3 4 1 2
>>       2 3 4 1
>>       1 2 3 4
>> 
>> So circulant matrices are a special case of Toeplitz matrices.
>> However a circulant matrix cannot be symmetric.
> 
> I suspect the confusion may lie in what's meant by "cyclically
> shifted". In Rolf's example above, each row is shifted right by 1
> and the one that falls off the end is put at the beginning. This
> cannot be symmetric for general values in the fist row.
> 
> However, if you shift left instead, then you get
> 
>         4 1 2 3
>         1 2 3 4
>         2 3 4 1
>         3 4 1 2
> 
> and this *is* symmetric (and indeed will always be so, for
> general values in the first row).

I just had a look at ?toeplitz

(We should have done that earlier!)

toeplitz                package:stats                R Documentation
Form Symmetric Toeplitz Matrix
     *********

Description:
     Forms a symmetric Toeplitz matrix given its first row.
             *********
[...]
Examples:

     x <- 1:5
     toeplitz (x)

> x <- 1:5
>      toeplitz (x)
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    2    3    4    5
[2,]    2    1    2    3    4
[3,]    3    2    1    2    3
[4,]    4    3    2    1    2
[5,]    5    4    3    2    1

Since "Globe Trotter's" construction was

  Y<-toeplitz(x)

it's not surprising what he got (and it *certainly* wasn't
a circulant!!!).

Everybody barking up the wring tree here!

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 02-May-05                                       Time: 22:27:32
------------------------------ XFMail ------------------------------

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Tue May  3 00:32:41 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 02 May 2005 15:32:41 -0700
Subject: [R] summary(as.factor(x) - force to not sort the result according
	factor levels
In-Reply-To: <4276A9CC.6030502@gmx.ch>
References: <4276A9CC.6030502@gmx.ch>
Message-ID: <4276AA89.3080900@pdf.com>



Christoph Lehmann wrote on 5/2/2005 3:29 PM:
> Hi
> The result of a summary(as.factor(x)) (see example below) call is sorted 
> according to the factor level. How can I get the result not sorted but 
> in the original order of the levels in x?
> 
> 
>  > test <- c(120402, 120402, 120402, 1323, 1323,200393, 200393, 200393, 
> 200393, 200393)
>  > summary(as.factor(test))
>   1323 120402 200393
>      2      3      5
> 
> I need:
> 120402 1323 200393
>      3    2      5
> 
> thanks for a hint
> 
> christoph


Use the levels argument of ?factor.

summary(factor(test, levels = unique(test)))

--sundar



From gerifalte28 at hotmail.com  Tue May  3 00:56:32 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 02 May 2005 22:56:32 +0000
Subject: [R] Reading in a dataset with uneven variable lengths
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076EB4@usctmx1106.merck.com>
Message-ID: <BAY103-F37416C2A91D44CF4145E42A6270@phx.gbl>

Is it OK to have NA's? If so you can use read.table(,fill=TRUE) or 
read.delim().  Notice that the default in the second option is fill=TRUE.

Cheers

Francisco

>From: "Liaw, Andy" <andy_liaw at merck.com>
>To: "'Owen, Jason'" <wowen at richmond.edu>, r-help at stat.math.ethz.ch
>Subject: RE: [R] Reading in a dataset with uneven variable lengths
>Date: Mon, 2 May 2005 16:24:29 -0400
>
>One possibility is to use read.fwf().
>
>Andy
>
> > From: Owen, Jason
> >
> > Suppose I have a text file that I want to read into R like the
> > following:
> >
> > X	Y
> > 649	699
> > 657	891
> > 714	632
> > 849	727
> > 721	597
> > 791	868
> > 874	652
> > 405	978
> > 	733
> > 	549
> > 	790
> >
> > This is a simple example -- I could have a huge file with many
> > columns of unequal lengths.
> >
> > What is the best way to do it?  I can't see how a data frame
> > can be used.  I checked the FAQ and did a web search on the topic
> > but I came up empty.
> >
> > Jason
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From huihan at yahoo-inc.com  Tue May  3 01:07:55 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Mon, 02 May 2005 16:07:55 -0700
Subject: [R] plot
Message-ID: <4276B2CB.30200@yahoo-inc.com>

Hi,

I want to plot two sets of (x, y) in the same window, that is, plot of 
(x1,y1) and plot of (x2, y2) appear in the same window, so that I can 
compare the two plots.
Can I get any help from you on how to do that?

Thanks!
Hui



From gunter.berton at gene.com  Tue May  3 01:19:41 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 2 May 2005 16:19:41 -0700
Subject: [R] plot
In-Reply-To: <4276B2CB.30200@yahoo-inc.com>
Message-ID: <200505022319.j42NJf9k020528@meitner.gene.com>

?points or
?xyplot in lattice package for a possibly better alternative


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hui Han
> Sent: Monday, May 02, 2005 4:08 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] plot
> 
> Hi,
> 
> I want to plot two sets of (x, y) in the same window, that 
> is, plot of 
> (x1,y1) and plot of (x2, y2) appear in the same window, so that I can 
> compare the two plots.
> Can I get any help from you on how to do that?
> 
> Thanks!
> Hui
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From huihan at yahoo-inc.com  Tue May  3 01:24:13 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Mon, 02 May 2005 16:24:13 -0700
Subject: [R] plot
In-Reply-To: <200505022319.j42NJf9k020528@meitner.gene.com>
References: <200505022319.j42NJf9k020528@meitner.gene.com>
Message-ID: <4276B69D.4000803@yahoo-inc.com>

Thanks a lot!

Hui

Berton Gunter wrote:

>?points or
>?xyplot in lattice package for a possibly better alternative
>
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
> 
> 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hui Han
>>Sent: Monday, May 02, 2005 4:08 PM
>>To: R-help at stat.math.ethz.ch
>>Subject: [R] plot
>>
>>Hi,
>>
>>I want to plot two sets of (x, y) in the same window, that 
>>is, plot of 
>>(x1,y1) and plot of (x2, y2) appear in the same window, so that I can 
>>compare the two plots.
>>Can I get any help from you on how to do that?
>>
>>Thanks!
>>Hui
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>
>
>
>  
>



From Rikki.Dunsmore at noaa.gov  Tue May  3 01:45:20 2005
From: Rikki.Dunsmore at noaa.gov (Rikki Dunsmore)
Date: Mon, 02 May 2005 16:45:20 -0700
Subject: [R] Nonparametric Tukey-type multiple comparisons "Nemenyi" test
Message-ID: <4276BB90.7090706@noaa.gov>

I am trying to do a Nonparametric Tukey-type multiple comparison 
post-hoc test to determine which groups are significantly different.  I 
have read the dialogue on this topic from the R-help, and am still not 
clear why no statistical packages include this test as an option?  Is it 
not an appropriate test to conduct on non-normally distributed data?  Is 
the only option to calculate it by hand using the (Zar 1996) formula?

Thank you in advance for your help. 

-- 
Rikki Grober- Dunsmore
National Marine Fisheries Service
National Marine Protected Areas Center
110 Shaffer Rd.
Santa Cruz, CA 95060
831-420-3991 


Unless someone like you,
Cares a whole awful lot,
Nothing is going to get better,
It's not.
       - The Lorax, by Dr. Seuss, 1971



From Gregor.Gorjanc at bfro.uni-lj.si  Tue May  3 01:46:22 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Tue, 3 May 2005 01:46:22 +0200
Subject: [R] "Special" characters in URI
Message-ID: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>

Hello!

I am crossposting this to R-help and BioC, since it is relevant to both
groups. 

I wrote a wrapper for Entrez search utility (link for this is provided bellow), 
which can add some new search functionality to existing code in Bioconductor's
package 'annotate'*.
 
http://eutils.ncbi.nlm.nih.gov/entrez/query/static/esearch_help.html

Entrez search utuility returns a XML document but I have a problem to
use URI to retrieve that file, since URI can also contain characters,
which should not be there according to 

http://www.faqs.org/rfcs/rfc2396.html

I encountered problems with "[" and "]" as well as with space characters.
However there might also be a problem with others i.e. reserved characters
in URI syntax.

My R example is:

R> library("annotate")
Loading required package: Biobase 
Loading required package: tools 
Welcome to Bioconductor 
         Vignettes contain introductory material.  To view, 
         simply type: openVignette() 
         For details on reading vignettes, see
         the openVignette help page.
R> library(XML)
R> tmp$term <- "gorjanc g[au]"
R> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc g[au]"
R> tmp
$term
[1] "gorjanc g[au]"

$URL
[1] "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc g[au]"
R> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
Error in xmlTreeParse(tmp$URL, isURL = TRUE, handlers = NULL, asTree = TRUE) : 
        error in creating parser for http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc g[au]

# so I have a problem with space and [ and ]
# let's reduce a problem to just space or [] to be sure
R> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc g"
R> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
Error in xmlTreeParse(tmp$URL, isURL = TRUE, handlers = NULL, asTree = TRUE) : 
        error in creating parser for http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc g
R> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc[au]"
R> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
Error in xmlTreeParse(tmp$URL, isURL = TRUE, handlers = NULL, asTree = TRUE) : 
        error in creating parser for http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc[au]

# now show that it works fine without special chars
R> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc"
R> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
$doc
$file
[1] "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc"

$version
[1] "1.0"

$children
...

# now show a workaround for space
tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc%20g"
xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
R> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc%20g"
R> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
$doc
$file
[1] "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc%20g"

$version
[1] "1.0"

$children
...

As can be seen from above there is a possibility to handle this special
characters and I wonder if this has already been done somewhere? If not
I thought on a function fixURLchar, which would replace reserved characters
with ther escaped sequences. Any comments, pointers, ... ?

from = c(" ", "\"", ",", "#"),
to = c("%20", "%22", "%2c", "%23"))

*When I'll solve problem I will send my code to 'annotate' maintainer 
and he can include it at his will in a package. 

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From gerifalte28 at hotmail.com  Tue May  3 01:53:44 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 02 May 2005 23:53:44 +0000
Subject: [R] Nonparametric Tukey-type multiple comparisons "Nemenyi" test
In-Reply-To: <4276BB90.7090706@noaa.gov>
Message-ID: <BAY103-F1857EDF6EC1AF5E740B9BFA6270@phx.gbl>

help.search("Tukey"). The fifth hit is TukeyHSD().  Also take a look at 
tmd(lattice)

Cheers

Francisco

>From: "Rikki Dunsmore" <Rikki.Dunsmore at noaa.gov>
>To: r-help at stat.math.ethz.ch
>Subject: [R] Nonparametric Tukey-type multiple comparisons "Nemenyi" test
>Date: Mon, 02 May 2005 16:45:20 -0700
>
>I am trying to do a Nonparametric Tukey-type multiple comparison post-hoc 
>test to determine which groups are significantly different.  I have read 
>the dialogue on this topic from the R-help, and am still not clear why no 
>statistical packages include this test as an option?  Is it not an 
>appropriate test to conduct on non-normally distributed data?  Is the only 
>option to calculate it by hand using the (Zar 1996) formula?
>
>Thank you in advance for your help.
>
>--
>Rikki Grober- Dunsmore
>National Marine Fisheries Service
>National Marine Protected Areas Center
>110 Shaffer Rd.
>Santa Cruz, CA 95060
>831-420-3991
>
>
>Unless someone like you,
>Cares a whole awful lot,
>Nothing is going to get better,
>It's not.
>       - The Lorax, by Dr. Seuss, 1971
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From dimitrijoe at yahoo.com.br  Tue May  3 02:29:31 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Mon, 2 May 2005 21:29:31 -0300
Subject: [R] bivariate log-normal distribution
Message-ID: <00bb01c54f77$4d1261c0$ca00a8c0@thesahajamach>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050502/e0b9bc0c/attachment.pl

From sundar.dorai-raj at pdf.com  Tue May  3 02:58:12 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 02 May 2005 17:58:12 -0700
Subject: [R] bivariate log-normal distribution
In-Reply-To: <00bb01c54f77$4d1261c0$ca00a8c0@thesahajamach>
References: <00bb01c54f77$4d1261c0$ca00a8c0@thesahajamach>
Message-ID: <4276CCA4.9090006@pdf.com>



Dimitri Joe wrote on 5/2/2005 5:29 PM:
> Hi,
> 
> I am looking for the density function of the bivariate log-normal distribution. Would anyone have it written?
> 
> Thanks a lot,
> 
> Dimitri
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This is in the archive.

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48580.html

--sundar



From p.murrell at auckland.ac.nz  Tue May  3 04:31:28 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 03 May 2005 14:31:28 +1200
Subject: [R] xinch/yinch equivalent for log axis
References: <20050502165343.GA26558@cl.cam.ac.uk>
Message-ID: <4276E280.9020509@stat.auckland.ac.nz>

Hi


Steven J. Murdoch wrote:
> I would like to draw a vertical line from a given point, in user
> coordinates, to x inches before from another point, also in user
> coordinates. This is easy enough to do for linear scales, using code
> based on xinch/yinch, but I do not know how to do this for logarithmic
> scales.
> 
> This code shows an example of what I mean[1]:
> 
> split.screen(c(2,1))
> screen(1)
> # Linear scale, works fine
> plot(1:100, cex=0.5, pch=19)
> plotheight <- diff(par("usr")[3:4])
> igap = 0.5 # intended gap, in inches
> gap <- igap/par("pin")[2]*plotheight # gap in user units
> lines(c(20,20),c(100,60+gap))
> lines(c(20,20),c(60-gap,1))
> points(20,60, pch=18)
> 
> screen(2)
> # Logarithmic scale, point no longer centered
> plot(1:100, cex=0.5, pch=19, log="y")
> plotheight <- diff(par("usr")[3:4])
> igap = 10
> gap <- igap/par("pin")[2]*plotheight
> lines(c(20,20),c(100,10+gap))
> lines(c(20,20),c(10-gap,1))
> points(20,10, pch=18)
> close.screen(all=TRUE)
> 
> The top graph is as I want. The diamond is centered in the gap, and
> the gap is 1 inch high (2*igap).
> 
> In bottom graph, using a log scale, the diamond is no longer centered
> in the gap and there is a non-linear relationship between the gap
> height in inches and igap.
> 
> I understand why this is happening, and this is why the xinch and
> yinch functions raise a warning, but is there a way to handle it? For
> example, is there a function which will return the user coordinate of
> a point, x inches above a given point p (in user coordinates), for a
> logarithmic y axis?


No function, but does this do what you want?  The basic idea is to work 
in logged values (which is what par("usr") is in) then convert back to 
plot (broken into many steps to hopefully aid clarity) ...

plot(1:100, cex=0.5, pch=19, log="y")
plotheight <- diff(par("usr")[3:4])
igap <- 0.5                       # intended gap, in inches
pigap <- igap/par("pin")[2]       # intended gap as propn of plot height
dy <- 10                          # y location for diamond
ldy <- log10(dy)                  # logged y location for diamond
upper <- ldy + pigap/2*plotheight # logged y location for top of gap
lower <- ldy - pigap/2*plotheight # logged y location for bottom of gap
lines(c(20,20),c(100, 10^upper))
lines(c(20,20),c(10^lower, 1))
points(20,10, pch=18)


Paul


> [1] I actually want to use this in a larger script, where I leave a
>  gap in the axis where the median in. This needs to be small but
>  legible to the eye, so that is why I am defining it in inches.
>  The source code is at:
>   http://www.cl.cam.ac.uk/users/sjm217/projects/graphics/fancyaxis.R
> 


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From itsme_410 at yahoo.com  Tue May  3 04:51:24 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Mon, 2 May 2005 19:51:24 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: 6667
Message-ID: <20050503025124.88536.qmail@web54505.mail.yahoo.com>

OK, here we go:

I am submitting two attachments. The first is the datafile called kinv used to
create my circulant matrix, using the following commands:


x<-scan("kinv")
y<-x[c(109:1,0:108)]
X=toeplitz(y)
eigen(X)
write(X,ncol=216,file="test.dat")

reports the following columns full of NaN's: 18, 58, 194, 200. (Note that
eigen(X,symmetric=T) makes no difference and I get the same as above).

The second attachment contains only the eigenvectors obtained on calling a
LAPACK routine directly (from C). The eigenvalues are essentially the same as
that obtained using R. Here, I use the LAPACK-recommended double precision
routine dspevd() routine for symmetric matrices in packed storage format. Note
the absence of the NaN's....I would be happy to send my C programs to whoever
is interested.

I am using 

:~> uname -a
Linux 2.6.11-1.14_FC3 #1 Thu Apr 7 19:23:49 EDT 2005 i686 i686 i386 GNU/Linux

and R.2.0.1.

Many thanks and best wishes!



From mingan at unm.edu  Tue May  3 05:16:28 2005
From: mingan at unm.edu (mingan yang)
Date: Mon, 02 May 2005 21:16:28 -0600
Subject: [R] maximization help :
Message-ID: <4276ED0C.7040805@unm.edu>



 Given a vector  : pvec=(p1,p2,.... p J)   with sum(pvec)=1,   all the 
elements are non-negative, that is, they are probabilities

 a  matrix   A  ( N* J ), with the elements  alpha(ij)  are 0 or 1


    I want to MAXIMIZE THE RESULT

      RESULT=   product( i=1, to N   [ sum (  alpha(ij)* pj , j =1,to J 
) ]  )

    thus, I need to get pvec. how should I do ?
 
      for example 

            
    say, A=  0   1   0   0
                 1    1  0   0
                 1   0   0   0
                0   0   1   0
                1  0    0   1
               0   0    0   1

   that is A is a matrix 6* 4    thus pvec=(p1,p2,p3,p4)

    I want to get  values of pvec such that , they can maximize

   p2 *  ( p1 + p2 ) * p1 * p3 * (p1+p4) * p4

 
  thanks



From Charles.Annis at StatisticalEngineering.com  Tue May  3 05:37:46 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 2 May 2005 23:37:46 -0400
Subject: [R] comparing lm(), survreg( ... ,
	dist="gaussian") and survreg( ... , dist="lognormal")
Message-ID: <200505030337.j433bcSb014567@hypatia.math.ethz.ch>

Dear R-Helpers:

I have tried everything I can think of and hope not to appear too foolish
when my error is pointed out to me.

I have some real data (18 points) that look linear on a log-log plot so I
used them for a comparison of lm() and survreg.  There are no suspensions.  

survreg.df <- data.frame(Cycles=c(2009000, 577000, 145000, 376000, 37000,
979000, 17420000, 71065000, 46397000, 70168000, 69120000, 68798000,
72615000, 133051000, 38384000, 15204000, 1558000, 14181000), stress=c(90,
100, 110, 90, 100, 80, 70, 60, 56, 62, 62, 59, 56, 53, 59, 70, 90, 70),
event=rep(1, 18))


sN.lm<- lm(log(Cycles) ~ log10(stress), data=survreg.df)

and 
                                             vvvvvvvvvvv
gaussian.survreg<- survreg(formula=Surv(time=log(Cycles), event) ~
log10(stress), dist="gaussian", data=survreg.df)

produce identical parameter estimates and differ slightly in the residual
standard error and scale, which is accounted for by scale being the MLE and
thus biased.  Correcting by sqrt(18/16) produces agreement.  Using predict()
for the lm, and predict.survreg() for the survreg model and correcting for
the differences in stdev, produces identical plots of the fit and the upper
and lower confidence intervals.  All of this is as it should be.

And, 
                                               vvvvvv
lognormal.survreg<- survreg(formula=Surv(time=(Cycles), event) ~
log10(stress), dist="lognormal", data=survreg.df)

produces summary() results that are identical to the earlier call to
survreg(), except for the call, of course.  The parameter estimates and SE
are identical.  Again this is as I would expect it.

But since the call uses Cycles, rather than log(Cycles) predict.survreg()
returns $fit in Cycles units, rather than logs, and of course the fits are
identical when plotted on a log-log grid and also agree with lm()

Here is the fly in the ointment:  The upper and lower confidence intervals,
based on the $se.fit for the dist="lognormal" are quite obviously different
from the other two methods, and although I have tried everything I could
imagine I cannot reconcile the differences.

I believe that the confidence bounds for both models should agree.  After
all, both calls to survreg() produce identical parameter estimates.  

So I have missed something.  Would some kind soul please point out my error?

Thanks.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:?? 614-455-3265
http://www.StatisticalEngineering.com
??



From itsme_410 at yahoo.com  Tue May  3 06:18:30 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Mon, 2 May 2005 21:18:30 -0700 (PDT)
Subject: Fwd: Re: [R] eigenvalues of a circulant matrix
Message-ID: <20050503041830.34415.qmail@web54506.mail.yahoo.com>

Looks like the files did not go through again. In any case, here is the kinv:
please cut and paste and save to a file:



   -1.16801E-03   -2.24310E-03   -1.16864E-03   -2.24634E-03   -1.17143E-03
   -2.25358E-03   -1.17589E-03   -2.26484E-03   -1.18271E-03   -2.27983E-03
   -1.19124E-03   -2.29896E-03   -1.20164E-03   -2.32206E-03   -1.21442E-03
   -2.34911E-03   -1.22939E-03   -2.38073E-03   -1.24626E-03   -2.41702E-03
   -1.26596E-03   -2.45828E-03   -1.28801E-03   -2.50458E-03   -1.31296E-03
   -2.55646E-03   -1.34048E-03   -2.61444E-03   -1.37127E-03   -2.67887E-03
   -1.40531E-03   -2.75026E-03   -1.44311E-03   -2.82930E-03   -1.48481E-03
   -2.91652E-03   -1.53081E-03   -3.01281E-03   -1.58131E-03   -3.11930E-03
   -1.63727E-03   -3.23708E-03   -1.69907E-03   -3.36712E-03   -1.76720E-03
   -3.51113E-03   -1.84251E-03   -3.67073E-03   -1.92580E-03   -3.84787E-03
   -2.01834E-03   -4.04507E-03   -2.12087E-03   -4.26509E-03   -2.23531E-03
   -4.51127E-03   -2.36357E-03   -4.78743E-03   -2.50664E-03   -5.09847E-03
   -2.66813E-03   -5.45027E-03   -2.85019E-03   -5.84987E-03   -3.05664E-03
   -6.30596E-03   -3.29224E-03   -6.82972E-03   -3.56187E-03   -7.43448E-03
   -3.87322E-03   -8.13766E-03   -4.23449E-03   -8.96182E-03   -4.65684E-03
   -9.93567E-03   -5.15519E-03   -1.10980E-02   -5.74887E-03   -1.25006E-02
   -6.46346E-03   -1.42143E-02   -7.33466E-03   -1.63391E-02   -8.41211E-03
   -1.90180E-02   -9.76709E-03   -2.24632E-02   -1.15055E-02   -2.70006E-02
   -1.37894E-02   -3.31497E-02   -1.68780E-02   -4.17865E-02   -2.12092E-02
   -5.44795E-02   -2.75722E-02   -7.42814E-02   -3.75180E-02  -0.107820   
-5.44778E-02
  -0.171908   -8.74660E-02  -0.320830  -0.167792  -0.826079  -0.486405   
-6.28085
    19.4495   -6.28085  -0.486405  -0.826079  -0.167792  -0.320830   
-8.74660E-02
  -0.171908   -5.44778E-02  -0.107820   -3.75180E-02   -7.42816E-02   
-2.75722E-02
   -5.44795E-02   -2.12092E-02   -4.17865E-02   -1.68780E-02   -3.31495E-02
   -1.37894E-02   -2.70004E-02   -1.15055E-02   -2.24632E-02   -9.76708E-03
   -1.90179E-02   -8.41210E-03   -1.63391E-02   -7.33465E-03   -1.42142E-02
   -6.46346E-03   -1.25005E-02   -5.74887E-03   -1.10979E-02   -5.15519E-03
   -9.93565E-03   -4.65686E-03   -8.96166E-03   -4.23449E-03   -8.13766E-03
   -3.87320E-03   -7.43444E-03   -3.56187E-03   -6.82968E-03   -3.29223E-03
   -6.30590E-03   -3.05665E-03   -5.84988E-03   -2.85020E-03   -5.45026E-03
   -2.66813E-03   -5.09848E-03   -2.50664E-03   -4.78742E-03   -2.36358E-03
   -4.51160E-03   -2.23531E-03   -4.26516E-03   -2.12088E-03   -4.04506E-03
   -2.01834E-03   -3.84791E-03   -1.92580E-03   -3.67066E-03   -1.84251E-03
   -3.51094E-03   -1.76720E-03   -3.36700E-03   -1.69907E-03   -3.23702E-03
   -1.63727E-03   -3.11926E-03   -1.58130E-03   -3.01269E-03   -1.53081E-03
   -2.91633E-03   -1.48480E-03   -2.82912E-03   -1.44311E-03   -2.75018E-03
   -1.40531E-03   -2.67891E-03   -1.37126E-03   -2.61459E-03   -1.34048E-03
   -2.55652E-03   -1.31296E-03   -2.50461E-03   -1.28801E-03   -2.45833E-03
   -1.26595E-03   -2.41717E-03   -1.24626E-03   -2.38063E-03   -1.22938E-03
   -2.34904E-03   -1.21443E-03   -2.32207E-03   -1.20164E-03   -2.29903E-03
  -1.19124E-03   -2.27985E-03   -1.18271E-03   -2.26497E-03   -1.17589E-03
   -2.25364E-03   -1.17145E-03   -2.24634E-03   -1.16863E-03   -2.24284E-03

Thanks!



--- Globe Trotter <itsme_410 at yahoo.com> wrote:
> Date: Mon, 2 May 2005 19:51:24 -0700 (PDT)
> From: Globe Trotter <itsme_410 at yahoo.com>
> Subject: Re: [R] eigenvalues of a circulant matrix
> To: r-help at stat.math.ethz.ch
> 
> OK, here we go:
> 
> I am submitting two attachments. The first is the datafile called kinv used
> to
> create my circulant matrix, using the following commands:
> 
> 
> x<-scan("kinv")
> y<-x[c(109:1,0:108)]
> X=toeplitz(y)
> eigen(X)
> write(X,ncol=216,file="test.dat")
> 
> reports the following columns full of NaN's: 18, 58, 194, 200. (Note that
> eigen(X,symmetric=T) makes no difference and I get the same as above).
> 
> The second attachment contains only the eigenvectors obtained on calling a
> LAPACK routine directly (from C). The eigenvalues are essentially the same as
> that obtained using R. Here, I use the LAPACK-recommended double precision
> routine dspevd() routine for symmetric matrices in packed storage format.
> Note
> the absence of the NaN's....I would be happy to send my C programs to whoever
> is interested.
> 
> I am using 
> 
> :~> uname -a
> Linux 2.6.11-1.14_FC3 #1 Thu Apr 7 19:23:49 EDT 2005 i686 i686 i386 GNU/Linux
> 
> and R.2.0.1.
> 
> Many thanks and best wishes!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jkawczak at uncc.edu  Tue May  3 06:33:19 2005
From: jkawczak at uncc.edu (Janusz Kawczak)
Date: Tue, 3 May 2005 00:33:19 -0400 (EDT)
Subject: Fwd: Re: [R] eigenvalues of a circulant matrix
In-Reply-To: <20050503041830.34415.qmail@web54506.mail.yahoo.com>
References: <20050503041830.34415.qmail@web54506.mail.yahoo.com>
Message-ID: <Pine.GSO.4.55.0505030026020.28783@is-sm1.uncc.edu>

I am not sure where your problem is, but I get these eigenvalues (no
NaNs):

> eigen(X)$values
  [1] 33.6609067 33.6609067 33.3657337 33.3657337 33.1688489 33.1688489
  [7] 32.9120603 32.9120603 32.6971529 32.6971529 32.4486757 32.4486757
 [13] 32.2266513 32.2266513 31.9811880 31.9811880 31.7549774 31.7549774
 [19] 31.5106802 31.5106802 31.2814636 31.2814636 31.0374623 31.0374623
 [25] 30.8058168 30.8058168 30.5616180 30.5616180 30.3278637 30.3278637
 [31] 30.0831665 30.0831665 29.8474910 29.8474910 29.6020821 29.6020821
 [37] 29.3645937 29.3645937 29.1183207 29.1183207 28.8790883 28.8790883
 [43] 28.6318199 28.6318199 28.3908814 28.3908814 28.1425171 28.1425171
 [49] 27.8998862 27.8998862 27.6503394 27.6503394 27.4060152 27.4060152
 [55] 27.1552045 27.1552045 26.9091770 26.9091770 26.6570228 26.6570228
 [61] 26.4092748 26.4092748 26.1557083 26.1557083 25.9062128 25.9062128
 [67] 25.6511554 25.6511554 25.3998797 25.3998797 25.1432660 25.1432660
 [73] 24.8901666 24.8901666 24.6319253 24.6319253 24.3769595 24.3769595
 [79] 24.1170167 24.1170167 23.8601266 23.8601266 23.5984086 23.5984086
 [85] 23.3395425 23.3395425 23.0759756 23.0759756 22.8150622 22.8150622
 [91] 22.5495580 22.5495580 22.2865330 22.2865330 22.0190111 22.0190111
 [97] 21.7537982 21.7537982 21.4841613 21.4841613 21.2166787 21.2166787
[103] 20.9448339 20.9448339 20.6749916 20.6749916 20.4008361 20.4008361
[109] 20.1285318 20.1285318 19.8519563 19.8519563 19.5770843 19.5770843
[115] 19.2979657 19.2979657 19.0204113 19.0204113 18.7386197 18.7386197
[121] 18.4582550 18.4582550 18.1736507 18.1736507 17.8903299 17.8903299
[127] 17.6027615 17.6027615 17.3163357 17.3163357 17.0256338 17.0256338
[133] 16.7359239 16.7359239 16.4419046 16.4419046 16.1487284 16.1487284
[139] 15.8511833 15.8511833 15.5543204 15.5543204 15.2530227 15.2530227
[145] 14.9522432 14.9522432 14.6469402 14.6469402 14.3419740 14.3419740
[151] 14.0323742 14.0323742 13.7229182 13.7229182 13.4087011 13.4087011
[157] 13.0944062 13.0944062 12.7751956 12.7751956 12.4556716 12.4556716
[163] 12.1310382 12.1310382 11.8058191 11.8058191 11.4752626 11.4752626
[169] 11.1438118 11.1438118 10.8067417 10.8067417 10.4684198 10.4684198
[175] 10.1241331 10.1241331  9.7781642  9.7781642  9.4258124  9.4258124
[181]  9.0712521  9.0712521  8.7097773  8.7097773  8.3454594  8.3454594
[187]  7.9735359  7.9735359  7.5979459  7.5979459  7.2138560  7.2138560
[193]  6.8250204  6.8250204  6.4264559  6.4264559  6.0216592  6.0216592
[199]  5.6053886  5.6053886  5.1807131  5.1807131  4.7419171  4.7419171
[205]  4.2912801  4.2912801  3.8221094  3.8221094  3.3349406  3.3349406
[211]  2.8206308  2.8206308  2.2746353  2.2746353  1.6787279  1.6787279
[217]  0.9988213


Again, what is the issue here?

Janusz.

On Mon, 2 May 2005, Globe Trotter wrote:

> Looks like the files did not go through again. In any case, here is the kinv:
> please cut and paste and save to a file:
>
>
>
>    -1.16801E-03   -2.24310E-03   -1.16864E-03   -2.24634E-03   -1.17143E-03
>    -2.25358E-03   -1.17589E-03   -2.26484E-03   -1.18271E-03   -2.27983E-03
>    -1.19124E-03   -2.29896E-03   -1.20164E-03   -2.32206E-03   -1.21442E-03
>    -2.34911E-03   -1.22939E-03   -2.38073E-03   -1.24626E-03   -2.41702E-03
>    -1.26596E-03   -2.45828E-03   -1.28801E-03   -2.50458E-03   -1.31296E-03
>    -2.55646E-03   -1.34048E-03   -2.61444E-03   -1.37127E-03   -2.67887E-03
>    -1.40531E-03   -2.75026E-03   -1.44311E-03   -2.82930E-03   -1.48481E-03
>    -2.91652E-03   -1.53081E-03   -3.01281E-03   -1.58131E-03   -3.11930E-03
>    -1.63727E-03   -3.23708E-03   -1.69907E-03   -3.36712E-03   -1.76720E-03
>    -3.51113E-03   -1.84251E-03   -3.67073E-03   -1.92580E-03   -3.84787E-03
>    -2.01834E-03   -4.04507E-03   -2.12087E-03   -4.26509E-03   -2.23531E-03
>    -4.51127E-03   -2.36357E-03   -4.78743E-03   -2.50664E-03   -5.09847E-03
>    -2.66813E-03   -5.45027E-03   -2.85019E-03   -5.84987E-03   -3.05664E-03
>    -6.30596E-03   -3.29224E-03   -6.82972E-03   -3.56187E-03   -7.43448E-03
>    -3.87322E-03   -8.13766E-03   -4.23449E-03   -8.96182E-03   -4.65684E-03
>    -9.93567E-03   -5.15519E-03   -1.10980E-02   -5.74887E-03   -1.25006E-02
>    -6.46346E-03   -1.42143E-02   -7.33466E-03   -1.63391E-02   -8.41211E-03
>    -1.90180E-02   -9.76709E-03   -2.24632E-02   -1.15055E-02   -2.70006E-02
>    -1.37894E-02   -3.31497E-02   -1.68780E-02   -4.17865E-02   -2.12092E-02
>    -5.44795E-02   -2.75722E-02   -7.42814E-02   -3.75180E-02  -0.107820
> -5.44778E-02
>   -0.171908   -8.74660E-02  -0.320830  -0.167792  -0.826079  -0.486405
> -6.28085
>     19.4495   -6.28085  -0.486405  -0.826079  -0.167792  -0.320830
> -8.74660E-02
>   -0.171908   -5.44778E-02  -0.107820   -3.75180E-02   -7.42816E-02
> -2.75722E-02
>    -5.44795E-02   -2.12092E-02   -4.17865E-02   -1.68780E-02   -3.31495E-02
>    -1.37894E-02   -2.70004E-02   -1.15055E-02   -2.24632E-02   -9.76708E-03
>    -1.90179E-02   -8.41210E-03   -1.63391E-02   -7.33465E-03   -1.42142E-02
>    -6.46346E-03   -1.25005E-02   -5.74887E-03   -1.10979E-02   -5.15519E-03
>    -9.93565E-03   -4.65686E-03   -8.96166E-03   -4.23449E-03   -8.13766E-03
>    -3.87320E-03   -7.43444E-03   -3.56187E-03   -6.82968E-03   -3.29223E-03
>    -6.30590E-03   -3.05665E-03   -5.84988E-03   -2.85020E-03   -5.45026E-03
>    -2.66813E-03   -5.09848E-03   -2.50664E-03   -4.78742E-03   -2.36358E-03
>    -4.51160E-03   -2.23531E-03   -4.26516E-03   -2.12088E-03   -4.04506E-03
>    -2.01834E-03   -3.84791E-03   -1.92580E-03   -3.67066E-03   -1.84251E-03
>    -3.51094E-03   -1.76720E-03   -3.36700E-03   -1.69907E-03   -3.23702E-03
>    -1.63727E-03   -3.11926E-03   -1.58130E-03   -3.01269E-03   -1.53081E-03
>    -2.91633E-03   -1.48480E-03   -2.82912E-03   -1.44311E-03   -2.75018E-03
>    -1.40531E-03   -2.67891E-03   -1.37126E-03   -2.61459E-03   -1.34048E-03
>    -2.55652E-03   -1.31296E-03   -2.50461E-03   -1.28801E-03   -2.45833E-03
>    -1.26595E-03   -2.41717E-03   -1.24626E-03   -2.38063E-03   -1.22938E-03
>    -2.34904E-03   -1.21443E-03   -2.32207E-03   -1.20164E-03   -2.29903E-03
>   -1.19124E-03   -2.27985E-03   -1.18271E-03   -2.26497E-03   -1.17589E-03
>    -2.25364E-03   -1.17145E-03   -2.24634E-03   -1.16863E-03   -2.24284E-03
>
> Thanks!
>
>
>
> --- Globe Trotter <itsme_410 at yahoo.com> wrote:
> > Date: Mon, 2 May 2005 19:51:24 -0700 (PDT)
> > From: Globe Trotter <itsme_410 at yahoo.com>
> > Subject: Re: [R] eigenvalues of a circulant matrix
> > To: r-help at stat.math.ethz.ch
> >
> > OK, here we go:
> >
> > I am submitting two attachments. The first is the datafile called kinv used
> > to
> > create my circulant matrix, using the following commands:
> >
> >
> > x<-scan("kinv")
> > y<-x[c(109:1,0:108)]
> > X=toeplitz(y)
> > eigen(X)
> > write(X,ncol=216,file="test.dat")
> >
> > reports the following columns full of NaN's: 18, 58, 194, 200. (Note that
> > eigen(X,symmetric=T) makes no difference and I get the same as above).
> >
> > The second attachment contains only the eigenvectors obtained on calling a
> > LAPACK routine directly (from C). The eigenvalues are essentially the same as
> > that obtained using R. Here, I use the LAPACK-recommended double precision
> > routine dspevd() routine for symmetric matrices in packed storage format.
> > Note
> > the absence of the NaN's....I would be happy to send my C programs to whoever
> > is interested.
> >
> > I am using
> >
> > :~> uname -a
> > Linux 2.6.11-1.14_FC3 #1 Thu Apr 7 19:23:49 EDT 2005 i686 i686 i386 GNU/Linux
> >
> > and R.2.0.1.
> >
> > Many thanks and best wishes!
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Davaren1 at aol.com  Tue May  3 07:15:56 2005
From: Davaren1 at aol.com (Davaren1@aol.com)
Date: Tue, 3 May 2005 01:15:56 EDT
Subject: [R] Combining numeric vs numeric & numeric vs factor graphs into
	one ps/pdf file
Message-ID: <126.5c440dfb.2fa8630c@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050503/a2d6005d/attachment.pl

From ripley at stats.ox.ac.uk  Tue May  3 08:17:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 May 2005 07:17:06 +0100 (BST)
Subject: [R] "Special" characters in URI
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>
Message-ID: <Pine.LNX.4.61.0505030709580.22671@gannet.stats>

On Tue, 3 May 2005, Gorjanc Gregor wrote:

> I am crossposting this to R-help and BioC, since it is relevant to both
> groups.

I don't see the relevance to R-help.  But the answer to your subject is 
unambiguous: valid URLs do not contain `special' characters -- they must 
be encoded.  See RFC1738 at e.g. ftp://ftp.funet.fi/pub/doc/rfc/rfc1738.txt

At some point (probably 2.2.0) I intend to ensure that the mapping to 
file:// URLs that is done is a few places is encoded as necessary.  This 
will likely result in a utility function filePathToURL or some such.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue May  3 08:54:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 May 2005 07:54:39 +0100 (BST)
Subject: [R] comparing lm(), survreg( ... , dist="gaussian") and survreg(
	... , dist="lognormal")
In-Reply-To: <200505030337.j433bcSb014567@hypatia.math.ethz.ch>
References: <200505030337.j433bcSb014567@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0505030728280.22908@gannet.stats>

On Mon, 2 May 2005, Charles Annis, P.E. wrote:

> I have tried everything I can think of and hope not to appear too foolish
> when my error is pointed out to me.
>
> I have some real data (18 points) that look linear on a log-log plot so I
> used them for a comparison of lm() and survreg.  There are no suspensions.
>
> survreg.df <- data.frame(Cycles=c(2009000, 577000, 145000, 376000, 37000,
> 979000, 17420000, 71065000, 46397000, 70168000, 69120000, 68798000,
> 72615000, 133051000, 38384000, 15204000, 1558000, 14181000), stress=c(90,
> 100, 110, 90, 100, 80, 70, 60, 56, 62, 62, 59, 56, 53, 59, 70, 90, 70),
> event=rep(1, 18))
>
>
> sN.lm<- lm(log(Cycles) ~ log10(stress), data=survreg.df)
>
> and
>                                             vvvvvvvvvvv
> gaussian.survreg<- survreg(formula=Surv(time=log(Cycles), event) ~
> log10(stress), dist="gaussian", data=survreg.df)
>
> produce identical parameter estimates and differ slightly in the residual
> standard error and scale, which is accounted for by scale being the MLE and
> thus biased.  Correcting by sqrt(18/16) produces agreement.  Using predict()
> for the lm, and predict.survreg() for the survreg model and correcting for
> the differences in stdev, produces identical plots of the fit and the upper
> and lower confidence intervals.  All of this is as it should be.

I trust you called predict() on both and let R choose the method.

> And,
>                                               vvvvvv
> lognormal.survreg<- survreg(formula=Surv(time=(Cycles), event) ~
> log10(stress), dist="lognormal", data=survreg.df)
>
> produces summary() results that are identical to the earlier call to
> survreg(), except for the call, of course.  The parameter estimates and SE
> are identical.  Again this is as I would expect it.
>
> But since the call uses Cycles, rather than log(Cycles) predict.survreg()
> returns $fit in Cycles units, rather than logs, and of course the fits are
> identical when plotted on a log-log grid and also agree with lm()
>
> Here is the fly in the ointment:  The upper and lower confidence intervals,
> based on the $se.fit for the dist="lognormal" are quite obviously different
> from the other two methods, and although I have tried everything I could
> imagine I cannot reconcile the differences.

How did you do this?  (BTW, I assume you mean upper and lower confidence 
>limits< for the predicted means.)  For the predictions and standard 
errors are (or should be) on the response scale, a non-linear function of 
the parameters.  In that case it is normal to form confidence limits on 
the linear predictor scale and transform.

> I believe that the confidence bounds for both models should agree.  After
> all, both calls to survreg() produce identical parameter estimates.

They will, if computed on the same basis.  On log-scale (to avoid large 
numbers)

pr1 <- predict(lognormal.survreg, se.fit=T)
log(cbind(pr1$fit - 1.96*pr1$se.fit, pr1$fit + 1.96*pr1$se.fit))
pr2 <- predict(gaussian.survreg, se.fit=T)
cbind(pr2$fit - 1.96*pr2$se.fit, pr2$fit + 1.96*pr2$se.fit)

are really pretty close.  The main difference is a slight shift, which 
comes about because the mean of a log(X) is not log(mean(X)).  Note that 
the second set at the preferred ones.  Transforming to log scale before 
making the confidence limits:

cbind(log(pr1$fit) - 1.96*pr1$se.fit/pr1$fit, log(pr1$fit) + 1.96*pr1$se.fit/pr1$fit)

does give identical answers.

Consider care is needed in interpreting what predict() is actually 
predicting in non-linear models.  For both glm() and survreg() it is 
closer to the median of the uncertainty in the predictions than to the 
mean.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolfram at fischer-zim.ch  Tue May  3 09:05:28 2005
From: wolfram at fischer-zim.ch (Wolfram Fischer - Z I M)
Date: Tue, 3 May 2005 09:05:28 +0200
Subject: [R] cmdscale: missing dimnames in R 2.1.0
Message-ID: <20050503070528.GA2752@s1x.local>

When running the first example of cmdscale I got in R 2.0.1:

> loc<-cmdscale(eurodist)
> str(loc)
 num [1:21, 1:2] 2290.3 -825.4   59.2  -82.8 -352.5 ...
 - attr(*, "dimnames")=List of 2
  ..$ : chr [1:21] "Athens" "Barcelona" "Brussels" "Calais" ...
  ..$ : NULL


In R 2.1.0 I get:

> loc<-cmdscale(eurodist)
> str(loc)
 num [1:21, 1:2] 2290.3 -825.4   59.2  -82.8 -352.5 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : NULL

I miss the names of the cities. What can I do?

Thanks - Wolfram



From Tom.Mulholland at dpi.wa.gov.au  Tue May  3 09:06:14 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 3 May 2005 15:06:14 +0800
Subject: [R] eigenvalues of a circulant matrix
Message-ID: <4702645135092E4497088F71D9C8F51A128B4C@afhex01.dpi.wa.gov.au>

Well since I know nothing about this topic I have lurked so far, but here's my two bob's worth.

Firstly I tried to make sense of Brian's initial reply. I have got no idea who Bellman is and you have not referenced (his/her) work in a way I can access the issues you refer to. So I assumed that's exactly what Brian was talking about.

Secondly. 

toeplitz(1:4)
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    2    1    2    3
[3,]    3    2    1    2
[4,]    4    3    2    1

require(magic)
 circulant(4)
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    4    1    2    3
[3,]    3    4    1    2
[4,]    2    3    4    1

So they are obviously two different things. Although I think you may have implied (not stated) that the particular combination you were using resulted in both being exactly the same.

It does appear as if in this case the (X) matrix is circulant. But then I'm no expert in even such simple things.

Then I had no idea where I was going. So I tried the variations in eigen.

I ran you code
x<-scan("h:/t.txt")
y<-x[c(109:216,1:108)]
X<-toeplitz(y)
 and then 

> X[is.na(X)]
numeric(0)

So I didn't get any NAs

t1 <- eigen(X)$vectors
t2 <- eigen(X,symmetric = TRUE)$vectors
> identical(t1,t2)
[1] TRUE
> 

Then

t2 <- eigen(X,symmetric = TRUE,EISPACK = TRUE)$vectors
> identical(t1,t2)
[1] FALSE
> 

So there'e obviously more than one way of getting the vectors. Does the second one make more sense to you?

I also noticed in the eigen help that there are references to issues such as "IEEE 754 arithmetic","(They may also differ between methods and between platforms.)" and "or Hermitian if complex". All of these are out of my competence but they do signal to me that there are issues which may relate to hardware, digital arithmetic and other things of that ilk.

I added the comment about complex because I have a vague idea that they are related to imaginary parts that you refer to.

So not coming to any conclusion that makes sense to me, and given that there are often threads about supposed inaccuracies that have answers such as the digits you see are not always what are held by the machine I set my options(digits = 22) and noticed that some of the numbers are still going at the 22 decimal place suggesting that the machine might be incapable of producing perfectly accurate results using digital arithmetic.

My other big sphere of ignorance is complex numbers.

So I tried
X<-toeplitz(complex(real = y))
t1 <- eigen(X)$vectors

> t1[1:20]
 [1]  0.068041577278880341+0i -0.068041577140546913+0i  0.068041576864811659+0i -0.068041576452430155+0i
 [5]  0.068041575907139579+0i -0.068041575231135451+0i  0.068041574435267163+0i -0.068041573525828514+0i
 [9]  0.068041572538722991+0i -0.068041571498323253+0i  0.068041570619888622+0i -0.068041570256170081+0i
[13]  0.068041568759931989+0i -0.068041566476633147+0i  0.068041563560502477+0i -0.068041560000305007+0i
[17]  0.068041555538765813+0i -0.068041549792984865+0i  0.068041544123969511+0i -0.068041537810956801+0i
> t2[1:20]
 [1]  0.068041381743976906 -0.068041381743976850  0.068041381743976781 -0.068041381743976753  0.068041381743976587
 [6] -0.068041381743976725  0.068041381743976920 -0.068041381743976836  0.068041381743976892 -0.068041381743976781
[11]  0.068041381743976781 -0.068041381743977392  0.068041381743976725 -0.068041381743976753  0.068041381743976753
[16] -0.068041381743976698  0.068041381743976587 -0.068041381743976642  0.068041381743976698 -0.068041381743976490
> 


Which is again different. I have no idea what I'm doing but you do seem to get slightly different answers depending upon which method you use. I do not know if one is superior to the others or where one draws the line in terms of accuracy.

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Globe Trotter
> Sent: Tuesday, 3 May 2005 10:51 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] eigenvalues of a circulant matrix
> 
> 
> OK, here we go:
> 
> I am submitting two attachments. The first is the datafile 
> called kinv used to
> create my circulant matrix, using the following commands:
> 
> 
> x<-scan("kinv")
> y<-x[c(109:1,0:108)]
> X=toeplitz(y)
> eigen(X)
> write(X,ncol=216,file="test.dat")
> 
> reports the following columns full of NaN's: 18, 58, 194, 
> 200. (Note that
> eigen(X,symmetric=T) makes no difference and I get the same as above).
> 
> The second attachment contains only the eigenvectors obtained 
> on calling a
> LAPACK routine directly (from C). The eigenvalues are 
> essentially the same as
> that obtained using R. Here, I use the LAPACK-recommended 
> double precision
> routine dspevd() routine for symmetric matrices in packed 
> storage format. Note
> the absence of the NaN's....I would be happy to send my C 
> programs to whoever
> is interested.
> 
> I am using 
> 
> :~> uname -a
> Linux 2.6.11-1.14_FC3 #1 Thu Apr 7 19:23:49 EDT 2005 i686 
> i686 i386 GNU/Linux
> 
> and R.2.0.1.
> 
> Many thanks and best wishes!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From jarioksa at sun3.oulu.fi  Tue May  3 09:26:18 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Tue, 03 May 2005 10:26:18 +0300
Subject: [R] cmdscale: missing dimnames in R 2.1.0
In-Reply-To: <20050503070528.GA2752@s1x.local>
References: <20050503070528.GA2752@s1x.local>
Message-ID: <1115105178.5017.2.camel@biol102145.oulu.fi>

On Tue, 2005-05-03 at 09:05 +0200, Wolfram Fischer - Z I M wrote:
> When running the first example of cmdscale I got in R 2.0.1:
> 
> > loc<-cmdscale(eurodist)
> > str(loc)
>  num [1:21, 1:2] 2290.3 -825.4   59.2  -82.8 -352.5 ...
>  - attr(*, "dimnames")=List of 2
>   ..$ : chr [1:21] "Athens" "Barcelona" "Brussels" "Calais" ...
>   ..$ : NULL
> 
> 
> In R 2.1.0 I get:
> 
> > loc<-cmdscale(eurodist)
> > str(loc)
>  num [1:21, 1:2] 2290.3 -825.4   59.2  -82.8 -352.5 ...
>  - attr(*, "dimnames")=List of 2
>   ..$ : NULL
>   ..$ : NULL
> 
> I miss the names of the cities. What can I do?
> 
You can use R 2.1.0-patched where this is fixed.

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From r.hankin at noc.soton.ac.uk  Tue May  3 10:05:22 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 3 May 2005 09:05:22 +0100
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <4702645135092E4497088F71D9C8F51A128B4C@afhex01.dpi.wa.gov.au>
References: <4702645135092E4497088F71D9C8F51A128B4C@afhex01.dpi.wa.gov.au>
Message-ID: <930cc1a05465343fa8c1456cac8dd049@soc.soton.ac.uk>

Hi everyone.

The following webpage gives a definition of circulant matrix, which 
agrees with the
definition given in the magic package.

http://mathworld.wolfram.com/CirculantMatrix.html

best  wishes

rksh



On May 3, 2005, at 08:06 am, Mulholland, Tom wrote:

> Well since I know nothing about this topic I have lurked so far, but 
> here's my two bob's worth.
>
> Firstly I tried to make sense of Brian's initial reply. I have got no 
> idea who Bellman is and you have not referenced (his/her) work in a 
> way I can access the issues you refer to. So I assumed that's exactly 
> what Brian was talking about.
>
> Secondly.
>
> toeplitz(1:4)
>      [,1] [,2] [,3] [,4]
> [1,]    1    2    3    4
> [2,]    2    1    2    3
> [3,]    3    2    1    2
> [4,]    4    3    2    1
>
> require(magic)
>  circulant(4)
>      [,1] [,2] [,3] [,4]
> [1,]    1    2    3    4
> [2,]    4    1    2    3
> [3,]    3    4    1    2
> [4,]    2    3    4    1
>
> So they are obviously two different things. Although I think you may 
> have implied (not stated) that the particular combination you were 
> using resulted in both being exactly the same.
>
> It does appear as if in this case the (X) matrix is circulant. But 
> then I'm no expert in even such simple things.
>
> Then I had no idea where I was going. So I tried the variations in 
> eigen.
>
> I ran you code
> x<-scan("h:/t.txt")
> y<-x[c(109:216,1:108)]
> X<-toeplitz(y)
>  and then
>
>> X[is.na(X)]
> numeric(0)
>
> So I didn't get any NAs
>
> t1 <- eigen(X)$vectors
> t2 <- eigen(X,symmetric = TRUE)$vectors
>> identical(t1,t2)
> [1] TRUE
>>
>
> Then
>
> t2 <- eigen(X,symmetric = TRUE,EISPACK = TRUE)$vectors
>> identical(t1,t2)
> [1] FALSE
>>
>
> So there'e obviously more than one way of getting the vectors. Does 
> the second one make more sense to you?
>
> I also noticed in the eigen help that there are references to issues 
> such as "IEEE 754 arithmetic","(They may also differ between methods 
> and between platforms.)" and "or Hermitian if complex". All of these 
> are out of my competence but they do signal to me that there are 
> issues which may relate to hardware, digital arithmetic and other 
> things of that ilk.
>
> I added the comment about complex because I have a vague idea that 
> they are related to imaginary parts that you refer to.
>
> So not coming to any conclusion that makes sense to me, and given that 
> there are often threads about supposed inaccuracies that have answers 
> such as the digits you see are not always what are held by the machine 
> I set my options(digits = 22) and noticed that some of the numbers are 
> still going at the 22 decimal place suggesting that the machine might 
> be incapable of producing perfectly accurate results using digital 
> arithmetic.
>
> My other big sphere of ignorance is complex numbers.
>
> So I tried
> X<-toeplitz(complex(real = y))
> t1 <- eigen(X)$vectors
>
>> t1[1:20]
>  [1]  0.068041577278880341+0i -0.068041577140546913+0i  
> 0.068041576864811659+0i -0.068041576452430155+0i
>  [5]  0.068041575907139579+0i -0.068041575231135451+0i  
> 0.068041574435267163+0i -0.068041573525828514+0i
>  [9]  0.068041572538722991+0i -0.068041571498323253+0i  
> 0.068041570619888622+0i -0.068041570256170081+0i
> [13]  0.068041568759931989+0i -0.068041566476633147+0i  
> 0.068041563560502477+0i -0.068041560000305007+0i
> [17]  0.068041555538765813+0i -0.068041549792984865+0i  
> 0.068041544123969511+0i -0.068041537810956801+0i
>> t2[1:20]
>  [1]  0.068041381743976906 -0.068041381743976850  0.068041381743976781 
> -0.068041381743976753  0.068041381743976587
>  [6] -0.068041381743976725  0.068041381743976920 -0.068041381743976836 
>  0.068041381743976892 -0.068041381743976781
> [11]  0.068041381743976781 -0.068041381743977392  0.068041381743976725 
> -0.068041381743976753  0.068041381743976753
> [16] -0.068041381743976698  0.068041381743976587 -0.068041381743976642 
>  0.068041381743976698 -0.068041381743976490
>>
>
>
> Which is again different. I have no idea what I'm doing but you do 
> seem to get slightly different answers depending upon which method you 
> use. I do not know if one is superior to the others or where one draws 
> the line in terms of accuracy.
>
> Tom
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Globe Trotter
>> Sent: Tuesday, 3 May 2005 10:51 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: Re: [R] eigenvalues of a circulant matrix
>>
>>
>> OK, here we go:
>>
>> I am submitting two attachments. The first is the datafile
>> called kinv used to
>> create my circulant matrix, using the following commands:
>>
>>
>> x<-scan("kinv")
>> y<-x[c(109:1,0:108)]
>> X=toeplitz(y)
>> eigen(X)
>> write(X,ncol=216,file="test.dat")
>>
>> reports the following columns full of NaN's: 18, 58, 194,
>> 200. (Note that
>> eigen(X,symmetric=T) makes no difference and I get the same as above).
>>
>> The second attachment contains only the eigenvectors obtained
>> on calling a
>> LAPACK routine directly (from C). The eigenvalues are
>> essentially the same as
>> that obtained using R. Here, I use the LAPACK-recommended
>> double precision
>> routine dspevd() routine for symmetric matrices in packed
>> storage format. Note
>> the absence of the NaN's....I would be happy to send my C
>> programs to whoever
>> is interested.
>>
>> I am using
>>
>> :~> uname -a
>> Linux 2.6.11-1.14_FC3 #1 Thu Apr 7 19:23:49 EDT 2005 i686
>> i686 i386 GNU/Linux
>>
>> and R.2.0.1.
>>
>> Many thanks and best wishes!
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From marco.tasin at vv.slu.se  Tue May  3 10:30:15 2005
From: marco.tasin at vv.slu.se (Marco Tasin)
Date: Tue, 3 May 2005 10:30:15 +0200
Subject: [R] Survival
Message-ID: <00a001c54fba$54915850$a8352fc2@intra.ismaa.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050503/44f71202/attachment.pl

From natascha_nadine at hotmail.com  Tue May  3 10:32:24 2005
From: natascha_nadine at hotmail.com (natascha_nadine@hotmail.com)
Date: Tue, 03 May 2005 08:32:24 GMT
Subject: [R] Ich bin's, was zum lachen ;)
Message-ID: <d2bcf.ab68b4d5db2f5@hotmail.com>


From christoph.lehmann at gmx.ch  Tue May  3 12:54:32 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 03 May 2005 12:54:32 +0200
Subject: [R] RMySQL installation: libz missing
Message-ID: <42775868.9070104@gmx.ch>

Hi
I run suse linux 9.1 and I installed MySQL server, client, devel, bench.
DBI is installed, when I try to install RMySQL I get an error saying, 
that libz is missing.

(paths to libs were set:export PKG_CPPFLAGS="-I/usr/include/mysql/"
export PKG_LIBS="-L/usr/lib/mysql/ -lmysqlclient")

so my question: where do I get the libz files (are these mysql files? if 
yes, why were they not installed at least by mysql-devel?)

thanks for your kind help

christoph



From Achim.Zeileis at wu-wien.ac.at  Tue May  3 11:46:36 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 3 May 2005 11:46:36 +0200
Subject: [R] Trying to understand kpss.test() in tseries package
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7E7A@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7E7A@pnlmse35.pnl.gov>
Message-ID: <20050503114636.6c5da3e8.Achim.Zeileis@wu-wien.ac.at>

On Mon, 02 May 2005 14:43:02 -0700 Waichler, Scott R wrote:

> 
> I'm trying to understand how to use kpss.test() properly.  If I have a
> level stationary series like rnorm() in the help page, shouldn't I get
> a small p-value with the null hypothesis set to "Trend"?

No, every level stationary series is also trend stationary (with a zero
trend). Hence, the test statistic is even smaller for null = "Trend"
because first the trend is removed (which is not exactly zero due to
random variation).
Z

>  The (condensed)
> output from kpss.test() for the two possible null hypotheses is given
> below.  I don't see any significant difference between these results. 
> 
> 
> > x <- rnorm(1000)  # is level stationary
> > kpss.test(x, null="Level")
>         KPSS Test for Level Stationarity
> KPSS Level = 0.0638, Truncation lag parameter = 7, p-value = 0.1
> Warning:  p-value greater than printed p-value
> 
> > kpss.test(x, null="Trend")
>         KPSS Test for Trend Stationarity
> KPSS Trend = 0.0275, Truncation lag parameter = 7, p-value = 0.1
> Warning:  p-value greater than printed p-value
> 
> I can't get the original reference easily.
> 
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler at pnl.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From dmk2 at st-andrews.ac.uk  Tue May  3 12:08:10 2005
From: dmk2 at st-andrews.ac.uk (David Kidd)
Date: Tue, 03 May 2005 11:08:10 +0100
Subject: [R] Lattice dotplot with symbols sized and colored
Message-ID: <6.2.0.14.0.20050503105458.039ebba8@bute.st-and.ac.uk>


Apologies if this is a naive beginners question.

I am trying to create a dotplot with the lattice dotplot function in which 
my dots are colored differently depending on if positive or negative and 
sized by sp.nc.bdrs.data$mwZ

I have tried...

dotplot(sporder ~ cvarorder | direct, data=sp.nc.bdrs.data,
cex=abs(sp.nc.bdrs.data$mwZ * 0.05),
xlab="climate variables",
ylab="species",col= sp.nc.bdrs.data$mysign,
scales = list(y = list(labels = as.character(spname),at = 1:12, cex = 0.5), 
x = list(labels = as.character(my.ylabel), at = 1:66, rot=90, cex = 0.5), 
alternating = 3))

This sizes my symbols correctly but colors all four conditional plots 
(direct = 'e', 'n', 's' & 'w') using the colors defined for the first 
direct condition (i.e. 'e').

I have also tried using 'groups' to set the colors, however, whenever I do 
this I cannot get cex to then size the symbols - I get open circles that do 
not scale. This problem also happens when I try setting up a panel function.

Many Thanks





----------------------------------------------------------------------------------------------
David M. Kidd
Research Assistant
School of Biology
Room 210, Sir Harold Mitchell Building
University of St. Andrews
St. Andrews, Fife
KY16 9TH
UK
http://www.st-and.ac.uk/~bugs/dave/dave.htm
Tel:  +44 (0)1334 463348
Fax: +44 (0)1334 463600



From Sebastian.Leuzinger at unibas.ch  Tue May  3 12:16:46 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Tue, 3 May 2005 12:16:46 +0200
Subject: [R] RMySQL installation: libz missing
In-Reply-To: <42775868.9070104@gmx.ch>
References: <42775868.9070104@gmx.ch>
Message-ID: <200505031216.47112.Sebastian.Leuzinger@unibas.ch>

hi christoph
i had the same problem recently, i ended up finding that the following linux 
packages were missing (or at least one of them).

php4-mysql
php5-mysql
php5-mysqli

maybe that helps, sebastian

On Tuesday 03 May 2005 12:54, Christoph Lehmann wrote:
> Hi
> I run suse linux 9.1 and I installed MySQL server, client, devel, bench.
> DBI is installed, when I try to install RMySQL I get an error saying,
> that libz is missing.
>
> (paths to libs were set:export PKG_CPPFLAGS="-I/usr/include/mysql/"
> export PKG_LIBS="-L/usr/lib/mysql/ -lmysqlclient")
>
> so my question: where do I get the libz files (are these mysql files? if
> yes, why were they not installed at least by mysql-devel?)
>
> thanks for your kind help
>
> christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From ajayshah at mayin.org  Tue May  3 12:28:07 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Tue, 3 May 2005 15:58:07 +0530
Subject: [R] R on Mac OS X: odd errors when doing install.packages()
Message-ID: <20050503102807.GF5168@lubyanka.local>

Should I be worried? The installation seems to go through fine and
apparently nothing is broken. The errors I repeatedly get are like this:

g++ -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  -I/usr/
local/include  -DUNIX -DOPTIM -DNONR -fno-common  -g -O2 -c unif.cpp -o unif.o
g++ -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o rgenoud.so c
hange_order.o eval.o evaluate.o frange_ran.o genoud.o gradient.o math.o multiply
.o numerics.o operators.o print_format.o rgenoud.o unif.o  -lcc_dynamic -framewo
rk R
ld: warning multiple definitions of symbol _xerbla_
/Library/Frameworks/R.framework/R(print.lo) definition of _xerbla_
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.fra
mework/Versions/A/libBLAS.dylib(single module) definition of _xerbla_
ld: warning multiple definitions of symbol _BC
/Library/Frameworks/R.framework/Versions/2.1.0/Resources/lib/libreadline.5.0.dyl
ib(terminal.so) definition of _BC
/usr/lib/libncurses.5.dylib(lib_termcap.o) definition of _BC
ld: warning multiple definitions of symbol _UP
/Library/Frameworks/R.framework/Versions/2.1.0/Resources/lib/libreadline.5.0.dyl
ib(terminal.so) definition of _UP
/usr/lib/libncurses.5.dylib(lib_termcap.o) definition of _UP
ld: warning multiple definitions of symbol _PC
/Library/Frameworks/R.framework/Versions/2.1.0/Resources/lib/libreadline.5.0.dyl
ib(terminal.so) definition of _PC
/usr/lib/libncurses.5.dylib(lib_tputs.o) definition of _PC
** R
** help
 >>> Building/Updating help pages for package 'rgenoud'
     Formats: text html latex example 
  genoud                            text    html    latex   example
** building package indices ...
* DONE (rgenoud)

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From gregor.gorjanc at bfro.uni-lj.si  Tue May  3 12:32:43 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Tue, 03 May 2005 12:32:43 +0200
Subject: [R] Locale settings on Debian
Message-ID: <4277534B.2030008@bfro.uni-lj.si>

Hello!

I have a problem with locales in R (debian package r-base) 2.1.0-1. I get
the following error:

$ R

R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.1.0  (2005-04-18), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.


R> plot(1:10)
Error in X11() : could not find any X11 fonts
Check that the Font Path is correct.
In addition: Warning messages:
1: locale not supported by Xlib: some X ops will operate in C locale
2: X cannot set locale modifiers

# In R I have the following locales, which are a mixture of mine setting
# (look at the end of this mail) and R settings i.e. some my locales
# settings were overriden by R and some added
R> Sys.getlocale()
[1]"LC_CTYPE=en_US.utf8;LC_NUMERIC=C;LC_TIME=en_US.utf8;LC_COLLATE=en_US.utf8;
LC_MONETARY=en_US.utf8;LC_MESSAGES=en_US.utf8;LC_PAPER=C;LC_NAME=C;LC_ADDRESS=C;
LC_TELEPHONE=C;LC_MEASUREMENT=C;LC_IDENTIFICATION=C"

# and new internationalization functions say that everything should be OK
R> l10n_info()
$MBCS
[1] TRUE

$"UTF-8"
[1] TRUE

# and It afcourse works fine if I set
R> Sys.setlocale("LC_CTYPE", "C")

# If I echo LC variables in terminal I have
$ echo $LC
$LC_ALL          $LC_MEASUREMENT  $LC_PAPER        $LC_TIME

$ echo $LC_PAPER
a4

$ echo $LC_TIME
en_US

$ echo $LC_MEASUREMENT
metric

$ echo $LC_ALL
en_US.utf8

$ echo $LAN
$LANG      $LANGUAGE

$ echo $LANG
en_US.utf8

$ echo $LANGUAGE
en_US.utf8

# I use en_US.utf8 since I need possibility to display also non-ascii
# characters in terminal.

Does anyone have any suggestions what should I do?

---
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From sundar.dorai-raj at pdf.com  Tue May  3 12:42:07 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 03 May 2005 03:42:07 -0700
Subject: [R] Lattice dotplot with symbols sized and colored
In-Reply-To: <6.2.0.14.0.20050503105458.039ebba8@bute.st-and.ac.uk>
References: <6.2.0.14.0.20050503105458.039ebba8@bute.st-and.ac.uk>
Message-ID: <4277557F.3000307@pdf.com>



David Kidd wrote on 5/3/2005 3:08 AM:
> 
> Apologies if this is a naive beginners question.
> 
> I am trying to create a dotplot with the lattice dotplot function in 
> which my dots are colored differently depending on if positive or 
> negative and sized by sp.nc.bdrs.data$mwZ
> 
> I have tried...
> 
> dotplot(sporder ~ cvarorder | direct, data=sp.nc.bdrs.data,
> cex=abs(sp.nc.bdrs.data$mwZ * 0.05),
> xlab="climate variables",
> ylab="species",col= sp.nc.bdrs.data$mysign,
> scales = list(y = list(labels = as.character(spname),at = 1:12, cex = 
> 0.5), x = list(labels = as.character(my.ylabel), at = 1:66, rot=90, cex 
> = 0.5), alternating = 3))
> 
> This sizes my symbols correctly but colors all four conditional plots 
> (direct = 'e', 'n', 's' & 'w') using the colors defined for the first 
> direct condition (i.e. 'e').
> 
> I have also tried using 'groups' to set the colors, however, whenever I 
> do this I cannot get cex to then size the symbols - I get open circles 
> that do not scale. This problem also happens when I try setting up a 
> panel function.
> 
> Many Thanks
> 
> 
> 
> 

Hi David,

I think you're on the right track using groups. However, you just have 
to set the trellis arguments correctly. For example,

library(lattice)
my.theme <- function() {
   theme <- col.whitebg()
   symb <- theme$superpose.symbol
   symb$cex <- seq(0.5, 1.5, length = length(symb$cex))
   # symb$pch is c(1, 3, 6, 0, 5, 16, 17) by default for col.whitebg
   theme$superpose.symbol <- symb
   theme
}
trellis.par.set(theme = my.theme())
# from ?dotplot
dotplot(variety ~ yield | year, data = barley, groups = site)

It's not necessary to create a theme. You can accomplish the same just 
using trellis.par.get and trellis.par.set. The above is just my personal 
preference.

HTH,

--sundar



From jqm475 at gmail.com  Tue May  3 12:46:45 2005
From: jqm475 at gmail.com (Jonathan Q.)
Date: Tue, 3 May 2005 06:46:45 -0400
Subject: [R] General Question on learning R...
Message-ID: <e206273d0505030346ac59dc9@mail.gmail.com>

In the process of learning R, with a specific interest on financial
time series.  While I continue to get through the documents I am more
a fan of learning by example and then looking up how each function is
used.  Any websites which post sample code for R?

Thanks in advance.  

-- 
Jonathan
jqm475 at gmail.com



From gregor.gorjanc at bfro.uni-lj.si  Tue May  3 12:54:09 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Tue, 03 May 2005 12:54:09 +0200
Subject: [R] "Special" characters in URI
In-Reply-To: <Pine.LNX.4.61.0505030709580.22671@gannet.stats>
References: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>
	<Pine.LNX.4.61.0505030709580.22671@gannet.stats>
Message-ID: <42775851.1090308@bfro.uni-lj.si>

Prof Brian Ripley wrote:
> On Tue, 3 May 2005, Gorjanc Gregor wrote:
> 
>> I am crossposting this to R-help and BioC, since it is relevant to both
>> groups.
> 
> I don't see the relevance to R-help.  But the answer to your subject is
Is it more rellevant for R-devel?

> unambiguous: valid URLs do not contain `special' characters -- they must
> be encoded.  See RFC1738 at e.g. ftp://ftp.funet.fi/pub/doc/rfc/rfc1738.txt
Yes, I understand that completely and I just wanted to know how other
handle or have solved this issue. Having this in mind I am looking forward
to your 'filePathToURL' function. Do you have any scratches of it already?
What do you think about this scratch, which afcourse doesn't solve all
"special" characters:

fixURLchar <- function(URL,
                       from = c(" ", "\"", ",", "#"),
                       to = c("%20", "%22", "%2c", "%23"))
{
  ## Checks
  if (length(from) != length(to))
    stop("Length of 'from' and 'to' must be the same")

  ## Core
  for (i in seq(along=from)) {
    URL <- gsub(pattern=from[i], replacement=to[i], x=URL)
  }

  return(URL)
}


> At some point (probably 2.2.0) I intend to ensure that the mapping to
> file:// URLs that is done is a few places is encoded as necessary.  This
> will likely result in a utility function filePathToURL or some such.
> 

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From sdavis2 at mail.nih.gov  Tue May  3 12:55:36 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 3 May 2005 06:55:36 -0400
Subject: [R] General Question on learning R...
In-Reply-To: <e206273d0505030346ac59dc9@mail.gmail.com>
References: <e206273d0505030346ac59dc9@mail.gmail.com>
Message-ID: <faea59c94331334ace64224572930fb3@mail.nih.gov>


On May 3, 2005, at 6:46 AM, Jonathan Q. wrote:

> In the process of learning R, with a specific interest on financial
> time series.  While I continue to get through the documents I am more
> a fan of learning by example and then looking up how each function is
> used.  Any websites which post sample code for R?
>

The largest source of example code is R itself.  If you have a command 
in which you are interested, you can often just type the command and 
the code will be shown to you.  Try typing:

ls()

Then:

ls

It will show you the code used to produce the result.  Also, each 
command has its own example(s) in the help.

Sean



From Ted.Harding at nessie.mcc.ac.uk  Tue May  3 13:07:50 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 03 May 2005 12:07:50 +0100 (BST)
Subject: Fwd: Re: [R] eigenvalues of a circulant matrix
In-Reply-To: <20050503041830.34415.qmail@web54506.mail.yahoo.com>
Message-ID: <XFMail.050503120750.Ted.Harding@nessie.mcc.ac.uk>

I think the statement of the problem and the questions asked
need clarifying. Some aspects puzzleme. See below.

On 03-May-05 Globe Trotter wrote:
> Looks like the files did not go through again. In any case,
> here is the kinv:
> please cut and paste and save to a file:
> 
> [data snipped]
> 
> --- Globe Trotter <itsme_410 at yahoo.com> wrote:
>> Date: Mon, 2 May 2005 19:51:24 -0700 (PDT)
>> From: Globe Trotter <itsme_410 at yahoo.com>
>> Subject: Re: [R] eigenvalues of a circulant matrix
>> To: r-help at stat.math.ethz.ch
>> 
>> OK, here we go:
>> 
>> I am submitting two attachments. The first is the datafile
>> called kinv used to create my circulant matrix, using the
>> following commands:
>> 
>> x<-scan("kinv")
>> y<-x[c(109:1,0:108)]
>> X=toeplitz(y)
>> eigen(X)
>> write(X,ncol=216,file="test.dat")

Having cut&pasted from the data placed in the body of the
message (omitted here) I get 216 numbers. Having put these
in a vector x (in my own way):

  length(x)
  ##[1] 216

Question 1:
===========
Is this correct? Or has there been a problem with your
posting of the data?

If it is correct, given that you seem to only use x[1:109],
was there some point in giving the rest?

Question 2:
===========
Next, using your command:

  y<-x[c(109:1,0:108)]

I now get

  length(y)
  ##[1] 217

(as expected). The "0" in "0:108" seems to have been ignored
(again as expected), so this is equivalent to

  y<-x[c(109:1,1:108)]

Is this as intended? If so, why use "0:108" instead of "1:108"?
Check:

  y[1]    ##[1] 19.4495
  x[109]  ##[1] 19.4495

  y[109]  ##[1] -0.00116801
  x[1]    ##[1] -0.00116801

  y[110]  ##[1] -0.00116801
  x[1]    ##[1] -0.00116801

  y[217]  ##[1] -6.28085
  x[108]  ##[1] -6.28085

Can you confirm that this is as intended?

Comment 3:
==========
You next command X=toeplitz(y): No apparent problems,
it gives a symmetric result:

  which(X != t(X)) ## numeric(0)

with 217 rows and columns:

  dim(X)  ##[1] 217 217

and looks circulant:

  X[(1:5),(1:5)]
            [,1]      [,2]      [,3]      [,4]      [,5]
  [1,] 19.449500 -6.280850 -0.486405 -0.826079 -0.167792
  [2,] -6.280850 19.449500 -6.280850 -0.486405 -0.826079
  [3,] -0.486405 -6.280850 19.449500 -6.280850 -0.486405
  [4,] -0.826079 -0.486405 -6.280850 19.449500 -6.280850
  [5,] -0.167792 -0.826079 -0.486405 -6.280850 19.449500

Question 4:
===========
Your next command, "eigen(X)", would simply output the results
to screen and does not assign to anything.

Your next command "write(X,ncol=216,file="test.dat")" as it
stands will write the toeplitz matrix X, constructed by
your command "X<-toeplitz(y)" to file, but with 216
columns instead of 217. However, the result consists
simply of numbers, and there is nothing like "NA" or "NaN"
in the file which I get.

Nor are there any NAs or NaNs in X itself, of course.

But, when you yourself did "write(X,ncol=216,file="test.dat")",
perhaps the "X" in this command was different from the "X"
which is the toeplitz matrix. So, was it the result of an
assignment from "eigen(X)" and, if so, which component or
components?

Question/Comment 5:
===================
So I have tried Z<-eigen(X). First of all, I get no problems
with NAs or NaNs:

  which(is.na(Z$values))    ##numeric(0)
  which(is.nan(Z$values))   ##numeric(0)
  which(is.na(Z$vectors))   ##numeric(0)
  which(is.nan(Z$vectors))  ##numeric(0)

Next, trying various options for wirting to file:

  write(Z,ncol=216,file="test.dat")

simply does not work (not a writable structure), while

  write(Z$values,ncol=216,file="test.dat")

produces simply a set of numbers, no NAs of NaNs, and likewise

  write(Z$vectors,ncol=216,file="test.dat")

(the only occurrences of non-numeric characters are "e", as
in "e-05").

>> reports the following columns full of NaN's: 18, 58, 194, 200.
>> (Note that eigen(X,symmetric=T) makes no difference and I get
>> the same as above).

Therefore I find myself completely unable to reproduce your
problem. However, for the various reasons stated in detail
above, I am not at all sure that what you wrote as the statement
of what you did in fact corresponds to what you really did!

I even wonder whether

Question 6:
===========
Was the file "test.dat" the result of your "write" command?
Or was it left over from a previous activity, the "write"
from this session having failed to execute for some reason?
(In which case the NaNs would have nothing to do with the
results of "eigen(X)").


>> The second attachment contains only the eigenvectors
>> obtained on calling a LAPACK routine directly (from C).
>> The eigenvalues are essentially the same as that obtained
>> using R. Here, I use the LAPACK-recommended double
>> precision routine dspevd() routine for symmetric matrices
>> in packed storage format.
>> Note the absence of the NaN's....I would be happy to send
>> my C programs to whoever is interested.

Well, I didn't get any NaNs in R either -- quite consistent
with your C program!

Please clarify according to the questions above.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 03-May-05                                       Time: 12:06:57
------------------------------ XFMail ------------------------------



From hb at maths.lth.se  Tue May  3 13:40:49 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 03 May 2005 13:40:49 +0200
Subject: [R] "Special" characters in URI
In-Reply-To: <42775851.1090308@bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>	<Pine.LNX.4.61.0505030709580.22671@gannet.stats>
	<42775851.1090308@bfro.uni-lj.si>
Message-ID: <42776341.2060905@maths.lth.se>

Gregor GORJANC wrote:
> Prof Brian Ripley wrote:
> 
>>On Tue, 3 May 2005, Gorjanc Gregor wrote:
>>
>>
>>>I am crossposting this to R-help and BioC, since it is relevant to both
>>>groups.
>>
>>I don't see the relevance to R-help.  But the answer to your subject is
> 
> Is it more rellevant for R-devel?
> 
> 
>>unambiguous: valid URLs do not contain `special' characters -- they must
>>be encoded.  See RFC1738 at e.g. ftp://ftp.funet.fi/pub/doc/rfc/rfc1738.txt
> 
> Yes, I understand that completely and I just wanted to know how other
> handle or have solved this issue. Having this in mind I am looking forward
> to your 'filePathToURL' function. Do you have any scratches of it already?
> What do you think about this scratch, which afcourse doesn't solve all
> "special" characters:
> 
> fixURLchar <- function(URL,
>                        from = c(" ", "\"", ",", "#"),
>                        to = c("%20", "%22", "%2c", "%23"))

Just a comment. It is much safer/easier to use named vectors for 
mapping, e.g.

  map <- c(" "="%20", "\""="%22", ","="%2c", "#"="%23")

/Henrik

> {
>   ## Checks
>   if (length(from) != length(to))
>     stop("Length of 'from' and 'to' must be the same")
> 
>   ## Core
>   for (i in seq(along=from)) {
>     URL <- gsub(pattern=from[i], replacement=to[i], x=URL)
>   }
> 
>   return(URL)
> }
> 
> 
> 
>>At some point (probably 2.2.0) I intend to ensure that the mapping to
>>file:// URLs that is done is a few places is encoded as necessary.  This
>>will likely result in a utility function filePathToURL or some such.
>>
> 
>



From dmk2 at st-andrews.ac.uk  Tue May  3 13:39:47 2005
From: dmk2 at st-andrews.ac.uk (David Kidd)
Date: Tue, 03 May 2005 12:39:47 +0100
Subject: [R] Lattice dotplot with symbols sized and colored 
Message-ID: <6.2.0.14.0.20050503122112.039d8360@bute.st-and.ac.uk>


Sundar suggested setting up a theme which I have done...

my.theme <- function() {
   theme <- col.whitebg()
   symb <- theme$superpose.symbol
   symb$col= c("red","blue","green","yellow","orange","black","purple")
   #symb$pch is c(1,3,6,0,5,16,17) by default for col.whitebg
   theme$superpose.symbol <- symb
   theme
}
trellis.par.set(theme = my.theme())
# from ?dotplot
dotplot(sporder ~ cvarorder | direct, data=sp.nc.bdrs.data,
groups = sp.nc.bdrs.data$mysign,
cex=abs(sp.nc.bdrs.data$mwZ * 0.05),
xlab="climate variables",
ylab="species",
scales = list(y = list(labels = as.character(spname),at = 1:12, cex = 0.5), 
x = list(labels = as.character(my.ylabel), at = 1:66, rot=90, cex = 0.5), 
alternating = 3))


Now if I am understanding lattice correctly setting symb$cex sets the size 
multiplier for symbols within each group but does not support the 
differential sizing of symbols for each record independently within each 
group. Hence I have tried to use a 'global' cex (after the groups 
statement), however, this has no effect on the displayed symbol size. Do I 
need to change the pch or another parameter to draw sized filled circles as 
is the default symbol for dotplot?

Thanks
Dave


----------------------------------------------------------------------------------------------
David M. Kidd
Research Assistant
School of Biology
Room 210, Sir Harold Mitchell Building
University of St. Andrews
St. Andrews, Fife
KY16 9TH
UK
http://www.st-and.ac.uk/~bugs/dave/dave.htm
Tel:  +44 (0)1334 463348
Fax: +44 (0)1334 463600



From ripley at stats.ox.ac.uk  Tue May  3 13:51:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 May 2005 12:51:42 +0100 (BST)
Subject: Fwd: Re: [R] eigenvalues of a circulant matrix
In-Reply-To: <XFMail.050503120750.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050503120750.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0505031247160.25777@gannet.stats>

I think we need to make clear that eigen() by default relies on LAPACK 
routines and they in turn rely on BLAS routines.  We have seen several 
instances in which LAPACK/BLAS return NaNs when they should not, but all 
that I am aware of are when (broken) external libraries were used.

So the occurrence of NaNs should lead you to question other aspects of 
your computational environment.  If you have such a broken environment,
calling eigen(EISPACK=TRUE) may be a palliative, but it is better to track 
the problem down.

On Tue, 3 May 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

> I think the statement of the problem and the questions asked
> need clarifying. Some aspects puzzleme. See below.
>
> On 03-May-05 Globe Trotter wrote:
>> Looks like the files did not go through again. In any case,
>> here is the kinv:
>> please cut and paste and save to a file:
>>
>> [data snipped]
>>
>> --- Globe Trotter <itsme_410 at yahoo.com> wrote:
>>> Date: Mon, 2 May 2005 19:51:24 -0700 (PDT)
>>> From: Globe Trotter <itsme_410 at yahoo.com>
>>> Subject: Re: [R] eigenvalues of a circulant matrix
>>> To: r-help at stat.math.ethz.ch
>>>
>>> OK, here we go:
>>>
>>> I am submitting two attachments. The first is the datafile
>>> called kinv used to create my circulant matrix, using the
>>> following commands:
>>>
>>> x<-scan("kinv")
>>> y<-x[c(109:1,0:108)]
>>> X=toeplitz(y)
>>> eigen(X)
>>> write(X,ncol=216,file="test.dat")
>
> Having cut&pasted from the data placed in the body of the
> message (omitted here) I get 216 numbers. Having put these
> in a vector x (in my own way):
>
>  length(x)
>  ##[1] 216
>
> Question 1:
> ===========
> Is this correct? Or has there been a problem with your
> posting of the data?
>
> If it is correct, given that you seem to only use x[1:109],
> was there some point in giving the rest?
>
> Question 2:
> ===========
> Next, using your command:
>
>  y<-x[c(109:1,0:108)]
>
> I now get
>
>  length(y)
>  ##[1] 217
>
> (as expected). The "0" in "0:108" seems to have been ignored
> (again as expected), so this is equivalent to
>
>  y<-x[c(109:1,1:108)]
>
> Is this as intended? If so, why use "0:108" instead of "1:108"?
> Check:
>
>  y[1]    ##[1] 19.4495
>  x[109]  ##[1] 19.4495
>
>  y[109]  ##[1] -0.00116801
>  x[1]    ##[1] -0.00116801
>
>  y[110]  ##[1] -0.00116801
>  x[1]    ##[1] -0.00116801
>
>  y[217]  ##[1] -6.28085
>  x[108]  ##[1] -6.28085
>
> Can you confirm that this is as intended?
>
> Comment 3:
> ==========
> You next command X=toeplitz(y): No apparent problems,
> it gives a symmetric result:
>
>  which(X != t(X)) ## numeric(0)
>
> with 217 rows and columns:
>
>  dim(X)  ##[1] 217 217
>
> and looks circulant:
>
>  X[(1:5),(1:5)]
>            [,1]      [,2]      [,3]      [,4]      [,5]
>  [1,] 19.449500 -6.280850 -0.486405 -0.826079 -0.167792
>  [2,] -6.280850 19.449500 -6.280850 -0.486405 -0.826079
>  [3,] -0.486405 -6.280850 19.449500 -6.280850 -0.486405
>  [4,] -0.826079 -0.486405 -6.280850 19.449500 -6.280850
>  [5,] -0.167792 -0.826079 -0.486405 -6.280850 19.449500
>
> Question 4:
> ===========
> Your next command, "eigen(X)", would simply output the results
> to screen and does not assign to anything.
>
> Your next command "write(X,ncol=216,file="test.dat")" as it
> stands will write the toeplitz matrix X, constructed by
> your command "X<-toeplitz(y)" to file, but with 216
> columns instead of 217. However, the result consists
> simply of numbers, and there is nothing like "NA" or "NaN"
> in the file which I get.
>
> Nor are there any NAs or NaNs in X itself, of course.
>
> But, when you yourself did "write(X,ncol=216,file="test.dat")",
> perhaps the "X" in this command was different from the "X"
> which is the toeplitz matrix. So, was it the result of an
> assignment from "eigen(X)" and, if so, which component or
> components?
>
> Question/Comment 5:
> ===================
> So I have tried Z<-eigen(X). First of all, I get no problems
> with NAs or NaNs:
>
>  which(is.na(Z$values))    ##numeric(0)
>  which(is.nan(Z$values))   ##numeric(0)
>  which(is.na(Z$vectors))   ##numeric(0)
>  which(is.nan(Z$vectors))  ##numeric(0)
>
> Next, trying various options for wirting to file:
>
>  write(Z,ncol=216,file="test.dat")
>
> simply does not work (not a writable structure), while
>
>  write(Z$values,ncol=216,file="test.dat")
>
> produces simply a set of numbers, no NAs of NaNs, and likewise
>
>  write(Z$vectors,ncol=216,file="test.dat")
>
> (the only occurrences of non-numeric characters are "e", as
> in "e-05").
>
>>> reports the following columns full of NaN's: 18, 58, 194, 200.
>>> (Note that eigen(X,symmetric=T) makes no difference and I get
>>> the same as above).
>
> Therefore I find myself completely unable to reproduce your
> problem. However, for the various reasons stated in detail
> above, I am not at all sure that what you wrote as the statement
> of what you did in fact corresponds to what you really did!
>
> I even wonder whether
>
> Question 6:
> ===========
> Was the file "test.dat" the result of your "write" command?
> Or was it left over from a previous activity, the "write"
> from this session having failed to execute for some reason?
> (In which case the NaNs would have nothing to do with the
> results of "eigen(X)").
>
>
>>> The second attachment contains only the eigenvectors
>>> obtained on calling a LAPACK routine directly (from C).
>>> The eigenvalues are essentially the same as that obtained
>>> using R. Here, I use the LAPACK-recommended double
>>> precision routine dspevd() routine for symmetric matrices
>>> in packed storage format.
>>> Note the absence of the NaN's....I would be happy to send
>>> my C programs to whoever is interested.
>
> Well, I didn't get any NaNs in R either -- quite consistent
> with your C program!
>
> Please clarify according to the questions above.
>
> Best wishes,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 03-May-05                                       Time: 12:06:57
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Jordi.Molins at drkw.com  Tue May  3 14:42:30 2005
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Tue, 3 May 2005 14:42:30 +0200
Subject: [R] memory error message using MASS and GLMMGibbs
Message-ID: <C5A76BA0CA4D734CA725124C4D6397AC8BFCA4@ibfftce502.de.ad.drkw.net>


Hello,

I was just testing the MASS code examples for chapter 10 (Random and Mixed
Effects) and I have pasted the following code in an R session (2.1.0 in
windows 2000 professional; I have also Xemacs + ESS installed, but I was not
using them at that time; my machine has quite a lot of RAM):

library(MASS)
library(lattice)
library(nlme)
library(GLMMGibbs)
# declare a random intercept for each subject
epil$subject <- Ra(data = factor(epil$subject))
glmm(y ~ lbase*trt + lage + V4 + subject, family = poisson,
     data = epil, keep = 100000, thin = 100)

and then an Application Error appears: 

"The instruction at "0x1001edc9" referenced memory at "0x00000008". The
memory could not be "written". "

It does not take long for this message to appear (less than 1s after I type
Enter).

Any help is welcome

Jordi




--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From walmir-rodrigues at uol.com.br  Tue May  3 14:53:19 2005
From: walmir-rodrigues at uol.com.br (walmir-rodrigues)
Date: Tue,  3 May 2005 09:53:19 -0300
Subject: [R] Step wise regression
Message-ID: <IFWZSV$980956B6152854A97BB17FC8EA634D80@uol.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050503/93ad9fed/attachment.pl

From gregor.gorjanc at bfro.uni-lj.si  Tue May  3 14:52:50 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Tue, 03 May 2005 14:52:50 +0200
Subject: [R] "Special" characters in URI
In-Reply-To: <42776341.2060905@maths.lth.se>
References: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>	<Pine.LNX.4.61.0505030709580.22671@gannet.stats>
	<42775851.1090308@bfro.uni-lj.si> <42776341.2060905@maths.lth.se>
Message-ID: <42777422.9060001@bfro.uni-lj.si>

Henrik Bengtsson wrote:
> Gregor GORJANC wrote:
...
>> What do you think about this scratch, which afcourse doesn't solve all
>> "special" characters:
>>
>> fixURLchar <- function(URL,
>>                        from = c(" ", "\"", ",", "#"),
>>                        to = c("%20", "%22", "%2c", "%23"))
> 
> 
> Just a comment. It is much safer/easier to use named vectors for
> mapping, e.g.
> 
>  map <- c(" "="%20", "\""="%22", ","="%2c", "#"="%23")
> 
...

Henrik, thanks. So you suggest something like

for (i in seq(along=map)) {
    URL <- gsub(pattern=names(map)[i], replacement=map[i], x=URL)
}

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From francoisromain at free.fr  Tue May  3 15:00:17 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 03 May 2005 15:00:17 +0200
Subject: [R] Step wise regression
In-Reply-To: <IFWZSV$980956B6152854A97BB17FC8EA634D80@uol.com.br>
References: <IFWZSV$980956B6152854A97BB17FC8EA634D80@uol.com.br>
Message-ID: <427775E1.1090005@free.fr>

Le 03.05.2005 14:53, walmir-rodrigues a ??crit :

>Dear Fellows,
>
>How can I do to proced a step wise regression in R, if it??s possible ?
>
>Thanks,
>
>Walmir 
>  
>
?step
?stepAIC

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From christoph.lehmann at gmx.ch  Tue May  3 16:25:12 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 03 May 2005 16:25:12 +0200
Subject: [R] RMySQL installation: libz missing SOLVED
In-Reply-To: <42775868.9070104@gmx.ch>
References: <42775868.9070104@gmx.ch>
Message-ID: <427789C8.50504@gmx.ch>

it seemed to be a problem with the rpm for suse 9.1.. I installed and 
compiled R2.1 using the sources, then installation of RMySQL succeeded

mmmh ..

Christoph

Christoph Lehmann wrote:
> Hi
> I run suse linux 9.1 and I installed MySQL server, client, devel, bench.
> DBI is installed, when I try to install RMySQL I get an error saying, 
> that libz is missing.
> 
> (paths to libs were set:export PKG_CPPFLAGS="-I/usr/include/mysql/"
> export PKG_LIBS="-L/usr/lib/mysql/ -lmysqlclient")
> 
> so my question: where do I get the libz files (are these mysql files? if 
> yes, why were they not installed at least by mysql-devel?)
> 
> thanks for your kind help
> 
> christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From itsme_410 at yahoo.com  Tue May  3 15:15:39 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Tue, 3 May 2005 06:15:39 -0700 (PDT)
Subject: Fwd: Re: [R] eigenvalues of a circulant matrix
In-Reply-To: <XFMail.050503120750.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20050503131539.4737.qmail@web54504.mail.yahoo.com>

Thanks for looking into this! Sure, I will try and provide more info, but sorry
I seem to be doing really badly with posting....


> 
> Having cut&pasted from the data placed in the body of the
> message (omitted here) I get 216 numbers. Having put these
> in a vector x (in my own way):
> 
>   length(x)
>   ##[1] 216


> 
> Question 1:
> ===========
> Is this correct? Or has there been a problem with your
> posting of the data?

Yes, this is correct: it is supposed to be such that the circulant matrix is
symmetric and so Toeplitz. I am just ignoring the rest (which are pretty close
to x[107:1].


> 
> If it is correct, given that you seem to only use x[1:109],
> was there some point in giving the rest?

No, I should have deleted it -- sorry.

> Question 2:
> ===========
> Next, using your command:
> 
>   y<-x[c(109:1,0:108)]
> 
> I now get
> 
>   length(y)
>   ##[1] 217

My mistake: that should be 1 instead of 0. I sincerely apologize!
> 
> (as expected). The "0" in "0:108" seems to have been ignored
> (again as expected), so this is equivalent to
> 
>   y<-x[c(109:1,1:108)]
> 
> Is this as intended? If so, why use "0:108" instead of "1:108"?
> Check:
> 
>   y[1]    ##[1] 19.4495
>   x[109]  ##[1] 19.4495
> 
>   y[109]  ##[1] -0.00116801
>   x[1]    ##[1] -0.00116801
> 
>   y[110]  ##[1] -0.00116801
>   x[1]    ##[1] -0.00116801
> 
>   y[217]  ##[1] -6.28085
>   x[108]  ##[1] -6.28085
> 
> Can you confirm that this is as intended?
> 
> Comment 3:
> ==========
> You next command X=toeplitz(y): No apparent problems,
> it gives a symmetric result:
> 
>   which(X != t(X)) ## numeric(0)
> 
> with 217 rows and columns:
> 
>   dim(X)  ##[1] 217 217
> 
> and looks circulant:
> 
>   X[(1:5),(1:5)]
>             [,1]      [,2]      [,3]      [,4]      [,5]
>   [1,] 19.449500 -6.280850 -0.486405 -0.826079 -0.167792
>   [2,] -6.280850 19.449500 -6.280850 -0.486405 -0.826079
>   [3,] -0.486405 -6.280850 19.449500 -6.280850 -0.486405
>   [4,] -0.826079 -0.486405 -6.280850 19.449500 -6.280850
>   [5,] -0.167792 -0.826079 -0.486405 -6.280850 19.449500
> 
> Question 4:
> ===========
> Your next command, "eigen(X)", would simply output the results
> to screen and does not assign to anything.

true. But in my case, eigen(X)$vectors indicates the four columns to be NaN.

> Your next command "write(X,ncol=216,file="test.dat")" as it
> stands will write the toeplitz matrix X, constructed by
> your command "X<-toeplitz(y)" to file, but with 216
> columns instead of 217. However, the result consists
> simply of numbers, and there is nothing like "NA" or "NaN"
> in the file which I get.

I made a mistake in typing -- it is 1:108, instead of 0. The file test.dat
contains the symmetric circulant matrix on which I run the eigendecomposition
using  LAPACK.

> Nor are there any NAs or NaNs in X itself, of course.

No, there are none.

> 
> But, when you yourself did "write(X,ncol=216,file="test.dat")",
> perhaps the "X" in this command was different from the "X"
> which is the toeplitz matrix. So, was it the result of an
> assignment from "eigen(X)" and, if so, which component or
> components?

No, no, X was the Toeplitz matrix and all the confusion stems from my typo
(extremely sorry again!)

> Question/Comment 5:
> ===================
> So I have tried Z<-eigen(X). First of all, I get no problems
> with NAs or NaNs:
> 
>   which(is.na(Z$values))    ##numeric(0)
>   which(is.nan(Z$values))   ##numeric(0)
>   which(is.na(Z$vectors))   ##numeric(0)
>   which(is.nan(Z$vectors))  ##numeric(0)
> 
> Next, trying various options for wirting to file:
> 
>   write(Z,ncol=216,file="test.dat")
> 
> simply does not work (not a writable structure), while
> 
>   write(Z$values,ncol=216,file="test.dat")
> 
> produces simply a set of numbers, no NAs of NaNs, and likewise
> 
>   write(Z$vectors,ncol=216,file="test.dat")
> 
> (the only occurrences of non-numeric characters are "e", as
> in "e-05").
> 
> >> reports the following columns full of NaN's: 18, 58, 194, 200.
> >> (Note that eigen(X,symmetric=T) makes no difference and I get
> >> the same as above).
> 
> Question 6:
> ===========
> Was the file "test.dat" the result of your "write" command?
> Or was it left over from a previous activity, the "write"
> from this session having failed to execute for some reason?
> (In which case the NaNs would have nothing to do with the
> results of "eigen(X)").
> 
> 
> >> The second attachment contains only the eigenvectors
> >> obtained on calling a LAPACK routine directly (from C).
> >> The eigenvalues are essentially the same as that obtained
> >> using R. Here, I use the LAPACK-recommended double
> >> precision routine dspevd() routine for symmetric matrices
> >> in packed storage format.
> >> Note the absence of the NaN's....I would be happy to send
> >> my C programs to whoever is interested.
> 
> Well, I didn't get any NaNs in R either -- quite consistent
> with your C program!
> 
> Please clarify according to the questions above.
> 
> Best wishes,
> Ted.
> 

I am very sorry for all the errors and extra extra work that my typo caused
you.I should have been more careful. I apologize again!

Many thanks and best wishes!



From f.harrell at vanderbilt.edu  Tue May  3 15:21:02 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 03 May 2005 08:21:02 -0500
Subject: [R] Step wise regression
In-Reply-To: <IFWZSV$980956B6152854A97BB17FC8EA634D80@uol.com.br>
References: <IFWZSV$980956B6152854A97BB17FC8EA634D80@uol.com.br>
Message-ID: <42777ABE.8090807@vanderbilt.edu>

walmir-rodrigues wrote:
> Dear Fellows,
> 
> How can I do to proced a step wise regression in R, if it??s possible ?
> 
> Thanks,
> 
> Walmir 

Here is an easy approach that will yield results only slightly less 
valid than one actually using the response variable:

x <- data.frame(x1,x2,x3,x4,..., other potential predictors)
x[,sample(ncol(x))]

:-)   -Frank


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Charles.Annis at StatisticalEngineering.com  Tue May  3 15:20:40 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Tue, 3 May 2005 09:20:40 -0400
Subject: [R] comparing lm(), survreg( ... ,
	dist="gaussian") and survreg(... , dist="lognormal")
In-Reply-To: <Pine.LNX.4.61.0505030728280.22908@gannet.stats>
Message-ID: <200505031320.j43DKVsJ004837@hypatia.math.ethz.ch>

Thank you, Professor Ripley.

cbind(log(pr1$fit) - 1.96*pr1$se.fit/pr1$fit, log(pr1$fit) +
1.96*pr1$se.fit/pr1$fit)

... is precisely what had eluded me, self-evident though it appears after
you have illuminated the way.

Again, thank you. 


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
Sent: Tuesday, May 03, 2005 2:55 AM
To: Charles Annis, P.E.
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] comparing lm(), survreg( ... , dist="gaussian") and
survreg(... , dist="lognormal")

On Mon, 2 May 2005, Charles Annis, P.E. wrote:

> I have tried everything I can think of and hope not to appear too foolish
> when my error is pointed out to me.
>
> I have some real data (18 points) that look linear on a log-log plot so I
> used them for a comparison of lm() and survreg.  There are no suspensions.
>
> survreg.df <- data.frame(Cycles=c(2009000, 577000, 145000, 376000, 37000,
> 979000, 17420000, 71065000, 46397000, 70168000, 69120000, 68798000,
> 72615000, 133051000, 38384000, 15204000, 1558000, 14181000), stress=c(90,
> 100, 110, 90, 100, 80, 70, 60, 56, 62, 62, 59, 56, 53, 59, 70, 90, 70),
> event=rep(1, 18))
>
>
> sN.lm<- lm(log(Cycles) ~ log10(stress), data=survreg.df)
>
> and
>                                             vvvvvvvvvvv
> gaussian.survreg<- survreg(formula=Surv(time=log(Cycles), event) ~
> log10(stress), dist="gaussian", data=survreg.df)
>
> produce identical parameter estimates and differ slightly in the residual
> standard error and scale, which is accounted for by scale being the MLE
and
> thus biased.  Correcting by sqrt(18/16) produces agreement.  Using
predict()
> for the lm, and predict.survreg() for the survreg model and correcting for
> the differences in stdev, produces identical plots of the fit and the
upper
> and lower confidence intervals.  All of this is as it should be.

I trust you called predict() on both and let R choose the method.

> And,
>                                               vvvvvv
> lognormal.survreg<- survreg(formula=Surv(time=(Cycles), event) ~
> log10(stress), dist="lognormal", data=survreg.df)
>
> produces summary() results that are identical to the earlier call to
> survreg(), except for the call, of course.  The parameter estimates and SE
> are identical.  Again this is as I would expect it.
>
> But since the call uses Cycles, rather than log(Cycles) predict.survreg()
> returns $fit in Cycles units, rather than logs, and of course the fits are
> identical when plotted on a log-log grid and also agree with lm()
>
> Here is the fly in the ointment:  The upper and lower confidence
intervals,
> based on the $se.fit for the dist="lognormal" are quite obviously
different
> from the other two methods, and although I have tried everything I could
> imagine I cannot reconcile the differences.

How did you do this?  (BTW, I assume you mean upper and lower confidence 
>limits< for the predicted means.)  For the predictions and standard 
errors are (or should be) on the response scale, a non-linear function of 
the parameters.  In that case it is normal to form confidence limits on 
the linear predictor scale and transform.

> I believe that the confidence bounds for both models should agree.  After
> all, both calls to survreg() produce identical parameter estimates.

They will, if computed on the same basis.  On log-scale (to avoid large 
numbers)

pr1 <- predict(lognormal.survreg, se.fit=T)
log(cbind(pr1$fit - 1.96*pr1$se.fit, pr1$fit + 1.96*pr1$se.fit))
pr2 <- predict(gaussian.survreg, se.fit=T)
cbind(pr2$fit - 1.96*pr2$se.fit, pr2$fit + 1.96*pr2$se.fit)

are really pretty close.  The main difference is a slight shift, which 
comes about because the mean of a log(X) is not log(mean(X)).  Note that 
the second set at the preferred ones.  Transforming to log scale before 
making the confidence limits:

cbind(log(pr1$fit) - 1.96*pr1$se.fit/pr1$fit, log(pr1$fit) +
1.96*pr1$se.fit/pr1$fit)

does give identical answers.

Consider care is needed in interpreting what predict() is actually 
predicting in non-linear models.  For both glm() and survreg() it is 
closer to the median of the uncertainty in the predictions than to the 
mean.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From hb at maths.lth.se  Tue May  3 15:20:31 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 03 May 2005 15:20:31 +0200
Subject: [R] "Special" characters in URI
In-Reply-To: <42777422.9060001@bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>	<Pine.LNX.4.61.0505030709580.22671@gannet.stats>
	<42775851.1090308@bfro.uni-lj.si> <42776341.2060905@maths.lth.se>
	<42777422.9060001@bfro.uni-lj.si>
Message-ID: <42777A9F.8040701@maths.lth.se>

Gregor GORJANC wrote:
> Henrik Bengtsson wrote:
> 
>>Gregor GORJANC wrote:
> 
> ...
> 
>>>What do you think about this scratch, which afcourse doesn't solve all
>>>"special" characters:
>>>
>>>fixURLchar <- function(URL,
>>>                       from = c(" ", "\"", ",", "#"),
>>>                       to = c("%20", "%22", "%2c", "%23"))
>>
>>
>>Just a comment. It is much safer/easier to use named vectors for
>>mapping, e.g.
>>
>> map <- c(" "="%20", "\""="%22", ","="%2c", "#"="%23")
>>
> 
> ...
> 
> Henrik, thanks. So you suggest something like
> 
> for (i in seq(along=map)) {
>     URL <- gsub(pattern=names(map)[i], replacement=map[i], x=URL)
> }
> 

Yes, something like that. To optimize, you might want to do

patterns <- names(map);
for (i in seq(along=map)) {
   URL <- gsub(pattern=patterns[i], replacement=map[i], x=URL)
}

More important is that you treat a standard "%" different from a "%" 
used in encoding, e.g. how do you want to convert the string "100% %20"? 
You probably have to utilize more "fancy" regular expressions to detect 
a standard "%". Maybe "%[^0-9a-fA-F]" will do. There should be much more 
details in the document Brian Ripley refered you to.

In other words, you have to be careful and try to think through all 
cases you function may be called. A good test is to call it twice, once 
on your original string and the on the escaped on; you should get the 
same result. It depends how complete you want your function to be.

Good luck

Henrik



From itsme_410 at yahoo.com  Tue May  3 15:27:31 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Tue, 3 May 2005 06:27:31 -0700 (PDT)
Subject: Fwd: Re: [R] eigenvalues of a circulant matrix
In-Reply-To: <Pine.LNX.4.61.0505031247160.25777@gannet.stats>
Message-ID: <20050503132731.93045.qmail@web54508.mail.yahoo.com>


--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> I think we need to make clear that eigen() by default relies on LAPACK 
> routines and they in turn rely on BLAS routines.  We have seen several 
> instances in which LAPACK/BLAS return NaNs when they should not, but all 
> that I am aware of are when (broken) external libraries were used.
> 
> So the occurrence of NaNs should lead you to question other aspects of 
> your computational environment.  If you have such a broken environment,
> calling eigen(EISPACK=TRUE) may be a palliative, but it is better to track 
> the problem down.
>

Dear Professor Ripley,

Very interesting! eigen(EISPACK=TRUE) indeed does not return NaN's. So, you
think my system is broken? How would anyone figure this out? However, why does
my C program work fine, using the same LAPACK? (I must say that LAPACK was
broken for a while on Fedora Core 3 -- when it was released, as documented on
bugzilla, but a recompile with a fortran option supposedly took care of it.)

Many thanks and best wishes!



From itsme_410 at yahoo.com  Tue May  3 15:29:27 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Tue, 3 May 2005 06:29:27 -0700 (PDT)
Subject: Fwd: Re: [R] eigenvalues of a circulant matrix
In-Reply-To: <Pine.LNX.4.61.0505031247160.25777@gannet.stats>
Message-ID: <20050503132927.78952.qmail@web54506.mail.yahoo.com>

--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> I think we need to make clear that eigen() by default relies on LAPACK 
> routines and they in turn rely on BLAS routines.  We have seen several 
> instances in which LAPACK/BLAS return NaNs when they should not, but all 
> that I am aware of are when (broken) external libraries were used.
> 
> So the occurrence of NaNs should lead you to question other aspects of 
> your computational environment.  If you have such a broken environment,
> calling eigen(EISPACK=TRUE) may be a palliative, but it is better to track 
> the problem down.
>

Dear Professor Ripley,

Very interesting! eigen(EISPACK=TRUE) indeed does not return NaN's. So, you
think my system is broken? How would anyone figure this out? However, why does
my C program work fine, using the same LAPACK? (I must say that LAPACK was
broken for a while on Fedora Core 3 -- when it was released, as documented on
bugzilla, but a recompile with a fortran option supposedly took care of it.)

Many thanks and best wishes!



From deepayan at stat.wisc.edu  Tue May  3 15:30:17 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 3 May 2005 08:30:17 -0500
Subject: [R] Lattice dotplot with symbols sized and colored
In-Reply-To: <6.2.0.14.0.20050503122112.039d8360@bute.st-and.ac.uk>
References: <6.2.0.14.0.20050503122112.039d8360@bute.st-and.ac.uk>
Message-ID: <200505030830.17364.deepayan@stat.wisc.edu>

On Tuesday 03 May 2005 06:39, David Kidd wrote:
> Sundar suggested setting up a theme which I have done...
>
> my.theme <- function() {
>    theme <- col.whitebg()
>    symb <- theme$superpose.symbol
>    symb$col=
> c("red","blue","green","yellow","orange","black","purple") #symb$pch
> is c(1,3,6,0,5,16,17) by default for col.whitebg
> theme$superpose.symbol <- symb
>    theme
> }
> trellis.par.set(theme = my.theme())
> # from ?dotplot
> dotplot(sporder ~ cvarorder | direct, data=sp.nc.bdrs.data,
> groups = sp.nc.bdrs.data$mysign,
> cex=abs(sp.nc.bdrs.data$mwZ * 0.05),
> xlab="climate variables",
> ylab="species",
> scales = list(y = list(labels = as.character(spname),at = 1:12, cex =
> 0.5), x = list(labels = as.character(my.ylabel), at = 1:66, rot=90,
> cex = 0.5), alternating = 3))
>
>
> Now if I am understanding lattice correctly setting symb$cex sets the
> size multiplier for symbols within each group but does not support
> the differential sizing of symbols for each record independently
> within each group. Hence I have tried to use a 'global' cex (after
> the groups statement), however, this has no effect on the displayed
> symbol size. Do I need to change the pch or another parameter to draw
> sized filled circles as is the default symbol for dotplot?

Yes, canned features of lattice are not enough for this. Here's 
something that should work:

with(sp.nc.bdrs.data,
     dotplot(sporder ~ cvarorder | direct, 
             col.var = factor(mysign),
             cex.var = abs(mwZ),
             col = c('green', 'red'),
             panel = function(x, y, cex, col, cex.var, col.var,
                              subscripts, ...) {
                 panel.dotplot(x, y,
                               col = col[col.var[subscripts]],
                               cex = cex.var[subscripts] * 0.05, ...)
             }))

Here's a version with the barley data (not a very good example, though):

with(barley,
     dotplot(site ~ yield, 
             col.var = factor(year),
             cex.var = yield,
             col = c('green', 'red'),
             panel = function(x, y, cex, col, cex.var, col.var,
                              subscripts, ...) {
                 panel.dotplot(x, y,
                               col = col[col.var[subscripts]],
                               cex = cex.var[subscripts] * 0.03, ...)
             }))

Deepayan



From andy_liaw at merck.com  Tue May  3 15:43:52 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 3 May 2005 09:43:52 -0400
Subject: [R] Step wise regression
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076EBB@usctmx1106.merck.com>

> From: Frank E Harrell Jr
> 
> walmir-rodrigues wrote:
> > Dear Fellows,
> > 
> > How can I do to proced a step wise regression in R, if it??s 
> possible ?
> > 
> > Thanks,
> > 
> > Walmir 
> 
> Here is an easy approach that will yield results only slightly less 
> valid than one actually using the response variable:
> 
> x <- data.frame(x1,x2,x3,x4,..., other potential predictors)
> x[,sample(ncol(x))]

Hmm...  Shouldn't that be something like:

x[, sample(ncol(x), ceiling(ncol(x) * runif(1)))]

?

Cheers,
Andy

 
> :-)   -Frank
> 
> 
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   
> Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From detlef.steuer at hsu-hh.de  Tue May  3 15:46:37 2005
From: detlef.steuer at hsu-hh.de (Detlef Steuer)
Date: Tue, 3 May 2005 15:46:37 +0200
Subject: [R] RMySQL installation: libz missing SOLVED
In-Reply-To: <427789C8.50504@gmx.ch>
References: <42775868.9070104@gmx.ch>
	<427789C8.50504@gmx.ch>
Message-ID: <20050503154637.6d05354f.detlef.steuer@hsu-hamburg.de>

Hi,

do you have zlib installed on your system?

(rpm -q zlib tells you if or not)

You can also try rpm -q --whatprovides libz .

zlib provides libz. If not installed, the manual compilation of R 
uses its own version of zlib, if I understand it correctly.

So, not really a problem with the rpm.

Detlef



On Tue, 03 May 2005 16:25:12 +0200
Christoph Lehmann <christoph.lehmann at gmx.ch> wrote:

> it seemed to be a problem with the rpm for suse 9.1.. I installed and 
> compiled R2.1 using the sources, then installation of RMySQL succeeded
> 
> mmmh ..
> 
> Christoph



From gregor.gorjanc at bfro.uni-lj.si  Tue May  3 15:48:52 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Tue, 03 May 2005 15:48:52 +0200
Subject: [R] "Special" characters in URI
In-Reply-To: <42777A9F.8040701@maths.lth.se>
References: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>	<Pine.LNX.4.61.0505030709580.22671@gannet.stats>
	<42775851.1090308@bfro.uni-lj.si> <42776341.2060905@maths.lth.se>
	<42777422.9060001@bfro.uni-lj.si> <42777A9F.8040701@maths.lth.se>
Message-ID: <42778144.5000905@bfro.uni-lj.si>

Henrik Bengtsson wrote:
...
>> Henrik, thanks. So you suggest something like
>>
>> for (i in seq(along=map)) {
>>     URL <- gsub(pattern=names(map)[i], replacement=map[i], x=URL)
>> }
>>
> 
> Yes, something like that. To optimize, you might want to do
> 
> patterns <- names(map);
> for (i in seq(along=map)) {
>   URL <- gsub(pattern=patterns[i], replacement=map[i], x=URL)
> }
> 
Do I gain anything more than readability by this optimization?

> More important is that you treat a standard "%" different from a "%"
> used in encoding, e.g. how do you want to convert the string "100% %20"?
> You probably have to utilize more "fancy" regular expressions to detect
> a standard "%". Maybe "%[^0-9a-fA-F]" will do. There should be much more
> details in the document Brian Ripley refered you to.
> 
> In other words, you have to be careful and try to think through all
> cases you function may be called. A good test is to call it twice, once
> on your original string and the on the escaped on; you should get the
> same result. It depends how complete you want your function to be.
Thanks again.

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From dmk2 at st-andrews.ac.uk  Tue May  3 15:46:03 2005
From: dmk2 at st-andrews.ac.uk (David Kidd)
Date: Tue, 03 May 2005 14:46:03 +0100
Subject: [R] Lattice dotplot with symbols sized and colored
Message-ID: <6.2.0.14.0.20050503144401.03b47230@bute.st-and.ac.uk>


Thank you very much Deepayan your code produces a beautiful plot.


----------------------------------------------------------------------------------------------
David M. Kidd
Research Assistant
School of Biology
Room 210, Sir Harold Mitchell Building
University of St. Andrews
St. Andrews, Fife
KY16 9TH
UK
http://www.st-and.ac.uk/~bugs/dave/dave.htm
Tel:  +44 (0)1334 463348
Fax: +44 (0)1334 463600



From kevinvol2002 at yahoo.com  Tue May  3 15:53:24 2005
From: kevinvol2002 at yahoo.com (Hai Lin)
Date: Tue, 3 May 2005 06:53:24 -0700 (PDT)
Subject: [R] Converting pc files to unix
In-Reply-To: 6667
Message-ID: <20050503135324.88597.qmail@web32414.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050503/aaf66e40/attachment.pl

From cAugenstein at student.ethz.ch  Tue May  3 16:30:24 2005
From: cAugenstein at student.ethz.ch (Clemens Augenstein)
Date: Tue, 03 May 2005 16:30:24 +0200
Subject: [R] Problem: R =?iso-8859-15?q?l=E4sst_sich_nicht_starten?=
Message-ID: <42778B00.9090105@student.ethz.ch>

Hallo,
ich schreibe einfach mal deutsch, und hoffe dass Du das auch verstehst 
(if not write me back in English).
OS X 10.3.9
Ich habe R 2.1.0 installiert, und das Programm hat auch funktioniert. 
Will wenig sp??ter wieder mit R arbeiten, es l??sst sich aber nicht mehr 
starten. "Das Programm R wurde unerwartet beendet", w??hrend ich versuche 
es durch Doppelklicken auf das R.app-symbol zum laufen zu bringen. 
L??sche alles was mit R zu tun hat (.app und die frameworks) und versuche 
es mit Version 2.0.1. Das l??sst sich aber von Anfang an nicht starten 
(gleiches Syndrom wie die 2.1.0 Version). Installiere wieder 2.1.0, und 
oh wunder, es funktioniert. Aber nur bis ich versuche ein Histogramm 
abzuspeichern, dann R wurde wieder unerwartet (von mir schon ein bischen 
erwartet) beendet. Starte R von Neuem, versuche wieder ein Histo ab zu 
speichern, wieder das gleiche. Von nun an l??sst sich R wieder gar nicht 
mehr starten.
Ich habe hier noch die Fehlermeldung angeh??ngt, die mir mein System 
angeboten hat an Apple zu schicken.
Mit sch??nen Gr??ssen

Clemens


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: FehlermeldungOSX10.3.9
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050503/e0efe8a6/FehlermeldungOSX10.3.pl

From hb at maths.lth.se  Tue May  3 16:33:17 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 03 May 2005 16:33:17 +0200
Subject: [R] "Special" characters in URI
In-Reply-To: <42778144.5000905@bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>	<Pine.LNX.4.61.0505030709580.22671@gannet.stats>
	<42775851.1090308@bfro.uni-lj.si> <42776341.2060905@maths.lth.se>
	<42777422.9060001@bfro.uni-lj.si> <42777A9F.8040701@maths.lth.se>
	<42778144.5000905@bfro.uni-lj.si>
Message-ID: <42778BAD.5080101@maths.lth.se>

Gregor GORJANC wrote:
> Henrik Bengtsson wrote:
> ...
> 
>>>Henrik, thanks. So you suggest something like
>>>
>>>for (i in seq(along=map)) {
>>>    URL <- gsub(pattern=names(map)[i], replacement=map[i], x=URL)
>>>}
>>>
>>
>>Yes, something like that. To optimize, you might want to do
>>
>>patterns <- names(map);
>>for (i in seq(along=map)) {
>>  URL <- gsub(pattern=patterns[i], replacement=map[i], x=URL)
>>}
>>
> 
> Do I gain anything more than readability by this optimization?

Yes, names() is only called once and not length(map) times. You won't 
probably notice it, but it is a good custom to do the above. Sometimes 
you're dealing with much larger vectors and then it will pay off.

/Henrik


> 
>>More important is that you treat a standard "%" different from a "%"
>>used in encoding, e.g. how do you want to convert the string "100% %20"?
>>You probably have to utilize more "fancy" regular expressions to detect
>>a standard "%". Maybe "%[^0-9a-fA-F]" will do. There should be much more
>>details in the document Brian Ripley refered you to.
>>
>>In other words, you have to be careful and try to think through all
>>cases you function may be called. A good test is to call it twice, once
>>on your original string and the on the escaped on; you should get the
>>same result. It depends how complete you want your function to be.
> 
> Thanks again.
>



From mkondrin at hppi.troitsk.ru  Tue May  3 16:28:08 2005
From: mkondrin at hppi.troitsk.ru (mkondrin)
Date: Tue, 03 May 2005 18:28:08 +0400
Subject: [R] grid and ps device (bg-color)
In-Reply-To: <4272B31F.8030005@stat.auckland.ac.nz>
References: <42721B5A.2060606@hppi.troitsk.ru>
	<4272B31F.8030005@stat.auckland.ac.nz>
Message-ID: <42778A78.8070501@hppi.troitsk.ru>

Hi!

>
>
> grid.rect(width=2, height=2,
>           gp=gpar(fill=ps.options()$bg.color))
>
Yes, it works (more or less), but the best is to use
grid.rect(width=unit(1,"npc")+unit(0.5,"inches"),...)
as postscript device leaves a 0.25-inch frame around the page.
BTW wouldn't you fix a bug with the determination of "strwidth" in the 
code like this:
l<-c("1","23","456","7890")
unit(1,"strwidth",l)
The last command calculates the width of the first element of l, which 
is not the thing one would like to have. The best would be the 
calculation of maximum length of vector's elements (a simple for-cycle 
in unit.c). It also solves the problems with
t<-grid.text(l,y=c(0.9,0.8,0.7,0.6))
unit(1,"grobwidth",t)
where the desired answer should be the length of the longest string in l 
(in R-2.0.1 this returns the length of the l[[0]] too).
I have fixed it in my R installation but the patch is obvious.
Thank you.



From reid_huntsinger at merck.com  Tue May  3 16:25:07 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 3 May 2005 10:25:07 -0400
Subject: [R] eigenvalues of a circulant matrix
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9407@uswpmx00.merck.com>

How was your R 2.0.1 built? Which Lapack did it link to, and which one does
it load? Which BLAS? Is the BLAS threaded? Does it link to the optimized
pthreads library? Etc. (I'm not a Fedora Core 3 user so I'm not sure what
the default setup is, and of course I don't know if that's what you have.) 

As I pointed out in my previous post, in eigen() the matrix is "inspected
for symmetry" so symmetric=TRUE is the same as not specifying this at all.
You could try symmetric=FALSE...

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Globe Trotter
Sent: Monday, May 02, 2005 10:51 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] eigenvalues of a circulant matrix


OK, here we go:

I am submitting two attachments. The first is the datafile called kinv used
to
create my circulant matrix, using the following commands:


x<-scan("kinv")
y<-x[c(109:1,0:108)]
X=toeplitz(y)
eigen(X)
write(X,ncol=216,file="test.dat")

reports the following columns full of NaN's: 18, 58, 194, 200. (Note that
eigen(X,symmetric=T) makes no difference and I get the same as above).

The second attachment contains only the eigenvectors obtained on calling a
LAPACK routine directly (from C). The eigenvalues are essentially the same
as
that obtained using R. Here, I use the LAPACK-recommended double precision
routine dspevd() routine for symmetric matrices in packed storage format.
Note
the absence of the NaN's....I would be happy to send my C programs to
whoever
is interested.

I am using 

:~> uname -a
Linux 2.6.11-1.14_FC3 #1 Thu Apr 7 19:23:49 EDT 2005 i686 i686 i386
GNU/Linux

and R.2.0.1.

Many thanks and best wishes!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue May  3 17:10:08 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 3 May 2005 17:10:08 +0200 (CEST)
Subject: [R] Nonparametric Tukey-type multiple comparisons "Nemenyi" test
In-Reply-To: <4276BB90.7090706@noaa.gov>
References: <4276BB90.7090706@noaa.gov>
Message-ID: <Pine.LNX.4.51.0505031706030.11199@artemis.imbe.med.uni-erlangen.de>


> I am trying to do a Nonparametric Tukey-type multiple comparison
> post-hoc test to determine which groups are significantly different.

the manual page to `oneway_test' (in package `coin') has an example of
the Nemenyi-Damico-Wolfe-Dunn test taken from Hollander & Wolfe (1999).

Best,

Torsten


> I
> have read the dialogue on this topic from the R-help, and am still not
> clear why no statistical packages include this test as an option?  Is it
> not an appropriate test to conduct on non-normally distributed data?  Is
> the only option to calculate it by hand using the (Zar 1996) formula?
>
> Thank you in advance for your help.
>
> --
> Rikki Grober- Dunsmore
> National Marine Fisheries Service
> National Marine Protected Areas Center
> 110 Shaffer Rd.
> Santa Cruz, CA 95060
> 831-420-3991
>
>
> Unless someone like you,
> Cares a whole awful lot,
> Nothing is going to get better,
> It's not.
>        - The Lorax, by Dr. Seuss, 1971
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From Ted.Harding at nessie.mcc.ac.uk  Tue May  3 17:00:10 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 03 May 2005 16:00:10 +0100 (BST)
Subject: [R] Step wise regression
In-Reply-To: <42777ABE.8090807@vanderbilt.edu>
Message-ID: <XFMail.050503160010.Ted.Harding@nessie.mcc.ac.uk>

On 03-May-05 Frank E Harrell Jr wrote:
> walmir-rodrigues wrote:
>> Dear Fellows,
>> 
>> How can I do to proced a step wise regression in R, if it??s possible ?
>> 
>> Thanks,
>> 
>> Walmir 
> 
> Here is an easy approach that will yield results only slightly less 
> valid than one actually using the response variable:
> 
> x <- data.frame(x1,x2,x3,x4,..., other potential predictors)
> x[,sample(ncol(x))]
> 
>:-)   -Frank

Frank, you are a sneaky subversive! But you have given me a
technical clue for a project long near to my heart (with
Andy's important "random parsimony" refinement).

This is to implement software-driven Clinical Trials articles,
for submission to standard peer-reviewed journals.

The underlying engine would be the dada-engine. For samples
of what this can generate, visit

  http://www.elsewhere.org/cgi-bin/postmodern

repeatedly (or click on the "To generate another essay, follow
this link" link you will find just below the article generated;
repeat ad libitum).

Also at the foot of the page, you will find links to explanations
of how it works. See also

  http://dev.null.org/dadaengine/

The basis is that articles of certain kinds have a predictable
structure, and utilise terminology, phrases, sentence-structures
and substantive elements drawn from typical usage, and are such
that the real thing is indistiguishable from the results of sampling
these elements at random, under the control of a rulebased recursive
transition network automaton.

Tailor-made for Clinical Trials, methinks. (Mind you, someone
once did a lot of work setting up the rulebase for the PostModern
Lit Crit genre which is what you get in the above samples).

Best wishes,
Ted.



From fpepin at cs.mcgill.ca  Tue May  3 17:24:05 2005
From: fpepin at cs.mcgill.ca (Francois Pepin)
Date: Tue, 03 May 2005 11:24:05 -0400
Subject: [R] Re: [BioC] "Special" characters in URI
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B700C2@pollux.bfro.uni-lj.si>
Message-ID: <1115133845.10331.3.camel@ybrig.MCB.McGill.CA>

There are safe ways of encoding URLs that contain funny characters:
  (space) %20
[ %5B
] %5D

so your url would be:

URL<-'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc%20g%5Bau%5D'

That makes your snippet work just fine.

http://www.macromedia.com/cfusion/knowledgebase/index.cfm?id=tn_14143
has the list.

Francois

On Mon, 2005-05-02 at 19:46, Gorjanc Gregor wrote:
> Hello!
> 
> I am crossposting this to R-help and BioC, since it is relevant to both
> groups. 
> 
> I wrote a wrapper for Entrez search utility (link for this is provided bellow), 
> which can add some new search functionality to existing code in Bioconductor's
> package 'annotate'*.
>  
> http://eutils.ncbi.nlm.nih.gov/entrez/query/static/esearch_help.html
> 
> Entrez search utuility returns a XML document but I have a problem to
> use URI to retrieve that file, since URI can also contain characters,
> which should not be there according to 
> 
> http://www.faqs.org/rfcs/rfc2396.html
> 
> I encountered problems with "[" and "]" as well as with space characters.
> However there might also be a problem with others i.e. reserved characters
> in URI syntax.
> 
> My R example is:
> 
> R> library("annotate")
> Loading required package: Biobase 
> Loading required package: tools 
> Welcome to Bioconductor 
>          Vignettes contain introductory material.  To view, 
>          simply type: openVignette() 
>          For details on reading vignettes, see
>          the openVignette help page.
> R> library(XML)
> R> tmp$term <- "gorjanc g[au]"
> R> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc g[au]"
> R> tmp
> $term
> [1] "gorjanc g[au]"
> 
> $URL
> [1] "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc g[au]"
> R> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
> Error in xmlTreeParse(tmp$URL, isURL = TRUE, handlers = NULL, asTree = TRUE) : 
>         error in creating parser for http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc g[au]
> 
> # so I have a problem with space and [ and ]
> # let's reduce a problem to just space or [] to be sure
> R> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc g"
> R> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
> Error in xmlTreeParse(tmp$URL, isURL = TRUE, handlers = NULL, asTree = TRUE) : 
>         error in creating parser for http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc g
> R> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc[au]"
> R> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
> Error in xmlTreeParse(tmp$URL, isURL = TRUE, handlers = NULL, asTree = TRUE) : 
>         error in creating parser for http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc[au]
> 
> # now show that it works fine without special chars
> R> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc"
> R> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
> $doc
> $file
> [1] "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc"
> 
> $version
> [1] "1.0"
> 
> $children
> ...
> 
> # now show a workaround for space
> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc%20g"
> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
> R> tmp$URL <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc%20g"
> R> xmlTreeParse(tmp$URL, isURL=TRUE, handlers=NULL, asTree=TRUE)
> $doc
> $file
> [1] "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=gorjanc%20g"
> 
> $version
> [1] "1.0"
> 
> $children
> ...
> 
> As can be seen from above there is a possibility to handle this special
> characters and I wonder if this has already been done somewhere? If not
> I thought on a function fixURLchar, which would replace reserved characters
> with ther escaped sequences. Any comments, pointers, ... ?
> 
> from = c(" ", "\"", ",", "#"),
> to = c("%20", "%22", "%2c", "%23"))
> 
> *When I'll solve problem I will send my code to 'annotate' maintainer 
> and he can include it at his will in a package. 
> 
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> ----------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> 
> _______________________________________________
> Bioconductor mailing list
> Bioconductor at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/bioconductor



From f.harrell at vanderbilt.edu  Tue May  3 17:27:39 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 03 May 2005 10:27:39 -0500
Subject: [R] Step wise regression
In-Reply-To: <XFMail.050503160010.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050503160010.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <4277986B.2080705@vanderbilt.edu>

(Ted Harding) wrote:
> On 03-May-05 Frank E Harrell Jr wrote:
> 
>>walmir-rodrigues wrote:
>>
>>>Dear Fellows,
>>>
>>>How can I do to proced a step wise regression in R, if it??s possible ?
>>>
>>>Thanks,
>>>
>>>Walmir 
>>
>>Here is an easy approach that will yield results only slightly less 
>>valid than one actually using the response variable:
>>
>>x <- data.frame(x1,x2,x3,x4,..., other potential predictors)
>>x[,sample(ncol(x))]
>>
>>:-)   -Frank
> 
> 
> Frank, you are a sneaky subversive! But you have given me a
> technical clue for a project long near to my heart (with
> Andy's important "random parsimony" refinement).

Now I get Andy's comment - missed before that we need to randomly set 
parsimony level then sample without replacement.


> 
> This is to implement software-driven Clinical Trials articles,
> for submission to standard peer-reviewed journals.
> 
> The underlying engine would be the dada-engine. For samples
> of what this can generate, visit
> 
>   http://www.elsewhere.org/cgi-bin/postmodern

What a great site!

> 
> repeatedly (or click on the "To generate another essay, follow
> this link" link you will find just below the article generated;
> repeat ad libitum).
> 
> Also at the foot of the page, you will find links to explanations
> of how it works. See also
> 
>   http://dev.null.org/dadaengine/
> 
> The basis is that articles of certain kinds have a predictable
> structure, and utilise terminology, phrases, sentence-structures
> and substantive elements drawn from typical usage, and are such
> that the real thing is indistiguishable from the results of sampling
> these elements at random, under the control of a rulebased recursive
> transition network automaton.
> 
> Tailor-made for Clinical Trials, methinks. (Mind you, someone
> once did a lot of work setting up the rulebase for the PostModern
> Lit Crit genre which is what you get in the above samples).

Great idea!!

Frank

> 
> Best wishes,
> Ted.
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From jjmichael at comcast.net  Tue May  3 17:33:23 2005
From: jjmichael at comcast.net (Jacob Michaelson)
Date: Tue, 03 May 2005 09:33:23 -0600
Subject: [R] Problem: R =?ISO-8859-15?Q?l=E4sst_sich_nicht_star?=
	=?ISO-8859-15?Q?ten?=
In-Reply-To: <42778B00.9090105@student.ethz.ch>
References: <42778B00.9090105@student.ethz.ch>
Message-ID: <427799C3.50802@comcast.net>

Ich habe schon lange mein Deutsch nicht ge??bt...

also aus diesem gleichen Grund habe ich nicht "geupgradet" (das Wort 
fehlt mir).  Ich glaube aber, dass das Problem liegt mit R.app und nicht 
R(framework)...ich hatte ??hnliche Probleme beim Upgrade ... R im 
Terminal funktionierte, doch R.app startete nicht.  Hast du das neuste 
Build von R.app?

Sorry I can't be of more help, but I think some bugs simply need to be 
worked out with R.app.  You'll probably just have to be patient...unless 
you want to use R from the terminal, which seems to work okay.  As far 
as I know they're still doing nightly builds of R.app, so check back 
every day to see if it works.

--Jake

Clemens Augenstein wrote:

> Hallo,
> ich schreibe einfach mal deutsch, und hoffe dass Du das auch verstehst 
> (if not write me back in English).
> OS X 10.3.9
> Ich habe R 2.1.0 installiert, und das Programm hat auch funktioniert. 
> Will wenig sp??ter wieder mit R arbeiten, es l??sst sich aber nicht mehr 
> starten. "Das Programm R wurde unerwartet beendet", w??hrend ich 
> versuche es durch Doppelklicken auf das R.app-symbol zum laufen zu 
> bringen. L??sche alles was mit R zu tun hat (.app und die frameworks) 
> und versuche es mit Version 2.0.1. Das l??sst sich aber von Anfang an 
> nicht starten (gleiches Syndrom wie die 2.1.0 Version). Installiere 
> wieder 2.1.0, und oh wunder, es funktioniert. Aber nur bis ich 
> versuche ein Histogramm abzuspeichern, dann R wurde wieder unerwartet 
> (von mir schon ein bischen erwartet) beendet. Starte R von Neuem, 
> versuche wieder ein Histo ab zu speichern, wieder das gleiche. Von nun 
> an l??sst sich R wieder gar nicht mehr starten.
> Ich habe hier noch die Fehlermeldung angeh??ngt, die mir mein System 
> angeboten hat an Apple zu schicken.
> Mit sch??nen Gr??ssen
>
> Clemens
>
>
>------------------------------------------------------------------------
>
>Date/Time:      2005-05-03 16:14:00 +0200
>OS Version:     10.3.9 (Build 7W98)
>Report Version: 2
>
>Command: R
>Path:    /Applications/R.app/Contents/MacOS/R
>Version: 1.10 (1.10)
>PID:     533
>Thread:  0
>
>Exception:  EXC_BAD_ACCESS (0x0001)
>Codes:      KERN_INVALID_ADDRESS (0x0001) at 0x4080d006
>
>Thread 0 Crashed:
>0   com.apple.CoreFoundation     0x901c0f74 CFRetain + 0x20
>1   com.apple.CoreFoundation     0x901dcc74 CFArrayCreate + 0x144
>2   com.apple.Foundation         0x90a6c60c -[NSArray initWithObjects:] + 0xbc
>3   org.R-project.R              0x00003034 -[RController init] + 0x174
>4   com.apple.AppKit             0x92f6bd80 -[NSCustomObject nibInstantiate] + 0x10c
>5   com.apple.AppKit             0x92e9af38 -[NSIBObjectData instantiateObject:] + 0xbc
>6   com.apple.AppKit             0x92ea1f64 -[NSIBObjectData nibInstantiateWithOwner:topLevelObjects:] + 0x88
>7   com.apple.AppKit             0x92f93d04 loadNib + 0xfc
>8   com.apple.AppKit             0x92eeaf28 +[NSBundle(NSNibLoading) _loadNibFile:nameTable:withZone:ownerBundle:] + 0x2e8
>9   com.apple.AppKit             0x92f69e00 +[NSBundle(NSNibLoading) loadNibFile:externalNameTable:withZone:] + 0x9c
>10  com.apple.AppKit             0x92f7b5f4 +[NSBundle(NSNibLoading) loadNibNamed:owner:] + 0x174
>11  com.apple.AppKit             0x92f69c68 NSApplicationMain + 0x174
>12  org.R-project.R              0x000028f0 _start + 0x188 (crt.c:267)
>13  dyld                         0x8fe1a278 _dyld_start + 0x64
>
>PPC Thread State:
>  srr0: 0x901c0f74 srr1: 0x0000f030                vrsave: 0x00000000
>    cr: 0x22000248  xer: 0x00000018   lr: 0x901c0f6c  ctr: 0x901c5acc
>    r0: 0x901dcc74   r1: 0xbffff5f0   r2: 0x22000248   r3: 0x4080d000
>    r4: 0x4080d000   r5: 0x004033aa   r6: 0x004033aa   r7: 0x004033aa
>    r8: 0x004033aa   r9: 0x0000000e  r10: 0xffffffff  r11: 0x00000001
>   r12: 0x901c5acc  r13: 0x00000000  r14: 0x00000000  r15: 0x00000000
>   r16: 0x00330160  r17: 0xa2eaac40  r18: 0xa2e9ac40  r19: 0xa2eaac40
>   r20: 0xa2eaac40  r21: 0x00038dd4  r22: 0x00000000  r23: 0x0000000f
>   r24: 0xbffff738  r25: 0xbffff73c  r26: 0x0039d388  r27: 0xa01c2e94
>   r28: 0x0000000f  r29: 0x0039d340  r30: 0x0039d384  r31: 0x901c0f6c
>
>Binary Images Description:
>    0x1000 -    0x35fff org.R-project.R 1.10    /Applications/R.app/Contents/MacOS/R
>  0x205000 -   0x226fff libreadline.5.0.dylib     /Library/Frameworks/R.framework/Resources/lib/libreadline.5.0.dylib
> 0x1008000 -  0x1194fff libR.dylib     /Library/Frameworks/R.framework/Resources/lib/libR.dylib
>0x806c0000 - 0x806e9fff libxslt.1.dylib     /usr/lib/libxslt.1.dylib
>0x80830000 - 0x8090efff libxml2.2.dylib     /usr/lib/libxml2.2.dylib
>0x8dcb0000 - 0x8dcb2fff com.apple.ExceptionHandling 1.2 (???)    /System/Library/Frameworks/ExceptionHandling.framework/Versions/A/ExceptionHandling
>0x8fe00000 - 0x8fe4ffff dyld     /usr/lib/dyld
>0x90000000 - 0x9014ffff libSystem.B.dylib     /usr/lib/libSystem.B.dylib
>0x901c0000 - 0x9026dfff com.apple.CoreFoundation 6.3.7 (299.35)    /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation
>0x902b0000 - 0x90529fff com.apple.CoreServices.CarbonCore 10.3.7    /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CarbonCore.framework/Versions/A/CarbonCore
>0x90584000 - 0x905f3fff com.apple.framework.IOKit 1.3.6 (???)    /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit
>0x90610000 - 0x9069afff com.apple.CoreServices.OSServices 3.0.1    /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/OSServices.framework/Versions/A/OSServices
>0x90700000 - 0x90700fff com.apple.CoreServices 10.3 (???)    /System/Library/Frameworks/CoreServices.framework/Versions/A/CoreServices
>0x90720000 - 0x90787fff com.apple.audio.CoreAudio 2.1.2    /System/Library/Frameworks/CoreAudio.framework/Versions/A/CoreAudio
>0x907c7000 - 0x907d2fff libCSync.A.dylib     /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/Resources/libCSync.A.dylib
>0x907f0000 - 0x907f9fff com.apple.DiskArbitration 2.0.5    /System/Library/PrivateFrameworks/DiskArbitration.framework/Versions/A/DiskArbitration
>0x90810000 - 0x90810fff com.apple.ApplicationServices 1.0 (???)    /System/Library/Frameworks/ApplicationServices.framework/Versions/A/ApplicationServices
>0x90830000 - 0x9089ffff libobjc.A.dylib     /usr/lib/libobjc.A.dylib
>0x908c5000 - 0x90915fff com.apple.HIServices 1.4.1 (0.0.1d1)    /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/HIServices.framework/Versions/A/HIServices
>0x90940000 - 0x909b3fff com.apple.DesktopServices 1.2.4    /System/Library/PrivateFrameworks/DesktopServicesPriv.framework/Versions/A/DesktopServicesPriv
>0x909f0000 - 0x90a08fff com.apple.WebServices 1.1.1 (1.1.0)    /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/WebServicesCore.framework/Versions/A/WebServicesCore
>0x90a20000 - 0x90b7bfff com.apple.Foundation 6.3.6 (500.58)    /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation
>0x90c32000 - 0x90c45fff com.apple.speech.synthesis.framework 3.2    /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/SpeechSynthesis.framework/Versions/A/SpeechSynthesis
>0x90d00000 - 0x90d1bfff com.apple.SystemConfiguration 1.7.1 (???)    /System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration
>0x90d40000 - 0x90d40fff com.apple.Carbon 10.3 (???)    /System/Library/Frameworks/Carbon.framework/Versions/A/Carbon
>0x90ec0000 - 0x90ec0fff com.apple.Cocoa 6.3 (???)    /System/Library/Frameworks/Cocoa.framework/Versions/A/Cocoa
>0x910b0000 - 0x91101fff com.apple.bom 1.2.5 (63.2)    /System/Library/PrivateFrameworks/Bom.framework/Versions/A/Bom
>0x912a0000 - 0x912bdfff com.apple.audio.SoundManager 3.8    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/CarbonSound.framework/Versions/A/CarbonSound
>0x912e0000 - 0x912f7fff com.apple.LangAnalysis 1.5.4    /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/LangAnalysis.framework/Versions/A/LangAnalysis
>0x91303000 - 0x9136cfff com.apple.htmlrendering 1.1.2    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HTMLRendering.framework/Versions/A/HTMLRendering
>0x913a0000 - 0x9145ffff ColorSync     /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ColorSync.framework/Versions/A/ColorSync
>0x915e0000 - 0x91699fff com.apple.QD 3.4.67 (???)    /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/QD.framework/Versions/A/QD
>0x916e0000 - 0x91773fff com.apple.print.framework.PrintCore 3.3    /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/PrintCore.framework/Versions/A/PrintCore
>0x917b6000 - 0x917cafff libCGATS.A.dylib     /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/Resources/libCGATS.A.dylib
>0x917e0000 - 0x917f0fff com.apple.speech.recognition.framework 3.3    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SpeechRecognition.framework/Versions/A/SpeechRecognition
>0x91810000 - 0x9182afff com.apple.openscripting 1.2.1 (???)    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/OpenScripting.framework/Versions/A/OpenScripting
>0x91850000 - 0x91860fff com.apple.ImageCapture 2.1.5    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/ImageCapture.framework/Versions/A/ImageCapture
>0x91890000 - 0x9189cfff com.apple.help 1.0.1    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Help.framework/Versions/A/Help
>0x918c0000 - 0x918cdfff com.apple.CommonPanels 1.2.1 (1.0)    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/CommonPanels.framework/Versions/A/CommonPanels
>0x918f0000 - 0x9193efff com.apple.print.framework.Print 3.3    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Print.framework/Versions/A/Print
>0x91990000 - 0x9199bfff com.apple.securityhi 1.2 (90)    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SecurityHI.framework/Versions/A/SecurityHI
>0x91a40000 - 0x91ab3fff com.apple.NavigationServices 3.3.3    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/NavigationServices.framework/Versions/A/NavigationServices
>0x91b10000 - 0x91b2afff libPDFRIP.A.dylib     /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/Resources/libPDFRIP.A.dylib
>0x91b50000 - 0x91b5ffff libPSRIP.A.dylib     /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/Resources/libPSRIP.A.dylib
>0x91b80000 - 0x91b93fff libRIP.A.dylib     /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/Resources/libRIP.A.dylib
>0x92070000 - 0x92096fff com.apple.FindByContent 1.4 (1.2)    /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/FindByContent.framework/Versions/A/FindByContent
>0x920c0000 - 0x922a7fff com.apple.security 2.4 (177)    /System/Library/Frameworks/Security.framework/Versions/A/Security
>0x92430000 - 0x92468fff com.apple.LaunchServices 10.3.5 (98.4)    /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/LaunchServices
>0x927b2000 - 0x927e0fff libssl.0.9.7.dylib     /usr/lib/libssl.0.9.7.dylib
>0x927f0000 - 0x92827fff com.apple.CFNetwork 1.2.1 (7)    /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CFNetwork.framework/Versions/A/CFNetwork
>0x92880000 - 0x92c05fff com.apple.HIToolbox 1.3.6 (???)    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/HIToolbox
>0x92e70000 - 0x9336ffff com.apple.AppKit 6.3.7 (743.36)    /System/Library/Frameworks/AppKit.framework/Versions/C/AppKit
>0x939a0000 - 0x939b4fff libcups.2.dylib     /usr/lib/libcups.2.dylib
>0x939d0000 - 0x939d4fff libmathCommon.A.dylib     /usr/lib/system/libmathCommon.A.dylib
>0x93b50000 - 0x93bf3fff com.apple.audio.toolbox.AudioToolbox 1.3.2    /System/Library/Frameworks/AudioToolbox.framework/Versions/A/AudioToolbox
>0x94120000 - 0x9414bfff libncurses.5.dylib     /usr/lib/libncurses.5.dylib
>0x9415d000 - 0x94165fff libbsm.dylib     /usr/lib/libbsm.dylib
>0x944c0000 - 0x944f8fff com.apple.AE 1.4    /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE
>0x945b0000 - 0x945b9fff libz.1.dylib     /usr/lib/libz.1.dylib
>0x94610000 - 0x9462afff libresolv.9.dylib     /usr/lib/libresolv.9.dylib
>0x94650000 - 0x946affff com.apple.SearchKit 1.0.2    /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SearchKit.framework/Versions/A/SearchKit
>0x94fe0000 - 0x95076fff com.apple.WebKit 312.1    /System/Library/Frameworks/WebKit.framework/Versions/A/WebKit
>0x954c0000 - 0x95ac6fff libBLAS.dylib     /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
>0x95b20000 - 0x95df0fff libLAPACK.dylib     /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib
>0x95e40000 - 0x95eadfff libvDSP.dylib     /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib
>0x95f00000 - 0x95f20fff libvMisc.dylib     /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib
>0x961b0000 - 0x96292fff com.apple.JavaScriptCore 1.1 (312)    /System/Library/Frameworks/WebKit.framework/Versions/A/Frameworks/JavaScriptCore.framework/Versions/A/JavaScriptCore
>0x968d0000 - 0x969b2fff libicucore.A.dylib     /usr/lib/libicucore.A.dylib
>0x96a20000 - 0x96ae2fff libcrypto.0.9.7.dylib     /usr/lib/libcrypto.0.9.7.dylib
>0x96b30000 - 0x96b30fff com.apple.audio.units.AudioUnit 1.3.2    /System/Library/Frameworks/AudioUnit.framework/Versions/A/AudioUnit
>0x96b50000 - 0x96bdffff ATS     /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/ATS
>0x96c00000 - 0x96c8cfff com.apple.ink.framework 101.1.4 (55.12)    /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Ink.framework/Versions/A/Ink
>0x96cb0000 - 0x96d9efff libiconv.2.dylib     /usr/lib/libiconv.2.dylib
>0x96e80000 - 0x96e90fff com.apple.vecLib 3.0.3 (vecLib 3.0.3)    /System/Library/Frameworks/vecLib.framework/Versions/A/vecLib
>0x96eca000 - 0x971a2fff com.apple.CoreGraphics 1.203.30 (???)    /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/CoreGraphics
>0x973e0000 - 0x976c6fff com.apple.WebCore 315    /System/Library/Frameworks/WebKit.framework/Versions/A/Frameworks/WebCore.framework/Versions/A/WebCore
>  
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jrclmilks at joimail.com  Tue May  3 17:34:11 2005
From: jrclmilks at joimail.com (Jim Milks)
Date: Tue, 3 May 2005 11:34:11 -0400
Subject: [R] Using "|"
Message-ID: <63fe52881dcdc124290c3aa826fd5d54@joimail.com>

Dear all,

I have a rodent dataset that I am reanalyzing.  The set consists of 
several variables (#Microtus captured, Grass stems/m^2, etc), among 
which is a Grid factor variable c(North,South).  I have evidence that 
there are significant differences in vegetation and rodent populations 
between the two locations (from chi-square tests) and would like to run 
separate analyses along North/South lines on the rest of my data.  For 
example, I type:

 > plot(Microtus.T~Grass | Grid)

However, I get the following:

Error in plot.window(xlim, ylim, log, asp, ...) :
	need finite xlim values
In addition: Warning messages:
1: "|" not meaningful for factors in: Ops.factor(Grass, Grid)
2: no finite arguments to min; returning Inf
3: no finite arguments to max; returning -Inf

Other than adding xlim arguments, what else can I do, especially in 
light of warning message 1?

Thank you in advance.

Sincerely,
Jim Milks

Graduate Student
Environmental Sciences Ph.D. Program
Wright State University
3640 Colonel Glenn Hwy
Dayton, OH 45435



From elvis at xlsolutions-corp.com  Tue May  3 17:36:36 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue,  3 May 2005 08:36:36 -0700
Subject: [R] Course***R/S-plus Fundamentals and Programming Techniques In
	New York
Message-ID: <20050503153636.20755.qmail@gem-wbe01.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" in New York: www.xlsolutions-corp.com/training.htm


****New York, NY ---------------------- May 26th - 27th,2005


Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From david.arenas at navy.mil  Tue May  3 17:44:07 2005
From: david.arenas at navy.mil (Arenas, David R.  CIV NAVAIR DEPT)
Date: Tue, 3 May 2005 08:44:07 -0700
Subject: [R] Combining numeric vs numeric & numeric vs factor graphs into
	one ps/pdf file 
Message-ID: <2800A210138FAD4780C77CE452DDDD873D34B5@NAWESDNIEX05VA.nadsuswe.nads.navy.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050503/1e40e11b/attachment.pl

From spencer.graves at pdf.com  Tue May  3 17:52:51 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 03 May 2005 08:52:51 -0700
Subject: [R] General Question on learning R...
In-Reply-To: <faea59c94331334ace64224572930fb3@mail.nih.gov>
References: <e206273d0505030346ac59dc9@mail.gmail.com>
	<faea59c94331334ace64224572930fb3@mail.nih.gov>
Message-ID: <42779E53.4060008@pdf.com>

	  I'm looking at the same thing.  A good source for this is to 
'install.packages(c("fBasics", "fCalendar", "fExtremes", "fMultivar", 
"fOptions", "fPotfolio", "fractdiff", "fSeries", "its", "lme4", 
"zoo"))', then 'update.packages()'.  These will install subdirectories 
or folders with the indicated names "fBasics", etc., in "library" with 
your R installation.  For example, in my Windows installation, I have 
"D:\Program files\R\rw2010pat\library", which contains many subfolders 
including ones named "fSeries".  These all contain files "*.R", which 
provide sample code.

	  spencer graves

Sean Davis wrote:

> 
> On May 3, 2005, at 6:46 AM, Jonathan Q. wrote:
> 
>> In the process of learning R, with a specific interest on financial
>> time series.  While I continue to get through the documents I am more
>> a fan of learning by example and then looking up how each function is
>> used.  Any websites which post sample code for R?
>>
> 
> The largest source of example code is R itself.  If you have a command 
> in which you are interested, you can often just type the command and the 
> code will be shown to you.  Try typing:
> 
> ls()
> 
> Then:
> 
> ls
> 
> It will show you the code used to produce the result.  Also, each 
> command has its own example(s) in the help.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From renaud.lancelot at cirad.fr  Tue May  3 17:52:53 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Tue, 03 May 2005 18:52:53 +0300
Subject: [R] Step wise regression
In-Reply-To: <42777ABE.8090807@vanderbilt.edu>
References: <IFWZSV$980956B6152854A97BB17FC8EA634D80@uol.com.br>
	<42777ABE.8090807@vanderbilt.edu>
Message-ID: <42779E55.1060207@cirad.fr>

Frank E Harrell Jr a ??crit :
> walmir-rodrigues wrote:
> 
>> Dear Fellows,
>>
>> How can I do to proced a step wise regression in R, if it??s possible ?
>>
>> Thanks,
>>
>> Walmir 
> 
> 
> Here is an easy approach that will yield results only slightly less 
> valid than one actually using the response variable:
> 
> x <- data.frame(x1,x2,x3,x4,..., other potential predictors)
> x[,sample(ncol(x))]
> 
> :-)   -Frank
> 
> 
competing for the "fortune" award ?... ;-)

Renaud

-- 
Dr Renaud Lancelot, v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From davidhughjones at gmail.com  Tue May  3 17:53:40 2005
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Tue, 3 May 2005 16:53:40 +0100
Subject: [R] nlme: "Deficient rank in gls_loglik" when creating corAR1()
Message-ID: <f5d8480605050308532ddbd468@mail.gmail.com>

I have a bunch of data which is structured by year and US state, so I
have created a nlme groupedData object for it:

formula(gd2)
DEPVAR ~ YEAR | ABREV

Now I am trying to run a gls regression on it. I want the error
correlation structure to be AR1 with a different rho for each state,
so I do

> mdyn.1.1 = gls(model = DEPVAR ~ BLAH + BLAH, data=gd2, corr=corAR1(form= ~ YEAR | ABREV),na.action=na.omit)

YEAR and ABREV are always present; DEPVAR is absent for one state.

I get the following error message:

Error in logLik.glsStruct(glsSt, glsPars) :
        Deficient rank in gls_loglik

Can anyone enlighten me? The error message goes away if I just do
corAR1(form = ~1), but this is not meaningful for my data.

Cheers
David



From sundar.dorai-raj at pdf.com  Tue May  3 17:57:24 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 03 May 2005 08:57:24 -0700
Subject: [R] Using "|"
In-Reply-To: <63fe52881dcdc124290c3aa826fd5d54@joimail.com>
References: <63fe52881dcdc124290c3aa826fd5d54@joimail.com>
Message-ID: <42779F64.7000308@pdf.com>



Jim Milks wrote on 5/3/2005 8:34 AM:
> Dear all,
> 
> I have a rodent dataset that I am reanalyzing.  The set consists of 
> several variables (#Microtus captured, Grass stems/m^2, etc), among 
> which is a Grid factor variable c(North,South).  I have evidence that 
> there are significant differences in vegetation and rodent populations 
> between the two locations (from chi-square tests) and would like to run 
> separate analyses along North/South lines on the rest of my data.  For 
> example, I type:
> 
>  > plot(Microtus.T~Grass | Grid)
> 
> However, I get the following:
> 
> Error in plot.window(xlim, ylim, log, asp, ...) :
>     need finite xlim values
> In addition: Warning messages:
> 1: "|" not meaningful for factors in: Ops.factor(Grass, Grid)
> 2: no finite arguments to min; returning Inf
> 3: no finite arguments to max; returning -Inf
> 
> Other than adding xlim arguments, what else can I do, especially in 
> light of warning message 1?
> 
> Thank you in advance.
> 
> Sincerely,
> Jim Milks


Are you confusing "plot.formula" with "lattice:xyplot"? Your example 
should work if you use:

library(lattice)
xyplot(Microtus.T ~ Grass | Grid)

HTH,

--sundar



From itsme_410 at yahoo.com  Tue May  3 18:21:26 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Tue, 3 May 2005 09:21:26 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: 6667
Message-ID: <20050503162126.33391.qmail@web54509.mail.yahoo.com>

Hi,

The R was downloaded in binary form (Fedora Core 3 RPM) from a CRAN mirror. I
do not know which LAPACK it links to, but the only LAPACK on my machine is
lapack-3.0-28 (RPM installation). How does one figure out which BLAS or whether
it is threaded?

Many thanks and best wishes!

--- "Huntsinger, Reid" <reid_huntsinger at merck.com> wrote:
> How was your R 2.0.1 built? Which Lapack did it link to, and which one does
> it load? Which BLAS? Is the BLAS threaded? Does it link to the optimized
> pthreads library? Etc. (I'm not a Fedora Core 3 user so I'm not sure what
> the default setup is, and of course I don't know if that's what you have.) 
> 
> As I pointed out in my previous post, in eigen() the matrix is "inspected
> for symmetry" so symmetric=TRUE is the same as not specifying this at all.
> You could try symmetric=FALSE...
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Globe Trotter
> Sent: Monday, May 02, 2005 10:51 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] eigenvalues of a circulant matrix
> 
> 
> OK, here we go:
> 
> I am submitting two attachments. The first is the datafile called kinv used
> to
> create my circulant matrix, using the following commands:
> 
> 
> x<-scan("kinv")
> y<-x[c(109:1,0:108)]
> X=toeplitz(y)
> eigen(X)
> write(X,ncol=216,file="test.dat")
> 
> reports the following columns full of NaN's: 18, 58, 194, 200. (Note that
> eigen(X,symmetric=T) makes no difference and I get the same as above).
> 
> The second attachment contains only the eigenvectors obtained on calling a
> LAPACK routine directly (from C). The eigenvalues are essentially the same
> as
> that obtained using R. Here, I use the LAPACK-recommended double precision
> routine dspevd() routine for symmetric matrices in packed storage format.
> Note
> the absence of the NaN's....I would be happy to send my C programs to
> whoever
> is interested.
> 
> I am using 
> 
> :~> uname -a
> Linux 2.6.11-1.14_FC3 #1 Thu Apr 7 19:23:49 EDT 2005 i686 i686 i386
> GNU/Linux
> 
> and R.2.0.1.
> 
> Many thanks and best wishes!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From itsme_410 at yahoo.com  Tue May  3 18:24:40 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Tue, 3 May 2005 09:24:40 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <930cc1a05465343fa8c1456cac8dd049@soc.soton.ac.uk>
Message-ID: <20050503162440.59326.qmail@web54508.mail.yahoo.com>

Good point: the Bellman reference is a book:

Introduction to Matrix Analysis by Bellman (1960). McGraw-Hill Series in Matrix
Theory.


--- Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:

> Hi everyone.
> 
> The following webpage gives a definition of circulant matrix, which 
> agrees with the
> definition given in the magic package.
> 
> http://mathworld.wolfram.com/CirculantMatrix.html
> 
> best  wishes
> 
> rksh
> 
> 
> 
> On May 3, 2005, at 08:06 am, Mulholland, Tom wrote:
> 
> > Well since I know nothing about this topic I have lurked so far, but 
> > here's my two bob's worth.
> >
> > Firstly I tried to make sense of Brian's initial reply. I have got no 
> > idea who Bellman is and you have not referenced (his/her) work in a 
> > way I can access the issues you refer to. So I assumed that's exactly 
> > what Brian was talking about.
> >
> > Secondly.
> >
> > toeplitz(1:4)
> >      [,1] [,2] [,3] [,4]
> > [1,]    1    2    3    4
> > [2,]    2    1    2    3
> > [3,]    3    2    1    2
> > [4,]    4    3    2    1
> >
> > require(magic)
> >  circulant(4)
> >      [,1] [,2] [,3] [,4]
> > [1,]    1    2    3    4
> > [2,]    4    1    2    3
> > [3,]    3    4    1    2
> > [4,]    2    3    4    1
> >
> > So they are obviously two different things. Although I think you may 
> > have implied (not stated) that the particular combination you were 
> > using resulted in both being exactly the same.
> >
> > It does appear as if in this case the (X) matrix is circulant. But 
> > then I'm no expert in even such simple things.
> >
> > Then I had no idea where I was going. So I tried the variations in 
> > eigen.
> >
> > I ran you code
> > x<-scan("h:/t.txt")
> > y<-x[c(109:216,1:108)]
> > X<-toeplitz(y)
> >  and then
> >
> >> X[is.na(X)]
> > numeric(0)
> >
> > So I didn't get any NAs
> >
> > t1 <- eigen(X)$vectors
> > t2 <- eigen(X,symmetric = TRUE)$vectors
> >> identical(t1,t2)
> > [1] TRUE
> >>
> >
> > Then
> >
> > t2 <- eigen(X,symmetric = TRUE,EISPACK = TRUE)$vectors
> >> identical(t1,t2)
> > [1] FALSE
> >>
> >
> > So there'e obviously more than one way of getting the vectors. Does 
> > the second one make more sense to you?
> >
> > I also noticed in the eigen help that there are references to issues 
> > such as "IEEE 754 arithmetic","(They may also differ between methods 
> > and between platforms.)" and "or Hermitian if complex". All of these 
> > are out of my competence but they do signal to me that there are 
> > issues which may relate to hardware, digital arithmetic and other 
> > things of that ilk.
> >
> > I added the comment about complex because I have a vague idea that 
> > they are related to imaginary parts that you refer to.
> >
> > So not coming to any conclusion that makes sense to me, and given that 
> > there are often threads about supposed inaccuracies that have answers 
> > such as the digits you see are not always what are held by the machine 
> > I set my options(digits = 22) and noticed that some of the numbers are 
> > still going at the 22 decimal place suggesting that the machine might 
> > be incapable of producing perfectly accurate results using digital 
> > arithmetic.
> >
> > My other big sphere of ignorance is complex numbers.
> >
> > So I tried
> > X<-toeplitz(complex(real = y))
> > t1 <- eigen(X)$vectors
> >
> >> t1[1:20]
> >  [1]  0.068041577278880341+0i -0.068041577140546913+0i  
> > 0.068041576864811659+0i -0.068041576452430155+0i
> >  [5]  0.068041575907139579+0i -0.068041575231135451+0i  
> > 0.068041574435267163+0i -0.068041573525828514+0i
> >  [9]  0.068041572538722991+0i -0.068041571498323253+0i  
> > 0.068041570619888622+0i -0.068041570256170081+0i
> > [13]  0.068041568759931989+0i -0.068041566476633147+0i  
> > 0.068041563560502477+0i -0.068041560000305007+0i
> > [17]  0.068041555538765813+0i -0.068041549792984865+0i  
> > 0.068041544123969511+0i -0.068041537810956801+0i
> >> t2[1:20]
> >  [1]  0.068041381743976906 -0.068041381743976850  0.068041381743976781 
> > -0.068041381743976753  0.068041381743976587
> >  [6] -0.068041381743976725  0.068041381743976920 -0.068041381743976836 
> >  0.068041381743976892 -0.068041381743976781
> > [11]  0.068041381743976781 -0.068041381743977392  0.068041381743976725 
> > -0.068041381743976753  0.068041381743976753
> > [16] -0.068041381743976698  0.068041381743976587 -0.068041381743976642 
> >  0.068041381743976698 -0.068041381743976490
> >>
> >
> >
> > Which is again different. I have no idea what I'm doing but you do 
> > seem to get slightly different answers depending upon which method you 
> > use. I do not know if one is superior to the others or where one draws 
> > the line in terms of accuracy.
> >
> > Tom
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch
> >> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Globe Trotter
> >> Sent: Tuesday, 3 May 2005 10:51 AM
> >> To: r-help at stat.math.ethz.ch
> >> Subject: Re: [R] eigenvalues of a circulant matrix
> >>
> >>
> >> OK, here we go:
> >>
> >> I am submitting two attachments. The first is the datafile
> >> called kinv used to
> >> create my circulant matrix, using the following commands:
> >>
> >>
> >> x<-scan("kinv")
> >> y<-x[c(109:1,0:108)]
> >> X=toeplitz(y)
> >> eigen(X)
> >> write(X,ncol=216,file="test.dat")
> >>
> >> reports the following columns full of NaN's: 18, 58, 194,
> >> 200. (Note that
> >> eigen(X,symmetric=T) makes no difference and I get the same as above).
> >>
> >> The second attachment contains only the eigenvectors obtained
> >> on calling a
> >> LAPACK routine directly (from C). The eigenvalues are
> >> essentially the same as
> >> that obtained using R. Here, I use the LAPACK-recommended
> >> double precision
> >> routine dspevd() routine for symmetric matrices in packed
> >> storage format. Note
> >> the absence of the NaN's....I would be happy to send my C
> >> programs to whoever
> >> is interested.
> >>
> >> I am using
> >>
> >> :~> uname -a
> >> Linux 2.6.11-1.14_FC3 #1 Thu Apr 7 19:23:49 EDT 2005 i686
> >> i686 i386 GNU/Linux
> >>
> >> and R.2.0.1.
> >>
> >> Many thanks and best wishes!
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> >
> --
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
>



From davidhughjones at gmail.com  Tue May  3 18:49:11 2005
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Tue, 3 May 2005 17:49:11 +0100
Subject: [R] Combining numeric vs numeric & numeric vs factor graphs into
	one ps/pdf file
In-Reply-To: <f5d8480605050309487b3fb8d3@mail.gmail.com>
References: <2800A210138FAD4780C77CE452DDDD873D34B5@NAWESDNIEX05VA.nadsuswe.nads.navy.mil>
	<f5d8480605050309487b3fb8d3@mail.gmail.com>
Message-ID: <f5d84806050503094957efd15e@mail.gmail.com>

Hi David

You probably want to write your own panel function and pass it into
xyplot(). Something like

mypanel <- function (x,y, groups, subscripts) {
  if (status[subscripts] == "pass") {
    panel.xyplot(H,CD)
  }
  else {
     panel.xyplot(site, CD)
  }
}

Check out the "groups" and "subscripts" arguments to xyplot.

cheers
Dave

On 03/05/05, Arenas, David R.  CIV NAVAIR DEPT <david.arenas at navy.mil> wrote:
> Dear R community,
>
> My previous email was incomplete because I used html format.  Here it is again and sorry for any inconvenience:
>
> xyplot (lattice) has been great in displaying tons of data for my research.  I have used the following two xyplot commands (with example dataframe) to create two separate postscript/pdf files with respect to the variable "acft" and subset "status":
>
> test.df <- data.frame(acft=factor(c("A","B","C","D")),
>                               status=factor(c("fail","pass","fail","pass")),
>                               site=factor(c("E1","E1","E2","E2")),
>                               CD=as.numeric(c(1,1,3,3)),
>                               H=as.numeric(c(80,NA,60,NA)))
>
> xyplot(H ~ CD | acft,
>           data=test.df,
>           subset=status=="fail",
>           layout=c(1,1) )
>
> xyplot(site ~ CD | acft,
>           data=test.df,
>           subset=status=="pass",
>           layout=c(1,1) )
>
>  I would like to combine all graphs into one file in alphabetical order of variable "acft".  The graphs would be one per page where in fact I use layout=c(1,1) for the nice and easily seen strip labels for "acft".  The problem I am having is combining x-y plots that are numeric vs numeric & numeric vs factor.  I have search the R-help archives and R-project references for an example to no avail.  I am thinking I may have to use something (lattice or not) like ...
>
> if any(test.df$Status=="fail")
> plot(H ~ CD)
> else
> plot(site ~ CD)
>
> with "for" in the beginning to loop through all data with respect to acft.  I need a hint on how to further this along.  I am using R.2.1.0 via Windows XP.
>
> Thank you for any help,
>
> D. Arenas
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From gierdien_sta_staff_sci_main_uct at mail.uct.ac.za  Tue May  3 18:52:13 2005
From: gierdien_sta_staff_sci_main_uct at mail.uct.ac.za (gierdien_sta_staff_sci_main_uct@mail.uct.ac.za)
Date: Tue, 03 May 2005 18:52:13 +0200
Subject: [R] (no subject)
Message-ID: <1115139133.4277ac3d6d946@webmail.uct.ac.za>


Hi

I'm a postgraduate student and I am currently busy with EVT. I am a new user to
R and have been making use of the fExtremes package. Could you please tell me
how to assign "blocks" by "months"?

Your help will be greatly appreciated.

Thanking you
Abdullah Gierdien



From reid_huntsinger at merck.com  Tue May  3 19:29:43 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 3 May 2005 13:29:43 -0400
Subject: [R] eigenvalues of a circulant matrix
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9408@uswpmx00.merck.com>

Under Linux run "ldd" on the binary to see what shared libraries the binary
has been linked against and will attempt to load. The command you run is a
shell script which sets and exports LD_LIBRARY_PATH and then runs R, so to
be sure first start R and execute

> system("echo $LD_LIBRARY_PATH")

and then using this value for LD_LIBRARY_PATH do

$ export LD_LIBRARY_PATH=<R's value for this>
$ ldd `R RHOME`/bin/exec/R

which will list the shared libraries R will try to load to resolve links to
shared libraries. 

Having said that, I have tried your example on several platforms with
several configurations, and I get NaNs only when R is calling the Lapack
routine dsyevr (R's builtin Lapack) with an external (optimized) blas
(either Goto or ATLAS's blas) but not R's builtin blas. Moreover even with
the optimized blas, if I use dysev instead of dsyevr, (eigen() executes a
.Call("La_rs",x,only.values,"dsyevr",PACKAGE="base"), I execute this from R
with "dsyev" in place of "dsyevr") I get no NaNs. 

So it looks like a problem with a blas routine used by dsyevr but not dsyev,
but I have yet to confirm.

I note that your C program does not use either of these lapack routines.

Reid Huntsinger


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Globe Trotter
Sent: Tuesday, May 03, 2005 12:21 PM
To: Huntsinger, Reid; r-help at stat.math.ethz.ch
Subject: RE: [R] eigenvalues of a circulant matrix


Hi,

The R was downloaded in binary form (Fedora Core 3 RPM) from a CRAN mirror.
I
do not know which LAPACK it links to, but the only LAPACK on my machine is
lapack-3.0-28 (RPM installation). How does one figure out which BLAS or
whether
it is threaded?

Many thanks and best wishes!

--- "Huntsinger, Reid" <reid_huntsinger at merck.com> wrote:
> How was your R 2.0.1 built? Which Lapack did it link to, and which one
does
> it load? Which BLAS? Is the BLAS threaded? Does it link to the optimized
> pthreads library? Etc. (I'm not a Fedora Core 3 user so I'm not sure what
> the default setup is, and of course I don't know if that's what you have.)

> 
> As I pointed out in my previous post, in eigen() the matrix is "inspected
> for symmetry" so symmetric=TRUE is the same as not specifying this at all.
> You could try symmetric=FALSE...
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Globe Trotter
> Sent: Monday, May 02, 2005 10:51 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] eigenvalues of a circulant matrix
> 
> 
> OK, here we go:
> 
> I am submitting two attachments. The first is the datafile called kinv
used
> to
> create my circulant matrix, using the following commands:
> 
> 
> x<-scan("kinv")
> y<-x[c(109:1,0:108)]
> X=toeplitz(y)
> eigen(X)
> write(X,ncol=216,file="test.dat")
> 
> reports the following columns full of NaN's: 18, 58, 194, 200. (Note that
> eigen(X,symmetric=T) makes no difference and I get the same as above).
> 
> The second attachment contains only the eigenvectors obtained on calling a
> LAPACK routine directly (from C). The eigenvalues are essentially the same
> as
> that obtained using R. Here, I use the LAPACK-recommended double precision
> routine dspevd() routine for symmetric matrices in packed storage format.
> Note
> the absence of the NaN's....I would be happy to send my C programs to
> whoever
> is interested.
> 
> I am using 
> 
> :~> uname -a
> Linux 2.6.11-1.14_FC3 #1 Thu Apr 7 19:23:49 EDT 2005 i686 i686 i386
> GNU/Linux
> 
> and R.2.0.1.
> 
> Many thanks and best wishes!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> 
> 
>
----------------------------------------------------------------------------
--
> Notice:  This e-mail message, together with any attachment...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From davidhughjones at gmail.com  Tue May  3 20:00:52 2005
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Tue, 3 May 2005 19:00:52 +0100
Subject: [R] Re: nlme: "Deficient rank in gls_loglik" when creating corAR1()
In-Reply-To: <f5d8480605050308532ddbd468@mail.gmail.com>
References: <f5d8480605050308532ddbd468@mail.gmail.com>
Message-ID: <f5d8480605050311005fd7e4f7@mail.gmail.com>

Is this a bug? Should I attach a test case?

D

On 03/05/05, David Hugh-Jones <davidhughjones at gmail.com> wrote:
> I have a bunch of data which is structured by year and US state, so I
> have created a nlme groupedData object for it:
> 
> formula(gd2)
> DEPVAR ~ YEAR | ABREV
> 
> Now I am trying to run a gls regression on it. I want the error
> correlation structure to be AR1 with a different rho for each state,
> so I do
> 
> > mdyn.1.1 = gls(model = DEPVAR ~ BLAH + BLAH, data=gd2, corr=corAR1(form= ~ YEAR | ABREV),na.action=na.omit)
> 
> YEAR and ABREV are always present; DEPVAR is absent for one state.
> 
> I get the following error message:
> 
> Error in logLik.glsStruct(glsSt, glsPars) :
>         Deficient rank in gls_loglik
> 
> Can anyone enlighten me? The error message goes away if I just do
> corAR1(form = ~1), but this is not meaningful for my data.
> 
> Cheers
> David
>



From jqm475 at gmail.com  Tue May  3 20:06:48 2005
From: jqm475 at gmail.com (Jonathan Q.)
Date: Tue, 3 May 2005 14:06:48 -0400
Subject: [R] General Question on learning R...
In-Reply-To: <42779E53.4060008@pdf.com>
References: <e206273d0505030346ac59dc9@mail.gmail.com>
	<faea59c94331334ace64224572930fb3@mail.nih.gov>
	<42779E53.4060008@pdf.com>
Message-ID: <e206273d050503110648a51696@mail.gmail.com>

assuming one has these installed already, you just look in the demo
folder under each?  i.e., fBasics\demo ???

On 5/3/05, Spencer Graves <spencer.graves at pdf.com> wrote:
>          I'm looking at the same thing.  A good source for this is to
> 'install.packages(c("fBasics", "fCalendar", "fExtremes", "fMultivar",
> "fOptions", "fPotfolio", "fractdiff", "fSeries", "its", "lme4",
> "zoo"))', then 'update.packages()'.  These will install subdirectories
> or folders with the indicated names "fBasics", etc., in "library" with
> your R installation.  For example, in my Windows installation, I have
> "D:\Program files\R\rw2010pat\library", which contains many subfolders
> including ones named "fSeries".  These all contain files "*.R", which
> provide sample code.
> 
>          spencer graves
> 
> Sean Davis wrote:
> 
> >
> > On May 3, 2005, at 6:46 AM, Jonathan Q. wrote:
> >
> >> In the process of learning R, with a specific interest on financial
> >> time series.  While I continue to get through the documents I am more
> >> a fan of learning by example and then looking up how each function is
> >> used.  Any websites which post sample code for R?
> >>
> >
> > The largest source of example code is R itself.  If you have a command
> > in which you are interested, you can often just type the command and the
> > code will be shown to you.  Try typing:
> >
> > ls()
> >
> > Then:
> >
> > ls
> >
> > It will show you the code used to produce the result.  Also, each
> > command has its own example(s) in the help.
> >
> > Sean
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 


-- 
Jonathan
jqm475 at gmail.com



From i.visser at uva.nl  Tue May  3 20:09:24 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Tue, 03 May 2005 14:09:24 -0400
Subject: [R] Rd.sty error
Message-ID: <BE9D3694.3DFB%i.visser@uva.nl>

I had written a vignette and included a
\usepackage{Rd} command to make it possible to include
latex'ed Rd files in the vignette. However, when loading
Rd.sty texShop produces the following error:

l. 180 ...d}[1]{\ifmmode\bm{#1}\else\textbf{#1}\fi}

This is on Max OS X 3.9 and R 2.0.1

Has anyone seen this before and/or is it problematic? I'm not sure whether
the output suffers from this but it does create a problem with R CMD check
because tex produces a warning/error there.

any hints are welcome, ingmar

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From spencer.graves at pdf.com  Tue May  3 20:40:12 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 03 May 2005 11:40:12 -0700
Subject: [R] General Question on learning R...
In-Reply-To: <e206273d050503110648a51696@mail.gmail.com>
References: <e206273d0505030346ac59dc9@mail.gmail.com>	<faea59c94331334ace64224572930fb3@mail.nih.gov>	<42779E53.4060008@pdf.com>
	<e206273d050503110648a51696@mail.gmail.com>
Message-ID: <4277C58C.9060905@pdf.com>

	  Yes.  Thanks for the elaboration.  What differences might one expect 
between the contents of "\demo" and "\R-ex"?  (I found "\R-ex" with all 
the packages I named, but not all had "\demo".)

	  spencer graves

There seem to be different

Jonathan Q. wrote:

> assuming one has these installed already, you just look in the demo
> folder under each?  i.e., fBasics\demo ???
> 
> On 5/3/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> 
>>         I'm looking at the same thing.  A good source for this is to
>>'install.packages(c("fBasics", "fCalendar", "fExtremes", "fMultivar",
>>"fOptions", "fPotfolio", "fractdiff", "fSeries", "its", "lme4",
>>"zoo"))', then 'update.packages()'.  These will install subdirectories
>>or folders with the indicated names "fBasics", etc., in "library" with
>>your R installation.  For example, in my Windows installation, I have
>>"D:\Program files\R\rw2010pat\library", which contains many subfolders
>>including ones named "fSeries".  These all contain files "*.R", which
>>provide sample code.
>>
>>         spencer graves
>>
>>Sean Davis wrote:
>>
>>
>>>On May 3, 2005, at 6:46 AM, Jonathan Q. wrote:
>>>
>>>
>>>>In the process of learning R, with a specific interest on financial
>>>>time series.  While I continue to get through the documents I am more
>>>>a fan of learning by example and then looking up how each function is
>>>>used.  Any websites which post sample code for R?
>>>>
>>>
>>>The largest source of example code is R itself.  If you have a command
>>>in which you are interested, you can often just type the command and the
>>>code will be shown to you.  Try typing:
>>>
>>>ls()
>>>
>>>Then:
>>>
>>>ls
>>>
>>>It will show you the code used to produce the result.  Also, each
>>>command has its own example(s) in the help.
>>>
>>>Sean
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>
> 
>



From lauraholt_983 at hotmail.com  Tue May  3 20:46:42 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 03 May 2005 13:46:42 -0500
Subject: [R] Classes and methods
Message-ID: <BAY10-F2978FBD436A960D3BF6651D6180@phx.gbl>

Hi R people:

I would like to learn about classes, methods, S3 and S4.

Which book would be the most helpful for this info, please:  the green one 
or the white(and blue) one?

Or is there something that would be even better, please?


Thanks in advance.

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com
R 2.1.0 Windows.
trying to learn



From ripley at stats.ox.ac.uk  Tue May  3 20:52:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 May 2005 19:52:43 +0100 (BST)
Subject: [R] Rd.sty error
In-Reply-To: <BE9D3694.3DFB%i.visser@uva.nl>
References: <BE9D3694.3DFB%i.visser@uva.nl>
Message-ID: <Pine.LNX.4.61.0505031950190.22862@gannet.stats>

I don't see an error message here.
And what is `texShop'?

I think we need (a lot) more details of what you actually are doing.

On Tue, 3 May 2005, Ingmar Visser wrote:

> I had written a vignette and included a
> \usepackage{Rd} command to make it possible to include
> latex'ed Rd files in the vignette. However, when loading
> Rd.sty texShop produces the following error:
>
> l. 180 ...d}[1]{\ifmmode\bm{#1}\else\textbf{#1}\fi}
>
> This is on Max OS X 3.9 and R 2.0.1
>
> Has anyone seen this before and/or is it problematic? I'm not sure whether
> the output suffers from this but it does create a problem with R CMD check
> because tex produces a warning/error there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Tue May  3 20:53:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 3 May 2005 14:53:59 -0400
Subject: [R] Classes and methods
In-Reply-To: <BAY10-F2978FBD436A960D3BF6651D6180@phx.gbl>
References: <BAY10-F2978FBD436A960D3BF6651D6180@phx.gbl>
Message-ID: <971536df05050311531d3fb301@mail.gmail.com>

On 5/3/05, Laura Holt <lauraholt_983 at hotmail.com> wrote:
> Hi R people:
> 
> I would like to learn about classes, methods, S3 and S4.
> 
> Which book would be the most helpful for this info, please:  the green one
> or the white(and blue) one?
> 
> Or is there something that would be even better, please?
> 

Reading the source of a few packages:  R2HTML (S3), zoo (S3)
and its (S4) would get you up to speed quickly.



From Achim.Zeileis at wu-wien.ac.at  Tue May  3 20:49:27 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 3 May 2005 20:49:27 +0200
Subject: [R] General Question on learning R...
In-Reply-To: <4277C58C.9060905@pdf.com>
References: <e206273d0505030346ac59dc9@mail.gmail.com>
	<faea59c94331334ace64224572930fb3@mail.nih.gov>
	<42779E53.4060008@pdf.com>
	<e206273d050503110648a51696@mail.gmail.com>
	<4277C58C.9060905@pdf.com>
Message-ID: <20050503204927.45ac30fc.Achim.Zeileis@wu-wien.ac.at>

On Tue, 03 May 2005 11:40:12 -0700 Spencer Graves wrote:

> 	  Yes.  Thanks for the elaboration.  What differences might one
> 	  expect 
> between the contents of "\demo" and "\R-ex"?  (I found "\R-ex" with
> all the packages I named, but not all had "\demo".)

`R-ex' contains the examples from the `man' pages and demo contains the
`demo' files. Whereas man pages are necessary to pass R CMD check, demo
files are not, so this is part of the reason while much more packages
have examples but not demos.

You can conveniently look at both, using examples() and demo() from
within R.
Z



> 	  spencer graves
> 
> There seem to be different
> 
> Jonathan Q. wrote:
> 
> > assuming one has these installed already, you just look in the demo
> > folder under each?  i.e., fBasics\demo ???
> > 
> > On 5/3/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> > 
> >>         I'm looking at the same thing.  A good source for this is
> >to>'install.packages(c("fBasics", "fCalendar", "fExtremes",
> >"fMultivar",>"fOptions", "fPotfolio", "fractdiff", "fSeries", "its",
> >"lme4",>"zoo"))', then 'update.packages()'.  These will install
> >subdirectories>or folders with the indicated names "fBasics", etc.,
> >in "library" with>your R installation.  For example, in my Windows
> >installation, I have>"D:\Program files\R\rw2010pat\library", which
> >contains many subfolders>including ones named "fSeries".  These all
> >contain files "*.R", which>provide sample code.
> >>
> >>         spencer graves
> >>
> >>Sean Davis wrote:
> >>
> >>
> >>>On May 3, 2005, at 6:46 AM, Jonathan Q. wrote:
> >>>
> >>>
> >>>>In the process of learning R, with a specific interest on
> >financial>>>time series.  While I continue to get through the
> >documents I am more>>>a fan of learning by example and then looking
> >up how each function is>>>used.  Any websites which post sample code
> >for R?>>>
> >>>
> >>>The largest source of example code is R itself.  If you have a
> >command>>in which you are interested, you can often just type the
> >command and the>>code will be shown to you.  Try typing:
> >>>
> >>>ls()
> >>>
> >>>Then:
> >>>
> >>>ls
> >>>
> >>>It will show you the code used to produce the result.  Also, each
> >>>command has its own example(s) in the help.
> >>>
> >>>Sean
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>>http://www.R-project.org/posting-guide.html
> >>
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue May  3 20:58:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 May 2005 19:58:53 +0100 (BST)
Subject: [R] memory error message using MASS and GLMMGibbs
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC8BFCA4@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC8BFCA4@ibfftce502.de.ad.drkw.net>
Message-ID: <Pine.LNX.4.61.0505031954080.22862@gannet.stats>

This works for me (after quite a while) using the pre-compiled binary from 
CRAN on Windows XP.

It is random, so try a different seed.

If you consult the rw-FAQ you will see how to get a more usable error 
message.

On Tue, 3 May 2005, Molins, Jordi wrote:

>
> Hello,
>
> I was just testing the MASS code examples for chapter 10 (Random and Mixed
> Effects) and I have pasted the following code in an R session (2.1.0 in
> windows 2000 professional; I have also Xemacs + ESS installed, but I was not
> using them at that time; my machine has quite a lot of RAM):
>
> library(MASS)
> library(lattice)
> library(nlme)
> library(GLMMGibbs)
> # declare a random intercept for each subject
> epil$subject <- Ra(data = factor(epil$subject))
> glmm(y ~ lbase*trt + lage + V4 + subject, family = poisson,
>     data = epil, keep = 100000, thin = 100)
>
> and then an Application Error appears:
>
> "The instruction at "0x1001edc9" referenced memory at "0x00000008". The
> memory could not be "written". "
>
> It does not take long for this message to appear (less than 1s after I type
> Enter).
>
> Any help is welcome
>
> Jordi
>
>
>
>
> --------------------------------------------------------------------------------
> The information contained herein is confidential and is inte...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gunter.berton at gene.com  Tue May  3 21:04:39 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 3 May 2005 12:04:39 -0700
Subject: [R] Classes and methods
In-Reply-To: <BAY10-F2978FBD436A960D3BF6651D6180@phx.gbl>
Message-ID: <200505031904.j43J4dU5011920@compton.gene.com>

I **Highly** recommend V&R's S PROGRAMMING.

AFAIK, the R (or S-Plus) implementation of S4 classes is not exactly as
given by the Green book, which is the authorative reference, however. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laura Holt
> Sent: Tuesday, May 03, 2005 11:47 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Classes and methods
> 
> Hi R people:
> 
> I would like to learn about classes, methods, S3 and S4.
> 
> Which book would be the most helpful for this info, please:  
> the green one 
> or the white(and blue) one?
> 
> Or is there something that would be even better, please?
> 
> 
> Thanks in advance.
> 
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> R 2.1.0 Windows.
> trying to learn
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Tue May  3 21:16:46 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 3 May 2005 15:16:46 -0400
Subject: [R] Classes and methods
In-Reply-To: <971536df05050311531d3fb301@mail.gmail.com>
References: <BAY10-F2978FBD436A960D3BF6651D6180@phx.gbl>
	<971536df05050311531d3fb301@mail.gmail.com>
Message-ID: <58ed06456bf16c5e3394d763931a7f6b@mail.nih.gov>


On May 3, 2005, at 2:53 PM, Gabor Grothendieck wrote:

> On 5/3/05, Laura Holt <lauraholt_983 at hotmail.com> wrote:
>> Hi R people:
>>
>> I would like to learn about classes, methods, S3 and S4.
>>
>> Which book would be the most helpful for this info, please:  the 
>> green one
>> or the white(and blue) one?
>>
>> Or is there something that would be even better, please?
>>
>
> Reading the source of a few packages:  R2HTML (S3), zoo (S3)
> and its (S4) would get you up to speed quickly.
>

I just did some of this learning myself.  Here are a couple of links 
that I found useful:

http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/S4Objects.pdf
http://eeyore.ucdavis.edu/stat250/OOP.html

I found the first particularly easy reading and it got me going quickly 
with S4 methods, which it seems to me are the way to go in most cases.

Sean



From rene.raupp at terra.com.br  Tue May  3 21:52:34 2005
From: rene.raupp at terra.com.br (rene.raupp)
Date: Tue,  3 May 2005 16:52:34 -0300
Subject: [R] (no subject)
Message-ID: <IFXJ7M$83F019763D6380D79C4C80C4738E88E3@terra.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050503/11035c18/attachment.pl

From rxg218 at psu.edu  Tue May  3 22:10:14 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Tue, 03 May 2005 16:10:14 -0400
Subject: [R] numerical integration of x,y data
Message-ID: <1115151014.12330.23.camel@blue.chem.psu.edu>

Hi, I was looking for a routine that would do numerical integration
using x,y data rather than integrating a function. 

Searching CRAN showed me the sfsmisc package, which contains
integrate.xy. However the documentation mentions that its not good for
noisy data and plans to implement the Romberg method.

Is this the package that is generally used to perform this type of
integration or are there other routines available?

Thanks,


-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Say it with flowers - give her a triffid



From phhs80 at gmail.com  Tue May  3 22:14:36 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 3 May 2005 21:14:36 +0100
Subject: [R] Command to add two vectors
Message-ID: <6ade6f6c050503131419a029ef@mail.gmail.com>

Dear All

Is there some command to add two vectors?

Thanks in advance,

Paul



From phhs80 at gmail.com  Tue May  3 22:27:38 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 3 May 2005 21:27:38 +0100
Subject: [R] Command to add two vectors
In-Reply-To: <Pine.LNX.4.62.0505032222080.6560@dns.unife.it>
References: <6ade6f6c050503131419a029ef@mail.gmail.com>
	<Pine.LNX.4.62.0505032222080.6560@dns.unife.it>
Message-ID: <6ade6f6c0505031327623cbeab@mail.gmail.com>

On 5/3/05, Josef Eschgfaeller <esg at felix.unife.it> wrote:
> > Is there some command to add two vectors?
> 
> x+y

Thanks a lot, Josef.

Paul



From p.murrell at auckland.ac.nz  Tue May  3 22:29:25 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 04 May 2005 08:29:25 +1200
Subject: [R] grid and ps device (bg-color)
References: <42721B5A.2060606@hppi.troitsk.ru>
	<4272B31F.8030005@stat.auckland.ac.nz>
	<42778A78.8070501@hppi.troitsk.ru>
Message-ID: <4277DF25.5080008@stat.auckland.ac.nz>

Hi


mkondrin wrote:
> Hi!
> 
>>
>>
>> grid.rect(width=2, height=2,
>>           gp=gpar(fill=ps.options()$bg.color))
>>
> Yes, it works (more or less), but the best is to use
> grid.rect(width=unit(1,"npc")+unit(0.5,"inches"),...)
> as postscript device leaves a 0.25-inch frame around the page.
> BTW wouldn't you fix a bug with the determination of "strwidth" in the 
> code like this:
> l<-c("1","23","456","7890")
> unit(1,"strwidth",l)
> The last command calculates the width of the first element of l, which 
> is not the thing one would like to have. The best would be the 
> calculation of maximum length of vector's elements (a simple for-cycle 
> in unit.c). It also solves the problems with
> t<-grid.text(l,y=c(0.9,0.8,0.7,0.6))
> unit(1,"grobwidth",t)
> where the desired answer should be the length of the longest string in l 
> (in R-2.0.1 this returns the length of the l[[0]] too).
> I have fixed it in my R installation but the patch is obvious.


This is fixed (different) in R 2.1.0.  If there are several strings, the 
width is taken from the bounding box of the strings.  For example

grid.rect(width=grobWidth(grid.text(c("different",
             "bits of", "text", "with the longest",
             "not the first"), x=.5, y=1:5/6)))


Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From rolf at math.unb.ca  Tue May  3 22:39:14 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 3 May 2005 17:39:14 -0300 (ADT)
Subject: [R] Classes and methods
Message-ID: <200505032039.j43KdEF1027841@erdos.math.unb.ca>

Sean Davis wrote:

> I just did some of this learning myself.  Here are a couple of links 
> that I found useful:
> 
> http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/S4Objects.pdf
> http://eeyore.ucdavis.edu/stat250/OOP.html
> 
> I found the first particularly easy reading and it got me going quickly 
> with S4 methods, which it seems to me are the way to go in most cases.

	If you want to simultaneously handcuff yourself, strap
	yourself into a strait jacket, and tie yourself in knots, and
	moreover write code which is incomprehensible to the human
	mind, then S4 methods are indeed the way to go.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From lederer at slcmsr.org  Tue May  3 22:59:16 2005
From: lederer at slcmsr.org (Christian Lederer)
Date: Tue, 03 May 2005 22:59:16 +0200
Subject: [R] Installing GO 1.7.0
In-Reply-To: <m2sm2dt3h5.fsf@macaroni.local>
References: <1112202606.12235.18.camel@in-141-199.dhcp-149-166.iupui.edu>
	<m2sm2dt3h5.fsf@macaroni.local>
Message-ID: <4277E624.5000106@slcmsr.org>

Hi,

is the announced solution for the GO 1.7.0 installation already
publicly available?
I am running into the same trouble as described in below, using
2.0.1 under Ubuntu Hoary.

Thanks,

Christian



 > Seth Falcon wrote:
> Hi Tom,
> 
> I'm cc'ing to Bioconductor as that is probably a better place for the
> discussion.
> 
> 
> "Tom 'spot' Callaway" <tcallawa at redhat.com> writes:
> 
> 
>>I'm in the process of packaging R (and R modules) for future inclusion
>>in Fedora Extras, and I've managed to get several hundred modules
>>installed without issue, however, the GO metadata package is refusing to
>>comply.
> 
> 
>>ERROR: installing package indices failed
>>
>>I let this run for over 6 hours, and it didn't seem to complete (or make
>>any changes).
> 
> 
> We have a solution for this and will send or make available an updated
> GO package shortly.  With the updated GO package you should be able to
> R CMD INSTALL without it taking much time.
> 
> Best,
> 
> + seth
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


> I'm in the process of packaging R (and R modules) for future inclusion
> in Fedora Extras, and I've managed to get several hundred modules
> installed without issue, however, the GO metadata package is refusing to
> comply.
> 
> Since I'm packaging this in rpm format, I can't use any of the automated
> functions for build, I've got to do it locally through R.
> 
> The following steps work for other metadata packages (directory names
> changing, obviously), but not for GO:
> 
> With the GO tarball unpacked into R-GO-1.7.0/GO...
> 
> cd R-GO-1.7.0/
> rm -rf /var/tmp/R-GO-1.7.0-1-root-root
> mkdir -p /var/tmp/R-GO-1.7.0-1-root-root/usr/lib/R/library
> export R_LIBS=/var/tmp/R-GO-1.7.0-1-root-root/usr/lib/R/library
> /usr/bin/R CMD INSTALL \
> -l /var/tmp/R-GO-1.7.0-1-root-root/usr/lib/R/library GO
> 
> I get the following output:
> * Installing *source* package 'GO' ...
> ** R
> ** data
> ** preparing package for lazy loading
> 
> ** help
>  >>> Building/Updating help pages for package 'GO'
>      Formats: text html latex example
>   GO                                text    html    latex
>   GOALLLOCUSID                      text    html    latex   example
>   GOBPANCESTOR                      text    html    latex   example
>   GOBPCHILDREN                      text    html    latex   example
>   GOBPOFFSPRING                     text    html    latex   example
>   GOBPPARENTS                       text    html    latex   example
>   GOCCANCESTOR                      text    html    latex   example
>   GOCCCHILDREN                      text    html    latex   example
>   GOCCOFFSPRING                     text    html    latex   example
>   GOCCPARENTS                       text    html    latex   example
>   GOLOCUSID                         text    html    latex   example
>   GOLOCUSID2ALLGO                   text    html    latex   example
>   GOLOCUSID2GO                      text    html    latex   example
>   GOMFANCESTOR                      text    html    latex   example
>   GOMFCHILDREN                      text    html    latex   example
>   GOMFOFFSPRING                     text    html    latex   example
>   GOMFPARENTS                       text    html    latex   example
>   GOQC                              text    html    latex
>   GOTERM                            text    html    latex   example
> 
> But I never get the "* DONE (GO)" that I'm expecting. Instead, all of
> the memory on the machine allocates (512MB), it starts to swap out, and
> never completes.
> 
> When I look at the output from ps, I see:
>  8290 pts/5    S+     0:00 /bin/sh /usr/lib/R/bin/INSTALL
> -l /var/tmp/R-GO-1.7.0-1-root-root/us 8364 ?        S      0:00 
>  8421 pts/5    D+     1:11 /usr/lib/R/bin/exec/R --vanilla
> 
> When I kill the 8421 process, I get:
> 
> /usr/lib/R/bin/INSTALL: line 381:  8088 Done                    echo
> "invisible(.libPaths(c(\"${lib}\", .libPaths())));
> tools:::.install_package_indices(\".\", \"${R_PACKAGE_DIR}\")"
>       8089 Killed                  | R_DEFAULT_PACKAGES=NULL LANG=C
> "${R_EXE}" --vanilla >/dev/null
> ERROR: installing package indices failed
> 
> I let this run for over 6 hours, and it didn't seem to complete (or make
> any changes).
> 
> Unfortunately, lots of Bioconductor seems to depend on GO... so any help
> on getting this to install is appreciated.
> 
> Thanks,
> 
> ~spot
> -- Tom "spot" Callaway: Red Hat Sales Engineer || GPG Fingerprint: 93054260 Fedora Extras Steering Committee Member (RPM Standards and Practices) Aurora Linux Project Leader: http://auroralinux.org Lemurs, llamas, and sparcs, oh my! ______________________________________________ R-help at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From aoganyan at niss.org  Tue May  3 11:29:35 2005
From: aoganyan at niss.org (Anna Oganyan)
Date: Tue, 03 May 2005 05:29:35 -0400
Subject: [R] multivariate Shapiro Wilks test
Message-ID: <4277447F.7010401@niss.org>

Hello,
I have a question about multivariate Shapiro-Wilks test.
I tried to analyze if the data I have are multivariate normal, or how 
far they are from being
multivariate normal. However, any time I did
 >mshapiro.test(mydata) 
I get the message:
Error in solve.default(R %*% t(R), tol = 1e-18) :
        system is computationally singular: reciprocal condition number 
= 5.38814e-021

I tried also to generate some multivariate normal data and to see how 
the test would be working on it. So I did:
a <- mvrnorm(1000, c(1,2,3,4,5), diag(5))
mshapiro.test(a)

But any time I get:
Error in solve.default(R %*% t(R), tol = 1e-18) :
        system is computationally singular: reciprocal condition number 
= small number


Could somebody help me what is wrong with this example?
Thank you very much in advance!
Anna



From apjaworski at mmm.com  Wed May  4 00:07:55 2005
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Tue, 3 May 2005 17:07:55 -0500
Subject: [R] multivariate Shapiro Wilks test
In-Reply-To: <4277447F.7010401@niss.org>
Message-ID: <OF4580CE37.109A388B-ON86256FF6.0079245B-86256FF6.0079932E@mmm.com>






Anna,

It looks like the mshapiro.test wants its data in the row format, that is,
for a k-variate sample of size n you need the data in a k-by-n matrix.

Try

a <- t(mvrnorm(1000, c(1,2,3,4,5), diag(5)))
mshapiro.test(a)

and it should work fine.

Cheers,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             Anna Oganyan                                                  
             <aoganyan at niss.or                                             
             g>                                                         To 
             Sent by:                  r-help at stat.math.ethz.ch            
             r-help-bounces at st                                          cc 
             at.math.ethz.ch                                               
                                                                   Subject 
                                       [R] multivariate Shapiro Wilks test 
             05/03/2005 04:29                                              
             AM                                                            
                                                                           
                                                                           
                                                                           
                                                                           




Hello,
I have a question about multivariate Shapiro-Wilks test.
I tried to analyze if the data I have are multivariate normal, or how
far they are from being
multivariate normal. However, any time I did
 >mshapiro.test(mydata)
I get the message:
Error in solve.default(R %*% t(R), tol = 1e-18) :
        system is computationally singular: reciprocal condition number
= 5.38814e-021

I tried also to generate some multivariate normal data and to see how
the test would be working on it. So I did:
a <- mvrnorm(1000, c(1,2,3,4,5), diag(5))
mshapiro.test(a)

But any time I get:
Error in solve.default(R %*% t(R), tol = 1e-18) :
        system is computationally singular: reciprocal condition number
= small number


Could somebody help me what is wrong with this example?
Thank you very much in advance!
Anna

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From i.visser at uva.nl  Wed May  4 00:08:25 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Tue, 03 May 2005 18:08:25 -0400
Subject: [R] Rd.sty error
In-Reply-To: <Pine.LNX.4.61.0505031950190.22862@gannet.stats>
Message-ID: <BE9D6E99.3E41%i.visser@uva.nl>

Dear Prof Ripley,

The problem is this:
I create a package with a vignette which also includes some of the help
files from the package. These are \input'ed into the vignette (after
generating them with Rdconv -t=latex. After running sweave, I get the
package-manual.tex file which runs succesfully through my tex application
(texshop is a mac os x tex gui that uses tetex), albeit with \batchmode on
to catch the error described below.
 
The package successfully passes  R CMD check. However, when I subsequently R
CMD build the package I get the following message:

* creating vignettes ... ERROR
/Library/Frameworks/R.framework/Resources/bin/texi2dvi: pdflatex exited with
bad status, quitting.
/Library/Frameworks/R.framework/Resources/bin/texi2dvi: see
depmix-manual.log for errors.
Error in texi2dvi(file = bft, pdf = TRUE, clean = FALSE, quiet = quiet) :
        running texi2dvi on depmix-manual.tex failed
Execution halted

> version
         _         
platform powerpc-apple-darwin6.8
arch     powerpc   
os       darwin6.8 
system   powerpc, darwin6.8
status             
major    2         
minor    0.1       
year     2004      
month    11        
day      15        
language R     


The error I described earlier appears after this line:
/usr/local/teTeX/share/texmf.tetex/tex/latex/url/url.sty

This error does not occur when I comment out \usepackage{Rd}, although then
of course many other errors occur such as \code and \pkg being undefined.

Any hints as to where to go from here are very welcome!

best, ingmar

The depmix-manual.log reads as follows:

This is pdfeTeX, Version 3.141592-1.20a-2.2 (Web2C 7.5.3) (format=pdflatex
2004.11.26)  3 MAY 2005 17:17
entering extended mode
**\nonstopmode \input
/Users/ivisser/Documents/Projects/HMM/depmixprog/depmix/i
nst/doc/depmix-manual.tex

(/Users/ivisser/Documents/Projects/HMM/depmixprog/depmix/inst/doc/depmix-man
ual
.tex (/usr/local/teTeX/share/texmf.tetex/tex/latex/base/report.cls
Document Class: report 2004/02/16 v1.4f Standard LaTeX document class
(/usr/local/teTeX/share/texmf.tetex/tex/latex/base/size10.clo
File: size10.clo 2004/02/16 v1.4f Standard LaTeX file (size option)
)
\c at part=\count79
\c at chapter=\count80
\c at section=\count81
\c at subsection=\count82
\c at subsubsection=\count83
\c at paragraph=\count84
\c at subparagraph=\count85
\c at figure=\count86
\c at table=\count87
\abovecaptionskip=\skip41
\belowcaptionskip=\skip42
\bibindent=\dimen102
)
(/usr/local/teTeX/share/texmf.tetex/tex/latex/amsmath/amsmath.sty
Package: amsmath 2000/07/18 v2.13 AMS math features
\@mathmargin=\skip43

For additional information on amsmath, use the `?' option.
(/usr/local/teTeX/share/texmf.tetex/tex/latex/amsmath/amstext.sty
Package: amstext 2000/06/29 v2.01

(/usr/local/teTeX/share/texmf.tetex/tex/latex/amsmath/amsgen.sty
File: amsgen.sty 1999/11/30 v2.0
\@emptytoks=\toks14
\ex@=\dimen103
))
(/usr/local/teTeX/share/texmf.tetex/tex/latex/amsmath/amsbsy.sty
Package: amsbsy 1999/11/29 v1.2d
\pmbraise@=\dimen104
)
(/usr/local/teTeX/share/texmf.tetex/tex/latex/amsmath/amsopn.sty
Package: amsopn 1999/12/14 v2.01 operator names
)
\inf at bad=\count88
LaTeX Info: Redefining \frac on input line 211.
\uproot@=\count89
\leftroot@=\count90
LaTeX Info: Redefining \overline on input line 307.
\classnum@=\count91
\DOTSCASE@=\count92
LaTeX Info: Redefining \ldots on input line 379.
LaTeX Info: Redefining \dots on input line 382.
LaTeX Info: Redefining \cdots on input line 467.
\Mathstrutbox@=\box26
\strutbox@=\box27
\big at size=\dimen105
LaTeX Font Info:    Redeclaring font encoding OML on input line 567.
LaTeX Font Info:    Redeclaring font encoding OMS on input line 568.
\macc at depth=\count93
\c at MaxMatrixCols=\count94
\dotsspace@=\muskip10
\c at parentequation=\count95
\dspbrk at lvl=\count96
\tag at help=\toks15
\row@=\count97
\column@=\count98
\maxfields@=\count99
\andhelp@=\toks16
\eqnshift@=\dimen106
\alignsep@=\dimen107
\tagshift@=\dimen108
\tagwidth@=\dimen109
\totwidth@=\dimen110
\lineht@=\dimen111
\@envbody=\toks17
\multlinegap=\skip44
\multlinetaggap=\skip45
\mathdisplay at stack=\toks18
LaTeX Info: Redefining \[ on input line 2666.
LaTeX Info: Redefining \] on input line 2667.
)
(/usr/local/teTeX/share/texmf.tetex/tex/latex/amsfonts/amsfonts.sty
Package: amsfonts 2001/10/25 v2.2f
\symAMSa=\mathgroup4
\symAMSb=\mathgroup5
LaTeX Font Info:    Overwriting math alphabet `\mathfrak' in version `bold'
(Font)                  U/euf/m/n --> U/euf/b/n on input line 132.
)
(/usr/local/teTeX/share/texmf.tetex/tex/latex/amscls/amsthm.sty
Package: amsthm 2004/08/06 v2.20
\thm at style=\toks19
\thm at bodyfont=\toks20
\thm at headfont=\toks21
\thm at notefont=\toks22
\thm at headpunct=\toks23
\thm at preskip=\skip46
\thm at postskip=\skip47
\thm at headsep=\skip48
\dth at everypar=\toks24
)
(/Library/Frameworks/R.framework/Resources/share/texmf/Sweave.sty
(/usr/local/t
eTeX/share/texmf.tetex/tex/latex/base/fontenc.sty
Package: fontenc 2004/02/22 v1.99f Standard LaTeX package
(/usr/local/teTeX/share/texmf.tetex/tex/latex/base/t1enc.def
File: t1enc.def 2004/02/22 v1.99f Standard LaTeX file
LaTeX Font Info:    Redeclaring font encoding T1 on input line 43.
)) (/usr/local/teTeX/share/texmf.tetex/tex/latex/graphics/graphicx.sty
Package: graphicx 1999/02/16 v1.0f Enhanced LaTeX Graphics (DPC,SPQR)
(/usr/local/teTeX/share/texmf.tetex/tex/latex/graphics/keyval.sty
Package: keyval 1999/03/16 v1.13 key=value parser (DPC)
\KV at toks@=\toks25
) (/usr/local/teTeX/share/texmf.tetex/tex/latex/graphics/graphics.sty
Package: graphics 2001/07/07 v1.0n Standard LaTeX Graphics (DPC,SPQR)
(/usr/local/teTeX/share/texmf.tetex/tex/latex/graphics/trig.sty
Package: trig 1999/03/16 v1.09 sin cos tan (DPC)
) (/usr/local/teTeX/share/texmf.tetex/tex/latex/graphics/graphics.cfg
File: graphics.cfg 2004/07/07 v1.2 graphics configuration of teTeX/TeXLive
)
Package graphics Info: Driver file: pdftex.def on input line 80.
(/usr/local/teTeX/share/texmf.tetex/tex/latex/graphics/pdftex.def
File: pdftex.def 2002/06/19 v0.03k graphics/color for pdftex
\Gread at gobject=\count100
))
\Gin at req@height=\dimen112
\Gin at req@width=\dimen113
) (/usr/local/teTeX/share/texmf.tetex/tex/latex/ae/ae.sty
Package: ae 2001/02/12 1.3 Almost European Computer Modern
(/usr/local/teTeX/share/texmf.tetex/tex/latex/base/fontenc.sty
Package: fontenc 2004/02/22 v1.99f Standard LaTeX package
(/usr/local/teTeX/share/texmf.tetex/tex/latex/base/t1enc.def
File: t1enc.def 2004/02/22 v1.99f Standard LaTeX file
LaTeX Font Info:    Redeclaring font encoding T1 on input line 43.
)
LaTeX Font Info:    Try loading font information for T1+aer on input line
100.
(/usr/local/teTeX/share/texmf.tetex/tex/latex/ae/t1aer.fd
File: t1aer.fd 1997/11/16 Font definitions for T1/aer.
))) (/usr/local/teTeX/share/texmf.tetex/tex/latex/fancyvrb/fancyvrb.sty
Package: fancyvrb 1998/07/17
Style option: `fancyvrb' v2.6, with DG/SPQR fixes <1998/07/17> (tvz)
\FV at CodeLineNo=\count101
\FV at InFile=\read1
\FV at TabBox=\box28
\c at FancyVerbLine=\count102
\FV at StepNumber=\count103
\FV at OutFile=\write3
No file fancyvrb.cfg.
) (/Library/Frameworks/R.framework/Resources/share/texmf/upquote.sty
Package: upquote 2003/08/11 v1.1 Covington's upright-quote modification to
verb
atim and verb
(/usr/local/teTeX/share/texmf.tetex/tex/latex/base/textcomp.sty
Package: textcomp 2004/02/22 v1.99f Standard LaTeX package
Package textcomp Info: Sub-encoding information:
(textcomp)               5 = only ISO-Adobe without \textcurrency
(textcomp)               4 = 5 + \texteuro
(textcomp)               3 = 4 + \textohm
(textcomp)               2 = 3 + \textestimated + \textcurrency
(textcomp)               1 = TS1 - \textcircled - \t
(textcomp)               0 = TS1 (full)
(textcomp)             Font families with sub-encoding setting implement
(textcomp)             only a restricted character set as indicated.
(textcomp)             Family '?' is the default used for unknown fonts.
(textcomp)             See the documentation for details.
Package textcomp Info: Setting ? sub-encoding to TS1/1 on input line 71.
(/usr/local/teTeX/share/texmf.tetex/tex/latex/base/ts1enc.def
File: ts1enc.def 2001/06/05 v3.0e (jk/car/fm) Standard LaTeX file
)
LaTeX Info: Redefining \oldstylenums on input line 266.
Package textcomp Info: Setting cmr sub-encoding to TS1/0 on input line 281.
Package textcomp Info: Setting cmss sub-encoding to TS1/0 on input line 282.
Package textcomp Info: Setting cmtt sub-encoding to TS1/0 on input line 283.
Package textcomp Info: Setting cmvtt sub-encoding to TS1/0 on input line
284.
Package textcomp Info: Setting cmbr sub-encoding to TS1/0 on input line 285.
Package textcomp Info: Setting cmtl sub-encoding to TS1/0 on input line 286.
Package textcomp Info: Setting ccr sub-encoding to TS1/0 on input line 287.
Package textcomp Info: Setting ptm sub-encoding to TS1/4 on input line 288.
Package textcomp Info: Setting pcr sub-encoding to TS1/4 on input line 289.
Package textcomp Info: Setting phv sub-encoding to TS1/4 on input line 290.
Package textcomp Info: Setting ppl sub-encoding to TS1/3 on input line 291.
Package textcomp Info: Setting pag sub-encoding to TS1/4 on input line 292.
Package textcomp Info: Setting pbk sub-encoding to TS1/4 on input line 293.
Package textcomp Info: Setting pnc sub-encoding to TS1/4 on input line 294.
Package textcomp Info: Setting pzc sub-encoding to TS1/4 on input line 295.
Package textcomp Info: Setting bch sub-encoding to TS1/4 on input line 296.
Package textcomp Info: Setting put sub-encoding to TS1/5 on input line 297.
Package textcomp Info: Setting uag sub-encoding to TS1/5 on input line 298.
Package textcomp Info: Setting ugq sub-encoding to TS1/5 on input line 299.
Package textcomp Info: Setting ul8 sub-encoding to TS1/4 on input line 300.
Package textcomp Info: Setting ul9 sub-encoding to TS1/4 on input line 301.
Package textcomp Info: Setting augie sub-encoding to TS1/5 on input line
302.
Package textcomp Info: Setting dayrom sub-encoding to TS1/3 on input line
303.
Package textcomp Info: Setting dayroms sub-encoding to TS1/3 on input line
304.

Package textcomp Info: Setting pxr sub-encoding to TS1/0 on input line 305.
Package textcomp Info: Setting pxss sub-encoding to TS1/0 on input line 306.
Package textcomp Info: Setting pxtt sub-encoding to TS1/0 on input line 307.
Package textcomp Info: Setting txr sub-encoding to TS1/0 on input line 308.
Package textcomp Info: Setting txss sub-encoding to TS1/0 on input line 309.
Package textcomp Info: Setting txtt sub-encoding to TS1/0 on input line 310.
Package textcomp Info: Setting futs sub-encoding to TS1/4 on input line 311.
Package textcomp Info: Setting futx sub-encoding to TS1/4 on input line 312.
Package textcomp Info: Setting futj sub-encoding to TS1/4 on input line 313.
Package textcomp Info: Setting hlh sub-encoding to TS1/3 on input line 314.
Package textcomp Info: Setting hls sub-encoding to TS1/3 on input line 315.
Package textcomp Info: Setting hlst sub-encoding to TS1/3 on input line 316.
Package textcomp Info: Setting hlct sub-encoding to TS1/5 on input line 317.
Package textcomp Info: Setting hlx sub-encoding to TS1/5 on input line 318.
Package textcomp Info: Setting hlce sub-encoding to TS1/5 on input line 319.
Package textcomp Info: Setting hlcn sub-encoding to TS1/5 on input line 320.
Package textcomp Info: Setting hlcw sub-encoding to TS1/5 on input line 321.
Package textcomp Info: Setting hlcf sub-encoding to TS1/5 on input line 322.
Package textcomp Info: Setting pplx sub-encoding to TS1/3 on input line 323.
Package textcomp Info: Setting pplj sub-encoding to TS1/3 on input line 324.
Package textcomp Info: Setting ptmx sub-encoding to TS1/4 on input line 325.
Package textcomp Info: Setting ptmj sub-encoding to TS1/4 on input line 326.
))) (./Rd.sty
Package: Rd 
(/usr/local/teTeX/share/texmf.tetex/tex/latex/base/ifthen.sty
Package: ifthen 2001/05/26 v1.1c Standard LaTeX ifthen package (DPC)
) (/usr/local/teTeX/share/texmf.tetex/tex/latex/tools/longtable.sty
Package: longtable 2004/02/01 v4.11 Multi-page Table package (DPC)
\LTleft=\skip49
\LTright=\skip50
\LTpre=\skip51
\LTpost=\skip52
\LTchunksize=\count104
\LTcapwidth=\dimen114
\LT at head=\box29
\LT at firsthead=\box30
\LT at foot=\box31
\LT at lastfoot=\box32
\LT at cols=\count105
\LT at rows=\count106
\c at LT@tables=\count107
\c at LT@chunks=\count108
\LT at p@ftn=\toks26
) (/usr/local/teTeX/share/texmf.tetex/tex/latex/tools/bm.sty
Package: bm 2004/02/26 v1.1c Bold Symbol Support (DPC/FMi)
\symboldoperators=\mathgroup6
\symboldletters=\mathgroup7
\symboldsymbols=\mathgroup8
LaTeX Font Info:    Redeclaring math alphabet \mathbf on input line 137.
LaTeX Info: Redefining \bm on input line 203.
) (/usr/local/teTeX/share/texmf.tetex/tex/latex/base/alltt.sty
Package: alltt 1997/06/16 v2.0g defines alltt environment
) (/usr/local/teTeX/share/texmf.tetex/tex/latex/tools/verbatim.sty
Package: verbatim 2003/08/22 v1.5q LaTeX2e package for verbatim enhancements
\every at verbatim=\toks27
\verbatim at line=\toks28
\verbatim at in@stream=\read2
) (/usr/local/teTeX/share/texmf.tetex/tex/latex/url/url.sty
\Urlmuskip=\muskip11
Package: url 2004/03/15  ver 3.1  Verb mode for urls, etc.
)
\ldescriptionwidth=\skip53

! LaTeX Error: Command \bold already defined.
               Or name \end... illegal, see p.192 of the manual.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
 ...               
                   
l.180 ...d}[1]{\ifmmode\bm{#1}\else\textbf{#1}\fi}
                   
Your command was ignored.
Type  I <command> <return>  to replace it with another command,
or  <return>  to continue without it.

NOT loading ae NOT loading times NOT loading lmodern)
(/usr/local/teTeX/share/t
exmf.tetex/tex/latex/natbib/natbib.sty
Package: natbib 2003/06/06 7.1 (PWD)
\bibhang=\skip54
\bibsep=\skip55
LaTeX Info: Redefining \cite on input line 518.
\c at NAT@ctr=\count109
) (./depmix-manual.aux)
\openout1 = `depmix-manual.aux'.

LaTeX Font Info:    Checking defaults for OML/cmm/m/it on input line 43.
LaTeX Font Info:    ... okay on input line 43.
LaTeX Font Info:    Checking defaults for T1/cmr/m/n on input line 43.
LaTeX Font Info:    ... okay on input line 43.
LaTeX Font Info:    Checking defaults for OT1/cmr/m/n on input line 43.
LaTeX Font Info:    ... okay on input line 43.
LaTeX Font Info:    Checking defaults for OMS/cmsy/m/n on input line 43.
LaTeX Font Info:    ... okay on input line 43.
LaTeX Font Info:    Checking defaults for OMX/cmex/m/n on input line 43.
LaTeX Font Info:    ... okay on input line 43.
LaTeX Font Info:    Checking defaults for U/cmr/m/n on input line 43.
LaTeX Font Info:    ... okay on input line 43.
LaTeX Font Info:    Checking defaults for TS1/cmr/m/n on input line 43.
LaTeX Font Info:    Try loading font information for TS1+cmr on input line
43.
(/usr/local/teTeX/share/texmf.tetex/tex/latex/base/ts1cmr.fd
File: ts1cmr.fd 1999/05/25 v2.5h Standard LaTeX font definitions
)
LaTeX Font Info:    ... okay on input line 43.
(/usr/local/teTeX/share/texmf.tetex/tex/context/base/supp-pdf.tex
(/usr/local/t
eTeX/share/texmf.tetex/tex/context/base/supp-mis.tex
loading : Context Support Macros / Miscellaneous (2004.10.26)
\protectiondepth=\count110
\scratchcounter=\count111
\scratchtoks=\toks29
\scratchdimen=\dimen115
\scratchskip=\skip56
\scratchmuskip=\muskip12
\scratchbox=\box33
\scratchread=\read3
\scratchwrite=\write4
\zeropoint=\dimen116
\onepoint=\dimen117
\onebasepoint=\dimen118
\minusone=\count112
\thousandpoint=\dimen119
\onerealpoint=\dimen120
\emptytoks=\toks30
\nextbox=\box34
\nextdepth=\dimen121
\everyline=\toks31
\!!counta=\count113
\!!countb=\count114
\recursecounter=\count115
)
loading : Context Support Macros / PDF (2004.10.26)
\nofMPsegments=\count116
\nofMParguments=\count117
\everyMPtoPDFconversion=\toks32
)
LaTeX Font Info:    Try loading font information for U+msa on input line 47.
(/usr/local/teTeX/share/texmf.tetex/tex/latex/amsfonts/umsa.fd
File: umsa.fd 2002/01/19 v2.2g AMS font definitions
)
LaTeX Font Info:    Try loading font information for U+msb on input line 47.
(/usr/local/teTeX/share/texmf.tetex/tex/latex/amsfonts/umsb.fd
File: umsb.fd 2002/01/19 v2.2g AMS font definitions
) [1

{/usr/local/teTeX/share/texmf.local/fonts/map/pdftex/updmap/pdftex.map}]
LaTeX Font Info:    Try loading font information for T1+aett on input line
61.
(/usr/local/teTeX/share/texmf.tetex/tex/latex/ae/t1aett.fd
File: t1aett.fd 1997/11/16 Font definitions for T1/aett.
) [1] (./depmix-manual.toc)
\tf at toc=\write5
\openout5 = `depmix-manual.toc'.

[1

]
Chapter 1.
[2

]
Chapter 2.
<graphs/depmix-003.pdf, id=47, 433.62pt x 433.62pt>
File: graphs/depmix-003.pdf Graphic file (type pdf)
<use graphs/depmix-003.pdf> [3

] [4 <./graphs/depmix-003.pdf>] [5] [6] [7] [8]
Chapter 3.
[9

] (./markovdata.tex
Overfull \hbox (102.57996pt too wide) in paragraph at lines 20--20
 []        \T1/aett/m/n/10
markovdata(dat,itemtypes,nitems=length(itemtypes),nt
imes=length(as.matrix(dat))/nitems,[]
 []

LaTeX Font Info:    Try loading font information for TS1+aett on input line
23.

(/Library/Frameworks/R.framework/Resources/share/texmf/ts1aett.fd
File: ts1aett.fd 
)
LaTeX Font Info:    Try loading font information for TS1+cmtt on input line
23.

(/usr/local/teTeX/share/texmf.tetex/tex/latex/base/ts1cmtt.fd
File: ts1cmtt.fd 1999/05/25 v2.5h Standard LaTeX font definitions
)
LaTeX Font Info:    Font shape `TS1/aett/m/n' in size <10> not available
(Font)              Font shape `TS1/cmtt/m/n' tried instead on input line
23.

Overfull \hbox (86.85999pt too wide) in paragraph at lines 27--27
 []                                                \T1/aett/m/n/10
1:(min(5,len
gth(attributes(x)$ntimes))),...)[]
 []

[10] [11]) (./rudy.tex [12]) [13] [14] <graphs/depmix-009.pdf, id=126,
433.62pt
 x 433.62pt>
File: graphs/depmix-009.pdf Graphic file (type pdf)
<use graphs/depmix-009.pdf> (./generate.tex [15] [16
<./graphs/depmix-009.pdf>]
) [17] [18] [19]
Chapter 4.
[20

] [21] [22] [23] [24] (./mgdmm.tex [25]) [26] [27] [28]
Chapter 5.
(./mixdmm.tex [29

] [30]) [31]
Appendix A.
(./dmm.tex
Overfull \hbox (8.26013pt too wide) in paragraph at lines 24--24
 []                                 \T1/aett/m/n/10 linmat = NULL, snames =
NUL
L, inames = NULL)[]
 []

[32

] [33]
Overfull \hbox (21.69644pt too wide) in paragraph at lines 119--127
\T1/aer/m/n/10 higher num-bers for pos-si-bly equal-ity con-strained
pa-ram-e-t
ers. E.g. \T1/aett/m/n/10 conpat=c(1,1,0,2,2,3,3,3)
 []

[34]
Overfull \hbox (42.93558pt too wide) in paragraph at lines 144--146
[]\T1/aer/m/n/10 The num-ber of freely es-ti-mated pa-ram-e-ters (it is
com-put
ed as sum(as.logical(fixed))-
 []


Overfull \hbox (42.93558pt too wide) in paragraph at lines 146--149
[]\T1/aer/m/n/10 The num-ber of freely es-ti-mated pa-ram-e-ters (it is
com-put
ed as sum(as.logical(fixed))-
 []


Overfull \hbox (2.77544pt too wide) in paragraph at lines 152--157
[]\T1/aer/m/n/10 The ma-trix A con-tains the gen-eral lin-ear con-straints
of t
he model. nrow(A)
 []

[35]) (./fitdmm.tex [36] [37] [38]
LaTeX Font Info:    Font shape `TS1/aett/m/n' in size <9> not available
(Font)              Font shape `TS1/cmtt/m/n' tried instead on input line
177.

Overfull \hbox (38.64618pt too wide) in paragraph at lines 197--197
 []\T1/aett/m/n/9 # add covariates to the model to incorporate the fact the
acc
uracy pay off changes per trial[]
 []

[39]
Overfull \hbox (90.52138pt too wide) in paragraph at lines 211--211
 []\T1/aett/m/n/9 
mod<-dmm(nstates=2,itemt=c("n",2),stval=stv,conpat=conpat,tdf
ix=tdfix,tdst=stcov,modname="twoboth+cov")[]
 []

[40]) [41]
Appendix B.
(./mbd.tex) (./mdall.tex [42

]) (./rain.tex [43]) (./depmix-manual.bbl [44]) [45

] (./depmix-manual.aux) )
Here is how much of TeX's memory you used:
 3689 strings out of 95217
 47272 string characters out of 1184537
 99667 words of memory out of 1000000
 6607 multiletter control sequences out of 10000+50000
 57915 words of font info for 131 fonts, out of 500000 for 2000
 58 hyphenation exceptions out of 1000
 35i,18n,27p,330b,399s stack positions out of 1500i,500n,5000p,200000b,5000s
 234 PDF objects out of 300000
 0 named destinations out of 131072
 11 words of extra memory for PDF output out of 65536
 </var/tmp/texfonts/pk/ljfour/jknappen/tc/tctt0900.600
pk></usr/local/teTeX/share/texmf.tetex/fonts/type1/bluesky/cm/cmtt9.pfb>
</var/
tmp/texfonts/pk/ljfour/jknappen/tc/tctt1000.600pk></usr/local/teTeX/share/te
xmf
.tetex/fonts/type1/bluesky/cm/cmti10.pfb></usr/local/teTeX/share/texmf.tetex
/fo
nts/type1/bluesky/cm/cmr5.pfb></usr/local/teTeX/share/texmf.tetex/fonts/type
1/b
luesky/cm/cmmi5.pfb></usr/local/teTeX/share/texmf.tetex/fonts/type1/bluesky/
cm/
cmsy7.pfb></usr/local/teTeX/share/texmf.tetex/fonts/type1/bluesky/cm/cmex10.
pfb
></usr/local/teTeX/share/texmf.tetex/fonts/type1/bluesky/cm/cmsy10.pfb></usr/lo
cal/teTeX/share/texmf.tetex/fonts/type1/bluesky/cm/cmsltt10.pfb></usr/local/
teT
eX/share/texmf.tetex/fonts/type1/bluesky/cm/cmmi7.pfb></usr/local/teTeX/shar
e/t
exmf.tetex/fonts/type1/bluesky/cm/cmr7.pfb></usr/local/teTeX/share/texmf.tet
ex/
fonts/type1/bluesky/cm/cmmi10.pfb></usr/local/teTeX/share/texmf.tetex/fonts/
typ
e1/bluesky/cm/cmbx12.pfb></usr/local/teTeX/share/texmf.tetex/fonts/type1/blu
esk
y/cm/cmtt10.pfb></usr/local/teTeX/share/texmf.tetex/fonts/type1/bluesky/cm/c
mr1
0.pfb></usr/local/teTeX/share/texmf.tetex/fonts/type1/bluesky/cm/cmb10.pfb><
/us
r/local/teTeX/share/texmf.tetex/fonts/type1/bluesky/cm/cmbx10.pfb></usr/loca
l/t
eTeX/share/texmf.tetex/fonts/type1/bluesky/cm/cmr9.pfb></usr/local/teTeX/sha
re/
texmf.tetex/fonts/type1/bluesky/cm/cmr6.pfb></usr/local/teTeX/share/texmf.te
tex
/fonts/type1/bluesky/cm/cmr8.pfb></usr/local/teTeX/share/texmf.tetex/fonts/t
ype
1/bluesky/cm/cmr12.pfb></usr/local/teTeX/share/texmf.tetex/fonts/type1/blues
ky/
cm/cmr17.pfb>
Output written on depmix-manual.pdf (47 pages, 270051 bytes).


On 5/3/05 2:52 PM, "Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote:

> I don't see an error message here.
> And what is `texShop'?
> 
> I think we need (a lot) more details of what you actually are doing.
> 
> On Tue, 3 May 2005, Ingmar Visser wrote:
> 
>> I had written a vignette and included a
>> \usepackage{Rd} command to make it possible to include
>> latex'ed Rd files in the vignette. However, when loading
>> Rd.sty texShop produces the following error:
>> 
>> l. 180 ...d}[1]{\ifmmode\bm{#1}\else\textbf{#1}\fi}
>> 
>> This is on Max OS X 3.9 and R 2.0.1
>> 
>> Has anyone seen this before and/or is it problematic? I'm not sure whether
>> the output suffers from this but it does create a problem with R CMD check
>> because tex produces a warning/error there.

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From bates at stat.wisc.edu  Wed May  4 01:05:44 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 03 May 2005 18:05:44 -0500
Subject: [R] Re: nlme: "Deficient rank in gls_loglik" when creating
	corAR1()
In-Reply-To: <f5d8480605050311005fd7e4f7@mail.gmail.com>
References: <f5d8480605050308532ddbd468@mail.gmail.com>
	<f5d8480605050311005fd7e4f7@mail.gmail.com>
Message-ID: <427803C8.2010106@stat.wisc.edu>

David Hugh-Jones wrote:
> Is this a bug? Should I attach a test case?

Try setting verbose=TRUE in the call to gls first and see if that gives
you any insight into what is happening.

> 
> D
> 
> On 03/05/05, David Hugh-Jones <davidhughjones at gmail.com> wrote:
> 
>>I have a bunch of data which is structured by year and US state, so I
>>have created a nlme groupedData object for it:
>>
>>formula(gd2)
>>DEPVAR ~ YEAR | ABREV
>>
>>Now I am trying to run a gls regression on it. I want the error
>>correlation structure to be AR1 with a different rho for each state,
>>so I do
>>
>>
>>>mdyn.1.1 = gls(model = DEPVAR ~ BLAH + BLAH, data=gd2, corr=corAR1(form= ~ YEAR | ABREV),na.action=na.omit)
>>
>>YEAR and ABREV are always present; DEPVAR is absent for one state.
>>
>>I get the following error message:
>>
>>Error in logLik.glsStruct(glsSt, glsPars) :
>>        Deficient rank in gls_loglik
>>
>>Can anyone enlighten me? The error message goes away if I just do
>>corAR1(form = ~1), but this is not meaningful for my data.
>>
>>Cheers
>>David
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From clifkorea at yahoo.com  Wed May  4 01:12:44 2005
From: clifkorea at yahoo.com (Clifton Emery)
Date: Tue, 3 May 2005 16:12:44 -0700 (PDT)
Subject: [R] polr function,
	Error in if (all(pr > 0)) -sum(wt * log(pr)) else Inf
Message-ID: <20050503231244.84960.qmail@web50907.mail.yahoo.com>


      I am trying to run a proportional odds model
with the code:

reg22<-polr(as.factor(dp29)~subid+cohort+wave+sex+ses_nc+age+educ_pc+mstat_pc+famsize+salary+employ+wiscraw+age1_pc+cc61+intern2+extern2+tcbcl+sv1a0+sv3a0+sv7a0+dp1+dp5+dp7+dp26+dp27+dp31+dp34+hg106+hg113+hg55+hg54+hg123+hg126+hg129+hg20+hg120+sr2a1+sb23+minorviolfem+sevviolfem+minorviolman+sevviolman+minabuse+sevabuse+crime,
data=misdat4, start=st)


Some of my models run, others (like the code above)
get an error indicating that the starting values are
infinite.  Someone suggested I try putting in the
starting values myself.  I created a vector which has
length 47 (45 predictors plus (3 levels of depvar) - 1
).  This now gets the message:

Error in if (all(pr > 0)) -sum(wt * log(pr)) else Inf
: 
        missing value where TRUE/FALSE needed


Can anyone help me to understand what this means and
how I might fix it?  Thanks!

                            Clif Emery



From spencer.graves at pdf.com  Wed May  4 01:21:20 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 03 May 2005 16:21:20 -0700
Subject: [R] maximization help :
In-Reply-To: <4276ED0C.7040805@unm.edu>
References: <4276ED0C.7040805@unm.edu>
Message-ID: <42780770.6090107@pdf.com>

	  Have you considered something like the following:

multilogit <- function(p){
	k <- length(p)
	z <- log(p)
	(z[-k]-z[k])
}

inv.multilogit <- function(z){
	k1 <- length(z)
	p. <- exp(z)
	p.i <- (1+sum(p.))
	(c(p., 1)/p.i)
}

multilogit(c(.1,.2,.7))
inv.multilogit(multilogit(c(.1,.2,.7)))
inv.multilogit(1:2)
multilogit(inv.multilogit(1:2))

prodSum <- function(x, A, log.=TRUE,
	trace.=FALSE, neg=TRUE){
	p <- inv.multilogit(x)
	if(trace.)cat("p =", p, ";")
	logP <- sum(log(A%*%p))
	{if(log.){
		if(neg) return(-logP)
		else return(logP)
		}
	else
		return(exp(logP))
	}
}

prodSum(1:2, diag(3), trace.=T)
sum(log(inv.multilogit(1:2)))

prodSum(0:1, diag(3), trace.=T)
sum(log(inv.multilogit(0:1)))

optim(c(0,0), fn=prodSum, hessian=TRUE, A=diag(3),
	method="CG", control=list(trace=999))
optim(1:2, fn=prodSum, hessian=TRUE, A=diag(3),
	method="CG", control=list(trace=999))

A <- array(c(1,1,1,0), dim=c(2,2))
optim(1, fn=prodSum, hessian=TRUE, A=A,
	method="CG", control=list(trace=999))

A <- array(c(1,1,0, 0, 1, 1), dim=c(3,2))
optim(1, fn=prodSum, hessian=TRUE, A=A,
	method="CG", control=list(trace=999))

There may be a more elegant solution based on singular values of A, but 
I don't see it.

	  hope this helps.
	  spencer graves

mingan yang wrote:

> 
> 
> Given a vector  : pvec=(p1,p2,.... p J)   with sum(pvec)=1,   all the 
> elements are non-negative, that is, they are probabilities
> 
> a  matrix   A  ( N* J ), with the elements  alpha(ij)  are 0 or 1
> 
> 
>    I want to MAXIMIZE THE RESULT
> 
>      RESULT=   product( i=1, to N   [ sum (  alpha(ij)* pj , j =1,to J ) 
> ]  )
> 
>    thus, I need to get pvec. how should I do ?
> 
>      for example
>               say, A=  0   1   0   0
>                 1    1  0   0
>                 1   0   0   0
>                0   0   1   0
>                1  0    0   1
>               0   0    0   1
> 
>   that is A is a matrix 6* 4    thus pvec=(p1,p2,p3,p4)
> 
>    I want to get  values of pvec such that , they can maximize
> 
>   p2 *  ( p1 + p2 ) * p1 * p3 * (p1+p4) * p4
> 
> 
>  thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From david.p.finlayson at gmail.com  Wed May  4 02:39:00 2005
From: david.p.finlayson at gmail.com (David Finlayson)
Date: Tue, 03 May 2005 17:39:00 -0700
Subject: [R] Calculate median from counts and values
Message-ID: <op.sp76raz51z0vet@cama.ocean.washington.edu>

I am tangled with a syntax question. I want to calculate basic statistics  
for a large dataset provided in weights and values and I can't figure out  
an elegant way to expand the data.

For example here are the counts:

> counts
    n4 n3 n2 n1 p0 p1 p2 p3  p4
1   0  0  0  1  1  3 16 55  24
2   0  0  0  0  2  8 28 47  15
3   1 17 17 13  4  5 12 24   8
...

and the values:

> values
      n4 n3 n2 n1 p0  p1   p2    p3     p4
[1,] 16  8  4  2  1 0.5 0.25 0.125 0.0625

What I want for each row is something like this (shown for row 1):

c( rep(16, 0), rep(8, 0), rep(4, 0), rep(2, 1), rep(1, 1), rep(0.5, 3),  
rep(0.25, 16), rep(0.125, 55), rep(0.0625, 24))

I am sure that this is a one-liner for an R-master, but I can't figure it  
out without a set of nested for loops iterating over each row in counts.

David



-- 
David Finlayson
Marine Geology & Geophysics
School of Oceanography
Box 357940
University of Washington
Seattle, WA  98195-7940
USA

Office: Marine Sciences Building, Room 112
Phone: (206) 616-9407
Web: http://students.washington.edu/dfinlays/



From kjetil at acelerate.com  Wed May  4 02:13:28 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 03 May 2005 20:13:28 -0400
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <20050503162440.59326.qmail@web54508.mail.yahoo.com>
References: <20050503162440.59326.qmail@web54508.mail.yahoo.com>
Message-ID: <427813A8.6000107@acelerate.com>

Globe Trotter wrote:

>Good point: the Bellman reference is a book:
>
>Introduction to Matrix Analysis by Bellman (1960). McGraw-Hill Series in Matrix
>Theory.
>  
>
--- and republished much later by SIAM.

Kjetil

>
>--- Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
>
>  
>
>>Hi everyone.
>>
>>The following webpage gives a definition of circulant matrix, which 
>>agrees with the
>>definition given in the magic package.
>>
>>http://mathworld.wolfram.com/CirculantMatrix.html
>>
>>best  wishes
>>
>>rksh
>>
>>
>>
>>On May 3, 2005, at 08:06 am, Mulholland, Tom wrote:
>>
>>    
>>
>>>Well since I know nothing about this topic I have lurked so far, but 
>>>here's my two bob's worth.
>>>
>>>Firstly I tried to make sense of Brian's initial reply. I have got no 
>>>idea who Bellman is and you have not referenced (his/her) work in a 
>>>way I can access the issues you refer to. So I assumed that's exactly 
>>>what Brian was talking about.
>>>
>>>Secondly.
>>>
>>>toeplitz(1:4)
>>>     [,1] [,2] [,3] [,4]
>>>[1,]    1    2    3    4
>>>[2,]    2    1    2    3
>>>[3,]    3    2    1    2
>>>[4,]    4    3    2    1
>>>
>>>require(magic)
>>> circulant(4)
>>>     [,1] [,2] [,3] [,4]
>>>[1,]    1    2    3    4
>>>[2,]    4    1    2    3
>>>[3,]    3    4    1    2
>>>[4,]    2    3    4    1
>>>
>>>So they are obviously two different things. Although I think you may 
>>>have implied (not stated) that the particular combination you were 
>>>using resulted in both being exactly the same.
>>>
>>>It does appear as if in this case the (X) matrix is circulant. But 
>>>then I'm no expert in even such simple things.
>>>
>>>Then I had no idea where I was going. So I tried the variations in 
>>>eigen.
>>>
>>>I ran you code
>>>x<-scan("h:/t.txt")
>>>y<-x[c(109:216,1:108)]
>>>X<-toeplitz(y)
>>> and then
>>>
>>>      
>>>
>>>>X[is.na(X)]
>>>>        
>>>>
>>>numeric(0)
>>>
>>>So I didn't get any NAs
>>>
>>>t1 <- eigen(X)$vectors
>>>t2 <- eigen(X,symmetric = TRUE)$vectors
>>>      
>>>
>>>>identical(t1,t2)
>>>>        
>>>>
>>>[1] TRUE
>>>      
>>>
>>>Then
>>>
>>>t2 <- eigen(X,symmetric = TRUE,EISPACK = TRUE)$vectors
>>>      
>>>
>>>>identical(t1,t2)
>>>>        
>>>>
>>>[1] FALSE
>>>      
>>>
>>>So there'e obviously more than one way of getting the vectors. Does 
>>>the second one make more sense to you?
>>>
>>>I also noticed in the eigen help that there are references to issues 
>>>such as "IEEE 754 arithmetic","(They may also differ between methods 
>>>and between platforms.)" and "or Hermitian if complex". All of these 
>>>are out of my competence but they do signal to me that there are 
>>>issues which may relate to hardware, digital arithmetic and other 
>>>things of that ilk.
>>>
>>>I added the comment about complex because I have a vague idea that 
>>>they are related to imaginary parts that you refer to.
>>>
>>>So not coming to any conclusion that makes sense to me, and given that 
>>>there are often threads about supposed inaccuracies that have answers 
>>>such as the digits you see are not always what are held by the machine 
>>>I set my options(digits = 22) and noticed that some of the numbers are 
>>>still going at the 22 decimal place suggesting that the machine might 
>>>be incapable of producing perfectly accurate results using digital 
>>>arithmetic.
>>>
>>>My other big sphere of ignorance is complex numbers.
>>>
>>>So I tried
>>>X<-toeplitz(complex(real = y))
>>>t1 <- eigen(X)$vectors
>>>
>>>      
>>>
>>>>t1[1:20]
>>>>        
>>>>
>>> [1]  0.068041577278880341+0i -0.068041577140546913+0i  
>>>0.068041576864811659+0i -0.068041576452430155+0i
>>> [5]  0.068041575907139579+0i -0.068041575231135451+0i  
>>>0.068041574435267163+0i -0.068041573525828514+0i
>>> [9]  0.068041572538722991+0i -0.068041571498323253+0i  
>>>0.068041570619888622+0i -0.068041570256170081+0i
>>>[13]  0.068041568759931989+0i -0.068041566476633147+0i  
>>>0.068041563560502477+0i -0.068041560000305007+0i
>>>[17]  0.068041555538765813+0i -0.068041549792984865+0i  
>>>0.068041544123969511+0i -0.068041537810956801+0i
>>>      
>>>
>>>>t2[1:20]
>>>>        
>>>>
>>> [1]  0.068041381743976906 -0.068041381743976850  0.068041381743976781 
>>>-0.068041381743976753  0.068041381743976587
>>> [6] -0.068041381743976725  0.068041381743976920 -0.068041381743976836 
>>> 0.068041381743976892 -0.068041381743976781
>>>[11]  0.068041381743976781 -0.068041381743977392  0.068041381743976725 
>>>-0.068041381743976753  0.068041381743976753
>>>[16] -0.068041381743976698  0.068041381743976587 -0.068041381743976642 
>>> 0.068041381743976698 -0.068041381743976490
>>>      
>>>
>>>Which is again different. I have no idea what I'm doing but you do 
>>>seem to get slightly different answers depending upon which method you 
>>>use. I do not know if one is superior to the others or where one draws 
>>>the line in terms of accuracy.
>>>
>>>Tom
>>>
>>>      
>>>
>>>>-----Original Message-----
>>>>From: r-help-bounces at stat.math.ethz.ch
>>>>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Globe Trotter
>>>>Sent: Tuesday, 3 May 2005 10:51 AM
>>>>To: r-help at stat.math.ethz.ch
>>>>Subject: Re: [R] eigenvalues of a circulant matrix
>>>>
>>>>
>>>>OK, here we go:
>>>>
>>>>I am submitting two attachments. The first is the datafile
>>>>called kinv used to
>>>>create my circulant matrix, using the following commands:
>>>>
>>>>
>>>>x<-scan("kinv")
>>>>y<-x[c(109:1,0:108)]
>>>>X=toeplitz(y)
>>>>eigen(X)
>>>>write(X,ncol=216,file="test.dat")
>>>>
>>>>reports the following columns full of NaN's: 18, 58, 194,
>>>>200. (Note that
>>>>eigen(X,symmetric=T) makes no difference and I get the same as above).
>>>>
>>>>The second attachment contains only the eigenvectors obtained
>>>>on calling a
>>>>LAPACK routine directly (from C). The eigenvalues are
>>>>essentially the same as
>>>>that obtained using R. Here, I use the LAPACK-recommended
>>>>double precision
>>>>routine dspevd() routine for symmetric matrices in packed
>>>>storage format. Note
>>>>the absence of the NaN's....I would be happy to send my C
>>>>programs to whoever
>>>>is interested.
>>>>
>>>>I am using
>>>>
>>>>:~> uname -a
>>>>Linux 2.6.11-1.14_FC3 #1 Thu Apr 7 19:23:49 EDT 2005 i686
>>>>i686 i386 GNU/Linux
>>>>
>>>>and R.2.0.1.
>>>>
>>>>Many thanks and best wishes!
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>>        
>>>>
>>>http://www.R-project.org/posting-guide.html
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>>      
>>>
>>--
>>Robin Hankin
>>Uncertainty Analyst
>>Southampton Oceanography Centre
>>European Way, Southampton SO14 3ZH, UK
>>  tel  023-8059-7743
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
Internal Virus Database is out-of-date.
Checked by AVG Anti-Virus.



From kjetil at acelerate.com  Wed May  4 02:19:18 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 03 May 2005 20:19:18 -0400
Subject: [R] How to prove R as good (Was: (no subject))
In-Reply-To: <IFXJ7M$83F019763D6380D79C4C80C4738E88E3@terra.com.br>
References: <IFXJ7M$83F019763D6380D79C4C80C4738E88E3@terra.com.br>
Message-ID: <42781506.30607@acelerate.com>

rene.raupp wrote:

>Does anybory knows any work comparing R with other (charged) statistical softwares (like Minitab, SPSS, SAS)?
>I work in a brasilian government bureau and I intend to use R as our preferable statistical software, but I have to show it's as good as the others. 
>
Sorry. That will be difficult. Could'nt it do to prove it is better?

Kjetil

>I also intend to use Weka, and for this one I have the same problem.
>Can anyone help me?
>Thanks
>Ren?? M. Raupp
>e-mail: rener at mpdft.gov.br
>        rene.raupp at terra.com.br
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
Internal Virus Database is out-of-date.
Checked by AVG Anti-Virus.



From ggrothendieck at gmail.com  Wed May  4 03:23:14 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 3 May 2005 21:23:14 -0400
Subject: [R] Calculate median from counts and values
In-Reply-To: <op.sp76raz51z0vet@cama.ocean.washington.edu>
References: <op.sp76raz51z0vet@cama.ocean.washington.edu>
Message-ID: <971536df05050318232beb8b64@mail.gmail.com>

On 5/3/05, David Finlayson <david.p.finlayson at gmail.com> wrote:
> I am tangled with a syntax question. I want to calculate basic statistics
> for a large dataset provided in weights and values and I can't figure out
> an elegant way to expand the data.
> 
> For example here are the counts:
> 
> > counts
>    n4 n3 n2 n1 p0 p1 p2 p3  p4
> 1   0  0  0  1  1  3 16 55  24
> 2   0  0  0  0  2  8 28 47  15
> 3   1 17 17 13  4  5 12 24   8
> ...
> 
> and the values:
> 
> > values
>      n4 n3 n2 n1 p0  p1   p2    p3     p4
> [1,] 16  8  4  2  1 0.5 0.25 0.125 0.0625
> 
> What I want for each row is something like this (shown for row 1):
> 
> c( rep(16, 0), rep(8, 0), rep(4, 0), rep(2, 1), rep(1, 1), rep(0.5, 3),
> rep(0.25, 16), rep(0.125, 55), rep(0.0625, 24))
> 
> I am sure that this is a one-liner for an R-master, but I can't figure it
> out without a set of nested for loops iterating over each row in counts.
> 

Is there supposed to be one row of values that apply to all
rows of counts or is there to be different rows of values for
different rows of counts?  Also in your example row 3 has
a different total than 1 or 2.  Is that right?

At any rate, I will assume that there is only one row of 
values and many rows of counts and that its not necessarily
true that counts sum to the same number in each row.
Then noting that  c(rep(4,1), rep(5,2), rep(6,3)) is the same
as rep(4:6, 1:3) is the same as, we have:

lapply(as.data.frame(t(counts)), rep, x = unlist(values))



From andy_liaw at merck.com  Wed May  4 03:57:49 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 3 May 2005 21:57:49 -0400
Subject: [R] General Question on learning R...
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076EC3@usctmx1106.merck.com>

For an installed package (rather than source), the R-ex/ directory contains
the code in the Example section of help pages; e.g., if you run
example(somefun), the code in somefun-ex.R is source()'ed.

Stuff in the demo/ directory are executed by the demo() function, and it's
optional:  Most packages do not have demo code.

Andy

> From: Spencer Graves
> 
> 	  Yes.  Thanks for the elaboration.  What differences 
> might one expect 
> between the contents of "\demo" and "\R-ex"?  (I found 
> "\R-ex" with all 
> the packages I named, but not all had "\demo".)
> 
> 	  spencer graves
> 
> There seem to be different
> 
> Jonathan Q. wrote:
> 
> > assuming one has these installed already, you just look in the demo
> > folder under each?  i.e., fBasics\demo ???
> > 
> > On 5/3/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> > 
> >>         I'm looking at the same thing.  A good source for 
> this is to
> >>'install.packages(c("fBasics", "fCalendar", "fExtremes", 
> "fMultivar",
> >>"fOptions", "fPotfolio", "fractdiff", "fSeries", "its", "lme4",
> >>"zoo"))', then 'update.packages()'.  These will install 
> subdirectories
> >>or folders with the indicated names "fBasics", etc., in 
> "library" with
> >>your R installation.  For example, in my Windows 
> installation, I have
> >>"D:\Program files\R\rw2010pat\library", which contains many 
> subfolders
> >>including ones named "fSeries".  These all contain files 
> "*.R", which
> >>provide sample code.
> >>
> >>         spencer graves
> >>
> >>Sean Davis wrote:
> >>
> >>
> >>>On May 3, 2005, at 6:46 AM, Jonathan Q. wrote:
> >>>
> >>>
> >>>>In the process of learning R, with a specific interest on 
> financial
> >>>>time series.  While I continue to get through the 
> documents I am more
> >>>>a fan of learning by example and then looking up how each 
> function is
> >>>>used.  Any websites which post sample code for R?
> >>>>
> >>>
> >>>The largest source of example code is R itself.  If you 
> have a command
> >>>in which you are interested, you can often just type the 
> command and the
> >>>code will be shown to you.  Try typing:
> >>>
> >>>ls()
> >>>
> >>>Then:
> >>>
> >>>ls
> >>>
> >>>It will show you the code used to produce the result.  Also, each
> >>>command has its own example(s) in the help.
> >>>
> >>>Sean
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>>http://www.R-project.org/posting-guide.html
> >>
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From spencer.graves at pdf.com  Wed May  4 04:06:19 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 03 May 2005 19:06:19 -0700
Subject: [R] How to prove R as good (Was: (no subject))
In-Reply-To: <42781506.30607@acelerate.com>
References: <IFXJ7M$83F019763D6380D79C4C80C4738E88E3@terra.com.br>
	<42781506.30607@acelerate.com>
Message-ID: <42782E1B.7040708@pdf.com>

	  Did you try to Google for R vs. your favorite alternative?  I just 
got 740 hits from Google for "R vs. SAS" and 82 from www.r-project.org 
-> search -> R site search.  This has been discussed on this list 
several times, and many benchmarks have been published.  If you don't 
find what you want fairly quickly, read the posting guide and ask a more 
specific question.  The benchmarks I've seen have rated R quite high.

	  Each commercial package may be able to claim that it is better for 
some particular purpose, e.g., SAS and the latest release of S-Plus with 
large data bases.  Minitab, SPSS, JMP and others may have an easier to 
use graphical user interface for naive users, although even that 
superiority is being challenged.

	  R has been changing and improving so fast that it is difficult for 
any of the commercial alternatives to keep up.  There are several 
reasons for this.  First,  R is easily extended.  Second, the R 
Foundatation for Statistical Computing has provided a supportive 
organizational framework that makes it easy for people to share.  Third, 
there are hundreds and perhaps thousands of competent professionals the 
world over who have been frustrated in the past by the steep price of 
commercial software for many things, and R provides a shockingly easy 
and open alternative that helps people share their latest developments 
with the entire world in a way that replaces that frustration with the 
pride of contributing to something incredibly useful.

	  Best Wishes,
	  spencer graves

Kjetil Brinchmann Halvorsen wrote:

> rene.raupp wrote:
> 
>> Does anybory knows any work comparing R with other (charged) 
>> statistical softwares (like Minitab, SPSS, SAS)?
>> I work in a brasilian government bureau and I intend to use R as our 
>> preferable statistical software, but I have to show it's as good as 
>> the others.
> 
> Sorry. That will be difficult. Could'nt it do to prove it is better?
> 
> Kjetil
> 
>> I also intend to use Weka, and for this one I have the same problem.
>> Can anyone help me?
>> Thanks
>> Ren?? M. Raupp
>> e-mail: rener at mpdft.gov.br
>>        rene.raupp at terra.com.br
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>>  
>>
> 
>



From david.p.finlayson at gmail.com  Wed May  4 04:23:39 2005
From: david.p.finlayson at gmail.com (David Finlayson)
Date: Tue, 3 May 2005 19:23:39 -0700
Subject: [R] Calculate median from counts and values
In-Reply-To: <971536df05050318232beb8b64@mail.gmail.com>
References: <op.sp76raz51z0vet@cama.ocean.washington.edu>
	<971536df05050318232beb8b64@mail.gmail.com>
Message-ID: <be6d17205050319235ee0478c@mail.gmail.com>

Thanks Gabor and Phil. That did it.

I've used R for years for plotting and run-of-the-mill data analysis
(the only kind I do). But the syntax of this language has just never
clicked for me. I can't seem to advance beyond the "mostly harmless"
stage. Python is roting my brain I guess.

Again, thanks for the tips

David 



On 5/3/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 5/3/05, David Finlayson <david.p.finlayson at gmail.com> wrote:
> > I am tangled with a syntax question. I want to calculate basic statistics
> > for a large dataset provided in weights and values and I can't figure out
> > an elegant way to expand the data.
> >
> > For example here are the counts:
> >
> > > counts
> >    n4 n3 n2 n1 p0 p1 p2 p3  p4
> > 1   0  0  0  1  1  3 16 55  24
> > 2   0  0  0  0  2  8 28 47  15
> > 3   1 17 17 13  4  5 12 24   8
> > ...
> >
> > and the values:
> >
> > > values
> >      n4 n3 n2 n1 p0  p1   p2    p3     p4
> > [1,] 16  8  4  2  1 0.5 0.25 0.125 0.0625
> >
> > What I want for each row is something like this (shown for row 1):
> >
> > c( rep(16, 0), rep(8, 0), rep(4, 0), rep(2, 1), rep(1, 1), rep(0.5, 3),
> > rep(0.25, 16), rep(0.125, 55), rep(0.0625, 24))
> >
> > I am sure that this is a one-liner for an R-master, but I can't figure it
> > out without a set of nested for loops iterating over each row in counts.
> >
> 
> Is there supposed to be one row of values that apply to all
> rows of counts or is there to be different rows of values for
> different rows of counts?  Also in your example row 3 has
> a different total than 1 or 2.  Is that right?
> 
> At any rate, I will assume that there is only one row of
> values and many rows of counts and that its not necessarily
> true that counts sum to the same number in each row.
> Then noting that  c(rep(4,1), rep(5,2), rep(6,3)) is the same
> as rep(4:6, 1:3) is the same as, we have:
> 
> lapply(as.data.frame(t(counts)), rep, x = unlist(values))
> 


-- 
David Finlayson
Marine Geology & Geophysics
School of Oceanography
Box 357940
University of Washington
Seattle, WA  98195-7940
USA

Office: Marine Sciences Building, Room 112
Phone: (206) 616-9407
Web: http://students.washington.edu/dfinlays



From deepayan at stat.wisc.edu  Wed May  4 04:59:21 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 3 May 2005 21:59:21 -0500
Subject: [R] Combining numeric vs numeric & numeric vs factor graphs into
	one ps/pdf file
In-Reply-To: <2800A210138FAD4780C77CE452DDDD873D34B5@NAWESDNIEX05VA.nadsuswe.nads.navy.mil>
References: <2800A210138FAD4780C77CE452DDDD873D34B5@NAWESDNIEX05VA.nadsuswe.nads.navy.mil>
Message-ID: <200505032159.21766.deepayan@stat.wisc.edu>

On Tuesday 03 May 2005 10:44, Arenas, David R.  CIV NAVAIR DEPT wrote:
> Dear R community,
>
> My previous email was incomplete because I used html format.  Here it
> is again and sorry for any inconvenience:
>
> xyplot (lattice) has been great in displaying tons of data for my
> research.  I have used the following two xyplot commands (with
> example dataframe) to create two separate postscript/pdf files with
> respect to the variable "acft" and subset "status":
>
> test.df <- data.frame(acft=factor(c("A","B","C","D")),
>                              
> status=factor(c("fail","pass","fail","pass")),
> site=factor(c("E1","E1","E2","E2")), CD=as.numeric(c(1,1,3,3)),
>                               H=as.numeric(c(80,NA,60,NA)))
>
> xyplot(H ~ CD | acft,
>           data=test.df,
>           subset=status=="fail",
>           layout=c(1,1) )
>
> xyplot(site ~ CD | acft,
>           data=test.df,
>           subset=status=="pass",
>           layout=c(1,1) )
>
>  I would like to combine all graphs into one file in alphabetical
> order of variable "acft".  The graphs would be one per page where in
> fact I use layout=c(1,1) for the nice and easily seen strip labels
> for "acft".  The problem I am having is combining x-y plots that are
> numeric vs numeric & numeric vs factor.  I have search the R-help
> archives and R-project references for an example to no avail.  I am
> thinking I may have to use something (lattice or not) like ...
>
> if any(test.df$Status=="fail")
> plot(H ~ CD)
> else
> plot(site ~ CD)
>
> with "for" in the beginning to loop through all data with respect to
> acft.  I need a hint on how to further this along.  I am using
> R.2.1.0 via Windows XP.

I can't think of a clean way to do this. You could of course do 

xyplot(ifelse(status == "pass", as.numeric(site), H) ~ CD| acft,
       data=test.df, scales = "free",
       layout=c(1,1) )

but this wouldn't give very nice axis labels for the factor (unless you 
specify them manually, which could be done).

Deepayan



From ra.webster at student.qut.edu.au  Wed May  4 05:50:36 2005
From: ra.webster at student.qut.edu.au (Ron Webster)
Date: Wed, 04 May 2005 13:50:36 +1000
Subject: [R] Hershey Fontsize Decrease When Sent To Printer
Message-ID: <6.2.1.2.2.20050504133818.01e9dae8@mail.qut.edu.au>

Hi,

I am running R 2.0.1 on Windows XP Version 5.1.

I called demo(Hershey) but when I used any network printer to print the 
output in R Graphics Device the font size of the Hershey characters were 
reduced so that they were printed as dots.

Is this a bug in R?
Or is it a problem with the Windows,  say the printers need to have the 
Hershey fonts loaded, that my technical staff may address?

Thanks
Ron Webster



From itsme_410 at yahoo.com  Wed May  4 06:07:24 2005
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Tue, 3 May 2005 21:07:24 -0700 (PDT)
Subject: [R] eigenvalues of a circulant matrix
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A9408@uswpmx00.merck.com>
Message-ID: <20050504040724.67837.qmail@web54509.mail.yahoo.com>


--- "Huntsinger, Reid" <reid_huntsinger at merck.com> wrote:

> Under Linux run "ldd" on the binary to see what shared libraries the binary
> has been linked against and will attempt to load. The command you run is a
> shell script which sets and exports LD_LIBRARY_PATH and then runs R, so to
> be sure first start R and execute
> 
> > system("echo $LD_LIBRARY_PATH")

/usr/lib/R/lib:/usr/local/lib:/usr/X11R6/lib:/usr/local/lib


> 
> and then using this value for LD_LIBRARY_PATH do
> 
> $ export LD_LIBRARY_PATH=<R's value for this>

I use tcsh, so had to switch to bash to use export....

> $ ldd `R RHOME`/bin/exec/R

 
        libblas.so.3 => /usr/lib/libblas.so.3 (0x00995000)
        libg2c.so.0 => /usr/lib/libg2c.so.0 (0x0068f000)
        libm.so.6 => /lib/tls/libm.so.6 (0x00664000)
        libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x00dd1000)
        libreadline.so.4 => /usr/lib/libreadline.so.4 (0x00d3d000)
        libncurses.so.5 => /usr/lib/libncurses.so.5 (0x03d2f000)
        libdl.so.2 => /lib/libdl.so.2 (0x00689000)
        libc.so.6 => /lib/tls/libc.so.6 (0x00538000)
        /lib/ld-linux.so.2 (0x0051e000)



> which will list the shared libraries R will try to load to resolve links to
> shared libraries. 
> 
> Having said that, I have tried your example on several platforms with
> several configurations, and I get NaNs only when R is calling the Lapack
> routine dsyevr (R's builtin Lapack) with an external (optimized) blas
> (either Goto or ATLAS's blas) but not R's builtin blas. Moreover even with
> the optimized blas, if I use dysev instead of dsyevr, (eigen() executes a
> .Call("La_rs",x,only.values,"dsyevr",PACKAGE="base"), I execute this from R
> with "dsyev" in place of "dsyevr") I get no NaNs. 
> 
> So it looks like a problem with a blas routine used by dsyevr but not dsyev,
> but I have yet to confirm.

Well, I have now written a test function in C to call dsyevr, but it goes
through fine -- no NaN's. (I can send the test function).

I wonder if there is an issue with workspace allocation in R: dsyevr has an
optimal way of doing that, and that is obtained by a call with iwork=-1. 

Many thanks and best wishes!


> I note that your C program does not use either of these lapack routines.
> 
> Reid Huntsinger
> 
>



From mingan at unm.edu  Wed May  4 07:30:07 2005
From: mingan at unm.edu (mingan)
Date: Tue, 03 May 2005 23:30:07 -0600
Subject: [R] contributed package source codes
Message-ID: <42785DDF.9060304@unm.edu>




 Is there a way for me to get the source code ( C/C++ or FORTRAN)  for a 
specific  R package ?

 

thanks



From ligges at statistik.uni-dortmund.de  Wed May  4 08:54:34 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 04 May 2005 08:54:34 +0200
Subject: [R] contributed package source codes
In-Reply-To: <42785DDF.9060304@unm.edu>
References: <42785DDF.9060304@unm.edu>
Message-ID: <427871AA.40200@statistik.uni-dortmund.de>

mingan wrote:

> 
> 
> 
> Is there a way for me to get the source code ( C/C++ or FORTRAN)  for a 
> specific  R package ?

Well, it is *included* in the (source) package...

???

Uwe Ligges

> 
> 
> thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From m.blizinski at wsisiz.edu.pl  Wed May  4 09:23:17 2005
From: m.blizinski at wsisiz.edu.pl (Maciej =?iso-8859-2?Q?Blizi=F1ski?=)
Date: Wed, 4 May 2005 09:23:17 +0200
Subject: [R] How to intepret a factor response model?
Message-ID: <20050504072317.GA5958@oceanic.wsisiz.edu.pl>

Hello,

I'd like to create a model with a factor-type response variable. This is
an example:

> mydata <- data.frame(factor_var = as.factor(c(rep('one', 100), rep('two', 100), rep('three', 100))), real_var = c(rnorm(150), rnorm(150) + 5))
> summary(mydata)
 factor_var     real_var        
 one  :100   Min.   :-2.742877  
 three:100   1st Qu.:-0.009493  
 two  :100   Median : 2.361669  
             Mean   : 2.490411  
             3rd Qu.: 4.822394  
             Max.   : 6.924588  
> mymodel = glm(factor_var ~ real_var, family = 'binomial', data = mydata)
> summary(mymodel)

Call:
glm(formula = factor_var ~ real_var, family = "binomial", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7442  -0.6774   0.1849   0.3133   2.1187  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -0.6798     0.1882  -3.613 0.000303 ***
real_var      0.8971     0.1066   8.417  < 2e-16 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 381.91  on 299  degrees of freedom
Residual deviance: 213.31  on 298  degrees of freedom
AIC: 217.31

Number of Fisher Scoring iterations: 6

---------------------------------------------------------------------

For models with real-type response variable it's easy to figure out,
what's the equation for the response variable (in the model). But here
- how do I interpret the model?

-- 
God made the world in six days, and was arrested on the seventh.



From m.gardiner-garden at garvan.org.au  Wed May  4 09:38:50 2005
From: m.gardiner-garden at garvan.org.au (Margaret Gardiner-Garden)
Date: Wed, 4 May 2005 17:38:50 +1000
Subject: [R] lmer error:flist must be a non-empty list
Message-ID: <GEEOKKGHHBBKNNOCKDOGIEMDCLAA.m.gardiner-garden@garvan.org.au>

Hi,

I was wondering if anyone could give me advice regarding using the lmer
command in lme4 package to do logistic regression (mixed effects model).

I use the following command
lmer(ISH ~ArrayPathology2, random=~1|PatientID, data=HSDB4.noNA,
family="binomial")

where ISH is outcome(0 or 1), ArrayPathology2 is the variable of
interest(factor), PatientID is random effect(factor), and HSDB4.noNA is the
complete dataframe (column names ArrayPathology2, Patient ID and ISH etc)

There are no NAs in the data.

I get the following error regardless of the R version (2.0.1, 2.1.0, or
2.1.0devel)

> lmer(ISH ~ArrayPathology2, random=~1|PatientID, data=HSDB4.noNA,
family="binomial")
Error in lmer(ISH ~ ArrayPathology2, random = ~1 | PatientID, data =
HSDB4.noNA,  :
        flist must be a non-empty list

Below is the sessionInfo() for R version 2.1.0

R version 2.1.0, 2005-04-18, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
        lme4 latticeExtra      lattice       Matrix
    "0.95-6"      "0.1-3"     "0.11-6"     "0.95-7"

I got the same error if I used the older version of lme4  "0.95-1" which was
mentioned in a mail to this list serve on 11th April 2005

I am using Microsoft Windows XP Professional Version 2002.

I would really appreciate any advice!!

Thanks so much

Dr Margaret Gardiner-Garden
Garvan Institute of Medical Research
Sydney Australia



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed May  4 09:57:28 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 4 May 2005 09:57:28 +0200
Subject: [R] lmer error:flist must be a non-empty list
References: <GEEOKKGHHBBKNNOCKDOGIEMDCLAA.m.gardiner-garden@garvan.org.au>
Message-ID: <010801c5507e$eabbe150$0540210a@www.domain>

Hi Margaret,

if you look at the help page of lmer() you'll see that there is no 
'random' argument (as there is in lme() in the nlme package). The way 
to specify random-effects structure in lmer() is through the 'formula' 
argument. So in your case try this:

lmer(ISH ~ ArrayPathology2 + (1|PatientID), data=HSDB4.noNA, 
family="binomial")

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Margaret Gardiner-Garden" <m.gardiner-garden at garvan.org.au>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, May 04, 2005 9:38 AM
Subject: [R] lmer error:flist must be a non-empty list


> Hi,
>
> I was wondering if anyone could give me advice regarding using the 
> lmer
> command in lme4 package to do logistic regression (mixed effects 
> model).
>
> I use the following command
> lmer(ISH ~ArrayPathology2, random=~1|PatientID, data=HSDB4.noNA,
> family="binomial")
>
> where ISH is outcome(0 or 1), ArrayPathology2 is the variable of
> interest(factor), PatientID is random effect(factor), and HSDB4.noNA 
> is the
> complete dataframe (column names ArrayPathology2, Patient ID and ISH 
> etc)
>
> There are no NAs in the data.
>
> I get the following error regardless of the R version (2.0.1, 2.1.0, 
> or
> 2.1.0devel)
>
>> lmer(ISH ~ArrayPathology2, random=~1|PatientID, data=HSDB4.noNA,
> family="binomial")
> Error in lmer(ISH ~ ArrayPathology2, random = ~1 | PatientID, data =
> HSDB4.noNA,  :
>        flist must be a non-empty list
>
> Below is the sessionInfo() for R version 2.1.0
>
> R version 2.1.0, 2005-04-18, i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils" 
> "datasets"
> [7] "base"
>
> other attached packages:
>        lme4 latticeExtra      lattice       Matrix
>    "0.95-6"      "0.1-3"     "0.11-6"     "0.95-7"
>
> I got the same error if I used the older version of lme4  "0.95-1" 
> which was
> mentioned in a mail to this list serve on 11th April 2005
>
> I am using Microsoft Windows XP Professional Version 2002.
>
> I would really appreciate any advice!!
>
> Thanks so much
>
> Dr Margaret Gardiner-Garden
> Garvan Institute of Medical Research
> Sydney Australia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ecu at info.fundp.ac.be  Wed May  4 10:22:01 2005
From: ecu at info.fundp.ac.be (Cuvelier Etienne)
Date: Wed, 04 May 2005 10:22:01 +0200
Subject: [R] (no subject)
In-Reply-To: <IFXJ7M$83F019763D6380D79C4C80C4738E88E3@terra.com.br>
Message-ID: <5.2.0.9.0.20050504101832.00a9b738@pop.info.fundp.ac.be>


At 16:52 3/05/2005 -0300, rene.raupp wrote:
>Does anybory knows any work comparing R with other (charged) statistical softwares (like Minitab, SPSS, SAS)?
>I work in a brasilian government bureau and I intend to use R as our preferable statistical software, but I have to show it's as good as the others. I also intend to use Weka, and for this one I have the same problem.
>Can anyone help me?

The results of a Benchmark test of various number crunching packages  (R 1.9.0, S-PLUS 6.1,  Matlab 6.0, O-Matrix 5.6 Ml mode, 
O-Matrix 5.6 native, Octave 2.1.42, Scilab 2.7, Ox 3.30) can be found at :
http://www.sciviews.org/benchmark/index.html


>Thanks
>Ren?? M. Raupp
>e-mail: rener at mpdft.gov.br
>        rene.raupp at terra.com.br
>
>        [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Cuvelier Etienne
Assistant
FUNDP - Institut d'Informatique
rue Grandgagnage, 21   B-5000 Namur (Belgique)
tel: 32.81.72.49.93    fax: 32.81.72.49.67



From epakpahan at gmail.com  Wed May  4 10:25:52 2005
From: epakpahan at gmail.com (Eduwin Pakpahan)
Date: Wed, 4 May 2005 15:25:52 +0700
Subject: [R] loading gap package
Message-ID: <646e05400505040125b315f1b@mail.gmail.com>

Dear R users, 
I did install R. 2.0.1, and try to load "gap" package. 
However, below is the message shown when I did load it. Can anybody
please let me know my mistakes?

----
> library()
> library(gap)
Loading required package: MASS 
Loading required package: genetics 
Loading required package: combinat 
Loading required package: gdata 
Error: package 'gdata' could not be loaded
In addition: Warning message: 
There is no package called 'gdata' in: library(pkg, character.only =
TRUE, logical = TRUE, lib.loc = lib.loc)
----

Thanks,

Edwin



From Ted.Harding at nessie.mcc.ac.uk  Wed May  4 10:21:11 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 04 May 2005 09:21:11 +0100 (BST)
Subject: [R] How to intepret a factor response model?
In-Reply-To: <20050504072317.GA5958@oceanic.wsisiz.edu.pl>
Message-ID: <XFMail.050504092111.Ted.Harding@nessie.mcc.ac.uk>

On 04-May-05 Maciej Blizi??ski wrote:
> Hello,
> 
> I'd like to create a model with a factor-type response variable.
> This is an example:
> 
>> mydata <- data.frame(factor_var = as.factor(c(rep('one', 100),
>> rep('two', 100), rep('three', 100))), real_var = c(rnorm(150),
>> rnorm(150) + 5))
>> summary(mydata)
>  factor_var     real_var        
>  one  :100   Min.   :-2.742877  
>  three:100   1st Qu.:-0.009493  
>  two  :100   Median : 2.361669  
>              Mean   : 2.490411  
>              3rd Qu.: 4.822394  
>              Max.   : 6.924588  
>> mymodel = glm(factor_var ~ real_var, family = 'binomial', data =
>> mydata)
>> summary(mymodel)
> 
> Call:
> glm(formula = factor_var ~ real_var, family = "binomial", data =
> mydata)
> 
> Deviance Residuals: 
>     Min       1Q   Median       3Q      Max  
> -1.7442  -0.6774   0.1849   0.3133   2.1187  
> 
> Coefficients:
>             Estimate Std. Error z value Pr(>|z|)    
> (Intercept)  -0.6798     0.1882  -3.613 0.000303 ***
> real_var      0.8971     0.1066   8.417  < 2e-16 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> (Dispersion parameter for binomial family taken to be 1)
> 
>     Null deviance: 381.91  on 299  degrees of freedom
> Residual deviance: 213.31  on 298  degrees of freedom
> AIC: 217.31
> 
> Number of Fisher Scoring iterations: 6

Have you noticed that you get identical results with

set.seed(214354)
mydata <- data.frame(factor.var = as.factor(c(rep('one', 100),
   rep('two',100), rep('three', 100))),
   real.var = c(rnorm(150), rnorm(150) + 5))

mymodel <- glm(factor.var ~ real.var, family='binomial', data=mydata)
summary(mymodel)

and

set.seed(214354)
mydata <- data.frame(factor.var = as.factor(c(rep('one', 100),
   rep('two',200))),real.var = c(rnorm(150),rnorm(150) + 5))

mymodel <- glm(factor.var ~ real.var, family='binomial', data=mydata)
summary(mymodel)

(I've left out the "summary(mydata)" since these do naturally
differ, and I've replaced "factor_var" with "factor.var" and
"real_var" with "real.var" because of potential complications
with "_"; also "mymodel =" to "mymodel <-").

So I think the interpretation of the results from your first
model is that, because of the "family='binomial'", glm is
treating "factor.var='one'" as binomial response "0", say,
and "factor.var='two'" or "factor.var='three'" as binomial
response "1".

You're trying to fit a multinomial response, but you've
specified a binomial family to 'glm'. 'glm' does not have
a multinomial response family.

You could try 'multinom' from package 'nnet' which fits
loglinear models to factor responses with more than 2 levels.

E.g.

  library(nnet)
  mymodel <- multinom(factor.var ~ real.var,data=mydata)
   ### weights:  9 (4 variable)
   ##  initial  value 329.583687 
   ##  iter  10 value 209.780666
   ##  final  value 209.779951 
   ##  converged
  summary(mymodel)
   ## Re-fitting to get Hessian
   ## Call:
   ## multinom(formula = factor.var ~ real.var, data = mydata)
   ##  Coefficients:
   ##        (Intercept)  real.var
   ##  three  -3.4262565 1.3838231
   ##  two    -0.6754253 0.7116955
   ##
   ## Std. Errors:
   ##   (Intercept)  real.var
   ## three   0.5028541 0.1480138
   ## two     0.1846827 0.1068821
   ##
   ## Residual Deviance: 419.5599 
   ## AIC: 427.5599 
   ##
   ## Correlation of Coefficients:
   ##             three:(Intercept) three:real.var two:(Intercept)
   ## three:real.var  -0.7286258                                      
   ## two:(Intercept)  0.1986995        -0.1261034                    
   ## two:real.var    -0.1411377         0.7012481     -0.3285741

This output does suggest a fairly clear interpretation!

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 04-May-05                                       Time: 09:18:03
------------------------------ XFMail ------------------------------



From murdoch at stats.uwo.ca  Wed May  4 10:31:56 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 May 2005 09:31:56 +0100
Subject: [R] contributed package source codes
In-Reply-To: <42785DDF.9060304@unm.edu>
References: <42785DDF.9060304@unm.edu>
Message-ID: <4278887C.6080702@stats.uwo.ca>

mingan wrote:
> 
> 
> 
> Is there a way for me to get the source code ( C/C++ or FORTRAN)  for a 
> specific  R package ?

The source to contributed packages is in <CRAN 
mirror>/src/contrib/PACKAGES.html.  You can find your CRAN mirror by 
looking at the CRAN entry in options("repos").  (You may need to call 
chooseCRANmirror() first, if it hasn't already been called.)

The source to base packages is in the R source, also on CRAN.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Wed May  4 10:35:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 04 May 2005 10:35:11 +0200
Subject: [R] loading gap package
In-Reply-To: <646e05400505040125b315f1b@mail.gmail.com>
References: <646e05400505040125b315f1b@mail.gmail.com>
Message-ID: <4278893F.4030007@statistik.uni-dortmund.de>

Eduwin Pakpahan wrote:

> Dear R users, 
> I did install R. 2.0.1, and try to load "gap" package. 
> However, below is the message shown when I did load it. Can anybody
> please let me know my mistakes?
> 
> ----
> 
>>library()
>>library(gap)
> 
> Loading required package: MASS 
> Loading required package: genetics 
> Loading required package: combinat 
> Loading required package: gdata 
> Error: package 'gdata' could not be loaded
> In addition: Warning message: 
> There is no package called 'gdata' in: library(pkg, character.only =
> TRUE, logical = TRUE, lib.loc = lib.loc)
> ----


Let me read the error message for you:
"There is no package 'gdata'" simply means the package is not there...
So what about installing it?

Hint: it is part of the package bundle "gregmisc".

Uwe Ligges


> Thanks,
> 
> Edwin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed May  4 10:37:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 May 2005 09:37:54 +0100 (BST)
Subject: [R] How to intepret a factor response model?
In-Reply-To: <20050504072317.GA5958@oceanic.wsisiz.edu.pl>
References: <20050504072317.GA5958@oceanic.wsisiz.edu.pl>
Message-ID: <Pine.LNX.4.61.0505040934470.725@gannet.stats>

On Wed, 4 May 2005, Maciej [iso-8859-2] BliziDski wrote:

> I'd like to create a model with a factor-type response variable. This is
> an example:

What you have done here is to fit a logistic regression.  The 
interpretation of that is covered in many good books: for example there 
are plots of the predicted values in MASS4.

I do wonder if that is what you intended, though.  You have fitted a model 
of 'two or three' vs 'one'.  You may have intended a multinomial logistic 
model: again MASS4 has details of such models.

>> mydata <- data.frame(factor_var = as.factor(c(rep('one', 100), rep('two', 100), rep('three', 100))), real_var = c(rnorm(150), rnorm(150) + 5))
>> summary(mydata)
> factor_var     real_var
> one  :100   Min.   :-2.742877
> three:100   1st Qu.:-0.009493
> two  :100   Median : 2.361669
>             Mean   : 2.490411
>             3rd Qu.: 4.822394
>             Max.   : 6.924588
>> mymodel = glm(factor_var ~ real_var, family = 'binomial', data = mydata)
>> summary(mymodel)
>
> Call:
> glm(formula = factor_var ~ real_var, family = "binomial", data = mydata)
>
> Deviance Residuals:
>    Min       1Q   Median       3Q      Max
> -1.7442  -0.6774   0.1849   0.3133   2.1187
>
> Coefficients:
>            Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -0.6798     0.1882  -3.613 0.000303 ***
> real_var      0.8971     0.1066   8.417  < 2e-16 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> (Dispersion parameter for binomial family taken to be 1)
>
>    Null deviance: 381.91  on 299  degrees of freedom
> Residual deviance: 213.31  on 298  degrees of freedom
> AIC: 217.31
>
> Number of Fisher Scoring iterations: 6
>
> ---------------------------------------------------------------------
>
> For models with real-type response variable it's easy to figure out,
> what's the equation for the response variable (in the model). But here
> - how do I interpret the model?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed May  4 10:40:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 May 2005 09:40:35 +0100 (BST)
Subject: [R] contributed package source codes
In-Reply-To: <4278887C.6080702@stats.uwo.ca>
References: <42785DDF.9060304@unm.edu> <4278887C.6080702@stats.uwo.ca>
Message-ID: <Pine.LNX.4.61.0505040938160.725@gannet.stats>

On Wed, 4 May 2005, Duncan Murdoch wrote:

> mingan wrote:
>> 
>> 
>> 
>> Is there a way for me to get the source code ( C/C++ or FORTRAN)  for a 
>> specific  R package ?
>
> The source to contributed packages is in <CRAN 
> mirror>/src/contrib/PACKAGES.html.  You can find your CRAN mirror by looking 
> at the CRAN entry in options("repos").  (You may need to call 
> chooseCRANmirror() first, if it hasn't already been called.)

And in 2.1.0 you can retrieve the source by e.g.

 	download.packages("AMORE", type="source", destdir="/tmp")

on any platform (MacOS X not tested).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed May  4 10:41:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 04 May 2005 10:41:47 +0200
Subject: [R] Converting pc files to unix
In-Reply-To: <20050503135324.88597.qmail@web32414.mail.mud.yahoo.com>
References: <20050503135324.88597.qmail@web32414.mail.mud.yahoo.com>
Message-ID: <42788ACB.1020307@statistik.uni-dortmund.de>

Hai Lin wrote:

> Hello R users, 
> 
> I have R script files writen in my pc laptop. I encountered problems when I run those files in R in Mac OS X (10.3.5).  
> 
> Are there any ways in R that can make .R files executable? 


It should work, at once, given you have not used Windows specific 
commands, hard coded paths etc...

Uwe Ligges



> Thanks in advance
> 
> Kevin
> 
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed May  4 10:46:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 May 2005 09:46:02 +0100 (BST)
Subject: [R] loading gap package
In-Reply-To: <646e05400505040125b315f1b@mail.gmail.com>
References: <646e05400505040125b315f1b@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505040941200.725@gannet.stats>

On Wed, 4 May 2005, Eduwin Pakpahan wrote:

> Dear R users,
> I did install R. 2.0.1, and try to load "gap" package.

Did you install the dependencies of that package and their 
dependencies ...

> library(help=gap)
...
Depends:             MASS, genetics, R (>= 2.0.0)
> library(help=genetics)
Depends:       combinat, gdata, MASS

gdata is part of the gregmisc bundle, and

install.packages("gap", dependencies=TRUE)

would have done this for you.


> However, below is the message shown when I did load it. Can anybody
> please let me know my mistakes?
>
> ----
>> library()
>> library(gap)
> Loading required package: MASS
> Loading required package: genetics
> Loading required package: combinat
> Loading required package: gdata
> Error: package 'gdata' could not be loaded
> In addition: Warning message:
> There is no package called 'gdata' in: library(pkg, character.only =
> TRUE, logical = TRUE, lib.loc = lib.loc)
> ----
>
> Thanks,
>
> Edwin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From costas at mit.edu  Wed May  4 10:46:15 2005
From: costas at mit.edu (Constantinos Antoniou)
Date: Wed, 4 May 2005 11:46:15 +0300
Subject: [R] loading gap package
In-Reply-To: <4278893F.4030007@statistik.uni-dortmund.de>
References: <646e05400505040125b315f1b@mail.gmail.com>
	<4278893F.4030007@statistik.uni-dortmund.de>
Message-ID: <fccc5dce2c62ef4b2304106b4426d0a5@mit.edu>


On 4 ?????? 2005, at 11:35 ????, Uwe Ligges wrote:

> Eduwin Pakpahan wrote:
>
>> Dear R users, I did install R. 2.0.1, and try to load "gap" package. 
>> However, below is the message shown when I did load it. Can anybody
>> please let me know my mistakes?
>> ----
>>> library()
>>> library(gap)
>> Loading required package: MASS Loading required package: genetics 
>> Loading required package: combinat Loading required package: gdata 
>> Error: package 'gdata' could not be loaded
>> In addition: Warning message: There is no package called 'gdata' in: 
>> library(pkg, character.only =
>> TRUE, logical = TRUE, lib.loc = lib.loc)
>> ----
>
>
> Let me read the error message for you:
> "There is no package 'gdata'" simply means the package is not there...
> So what about installing it?
>
> Hint: it is part of the package bundle "gregmisc".
>

I have a similar problem with R 2.1.0 on Mac OSX 10.3.9.

 > library(gregmisc)
Loading required package: gdata
Error: package 'gdata' could not be loaded
In addition: Warning message:
there is no package called 'gdata' in: library(pkg, character.only = 
TRUE, logical = TRUE, lib.loc = lib.loc)

At first I tried getting this gdata from somewhere else, but I could 
not find it (evidence indicated that it is part of the gregmisc, as I 
suspected originally... You email confirms this...)

(For the record, my goal is to get read.xls, which is part (?) of gdata)

Thanks,

Costas




> Uwe Ligges
>
>
>> Thanks,
>> Edwin
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 
Constantinos Antoniou, Ph.D.
Massachusetts Institute of Technology
Intelligent Transportation Systems Program
77 Massachusetts Ave., Rm. 1-249, Cambridge, MA 02139



From davidhughjones at gmail.com  Wed May  4 10:57:15 2005
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Wed, 4 May 2005 09:57:15 +0100
Subject: [R] Re: nlme: "Deficient rank in gls_loglik" when creating
	corAR1()
In-Reply-To: <427803C8.2010106@stat.wisc.edu>
References: <f5d8480605050308532ddbd468@mail.gmail.com>
	<f5d8480605050311005fd7e4f7@mail.gmail.com>
	<427803C8.2010106@stat.wisc.edu>
Message-ID: <f5d8480605050401575651dae6@mail.gmail.com>

I tried that but it didn't make any difference to the output.

David

On 04/05/05, Douglas Bates <bates at stat.wisc.edu> wrote:
> David Hugh-Jones wrote:
> > Is this a bug? Should I attach a test case?
> 
> Try setting verbose=TRUE in the call to gls first and see if that gives
> you any insight into what is happening.
> 
> >
> > D
> >
> > On 03/05/05, David Hugh-Jones <davidhughjones at gmail.com> wrote:
> >
> >>I have a bunch of data which is structured by year and US state, so I
> >>have created a nlme groupedData object for it:
> >>
> >>formula(gd2)
> >>DEPVAR ~ YEAR | ABREV
> >>
> >>Now I am trying to run a gls regression on it. I want the error
> >>correlation structure to be AR1 with a different rho for each state,
> >>so I do
> >>
> >>
> >>>mdyn.1.1 = gls(model = DEPVAR ~ BLAH + BLAH, data=gd2, corr=corAR1(form= ~ YEAR | ABREV),na.action=na.omit)
> >>
> >>YEAR and ABREV are always present; DEPVAR is absent for one state.
> >>
> >>I get the following error message:
> >>
> >>Error in logLik.glsStruct(glsSt, glsPars) :
> >>        Deficient rank in gls_loglik
> >>
> >>Can anyone enlighten me? The error message goes away if I just do
> >>corAR1(form = ~1), but this is not meaningful for my data.
> >>
> >>Cheers
> >>David
> >>
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From 0034058 at fudan.edu.cn  Wed May  4 11:00:40 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Wed, 04 May 2005 17:00:40 +0800
Subject: [R] (no subject)
In-Reply-To: <5.2.0.9.0.20050504101832.00a9b738@pop.info.fundp.ac.be>
References: <IFXJ7M$83F019763D6380D79C4C80C4738E88E3@terra.com.br>
	<5.2.0.9.0.20050504101832.00a9b738@pop.info.fundp.ac.be>
Message-ID: <20050504170040.2dab84c7@localhost.localdomain>

On Wed, 04 May 2005 10:22:01 +0200
Cuvelier Etienne <ecu at info.fundp.ac.be> wrote:

> 
> At 16:52 3/05/2005 -0300, rene.raupp wrote:
> >Does anybory knows any work comparing R with other (charged) statistical softwares (like Minitab, SPSS, SAS)?
> >I work in a brasilian government bureau and I intend to use R as our preferable statistical software, but I have to show it's as good as the others. I also intend to use Weka, and for this one I have the same problem.
> >Can anyone help me?
> 
> The results of a Benchmark test of various number crunching packages  (R 1.9.0, S-PLUS 6.1,  Matlab 6.0, O-Matrix 5.6 Ml mode, 
> O-Matrix 5.6 native, Octave 2.1.42, Scilab 2.7, Ox 3.30) can be found at :
> http://www.sciviews.org/benchmark/index.html

i can NOT connect with the website .i do not why.anyone else come across this problem?


 
> 
> >Thanks
> >RenÅ®Å¶ M. Raupp
> >e-mail: rener at mpdft.gov.br
> >        rene.raupp at terra.com.br
> >
> >        [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> Cuvelier Etienne
> Assistant
> FUNDP - Institut d'Informatique
> rue Grandgagnage, 21   B-5000 Namur (Belgique)
> tel: 32.81.72.49.93    fax: 32.81.72.49.67
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From costas at mit.edu  Wed May  4 11:03:02 2005
From: costas at mit.edu (Constantinos Antoniou)
Date: Wed, 4 May 2005 12:03:02 +0300
Subject: [R] Converting pc files to unix
In-Reply-To: <42788ACB.1020307@statistik.uni-dortmund.de>
References: <20050503135324.88597.qmail@web32414.mail.mud.yahoo.com>
	<42788ACB.1020307@statistik.uni-dortmund.de>
Message-ID: <3a7906453b90657b5823449c3828a8f5@mit.edu>


On 4 ?????? 2005, at 11:41 ????, Uwe Ligges wrote:

> Hai Lin wrote:
>
>> Hello R users, I have R script files writen in my pc laptop. I 
>> encountered problems when I run those files in R in Mac OS X 
>> (10.3.5).  Are there any ways in R that can make .R files executable?
>
>
> It should work, at once, given you have not used Windows specific 
> commands, hard coded paths etc...
>
> Uwe Ligges
>


Hai,

On the mac, try to open the files in a "unixy" editor, such as emacs, 
pico, vi. If your file looks something like this (look for ^M):

first line of R commands^Msecond line of R commands^Metc

then the problem is line endings (that are different between platforms).

To fix this, you can open the file(s) in an editor, e.g. SubEthaEdit 
(free for personal and educational use) and then change the line 
endings (Format->Line endings->Convert to unix line endings..)

Another approach (useful if you want to batch-convert a lot of files) 
is to run a script like this:

  #!/bin/sh
     tr '\r' '\n' < $1 >MKTMP001
     mv -f MKTMP001 $1

You can save this as a file, e.g. call it stripCtrlM, make it 
executable (chmod +x stripCtrlM) and run it:

stripCtrlM filesWithCtrlM.R

or stripCtrlM *.R to convert all .R files in a directory

Hope this helps,

Costas




>
>
>> Thanks in advance
>> Kevin
>> __________________________________________________
>> 	[[alternative HTML version deleted]]
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 
Constantinos Antoniou, Ph.D.
Massachusetts Institute of Technology
Intelligent Transportation Systems Program
77 Massachusetts Ave., Rm. 1-249, Cambridge, MA 02139



From ripley at stats.ox.ac.uk  Wed May  4 11:06:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 May 2005 10:06:49 +0100 (BST)
Subject: [R] loading gap package
In-Reply-To: <fccc5dce2c62ef4b2304106b4426d0a5@mit.edu>
References: <646e05400505040125b315f1b@mail.gmail.com>
	<4278893F.4030007@statistik.uni-dortmund.de>
	<fccc5dce2c62ef4b2304106b4426d0a5@mit.edu>
Message-ID: <Pine.LNX.4.61.0505041002420.3752@gannet.stats>

On Wed, 4 May 2005, Constantinos Antoniou wrote:

>
> On 4 ??? 2005, at 11:35 ??, Uwe Ligges wrote:
>
>> Eduwin Pakpahan wrote:
>> 
>>> Dear R users, I did install R. 2.0.1, and try to load "gap" package. 
>>> However, below is the message shown when I did load it. Can anybody
>>> please let me know my mistakes?
>>> ----
>>>> library()
>>>> library(gap)
>>> Loading required package: MASS Loading required package: genetics Loading 
>>> required package: combinat Loading required package: gdata Error: package 
>>> 'gdata' could not be loaded
>>> In addition: Warning message: There is no package called 'gdata' in: 
>>> library(pkg, character.only =
>>> TRUE, logical = TRUE, lib.loc = lib.loc)
>>> ----
>> 
>> 
>> Let me read the error message for you:
>> "There is no package 'gdata'" simply means the package is not there...
>> So what about installing it?
>> 
>> Hint: it is part of the package bundle "gregmisc".
>> 
>
> I have a similar problem with R 2.1.0 on Mac OSX 10.3.9.
>
>> library(gregmisc)
> Loading required package: gdata
> Error: package 'gdata' could not be loaded
> In addition: Warning message:
> there is no package called 'gdata' in: library(pkg, character.only = TRUE, 
> logical = TRUE, lib.loc = lib.loc)
>
> At first I tried getting this gdata from somewhere else, but I could not find 
> it (evidence indicated that it is part of the gregmisc, as I suspected 
> originally... You email confirms this...)
>
> (For the record, my goal is to get read.xls, which is part (?) of gdata)

The current version of gregmisc is 2.0.6.  That does contain gdata.  It 
looks to me as if the MacOS X binary is broken (2kb) so you will need to 
install from the sources.

I think we have been here before: it seems the MacOS X binary packaging 
does not work for bundles.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ligges at statistik.uni-dortmund.de  Wed May  4 11:09:08 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 04 May 2005 11:09:08 +0200
Subject: [R] loading gap package
In-Reply-To: <fccc5dce2c62ef4b2304106b4426d0a5@mit.edu>
References: <646e05400505040125b315f1b@mail.gmail.com>	<4278893F.4030007@statistik.uni-dortmund.de>
	<fccc5dce2c62ef4b2304106b4426d0a5@mit.edu>
Message-ID: <42789134.8010206@statistik.uni-dortmund.de>

Constantinos Antoniou wrote:

> 
> On 4 ?????? 2005, at 11:35 ????, Uwe Ligges wrote:
> 
>> Eduwin Pakpahan wrote:
>>
>>> Dear R users, I did install R. 2.0.1, and try to load "gap" package. 
>>> However, below is the message shown when I did load it. Can anybody
>>> please let me know my mistakes?
>>> ----
>>>
>>>> library()
>>>> library(gap)
>>>
>>> Loading required package: MASS Loading required package: genetics 
>>> Loading required package: combinat Loading required package: gdata 
>>> Error: package 'gdata' could not be loaded
>>> In addition: Warning message: There is no package called 'gdata' in: 
>>> library(pkg, character.only =
>>> TRUE, logical = TRUE, lib.loc = lib.loc)
>>> ----
>>
>>
>>
>> Let me read the error message for you:
>> "There is no package 'gdata'" simply means the package is not there...
>> So what about installing it?
>>
>> Hint: it is part of the package bundle "gregmisc".
>>
> 
> I have a similar problem with R 2.1.0 on Mac OSX 10.3.9.
> 
>  > library(gregmisc)
> Loading required package: gdata
> Error: package 'gdata' could not be loaded
> In addition: Warning message:
> there is no package called 'gdata' in: library(pkg, character.only = 
> TRUE, logical = TRUE, lib.loc = lib.loc)
> 
> At first I tried getting this gdata from somewhere else, but I could not 
> find it (evidence indicated that it is part of the gregmisc, as I 
> suspected originally... You email confirms this...)
> 
> (For the record, my goal is to get read.xls, which is part (?) of gdata)


Please check whether your gregmisc installation is up to date (version 
2.0.6).
Please reinstall and try again. Works perfectly for me.

Uwe Ligges


> Thanks,
> 
> Costas
> 
> 
> 
> 
>> Uwe Ligges
>>
>>
>>> Thanks,
>>> Edwin
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
> 
>



From stefan.albrecht at allianz.com  Wed May  4 11:31:13 2005
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Wed, 4 May 2005 11:31:13 +0200
Subject: [R] its package: inexplicable date-shifting 
Message-ID: <OF608628B3.E3A8583C-ONC1256FF7.00342160-C1256FF7.00344C36@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050504/81be5135/attachment.pl

From m.blizinski at wsisiz.edu.pl  Wed May  4 12:02:14 2005
From: m.blizinski at wsisiz.edu.pl (Maciej =?iso-8859-2?Q?Blizi=F1ski?=)
Date: Wed, 4 May 2005 12:02:14 +0200
Subject: [R] How to intepret a factor response model?
In-Reply-To: <Pine.LNX.4.61.0505040934470.725@gannet.stats>
References: <20050504072317.GA5958@oceanic.wsisiz.edu.pl>
	<Pine.LNX.4.61.0505040934470.725@gannet.stats>
Message-ID: <20050504100214.GB16138@oceanic.wsisiz.edu.pl>

Thanks a lot for the answers (prof Ripley and Ted).

I'm trying to analyze a survey. Most of the variables are of factor
type, with values for example {"no_at_all", "a_little", "mostly",
"a_lot"}.

I thought about mapping those answers to numbers, but I didn't know what
numbers should I assign them to: {1, 2, 3, 4} (linear) or maybe
{1, 2, 4, 8} (exponential)? So I rather tried to analyze the original
factor survey data.

Multinomial factor response wasn't covered in the lectures in my school
so I'm trying to use my intuition and trial/error technique (please
forgive me :-) ).

Prof Brian Ripley wrote:
> I do wonder if that is what you intended, though.

I'd like to find possible correlations between factors in my survey. The
survey is about allergies and I'd like to find out if there is
correlation between the degree of allergic problems and the breast milk
(and artificial milk) feeding of the person as a child.

> You have fitted a model of 'two or three' vs 'one'.  You may have
> intended a multinomial logistic model: again MASS4 has details of such
> models.

I'll go on reading, the "fullrefman.pdf" file.

Regards,
Maciej Blizinski
Danmarks Tekniske Universitet



From Beatrijs.Moerkerke at UGent.be  Wed May  4 12:03:59 2005
From: Beatrijs.Moerkerke at UGent.be (Beatrijs Moerkerke)
Date: Wed, 04 May 2005 12:03:59 +0200
Subject: [R] lme versus proc mixed in SAS
Message-ID: <42789E0F.2070204@UGent.be>

Dear all,

I am trying to simulate the null distribution for the likelihood ratio 
test statistic for testing 1 random effect versus no random effect.  The 
asymptotic null distribution should be a mixture of a chi-squared 
distribution with 0 degrees of freedom and a chi-squared distribution 
with 1 degree of freedom.  This means that I expect a point mass of 50% 
on 0 for the likelihood ratio test statistic.
However, when I generate data using no random effects and when I 
calculate the test statistics for these data, I never obtain exactly 
zero.  I think this might be due to rounding errors but in fact, 70% of 
the calculated test statistics are negative.  I have compared a few of 
these results with the results in proc MIXED and I found that SAS does 
give test statistics that are exactly zero and gives no negative results.

The code I use for calculating the likelihood ratio test statistics is 
as follows:

a1<-summary(lme(y~x,random=~1|gr,method="ML"))$logLik
a2<-logLik(lm(y~x))
(-2*(a2-a1))

I don't know how I can simulate the null distribution in R using lme.

Thanks for your help,

Kind regards,
Beatrijs Moerkerke

-- 
Beatrijs Moerkerke
Department of Applied Mathematics and Computer Science
Ghent University
Krijgslaan 281 - S9
B-9000 GENT
Tel: +32-(0)9-264.47.56      Fax: +32-(0)9-264.49.95
E-mail: Beatrijs.Moerkerke at UGent.be



From costas at mit.edu  Wed May  4 12:35:36 2005
From: costas at mit.edu (Constantinos Antoniou)
Date: Wed, 4 May 2005 13:35:36 +0300
Subject: [R] loading gap package
In-Reply-To: <Pine.LNX.4.61.0505041002420.3752@gannet.stats>
References: <646e05400505040125b315f1b@mail.gmail.com>
	<4278893F.4030007@statistik.uni-dortmund.de>
	<fccc5dce2c62ef4b2304106b4426d0a5@mit.edu>
	<Pine.LNX.4.61.0505041002420.3752@gannet.stats>
Message-ID: <ed423c169b32d2cac693c216eeb7d4d0@mit.edu>


On 4 ÅŒúÅŒÅ±Åœä 2005, at 12:06 ÅŒÅºÅŒÅº, Prof Brian Ripley wrote:

> On Wed, 4 May 2005, Constantinos Antoniou wrote:
>
>>
>> On 4 Å≈ìÅ¬Å±Å√ä 2005, at 11:35 Å√ÄÅ¬Åº, Uwe Ligges wrote:
>>
>>> Eduwin Pakpahan wrote:
>>>> Dear R users, I did install R. 2.0.1, and try to load "gap" 
>>>> package. However, below is the message shown when I did load it. 
>>>> Can anybody
>>>> please let me know my mistakes?
>>>> ----
>>>>> library()
>>>>> library(gap)
>>>> Loading required package: MASS Loading required package: genetics 
>>>> Loading required package: combinat Loading required package: gdata 
>>>> Error: package 'gdata' could not be loaded
>>>> In addition: Warning message: There is no package called 'gdata' 
>>>> in: library(pkg, character.only =
>>>> TRUE, logical = TRUE, lib.loc = lib.loc)
>>>> ----
>>> Let me read the error message for you:
>>> "There is no package 'gdata'" simply means the package is not 
>>> there...
>>> So what about installing it?
>>> Hint: it is part of the package bundle "gregmisc".
>>
>> I have a similar problem with R 2.1.0 on Mac OSX 10.3.9.
>>
>>> library(gregmisc)
>> Loading required package: gdata
>> Error: package 'gdata' could not be loaded
>> In addition: Warning message:
>> there is no package called 'gdata' in: library(pkg, character.only = 
>> TRUE, logical = TRUE, lib.loc = lib.loc)
>>
>> At first I tried getting this gdata from somewhere else, but I could 
>> not find it (evidence indicated that it is part of the gregmisc, as I 
>> suspected originally... You email confirms this...)
>>
>> (For the record, my goal is to get read.xls, which is part (?) of 
>> gdata)
>
> The current version of gregmisc is 2.0.6.  That does contain gdata.  
> It looks to me as if the MacOS X binary is broken (2kb) so you will 
> need to install from the sources.
>

Thank you. I had the current  version (binary). Reinstalling from 
source fixed the issue....

Costas

> I think we have been here before: it seems the MacOS X binary 
> packaging does not work for bundles.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From schuurmans at geog.uu.nl  Wed May  4 12:43:51 2005
From: schuurmans at geog.uu.nl (schuurmans@geog.uu.nl)
Date: Wed, 4 May 2005 12:43:51 +0200 (CEST)
Subject: [R] segmentation fault using hdf5load() under Unix
Message-ID: <3404.134.221.105.154.1115203431.squirrel@134.221.105.154>

Dear R-users,

I'm experiencing a segmentation fault when using
hdf5load(file,load=FALSE). Library(hdf5) loads without problems but when
loading a file, R crashes. I compiled R under Unix (Solaris for Sun).
There is nothing wrong with the files, as I can run the same script at
another place where R runs under Linux.

Is it possible it has something to do with the hdf5 libraries where
package(hdf5) refers to?

Regards,

Hanneke


-- 
ir. J.M. (Hanneke) Schuurmans
PhD student hydrology
Faculty of Geosciences
Department of Physical Geography
Heidelberglaan 2
Postbus 80115
3508 TC  Utrecht
The Netherlands
T +31 (0)30 2532988
F +31 (0)30 2531145
E h.schuurmans at geog.uu.nl



From lyang at hkusua.hku.hk  Wed May  4 12:50:32 2005
From: lyang at hkusua.hku.hk (lin yang)
Date: Wed, 4 May 2005 18:50:32 +0800
Subject: [R] question about spectral analysis
Message-ID: <000601c55097$1befa0a0$c94d0893@yanglin>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050504/3f52ae25/attachment.pl

From stat_ramesh at rediffmail.com  Wed May  4 13:13:34 2005
From: stat_ramesh at rediffmail.com (Ramesh Kolluru)
Date: 4 May 2005 11:13:34 -0000
Subject: [R] Imputation
Message-ID: <20050504111334.9089.qmail@webmail46.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050504/b7d73b44/attachment.pl

From yukangtu at hotmail.com  Wed May  4 13:14:10 2005
From: yukangtu at hotmail.com (Tu Yu-Kang)
Date: Wed, 04 May 2005 11:14:10 +0000
Subject: [R] selections of data by one variable
Message-ID: <BAY12-F396743097AC183B6ABD31C1190@phx.gbl>

Dear R experts,

My problem is as follows:

Suppose I have a data frame d comprising two variable a<-c(1:10) & 
b<-c(11:20).

I now want to select a subgroup according the values of b.

I know if I just want to select, say, b=17, I can use f<-d[d$b==17] and R 
will give me 

> f
  a  b
7 7 17

However, if now I want to select a subgroup according to b==e<-c(13,15,17), 
then the same syntx doesn't work.

What is the correct way to do it?  My data have more than one million 
subjects, and I want to select part of them according to their id numbers.

Your help will be highly appreciated.

Best regards,

Yu-Kang



From stat_ramesh at rediffmail.com  Wed May  4 13:19:26 2005
From: stat_ramesh at rediffmail.com (Ramesh Kolluru)
Date: 4 May 2005 11:19:26 -0000
Subject: [R] Imputation
Message-ID: <20050504111926.22108.qmail@webmail29.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050504/082a2561/attachment.pl

From ozric at web.de  Wed May  4 13:40:18 2005
From: ozric at web.de (christian schulz)
Date: Wed, 04 May 2005 13:40:18 +0200
Subject: [R] Avoiding Sweave formula cut's?
Message-ID: <4278B4A2.80502@web.de>

Hi,

anybody know a possibilities how i could avoid less nicely  cut's  in 
function calls with sweave  like:

randomForest(x = RELSHIP ~ ., data = R_RELSHIP[splitR_RELSHIP ==      1, 
], importance = T, n(*here is the cut in the pdf*)
 tree = 1000, na.action = na.omit)

Until now i add expost linebreaks in the *.tex file, but with dozend of 
models it might be painfuel for my hands.

Many thanks & regards,
Christian



From Christoph.Scherber at uni-jena.de  Wed May  4 13:44:13 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Wed, 04 May 2005 13:44:13 +0200
Subject: [R] JGR and R 2.0.X
Message-ID: <4278B58D.4020609@uni-jena.de>

Dear all,

I am running Windows XP with several parallel installations of R (2.0.1; 
2.1 and so on). How can I install JGR for the 2.0.1 version? I keep on 
getting error messages when trying to install it.

Best wishes
Christoph



From flom at ndri.org  Wed May  4 14:08:34 2005
From: flom at ndri.org (Peter Flom)
Date: Wed, 04 May 2005 08:08:34 -0400
Subject: [R] How to intepret a factor response model?
Message-ID: <s2788309.064@MAIL.NDRI.ORG>

>>> Maciej Blizi(B??ski <m.blizinski at wsisiz.edu.pl> 5/4/2005 6:02:14 AM >>>
<<<
I'm trying to analyze a survey. Most of the variables are of factor
type, with values for example {"no_at_all", "a_little", "mostly",
"a_lot"}.
>>>

In that case, you probably want to look at ordinal logistic regression.  This is covered in numerous texts, one good one which uses R is Harrell's Regression Modeling Strategies (an excellent book in other regards, as well).

Another book which might be useful (although not R specific) isLong's Regression Models for Categorical and Limited Dependent Variables

<<<
I thought about mapping those answers to numbers, but I didn't know what numbers should I assign them to: {1, 2, 3, 4} (linear) or maybe
{1, 2, 4, 8} (exponential)? So I rather tried to analyze the original
factor survey data.

Multinomial factor response wasn't covered in the lectures in my school
so I'm trying to use my intuition and trial/error technique (please
forgive me :-) ).
>>>

Using your intuition and trial and error seems to me to be a way to guarantee lots of trials and lots of errors, but not necessarily to guarantee success.  You might want to consult a statistician before proceeding; you certainly want to consult a text.

HTH

Peter


Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From bela_b at gmx.net  Wed May  4 14:41:09 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Wed, 04 May 2005 14:41:09 +0200
Subject: [R] Huynh-Feldt R vs SAS Bug
Message-ID: <4278C2E5.3020802@gmx.net>

Hi,

I'm using anova.mlm sphericity tests/corrections, and I'm getting 
different values than SAS. In order to be able to use these values for 
publications, I'd need to know more about the SAS bug that is mentioned 
in the Reference Manual.
- What exactly causes the different values?
- Is it just a slight difference, or can I expect significant 
differences in H-F/G-G epsilons and corrected p-Values? With the data 
sets I'm using, the SAS value for H-F epsilon is almost twice the value 
from R, and I'm wondering if there's a mistake on my side or if it is 
just caused by the SAS bug.

Thanks for any hints...

Bela



From baron at psych.upenn.edu  Wed May  4 14:48:35 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 4 May 2005 08:48:35 -0400
Subject: [R] Imputation
In-Reply-To: <20050504111334.9089.qmail@webmail46.rediffmail.com>
References: <20050504111334.9089.qmail@webmail46.rediffmail.com>
Message-ID: <20050504124835.GA19277@psych>

On 05/04/05 11:13, Ramesh Kolluru wrote:
  Å†
 I have timeseries data for some factors, and some missing values are there in those
 factors, I want impute those missing values without disturbing the distribution of
 that factor, and maintaining the correlation with other factors. Pl. suggest me some
 imputation methods.
 I tried some functions in R like aregImpute, transcan. After the imputation I am
 unable to retrive the data with imputed values. Please give me some way to get the
 data with imputed values.

Here is one way to do it with transcan(), but I'm looking forward
to seeing other answers.  The data are in s.m, and the missing
values are NA.  The imputed values are in s.imp$imputed, in
order, and the third line simply replaces the NAs with these
values.  (I posted this before.  You might have found it by
searching the R search page below.)  This is for the simplest
possible sort of imputation.  I'm not sure that it meets your
requirements.  (In fact, I'm pretty sure it doesn't.)  So you'd
have to change the options for transcan, or do something else.

s.imp <- transcan(s.m,asis="*",data=s.m,imputed=T,long=T,pl=F)
s.na <- is.na(s.m) # which data are imputed
s.m[which(s.na)] <- unlist(s.imp$imputed)

As for aregImpute(), that has to be more difficult, because
aregImpute() does multiple imputation.  Very roughly, it produces
an whole set of imputed values, for the purpose of statistical
inference.  I don't know how to get a single best estimate out of
this set, or even whether this is a good idea.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From MSchwartz at MedAnalytics.com  Wed May  4 14:56:01 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 04 May 2005 07:56:01 -0500
Subject: [R] selections of data by one variable
In-Reply-To: <BAY12-F396743097AC183B6ABD31C1190@phx.gbl>
References: <BAY12-F396743097AC183B6ABD31C1190@phx.gbl>
Message-ID: <1115211361.24130.34.camel@horizons.localdomain>

On Wed, 2005-05-04 at 11:14 +0000, Tu Yu-Kang wrote:
> Dear R experts,
> 
> My problem is as follows:
> 
> Suppose I have a data frame d comprising two variable a<-c(1:10) & 
> b<-c(11:20).
> 
> I now want to select a subgroup according the values of b.
> 
> I know if I just want to select, say, b=17, I can use f<-d[d$b==17] and R 
> will give me 
> 
> > f
>   a  b
> 7 7 17
> 
> However, if now I want to select a subgroup according to b==e<-c(13,15,17), 
> then the same syntx doesn't work.
> 
> What is the correct way to do it?  My data have more than one million 
> subjects, and I want to select part of them according to their id numbers.
> 
> Your help will be highly appreciated.
> 
> Best regards,
> 
> Yu-Kang

You would want to use something like the following:

> df <- data.frame(a = 1:10, b = 11:20)

> df
    a  b
1   1 11
2   2 12
3   3 13
4   4 14
5   5 15
6   6 16
7   7 17
8   8 18
9   9 19
10 10 20

> df[df$b %in% c(13, 15, 17), ]
  a  b
3 3 13
5 5 15
7 7 17


See ?"%in%" for more information.

Also, see ?subset for more flexibility in using complex boolean
expressions for subsetting.

HTH,

Marc Schwartz



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed May  4 15:00:32 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 4 May 2005 15:00:32 +0200
Subject: [R] selections of data by one variable
References: <BAY12-F396743097AC183B6ABD31C1190@phx.gbl>
Message-ID: <001e01c550a9$416b66e0$0540210a@www.domain>

try this:

d <- data.frame(a=1:10, b=11:20)
e <- c(13, 15, 17)
##############
d. <- subset(d, b %in% e)
d.


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Tu Yu-Kang" <yukangtu at hotmail.com>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, May 04, 2005 1:14 PM
Subject: [R] selections of data by one variable


> Dear R experts,
>
> My problem is as follows:
>
> Suppose I have a data frame d comprising two variable a<-c(1:10) & 
> b<-c(11:20).
>
> I now want to select a subgroup according the values of b.
>
> I know if I just want to select, say, b=17, I can use f<-d[d$b==17] 
> and R will give me
>> f
>  a  b
> 7 7 17
>
> However, if now I want to select a subgroup according to 
> b==e<-c(13,15,17), then the same syntx doesn't work.
>
> What is the correct way to do it?  My data have more than one 
> million subjects, and I want to select part of them according to 
> their id numbers.
>
> Your help will be highly appreciated.
>
> Best regards,
>
> Yu-Kang
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed May  4 15:04:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 May 2005 14:04:53 +0100 (BST)
Subject: [R] segmentation fault using hdf5load() under Unix
In-Reply-To: <3404.134.221.105.154.1115203431.squirrel@134.221.105.154>
References: <3404.134.221.105.154.1115203431.squirrel@134.221.105.154>
Message-ID: <Pine.LNX.4.61.0505041400590.6101@gannet.stats>

Please check the hdf5 package version numbers are the current one.

Does the file contain text strings?  If so there has been a known problem 
with hdf5 crashes and a recent update of hdf5 (which I do not know for 
sure whether it fixes the problem or not).  This manifested itself as 
crashes on some machines and not others.

On Wed, 4 May 2005 schuurmans at geog.uu.nl wrote:

> I'm experiencing a segmentation fault when using
> hdf5load(file,load=FALSE). Library(hdf5) loads without problems but when
> loading a file, R crashes. I compiled R under Unix (Solaris for Sun).
> There is nothing wrong with the files, as I can run the same script at
> another place where R runs under Linux.
>
> Is it possible it has something to do with the hdf5 libraries where
> package(hdf5) refers to?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at pdf.com  Wed May  4 15:16:10 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 04 May 2005 06:16:10 -0700
Subject: [R] JGR and R 2.0.X
In-Reply-To: <4278B58D.4020609@uni-jena.de>
References: <4278B58D.4020609@uni-jena.de>
Message-ID: <4278CB1A.8000004@pdf.com>



Christoph Scherber wrote on 5/4/2005 4:44 AM:
> Dear all,
> 
> I am running Windows XP with several parallel installations of R (2.0.1; 
> 2.1 and so on). How can I install JGR for the 2.0.1 version? I keep on 
> getting error messages when trying to install it.
> 
> Best wishes
> Christoph
> 

Hi Christoph,

Which version of R comes first in your PATH?

--sundar



From ligges at statistik.uni-dortmund.de  Wed May  4 15:18:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 04 May 2005 15:18:59 +0200
Subject: [R] JGR and R 2.0.X
In-Reply-To: <4278B58D.4020609@uni-jena.de>
References: <4278B58D.4020609@uni-jena.de>
Message-ID: <4278CBC3.2010803@statistik.uni-dortmund.de>

Christoph Scherber wrote:

> Dear all,
> 
> I am running Windows XP with several parallel installations of R (2.0.1; 
> 2.1 and so on). How can I install JGR for the 2.0.1 version? I keep on 
> getting error messages when trying to install it.


I am sure Simon Urbanek et al. are happy to help you, but even the 
winners of the John Chambers Statistical Software Award are unable to 
help if you don't say what the error messages are and which versions of 
JGR you tried...

Uwe Ligges


> Best wishes
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed May  4 15:27:58 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 04 May 2005 15:27:58 +0200
Subject: [R] selections of data by one variable
In-Reply-To: <BAY12-F396743097AC183B6ABD31C1190@phx.gbl>
References: <BAY12-F396743097AC183B6ABD31C1190@phx.gbl>
Message-ID: <4278CDDE.2010702@statistik.uni-dortmund.de>

Tu Yu-Kang wrote:

> Dear R experts,
> 
> My problem is as follows:
> 
> Suppose I have a data frame d comprising two variable a<-c(1:10) & 
> b<-c(11:20).
> 
> I now want to select a subgroup according the values of b.
> 
> I know if I just want to select, say, b=17, I can use f<-d[d$b==17] and 
> R will give me
> 
>> f
> 
>  a  b
> 7 7 17
> 
> However, if now I want to select a subgroup according to 
> b==e<-c(13,15,17), then the same syntx doesn't work.


Which language is this???

To summarize, all the code you specified is:

a<-c(1:10) & b<-c(11:20)
f<-d[d$b==17]
b==e<-c(13,15,17)

In R, each line for itself is syntacically completely incorrect (even if
you say something would work, which is definitely not the case)!
"PLEASE do read the posting guide!"

I guess you want something like

 d <- data.frame(a = 1:10, b = 11:20)
 subset(d, b == 17)
 e <- c(13, 15, 17)
 subset(d, b %in% e)

Uwe Ligges



> What is the correct way to do it?  My data have more than one million 
> subjects, and I want to select part of them according to their id numbers.
> 
> Your help will be highly appreciated.
> 
> Best regards,
> 
> Yu-Kang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed May  4 15:42:52 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 4 May 2005 15:42:52 +0200
Subject: [R] lme versus proc mixed in SAS
References: <42789E0F.2070204@UGent.be>
Message-ID: <009101c550af$2b120fb0$0540210a@www.domain>

check this:

library(nlme)
B <- 1000
N <- 100
n <- 5
x <- rep(runif(N, -4, 4), each=n)
gr <- rep(1:N, each=n)
####################
T <- numeric(B)
for(i in 1:B){
    y <- rnorm(N*n, 1 + 1.5*x)
    L0 <- lm(y~x)
    L1 <- lme(y~x, random=~1|gr, method="ML")
    T[i] <- anova(L1, L0)$L.Ratio[2]
}
hist(T, prob=TRUE, breaks=100)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Beatrijs Moerkerke" <Beatrijs.Moerkerke at UGent.be>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, May 04, 2005 12:03 PM
Subject: [R] lme versus proc mixed in SAS


> Dear all,
>
> I am trying to simulate the null distribution for the likelihood 
> ratio test statistic for testing 1 random effect versus no random 
> effect.  The asymptotic null distribution should be a mixture of a 
> chi-squared distribution with 0 degrees of freedom and a chi-squared 
> distribution with 1 degree of freedom.  This means that I expect a 
> point mass of 50% on 0 for the likelihood ratio test statistic.
> However, when I generate data using no random effects and when I 
> calculate the test statistics for these data, I never obtain exactly 
> zero.  I think this might be due to rounding errors but in fact, 70% 
> of the calculated test statistics are negative.  I have compared a 
> few of these results with the results in proc MIXED and I found that 
> SAS does give test statistics that are exactly zero and gives no 
> negative results.
>
> The code I use for calculating the likelihood ratio test statistics 
> is as follows:
>
> a1<-summary(lme(y~x,random=~1|gr,method="ML"))$logLik
> a2<-logLik(lm(y~x))
> (-2*(a2-a1))
>
> I don't know how I can simulate the null distribution in R using 
> lme.
>
> Thanks for your help,
>
> Kind regards,
> Beatrijs Moerkerke
>
> -- 
> Beatrijs Moerkerke
> Department of Applied Mathematics and Computer Science
> Ghent University
> Krijgslaan 281 - S9
> B-9000 GENT
> Tel: +32-(0)9-264.47.56      Fax: +32-(0)9-264.49.95
> E-mail: Beatrijs.Moerkerke at UGent.be
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From drcarbon at gmail.com  Wed May  4 15:54:50 2005
From: drcarbon at gmail.com (Dr Carbon)
Date: Wed, 4 May 2005 09:54:50 -0400
Subject: [R] Difference between "tree" and "rpart"
Message-ID: <e89bb7ac050504065444db7a5d@mail.gmail.com>

In the help for rpart it says, "This differs from the tree function
mainly in its handling of surrogate variables." And it says that an
rpart object is a superset of a tree object. Both cite Brieman et al.
1984. Both call external code which looks like martian poetry to me.

I've seen posts in the archives where BDR, and other knowledgeable
folks, have said that rpart() is to be prefered over tree()

Is there a simple reason why? They use the same fundamental algorithm.
Are there differences in processing time? Bells and whistles?

TIA, DRC



From sabrina.carpentier at curie.fr  Wed May  4 16:00:08 2005
From: sabrina.carpentier at curie.fr (Sabrina Carpentier)
Date: Wed, 4 May 2005 16:00:08 +0200
Subject: [R] error with the function GOHyperG from GOstats package
Message-ID: <028e01c550b1$94fb0740$020a140a@Gaspesie>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050504/5c85160a/attachment.pl

From kkiely at insightful.com  Wed May  4 16:20:21 2005
From: kkiely at insightful.com (Kathy Kiely)
Date: Wed, 4 May 2005 15:20:21 +0100
Subject: [R] S-PLUS Essentials Training 23rd-26th May
Message-ID: <B796B8C05975394DA24E457D1985BDB4641A7A@uk2kexch01.insightful.com>

 

S-PLUS Essentials -  Lee Perry 23-26th  May 2005 
 
This is the last chance to register for the course S-PLUS Essentials to be
held in Basingstoke on 23-26th May 2005.  If you are unfamiliar with S-PLUS
or have been meaning to upgrade your S-PLUS skills, this is the course for
you.
 

Extract:
 
S-PLUS Essentials is a hands-on training course that covers a broad spectrum
of topics from importing data to fitting statistical models.  At the
conclusion of the course the student will have a working knowledge of the
syntax of the S-PLUS language.  Both traditional and trellis graphs will be
explored.  In addition to function writing in the context of graphing and
statistics, the course will present tools for organising your S-PLUS work.
While the material is presented here in the Windows environment, references
to Unix are made where appropriate.
 
Feedback from delegates at previous S-PLUS Essentials courses:
 
"Had some exposure to S-PLUS before - definitely opened my eyes to the
capabilities" - Citigroup
 
"Statisticians and graphics experts will appreciate the course" -National
Audit Office
 
"This was definitely a worthwhile investment of time" - Serono International
 
"Fantastic!" - Amgen

Refreshments, Lunch and Laptops are provided for the duration of the course.
 

Reserve your seat now!

Full Course Description:     Course Information 
 
 
 
Course:       S-PLUS Essentials
 
Presenter:    Lee Perry
 
Dates:        May 23 - 26th May 2005
 
Times:        09:30 - 17:00, Monday - Thursday
 
Location:     UK Office, Network House, Basing View, Basingstoke, 
              Hampshire RG21 4HG
 
 
 
Price:   Commercial Delegates:  ??1,200 GBP + VAT (??1,410 GBP)
 
         Academic Delegates:    ??800 GBP + VAT (??940 GBP)
 
To Register:
Web:             http://www.insightful.com/services/intl_training.asp
 
Email:           kkiely at insightful.com
 
 
 
Kathy Kiely
Sales & Marketing Administrator
Insightful Limited
Network House, Basing View Basingstoke, Hampshire, RG21 4HG
Tel : 01256 339822
Fax : 01256 339839
e mail : kkiely at insightful.com


If you do not want to receive information from Insightful then please contact
Kathy Kiely.



From przorg at yahoo.com  Wed May  4 16:23:37 2005
From: przorg at yahoo.com (Peter Zorg)
Date: Wed, 4 May 2005 07:23:37 -0700 (PDT)
Subject: [R] Difference between difference of means
Message-ID: <20050504142337.90100.qmail@web31311.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050504/2140eee4/attachment.pl

From jmacdon at med.umich.edu  Wed May  4 16:35:24 2005
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Wed, 04 May 2005 10:35:24 -0400
Subject: [R] error with the function GOHyperG from GOstats package
In-Reply-To: <028e01c550b1$94fb0740$020a140a@Gaspesie>
References: <028e01c550b1$94fb0740$020a140a@Gaspesie>
Message-ID: <4278DDAC.3070200@med.umich.edu>

Sabrina Carpentier wrote:
> I am running R 2.0.0, GOstats 1.1.1 and GO 1.7.0,

The release versions of BioC require R-2.0.1. You might try upgrading 
your R version and trying again, because I cannot replicate this error.

Note that if you upgrade to R-2.1.0 (the current version of R), you will 
need to use the development versions of BioC packages (use develOK=TRUE 
in your call to getBioC()). These devel versions of the BioC packages 
are not really that developmental right now, as we are fast approaching 
the release date for BioC 1.6.

Best,

Jim


> 
> and when I use the function GOHyperG, I have the following error:
> 
> w1<-as.list(hgu95av2LOCUSID)
> 
> w2<-unique(unlist(w1))
> 
> set.seed(123)
> myLL<-sample(w2,100)
> 
> xx <- GOHyperG(myLL)
> Error in mget(x, env = GOTERM, ifnotfound = NA) : 
> recursive default argument reference
> 
> In fact first I tried this function with my locusId ' list (with affymetrix hgu133plus2), but I had the same error...
> 
> Any help is appreciated 
> 
> Regards,
> Sabrina
> 
> 
> 
> Sabrina Carpentier
> Service Bioinformatique
> Institut Curie - Bat. Trouillet Rossignol (4e ??tage)
> 26 rue d'Ulm - 75248 Paris Cedex 5 - FRANCE
> Tel : +33 1 42 34 65 21 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623



From gregory.r.warnes at pfizer.com  Wed May  4 16:56:58 2005
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Wed, 4 May 2005 10:56:58 -0400 
Subject: [R] Unbundling gregmisc (was: loading gap package)
Message-ID: <915D2D65A9986440A277AC5C98AA466F01862B25@groamrexm02.amer.pfizer.com>

Let me redirect the topic a bit.  I've been considering unbundling gregmisc.
The pro would be that people would find the component packages (i.e. gdata)
more easily.  The con is that the packages have a number of
interdependencies, so you pretty much will need to get most of them anyway.

As the latest gregmisc bundle contains a gregmisc package that is just a
stub that depends on and loads the individual packages, there would still be
a gregmisc object.

Comments?

-Greg

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Constantinos
> Antoniou
> Sent: Wednesday, May 04, 2005 6:36 AM
> To: Prof Brian Ripley
> Cc: R-help at stat.math.ethz.ch; Uwe Ligges
> Subject: Re: [R] loading gap package
> 
> 
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
> =-=-=-=-=
> !!! IMPORTANT NOTICE !!!
> This email was addressed to you from the Internet using a 
> legacy E-Mail Domain address.
> 
> 
> This email domain will no longer be in service on the 
> Internet after 15 July 2005.
> 
> If this is legitimate business email, please inform the 
> sender to address all future correspondence to your 
> @pfizer.com address.
> 
> The original text of this email appears below this notice.
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
> =-=-=-=-=
> 
> 
> On 4 ÅŒúÅŒÅ±Åœä 2005, at 12:06 Å¬ÅµÅ¬Åµ, Prof Brian Ripley wrote:
> 
> > On Wed, 4 May 2005, Constantinos Antoniou wrote:
> >
> >>
> >> On 4 Å≈ìÅ¬Å±Å√ä 2005, at 11:35 Å√ÄÅ¬Åº, Uwe Ligges wrote:
> >>
> >>> Eduwin Pakpahan wrote:
> >>>> Dear R users, I did install R. 2.0.1, and try to load "gap"
> 
> >>>> package. However, below is the message shown when I did load it.
> 
> >>>> Can anybody
> >>>> please let me know my mistakes?
> >>>> ----
> >>>>> library()
> >>>>> library(gap)
> >>>> Loading required package: MASS Loading required package: genetics
> 
> >>>> Loading required package: combinat Loading required 
> package: gdata
> 
> >>>> Error: package 'gdata' could not be loaded
> >>>> In addition: Warning message: There is no package called 'gdata'
> 
> >>>> in: library(pkg, character.only =
> >>>> TRUE, logical = TRUE, lib.loc = lib.loc)
> >>>> ----
> >>> Let me read the error message for you:
> >>> "There is no package 'gdata'" simply means the package is not
> 
> >>> there...
> >>> So what about installing it?
> >>> Hint: it is part of the package bundle "gregmisc".
> >>
> >> I have a similar problem with R 2.1.0 on Mac OSX 10.3.9.
> >>
> >>> library(gregmisc)
> >> Loading required package: gdata
> >> Error: package 'gdata' could not be loaded
> >> In addition: Warning message:
> >> there is no package called 'gdata' in: library(pkg, 
> character.only =
> 
> >> TRUE, logical = TRUE, lib.loc = lib.loc)
> >>
> >> At first I tried getting this gdata from somewhere else, 
> but I could
> 
> >> not find it (evidence indicated that it is part of the 
> gregmisc, as I
> 
> >> suspected originally... You email confirms this...)
> >>
> >> (For the record, my goal is to get read.xls, which is part (?) of
> 
> >> gdata)
> >
> > The current version of gregmisc is 2.0.6.  That does contain gdata. 
> 
> > It looks to me as if the MacOS X binary is broken (2kb) so you will
> 
> > need to install from the sources.
> >
> 
> Thank you. I had the current  version (binary). Reinstalling from
> 
> source fixed the issue....
> 
> Costas
> 
> > I think we have been here before: it seems the MacOS X binary
> 
> > packaging does not work for bundles.
> >
> > --
> 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From christoph.lehmann at gmx.ch  Wed May  4 18:23:54 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Wed, 04 May 2005 18:23:54 +0200
Subject: [R] selections of data by one variable
In-Reply-To: <BAY12-F396743097AC183B6ABD31C1190@phx.gbl>
References: <BAY12-F396743097AC183B6ABD31C1190@phx.gbl>
Message-ID: <4278F71A.8050102@gmx.ch>

test <- data.frame(cbind(1:10,11:20))
names(test) <- c("a", "b")
test[test$b == 17,]
test[test$b %in% c(13, 15, 17),]


Tu Yu-Kang wrote:
> Dear R experts,
> 
> My problem is as follows:
> 
> Suppose I have a data frame d comprising two variable a<-c(1:10) & 
> b<-c(11:20).
> 
> I now want to select a subgroup according the values of b.
> 
> I know if I just want to select, say, b=17, I can use f<-d[d$b==17] and 
> R will give me
>> f
>  a  b
> 7 7 17
> 
> However, if now I want to select a subgroup according to 
> b==e<-c(13,15,17), then the same syntx doesn't work.
> 
> What is the correct way to do it?  My data have more than one million 
> subjects, and I want to select part of them according to their id numbers.
> 
> Your help will be highly appreciated.
> 
> Best regards,
> 
> Yu-Kang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From rchandler at forwild.umass.edu  Wed May  4 17:16:11 2005
From: rchandler at forwild.umass.edu (Richard Chandler)
Date: Wed,  4 May 2005 11:16:11 -0400
Subject: [R] stratified bootstrap with boot
Message-ID: <1115219771.4278e73be0dff@mail-www2.oit.umass.edu>

Hello,

I am new to R, and am having trouble getting the output I want from a
stratified bootstrap. I didn't recieve a reply the first time I posted
this question so I have tried to make it more clear. My data frame
(denboot) is set up as follows:

        SITE cswa parea treat
1      BeanA    3  1.20     m
2     BeanBC    3  1.05     m
3      BeanD    1  0.93     m
4     BlackB    1  1.01     m
5     Brooks    3  4.00     b
6      BullL    3  1.32     b
7      BullM    1  0.20     m
8      BullU    4  2.06     b
...

#Here is my code:

cswafun <- function(denboot, i) sum(cswa[i])/(sum(parea[i])
attach(denboot)
cswa.boot <- boot(denboot, cswafun, R = 10000, strata = treat)

#Here is the output

Call:
boot(data = denboot, statistic = cswafun, R = 10000, strata = treat)


Bootstrap Statistics :
    original     bias    std. error
t1* 1.109612 0.02641786   0.2032927

#Can anyone tell me why I'm not getting two estimates of
sum(cswa)/sum(parea), one for each #strata ('b' and 'm')? Any help
would be appreciated. Thanks

Richard

-- 
Richard Chandler, M.S. candidate
Department of Natural Resources Conservation
UMass Amherst
(413)545-1237



From murdoch at stats.uwo.ca  Wed May  4 17:20:42 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 May 2005 16:20:42 +0100
Subject: [R] Unbundling gregmisc (was: loading gap package)
In-Reply-To: <915D2D65A9986440A277AC5C98AA466F01862B25@groamrexm02.amer.pfizer.com>
References: <915D2D65A9986440A277AC5C98AA466F01862B25@groamrexm02.amer.pfizer.com>
Message-ID: <4278E84A.3090904@stats.uwo.ca>

Warnes, Gregory R wrote:
> Let me redirect the topic a bit.  I've been considering unbundling gregmisc.
> The pro would be that people would find the component packages (i.e. gdata)
> more easily.  The con is that the packages have a number of
> interdependencies, so you pretty much will need to get most of them anyway.
> 
> As the latest gregmisc bundle contains a gregmisc package that is just a
> stub that depends on and loads the individual packages, there would still be
> a gregmisc object.
> 
> Comments?

Currently R can follow dependencies when you install a package from CRAN 
(and this is the default in Windows, not sure about other platforms), so 
I'd say this would be a good thing to do.  I don't know your revision 
history on the components, but I'd guess some change more often than 
others, so there'll be no need to update all of them every time one of 
them changes.

Duncan Murdoch



From apjaworski at mmm.com  Wed May  4 17:29:16 2005
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Wed, 4 May 2005 10:29:16 -0500
Subject: [R] Unbundling gregmisc (was: loading gap package)
In-Reply-To: <915D2D65A9986440A277AC5C98AA466F01862B25@groamrexm02.amer.pfizer.com>
Message-ID: <OFA13F4788.B069B707-ON86256FF7.005446BE-86256FF7.005513D0@mmm.com>






I have one comment, although it is a little off topic.

I have the gregmisc package installed (R-2.0.1 on Win2000).  Whenever I use
the help.search function it comes back with the following warning:

Warning message:
no Rd contents for package 'gregmisc' in 'C:/local/R/rw210/library' in:
help.search("bundle")

Apparently, the help.search function looks for the CONTENTS file in the
.../library/gregmisc directory.  I created an empty one and the warning
went away.  I know it is purely cosmetic, but if the gregmisc bundle is
kept intact, and I think it should, I would suggest adding the empty
CONTENTS file to its distribution.

Perhaps there is a better way of handling this.  For example, VR is a
bundle but it does not create its VR directory in .../library.

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             "Warnes, Gregory                                              
             R"                                                            
             <gregory.r.warnes                                          To 
             @pfizer.com>              "'Constantinos Antoniou'"           
             Sent by:                  <costas at mit.edu>, Prof Brian Ripley 
             r-help-bounces at st         <ripley at stats.ox.ac.uk>             
             at.math.ethz.ch                                            cc 
                                       R-help at stat.math.ethz.ch, Uwe       
                                       Ligges                              
             05/04/2005 09:56          <ligges at statistik.uni-dortmund.de>  
             AM                                                    Subject 
                                       [R] Unbundling gregmisc (was:       
                                       loading gap package)                
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Let me redirect the topic a bit.  I've been considering unbundling
gregmisc.
The pro would be that people would find the component packages (i.e. gdata)
more easily.  The con is that the packages have a number of
interdependencies, so you pretty much will need to get most of them anyway.

As the latest gregmisc bundle contains a gregmisc package that is just a
stub that depends on and loads the individual packages, there would still
be
a gregmisc object.

Comments?

-Greg

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Constantinos
> Antoniou
> Sent: Wednesday, May 04, 2005 6:36 AM
> To: Prof Brian Ripley
> Cc: R-help at stat.math.ethz.ch; Uwe Ligges
> Subject: Re: [R] loading gap package
>
>
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
> =-=-=-=-=
> !!! IMPORTANT NOTICE !!!
> This email was addressed to you from the Internet using a
> legacy E-Mail Domain address.
>
>
> This email domain will no longer be in service on the
> Internet after 15 July 2005.
>
> If this is legitimate business email, please inform the
> sender to address all future correspondence to your
> @pfizer.com address.
>
> The original text of this email appears below this notice.
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
> =-=-=-=-=
>
>
> On 4 ÅŒúÅŒÅ±Åœä 2005, at 12:06 Å¬ÅµÅ¬Åµ, Prof Brian Ripley wrote:
>
> > On Wed, 4 May 2005, Constantinos Antoniou wrote:
> >
> >>
> >> On 4 Å≈ìÅ¬Å±Å√ä 2005, at 11:35 Å√ÄÅ¬Åº, Uwe Ligges wrote:
> >>
> >>> Eduwin Pakpahan wrote:
> >>>> Dear R users, I did install R. 2.0.1, and try to load "gap"
>
> >>>> package. However, below is the message shown when I did load it.
>
> >>>> Can anybody
> >>>> please let me know my mistakes?
> >>>> ----
> >>>>> library()
> >>>>> library(gap)
> >>>> Loading required package: MASS Loading required package: genetics
>
> >>>> Loading required package: combinat Loading required
> package: gdata
>
> >>>> Error: package 'gdata' could not be loaded
> >>>> In addition: Warning message: There is no package called 'gdata'
>
> >>>> in: library(pkg, character.only =
> >>>> TRUE, logical = TRUE, lib.loc = lib.loc)
> >>>> ----
> >>> Let me read the error message for you:
> >>> "There is no package 'gdata'" simply means the package is not
>
> >>> there...
> >>> So what about installing it?
> >>> Hint: it is part of the package bundle "gregmisc".
> >>
> >> I have a similar problem with R 2.1.0 on Mac OSX 10.3.9.
> >>
> >>> library(gregmisc)
> >> Loading required package: gdata
> >> Error: package 'gdata' could not be loaded
> >> In addition: Warning message:
> >> there is no package called 'gdata' in: library(pkg,
> character.only =
>
> >> TRUE, logical = TRUE, lib.loc = lib.loc)
> >>
> >> At first I tried getting this gdata from somewhere else,
> but I could
>
> >> not find it (evidence indicated that it is part of the
> gregmisc, as I
>
> >> suspected originally... You email confirms this...)
> >>
> >> (For the record, my goal is to get read.xls, which is part (?) of
>
> >> gdata)
> >
> > The current version of gregmisc is 2.0.6.  That does contain gdata.
>
> > It looks to me as if the MacOS X binary is broken (2kb) so you will
>
> > need to install from the sources.
> >
>
> Thank you. I had the current  version (binary). Reinstalling from
>
> source fixed the issue....
>
> Costas
>
> > I think we have been here before: it seems the MacOS X binary
>
> > packaging does not work for bundles.
> >
> > --
>
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>
>


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From slist at oomvanlieshout.net  Wed May  4 17:30:04 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Wed, 04 May 2005 17:30:04 +0200
Subject: [R] Plotting means and confidence intervals by group factor using
 lattice graphics?
Message-ID: <4278EA7C.9030301@oomvanlieshout.net>

Dear R graphics gurus,

Another question about lattice graphics. This time I would like to plot 
means and confidence intervals by group factor in a lattice graph. I can 
not find any working lattice examples. Maybe a custom panel function is 
the answer, but that is a bit beyond me for now.

The individual plots within the lattice graph could look like this:

# Example with confidence intervals and grid
hh <- t(VADeaths)[, 5:1]
mybarcol <- "gray20"
ci.l <- hh * 0.85
ci.u <- hh * 1.15
mp <- barplot2(hh, beside = TRUE,
         col = c("lightblue", "mistyrose",
                 "lightcyan", "lavender"),
         legend = colnames(VADeaths), ylim = c(0, 100),
         main = "Death Rates in Virginia", font.main = 4,
         sub = "Faked 95 percent error bars", col.sub = mybarcol,
         cex.names = 1.5, plot.ci = TRUE, ci.l = ci.l, ci.u = ci.u,
         plot.grid = TRUE)
mtext(side = 1, at = colMeans(mp), line = -2,
       text = paste("Mean", formatC(colMeans(hh))), col = "red")
box()

Or like this:

data(state)
plotmeans(state.area ~ state.region)

Both plotmeans and barplot2 give interesting options such as printing of 
nobs, among other things. In case of a barplot, there should be an 
option to plot the confidence intervals in one direction only (up) as to 
avoid interference with any black and white shading. The plotMeans 
function provides a useful option error.bars ("se", "sd", "conf.int", 
"none").

The following test data is still useful:

tmp <- expand.grid(geology = c("Sand","Clay","Silt","Rock"),
   species = 
c("ArisDiff","BracSera","CynDact","ElioMuti","EragCurS","EragPseu"),
   dist = seq(1,9,1) )
tmp$height <- rnorm(216)

For instance plotting height versus dist by geology.

Any help very welcome!

Cheers,

Sander.

PS Of course the resulting graph will go to the R graph gallery!

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From spencer.graves at pdf.com  Wed May  4 17:32:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 04 May 2005 08:32:08 -0700
Subject: [R] (no subject)
In-Reply-To: <20050504170040.2dab84c7@localhost.localdomain>
References: <IFXJ7M$83F019763D6380D79C4C80C4738E88E3@terra.com.br>	<5.2.0.9.0.20050504101832.00a9b738@pop.info.fundp.ac.be>
	<20050504170040.2dab84c7@localhost.localdomain>
Message-ID: <4278EAF8.2070309@pdf.com>

	  It came up for me.  However, that's just a speed comparison, which
said that R 1.9.0 was comparable with the best available, better than
most but not necessarily the best depending on the task.

	   I would think you might be more concerned with accuracy and ease of
use, and I suggest you Google and do an R site search, as I suggested.
This has been discussed repeately on this list, and a search should
expose many useful comments.

	  spencer graves

ronggui wrote:

> On Wed, 04 May 2005 10:22:01 +0200
> Cuvelier Etienne <ecu at info.fundp.ac.be> wrote:
> 
> 
>>At 16:52 3/05/2005 -0300, rene.raupp wrote:
>>
>>>Does anybory knows any work comparing R with other (charged) statistical softwares (like Minitab, SPSS, SAS)?
>>>I work in a brasilian government bureau and I intend to use R as our preferable statistical software, but I have to show it's as good as the others. I also intend to use Weka, and for this one I have the same problem.
>>>Can anyone help me?
>>
>>The results of a Benchmark test of various number crunching packages  (R 1.9.0, S-PLUS 6.1,  Matlab 6.0, O-Matrix 5.6 Ml mode, 
>>O-Matrix 5.6 native, Octave 2.1.42, Scilab 2.7, Ox 3.30) can be found at :
>>http://www.sciviews.org/benchmark/index.html
> 
> 
> i can NOT connect with the website .i do not why.anyone else come across this problem?
> 
> 
>  
> 
>>>Thanks
>>>RenÅ®Å¶ M. Raupp
>>>e-mail: rener at mpdft.gov.br
>>>       rene.raupp at terra.com.br
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>Cuvelier Etienne
>>Assistant
>>FUNDP - Institut d'Informatique
>>rue Grandgagnage, 21   B-5000 Namur (Belgique)
>>tel: 32.81.72.49.93    fax: 32.81.72.49.67
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sethfalcon at comcast.net  Wed May  4 17:33:06 2005
From: sethfalcon at comcast.net (Seth Falcon)
Date: Wed, 04 May 2005 08:33:06 -0700
Subject: [R] Unbundling gregmisc
References: <915D2D65A9986440A277AC5C98AA466F01862B25@groamrexm02.amer.pfizer.com>
Message-ID: <m2d5s7gg5p.fsf@macaroni.local>

"Warnes, Gregory R" <gregory.r.warnes at pfizer.com> writes:
> Let me redirect the topic a bit.  I've been considering unbundling
> gregmisc.  The pro would be that people would find the component
> packages (i.e. gdata) more easily.  The con is that the packages
> have a number of interdependencies, so you pretty much will need to
> get most of them anyway.

I'm +1, heck, +2.  

Given R's new ability to follow dependencies, I think the most likely
outcome is a friendlier user experience: it will be easier to find the
packages.  

As Duncan pointed out, another pro is that if some of the packages
change less, then users won't have to update them.


+ seth



From kkurikka at cc.helsinki.fi  Wed May  4 17:40:37 2005
From: kkurikka at cc.helsinki.fi (Kyosti H Kurikka)
Date: Wed, 4 May 2005 18:40:37 +0300 (EEST)
Subject: [R] Double hurdle model in R
Message-ID: <Pine.OSF.4.58.0505041807100.261715@sirppi.helsinki.fi>

I am interested in utilizing this so called "double hurdle" model
in my study. We can write the model in the following way:

if (z'a + u > 0 & x'b + e > 0) y = x'b + e, else y = 0

In the model, consumption y is the (left-) censored dependent variable. e
and u are the normally distributed error terms. z'a is the participation
equation and x'b is the expenditure equation. If we would remove the
participation equation from the model we would end up with a type I
tobit-model.

In the tobit-model the same set of paprameters and variables determine
both the discrete probability of non-zero outcome and the level of
positive expenditure. In the double-hurdle-model we have separate
parametrization of the participation and consumption decisions.

I can estimate tobit-model using function survreg(). But what about this
double hurdle thing? Has somebody written a R-function for estimating this
sort of a model?


Best regards,
Ky??sti Kurikka



From jfox at mcmaster.ca  Wed May  4 17:42:35 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 4 May 2005 11:42:35 -0400
Subject: [R] Difference between difference of means
In-Reply-To: <20050504142337.90100.qmail@web31311.mail.mud.yahoo.com>
Message-ID: <20050504154234.KRYT21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter,

This looks like a two-way ANOVA, with product as one factor and condition as
the other. The test that you describe is for the interaction between product
and condition.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Zorg
> Sent: Wednesday, May 04, 2005 9:24 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Difference between difference of means
> 
> Hi
> I would like to compare two differences of means.
> I have the means of the log of one variable for two 
> conditions for one product, and for another product.
> The differences look different between both products for the 
> two conditions, but a statistical test is required.
> Thank you for any hint
>  
> Peter
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From alxmilton at yahoo.it  Wed May  4 17:43:02 2005
From: alxmilton at yahoo.it (alessandro carletti)
Date: Wed, 4 May 2005 17:43:02 +0200 (CEST)
Subject: [R] selecting maximum values
Message-ID: <20050504154302.64242.qmail@web26606.mail.ukl.yahoo.com>

Sorry for disturbing you with another newbie question!
I have a data frame about coastal waters quality
parameters: for some parameters (e.g. NH3) I have only
1 observation for each sampling station and each
sampling date, while in other cases (chlorophyll) I
have 1 obs for each meter-depth for each station and
date. How can I select only the max chlorophyll value
for each station/date?

example

row  station         date        depth     chlorophyll
1     Castagneto      21/06/01     -0.5         2.0
2     Castagneto      21/06/01     -1.5         2.2
3     Castagneto      21/06/01     -2.5         2.4
4     Castagneto      21/06/01     -3.5         2.1
5     Ancona          23/06/01     -0.5         2.4
6     Ancona          23/06/01     -1.5         2.5
7     Ancona          23/06/01     -2.5         2.2
8     Ancona          23/06/01     -3.5         2.1
9     Ancona          23/06/01     -4.5         1.9
...

I'd like to select only row 3 and 6, the ones with max
chlorophyll values, or have the mean for the rows 1:4
and 5:9

Thanks



From mingan at unm.edu  Wed May  4 17:54:00 2005
From: mingan at unm.edu (mingan)
Date: Wed, 04 May 2005 09:54:00 -0600
Subject: [R] rank of a matrix
Message-ID: <4278F018.8060101@unm.edu>



how do I check the rank of a matrix ?

say

A=  1   0   0
     0   1   0

then rank(A)=2

what is this function?

thanks


  I did try help.search("rank"), but all the returned help information 
seem irrelevant to what I want.

  I would like to know how people search for help information like this.






rank(base)              Sample Ranks
SignRank(stats)         Distribution of the Wilcoxon Signed Rank
                        Statistic
Wilcoxon(stats)         Distribution of the Wilcoxon Rank Sum
                        Statistic
friedman.test(stats)    Friedman Rank Sum Test
kruskal.test(stats)     Kruskal-Wallis Rank Sum Test
pairwise.wilcox.test(stats)
                        Pairwise Wilcoxon rank sum tests
wilcox.test(stats)      Wilcoxon Rank Sum and Signed Rank Tests



From ripley at stats.ox.ac.uk  Wed May  4 18:04:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 May 2005 17:04:02 +0100 (BST)
Subject: [R] Difference between "tree" and "rpart"
In-Reply-To: <e89bb7ac050504065444db7a5d@mail.gmail.com>
References: <e89bb7ac050504065444db7a5d@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505041700170.7964@gannet.stats>

rpart does much more at C level, including pruning and cross-validation so 
can be much faster.

It is also user-extensible.

tree was actually written to track down bugs in the then S implementation, 
and so is much closer to the functionality in S.  It is not where I would 
have started from.  It is really only available for R to support MASS and 
PRNN (my books).

On Wed, 4 May 2005, Dr Carbon wrote:

> In the help for rpart it says, "This differs from the tree function
> mainly in its handling of surrogate variables." And it says that an
> rpart object is a superset of a tree object. Both cite Brieman et al.
> 1984. Both call external code which looks like martian poetry to me.
>
> I've seen posts in the archives where BDR, and other knowledgeable
> folks, have said that rpart() is to be prefered over tree()
>
> Is there a simple reason why? They use the same fundamental algorithm.
> Are there differences in processing time? Bells and whistles?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sethfalcon at comcast.net  Wed May  4 18:02:37 2005
From: sethfalcon at comcast.net (Seth Falcon)
Date: Wed, 04 May 2005 09:02:37 -0700
Subject: [R] Unbundling gregmisc
References: <915D2D65A9986440A277AC5C98AA466F01862B25@groamrexm02.amer.pfizer.com>
	<OFA13F4788.B069B707-ON86256FF7.005446BE-86256FF7.005513D0@mmm.com>
Message-ID: <m2br7rf082.fsf@macaroni.local>

apjaworski at mmm.com writes:

> I have one comment, although it is a little off topic.
>
> I have the gregmisc package installed (R-2.0.1 on Win2000).  Whenever I use
> the help.search function it comes back with the following warning:
>
> Warning message:
> no Rd contents for package 'gregmisc' in 'C:/local/R/rw210/library' in:
> help.search("bundle")
>
> Apparently, the help.search function looks for the CONTENTS file in the
> .../library/gregmisc directory.  I created an empty one and the warning
> went away.  I know it is purely cosmetic, but if the gregmisc bundle is
> kept intact, and I think it should, I would suggest adding the empty
> CONTENTS file to its distribution.
>
> Perhaps there is a better way of handling this.  For example, VR is a
> bundle but it does not create its VR directory in .../library.

IMO, this is another reason to move away from the bundle approach.  It
seems to add complexity and confusion in a number of places and,
personally, I don't think it is worth it (again, given auto dependency
following).

Best,

+ seth



From ripley at stats.ox.ac.uk  Wed May  4 18:06:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 May 2005 17:06:25 +0100 (BST)
Subject: [R] stratified bootstrap with boot
In-Reply-To: <1115219771.4278e73be0dff@mail-www2.oit.umass.edu>
References: <1115219771.4278e73be0dff@mail-www2.oit.umass.edu>
Message-ID: <Pine.LNX.4.61.0505041704550.7964@gannet.stats>

You asked for a stratified bootstrap, not a stratified estimator.  This is 
explained in the reference (Davison & Hinkley).

On Wed, 4 May 2005, Richard Chandler wrote:

> Hello,
>
> I am new to R, and am having trouble getting the output I want from a
> stratified bootstrap. I didn't recieve a reply the first time I posted
> this question so I have tried to make it more clear. My data frame
> (denboot) is set up as follows:
>
>        SITE cswa parea treat
> 1      BeanA    3  1.20     m
> 2     BeanBC    3  1.05     m
> 3      BeanD    1  0.93     m
> 4     BlackB    1  1.01     m
> 5     Brooks    3  4.00     b
> 6      BullL    3  1.32     b
> 7      BullM    1  0.20     m
> 8      BullU    4  2.06     b
> ...
>
> #Here is my code:
>
> cswafun <- function(denboot, i) sum(cswa[i])/(sum(parea[i])
> attach(denboot)
> cswa.boot <- boot(denboot, cswafun, R = 10000, strata = treat)
>
> #Here is the output
>
> Call:
> boot(data = denboot, statistic = cswafun, R = 10000, strata = treat)
>
>
> Bootstrap Statistics :
>    original     bias    std. error
> t1* 1.109612 0.02641786   0.2032927
>
> #Can anyone tell me why I'm not getting two estimates of
> sum(cswa)/sum(parea), one for each #strata ('b' and 'm')? Any help
> would be appreciated. Thanks

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pdebruic at gmail.com  Wed May  4 18:06:25 2005
From: pdebruic at gmail.com (Paul DeBruicker)
Date: Wed, 04 May 2005 11:06:25 -0500
Subject: [R] Step wise regression
In-Reply-To: <42777ABE.8090807@vanderbilt.edu>
References: <IFWZSV$980956B6152854A97BB17FC8EA634D80@uol.com.br>
	<42777ABE.8090807@vanderbilt.edu>
Message-ID: <1115222785.1613.47.camel@localhost>

Hi Walmir,

Here is the long version of Frank Harrell's answer.  

http://www.pitt.edu/~wpilib/statfaq/regrfaq.html


Paul


On Tue, 2005-05-03 at 08:21 -0500, Frank E Harrell Jr wrote:
> walmir-rodrigues wrote:
> > Dear Fellows,
> > 
> > How can I do to proced a step wise regression in R, if it??s possible ?
> > 
> > Thanks,
> > 
> > Walmir 
> 
> Here is an easy approach that will yield results only slightly less 
> valid than one actually using the response variable:
> 
> x <- data.frame(x1,x2,x3,x4,..., other potential predictors)
> x[,sample(ncol(x))]
> 
> :-)   -Frank
> 
>



From sdavis2 at mail.nih.gov  Wed May  4 18:12:06 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 4 May 2005 12:12:06 -0400
Subject: [R] selecting maximum values
In-Reply-To: <20050504154302.64242.qmail@web26606.mail.ukl.yahoo.com>
References: <20050504154302.64242.qmail@web26606.mail.ukl.yahoo.com>
Message-ID: <ddf4ed79b9104d6e3233f500e6621e6b@mail.nih.gov>

see ?aggregate.

Sean

On May 4, 2005, at 11:43 AM, alessandro carletti wrote:

> Sorry for disturbing you with another newbie question!
> I have a data frame about coastal waters quality
> parameters: for some parameters (e.g. NH3) I have only
> 1 observation for each sampling station and each
> sampling date, while in other cases (chlorophyll) I
> have 1 obs for each meter-depth for each station and
> date. How can I select only the max chlorophyll value
> for each station/date?
>
> example
>
> row  station         date        depth     chlorophyll
> 1     Castagneto      21/06/01     -0.5         2.0
> 2     Castagneto      21/06/01     -1.5         2.2
> 3     Castagneto      21/06/01     -2.5         2.4
> 4     Castagneto      21/06/01     -3.5         2.1
> 5     Ancona          23/06/01     -0.5         2.4
> 6     Ancona          23/06/01     -1.5         2.5
> 7     Ancona          23/06/01     -2.5         2.2
> 8     Ancona          23/06/01     -3.5         2.1
> 9     Ancona          23/06/01     -4.5         1.9
> ...
>
> I'd like to select only row 3 and 6, the ones with max
> chlorophyll values, or have the mean for the rows 1:4
> and 5:9
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Friedrich.Leisch at tuwien.ac.at  Wed May  4 18:14:41 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Wed, 4 May 2005 18:14:41 +0200
Subject: [R] Avoiding Sweave formula cut's?
In-Reply-To: <4278B4A2.80502@web.de>
References: <4278B4A2.80502@web.de>
Message-ID: <17016.62705.818528.614354@celebrian.ci.tuwien.ac.at>

>>>>> On Wed, 04 May 2005 13:40:18 +0200,
>>>>> christian schulz (cs) wrote:

  > Hi,
  > anybody know a possibilities how i could avoid less nicely  cut's  in 
  > function calls with sweave  like:

  > randomForest(x = RELSHIP ~ ., data = R_RELSHIP[splitR_RELSHIP ==      1, 
  > ], importance = T, n(*here is the cut in the pdf*)
  >  tree = 1000, na.action = na.omit)

  > Until now i add expost linebreaks in the *.tex file, but with dozend of 
  > models it might be painfuel for my hands.

Currently there is no way of doing this, because the code you see is
the result of a parse() and deparse() and I need the parse() step to
know where to insert output, i.e., where expressions are complete. I
have ideas about chunks where the code is exactly as in the source
file, but no time to implement them yet.

what you always can do is to write the code in a separate source file,
include that with, e.g., latex's listings package and do a source() on
it with echo=FALSE. If you want code and output mixed one needs to
parse() -> you loose the original formatting.

.f

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From ligges at statistik.uni-dortmund.de  Wed May  4 18:32:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 04 May 2005 18:32:25 +0200
Subject: [R] rank of a matrix
In-Reply-To: <4278F018.8060101@unm.edu>
References: <4278F018.8060101@unm.edu>
Message-ID: <4278F919.6080600@statistik.uni-dortmund.de>

mingan wrote:

> 
> 
> how do I check the rank of a matrix ?
> 
> say
> 
> A=  1   0   0
>     0   1   0
> 
> then rank(A)=2
> 
> what is this function?


Well, calculating the rank is from the computational point of view a 
very hard problem.
See ?qr

Uwe Ligges


> thanks
> 
> 
>  I did try help.search("rank"), but all the returned help information 
> seem irrelevant to what I want.
> 
>  I would like to know how people search for help information like this.
> 
> 
> 
> 
> 
> 
> rank(base)              Sample Ranks
> SignRank(stats)         Distribution of the Wilcoxon Signed Rank
>                        Statistic
> Wilcoxon(stats)         Distribution of the Wilcoxon Rank Sum
>                        Statistic
> friedman.test(stats)    Friedman Rank Sum Test
> kruskal.test(stats)     Kruskal-Wallis Rank Sum Test
> pairwise.wilcox.test(stats)
>                        Pairwise Wilcoxon rank sum tests
> wilcox.test(stats)      Wilcoxon Rank Sum and Signed Rank Tests
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed May  4 18:31:41 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 04 May 2005 09:31:41 -0700
Subject: [R] rank of a matrix
In-Reply-To: <4278F018.8060101@unm.edu>
References: <4278F018.8060101@unm.edu>
Message-ID: <4278F8ED.2040207@pdf.com>

	  Have you considered something like the following:

matrix.rank <- function(A, eps=sqrt(.Machine$double.eps)){
	sv. <- abs(svd(A)$d)
	sum((sv./max(sv.))>eps)
}

matrix.rank(A=diag(3))
#[1] 3
A <- array(c(1,1,0,0), dim=c(2,2))
matrix.rank(A)
#[1] 1

mingan wrote:

> 
> 
> how do I check the rank of a matrix ?
> 
> say
> 
> A=  1   0   0
>     0   1   0
> 
> then rank(A)=2
> 
> what is this function?
> 
> thanks
> 
> 
>  I did try help.search("rank"), but all the returned help information 
> seem irrelevant to what I want.
> 
>  I would like to know how people search for help information like this.
> 
> 
> 
> 
> 
> 
> rank(base)              Sample Ranks
> SignRank(stats)         Distribution of the Wilcoxon Signed Rank
>                        Statistic
> Wilcoxon(stats)         Distribution of the Wilcoxon Rank Sum
>                        Statistic
> friedman.test(stats)    Friedman Rank Sum Test
> kruskal.test(stats)     Kruskal-Wallis Rank Sum Test
> pairwise.wilcox.test(stats)
>                        Pairwise Wilcoxon rank sum tests
> wilcox.test(stats)      Wilcoxon Rank Sum and Signed Rank Tests
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Wed May  4 18:33:03 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 04 May 2005 09:33:03 -0700
Subject: [R] rank of a matrix
In-Reply-To: <4278F018.8060101@unm.edu>
References: <4278F018.8060101@unm.edu>
Message-ID: <4278F93F.6070107@pdf.com>


mingan wrote on 5/4/2005 8:54 AM:
> 
> 
> how do I check the rank of a matrix ?
> 
> say
> 
> A=  1   0   0
>     0   1   0
> 
> then rank(A)=2
> 
> what is this function?
> 
> thanks
> 
> 
>  I did try help.search("rank"), but all the returned help information 
> seem irrelevant to what I want.
> 
>  I would like to know how people search for help information like this.
> 
> 
> 
> 
> 
> 
> rank(base)              Sample Ranks
> SignRank(stats)         Distribution of the Wilcoxon Signed Rank
>                        Statistic
> Wilcoxon(stats)         Distribution of the Wilcoxon Rank Sum
>                        Statistic
> friedman.test(stats)    Friedman Rank Sum Test
> kruskal.test(stats)     Kruskal-Wallis Rank Sum Test
> pairwise.wilcox.test(stats)
>                        Pairwise Wilcoxon rank sum tests
> wilcox.test(stats)      Wilcoxon Rank Sum and Signed Rank Tests
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


See ?qr or ?svd.

tol <- 1e-7
qr(A, tol)$rank
sum(svd(A)$d > tol)

--sundar



From uofiowa at gmail.com  Wed May  4 18:51:55 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 4 May 2005 12:51:55 -0400
Subject: [R] help track a segmentation fault
Message-ID: <3f87cc6d0505040951610935f6@mail.gmail.com>

I have an R script that I run using 

nohup R CMD BATCH r.in r.out &

The code loops through data from the database and takes hours. The
problem is, in about an hour and a half after I start the script the
program stops and I get

/usr/lib/R/bin/BATCH: line 55: 14067 Done                    ( echo
"invisible(options(echo = TRUE))"; cat ${in}; echo "proc.time()" )
     14068 Segmentation fault      | ${R_HOME}/bin/R ${opts} >${out} 2>&1

in the nohup.out file.
If I run the code from within R, not using CMD BATCH, R sig faults
after the hour and a half (roughly).
I monitored the process using "top" and found nothing usual, the mem
utilization is around 15% and  CPU time in the 90s%. I do not see a
steady increase in mem usage signaling memory leak.
I have a core dump file generated but I do not know what to do with it.
Can someone, please, suggest to me what I can do to track this
problem, probably using the -d flag with R, which I do not know how to
use.
I am running R 2.1.0 on debian.



From sethfalcon at comcast.net  Wed May  4 19:03:19 2005
From: sethfalcon at comcast.net (Seth Falcon)
Date: Wed, 04 May 2005 10:03:19 -0700
Subject: [R] ANN: Two Upcoming Bioconductor Short Courses
Message-ID: <m2u0lic4a0.fsf@macaroni.local>

Announcing two upcoming Bioconductor short courses:


June 1-3 in Seattle, WA
Bioconductor Short Course
Details: http://www.bioconductor.org/signup

June 19-25 in Bressanone-Brixen
Computational and Statistical Aspects of Microarray Analysis
Details: http://www.economia.unimi.it/marray/2005/

Best Wishes,

+ seth



From reid_huntsinger at merck.com  Wed May  4 19:18:23 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 4 May 2005 13:18:23 -0400
Subject: [R] rank of a matrix
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A940E@uswpmx00.merck.com>

qr() returns an estimate of the rank. However the rank of a matrix isn't
really computable (or useful) in general in finite precision arithmetic. The
Hilbert matrix example (from help(svd)) is a good illustration:

> hilbert <- function(n) { i <- 1:n; 1 / outer(i - 1, i, "+") }
> qr(hilbert(9))$rank
[1] 7

but it's actually an invertible 9 x 9 matrix. 


Rather you can estimate how far a matrix is from having rank <= k for
example. A book on numerical linear algebra would be a good reference.

A common approach to statistical analysis of certain kinds of data deals
with the ranks of the data values, and that's why you got so many hits for
"rank". 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of mingan
Sent: Wednesday, May 04, 2005 11:54 AM
To: r-help at stat.math.ethz.ch
Subject: [R] rank of a matrix




how do I check the rank of a matrix ?

say

A=  1   0   0
     0   1   0

then rank(A)=2

what is this function?

thanks


  I did try help.search("rank"), but all the returned help information 
seem irrelevant to what I want.

  I would like to know how people search for help information like this.






rank(base)              Sample Ranks
SignRank(stats)         Distribution of the Wilcoxon Signed Rank
                        Statistic
Wilcoxon(stats)         Distribution of the Wilcoxon Rank Sum
                        Statistic
friedman.test(stats)    Friedman Rank Sum Test
kruskal.test(stats)     Kruskal-Wallis Rank Sum Test
pairwise.wilcox.test(stats)
                        Pairwise Wilcoxon rank sum tests
wilcox.test(stats)      Wilcoxon Rank Sum and Signed Rank Tests

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Roger.Bivand at nhh.no  Wed May  4 19:35:47 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 4 May 2005 19:35:47 +0200 (CEST)
Subject: [R] selecting maximum values
In-Reply-To: <ddf4ed79b9104d6e3233f500e6621e6b@mail.nih.gov>
Message-ID: <Pine.LNX.4.44.0505041925580.23757-100000@reclus.nhh.no>

On Wed, 4 May 2005, Sean Davis wrote:

> see ?aggregate.

Or maybe tapply, or its close relative, by:

> by(df, list(df$station, df$date), function(x) 
+   x$row[which.max(x$chlorophyll)]) 
: Ancona
: 21/06/01
[1] NA
------------------------------------------------------------ 
: Castagneto
: 21/06/01
[1] 3
------------------------------------------------------------ 
: Ancona
: 23/06/01
[1] 6
------------------------------------------------------------ 
: Castagneto
: 23/06/01
[1] NA

since happily a row ID column was included in the data frame. Note that 
which.max only reports the row of the first maximum if there are ties.

> 
> Sean
> 
> On May 4, 2005, at 11:43 AM, alessandro carletti wrote:
> 
> > Sorry for disturbing you with another newbie question!
> > I have a data frame about coastal waters quality
> > parameters: for some parameters (e.g. NH3) I have only
> > 1 observation for each sampling station and each
> > sampling date, while in other cases (chlorophyll) I
> > have 1 obs for each meter-depth for each station and
> > date. How can I select only the max chlorophyll value
> > for each station/date?
> >
> > example
> >
> > row  station         date        depth     chlorophyll
> > 1     Castagneto      21/06/01     -0.5         2.0
> > 2     Castagneto      21/06/01     -1.5         2.2
> > 3     Castagneto      21/06/01     -2.5         2.4
> > 4     Castagneto      21/06/01     -3.5         2.1
> > 5     Ancona          23/06/01     -0.5         2.4
> > 6     Ancona          23/06/01     -1.5         2.5
> > 7     Ancona          23/06/01     -2.5         2.2
> > 8     Ancona          23/06/01     -3.5         2.1
> > 9     Ancona          23/06/01     -4.5         1.9
> > ...
> >
> > I'd like to select only row 3 and 6, the ones with max
> > chlorophyll values, or have the mean for the rows 1:4
> > and 5:9
> >
> > Thanks
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ggrothendieck at gmail.com  Wed May  4 19:48:20 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 May 2005 13:48:20 -0400
Subject: [R] rank of a matrix
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A940E@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA31999056A940E@uswpmx00.merck.com>
Message-ID: <971536df050504104862e6d53@mail.gmail.com>

In this case, try a lower tolerance (1e-7 is the default):

> qr(hilbert(9), tol = 1e-8)$rank
[1] 9


On 5/4/05, Huntsinger, Reid <reid_huntsinger at merck.com> wrote:
> qr() returns an estimate of the rank. However the rank of a matrix isn't
> really computable (or useful) in general in finite precision arithmetic. The
> Hilbert matrix example (from help(svd)) is a good illustration:
> 
> > hilbert <- function(n) { i <- 1:n; 1 / outer(i - 1, i, "+") }
> > qr(hilbert(9))$rank
> [1] 7
> 
> but it's actually an invertible 9 x 9 matrix.
> 
> Rather you can estimate how far a matrix is from having rank <= k for
> example. A book on numerical linear algebra would be a good reference.
> 
> A common approach to statistical analysis of certain kinds of data deals
> with the ranks of the data values, and that's why you got so many hits for
> "rank".
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of mingan
> Sent: Wednesday, May 04, 2005 11:54 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] rank of a matrix
> 
> how do I check the rank of a matrix ?
> 
> say
> 
> A=  1   0   0
>     0   1   0
> 
> then rank(A)=2
> 
> what is this function?
> 
> thanks
> 
>  I did try help.search("rank"), but all the returned help information
> seem irrelevant to what I want.
> 
>  I would like to know how people search for help information like this.
> 
> rank(base)              Sample Ranks
> SignRank(stats)         Distribution of the Wilcoxon Signed Rank
>                        Statistic
> Wilcoxon(stats)         Distribution of the Wilcoxon Rank Sum
>                        Statistic
> friedman.test(stats)    Friedman Rank Sum Test
> kruskal.test(stats)     Kruskal-Wallis Rank Sum Test
> pairwise.wilcox.test(stats)
>                        Pairwise Wilcoxon rank sum tests
> wilcox.test(stats)      Wilcoxon Rank Sum and Signed Rank Tests
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rolf at math.unb.ca  Wed May  4 20:04:25 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 4 May 2005 15:04:25 -0300 (ADT)
Subject: [R] Step wise regression
Message-ID: <200505041804.j44I4PsQ025679@erdos.math.unb.ca>


I would like to add to this discussion a quote from the book
``Subset Selection in Regression'' by Alan J. Miller (Chapman and
Hall, London, 1990).  On page 12 the author remarks that when using a
``black box'' variable selection technique

	``Throwing in a few more variables produced by a
	  random number generator or from the pages of a
	  telephone directory could have a very salutary
	  effect!''

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From ipardoe at lcbmail.uoregon.edu  Wed May  4 20:08:33 2005
From: ipardoe at lcbmail.uoregon.edu (Iain Pardoe)
Date: Wed, 4 May 2005 11:08:33 -0700
Subject: [R] Multivariate multiple regression
Message-ID: <A11B20BC51EEFA41AE1516AA38CF8941016E7ABB@mail.lcb.uoregon.edu>

I'd like to model the relationship between m responses Y1, ..., Ym and a
single set of predictor variables X1, ..., Xr.  Each response is assumed
to follow its own regression model, and the error terms in each model
can be correlated.  My understanding is that although lm() handles
vector Y's on the left-hand side of the model formula, it really just
fits m separate lm models.  What should I use to do a full multivariate
analysis (as in section 7.7 of Johnson/Wichern)?  Thanks.

Iain Pardoe <ipardoe at lcbmail.uoregon.edu>
University of Oregon



From uofiowa at gmail.com  Wed May  4 20:13:57 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 4 May 2005 14:13:57 -0400
Subject: [R] Fwd: help track a segmentation fault
In-Reply-To: <3f87cc6d0505040951610935f6@mail.gmail.com>
References: <3f87cc6d0505040951610935f6@mail.gmail.com>
Message-ID: <3f87cc6d05050411133799510f@mail.gmail.com>

This is the back trace.
Is my best shot recompiling R with debug flag to get better info from
teh debugger?

(gdb) r --no-save < r.in > r.out
Starting program: /usr/lib/R/bin/exec/R --no-save < r.in > r.out
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
(no debugging symbols found)
[Thread debugging using libthread_db enabled]
[New Thread 16384 (LWP 14949)]
Creating a new generic function for 'names' in 'its'
Creating a new generic function for 'names<-' in 'its'
Creating a new generic function for 'print' in 'its'
Creating a new generic function for 'start' in 'its'
Creating a new generic function for 'end' in 'its'
Creating a new generic function for 'summary' in 'its'
Creating a new generic function for 'diff' in 'its'
Creating a new generic function for 'union' in 'its'
Creating a new generic function for 'intersect' in 'its'

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 16384 (LWP 14949)]
0x00000000 in ?? ()
(gdb) bt
#0  0x00000000 in ?? ()
#1  0x00000001 in ?? ()
#2  0x08647304 in ?? ()
#3  0x08e7bb5c in ?? ()
#4  0x4009f754 in Rf_eval () from /usr/lib/R/lib/libR.so
#5  0x4009f9e6 in Rf_eval () from /usr/lib/R/lib/libR.so
#6  0x400a2501 in Rf_evalList () from /usr/lib/R/lib/libR.so
#7  0x400d1eed in do_internal () from /usr/lib/R/lib/libR.so
#8  0x400d3cc0 in do_nextmethod () from /usr/lib/R/lib/libR.so
#9  0x400d1a41 in do_internal () from /usr/lib/R/lib/libR.so
#10 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#11 0x400a01b7 in Rf_applyClosure () from /usr/lib/R/lib/libR.so
#12 0x4009fb2b in Rf_eval () from /usr/lib/R/lib/libR.so
#13 0x400a1845 in do_begin () from /usr/lib/R/lib/libR.so
#14 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#15 0x400a01b7 in Rf_applyClosure () from /usr/lib/R/lib/libR.so
#16 0x400a4054 in Rf_DispatchGroup () from /usr/lib/R/lib/libR.so
#17 0x4012574b in do_relop () from /usr/lib/R/lib/libR.so
#18 0x4009fd62 in Rf_eval () from /usr/lib/R/lib/libR.so
#19 0x400a2501 in Rf_evalList () from /usr/lib/R/lib/libR.so
#20 0x4009fcca in Rf_eval () from /usr/lib/R/lib/libR.so
#21 0x4009f9e6 in Rf_eval () from /usr/lib/R/lib/libR.so
#22 0x400a26f7 in Rf_evalListKeepMissing () from /usr/lib/R/lib/libR.so
#23 0x400a3300 in Rf_EvalArgs () from /usr/lib/R/lib/libR.so
#24 0x400a36d7 in Rf_DispatchOrEval () from /usr/lib/R/lib/libR.so
#25 0x40145387 in do_subset () from /usr/lib/R/lib/libR.so
#26 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#27 0x400a2501 in Rf_evalList () from /usr/lib/R/lib/libR.so
#28 0x4009fcca in Rf_eval () from /usr/lib/R/lib/libR.so
#29 0x400a2501 in Rf_evalList () from /usr/lib/R/lib/libR.so
#30 0x4009fcca in Rf_eval () from /usr/lib/R/lib/libR.so
#31 0x400a22db in do_set () from /usr/lib/R/lib/libR.so
#32 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#33 0x400a0dd4 in do_if () from /usr/lib/R/lib/libR.so
#34 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#35 0x400a1845 in do_begin () from /usr/lib/R/lib/libR.so
#36 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#37 0x400a14da in do_while () from /usr/lib/R/lib/libR.so
#38 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#39 0x400a1845 in do_begin () from /usr/lib/R/lib/libR.so
#40 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#41 0x400a01b7 in Rf_applyClosure () from /usr/lib/R/lib/libR.so
#42 0x4009fb2b in Rf_eval () from /usr/lib/R/lib/libR.so
#43 0x400a22db in do_set () from /usr/lib/R/lib/libR.so
#44 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#45 0x400a1845 in do_begin () from /usr/lib/R/lib/libR.so
#46 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#47 0x400a14da in do_while () from /usr/lib/R/lib/libR.so
#48 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#49 0x400a1845 in do_begin () from /usr/lib/R/lib/libR.so
#50 0x4009fe22 in Rf_eval () from /usr/lib/R/lib/libR.so
#51 0x400a01b7 in Rf_applyClosure () from /usr/lib/R/lib/libR.so
#52 0x4009fb2b in Rf_eval () from /usr/lib/R/lib/libR.so
#53 0x400c305b in Rf_ReplIteration () from /usr/lib/R/lib/libR.so
#54 0x400c3273 in Rf_ReplIteration () from /usr/lib/R/lib/libR.so
---Type <return> to continue, or q <return> to quit---
#55 0x400c3df1 in run_Rmainloop () from /usr/lib/R/lib/libR.so
#56 0x400c3e2e in Rf_mainloop () from /usr/lib/R/lib/libR.so
#57 0x080486d0 in main ()
(gdb) 

---------- Forwarded message ----------
From: Omar Lakkis <uofiowa at gmail.com>
Date: May 4, 2005 12:51 PM
Subject: help track a segmentation fault
To: r-help at stat.math.ethz.ch


I have an R script that I run using

nohup R CMD BATCH r.in r.out &

The code loops through data from the database and takes hours. The
problem is, in about an hour and a half after I start the script the
program stops and I get

/usr/lib/R/bin/BATCH: line 55: 14067 Done                    ( echo
"invisible(options(echo = TRUE))"; cat ${in}; echo "proc.time()" )
     14068 Segmentation fault      | ${R_HOME}/bin/R ${opts} >${out} 2>&1

in the nohup.out file.
If I run the code from within R, not using CMD BATCH, R sig faults
after the hour and a half (roughly).
I monitored the process using "top" and found nothing usual, the mem
utilization is around 15% and  CPU time in the 90s%. I do not see a
steady increase in mem usage signaling memory leak.
I have a core dump file generated but I do not know what to do with it.
Can someone, please, suggest to me what I can do to track this
problem, probably using the -d flag with R, which I do not know how to
use.
I am running R 2.1.0 on debian.



From sethfalcon at comcast.net  Wed May  4 20:35:04 2005
From: sethfalcon at comcast.net (Seth Falcon)
Date: Wed, 04 May 2005 11:35:04 -0700
Subject: [R] Installing GO 1.7.0
References: <1112202606.12235.18.camel@in-141-199.dhcp-149-166.iupui.edu>
	<m2sm2dt3h5.fsf@macaroni.local> <4277E624.5000106@slcmsr.org>
Message-ID: <m2fyx2algn.fsf@macaroni.local>

Hi Cristian,

Christian Lederer <lederer at slcmsr.org> writes:
> is the announced solution for the GO 1.7.0 installation already
> publicly available?
> I am running into the same trouble as described in below, using
> 2.0.1 under Ubuntu Hoary.

Sorry about that.  No, the solution was not publicly available when
you wrote.

I've just pushed the fix to the metaData-devel repository.

You can download the fixed package here:

http://www.bioconductor.org/data/metaData-devel/html/GOv1.7.1.html

Note that we are working towards a release of Bioconductor 1.6
scheduled for 18 May.  The release will include an updated GO data
package.

Best,

+ seth



From nair at sdsc.edu  Wed May  4 20:41:17 2005
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Wed, 04 May 2005 11:41:17 -0700
Subject: [R] LDA/PCA and more
Message-ID: <4279174D.3050702@sdsc.edu>

Hi!!
I was trying to  analyze my data by doing a linear discriminants and I 
encountered
problems with collinearity.  So I am planning to do a PCA to extract 
orthogonal
components and then analyze them using linear discriminants.  I don't know
 how to relate the orthogonal components back to my real data. Is there some
way I can do this  and are there any methods in R that will help me ?
Many thanks. Cheers ../Murli

-



From simon.urbanek at r-project.org  Wed May  4 20:47:53 2005
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Wed, 4 May 2005 14:47:53 -0400
Subject: [R] JGR and R 2.0.X
In-Reply-To: <4278CBC3.2010803@statistik.uni-dortmund.de>
References: <4278B58D.4020609@uni-jena.de>
	<4278CBC3.2010803@statistik.uni-dortmund.de>
Message-ID: <2603D7D1-89A8-4893-B286-C59A0BC7075A@r-project.org>

On May 4, 2005, at 9:18 AM, Uwe Ligges wrote:

> Christoph Scherber wrote:
>
>> Dear all,
>> I am running Windows XP with several parallel installations of R  
>> (2.0.1; 2.1 and so on). How can I install JGR for the 2.0.1  
>> version? I keep on getting error messages when trying to install it.
>
> I am sure Simon Urbanek et al. are happy to help you, but even the  
> winners of the John Chambers Statistical Software Award are unable  
> to help if you don't say what the error messages are and which  
> versions of JGR you tried...

Markus is the winner, I'm just changing oil and doing dirty  
things ;). I'd be glad to help and, yes, http://www.rosuda.org/ 
lists.shtml is the right place to discuss JGR-related things in more  
detail.

In general JGR (and also JRI, rJava and JavaGD) uses the registry  
setting to determine the location and version of R. This setting is  
updated by the R installer if you leave the (D)COM setting checked  
(which is the default). This implies, however, that the sequence in  
which you install the various R versions matters - you should install  
the latest version last. If you want to edit the setting manually,  
use regedit and look into HKEY_LOCAL_MACHINE/SOFTWARE/R-core/R - it  
contains both the version and the path (at your own risk).

The most recent JGR for Windows can be installed simply with
install.packages("JGR",,"http://www.rosuda.org/R")
and then starting JGR.exe which can be found in $R_HOME/library/JGR.  
The package installation fetches the correct version for your R (2.0  
and 2.1 are supported only). The convenience installer is still  
provided at http://www.rosuda.org/JGR/1.2/JGR-12.exe
Mac binary and Linux sources of the new release should become  
available this week.

Cheers,
Simon



From murdoch at stats.uwo.ca  Wed May  4 21:32:14 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 May 2005 20:32:14 +0100
Subject: [R] rank of a matrix
In-Reply-To: <971536df050504104862e6d53@mail.gmail.com>
References: <D9A95B4B7B20354992E165EEADA31999056A940E@uswpmx00.merck.com>
	<971536df050504104862e6d53@mail.gmail.com>
Message-ID: <4279233E.6010604@stats.uwo.ca>

Gabor Grothendieck wrote:
> In this case, try a lower tolerance (1e-7 is the default):
> 
> 
>>qr(hilbert(9), tol = 1e-8)$rank
> 
> [1] 9

But don't trust the results.  For example, create a matrix with 4 
identical copies of hilbert(9).  This still has rank 9.  It's hard to 
find, though:

 > h9 <- hilbert(9)
 > temp <- cbind(h9, h9)
 > h9times4 <- rbind(temp, temp)
 >
 > qr(h9times4,tol=1e-7)$rank
[1] 7
 > qr(h9times4, tol=1e-8)$rank
[1] 10
 > qr(h9times4, tol=1e-9)$rank
[1] 11
 > qr(h9times4, tol=1e-10)$rank
[1] 12


There's a tolerance that gives the right answer (1.5e-8 works for me), 
but how would I know that in a real problem where I didn't already know 
the answer?

Duncan Murdoch



From r+Steven.Murdoch at cl.cam.ac.uk  Wed May  4 21:47:08 2005
From: r+Steven.Murdoch at cl.cam.ac.uk (r+Steven.Murdoch@cl.cam.ac.uk)
Date: Wed, 4 May 2005 20:47:08 +0100
Subject: [R] xinch/yinch equivalent for log axis
In-Reply-To: <4276E280.9020509@stat.auckland.ac.nz>
References: <20050502165343.GA26558@cl.cam.ac.uk>
	<4276E280.9020509@stat.auckland.ac.nz>
Message-ID: <20050504194708.GA30069@cl.cam.ac.uk>

On Tue, May 03, 2005 at 02:31:28PM +1200, Paul Murrell wrote:
> No function, but does this do what you want?
[snip]

Yes, that worked perfectly. Thanks for your help.

Steven Murdoch.

-- 
w: http://www.cl.cam.ac.uk/users/sjm217/



From Roosen at insightful.com  Wed May  4 22:05:30 2005
From: Roosen at insightful.com (Charlie Roosen)
Date: Wed, 4 May 2005 13:05:30 -0700
Subject: [R] JOB: Financial Software Engineering position at Insightful
Message-ID: <0AED0D29DB783641A47A3BD7C4FE1476CCF3FE@se2kexch02.insightful.com>

Dear R-users,

Insightful has a position open for a Senior Software Engineer in
computational finance.  We are looking for someone with both solid software
engineering skills and experience in computational finance.  It's a very
interesting position for the right person.

The job description is below.  

Thanks,
Charlie Roosen
Software Engineering Manager
Insightful Corp

---------

http://www.insightful.com/company/jobdescription.asp?JobID=49

Senior Software Engineer: Computational Finance 

Department: Development  
Reports To: Software Engineering Manager 
Status: Exempt  


SUMMARY OF RESPONSIBILITIES & DUTIES:  
This position requires a hands-on, driven, well organized professional with
experience in computational finance and software engineering.  This
individual will be the primary developer for S+FinMetrics and related S-PLUS
modules for financial engineering.


Duties and Responsibilities:

* Design and implement new functionality in C/C++, S-PLUS, and other
languages as needed.  Create unit tests.  Write help files and other
documentation.

* Project and program management for the financial engineering modules, in
collaboration with the Software Engineering Manager and Product Manager.  

* Contribute to the identification of technical requirements to meet market
needs.  Develop technical specifications and work plans.

* Provide second-tier support for financial engineering questions.     
 

Qualifications 

Education and Training:  M.S. or Ph.D. degree required in computer science,
computational finance, or a related field.


Specialized Knowledge and Skills:

* Experience in computational finance.  Areas of focus include financial
econometrics, valuation, risk, portfolio optimization, and time series
databases. 

* Experience creating production-quality code in C/C++ required.  Experience
with other languages such as S-PLUS, Fortran, Java, and Matlab highly
beneficial.

* Experience in numerical analysis, statistical computing, and optimization.
Familiar with accuracy, stability, and convergence issues in computation.

* Strong software engineering practices.  Experience with source control
systems, defect tracking systems, unit test frameworks, and similar tools.
Strong time management and organization skills.  Self-directed.

WORK ENVIRONMENT/ PHYSICAL DEMANDS: General office environment.  Ability to
lift computer equipment is required.  Position located in Seattle,
Washington.


Contact Info 
 
Please send a resume and cover letter to: 

Human Resources
Insightful Corporation
1700 Westlake Ave North, 5th Floor
Seattle, WA 98109-3044
Or email (text, MS Word or PDF) to hr at insightful.com



From gerifalte28 at hotmail.com  Wed May  4 22:11:07 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 04 May 2005 20:11:07 +0000
Subject: [R] (no subject)
In-Reply-To: <IFXJ7M$83F019763D6380D79C4C80C4738E88E3@terra.com.br>
Message-ID: <BAY103-F42E7B7A3155BB2D9D675E8A6190@phx.gbl>

You can search on this forum for different discussions on this topic. Also, 
there are 2 somehow outdated publications comparing different software 
including SAS, SPSS and S-Plus, which might be of some use for you.  The 
references are:

McCullough, B. D. (1998), "Assessing the reliability of statistical
software: Part I", The American Statistician, 52,  358-366.

McCullough, B. D. (1999), "Assessing the reliability of statistical
software: Part II", The American Statistician, 53,149-159.

I hope this helps

Francisco

>From: "rene.raupp" <rene.raupp at terra.com.br>
>To: "r-help" <r-help at stat.math.ethz.ch>
>Subject: [R] (no subject)
>Date: Tue,  3 May 2005 16:52:34 -0300
>
>Does anybory knows any work comparing R with other (charged) statistical 
>softwares (like Minitab, SPSS, SAS)?
>I work in a brasilian government bureau and I intend to use R as our 
>preferable statistical software, but I have to show it's as good as the 
>others. I also intend to use Weka, and for this one I have the same 
>problem.
>Can anyone help me?
>Thanks
>RenÅÈ M. Raupp
>e-mail: rener at mpdft.gov.br
>         rene.raupp at terra.com.br
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed May  4 22:36:06 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 May 2005 22:36:06 +0200
Subject: [R] Huynh-Feldt R vs SAS Bug
In-Reply-To: <4278C2E5.3020802@gmx.net>
References: <4278C2E5.3020802@gmx.net>
Message-ID: <x2k6me91ah.fsf@turmalin.kubism.ku.dk>

Bela Bauer <bela_b at gmx.net> writes:

> Hi,
> 
> I'm using anova.mlm sphericity tests/corrections, and I'm getting
> different values than SAS. In order to be able to use these values for
> publications, I'd need to know more about the SAS bug that is
> mentioned in the Reference Manual.
> - What exactly causes the different values?
> - Is it just a slight difference, or can I expect significant
> differences in H-F/G-G epsilons and corrected p-Values? With the data
> sets I'm using, the SAS value for H-F epsilon is almost twice the
> value from R, and I'm wondering if there's a mistake on my side or if
> it is just caused by the SAS bug.
> 
> Thanks for any hints...

R has

    HF.eps <- ((n + 1) * pp * GG.eps - 2)/(pp * (n - pp * GG.eps))

where n is the degrees of freedom for the SSD matrix and pp is the
dimension after transformation. As far as I could fathom from the SAS
output, SAS is using

    (N * pp * GG.eps - 2)/(pp * (n - pp * GG.eps))

which coincides with the above when n == N - 1. This suggests that
whoever coded up the SAS version generalised (N - 1) in the
denominator to DF but not the N in the numerator.

Some fairly simple invariance considerations show that the SAS formula
cannot be right - it's a bias correction for GG.eps which is
calculated from the eigenvalues of the SSD, and the distribution of
the SSD depends on degrees of freedom only.

> 
> Bela
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gcll688 at yahoo.com  Wed May  4 23:56:29 2005
From: gcll688 at yahoo.com (Grace Clinton)
Date: Wed, 4 May 2005 14:56:29 -0700 (PDT)
Subject: [R] Questions about the intersection area under two kernel densities
Message-ID: <20050504215629.19077.qmail@web31301.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050504/efa547ef/attachment.pl

From rvaradha at jhsph.edu  Thu May  5 00:03:18 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Wed, 4 May 2005 18:03:18 -0400
Subject: [R] rank of a matrix
In-Reply-To: <4279233E.6010604@stats.uwo.ca>
Message-ID: <OWA-22oHzGkzMcvsq9I0000674e@owa-2.sph.ad.jhsph.edu>

For the example that Duncan just gave, using the "matrix.rank" function of
Spencer Graves (which uses singular value decomposition) I obtained the
following result:

> exponent <- -(7:16)
> eps <- 10^exponent
> sapply(eps,mat=h9times4,function(x,mat)matrix.rank(mat,x))
 [1] 6 7 7 8 8 9 9 9 9 9

This tells me that the correct rank should be 9, since the rank stabilizes
for smaller tolerances.  I realize that this may not work generally, and one
could create counter-examples to defeat this strategy.

Best,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
> Sent: Wednesday, May 04, 2005 3:32 PM
> To: Gabor Grothendieck
> Cc: mingan; r-help at stat.math.ethz.ch; Huntsinger,Reid
> Subject: Re: [R] rank of a matrix
> 
> Gabor Grothendieck wrote:
> > In this case, try a lower tolerance (1e-7 is the default):
> >
> >
> >>qr(hilbert(9), tol = 1e-8)$rank
> >
> > [1] 9
> 
> But don't trust the results.  For example, create a matrix with 4
> identical copies of hilbert(9).  This still has rank 9.  It's hard to
> find, though:
> 
>  > h9 <- hilbert(9)
>  > temp <- cbind(h9, h9)
>  > h9times4 <- rbind(temp, temp)
>  >
>  > qr(h9times4,tol=1e-7)$rank
> [1] 7
>  > qr(h9times4, tol=1e-8)$rank
> [1] 10
>  > qr(h9times4, tol=1e-9)$rank
> [1] 11
>  > qr(h9times4, tol=1e-10)$rank
> [1] 12
> 
> 
> There's a tolerance that gives the right answer (1.5e-8 works for me),
> but how would I know that in a real problem where I didn't already know
> the answer?
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From phhs80 at gmail.com  Thu May  5 00:12:05 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 4 May 2005 23:12:05 +0100
Subject: [R] Density of the sum of two random variables
Message-ID: <6ade6f6c0505041512795533db@mail.gmail.com>

Dear All

I would like to know whether it is possible with R to get the
mathematical expression of the density of a sum of two independent
continuous random variables.

Thanks in advance,

Paul



From rich.fitzjohn at gmail.com  Thu May  5 01:12:55 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Thu, 5 May 2005 11:12:55 +1200
Subject: [R] Questions about the intersection area under two kernel
	densities
In-Reply-To: <20050504215629.19077.qmail@web31301.mail.mud.yahoo.com>
References: <20050504215629.19077.qmail@web31301.mail.mud.yahoo.com>
Message-ID: <5934ae5705050416126342343a@mail.gmail.com>

Hi,

A "density" object's x and y values can be accessed using $x and $y
(this is in ?density, under "Value:")

obj <- density(rnorm(100, 0, 1))
obj$x
obj$y

You may find it useful to force the "from" and "to" arguments to be
the same for the two calls to density(), if you want to work directly
from the x and y values.

x <- rnorm(100, 0, 1)
y <- rnorm(100, .2, 1)
dx <- density(x)
dy <- density(y)

## Re-do the densities so they have the same x-values:
lim <- range(dx$x, dy$x)
dx <- density(x, from=lim[1], to=lim[2])
dy <- density(y, from=lim[1], to=lim[2])

plot(dx, col="blue", ylim=range(dx$y, dy$y))
lines(dy, col="red")
polygon(dx$x, pmin(dx$y, dy$y), col="purple")

Cheers,
Rich

On 5/5/05, Grace Clinton <gcll688 at yahoo.com> wrote:
> Hi there,
> 
> I am working on a project which needs the value of the interaction area under two distributions( eatimated by kernel density estimators).
> 
> For example:
> 
> x<-rnorm(100,0,1)
> y<-rnorm(100,0.2,1)
> density(x) # This produces the summary of dependent variable and independent variable.
> 
> How can I get the individual values of variables and reform a curve to calculate the area under curve?
> 
> Is there a way to get aroud this and give me the solution?
> 
> Thanks a lot.
> 
> Grace
> 
> ---------------------------------
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From ggrothendieck at gmail.com  Thu May  5 01:18:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 May 2005 19:18:28 -0400
Subject: [R] rank of a matrix
In-Reply-To: <OWA-22oHzGkzMcvsq9I0000674e@owa-2.sph.ad.jhsph.edu>
References: <4279233E.6010604@stats.uwo.ca>
	<OWA-22oHzGkzMcvsq9I0000674e@owa-2.sph.ad.jhsph.edu>
Message-ID: <971536df050504161843bce73@mail.gmail.com>

One could also examine the eigenvalues themselves:

    plot(log(abs(sort(-eigen(h9times4, T, T)$values))))

shows a graph with a definite gap between 9 and 10 suggesting
that 9 is the right number.  

On 5/4/05, Ravi Varadhan <rvaradha at jhsph.edu> wrote:
> For the example that Duncan just gave, using the "matrix.rank" function of
> Spencer Graves (which uses singular value decomposition) I obtained the
> following result:
> 
> > exponent <- -(7:16)
> > eps <- 10^exponent
> > sapply(eps,mat=h9times4,function(x,mat)matrix.rank(mat,x))
> [1] 6 7 7 8 8 9 9 9 9 9
> 
> This tells me that the correct rank should be 9, since the rank stabilizes
> for smaller tolerances.  I realize that this may not work generally, and one
> could create counter-examples to defeat this strategy.
> 
> Best,
> Ravi.
> 
> --------------------------------------------------------------------------
> Ravi Varadhan, Ph.D.
> Assistant Professor,  The Center on Aging and Health
> Division of Geriatric Medicine and Gerontology
> Johns Hopkins University
> Ph: (410) 502-2619
> Fax: (410) 614-9625
> Email:  rvaradhan at jhmi.edu
> --------------------------------------------------------------------------
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
> > Sent: Wednesday, May 04, 2005 3:32 PM
> > To: Gabor Grothendieck
> > Cc: mingan; r-help at stat.math.ethz.ch; Huntsinger,Reid
> > Subject: Re: [R] rank of a matrix
> >
> > Gabor Grothendieck wrote:
> > > In this case, try a lower tolerance (1e-7 is the default):
> > >
> > >
> > >>qr(hilbert(9), tol = 1e-8)$rank
> > >
> > > [1] 9
> >
> > But don't trust the results.  For example, create a matrix with 4
> > identical copies of hilbert(9).  This still has rank 9.  It's hard to
> > find, though:
> >
> >  > h9 <- hilbert(9)
> >  > temp <- cbind(h9, h9)
> >  > h9times4 <- rbind(temp, temp)
> >  >
> >  > qr(h9times4,tol=1e-7)$rank
> > [1] 7
> >  > qr(h9times4, tol=1e-8)$rank
> > [1] 10
> >  > qr(h9times4, tol=1e-9)$rank
> > [1] 11
> >  > qr(h9times4, tol=1e-10)$rank
> > [1] 12
> >
> >
> > There's a tolerance that gives the right answer (1.5e-8 works for me),
> > but how would I know that in a real problem where I didn't already know
> > the answer?
> >
> > Duncan Murdoch
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
>



From arepper at hotmail.com  Thu May  5 01:26:26 2005
From: arepper at hotmail.com (Angus Repper)
Date: Wed, 4 May 2005 19:26:26 -0400
Subject: [R] Help with R
Message-ID: <BAY103-DAV8D5B53012E331584750C5AE190@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050504/798ccf5f/attachment.pl

From Simon.Blomberg at anu.edu.au  Thu May  5 02:34:14 2005
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 5 May 2005 10:34:14 +1000
Subject: [R] Double hurdle model in R
In-Reply-To: <Pine.OSF.4.58.0505041807100.261715@sirppi.helsinki.fi>
References: <Pine.OSF.4.58.0505041807100.261715@sirppi.helsinki.fi>
Message-ID: <a06110400be9f16306b1a@[150.203.51.113]>

http://pscl.stanford.edu/content.html has code 
for poisson and negative binomial hurdle models, 
as well as for zip models. Package zicounts on 
CRAN may also be of interest.

HTH,

Simon.


>I am interested in utilizing this so called "double hurdle" model
>in my study. We can write the model in the following way:
>
>if (z'a + u > 0 & x'b + e > 0) y = x'b + e, else y = 0
>
>In the model, consumption y is the (left-) censored dependent variable. e
>and u are the normally distributed error terms. z'a is the participation
>equation and x'b is the expenditure equation. If we would remove the
>participation equation from the model we would end up with a type I
>tobit-model.
>
>In the tobit-model the same set of paprameters and variables determine
>both the discrete probability of non-zero outcome and the level of
>positive expenditure. In the double-hurdle-model we have separate
>parametrization of the participation and consumption decisions.
>
>I can estimate tobit-model using function survreg(). But what about this
>double hurdle thing? Has somebody written a R-function for estimating this
>sort of a model?
>
>
>Best regards,
>Ky??sti Kurikka
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Visiting Fellow
School of Botany & Zoology
The Australian National University
Canberra ACT 0200
Australia

T: +61 2 6125 8057  email: Simon.Blomberg at anu.edu.au
F: +61 2 6125 5573

CRICOS Provider # 00120C



From raftery at stat.washington.edu  Thu May  5 03:30:33 2005
From: raftery at stat.washington.edu (Adrian Raftery)
Date: Wed, 4 May 2005 18:30:33 -0700 (PDT)
Subject: [R] [R-pkgs] Bayesian Model Averaging (BMA) R Package available
In-Reply-To: <Pine.OSF.4.58.0312041933390.334531@lisbon2.stat.washington.edu>
References: <Pine.OSF.4.58.0312041933390.334531@lisbon2.stat.washington.edu>
Message-ID: <Pine.LNX.4.61.0505041830150.31655@madrid1.stat.washington.edu>


Bayesian Model Averaging (BMA) R Package available

Free software to carry out Bayesian Model Averaging (BMA) 
in R is now available at
http://cran.us.r-project.org/src/contrib/Descriptions/BMA.html
BMA is a way of taking account of uncertainty about
the form of the model used by averaging across the models
considered. The package includes BMA software for linear
regression, generalized linear models and survival analysis,
as well as various plotting routines for visualizing the BMA output.

Adrian Raftery


  -------------------------------------------------------------------
  Adrian E. Raftery
  Professor of Statistics and Sociology
  Director, Center for Statistics and the Social Sciences
  University of Washington, Box 354320	 Phone: (206) 543-4505
  Seattle, WA 98195-4320.		 FAX:   (206) 221-6873
  Web: http://www.stat.washington.edu/raftery
  -------------------------------------------------------------------

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From henric.nilsson at statisticon.se  Thu May  5 09:01:52 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 05 May 2005 09:01:52 +0200
Subject: [R] Multivariate multiple regression
In-Reply-To: <A11B20BC51EEFA41AE1516AA38CF8941016E7ABB@mail.lcb.uoregon.edu>
References: <A11B20BC51EEFA41AE1516AA38CF8941016E7ABB@mail.lcb.uoregon.edu>
Message-ID: <4279C4E0.5010408@statisticon.se>

Iain Pardoe said the following on 2005-05-04 20:08:

> I'd like to model the relationship between m responses Y1, ..., Ym and a
> single set of predictor variables X1, ..., Xr.  Each response is assumed
> to follow its own regression model, and the error terms in each model
> can be correlated.  My understanding is that although lm() handles
> vector Y's on the left-hand side of the model formula, it really just
> fits m separate lm models.  What should I use to do a full multivariate
> analysis (as in section 7.7 of Johnson/Wichern)?  Thanks.

Have you tried using the `lm' function? Note that R 2.1.0 added several 
useful functions for multivariate responses.

Let's replicate Johnson & Wichern's Example 7.8 (p. 413, in the 4th Ed.) 
using `lm':

 > ex7.8 <- data.frame(z1 = c(0, 1, 2, 3, 4),
+                     y1 = c(1, 4, 3, 8, 9),
+                     y2 = c(-1, -1, 2, 3, 2))
 >
 > f.mlm <- lm(cbind(y1, y2) ~ z1, data = ex7.8)
 > summary(f.mlm)
Response y1 :

Call:
lm(formula = y1 ~ z1, data = ex7.8)

Residuals:
          1          2          3          4          5
  6.880e-17  1.000e+00 -2.000e+00  1.000e+00 -1.326e-16

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept)   1.0000     1.0954   0.913   0.4286
z1            2.0000     0.4472   4.472   0.0208 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.414 on 3 degrees of freedom
Multiple R-Squared: 0.8696,     Adjusted R-squared: 0.8261
F-statistic:    20 on 1 and 3 DF,  p-value: 0.02084


Response y2 :

Call:
lm(formula = y2 ~ z1, data = ex7.8)

Residuals:
          1          2          3          4          5
  9.931e-17 -1.000e+00  1.000e+00  1.000e+00 -1.000e+00

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept)  -1.0000     0.8944  -1.118   0.3450
z1            1.0000     0.3651   2.739   0.0714 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.155 on 3 degrees of freedom
Multiple R-Squared: 0.7143,     Adjusted R-squared: 0.619
F-statistic:   7.5 on 1 and 3 DF,  p-value: 0.07142


 > SSD(f.mlm)
$SSD
    y1 y2
y1  6 -2
y2 -2  4

$call
lm(formula = cbind(y1, y2) ~ z1, data = ex7.8)

$df
[1] 3

attr(,"class")
[1] "SSD"
 > f.mlm1 <- update(f.mlm, ~ 1)
 > anova(f.mlm, f.mlm1)
Analysis of Variance Table

Model 1: cbind(y1, y2) ~ z1
Model 2: cbind(y1, y2) ~ 1
   Res.Df Df Gen.var.  Pillai approx F num Df den Df Pr(>F)
1      3      1.4907
2      4  1   4.4721  0.9375  15.0000      2      2 0.0625 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


HTH,
Henric



From murdoch at stats.uwo.ca  Thu May  5 10:02:17 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 05 May 2005 09:02:17 +0100
Subject: [R] Density of the sum of two random variables
In-Reply-To: <6ade6f6c0505041512795533db@mail.gmail.com>
References: <6ade6f6c0505041512795533db@mail.gmail.com>
Message-ID: <4279D309.4060509@stats.uwo.ca>

Paul Smith wrote:
> Dear All
> 
> I would like to know whether it is possible with R to get the
> mathematical expression of the density of a sum of two independent
> continuous random variables.

No, that corresponds to a convolution of the two densities, and R can't 
do any symbolic integration.

You could get numerical approximations to the density at any point using 
  integrate() (or sum(), if a discrete distribution is involved).

Duncan Murdoch



From spencer.graves at pdf.com  Thu May  5 11:46:03 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 05 May 2005 02:46:03 -0700
Subject: [R] Density of the sum of two random variables
In-Reply-To: <4279D309.4060509@stats.uwo.ca>
References: <6ade6f6c0505041512795533db@mail.gmail.com>
	<4279D309.4060509@stats.uwo.ca>
Message-ID: <4279EB5B.7060801@pdf.com>

	  Have you considered package "distr"?  It will do something similar to 
what you request, I think;  it may or may not be adequate for your 
purposes.

	  spencer graves

Duncan Murdoch wrote:

> Paul Smith wrote:
> 
>> Dear All
>>
>> I would like to know whether it is possible with R to get the
>> mathematical expression of the density of a sum of two independent
>> continuous random variables.
> 
> 
> No, that corresponds to a convolution of the two densities, and R can't 
> do any symbolic integration.
> 
> You could get numerical approximations to the density at any point using 
>  integrate() (or sum(), if a discrete distribution is involved).
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu May  5 12:34:43 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 May 2005 12:34:43 +0200
Subject: [R] Help with R
In-Reply-To: <BAY103-DAV8D5B53012E331584750C5AE190@phx.gbl>
References: <BAY103-DAV8D5B53012E331584750C5AE190@phx.gbl>
Message-ID: <x2r7gmm058.fsf@turmalin.kubism.ku.dk>

"Angus Repper" <arepper at hotmail.com> writes:

> Hello
> 
>  
> 
> I am a long-time SAS user, but am new to R. I specifically am looking for
> information pertaining to generating graphics for web output. I would like
> to create dynamic graphics (in the form of generalized reports)  for my web
> site that is written with php and mysql. Is 'R' capable of doing
> this? 

Yes, people have done that. I'm not the one to ask for the details,
but it comes up on the mailing lists from time to time (hint: we have
archives...). I gather that the hardest part is to get the bitmapped
graphics to look right.

> I
> heard that 'R' does not do a very good job at handling large datasets, is
> this true? 

Yes, with qualifications: R stores entire data sets in memory, which
is a disadvantage for procedures that can be implemented using
sequential file access. However, these days PCs routinely ship with
more RAM than we had on our harddisks 5 years ago. The benefit of R is
that it allows nonsequential or multipass procedures to be specified
simply: R's x - mean(x) in SAS would be PROC MEANS followed by a DATA
step (there are various other options, I'm sure, but none involving a
single DATA step). 

For some statistical procedures, SAS also needs to store data in
memory, which makes the comparison more of a toss-up. R has generally
a bit of a cavalier attitude towards conserving memory, so often runs
into memory limitations more quickly, but carefully coded routines
like the lmer function can handle considerably larger data sets than
PROC MIXED  via the use of sparse-matrix techniques.

Both systems are victims of the curse of the rectangular data set to
some extent. Prototypically, you record the sex of a rat along with
every single measurement on it, as if the rat could change sex at
millisecond resolution. This probably applies to all current
statistical systems, but there is some hope that R's more flexible
data structures can be leveraged to better handle multilevel data.
(Cue Probabilistic Relational Models a.m. Getoor et al., which Peter
Green brought up at the recent gR meeting.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From knockaert77 at yahoo.com  Thu May  5 12:52:19 2005
From: knockaert77 at yahoo.com (Dominique Knockaert)
Date: Thu, 5 May 2005 03:52:19 -0700 (PDT)
Subject: [R] problem to get a correct output
Message-ID: <20050505105219.95227.qmail@web60217.mail.yahoo.com>

Dear,

I am new user of 'R' (package e1071) and the first
thing I did was reading the manuals. Although it was
not clear for me how it works.
My problem is the following:
I have a data set of customers data of insurances with
variables  Class, Group Distance Claim. The file
called 'xx.dat'. Using the manuals I think I had
entered the data in the program. I used several
function, but I never get an output.
In fact, I want to use the SVM with Vapnik's loss
function for classification for the response variable
'Claim' and discrete explanatory variables Class,
Group, Distance whitout modelling interaction terms
manually. I want to use epsilon = 0.1, cast penalty
constant (C) cost =1, and  a Gaussian radial basis
function kernel with y =1, i.e. k(x,x')=
exp(-y||x-x'||Å≤).
I would like to aply here the classification rule to
the whole data set and construct a 2-by-2 table of
claim and predicted levels of Claim.

I hope you can help me further !!
Many thanks in advance !

best regards,
Dominique



From elio_mineo at economia.unipa.it  Thu May  5 13:16:42 2005
From: elio_mineo at economia.unipa.it (Elio Mineo)
Date: Thu, 05 May 2005 13:16:42 +0200
Subject: [R] names(dist(mat)) gives NULL in R 2.1.0
Message-ID: <427A009A.6020008@economia.unipa.it>

Dear list,
I do not know if this is a bug.
Let's suppose mat is a matrix derived from this code:

> x<-rnorm(10)
> y<-rnorm(10)
> names(x)<-LETTERS[1:10]
> names(y)<-LETTERS[1:10]
> mat<-cbind(x,y)

Now in R 2.0.1 I have:

> names(dist(mat))
"A" "B" "C" "D" "E" "F" "G" "H" "I" "J"


In R 2.1.0 I have:

> names(dist(mat))
NULL

I have tried this on Windows (precompiled binary distribution) and on
Linux Mandrake 10.0 (R compiled from source code).
Is this a bug or not?
Thanks in advance,
Elio

-- 
----------------------------------------------------------------------------------
Angelo M. Mineo
Dipartimento di Scienze Statistiche e Matematiche "S. Vianelli"
Universit?? degli Studi di Palermo
Viale delle Scienze
90128 Palermo
url: http://dssm.unipa.it/elio



From p.dalgaard at biostat.ku.dk  Thu May  5 13:33:05 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 May 2005 13:33:05 +0200
Subject: [R] names(dist(mat)) gives NULL in R 2.1.0
In-Reply-To: <427A009A.6020008@economia.unipa.it>
References: <427A009A.6020008@economia.unipa.it>
Message-ID: <x2is1xnc0e.fsf@turmalin.kubism.ku.dk>

Elio Mineo <elio_mineo at economia.unipa.it> writes:

> In R 2.1.0 I have:
> 
> > names(dist(mat))
> NULL
> 
> I have tried this on Windows (precompiled binary distribution) and on
> Linux Mandrake 10.0 (R compiled from source code).
> Is this a bug or not?

Not.

>From the NEWS file:

    o   The S3 methods for getting and setting names of "dist" objects
        have been removed (as they provided names with a different
        length from the "dist" object itself).

(some side effects are fixed in 2.1.0-patched)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From phhs80 at gmail.com  Thu May  5 13:43:22 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Thu, 5 May 2005 12:43:22 +0100
Subject: [R] Histogram for mixed random variables
Message-ID: <6ade6f6c05050504431afa4a90@mail.gmail.com>

Dear All

I would like to get the histogram for the following model with
discrete and continuous random variables:

* with probability 1/3, a random number is drawn from the continuous
uniform distribution (min=0, max=1);
* with probability 2/3, a random number is drawn from a different
continuous uniform distribution (min=-1/2, max=1/2).

Any help would be most welcome.

Thanks in advance,

Paul



From phhs80 at gmail.com  Thu May  5 13:45:00 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Thu, 5 May 2005 12:45:00 +0100
Subject: [R] Density of the sum of two random variables
In-Reply-To: <4279EB5B.7060801@pdf.com>
References: <6ade6f6c0505041512795533db@mail.gmail.com>
	<4279D309.4060509@stats.uwo.ca> <4279EB5B.7060801@pdf.com>
Message-ID: <6ade6f6c05050504454ee7dab8@mail.gmail.com>

On 5/5/05, Spencer Graves <spencer.graves at pdf.com> wrote:
>           Have you considered package "distr"?  It will do something similar to
> what you request, I think;  it may or may not be adequate for your
> purposes.

Thanks, Spencer and Duncan. Maybe, the best choice is to use Maple or
MuPAD for that.

Paul



From andy_liaw at merck.com  Thu May  5 13:50:04 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 May 2005 07:50:04 -0400
Subject: [R] Histogram for mixed random variables
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076ECC@usctmx1106.merck.com>

See http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48428.html

Andy

> From: Paul Smith
> 
> Dear All
> 
> I would like to get the histogram for the following model with
> discrete and continuous random variables:
> 
> * with probability 1/3, a random number is drawn from the continuous
> uniform distribution (min=0, max=1);
> * with probability 2/3, a random number is drawn from a different
> continuous uniform distribution (min=-1/2, max=1/2).
> 
> Any help would be most welcome.
> 
> Thanks in advance,
> 
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From christoph.lehmann at gmx.ch  Thu May  5 15:05:55 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Thu, 05 May 2005 15:05:55 +0200
Subject: [R] Help with R
In-Reply-To: <x2r7gmm058.fsf@turmalin.kubism.ku.dk>
References: <BAY103-DAV8D5B53012E331584750C5AE190@phx.gbl>
	<x2r7gmm058.fsf@turmalin.kubism.ku.dk>
Message-ID: <427A1A33.1050206@gmx.ch>

>>I
>>heard that 'R' does not do a very good job at handling large datasets, is
>>this true? 
> 
importing huge datasets in a data.frame with e.g. a subsequent step of 
conversion of some columns into factors may lead into memory troubles 
(probably due to memory overhead when building out factors). But we 
currently succeeded in  importing 12 millions of data records stored in 
a MySQL database, using RMySQL package. The procedure which lead to 
success was:

0 define a data.frame 'data.total' with the size necessary to keep the 
whole data set to be imported
in a loop do:
   1 import the data in chunks of eg 30000 records per chunk and save it 
in a temporary data.frame 'data.chunk'
   2 the conversion into factors and other preprocessing steps, such as 
data aggregation should be done for each single chunk saved in 
'data.chunk' after import
   3 the now preprocessed chunk is saved into the appropriate part of 
the at the beginning defined data.frame 'data.total'

4 whole dataset is imported and data.frame 'data.total' is ready for 
further computational steps

in a nutshell: preprocessing steps such as conversion into factors yield 
memory troubles, even for data.sets which per se don't take too much 
memory- but done separately in smaller chunks of data, it can be done 
with R very efficiently. The 'team' MySQL together with R is VERY powerful

Cheers
Christoph



From mineoeli at unipa.it  Thu May  5 13:58:02 2005
From: mineoeli at unipa.it (Elio Mineo)
Date: Thu, 05 May 2005 13:58:02 +0200
Subject: [R] names(dist(mat)) gives NULL in R 2.1.0
In-Reply-To: <x2is1xnc0e.fsf@turmalin.kubism.ku.dk>
References: <427A009A.6020008@economia.unipa.it>
	<x2is1xnc0e.fsf@turmalin.kubism.ku.dk>
Message-ID: <427A0A4A.7090603@unipa.it>

Thanks, Peter.
So, in R 2.1.0 there is no way to get names from a "dist" object, isn't 
there?


Peter Dalgaard wrote:

>Elio Mineo <elio_mineo at economia.unipa.it> writes:
>
>  
>
>>In R 2.1.0 I have:
>>
>>    
>>
>>>names(dist(mat))
>>>      
>>>
>>NULL
>>
>>I have tried this on Windows (precompiled binary distribution) and on
>>Linux Mandrake 10.0 (R compiled from source code).
>>Is this a bug or not?
>>    
>>
>
>Not.
>
>>From the NEWS file:
>
>    o   The S3 methods for getting and setting names of "dist" objects
>        have been removed (as they provided names with a different
>        length from the "dist" object itself).
>
>(some side effects are fixed in 2.1.0-patched)
>
>  
>

-- 
----------------------------------------------------------------------------------
Prof. Angelo M. Mineo
Dipartimento di Scienze Statistiche e Matematiche "S. Vianelli"
Universit?? degli Studi di Palermo
Viale delle Scienze
90128 Palermo
url: http://dssm.unipa.it/elio



From andy_liaw at merck.com  Thu May  5 14:03:42 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 May 2005 08:03:42 -0400
Subject: [R] problem to get a correct output
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076ECD@usctmx1106.merck.com>

You gave no detail for anyone to provide useful help.  What exact commands
did you use, and what, if any, error/warning did you get?  Please (re-)read
the posting guide.  You may also want to do

  vignette("svmdoc")

Andy

> From: Dominique Knockaert
> 
> Dear,
> 
> I am new user of 'R' (package e1071) and the first
> thing I did was reading the manuals. Although it was
> not clear for me how it works.
> My problem is the following:
> I have a data set of customers data of insurances with
> variables  Class, Group Distance Claim. The file
> called 'xx.dat'. Using the manuals I think I had
> entered the data in the program. I used several
> function, but I never get an output.
> In fact, I want to use the SVM with Vapnik's loss
> function for classification for the response variable
> 'Claim' and discrete explanatory variables Class,
> Group, Distance whitout modelling interaction terms
> manually. I want to use epsilon = 0.1, cast penalty
> constant (C) cost =1, and  a Gaussian radial basis
> function kernel with y =1, i.e. k(x,x')=
> exp(-y||x-x'||??).
> I would like to aply here the classification rule to
> the whole data set and construct a 2-by-2 table of
> claim and predicted levels of Claim.
> 
> I hope you can help me further !!
> Many thanks in advance !
> 
> best regards,
> Dominique
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From andy_liaw at merck.com  Thu May  5 14:18:59 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 May 2005 08:18:59 -0400
Subject: [R] names(dist(mat)) gives NULL in R 2.1.0
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076ECE@usctmx1106.merck.com>

> From: Elio Mineo
> 
> Thanks, Peter.
> So, in R 2.1.0 there is no way to get names from a "dist" 
> object, isn't 
> there?

If you are referring to the row names of the matrix from which distances are
computed, see the "Value" section of ?dist, which has:

The object has the following attributes (besides "class" equal to "dist"): 

Size   integer, the number of observations in the dataset. 
Labels optionally, contains the labels, if any, of the observations of the
dataset. 
[...]

Andy
 
 
> Peter Dalgaard wrote:
> 
> >Elio Mineo <elio_mineo at economia.unipa.it> writes:
> >
> >  
> >
> >>In R 2.1.0 I have:
> >>
> >>    
> >>
> >>>names(dist(mat))
> >>>      
> >>>
> >>NULL
> >>
> >>I have tried this on Windows (precompiled binary 
> distribution) and on
> >>Linux Mandrake 10.0 (R compiled from source code).
> >>Is this a bug or not?
> >>    
> >>
> >
> >Not.
> >
> >>From the NEWS file:
> >
> >    o   The S3 methods for getting and setting names of 
> "dist" objects
> >        have been removed (as they provided names with a different
> >        length from the "dist" object itself).
> >
> >(some side effects are fixed in 2.1.0-patched)
> >
> >  
> >
> 
> -- 
> --------------------------------------------------------------
> --------------------
> Prof. Angelo M. Mineo
> Dipartimento di Scienze Statistiche e Matematiche "S. Vianelli"
> Universit?? degli Studi di Palermo
> Viale delle Scienze
> 90128 Palermo
> url: http://dssm.unipa.it/elio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ripley at stats.ox.ac.uk  Thu May  5 14:56:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 May 2005 13:56:23 +0100 (BST)
Subject: [R] Help with R
In-Reply-To: <427A1A33.1050206@gmx.ch>
References: <BAY103-DAV8D5B53012E331584750C5AE190@phx.gbl>
	<x2r7gmm058.fsf@turmalin.kubism.ku.dk> <427A1A33.1050206@gmx.ch>
Message-ID: <Pine.LNX.4.61.0505051354070.5836@gannet.stats>

On Thu, 5 May 2005, Christoph Lehmann wrote:

>>> I heard that 'R' does not do a very good job at handling large 
>>> datasets, is this true?
>> 
> importing huge datasets in a data.frame with e.g. a subsequent step of 
> conversion of some columns into factors may lead into memory troubles 
> (probably due to memory overhead when building out factors). But we currently

Note that R's scan() does this very much more efficiently.  It is not fair 
to R to generalize from the method of a contributed package.

> succeeded in  importing 12 millions of data records stored in a MySQL 
> database, using RMySQL package.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitrijoe at yahoo.com.br  Thu May  5 15:07:28 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Thu, 5 May 2005 10:07:28 -0300
Subject: [R] creating names for regressios using the assign()
Message-ID: <002801c55173$6c18a310$ca00a8c0@thesahajamach>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050505/c687e953/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Thu May  5 15:17:00 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 05 May 2005 14:17:00 +0100 (BST)
Subject: [R] selecting maximum values
In-Reply-To: <Pine.LNX.4.44.0505041925580.23757-100000@reclus.nhh.no>
Message-ID: <XFMail.050505141700.Ted.Harding@nessie.mcc.ac.uk>

On 04-May-05 Roger Bivand wrote:
> On Wed, 4 May 2005, Sean Davis wrote:
> 
>> see ?aggregate.
> 
> Or maybe tapply, or its close relative, by:
> 
>> by(df, list(df$station, df$date), function(x) 
> +   x$row[which.max(x$chlorophyll)]) 
>: Ancona
>: 21/06/01
> [1] NA
> ------------------------------------------------------------ 
>: Castagneto
>: 21/06/01
> [1] 3
> ------------------------------------------------------------ 
>: Ancona
>: 23/06/01
> [1] 6
> ------------------------------------------------------------ 
>: Castagneto
>: 23/06/01
> [1] NA
> 
> since happily a row ID column was included in the data frame. Note that
> which.max only reports the row of the first maximum if there are ties.

I've tried to work out a method which gives a cleaner result
(for instance, the NAs are ugly and unnecessary).

I've called Alessandro's data (below) "chl" (for chlorophyll),
and using Roger's command above assign the result to "tmp":

tmp<-by(chl, list(chl$station, chl$date),
        function(x) x$row[which.max(x$chlorophyll)] )

Then, using either tmp[1:2,] or tmp[,1:2] we get

  tmp[,1:2]
  ##            21/06/01 23/06/01
  ## Ancona           NA        6
  ## Castagneto        3       NA

which is a better layout but still has the NAs.

It would be better to be able to get something like

  ## Ancona     23/06/01        6
  ## Castagneto 21/06/01        3

but I don't see how to do it even for just these 2 stations.

Now, however, suppose we want not just the rows but the values
as well. Try a modified function

  tmp<-by(chl, list(chl$station, chl$date),
          function(x) list(Row=x$row[which.max(x$chlorophyll)],
                           Val=max(x$chlorophyll))
         )

Now

  str(tmp)
  ## List of 4
  ##  $ : NULL
  ##  $ :List of 2
  ##   ..$ Row: int 3
  ##   ..$ Val: num 2.4
  ##  $ :List of 2
  ##   ..$ Row: int 6
  ##   ..$ Val: num 2.5
  ##  $ : NULL
  ##  - attr(*, "dim")= int [1:2] 2 2
  ##  - attr(*, "dimnames")=List of 2
  ##   ..$ : chr [1:2] "Ancona" "Castagneto"
  ##   ..$ : chr [1:2] "21/06/01" "23/06/01"
  ##  - attr(*, "call")= language by.data.frame(data = chl, INDICES =
  ##  list(chl$station, chl$date),      FUN = function(x) list(Row =
  ## x$row[which.max(x$chlorophyll)],  ...
  ##  - attr(*, "class")= chr "by"

I've not succeeded (though experience tells me that others could)
in extracting from this something like the following:

  ##        Ancona Castagneto 
  ##Row          6          3 
  ##Val        2.5        2.4 
  ##Date  23/06/01   21/06/01

Questions: (a) What's the trick? (b) How to generalise it?

Ted.

> 
>> 
>> Sean
>> 
>> On May 4, 2005, at 11:43 AM, alessandro carletti wrote:
>> 
>> > Sorry for disturbing you with another newbie question!
>> > I have a data frame about coastal waters quality
>> > parameters: for some parameters (e.g. NH3) I have only
>> > 1 observation for each sampling station and each
>> > sampling date, while in other cases (chlorophyll) I
>> > have 1 obs for each meter-depth for each station and
>> > date. How can I select only the max chlorophyll value
>> > for each station/date?
>> >
>> > example
>> >
>> > row  station         date        depth     chlorophyll
>> > 1     Castagneto      21/06/01     -0.5         2.0
>> > 2     Castagneto      21/06/01     -1.5         2.2
>> > 3     Castagneto      21/06/01     -2.5         2.4
>> > 4     Castagneto      21/06/01     -3.5         2.1
>> > 5     Ancona          23/06/01     -0.5         2.4
>> > 6     Ancona          23/06/01     -1.5         2.5
>> > 7     Ancona          23/06/01     -2.5         2.2
>> > 8     Ancona          23/06/01     -3.5         2.1
>> > 9     Ancona          23/06/01     -4.5         1.9
>> > ...
>> >
>> > I'd like to select only row 3 and 6, the ones with max
>> > chlorophyll values, or have the mean for the rows 1:4
>> > and 5:9
>> >
>> > Thanks


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-May-05                                       Time: 14:13:13
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Thu May  5 15:17:00 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 05 May 2005 14:17:00 +0100 (BST)
Subject: [R] Help with R
In-Reply-To: <x2r7gmm058.fsf@turmalin.kubism.ku.dk>
Message-ID: <XFMail.050505141700.Ted.Harding@nessie.mcc.ac.uk>

On 05-May-05 Peter Dalgaard wrote:
> [...]
> Both systems are victims of the curse of the rectangular data set to
> some extent. Prototypically, you record the sex of a rat along with
> every single measurement on it, as if the rat could change sex at
> millisecond resolution. This probably applies to all current
> statistical systems, but there is some hope that R's more flexible
> data structures can be leveraged to better handle multilevel data.
> (Cue Probabilistic Relational Models a.m. Getoor et al., which Peter
> Green brought up at the recent gR meeting.)

I would agree with this hope. Indeed I was reminded of the issue
by Alessandro Carletti's recent query about extracting features
from the data at different marine sampling stations.

My involvement goes back to the days (around 1980) when, with
Jan Bo??tius, I was examining Johannes Schmidt's data on eel larvae
obtained during his Atlantic cruises to investigate the "spawning
question" of the European eel (funded by the Carlsberg Foundation,
Peter!).

Each Cruise consisted of a series of Stations by a given Ship
at different Geographic positions, at each of which a number of Hauls
would be made in different Years and different Months on different
Days at different Times of day, using different Equipments and at
different Depths or ranges of Depth, and of different Durations,
and at different Speeds, resulting in capture of none or several
specimens each of which would be examined for length, numbers of
myomeres (muscle segments), and other features, along with hydrographic
measurements.

This could have been embodied in a huge "rectangular table" with of
course much repetition of all the information that remains constant
for each specimen in a haul. The specimen-specific data consisted of
only 2-4 items, while the "constant" data consisted of 12-15
items. There were nearly 20,000 larvae, so the "rectangular table"
could have occupied well over a Megabyte.

The alternative is a "list" representation, like:

Investigation = list(Cruises)
Cruise = list(Ship,list(Stations))
Station = list((Position,list(Hauls))
Haul = list((Year,Month,Day,Time,Duration,(Equipment data),(Depths),
            Speed,list(Specimens))
Specimen=list(Length,Myomeres,...)

In the end, the "list-like" view was the one adopted (I was limited
to CP/M BASIC in some 48K of free RAM, with 256KB floppies, in those
days), though not fully formally programmed (some of the "list
parsing" was done by hand, i.e. replacing one floppy with another),
though the BASIC program did retain the previously read data
for a given Station when reading in new Haul data, and the Haul
data when reading in Specimen data.

Later, when I began to study C, I realised that the language
was well adapted to implementing such structures in a program,
though by then following this up would have been motivated by
curiosity rather than needing to get the job done (it already
was done).

Now, in R, I see that in principle such data representations
are well integrated into the language, and I've been yet again
tempted to look at the question!

However, while representing the raw data in such a form is
well supported by R, it seems to me that extracting data
in a way adapted to different analyses requires users to
create their own methods, using the list-access primitives .

For example, to study the changes in the distribution of
lengths of specimens in relation to Position and Date
(which was one of the important issues in that investigation),
I don't think there are any "list processing" functions
available in R which, given the list-based structure described
above, would allow a simple query of the form

  means( Length , ~ Position:Date , data=Cruise )

It's quite feasible to write one's own; but I think Peter's
hope (expressed in excerpt above) looks like a first call
for thinking about general methods for this sort of thing.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-May-05                                       Time: 13:28:29
------------------------------ XFMail ------------------------------



From michael.watson at bbsrc.ac.uk  Thu May  5 15:33:11 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 5 May 2005 14:33:11 +0100
Subject: [R] Wilcoxon paired signed rank test and continuity correction
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172D3C8@iahce2knas1.iah.bbsrc.reserved>

Hi

I am applying the wilcoxon paired signed rank test to many sub-groups of
a data frame using by() and wilcox.test, and sometimes the test uses
continuity correction and sometimes it doesn't.  What I want to know is:

1) what is continuity correction?  The docs say:

 correct: a logical indicating whether to apply continuity correction
          in the normal approximation for the p-value.

But come on, I'm a biologist....

2) what makes wilcox.test sometimes decide to implement continuity
correction and sometimes not?

3) should I be letting wilcox.test use continuity correction when it
wants to?

I know I can turn this on/off using the correct and exact options, but I
want to know what is best to do.  Presently I have it turned off because
as I am doing ~200 tests, I want the tests to be consistent (ie none of
them use continuity correction rather than some of them do, some of them
don't)

Many thanks in advance for your replies :-)

Mick

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7990 827831
E-mail: michael.watson at bbsrc.ac.uk

"To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of." R.A. Fisher, 1938.



From fauna at pngp.it  Fri May  6 15:26:41 2005
From: fauna at pngp.it (Achaz von Hardenberg)
Date: Fri, 6 May 2005 15:26:41 +0200
Subject: [R] reordering a data.frame
Message-ID: <KH582.7835E57@pngp.it>

Hi,
I have a data.frame which looks like this:
  
id     est0     est1	est2	est3	est4	est5	est6    est7
 1   	1	2	3	1	7	9	3	4
 2	4	1	1	7	6	5	1	2  
[...]


I would like to reorder it to obtain the following:

id  	est	VALUE
1	0	1
1	1	2
1	2	3
1	3	1
1	4	7
1	5	9
1	6	3
1	7	4
2	0	4
2	1	1
2	2	1
2	3	7
2	4	6
2	5	5
2	6	1
2	7	2
[...]
	
Can anybody help me out?

Thanks in advance!

Achaz


Achaz von Hardenberg
--------------------------------------------------------------------------------------
Centro Studi Fauna Alpina - Alpine Wildlife Research Centre
Parco Nazionale Gran Paradiso, via della Rocca 47, 10123 Torino, Italy

e-mail: fauna at pngp.it
Tel. (office): +39.011.8606212
Tel. (mobile): +39.328.8736291
Fax: +39.011.8121305
--------------------------------------------------------------------------------------
Open access to all papers published in the Journal of Mountain Ecology:
http://www.mountainecology.org

GSE-AIESG (Gruppo Stambecco Europa - Alpine Ibex European Specialist Group):
http://www.gseonline.org



From perbak at tdcadsl.dk  Thu May  5 15:37:07 2005
From: perbak at tdcadsl.dk (Per Bak)
Date: Thu, 5 May 2005 15:37:07 +0200
Subject: [R] Exact quantile regression
Message-ID: <200505051537.07456.perbak@tdcadsl.dk>

Hi,

I have been returning to the same problem a number of times without success 
and now look for help with the following:

How do I fit a distribution function with the same number of parameters as 
there are quantiles and values such that I get an exact solution as opposed 
to a minimum least squares type solution? Say, which lognormal distribution 
has a 15% quantile of 10 and a 50% quantile of 30? My hope is that the 
solution to this problem can be expanded, such that I can fit three quantiles 
with the Generalized Weibull distribution (which has three parameters).

This is what I attempt without success:
=======================

library(stats)

Target <- data.frame(
	quantiles = c(0.15, 0.50),
	values = c(10, 30))

dist <- nls(values ~ qlnorm(quantiles, mu, sd), data = Target,
                 start = list(mu = 30, sd = 5))

I can see that it works with one more value:
==========================
Target <- data.frame(
	quantiles = c(0.15, 0.50, 0.85),
	values = c(10, 30, 60))

dist <- nls(values ~ qlnorm(quantiles, mu, sd), data = Target,
                 start = list(mu = 30, sd = 5))


Kind regards,

Per Bak
Copenhagen, Denmark



From elio_mineo at economia.unipa.it  Thu May  5 15:45:48 2005
From: elio_mineo at economia.unipa.it (Elio Mineo)
Date: Thu, 05 May 2005 15:45:48 +0200
Subject: [R] Solved: was names(dist(mat)) gives NULL in R 2.1.0
Message-ID: <427A238C.5010308@economia.unipa.it>

Thanks to Andy Liaw to suggest me the right solution (at least for me):

attributes(dist(mat))$Labels

or

attr(dist(mat),"Labels")

Bye,
Elio

-- 
----------------------------------------------------------------------------------
Prof. Angelo M. Mineo
Dipartimento di Scienze Statistiche e Matematiche "S. Vianelli"
Universit?? degli Studi di Palermo
Viale delle Scienze
90128 Palermo
url: http://dssm.unipa.it/elio



From ligges at statistik.uni-dortmund.de  Thu May  5 15:55:05 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 May 2005 15:55:05 +0200
Subject: [R] reordering a data.frame
In-Reply-To: <KH582.7835E57@pngp.it>
References: <KH582.7835E57@pngp.it>
Message-ID: <427A25B9.5060402@statistik.uni-dortmund.de>

See ?reshape

Uwe Ligges



Achaz von Hardenberg wrote:

> Hi,
> I have a data.frame which looks like this:
>   
> id     est0     est1	est2	est3	est4	est5	est6    est7
>  1   	1	2	3	1	7	9	3	4
>  2	4	1	1	7	6	5	1	2  
> [...]
> 
> 
> I would like to reorder it to obtain the following:
> 
> id  	est	VALUE
> 1	0	1
> 1	1	2
> 1	2	3
> 1	3	1
> 1	4	7
> 1	5	9
> 1	6	3
> 1	7	4
> 2	0	4
> 2	1	1
> 2	2	1
> 2	3	7
> 2	4	6
> 2	5	5
> 2	6	1
> 2	7	2
> [...]
> 	
> Can anybody help me out?
> 
> Thanks in advance!
> 
> Achaz
> 
> 
> Achaz von Hardenberg
> --------------------------------------------------------------------------------------
> Centro Studi Fauna Alpina - Alpine Wildlife Research Centre
> Parco Nazionale Gran Paradiso, via della Rocca 47, 10123 Torino, Italy
> 
> e-mail: fauna at pngp.it
> Tel. (office): +39.011.8606212
> Tel. (mobile): +39.328.8736291
> Fax: +39.011.8121305
> --------------------------------------------------------------------------------------
> Open access to all papers published in the Journal of Mountain Ecology:
> http://www.mountainecology.org
> 
> GSE-AIESG (Gruppo Stambecco Europa - Alpine Ibex European Specialist Group):
> http://www.gseonline.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu May  5 15:56:02 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 May 2005 15:56:02 +0200
Subject: [R] creating names for regressios using the assign()
In-Reply-To: <002801c55173$6c18a310$ca00a8c0@thesahajamach>
References: <002801c55173$6c18a310$ca00a8c0@thesahajamach>
Message-ID: <427A25F2.7020101@statistik.uni-dortmund.de>

Dimitri Joe wrote:

> Hi there. I have a data frame, X, with n+m columns. I want to regress each of the first n columns on the last m. This is what I am trying to do:
> 
> for ( i in 1:n )
> 
> assign(   paste ("reg",1:14,sep="")[i]  , lm(  X[,i]  ~ X[,i+1] + ... + X[,i+m],  data= X  )  )
> 
> It happens that some of the regressions, say the 3rd, seems to be a singular fit (or something else I don't know what ). I got the following error message:
> 
> Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
>         NA/NaN/Inf in foreign function call (arg 4)
> 
> Since it first happens with, say the 3rd regression, only the first two are computed. R does not run the 4th regression and so on. I don't want to necessarly remove the columns from the data frame, though, because n is large and there could be many singular fits, implying I had to remove many columns from the data frame. What should I do?


See ?try

Uwe Ligges



> Thanks,
> 
> Dimitri
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu May  5 16:02:32 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 May 2005 10:02:32 -0400
Subject: [R] reordering a data.frame
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076ED2@usctmx1106.merck.com>

Use reshape(), as:

> dat
  id est0 est1 est2 est3 est4 est5 est6 est7
1  1    1    2    3    1    7    9    3    4
2  2    4    1    1    7    6    5    1    2
> dat2 <- reshape(dat, varying=list(names(dat)[-1]), times=0:7,
direction="long")
> dat2
    id time est0
1.0  1    0    1
2.0  2    0    4
1.1  1    1    2
2.1  2    1    1
1.2  1    2    3
2.2  2    2    1
1.3  1    3    1
2.3  2    3    7
1.4  1    4    7
2.4  2    4    6
1.5  1    5    9
2.5  2    5    5
1.6  1    6    3
2.6  2    6    1
1.7  1    7    4
2.7  2    7    2

Andy 

> From: Achaz von Hardenberg
> 
> Hi,
> I have a data.frame which looks like this:
>   
> id     est0     est1	est2	est3	est4	est5	est6    est7
>  1   	1	2	3	1	7	9	3	4
>  2	4	1	1	7	6	5	1	2  
> [...]
> 
> 
> I would like to reorder it to obtain the following:
> 
> id  	est	VALUE
> 1	0	1
> 1	1	2
> 1	2	3
> 1	3	1
> 1	4	7
> 1	5	9
> 1	6	3
> 1	7	4
> 2	0	4
> 2	1	1
> 2	2	1
> 2	3	7
> 2	4	6
> 2	5	5
> 2	6	1
> 2	7	2
> [...]
> 	
> Can anybody help me out?
> 
> Thanks in advance!
> 
> Achaz
> 
> 
> Achaz von Hardenberg
> --------------------------------------------------------------
> ------------------------
> Centro Studi Fauna Alpina - Alpine Wildlife Research Centre
> Parco Nazionale Gran Paradiso, via della Rocca 47, 10123 Torino, Italy
> 
> e-mail: fauna at pngp.it
> Tel. (office): +39.011.8606212
> Tel. (mobile): +39.328.8736291
> Fax: +39.011.8121305
> --------------------------------------------------------------
> ------------------------
> Open access to all papers published in the Journal of 
> Mountain Ecology:
> http://www.mountainecology.org
> 
> GSE-AIESG (Gruppo Stambecco Europa - Alpine Ibex European 
> Specialist Group):
> http://www.gseonline.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From r.hankin at noc.soton.ac.uk  Thu May  5 16:07:35 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 5 May 2005 15:07:35 +0100
Subject: [R] reordering a data.frame
In-Reply-To: <KH582.7835E57@pngp.it>
References: <KH582.7835E57@pngp.it>
Message-ID: <727a17fba7db12bd2c50a5f9fa492229@soc.soton.ac.uk>

Hi



R> d <-  
data.frame(id=1:3,est0=c(1,10,100),est1=c(6,7,8),est2=c(-5,-5,-5))

R> d
   id est0 est1 est2
1  1    1    6   -5
2  2   10    7   -5
3  3  100    8   -5

R> out <- as.vector(t(d[,-1]))
R>   
cbind(id=rep(d$id,each=nrow(d)),est=as.vector(row(as.matrix(d[, 
-1]))),out)
       id est out
  [1,]  1   1   1
  [2,]  1   2   6
  [3,]  1   3  -5
  [4,]  2   1  10
  [5,]  2   2   7
  [6,]  2   3  -5
  [7,]  3   1 100
  [8,]  3   2   8
  [9,]  3   3  -5
R>

Hope this helps

rksh



On May 6, 2005, at 02:26 pm, Achaz von Hardenberg wrote:

> Hi,
> I have a data.frame which looks like this:
>
> id     est0     est1	est2	est3	est4	est5	est6    est7
>  1   	1	2	3	1	7	9	3	4
>  2	4	1	1	7	6	5	1	2
> [...]
>
>
> I would like to reorder it to obtain the following:
>
> id  	est	VALUE
> 1	0	1
> 1	1	2
> 1	2	3
> 1	3	1
> 1	4	7
> 1	5	9
> 1	6	3
> 1	7	4
> 2	0	4
> 2	1	1
> 2	2	1
> 2	3	7
> 2	4	6
> 2	5	5
> 2	6	1
> 2	7	2
> [...]
> 	
> Can anybody help me out?
>
> Thanks in advance!
>
> Achaz
>
>
> Achaz von Hardenberg
> ----------------------------------------------------------------------- 
> ---------------
> Centro Studi Fauna Alpina - Alpine Wildlife Research Centre
> Parco Nazionale Gran Paradiso, via della Rocca 47, 10123 Torino, Italy
>
> e-mail: fauna at pngp.it
> Tel. (office): +39.011.8606212
> Tel. (mobile): +39.328.8736291
> Fax: +39.011.8121305
> ----------------------------------------------------------------------- 
> ---------------
> Open access to all papers published in the Journal of Mountain Ecology:
> http://www.mountainecology.org
>
> GSE-AIESG (Gruppo Stambecco Europa - Alpine Ibex European Specialist  
> Group):
> http://www.gseonline.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From MSchwartz at MedAnalytics.com  Thu May  5 16:22:30 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 05 May 2005 09:22:30 -0500
Subject: [R] reordering a data.frame
In-Reply-To: <KH582.7835E57@pngp.it>
References: <KH582.7835E57@pngp.it>
Message-ID: <1115302950.12345.20.camel@horizons.localdomain>

On Fri, 2005-05-06 at 15:26 +0200, Achaz von Hardenberg wrote:
> Hi,
> I have a data.frame which looks like this:
>   
> id     est0     est1	est2	est3	est4	est5	est6    est7
>  1   	1	2	3	1	7	9	3	4
>  2	4	1	1	7	6	5	1	2  
> [...]
> 
> 
> I would like to reorder it to obtain the following:
> 
> id  	est	VALUE
> 1	0	1
> 1	1	2
> 1	2	3
> 1	3	1
> 1	4	7
> 1	5	9
> 1	6	3
> 1	7	4
> 2	0	4
> 2	1	1
> 2	2	1
> 2	3	7
> 2	4	6
> 2	5	5
> 2	6	1
> 2	7	2
> [...]
> 	
> Can anybody help me out?
> 
> Thanks in advance!
> 
> Achaz

Here is one approach, with your data in 'df' from read.table():

# use reshape()
df.new <- reshape(df, direction = "long", idvar = "id", 
                  varying = list(names(df[2:9])), times = 0:7)

# use order() for row sequence
> df.new <- df.new[order(df.new$id, df.new$time), ]

# set colnames as above
> colnames(df.new) <- c("id", "est", "VALUE")

> df.new
    id est VALUE
1.1  1   0     1
1.2  1   1     2
1.3  1   2     3
1.4  1   3     1
1.5  1   4     7
1.6  1   5     9
1.7  1   6     3
1.8  1   7     4
2.1  2   0     4
2.2  2   1     1
2.3  2   2     1
2.4  2   3     7
2.5  2   4     6
2.6  2   5     5
2.7  2   6     1
2.8  2   7     2


Note that the rownames above are sequenced from the original 1:2.

See ?reshape for more information.

HTH,

Marc Schwartz



From blindglobe at gmail.com  Thu May  5 16:27:27 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Thu, 5 May 2005 16:27:27 +0200
Subject: [R] Classes and methods
In-Reply-To: <200505032039.j43KdEF1027841@erdos.math.unb.ca>
References: <200505032039.j43KdEF1027841@erdos.math.unb.ca>
Message-ID: <1abe3fa905050507276abb509d@mail.gmail.com>

On 5/3/05, Rolf Turner <rolf at math.unb.ca> wrote:
> Sean Davis wrote:
> 
> > I just did some of this learning myself.  Here are a couple of links
> > that I found useful:
> >
> > http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/S4Objects.pdf
> > http://eeyore.ucdavis.edu/stat250/OOP.html
> >
> > I found the first particularly easy reading and it got me going quickly
> > with S4 methods, which it seems to me are the way to go in most cases.
> 
>        If you want to simultaneously handcuff yourself, strap
>        yourself into a strait jacket, and tie yourself in knots, and
>        moreover write code which is incomprehensible to the human
>        mind, then S4 methods are indeed the way to go.

Which humans?  I know quite a few for which the above isn't true.


best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From ggrothendieck at gmail.com  Thu May  5 16:36:27 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 5 May 2005 10:36:27 -0400
Subject: [R] Help with R
In-Reply-To: <XFMail.050505141700.Ted.Harding@nessie.mcc.ac.uk>
References: <x2r7gmm058.fsf@turmalin.kubism.ku.dk>
	<XFMail.050505141700.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <971536df05050507365e9f48a2@mail.gmail.com>

On 5/5/05, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
> On 05-May-05 Peter Dalgaard wrote:
> > [...]
> > Both systems are victims of the curse of the rectangular data set to
> > some extent. Prototypically, you record the sex of a rat along with
> > every single measurement on it, as if the rat could change sex at
> > millisecond resolution. This probably applies to all current
> > statistical systems, but there is some hope that R's more flexible
> > data structures can be leveraged to better handle multilevel data.
> > (Cue Probabilistic Relational Models a.m. Getoor et al., which Peter
> > Green brought up at the recent gR meeting.)
> 
> I would agree with this hope. Indeed I was reminded of the issue
> by Alessandro Carletti's recent query about extracting features
> from the data at different marine sampling stations.
> 
> My involvement goes back to the days (around 1980) when, with
> Jan Bo??tius, I was examining Johannes Schmidt's data on eel larvae
> obtained during his Atlantic cruises to investigate the "spawning
> question" of the European eel (funded by the Carlsberg Foundation,
> Peter!).
> 
> Each Cruise consisted of a series of Stations by a given Ship
> at different Geographic positions, at each of which a number of Hauls
> would be made in different Years and different Months on different
> Days at different Times of day, using different Equipments and at
> different Depths or ranges of Depth, and of different Durations,
> and at different Speeds, resulting in capture of none or several
> specimens each of which would be examined for length, numbers of
> myomeres (muscle segments), and other features, along with hydrographic
> measurements.
> 
> This could have been embodied in a huge "rectangular table" with of
> course much repetition of all the information that remains constant
> for each specimen in a haul. The specimen-specific data consisted of
> only 2-4 items, while the "constant" data consisted of 12-15
> items. There were nearly 20,000 larvae, so the "rectangular table"
> could have occupied well over a Megabyte.
> 
> The alternative is a "list" representation, like:
> 
> Investigation = list(Cruises)
> Cruise = list(Ship,list(Stations))
> Station = list((Position,list(Hauls))
> Haul = list((Year,Month,Day,Time,Duration,(Equipment data),(Depths),
>            Speed,list(Specimens))
> Specimen=list(Length,Myomeres,...)
> 
> In the end, the "list-like" view was the one adopted (I was limited
> to CP/M BASIC in some 48K of free RAM, with 256KB floppies, in those
> days), though not fully formally programmed (some of the "list
> parsing" was done by hand, i.e. replacing one floppy with another),
> though the BASIC program did retain the previously read data
> for a given Station when reading in new Haul data, and the Haul
> data when reading in Specimen data.
> 
> Later, when I began to study C, I realised that the language
> was well adapted to implementing such structures in a program,
> though by then following this up would have been motivated by
> curiosity rather than needing to get the job done (it already
> was done).
> 
> Now, in R, I see that in principle such data representations
> are well integrated into the language, and I've been yet again
> tempted to look at the question!
> 
> However, while representing the raw data in such a form is
> well supported by R, it seems to me that extracting data
> in a way adapted to different analyses requires users to
> create their own methods, using the list-access primitives .
> 
> For example, to study the changes in the distribution of
> lengths of specimens in relation to Position and Date
> (which was one of the important issues in that investigation),
> I don't think there are any "list processing" functions
> available in R which, given the list-based structure described
> above, would allow a simple query of the form
> 
>  means( Length , ~ Position:Date , data=Cruise )
> 
> It's quite feasible to write one's own; but I think Peter's
> hope (expressed in excerpt above) looks like a first call
> for thinking about general methods for this sort of thing.
> 

The Green Book defines a recursive apply function, rapply,
that provides a general means of traversing that
sort of structure.



From aoganyan at niss.org  Thu May  5 16:42:00 2005
From: aoganyan at niss.org (Anna Oganyan)
Date: Thu, 05 May 2005 10:42:00 -0400
Subject: [R] body of non-visible function
Message-ID: <427A30B8.70705@niss.org>

Hello,
Is there any possibility in R to see the body of the ìnon-visibleî 
function, for
example ìprincompî?
If I do :

 > methods(princomp)

so, I get that princomp.default and princomp.formula are non-visible 
functions and
body(princomp.default) doesnít show it.

In particular, I guess I have a very naÅÔve questionÖ
Iíd like to see how scores calculation is implemented in the function
princomp. Because when I multiply my data matrix on the matrix of loadings
 >data.matrix %*% princomp(data.matrix, scores=T)$loadings

I get different result than just doing

 >princomp(data.matrix, scores=T)$scores.

Thanks.
Anna



From efg at stowers-institute.org  Thu May  5 16:42:34 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 5 May 2005 09:42:34 -0500
Subject: [R] Imputation
References: <20050504111334.9089.qmail@webmail46.rediffmail.com>
Message-ID: <d5db09$ftm$1@sea.gmane.org>

> "Ramesh Kolluru" <stat_ramesh at rediffmail.com> wrote in message
news:20050504111334.9089.qmail at webmail46.rediffmail.com...
> Please give me some way to get the data with imputed values.

You might consider this (see steps 10 and 11):

loess Smoothing and Data Imputation

http://research.stowers-institute.org/efg/R/Statistics/loess.htm
efg
--
Earl F. Glynn
Stowers Institute for Medical Research



From greg.snow at ihc.com  Thu May  5 17:07:01 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Thu, 05 May 2005 09:07:01 -0600
Subject: [R] Exact quantile regression
Message-ID: <s279e246.019@lp-msg1.co.ihc.com>

Here is an approach using optim:

tmpfunc <- function(param){
	ml<-param[1]
	sl<-param[2]
	(qlnorm(.15,ml,sl)-10)^2 + (qlnorm(.5,ml,sl)-30)^2
}

res <- optim(c(1,2), tmpfunc)
res <- optim(res$par, tmpfunc)
res

hope this helps,

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> Per Bak <perbak at tdcadsl.dk> 05/05/05 07:37AM >>>
Hi,

I have been returning to the same problem a number of times without
success 
and now look for help with the following:

How do I fit a distribution function with the same number of parameters
as 
there are quantiles and values such that I get an exact solution as
opposed 
to a minimum least squares type solution? Say, which lognormal
distribution 
has a 15% quantile of 10 and a 50% quantile of 30? My hope is that the

solution to this problem can be expanded, such that I can fit three
quantiles 
with the Generalized Weibull distribution (which has three
parameters).

This is what I attempt without success:
=======================

library(stats)

Target <- data.frame(
	quantiles = c(0.15, 0.50),
	values = c(10, 30))

dist <- nls(values ~ qlnorm(quantiles, mu, sd), data = Target,
                 start = list(mu = 30, sd = 5))

I can see that it works with one more value:
==========================
Target <- data.frame(
	quantiles = c(0.15, 0.50, 0.85),
	values = c(10, 30, 60))

dist <- nls(values ~ qlnorm(quantiles, mu, sd), data = Target,
                 start = list(mu = 30, sd = 5))


Kind regards,

Per Bak
Copenhagen, Denmark

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu May  5 17:16:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 May 2005 16:16:55 +0100 (BST)
Subject: [R] Solved: was names(dist(mat)) gives NULL in R 2.1.0
In-Reply-To: <427A238C.5010308@economia.unipa.it>
References: <427A238C.5010308@economia.unipa.it>
Message-ID: <Pine.LNX.4.61.0505051616020.10592@gannet.stats>

On Thu, 5 May 2005, Elio Mineo wrote:

> Thanks to Andy Liaw to suggest me the right solution (at least for me):
>
> attributes(dist(mat))$Labels
>
> or
>
> attr(dist(mat),"Labels")

R 2.2.0 will allow labels(dist_object), which seem natural (and unlike 
names promises nothing about the returned object).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Thu May  5 17:24:02 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 May 2005 11:24:02 -0400
Subject: [R] body of non-visible function
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076ED5@usctmx1106.merck.com>

> From: Anna Oganyan
> 
> Hello,
> Is there any possibility in R to see the body of the "non-visible" 
> function, for
> example "princomp"?
> If I do :
> 
>  > methods(princomp)
> 
> so, I get that princomp.default and princomp.formula are non-visible 
> functions and
> body(princomp.default) doesn't show it.

You may want to learn about getAnywhere() and getS3method().

 
> In particular, I guess I have a very na??ve question...
> I'd like to see how scores calculation is implemented in the function
> princomp. Because when I multiply my data matrix on the 
> matrix of loadings
>  >data.matrix %*% princomp(data.matrix, scores=T)$loadings
> 
> I get different result than just doing
> 
>  >princomp(data.matrix, scores=T)$scores.

Variables are centered and scaled, so you want to do

  scale(data.matrix) %*% loadings(princomp(data.matrix))

Andy

 
> Thanks.
> Anna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ripley at stats.ox.ac.uk  Thu May  5 17:28:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 May 2005 16:28:40 +0100 (BST)
Subject: [R] body of non-visible function
In-Reply-To: <427A30B8.70705@niss.org>
References: <427A30B8.70705@niss.org>
Message-ID: <Pine.LNX.4.61.0505051625410.10592@gannet.stats>

getS3method("princomp", "default").
getAnywhere("princomp.default")

The first of these is in the See Also of ?methods.

On Thu, 5 May 2005, Anna Oganyan wrote:

> Hello,
> Is there any possibility in R to see the body of the 
non-visible
 
> function, for
> example 
princomp
?
> If I do :
>
>> methods(princomp)
>
> so, I get that princomp.default and princomp.formula are non-visible 
> functions and
> body(princomp.default) doesnt show it.
>
> In particular, I guess I have a very na?ve question&
> Id like to see how scores calculation is implemented in the function
> princomp. Because when I multiply my data matrix on the matrix of loadings
>> data.matrix %*% princomp(data.matrix, scores=T)$loadings
>
> I get different result than just doing
>
>> princomp(data.matrix, scores=T)$scores.

You have forgotten to centre your data.  It may be more helpful to look at 
the predict method.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From sundar.dorai-raj at pdf.com  Thu May  5 17:30:30 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 05 May 2005 08:30:30 -0700
Subject: [R] body of non-visible function
In-Reply-To: <427A30B8.70705@niss.org>
References: <427A30B8.70705@niss.org>
Message-ID: <427A3C16.2040801@pdf.com>



Anna Oganyan wrote on 5/5/2005 7:42 AM:
> Hello,
> Is there any possibility in R to see the body of the ìnon-visibleî 
> function, for
> example ìprincompî?
> If I do :
> 
>  > methods(princomp)
> 
> so, I get that princomp.default and princomp.formula are non-visible 
> functions and
> body(princomp.default) doesnít show it.
> 
> In particular, I guess I have a very naÅÔve questionÖ
> Iíd like to see how scores calculation is implemented in the function
> princomp. Because when I multiply my data matrix on the matrix of loadings
>  >data.matrix %*% princomp(data.matrix, scores=T)$loadings
> 
> I get different result than just doing
> 
>  >princomp(data.matrix, scores=T)$scores.
> 
> Thanks.
> Anna
> 


Hi Anna,

Use getAnywhere("princomp.default") or stats:::princomp.default. This 
has to do with princomp.default not being exported in the stats package 
NAMESPACE.

As for the difference, the scores are based on centering the columns of 
data.matrix before determining the principal components. For example:

 > X <- as.matrix(USArrests)
 > Xc <- sweep(X, 2, colMeans(X), "-")
 > pc <- princomp(X, scores = TRUE) # inappropriate, see ?princomp
 > str(pc$scores)
  num [1:50, 1:4]  -64.8  -92.8 -124.1  -18.3 -107.4 ...
  - attr(*, "dimnames")=List of 2
   ..$ : chr [1:50] "Alabama" "Alaska" "Arizona" "Arkansas" ...
   ..$ : chr [1:4] "Comp.1" "Comp.2" "Comp.3" "Comp.4"
 > Xl <- Xc %*% loadings(pc)
 > str(Xl)
  num [1:50, 1:4]  -64.8  -92.8 -124.1  -18.3 -107.4 ...
  - attr(*, "dimnames")=List of 2
   ..$ : chr [1:50] "Alabama" "Alaska" "Arizona" "Arkansas" ...
   ..$ : chr [1:4] "Comp.1" "Comp.2" "Comp.3" "Comp.4"
 > sum(abs(Xl - pc$scores))
[1] 4.97999e-12
 >

Finally, "data.matrix" is a function in the base package. I would avoid 
using it as a variable name.

--sundar



From pieterprovoost at gmail.com  Thu May  5 17:43:26 2005
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Thu, 5 May 2005 17:43:26 +0200
Subject: [R] some pairs() questions
References: <x2r7gmm058.fsf@turmalin.kubism.ku.dk><XFMail.050505141700.Ted.Harding@nessie.mcc.ac.uk>
	<971536df05050507365e9f48a2@mail.gmail.com>
Message-ID: <003501c55189$2e544050$9fa4c19d@nioo.int>

I have a few problems with pairs(). I want to add the variables names to the
plot, but I don't know how. I tried different things with the labels
argument but nothing worked out. This is the code:

read.table("bay1.bay",header=T)->Rdata
(histogram part skipped)
pairs(Rdata,
pch='.',diag.panel=panel.hist,labels=NULL,gap=0,xaxt="n",yaxt="n",upper.pane
l=NULL)

The other problem is that I want to "highlight" one or more of the records
of Rdata (give it another symbol and another color). What's the easiest way
to do this? The record to be highlighted should be selected based on the
value of one of the variables (smaller than 2.8 is this case).

Many thanks in advance,
Pieter Provoost



From ripley at stats.ox.ac.uk  Thu May  5 17:54:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 May 2005 16:54:17 +0100 (BST)
Subject: [R] body of non-visible function
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076ED5@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076ED5@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.61.0505051649510.21164@gannet.stats>

On Thu, 5 May 2005, Liaw, Andy wrote:

>> From: Anna Oganyan
>>
>> Hello,
>> Is there any possibility in R to see the body of the "non-visible"
>> function, for
>> example "princomp"?
>> If I do :
>>
>> > methods(princomp)
>>
>> so, I get that princomp.default and princomp.formula are non-visible
>> functions and
>> body(princomp.default) doesn't show it.
>
> You may want to learn about getAnywhere() and getS3method().
>
>
>> In particular, I guess I have a very na?ve question...
>> I'd like to see how scores calculation is implemented in the function
>> princomp. Because when I multiply my data matrix on the
>> matrix of loadings
>> >data.matrix %*% princomp(data.matrix, scores=T)$loadings
>>
>> I get different result than just doing
>>
>> >princomp(data.matrix, scores=T)$scores.
>
> Variables are centered and scaled, so you want to do
>
>  scale(data.matrix) %*% loadings(princomp(data.matrix))

Only centred.  Had this been princomp(data.matrix, cor=TRUE, scores=TRUE)
they would have been scaled.  It is better to follow my hint of looking at
predict.princomp:

     object <- princomp(data.matrix)
     scale(data.matrix, object$center, object$scale) %*% object$loadings

BTW, data.matrix is an R function, so not a good choice of name.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ggrothendieck at gmail.com  Thu May  5 18:00:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 5 May 2005 12:00:40 -0400
Subject: [R] reordering a data.frame
In-Reply-To: <427A25B9.5060402@statistik.uni-dortmund.de>
References: <KH582.7835E57@pngp.it>
	<427A25B9.5060402@statistik.uni-dortmund.de>
Message-ID: <971536df0505050900716b65e0@mail.gmail.com>

On 5/5/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> See ?reshape
> 
> Uwe Ligges
> 
> 
> Achaz von Hardenberg wrote:
> 
> > Hi,
> > I have a data.frame which looks like this:
> >
> > id     est0     est1  est2    est3    est4    est5    est6    est7
> >  1    1       2       3       1       7       9       3       4
> >  2    4       1       1       7       6       5       1       2
> > [...]
> >
> >
> > I would like to reorder it to obtain the following:
> >
> > id    est     VALUE
> > 1     0       1
> > 1     1       2
> > 1     2       3
> > 1     3       1
> > 1     4       7
> > 1     5       9
> > 1     6       3
> > 1     7       4
> > 2     0       4
> > 2     1       1
> > 2     2       1
> > 2     3       7
> > 2     4       6
> > 2     5       5
> > 2     6       1
> > 2     7       2
> > [...]


I would probably use reshape as others have already pointed out
but just for the fun of it note that if you convert it from a data frame
to a matrix to a table to a data frame in this order:

  data.frame ==> matrix ==> table ==> data.frame

We can get the result by using matrix() followed by as.data.frame.table()
(which converts it to table and then data frame all in one).  The
one feature of this solution is that each step is relatively simple.

I assume the row and column ordering is important, given the Subject, 
but if it is not then the second line can be simplified to 
just:   as.data.frame.table(mat, resp = "VALUE")


> mat <- matrix(unlist(dat[,-1]), nrow(dat), dimnames = list(id = dat$id, est = 0:7))
> as.data.frame.table(t(mat), resp = "VALUE")[c(2:1,3)]
   id est VALUE
1   1   0     1
2   1   1     2
3   1   2     3
4   1   3     1
5   1   4     7
6   1   5     9
7   1   6     3
8   1   7     4
9   2   0     4
10  2   1     1
11  2   2     1
12  2   3     7
13  2   4     6
14  2   5     5
15  2   6     1
16  2   7     2



From gunter.berton at gene.com  Thu May  5 18:40:15 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 5 May 2005 09:40:15 -0700
Subject: [R] some pairs() questions
In-Reply-To: <003501c55189$2e544050$9fa4c19d@nioo.int>
Message-ID: <200505051640.j45GeFkN018656@ohm.gene.com>

(Lazy answer, not checking your code in detail)

Try using splom() in the lattice package, as it gives more extensive control
to do the sorts of things you seem to want.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pieter Provoost
> Sent: Thursday, May 05, 2005 8:43 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] some pairs() questions
> 
> I have a few problems with pairs(). I want to add the 
> variables names to the
> plot, but I don't know how. I tried different things with the labels
> argument but nothing worked out. This is the code:
> 
> read.table("bay1.bay",header=T)->Rdata
> (histogram part skipped)
> pairs(Rdata,
> pch='.',diag.panel=panel.hist,labels=NULL,gap=0,xaxt="n",yaxt=
"n",upper.pane
> l=NULL)
> 
> The other problem is that I want to "highlight" one or more 
> of the records
> of Rdata (give it another symbol and another color). What's 
> the easiest way
> to do this? The record to be highlighted should be selected 
> based on the
> value of one of the variables (smaller than 2.8 is this case).
> 
> Many thanks in advance,
> Pieter Provoost
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ccleland at optonline.net  Thu May  5 18:54:23 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 05 May 2005 12:54:23 -0400
Subject: [R] Using add1() as the FUN argument to by()
Message-ID: <427A4FBF.7010104@optonline.net>

Why does the following not work?

by(warpbreaks, warpbreaks$tension, function(x) add1(lm(breaks ~ 1, 
data=x), ~ . + wool))

Error in inherits(x, "data.frame") : Object "x" not found

Is there another efficient way to apply add1() to subsets of a data frame?

Thanks.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From deepayan at stat.wisc.edu  Thu May  5 19:28:36 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 5 May 2005 12:28:36 -0500
Subject: [R] Plotting means and confidence intervals by group factor using
	lattice graphics?
In-Reply-To: <4278EA7C.9030301@oomvanlieshout.net>
References: <4278EA7C.9030301@oomvanlieshout.net>
Message-ID: <200505051228.36116.deepayan@stat.wisc.edu>

On Wednesday 04 May 2005 10:30, Sander Oom wrote:
> Dear R graphics gurus,
>
> Another question about lattice graphics. This time I would like to plot
> means and confidence intervals by group factor in a lattice graph. I can
> not find any working lattice examples. Maybe a custom panel function is
> the answer, but that is a bit beyond me for now.

There's an example in this thread:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/50299.html

There might be useful tools in the Hmisc package as well.

> The individual plots within the lattice graph could look like this:
>
> # Example with confidence intervals and grid
> hh <- t(VADeaths)[, 5:1]
> mybarcol <- "gray20"
> ci.l <- hh * 0.85
> ci.u <- hh * 1.15
> mp <- barplot2(hh, beside = TRUE,
>          col = c("lightblue", "mistyrose",
>                  "lightcyan", "lavender"),
>          legend = colnames(VADeaths), ylim = c(0, 100),
>          main = "Death Rates in Virginia", font.main = 4,
>          sub = "Faked 95 percent error bars", col.sub = mybarcol,
>          cex.names = 1.5, plot.ci = TRUE, ci.l = ci.l, ci.u = ci.u,
>          plot.grid = TRUE)

This gives me

Error: couldn't find function "barplot2"

Maybe you missed a library() call?

Deepayan


> mtext(side = 1, at = colMeans(mp), line = -2,
>        text = paste("Mean", formatC(colMeans(hh))), col = "red")
> box()
>
> Or like this:
>
> data(state)
> plotmeans(state.area ~ state.region)
>
> Both plotmeans and barplot2 give interesting options such as printing of
> nobs, among other things. In case of a barplot, there should be an
> option to plot the confidence intervals in one direction only (up) as to
> avoid interference with any black and white shading. The plotMeans
> function provides a useful option error.bars ("se", "sd", "conf.int",
> "none").
>
> The following test data is still useful:
>
> tmp <- expand.grid(geology = c("Sand","Clay","Silt","Rock"),
>    species =
> c("ArisDiff","BracSera","CynDact","ElioMuti","EragCurS","EragPseu"),
>    dist = seq(1,9,1) )
> tmp$height <- rnorm(216)
>
> For instance plotting height versus dist by geology.
>
> Any help very welcome!
>
> Cheers,
>
> Sander.
>
> PS Of course the resulting graph will go to the R graph gallery!



From franciscofuentes2005 at yahoo.co.uk  Thu May  5 20:01:43 2005
From: franciscofuentes2005 at yahoo.co.uk (Francisco Fuentes)
Date: Thu, 5 May 2005 19:01:43 +0100 (BST)
Subject: [R] Precision in R
Message-ID: <20050505180144.30550.qmail@web26905.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050505/0a08efff/attachment.pl

From sms13+ at pitt.edu  Thu May  5 20:05:58 2005
From: sms13+ at pitt.edu (sms13+@pitt.edu)
Date: Thu, 05 May 2005 14:05:58 -0400
Subject: [R] efficient filtering of matrices
Message-ID: <698218093.1115301958@Lab26.DOMAIN.IE.PITT.EDU>

I was wondering if someone can tell me the best way to search through a 
matrix and choose certain rows (based on certain conditions) to put into a 
separate matrix.
What I have tried so far is very slow for a large dataset I'm working with. 
e.g., I have this piece of code to create a new matrix (newmat) based on my 
filtering conditions.  Do I need to do this kind of thing where I keep 
rbinding?

newmat<-rep(NA,12)
for (i in 1:length(origmat[,1])
{
	if (  is.na(origmat[i,10])  |  (!is.na(origmat[i,10]) & (origmat[i,2] <= 
origmat[i,10]) )   )
		newmat<-rbind(newmat, origmat[i,])
}


Thanks,
Steven



From pieterprovoost at gmail.com  Thu May  5 20:06:32 2005
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Thu, 5 May 2005 20:06:32 +0200
Subject: [R] some pairs() questions
References: <200505051640.j45GeFkN018656@ohm.gene.com>
Message-ID: <008901c5519d$2bea7c30$9fa4c19d@nioo.int>

I'm sure it's possible to display the variable names by making a small
change to the code. Someone did this last week but I can't contact this
person now to ask how. I tried this

vars <- names(Rdata)
pairs(Rdata, labels=vars,...)

but that doesn't work (formal argument "labels" matched by multiple actual
arguments). Unfortunately I don't have the time right now to get to know new
packages and write new scripts...

Thanks
Pieter

----- Original Message -----
From: "Berton Gunter" <gunter.berton at gene.com>
To: "'Pieter Provoost'" <pieterprovoost at gmail.com>;
<r-help at stat.math.ethz.ch>
Sent: Thursday, May 05, 2005 6:40 PM
Subject: RE: [R] some pairs() questions


> (Lazy answer, not checking your code in detail)
>
> Try using splom() in the lattice package, as it gives more extensive
control
> to do the sorts of things you seem to want.
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pieter Provoost
> > Sent: Thursday, May 05, 2005 8:43 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] some pairs() questions
> >
> > I have a few problems with pairs(). I want to add the
> > variables names to the
> > plot, but I don't know how. I tried different things with the labels
> > argument but nothing worked out. This is the code:
> >
> > read.table("bay1.bay",header=T)->Rdata
> > (histogram part skipped)
> > pairs(Rdata,
> > pch='.',diag.panel=panel.hist,labels=NULL,gap=0,xaxt="n",yaxt=
> "n",upper.pane
> > l=NULL)
> >
> > The other problem is that I want to "highlight" one or more
> > of the records
> > of Rdata (give it another symbol and another color). What's
> > the easiest way
> > to do this? The record to be highlighted should be selected
> > based on the
> > value of one of the variables (smaller than 2.8 is this case).
> >
> > Many thanks in advance,
> > Pieter Provoost
> >



From sghosh at lexgen.com  Thu May  5 20:10:06 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Thu, 5 May 2005 13:10:06 -0500
Subject: [R] Need some quick help with lattice - barchart
Message-ID: <2B47B68F97330841AC8C670749084A7D06C497@wdexchmb01.lexicon.lexgen.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050505/df9938a0/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu May  5 20:18:19 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 May 2005 20:18:19 +0200
Subject: [R] efficient filtering of matrices
In-Reply-To: <698218093.1115301958@Lab26.DOMAIN.IE.PITT.EDU>
References: <698218093.1115301958@Lab26.DOMAIN.IE.PITT.EDU>
Message-ID: <427A636B.8010003@statistik.uni-dortmund.de>

sms13+ at pitt.edu wrote:

> I was wondering if someone can tell me the best way to search through a 
> matrix and choose certain rows (based on certain conditions) to put into 
> a separate matrix.
> What I have tried so far is very slow for a large dataset I'm working 
> with. e.g., I have this piece of code to create a new matrix (newmat) 
> based on my filtering conditions.  Do I need to do this kind of thing 
> where I keep rbinding?
> 
> newmat<-rep(NA,12)
> for (i in 1:length(origmat[,1])
> {
>     if (  is.na(origmat[i,10])  |  (!is.na(origmat[i,10]) & 
> (origmat[i,2] <= origmat[i,10]) )   )
>         newmat<-rbind(newmat, origmat[i,])
> }


What about vectorizing the stuff?

   index <- is.na(origmat[ ,10]) |
     (!is.na(origmat[ ,10]) &
       (origmat[ ,2] <= origmat[ ,10]))
   newmat <- origmat[index,]

Uwe Ligges


> 
> Thanks,
> Steven
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Thu May  5 20:18:50 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 5 May 2005 14:18:50 -0400
Subject: [R] Precision in R
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9415@uswpmx00.merck.com>

The summands can get pretty large: the 2^(2*r) factor will erase a lot of
precision. Is there another way to compute this? Perhaps use the recurrence
relation for the binomial coefficient to get a recurrence relation for your
function? 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Francisco Fuentes
Sent: Thursday, May 05, 2005 2:02 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Precision in R


Could anyone help me with the following issue.
Using the GSL library in R I define the following code:

#########
library(gsl);
S<-function(n)
{ r<-0:n;
ans<-sum(gsl_sf_choose(n,r)*(-1)^r*2^(2*r)*gamma_inc(6-2*r,2))
ans }
#########
>SS(10)   yields 34.91868
>SS(40)   yields 5.340422
>SS(60)   yields 180.3162
Doing the same computations in maple I get
 
34.918679360927169740821310620770402166885975646756
5.340473872869891061658721253647686930049562214921
2.2269219454888559341895277572725106
 
respectively.  Is this a precision problem?


		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tyler.smith at mail.mcgill.ca  Thu May  5 20:14:22 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Thu,  5 May 2005 14:14:22 -0400
Subject: [R] efficient filtering of matrices
Message-ID: <1115316862.427a627e2b811@webmail.mcgill.ca>

Yikes, that looks complicated. I think what you're trying to do is available
already, see:

?subset

-- 
Tyler Smith

PhD Candidate
Plant Science Department
Faculty of Agricultural and Environmental Sciences
McGill University
MacDonald Campus
21,111 Lakeshore
Ste. Anne de Bellevue, Quebec, Canada H9X 3V9

Tel.: 514-398-7851 ext. 8726
Fax: 514-398-7897
tsmith20 at po-box.mcgill.ca



From gunter.berton at gene.com  Thu May  5 20:34:45 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 5 May 2005 11:34:45 -0700
Subject: [R] some pairs() questions
In-Reply-To: <008901c5519d$2bea7c30$9fa4c19d@nioo.int>
Message-ID: <200505051834.j45IYjx3023012@volta.gene.com>


If you want variable names, why do you have labels = NULL in your call?? 

Your highlighting criterion is not clear. If you want to choose one variable
and highlight points in all plots for which the value of that variable meets
your criterion, then determine the indices (outside of pairs) for which this
is true and write a panel function via points() to highlight. If you want to
highlight points in individual plots according to the variables in the
panel, then write a panel function via points()to do this. e.g.
points(x,y,..., col=ifelse(x<2,'red','blue'))

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pieter Provoost
> Sent: Thursday, May 05, 2005 11:07 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] some pairs() questions
> 
> I'm sure it's possible to display the variable names by making a small
> change to the code. Someone did this last week but I can't 
> contact this
> person now to ask how. I tried this
> 
> vars <- names(Rdata)
> pairs(Rdata, labels=vars,...)
> 
> but that doesn't work (formal argument "labels" matched by 
> multiple actual
> arguments). Unfortunately I don't have the time right now to 
> get to know new
> packages and write new scripts...
> 
> Thanks
> Pieter
> 
> ----- Original Message -----
> From: "Berton Gunter" <gunter.berton at gene.com>
> To: "'Pieter Provoost'" <pieterprovoost at gmail.com>;
> <r-help at stat.math.ethz.ch>
> Sent: Thursday, May 05, 2005 6:40 PM
> Subject: RE: [R] some pairs() questions
> 
> 
> > (Lazy answer, not checking your code in detail)
> >
> > Try using splom() in the lattice package, as it gives more extensive
> control
> > to do the sorts of things you seem to want.
> >
> > -- Bert Gunter
> > Genentech Non-Clinical Statistics
> > South San Francisco, CA
> >
> > "The business of the statistician is to catalyze the 
> scientific learning
> > process."  - George E. P. Box
> >
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Pieter Provoost
> > > Sent: Thursday, May 05, 2005 8:43 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] some pairs() questions
> > >
> > > I have a few problems with pairs(). I want to add the
> > > variables names to the
> > > plot, but I don't know how. I tried different things with 
> the labels
> > > argument but nothing worked out. This is the code:
> > >
> > > read.table("bay1.bay",header=T)->Rdata
> > > (histogram part skipped)
> > > pairs(Rdata,
> > > pch='.',diag.panel=panel.hist,labels=NULL,gap=0,xaxt="n",yaxt=
> > "n",upper.pane
> > > l=NULL)
> > >
> > > The other problem is that I want to "highlight" one or more
> > > of the records
> > > of Rdata (give it another symbol and another color). What's
> > > the easiest way
> > > to do this? The record to be highlighted should be selected
> > > based on the
> > > value of one of the variables (smaller than 2.8 is this case).
> > >
> > > Many thanks in advance,
> > > Pieter Provoost
> > >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Thu May  5 20:44:11 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 05 May 2005 11:44:11 -0700
Subject: [R] Need some quick help with lattice - barchart
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C497@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C497@wdexchmb01.lexicon.lexgen.com>
Message-ID: <427A697B.4060505@pdf.com>



Ghosh, Sandeep wrote on 5/5/2005 11:10 AM:
> For the following code below, the x-axis ticks are 1,2,3,4,5,6,7 when I was expection them to be 1,2,8,9,10,11,12. Please help me figure out where is the mistake.
> 
> 
> library(lattice)
> 
> testdata <- as.data.frame(t(structure(c(
> 1,2005,9.24,6.18,634,
> 2,2005,8.65,6.05,96,
> 8,2004,6.81,6.51,16,
> 9,2004,9.0,7.29,8,
> 10,2004,8.84,6.18,524,
> 11,2004,8.54,6.35,579,
> 12,2004,9.97,6.3,614,
> 12,2005,8.75,5.84,32,
> ), .Dim=c(5,8))))
> 
> colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
> testdata$month <- as.numeric(testdata$month)
> testdata$year <- factor(testdata$year)
> testdata <- testdata[do.call("order", testdata), ]
> 
> png('lexstar_3241.png', width=600, height=as.numeric(length(levels(testdata$year))*200), pointsize=8)
> trellis.par.set(theme = col.whitebg())
> 
> with(testdata, print(barchart(as.numeric(mean) ~ month | year, data=testdata, 
>     layout=c(1,length(levels(year))), 
>     horizontal=FALSE, 
>     scales=list(y=list(limits=c(1,max(as.numeric(mean))+max(as.numeric(stdDev))))),
>     main='Marble Burying - Level I',
>     xlab='Months',
>     ylab='Mean',
>     sd = as.numeric(as.character(stdDev)),
>     panel= function(x, y, ..., sd, subscripts) {
> 		panel.barchart(x, y, ...);
> 		sd <- sd[subscripts];
> 		panel.segments(as.numeric(x), y - sd, as.numeric(x), y + sd, col = 'red', lwd = 2);
> 	   }
> )))
> 
> dev.off()
> 
> Any help is greatly appreciated...
> 
> Thanks,
> Sandeep
> 
> 	[[alternative HTML version deleted]]

Hi Sandeep,

First, you are overusing as.numeric and as.character. Is there a reason 
for this that the example doesn't demonstrate.

Second, scales$y$limits is the same as setting ylim. The latter is more 
readable in my opinion.

As to your actual question, you should supply a scales argument for the 
x-axis. Below is what I came up with:

barchart(mean ~ month | year, data = testdata,
          layout = c(1, nlevels(testdata$year)),
          horizontal = FALSE,
          ylim = c(1, max(testdata$mean) + max(testdata$stdDev)),
          main = 'Marble Burying - Level I',
          xlab = 'Months',
          ylab = 'Mean',
          sd = testdata$stdDev,
          scales = list(x = list(at = 1:7,
                          labels = unique(sort(testdata$month)))),
          panel = function(x, y, ..., sd, subscripts) {
            panel.barchart(x, y, ...)
            sd <- sd[subscripts]
            panel.segments(x, y - sd, x, y + sd, col = 'red', lwd = 2)
          })


HTH,

--sundar



From deepayan at stat.wisc.edu  Thu May  5 21:03:45 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 5 May 2005 14:03:45 -0500
Subject: [R] Re: Need some quick help with lattice - barchart
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C497@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C497@wdexchmb01.lexicon.lexgen.com>
Message-ID: <200505051403.45100.deepayan@stat.wisc.edu>

On Thursday 05 May 2005 13:10, Ghosh, Sandeep wrote:
> For the following code below, the x-axis ticks are 1,2,3,4,5,6,7 when I was
> expection them to be 1,2,8,9,10,11,12. Please help me figure out where is
> the mistake.
[...]
> colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
> testdata$month <- as.numeric(testdata$month)
[...]
> with(testdata, print(barchart(as.numeric(mean) ~ month | year,
> data=testdata, layout=c(1,length(levels(year))),
>     horizontal=FALSE,
[...]

'month' is numeric, so it's being coerced to be a shingle. Try using 
'factor(month)' instead in the formula.

Deepayan



From sghosh at lexgen.com  Thu May  5 21:01:37 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Thu, 5 May 2005 14:01:37 -0500
Subject: [R] Need some quick help with lattice - barchart
Message-ID: <2B47B68F97330841AC8C670749084A7D06C498@wdexchmb01.lexicon.lexgen.com>

Hi Sundar,

Greatly appreicate your response, but I get this error on running the barchart cmd 

with(testdata, print(barchart(mean ~ month | year, data = testdata,
          layout = c(1, nlevels(testdata$year)),
          horizontal = FALSE,
          ylim = c(1, max(testdata$mean) + max(testdata$stdDev)),
          main = 'Marble Burying - Level I',
          xlab = 'Months',
          ylab = 'Mean',
          sd = testdata$stdDev,
          scales = list(x = list(at = 1:7, labels = unique(sort(testdata$month)))),
          panel = function(x, y, ..., sd, subscripts) {
            panel.barchart(x, y, ...)
            sd <- sd[subscripts]
            panel.segments(x, y - sd, x, y + sd, col = 'red', lwd = 2)
          })
))

Error in calculateGridLayout(x, rows.per.page, cols.per.page, number.of.cond,  : 
        Invalid value for labels


-Sandeep

-----Original Message-----
From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com]
Sent: Thursday, May 05, 2005 1:44 PM
To: Ghosh, Sandeep
Cc: r-help at stat.math.ethz.ch; MSchwartz at medanalytics.com
Subject: Re: [R] Need some quick help with lattice - barchart




Ghosh, Sandeep wrote on 5/5/2005 11:10 AM:
> For the following code below, the x-axis ticks are 1,2,3,4,5,6,7 when I was expection them to be 1,2,8,9,10,11,12. Please help me figure out where is the mistake.
> 
> 
> library(lattice)
> 
> testdata <- as.data.frame(t(structure(c(
> 1,2005,9.24,6.18,634,
> 2,2005,8.65,6.05,96,
> 8,2004,6.81,6.51,16,
> 9,2004,9.0,7.29,8,
> 10,2004,8.84,6.18,524,
> 11,2004,8.54,6.35,579,
> 12,2004,9.97,6.3,614,
> 12,2005,8.75,5.84,32,
> ), .Dim=c(5,8))))
> 
> colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
> testdata$month <- as.numeric(testdata$month)
> testdata$year <- factor(testdata$year)
> testdata <- testdata[do.call("order", testdata), ]
> 
> png('lexstar_3241.png', width=600, height=as.numeric(length(levels(testdata$year))*200), pointsize=8)
> trellis.par.set(theme = col.whitebg())
> 
> with(testdata, print(barchart(as.numeric(mean) ~ month | year, data=testdata, 
>     layout=c(1,length(levels(year))), 
>     horizontal=FALSE, 
>     scales=list(y=list(limits=c(1,max(as.numeric(mean))+max(as.numeric(stdDev))))),
>     main='Marble Burying - Level I',
>     xlab='Months',
>     ylab='Mean',
>     sd = as.numeric(as.character(stdDev)),
>     panel= function(x, y, ..., sd, subscripts) {
> 		panel.barchart(x, y, ...);
> 		sd <- sd[subscripts];
> 		panel.segments(as.numeric(x), y - sd, as.numeric(x), y + sd, col = 'red', lwd = 2);
> 	   }
> )))
> 
> dev.off()
> 
> Any help is greatly appreciated...
> 
> Thanks,
> Sandeep
> 
> 	[[alternative HTML version deleted]]

Hi Sandeep,

First, you are overusing as.numeric and as.character. Is there a reason 
for this that the example doesn't demonstrate.

Second, scales$y$limits is the same as setting ylim. The latter is more 
readable in my opinion.

As to your actual question, you should supply a scales argument for the 
x-axis. Below is what I came up with:

barchart(mean ~ month | year, data = testdata,
          layout = c(1, nlevels(testdata$year)),
          horizontal = FALSE,
          ylim = c(1, max(testdata$mean) + max(testdata$stdDev)),
          main = 'Marble Burying - Level I',
          xlab = 'Months',
          ylab = 'Mean',
          sd = testdata$stdDev,
          scales = list(x = list(at = 1:7,
                          labels = unique(sort(testdata$month)))),
          panel = function(x, y, ..., sd, subscripts) {
            panel.barchart(x, y, ...)
            sd <- sd[subscripts]
            panel.segments(x, y - sd, x, y + sd, col = 'red', lwd = 2)
          })


HTH,

--sundar



From sundar.dorai-raj at pdf.com  Thu May  5 21:05:22 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 05 May 2005 12:05:22 -0700
Subject: [R] Re: Need some quick help with lattice - barchart
In-Reply-To: <200505051403.45100.deepayan@stat.wisc.edu>
References: <2B47B68F97330841AC8C670749084A7D06C497@wdexchmb01.lexicon.lexgen.com>
	<200505051403.45100.deepayan@stat.wisc.edu>
Message-ID: <427A6E72.5080700@pdf.com>


Deepayan Sarkar wrote on 5/5/2005 12:03 PM:
> On Thursday 05 May 2005 13:10, Ghosh, Sandeep wrote:
> 
>>For the following code below, the x-axis ticks are 1,2,3,4,5,6,7 when I was
>>expection them to be 1,2,8,9,10,11,12. Please help me figure out where is
>>the mistake.
> 
> [...]
> 
>>colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
>>testdata$month <- as.numeric(testdata$month)
> 
> [...]
> 
>>with(testdata, print(barchart(as.numeric(mean) ~ month | year,
>>data=testdata, layout=c(1,length(levels(year))),
>>    horizontal=FALSE,
> 
> [...]
> 
> 'month' is numeric, so it's being coerced to be a shingle. Try using 
> 'factor(month)' instead in the formula.
> 
> Deepayan
> 

Hi Deepayan,

That was my original thought too. But when I tried it I got:

Error in unit(x0, default.units, units.per.obs) :
	'x' must be numeric

Changing horizontal to TRUE produces the plot, but I'm sure that's not 
what Sandeep wants.

--sundar



From ewr at uclink.berkeley.edu  Thu May  5 21:14:45 2005
From: ewr at uclink.berkeley.edu (Errol Wayne Robinson)
Date: Thu, 05 May 2005 12:14:45 -0700
Subject: [R] problem with plot() and R 2.1.0
Message-ID: <web-6207125@calmail-be3.berkeley.edu>

The following line when pasted into an R Console window causes Windows XP to flash a blue screen 
and then restart.

R version 2.1.0
>plot(x=c(1:100000),y=sin(c(1:100000)), type="l")

Windows XP SP2
I installed the precompiled version of R 2.1.0

This isn't the data I was originally trying to graph, but a way to reproduce the error I observed. 
  Plotting without the sin() did not reproduce the error.

I realize this might be unique to my system, but I thought I would report the problem and see if 
anyone else had encountered this difficulty.  I have also tried a variety of other plots and those 
results are summarized below.

Other plot() attempts:

R version 2.0.0
> plot(x=c(1:100),y=c(1:100), type="l")
> plot(x=c(1:100),y=sin(c(1:100)), type="l")
> plot(x=c(1:1000),y=sin(c(1:1000)), type="l")
> plot(x=c(1:10000),y=sin(c(1:10000)), type="l")
> plot(x=c(1:100000),y=sin(c(1:100000)), type="l")
> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
> plot(x=c(1:10000000),y=sin(c(1:10000000)), type="l")
all plotted without problem (if you try the last one it might require some patience to plot)

R version 2.0.1

> plot(x=c(1:100),y=c(1:100), type="l")
> plot(x=c(1:100),y=sin(c(1:100)), type="l")
> plot(x=c(1:1000),y=sin(c(1:1000)), type="l")
> plot(x=c(1:10000),y=sin(c(1:10000)), type="l")
> plot(x=c(1:100000),y=sin(c(1:100000)), type="l")
> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
> plot(x=c(1:10000000),y=sin(c(1:10000000)), type="l")
> 
also all plotted without problem

R version 2.1.0 (further tests)

> plot(x=c(1:100000),y=sin(c(1:100000)), type="p")
works
> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="p")
works
> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="b")
works
> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="o")
works
> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="n")
> lines(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
no plotting
> plot(x=c(1,1000000), y=c(1,-1), type="p")
> lines(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
crashes the computer

> plot(x=c(1:10000),y=sin(c(1:10000)), type="l")
> plot(x=c(1:15000),y=sin(c(1:15000)), type="l")
>works
> plot(x=c(1:20000),y=sin(c(1:20000)), type="l")
The screen resolution is set to a very low level.
The color depth has been set to a very low level.
windows displays an error which reads:
"The ati2dvag display driver has stopped working normally.  Save your work and reboot the system 
to restore full display functionality.  The next time you reboot the machine a dialog will be 
displayed giving you a chance to upload data about this failure to Microsoft"

I was convinced that I had a hardware /driver issue until I saw the plots work in older versions 
of R.  Now I don't know what to think.  So I'm sending out this email to see if anyone else has 
similar problems or can suggest a solution.

Errol Robinson
ewr at berkeley.edu



From andy_liaw at merck.com  Thu May  5 21:21:28 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 May 2005 15:21:28 -0400
Subject: [R] some pairs() questions
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076EDA@usctmx1106.merck.com>

> From: Pieter Provoost
> 
> I'm sure it's possible to display the variable names by making a small
> change to the code. Someone did this last week but I can't 
> contact this
> person now to ask how. I tried this
> 
> vars <- names(Rdata)
> pairs(Rdata, labels=vars,...)
> 
> but that doesn't work (formal argument "labels" matched by 
> multiple actual
> arguments). Unfortunately I don't have the time right now to 
> get to know new
> packages and write new scripts...

Please show the exact code and the exact error message, because I just tried
this and it works:

  pairs(iris[,-5], labels=letters[1:4], 
        pch=c(1, 21)[factor(iris$Petal.Width < 1.8)])

This also shows you how to do highlighting, and is a small modification of
the first example shown in ?pairs.

Andy

 
> Thanks
> Pieter
> 
> ----- Original Message -----
> From: "Berton Gunter" <gunter.berton at gene.com>
> To: "'Pieter Provoost'" <pieterprovoost at gmail.com>;
> <r-help at stat.math.ethz.ch>
> Sent: Thursday, May 05, 2005 6:40 PM
> Subject: RE: [R] some pairs() questions
> 
> 
> > (Lazy answer, not checking your code in detail)
> >
> > Try using splom() in the lattice package, as it gives more extensive
> control
> > to do the sorts of things you seem to want.
> >
> > -- Bert Gunter
> > Genentech Non-Clinical Statistics
> > South San Francisco, CA
> >
> > "The business of the statistician is to catalyze the 
> scientific learning
> > process."  - George E. P. Box
> >
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Pieter Provoost
> > > Sent: Thursday, May 05, 2005 8:43 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] some pairs() questions
> > >
> > > I have a few problems with pairs(). I want to add the
> > > variables names to the
> > > plot, but I don't know how. I tried different things with 
> the labels
> > > argument but nothing worked out. This is the code:
> > >
> > > read.table("bay1.bay",header=T)->Rdata
> > > (histogram part skipped)
> > > pairs(Rdata,
> > > pch='.',diag.panel=panel.hist,labels=NULL,gap=0,xaxt="n",yaxt=
> > "n",upper.pane
> > > l=NULL)
> > >
> > > The other problem is that I want to "highlight" one or more
> > > of the records
> > > of Rdata (give it another symbol and another color). What's
> > > the easiest way
> > > to do this? The record to be highlighted should be selected
> > > based on the
> > > value of one of the variables (smaller than 2.8 is this case).
> > >
> > > Many thanks in advance,
> > > Pieter Provoost
> > >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From sghosh at lexgen.com  Thu May  5 21:22:32 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Thu, 5 May 2005 14:22:32 -0500
Subject: [R] Re: Need some quick help with lattice - barchart
Message-ID: <2B47B68F97330841AC8C670749084A7D06C499@wdexchmb01.lexicon.lexgen.com>

It worked for me..Here are the commands..

library(lattice)

testdata <- as.data.frame(t(structure(c(
1,2005,9.24,6.18,634,
2,2005,8.65,6.05,96,
8,2004,6.81,6.51,16,
9,2004,9.0,7.29,8,
10,2004,8.84,6.18,524,
11,2004,8.54,6.35,579,
12,2004,9.97,6.3,614,
12,2005,8.75,5.84,32,
), .Dim=c(5,8))))
colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
testdata$month <- factor(testdata$month)
testdata$year <- factor(testdata$year)
testdata <- testdata[do.call("order", testdata), ]

png('lexstar_3241.png', width=600, height=as.numeric(length(levels(testdata$year))*200), pointsize=8)
trellis.par.set(theme = col.whitebg())

with(testdata, print(barchart(as.numeric(mean) ~ month | year, data=testdata, 
    layout=c(1,length(levels(year))), 
    horizontal=FALSE, 
    scales=list(y=list(limits=c(1,max(as.numeric(mean))+max(as.numeric(stdDev))))),
    main='Marble Burying - Level I',
    xlab='Months',
    ylab='Mean',
    sd = as.numeric(as.character(stdDev)),
    panel= function(x, y, ..., sd, subscripts) {
		panel.barchart(x, y, ...);
		sd <- sd[subscripts];
		panel.segments(as.numeric(x), y - sd, as.numeric(x), y + sd, col = 'red', lwd = 2);
	   }
)))

dev.off()

Thanks a lot Deepayan..

-Sandeep

-----Original Message-----
From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com]
Sent: Thursday, May 05, 2005 2:05 PM
To: Deepayan Sarkar
Cc: Ghosh, Sandeep; r-help at stat.math.ethz.ch
Subject: Re: [R] Re: Need some quick help with lattice - barchart



Deepayan Sarkar wrote on 5/5/2005 12:03 PM:
> On Thursday 05 May 2005 13:10, Ghosh, Sandeep wrote:
> 
>>For the following code below, the x-axis ticks are 1,2,3,4,5,6,7 when I was
>>expection them to be 1,2,8,9,10,11,12. Please help me figure out where is
>>the mistake.
> 
> [...]
> 
>>colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
>>testdata$month <- as.numeric(testdata$month)
> 
> [...]
> 
>>with(testdata, print(barchart(as.numeric(mean) ~ month | year,
>>data=testdata, layout=c(1,length(levels(year))),
>>    horizontal=FALSE,
> 
> [...]
> 
> 'month' is numeric, so it's being coerced to be a shingle. Try using 
> 'factor(month)' instead in the formula.
> 
> Deepayan
> 

Hi Deepayan,

That was my original thought too. But when I tried it I got:

Error in unit(x0, default.units, units.per.obs) :
	'x' must be numeric

Changing horizontal to TRUE produces the plot, but I'm sure that's not 
what Sandeep wants.

--sundar



From david.arenas at navy.mil  Thu May  5 21:38:31 2005
From: david.arenas at navy.mil (Arenas, David R.  CIV NAVAIR DEPT)
Date: Thu, 5 May 2005 12:38:31 -0700
Subject: [R] Combining numeric vs numeric & numeric vs factor graphs into
	one ps/pdf file
Message-ID: <2800A210138FAD4780C77CE452DDDD873D34B9@NAWESDNIEX05VA.nadsuswe.nads.navy.mil>

Deepayan,

Thank you for your response.  I tried your suggestion to manually specify the y-axis label for the factor but I am stuck.  My attempt is below.  I once again appreciate any input as well as your time.  lattice/R-project is great.

Thanks once again,

David Arenas

# Example dataframe
test.df <- data.frame(acft=factor(c("A","B","C","D")),
                      status=factor(c("fail","pass","fail","pass")), 
                      site=factor(c("E1","E1","E2","E2")),
                      CD=as.numeric(c(1,1,3,3)),
                      H=as.numeric(c(80,NA,60,NA)))

# Your previous suggestion
xyplot(ifelse(status=="pass", as.numeric(site), H) ~ CD| acft,
       data=test.df,
       scales="free",
       xlim=c(0,4))

# My attempt to manually adjust the y-axis label for the factor "site"
# site has E1 & E2 that are read as 1 and 2, so I am trying to adjust
# the ylim for acft B and D to c(0,2) but I get an error
xyplot(ifelse(status=="pass", as.numeric(site), H) ~ CD| acft,
       data=test.df,
       scales="free",
       prepanel=function(x,y) {
                ylim <- if(test.df$status=="pass") 2 else 90
                list(xlim=range(0:4),
                     ylim=range(0:ylim)) }  ) 


-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu]
Sent: Tuesday, May 03, 2005 19:59
To: r-help at stat.math.ethz.ch
Cc: Arenas, David R. CIV NAVAIR DEPT
Subject: Re: [R] Combining numeric vs numeric & numeric vs factor graphs
into one ps/pdf file


On Tuesday 03 May 2005 10:44, Arenas, David R.  CIV NAVAIR DEPT wrote:
> Dear R community,
>
> My previous email was incomplete because I used html format.  Here it
> is again and sorry for any inconvenience:
>
> xyplot (lattice) has been great in displaying tons of data for my
> research.  I have used the following two xyplot commands (with
> example dataframe) to create two separate postscript/pdf files with
> respect to the variable "acft" and subset "status":
>
> test.df <- data.frame(acft=factor(c("A","B","C","D")),
>                              
> status=factor(c("fail","pass","fail","pass")),
> site=factor(c("E1","E1","E2","E2")), CD=as.numeric(c(1,1,3,3)),
>                               H=as.numeric(c(80,NA,60,NA)))
>
> xyplot(H ~ CD | acft,
>           data=test.df,
>           subset=status=="fail",
>           layout=c(1,1) )
>
> xyplot(site ~ CD | acft,
>           data=test.df,
>           subset=status=="pass",
>           layout=c(1,1) )
>
>  I would like to combine all graphs into one file in alphabetical
> order of variable "acft".  The graphs would be one per page where in
> fact I use layout=c(1,1) for the nice and easily seen strip labels
> for "acft".  The problem I am having is combining x-y plots that are
> numeric vs numeric & numeric vs factor.  I have search the R-help
> archives and R-project references for an example to no avail.  I am
> thinking I may have to use something (lattice or not) like ...
>
> if any(test.df$Status=="fail")
> plot(H ~ CD)
> else
> plot(site ~ CD)
>
> with "for" in the beginning to loop through all data with respect to
> acft.  I need a hint on how to further this along.  I am using
> R.2.1.0 via Windows XP.

I can't think of a clean way to do this. You could of course do 

xyplot(ifelse(status == "pass", as.numeric(site), H) ~ CD| acft,
       data=test.df, scales = "free",
       layout=c(1,1) )

but this wouldn't give very nice axis labels for the factor (unless you 
specify them manually, which could be done).

Deepayan



From gunter.berton at gene.com  Thu May  5 21:42:27 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 5 May 2005 12:42:27 -0700
Subject: [R] problem with plot() and R 2.1.0
In-Reply-To: <web-6207125@calmail-be3.berkeley.edu>
Message-ID: <200505051942.j45JgRvk005232@faraday.gene.com>

Yup, it crashed my Windows 2000 laptop! In fact, so bad, I had to remove the
battery to restart.

My guess is that this is deep in the heart of the C code in the display
libraries that 2.1.0 uses. Good luck to the R gurus in finding it. In case
it's of any use, here are particulars of my computer:

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R      

More importantly, perhaps:
Display adapter = ATI Radeon IGP 350M 

You seem to have an ati adapter and driver, also. Does this only occur with
their drivers?


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Errol 
> Wayne Robinson
> Sent: Thursday, May 05, 2005 12:15 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] problem with plot() and R 2.1.0
> 
> The following line when pasted into an R Console window 
> causes Windows XP to flash a blue screen 
> and then restart.
> 
> R version 2.1.0
> >plot(x=c(1:100000),y=sin(c(1:100000)), type="l")
> 
> Windows XP SP2
> I installed the precompiled version of R 2.1.0
> 
> This isn't the data I was originally trying to graph, but a 
> way to reproduce the error I observed. 
>   Plotting without the sin() did not reproduce the error.
> 
> I realize this might be unique to my system, but I thought I 
> would report the problem and see if 
> anyone else had encountered this difficulty.  I have also 
> tried a variety of other plots and those 
> results are summarized below.
> 
> Other plot() attempts:
> 
> R version 2.0.0
> > plot(x=c(1:100),y=c(1:100), type="l")
> > plot(x=c(1:100),y=sin(c(1:100)), type="l")
> > plot(x=c(1:1000),y=sin(c(1:1000)), type="l")
> > plot(x=c(1:10000),y=sin(c(1:10000)), type="l")
> > plot(x=c(1:100000),y=sin(c(1:100000)), type="l")
> > plot(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
> > plot(x=c(1:10000000),y=sin(c(1:10000000)), type="l")
> all plotted without problem (if you try the last one it might 
> require some patience to plot)
> 
> R version 2.0.1
> 
> > plot(x=c(1:100),y=c(1:100), type="l")
> > plot(x=c(1:100),y=sin(c(1:100)), type="l")
> > plot(x=c(1:1000),y=sin(c(1:1000)), type="l")
> > plot(x=c(1:10000),y=sin(c(1:10000)), type="l")
> > plot(x=c(1:100000),y=sin(c(1:100000)), type="l")
> > plot(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
> > plot(x=c(1:10000000),y=sin(c(1:10000000)), type="l")
> > 
> also all plotted without problem
> 
> R version 2.1.0 (further tests)
> 
> > plot(x=c(1:100000),y=sin(c(1:100000)), type="p")
> works
> > plot(x=c(1:1000000),y=sin(c(1:1000000)), type="p")
> works
> > plot(x=c(1:1000000),y=sin(c(1:1000000)), type="b")
> works
> > plot(x=c(1:1000000),y=sin(c(1:1000000)), type="o")
> works
> > plot(x=c(1:1000000),y=sin(c(1:1000000)), type="n")
> > lines(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
> no plotting
> > plot(x=c(1,1000000), y=c(1,-1), type="p")
> > lines(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
> crashes the computer
> 
> > plot(x=c(1:10000),y=sin(c(1:10000)), type="l")
> > plot(x=c(1:15000),y=sin(c(1:15000)), type="l")
> >works
> > plot(x=c(1:20000),y=sin(c(1:20000)), type="l")
> The screen resolution is set to a very low level.
> The color depth has been set to a very low level.
> windows displays an error which reads:
> "The ati2dvag display driver has stopped working normally.  
> Save your work and reboot the system 
> to restore full display functionality.  The next time you 
> reboot the machine a dialog will be 
> displayed giving you a chance to upload data about this 
> failure to Microsoft"
> 
> I was convinced that I had a hardware /driver issue until I 
> saw the plots work in older versions 
> of R.  Now I don't know what to think.  So I'm sending out 
> this email to see if anyone else has 
> similar problems or can suggest a solution.
> 
> Errol Robinson
> ewr at berkeley.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From pburns at pburns.seanet.com  Thu May  5 21:55:35 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 05 May 2005 20:55:35 +0100
Subject: [R] problem with plot() and R 2.1.0
In-Reply-To: <web-6207125@calmail-be3.berkeley.edu>
References: <web-6207125@calmail-be3.berkeley.edu>
Message-ID: <427A7A37.3080606@pburns.seanet.com>

I can confirm that on my system running Windows 2000 that the
command using R 2.1.0 hangs -- the graphics window appears but
nothing happens subsequently.  Under 2.0.0 it works okay.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Errol Wayne Robinson wrote:

> The following line when pasted into an R Console window causes Windows 
> XP to flash a blue screen and then restart.
>
> R version 2.1.0
>
>> plot(x=c(1:100000),y=sin(c(1:100000)), type="l")
>
>
> Windows XP SP2
> I installed the precompiled version of R 2.1.0
>
> This isn't the data I was originally trying to graph, but a way to 
> reproduce the error I observed.  Plotting without the sin() did not 
> reproduce the error.
>
> I realize this might be unique to my system, but I thought I would 
> report the problem and see if anyone else had encountered this 
> difficulty.  I have also tried a variety of other plots and those 
> results are summarized below.
>
> Other plot() attempts:
>
> R version 2.0.0
>
>> plot(x=c(1:100),y=c(1:100), type="l")
>> plot(x=c(1:100),y=sin(c(1:100)), type="l")
>> plot(x=c(1:1000),y=sin(c(1:1000)), type="l")
>> plot(x=c(1:10000),y=sin(c(1:10000)), type="l")
>> plot(x=c(1:100000),y=sin(c(1:100000)), type="l")
>> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
>> plot(x=c(1:10000000),y=sin(c(1:10000000)), type="l")
>
> all plotted without problem (if you try the last one it might require 
> some patience to plot)
>
> R version 2.0.1
>
>> plot(x=c(1:100),y=c(1:100), type="l")
>> plot(x=c(1:100),y=sin(c(1:100)), type="l")
>> plot(x=c(1:1000),y=sin(c(1:1000)), type="l")
>> plot(x=c(1:10000),y=sin(c(1:10000)), type="l")
>> plot(x=c(1:100000),y=sin(c(1:100000)), type="l")
>> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
>> plot(x=c(1:10000000),y=sin(c(1:10000000)), type="l")
>>
> also all plotted without problem
>
> R version 2.1.0 (further tests)
>
>> plot(x=c(1:100000),y=sin(c(1:100000)), type="p")
>
> works
>
>> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="p")
>
> works
>
>> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="b")
>
> works
>
>> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="o")
>
> works
>
>> plot(x=c(1:1000000),y=sin(c(1:1000000)), type="n")
>> lines(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
>
> no plotting
>
>> plot(x=c(1,1000000), y=c(1,-1), type="p")
>> lines(x=c(1:1000000),y=sin(c(1:1000000)), type="l")
>
> crashes the computer
>
>> plot(x=c(1:10000),y=sin(c(1:10000)), type="l")
>> plot(x=c(1:15000),y=sin(c(1:15000)), type="l")
>> works
>> plot(x=c(1:20000),y=sin(c(1:20000)), type="l")
>
> The screen resolution is set to a very low level.
> The color depth has been set to a very low level.
> windows displays an error which reads:
> "The ati2dvag display driver has stopped working normally.  Save your 
> work and reboot the system to restore full display functionality.  The 
> next time you reboot the machine a dialog will be displayed giving you 
> a chance to upload data about this failure to Microsoft"
>
> I was convinced that I had a hardware /driver issue until I saw the 
> plots work in older versions of R.  Now I don't know what to think.  
> So I'm sending out this email to see if anyone else has similar 
> problems or can suggest a solution.
>
> Errol Robinson
> ewr at berkeley.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>



From jerk_alert at hotmail.com  Thu May  5 22:14:42 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Thu, 05 May 2005 20:14:42 +0000
Subject: [R] Intersection of more than two groups in one function?
Message-ID: <BAY101-F42F77ED43D7DE628A7665FE81A0@phx.gbl>

Hi all,

As far as I can tell, the only canned way to do an intersect between two 
vectors of ints is the intersect(vec1, vec2) function -- is there another 
function I'm missing for intersecting more than two vectors??

TIA,
Ken



From jorelien at scimetrika.com  Thu May  5 22:21:38 2005
From: jorelien at scimetrika.com (Jean G Orelien)
Date: Thu, 5 May 2005 16:21:38 -0400
Subject: [R] GAM/MGCV: UBRE Score
Message-ID: <NC-TDA03i03LojJoPUp0000077d@mail.nctda.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050505/16a877b5/attachment.pl

From andy_liaw at merck.com  Thu May  5 22:21:40 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 May 2005 16:21:40 -0400
Subject: [R] Intersection of more than two groups in one function?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076EDB@usctmx1106.merck.com>

Here's a brute-force approach:

> "%i%" <- intersect
> x1 <- c(1, 3, 4, 7)
> x2 <- c(3, 7, 8)
> x3 <- c(7, 1)
> x1 %i% x2 %i% x3
[1] 7

Andy

> From: Ken Termiso
> 
> Hi all,
> 
> As far as I can tell, the only canned way to do an intersect 
> between two 
> vectors of ints is the intersect(vec1, vec2) function -- is 
> there another 
> function I'm missing for intersecting more than two vectors??
> 
> TIA,
> Ken
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From francesco.pauli at gmail.com  Thu May  5 22:24:17 2005
From: francesco.pauli at gmail.com (Francesco Pauli)
Date: Thu, 5 May 2005 22:24:17 +0200
Subject: [R] problem with plot() and R 2.1.0
In-Reply-To: <427A7A37.3080606@pburns.seanet.com>
References: <web-6207125@calmail-be3.berkeley.edu>
	<427A7A37.3080606@pburns.seanet.com>
Message-ID: <eae32e0805050513247d1f7c6c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050505/8ada5d00/attachment.pl

From pieterprovoost at gmail.com  Thu May  5 23:00:29 2005
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Thu, 5 May 2005 23:00:29 +0200
Subject: [R] some pairs() questions
References: <200505051834.j45IYjx3023012@volta.gene.com>
Message-ID: <003601c551b5$79456a90$6602a8c0@nioo.int>

I'm really sorry, I didn't notice the existing labels argument in the
script. I will have another try with the highlighting. This is the complete
code:

setwd("c:/data/marelac/thesis/r/")
read.table("bay1.bay",header=T)->Rdata
names(Rdata)
Rdata[-1] -> Rdata
Rdata[1:7] -> Rdata
panel.hist <- function(x, ...)
      {
          usr <- par("usr"); on.exit(par(usr))
          par(usr = c(usr[1:2], 0, 1.5) )
          h <- hist(x, plot = FALSE)
          breaks <- h$breaks; nB <- length(breaks)
          y <- h$counts; y <- y/max(y)
          rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
      }
pairs(Rdata,pch='.',diag.panel=panel.hist,labels=names(Rdata),gap=0,xaxt="n"
,yaxt="n",upper.panel=NULL)

The datafile looks like this:

Run    PAR_DECAYRATECHLPHY    PAR_DECAYRATECHLCLD ...
1    0.200000E-002    0.500000E-001 ...
2    0.222888E-002    0.477971E-001 ...
...

In total there are 14 columns and 4000 records. These are all model runs. I
now want to to highlight the run with the lowest model cost in all the plots
(modelcost is the last column of the datafile). I'm absolutely new to R, but
it looks like I'll need to have a look at panels (whatever they may be).

Thanks again for your assistance,
Pieter

----- Original Message -----
From: "Berton Gunter" <gunter.berton at gene.com>
To: "'Pieter Provoost'" <pieterprovoost at gmail.com>;
<r-help at stat.math.ethz.ch>
Sent: Thursday, May 05, 2005 8:34 PM
Subject: RE: [R] some pairs() questions


>
> If you want variable names, why do you have labels = NULL in your call??
>
> Your highlighting criterion is not clear. If you want to choose one
variable
> and highlight points in all plots for which the value of that variable
meets
> your criterion, then determine the indices (outside of pairs) for which
this
> is true and write a panel function via points() to highlight. If you want
to
> highlight points in individual plots according to the variables in the
> panel, then write a panel function via points()to do this. e.g.
> points(x,y,..., col=ifelse(x<2,'red','blue'))
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pieter Provoost
> > Sent: Thursday, May 05, 2005 11:07 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: Re: [R] some pairs() questions
> >
> > I'm sure it's possible to display the variable names by making a small
> > change to the code. Someone did this last week but I can't
> > contact this
> > person now to ask how. I tried this
> >
> > vars <- names(Rdata)
> > pairs(Rdata, labels=vars,...)
> >
> > but that doesn't work (formal argument "labels" matched by
> > multiple actual
> > arguments). Unfortunately I don't have the time right now to
> > get to know new
> > packages and write new scripts...
> >
> > Thanks
> > Pieter
> >
> > ----- Original Message -----
> > From: "Berton Gunter" <gunter.berton at gene.com>
> > To: "'Pieter Provoost'" <pieterprovoost at gmail.com>;
> > <r-help at stat.math.ethz.ch>
> > Sent: Thursday, May 05, 2005 6:40 PM
> > Subject: RE: [R] some pairs() questions
> >
> >
> > > (Lazy answer, not checking your code in detail)
> > >
> > > Try using splom() in the lattice package, as it gives more extensive
> > control
> > > to do the sorts of things you seem to want.
> > >
> > > -- Bert Gunter
> > > Genentech Non-Clinical Statistics
> > > South San Francisco, CA
> > >
> > > "The business of the statistician is to catalyze the
> > scientific learning
> > > process."  - George E. P. Box
> > >
> > >
> > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > Pieter Provoost
> > > > Sent: Thursday, May 05, 2005 8:43 AM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: [R] some pairs() questions
> > > >
> > > > I have a few problems with pairs(). I want to add the
> > > > variables names to the
> > > > plot, but I don't know how. I tried different things with
> > the labels
> > > > argument but nothing worked out. This is the code:
> > > >
> > > > read.table("bay1.bay",header=T)->Rdata
> > > > (histogram part skipped)
> > > > pairs(Rdata,
> > > > pch='.',diag.panel=panel.hist,labels=NULL,gap=0,xaxt="n",yaxt=
> > > "n",upper.pane
> > > > l=NULL)
> > > >
> > > > The other problem is that I want to "highlight" one or more
> > > > of the records
> > > > of Rdata (give it another symbol and another color). What's
> > > > the easiest way
> > > > to do this? The record to be highlighted should be selected
> > > > based on the
> > > > value of one of the variables (smaller than 2.8 is this case).
> > > >
> > > > Many thanks in advance,
> > > > Pieter Provoost
> > > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>



From gunter.berton at gene.com  Thu May  5 23:05:29 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 5 May 2005 14:05:29 -0700
Subject: [R] Intersection of more than two groups in one function?
In-Reply-To: <BAY101-F42F77ED43D7DE628A7665FE81A0@phx.gbl>
Message-ID: <200505052105.j45L5T1G000357@hertz.gene.com>

No, I know of no "canned" way to do it, as it's based on match(), which is
for two vectors only.

A brute force loop to do it is simple, as I suspect you realize. Perhaps a
faster way is to use tabulate() to find values that appear n times (for n
vectors). That is:

## assume your integer vectors are components of a list, mylist.

## warning: not tested
bigvec<-unlist(mylist)
sort(unique(bigvec))[tabulate(bigvec)==length(mylist)]



-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ken Termiso
> Sent: Thursday, May 05, 2005 1:15 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Intersection of more than two groups in one function?
> 
> Hi all,
> 
> As far as I can tell, the only canned way to do an intersect 
> between two 
> vectors of ints is the intersect(vec1, vec2) function -- is 
> there another 
> function I'm missing for intersecting more than two vectors??
> 
> TIA,
> Ken
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rdiaz at cnio.es  Thu May  5 23:35:12 2005
From: rdiaz at cnio.es (Diaz.Ramon)
Date: Thu, 5 May 2005 23:35:12 +0200
Subject: [R] building from source after installing binary package
Message-ID: <A5980E51E88D07418B8951F976FD725C8C9A16@SRVEXCH2.cnio.es>

Dear All,

I've got into the habit of installing R from the precompiled Debian binaries, including many of the packages from the r-cran-* Debian packages, and later building from source (e.g., to link against Goto's BLAS, or to build patched versions, etc). I install the newly built R to the very same place (/usr/lib/R). This allows me to build and update R when I wish, AND provides the ease of quickly updating many packages.

Things have always worked fine, but after a few funny problems (which could be unrelated to the process itself) I've started wondering if this is a rather silly thing to do, and if I should keep my own build separate from the Debian stuff. Any advice would be much appreciated.

Thanks,

R.
--
RamÅÛn DÅÌaz-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones OncolÅÛgicas (CNIO)
(Spanish National Cancer Center)
Melchor FernÅ·ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)









**NOTA DE CONFIDENCIALIDAD** Este correo electrÅÛnico, y en su caso los ficheros adjuntos, pueden contener informaciÅÛn protegida para el uso exclusivo de su destinatario. Se prohÅÌbe la distribuciÅÛn, reproducciÅÛn o cualquier otro tipo de transmisiÅÛn por parte de otra persona que no sea el destinatario. Si usted recibe por error este correo, se ruega comunicarlo al remitente y borrar el mensaje recibido. 
**CONFIDENTIALITY NOTICE** This email communication and any attachments may contain confidential and privileged information for the sole use of the designated recipient named above. Distribution, reproduction or any other use of this transmission by any party other than the intended recipient is prohibited. If you are not the intended recipient please contact the sender and delete all copies.



From tolga.uzuner at csfb.com  Fri May  6 00:20:57 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Thu, 5 May 2005 23:20:57 +0100
Subject: [R] Numerical Derivative / Numerical Differentiation of unknown
 funct ion
Message-ID: <BDF571786CAD224F966FEB86BEDED52F1433DABB@elon12p32001.csfp.co.uk>

Hi,

I have been trying to do numerical differentiation using R. 

I found some old S code using Richardson Extrapolation which I managed to get
to work.

I am posting it here in case anyone needs it.


########################################################################
richardson.grad <- function(func, x, d=0.01, eps=1e-4, r=6, show=F){
# This function calculates a numerical approximation of the first
#   derivative of func at the point x. The calculation
#   is done by Richardson's extrapolation (see eg. G.R.Linfield and
J.E.T.Penny
#   "Microcomputers in Numerical Analysis"). The method should be used if
#   accuracy, as opposed to speed, is important.
#
#  *  modified by Paul Gilbert from orginal code by XINGQIAO LIU.
# CALCULATES THE FIRST ORDER DERIVATIVE 
#     VECTOR using a Richardson  extrapolation.
#
#  GENERAL APPROACH
#     --  GIVEN THE FOLLOWING INITIAL VALUES:
#             INTERVAL VALUE D, NUMBER OF ITERATIONS R, AND
#             REDUCED FACTOR V.
#      - THE FIRST ORDER aproximation to the DERIVATIVE WITH RESPECT TO Xi IS
#
#           F'(Xi)={F(X1,...,Xi+D,...,Xn) - F(X1,...,Xi-D,...,Xn)}/(2*D)
#       
#     --  REPEAT r TIMES  with successively smaller D  and 
#          then apply Richardson extraplolation
#
#  INPUT
#       func    Name of the function.
#       x       The parameters of func.
#       d       Initial interval value (real) by default set to 0.01*x or
#               eps if x is 0.0.
#       r       The number of Richardson improvement iterations.
#       show    If T show intermediate results.
#  OUTPUT
#
#       The gradient vector.

  v <- 2               # reduction factor.
  n <- length(x)       # Integer, number of variables.
  a.mtr <- matrix(1, r, n) 
  b.mtr <- matrix(1, (r - 1), n)
#------------------------------------------------------------------------
# 1 Calculate the derivative formula given in 'GENERAL APPROACH' section.
#   --  The first order derivatives are stored in the matrix a.mtr[k,i], 
#        where the indexing variables k for rows(1 to r),  i for columns
#       (1 to n),  r is the number of iterations, and n is the number of
#       variables.
#-------------------------------------------------------------------------  

  h <- abs(d*x)+eps*(x==0.0)
  for(k in 1:r)  { # successively reduce h                
     for(i in 1:n)  {
         x1.vct <- x2.vct <- x
         x1.vct[i]  <- x[i] + h[i]
         x2.vct[i]  <- x[i] - h[i]
         if(k == 1) a.mtr[k,i] <- (func(x1.vct) - func(x2.vct))/(2*h[i])
         else{
           if(abs(a.mtr[(k-1),i])>1e-20)
                 # some functions are unstable near 0.0              
                 a.mtr[k,i] <- (func(x1.vct)-func(x2.vct))/(2*h[i])
            else  a.mtr[k, i] <- 0
          }
      }
     h <- h/v     # Reduced h by 1/v.
    }	
   if(show)  {
        cat("\n","first order approximations", "\n")		
        print(a.mtr, 12)
    }

#------------------------------------------------------------------------
# 1 Applying Richardson Extrapolation to improve the accuracy of 
#   the first and second order derivatives. The algorithm as follows:
#
#   --  For each column of the 1st and 2nd order derivatives matrix a.mtr,
#       say, A1, A2, ..., Ar, by Richardson Extrapolation, to calculate a
#       new sequence of approximations B1, B2, ..., Br used the formula
#
#          B(i) =( A(i+1)*4^m - A(i) ) / (4^m - 1) ,  i=1,2,...,r-m
#
#             N.B. This formula assumes v=2.
#
#   -- Initially m is taken as 1  and then the process is repeated 
#      restarting with the latest improved values and increasing the 
#      value of m by one each until m equals r-1
#
# 2 Display the improved derivatives for each
#   m from 1 to r-1 if the argument show=T.
#
# 3 Return the final improved  derivative vector.
#-------------------------------------------------------------------------

  for(m in 1:(r - 1)) {		
     for(i in 1:(r - m)) b.mtr[i,]<- (a.mtr[(i+1),]*(4^m)-a.mtr[i,])/(4^m-1)
#     a.mtr<- b.mtr
#     a.mtr<- (a.mtr[2:(r+1-m),]*(4^m)-a.mtr[1:(r-m),])/(4^m-1)
     if(show & m!=(r-1) )  {
        cat("\n","Richarson improvement group No. ", m, "\n")		
        print(a.mtr[1:(r-m),], 12)
      }
   }
a.mtr[length(a.mtr)]
}

## try it out
richardson.grad(function(x){x^3},2)
########################################################################################


Regards,
Tolga Uzuner


==============================================================================
This message is for the sole use of the intended recipient. ...{{dropped}}



From chess.player at oninet.pt  Fri May  6 00:31:08 2005
From: chess.player at oninet.pt (jose silva)
Date: Thu, 05 May 2005 23:31:08 +0100
Subject: [R] question about subset
Message-ID: <aaf3ac8b7a434ab4b0c32079c84d4487@oninet.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050505/be88e590/attachment.pl

From gunter.berton at gene.com  Fri May  6 00:34:06 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 5 May 2005 15:34:06 -0700
Subject: [R] Numerical Derivative / Numerical Differentiation of unknown
	funct ion
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433DABB@elon12p32001.csfp.co.uk>
Message-ID: <200505052234.j45MY614021166@meitner.gene.com>

But...

See ?numericDeriv which already does it via a C call and hence is much
faster (and probably more accurate,too).



-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uzuner, Tolga
> Sent: Thursday, May 05, 2005 3:21 PM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] Numerical Derivative / Numerical Differentiation 
> of unknown funct ion
> 
> Hi,
> 
> I have been trying to do numerical differentiation using R. 
> 
> I found some old S code using Richardson Extrapolation which 
> I managed to get
> to work.
> 
> I am posting it here in case anyone needs it.
> 
> 
> ##############################################################
> ##########
> richardson.grad <- function(func, x, d=0.01, eps=1e-4, r=6, show=F){
> # This function calculates a numerical approximation of the first
> #   derivative of func at the point x. The calculation
> #   is done by Richardson's extrapolation (see eg. G.R.Linfield and
> J.E.T.Penny
> #   "Microcomputers in Numerical Analysis"). The method 
> should be used if
> #   accuracy, as opposed to speed, is important.
> #
> #  *  modified by Paul Gilbert from orginal code by XINGQIAO LIU.
> # CALCULATES THE FIRST ORDER DERIVATIVE 
> #     VECTOR using a Richardson  extrapolation.
> #
> #  GENERAL APPROACH
> #     --  GIVEN THE FOLLOWING INITIAL VALUES:
> #             INTERVAL VALUE D, NUMBER OF ITERATIONS R, AND
> #             REDUCED FACTOR V.
> #      - THE FIRST ORDER aproximation to the DERIVATIVE WITH 
> RESPECT TO Xi IS
> #
> #           F'(Xi)={F(X1,...,Xi+D,...,Xn) - 
> F(X1,...,Xi-D,...,Xn)}/(2*D)
> #       
> #     --  REPEAT r TIMES  with successively smaller D  and 
> #          then apply Richardson extraplolation
> #
> #  INPUT
> #       func    Name of the function.
> #       x       The parameters of func.
> #       d       Initial interval value (real) by default set 
> to 0.01*x or
> #               eps if x is 0.0.
> #       r       The number of Richardson improvement iterations.
> #       show    If T show intermediate results.
> #  OUTPUT
> #
> #       The gradient vector.
> 
>   v <- 2               # reduction factor.
>   n <- length(x)       # Integer, number of variables.
>   a.mtr <- matrix(1, r, n) 
>   b.mtr <- matrix(1, (r - 1), n)
> #-------------------------------------------------------------
> -----------
> # 1 Calculate the derivative formula given in 'GENERAL 
> APPROACH' section.
> #   --  The first order derivatives are stored in the matrix 
> a.mtr[k,i], 
> #        where the indexing variables k for rows(1 to r),  i 
> for columns
> #       (1 to n),  r is the number of iterations, and n is 
> the number of
> #       variables.
> #-------------------------------------------------------------
> ------------  
> 
>   h <- abs(d*x)+eps*(x==0.0)
>   for(k in 1:r)  { # successively reduce h                
>      for(i in 1:n)  {
>          x1.vct <- x2.vct <- x
>          x1.vct[i]  <- x[i] + h[i]
>          x2.vct[i]  <- x[i] - h[i]
>          if(k == 1) a.mtr[k,i] <- (func(x1.vct) - 
> func(x2.vct))/(2*h[i])
>          else{
>            if(abs(a.mtr[(k-1),i])>1e-20)
>                  # some functions are unstable near 0.0              
>                  a.mtr[k,i] <- (func(x1.vct)-func(x2.vct))/(2*h[i])
>             else  a.mtr[k, i] <- 0
>           }
>       }
>      h <- h/v     # Reduced h by 1/v.
>     }	
>    if(show)  {
>         cat("\n","first order approximations", "\n")		
>         print(a.mtr, 12)
>     }
> 
> #-------------------------------------------------------------
> -----------
> # 1 Applying Richardson Extrapolation to improve the accuracy of 
> #   the first and second order derivatives. The algorithm as follows:
> #
> #   --  For each column of the 1st and 2nd order derivatives 
> matrix a.mtr,
> #       say, A1, A2, ..., Ar, by Richardson Extrapolation, to 
> calculate a
> #       new sequence of approximations B1, B2, ..., Br used 
> the formula
> #
> #          B(i) =( A(i+1)*4^m - A(i) ) / (4^m - 1) ,  i=1,2,...,r-m
> #
> #             N.B. This formula assumes v=2.
> #
> #   -- Initially m is taken as 1  and then the process is repeated 
> #      restarting with the latest improved values and increasing the 
> #      value of m by one each until m equals r-1
> #
> # 2 Display the improved derivatives for each
> #   m from 1 to r-1 if the argument show=T.
> #
> # 3 Return the final improved  derivative vector.
> #-------------------------------------------------------------
> ------------
> 
>   for(m in 1:(r - 1)) {		
>      for(i in 1:(r - m)) b.mtr[i,]<- 
> (a.mtr[(i+1),]*(4^m)-a.mtr[i,])/(4^m-1)
> #     a.mtr<- b.mtr
> #     a.mtr<- (a.mtr[2:(r+1-m),]*(4^m)-a.mtr[1:(r-m),])/(4^m-1)
>      if(show & m!=(r-1) )  {
>         cat("\n","Richarson improvement group No. ", m, "\n")		
>         print(a.mtr[1:(r-m),], 12)
>       }
>    }
> a.mtr[length(a.mtr)]
> }
> 
> ## try it out
> richardson.grad(function(x){x^3},2)
> ##############################################################
> ##########################
> 
> 
> Regards,
> Tolga Uzuner
> 
> 
> ==============================================================
> ================
> This message is for the sole use of the intended recipient...{{dropped}}



From gunter.berton at gene.com  Fri May  6 00:36:20 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 5 May 2005 15:36:20 -0700
Subject: [R] question about subset
In-Reply-To: <aaf3ac8b7a434ab4b0c32079c84d4487@oninet.pt>
Message-ID: <200505052236.j45MaK1u026386@ohm.gene.com>

?row.names

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jose silva
> Sent: Thursday, May 05, 2005 3:31 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] question about subset
> 
> Dear R users:
> 
> I have a quick question:
> 
> is there any way to get a subset of a data frame with the 
> ordered indexes or without these indexes?
> 
> example:
> 
> mydata<- data.frame(A=seq(1,10), B=c(-5,2,-4,6,-8,-9,2,5,7,0))
> subset(mydata, B>0)
> ?? A B
> 2 2 2
> 4 4 6
> 7 7 2
> 8 8 5
> 9 9 7
> 
> I would like to obtain this:
> 
> ?? A B
> 1 2 2
> 2 4 6
> 3 7 2
> 4 8 5
> 5 9 7
> 
> or this
> 
> ?? A B
> ????2 2
> ????4 6
> ????7 2
> ????8 5
> ????9 7
> 
> I think this is possible but I dont know how...
> any sugestion?
> thanks
> 
> jose silva
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tolga.uzuner at csfb.com  Fri May  6 00:38:50 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Thu, 5 May 2005 23:38:50 +0100
Subject: [R] Numerical Derivative / Numerical Differentiation of
	unkno wn funct ion
Message-ID: <BDF571786CAD224F966FEB86BEDED52F1433DABD@elon12p32001.csfp.co.uk>

Ah... I searched for half an hour for this function... you know, the help function in R could really be a lot better...

But wait a minute... looking at this, it appears you have to pass in an expression. What if it is an unknown function, where you only have a handle to the function, but you cannot see it's implementation ? Will this work then ?

-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com]
Sent: 05 May 2005 23:34
To: 'Uzuner, Tolga'; r-help at stat.math.ethz.ch
Subject: RE: [R] Numerical Derivative / Numerical Differentiation of
unknown funct ion


But...

See ?numericDeriv which already does it via a C call and hence is much
faster (and probably more accurate,too).



-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uzuner, Tolga
> Sent: Thursday, May 05, 2005 3:21 PM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] Numerical Derivative / Numerical Differentiation 
> of unknown funct ion
> 
> Hi,
> 
> I have been trying to do numerical differentiation using R. 
> 
> I found some old S code using Richardson Extrapolation which 
> I managed to get
> to work.
> 
> I am posting it here in case anyone needs it.
> 
> 
> ##############################################################
> ##########
> richardson.grad <- function(func, x, d=0.01, eps=1e-4, r=6, show=F){
> # This function calculates a numerical approximation of the first
> #   derivative of func at the point x. The calculation
> #   is done by Richardson's extrapolation (see eg. G.R.Linfield and
> J.E.T.Penny
> #   "Microcomputers in Numerical Analysis"). The method 
> should be used if
> #   accuracy, as opposed to speed, is important.
> #
> #  *  modified by Paul Gilbert from orginal code by XINGQIAO LIU.
> # CALCULATES THE FIRST ORDER DERIVATIVE 
> #     VECTOR using a Richardson  extrapolation.
> #
> #  GENERAL APPROACH
> #     --  GIVEN THE FOLLOWING INITIAL VALUES:
> #             INTERVAL VALUE D, NUMBER OF ITERATIONS R, AND
> #             REDUCED FACTOR V.
> #      - THE FIRST ORDER aproximation to the DERIVATIVE WITH 
> RESPECT TO Xi IS
> #
> #           F'(Xi)={F(X1,...,Xi+D,...,Xn) - 
> F(X1,...,Xi-D,...,Xn)}/(2*D)
> #       
> #     --  REPEAT r TIMES  with successively smaller D  and 
> #          then apply Richardson extraplolation
> #
> #  INPUT
> #       func    Name of the function.
> #       x       The parameters of func.
> #       d       Initial interval value (real) by default set 
> to 0.01*x or
> #               eps if x is 0.0.
> #       r       The number of Richardson improvement iterations.
> #       show    If T show intermediate results.
> #  OUTPUT
> #
> #       The gradient vector.
> 
>   v <- 2               # reduction factor.
>   n <- length(x)       # Integer, number of variables.
>   a.mtr <- matrix(1, r, n) 
>   b.mtr <- matrix(1, (r - 1), n)
> #-------------------------------------------------------------
> -----------
> # 1 Calculate the derivative formula given in 'GENERAL 
> APPROACH' section.
> #   --  The first order derivatives are stored in the matrix 
> a.mtr[k,i], 
> #        where the indexing variables k for rows(1 to r),  i 
> for columns
> #       (1 to n),  r is the number of iterations, and n is 
> the number of
> #       variables.
> #-------------------------------------------------------------
> ------------  
> 
>   h <- abs(d*x)+eps*(x==0.0)
>   for(k in 1:r)  { # successively reduce h                
>      for(i in 1:n)  {
>          x1.vct <- x2.vct <- x
>          x1.vct[i]  <- x[i] + h[i]
>          x2.vct[i]  <- x[i] - h[i]
>          if(k == 1) a.mtr[k,i] <- (func(x1.vct) - 
> func(x2.vct))/(2*h[i])
>          else{
>            if(abs(a.mtr[(k-1),i])>1e-20)
>                  # some functions are unstable near 0.0              
>                  a.mtr[k,i] <- (func(x1.vct)-func(x2.vct))/(2*h[i])
>             else  a.mtr[k, i] <- 0
>           }
>       }
>      h <- h/v     # Reduced h by 1/v.
>     }	
>    if(show)  {
>         cat("\n","first order approximations", "\n")		
>         print(a.mtr, 12)
>     }
> 
> #-------------------------------------------------------------
> -----------
> # 1 Applying Richardson Extrapolation to improve the accuracy of 
> #   the first and second order derivatives. The algorithm as follows:
> #
> #   --  For each column of the 1st and 2nd order derivatives 
> matrix a.mtr,
> #       say, A1, A2, ..., Ar, by Richardson Extrapolation, to 
> calculate a
> #       new sequence of approximations B1, B2, ..., Br used 
> the formula
> #
> #          B(i) =( A(i+1)*4^m - A(i) ) / (4^m - 1) ,  i=1,2,...,r-m
> #
> #             N.B. This formula assumes v=2.
> #
> #   -- Initially m is taken as 1  and then the process is repeated 
> #      restarting with the latest improved values and increasing the 
> #      value of m by one each until m equals r-1
> #
> # 2 Display the improved derivatives for each
> #   m from 1 to r-1 if the argument show=T.
> #
> # 3 Return the final improved  derivative vector.
> #-------------------------------------------------------------
> ------------
> 
>   for(m in 1:(r - 1)) {		
>      for(i in 1:(r - m)) b.mtr[i,]<- 
> (a.mtr[(i+1),]*(4^m)-a.mtr[i,])/(4^m-1)
> #     a.mtr<- b.mtr
> #     a.mtr<- (a.mtr[2:(r+1-m),]*(4^m)-a.mtr[1:(r-m),])/(4^m-1)
>      if(show & m!=(r-1) )  {
>         cat("\n","Richarson improvement group No. ", m, "\n")		
>         print(a.mtr[1:(r-m),], 12)
>       }
>    }
> a.mtr[length(a.mtr)]
> }
> 
> ## try it out
> richardson.grad(function(x){x^3},2)
> ##############################################################
> ##########################
> 
> 
> Regards,
> Tolga Uzuner
> 
> 
> ==============================================================
> ================
> This message is for the sole use of the intended recipient...{{dropped}}



From sdavis2 at mail.nih.gov  Fri May  6 00:45:11 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 5 May 2005 18:45:11 -0400
Subject: [R] question about subset
In-Reply-To: <aaf3ac8b7a434ab4b0c32079c84d4487@oninet.pt>
References: <aaf3ac8b7a434ab4b0c32079c84d4487@oninet.pt>
Message-ID: <472f35a1c70826f11875f086c43a2b03@mail.nih.gov>


On May 5, 2005, at 6:31 PM, jose silva wrote:

> rois there any way to get a subset of a data frame with the ordered 
> indexes or without these indexes?
>
> example:
>
> mydata<- data.frame(A=seq(1,10), B=c(-5,2,-4,6,-8,-9,2,5,7,0))
> subset(mydata, B>0)
>   A B
> 2 2 2
> 4 4 6
> 7 7 2
> 8 8 5
> 9 9 7
>
> I would like to obtain this:
>
>   A B
> 1 2 2
> 2 4 6
> 3 7 2
> 4 8 5
> 5 9 7
>

 > x <- subset(mydata, B>0)
 > row.names(x) <- rank(x$A)
 > x
   A B
1 2 2
2 4 6
3 7 2
4 8 5
5 9 7

> or this
>
>   A B
>   2 2
>   4 6
>   7 2
>   8 5
>   9 7
>

I think you already got this using subset.  The row.names can be used 
to set the row names to whatever you want.

Sean



From gerifalte28 at hotmail.com  Fri May  6 00:56:33 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 05 May 2005 22:56:33 +0000
Subject: [R] Precision in R
Message-ID: <BAY103-F12EEF6386DD0FDF0D6BE5BA61A0@phx.gbl>

Did you try format(x,digits=22).  22 digits is the maximum supported.

I hope this helps

Francisco


>From: "Huntsinger, Reid" <reid_huntsinger at merck.com>
>To: "'Francisco Fuentes'" <franciscofuentes2005 at yahoo.co.uk>,        
>r-help at stat.math.ethz.ch
>Subject: RE: [R] Precision in R
>Date: Thu, 5 May 2005 14:18:50 -0400
>
>The summands can get pretty large: the 2^(2*r) factor will erase a lot of
>precision. Is there another way to compute this? Perhaps use the recurrence
>relation for the binomial coefficient to get a recurrence relation for your
>function?
>
>Reid Huntsinger
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Francisco Fuentes
>Sent: Thursday, May 05, 2005 2:02 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] Precision in R
>
>
>Could anyone help me with the following issue.
>Using the GSL library in R I define the following code:
>
>#########
>library(gsl);
>S<-function(n)
>{ r<-0:n;
>ans<-sum(gsl_sf_choose(n,r)*(-1)^r*2^(2*r)*gamma_inc(6-2*r,2))
>ans }
>#########
> >SS(10)   yields 34.91868
> >SS(40)   yields 5.340422
> >SS(60)   yields 180.3162
>Doing the same computations in maple I get
>
>34.918679360927169740821310620770402166885975646756
>5.340473872869891061658721253647686930049562214921
>2.2269219454888559341895277572725106
>
>respectively.  Is this a precision problem?
>
>
>
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From khobson at aaahawk.com  Fri May  6 00:57:59 2005
From: khobson at aaahawk.com (Kenneth Hobson)
Date: Thu, 5 May 2005 17:57:59 -0500
Subject: [R] Re: Hoaglin Outlier Method
Message-ID: <000f01c551c5$e5599e30$b335f504@okladot.state.ok.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050505/bbcb5840/attachment.pl

From vograno at evafunds.com  Fri May  6 02:36:52 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 5 May 2005 17:36:52 -0700
Subject: [R] distance between distributions
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A59E92BC@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050505/93d99d15/attachment.pl

From ipardoe at lcbmail.uoregon.edu  Fri May  6 02:49:21 2005
From: ipardoe at lcbmail.uoregon.edu (Iain Pardoe)
Date: Thu, 5 May 2005 17:49:21 -0700
Subject: [R] Multivariate multiple regression
Message-ID: <A11B20BC51EEFA41AE1516AA38CF8941016E7ECD@mail.lcb.uoregon.edu>

Thanks Henric.

If I may I'd like to go a little further ...  For example, Johnson and
Wichern's example 7.10:

> ex7.10 <-
+   data.frame(y1 = c(141.5, 168.9, 154.8, 146.5, 172.8, 160.1, 108.5),
+              y2 = c(301.8, 396.1, 328.2, 307.4, 362.4, 369.5, 229.1),
+              z1 = c(123.5, 146.1, 133.9, 128.5, 151.5, 136.2, 92),
+              z2 = c(2.108, 9.213, 1.905, .815, 1.061, 8.603, 1.125))
> attach(ex7.10)
> f.mlm <- lm(cbind(y1,y2)~z1+z2)
> y.hat <- c(1, 130, 7.5) %*% coef(f.mlm)
> round(y.hat, 2)
         y1     y2
[1,] 151.84 349.63
> qf.z <- t(c(1, 130, 7.5)) %*%
+   solve(t(cbind(1,z1,z2)) %*% cbind(1,z1,z2)) %*%
+   c(1, 130, 7.5)
> round(qf.z, 5)
        [,1]
[1,] 0.36995
> n.sigma.hat <- SSD(f.mlm)$SSD
> round(n.sigma.hat, 2)
     y1    y2
y1 5.80  5.22
y2 5.22 12.57
> F.quant <- qf(.95,2,3)
> round(F.quant, 2)
[1] 9.55

This gives me all the information I need to calculate a 95% confidence
ellipse for y=(y1,y2) at (z1,z2)=(130,7.5) using JW's equation (7-46):

(y-y.hat) %*% ((n-r-1) * solve(n.sigma.hat)) %*% t(y-y.hat)
<= qf.z * (m*(n-r-1)/(n-r-m)) * F.quant

But, what if instead I'd like to sample (y1,y2) values from this
distribution?  I can sample from an F(m,n-r-m) distribution easily
enough, but then how do I transform this to a single point in (y1,y2)
space?

Any ideas would be gratefully appreciated.  Thanks.



From p.dalgaard at biostat.ku.dk  Fri May  6 02:51:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 May 2005 02:51:08 +0200
Subject: [R] Numerical Derivative / Numerical Differentiation of unkno wn
	funct ion
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433DABD@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433DABD@elon12p32001.csfp.co.uk>
Message-ID: <x2psw5gosj.fsf@turmalin.kubism.ku.dk>

"Uzuner, Tolga" <tolga.uzuner at csfb.com> writes:

> Ah... I searched for half an hour for this function... you know, the
> help function in R could really be a lot better...
> 
> But wait a minute... looking at this, it appears you have to pass in
> an expression. What if it is an unknown function, where you only
> have a handle to the function, but you cannot see it's
> implementation ? Will this work then ?
> 
> -----Original Message-----
> From: Berton Gunter [mailto:gunter.berton at gene.com]
> Sent: 05 May 2005 23:34
> To: 'Uzuner, Tolga'; r-help at stat.math.ethz.ch
> Subject: RE: [R] Numerical Derivative / Numerical Differentiation of
> unknown funct ion
> 
> 
> But...
> 
> See ?numericDeriv which already does it via a C call and hence is much
> faster (and probably more accurate,too).
> 

The expression passed to numericDeriv can easily be a call to .C or
similar.

Actually, numericDeriv can get you in trouble if the function is not
smooth enough. It basically just calculates (f(a+d)-f(a))/d where d is
on the order of 1e-7 * a for each parameter. Sometimes a larger d and
a higher order approximation is need to avoid getting stuck in the
rough. 

(Yes, Bill, I do remember that you wanted an R News Programmer's Niche
item from me on this...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From slist at oomvanlieshout.net  Fri May  6 08:44:14 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 06 May 2005 08:44:14 +0200
Subject: [R] Plotting means and confidence intervals by group factor using
	lattice graphics?
In-Reply-To: <200505051228.36116.deepayan@stat.wisc.edu>
References: <4278EA7C.9030301@oomvanlieshout.net>
	<200505051228.36116.deepayan@stat.wisc.edu>
Message-ID: <427B123E.4010601@oomvanlieshout.net>

Thanks for your help Deepayan!

I noticed the other thread before, but I was really looking for a dot 
plot with error bars. This looks better indeed, as you state yourself in 
that thread.

The package Hmisc gives the solution through Cbind and xYplot! I will 
prepare an example graph for the gallery.

Thanks again,

Sander.

library(Hmisc)
# Examples of plotting raw data
dfr <- expand.grid(month=1:12, continent=c('Europe','USA'),
                    sex=c('female','male'))
set.seed(1)
dfr <- upData(dfr,
               y=month/10 + 1*(sex=='female') + 2*(continent=='Europe') +
                 runif(48,-.15,.15),
               lower=y - runif(48,.05,.15),
               upper=y + runif(48,.05,.15))

xYplot(Cbind(y,lower,upper) ~ month,subset=sex=='male' & continent=='USA',
        data=dfr)
xYplot(Cbind(y,lower,upper) ~ month|continent, subset=sex=='male',data=dfr)
xYplot(Cbind(y,lower,upper) ~ month|continent, groups=sex, data=dfr); Key()
# add ,label.curves=FALSE to suppress use of labcurve to label curves where
# farthest apart




Deepayan Sarkar wrote:
> On Wednesday 04 May 2005 10:30, Sander Oom wrote:
>>Dear R graphics gurus,
>>
>>Another question about lattice graphics. This time I would like to plot
>>means and confidence intervals by group factor in a lattice graph. I can
>>not find any working lattice examples. Maybe a custom panel function is
>>the answer, but that is a bit beyond me for now.
> 
> There's an example in this thread:
> 
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/50299.html
> 
> There might be useful tools in the Hmisc package as well.
> 
>>The individual plots within the lattice graph could look like this:
>>
>># Example with confidence intervals and grid
>>hh <- t(VADeaths)[, 5:1]
>>mybarcol <- "gray20"
>>ci.l <- hh * 0.85
>>ci.u <- hh * 1.15
>>mp <- barplot2(hh, beside = TRUE,
>>         col = c("lightblue", "mistyrose",
>>                 "lightcyan", "lavender"),
>>         legend = colnames(VADeaths), ylim = c(0, 100),
>>         main = "Death Rates in Virginia", font.main = 4,
>>         sub = "Faked 95 percent error bars", col.sub = mybarcol,
>>         cex.names = 1.5, plot.ci = TRUE, ci.l = ci.l, ci.u = ci.u,
>>         plot.grid = TRUE)
> 
> This gives me
> 
> Error: couldn't find function "barplot2"
> 
> Maybe you missed a library() call?
> 
> Deepayan
> 
> 
>>mtext(side = 1, at = colMeans(mp), line = -2,
>>       text = paste("Mean", formatC(colMeans(hh))), col = "red")
>>box()
>>
>>Or like this:
>>
>>data(state)
>>plotmeans(state.area ~ state.region)
>>
>>Both plotmeans and barplot2 give interesting options such as printing of
>>nobs, among other things. In case of a barplot, there should be an
>>option to plot the confidence intervals in one direction only (up) as to
>>avoid interference with any black and white shading. The plotMeans
>>function provides a useful option error.bars ("se", "sd", "conf.int",
>>"none").
>>
>>The following test data is still useful:
>>
>>tmp <- expand.grid(geology = c("Sand","Clay","Silt","Rock"),
>>   species =
>>c("ArisDiff","BracSera","CynDact","ElioMuti","EragCurS","EragPseu"),
>>   dist = seq(1,9,1) )
>>tmp$height <- rnorm(216)
>>
>>For instance plotting height versus dist by geology.
>>
>>Any help very welcome!
>>
>>Cheers,
>>
>>Sander.
>>
>>PS Of course the resulting graph will go to the R graph gallery!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From ligges at statistik.uni-dortmund.de  Fri May  6 08:53:02 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 06 May 2005 08:53:02 +0200
Subject: [R] building from source after installing binary package
In-Reply-To: <A5980E51E88D07418B8951F976FD725C8C9A16@SRVEXCH2.cnio.es>
References: <A5980E51E88D07418B8951F976FD725C8C9A16@SRVEXCH2.cnio.es>
Message-ID: <427B144E.4000305@statistik.uni-dortmund.de>

Diaz.Ramon wrote:
> Dear All,
> 
> I've got into the habit of installing R from the precompiled Debian binaries, including many of the packages from the r-cran-* Debian packages, and later building from source (e.g., to link against Goto's BLAS, or to build patched versions, etc). I install the newly built R to the very same place (/usr/lib/R). This allows me to build and update R when I wish, AND provides the ease of quickly updating many packages.
> 
> Things have always worked fine, but after a few funny problems (which could be unrelated to the process itself) I've started wondering if this is a rather silly thing to do, and if I should keep my own build separate from the Debian stuff. Any advice would be much appreciated.


Yes, simply install to another directory, e.g. by telling configure:

./configure --prefix=/I/want/to/have/R/installed/here


Uwe Ligges


> Thanks,
> 
> R.
> --
> RamÅÛn DÅÌaz-Uriarte
> Bioinformatics Unit
> Centro Nacional de Investigaciones OncolÅÛgicas (CNIO)
> (Spanish National Cancer Center)
> Melchor FernÅ·ndez Almagro, 3
> 28029 Madrid (Spain)
> Fax: +-34-91-224-6972
> Phone: +-34-91-224-6900
> 
> http://ligarto.org/rdiaz
> PGP KeyID: 0xE89B3462
> (http://ligarto.org/rdiaz/0xE89B3462.asc)
> 
> 
> 
> 
> 
> 
> 
> 
> 
> **NOTA DE CONFIDENCIALIDAD** Este correo electrÅÛnico, y en su caso los ficheros adjuntos, pueden contener informaciÅÛn protegida para el uso exclusivo de su destinatario. Se prohÅÌbe la distribuciÅÛn, reproducciÅÛn o cualquier otro tipo de transmisiÅÛn por parte de otra persona que no sea el destinatario. Si usted recibe por error este correo, se ruega comunicarlo al remitente y borrar el mensaje recibido. 
> **CONFIDENTIALITY NOTICE** This email communication and any attachments may contain confidential and privileged information for the sole use of the designated recipient named above. Distribution, reproduction or any other use of this transmission by any party other than the intended recipient is prohibited. If you are not the intended recipient please contact the sender and delete all copies.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ecu at info.fundp.ac.be  Fri May  6 09:03:26 2005
From: ecu at info.fundp.ac.be (Cuvelier Etienne)
Date: Fri, 6 May 2005 09:03:26 +0200
Subject: [R] Numerical Derivative / Numerical Differentiation of unkno
	wnfunct ion
References: <BDF571786CAD224F966FEB86BEDED52F1433DABD@elon12p32001.csfp.co.uk>
	<x2psw5gosj.fsf@turmalin.kubism.ku.dk>
Message-ID: <00c001c55209$b385c980$6600a8c0@winXP>

> > -----Original Message-----
> > From: Berton Gunter [mailto:gunter.berton at gene.com]
> > Sent: 05 May 2005 23:34
> > To: 'Uzuner, Tolga'; r-help at stat.math.ethz.ch
> > Subject: RE: [R] Numerical Derivative / Numerical Differentiation of
> > unknown funct ion
> >
> >
> > But...
> >
> > See ?numericDeriv which already does it via a C call and hence is much
> > faster (and probably more accurate,too).
> >

Is there is a similar function to calculate the numerical value of the
density of a given
multivariable distribution?
I have a function of a distribution H(x1, ...,xn) (not one of the known
distributions),
i.e.  I can calculate a value of H for any (x1..., xn) .
And I want to calculate h(x1...,xn) for  any (x1...,xn) BUT I don't know the
analytical
expression of the density H.




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From r.hankin at noc.soton.ac.uk  Fri May  6 09:27:11 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 6 May 2005 08:27:11 +0100
Subject: [R] FAQ 7.31
Message-ID: <4620f44d94d18edcd39737ee9294f276@soc.soton.ac.uk>

Jacobi's theta functions crop up here and there in various branches of 
mathematics,
such as unimodular and elliptic functions.

They have Taylor expansions, but the powers increase as the square of 
n, as in

1 + z + z^4 + z^9 + z^16 + z^25 + . . .

so they converge very quickly, provided |z|<1

The following toy function shows how I'm implementing these objects.  I 
just add terms until
they make no difference:


f <- function(z, maxiter=10){
    out.old <- 1
    for(n in 1:maxiter){
      out.new <- out.old + z^(n*n)
      if(identical(out.new, out.old)){
        return(out.new)
      }
      out.old <- out.new
    }
    stop("not converged")
  }

[NB this is not a real theta function!  Real theta functions  are more 
complicated. f() just shows the issues]

I worry about the use of identical() here, because I am comparing two 
floats for equality
as discussed in FAQ 7.31.    Perhaps all.equal() would be better:

  all.equal(1e99,1e99+1e83, tol=.Machine$double.eps)


Does the List have any comments to make?


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ripley at stats.ox.ac.uk  Fri May  6 09:48:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 May 2005 08:48:56 +0100 (BST)
Subject: [R] building from source after installing binary package
In-Reply-To: <427B144E.4000305@statistik.uni-dortmund.de>
References: <A5980E51E88D07418B8951F976FD725C8C9A16@SRVEXCH2.cnio.es>
	<427B144E.4000305@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0505060842350.8920@gannet.stats>

On Fri, 6 May 2005, Uwe Ligges wrote:

> Diaz.Ramon wrote:
>> Dear All,
>> 
>> I've got into the habit of installing R from the precompiled Debian 
>> binaries, including many of the packages from the r-cran-* Debian packages, 
>> and later building from source (e.g., to link against Goto's BLAS, or to 
>> build patched versions, etc). I install the newly built R to the very same 
>> place (/usr/lib/R). This allows me to build and update R when I wish, AND 
>> provides the ease of quickly updating many packages.
>> 
>> Things have always worked fine, but after a few funny problems (which could 
>> be unrelated to the process itself) I've started wondering if this is a 
>> rather silly thing to do, and if I should keep my own build separate from 
>> the Debian stuff. Any advice would be much appreciated.
>
>
> Yes, simply install to another directory, e.g. by telling configure:
>
> ./configure --prefix=/I/want/to/have/R/installed/here

I don't think that is the point: Ramon must have done that as the default 
installation place is /usr/local/lib/R.

I think this is a Debian-specific question (there is a R-debian list) and 
the point may be to make use of the binary Debian packages.  I would 
advocate installing R from the sources into /usr/local, and having
separate directory trees both for packages you install and for Debian 
packages.  Then you can manipulate which packages are seen via R_LIBS.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri May  6 09:53:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 May 2005 09:53:07 +0200
Subject: [R] FAQ 7.31
In-Reply-To: <4620f44d94d18edcd39737ee9294f276@soc.soton.ac.uk>
References: <4620f44d94d18edcd39737ee9294f276@soc.soton.ac.uk>
Message-ID: <x2wtqc4wpo.fsf@turmalin.kubism.ku.dk>

Robin Hankin <r.hankin at noc.soton.ac.uk> writes:

> Jacobi's theta functions crop up here and there in various branches of
> mathematics,
> such as unimodular and elliptic functions.
> 
> They have Taylor expansions, but the powers increase as the square of
> n, as in
> 
> 1 + z + z^4 + z^9 + z^16 + z^25 + . . .
> 
> so they converge very quickly, provided |z|<1
> 
> The following toy function shows how I'm implementing these objects.
> I just add terms until
> they make no difference:
> 
> 
> f <- function(z, maxiter=10){
>     out.old <- 1
>     for(n in 1:maxiter){
>       out.new <- out.old + z^(n*n)
>       if(identical(out.new, out.old)){
>         return(out.new)
>       }
>       out.old <- out.new
>     }
>     stop("not converged")
>   }
> 
> [NB this is not a real theta function!  Real theta functions  are more
> complicated. f() just shows the issues]
> 
> I worry about the use of identical() here, because I am comparing two
> floats for equality
> as discussed in FAQ 7.31.    Perhaps all.equal() would be better:
> 
>   all.equal(1e99,1e99+1e83, tol=.Machine$double.eps)
> 
> 
> Does the List have any comments to make?

Shouldn't matter in this sort of case, since the added term is bound
to underflow relative to the partial sum. 

The things to watch out for is that mathematical equality doesn't
imply numerical equality (3 * 0.1 != 0.3), and sometimes granularity
effects in correction terms. We've been bitten by code of the type

   x(n+1) = x(n) + f(x(n))

a couple of times in the numerical code, typically in Newton-type
iterations where the correction term alternates by +/- a few units in
the last place, and the program goes into an infinite loop. Optimizing
compilers can introduce such effects in otherwise functioning code by
applying mathematical transformations (see above...) to the code,
rearranging terms, moving terms outside of loops, folding constants,
etc.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri May  6 10:00:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 May 2005 09:00:30 +0100 (BST)
Subject: [R] FAQ 7.31
In-Reply-To: <4620f44d94d18edcd39737ee9294f276@soc.soton.ac.uk>
References: <4620f44d94d18edcd39737ee9294f276@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.61.0505060850110.8920@gannet.stats>

On Fri, 6 May 2005, Robin Hankin wrote:

> Jacobi's theta functions crop up here and there in various branches of 
> mathematics,
> such as unimodular and elliptic functions.
>
> They have Taylor expansions, but the powers increase as the square of n, as 
> in
>
> 1 + z + z^4 + z^9 + z^16 + z^25 + . . .
>
> so they converge very quickly, provided |z|<1
>
> The following toy function shows how I'm implementing these objects.  I just 
> add terms until
> they make no difference:
>
>
> f <- function(z, maxiter=10){
>   out.old <- 1
>   for(n in 1:maxiter){
>     out.new <- out.old + z^(n*n)
>     if(identical(out.new, out.old)){
>       return(out.new)
>     }
>     out.old <- out.new
>   }
>   stop("not converged")
> }
>
> [NB this is not a real theta function!  Real theta functions  are more 
> complicated. f() just shows the issues]
>
> I worry about the use of identical() here, because I am comparing two floats 
> for equality
> as discussed in FAQ 7.31.    Perhaps all.equal() would be better:
>
> all.equal(1e99,1e99+1e83, tol=.Machine$double.eps)

Well, probably 2*.Machine$double.eps since each of two numbers could be 
one bit from the truth.

The registers on ix86 machines have more precision than the representation 
stored in RAM.  So it is possible that out.new has more precision because 
it has been kept in registers and out.old has been retrieved from RAM and 
so has less.  The result may well be continued looping.

I think the scenario in the previous para is quite unlikely (it would need 
a very clever C compiler), but it is theoretically possible and may become 
likely with semi-compiled versions of R.  The C-level equivalent is quite 
common.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ahenningsen at email.uni-kiel.de  Fri May  6 10:00:37 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 6 May 2005 10:00:37 +0200
Subject: [R] Multivariate multiple regression
In-Reply-To: <A11B20BC51EEFA41AE1516AA38CF8941016E7ABB@mail.lcb.uoregon.edu>
References: <A11B20BC51EEFA41AE1516AA38CF8941016E7ABB@mail.lcb.uoregon.edu>
Message-ID: <200505061000.37346.ahenningsen@email.uni-kiel.de>

On Wednesday 04 May 2005 20:08, Iain Pardoe wrote:
> I'd like to model the relationship between m responses Y1, ..., Ym and a
> single set of predictor variables X1, ..., Xr.  Each response is assumed
> to follow its own regression model, and the error terms in each model
> can be correlated.  My understanding is that although lm() handles
> vector Y's on the left-hand side of the model formula, it really just
> fits m separate lm models.  What should I use to do a full multivariate
> analysis (as in section 7.7 of Johnson/Wichern)?  Thanks.

I don't know Johnson/Wichern. However, it seems to me that you want to do a 
"seemingly unrelated regression" (SUR). You can do this with the package 
"systemfit".

Arne

> Iain Pardoe <ipardoe at lcbmail.uoregon.edu>
> University of Oregon
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From rdiaz at cnio.es  Fri May  6 11:12:05 2005
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 6 May 2005 11:12:05 +0200
Subject: [R] building from source after installing binary package
In-Reply-To: <427B144E.4000305@statistik.uni-dortmund.de>
References: <A5980E51E88D07418B8951F976FD725C8C9A16@SRVEXCH2.cnio.es>
	<427B144E.4000305@statistik.uni-dortmund.de>
Message-ID: <200505061112.06254.rdiaz@cnio.es>

Dear Uwe,

Yes, sure, I understand how to install to another directory. I think I was not 
very clear, because my doubt is whether I should do that, or if it is OK to 
install to the very same place where Debian left the previous installation. 
By doing the later I save myself having to reinstall packages, etc, etc.

R.

On Friday 06 May 2005 08:53, Uwe Ligges wrote:
> Diaz.Ramon wrote:
> > Dear All,
> >
> > I've got into the habit of installing R from the precompiled Debian
> > binaries, including many of the packages from the r-cran-* Debian
> > packages, and later building from source (e.g., to link against Goto's
> > BLAS, or to build patched versions, etc). I install the newly built R to
> > the very same place (/usr/lib/R). This allows me to build and update R
> > when I wish, AND provides the ease of quickly updating many packages.
> >
> > Things have always worked fine, but after a few funny problems (which
> > could be unrelated to the process itself) I've started wondering if this
> > is a rather silly thing to do, and if I should keep my own build separate
> > from the Debian stuff. Any advice would be much appreciated.
>
> Yes, simply install to another directory, e.g. by telling configure:
>
> ./configure --prefix=/I/want/to/have/R/installed/here
>
>
> Uwe Ligges
>
> > Thanks,
> >
> > R.
> > --
> > RamÅÛn DÅÌaz-Uriarte
> > Bioinformatics Unit
> > Centro Nacional de Investigaciones OncolÅÛgicas (CNIO)
> > (Spanish National Cancer Center)
> > Melchor FernÅ·ndez Almagro, 3
> > 28029 Madrid (Spain)
> > Fax: +-34-91-224-6972
> > Phone: +-34-91-224-6900
> >
> > http://ligarto.org/rdiaz
> > PGP KeyID: 0xE89B3462
> > (http://ligarto.org/rdiaz/0xE89B3462.asc)
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > **NOTA DE CONFIDENCIALIDAD** Este correo electrÅÛnico, y en su caso los
> > ficheros adjuntos, pueden contener informaciÅÛn protegida para el uso
> > exclusivo de su destinatario. Se prohÅÌbe la distribuciÅÛn, reproducciÅÛn o
> > cualquier otro tipo de transmisiÅÛn por parte de otra persona que no sea
> > el destinatario. Si usted recibe por error este correo, se ruega
> > comunicarlo al remitente y borrar el mensaje recibido. **CONFIDENTIALITY
> > NOTICE** This email communication and any attachments may contain
> > confidential and privileged information for the sole use of the
> > designated recipient named above. Distribution, reproduction or any other
> > use of this transmission by any party other than the intended recipient
> > is prohibited. If you are not the intended recipient please contact the
> > sender and delete all copies.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html

-- 
RamÅÛn DÅÌaz-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones OncolÅÛgicas (CNIO)
(Spanish National Cancer Center)
Melchor FernÅ·ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)




**NOTA DE CONFIDENCIALIDAD** Este correo electrÅÛnico, y en su caso los ficheros adjuntos, pueden contener informaciÅÛn protegida para el uso exclusivo de su destinatario. Se prohÅÌbe la distribuciÅÛn, reproducciÅÛn o cualquier otro tipo de transmisiÅÛn por parte de otra persona que no sea el destinatario. Si usted recibe por error este correo, se ruega comunicarlo al remitente y borrar el mensaje recibido. 
**CONFIDENTIALITY NOTICE** This email communication and any attachments may contain confidential and privileged information for the sole use of the designated recipient named above. Distribution, reproduction or any other use of this transmission by any party other than the intended recipient is prohibited. If you are not the intended recipient please contact the sender and delete all copies.



From murdoch at stats.uwo.ca  Fri May  6 11:24:59 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 06 May 2005 10:24:59 +0100
Subject: [R] Numerical Derivative / Numerical Differentiation of	unkno
	wn funct ion
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433DABD@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433DABD@elon12p32001.csfp.co.uk>
Message-ID: <427B37EB.4050007@stats.uwo.ca>

Uzuner, Tolga wrote:
> Ah... I searched for half an hour for this function... you know, the help function in R could really be a lot better...

help() assumes you know the name of the thing you're looking for.  You 
should be using help.search('derivative'), which for me gave

width.SJ(MASS)          Bandwidth Selection by Pilot Estimation of
                         Derivatives
D(stats)                Symbolic and Algorithmic Derivatives of Simple
                         Expressions
numericDeriv(stats)     Evaluate derivatives numerically

How could that be better?

Duncan Murdoch



From p.campbell at econ.bbk.ac.uk  Fri May  6 12:19:25 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Fri, 06 May 2005 11:19:25 +0100
Subject: [R] distance between distributions
Message-ID: <s27b52cf.048@markets.econ.bbk.ac.uk>

I may have missed the point here but isn't this an obvious case for
using the bootstrap.  A paper by Mallows, the exact reference escapes
me, establishes the conditions under which asymtotics of the marginal
distribution imply a well behaved limit.  Perhaps a better discussion of
the issues can be found and a pair of papers by
Bickel and Freedman, see Annals Of Statistics Vol 9, Number 6.

HTH
Phineas Campbell




>>> Vadim Ogranovich <vograno at evafunds.com> 05/06/05 1:36 AM >>>
Hi,
 
This is more of a general stat question. I am looking for a easily
computable measure of a distance between two empirical distributions.
Say I have two samples x and y drawn from X and Y. I want to compute a
statistics rho(x,y) which is zero if X = Y and grows as X and Y become
less similar.
 
Kullback-Leibler distance is the most "official" choice, however it
needs estimation of the density. The estimation of the density requires
one to choose a family of the distributions to fit from or to use some
sort of non-parametric estimation. I have no intuition whether the
resulting KL distance will be sensitive to the choice of the family of
the distribution or of the fitting method.
 
Any suggestion of an alternative measure or insight into sensitivity of
the KL distance will be highly appreciated.
 
The distributions I deal with are those of stock returns and
qualitatively close to the normal dist with much fatter tails. The tails
in general should be modeled non-parametrically.
 
Thanks,
Vadim

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From robin.smit at tno.nl  Fri May  6 13:37:25 2005
From: robin.smit at tno.nl (Smit, R. (Robin) (IenT))
Date: Fri, 6 May 2005 13:37:25 +0200
Subject: [R] Change class factor to numeric
Message-ID: <2395774549BBDA40AC83BC9E6223FBFF22F95F@MS-DT01VS01.tsn.tno.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050506/bfdca607/attachment.pl

From paulojus at est.ufpr.br  Fri May  6 13:51:52 2005
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Fri, 6 May 2005 08:51:52 -0300 (BRT)
Subject: [R] Change class factor to numeric
In-Reply-To: <2395774549BBDA40AC83BC9E6223FBFF22F95F@MS-DT01VS01.tsn.tno.nl>
References: <2395774549BBDA40AC83BC9E6223FBFF22F95F@MS-DT01VS01.tsn.tno.nl>
Message-ID: <Pine.LNX.4.58L0.0505060851460.9062@est.ufpr.br>

as.numeric()



On Fri, 6 May 2005, Smit, R. (Robin) (IenT) wrote:

> I am attempting to develop a multiple regression model using selected
> model variables that should all be treated as numeric (mostly real)
> values.
> However, R considers one specific variable "mass" automatically to be of
> class "factor", probably because "mass" consists of integer values that
> are repeated.
> I now want to force R to treat "mass" as a numeric variable in the
> regression but am not sure how to do this.
>
> > class(mass) <- "numeric"
>
> does not help me.
>
> Could anyone advise me on this?
>
> Kind regards,
> Robin Smit
>
>
> --------------------------------------------
> TNO Science & Technology
>
> Business Unit Automotive
> Environmental Studies & Testing
> PO Box 6033, 2600 JA Delft
> THE NETHERLANDS
>
> ph. +31 (0)15 269 7464
> fax +31 (0)15 269 6874
> robin.smit at tno.nl
> http://www.tno.nl/industrie_en_techniek/mobiliteit_en_(transport)/milieu
> studies/environmental_studies_and/
> <http://www.tno.nl/industrie_en_techniek/mobiliteit_en_(transport)/milie
> ustudies/environmental_studies_and/>
>
> --------------------------------------------
>
>
>
>
>
> This e-mail and its contents are subject to the DISCLAIMER at http://www.tno.nl/disclaimer/email.html
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
LEG (Laborat??rio de Estat??stica e Geoinforma????o)
Departamento de Estat??stica
Universidade Federal do Paran??
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3573
Fax: (+55) 41 361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From gavin.simpson at ucl.ac.uk  Fri May  6 13:50:41 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 06 May 2005 12:50:41 +0100
Subject: [R] Change class factor to numeric
In-Reply-To: <2395774549BBDA40AC83BC9E6223FBFF22F95F@MS-DT01VS01.tsn.tno.nl>
References: <2395774549BBDA40AC83BC9E6223FBFF22F95F@MS-DT01VS01.tsn.tno.nl>
Message-ID: <427B5A11.1020101@ucl.ac.uk>

Smit, R. (Robin) (IenT) wrote:
> I am attempting to develop a multiple regression model using selected
> model variables that should all be treated as numeric (mostly real)
> values.
> However, R considers one specific variable "mass" automatically to be of
> class "factor", probably because "mass" consists of integer values that
> are repeated.
> I now want to force R to treat "mass" as a numeric variable in the
> regression but am not sure how to do this.
>  
> 
>>class(mass) <- "numeric"
> 
>  
> does not help me.
>  
> Could anyone advise me on this?

It is in the FAQ

7.10 How do I convert factors to numeric?

http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-do-I-convert-factors-to-numeric_003f

HTH

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ripley at stats.ox.ac.uk  Fri May  6 13:57:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 May 2005 12:57:12 +0100 (BST)
Subject: [R] Change class factor to numeric
In-Reply-To: <Pine.LNX.4.58L0.0505060851460.9062@est.ufpr.br>
References: <2395774549BBDA40AC83BC9E6223FBFF22F95F@MS-DT01VS01.tsn.tno.nl>
	<Pine.LNX.4.58L0.0505060851460.9062@est.ufpr.br>
Message-ID: <Pine.LNX.4.61.0505061254350.3015@gannet.stats>

On Fri, 6 May 2005, Paulo Justiniano Ribeiro Jr wrote:

> as.numeric()

No, please see FAQ 7.10.

> On Fri, 6 May 2005, Smit, R. (Robin) (IenT) wrote:
>
>> I am attempting to develop a multiple regression model using selected
>> model variables that should all be treated as numeric (mostly real)
>> values.
>> However, R considers one specific variable "mass" automatically to be of
>> class "factor", probably because "mass" consists of integer values that
>> are repeated.

Rather, you made it of class factor: R just follows instructions.

>> I now want to force R to treat "mass" as a numeric variable in the
>> regression but am not sure how to do this.
>>
>>> class(mass) <- "numeric"
>>
>> does not help me.
>>
>> Could anyone advise me on this?
>>
>> Kind regards,
>> Robin Smit
>>
>>
>> --------------------------------------------
>> TNO Science & Technology
>>
>> Business Unit Automotive
>> Environmental Studies & Testing
>> PO Box 6033, 2600 JA Delft
>> THE NETHERLANDS
>>
>> ph. +31 (0)15 269 7464
>> fax +31 (0)15 269 6874
>> robin.smit at tno.nl
>> http://www.tno.nl/industrie_en_techniek/mobiliteit_en_(transport)/milieu
>> studies/environmental_studies_and/
>> <http://www.tno.nl/industrie_en_techniek/mobiliteit_en_(transport)/milie
>> ustudies/environmental_studies_and/>
>>
>> --------------------------------------------
>>
>>
>>
>>
>>
>> This e-mail and its contents are subject to the DISCLAIMER at http://www.tno.nl/disclaimer/email.html
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>
> Paulo Justiniano Ribeiro Jr
> LEG (Laborat?rio de Estat?stica e Geoinforma??o)
> Departamento de Estat?stica
> Universidade Federal do Paran?
> Caixa Postal 19.081
> CEP 81.531-990
> Curitiba, PR  -  Brasil
> Tel: (+55) 41 361 3573
> Fax: (+55) 41 361 3141
> e-mail: paulojus at est.ufpr.br
> http://www.est.ufpr.br/~paulojus
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From petr.pikal at precheza.cz  Fri May  6 13:58:14 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 06 May 2005 13:58:14 +0200
Subject: [R] Change class factor to numeric
In-Reply-To: <Pine.LNX.4.58L0.0505060851460.9062@est.ufpr.br>
References: <2395774549BBDA40AC83BC9E6223FBFF22F95F@MS-DT01VS01.tsn.tno.nl>
Message-ID: <427B77F6.16750.6AEF00@localhost>

Hi

On 6 May 2005 at 8:51, Paulo Justiniano Ribeiro Jr wrote:

> as.numeric()

Not exactly correct.

as.numeric(as.character())

gives you what you probably want ,if "mass" is really factor ;)

see

str(mass)

Cheers
Petr


> 
> 
> 
> On Fri, 6 May 2005, Smit, R. (Robin) (IenT) wrote:
> 
> > I am attempting to develop a multiple regression model using
> > selected model variables that should all be treated as numeric
> > (mostly real) values. However, R considers one specific variable
> > "mass" automatically to be of class "factor", probably because
> > "mass" consists of integer values that are repeated. I now want to
> > force R to treat "mass" as a numeric variable in the regression but
> > am not sure how to do this.
> >
> > > class(mass) <- "numeric"
> >
> > does not help me.
> >
> > Could anyone advise me on this?
> >
> > Kind regards,
> > Robin Smit
> >
> >
> > --------------------------------------------
> > TNO Science & Technology
> >
> > Business Unit Automotive
> > Environmental Studies & Testing
> > PO Box 6033, 2600 JA Delft
> > THE NETHERLANDS
> >
> > ph. +31 (0)15 269 7464
> > fax +31 (0)15 269 6874
> > robin.smit at tno.nl
> > http://www.tno.nl/industrie_en_techniek/mobiliteit_en_(transport)/mi
> > lieu studies/environmental_studies_and/
> > <http://www.tno.nl/industrie_en_techniek/mobiliteit_en_(transport)/m
> > ilie ustudies/environmental_studies_and/>
> >
> > --------------------------------------------
> >
> >
> >
> >
> >
> > This e-mail and its contents are subject to the DISCLAIMER at
> > http://www.tno.nl/disclaimer/email.html 	[[alternative HTML version
> > deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> Paulo Justiniano Ribeiro Jr
> LEG (Laborat??rio de Estat??stica e Geoinforma????o)
> Departamento de Estat??stica
> Universidade Federal do Paran??
> Caixa Postal 19.081
> CEP 81.531-990
> Curitiba, PR  -  Brasil
> Tel: (+55) 41 361 3573
> Fax: (+55) 41 361 3141
> e-mail: paulojus at est.ufpr.br
> http://www.est.ufpr.br/~paulojus
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From r.hankin at noc.soton.ac.uk  Fri May  6 14:14:11 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 6 May 2005 13:14:11 +0100
Subject: [R] FAQ 7.31
In-Reply-To: <Pine.LNX.4.61.0505060850110.8920@gannet.stats>
References: <4620f44d94d18edcd39737ee9294f276@soc.soton.ac.uk>
	<Pine.LNX.4.61.0505060850110.8920@gannet.stats>
Message-ID: <a86ca97a716e8680792d7c60f6709f23@soc.soton.ac.uk>


On May 6, 2005, at 09:00 am, Prof Brian Ripley wrote:

> On Fri, 6 May 2005, Robin Hankin wrote:
>>
[snip]

>> I worry about the use of identical() here, because I am comparing two 
>> floats for equality
>> as discussed in FAQ 7.31.    Perhaps all.equal() would be better:
>>
>> all.equal(1e99,1e99+1e83, tol=.Machine$double.eps)
>
> Well, probably 2*.Machine$double.eps since each of two numbers could 
> be one bit from the truth.
>

Yes, this makes a lot of sense, and will likely prevent me from 
flogging a dead horse by executing
dozens of pointless iterations in pursuit of the dregs of those last 
few binary digits at the tail end of
some float (not to mention infinite loops).

I would have thought that if x, y are scalars then identical(x,y) would 
be the same as
all.equal(x,y,tol=.Machine$double.eps).

And it is, most of the time.

But this caught me out just now:

R>  x <- 5352970674736366
R>  identical(x,x+1)
[1] FALSE

R> all.equal(x,x+1,tol=.Machine$double.eps)
[1] TRUE


Thus  x and x+1  are not identical but nevertheless agree to within my 
machine precision, and
evidently my understanding of "machine precision" doesn't tally with 
Rreality.

Any comments?


> The registers on ix86 machines have more precision than the 
> representation stored in RAM.  So it is possible that out.new has more 
> precision because it has been kept in registers and out.old has been 
> retrieved from RAM and so has less.  The result may well be continued 
> looping.
>
> I think the scenario in the previous para is quite unlikely (it would 
> need a very clever C compiler), but it is theoretically possible and 
> may become likely with semi-compiled versions of R.  The C-level 
> equivalent is quite common.
>

>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From robin.smit at tno.nl  Fri May  6 14:17:56 2005
From: robin.smit at tno.nl (Smit, R. (Robin) (IenT))
Date: Fri, 6 May 2005 14:17:56 +0200
Subject: [R] conversion factor into numeric
Message-ID: <2395774549BBDA40AC83BC9E6223FBFF22F960@MS-DT01VS01.tsn.tno.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050506/56328968/attachment.pl

From christoph.lehmann at gmx.ch  Fri May  6 15:44:28 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 06 May 2005 15:44:28 +0200
Subject: [R] Change class factor to numeric
In-Reply-To: <427B77F6.16750.6AEF00@localhost>
References: <2395774549BBDA40AC83BC9E6223FBFF22F95F@MS-DT01VS01.tsn.tno.nl>
	<427B77F6.16750.6AEF00@localhost>
Message-ID: <427B74BC.302@gmx.ch>

?as.factor states:

To "revert" a factor 'f' to its original
      numeric values, 'as.numeric(levels(f))[f]' is recommended and
      slightly more efficient than 'as.numeric(as.character(f))


christoph

Petr Pikal wrote:
> Hi
> 
> On 6 May 2005 at 8:51, Paulo Justiniano Ribeiro Jr wrote:
> 
>>as.numeric()
> 
> Not exactly correct.
> 
> as.numeric(as.character())
> 
> gives you what you probably want ,if "mass" is really factor ;)
> 
> see
> 
> str(mass)
> 
> Cheers
> Petr
> 
> 
>>
>>
>>On Fri, 6 May 2005, Smit, R. (Robin) (IenT) wrote:
>>
>>>I am attempting to develop a multiple regression model using
>>>selected model variables that should all be treated as numeric
>>>(mostly real) values. However, R considers one specific variable
>>>"mass" automatically to be of class "factor", probably because
>>>"mass" consists of integer values that are repeated. I now want to
>>>force R to treat "mass" as a numeric variable in the regression but
>>>am not sure how to do this.
>>>
>>>>class(mass) <- "numeric"
>>>does not help me.
>>>
>>>Could anyone advise me on this?
>>>
>>>Kind regards,
>>>Robin Smit
>>>
>>>
>>>--------------------------------------------
>>>TNO Science & Technology
>>>
>>>Business Unit Automotive
>>>Environmental Studies & Testing
>>>PO Box 6033, 2600 JA Delft
>>>THE NETHERLANDS
>>>
>>>ph. +31 (0)15 269 7464
>>>fax +31 (0)15 269 6874
>>>robin.smit at tno.nl
>>>http://www.tno.nl/industrie_en_techniek/mobiliteit_en_(transport)/mi
>>>lieu studies/environmental_studies_and/
>>><http://www.tno.nl/industrie_en_techniek/mobiliteit_en_(transport)/m
>>>ilie ustudies/environmental_studies_and/>
>>>
>>>--------------------------------------------
>>>
>>>
>>>
>>>
>>>
>>>This e-mail and its contents are subject to the DISCLAIMER at
>>>http://www.tno.nl/disclaimer/email.html 	[[alternative HTML version
>>>deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>Paulo Justiniano Ribeiro Jr
>>LEG (Laborat??rio de Estat??stica e Geoinforma????o)
>>Departamento de Estat??stica
>>Universidade Federal do Paran??
>>Caixa Postal 19.081
>>CEP 81.531-990
>>Curitiba, PR  -  Brasil
>>Tel: (+55) 41 361 3573
>>Fax: (+55) 41 361 3141
>>e-mail: paulojus at est.ufpr.br
>>http://www.est.ufpr.br/~paulojus
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From koen.pelleriaux at gmail.com  Fri May  6 14:34:24 2005
From: koen.pelleriaux at gmail.com (Koen Pelleriaux)
Date: Fri, 6 May 2005 14:34:24 +0200
Subject: [R] conversion factor into numeric
In-Reply-To: <2395774549BBDA40AC83BC9E6223FBFF22F960@MS-DT01VS01.tsn.tno.nl>
References: <2395774549BBDA40AC83BC9E6223FBFF22F960@MS-DT01VS01.tsn.tno.nl>
Message-ID: <5a165e4f050506053444e3606@mail.gmail.com>

Robin,
It will work if you use decimal points (not comma)

HTH
Koen


On 5/6/05, Smit, R. (Robin) (IenT) <robin.smit at tno.nl> wrote:
> Thank you all for your (fast) comments.
> 
> Unfortunately I could not make the advise work:
> 
> > mass
>   [1] 800   800   800   800   800   800   800   800   800   800   800
> 800   800   800   800   800   800   800   910   910   910   910   910
> 910   910
>  [26] 910   910   910   910   910   910   910   910   910   910   910
> 910   910   910   1,020 1,020 1,020 1,020 1,020 1,020 1,020 1,020 1,020
> 1,020 1,020
>  [51] 1,020 1,020 1,020 1,020 1,020 1,020 1,020 1,130 1,130 1,130 1,130
> 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130
> 1,130 1,130
>  [76] 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250
> 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250
> 1,250 1,250
> [101] 1,250 1,250 1,250 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360
> 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360
> 1,360 1,360
> [126] 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360
> 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360
> 1,360 1,360
> [151] 1,360 1,360 1,360 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470
> 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470
> 1,470 1,470
> [176] 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470
> 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470
> 1,470 1,470
> [201] 1,470 1,470 1,470 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590
> 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590
> 1,590 1,810
> [226] 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810
> 1,810
> Levels: 1,020 1,130 1,250 1,360 1,470 1,590 1,810 800 910
> 
> > str(mass)
>  Factor w/ 9 levels "1,020","1,130",..: 8 8 8 8 8 8 8 8 8 8 ...
> 
> > as.numeric(as.character(mass))
>   [1] 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800
> 800 800 910 910 910 910 910 910 910 910 910 910 910 910 910 910 910 910
> 910 910 910 910
>  [39] 910  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA
>  [77]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA
> [115]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA
> [153]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA
> [191]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA
> [229]  NA  NA  NA  NA  NA  NA  NA  NA  NA
> Warning message:
> NAs introduced by coercion
> 
> > as.numeric(levels(mass))[as.integer(mass)]
>   [1] 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800
> 800 800 910 910 910 910 910 910 910 910 910 910 910 910 910 910 910 910
> 910 910 910 910
>  [39] 910  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA
>  [77]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA
> [115]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA
> [153]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA
> [191]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA
> [229]  NA  NA  NA  NA  NA  NA  NA  NA  NA
> Warning message:
> NAs introduced by coercion
> 
> > var.matrix$mass <- numeric(var.matrix$mass)
> Error in "$<-.data.frame"(`*tmp*`, "mass", value = c(0, 0, 0, 0, 0, 0,
> :
>         replacement has 8 rows, data has 237
> 
> Kind regards,
> Robin Smit
> 
> This e-mail and its contents are subject to the DISCLAIMER at http://www.tno.nl/disclaimer/email.html
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Koen Pelleriaux



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri May  6 14:38:57 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 6 May 2005 14:38:57 +0200
Subject: [R] conversion factor into numeric
References: <2395774549BBDA40AC83BC9E6223FBFF22F960@MS-DT01VS01.tsn.tno.nl>
Message-ID: <005701c55238$92162c20$0540210a@www.domain>

I presume that in the place of "1,020" you need "1.02". Then use 
something like this:

mass <- factor(c("800", "1,020", "1,130", "1,250"))
as.numeric(gsub(",", ".", as.character(mass)))

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Smit, R. (Robin) (IenT)" <robin.smit at tno.nl>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, May 06, 2005 2:17 PM
Subject: [R] conversion factor into numeric


> Thank you all for your (fast) comments.
>
> Unfortunately I could not make the advise work:
>
>> mass
>  [1] 800   800   800   800   800   800   800   800   800   800   800
> 800   800   800   800   800   800   800   910   910   910   910 
> 910
> 910   910
> [26] 910   910   910   910   910   910   910   910   910   910   910
> 910   910   910   1,020 1,020 1,020 1,020 1,020 1,020 1,020 1,020 
> 1,020
> 1,020 1,020
> [51] 1,020 1,020 1,020 1,020 1,020 1,020 1,020 1,130 1,130 1,130 
> 1,130
> 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 
> 1,130
> 1,130 1,130
> [76] 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 
> 1,250
> 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 
> 1,250
> 1,250 1,250
> [101] 1,250 1,250 1,250 1,360 1,360 1,360 1,360 1,360 1,360 1,360 
> 1,360
> 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 
> 1,360
> 1,360 1,360
> [126] 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 
> 1,360
> 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 
> 1,360
> 1,360 1,360
> [151] 1,360 1,360 1,360 1,470 1,470 1,470 1,470 1,470 1,470 1,470 
> 1,470
> 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 
> 1,470
> 1,470 1,470
> [176] 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 
> 1,470
> 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 
> 1,470
> 1,470 1,470
> [201] 1,470 1,470 1,470 1,590 1,590 1,590 1,590 1,590 1,590 1,590 
> 1,590
> 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 
> 1,590
> 1,590 1,810
> [226] 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810 
> 1,810
> 1,810
> Levels: 1,020 1,130 1,250 1,360 1,470 1,590 1,810 800 910
>
>> str(mass)
> Factor w/ 9 levels "1,020","1,130",..: 8 8 8 8 8 8 8 8 8 8 ...
>
>> as.numeric(as.character(mass))
>  [1] 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800
> 800 800 910 910 910 910 910 910 910 910 910 910 910 910 910 910 910 
> 910
> 910 910 910 910
> [39] 910  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA
> [77]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA
> [115]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA
> [153]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA
> [191]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA
> [229]  NA  NA  NA  NA  NA  NA  NA  NA  NA
> Warning message:
> NAs introduced by coercion
>
>> as.numeric(levels(mass))[as.integer(mass)]
>  [1] 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800
> 800 800 910 910 910 910 910 910 910 910 910 910 910 910 910 910 910 
> 910
> 910 910 910 910
> [39] 910  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA
> [77]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA
> [115]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA
> [153]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA
> [191]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
> NA
> NA  NA  NA  NA
> [229]  NA  NA  NA  NA  NA  NA  NA  NA  NA
> Warning message:
> NAs introduced by coercion
>
>> var.matrix$mass <- numeric(var.matrix$mass)
> Error in "$<-.data.frame"(`*tmp*`, "mass", value = c(0, 0, 0, 0, 0, 
> 0,
> :
>        replacement has 8 rows, data has 237
>
>
>
> Kind regards,
> Robin Smit
>
>
>
> This e-mail and its contents are subject to the DISCLAIMER at 
> http://www.tno.nl/disclaimer/email.html
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bates at stat.wisc.edu  Fri May  6 14:48:32 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 06 May 2005 07:48:32 -0500
Subject: [R] conversion factor into numeric
In-Reply-To: <005701c55238$92162c20$0540210a@www.domain>
References: <2395774549BBDA40AC83BC9E6223FBFF22F960@MS-DT01VS01.tsn.tno.nl>
	<005701c55238$92162c20$0540210a@www.domain>
Message-ID: <427B67A0.5090607@stat.wisc.edu>

Dimitris Rizopoulos wrote:
> I presume that in the place of "1,020" you need "1.02". 

In this context I believe that "1,020" means "1020" in which case the
gsub call could be as.numeric(gsub(",", "", levels(mass)))[mass]


> Then use
> something like this:
> 
> mass <- factor(c("800", "1,020", "1,130", "1,250"))
> as.numeric(gsub(",", ".", as.character(mass)))
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- From: "Smit, R. (Robin) (IenT)"
> <robin.smit at tno.nl>
> To: <r-help at stat.math.ethz.ch>
> Sent: Friday, May 06, 2005 2:17 PM
> Subject: [R] conversion factor into numeric
> 
> 
>> Thank you all for your (fast) comments.
>>
>> Unfortunately I could not make the advise work:
>>
>>> mass
>>
>>  [1] 800   800   800   800   800   800   800   800   800   800   800
>> 800   800   800   800   800   800   800   910   910   910   910 910
>> 910   910
>> [26] 910   910   910   910   910   910   910   910   910   910   910
>> 910   910   910   1,020 1,020 1,020 1,020 1,020 1,020 1,020 1,020 1,020
>> 1,020 1,020
>> [51] 1,020 1,020 1,020 1,020 1,020 1,020 1,020 1,130 1,130 1,130 1,130
>> 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130 1,130
>> 1,130 1,130
>> [76] 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250
>> 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250 1,250
>> 1,250 1,250
>> [101] 1,250 1,250 1,250 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360
>> 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360
>> 1,360 1,360
>> [126] 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360
>> 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360 1,360
>> 1,360 1,360
>> [151] 1,360 1,360 1,360 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470
>> 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470
>> 1,470 1,470
>> [176] 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470
>> 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470 1,470
>> 1,470 1,470
>> [201] 1,470 1,470 1,470 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590
>> 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590 1,590
>> 1,590 1,810
>> [226] 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810 1,810
>> 1,810
>> Levels: 1,020 1,130 1,250 1,360 1,470 1,590 1,810 800 910
>>
>>> str(mass)
>>
>> Factor w/ 9 levels "1,020","1,130",..: 8 8 8 8 8 8 8 8 8 8 ...
>>
>>> as.numeric(as.character(mass))
>>
>>  [1] 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800
>> 800 800 910 910 910 910 910 910 910 910 910 910 910 910 910 910 910 910
>> 910 910 910 910
>> [39] 910  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
>> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA
>> [77]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
>> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA
>> [115]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA
>> [153]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA
>> [191]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA
>> [229]  NA  NA  NA  NA  NA  NA  NA  NA  NA
>> Warning message:
>> NAs introduced by coercion
>>
>>> as.numeric(levels(mass))[as.integer(mass)]
>>
>>  [1] 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800 800
>> 800 800 910 910 910 910 910 910 910 910 910 910 910 910 910 910 910 910
>> 910 910 910 910
>> [39] 910  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
>> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA
>> [77]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
>> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA
>> [115]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA
>> [153]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA
>> [191]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA NA
>> NA  NA  NA  NA
>> [229]  NA  NA  NA  NA  NA  NA  NA  NA  NA
>> Warning message:
>> NAs introduced by coercion
>>
>>> var.matrix$mass <- numeric(var.matrix$mass)
>>
>> Error in "$<-.data.frame"(`*tmp*`, "mass", value = c(0, 0, 0, 0, 0, 0,
>> :
>>        replacement has 8 rows, data has 237



From weigand.stephen at charter.net  Fri May  6 15:04:40 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Fri, 6 May 2005 08:04:40 -0500
Subject: [R] Histogram for mixed random variables
In-Reply-To: <6ade6f6c05050504431afa4a90@mail.gmail.com>
References: <6ade6f6c05050504431afa4a90@mail.gmail.com>
Message-ID: <f3ca51ad4132cea74e3b15984a0b7472@charter.net>

Dear Paul,

On May 5, 2005, at 6:43 AM, Paul Smith wrote:

> Dear All
>
> I would like to get the histogram for the following model with
> discrete and continuous random variables:
>
> * with probability 1/3, a random number is drawn from the continuous
> uniform distribution (min=0, max=1);
> * with probability 2/3, a random number is drawn from a different
> continuous uniform distribution (min=-1/2, max=1/2).
>
> Any help would be most welcome.
>
> Thanks in advance,
>
> Paul

My approach to generate the data would be

rMyfun <- function(n){
   x1 <- runif(n, 0, 1)
   x2 <- runif(n, -0.5, 0.5)
   ifelse(rbinom(n, 1, 1/3), x1, x2)
}

Hope this helps,

Stephen



From bioconductor.cn at gmail.com  Fri May  6 15:21:49 2005
From: bioconductor.cn at gmail.com (Xiao Shi)
Date: Fri, 6 May 2005 21:21:49 +0800
Subject: [R] how to get such a subset of a matrix?
Message-ID: <cedaa40b050506062164a956e1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050506/e48b586b/attachment.pl

From fooms at euroscreen.be  Fri May  6 15:31:13 2005
From: fooms at euroscreen.be (=?iso-8859-1?Q?Fr=E9d=E9ric_Ooms?=)
Date: Fri, 6 May 2005 15:31:13 +0200
Subject: [R] R for HTS data analysis
Message-ID: <5198ADA420721246BC35BFA666E24F16D8D813@euromail.euroscreen.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050506/6a677860/attachment.pl

From r.hankin at noc.soton.ac.uk  Fri May  6 15:31:45 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 6 May 2005 14:31:45 +0100
Subject: [R] how to get such a subset of a matrix?
In-Reply-To: <cedaa40b050506062164a956e1@mail.gmail.com>
References: <cedaa40b050506062164a956e1@mail.gmail.com>
Message-ID: <dc807b94a0a0fcbd61a17b5a2c36dd98@soc.soton.ac.uk>



R> a <- matrix(1:36,6,6)
R> is.na(a[row(a)<col(a)]) <- TRUE
R> a
      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1   NA   NA   NA   NA   NA
[2,]    2    8   NA   NA   NA   NA
[3,]    3    9   15   NA   NA   NA
[4,]    4   10   16   22   NA   NA
[5,]    5   11   17   23   29   NA
[6,]    6   12   18   24   30   36

 > a[apply(a,1,function(x){sum(is.na(x))<=2}),]
      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    4   10   16   22   NA   NA
[2,]    5   11   17   23   29   NA
[3,]    6   12   18   24   30   36
 >

Hope this helps

rksh

On May 6, 2005, at 02:21 pm, Xiao Shi wrote:

> Hi everybody,
> Maybe this question is quite simple but i just don't know how to make 
> it.
> I have a matrix a somewhat like this one but bigger:
>> a
> f g h i j k
> a NA NA 11 16 21 26
> b NA NA 12 17 22 27
> c NA 8 13 18 23 28
> d NA 9 14 19 24 29
> e NA 10 15 20 25 30
> And i want to get the rows which at most have 2 Na.
> Thanks in advance.
> Shi Jiantao
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri May  6 15:36:51 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 6 May 2005 15:36:51 +0200
Subject: [R] how to get such a subset of a matrix?
References: <cedaa40b050506062164a956e1@mail.gmail.com>
Message-ID: <00f501c55240$a8cc6800$0540210a@www.domain>

one way to do it is:

mat <- matrix(rnorm(20*20), 20, 20)
mat[sample(400, 40)] <- NA
#####
mat
mat[rowSums(is.na(mat)) <= 2, ]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Xiao Shi" <bioconductor.cn at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, May 06, 2005 3:21 PM
Subject: [R] how to get such a subset of a matrix?


> Hi everybody,
> Maybe this question is quite simple but i just don't know how to 
> make it.
> I have a matrix a somewhat like this one but bigger:
>> a
> f g h i j k
> a NA NA 11 16 21 26
> b NA NA 12 17 22 27
> c NA 8 13 18 23 28
> d NA 9 14 19 24 29
> e NA 10 15 20 25 30
> And i want to get the rows which at most have 2 Na.
> Thanks in advance.
> Shi Jiantao
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Fri May  6 15:34:47 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 06 May 2005 06:34:47 -0700
Subject: [R] how to get such a subset of a matrix?
In-Reply-To: <cedaa40b050506062164a956e1@mail.gmail.com>
References: <cedaa40b050506062164a956e1@mail.gmail.com>
Message-ID: <427B7277.8010305@pdf.com>



Xiao Shi wrote on 5/6/2005 6:21 AM:
> Hi everybody,
> Maybe this question is quite simple but i just don't know how to make it.
> I have a matrix a somewhat like this one but bigger:
> 
>>a
> 
> f g h i j k
> a NA NA 11 16 21 26
> b NA NA 12 17 22 27
> c NA 8 13 18 23 28
> d NA 9 14 19 24 29
> e NA 10 15 20 25 30
> And i want to get the rows which at most have 2 Na.
> Thanks in advance.
> Shi Jiantao
> 


How about:

a[rowSums(is.na(a)) < 2, ]

HTH,

--sundar



From andy_liaw at merck.com  Fri May  6 15:42:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 6 May 2005 09:42:03 -0400
Subject: [R] how to get such a subset of a matrix?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E7F5@usctmx1106.merck.com>

Try:

  a[rowSums(is.na(a)) <= 2, ]

Andy

> From: Xiao Shi
> 
> Hi everybody,
> Maybe this question is quite simple but i just don't know how 
> to make it.
> I have a matrix a somewhat like this one but bigger:
> > a
> f g h i j k
> a NA NA 11 16 21 26
> b NA NA 12 17 22 27
> c NA 8 13 18 23 28
> d NA 9 14 19 24 29
> e NA 10 15 20 25 30
> And i want to get the rows which at most have 2 Na.
> Thanks in advance.
> Shi Jiantao
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From sasprog474 at yahoo.com  Fri May  6 16:18:08 2005
From: sasprog474 at yahoo.com (Greg Tarpinian)
Date: Fri, 6 May 2005 07:18:08 -0700 (PDT)
Subject: [R] persp( ) Question
Message-ID: <20050506141809.86051.qmail@web41422.mail.yahoo.com>

I have successfully fitted the model

    loess.fit1 <- loess(response ~ X*Y)

and plotted it in 3D using


X.grid <- seq(0,10,length=100)
Y.grid <- seq(0,1000,length=100)
pred.loess1 <- predict(loess.fit1, 
        expand.grid(x = X.grid, y = Y.grid))
persp(X.grid, Y.grid, pred.loess1, theta = 0, phi =
12)


I would like to add a series of points along the
fitted surface at X.grid = 2, in red.  The example
in the online help uses


f <- function(x,y) 
  { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
trans3d <- function(x,y,z, pmat) 
{
  tr <- cbind(x,y,z,1) %*% pmat
  list(x = tr[,1]/tr[,4], y= tr[,2]/tr[,4])
}
phi <- seq(0, 2*pi, len = 201)
r1 <- 7.725 # radius of 2nd maximum
xr <- r1 * cos(phi)
yr <- r1 * sin(phi)
lines(trans3d(xr,yr, f(xr,yr), res), 
      col = "pink", lwd=2)


to add points to the fitted mathematical surface. I
have not been able to reproduce this sort of 
functionality partly because I don't have a functional
representation of my surface and partly because I
don't understand what trans3d( ) is doing.  Any help
would be greatly appreciated.


Kind regards,

    Greg




		

Stay connected, organized, and protected. Take the tour:



From csad4933 at uibk.ac.at  Fri May  6 16:57:32 2005
From: csad4933 at uibk.ac.at (Sebastian Schoenherr)
Date: Fri,  6 May 2005 16:57:32 +0200
Subject: [R] R-help Digest
In-Reply-To: <200505061012.j46A2xSH029976@hypatia.math.ethz.ch>
References: <200505061012.j46A2xSH029976@hypatia.math.ethz.ch>
Message-ID: <1115391452.427b85dc823cd@web-mail2.uibk.ac.at>

Hi folks,
I have to create my own time series, Is it possible to generate ARIMA time
series, where i can define the range of the values in the y axis. (e.g: Values
only between 0 and 1)

Best regards
Sebastian



From rvaradha at jhsph.edu  Fri May  6 16:59:02 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 6 May 2005 10:59:02 -0400
Subject: [R] Numerical Derivative / Numerical Differentiation of unkno
	wnfunct ion
In-Reply-To: <x2psw5gosj.fsf@turmalin.kubism.ku.dk>
Message-ID: <OWA-2J6gX2XdoYfYoiB00006dc0@owa-2.sph.ad.jhsph.edu>

Hi,

There is a nice article by Fornberg and Sloan (Acta Numerica 1994) on
various order accuracy (Taylor-series based) approximations for different
order derivatives. I had coded a couple of these in R for first and second
order derivatives, with truncation errors of orders 2 and 4.  

Here is the code and a simple example demonstrating its accuracy for a
sharply oscillating function:

############################################
# A function to compute highly accurate first- and second-order derivatives
# From Fornberg and Sloan (Acta Numerica, 1994, p. 203-267; Table 1, page
213)
deriv <- function(x, fun, h=NULL, order=1, accur=4) {
macheps <- .Machine$double.eps

if (order==1) {
if(is.null(h)) h <- macheps^(1/3)* abs(x)
ifelse (accur==2, w <- c(-1/2,1/2), w <- c(1/12,-2/3, 2/3,-1/12))
ifelse (accur==2, xd <- x + h*c(-1,1), xd <- x + h*c(-2,-1,1,2))
return(sum(w*fun(xd))/h)
}
else if (order==2) {
if(is.null(h)) h <- macheps^(1/4)* abs(x)
ifelse (accur==2, w <- c(1,-2,1), w <- c(-1/12,4/3,-5/2,4/3,-1/12))
ifelse (accur==2, xd <- x + h*c(-1,0,1), xd <- x + h*c(-2,-1,0,1,2))
return(sum(w*fun(xd))/h^2)
}
}
############################################
func1 <- function(x){
sin(10*x) - exp(-x)
}
#############################################
> curve(func1,from=0,to=5)

> x <- 2.04
# first order derivative 
> numd1 <- deriv(x,f=func1)
> exact <- 10*cos(10*x) + exp(-x)
> c(numd1,exact,numd1/exact-1)
[1] 3.335371e-01 3.335371e-01 1.981793e-11
# second order derivative 
> numd2 <- deriv(x,f=func1,order=2)
> exact <- -100*sin(10*x) - exp(-x)
> c(numd2,exact,numd2/exact-1)
[1] -1.001093e+02 -1.001093e+02 -2.300948e-11

Hope this is helpful.

Best,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Thursday, May 05, 2005 8:51 PM
> To: Uzuner, Tolga
> Cc: 'r-help at stat.math.ethz.ch'; 'Berton Gunter'
> Subject: Re: [R] Numerical Derivative / Numerical Differentiation of unkno
> wnfunct ion
> 
> "Uzuner, Tolga" <tolga.uzuner at csfb.com> writes:
> 
> > Ah... I searched for half an hour for this function... you know, the
> > help function in R could really be a lot better...
> >
> > But wait a minute... looking at this, it appears you have to pass in
> > an expression. What if it is an unknown function, where you only
> > have a handle to the function, but you cannot see it's
> > implementation ? Will this work then ?
> >
> > -----Original Message-----
> > From: Berton Gunter [mailto:gunter.berton at gene.com]
> > Sent: 05 May 2005 23:34
> > To: 'Uzuner, Tolga'; r-help at stat.math.ethz.ch
> > Subject: RE: [R] Numerical Derivative / Numerical Differentiation of
> > unknown funct ion
> >
> >
> > But...
> >
> > See ?numericDeriv which already does it via a C call and hence is much
> > faster (and probably more accurate,too).
> >
> 
> The expression passed to numericDeriv can easily be a call to .C or
> similar.
> 
> Actually, numericDeriv can get you in trouble if the function is not
> smooth enough. It basically just calculates (f(a+d)-f(a))/d where d is
> on the order of 1e-7 * a for each parameter. Sometimes a larger d and
> a higher order approximation is need to avoid getting stuck in the
> rough.
> 
> (Yes, Bill, I do remember that you wanted an R News Programmer's Niche
> item from me on this...)
> 
> --
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From rdiaz at cnio.es  Fri May  6 17:00:30 2005
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 6 May 2005 17:00:30 +0200
Subject: [R] building from source after installing binary package
In-Reply-To: <Pine.LNX.4.61.0505060842350.8920@gannet.stats>
References: <A5980E51E88D07418B8951F976FD725C8C9A16@SRVEXCH2.cnio.es>
	<427B144E.4000305@statistik.uni-dortmund.de>
	<Pine.LNX.4.61.0505060842350.8920@gannet.stats>
Message-ID: <200505061700.30184.rdiaz@cnio.es>

On Friday 06 May 2005 09:48, Prof Brian Ripley wrote:
> On Fri, 6 May 2005, Uwe Ligges wrote:
> > Diaz.Ramon wrote:
> >> Dear All,
> >>
> >> I've got into the habit of installing R from the precompiled Debian
> >> binaries, including many of the packages from the r-cran-* Debian
> >> packages, and later building from source (e.g., to link against Goto's
> >> BLAS, or to build patched versions, etc). I install the newly built R to
> >> the very same place (/usr/lib/R). This allows me to build and update R
> >> when I wish, AND provides the ease of quickly updating many packages.
> >>
> >> Things have always worked fine, but after a few funny problems (which
> >> could be unrelated to the process itself) I've started wondering if this
> >> is a rather silly thing to do, and if I should keep my own build
> >> separate from the Debian stuff. Any advice would be much appreciated.
> >
> > Yes, simply install to another directory, e.g. by telling configure:
> >
> > ./configure --prefix=/I/want/to/have/R/installed/here
>
> I don't think that is the point: Ramon must have done that as the default
> installation place is /usr/local/lib/R.

Yes, I did change the --prefix because Debian installs to /usr/lib.

> I think this is a Debian-specific question (there is a R-debian list) and
> the point may be to make use of the binary Debian packages.  I would

Yes, that is correct (I guess I was not being very clear... too late last 
night). I'll ask in the debian list; I asked here just in case people with 
other GNU/Linux distributions did (or did not) do similar things.

> advocate installing R from the sources into /usr/local, and having
> separate directory trees both for packages you install and for Debian
> packages.  Then you can manipulate which packages are seen via R_LIBS.

Thanks. I'll try that.

Best,

R.
-- 
Ram??n D??az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol??gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern??ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)




**NOTA DE CONFIDENCIALIDAD** Este correo electr??nico, y en su caso los ficheros adjuntos, pueden contener informaci??n protegida para el uso exclusivo de su destinatario. Se proh??be la distribuci??n, reproducci??n o cualquier otro tipo de transmisi??n por parte de otra persona que no sea el destinatario. Si usted recibe por error este correo, se ruega comunicarlo al remitente y borrar el mensaje recibido. 
**CONFIDENTIALITY NOTICE** This email communication and any attachments may contain confidential and privileged information for the sole use of the designated recipient named above. Distribution, reproduction or any other use of this transmission by any party other than the intended recipient is prohibited. If you are not the intended recipient please contact the sender and delete all copies.



From sms13+ at pitt.edu  Fri May  6 17:07:43 2005
From: sms13+ at pitt.edu (sms13+@pitt.edu)
Date: Fri, 06 May 2005 11:07:43 -0400
Subject: [R] 2 simple questions
Message-ID: <773926390.1115377663@Lab26.DOMAIN.IE.PITT.EDU>

Please excuse what I'm sure are very easy questions but I'm relatively new 
to the R environment.
How can I view a range of list elements, but not all.  e.g., I had a matrix 
of patients and then split them out by patient id.  I know I can do 
patlist[[1]] to see the first one, but how can I view, say, the first ten 
patients?

My other question is how to count how many patients have a record in which 
a certain condition holds.  E.g., I was trying something like this to get a 
count:
ctr<-0
temp<-lapply(mylist, function(x){is.na(x$date1[1]) & !is.na(x$date2[1])) 
ctr<-ctr+1})

But I don't think that's working correctly.

Thanks,
Steven



From neuro3000 at hotmail.com  Fri May  6 17:16:32 2005
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Fri, 06 May 2005 11:16:32 -0400
Subject: [R] Percent (%) format in barplot
Message-ID: <BAY104-F1F1F409FA1AAA45B98E47AF1B0@phx.gbl>

Percent format in barplot

Hello,

In a barplot, I need:

1-The xaxis labels to be in percent format with one decimal. Example: 1.5%

barplot(height, horiz = TRUE, names=as.character(data$name))

2- The data labels to be in the same format (percent format with one 
decimal)

text(x,y,as.character(x),pos=4)

How do I do that?

Thanks



From ripley at stats.ox.ac.uk  Fri May  6 17:17:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 May 2005 16:17:17 +0100 (BST)
Subject: [R] R-help Digest
In-Reply-To: <1115391452.427b85dc823cd@web-mail2.uibk.ac.at>
References: <200505061012.j46A2xSH029976@hypatia.math.ethz.ch>
	<1115391452.427b85dc823cd@web-mail2.uibk.ac.at>
Message-ID: <Pine.LNX.4.61.0505061613190.27052@gannet.stats>

On Fri, 6 May 2005, Sebastian Schoenherr wrote:

> Hi folks,
> I have to create my own time series, Is it possible to generate ARIMA time
> series, where i can define the range of the values in the y axis. (e.g: Values
> only between 0 and 1)

No.

Take a look at the definition of an ARIMA process.  Suppose e.g. you have 
an AR(1) process.  Then if innovations are positive and the coefficient is 
positive the value can be arbitrarily large.  You can construct all sorts 
of similar counter-examples.

This isn't a real problem is it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gunter.berton at gene.com  Fri May  6 17:31:40 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 6 May 2005 08:31:40 -0700
Subject: [R] 2 simple questions
In-Reply-To: <773926390.1115377663@Lab26.DOMAIN.IE.PITT.EDU>
Message-ID: <200505061531.j46FVe2q013164@ohm.gene.com>

Your questions are confusing because you don't seem to understand R's basic
data structures and manipulations. Please read the "Introduction to R"
manual and appropriate portions of the R languuage definition before
posting.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of sms13+ at pitt.edu
> Sent: Friday, May 06, 2005 8:08 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] 2 simple questions
> 
> Please excuse what I'm sure are very easy questions but I'm 
> relatively new 
> to the R environment.
> How can I view a range of list elements, but not all.  e.g., 
> I had a matrix 
> of patients and then split them out by patient id.  I know I can do 
> patlist[[1]] to see the first one, but how can I view, say, 
> the first ten 
> patients?
> 
> My other question is how to count how many patients have a 
> record in which 
> a certain condition holds.  E.g., I was trying something like 
> this to get a 
> count:
> ctr<-0
> temp<-lapply(mylist, function(x){is.na(x$date1[1]) & 
> !is.na(x$date2[1])) 
> ctr<-ctr+1})
> 
> But I don't think that's working correctly.
> 
> Thanks,
> Steven
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Fri May  6 17:38:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 06 May 2005 17:38:20 +0200
Subject: [R] Percent (%) format in barplot
In-Reply-To: <BAY104-F1F1F409FA1AAA45B98E47AF1B0@phx.gbl>
References: <BAY104-F1F1F409FA1AAA45B98E47AF1B0@phx.gbl>
Message-ID: <427B8F6C.3010704@statistik.uni-dortmund.de>

Neuro LeSuperH??ros wrote:

> Percent format in barplot
> 
> Hello,
> 
> In a barplot, I need:
> 
> 1-The xaxis labels to be in percent format with one decimal. Example: 1.5%
> 
> barplot(height, horiz = TRUE, names=as.character(data$name))
> 
> 2- The data labels to be in the same format (percent format with one 
> decimal)
> 
> text(x,y,as.character(x),pos=4)
> 
> How do I do that?


What about

   paste(formatC(x, 1, format="f"), "%", sep="")

given x is scaled in percent, of course.


Uwe Ligges



> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri May  6 17:42:30 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 06 May 2005 17:42:30 +0200
Subject: [R] 2 simple questions
In-Reply-To: <773926390.1115377663@Lab26.DOMAIN.IE.PITT.EDU>
References: <773926390.1115377663@Lab26.DOMAIN.IE.PITT.EDU>
Message-ID: <427B9066.3010805@statistik.uni-dortmund.de>

sms13+ at pitt.edu wrote:

> Please excuse what I'm sure are very easy questions but I'm relatively 
> new to the R environment.
> How can I view a range of list elements, but not all.  e.g., I had a 
> matrix of patients and then split them out by patient id.  I know I can 
> do patlist[[1]] to see the first one, but how can I view, say, the first 
> ten patients?

Use vector indexing on patlist such as
   patlist[1:10]
which return a list of the first 10 elements.
Please read the docs on object indexing.


> My other question is how to count how many patients have a record in 
> which a certain condition holds.  E.g., I was trying something like this 
> to get a count:
> ctr<-0
> temp<-lapply(mylist, function(x){is.na(x$date1[1]) & !is.na(x$date2[1])) 
> ctr<-ctr+1})

No, lapply is NOT a loop, instead think vectorized:

   ## at first look at a logical vector, where the condition holds:
   temp <- sapply(mylist, function(x)
       (is.na(x$date1[1]) & !is.na(x$date2[1])))
   ctr <- sum(temp) # and sum that vector

Uwe Ligges



> But I don't think that's working correctly.
> 
> Thanks,
> Steven
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From blindglobe at gmail.com  Fri May  6 18:33:54 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Fri, 6 May 2005 18:33:54 +0200
Subject: [R] R for HTS data analysis
In-Reply-To: <5198ADA420721246BC35BFA666E24F16D8D813@euromail.euroscreen.be>
References: <5198ADA420721246BC35BFA666E24F16D8D813@euromail.euroscreen.be>
Message-ID: <1abe3fa90505060933cdfca37@mail.gmail.com>

Define HTS please -- if you mean HCS/HTS, then there are a few
packages in BioConductor which will help (prada, rflowcyt).

On 5/6/05, Fr??d??ric Ooms <fooms at euroscreen.be> wrote:
> Hello,
> I am looking for any packages, tutorials, documents,... about the use of R for the analysis of HTS data.
> Thanks for your help
> Fred
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From mzp3769 at yahoo.com  Fri May  6 19:52:53 2005
From: mzp3769 at yahoo.com (m p)
Date: Fri, 6 May 2005 10:52:53 -0700 (PDT)
Subject: [R] plotting image/contour on irregular grid
Message-ID: <20050506175254.85680.qmail@web51001.mail.yahoo.com>

Hello,
I'd like to make a z(x,y) plot for irregularly spaced
x,y. What are routines are available in R for this
purpose? 
Thanks,
Mark



From David.Brahm at geodecapital.com  Fri May  6 20:16:19 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Fri, 6 May 2005 14:16:19 -0400
Subject: [R] persp( ) Question
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBBE@MSGBOSCLF2WIN.DMN1.FMR.COM>

Greg,

Assign the output of "persp" to a variable "pmat":
R> pmat <- persp(X.grid, Y.grid, pred.loess1, theta=0, phi=12)

Now you can add points to your plot with the usual "points" command.
But you have to translate your 3D coordinates (x,y,z) into 2D
coordinates for "points" to understand, and that's what "trans3d" does:

R> points(trans3d(x,y,z, pmat), col="red")

You supply the (x,y,z) values, of course.  It's a mystery to me why
"trans3d" is not included in the graphics package (where "persp" lives).
HTH.

-- David Brahm (brahm at alum.mit.edu)



From Zack.Apoian at sac.com  Fri May  6 20:56:21 2005
From: Zack.Apoian at sac.com (Apoian, Zack)
Date: Fri, 6 May 2005 14:56:21 -0400
Subject: [R] data frame output in loops
Message-ID: <62B869B2AEF6624EB237643D7B81DEBA01B10F48@MAILISNY2.saccap.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050506/b4dc9b79/attachment.pl

From dr.mike at ntlworld.com  Fri May  6 21:04:13 2005
From: dr.mike at ntlworld.com (Mike Waters)
Date: Fri, 6 May 2005 20:04:13 +0100
Subject: FW: [R] distance between distributions
Message-ID: <20050506190418.FDTA1352.aamta04-winn.mailhost.ntl.com@d600>

Sorry, forgot to send this to the list originally.

-----Original Message-----
From: Mike Waters [mailto:dr.mike at ntlworld.com] 
Sent: 06 May 2005 18:40
To: 'Campbell'
Subject: RE: [R] distance between distributions

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Campbell
Sent: 06 May 2005 11:19
To: vograno; r-help
Subject: Re: [R] distance between distributions

I may have missed the point here but isn't this an obvious case for using
the bootstrap.  A paper by Mallows, the exact reference escapes me,
establishes the conditions under which asymtotics of the marginal
distribution imply a well behaved limit.  Perhaps a better discussion of the
issues can be found and a pair of papers by Bickel and Freedman, see Annals
Of Statistics Vol 9, Number 6.

HTH
Phineas Campbell




>>> Vadim Ogranovich <vograno at evafunds.com> 05/06/05 1:36 AM >>>
Hi,
 
This is more of a general stat question. I am looking for a easily
computable measure of a distance between two empirical distributions.
Say I have two samples x and y drawn from X and Y. I want to compute a
statistics rho(x,y) which is zero if X = Y and grows as X and Y become less
similar.
 
Kullback-Leibler distance is the most "official" choice, however it needs
estimation of the density. The estimation of the density requires one to
choose a family of the distributions to fit from or to use some sort of
non-parametric estimation. I have no intuition whether the resulting KL
distance will be sensitive to the choice of the family of the distribution
or of the fitting method.
 
Any suggestion of an alternative measure or insight into sensitivity of the
KL distance will be highly appreciated.
 
The distributions I deal with are those of stock returns and qualitatively
close to the normal dist with much fatter tails. The tails in general should
be modeled non-parametrically.
 
Thanks,
Vadim

	[[alternative HTML version deleted]]

____________________________________________________________________________
_____________

Was this the reference you were thinking of?

C. L. Mallows. A note on asymptotic joint normality. Annals of Mathematical
Statistics,43(2):508-515, 1972

Another reference that might be of relevance is:

Bootstrap Methods for the Nonparametric Assessment of Population
Bioequivelance and Similarity of Distributions. Czado C. and Munk A. ; this
can be obtained as a postscript document from:
http://www-m4.ma.tum.de/Papers/Czado/simrev.ps

HTH

Mike

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From davidr at rhotrading.com  Fri May  6 21:06:25 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Fri, 6 May 2005 14:06:25 -0500
Subject: [R] Choices from a matrix
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FAEC8@rhosvr02.rhotrading.com>

Could someone please suggest a more clever solution to the following problem than my loop below?

Given X a 2xN matrix X, and I a k-subset of N, 
Generate the (2^k)xN matrix Y with columns not in I all zero and the other columns with all choices of an entry from the first or second row of X.

For example, with
X <- matrix(1:8, nrow=2)
I <- c(1,3)

X is
1 3 5 7
2 4 6 8

and Y should be
1 0 5 0
2 0 5 0
1 0 6 0
2 0 6 0

The order of the rows is unimportant.
---
I solved this using a loop over the rows of Y after forming some preliminary matrices. I think it could be improved.

N <- NCOL(X)
k <- length(I)
G <- as.matrix(expand.grid(rep(list(c(1,2)),k)))
Y <- matrix(0,nc=N,nr=NROW(G))

for(i in 1:NROW(G)){
   ind <- rep(1,N)
   ind[I] <- G[i,]
   Y[i,] <- X[array(c(ind,1:N),dim=c(N,2))]
}
Y[,-I] <- 0




David L. Reiner
??
Rho Trading
440 S. LaSalle St -- Suite 620
Chicago?? IL?? 60605
??
312-362-4963 (voice)
312-362-4941 (fax)
??



From dpowers at mail.la.utexas.edu  Fri May  6 21:38:44 2005
From: dpowers at mail.la.utexas.edu (Daniel A. Powers)
Date: Fri, 06 May 2005 14:38:44 -0500
Subject: [R] bivariate normal cdf
Message-ID: <427BC7C4.3080607@mail.la.utexas.edu>


-- R Help List --

I am looking for a bivariate normal cdf routine in R. I have some fortran routines for this, which appear to be based on 15-point quadrature. Any guidance/suggestions on making these in loadable R-functions would be appreciated.

Thanks,
Dan
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Daniel A. Powers, Ph.D.  
Department of Sociology 
University of Texas at Austin                    
1 University Station A1700
Austin, TX  78712-0118
phone: 512-232-6335
fax:   512-471-1748
dpowers at mail.la.utexas.edu



From Dan.Kelley at Dal.Ca  Fri May  6 21:39:42 2005
From: Dan.Kelley at Dal.Ca (Dan Kelley)
Date: Fri, 6 May 2005 16:39:42 -0300
Subject: [R] R on Macintosh OSX Tiger
Message-ID: <998FB46D-5D67-4129-8C26-8547465959F6@Dal.Ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050506/dd5e409c/attachment.pl

From Roger.Bivand at nhh.no  Fri May  6 21:45:39 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 6 May 2005 21:45:39 +0200 (CEST)
Subject: [R] plotting image/contour on irregular grid
In-Reply-To: <20050506175254.85680.qmail@web51001.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0505062140330.25599-100000@reclus.nhh.no>

On Fri, 6 May 2005, m p wrote:

> Hello,
> I'd like to make a z(x,y) plot for irregularly spaced
> x,y. What are routines are available in R for this
> purpose? 

One possibility is to interpolate a regular grid using interp() in the 
akima package, then use image() or contour(). Another is to use 
levelplot() with formula z ~ x + y in the lattice package, and the 
equivalent contourplot(); here, the x,y pairs must lie on a grid, but do 
not need to fill the grid (so are regularly spaced with missing grid 
cells).

> Thanks,
> Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From gunter.berton at gene.com  Fri May  6 21:48:59 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 6 May 2005 12:48:59 -0700
Subject: [R] Choices from a matrix
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A5FAEC8@rhosvr02.rhotrading.com>
Message-ID: <200505061949.j46JmxpQ007025@volta.gene.com>

If I understand you correctly, here's one way based on expand.grid().

I is just an index set, and so all you really need to do is generate your
2^k rows from the part of the matrix you're using in the right places via
replacement:  

e.g. newX<-matrix(0, ncol=ncol(X),nrow=2^length(I))
newX[,I]<-expand.grid(as.list(as.data.frame(X[,I]))) 


N.B. I tried to do the this without the explicit as.list() cast, but got an
error message. I would have thought that expand.grid should have recognized
that a data.frame IS a list without the cast.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> davidr at rhotrading.com
> Sent: Friday, May 06, 2005 12:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Choices from a matrix
> 
> Could someone please suggest a more clever solution to the 
> following problem than my loop below?
> 
> Given X a 2xN matrix X, and I a k-subset of N, 
> Generate the (2^k)xN matrix Y with columns not in I all zero 
> and the other columns with all choices of an entry from the 
> first or second row of X.
> 
> For example, with
> X <- matrix(1:8, nrow=2)
> I <- c(1,3)
> 
> X is
> 1 3 5 7
> 2 4 6 8
> 
> and Y should be
> 1 0 5 0
> 2 0 5 0
> 1 0 6 0
> 2 0 6 0
> 
> The order of the rows is unimportant.
> ---
> I solved this using a loop over the rows of Y after forming 
> some preliminary matrices. I think it could be improved.
> 
> N <- NCOL(X)
> k <- length(I)
> G <- as.matrix(expand.grid(rep(list(c(1,2)),k)))
> Y <- matrix(0,nc=N,nr=NROW(G))
> 
> for(i in 1:NROW(G)){
>    ind <- rep(1,N)
>    ind[I] <- G[i,]
>    Y[i,] <- X[array(c(ind,1:N),dim=c(N,2))]
> }
> Y[,-I] <- 0
> 
> 
> 
> 
> David L. Reiner
> ??
> Rho Trading
> 440 S. LaSalle St -- Suite 620
> Chicago?? IL?? 60605
> ??
> 312-362-4963 (voice)
> 312-362-4941 (fax)
> ??
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From i.visser at uva.nl  Fri May  6 21:50:17 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Fri, 06 May 2005 15:50:17 -0400
Subject: [R] bivariate normal cdf
In-Reply-To: <427BC7C4.3080607@mail.la.utexas.edu>
Message-ID: <BEA142B9.4072%i.visser@uva.nl>

The Writing R-extensions Manual tells you all about how to compile/build and
then load shared libraries into R.
hth, ingmar visser

On 5/6/05 3:38 PM, "Daniel A. Powers" <dpowers at mail.la.utexas.edu> wrote:

> 
> -- R Help List --
> 
> I am looking for a bivariate normal cdf routine in R. I have some fortran
> routines for this, which appear to be based on 15-point quadrature. Any
> guidance/suggestions on making these in loadable R-functions would be
> appreciated.
> 
> Thanks,
> Dan
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> Daniel A. Powers, Ph.D.
> Department of Sociology
> University of Texas at Austin
> 1 University Station A1700
> Austin, TX  78712-0118
> phone: 512-232-6335
> fax:   512-471-1748
> dpowers at mail.la.utexas.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From uofiowa at gmail.com  Fri May  6 21:55:21 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 6 May 2005 15:55:21 -0400
Subject: [R] CGIwithR ERROR: unknown GUI none
Message-ID: <3f87cc6d05050612554c87ca3e@mail.gmail.com>

I write the simplest cgi example in R but I keep getting 

ERROR: unknown GUI none

any idea?



From moriadoc at mickeyfan.com  Fri May  6 21:52:50 2005
From: moriadoc at mickeyfan.com (tad sneathen)
Date: Fri, 06 May 2005 19:52:50 +0000
Subject: [R] Three simple steps help you sav a lot on quality tablets.
Message-ID: <5E3818D4.857EFBD@mickeyfan.com>


   "Select  from  a  wide  variety of brand name and generic rneds. It is
   legitimate   to   select   licensed   chemist-sites   to   place   the
   or-der.,Customers  have  better  selections for rnedicals on ereection
   dysfunction, pain, man's care, highcholesterol, stress and obesity. It
   is easier to stay healthier."
   [1]Embrace super value and check quality medicals on promo.

   "Check  our  store  if  you  prefer  timely  and reliable distribution
   services. Check the lovvprices on all items at our store."
   -----original message-----
   From: Kristopher at djfo.com [mailto:Mark at dw.com]
   Sent: Thursday, March 8, 2005 0:55PM
   To: Kendrick; Kendrick at x.com; ; Trent; Alfredo
   Subject:  At our store, customers can gget their case profile reviewed
   for ffree.
   It  is  quicker.  It  is  easier. It is rnore convenient. It is such a
   great choice for me and it is to shoppe for rneds at your store. Thank
   you for providing this nevv model for rned or-der. -Jane D. in NM
   stained   with   their  red  juice.  Here  was  a  barrow  and  yonder
   another.Then  c  ll  one  and  the  same in the whole dozen.It is most
   extraordinary good fortune."Years p
   ere  digging  the  foundation;  and the clerk has said it had no value
   except in being an old olumns of smoke rose into the still air; it was
   a heath fire,they told him- how brightly
   it blazed in the dark evening!The fourth day came, and the
   funeral  festivities  were  at  an  end;they  were to go back from the
   uneven  0  would  land-dunes to the sand-dunes."Ours are better," said
   the  old  fisherman, Jurg 1 down she en's foster-father;"these have no
   strength."And they spoke of the way in which the sand-dunes

References

   1. http://IW.6k.anewdoorforyou.com/ks/


From gerifalte28 at hotmail.com  Fri May  6 22:02:05 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 06 May 2005 20:02:05 +0000
Subject: [R] data frame output in loops
In-Reply-To: <62B869B2AEF6624EB237643D7B81DEBA01B10F48@MAILISNY2.saccap.int>
Message-ID: <BAY103-F99FDBDD16417DBCF4715EA61B0@phx.gbl>

No need for computer-intensive loops.  Try the following:

a <- data.frame(x = c(1,2,3,4), y = c(1,2,1,2))
xx=data.frame(x.1=a[,1],x.2=2*a[,1])
yy=data.frame(y.1=a[,2],y.2=2*a[,2])

Cheers

Francisco


>From: "Apoian, Zack" <Zack.Apoian at sac.com>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] data frame output in loops
>Date: Fri, 6 May 2005 14:56:21 -0400
>
>I know this is very basic--I'm wondering if there is a  way to write data 
>frames as outputs from a loop.
>In other words, take this simple example:
>
> > a <- data.frame(x = c(1,2,3,4), y = c(1,2,1,2))
>
>Given a, how would you write a loop that creates two data frames, x and y, 
>where the first column is a column of a and the second column is two times 
>the first column, or:
>
> > x
>   x.1 x.2
>1   1   2
>2   2   4
>3   3   6
>4   4   8
>
>
>and
>
> > y
>   y.1 y.2
>1   1   2
>2   2   4
>3   1   2
>4   2   4
>
>
>
>
>DISCLAIMER: This e-mail message and any attachments are inte...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From JAROSLAW.W.TUSZYNSKI at saic.com  Fri May  6 22:03:03 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 6 May 2005 16:03:03 -0400 
Subject: [R] Latex can not find Rd.sty
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4051@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050506/8fd449bc/attachment.pl

From rvaradha at jhsph.edu  Fri May  6 22:02:51 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 6 May 2005 16:02:51 -0400
Subject: [R] Numerical Derivative / Numerical Differentiation of
	unknownfunct ion
In-Reply-To: <00c001c55209$b385c980$6600a8c0@winXP>
Message-ID: <OWA-2eo4lB4BwOZxYS000006fa7@owa-2.sph.ad.jhsph.edu>

In general, any n-th order partial derivative can be approximated by forming
the appropriate tensor product of n univariate approximations.  If each
univariate approximation is based on a two-point central difference (which
involves 2 function evaluations), then the total number of function
evaluations in the tensor product is 2^n.  So, if you have a bivariate
distribution, then its density is simply the second-order cross partial
derivative, which can be evaluated accurately with 4 function evaluations.
You can see that this problem quickly becomes non-trivial due to curse of
dimensionality.  

Hope this helps.

Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Cuvelier Etienne
> Sent: Friday, May 06, 2005 3:03 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Numerical Derivative / Numerical Differentiation of
> unknownfunct ion
> 
> > > -----Original Message-----
> > > From: Berton Gunter [mailto:gunter.berton at gene.com]
> > > Sent: 05 May 2005 23:34
> > > To: 'Uzuner, Tolga'; r-help at stat.math.ethz.ch
> > > Subject: RE: [R] Numerical Derivative / Numerical Differentiation of
> > > unknown funct ion
> > >
> > >
> > > But...
> > >
> > > See ?numericDeriv which already does it via a C call and hence is much
> > > faster (and probably more accurate,too).
> > >
> 
> Is there is a similar function to calculate the numerical value of the
> density of a given
> multivariable distribution?
> I have a function of a distribution H(x1, ...,xn) (not one of the known
> distributions),
> i.e.  I can calculate a value of H for any (x1..., xn) .
> And I want to calculate h(x1...,xn) for  any (x1...,xn) BUT I don't know
> the
> analytical
> expression of the density H.
> 
> 
> 
> 
> --
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From dpowers at mail.la.utexas.edu  Fri May  6 22:04:41 2005
From: dpowers at mail.la.utexas.edu (Daniel A. Powers)
Date: Fri, 6 May 2005 15:04:41 -0500 (CDT)
Subject: [R] bivariate normal cdf
In-Reply-To: <427BC7C4.3080607@mail.la.utexas.edu>
Message-ID: <Pine.GSO.4.33.0505061503200.20293-100000@honoria.la.utexas.edu>

R-Help --

 mvtnorm



Thanks.
Dan

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Daniel A. Powers, Ph.D.
Department of Sociology
University of Texas at Austin
1 University Station A1700
Austin, TX  78712-0118
phone: 512-232-6335
fax:   512-471-1748
dpowers at mail.la.utexas.edu


On Fri, 6 May 2005, Daniel A. Powers wrote:

>
> -- R Help List --
>
> I am looking for a bivariate normal cdf routine in R. I have some fortran routines for this, which appear to be based on 15-point quadrature. Any guidance/suggestions on making these in loadable R-functions would be appreciated.
>
> Thanks,
> Dan
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> Daniel A. Powers, Ph.D.
> Department of Sociology
> University of Texas at Austin
> 1 University Station A1700
> Austin, TX  78712-0118
> phone: 512-232-6335
> fax:   512-471-1748
> dpowers at mail.la.utexas.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From goedman at mac.com  Fri May  6 22:09:00 2005
From: goedman at mac.com (Rob J Goedman)
Date: Fri, 6 May 2005 13:09:00 -0700
Subject: [R] R on Macintosh OSX Tiger
In-Reply-To: <998FB46D-5D67-4129-8C26-8547465959F6@Dal.Ca>
References: <998FB46D-5D67-4129-8C26-8547465959F6@Dal.Ca>
Message-ID: <E70EF8CF-ED59-41DF-9E1A-E7C2E296674C@mac.com>

Dan,

When installing Tiger, libxml.2.2.dylib version 9.0.0 should have been
installed.

Just a guess from my side, but did you update X11 while upgrading to  
Tiger
(you have to select that when the upgrade process shows customize at
the bottom). Did you install the new Xcode version from the u/g cd?

Rob


On May 6, 2005, at 12:39 PM, Dan Kelley wrote:


> Hi.  I'm not having any luck installing the binary for the new
> Macintosh OS called "Tiger".
>
> I get error messages as below.
>
> Can anyone offer me some advice?  (PS: I checked the bug list first
> but a search on "Tiger" turned up nothing so I hope it's OK to post
> here.)
>
>
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>          unable to load shared library '/Library/Frameworks/
> R.framework/Resources/library/grDevices/libs/grDevices.so':
>    dlopen(/Library/Frameworks/R.framework/Resources/library/grDevices/
> libs/grDevices.so, 6): Library not loaded: /usr/lib/libxml2.2.dylib
>    Referenced from: /System/Library/Frameworks/AppKit.framework/
> Versions/C/AppKit
>    Reason: Incompatible library version: AppKit requires version
> 9.0.0 or later, but libxml2.2.dylib provides version 8.0.0
> Loading required package: grDevices
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>          unable to load shared library '/Library/Frameworks/
> R.framework/Resources/library/grDevices/libs/grDevices.so':
>    dlopen(/Library/Frameworks/R.framework/Resources/library/grDevices/
> libs/grDevices.so, 6): corrupt binary, library ordinal too big
> In addition: Warning message:
> package grDevices in options("defaultPackages") was not found
> Error: package 'grDevices' could not be loaded
>
>
> Dan E. Kelley, Associate Professor                phone:(902)494-1694
> Oceanography Department, Dalhousie University       fax:(902)494-2885
> Halifax, Nova Scotia                         mailto:Dan.Kelley at Dal.CA
> Canada B3H 4J1   http://www.phys.ocean.dal.ca/~kelley/Kelley_Dan.html
>
>
>     [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>
>



From davidr at rhotrading.com  Fri May  6 22:11:15 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Fri, 6 May 2005 15:11:15 -0500
Subject: [R] Choices from a matrix
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FAED0@rhosvr02.rhotrading.com>

Thanks, Bert, but when I tried that, I didn't get a matrix:
> newX
[[1]]
[1] 1 2 1 2

[[2]]
[1] 5 5 6 6

[[3]]
[1] 1 2 1 2

[[4]]
[1] 5 5 6 6

[[5]]
[1] 0

[[6]]
[1] 0

[[7]]
[1] 0

[[8]]
[1] 0

[[9]]
[1] 1 2 1 2

[[10]]
[1] 5 5 6 6

[[11]]
[1] 1 2 1 2

[[12]]
[1] 5 5 6 6

[[13]]
[1] 0

[[14]]
[1] 0

[[15]]
[1] 0

[[16]]
[1] 0

The matrix is in there, four times in fact. For larger problems, I believe it would be in 2^k times? It is certainly a one-liner, though, so if the matrix could be extracted simply, it could be of use.

Best regards,
David

-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com] 
Sent: Friday, May 06, 2005 2:49 PM
To: David Reiner <davidr at rhotrading.com>; r-help at stat.math.ethz.ch
Subject: RE: [R] Choices from a matrix

If I understand you correctly, here's one way based on expand.grid().

I is just an index set, and so all you really need to do is generate your
2^k rows from the part of the matrix you're using in the right places via
replacement:  

e.g. newX<-matrix(0, ncol=ncol(X),nrow=2^length(I))
newX[,I]<-expand.grid(as.list(as.data.frame(X[,I]))) 


N.B. I tried to do the this without the explicit as.list() cast, but got an
error message. I would have thought that expand.grid should have recognized
that a data.frame IS a list without the cast.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> davidr at rhotrading.com
> Sent: Friday, May 06, 2005 12:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Choices from a matrix
> 
> Could someone please suggest a more clever solution to the 
> following problem than my loop below?
> 
> Given X a 2xN matrix X, and I a k-subset of N, 
> Generate the (2^k)xN matrix Y with columns not in I all zero 
> and the other columns with all choices of an entry from the 
> first or second row of X.
> 
> For example, with
> X <- matrix(1:8, nrow=2)
> I <- c(1,3)
> 
> X is
> 1 3 5 7
> 2 4 6 8
> 
> and Y should be
> 1 0 5 0
> 2 0 5 0
> 1 0 6 0
> 2 0 6 0
> 
> The order of the rows is unimportant.
> ---
> I solved this using a loop over the rows of Y after forming 
> some preliminary matrices. I think it could be improved.
> 
> N <- NCOL(X)
> k <- length(I)
> G <- as.matrix(expand.grid(rep(list(c(1,2)),k)))
> Y <- matrix(0,nc=N,nr=NROW(G))
> 
> for(i in 1:NROW(G)){
>    ind <- rep(1,N)
>    ind[I] <- G[i,]
>    Y[i,] <- X[array(c(ind,1:N),dim=c(N,2))]
> }
> Y[,-I] <- 0
> 
> 
> 
> 
> David L. Reiner
> ??
> Rho Trading
> 440 S. LaSalle St -- Suite 620
> Chicago?? IL?? 60605
> ??
> 312-362-4963 (voice)
> 312-362-4941 (fax)
> ??
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From davidr at rhotrading.com  Fri May  6 22:17:24 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Fri, 6 May 2005 15:17:24 -0500
Subject: [R] Choices from a matrix
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FAED1@rhosvr02.rhotrading.com>

Bert, that was almost it, we just need one more as.matrix:

newX[,I]<-as.matrix(expand.grid(as.list(as.data.frame(X[,I]))))

This works as advertised.

Best regards,
David

p.s. I agree that the extra as.list is counterintuitive.
It might be nice if there were a matrix form of expand.grid.

-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com] 
Sent: Friday, May 06, 2005 2:49 PM
To: David Reiner <davidr at rhotrading.com>; r-help at stat.math.ethz.ch
Subject: RE: [R] Choices from a matrix

If I understand you correctly, here's one way based on expand.grid().

I is just an index set, and so all you really need to do is generate your
2^k rows from the part of the matrix you're using in the right places via
replacement:  

e.g. newX<-matrix(0, ncol=ncol(X),nrow=2^length(I))
newX[,I]<-expand.grid(as.list(as.data.frame(X[,I]))) 


N.B. I tried to do the this without the explicit as.list() cast, but got an
error message. I would have thought that expand.grid should have recognized
that a data.frame IS a list without the cast.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> davidr at rhotrading.com
> Sent: Friday, May 06, 2005 12:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Choices from a matrix
> 
> Could someone please suggest a more clever solution to the 
> following problem than my loop below?
> 
> Given X a 2xN matrix X, and I a k-subset of N, 
> Generate the (2^k)xN matrix Y with columns not in I all zero 
> and the other columns with all choices of an entry from the 
> first or second row of X.
> 
> For example, with
> X <- matrix(1:8, nrow=2)
> I <- c(1,3)
> 
> X is
> 1 3 5 7
> 2 4 6 8
> 
> and Y should be
> 1 0 5 0
> 2 0 5 0
> 1 0 6 0
> 2 0 6 0
> 
> The order of the rows is unimportant.
> ---
> I solved this using a loop over the rows of Y after forming 
> some preliminary matrices. I think it could be improved.
> 
> N <- NCOL(X)
> k <- length(I)
> G <- as.matrix(expand.grid(rep(list(c(1,2)),k)))
> Y <- matrix(0,nc=N,nr=NROW(G))
> 
> for(i in 1:NROW(G)){
>    ind <- rep(1,N)
>    ind[I] <- G[i,]
>    Y[i,] <- X[array(c(ind,1:N),dim=c(N,2))]
> }
> Y[,-I] <- 0
> 
> 
> 
> 
> David L. Reiner
> ??
> Rho Trading
> 440 S. LaSalle St -- Suite 620
> Chicago?? IL?? 60605
> ??
> 312-362-4963 (voice)
> 312-362-4941 (fax)
> ??
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From apjaworski at mmm.com  Fri May  6 22:19:01 2005
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Fri, 6 May 2005 15:19:01 -0500
Subject: [R] slowness of plot(x, type='l')
Message-ID: <OF5E3EC117.95122074-ON86256FF9.006DC5DC-86256FF9.006F9AE2@mmm.com>





A couple of days ago a few messages indicated that something changed in the
basic plot routine that made plot(*, type='l') slow for large data sets.
Some people even reported crashes for very large data sets.  As far as I
remember, this was not reported as a formal bug.

I am still not sure if this is a bug, so I report my findings here.  First
of all, I think I see a slowdown of the plot function, although I do not
have older versions of R installed, so I cannot do side-by-side
comparisons.  Secondly, I noticed that the behavior of plot(*, type='l')
differs.  Before R-2.1, the plotted lines would appear on the plot
gradually.  Now, after the wait, the whole plot appears at once.

Here are my timing results.  I am on Windows2000, IBM Intellistation with
Xenon 2.8MHz with 1Gb of memory.  I checked May-06 versions of R-patched
and R-devel built from sources.  I ran the following simple test:

x <- rnorm(n)
date(); plot(x, type='l'); date()

Here are the timings:

      n     seconds
      5000  1
      6000  2
      7000  4
      8000  6
      9000  9
      10000 13
      12000 22
      14000 36
      20000 91

It looks like only type='l' and type='o' exhibit this behavior.  All other
types produce plots in approximately 1 second.  Also, the (long) wait and
plot at once behavior happens with the two types mentioned.  All others
(except 'n' of course) produce gradually appearing plots.

Hope this helps,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122



From gunter.berton at gene.com  Fri May  6 22:27:52 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 6 May 2005 13:27:52 -0700
Subject: [R] Choices from a matrix
Message-ID: <200505062027.j46KRqlT017442@faraday.gene.com>

Quite right (ALWAYS TEST, Bert!), because expand.grid() returns a
data.frame. But all you have to do is create newX as a data.frame:

newX<-data.frame(matrix(0, ncol=ncol(X),nrow=2^length(I)))

and **now** it will work:

>X<-matrix(1:6,nr=2)
>I<- c(1,3)
>newX<-data.frame(matrix(0, ncol=ncol(X),nrow=2^length(I)))
>newX[,I]<-expand.grid(as.list(as.data.frame(X[,I])))
>newX

  X1 X2 X3
1  1  0  5
2  2  0  5
3  1  0  6
4  2  0  6

Note that newX is a data.frame -- you may wish to recast it as a matrix. Of
course, I'm still not sure that this is what you wanted.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: bgunter 
> Sent: Friday, May 06, 2005 12:49 PM
> To: davidr at rhotrading.com; r-help at stat.math.ethz.ch
> Subject: RE: [R] Choices from a matrix
> 
> If I understand you correctly, here's one way based on expand.grid().
> 
> I is just an index set, and so all you really need to do is 
> generate your 2^k rows from the part of the matrix you're 
> using in the right places via replacement:  
> 
> e.g. newX<-matrix(0, ncol=ncol(X),nrow=2^length(I))
> newX[,I]<-expand.grid(as.list(as.data.frame(X[,I]))) 
> 
> 
> N.B. I tried to do the this without the explicit as.list() 
> cast, but got an error message. I would have thought that 
> expand.grid should have recognized that a data.frame IS a 
> list without the cast.
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the 
> scientific learning process."  - George E. P. Box
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > davidr at rhotrading.com
> > Sent: Friday, May 06, 2005 12:06 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Choices from a matrix
> > 
> > Could someone please suggest a more clever solution to the 
> > following problem than my loop below?
> > 
> > Given X a 2xN matrix X, and I a k-subset of N, 
> > Generate the (2^k)xN matrix Y with columns not in I all zero 
> > and the other columns with all choices of an entry from the 
> > first or second row of X.
> > 
> > For example, with
> > X <- matrix(1:8, nrow=2)
> > I <- c(1,3)
> > 
> > X is
> > 1 3 5 7
> > 2 4 6 8
> > 
> > and Y should be
> > 1 0 5 0
> > 2 0 5 0
> > 1 0 6 0
> > 2 0 6 0
> > 
> > The order of the rows is unimportant.
> > ---
> > I solved this using a loop over the rows of Y after forming 
> > some preliminary matrices. I think it could be improved.
> > 
> > N <- NCOL(X)
> > k <- length(I)
> > G <- as.matrix(expand.grid(rep(list(c(1,2)),k)))
> > Y <- matrix(0,nc=N,nr=NROW(G))
> > 
> > for(i in 1:NROW(G)){
> >    ind <- rep(1,N)
> >    ind[I] <- G[i,]
> >    Y[i,] <- X[array(c(ind,1:N),dim=c(N,2))]
> > }
> > Y[,-I] <- 0
> > 
> > 
> > 
> > 
> > David L. Reiner
> > ??
> > Rho Trading
> > 440 S. LaSalle St -- Suite 620
> > Chicago?? IL?? 60605
> > ??
> > 312-362-4963 (voice)
> > 312-362-4941 (fax)
> > ??
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
>



From roger at ysidro.econ.uiuc.edu  Fri May  6 22:39:00 2005
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Fri, 6 May 2005 15:39:00 -0500
Subject: [R] plotting image/contour on irregular grid
In-Reply-To: <Pine.LNX.4.44.0505062140330.25599-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0505062140330.25599-100000@reclus.nhh.no>
Message-ID: <C27E9D59-24B6-4B7C-A228-6E42D1EE3F22@ysidro.econ.uiuc.edu>



On May 6, 2005, at 2:45 PM, Roger Bivand wrote:

> On Fri, 6 May 2005, m p wrote:
>
>> Hello,
>> I'd like to make a z(x,y) plot for irregularly spaced
>> x,y. What are routines are available in R for this
>> purpose?
>>
>
> One possibility is to interpolate a regular grid using interp() in the
> akima package, then use image() or contour(). Another is to use
> levelplot() with formula z ~ x + y in the lattice package, and the
> equivalent contourplot(); here, the x,y pairs must lie on a grid,  
> but do
> not need to fill the grid (so are regularly spaced with missing grid
> cells).
>>

You could also try tripack and rgl.triangles to produce piecewise linear
surfaces on the Delaunay triangulation of the x,y points.

Roger



From bates at stat.wisc.edu  Fri May  6 22:41:11 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 06 May 2005 15:41:11 -0500
Subject: [R] R News, 2005, issue 1
Message-ID: <427BD667.7000906@stat.wisc.edu>

RNews_2005-1.pdf has been uploaded to CRAN.  It should be on the main
CRAN site Saturday and on the mirrors by Sunday or Monday.

This issue is being distributed as PDF only.  Please email
rnews-editors at r-project.org if not having a gzipped PostScript version
will be a problem for you.

My thanks to all the contributors to R News and especially to the
editorial team.  We hope you will enjoy this issue.

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From pgilbert at bank-banque-canada.ca  Fri May  6 23:33:45 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 06 May 2005 17:33:45 -0400
Subject: [R] Numerical Derivative / Numerical Differentiation of unknown
	funct ion
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433DABB@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433DABB@elon12p32001.csfp.co.uk>
Message-ID: <427BE2B9.4040809@bank-banque-canada.ca>

A current version of this code is in the dseplus bundle in the devel
section of CRAN. It does Richardson's extrapolation, which gives an
accurate numerical estimate at the expense of speed. (It does a very
large number of function evaluations.) That may not be what you want.

Paul Gilbert

Uzuner, Tolga wrote:


>Hi,
>
>I have been trying to do numerical differentiation using R. 
>
>I found some old S code using Richardson Extrapolation which I managed to get
>to work.
>
>I am posting it here in case anyone needs it.
>
>
>########################################################################
>richardson.grad <- function(func, x, d=0.01, eps=1e-4, r=6, show=F){
># This function calculates a numerical approximation of the first
>#   derivative of func at the point x. The calculation
>#   is done by Richardson's extrapolation (see eg. G.R.Linfield and
>J.E.T.Penny
>#   "Microcomputers in Numerical Analysis"). The method should be used if
>#   accuracy, as opposed to speed, is important.
>#
>#  *  modified by Paul Gilbert from orginal code by XINGQIAO LIU.
>...
>



From ggrothendieck at gmail.com  Sat May  7 02:27:17 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 6 May 2005 20:27:17 -0400
Subject: [R] Latex can not find Rd.sty
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F4051@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F4051@us-arlington-0668.mail.saic.com>
Message-ID: <971536df05050617271c287583@mail.gmail.com>

On 5/6/05, Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI at saic.com> wrote:
> Hi,
> 
> Lately my Latex engine used for checking packages stop working, and I can
> not figure out how to fix it.
> When creating my package (caMassClass) I get the
> 
>   * checking caMassClass-manual.tex ... ERROR
>   LaTeX errors when creating DVI version.
>   This typically indicates Rd problems.
> 
> message. I look in caMassClass-manual.log file and find
> 
>   ! LaTeX Error: File `Rd.sty' not found.
> 
> message.
> 
> So I went to "R Installation and Administration" manual which recommended
> "http://www.murdoch-sutherland.com/Rtools/miktex.html
> <http://www.murdoch-sutherland.com/Rtools/miktex.html> " website to fix this
> exact problem. This page suggested "findtexmf Rd.sty" command to "check
> whether it is finding Rd.sty in the right place". When I try that I get:
> 
>   C:\>findtexmf Rd.sty
>   C:\programs\R\rw2010\share\texmf\Rd.sty
> 
> Which is the correct place where the file lives. I also tried workarounds 1
> and 4, but it did not help.

The findtexmf command was part of the Workaround #4 recommendation 
for which the correct answer to findtexmf Rd.sty should be:

    C:\localtexmf\tex\Rd.sty



From 0034058 at fudan.edu.cn  Sat May  7 02:55:01 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 07 May 2005 08:55:01 +0800
Subject: [R] slowness of plot(x, type='l')
In-Reply-To: <OF5E3EC117.95122074-ON86256FF9.006DC5DC-86256FF9.006F9AE2@mmm.com>
References: <OF5E3EC117.95122074-ON86256FF9.006DC5DC-86256FF9.006F9AE2@mmm.com>
Message-ID: <20050507085502.025d4703@localhost.localdomain>

On Fri, 06 May 2005 15:19:01 -0500
apjaworski at mmm.com wrote:

> 
> 
> 
> 
> A couple of days ago a few messages indicated that something changed in the
> basic plot routine that made plot(*, type='l') slow for large data sets.
> Some people even reported crashes for very large data sets.  As far as I
> remember, this was not reported as a formal bug.
> 
> I am still not sure if this is a bug, so I report my findings here.  First
> of all, I think I see a slowdown of the plot function, although I do not
> have older versions of R installed, so I cannot do side-by-side
> comparisons.  Secondly, I noticed that the behavior of plot(*, type='l')
> differs.  Before R-2.1, the plotted lines would appear on the plot
> gradually.  Now, after the wait, the whole plot appears at once.
> 
> Here are my timing results.  I am on Windows2000, IBM Intellistation with
> Xenon 2.8MHz with 1Gb of memory.  I checked May-06 versions of R-patched
> and R-devel built from sources.  I ran the following simple test:
> 
> x <- rnorm(n)
> date(); plot(x, type='l'); date()
> 
> Here are the timings:
> 
>       n     seconds
>       5000  1
>       6000  2
>       7000  4
>       8000  6
>       9000  9
>       10000 13
>       12000 22
>       14000 36
>       20000 91
~~~~~~~~~~~~~~~~~~~~~~~
I have no such porblem,my OS is debian,256M ram and 2Gswap.

> x <- rnorm(200000)
> date(); plot(x, type='l'); date()
[1] "Sat May  7 08:49:18 2005"
[1] "Sat May  7 08:49:20 2005"
> version
         _
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R


> It looks like only type='l' and type='o' exhibit this behavior.  All other
> types produce plots in approximately 1 second.  Also, the (long) wait and
> plot at once behavior happens with the two types mentioned.  All others
> (except 'n' of course) produce gradually appearing plots.
> 
> Hope this helps,
> 
> Andy
> 
> __________________________________
> Andy Jaworski
> 518-1-01
> Process Laboratory
> 3M Corporate Research Laboratory
> -----
> E-mail: apjaworski at mmm.com
> Tel:  (651) 733-6092
> Fax:  (651) 736-3122
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mwgrant2001 at yahoo.com  Sat May  7 07:27:19 2005
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Fri, 6 May 2005 22:27:19 -0700 (PDT)
Subject: [R] plotting image/contour on irregular grid
In-Reply-To: 6667
Message-ID: <20050507052719.78676.qmail@web52009.mail.yahoo.com>


--- roger koenker <roger at ysidro.econ.uiuc.edu> wrote:
> 
> 
> On May 6, 2005, at 2:45 PM, Roger Bivand wrote:
> 
> > On Fri, 6 May 2005, m p wrote:
> >
> >> Hello,
> >> I'd like to make a z(x,y) plot for irregularly
> spaced
> >> x,y. What are routines are available in R for
> this
> >> purpose?
> >>
> >
> > One possibility is to interpolate a regular grid
> using interp() in the
> > akima package, then use image() or contour().
> Another is to use
> > levelplot() with formula z ~ x + y in the lattice
> package, and the
> > equivalent contourplot(); here, the x,y pairs must
> lie on a grid,  
> > but do
> > not need to fill the grid (so are regularly spaced
> with missing grid
> > cells).
> >>
> 
> You could also try tripack and rgl.triangles to
> produce piecewise linear
> surfaces on the Delaunay triangulation of the x,y
> points.
> 

Or perhaps try gridding first using one of the kriging
packages. 

It is just a little thing, but the method you decide
to use is dependent on how you intend to use/represent
the result ;O). Then there is the need for judgement,
and then judgement, and oh, yes, more judgement. 
Density of points, clustering, blah, blah,... . 

Why do I say this? My experience with data if this
nature is that you really do have to be careful or you
may have yourself and others interpreting artifacts.

If you are at the beginning of getting involved in
some serious application(s) and time investment, you
should explore the 'art and craft' of
estimation....curl up with a nice geostatistics book,
usw. That is, there is more to it than finding a
package in R. If it is a 'once-over' with tame data,
who cares? huh?

Good luck and enjoy...

Michael Grant



		

Stay connected, organized, and protected. Take the tour:



From ripley at stats.ox.ac.uk  Sat May  7 07:36:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 May 2005 06:36:35 +0100 (BST)
Subject: [R] slowness of plot(x, type='l')
In-Reply-To: <OF5E3EC117.95122074-ON86256FF9.006DC5DC-86256FF9.006F9AE2@mmm.com>
References: <OF5E3EC117.95122074-ON86256FF9.006DC5DC-86256FF9.006F9AE2@mmm.com>
Message-ID: <Pine.LNX.4.61.0505070551370.3113@gannet.stats>

Plotting times depend on the graphics device.  That is nowhere mentioned 
here, which is unhelpful, and we have already seen a post saying it does 
not happen on another unmentioned device (presumably X11).

Let us assume the unmentioned device was windows(), as that is the only 
one I see any slowdown for.  (Others like win.metafile are windows() under 
the skin.)

On Fri, 6 May 2005 apjaworski at mmm.com wrote:

> A couple of days ago a few messages indicated that something changed in the
> basic plot routine that made plot(*, type='l') slow for large data sets.
> Some people even reported crashes for very large data sets.  As far as I
> remember, this was not reported as a formal bug.

Well, _is_ there a bug in R (as distinct from in Windows graphics 
internals)?  I am almost certain there is not in R and this is a bug in 
Windows.

> I am still not sure if this is a bug, so I report my findings here.  First
> of all, I think I see a slowdown of the plot function, although I do not
> have older versions of R installed, so I cannot do side-by-side
> comparisons.  Secondly, I noticed that the behavior of plot(*, type='l')
> differs.  Before R-2.1, the plotted lines would appear on the plot
> gradually.  Now, after the wait, the whole plot appears at once.
>
> Here are my timing results.  I am on Windows2000, IBM Intellistation with
> Xenon 2.8MHz with 1Gb of memory.  I checked May-06 versions of R-patched
> and R-devel built from sources.  I ran the following simple test:
>
> x <- rnorm(n)
> date(); plot(x, type='l'); date()

Oh, PLEASE, use system.time() to time things.  Had you done so you might 
have seen things like

> windows()
> n <- 10000
> system.time(plot(rnorm(n), type="l"))
[1]  0.03 13.11 13.21    NA    NA
> postscript()
> system.time(plot(rnorm(n), type="l"))
[1] 0.07 0.00 0.08   NA   NA
> dev.off()
> system.time(plot(rnorm(n), type="p"))
[1] 0.07 0.93 1.00   NA   NA

so the time is not being taken by R but by Windows.

I can tell you the reason: it is the support for mitred etc line ends 
introduced in R 2.0.0 and only supported in windows() from 2.1.0.  This 
has slowed solid lines down to the sort of times taken for dashed lines 
previously.

Now, the best we can do to work around this is to follow what we did for 
dashed lines, and not attempt to be accurate for very large numbers of 
line segments.  By plotting in bunches of 1000 lines I get

> system.time(plot(rnorm(n), type="l"))
[1] 0.03 0.36 0.42   NA   NA
> system.time(plot(rnorm(n), type="l", lty=3))
[1] 0.22 2.89 3.11   NA   NA

We have been here before, and as I recall this slowdown happens only in 
NT-based versions of Windows which seem _de facto_ restricted to about 
1000 line elements in a path: what we were not aware of was that it 
happened for solid lines as well as dashed ones.

I've put the bunching into R-patched.

It is very regretable that this sort of thing was not tested for during 
beta-testing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat May  7 07:43:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 May 2005 06:43:19 +0100 (BST)
Subject: [R] CGIwithR ERROR: unknown GUI none
In-Reply-To: <3f87cc6d05050612554c87ca3e@mail.gmail.com>
References: <3f87cc6d05050612554c87ca3e@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505070640400.3113@gannet.stats>

On Fri, 6 May 2005, Omar Lakkis wrote:

> I write the simplest cgi example in R but I keep getting
>
> ERROR: unknown GUI none
>
> any idea?

Are your perchance running code written for R < 2.1.0 on R 2.1.0?

--gui=none  has been withdrawn: it has not done anything useful for quite 
a while.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From charles_loboz at yahoo.com  Sat May  7 10:36:48 2005
From: charles_loboz at yahoo.com (charles loboz)
Date: Sat, 7 May 2005 01:36:48 -0700 (PDT)
Subject: [R] string syntactic sugar in R? - long post
In-Reply-To: <200505061005.j46A2xRg029976@hypatia.math.ethz.ch>
Message-ID: <20050507083648.13593.qmail@web60817.mail.yahoo.com>

Currently in R, constructing a string containing
values of variables is done using 'paste' and can be
an error-prone and traumatic experience. For example,
when constructing a db query we have to write,
          paste("SELECT " value " FROM table  where
date ='",cdate,"'")
we are getting null result from it, because without
(forgotten...) sep=""  we get
         SELECT value FROM table where date='
2005-05-05 '
instead of
	SELECT value FROM table where date='2005-05-05'
Adding sep="" as a habit results in other errors, like
column names joined with keywords - because of
forgotten spaces. Not to mention mixing up or
unbalancing quote marks etc. The approach used by
paste is similar to that of many other languages (like
early Java, VB etc) and is inherently error-prone
because of poor visualization. There is a way to
improve it.

In the Java world gstrings were introduced
specifically for this purpose. A gstring is a string
with variable names embedded and replaced by values
(converted to strings, lazy eval) before use. An
example in R-syntax would be:

>alpha <- 8; beta="xyz"
>gstr <- "the result is ${alpha} with the comment
${beta}"
>cat(gstr)
      the result is 8 with the comment xyz

This syntactic sugar reduces significantly the number
of mistakes made with normal string concatenations.
Gstrings are used in ant and groovy - (for details see
http://groovy.codehaus.org/Strings, jump to GStrings).
They are particularly useful for creating readable and
error-free SQL statements, but obviously the simplify
'normal' string+value handling in all situations. [ps:
gstrings are not nestable]

I was wondering how difficult it would be to add such
syntactic sugar to R and would that create some
language problems? May be it is possible that it could
be done as some gpaste function, parsing the argument
for ${var}, extracting variables from the environment,
evaluating them and producing the final string?

I admit my bias - using ant for years and groovy for
months and having to do a lot of SQL queries does not
put me in the mainstream of R users - so it may be
that this idea is not usable to a wider group of
users.



From rgentlem at fhcrc.org  Sat May  7 11:00:23 2005
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Sat, 7 May 2005 02:00:23 -0700
Subject: [R] string syntactic sugar in R? - long post
In-Reply-To: <20050507083648.13593.qmail@web60817.mail.yahoo.com>
References: <20050507083648.13593.qmail@web60817.mail.yahoo.com>
Message-ID: <d470e14ca37e681850d9b2f8ccc95083@fhcrc.org>

Hi,
  In Bioconductor, we have something called copySubstitute, which does  
what you want, I believe,

  x="select @var1@ from @tab1@"
  copySubstitute(textConnection(x), symbolValues= list(var1="Race",  
tab1="ReallyBigTable"), dest=stdout())

yields
select Race from ReallyBigTable

you can read in from any connection and write out to any connection,  
change the delimiter, etc.
We use it to autogenerate manual pages and other documentation for  
packages that have lots of similar structure, as well as for things  
like what you want to do.

Best wishes,
  Robert



On May 7, 2005, at 1:36 AM, charles loboz wrote:

> Currently in R, constructing a string containing
> values of variables is done using 'paste' and can be
> an error-prone and traumatic experience. For example,
> when constructing a db query we have to write,
>           paste("SELECT " value " FROM table  where
> date ='",cdate,"'")
> we are getting null result from it, because without
> (forgotten...) sep=""  we get
>          SELECT value FROM table where date='
> 2005-05-05 '
> instead of
> 	SELECT value FROM table where date='2005-05-05'
> Adding sep="" as a habit results in other errors, like
> column names joined with keywords - because of
> forgotten spaces. Not to mention mixing up or
> unbalancing quote marks etc. The approach used by
> paste is similar to that of many other languages (like
> early Java, VB etc) and is inherently error-prone
> because of poor visualization. There is a way to
> improve it.
>
> In the Java world gstrings were introduced
> specifically for this purpose. A gstring is a string
> with variable names embedded and replaced by values
> (converted to strings, lazy eval) before use. An
> example in R-syntax would be:
>
>> alpha <- 8; beta="xyz"
>> gstr <- "the result is ${alpha} with the comment
> ${beta}"
>> cat(gstr)
>       the result is 8 with the comment xyz
>
> This syntactic sugar reduces significantly the number
> of mistakes made with normal string concatenations.
> Gstrings are used in ant and groovy - (for details see
> http://groovy.codehaus.org/Strings, jump to GStrings).
> They are particularly useful for creating readable and
> error-free SQL statements, but obviously the simplify
> 'normal' string+value handling in all situations. [ps:
> gstrings are not nestable]
>
> I was wondering how difficult it would be to add such
> syntactic sugar to R and would that create some
> language problems? May be it is possible that it could
> be done as some gpaste function, parsing the argument
> for ${var}, extracting variables from the environment,
> evaluating them and producing the final string?
>
> I admit my bias - using ant for years and groovy for
> months and having to do a lot of SQL queries does not
> put me in the mainstream of R users - so it may be
> that this idea is not usable to a wider group of
> users.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>
>
+----------------------------------------------------------------------- 
----------------+
| Robert Gentleman              phone: (206) 667-7700                    
          |
| Head, Program in Computational Biology   fax:  (206) 667-1319   |
| Division of Public Health Sciences       office: M2-B865               
       |
| Fred Hutchinson Cancer Research Center                                 
          |
| email: rgentlem at fhcrc.org                                              
                          |
+----------------------------------------------------------------------- 
----------------+



From bullard at berkeley.edu  Sat May  7 11:27:45 2005
From: bullard at berkeley.edu (James Bullard)
Date: Sat, 07 May 2005 02:27:45 -0700
Subject: [R] string syntactic sugar in R? - long post
In-Reply-To: <20050507083648.13593.qmail@web60817.mail.yahoo.com>
References: <20050507083648.13593.qmail@web60817.mail.yahoo.com>
Message-ID: <427C8A11.7090307@berkeley.edu>

The other thing to use is 'sprintf', which would be fantastic in R if it imputed types based on the format string.

As it is now, for your query you would do:

> sprintf("SELECT %s FROM table WHERE date = '%s'", "column", "2005-10-12")
[1] "SELECT column FROM table WHERE date = '2005-10-12'"

Which, in my opinion is nicer than the corresponding paste, and about as nice as gstring. The issue that I always have with sprintf is when I use numbers, specifically integers. As the function is just a wrapper for the C function  and because numbers are implicitly doubles the following doesnt work:

>  sprintf("SELECT %s FROM table WHERE age = %d", "column", 1)
Error in sprintf("SELECT %s FROM table WHERE age = %d", "column", 1) : 
        use format %f, %e or %g for numeric objects

It does work however if you do

> sprintf("SELECT %s FROM table WHERE age = %d", "column", as.integer(1))
[1] "SELECT column FROM table WHERE age = 1"

This however, is not so nice - are there reasons why this has to be like this? This might be naive but I would think it would be pretty simple in R to do this automatically. Thanks for any insight. 


jim



charles loboz wrote:

>Currently in R, constructing a string containing
>values of variables is done using 'paste' and can be
>an error-prone and traumatic experience. For example,
>when constructing a db query we have to write,
>          paste("SELECT " value " FROM table  where
>date ='",cdate,"'")
>we are getting null result from it, because without
>(forgotten...) sep=""  we get
>         SELECT value FROM table where date='
>2005-05-05 '
>instead of
>	SELECT value FROM table where date='2005-05-05'
>Adding sep="" as a habit results in other errors, like
>column names joined with keywords - because of
>forgotten spaces. Not to mention mixing up or
>unbalancing quote marks etc. The approach used by
>paste is similar to that of many other languages (like
>early Java, VB etc) and is inherently error-prone
>because of poor visualization. There is a way to
>improve it.
>
>In the Java world gstrings were introduced
>specifically for this purpose. A gstring is a string
>with variable names embedded and replaced by values
>(converted to strings, lazy eval) before use. An
>example in R-syntax would be:
>
>  
>
>>alpha <- 8; beta="xyz"
>>gstr <- "the result is ${alpha} with the comment
>>    
>>
>${beta}"
>  
>
>>cat(gstr)
>>    
>>
>      the result is 8 with the comment xyz
>
>This syntactic sugar reduces significantly the number
>of mistakes made with normal string concatenations.
>Gstrings are used in ant and groovy - (for details see
>http://groovy.codehaus.org/Strings, jump to GStrings).
>They are particularly useful for creating readable and
>error-free SQL statements, but obviously the simplify
>'normal' string+value handling in all situations. [ps:
>gstrings are not nestable]
>
>I was wondering how difficult it would be to add such
>syntactic sugar to R and would that create some
>language problems? May be it is possible that it could
>be done as some gpaste function, parsing the argument
>for ${var}, extracting variables from the environment,
>evaluating them and producing the final string?
>
>I admit my bias - using ant for years and groovy for
>months and having to do a lot of SQL queries does not
>put me in the mainstream of R users - so it may be
>that this idea is not usable to a wider group of
>users.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
James Bullard
bullard at berkeley.edu
760.267.0986



From erich.neuwirth at univie.ac.at  Sat May  7 12:12:43 2005
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Sat, 07 May 2005 12:12:43 +0200
Subject: [R] ScieViews installer
Message-ID: <427C949B.6000507@univie.ac.at>

I tried to install SciViews in R 2.1.0 (on Windows)
and on all machines I get:

bundle 'SciViews' successfully unpacked and MD5 sums checked
Error in sprintf(gettext("unable to move temp installation '%d' to
'%s'"),  :
        use format %s for character objects

how can I solve this problem?



-- 
Erich Neuwirth, University of Vienna
Faculty of Computer Science
Didactic Center for Computer Science
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-39464 Fax: +43-1-4277-39459



From Ted.Harding at nessie.mcc.ac.uk  Sat May  7 12:26:39 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 07 May 2005 11:26:39 +0100 (BST)
Subject: [R] string syntactic sugar in R? - long post
In-Reply-To: <427C8A11.7090307@berkeley.edu>
Message-ID: <XFMail.050507112639.Ted.Harding@nessie.mcc.ac.uk>

On 07-May-05 James Bullard wrote:
> The other thing to use is 'sprintf', which would be fantastic in R if
> it imputed types based on the format string.
> 
> As it is now, for your query you would do:
> 
>> sprintf("SELECT %s FROM table WHERE date = '%s'", "column",
>> "2005-10-12")
> [1] "SELECT column FROM table WHERE date = '2005-10-12'"
> 
> Which, in my opinion is nicer than the corresponding paste, and about
> as nice as gstring. The issue that I always have with sprintf is when I
> use numbers, specifically integers. As the function is just a wrapper
> for the C function  and because numbers are implicitly doubles the
> following doesnt work:
> 
>>  sprintf("SELECT %s FROM table WHERE age = %d", "column", 1)
> Error in sprintf("SELECT %s FROM table WHERE age = %d", "column", 1) : 
>         use format %f, %e or %g for numeric objects
> 
> It does work however if you do
> 
>> sprintf("SELECT %s FROM table WHERE age = %d", "column",
>> as.integer(1))
> [1] "SELECT column FROM table WHERE age = 1"
> 
> This however, is not so nice - are there reasons why this has to be
> like this? This might be naive but I would think it would be pretty
> simple in R to do this automatically. Thanks for any insight. 

You can force integer format using %f if you use it as "%.0f":

  sprintf("SELECT %s FROM table WHERE age = %.0f", "column", 1)
  ## [1] "SELECT column FROM table WHERE age = 1"

The rule (as in C) is that "%a.bf" outputs a format for the
floating-point number in a minimum width of 'a' characters
("field width", left-padded with space), with 'b' digits following
the decimal point (and no decimal point is printed if b=0);
if either 'a' or 'b' is missing then no corresponding restriction
is imposed.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-May-05                                       Time: 11:05:06
------------------------------ XFMail ------------------------------



From studenti at unipd.it  Sat May  7 13:05:12 2005
From: studenti at unipd.it (messaggio automatico)
Date: Sat,  7 May 2005 13:05:12 +0200 (MET DST)
Subject: [R] scrivete a ...
Message-ID: <20050507110512.5E2D9948BD@mail.unipd.it>

Si segnala di rivolgersi per 
 tasse e pagamenti a service.studenti at unipd.it
 di contattare la propria segreteria studenti agli indirizzi:
 SegStud.Agraria at unipd.it
 SegStud.Economia at unipd.it
 SegStud.Farmacia at unipd.it
 SegStud.Giurisprudenza at unipd.it
 SegStud.Ingegneria at unipd.it
 SegStud.Lettere at unipd.it
 SegStud.Medicina at unipd.it
 SegStud.Psicologia at unipd.it
 SegStud.Veterinaria at unipd.it
 SegStud.ScienzeMFN at unipd.it
 SegStud.ScFormazione at unipd.it
 SegStud.ScPolitiche at unipd.it
 SegStud.ScStatistiche at unipd.it
 per informazioni generali:
 callcentre at unipd.it



From telsehenschel at web.de  Sat May  7 13:30:17 2005
From: telsehenschel at web.de (Telse Henschel)
Date: Sat, 07 May 2005 13:30:17 +0200
Subject: [R] Test on mu with multivariate normal distribution
Message-ID: <1953841232@web.de>

Dear WizaRds,

I am sorry to bother you with a newbie question, but although I tried to solve my problem using the various .pdf files (Introduction, help pages etc.), I have come to a complete stop. Please be so kind as to guide me a little bit along my way of exploring multivariate analysis in R.

I want to test wether the means-vector mu1 of  X, consisting of the means per column of that matrix , and mu2, i.e. the means per column of Y,  are distributed equally  under the assumption of a multivariate normal distribution. I thought ?simtest? could be the right function to get the p-value, but I fail to use R correctly. Here is what I tried to do:

f1		<- factor(c("8", "10", "12", "14"))

X		<- matrix(1:16, ncol=4)
colnames(X)	<- c("Obj1","Obj2","Obj3","Obj4")
X.frame	<- data.frame(f1,X)

Y		<- matrix(1:12, ncol=4)
colnames(Y)	<- c("Obj1","Obj2","Obj3","Obj4")
Y.frame	<- data.frame(f1,Y)
XY		<- data.frame(X.frame, Y.frame) # won?t work, because of different nr of rows

test.stat	<- lm(XY ~ f1)	# ???

# simtest(test.stat, XY, type="Tukey")

creates errors already by just staring at it. I apologize.


Thank you so much for your support,
Telse



From peter.moser at statistik.ji.zh.ch  Sat May  7 14:01:17 2005
From: peter.moser at statistik.ji.zh.ch (peter.moser@statistik.ji.zh.ch)
Date: Sat, 7 May 2005 14:01:17 +0200
Subject: [R] Peter Moser ist =?iso-8859-1?q?au=DFer_Haus=2E?=
Message-ID: <OF4C203A4A.3E4EDE21-ONC1256FFA.00420966-C1256FFA.00420966@ji.zh.ch>

Ich werde ab  04.05.2005 nicht im B??ro sein. Ich kehre zur??ck am
16.05.2005.

Ich werde Ihre Nachricht nach meiner R??ckkehr beantworten.



From ripley at stats.ox.ac.uk  Sat May  7 14:21:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 May 2005 13:21:18 +0100 (BST)
Subject: [R] string syntactic sugar in R? - long post
In-Reply-To: <427C8A11.7090307@berkeley.edu>
References: <20050507083648.13593.qmail@web60817.mail.yahoo.com>
	<427C8A11.7090307@berkeley.edu>
Message-ID: <Pine.LNX.4.61.0505071314300.15176@gannet.stats>

On Sat, 7 May 2005, James Bullard wrote:

> The other thing to use is 'sprintf', which would be fantastic in R if it 
> imputed types based on the format string.

But it does in 2.1.0, the current version.

> As it is now, for your query you would do:
>
>> sprintf("SELECT %s FROM table WHERE date = '%s'", "column", "2005-10-12")
> [1] "SELECT column FROM table WHERE date = '2005-10-12'"
>
> Which, in my opinion is nicer than the corresponding paste, and about as nice 
> as gstring. The issue that I always have with sprintf is when I use numbers, 
> specifically integers. As the function is just a wrapper for the C function

It is not `just a wrapper': someone put a lot of working into writing an 
intelligent wrapper.

> and because numbers are implicitly doubles the following doesnt work:
>
>>  sprintf("SELECT %s FROM table WHERE age = %d", "column", 1)
> Error in sprintf("SELECT %s FROM table WHERE age = %d", "column", 1) : 
> use format %f, %e or %g for numeric objects
>
> It does work however if you do
>
>> sprintf("SELECT %s FROM table WHERE age = %d", "column", as.integer(1))
> [1] "SELECT column FROM table WHERE age = 1"
>
> This however, is not so nice - are there reasons why this has to be like 
> this? This might be naive but I would think it would be pretty simple in R to 
> do this automatically. Thanks for any insight.

In R 2.1.0:

>  sprintf("SELECT %s FROM table WHERE age = %d", "column", 1)
[1] "SELECT column FROM table WHERE age = 1"

!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Sat May  7 14:22:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 7 May 2005 08:22:40 -0400
Subject: [R] string syntactic sugar in R? - long post
In-Reply-To: <20050507083648.13593.qmail@web60817.mail.yahoo.com>
References: <200505061005.j46A2xRg029976@hypatia.math.ethz.ch>
	<20050507083648.13593.qmail@web60817.mail.yahoo.com>
Message-ID: <971536df05050705223e5dc88e@mail.gmail.com>

On 5/7/05, charles loboz <charles_loboz at yahoo.com> wrote:
> Currently in R, constructing a string containing
> values of variables is done using 'paste' and can be
> an error-prone and traumatic experience. For example,
> when constructing a db query we have to write,
>          paste("SELECT " value " FROM table  where
> date ='",cdate,"'")
> we are getting null result from it, because without
> (forgotten...) sep=""  we get
>         SELECT value FROM table where date='
> 2005-05-05 '
> instead of
>        SELECT value FROM table where date='2005-05-05'
> Adding sep="" as a habit results in other errors, like
> column names joined with keywords - because of
> forgotten spaces. Not to mention mixing up or
> unbalancing quote marks etc. The approach used by
> paste is similar to that of many other languages (like
> early Java, VB etc) and is inherently error-prone
> because of poor visualization. There is a way to
> improve it.
> 
> In the Java world gstrings were introduced
> specifically for this purpose. A gstring is a string
> with variable names embedded and replaced by values
> (converted to strings, lazy eval) before use. An
> example in R-syntax would be:
> 
> >alpha <- 8; beta="xyz"
> >gstr <- "the result is ${alpha} with the comment
> ${beta}"
> >cat(gstr)
>      the result is 8 with the comment xyz
> 
> This syntactic sugar reduces significantly the number
> of mistakes made with normal string concatenations.
> Gstrings are used in ant and groovy - (for details see
> http://groovy.codehaus.org/Strings, jump to GStrings).
> They are particularly useful for creating readable and
> error-free SQL statements, but obviously the simplify
> 'normal' string+value handling in all situations. [ps:
> gstrings are not nestable]
> 
> I was wondering how difficult it would be to add such
> syntactic sugar to R and would that create some
> language problems? May be it is possible that it could
> be done as some gpaste function, parsing the argument
> for ${var}, extracting variables from the environment,
> evaluating them and producing the final string?
> 
> I admit my bias - using ant for years and groovy for
> months and having to do a lot of SQL queries does not
> put me in the mainstream of R users - so it may be
> that this idea is not usable to a wider group of
> users.

Here is one attempt.  It eliminates the necessity to quote the
elements altogether but in exchange requires that the
argument be a valid R expression.  It is based on the 
R bquote function.  

gpaste <- function(expr, where = parent.frame()) {
  dequote <- function(e) as.name(noquote(as.character(e)))
  unquote <- function(e) {
      if (length(e) <= 1) 
          dequote(e)
      else if (e[[1]] == as.name(".")) 
          dequote(eval(e[[2]], where))
      else as.call(lapply(e, unquote))
  }
  rval <- paste(unquote(substitute(expr)), collapse = " ")
  rval <- gsub("+ ", "", rval, fix = TRUE)
  gsub("`", "", rval)
}	

# test
var <- "myvar"
gpaste( select + .(var) + from + table + where + 
	date +" =" + .(sQuote(Sys.Date())) )

When you run it you get this:

> gpaste( select + .(var) + from + table + where + 
+                   date +" =" + .(sQuote(Sys.Date())) )
[1] "select myvar from table where date  = '2005-05-07'"



From ripley at stats.ox.ac.uk  Sat May  7 14:33:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 May 2005 13:33:28 +0100 (BST)
Subject: [R] ScieViews installer
In-Reply-To: <427C949B.6000507@univie.ac.at>
References: <427C949B.6000507@univie.ac.at>
Message-ID: <Pine.LNX.4.61.0505071328570.15176@gannet.stats>

On Sat, 7 May 2005, Erich Neuwirth wrote:

> I tried to install SciViews in R 2.1.0 (on Windows)
> and on all machines I get:
>
> bundle 'SciViews' successfully unpacked and MD5 sums checked
> Error in sprintf(gettext("unable to move temp installation '%d' to
> '%s'"),  :
>        use format %s for character objects
>
> how can I solve this problem?

Well, use R-patched where that exact problem goes away, but that is not 
the problem that needs solving.

The underlying cause is a problem on your Windows file system.  Do you 
have a version of SciViews left over that Windows cannot remove?

I've just tested this, and it works for me even in 2.1.0.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gillian.rutherford at wsl.ch  Sat May  7 14:43:26 2005
From: gillian.rutherford at wsl.ch (Gillian Rutherford)
Date: Sat, 07 May 2005 14:43:26 +0200
Subject: [R] converting NA/non-NA's to a binary variable
Message-ID: <5.2.1.1.1.20050507143256.00bd99c8@mail.wsl.ch>

Dear R colleagues,

I am trying to create a new column in a data frame, which converts values 
and NA's from another column into binary format. Essentially I need the 
NA's to become 1 and the rest to be 0. The code I wrote is returning the 
following error message:

Error in if (mort[i, 4] != NA) mort[i, 8] <- 0 else if (mort[i, 4] ==  :
         missing value where TRUE/FALSE needed

The code is as follows:

for(i in 1:length(mort[,4]))
	{
		if(mort[i,4] != NA) mort[i,8] <- 0
		else if(mort[i,4] == NA) mort[i,8] <- 1
	}

I'd appreciate any advice or recommendations as to a better way of 
achieving this.

Thanks
Gillian
-----------------------------------------------------------------------------------------------------
Gillian Rutherford
Eidg. Forschungsanstalt f??r Wald, Schnee und Landschaft
Swiss Federal Research Institute WSL
Economy Section, Forest Division
Z??rcherstrasse 111
CH - 8903 Birmensdorf
Phone: ++ 41 1 739 26 65
E-mail: gillian.rutherford at wsl.ch
http://www.wsl.ch/staff/gillian.rutherford/



From ggrothendieck at gmail.com  Sat May  7 15:02:10 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 7 May 2005 09:02:10 -0400
Subject: [R] converting NA/non-NA's to a binary variable
In-Reply-To: <5.2.1.1.1.20050507143256.00bd99c8@mail.wsl.ch>
References: <5.2.1.1.1.20050507143256.00bd99c8@mail.wsl.ch>
Message-ID: <971536df05050706021774e8c0@mail.gmail.com>

See ?is.na

On 5/7/05, Gillian Rutherford <gillian.rutherford at wsl.ch> wrote:
> Dear R colleagues,
> 
> I am trying to create a new column in a data frame, which converts values
> and NA's from another column into binary format. Essentially I need the
> NA's to become 1 and the rest to be 0. The code I wrote is returning the
> following error message:
> 
> Error in if (mort[i, 4] != NA) mort[i, 8] <- 0 else if (mort[i, 4] ==  :
>         missing value where TRUE/FALSE needed
> 
> The code is as follows:
> 
> for(i in 1:length(mort[,4]))
>        {
>                if(mort[i,4] != NA) mort[i,8] <- 0
>                else if(mort[i,4] == NA) mort[i,8] <- 1
>        }
> 
> I'd appreciate any advice or recommendations as to a better way of
> achieving this.
> 
> Thanks
> Gillian
> -----------------------------------------------------------------------------------------------------
> Gillian Rutherford
> Eidg. Forschungsanstalt f??r Wald, Schnee und Landschaft
> Swiss Federal Research Institute WSL
> Economy Section, Forest Division
> Z??rcherstrasse 111
> CH - 8903 Birmensdorf
> Phone: ++ 41 1 739 26 65
> E-mail: gillian.rutherford at wsl.ch
> http://www.wsl.ch/staff/gillian.rutherford/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Sat May  7 15:14:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 May 2005 14:14:21 +0100 (BST)
Subject: [R] converting NA/non-NA's to a binary variable
In-Reply-To: <5.2.1.1.1.20050507143256.00bd99c8@mail.wsl.ch>
References: <5.2.1.1.1.20050507143256.00bd99c8@mail.wsl.ch>
Message-ID: <Pine.LNX.4.61.0505071411230.15796@gannet.stats>

mort[8] <- is.na(mort[4])

(If you really want 1/0, add  '+ 0' to this expression.)

Testing (in)equality with NA always gives NA.  This is discussed in all 
good books on S/R, e.g. in MASS (see the FAQ).

On Sat, 7 May 2005, Gillian Rutherford wrote:

> Dear R colleagues,
>
> I am trying to create a new column in a data frame, which converts values and 
> NA's from another column into binary format. Essentially I need the NA's to 
> become 1 and the rest to be 0. The code I wrote is returning the following 
> error message:
>
> Error in if (mort[i, 4] != NA) mort[i, 8] <- 0 else if (mort[i, 4] ==  :
>        missing value where TRUE/FALSE needed
>
> The code is as follows:
>
> for(i in 1:length(mort[,4]))
> 	{
> 		if(mort[i,4] != NA) mort[i,8] <- 0
> 		else if(mort[i,4] == NA) mort[i,8] <- 1
> 	}
>
> I'd appreciate any advice or recommendations as to a better way of achieving 
> this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Sat May  7 15:47:19 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 07 May 2005 14:47:19 +0100 (BST)
Subject: [R] converting NA/non-NA's to a binary variable
In-Reply-To: <5.2.1.1.1.20050507143256.00bd99c8@mail.wsl.ch>
Message-ID: <XFMail.050507144719.Ted.Harding@nessie.mcc.ac.uk>

On 07-May-05 Gillian Rutherford wrote:
> Dear R colleagues,
> 
> I am trying to create a new column in a data frame, which
> converts values and NA's from another column into binary format.
> Essentially I need the NA's to become 1 and the rest to be 0.
> The code I wrote is returning the  following error message:
> 
> Error in if (mort[i, 4] != NA) mort[i, 8] <- 0 else if (mort[i, 4] == 
>:
>          missing value where TRUE/FALSE needed
> 
> The code is as follows:
> 
> for(i in 1:length(mort[,4]))
>       {
>               if(mort[i,4] != NA) mort[i,8] <- 0
>               else if(mort[i,4] == NA) mort[i,8] <- 1
>       }
> 
> I'd appreciate any advice or recommendations as to a better way of 
> achieving this.
> 
> Thanks
> Gillian

I think the following should do what you want, provided the column
mort[,8] exists:

  mort[,8] <- 0
  mort[is.na(mort[,4]),8] <- 1

Incidentally, tests like "== NA" or "!= NA" can produce unexpected
results! Use is.na() instead:

  tmp<-NA
  ## [1] NA
  tmp==NA
  ## [1] NA
  tmp!=NA
  ## [1] NA
  if(tmp==NA) 1 else 2
  ## Error in if (tmp == NA) 1 else 2 :
  ## missing value where TRUE/FALSE needed

  if(TRUE) 1 else 2
  ## [1] 1
  is.na(tmp)
  ## [1] TRUE

  if(is.na(tmp)) 1 else 2
  ## [1] 1
  if(!is.na(tmp)) 1 else 2
  ## [1] 2

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-May-05                                       Time: 14:46:19
------------------------------ XFMail ------------------------------



From sboker at nd.edu  Sat May  7 16:30:37 2005
From: sboker at nd.edu (Steven Boker)
Date: Sat, 7 May 2005 09:30:37 -0500
Subject: [R] Incorrect libxml2.2.dylib version on Tiger install
Message-ID: <ba8949492f55d7de3543e605bd79b719@nd.edu>

Hi all,

I have just installed OSX Server 10.4 and R comes up with the 
incompatible libxml library message reported by Dan Kelley a few 
messages ago.  Xcode 2 does not ship with Tiger Server.  I installed 
the X-Windows code.  I can report that the version of libxml2.2 that is 
installed in this case is the version 8.0.0 dylib.

[6]sboker at munimula:/usr/lib % ls -l libxml2.2*
-rwxr-xr-x   1 root  wheel  1061704 Apr 15 23:44 libxml2.2.6.16.dylib*
-rwxr-xr-x   1 root  wheel  1061804 May  6 14:15 libxml2.2.dylib*

I'm working on getting a copy of the Xcode 2 CD.  I was surprised it 
wasn't in the 10.4 server CD set.  I have a production problem today, 
though, if anyone can help me with a copy of this library.

Best,
Steve Boker

On May 6, 2005, at 12:39 PM, Dan Kelley wrote:
 >
 > Error in dyn.load(x, as.logical(local), as.logical(now)) :
 > unable to load shared library '/Library/Frameworks/
 > R.framework/Resources/library/grDevices/libs/grDevices.so':
 > dlopen(/Library/Frameworks/R.framework/Resources/library/grDevices/
 > libs/grDevices.so, 6): Library not loaded: /usr/lib/libxml2.2.dylib
 > Referenced from: /System/Library/Frameworks/AppKit.framework/
 > Versions/C/AppKit
 > Reason: Incompatible library version: AppKit requires version
 > 9.0.0 or later, but libxml2.2.dylib provides version 8.0.0

---------------------------------------------------------------------
  Steven M. Boker                    574-339-0735 (cell/page/message)
  sboker at nd.edu                      574-631-4941 (office)
  http://www.nd.edu/~sboker/         574-631-8883 (fax)
  Dept. of Psychology, University of Notre Dame, Notre Dame, IN 46556



From fsaldan1 at gmail.com  Sat May  7 16:32:31 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Sat, 7 May 2005 10:32:31 -0400
Subject: [R] undebug all
Message-ID: <10dee469050507073279fa984b@mail.gmail.com>

Is there a fast way to undebug() all functions that are currently
being debugged in all environments?

Thanks.

FS



From ripley at stats.ox.ac.uk  Sat May  7 17:09:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 May 2005 16:09:49 +0100 (BST)
Subject: [R] undebug all
In-Reply-To: <10dee469050507073279fa984b@mail.gmail.com>
References: <10dee469050507073279fa984b@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505071603061.17116@gannet.stats>

On Sat, 7 May 2005, Fernando Saldanha wrote:

> Is there a fast way to undebug() all functions that are currently
> being debugged in all environments?

I guess you mean `marked for debugging' and not `currently being executed 
under a browser started by being marked for debug'?

Debugging is a property of a function object (a bit in the sxpinfo) 
and so you would have to traverse all reachable objects (as gc does) to 
find them all.

AFAIK once a function has been entered you cannot turn off debugging for 
it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From erich.neuwirth at univie.ac.at  Sat May  7 17:29:07 2005
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Sat, 07 May 2005 17:29:07 +0200
Subject: [R] ScieViews installer
In-Reply-To: <Pine.LNX.4.61.0505071328570.15176@gannet.stats>
References: <427C949B.6000507@univie.ac.at>
	<Pine.LNX.4.61.0505071328570.15176@gannet.stats>
Message-ID: <427CDEC3.1090509@univie.ac.at>

Thank you for your help.
The library directory contained some very strangely named
subdirectories, file3132 and similarly.
After removing these and vereything related to SciViews
I was able to install SciViews.
Looking back I seem to remember that form time to time
these strange directories appear and I do not understand
where they come from.
Could it be a timing issue, i.e. Windows trying to remove
a directory too early? I remember similar problems with
temporary files not being deletable by Excel.



Prof Brian Ripley wrote:
> On Sat, 7 May 2005, Erich Neuwirth wrote:
>> bundle 'SciViews' successfully unpacked and MD5 sums checked
>> Error in sprintf(gettext("unable to move temp installation '%d' to
>> '%s'"),  :
>>        use format %s for character objects
>>
>> how can I solve this problem?
> 
> 
> Well, use R-patched where that exact problem goes away, but that is not
> the problem that needs solving.
> 
> The underlying cause is a problem on your Windows file system.  Do you
> have a version of SciViews left over that Windows cannot remove?
> 
> I've just tested this, and it works for me even in 2.1.0.
> 

-- 
Erich Neuwirth, University of Vienna
Faculty of Computer Science
Didactic Center for Computer Science
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-39464 Fax: +43-1-4277-39459



From ripley at stats.ox.ac.uk  Sat May  7 17:36:20 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 May 2005 16:36:20 +0100 (BST)
Subject: [R] ScieViews installer
In-Reply-To: <427CDEC3.1090509@univie.ac.at>
References: <427C949B.6000507@univie.ac.at>
	<Pine.LNX.4.61.0505071328570.15176@gannet.stats>
	<427CDEC3.1090509@univie.ac.at>
Message-ID: <Pine.LNX.4.61.0505071635180.17532@gannet.stats>

On Sat, 7 May 2005, Erich Neuwirth wrote:

> Thank you for your help.
> The library directory contained some very strangely named
> subdirectories, file3132 and similarly.
> After removing these and vereything related to SciViews
> I was able to install SciViews.
> Looking back I seem to remember that form time to time
> these strange directories appear and I do not understand
> where they come from.
> Could it be a timing issue, i.e. Windows trying to remove
> a directory too early? I remember similar problems with
> temporary files not being deletable by Excel.

No, on NT a Windows atomic file rename is used.  Neither Duncan nor I have 
ever encountered these: they seem to result from a Windows OS bug.

>
>
>
> Prof Brian Ripley wrote:
>> On Sat, 7 May 2005, Erich Neuwirth wrote:
>>> bundle 'SciViews' successfully unpacked and MD5 sums checked
>>> Error in sprintf(gettext("unable to move temp installation '%d' to
>>> '%s'"),  :
>>>        use format %s for character objects
>>>
>>> how can I solve this problem?
>>
>>
>> Well, use R-patched where that exact problem goes away, but that is not
>> the problem that needs solving.
>>
>> The underlying cause is a problem on your Windows file system.  Do you
>> have a version of SciViews left over that Windows cannot remove?
>>
>> I've just tested this, and it works for me even in 2.1.0.
>>
>
> -- 
> Erich Neuwirth, University of Vienna
> Faculty of Computer Science
> Didactic Center for Computer Science
> Visit our SunSITE at http://sunsite.univie.ac.at
> Phone: +43-1-4277-39464 Fax: +43-1-4277-39459
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jqm475 at gmail.com  Sat May  7 19:25:20 2005
From: jqm475 at gmail.com (Jonathan Q.)
Date: Sat, 7 May 2005 13:25:20 -0400
Subject: [R] General Question re R vs S-Plus
Message-ID: <e206273d050507102520ba5bd0@mail.gmail.com>

Coming up to speed on both R and S-Plus.  My access to S-Plus will end
soon so I want to get up to speed on R. The big initial difference
seems that R has only the command editor where S-Plus also has a
windows interface.

My preference is to learn the language and the windows interface
doesn't really do the trick.  My question is, if one uses the windows
interface for some functions, is there a way to see what the
equivalent code would be?  I have checked the manuals, help etc and
can't seem to find a way.

Thanks



-- 
Jonathan
jqm475 at gmail.com



From gbiondizoccai at gmail.com  Sat May  7 19:52:55 2005
From: gbiondizoccai at gmail.com (Giuseppe Biondi Zoccai)
Date: Sat, 7 May 2005 19:52:55 +0200
Subject: [R] help for bootstrap of backward stepwise logistic regression
Message-ID: <f1581580050507105243cfe95a@mail.gmail.com>

I would like to perform a bootstrap validation of a backward stepwise
logistic regression analysis, but I am a beginner with R and I am not
sure of how to do it.
Is there anyone that can send me a sample file in tab format (that I
can modify in Excel by pasting my data) and the pertinent R algorithm?
Many thanks
Giuseppe 

-- 

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Dr. Giuseppe Biondi Zoccai
Interventional Cardiology Unit
St Raffaele Hospital
via Olgettina 60
20132 Milan ITALY
Phone: +39-3408626829
Fax: +39-0226437339
Email: gbiondizoccai at gmail.com

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



From fsaldan1 at gmail.com  Sat May  7 20:18:48 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Sat, 7 May 2005 14:18:48 -0400
Subject: [R] Vectorize a bootstrapped calculation
Message-ID: <10dee46905050711186478b140@mail.gmail.com>

I would like to vectorize a calculation that is bootstrapped, in the
sense that each step uses the results of the previous steps. For
example:

x <- rep(NA, 5)
y <- rnorm(5)
for (i in 1:5) {
	x[i] <- max(y[i], ifelse(i > 0, min(x[1:i-1], 0)))
}

(the functions max and min are used just as an example, I would like
to do something like this for arbitrary functions f and g).

It would be nice to be able to write something like:

x <- cumsapply(y, max, min)

where the first function, max in this example, would be applied to the
i-th element of y, and the second function, min in the example, would
be applied to the vector x[1:i-1].

Of course one can program such a function, the issue is can it be done
efficiently, and not just as a wrapping of the for loop above?

FS



From sboker at nd.edu  Sat May  7 20:33:22 2005
From: sboker at nd.edu (Steven Boker)
Date: Sat, 7 May 2005 13:33:22 -0500
Subject: [R] Incorrect libxml2.2.dylib version on Tiger install
In-Reply-To: <ba8949492f55d7de3543e605bd79b719@nd.edu>
References: <ba8949492f55d7de3543e605bd79b719@nd.edu>
Message-ID: <ea6e58a41a2559b14aa291c352949c5f@nd.edu>

Sorry to reply to my own post, but I have more info.  This probably 
needs to come to the attention of the Mac R developers.

Xcode 2.0 does not have a more recent libxml2.2.dylib.

The problem only manifests itself when running R 2.1.0 from the command 
line, not within the Aqua GUI.

Thanks for your patience.

On May 7, 2005, at 9:30 AM, Steven Boker wrote:

> Hi all,
>
> I have just installed OSX Server 10.4 and R comes up with the 
> incompatible libxml library message reported by Dan Kelley a few 
> messages ago.  Xcode 2 does not ship with Tiger Server.  I installed 
> the X-Windows code.  I can report that the version of libxml2.2 that 
> is installed in this case is the version 8.0.0 dylib.
>
> [6]sboker at munimula:/usr/lib % ls -l libxml2.2*
> -rwxr-xr-x   1 root  wheel  1061704 Apr 15 23:44 libxml2.2.6.16.dylib*
> -rwxr-xr-x   1 root  wheel  1061804 May  6 14:15 libxml2.2.dylib*
>
> I'm working on getting a copy of the Xcode 2 CD.  I was surprised it 
> wasn't in the 10.4 server CD set.  I have a production problem today, 
> though, if anyone can help me with a copy of this library.
>
> Best,
> Steve Boker
>
> On May 6, 2005, at 12:39 PM, Dan Kelley wrote:
> >
> > Error in dyn.load(x, as.logical(local), as.logical(now)) :
> > unable to load shared library '/Library/Frameworks/
> > R.framework/Resources/library/grDevices/libs/grDevices.so':
> > dlopen(/Library/Frameworks/R.framework/Resources/library/grDevices/
> > libs/grDevices.so, 6): Library not loaded: /usr/lib/libxml2.2.dylib
> > Referenced from: /System/Library/Frameworks/AppKit.framework/
> > Versions/C/AppKit
> > Reason: Incompatible library version: AppKit requires version
> > 9.0.0 or later, but libxml2.2.dylib provides version 8.0.0
>
> ---------------------------------------------------------------------
>  Steven M. Boker                    574-339-0735 (cell/page/message)
>  sboker at nd.edu                      574-631-4941 (office)
>  http://www.nd.edu/~sboker/         574-631-8883 (fax)
>  Dept. of Psychology, University of Notre Dame, Notre Dame, IN 46556
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
---------------------------------------------------------------------
  Steven M. Boker                    574-339-0735 (cell/page/message)
  sboker at nd.edu                      574-631-4941 (office)
  http://www.nd.edu/~sboker/         574-631-8883 (fax)
  Dept. of Psychology, University of Notre Dame, Notre Dame, IN 46556



From fw at acoustics.dk  Thu May  5 13:23:02 2005
From: fw at acoustics.dk (Florian Wickelmaier)
Date: Thu, 5 May 2005 13:23:02 +0200 (MEST)
Subject: [R] [R-pkgs] updated package eba 1.4-0
Message-ID: <Pine.GSO.4.33.0505051100300.17225-100000@zil.kom.auc.dk>

Dear all,

There is an updated version of the eba package for
elimination-by-aspects (EBA) choice models available on CRAN.

It features new functions for extracting and plotting
the residuals, for testing hypotheses on the parameters
(wald.test), and for comparing sub-samples (group.test).
The documentation has also been updated.
Any feedback is welcome.

Happy modeling!

Florian Wickelmaier

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From andy_liaw at merck.com  Sat May  7 23:52:51 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 7 May 2005 17:52:51 -0400
Subject: [R] General Question re R vs S-Plus
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E7FC@usctmx1106.merck.com>

> From: Jonathan Q.
> 
> Coming up to speed on both R and S-Plus.  My access to S-Plus will end
> soon so I want to get up to speed on R. The big initial difference
> seems that R has only the command editor where S-Plus also has a
> windows interface.

What exactly do you mean by this?  What would you call Rgui.exe?
 
> My preference is to learn the language and the windows interface
> doesn't really do the trick.  My question is, if one uses the windows
> interface for some functions, is there a way to see what the
> equivalent code would be?  I have checked the manuals, help etc and
> can't seem to find a way.

That sounds more like a question for S-news than R-help...  In any case, my
impression is that commands linked to the S-PLUS GUI menus and buttons are
functions written for that.  You're not likely to learn much that will carry
over to R.

Andy

 
> Thanks
> 
> 
> 
> -- 
> Jonathan
> jqm475 at gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From f.harrell at vanderbilt.edu  Sun May  8 00:31:45 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 07 May 2005 18:31:45 -0400
Subject: [R] help for bootstrap of backward stepwise logistic regression
In-Reply-To: <f1581580050507105243cfe95a@mail.gmail.com>
References: <f1581580050507105243cfe95a@mail.gmail.com>
Message-ID: <427D41D1.2070808@vanderbilt.edu>

Giuseppe Biondi Zoccai wrote:
> I would like to perform a bootstrap validation of a backward stepwise
> logistic regression analysis, but I am a beginner with R and I am not
> sure of how to do it.
> Is there anyone that can send me a sample file in tab format (that I
> can modify in Excel by pasting my data) and the pertinent R algorithm?
> Many thanks
> Giuseppe 
> 

You are asking for a lot and not saying that you are willing to read 
background material or help files.

To get you started, install the Design and Hmisc packages and look at 
help files for the following in Design: fastbw, lrm, validate.lrm, 
calibrate.lrm

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From phgrosjean at sciviews.org  Sun May  8 08:42:44 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sun, 08 May 2005 08:42:44 +0200
Subject: [R] ScieViews installer
In-Reply-To: <Pine.LNX.4.61.0505071635180.17532@gannet.stats>
References: <427C949B.6000507@univie.ac.at>	<Pine.LNX.4.61.0505071328570.15176@gannet.stats>	<427CDEC3.1090509@univie.ac.at>
	<Pine.LNX.4.61.0505071635180.17532@gannet.stats>
Message-ID: <427DB4E4.8080906@sciviews.org>

Prof Brian Ripley wrote:
> On Sat, 7 May 2005, Erich Neuwirth wrote:
> 
>> Thank you for your help.
>> The library directory contained some very strangely named
>> subdirectories, file3132 and similarly.
>> After removing these and vereything related to SciViews
>> I was able to install SciViews.
>> Looking back I seem to remember that form time to time
>> these strange directories appear and I do not understand
>> where they come from.
>> Could it be a timing issue, i.e. Windows trying to remove
>> a directory too early? I remember similar problems with
>> temporary files not being deletable by Excel.
> 
> 
> No, on NT a Windows atomic file rename is used.  Neither Duncan nor I 
> have ever encountered these: they seem to result from a Windows OS bug.

This happens occasionally when R is not able to unzip a file because it 
is corrupted. In such a case, you are better to reload the file.
Best,

Philippe Grosjean

P.S.: note that the SciViews bundle is not compatible yet with 
internationalization introduced in R 2.1.0. I am currently working on it.



>> Prof Brian Ripley wrote:
>>
>>> On Sat, 7 May 2005, Erich Neuwirth wrote:
>>>
>>>> bundle 'SciViews' successfully unpacked and MD5 sums checked
>>>> Error in sprintf(gettext("unable to move temp installation '%d' to
>>>> '%s'"),  :
>>>>        use format %s for character objects
>>>>
>>>> how can I solve this problem?
>>>
>>>
>>>
>>> Well, use R-patched where that exact problem goes away, but that is not
>>> the problem that needs solving.
>>>
>>> The underlying cause is a problem on your Windows file system.  Do you
>>> have a version of SciViews left over that Windows cannot remove?
>>>
>>> I've just tested this, and it works for me even in 2.1.0.
>>>
>>
>> -- 
>> Erich Neuwirth, University of Vienna
>> Faculty of Computer Science
>> Didactic Center for Computer Science
>> Visit our SunSITE at http://sunsite.univie.ac.at
>> Phone: +43-1-4277-39464 Fax: +43-1-4277-39459
>>
>>
>



From p.dalgaard at biostat.ku.dk  Sun May  8 09:25:04 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 May 2005 09:25:04 +0200
Subject: [R] Test on mu with multivariate normal distribution
In-Reply-To: <1953841232@web.de>
References: <1953841232@web.de>
Message-ID: <x2fywych7z.fsf@turmalin.kubism.ku.dk>

"Telse Henschel" <telsehenschel at web.de> writes:

> Dear WizaRds,
> 
> I am sorry to bother you with a newbie question, but although I tried to solve my problem using the various .pdf files (Introduction, help pages etc.), I have come to a complete stop. Please be so kind as to guide me a little bit along my way of exploring multivariate analysis in R.
> 
> I want to test wether the means-vector mu1 of  X, consisting of the means per column of that matrix , and mu2, i.e. the means per column of Y,  are distributed equally  under the assumption of a multivariate normal distribution. I thought ?simtest? could be the right function to get the p-value, but I fail to use R correctly. Here is what I tried to do:
> 
> f1		<- factor(c("8", "10", "12", "14"))
> 
> X		<- matrix(1:16, ncol=4)
> colnames(X)	<- c("Obj1","Obj2","Obj3","Obj4")
> X.frame	<- data.frame(f1,X)
> 
> Y		<- matrix(1:12, ncol=4)
> colnames(Y)	<- c("Obj1","Obj2","Obj3","Obj4")
> Y.frame	<- data.frame(f1,Y)
> XY		<- data.frame(X.frame, Y.frame) # won?t work, because of different nr of rows
> 
> test.stat	<- lm(XY ~ f1)	# ???

Try this:

X <- matrix(rnorm(16), ncol=4)
Y <- matrix(rnorm(12), ncol=4)
XY <- rbind(X,Y)
g <- factor(rep(1:2,c(4,3)))
fit1 <- lm(XY ~ g)
fit0 <- lm(XY ~ 1)
anova(fit0,fit1)

(using 1:16 and 1:12 for X and Y gives 

Error in anova.mlmlist(object = fit0, fit1) :
        residuals have rank 1 < 4

)
 
> # simtest(test.stat, XY, type="Tukey")
> 
> creates errors already by just staring at it. I apologize.
> 
> 
> Thank you so much for your support,
> Telse
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Sun May  8 10:11:47 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 May 2005 10:11:47 +0200
Subject: [R] General Question re R vs S-Plus
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E7FC@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E7FC@usctmx1106.merck.com>
Message-ID: <x2br7mcf24.fsf@turmalin.kubism.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> > My preference is to learn the language and the windows interface
> > doesn't really do the trick.  My question is, if one uses the windows
> > interface for some functions, is there a way to see what the
> > equivalent code would be?  I have checked the manuals, help etc and
> > can't seem to find a way.
> 
> That sounds more like a question for S-news than R-help...  In any case, my
> impression is that commands linked to the S-PLUS GUI menus and buttons are
> functions written for that.  You're not likely to learn much that will carry
> over to R.

As I recall, the Splus GUI functions are (mostly?) written in S, so
you could study the source code. However, it might be a better idea to
fire up John Fox's Rcmdr package and try things similar to what you'd
do in Splus. Rcmdr makes a point of showing the code as it goes along.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jqm475 at gmail.com  Sun May  8 15:26:48 2005
From: jqm475 at gmail.com (Jonathan Q.)
Date: Sun, 8 May 2005 09:26:48 -0400
Subject: [R] General Question re R vs S-Plus
In-Reply-To: <x2br7mcf24.fsf@turmalin.kubism.ku.dk>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E7FC@usctmx1106.merck.com>
	<x2br7mcf24.fsf@turmalin.kubism.ku.dk>
Message-ID: <e206273d050508062612143f33@mail.gmail.com>

thanks, that does the trick

On 08 May 2005 10:11:47 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> "Liaw, Andy" <andy_liaw at merck.com> writes:
> 
> > > My preference is to learn the language and the windows interface
> > > doesn't really do the trick.  My question is, if one uses the windows
> > > interface for some functions, is there a way to see what the
> > > equivalent code would be?  I have checked the manuals, help etc and
> > > can't seem to find a way.
> >
> > That sounds more like a question for S-news than R-help...  In any case, my
> > impression is that commands linked to the S-PLUS GUI menus and buttons are
> > functions written for that.  You're not likely to learn much that will carry
> > over to R.
> 
> As I recall, the Splus GUI functions are (mostly?) written in S, so
> you could study the source code. However, it might be a better idea to
> fire up John Fox's Rcmdr package and try things similar to what you'd
> do in Splus. Rcmdr makes a point of showing the code as it goes along.
> 
> --
>   O__  ---- Peter Dalgaard             Blegdamsvej 3
>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 


-- 
Jonathan
jqm475 at gmail.com



From ajayshah at mayin.org  Sun May  8 15:56:55 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sun, 8 May 2005 19:26:55 +0530
Subject: [R] Need a factor level even though there are no observations
Message-ID: <20050508135655.GD9427@lubyanka.local>

I'm in this situation:

     factorlabels <- c("School", "College", "Beyond")

with data for 8 families:

     education.man  <- c(1,2,1,2,1,2,1,2)       # Note : no "3" values
     education.wife <- c(1,2,3,1,2,3,1,2)       # 1,2,3 are all present.

My goal is to create this table:

                     School     College      Beyond
       Husband       4          4            0
       Wife          3          3            2


How do I do this?

I can readily do:
     education.wife <- factor(education.wife, labels=factorlabels)

But this breaks:
     education.man <- factor(education.man,   labels=factorlabels)

because none of the families have a husband who went beyond college.

I get around this problem in a limited way by:
     cautiously <- function(x, labels) {
       factor(x, labels=factorlabels[as.numeric(levels(factor(x)))])
     }
     education.man <- cautiously(education.man, labels=factorlabels)

Now I get:

     > table(education.man)
     School College 
          4       4 
     > table(education.wife)
     School College  Beyond 
          3       3       2 

This is a pain because now the two tables are not conformable. How do
I get to my end goal, which is the table:

                     School     College      Beyond
       Husband       4          4            0
       Wife          3          3            2

In other words, how do I force education.man to have a factor with 3
levels - "School" "College" "Beyond" - even though there is no
observation in "Beyond".

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From falissard_b at wanadoo.fr  Sun May  8 17:23:27 2005
From: falissard_b at wanadoo.fr (falissard)
Date: Sun, 8 May 2005 17:23:27 +0200
Subject: [R] Need a factor level even though there are no observations
In-Reply-To: <20050508135655.GD9427@lubyanka.local>
Message-ID: <20050508152328.10D9070000A9@mwinf1402.wanadoo.fr>

Hello,

Does this work?
m <- as.factor(education.man)
levels(m)[1:3] <- factorlabels
table(m)

Bruno

----------------------------------------------------------------------------
Bruno Falissard
INSERM U669, PSIGIAM
"Paris Sud Innovation Group in Adolescent Mental Health"
Maison de Solenn
97 Boulevard de Port Royal
75679 Paris cedex 14, France
tel : (+33) 6 81 82 70 76
fax : (+33) 1 45 59 34 18
web site : http://perso.wanadoo.fr/bruno.falissard/
----------------------------------------------------------------------------
 

-----Message d'origine-----
De??: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] De la part de Ajay Narottam Shah
Envoy????: dimanche 8 mai 2005 15:57
????: r-help
Objet??: [R] Need a factor level even though there are no observations

I'm in this situation:

     factorlabels <- c("School", "College", "Beyond")

with data for 8 families:

     education.man  <- c(1,2,1,2,1,2,1,2)       # Note : no "3" values
     education.wife <- c(1,2,3,1,2,3,1,2)       # 1,2,3 are all present.

My goal is to create this table:

                     School     College      Beyond
       Husband       4          4            0
       Wife          3          3            2


How do I do this?

I can readily do:
     education.wife <- factor(education.wife, labels=factorlabels)

But this breaks:
     education.man <- factor(education.man,   labels=factorlabels)

because none of the families have a husband who went beyond college.

I get around this problem in a limited way by:
     cautiously <- function(x, labels) {
       factor(x, labels=factorlabels[as.numeric(levels(factor(x)))])
     }
     education.man <- cautiously(education.man, labels=factorlabels)

Now I get:

     > table(education.man)
     School College 
          4       4 
     > table(education.wife)
     School College  Beyond 
          3       3       2 

This is a pain because now the two tables are not conformable. How do
I get to my end goal, which is the table:

                     School     College      Beyond
       Husband       4          4            0
       Wife          3          3            2

In other words, how do I force education.man to have a factor with 3
levels - "School" "College" "Beyond" - even though there is no
observation in "Beyond".

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Gregor.Gorjanc at bfro.uni-lj.si  Sun May  8 18:29:25 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sun, 8 May 2005 18:29:25 +0200
Subject: [R] Extract just some fields from XML
Message-ID: <7FFEE688B57D7346BC6241C55900E730B700E8@pollux.bfro.uni-lj.si>

Hello!

I am trying to get specific fields from an XML document and I am totally
puzzled. I hope someone can help me.

# URL
URL<-"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=11877539,11822933,11871444&retmode=xml&rettype=citation"
# download a XML file
tmp <- xmlTreeParse(URL, isURL = TRUE)
tmp <- xmlRoot(tmp)

Now I want to extract only node 'pubdate' and its children, but I don't
know how to do that unless I try to dig into the structure of the XML
file. The problem is that structure can differ and then hardcoded set
of list indices i.e. tmp[[i]][[j]]... doesn't help me.

I've read xmlEventParse but I don't understand handlers part up to the 
point that I could get anything usable from it. Here is something not
very usable ;)

  PubDate <- function(x, ...)
  {
    print(x)
  }
  xmlEventParse(URL, isURL = TRUE,
                handlers=list(PubDate=PubDate),
                addContext = FALSE)

Thanks in advance!

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From spencer.graves at pdf.com  Sun May  8 18:47:21 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 08 May 2005 09:47:21 -0700
Subject: [R] General Question re R vs S-Plus
In-Reply-To: <e206273d050508062612143f33@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E7FC@usctmx1106.merck.com>	<x2br7mcf24.fsf@turmalin.kubism.ku.dk>
	<e206273d050508062612143f33@mail.gmail.com>
Message-ID: <427E4299.9060906@pdf.com>

	  Unfortunately, many of the S-Plus GUI functions are NOT written in S. 
  For example, S-Plus 6.2 put the following into History while executing 
the GUI equivalent of "plot(1:3)":


guiPlot( PlotType = "Scatter", AxisType = "Linear")

guiModify( "LinePlot", Name = "GS1$1$1",
	DataSet = "1:3")


	  One thing I've found quite powerful in S-Plus 6.2 is File -> "Import 
Data".  Unfortunately, an import of an Excel file was implemented as 
follows:

guiImportData(FileName = "d:\\spencerg\\statmtds\\yield\\Sample 
Size0\\enrollment.xls", FileTypes = "Excel Worksheet (xl?)", 
TargetDataFrame = "enrollment", TargetStartCol = "<END>", 
TargetInsertOverwrite = "Create new data set", NameRowAuto = "Auto", 
NameColAuto = "Auto", StartCol = 1, EndCol = "<END>", StartRow = 1, 
EndRow = "<END>", PageNumberAuto = "Auto", StringsAsFactors = T, 
SortFactorLevels = T, LabelsAsNumbers = F, CenturyCutoffYear = 1930, 
KeepOrDropList = "<ALL>", SeparateDelimiters = T, ASCIIDateInFormat = 
"M/d/yyyy", ASCIITimeInFormat = "h:mm:ss tt", ASCIIDecimalPoint = 
"Period (.)", ASCIIThousandsSeparator = "None")

	  I'm unaware of any documentation for these "gui*" commands.  A middle 
manager I know complained one day, "the S-Plus GUI is worthless."  I 
don't go quite to that extreme, but I have rarely used anything other 
than "guiImportData", and when I've tried, the results did not encourage 
me to try again.

	  Best Wishes,
	  spencer graves

Jonathan Q. wrote:
> thanks, that does the trick
> 
> On 08 May 2005 10:11:47 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> 
>>"Liaw, Andy" <andy_liaw at merck.com> writes:
>>
>>
>>>>My preference is to learn the language and the windows interface
>>>>doesn't really do the trick.  My question is, if one uses the windows
>>>>interface for some functions, is there a way to see what the
>>>>equivalent code would be?  I have checked the manuals, help etc and
>>>>can't seem to find a way.
>>>
>>>That sounds more like a question for S-news than R-help...  In any case, my
>>>impression is that commands linked to the S-PLUS GUI menus and buttons are
>>>functions written for that.  You're not likely to learn much that will carry
>>>over to R.
>>
>>As I recall, the Splus GUI functions are (mostly?) written in S, so
>>you could study the source code. However, it might be a better idea to
>>fire up John Fox's Rcmdr package and try things similar to what you'd
>>do in Splus. Rcmdr makes a point of showing the code as it goes along.
>>
>>--
>>  O__  ---- Peter Dalgaard             Blegdamsvej 3
>> c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>>(*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>>
> 
> 
>



From jqm475 at gmail.com  Sun May  8 18:52:16 2005
From: jqm475 at gmail.com (Jonathan Q.)
Date: Sun, 8 May 2005 12:52:16 -0400
Subject: [R] General Question re R vs S-Plus
In-Reply-To: <427E4299.9060906@pdf.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E7FC@usctmx1106.merck.com>
	<x2br7mcf24.fsf@turmalin.kubism.ku.dk>
	<e206273d050508062612143f33@mail.gmail.com> <427E4299.9060906@pdf.com>
Message-ID: <e206273d050508095249b7cfff@mail.gmail.com>

so if I read all the answers to my post correctly, the best is to just
use the command letter in S-Plus or better yet use R.  thanks to all.

On 5/8/05, Spencer Graves <spencer.graves at pdf.com> wrote:
>          Unfortunately, many of the S-Plus GUI functions are NOT written in S.
>  For example, S-Plus 6.2 put the following into History while executing
> the GUI equivalent of "plot(1:3)":
> 
> guiPlot( PlotType = "Scatter", AxisType = "Linear")
> 
> guiModify( "LinePlot", Name = "GS1$1$1",
>        DataSet = "1:3")
> 
>          One thing I've found quite powerful in S-Plus 6.2 is File -> "Import
> Data".  Unfortunately, an import of an Excel file was implemented as
> follows:
> 
> guiImportData(FileName = "d:\\spencerg\\statmtds\\yield\\Sample
> Size0\\enrollment.xls", FileTypes = "Excel Worksheet (xl?)",
> TargetDataFrame = "enrollment", TargetStartCol = "<END>",
> TargetInsertOverwrite = "Create new data set", NameRowAuto = "Auto",
> NameColAuto = "Auto", StartCol = 1, EndCol = "<END>", StartRow = 1,
> EndRow = "<END>", PageNumberAuto = "Auto", StringsAsFactors = T,
> SortFactorLevels = T, LabelsAsNumbers = F, CenturyCutoffYear = 1930,
> KeepOrDropList = "<ALL>", SeparateDelimiters = T, ASCIIDateInFormat =
> "M/d/yyyy", ASCIITimeInFormat = "h:mm:ss tt", ASCIIDecimalPoint =
> "Period (.)", ASCIIThousandsSeparator = "None")
> 
>          I'm unaware of any documentation for these "gui*" commands.  A middle
> manager I know complained one day, "the S-Plus GUI is worthless."  I
> don't go quite to that extreme, but I have rarely used anything other
> than "guiImportData", and when I've tried, the results did not encourage
> me to try again.
> 
>          Best Wishes,
>          spencer graves
> 
> Jonathan Q. wrote:
> > thanks, that does the trick
> >
> > On 08 May 2005 10:11:47 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> >
> >>"Liaw, Andy" <andy_liaw at merck.com> writes:
> >>
> >>
> >>>>My preference is to learn the language and the windows interface
> >>>>doesn't really do the trick.  My question is, if one uses the windows
> >>>>interface for some functions, is there a way to see what the
> >>>>equivalent code would be?  I have checked the manuals, help etc and
> >>>>can't seem to find a way.
> >>>
> >>>That sounds more like a question for S-news than R-help...  In any case, my
> >>>impression is that commands linked to the S-PLUS GUI menus and buttons are
> >>>functions written for that.  You're not likely to learn much that will carry
> >>>over to R.
> >>
> >>As I recall, the Splus GUI functions are (mostly?) written in S, so
> >>you could study the source code. However, it might be a better idea to
> >>fire up John Fox's Rcmdr package and try things similar to what you'd
> >>do in Splus. Rcmdr makes a point of showing the code as it goes along.
> >>
> >>--
> >>  O__  ---- Peter Dalgaard             Blegdamsvej 3
> >> c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
> >>(*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> >>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> >>
> >
> >
> >
> 


-- 
Jonathan
jqm475 at gmail.com



From jqm475 at gmail.com  Sun May  8 19:24:58 2005
From: jqm475 at gmail.com (Jonathan Q.)
Date: Sun, 8 May 2005 13:24:58 -0400
Subject: [R] what is a good learning book for R?
Message-ID: <e206273d05050810241777c798@mail.gmail.com>

Aside from An Introduction to R by W. N. Venables, D. M. Smith (the
PDF is free), what would people recommend as a good starter book?  I
was thinking of introductory Statistics with R by Peter Dalgaard.  Any
thoughts??

My knowledge of Stats is stale and the primary use of R is for time
series analysis. Any advice would be greatly appreciated.  Thanks.




-- 
Jonathan
jqm475 at gmail.com



From ripley at stats.ox.ac.uk  Sun May  8 19:32:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 8 May 2005 18:32:19 +0100 (BST)
Subject: [R] Need a factor level even though there are no observations
In-Reply-To: <20050508135655.GD9427@lubyanka.local>
References: <20050508135655.GD9427@lubyanka.local>
Message-ID: <Pine.LNX.4.61.0505081831110.21600@gannet.stats>

Set levels not labels in the factor call.  E.g.

> factor("School", levels = factorlabels)

On Sun, 8 May 2005, Ajay Narottam Shah wrote:

> I'm in this situation:
>
>     factorlabels <- c("School", "College", "Beyond")
>
> with data for 8 families:
>
>     education.man  <- c(1,2,1,2,1,2,1,2)       # Note : no "3" values
>     education.wife <- c(1,2,3,1,2,3,1,2)       # 1,2,3 are all present.
>
> My goal is to create this table:
>
>                     School     College      Beyond
>       Husband       4          4            0
>       Wife          3          3            2
>
>
> How do I do this?
>
> I can readily do:
>     education.wife <- factor(education.wife, labels=factorlabels)
>
> But this breaks:
>     education.man <- factor(education.man,   labels=factorlabels)
>
> because none of the families have a husband who went beyond college.
>
> I get around this problem in a limited way by:
>     cautiously <- function(x, labels) {
>       factor(x, labels=factorlabels[as.numeric(levels(factor(x)))])
>     }
>     education.man <- cautiously(education.man, labels=factorlabels)
>
> Now I get:
>
>     > table(education.man)
>     School College
>          4       4
>     > table(education.wife)
>     School College  Beyond
>          3       3       2
>
> This is a pain because now the two tables are not conformable. How do
> I get to my end goal, which is the table:
>
>                     School     College      Beyond
>       Husband       4          4            0
>       Wife          3          3            2
>
> In other words, how do I force education.man to have a factor with 3
> levels - "School" "College" "Beyond" - even though there is no
> observation in "Beyond".
>
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From j_brindle at hotmail.com  Sun May  8 19:40:49 2005
From: j_brindle at hotmail.com (Jim BRINDLE)
Date: Sun, 8 May 2005 13:40:49 -0400
Subject: [R] Maximum # of predictors?
Message-ID: <BAY20-DAV1184A9DD237E8339434FFF801D0@phx.gbl>

Hello,

Is there anyway one can have more than 11 independent variables in a 
regression model?  To hopefully illustrate: with 13 predictors in my model, 
the last 2 coefficient estimates are "NA".  When I view the summary, it 
indicates that 2 coefficients are "not defined because of singularities".  I 
am using the command:

mod <- lm(y ~ ., data = volumes[,3:22])

I am fairly new to R so I don't know if my issue is something inherent to R 
or my modeling approach.

Any insight would be most appreciated... Thanks in advance



From blindglobe at gmail.com  Sun May  8 20:11:19 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Sun, 8 May 2005 20:11:19 +0200
Subject: [R] Maximum # of predictors?
In-Reply-To: <BAY20-DAV1184A9DD237E8339434FFF801D0@phx.gbl>
References: <BAY20-DAV1184A9DD237E8339434FFF801D0@phx.gbl>
Message-ID: <1abe3fa9050508111152f90f30@mail.gmail.com>

Your data won't support it.  It has nothing to do with R or any other
decent linear regression fitting software.

I've seen up to 60 indep variables, with a data set for which it
almost made sense.

best,
-tony

On 5/8/05, Jim BRINDLE <j_brindle at hotmail.com> wrote:
> Hello,
> 
> Is there anyway one can have more than 11 independent variables in a
> regression model?  To hopefully illustrate: with 13 predictors in my model,
> the last 2 coefficient estimates are "NA".  When I view the summary, it
> indicates that 2 coefficients are "not defined because of singularities".  I
> am using the command:
> 
> mod <- lm(y ~ ., data = volumes[,3:22])
> 
> I am fairly new to R so I don't know if my issue is something inherent to R
> or my modeling approach.
> 
> Any insight would be most appreciated... Thanks in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From ligges at statistik.uni-dortmund.de  Sun May  8 20:13:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 08 May 2005 20:13:43 +0200
Subject: [R] Maximum # of predictors?
In-Reply-To: <BAY20-DAV1184A9DD237E8339434FFF801D0@phx.gbl>
References: <BAY20-DAV1184A9DD237E8339434FFF801D0@phx.gbl>
Message-ID: <427E56D7.20202@statistik.uni-dortmund.de>

Jim BRINDLE wrote:

> Hello,
> 
> Is there anyway one can have more than 11 independent variables in a 
> regression model?  To hopefully illustrate: with 13 predictors in my model, 
> the last 2 coefficient estimates are "NA".  When I view the summary, it 
> indicates that 2 coefficients are "not defined because of singularities".  I 
> am using the command:
> 
> mod <- lm(y ~ ., data = volumes[,3:22])
> 
> I am fairly new to R so I don't know if my issue is something inherent to R 
> or my modeling approach.
> 
> Any insight would be most appreciated... Thanks in advance


The question has not much to do with R, but with linear models.
Probably you don't have enough observations in order to estimate those 
coefficients. Please read some statistical textbook on the topic of 
degrees of freedom in linear models and regression analysis.

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From weigand.stephen at charter.net  Sun May  8 20:23:10 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Sun, 8 May 2005 13:23:10 -0500
Subject: [R] what is a good learning book for R?
In-Reply-To: <e206273d05050810241777c798@mail.gmail.com>
References: <e206273d05050810241777c798@mail.gmail.com>
Message-ID: <3b67c435dab43b4f06c2cd14ae7e2992@charter.net>


On May 8, 2005, at 12:24 PM, Jonathan Q. wrote:

> Aside from An Introduction to R by W. N. Venables, D. M. Smith (the
> PDF is free), what would people recommend as a good starter book?  I
> was thinking of introductory Statistics with R by Peter Dalgaard.  Any
> thoughts??
>
> My knowledge of Stats is stale and the primary use of R is for time
> series analysis. Any advice would be greatly appreciated.  Thanks.


Jonathan,

The Contributed Documentation at

http://cran.r-project.org/other-docs.html

has plenty of resources, which I'm sure you've seen.

I've gotten a lot from "An Introduction to S and the
Hmisc and Design Libraries? by Carlos Alzola and Frank
E. Harrell. I also noticed a ?Time series reference card?
by Vito Ricci near the bottom of the page which might help
you get acquainted with some time series functions.

Personally, I'd say the particular text is less important
than trying to use R in everyday work. And doing this
leads one to the help pages all the time. These took, for
me, a special kind of critical line-by-line reading, at
least in the beginning.

Hope this helps,

Stephen



From chris at psyctc.org  Sun May  8 20:45:07 2005
From: chris at psyctc.org (Chris Evans)
Date: Sun, 08 May 2005 19:45:07 +0100
Subject: [R] working with CGIwithR
Message-ID: <427E6C43.9293.604E892@localhost>

<headline>Short question</headline>: 
Do people have advice on debugging R programs running after CGIwithR 
inputting of data from forms?  Is there a way of setting up fast 
local versions if your local machine has to be a windoze (2k) machine 
(R 2.1.0) and your server is a Debian, ssh shell only set up running 
R 1.8.0?  Are there simple guides to ways of not having to invoke R 
each time you submit data from a form?  No complaints here about R or 
CGIwithR: both brilliant.  I just need some help partly because my 
hardware resources are slow.

TIA, Chris

<H>Long question for those with time and wanting more detail</H>:

I'm not a statistician though I've spent a lot of the last thirty 
years of my life working with statisticians and doing my own stats 
part time.  I've worked from roll my own in dBaseII, through BMDP, 
loads of SPSS, SAS & SAS/IML, S+ and now, over the last two to three 
years, from S+ to R and loving it.  However, at times I really hit 
the limits of my amateur skills. .... and this is one of them!

I've put together some scripts to deal with Jacobson et al.'s ideas 
of "reliable change and clinically significant" change.  They work 
fine on R 2.10 on my Win2K box and can output to ascii & a windows 
graphics plot or to HTML and jpg using R2HTML.  Now I'm trying to 
transfer them to my www server which runs Debian stable and R 1.8.0 
and I'm using CGIwithR to get the input from people.  

All probably fine but the of course I struggled with some aspects of 
getting the data entry flexible and now something is simply not 
working as I pass the data to the main bits of the program that I've 
worked out for local use.  I'm sure the problem will turn out to be 
some trivial incorrect class or the like that's afflicting one of the 
parameters or bits of data.  I won't (yet) waste time here with 
detail but I'd love advice on ways to get more debugging information 
and on speeding things up as my server is slow, the only one I can 
currently afford, and each invocation of the script takes minutes to 
turn around and even with:
  zz <-file("Rmess.all",open="wt")
  sink(zz,type=c("output","message"))
I'm not trapping any error messages, the program just appears to skp 
entering a function I've written and continue on with nary an apology 
or gentle complaint about my stupidity!  Most unlike R!

Thanks to anyone with suggestions!

Chris



-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From spencer.graves at pdf.com  Sun May  8 21:01:07 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 08 May 2005 12:01:07 -0700
Subject: [R] what is a good learning book for R?
In-Reply-To: <3b67c435dab43b4f06c2cd14ae7e2992@charter.net>
References: <e206273d05050810241777c798@mail.gmail.com>
	<3b67c435dab43b4f06c2cd14ae7e2992@charter.net>
Message-ID: <427E61F3.6090008@pdf.com>

	  I would endorse Stephen's suggestions and add a couple of my own:  I 
find that the examples on the help pages are often quite useful.  If you 
are on Windows, you might also check "~\R\Rw2010pat\library" for 
specific packages, including "\demo" files.

	  There are many different packages that provide different capabilites 
for time series, and I have yet to reach a critical mass with any of 
them.  Venables and Ripley, Modern Applied Statistics with S (Springer) 
has a chapter on time series.  That book is quite good for many other 
things as well.

	  What kind of time series will you be analyzing, using what kinds of 
models?  Apart from functions acf, arima, etc., you may also need to 
learn how to associate times with the observations.  There are classes 
"POSIXct", "POSIXlt" and "Date" in the base package and "date" in the 
survival package that are unfortunately different.  There is also a 
"time series" class in the base Stats package.  Beyond this, I would 
check the "zoo" package, which attempts to reconcile different systems 
for storing dates.  In addition to "its" (irregular time series), you 
may wish to investigate the "Rmetrics" project.

	  There are other facilities as well.  Please read the posting guide 
"http://www.R-project.org/posting-guide.html" and submit more specific 
questions.  Other people's answers to your questions will likely help 
educate me.

	  Best Wishes,
	  spencer graves

Stephen D. Weigand wrote:
> 
> On May 8, 2005, at 12:24 PM, Jonathan Q. wrote:
> 
>> Aside from An Introduction to R by W. N. Venables, D. M. Smith (the
>> PDF is free), what would people recommend as a good starter book?  I
>> was thinking of introductory Statistics with R by Peter Dalgaard.  Any
>> thoughts??
>>
>> My knowledge of Stats is stale and the primary use of R is for time
>> series analysis. Any advice would be greatly appreciated.  Thanks.
> 
> 
> 
> Jonathan,
> 
> The Contributed Documentation at
> 
> http://cran.r-project.org/other-docs.html
> 
> has plenty of resources, which I'm sure you've seen.
> 
> I've gotten a lot from "An Introduction to S and the
> Hmisc and Design Libraries? by Carlos Alzola and Frank
> E. Harrell. I also noticed a ?Time series reference card?
> by Vito Ricci near the bottom of the page which might help
> you get acquainted with some time series functions.
> 
> Personally, I'd say the particular text is less important
> than trying to use R in everyday work. And doing this
> leads one to the help pages all the time. These took, for
> me, a special kind of critical line-by-line reading, at
> least in the beginning.
> 
> Hope this helps,
> 
> Stephen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From scott.br at gmail.com  Sun May  8 21:08:46 2005
From: scott.br at gmail.com (Scott Briggs)
Date: Sun, 8 May 2005 21:08:46 +0200
Subject: [R] Monotonic regression
Message-ID: <31fee4f0050508120816c29f7e@mail.gmail.com>

Hi, I'm trying to find an implementation of monotonic regression in R
and I haven't been able to find anything that's really related to
this.  isoMDS in the MASS package uses monotonic regression, however,
I was wondering if there is any standalone function for monotonic
regression?

Basically what I'm trying to do is implement monotonic regression
where I can see not just the results of each iteration but also be
able to tweak the input in order to test for or "kick" the regression
out of a local minimum so that I can make sure I have the global
minimum.

Any help would be much appreciated.  Thanks!

Scott



From Ted.Harding at nessie.mcc.ac.uk  Sun May  8 23:13:41 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 08 May 2005 22:13:41 +0100 (BST)
Subject: [R] Monotonic regression
In-Reply-To: <31fee4f0050508120816c29f7e@mail.gmail.com>
Message-ID: <XFMail.050508205706.Ted.Harding@nessie.mcc.ac.uk>

On 08-May-05 Scott Briggs wrote:
> Hi, I'm trying to find an implementation of monotonic regression in R
> and I haven't been able to find anything that's really related to
> this.  isoMDS in the MASS package uses monotonic regression, however,
> I was wondering if there is any standalone function for monotonic
> regression?
> 
> Basically what I'm trying to do is implement monotonic regression
> where I can see not just the results of each iteration but also be
> able to tweak the input in order to test for or "kick" the regression
> out of a local minimum so that I can make sure I have the global
> minimum.
> 
> Any help would be much appreciated.  Thanks!
> 
> Scott

You may probably find PAVA ("Pool Adjacent Violators Algorithm")
useful. Below is code for a simple version which I have been using
for a few years. I forget where I found it!

An R site search comes up with code for a version with more
complex functionality at

  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/9807.html

(contributed to r-help by Jan de Leeuw on 01 Jul 2004). I have
not tested this code.


pava<-function(x,wt=rep(1,length(x)))
{
  n<-length(x)
  if(n<=1) return(x)
  if(any(is.na(x)) || any(is.na(wt))) {
    stop("Missing values in 'x' or 'wt' not allowed")
  }
  lvlsets<-(1:n)
  repeat {
    viol<-(as.vector(diff(x))<0)
    if(!(any(viol))) break
    i<-min( (1:(n-1))[viol])
    lvl1<-lvlsets[i]
    lvl2<-lvlsets[i+1]
    ilvl<-(lvlsets==lvl1 | lvlsets==lvl2)
    x[ilvl]<-sum(x[ilvl]*wt[ilvl])/sum(wt[ilvl])
    lvlsets[ilvl]<-lvl1
  }
  x
}
# Examples:
# > x<-c(1,0,1,0,0,1,0,1,1,0,1,0)
# > x
# [1] 1 0 1 0 0 1 0 1 1 0 1 0
# > pava(x)
#  [1] 0.4 0.4 0.4 0.4 0.4 0.5 0.5 0.6 0.6 0.6 0.6 0.6
# > 
# > pava(c(0,0,2/4,1/5,2/4,1/2,4/5,5/8,7/11,10/11),
#        c(5,4,4,5,4,2,5,8,11,11))
# [1] 0.0000000 0.0000000 0.3333333 0.3333333 0.5000000
# [6] 0.5000000 0.6666667 0.6666667 0.6666667 0.9090909
# (example where data are {ri,ni} so x={ri/ni} and w={ni})


Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-May-05                                       Time: 20:56:28
------------------------------ XFMail ------------------------------



From jasonou at yahoo.com  Mon May  9 01:56:49 2005
From: jasonou at yahoo.com (Jingzhao Ou)
Date: Sun, 8 May 2005 16:56:49 -0700 (PDT)
Subject: [R] Implementing an ARMA filter
Message-ID: <20050508235649.52117.qmail@web80402.mail.yahoo.com>

Dear all,

I am new to R. I need to implement an ARMA filter, some thing like:

y(n) = a0*x(n) + a1*x(n-1) + b1*y(n-1) + b2*y(n-2)

I checked out the filter manual page. It doesn't seem that the filter function
can do this job for me. Can any one help me out? 

Thanks a lot!

Best regards,
Jingzhao



From skene at berkeley.edu  Mon May  9 01:32:00 2005
From: skene at berkeley.edu (Jennifer Skene)
Date: Sun, 8 May 2005 16:32:00 -0700
Subject: [R] z limits
Message-ID: <p06110400bea4516325c8@[136.152.194.130]>

Hello,

I am using R to run a program called GRASP - Generalized Regression 
Analysis and Spatial Prediction.

I am trying to draw a figure using GRASP, and I get the following 
error message in R:

Error in image.default(map, col = heat.colors(12)) :
	invalid z limits

What are z limits?  Can I manually set them?

Please let me know if you have any insight on this!

Thanks,
Jennifer Skene



From aoganyan at niss.org  Mon May  9 02:10:04 2005
From: aoganyan at niss.org (Anna Oganyan)
Date: Sun, 08 May 2005 20:10:04 -0400
Subject: [R] mutilvariate density estimation
Message-ID: <427EAA5C.8060700@niss.org>

I would appreciate it if anyone could share or point me to an R 
implementation of mutilvariate density estimation (6 or higher dimensions).
 

Many thanks.

Anna



From sdavis2 at mail.nih.gov  Mon May  9 02:38:41 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sun, 8 May 2005 20:38:41 -0400
Subject: [R] Extract just some fields from XML
References: <7FFEE688B57D7346BC6241C55900E730B700E8@pollux.bfro.uni-lj.si>
Message-ID: <000c01c5542f$72d7d270$5179f345@WATSON>

Gregor,

I'm not answering your question directly, but have you looked at the 
bioconductor package "annotate"?  I bet it does much of what you are trying 
to do....

http://www.bioconductor.org/repository/release1.5/package/html/index.html

List of functions:

http://www.bioconductor.org/repository/release1.5/package/html/descrips/annotateDesc.html

Sean

----- Original Message ----- 
From: "Gorjanc Gregor" <Gregor.Gorjanc at bfro.uni-lj.si>
To: <r-help at stat.math.ethz.ch>
Sent: Sunday, May 08, 2005 12:29 PM
Subject: [R] Extract just some fields from XML


> Hello!
>
> I am trying to get specific fields from an XML document and I am totally
> puzzled. I hope someone can help me.
>
> # URL
> URL<-"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=11877539,11822933,11871444&retmode=xml&rettype=citation"
> # download a XML file
> tmp <- xmlTreeParse(URL, isURL = TRUE)
> tmp <- xmlRoot(tmp)
>
> Now I want to extract only node 'pubdate' and its children, but I don't
> know how to do that unless I try to dig into the structure of the XML
> file. The problem is that structure can differ and then hardcoded set
> of list indices i.e. tmp[[i]][[j]]... doesn't help me.
>
> I've read xmlEventParse but I don't understand handlers part up to the
> point that I could get anything usable from it. Here is something not
> very usable ;)
>
>  PubDate <- function(x, ...)
>  {
>    print(x)
>  }
>  xmlEventParse(URL, isURL = TRUE,
>                handlers=list(PubDate=PubDate),
>                addContext = FALSE)
>
> Thanks in advance!
>
> Lep pozdrav / With regards,
>    Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
> you have no certainty until you try." Sophocles ~ 450 B.C.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jwd at surewest.net  Mon May  9 02:44:27 2005
From: jwd at surewest.net (John Dougherty)
Date: Sun, 8 May 2005 17:44:27 -0700
Subject: [R] z limits
In-Reply-To: <p06110400bea4516325c8@[136.152.194.130]>
References: <p06110400bea4516325c8@[136.152.194.130]>
Message-ID: <200505081744.28092.jwd@surewest.net>

On Sunday 08 May 2005 16:32, Jennifer Skene wrote:
> Hello,
>
> I am using R to run a program called GRASP - Generalized Regression
> Analysis and Spatial Prediction.
>
> I am trying to draw a figure using GRASP, and I get the following
> error message in R:
>
> Error in image.default(map, col = heat.colors(12)) :
> 	invalid z limits
>
> What are z limits?  Can I manually set them?
>
> Please let me know if you have any insight on this!
>
> Thanks,
> Jennifer Skene

I believe that the "z" limits in GRASP relate to the third dimensional 
variable.  If you are using spatial data that uses either altitude or another 
variable that gives a pseudo-surface, you should probably check the 
variable's range.

JWD



From ripley at stats.ox.ac.uk  Mon May  9 06:48:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 May 2005 05:48:53 +0100 (BST)
Subject: [R] Implementing an ARMA filter
In-Reply-To: <20050508235649.52117.qmail@web80402.mail.yahoo.com>
References: <20050508235649.52117.qmail@web80402.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0505090546320.28603@gannet.stats>

On Sun, 8 May 2005, Jingzhao Ou wrote:

> I am new to R. I need to implement an ARMA filter, some thing like:
>
> y(n) = a0*x(n) + a1*x(n-1) + b1*y(n-1) + b2*y(n-2)
>
> I checked out the filter manual page. It doesn't seem that the filter function
> can do this job for me. Can any one help me out?

You can help yourself by reading that page more closely.

Check the code for arima.sim, which uses filter for exactly this purpose.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Gregor.Gorjanc at bfro.uni-lj.si  Mon May  9 07:28:42 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 9 May 2005 07:28:42 +0200
Subject: [R] Extract just some fields from XML
Message-ID: <7FFEE688B57D7346BC6241C55900E730B700ED@pollux.bfro.uni-lj.si>

-----Original Message-----
From: Sean Davis [mailto:sdavis2 at mail.nih.gov]
Sent: pon 2005-05-09 02:38
To: Gorjanc Gregor; r-help at stat.math.ethz.ch
Subject: Re: [R] Extract just some fields from XML
 
>Gregor,
>
>I'm not answering your question directly, but have you looked at the 
>bioconductor package "annotate"?  I bet it does much of what you are trying 
>to do....
>
>http://www.bioconductor.org/repository/release1.5/package/html/index.html
>
>List of functions:
>
>http://www.bioconductor.org/repository/release1.5/package/html/descrips/annotateDesc.html

Sean,

thank you for this. I'm aware of functions in 'annotate' and I also usem them in my 
work.

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From jasonou at yahoo.com  Mon May  9 08:43:35 2005
From: jasonou at yahoo.com (Jingzhao Ou)
Date: Sun, 8 May 2005 23:43:35 -0700 (PDT)
Subject: [R] Implementing an ARMA filter
In-Reply-To: 6667
Message-ID: <20050509064335.8501.qmail@web80401.mail.yahoo.com>

Dear Prof. Ripley,

Thanks a lot for your reply. I just made it work after checking out arima.sim.

I am switching from MATLAB to R recently and am working hard to overcome the
learning curve at this time. The filter command in MATLAB is different from
that in R, which caused some confusion to me. :-)

Wish you have a nice day!

Best regards,
Jingzhao

--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> You can help yourself by reading that page more closely.
> 
> Check the code for arima.sim, which uses filter for exactly this purpose.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From r.hankin at noc.soton.ac.uk  Mon May  9 08:55:08 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 9 May 2005 07:55:08 +0100
Subject: [R] functions of functions
Message-ID: <726a71a67bd0be570db5f8ce636b2d86@soc.soton.ac.uk>

Hi

I have an application where my difficulty boils down to not
being able to define a function f() with the following properties:

f("sin",0:2,x)               #returns sin(x+0) + sin(x+1) + sin(x+2)
f(c("sin","cos"), 1:2,x)     #returns sin(x+1) + cos(x+2)
f(c("sin","cos","exp"),3,x)  #returns sin(x+3) + cos(x+3) + exp(x+3)

anyone?


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ajayshah at mayin.org  Mon May  9 09:01:08 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Mon, 9 May 2005 12:31:08 +0530
Subject: [R] Summary: My question about factor levels versus factor labels.
Message-ID: <20050509070108.GN9427@lubyanka.local>

Yesterday, I had asked for help on the list. Brian Ripley and Bruno
Falissard had most kindly responded to me. Here is the solution.

  > factorlabels <- c("School", "College", "Beyond")
  > #                       1          2         3

  > education.man  <- c(1,2,1,2,1,2,1,2)  # PROBLEM: Level "3" doesn't occur.
  > education.wife <- c(1,2,3,1,2,3,1,2)

  > education.wife <- factor(education.wife, labels=factorlabels)  # Is fine.

  > # But this breaks --
  > # education.man <- factor(education.man,   labels=factorlabels)

  > # Solution --
  > education.man <- factor(education.man, levels = c(1,2,3),
                                           labels=factorlabels)

  > # So now we can do --
  > a <- rbind(table(education.wife), table(education.man))
  > rownames(a) <- c("Wife", "Man")
  > print(a)
       School College Beyond
  Wife      3       3      2
  Man       4       4      0

which was the table that I had wanted.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From timdowns at telstra.com  Mon May  9 09:26:52 2005
From: timdowns at telstra.com (Timothy Downs)
Date: Mon, 9 May 2005 17:26:52 +1000
Subject: [R] Lists, Vectors, TclTk
Message-ID: <16629501.1115623612371.JavaMail.imail@web10sl>

I am trying to create an array of tkwidgets in R, as in the following example:

require(tcltk)
tt <- tktoplevel()
cbs <- c()
for(i in 1:40) cbs[i] <- tkcheckbutton(tt)

This only stores the $ID part returned by tkcheckbutton. How would I go about doing correctly?

Thanks.

Timothy



From maechler at stat.math.ethz.ch  Mon May  9 09:30:54 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 9 May 2005 09:30:54 +0200
Subject: [R] Monotonic regression
In-Reply-To: <31fee4f0050508120816c29f7e@mail.gmail.com>
References: <31fee4f0050508120816c29f7e@mail.gmail.com>
Message-ID: <17023.4526.330540.369436@stat.math.ethz.ch>

>>>>> "Scott" == Scott Briggs <scott.br at gmail.com>
>>>>>     on Sun, 8 May 2005 21:08:46 +0200 writes:

    Scott> Hi, I'm trying to find an implementation of monotonic regression in R
    Scott> and I haven't been able to find anything that's really related to
    Scott> this.  isoMDS in the MASS package uses monotonic regression, however,
    Scott> I was wondering if there is any standalone function for monotonic
    Scott> regression?

yes, isoreg()  which actually was built by ``exporting'' the
relevant bits & pieaces from MASS' isoMDS.


    Scott> Basically what I'm trying to do is implement monotonic regression
    Scott> where I can see not just the results of each iteration but also be
    Scott> able to tweak the input in order to test for or "kick" the regression
    Scott> out of a local minimum so that I can make sure I have the global
    Scott> minimum.

(Ted Harding has already answered this part.)



From ligges at statistik.uni-dortmund.de  Mon May  9 09:34:32 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 May 2005 09:34:32 +0200
Subject: [R] functions of functions
In-Reply-To: <726a71a67bd0be570db5f8ce636b2d86@soc.soton.ac.uk>
References: <726a71a67bd0be570db5f8ce636b2d86@soc.soton.ac.uk>
Message-ID: <427F1288.3010401@statistik.uni-dortmund.de>

Robin Hankin wrote:

> Hi
> 
> I have an application where my difficulty boils down to not
> being able to define a function f() with the following properties:
> 
> f("sin",0:2,x)               #returns sin(x+0) + sin(x+1) + sin(x+2)
> f(c("sin","cos"), 1:2,x)     #returns sin(x+1) + cos(x+2)
> f(c("sin","cos","exp"),3,x)  #returns sin(x+3) + cos(x+3) + exp(x+3)
> 
> anyone?

Not really nice, but hopefully works:

f <- function(foo, int, x){
   # too lazy to think myself about recycling:
   X <- cbind(foo, x + int)
   # mapply-ing over both columns
   values <- mapply(function(foo, x) do.call(foo, list(x)),
        X[,1], as.integer(X[,2]))
   # caculating the sum:
   return(sum(values))
}


Uwe




> 
> -- 
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Mon May  9 09:36:43 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 09 May 2005 08:36:43 +0100
Subject: [R] Lists, Vectors, TclTk
In-Reply-To: <16629501.1115623612371.JavaMail.imail@web10sl>
References: <16629501.1115623612371.JavaMail.imail@web10sl>
Message-ID: <427F130B.8060802@stats.uwo.ca>

Timothy Downs wrote:
> I am trying to create an array of tkwidgets in R, as in the following example:
> 
> require(tcltk)
> tt <- tktoplevel()
> cbs <- c()
> for(i in 1:40) cbs[i] <- tkcheckbutton(tt)
> 
> This only stores the $ID part returned by tkcheckbutton. How would I go about doing correctly?

Use cbs[[i]] <- tkcheckbutton(tt).

I'd also recommend using cbs <- list() instead of cbs <- c() because I 
find it represents your intentions more clearly, but both work.

Duncan Murdoch



From r.hankin at noc.soton.ac.uk  Mon May  9 09:54:38 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 9 May 2005 08:54:38 +0100
Subject: [R] functions of functions
In-Reply-To: <427F1288.3010401@statistik.uni-dortmund.de>
References: <726a71a67bd0be570db5f8ce636b2d86@soc.soton.ac.uk>
	<427F1288.3010401@statistik.uni-dortmund.de>
Message-ID: <9e9a6b04bf80630864325afb52513949@soc.soton.ac.uk>

Hello Uwe

thanks for this.   Unfortunately it doesn't quite do what I want:


R> x <- c(0.3,0.3,0.5)
R> f(c("sin","cos"),1:2,x)


1] 1.266795
Warning messages:
1: longer object length
	is not a multiple of shorter object length in: x + int
2: number of rows of result
	is not a multiple of vector length (arg 1) in: cbind(foo, x + int)
R>


[


I need

R> sin(x+1) + cos(x+2)
[1] 0.2972822 0.2972822 0.1963514

]


best wishes

Robin



On May 9, 2005, at 08:34 am, Uwe Ligges wrote:

> Robin Hankin wrote:
>
>> Hi
>> I have an application where my difficulty boils down to not
>> being able to define a function f() with the following properties:
>> f("sin",0:2,x)               #returns sin(x+0) + sin(x+1) + sin(x+2)
>> f(c("sin","cos"), 1:2,x)     #returns sin(x+1) + cos(x+2)
>> f(c("sin","cos","exp"),3,x)  #returns sin(x+3) + cos(x+3) + exp(x+3)
>> anyone?
>
> Not really nice, but hopefully works:
>
> f <- function(foo, int, x){
>   # too lazy to think myself about recycling:
>   X <- cbind(foo, x + int)
>   # mapply-ing over both columns
>   values <- mapply(function(foo, x) do.call(foo, list(x)),
>        X[,1], as.integer(X[,2]))
>   # caculating the sum:
>   return(sum(values))
> }
>
>
> Uwe
>
>
>
>
>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ligges at statistik.uni-dortmund.de  Mon May  9 10:13:19 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 May 2005 10:13:19 +0200
Subject: [R] functions of functions
In-Reply-To: <9e9a6b04bf80630864325afb52513949@soc.soton.ac.uk>
References: <726a71a67bd0be570db5f8ce636b2d86@soc.soton.ac.uk>
	<427F1288.3010401@statistik.uni-dortmund.de>
	<9e9a6b04bf80630864325afb52513949@soc.soton.ac.uk>
Message-ID: <427F1B9F.2050809@statistik.uni-dortmund.de>

Robin Hankin wrote:

> Hello Uwe
> 
> thanks for this.   Unfortunately it doesn't quite do what I want:
> 
> 
> R> x <- c(0.3,0.3,0.5)
> R> f(c("sin","cos"),1:2,x)
> 
> 
> 1] 1.266795
> Warning messages:
> 1: longer object length
>     is not a multiple of shorter object length in: x + int
> 2: number of rows of result
>     is not a multiple of vector length (arg 1) in: cbind(foo, x + int)
> R>
> 
> [
> 
> 
> I need
> 
> R> sin(x+1) + cos(x+2)
> [1] 0.2972822 0.2972822 0.1963514
> 
> ]

I see, now the problem is much clearer, so what about:

f <- function(foo, int, x){
     lf <- length(foo)
     li <- length(int)
     l <- max(lf, li)
     if(l > lf) foo <- rep(foo, length = l)
     else if(l > li) int <- rep(int, length = l)
     values <- mapply(function(foo, int)
         do.call(foo, list(x+int)), foo, int)
     return(rowSums(values))
}


Uwe


> 
> best wishes
> 
> Robin
> 
> 
> 
> On May 9, 2005, at 08:34 am, Uwe Ligges wrote:
> 
>> Robin Hankin wrote:
>>
>>> Hi
>>> I have an application where my difficulty boils down to not
>>> being able to define a function f() with the following properties:
>>> f("sin",0:2,x)               #returns sin(x+0) + sin(x+1) + sin(x+2)
>>> f(c("sin","cos"), 1:2,x)     #returns sin(x+1) + cos(x+2)
>>> f(c("sin","cos","exp"),3,x)  #returns sin(x+3) + cos(x+3) + exp(x+3)
>>> anyone?
>>
>>
>> Not really nice, but hopefully works:
>>
>> f <- function(foo, int, x){
>>   # too lazy to think myself about recycling:
>>   X <- cbind(foo, x + int)
>>   # mapply-ing over both columns
>>   values <- mapply(function(foo, x) do.call(foo, list(x)),
>>        X[,1], as.integer(X[,2]))
>>   # caculating the sum:
>>   return(sum(values))
>> }
>>
>>
>> Uwe
>>
>>
>>
>>
>>
> -- 
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743



From rich.fitzjohn at gmail.com  Mon May  9 10:14:32 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Mon, 9 May 2005 20:14:32 +1200
Subject: [R] functions of functions
In-Reply-To: <9e9a6b04bf80630864325afb52513949@soc.soton.ac.uk>
References: <726a71a67bd0be570db5f8ce636b2d86@soc.soton.ac.uk>
	<427F1288.3010401@statistik.uni-dortmund.de>
	<9e9a6b04bf80630864325afb52513949@soc.soton.ac.uk>
Message-ID: <5934ae57050509011477213643@mail.gmail.com>

This has a fairly brutal approach to recycling, and won't warn you if
one argument's length is not a multiple of another, etc.  

But seems to do the trick.

f <- function(f, y, x) {
  n <- max(c(length(f), length(y)))
  f <- rep(f, length.out=n)
  y <- rep(y, length.out=n)
  rowSums(mapply(function(f, y) (match.fun(f))(y + x), f=f, y=y))
}

Testing, it matches your examples...
x <- rnorm(20)
identical(all.equal(sin(x+0) + sin(x+1) + sin(x+2),
                    f("sin",0:2,x)), TRUE)
identical(all.equal(sin(x+1) + cos(x+2),
                    f(c("sin","cos"), 1:2,x)), TRUE)
identical(all.equal(sin(x+3) + cos(x+3) + exp(x+3),
                    f(c("sin","cos","exp"),3,x)), TRUE)

Cheers,
Rich


On 5/9/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hello Uwe
> 
> thanks for this.   Unfortunately it doesn't quite do what I want:
> 
> R> x <- c(0.3,0.3,0.5)
> R> f(c("sin","cos"),1:2,x)
> 
> 1] 1.266795
> Warning messages:
> 1: longer object length
>         is not a multiple of shorter object length in: x + int
> 2: number of rows of result
>         is not a multiple of vector length (arg 1) in: cbind(foo, x + int)
> R>
> 
> [
> 
> I need
> 
> R> sin(x+1) + cos(x+2)
> [1] 0.2972822 0.2972822 0.1963514
> 
> ]
> 
> best wishes
> 
> Robin
> 
> 
> On May 9, 2005, at 08:34 am, Uwe Ligges wrote:
> 
> > Robin Hankin wrote:
> >
> >> Hi
> >> I have an application where my difficulty boils down to not
> >> being able to define a function f() with the following properties:
> >> f("sin",0:2,x)               #returns sin(x+0) + sin(x+1) + sin(x+2)
> >> f(c("sin","cos"), 1:2,x)     #returns sin(x+1) + cos(x+2)
> >> f(c("sin","cos","exp"),3,x)  #returns sin(x+3) + cos(x+3) + exp(x+3)
> >> anyone?
> >
> > Not really nice, but hopefully works:
> >
> > f <- function(foo, int, x){
> >   # too lazy to think myself about recycling:
> >   X <- cbind(foo, x + int)
> >   # mapply-ing over both columns
> >   values <- mapply(function(foo, x) do.call(foo, list(x)),
> >        X[,1], as.integer(X[,2]))
> >   # caculating the sum:
> >   return(sum(values))
> > }
> >
> >
> > Uwe
> >
> >
> >
> >
> >
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From r.hankin at noc.soton.ac.uk  Mon May  9 10:16:50 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 9 May 2005 09:16:50 +0100
Subject: [R] functions of functions
In-Reply-To: <427F1B9F.2050809@statistik.uni-dortmund.de>
References: <726a71a67bd0be570db5f8ce636b2d86@soc.soton.ac.uk>
	<427F1288.3010401@statistik.uni-dortmund.de>
	<9e9a6b04bf80630864325afb52513949@soc.soton.ac.uk>
	<427F1B9F.2050809@statistik.uni-dortmund.de>
Message-ID: <8764759e2a0db1d6dc52d622b741c004@soc.soton.ac.uk>

Yes!

perfect!


very best wishes

Robin


On May 9, 2005, at 09:13 am, Uwe Ligges wrote:

> Robin Hankin wrote:
>
>> Hello Uwe
>
[snip]


> I see, now the problem is much clearer, so what about:
>
> f <- function(foo, int, x){
>     lf <- length(foo)
>     li <- length(int)
>     l <- max(lf, li)
>     if(l > lf) foo <- rep(foo, length = l)
>     else if(l > li) int <- rep(int, length = l)
>     values <- mapply(function(foo, int)
>         do.call(foo, list(x+int)), foo, int)
>     return(rowSums(values))
> }
>
>
> Uwe
>
>
>> best wishes
>> Robin
>> On May 9, 2005, at 08:34 am, Uwe Ligges wrote:
>>> Robin Hankin wrote:
>>>
>>>> Hi
>>>> I have an application where my difficulty boils down to not
>>>> being able to define a function f() with the following properties:
>>>> f("sin",0:2,x)               #returns sin(x+0) + sin(x+1) + sin(x+2)
>>>> f(c("sin","cos"), 1:2,x)     #returns sin(x+1) + cos(x+2)
>>>> f(c("sin","cos","exp"),3,x)  #returns sin(x+3) + cos(x+3) + exp(x+3)
>>>> anyone?
>>>
>>>
>>> Not really nice, but hopefully works:
>>>


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From mathiasd at zedat.fu-berlin.de  Mon May  9 10:48:29 2005
From: mathiasd at zedat.fu-berlin.de (Mathias Ditzen)
Date: Mon, 09 May 2005 10:48:29 +0200
Subject: [R] units of a biplot
Message-ID: <427F23DD.4000803@zedat.fu-berlin.de>

Hello everybody,
I am afraid this might be a stupid question, but after scanning a little 
bit through the help archive and the internet, I didn't come up with an 
answer myself.
I have done a principle component analysis on a data set, which yields 
the mean response of 14 olfactory neurons to 16 different odors. When 
plotting the result in a biplot, it nicely shows me the odors in 
different positions in the principle component space. Additionally, the 
neurons are shown as vectors within this plot. My question now is: what 
are the units of the axes? When doing the principle component in IDL, 
the relative positions of the odors are the same, but units on the axes 
span a much wider range. Additionally I couldn't figure out what the 
units in the variable space (right and upper border in the biplot) do 
represent.
I really hope somebody can helpme with this,
Mathias Ditzen



From relocation at fortner.net  Mon May  9 11:35:53 2005
From: relocation at fortner.net (Christy)
Date: Mon, 9 May 2005 11:35:53 +0200
Subject: [R] A Special Alert to Investors
Message-ID: <13092099646.15964@xdsl-81-173-248-185.netcologne.de>

Agent 155 Media Group Inc. (OTC: AGMG)

"Agent155.com evolved out of the passion for creativity - both it's
expression, and its exposure. Agent155.com is a street-level creative arena, providing an affordable, high-quality online presence for the global artistic and athletic communities. Our mission is to make this opportunity for exposure, collaboration, and networking accessible to  everyoneworldwide".

CEO-Christopher J. Martinez
Agent155 Media Group, Inc. has emerged as a multimedia company for Performers, athletes, filmmakers, models, musicians, and artists contribute trailers, short films, clips, MP3s, photography, multimedia content, and much more. Our mission is to make this opportunity for exposure, collaboration, and networking accessible to all talent worldwide.



"This is truly a multi-media distribution play, with a traditional
distribution opportunity. Agent155.com attracts talent that is yet to be discovered, with the potential of one superstar delivering millions of dollars in revenues to the label. The possibilities are endless when you consider the worldwide pool of talent from which we pull. There is a revolution occurring in the artistic community; Agent155.com is leading it."

Within 3 months AGMG is launching a music service for Musicians and consumers worldwide. Musicians will be able to upload, license and sell their songs per download for the iPod and all mobile devices and keep 100% of the revenue. Agent 155 Media Group will generate revenue from a membership upgrade which will be $15.50 per month totaling $31.00 per month for members utilizing service. Another revenue stream that will be exercised is to enable artists to sell their work and video will also be monetized for mobile devices.

Company Profile

Tired of being told who has talent and who doesn't? Corporate America and Hollywood often control the artists and athletes we see, hear and experience. Extremely passionate about the creative and athletic talent that most of us will never see, the team at Agent155 Media Group has launched a grassroots effort and emerged as a multimedia company where talent from around the world can be seen and heard at Agent155.com -- with video and lots of it, something radically different. Any performer,athlete, filmmaker, model, musician/singer, photographer and artist can
be their own agent and upload trailers, short films, reels, MP3s, clips,photography, bios and other content, up to 500MB, on the secure site.

Agent155 Media Group Inc. is based in San Diego where its website (Agent155.com) and related services are produced for worldwide distribution. The company offers an affordable, high-quality online presence for the global artistic and athletic communities through its website, Agent155.com.


Agent155.com offers models, performers, artists, athletes, musicians,filmmakers and agencies a multi-media content management solution,enabling the creative world a collaborative forum to network and develop.



Investor Overview

Agent155 Media Group Inc. is based in San Diego where its website (Agent155.com) and related services are produced for worldwide  distribution. The company offers an affordable, high-quality online presence for the global artistic and athletic communities through its website, Agent155.com. Agent155.com offers models, performers, artists,athletes, musicians, filmmakers and agencies a multi-media content management solution, enabling the creative world a collaborative forum to network and develop. Agent155.com provides talent agencies,agents,producers, directors, and recording companies a one-stop location to search and view the profiles and work of emerging talent. Agent155 Media Group Inc. also re-distributes member content through traditional media channels like TV, radio, film and print. Agent155 Media Group Inc. produces films, music tours, commercials, and various events using talent exclusively from Agent155.com.

Any performer, athlete, filmmaker, model, musician/singer, photographer and artist can be their own agent and upload trailers, short films,reels, MP3s, clips, photography, bios and other content, up to 500MB, on the secure site. Membership is growing by 150 subscribers per month. Agent155 has recently launched a record label, and expected royalties will add to this revenue. Revenue is expected to be $1 million in fiscal 2005, and over $5 million in 2006.

The advantage that Agent155 has over its competition is that it has no competition!

Within 3 months AGMG is launching a music service for Musicians and consumers worldwide. Musicians will be able to upload, license and sell their songs per download for the iPod and all mobile devices and keep 100% of the revenue. Agent 155 Media Group will generate revenue from a membership upgrade which will be $15.50 per month totaling $31.00 per month for members utilizing service. Another revenue stream that will be exercised is to enable artists to sell their work and video will also be monetized for mobile devices.



From ripley at stats.ox.ac.uk  Mon May  9 12:54:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 May 2005 11:54:00 +0100 (BST)
Subject: [R] units of a biplot
In-Reply-To: <427F23DD.4000803@zedat.fu-berlin.de>
References: <427F23DD.4000803@zedat.fu-berlin.de>
Message-ID: <Pine.LNX.4.61.0505091148330.27774@gannet.stats>

First, I think you are talking about the princomp method of biplot, and 
not a general biplot.

There are many different biplots for PCA, some incorrectly implemented. 
The derivation is in MASS4 pp. 310-1, as well as the interpretation.  You 
need to understand what a biplot is to understand the units, which depend 
on the value of 'scale' and 'pc.biplot'.  See the note on the help page 
for biplot.princomp.

On Mon, 9 May 2005, Mathias Ditzen wrote:

> I am afraid this might be a stupid question, but after scanning a little bit 
> through the help archive and the internet, I didn't come up with an answer 
> myself.
> I have done a principle component analysis on a data set, which yields the 
> mean response of 14 olfactory neurons to 16 different odors. When plotting 
> the result in a biplot, it nicely shows me the odors in different positions 
> in the principle component space. Additionally, the neurons are shown as 
> vectors within this plot. My question now is: what are the units of the axes? 
> When doing the principle component in IDL, the relative positions of the 
> odors are the same, but units on the axes span a much wider range. 
> Additionally I couldn't figure out what the units in the variable space 
> (right and upper border in the biplot) do represent.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From thomas.fabbro at unifr.ch  Mon May  9 14:54:02 2005
From: thomas.fabbro at unifr.ch (FabbroThomas)
Date: Mon, 9 May 2005 14:54:02 +0200
Subject: [R] bootstap and lme4
Message-ID: <5b97cff94782bcac9766efca767628d9@unifr.ch>

Hi,

I am trying to get bootstrap confidence intervals on  variance 
components and related statistics. To calculate the variance components 
I use the package lme4.

 > off.fun <- function(data, i){
   d <- data[i,]
   lme1<- lmer(y ~ trt + (trt-1|group), d)
   VarCorr(lme1)@reSumry$group[2,1] #just as an example
}
 > off.boot <- boot(data=data.sim, statistic=off.fun, R=100)

If I choose small values of R (<10) then I get very reasonable results 
but for large R I get the following error massage:


      *** malloc: vm_allocate(size=1069056) failed (error code=3)
      *** malloc[682]: error: Can't allocate region
      Error in .local(x, ...) : Calloc could not allocate (48 of 8) 
memory


Can anyone tell me why I get this error massage and  what I can do to 
avoid this problem?

Thank you very much for your help!

Thomas



I am working with:
R 2.1.0
boot 1.2-22
lme4 0.95-6
Matrix 0.95-7
lattice 0.11-6
latticeExtra 0.1-1

On Mac OS X 10.3.9



From bates at stat.wisc.edu  Mon May  9 15:14:29 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 09 May 2005 08:14:29 -0500
Subject: [R] bootstap and lme4
In-Reply-To: <5b97cff94782bcac9766efca767628d9@unifr.ch>
References: <5b97cff94782bcac9766efca767628d9@unifr.ch>
Message-ID: <427F6235.4030904@stat.wisc.edu>

FabbroThomas wrote:
> Hi,
> 
> I am trying to get bootstrap confidence intervals on  variance
> components and related statistics. To calculate the variance components
> I use the package lme4.
> 
>> off.fun <- function(data, i){
>   d <- data[i,]
>   lme1<- lmer(y ~ trt + (trt-1|group), d)
>   VarCorr(lme1)@reSumry$group[2,1] #just as an example
> }
>> off.boot <- boot(data=data.sim, statistic=off.fun, R=100)
> 
> If I choose small values of R (<10) then I get very reasonable results
> but for large R I get the following error massage:
> 
> 
>      *** malloc: vm_allocate(size=1069056) failed (error code=3)
>      *** malloc[682]: error: Can't allocate region
>      Error in .local(x, ...) : Calloc could not allocate (48 of 8) memory
> 
> 
> Can anyone tell me why I get this error massage and  what I can do to
> avoid this problem?
> 
> Thank you very much for your help!
> 
> Thomas
> 
> 
> 
> I am working with:
> R 2.1.0
> boot 1.2-22
> lme4 0.95-6
> Matrix 0.95-7
> lattice 0.11-6
> latticeExtra 0.1-1
> 

Could you run

traceback()

after a failure and see where the failure is occurring?

This may not have an effect but you could avoid some of the assignments
in your off.fun function.  It should work to use

off.fun <- function(data, i) VarCorr(lmer(y ~ trt + (trt - 1|group),
data, subset = i))@reSumry$group[2,1]

If that call to lmer doesn't work let me know please.



From chrysopa at gmail.com  Mon May  9 15:34:21 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Mon, 9 May 2005 10:34:21 -0300
Subject: [R] Error in F test on version 2.1.0
Message-ID: <200505091034.21778.chrysopa@gmail.com>

Hi,

I make a upgrade to R 2.1.0 and in some analysis I give an error:

anova(model,test="F")
Analysis of Deviance Table

Model: binomial, link: logit

Response: landing/total

Terms added sequentially (first to last)


    Df Deviance Resid. Df Resid. Dev    F Pr(>F)
NULL                    16    105.079            
trat  1  93.149        15    11.930 93.15      
Warning message:
NaNs produced in: pf(q, df1, df2, lower.tail, log.p) 

In old version it work.

Inte
Ronaldo
-- 
You look tired.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From ligges at statistik.uni-dortmund.de  Mon May  9 15:53:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 May 2005 15:53:11 +0200
Subject: [R] Error in F test on version 2.1.0
In-Reply-To: <200505091034.21778.chrysopa@gmail.com>
References: <200505091034.21778.chrysopa@gmail.com>
Message-ID: <427F6B47.1060202@statistik.uni-dortmund.de>

Ronaldo Reis-Jr. wrote:

> Hi,
> 
> I make a upgrade to R 2.1.0 and in some analysis I give an error:
> 
> anova(model,test="F")
> Analysis of Deviance Table
> 
> Model: binomial, link: logit
> 
> Response: landing/total
> 
> Terms added sequentially (first to last)
> 
> 
>     Df Deviance Resid. Df Resid. Dev    F Pr(>F)
> NULL                    16    105.079            
> trat  1  93.149        15    11.930 93.15      
> Warning message:
> NaNs produced in: pf(q, df1, df2, lower.tail, log.p) 
> 
> In old version it work.
> 
> Inte
> Ronaldo

Have you read the posting guide?
It asks you to specify a reproducible example ........
Please so so (both, read the psoting guide and specify the example).

Uwe Ligges



From Achim.Zeileis at wu-wien.ac.at  Mon May  9 15:47:34 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 9 May 2005 15:47:34 +0200
Subject: [R] Error in F test on version 2.1.0
In-Reply-To: <200505091034.21778.chrysopa@gmail.com>
References: <200505091034.21778.chrysopa@gmail.com>
Message-ID: <20050509154734.659d82df.Achim.Zeileis@wu-wien.ac.at>

On Mon, 9 May 2005 10:34:21 -0300 Ronaldo Reis-Jr. wrote:

> Hi,
> 
> I make a upgrade to R 2.1.0 and in some analysis I give an error:
> 
> anova(model,test="F")
> Analysis of Deviance Table
> 
> Model: binomial, link: logit
> 
> Response: landing/total
> 
> Terms added sequentially (first to last)
> 
> 
>     Df Deviance Resid. Df Resid. Dev    F Pr(>F)
> NULL                    16    105.079            
> trat  1  93.149        15    11.930 93.15      
> Warning message:
> NaNs produced in: pf(q, df1, df2, lower.tail, log.p) 
> 
> In old version it work.

I also stumbled over this, the other day. The source of the problem was
already discussed in this thread:
  https://stat.ethz.ch/pipermail/r-devel/2005-April/032923.html
Z

> Inte
> Ronaldo
> -- 
> You look tired.
> --
> |>   // | \\   [***********************************]
> |   ( ??   ?? )  [Ronaldo Reis J??nior                ]
> |>      V      [UFV/DBA-Entomologia                ]
> |    /     \   [36570-000 Vi??osa - MG              ]
> |>  /(.''`.)\  [Fone: 31-3899-4007                 ]
> |  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |    ( `-  )   [***********************************]
> |>>  _/   \_Powered by GNU/Debian Woody/Sarge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Achim.Zeileis at wu-wien.ac.at  Mon May  9 15:49:43 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 9 May 2005 15:49:43 +0200
Subject: [R] Error in F test on version 2.1.0
In-Reply-To: <427F6B47.1060202@statistik.uni-dortmund.de>
References: <200505091034.21778.chrysopa@gmail.com>
	<427F6B47.1060202@statistik.uni-dortmund.de>
Message-ID: <20050509154943.2bc0c1d5.Achim.Zeileis@wu-wien.ac.at>

On Mon, 09 May 2005 15:53:11 +0200 Uwe Ligges wrote:

> Ronaldo Reis-Jr. wrote:
> 
> > Hi,
> > 
> > I make a upgrade to R 2.1.0 and in some analysis I give an error:
> > 
> > anova(model,test="F")
> > Analysis of Deviance Table
> > 
> > Model: binomial, link: logit
> > 
> > Response: landing/total
> > 
> > Terms added sequentially (first to last)
> > 
> > 
> >     Df Deviance Resid. Df Resid. Dev    F Pr(>F)
> > NULL                    16    105.079            
> > trat  1  93.149        15    11.930 93.15      
> > Warning message:
> > NaNs produced in: pf(q, df1, df2, lower.tail, log.p) 
> > 
> > In old version it work.
> > 
> > Inte
> > Ronaldo
> 
> Have you read the posting guide?
> It asks you to specify a reproducible example ........
> Please so so (both, read the psoting guide and specify the example).

Valid point, try:

set.seed(1071)
y <- factor(rnorm(10) > 0)
x <- rnorm(10)
fm1 <- glm(y ~ 1, family = binomial)
fmx <- glm(y ~ x, family = binomial)
anova(fm1, fmx, test = "F")

and as I said in another reply already. This boils down to the problems
in pf() reported by Gordon Smyth:
  https://stat.ethz.ch/pipermail/r-devel/2005-April/032923.html
Z

> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From chrysopa at gmail.com  Mon May  9 16:21:00 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Mon, 9 May 2005 11:21:00 -0300
Subject: [R] Error in F test on version 2.1.0
In-Reply-To: <427F6B47.1060202@statistik.uni-dortmund.de>
References: <200505091034.21778.chrysopa@gmail.com>
	<427F6B47.1060202@statistik.uni-dortmund.de>
Message-ID: <200505091121.01152.chrysopa@gmail.com>

Em Seg 09 Mai 2005 10:53, Uwe Ligges escreveu:
> Have you read the posting guide?
> It asks you to specify a reproducible example ........
> Please so so (both, read the psoting guide and specify the example).
>
> Uwe Ligges

> tot <- c(16,20,23,18,10,18,25,20,24,24,29,12,23,23,21,23,11)
> trat <- as.factor(rep(c("A","B"),c(8,9)))
> dep <- c(0,1,1,0,1,0,1,1,15,13,12,4,12,10,9,10,5)
> tot <- c(16,20,23,18,10,18,25,20,24,24,29,12,23,23,21,23,11)
> m <- glm(dep/tot~trat,family=binomial,weights=tot)
> anova(m,test="F")
Analysis of Deviance Table

Model: binomial, link: logit

Response: dep/tot

Terms added sequentially (first to last)


     Df Deviance Resid. Df Resid. Dev      F Pr(>F)
NULL                    16    105.685              
trat  1   96.117        15      9.568 96.117       
Mensagem de aviso:
NaNs produzidos in: pf(q, df1, df2, lower.tail, log.p) 

INte
Ronaldo

-- 
It may or may not be worthwhile, but it still has to be done.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From Brian.Beckage at uvm.edu  Mon May  9 16:25:53 2005
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Mon, 9 May 2005 10:25:53 -0400
Subject: [R] Incorrect libxml2.2.dylib version on Tiger install
In-Reply-To: <ea6e58a41a2559b14aa291c352949c5f@nd.edu>
References: <ba8949492f55d7de3543e605bd79b719@nd.edu>
	<ea6e58a41a2559b14aa291c352949c5f@nd.edu>
Message-ID: <p06210204bea5213c4477@[10.0.1.2]>

I encountered a similar problem after installing the new version of R 
(2.1.0) on my Mac OSX 10.3.8--I have not updated to Tiger.  The error 
message I get is below.  As Steven reported, I do not get this error 
when running R from within the Aqua GUI, but only at the command line 
(which is unfortunately how I use R).  Is the solution to install the 
newer version of Xcode (2.0) as suggested in earlier emails?

Thanks,
Brian


Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library 
'/Library/Frameworks/R.framework/Resources/library/grDevices/libs/grDevices.so':
   dlcompat: dyld: 
/Library/Frameworks/R.framework/Resources/bin/exec/R version mismatch 
for library: /usr/local/lib/libxml2.2.dylib (compatibility version of 
user: 9.0.0 greater than library's version: 8.0.0)
Loading required package: grDevices
Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library 
'/Library/Frameworks/R.framework/Resources/library/grDevices/libs/grDevices.so':
   dlcompat: dyld: 
/Library/Frameworks/R.framework/Resources/bin/exec/R version mismatch 
for library: /usr/local/lib/libxml2.2.dylib (compatibility version of 
user: 9.0.0 greater than library's version: 8.0.0)
In addition: Warning message:
package grDevices in options("defaultPackages") was not found
Error: package 'grDevices' could not be loaded



>Sorry to reply to my own post, but I have more info.  This probably 
>needs to come to the attention of the Mac R developers.
>
>Xcode 2.0 does not have a more recent libxml2.2.dylib.
>
>The problem only manifests itself when running R 2.1.0 from the 
>command line, not within the Aqua GUI.
>
>Thanks for your patience.
>
>On May 7, 2005, at 9:30 AM, Steven Boker wrote:
>
>>Hi all,
>>
>>I have just installed OSX Server 10.4 and R comes up with the 
>>incompatible libxml library message reported by Dan Kelley a few 
>>messages ago.  Xcode 2 does not ship with Tiger Server.  I 
>>installed the X-Windows code.  I can report that the version of 
>>libxml2.2 that is installed in this case is the version 8.0.0 dylib.
>>
>>[6]sboker at munimula:/usr/lib % ls -l libxml2.2*
>>-rwxr-xr-x   1 root  wheel  1061704 Apr 15 23:44 libxml2.2.6.16.dylib*
>>-rwxr-xr-x   1 root  wheel  1061804 May  6 14:15 libxml2.2.dylib*
>>
>>I'm working on getting a copy of the Xcode 2 CD.  I was surprised 
>>it wasn't in the 10.4 server CD set.  I have a production problem 
>>today, though, if anyone can help me with a copy of this library.
>>
>>Best,
>>Steve Boker
>>
>>On May 6, 2005, at 12:39 PM, Dan Kelley wrote:
>>>
>>>  Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>>  unable to load shared library '/Library/Frameworks/
>>>  R.framework/Resources/library/grDevices/libs/grDevices.so':
>>>  dlopen(/Library/Frameworks/R.framework/Resources/library/grDevices/
>>>  libs/grDevices.so, 6): Library not loaded: /usr/lib/libxml2.2.dylib
>>>  Referenced from: /System/Library/Frameworks/AppKit.framework/
>>>  Versions/C/AppKit
>>>  Reason: Incompatible library version: AppKit requires version
>>>  9.0.0 or later, but libxml2.2.dylib provides version 8.0.0
>>
>>---------------------------------------------------------------------
>>  Steven M. Boker                    574-339-0735 (cell/page/message)
>>  sboker at nd.edu                      574-631-4941 (office)
>>  http://www.nd.edu/~sboker/         574-631-8883 (fax)
>>  Dept. of Psychology, University of Notre Dame, Notre Dame, IN 46556
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>---------------------------------------------------------------------
>  Steven M. Boker                    574-339-0735 (cell/page/message)
>  sboker at nd.edu                      574-631-4941 (office)
>  http://www.nd.edu/~sboker/         574-631-8883 (fax)
>  Dept. of Psychology, University of Notre Dame, Notre Dame, IN 46556
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405

Phone:  802 656-0197
Fax  :  802 656-0440
email:  Brian.Beckage at uvm.edu
web  :  www.uvm.edu/~bbeckage



From Brian.Beckage at uvm.edu  Mon May  9 16:51:54 2005
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Mon, 9 May 2005 10:51:54 -0400
Subject: [R] Incorrect libxml2.2.dylib version on Tiger install
In-Reply-To: <p06210204bea5213c4477@[10.0.1.2]>
References: <ba8949492f55d7de3543e605bd79b719@nd.edu>
	<ea6e58a41a2559b14aa291c352949c5f@nd.edu>
	<p06210204bea5213c4477@[10.0.1.2]>
Message-ID: <p06210205bea528f012b2@[10.0.1.2]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050509/47ff4b01/attachment.pl

From HDoran at air.org  Mon May  9 17:37:37 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 9 May 2005 11:37:37 -0400
Subject: [R] Sweep statistics
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7408DAB24E@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050509/83d05650/attachment.pl

From fsaldan1 at gmail.com  Mon May  9 17:50:06 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Mon, 9 May 2005 11:50:06 -0400
Subject: [R] Cannot get help after reinstalling
Message-ID: <10dee469050509085047ebadfd@mail.gmail.com>

I installed the latest version of R after unistalling the previous
version. Now I am having trouble getting help. For example,

> ?mean
Error in int.unzip(file.path(path, zipname), topic, tmpd) : 
        'destination' does not exist

What did I do wrong?

(OS is Windows XP Pro)

Thanks.

FS



From ligges at statistik.uni-dortmund.de  Mon May  9 18:01:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 May 2005 18:01:45 +0200
Subject: [R] Cannot get help after reinstalling
In-Reply-To: <10dee469050509085047ebadfd@mail.gmail.com>
References: <10dee469050509085047ebadfd@mail.gmail.com>
Message-ID: <427F8969.7080803@statistik.uni-dortmund.de>

Fernando Saldanha wrote:

> I installed the latest version of R after unistalling the previous
> version. Now I am having trouble getting help. For example,
> 
> 
>>?mean
> 
> Error in int.unzip(file.path(path, zipname), topic, tmpd) : 
>         'destination' does not exist
> 
> What did I do wrong?


Don't know, please try to reinstall again and check whether you 
downloaded the full tarball or rw2010.exe (I guess you are not compiling 
yourself?).
Please also check that you have write permissions on the destination, 
you are not trying to overwrite existing but locked files, and you have 
sufficient disc space left.

Uwe Ligges


> (OS is Windows XP Pro)
> 
> Thanks.
> 
> FS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon May  9 18:18:16 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 May 2005 17:18:16 +0100 (BST)
Subject: [R] Error in F test on version 2.1.0
In-Reply-To: <20050509154943.2bc0c1d5.Achim.Zeileis@wu-wien.ac.at>
References: <200505091034.21778.chrysopa@gmail.com>
	<427F6B47.1060202@statistik.uni-dortmund.de>
	<20050509154943.2bc0c1d5.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <Pine.LNX.4.61.0505091715380.9679@gannet.stats>

On Mon, 9 May 2005, Achim Zeileis wrote:

> On Mon, 09 May 2005 15:53:11 +0200 Uwe Ligges wrote:
>
>> Ronaldo Reis-Jr. wrote:
>>
>>> Hi,
>>>
>>> I make a upgrade to R 2.1.0 and in some analysis I give an error:
>>>
>>> anova(model,test="F")
>>> Analysis of Deviance Table
>>>
>>> Model: binomial, link: logit
>>>
>>> Response: landing/total
>>>
>>> Terms added sequentially (first to last)
>>>
>>>
>>>     Df Deviance Resid. Df Resid. Dev    F Pr(>F)
>>> NULL                    16    105.079
>>> trat  1  93.149        15    11.930 93.15
>>> Warning message:
>>> NaNs produced in: pf(q, df1, df2, lower.tail, log.p)
>>>
>>> In old version it work.
>>>
>>> Inte
>>> Ronaldo
>>
>> Have you read the posting guide?
>> It asks you to specify a reproducible example ........
>> Please so so (both, read the psoting guide and specify the example).
>
> Valid point, try:
>
> set.seed(1071)
> y <- factor(rnorm(10) > 0)
> x <- rnorm(10)
> fm1 <- glm(y ~ 1, family = binomial)
> fmx <- glm(y ~ x, family = binomial)
> anova(fm1, fmx, test = "F")
>
> and as I said in another reply already. This boils down to the problems
> in pf() reported by Gordon Smyth:
>  https://stat.ethz.ch/pipermail/r-devel/2005-April/032923.html

Hmm,

1) It is not documented that df=Inf is valid for pf(), so the problem is 
rather using an "F" test when it is inappropriate.  If you specify an F 
test, you almost certainly meant a quasibinomial model.

2) This does work in the current R-patched.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon May  9 18:20:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 May 2005 17:20:26 +0100 (BST)
Subject: [R] Error in F test on version 2.1.0
In-Reply-To: <200505091121.01152.chrysopa@gmail.com>
References: <200505091034.21778.chrysopa@gmail.com>
	<427F6B47.1060202@statistik.uni-dortmund.de>
	<200505091121.01152.chrysopa@gmail.com>
Message-ID: <Pine.LNX.4.61.0505091718280.9679@gannet.stats>

Why are you using an F test here?  The denominator is known, not 
estimated ....

On Mon, 9 May 2005, Ronaldo Reis-Jr. wrote:

> Em Seg 09 Mai 2005 10:53, Uwe Ligges escreveu:
>> Have you read the posting guide?
>> It asks you to specify a reproducible example ........
>> Please so so (both, read the psoting guide and specify the example).
>>
>> Uwe Ligges
>
>> tot <- c(16,20,23,18,10,18,25,20,24,24,29,12,23,23,21,23,11)
>> trat <- as.factor(rep(c("A","B"),c(8,9)))
>> dep <- c(0,1,1,0,1,0,1,1,15,13,12,4,12,10,9,10,5)
>> tot <- c(16,20,23,18,10,18,25,20,24,24,29,12,23,23,21,23,11)
>> m <- glm(dep/tot~trat,family=binomial,weights=tot)
>> anova(m,test="F")
> Analysis of Deviance Table
>
> Model: binomial, link: logit
>
> Response: dep/tot
>
> Terms added sequentially (first to last)
>
>
>     Df Deviance Resid. Df Resid. Dev      F Pr(>F)
> NULL                    16    105.685
> trat  1   96.117        15      9.568 96.117
> Mensagem de aviso:
> NaNs produzidos in: pf(q, df1, df2, lower.tail, log.p)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pmathews at apk.net  Mon May  9 18:29:52 2005
From: pmathews at apk.net (Paul Mathews)
Date: Mon, 09 May 2005 12:29:52 -0400
Subject: [R] R code to reproduce solutions to example problems in Mathews,
 Design of Experiments with MINITAB
Message-ID: <427F9000.4070909@apk.net>

The R code to reproduce the solutions to the example problems in Design 
of Experiments with MINITAB by Paul Mathews (ASQ Quality Press, 2005) is 
now posted at http://www.mmbstatistical.com/DOEwithMINITAB.html.

Enjoy!

Paul Mathews

-- 
Paul Mathews
Mathews Malnar and Bailey, Inc.
217 Third Street, Fairport Harbor, OH 44077
Phone: 440-350-0911
Fax: 440-350-7210
E-mail:
        Paul: pmathews at apk.net, paul at mmbstatistical.com
        Rebecca: rmalnar at apk.net, bek at mmbstatistical.com
Web: www.mmbstatistical.com



From daniel at nr.no  Mon May  9 18:31:50 2005
From: daniel at nr.no (Daniel Berg)
Date: Mon, 9 May 2005 18:31:50 +0200
Subject: [R] replacing a for-loop with lapply
Message-ID: <002801c554b4$9c3b36e0$f902749c@HP14154250313>

Dear All,

I am trying to compute a goodness-of-fit statistic for a copula, based on an
empirical density estimate of this copula. 
To do this I can use the following code:


> n <- dim(data)[1]
> d <- dim(data)[2]
> Chat <- rep(0,n)
> for(i in 1:n)
+ Chat[i] <- sum(apply(t(data)<=data[i,],2,prod))/(n+1)


However, I have a feeling this can be done more effectively than using a
for-loop. I have also tried the following:


> tmp1 <- lapply(1:n,function(i) t(data)<=data[i,])
> tmp2 <- lapply(1:n,function(i) apply(tmp1[[i]],2,prod))
> Chat <- as.numeric(lapply(1:n, function(i) sum(tmp2[[i]])))


but there is no improvement. I ran the following timing test:


> data <- matrix(runif(300),100,3)
> n = dim(data)[1]
> d = dim(data)[2]
> Chat = vector("numeric",n)
> M <- 30
> a <- rep(0,M)
> for(m in 1:M){
+ a[m] <- system.time({
+ tmp1 <- lapply(1:n,function(i) t(data)<=data[i,])
+ tmp2 <- lapply(1:n,function(i) apply(tmp1[[i]],2,prod))
+ Chat <- as.numeric(lapply(1:n, function(i) sum(tmp2[[i]])))})[3]}
> b <- rep(0,M)
> for(m in 1:30){
+ b[m] <- system.time(	
+ for (i in 1:n)
+ Chat[i] = sum(apply(t(data)<=data[i,],2,prod))/(n+1))[3]}
> summary(a)
> summary(b)


and the output was:


> summary(a)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.8500  0.8700  0.8900  0.9013  0.9300  0.9800 
> summary(b)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.8400  0.8600  0.8800  0.8883  0.9075  0.9900


Is there any way I can code this more efficiently in R or will I have to
turn to C? The data sets, on which I am actually going to run this code,
will be of sizes up to (5000x100) and I need hundreds of realizations...

Thank you for your time.

Rgds,
Daniel



From ipardoe at lcbmail.uoregon.edu  Mon May  9 18:43:36 2005
From: ipardoe at lcbmail.uoregon.edu (Iain Pardoe)
Date: Mon, 9 May 2005 09:43:36 -0700
Subject: [R] Sampling from multivariate multiple regression prediction
	regions
Message-ID: <A11B20BC51EEFA41AE1516AA38CF894101775EA8@mail.lcb.uoregon.edu>

I'd like to sample multiple response values from a multivariate
regression fit.  For example, suppose I have m=2 responses (y1,y2) and a
single set of predictor variables (z1,z2).  Each response is assumed to
follow its own regression model, and the error terms in each model can
be correlated (as in example 7.10 of section 7.7 of Johnson/Wichern):

> ex7.10 <-
+   data.frame(y1 = c(141.5, 168.9, 154.8, 146.5, 172.8, 160.1, 108.5),
+              y2 = c(301.8, 396.1, 328.2, 307.4, 362.4, 369.5, 229.1),
+              z1 = c(123.5, 146.1, 133.9, 128.5, 151.5, 136.2, 92),
+              z2 = c(2.108, 9.213, 1.905, .815, 1.061, 8.603, 1.125))
> attach(ex7.10)
> f.mlm <- lm(cbind(y1,y2)~z1+z2)
> y.hat <- c(1, 130, 7.5) %*% coef(f.mlm)
> round(y.hat, 2)
         y1     y2
[1,] 151.84 349.63
> qf.z <- t(c(1, 130, 7.5)) %*%
+   solve(t(cbind(1,z1,z2)) %*% cbind(1,z1,z2)) %*%
+   c(1, 130, 7.5)
> round(qf.z, 5)
        [,1]
[1,] 0.36995
> n.sigma.hat <- SSD(f.mlm)$SSD # same as t(resid(f.mlm)) %*%
resid(f.mlm)
> round(n.sigma.hat, 2)
     y1    y2
y1 5.80  5.22
y2 5.22 12.57
> F.quant <- qf(.95,2,3)
> round(F.quant, 2)
[1] 9.55

This gives me all the information I need to calculate a 95% confidence
ellipse for y=(y1,y2) at (z1,z2)=(130,7.5) using JW's equation (7-48)
(written using R syntax, although R cannot "literally" calculate this as
it is written):

(y-y.hat) %*% ((n-r-1) * solve(n.sigma.hat)) %*% t(y-y.hat)
<= (1+qf.z) * (m*(n-r-1)/(n-r-m)) * F.quant

But, what if instead I'd like to sample (y1,y2) values from this
distribution?  I can sample from an F(m,n-r-m) distribution easily
enough, but then how can I transform this to a single point in (y1,y2)
space?

Any ideas would be gratefully appreciated.  Thanks.

Iain Pardoe <ipardoe at lcbmail.uoregon.edu>
University of Oregon



From apjaworski at mmm.com  Mon May  9 19:06:10 2005
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Mon, 9 May 2005 12:06:10 -0500
Subject: [R] slowness of plot(x, type='l')
In-Reply-To: <Pine.LNX.4.61.0505070551370.3113@gannet.stats>
Message-ID: <OF1A51B406.8F9BE2AA-ON86256FFC.00517B4E-86256FFC.005DF2DC@mmm.com>






Thanks for the quick response and apologies for my sloppy post (nor
mentioning the device) and sloppy example (the use of date()).

Indeed, I was using the windows device.  There is no timing problem with
the postscript device.

I just installed the R-devel May-9 and the problem went away.  Here is an
example:

> n <- 15000
> system.time(plot(rnorm(n), type='l'))
[1] 0.07 0.41 0.48   NA   NA

An interesting footnote is the fact that the slowdown of the windows device
did not happen for plot(*, type='c').  The 'c' option plots line segments
without points, so it probably used the "bunches of lines" anyway.

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             Prof Brian Ripley                                             
             <ripley at stats.ox.                                             
             ac.uk>                                                     To 
             Sent by:                  apjaworski at mmm.com                  
             r-help-bounces at st                                          cc 
             at.math.ethz.ch           r-help at stat.math.ethz.ch            
                                                                   Subject 
                                       Re: [R] slowness of plot(x,         
             05/07/2005 12:36          type='l')                           
             AM                                                            
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Plotting times depend on the graphics device.  That is nowhere mentioned
here, which is unhelpful, and we have already seen a post saying it does
not happen on another unmentioned device (presumably X11).

Let us assume the unmentioned device was windows(), as that is the only
one I see any slowdown for.  (Others like win.metafile are windows() under
the skin.)

On Fri, 6 May 2005 apjaworski at mmm.com wrote:

> A couple of days ago a few messages indicated that something changed in
the
> basic plot routine that made plot(*, type='l') slow for large data sets.
> Some people even reported crashes for very large data sets.  As far as I
> remember, this was not reported as a formal bug.

Well, _is_ there a bug in R (as distinct from in Windows graphics
internals)?  I am almost certain there is not in R and this is a bug in
Windows.

> I am still not sure if this is a bug, so I report my findings here.
First
> of all, I think I see a slowdown of the plot function, although I do not
> have older versions of R installed, so I cannot do side-by-side
> comparisons.  Secondly, I noticed that the behavior of plot(*, type='l')
> differs.  Before R-2.1, the plotted lines would appear on the plot
> gradually.  Now, after the wait, the whole plot appears at once.
>
> Here are my timing results.  I am on Windows2000, IBM Intellistation with
> Xenon 2.8MHz with 1Gb of memory.  I checked May-06 versions of R-patched
> and R-devel built from sources.  I ran the following simple test:
>
> x <- rnorm(n)
> date(); plot(x, type='l'); date()

Oh, PLEASE, use system.time() to time things.  Had you done so you might
have seen things like

> windows()
> n <- 10000
> system.time(plot(rnorm(n), type="l"))
[1]  0.03 13.11 13.21    NA    NA
> postscript()
> system.time(plot(rnorm(n), type="l"))
[1] 0.07 0.00 0.08   NA   NA
> dev.off()
> system.time(plot(rnorm(n), type="p"))
[1] 0.07 0.93 1.00   NA   NA

so the time is not being taken by R but by Windows.

I can tell you the reason: it is the support for mitred etc line ends
introduced in R 2.0.0 and only supported in windows() from 2.1.0.  This
has slowed solid lines down to the sort of times taken for dashed lines
previously.

Now, the best we can do to work around this is to follow what we did for
dashed lines, and not attempt to be accurate for very large numbers of
line segments.  By plotting in bunches of 1000 lines I get

> system.time(plot(rnorm(n), type="l"))
[1] 0.03 0.36 0.42   NA   NA
> system.time(plot(rnorm(n), type="l", lty=3))
[1] 0.22 2.89 3.11   NA   NA

We have been here before, and as I recall this slowdown happens only in
NT-based versions of Windows which seem _de facto_ restricted to about
1000 line elements in a path: what we were not aware of was that it
happened for solid lines as well as dashed ones.

I've put the bunching into R-patched.

It is very regretable that this sort of thing was not tested for during
beta-testing.

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Mon May  9 19:22:31 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 9 May 2005 13:22:31 -0400
Subject: [R] replacing a for-loop with lapply
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9424@uswpmx00.merck.com>

I suggest

1. Transpose "data" once at the beginning. 
2. Replace "apply" with "colSums" to find cols with sum = d. Since you have
logical values, the sum count the number of TRUES and you want them all
TRUE, it looks to me.

With further work you could vectorize this, but loops in R are actually
pretty good once you can streamline the code inside. 

I get

> system.time(for(i in 1:n) Chat[i] <-
sum(apply(t(data)<=data[i,],2,prod))/(n+1))
[1] 0.62 0.01 0.73   NA   NA

while with

> tdata <- t(data)

I get much improved

> system.time(for(i in 1:n) Chat[i] <- sum(colSums(tdata <= tdata[,i]) ==
d)/(n+1))
[1] 0.04 0.00 0.04   NA   NA

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Daniel Berg
Sent: Monday, May 09, 2005 12:32 PM
To: r-help at stat.math.ethz.ch
Subject: [R] replacing a for-loop with lapply


Dear All,

I am trying to compute a goodness-of-fit statistic for a copula, based on an
empirical density estimate of this copula. 
To do this I can use the following code:


> n <- dim(data)[1]
> d <- dim(data)[2]
> Chat <- rep(0,n)
> for(i in 1:n)
+ Chat[i] <- sum(apply(t(data)<=data[i,],2,prod))/(n+1)


However, I have a feeling this can be done more effectively than using a
for-loop. I have also tried the following:


> tmp1 <- lapply(1:n,function(i) t(data)<=data[i,])
> tmp2 <- lapply(1:n,function(i) apply(tmp1[[i]],2,prod))
> Chat <- as.numeric(lapply(1:n, function(i) sum(tmp2[[i]])))


but there is no improvement. I ran the following timing test:


> data <- matrix(runif(300),100,3)
> n = dim(data)[1]
> d = dim(data)[2]
> Chat = vector("numeric",n)
> M <- 30
> a <- rep(0,M)
> for(m in 1:M){
+ a[m] <- system.time({
+ tmp1 <- lapply(1:n,function(i) t(data)<=data[i,])
+ tmp2 <- lapply(1:n,function(i) apply(tmp1[[i]],2,prod))
+ Chat <- as.numeric(lapply(1:n, function(i) sum(tmp2[[i]])))})[3]}
> b <- rep(0,M)
> for(m in 1:30){
+ b[m] <- system.time(	
+ for (i in 1:n)
+ Chat[i] = sum(apply(t(data)<=data[i,],2,prod))/(n+1))[3]}
> summary(a)
> summary(b)


and the output was:


> summary(a)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.8500  0.8700  0.8900  0.9013  0.9300  0.9800 
> summary(b)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.8400  0.8600  0.8800  0.8883  0.9075  0.9900


Is there any way I can code this more efficiently in R or will I have to
turn to C? The data sets, on which I am actually going to run this code,
will be of sizes up to (5000x100) and I need hundreds of realizations...

Thank you for your time.

Rgds,
Daniel

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From richard_raubertas at merck.com  Mon May  9 19:51:03 2005
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Mon, 9 May 2005 13:51:03 -0400
Subject: [R] Monotonic regression
Message-ID: <B88F4BCF37DD0847937C1C98255291FB07DCEE8A@uswsmx05.merck.com>

The 'pava' function below looks like code that I wrote
(with all the comments removed).  I have posted it two or
three times over the years to the S/R lists.

As Martin Maechler has noted, the function 'isoreg' will also
do monotonic regression (much faster for large data sets).
However, it does not allow weights (at least as of R 2.0.1).

I don't understand the original poster's comment about
"local minimum".  Isotonic regression is a convex optimization
problem and 'pava' or 'isoreg' will always produce the 
unique solution.

Rich Raubertas
Merck & Co.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Ted.Harding at nessie.mcc.ac.uk
> Sent: Sunday, May 08, 2005 5:14 PM
> To: Scott Briggs
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] Monotonic regression
> 
> 
> On 08-May-05 Scott Briggs wrote:
> > Hi, I'm trying to find an implementation of monotonic 
> regression in R
> > and I haven't been able to find anything that's really related to
> > this.  isoMDS in the MASS package uses monotonic 
> regression, however,
> > I was wondering if there is any standalone function for monotonic
> > regression?
> > 
> > Basically what I'm trying to do is implement monotonic regression
> > where I can see not just the results of each iteration but also be
> > able to tweak the input in order to test for or "kick" the 
> regression
> > out of a local minimum so that I can make sure I have the global
> > minimum.
> > 
> > Any help would be much appreciated.  Thanks!
> > 
> > Scott
> 
> You may probably find PAVA ("Pool Adjacent Violators Algorithm")
> useful. Below is code for a simple version which I have been using
> for a few years. I forget where I found it!
> 
> An R site search comes up with code for a version with more
> complex functionality at
> 
>   http://finzi.psych.upenn.edu/R/Rhelp02a/archive/9807.html
> 
> (contributed to r-help by Jan de Leeuw on 01 Jul 2004). I have
> not tested this code.
> 
> 
> pava<-function(x,wt=rep(1,length(x)))
> {
>   n<-length(x)
>   if(n<=1) return(x)
>   if(any(is.na(x)) || any(is.na(wt))) {
>     stop("Missing values in 'x' or 'wt' not allowed")
>   }
>   lvlsets<-(1:n)
>   repeat {
>     viol<-(as.vector(diff(x))<0)
>     if(!(any(viol))) break
>     i<-min( (1:(n-1))[viol])
>     lvl1<-lvlsets[i]
>     lvl2<-lvlsets[i+1]
>     ilvl<-(lvlsets==lvl1 | lvlsets==lvl2)
>     x[ilvl]<-sum(x[ilvl]*wt[ilvl])/sum(wt[ilvl])
>     lvlsets[ilvl]<-lvl1
>   }
>   x
> }
> # Examples:
> # > x<-c(1,0,1,0,0,1,0,1,1,0,1,0)
> # > x
> # [1] 1 0 1 0 0 1 0 1 1 0 1 0
> # > pava(x)
> #  [1] 0.4 0.4 0.4 0.4 0.4 0.5 0.5 0.6 0.6 0.6 0.6 0.6
> # > 
> # > pava(c(0,0,2/4,1/5,2/4,1/2,4/5,5/8,7/11,10/11),
> #        c(5,4,4,5,4,2,5,8,11,11))
> # [1] 0.0000000 0.0000000 0.3333333 0.3333333 0.5000000
> # [6] 0.5000000 0.6666667 0.6666667 0.6666667 0.9090909
> # (example where data are {ri,ni} so x={ri/ni} and w={ni})
> 
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 08-May-05                                       Time: 20:56:28
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From mark.r.berry at pfizer.com  Mon May  9 20:33:19 2005
From: mark.r.berry at pfizer.com (Berry, Mark R)
Date: Mon, 9 May 2005 14:33:19 -0400 
Subject: [R] formula restriction in multinom?
Message-ID: <4A77CC68EA0C424896CF9A5CB2C0F9AC010577DB@groamrexm02.amer.pfizer.com>

Good Day: When I used: 
multinom(formula = Y ~ X1 + X2 + X3 + X1:X2 + X1:X3 + X3:X2 + X1^2 + X2^2 +
X3^2, data = DATASET),
I get estimates and AIC for the model containing main effects and
interactions only (no squared terms)...and FYI, all predictors are
continuous. Is this "normal" behavior? If I run this in S-Plus I get
estimates and AIC for the model containing all terms(including the squared
terms).

   Thanks,
   Mark Berry




LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From uofiowa at gmail.com  Mon May  9 20:38:30 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Mon, 9 May 2005 14:38:30 -0400
Subject: [R] Data source name not found, and no default driver specified
Message-ID: <3f87cc6d05050911381e5f8722@mail.gmail.com>

Using R 2.1.0, RODBC works very well for me. When I move my
application to run for the www-data user under apache I get the
message:

[unixODBC][Driver Manager]Data source name not found, and no default
driver specified

I made sure that the .odbc.ini file is in www-data home's directory
and it is readable. I also defined its location using the environment
variable ODBCINI and verified its value from within R using
Sys.getenv().

I connect using odbcConnect("mydsn") where "mydsn" is defined in the
.ini file. I also tried
odbcDriverConnect("DRIVER=/opt/informix/lib/cli/libifcli.so;UID=myid;PWD=mypwd;DATABASE=dbname;HOST=myhost;SERVER=myserver;SERVICE=service_port;PROTOCOL=soctcp;")
but kept getting teh same error. 

What are the usual suspects in this case? how can I track the source
of this problem?



From Ted.Harding at nessie.mcc.ac.uk  Mon May  9 20:34:33 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 09 May 2005 19:34:33 +0100 (BST)
Subject: [R] Monotonic regression
In-Reply-To: <B88F4BCF37DD0847937C1C98255291FB07DCEE8A@uswsmx05.merck.com>
Message-ID: <XFMail.050509193433.Ted.Harding@nessie.mcc.ac.uk>

On 09-May-05 Raubertas, Richard wrote:
> The 'pava' function below looks like code that I wrote
> (with all the comments removed).  I have posted it two or
> three times over the years to the S/R lists.

If you recognise the code, then it is likely that it is what
I came across abut 3 years ago when I was browsing arond
for methods foer isotonic regression. At the time I was still
mainly using 'octave' for much statistics, and I "ported"
it from R to octave (a very simple transcription in this
case).

So thanks for contributing it: I have found it useful many
times, and happily acknowledge your authorship!

> As Martin Maechler has noted, the function 'isoreg' will also
> do monotonic regression (much faster for large data sets).
> However, it does not allow weights (at least as of R 2.0.1).

Nor (looking at the code for 'isoreg' today, now that the
issue has come up) could I see any easy way to incorporate
weights into that code.

> I don't understand the original poster's comment about
> "local minimum".  Isotonic regression is a convex optimization
> problem and 'pava' or 'isoreg' will always produce the 
> unique solution.

Agreed. Nor did I!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 09-May-05                                       Time: 19:32:46
------------------------------ XFMail ------------------------------



From gunter.berton at gene.com  Mon May  9 21:09:25 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 9 May 2005 12:09:25 -0700
Subject: [R] formula restriction in multinom?
In-Reply-To: <4A77CC68EA0C424896CF9A5CB2C0F9AC010577DB@groamrexm02.amer.pfizer.com>
Message-ID: <200505091909.j49J9P1s011737@hertz.gene.com>

Note: multinom() is in nnet package.

In any case, the answer is, yes, this is "normal" behavior:

x^2 <==> x:x <==> x in S language linear model notation. So if you want a
quadratic you must use I(x^2). S-Plus does this silently, but it is not
required by the language definition.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berry, Mark R
> Sent: Monday, May 09, 2005 11:33 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] formula restriction in multinom?
> 
> Good Day: When I used: 
> multinom(formula = Y ~ X1 + X2 + X3 + X1:X2 + X1:X3 + X3:X2 + 
> X1^2 + X2^2 +
> X3^2, data = DATASET),
> I get estimates and AIC for the model containing main effects and
> interactions only (no squared terms)...and FYI, all predictors are
> continuous. Is this "normal" behavior? If I run this in S-Plus I get
> estimates and AIC for the model containing all 
> terms(including the squared
> terms).
> 
>    Thanks,
>    Mark Berry
> 
> 
> 
> 
> LEGAL NOTICE\ Unless expressly stated otherwise, this 
> messag...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rolf at math.unb.ca  Mon May  9 22:19:35 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 9 May 2005 17:19:35 -0300 (ADT)
Subject: [R] Monotonic regression
Message-ID: <200505092019.j49KJZjJ008528@erdos.math.unb.ca>


I have code to do isotonic regression --- linear and ``unimodal'' ---
that incorporates weights.  I can make this available if anyone is
interested.

			cheers,

				Rolf Turner
				rolf at math.unb.ca



From sghosh at lexgen.com  Mon May  9 23:39:06 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Mon, 9 May 2005 16:39:06 -0500
Subject: [R] Re: Need some quick help with lattice - barchart
Message-ID: <2B47B68F97330841AC8C670749084A7D06C49C@wdexchmb01.lexicon.lexgen.com>

Hi Deepayan,

Thanks a lot for your last Friday response. I have yet not got a chance to implement the month names for x -axis ticks yet, but will be on it as soon as some of the bugs reported by users have been resolved. In regard to that one of the users pointed out a problem with appeared to happen whenever the mean data getting plotted is less than 1.


Here's the r code generated by my prog

library(lattice)

testdata <- as.data.frame(t(structure(c(
1,2005,0.53,0.05858,159,
2,2005,0.52,0.05413,143,
3,2005,0.49,0.04986,160,
4,2005,0.5,0.05679,166,
5,2005,0.5,0.05938,32,
1,2004,0.54,0.05774,94,
2,2004,0.56,0.05748,101,
3,2004,0.54,0.0571,101,
4,2004,0.56,0.07045,96,
5,2004,0.55,0.0584,92,
6,2004,0.55,0.06136,96,
7,2004,0.55,0.06219,119,
8,2004,0.55,0.06302,124,
9,2004,0.55,0.06094,127,
10,2004,0.54,0.06482,137,
11,2004,0.54,0.06485,142,
12,2004,0.53,0.05624,157,
1,2003,0.58,0.07141,152,
2,2003,0.57,0.06181,87,
3,2003,0.54,0.05462,116,
4,2003,0.53,0.06139,124,
5,2003,0.52,0.05209,110,
6,2003,0.52,0.0596,125,
7,2003,0.54,0.06634,138,
8,2003,0.52,0.05226,84,
9,2003,0.52,0.05892,128,
10,2003,0.51,0.06095,109,
11,2003,0.53,0.06355,87,
12,2003,0.55,0.0591,117,
1,2002,0.58,0.05102,59,
2,2002,0.56,0.06422,61,
3,2002,0.55,0.06219,64,
4,2002,0.56,0.06965,65,
5,2002,0.56,0.0557,87,
6,2002,0.57,0.06578,73,
7,2002,0.57,0.06617,85,
8,2002,0.58,0.06153,76,
9,2002,0.58,0.06301,107,
10,2002,0.59,0.0747,117,
11,2002,0.59,0.0737,92,
12,2002,0.59,0.06888,100,
1,2001,0.51,0.07221,28,
2,2001,0.48,0.06705,59,
3,2001,0.51,0.06753,71,
4,2001,0.52,0.0738,37,
5,2001,0.49,0.07891,92,
6,2001,0.48,0.06521,66,
7,2001,0.49,0.05509,53,
8,2001,0.53,0.07104,61,
9,2001,0.53,0.06878,45,
10,2001,0.54,0.06661,38,
11,2001,0.51,0.06486,52,
12,2001,0.56,0.05969,59,
7,2000,0.51,0.07261,29,
8,2000,0.54,0.07678,25,
9,2000,0.53,0.07148,47,
10,2000,0.51,0.06535,54,
11,2000,0.44,0.09065,36,
12,2000,0.48,0.04639,4,
), .Dim=c(5,59))))
colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
testdata$month <- factor(testdata$month)
testdata$year <- factor(testdata$year)
testdata <- testdata[do.call("order", testdata), ]

trellis.par.set(theme = col.whitebg())

monthLabel <- c('Jan','Feb','Mar','Apr','May','Jun', 'Jul','Aug','Sep','Oct','Nov','Dec')

with(testdata, print(barchart(as.numeric(mean) ~ month | year, data=testdata, 
    layout=c(1,length(levels(year))), 
    horizontal=FALSE, 
    scales=list(y=list(limits=c(1,max(as.numeric(mean))+max(as.numeric(stdDev))))),
    main='DEXA | M | Total Body | BMC - Level I',
    xlab='Months',
    ylab='Mean',
    sd = as.numeric(as.character(stdDev)),
    panel= function(x, y, ..., sd, subscripts) {
		panel.barchart(x, y, ...);
		sd <- sd[subscripts];
		panel.segments(as.numeric(x), y - sd, as.numeric(x), y + sd, col = 'red', lwd = 2);
	   }
)))

I'm having a feeling that the as.numeric conversions is probably the cause of the problem. 

Please advise at your earliest convinience. 

-Sandeep


-----Original Message-----
From: Ghosh, Sandeep 
Sent: Friday, May 06, 2005 11:16 AM
To: Deepayan Sarkar
Cc: 'Sundar Dorai-Raj'
Subject: RE: [R] Re: Need some quick help with lattice - barchart


Want to add a little icing to the cake. Instead of showing the tick to be mnth numbers I want them to be month labels. For this I declare an array 
monthLabels <- c('Jan','Feb','Mar','Apr','May','Jun', 'Jul','Aug','Sep','Oct','Nov','Dec').

I know how to do but not sure how to implement in R syntax. How do I get the current row handle in barchart.. subscripts seems to give the current row index but is only available in panel function. what I nee is function that return 
function(index)(
for(testdata)
	list <- monthLabels [testdata$month[index]]
)

As always any help is greatly appreciated.

-Sandeep 

-----Original Message-----
From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com]
Sent: Thursday, May 05, 2005 3:09 PM
To: Ghosh, Sandeep
Cc: Deepayan Sarkar
Subject: Re: [R] Re: Need some quick help with lattice - barchart


Sorry. I copied your original script incorrectly. Changing to factor 
does indeed work.


--sundar

Ghosh, Sandeep wrote on 5/5/2005 12:22 PM:
> It worked for me..Here are the commands..
> 
> library(lattice)
> 
> testdata <- as.data.frame(t(structure(c(
> 1,2005,9.24,6.18,634,
> 2,2005,8.65,6.05,96,
> 8,2004,6.81,6.51,16,
> 9,2004,9.0,7.29,8,
> 10,2004,8.84,6.18,524,
> 11,2004,8.54,6.35,579,
> 12,2004,9.97,6.3,614,
> 12,2005,8.75,5.84,32,
> ), .Dim=c(5,8))))
> colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
> testdata$month <- factor(testdata$month)
> testdata$year <- factor(testdata$year)
> testdata <- testdata[do.call("order", testdata), ]
> 
> png('lexstar_3241.png', width=600, height=as.numeric(length(levels(testdata$year))*200), pointsize=8)
> trellis.par.set(theme = col.whitebg())
> 
> with(testdata, print(barchart(as.numeric(mean) ~ month | year, data=testdata, 
>     layout=c(1,length(levels(year))), 
>     horizontal=FALSE, 
>     scales=list(y=list(limits=c(1,max(as.numeric(mean))+max(as.numeric(stdDev))))),
>     main='Marble Burying - Level I',
>     xlab='Months',
>     ylab='Mean',
>     sd = as.numeric(as.character(stdDev)),
>     panel= function(x, y, ..., sd, subscripts) {
> 		panel.barchart(x, y, ...);
> 		sd <- sd[subscripts];
> 		panel.segments(as.numeric(x), y - sd, as.numeric(x), y + sd, col = 'red', lwd = 2);
> 	   }
> )))
> 
> dev.off()
> 
> Thanks a lot Deepayan..
> 
> -Sandeep
> 
> -----Original Message-----
> From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com]
> Sent: Thursday, May 05, 2005 2:05 PM
> To: Deepayan Sarkar
> Cc: Ghosh, Sandeep; r-help at stat.math.ethz.ch
> Subject: Re: [R] Re: Need some quick help with lattice - barchart
> 
> 
> 
> Deepayan Sarkar wrote on 5/5/2005 12:03 PM:
> 
>>On Thursday 05 May 2005 13:10, Ghosh, Sandeep wrote:
>>
>>
>>>For the following code below, the x-axis ticks are 1,2,3,4,5,6,7 when I was
>>>expection them to be 1,2,8,9,10,11,12. Please help me figure out where is
>>>the mistake.
>>
>>[...]
>>
>>
>>>colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
>>>testdata$month <- as.numeric(testdata$month)
>>
>>[...]
>>
>>
>>>with(testdata, print(barchart(as.numeric(mean) ~ month | year,
>>>data=testdata, layout=c(1,length(levels(year))),
>>>   horizontal=FALSE,
>>
>>[...]
>>
>>'month' is numeric, so it's being coerced to be a shingle. Try using 
>>'factor(month)' instead in the formula.
>>
>>Deepayan
>>
> 
> 
> Hi Deepayan,
> 
> That was my original thought too. But when I tried it I got:
> 
> Error in unit(x0, default.units, units.per.obs) :
> 	'x' must be numeric
> 
> Changing horizontal to TRUE produces the plot, but I'm sure that's not 
> what Sandeep wants.
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sghosh at lexgen.com  Mon May  9 23:53:37 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Mon, 9 May 2005 16:53:37 -0500
Subject: [R] Re: Need some quick help with lattice - barchart
Message-ID: <2B47B68F97330841AC8C670749084A7D06C49D@wdexchmb01.lexicon.lexgen.com>

I figured the problem.. the y limit scales need to start from 0 instead of 1. 

-----Original Message-----
From: Ghosh, Sandeep 
Sent: Monday, May 09, 2005 4:39 PM
To: 'Deepayan Sarkar'
Cc: 'r-help at stat.math.ethz.ch'
Subject: RE: [R] Re: Need some quick help with lattice - barchart


Hi Deepayan,

Thanks a lot for your last Friday response. I have yet not got a chance to implement the month names for x -axis ticks yet, but will be on it as soon as some of the bugs reported by users have been resolved. In regard to that one of the users pointed out a problem with appeared to happen whenever the mean data getting plotted is less than 1.


Here's the r code generated by my prog

library(lattice)

testdata <- as.data.frame(t(structure(c(
1,2005,0.53,0.05858,159,
2,2005,0.52,0.05413,143,
3,2005,0.49,0.04986,160,
4,2005,0.5,0.05679,166,
5,2005,0.5,0.05938,32,
1,2004,0.54,0.05774,94,
2,2004,0.56,0.05748,101,
3,2004,0.54,0.0571,101,
4,2004,0.56,0.07045,96,
5,2004,0.55,0.0584,92,
6,2004,0.55,0.06136,96,
7,2004,0.55,0.06219,119,
8,2004,0.55,0.06302,124,
9,2004,0.55,0.06094,127,
10,2004,0.54,0.06482,137,
11,2004,0.54,0.06485,142,
12,2004,0.53,0.05624,157,
1,2003,0.58,0.07141,152,
2,2003,0.57,0.06181,87,
3,2003,0.54,0.05462,116,
4,2003,0.53,0.06139,124,
5,2003,0.52,0.05209,110,
6,2003,0.52,0.0596,125,
7,2003,0.54,0.06634,138,
8,2003,0.52,0.05226,84,
9,2003,0.52,0.05892,128,
10,2003,0.51,0.06095,109,
11,2003,0.53,0.06355,87,
12,2003,0.55,0.0591,117,
1,2002,0.58,0.05102,59,
2,2002,0.56,0.06422,61,
3,2002,0.55,0.06219,64,
4,2002,0.56,0.06965,65,
5,2002,0.56,0.0557,87,
6,2002,0.57,0.06578,73,
7,2002,0.57,0.06617,85,
8,2002,0.58,0.06153,76,
9,2002,0.58,0.06301,107,
10,2002,0.59,0.0747,117,
11,2002,0.59,0.0737,92,
12,2002,0.59,0.06888,100,
1,2001,0.51,0.07221,28,
2,2001,0.48,0.06705,59,
3,2001,0.51,0.06753,71,
4,2001,0.52,0.0738,37,
5,2001,0.49,0.07891,92,
6,2001,0.48,0.06521,66,
7,2001,0.49,0.05509,53,
8,2001,0.53,0.07104,61,
9,2001,0.53,0.06878,45,
10,2001,0.54,0.06661,38,
11,2001,0.51,0.06486,52,
12,2001,0.56,0.05969,59,
7,2000,0.51,0.07261,29,
8,2000,0.54,0.07678,25,
9,2000,0.53,0.07148,47,
10,2000,0.51,0.06535,54,
11,2000,0.44,0.09065,36,
12,2000,0.48,0.04639,4,
), .Dim=c(5,59))))
colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
testdata$month <- factor(testdata$month)
testdata$year <- factor(testdata$year)
testdata <- testdata[do.call("order", testdata), ]

trellis.par.set(theme = col.whitebg())

monthLabel <- c('Jan','Feb','Mar','Apr','May','Jun', 'Jul','Aug','Sep','Oct','Nov','Dec')

with(testdata, print(barchart(as.numeric(mean) ~ month | year, data=testdata, 
    layout=c(1,length(levels(year))), 
    horizontal=FALSE, 
    scales=list(y=list(limits=c(1,max(as.numeric(mean))+max(as.numeric(stdDev))))),
    main='DEXA | M | Total Body | BMC - Level I',
    xlab='Months',
    ylab='Mean',
    sd = as.numeric(as.character(stdDev)),
    panel= function(x, y, ..., sd, subscripts) {
		panel.barchart(x, y, ...);
		sd <- sd[subscripts];
		panel.segments(as.numeric(x), y - sd, as.numeric(x), y + sd, col = 'red', lwd = 2);
	   }
)))

I'm having a feeling that the as.numeric conversions is probably the cause of the problem. 

Please advise at your earliest convinience. 

-Sandeep


-----Original Message-----
From: Ghosh, Sandeep 
Sent: Friday, May 06, 2005 11:16 AM
To: Deepayan Sarkar
Cc: 'Sundar Dorai-Raj'
Subject: RE: [R] Re: Need some quick help with lattice - barchart


Want to add a little icing to the cake. Instead of showing the tick to be mnth numbers I want them to be month labels. For this I declare an array 
monthLabels <- c('Jan','Feb','Mar','Apr','May','Jun', 'Jul','Aug','Sep','Oct','Nov','Dec').

I know how to do but not sure how to implement in R syntax. How do I get the current row handle in barchart.. subscripts seems to give the current row index but is only available in panel function. what I nee is function that return 
function(index)(
for(testdata)
	list <- monthLabels [testdata$month[index]]
)

As always any help is greatly appreciated.

-Sandeep 

-----Original Message-----
From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com]
Sent: Thursday, May 05, 2005 3:09 PM
To: Ghosh, Sandeep
Cc: Deepayan Sarkar
Subject: Re: [R] Re: Need some quick help with lattice - barchart


Sorry. I copied your original script incorrectly. Changing to factor 
does indeed work.


--sundar

Ghosh, Sandeep wrote on 5/5/2005 12:22 PM:
> It worked for me..Here are the commands..
> 
> library(lattice)
> 
> testdata <- as.data.frame(t(structure(c(
> 1,2005,9.24,6.18,634,
> 2,2005,8.65,6.05,96,
> 8,2004,6.81,6.51,16,
> 9,2004,9.0,7.29,8,
> 10,2004,8.84,6.18,524,
> 11,2004,8.54,6.35,579,
> 12,2004,9.97,6.3,614,
> 12,2005,8.75,5.84,32,
> ), .Dim=c(5,8))))
> colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
> testdata$month <- factor(testdata$month)
> testdata$year <- factor(testdata$year)
> testdata <- testdata[do.call("order", testdata), ]
> 
> png('lexstar_3241.png', width=600, height=as.numeric(length(levels(testdata$year))*200), pointsize=8)
> trellis.par.set(theme = col.whitebg())
> 
> with(testdata, print(barchart(as.numeric(mean) ~ month | year, data=testdata, 
>     layout=c(1,length(levels(year))), 
>     horizontal=FALSE, 
>     scales=list(y=list(limits=c(1,max(as.numeric(mean))+max(as.numeric(stdDev))))),
>     main='Marble Burying - Level I',
>     xlab='Months',
>     ylab='Mean',
>     sd = as.numeric(as.character(stdDev)),
>     panel= function(x, y, ..., sd, subscripts) {
> 		panel.barchart(x, y, ...);
> 		sd <- sd[subscripts];
> 		panel.segments(as.numeric(x), y - sd, as.numeric(x), y + sd, col = 'red', lwd = 2);
> 	   }
> )))
> 
> dev.off()
> 
> Thanks a lot Deepayan..
> 
> -Sandeep
> 
> -----Original Message-----
> From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com]
> Sent: Thursday, May 05, 2005 2:05 PM
> To: Deepayan Sarkar
> Cc: Ghosh, Sandeep; r-help at stat.math.ethz.ch
> Subject: Re: [R] Re: Need some quick help with lattice - barchart
> 
> 
> 
> Deepayan Sarkar wrote on 5/5/2005 12:03 PM:
> 
>>On Thursday 05 May 2005 13:10, Ghosh, Sandeep wrote:
>>
>>
>>>For the following code below, the x-axis ticks are 1,2,3,4,5,6,7 when I was
>>>expection them to be 1,2,8,9,10,11,12. Please help me figure out where is
>>>the mistake.
>>
>>[...]
>>
>>
>>>colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount')
>>>testdata$month <- as.numeric(testdata$month)
>>
>>[...]
>>
>>
>>>with(testdata, print(barchart(as.numeric(mean) ~ month | year,
>>>data=testdata, layout=c(1,length(levels(year))),
>>>   horizontal=FALSE,
>>
>>[...]
>>
>>'month' is numeric, so it's being coerced to be a shingle. Try using 
>>'factor(month)' instead in the formula.
>>
>>Deepayan
>>
> 
> 
> Hi Deepayan,
> 
> That was my original thought too. But when I tried it I got:
> 
> Error in unit(x0, default.units, units.per.obs) :
> 	'x' must be numeric
> 
> Changing horizontal to TRUE produces the plot, but I'm sure that's not 
> what Sandeep wants.
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From zwu at jhsph.edu  Tue May 10 00:43:35 2005
From: zwu at jhsph.edu (Zhijin Wu)
Date: Mon, 9 May 2005 18:43:35 -0400 (EDT)
Subject: [R] use "integrate" for functions defined in C, not R
Message-ID: <Pine.GSO.4.10.10505091823330.609-100000@athena.biostat.jhsph.edu>

Dear all,
 I am trying to use the C code for "integrate" function ( that calls 
Rdqagi and Rdqags) so that I can integrate a function defined in C,
instead of passing from R. 
  Is there a way for doing this? 
  
My unsuccessful attempt:
  I looked into the files (including integrate.c, Applic.h) and
1. modified the definition of
"integr_fn" by droping the environment "*ex", 
        void integr_fn(double *x, int n) 
and dropped all use of "ex" used in the code

2. defined my checker function f1 and the vectorizing function "Cintfn" in
place of "Rintfn"

          double f1(double x){ return(x);}
          static void Cintfn(double *x, int n)
           {
           int i;
           for(i = 0; i < n; i++) 
           x[i] = f1(x[i]);
           return;
           }
 
3. Similar to  "call_dqags", I define a C function "my_call_dqags" that
has it's own parameters of "lower, upper"  and etc define in C, instead of
parsing from R. And I call 
  Rdqags(Cintfn,
         &lower, &upper, &epsabs, &epsrel, &result,
         &abserr, &neval, &ier, &limit, &lenw, &last, iwork, work);
instead of 
  Rdqags(Rintfn, (void*)&is,
           &lower, &upper, &epsabs, &epsrel, &result,
           &abserr, &neval, &ier, &limit, &lenw, &last, iwork, work);

I am not passing (void*)&is because I no longer have the "environment".


The code compiles fine with R CMD SHLIB. But it returns 5.3e-317 for my
checker function f(x)=x, integration interval (1,2).

Thanks for any hint!

Jean



From aw-confirm at paypal.com  Tue May 10 01:20:33 2005
From: aw-confirm at paypal.com (PayPal)
Date: 9 May 2005 18:20:33 -0500
Subject: [R] Account compromised: billing information moved or changed.
Message-ID: <20050509232033.18888.qmail@mail.twister-alley.com>


              [paypal_logo.gif]
                                   [action.gif] LEGAL NOTICE
    The following message is an email sent to you by an administrator of
                               "PayPal.com".
    If this message is spam, contains abusive or other comments you find
      offensive please contact the webmaster at the following address:
                            [1]admin at paypal.com
                                  Message sent to you follows:
                            Dear PayPal client,
   While performing it's regular scheduled monthly billing address check
   our system found incompatible information which seams to be no longer
     the same with your current credit card information that we have on
   file. If you changed your billing information or if you moved from you
     previous address please follow up the link bellow and update your
   billing information: If you didn't change any of this information you
     still need to follow up the previous link and update your existing
       billing information because it means that our database regular
      scheduled update wasn't made correctly. Choosing to ignore this
      message will result in to a temporary suspension of your account
      within 24 hours, until you will choose to solve this unpleasant
                                 situation.
   We apologies for any inconvinience this may caused you and we strongly
      advise you to update your information you have on file with us.
   Clicking [2]https://www.paypal.com/cgi-bin/webscr?cmd=_login_ you will
      avoid any possible futuring billing problems with your account.
                               Best regards,
                               - PayPal Team.

References

   1. mailto:admin at paypal.com
   2. http://12.30.229.107/PayPal/index.php


From jacob.etches at utoronto.ca  Tue May 10 02:10:34 2005
From: jacob.etches at utoronto.ca (Jacob Etches)
Date: Mon, 9 May 2005 20:10:34 -0400
Subject: [R] horizontal position of plot title
In-Reply-To: <427354E2.6000105@stats.uwo.ca>
References: <20050430044413.GA1524@lubyanka.local>
	<427354E2.6000105@stats.uwo.ca>
Message-ID: <973d0836e8385ed78ab7e8835800b6e5@utoronto.ca>

Is it possible to "left-justify" a plot title rather than centering it? 
  What parameters, if any, control title position?  I didn't see any 
options for this in ?par or ?title.

Thanks,
Jacob Etches


Doctoral candidate, Epidemiology
Department of Public Health Sciences
University of Toronto Faculty of Medicine

Research Associate
Institute for Work & Health
800-481 University Ave.
Toronto, ON
M5G 2E9
416.927.2027x2290
www.iwh.on.ca



From gerifalte28 at hotmail.com  Tue May 10 02:47:56 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 10 May 2005 00:47:56 +0000
Subject: [R] horizontal position of plot title
In-Reply-To: <973d0836e8385ed78ab7e8835800b6e5@utoronto.ca>
Message-ID: <BAY103-F34A150B4F4D402F4FDA3FEA61F0@phx.gbl>

par(adj=0) will do the job.  It is described in details under ?par.  You 
just need to look a little bit closer ;)

Cheers

Francisco


>From: Jacob Etches <jacob.etches at utoronto.ca>
>To: r-help at stat.math.ethz.ch
>Subject: [R] horizontal position of plot title
>Date: Mon, 9 May 2005 20:10:34 -0400
>
>Is it possible to "left-justify" a plot title rather than centering it?  
>What parameters, if any, control title position?  I didn't see any options 
>for this in ?par or ?title.
>
>Thanks,
>Jacob Etches
>
>
>Doctoral candidate, Epidemiology
>Department of Public Health Sciences
>University of Toronto Faculty of Medicine
>
>Research Associate
>Institute for Work & Health
>800-481 University Ave.
>Toronto, ON
>M5G 2E9
>416.927.2027x2290
>www.iwh.on.ca
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From fsaldan1 at gmail.com  Tue May 10 05:07:01 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Mon, 9 May 2005 23:07:01 -0400
Subject: [R] Does R have a command for sending emails?
Message-ID: <10dee46905050920073d17f041@mail.gmail.com>

Is there a way to have an R program send an email?

Something like this:

address <- 'abc at d.com'
text <- 'This is the email body'
send.email(address, text)

Thanks.

FS



From ripley at stats.ox.ac.uk  Tue May 10 06:43:31 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 May 2005 05:43:31 +0100 (BST)
Subject: [R] Does R have a command for sending emails?
In-Reply-To: <10dee46905050920073d17f041@mail.gmail.com>
References: <10dee46905050920073d17f041@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505100540300.17882@gannet.stats>

On Mon, 9 May 2005, Fernando Saldanha wrote:

> Is there a way to have an R program send an email?

No.  There have been proposals to do this, but it is highly OS-specific, 
and also likely to hit security issues.

You could take a look at what bug.report() does (which is nothing on some 
platforms).

> Something like this:
>
> address <- 'abc at d.com'
> text <- 'This is the email body'
> send.email(address, text)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue May 10 06:51:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 May 2005 05:51:25 +0100 (BST)
Subject: [R] use "integrate" for functions defined in C, not R
In-Reply-To: <Pine.GSO.4.10.10505091823330.609-100000@athena.biostat.jhsph.edu>
References: <Pine.GSO.4.10.10505091823330.609-100000@athena.biostat.jhsph.edu>
Message-ID: <Pine.LNX.4.61.0505100544290.17882@gannet.stats>

On Mon, 9 May 2005, Zhijin Wu wrote:

> Dear all,
> I am trying to use the C code for "integrate" function ( that calls
> Rdqagi and Rdqags) so that I can integrate a function defined in C,
> instead of passing from R.
>  Is there a way for doing this?
>
> My unsuccessful attempt:
>  I looked into the files (including integrate.c, Applic.h) and
> 1. modified the definition of
> "integr_fn" by droping the environment "*ex",
>        void integr_fn(double *x, int n)
> and dropped all use of "ex" used in the code
>
> 2. defined my checker function f1 and the vectorizing function "Cintfn" in
> place of "Rintfn"
>
>          double f1(double x){ return(x);}
>          static void Cintfn(double *x, int n)
>           {
>           int i;
>           for(i = 0; i < n; i++)
>           x[i] = f1(x[i]);
>           return;
>           }
>
> 3. Similar to  "call_dqags", I define a C function "my_call_dqags" that
> has it's own parameters of "lower, upper"  and etc define in C, instead of
> parsing from R. And I call
>  Rdqags(Cintfn,
>         &lower, &upper, &epsabs, &epsrel, &result,
>         &abserr, &neval, &ier, &limit, &lenw, &last, iwork, work);
> instead of
>  Rdqags(Rintfn, (void*)&is,
>           &lower, &upper, &epsabs, &epsrel, &result,
>           &abserr, &neval, &ier, &limit, &lenw, &last, iwork, work);
>
> I am not passing (void*)&is because I no longer have the "environment".
>
> The code compiles fine with R CMD SHLIB. But it returns 5.3e-317 for my
> checker function f(x)=x, integration interval (1,2).
>
> Thanks for any hint!

1) This is the wrong list: please read the posting guide.

2) You cannot just leave out arguments in C calls, so it seems that you 
need help with C programming rather than R.  If you include the 
appropriate headers this will be checked, so I guess you have not.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Meredith.Briggs at team.telstra.com  Tue May 10 06:44:59 2005
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Tue, 10 May 2005 14:44:59 +1000
Subject: [R] Using function 'boot on a bi-polar sample'
Message-ID: <3B5823541A25D311B3B90008C7F90564199CB53B@ntmsg0092.corpmail.telstra.com.au>


Hello

I'm not sure I'm using boot correctly:

I have a list of values for a variable in BUCKET[, j]
I want to use function 'boot' to estimate a confidence interval on the mean of the non-zero data. The data can be bi-polar or skewed.
Is this the correct use of boot to establish a mean and standard deviation or median and percentiles?  


	x<-BUCKET[, j]
          	mean.fun<-function(x,i) {mean(x[x>0])}
	median.fun<-function(x,i) {median(x[x>0])}
	print(mean.fun(x))
	print(median.fun(x))
      	B.mean<-boot(x,mean.fun,R=999)
	B.median<-boot(x,mean.fun,R=999)
      	print(B.mean)
	print(B.median)

I need the 'boot' function to sample repeatedly from the density distribution of the raw data and calculate means, medians etc.

How do I pull out the mean, median, deviations and percentiles into variables within R?

thanks
Meredith



From ahrens at zoology.ubc.ca  Tue May 10 07:19:31 2005
From: ahrens at zoology.ubc.ca (Robert Ahrens)
Date: Mon, 09 May 2005 22:19:31 -0700
Subject: [R] Re: gamma cannot be modified on this device
Message-ID: <42804463.4040603@zoology.ubc.ca>

Problem:  dev.copy2eps results in gamma cannot be modified on this 
device.  File cannot be opened by Adobe Illistrator.  File can be 
converted by Adobe distiler.  Any help on fixing gamma error would be 
appreciated.


Code:  Utilizes PBS Mapping
plots<-function(){
    library(PBSmapping)
    par(mfrow=c(2,1))
    par(omi=c(.5,.5,.5,.5))
    par(mai=c(0,0.5,0.0,.7))
    par(ask=T)
    nm<-c("Albacore Tuna","Bigeye Tuna","Yellowfin 
Tuna","Swordfish","Blue Marlin","Striped Marlin","Black Marlin","White 
Marlin","Bluefin Tuna","Southern Bluefin Tuna")
    nm2<-c("ALB","BET","YFT","SWO","BUM","STM","BLM","WHM","BFT","SBT")
    data(indLLhigh)
    data(recoutglo30yr)
    dfile<-recoutglo30yr
    for(i in 1:levels(nm2)){
        fl<-dfile[dfile$Sp==nm2[i],]
        species<-nm[i]
        nrec=length(fl$X)
        Z<-vector(length=length(fl$X))
        for (j in 1:nrec){
            xx<-fl$X[j]
            if(xx<20){
                Z[j]=360+fl$X[j]
            }else{
                Z[j]=fl$X[j]
            }
        }
        fl$X<-Z
        makeplots(indLLhigh,fl,species,c(20,380),1)
        makeplots(indLLhigh,fl,species,c(20,380),2)
        outfile<-paste(nm2[i],"RN.eps","")
        dev.copy2eps(file=outfile)
    }
}
makeplots<-function(mfile,dfile,ttl,xl,zvariable){
    yl<-c(-70,80)
        if (zvariable==1) {
            tlabel<-paste(ttl,"Recruitment",sep=" ")
    } else {
            tlabel<-paste(ttl,"Numbers",sep=" ")
    }
    plotMap(mfile,col="tan",bg="transparent",xlim=xl,ylim=yl,
        main=tlabel,plt=NULL,type="n",projection="LL")
    mtext("(Thousands)",side=3,line=0.5,cex=0.7)
    
grid<-makeGrid(x=seq(xl[1],xl[2],5),y=seq(yl[1],yl[2],5),projection="LL")
    events<-dfile
        if (zvariable==1) {
            events$Z<-events$Rec
    } else {
            events$Z<-events$Ninit
    }
   
    locData<-findPolys(events,grid)
    pdata<-combineEvents(events,locData,FUN=mean)
    brksmax<-max(events$Z)
    brksmin<-min(events$Z)
    nbrks<-20
    brks<-seq(brksmin,brksmax,by=(brksmax-brksmin)/nbrks)
    #cols<-rev(grey(seq(0,1,by=1/nbrks)))
    cols=rev(rainbow(nbrks,start=0,end=0.6))
    cols[1]<-"darkblue"
    pdata<-makeProps(pdata,brks,"col",cols)
    addPolys(grid,polyProps=pdata,border=F)
    addPolys(mfile,col="tan")
    abline(h=0,lty="dashed",col="red")   
    abline(h=40,lty="dashed",col="red")   
    abline(h=-40,lty="dashed",col="red")
    ltext<-vector(length=length(brks))
    ltext<-NA
    
ltext[1]<-as.integer(brks[1]/1000);ltext[nbrks]<-as.integer(brks[nbrks]/1000);ltext[as.integer(nbrks/2)]<-as.integer(brks[as.integer(nbrks/2)]/1000)
    par(xpd=NA)
    
tmp<-legend(xl[2]+.1,yl[2]-.1,legend=rep("",nbrks),col=rev(cols),pch=15,pt.cex=1.2,bty="n",y.intersp=.45)
    text(tmp$rect$left+10,tmp$text$y,pos=4,rev(ltext),cex=.7)
    par(xpd=FALSE)
}

-- 
_____________________________
Robert Ahrens
Department of Zoology
University of British Columbia
6270 University Blvd.
Vancouver, B.C. V6T 1Z4
604.822.0046
ahrens at zoology.ubc.ca



From m.blizinski at wsisiz.edu.pl  Tue May 10 07:28:28 2005
From: m.blizinski at wsisiz.edu.pl (Maciej =?iso-8859-2?Q?Blizi=F1ski?=)
Date: Tue, 10 May 2005 07:28:28 +0200
Subject: [R] Monotonic regression
In-Reply-To: <200505092019.j49KJZjJ008528@erdos.math.unb.ca>
References: <200505092019.j49KJZjJ008528@erdos.math.unb.ca>
Message-ID: <20050510052828.GA5206@oceanic.wsisiz.edu.pl>

Rolf Turner wrote:
> I have code to do isotonic regression --- linear and ``unimodal'' ---
> that incorporates weights.

What is it? And what is it used for? :-)

-- 
Maciej Blizi??ski <s050208 at student.dtu.dk>
Danmarks Tekniske Universitet http://www.dtu.dk
International guest student



From ripley at stats.ox.ac.uk  Tue May 10 08:00:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 May 2005 07:00:07 +0100 (BST)
Subject: [R] Using function 'boot on a bi-polar sample'
In-Reply-To: <3B5823541A25D311B3B90008C7F90564199CB53B@ntmsg0092.corpmail.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F90564199CB53B@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <Pine.LNX.4.61.0505100656210.3828@gannet.stats>

On Tue, 10 May 2005, Briggs, Meredith M wrote:

> I'm not sure I'm using boot correctly:

You are not using argument `i'.  Have you read the reference: `boot' is
support software for a book?  (Or even the definition of `statistic' on 
the help page?)  The bootstrap sample in your case is x[i], not x.

> I have a list of values for a variable in BUCKET[, j] I want to use 
> function 'boot' to estimate a confidence interval on the mean of the 
> non-zero data. The data can be bi-polar or skewed. Is this the correct 
> use of boot to establish a mean and standard deviation or median and 
> percentiles?
>
>
> 	x<-BUCKET[, j]
>          	mean.fun<-function(x,i) {mean(x[x>0])}
> 	median.fun<-function(x,i) {median(x[x>0])}
> 	print(mean.fun(x))
> 	print(median.fun(x))
>      	B.mean<-boot(x,mean.fun,R=999)
> 	B.median<-boot(x,mean.fun,R=999)
>      	print(B.mean)
> 	print(B.median)
>
> I need the 'boot' function to sample repeatedly from the density 
> distribution of the raw data and calculate means, medians etc.
>
> How do I pull out the mean, median, deviations and percentiles into 
> variables within R?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lxiaolei at stat.wisc.edu  Tue May 10 08:28:41 2005
From: lxiaolei at stat.wisc.edu (Li, Xiaolei)
Date: Tue, 10 May 2005 01:28:41 -0500 (CDT)
Subject: [R] Re: Welcome to the "R-help" mailing list
In-Reply-To: <mailman.1907.1115705640.1607.r-help@stat.math.ethz.ch>
References: <mailman.1907.1115705640.1607.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0505100114550.25592@chi51.stat.wisc.edu>


I am trying to pass some R objects to C++ code. However, after reading
through the mannual "Writing R extensioms" for version 2.1.0
(2005-04-18),
I still couldn't figure out:

1) How can I see .c files in a package? For example, I am interested in
looking at source files in "Spdep".

2) Can I include whichever header files I find in my C++ code, if I
eventually will load the C++ code into R?

3) If yes to 2), where are header files of packages such as "Spdep"?

4) Any written code to handle matrices in this interface with C++?

Thanks

Xiaolei

______________________________________
Xiaolei Li

Department of Statistics
University of Wisconsin-Madison
1130A Medical Science Center
1300 University Avenue
Madison, WI 53706
USA

Office: (608)265-6217
Email: lxiaolei at stat.wisc.edu



From Roger.Bivand at nhh.no  Tue May 10 09:18:13 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 10 May 2005 09:18:13 +0200 (CEST)
Subject: [R] Re: Welcome to the "R-help" mailing list
In-Reply-To: <Pine.LNX.4.58.0505100114550.25592@chi51.stat.wisc.edu>
Message-ID: <Pine.LNX.4.44.0505100905370.6209-100000@reclus.nhh.no>

On Tue, 10 May 2005, Li, Xiaolei wrote:

Yes, welcome, and please note the clear instructions for users in the
posting guide, to wit:  give an informative subject, and read the
necessary documentation first.

In your case, just accessing the "Packages" link in the navigation bar of 
the CRAN website would take you straight to the source package you are 
interested in. As the name suggests, source packages contain the source 
code for a package. The posting guide also suggests (firmly) that 
questions about packages should be addressed to their maintainers. 

As the maintainer of spdep, I may be able to respond to motivated 
questions off-list, but I doubt that your approach is going to be 
fruitful; there are usually better ways of doing things than writing C++, 
in fact writing R can be just as productive, and is much easier to debug.

> 
> I am trying to pass some R objects to C++ code. However, after reading
> through the mannual "Writing R extensioms" for version 2.1.0
> (2005-04-18),
> I still couldn't figure out:
> 
> 1) How can I see .c files in a package? For example, I am interested in
> looking at source files in "Spdep".
> 
> 2) Can I include whichever header files I find in my C++ code, if I
> eventually will load the C++ code into R?
> 
> 3) If yes to 2), where are header files of packages such as "Spdep"?
> 
> 4) Any written code to handle matrices in this interface with C++?
> 
> Thanks
> 
> Xiaolei
> 
> ______________________________________
> Xiaolei Li
> 
> Department of Statistics
> University of Wisconsin-Madison
> 1130A Medical Science Center
> 1300 University Avenue
> Madison, WI 53706
> USA
> 
> Office: (608)265-6217
> Email: lxiaolei at stat.wisc.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From michael_shen at hotmail.com  Tue May 10 09:48:02 2005
From: michael_shen at hotmail.com (Michael S)
Date: Tue, 10 May 2005 07:48:02 +0000
Subject: [R] about sink and scan 
Message-ID: <BAY1-F237054002A67264B76D463E71F0@phx.gbl>

Hi All r-helper,

my question like this:
I use "sink" function to get output into one file,during the sink process , 
output keep writing into this file,I want to check is there any new output 
write into this file(which "sink function create"), using "scan" function to 
read new output from that file.
if I use the cat with "\n" ,and scan(....,skip=linenumber) ,I can get what I 
want.but I just use "cat" without "\n",every new output just append at the 
end of last output. I can not use skip option in scan function . what should 
I do ?
I know file.info(..)$size can get current size of file. but how can I check 
which is new output or old one within the same line ,then scan the latest.

example:
sink('test.txt')
cat("r sink")
scan('text.txt',what='character',blank.lines.skip=FALSE,sep='\\n',skip=0,quiet=TRUE)
cat("htuierghtue")
scan#this time I just want to scan the "htuierghtue"
sink()

thanks in advance



From sabine.navarre at siemens.com  Tue May 10 10:19:50 2005
From: sabine.navarre at siemens.com (Navarre Sabine (stu))
Date: Tue, 10 May 2005 10:19:50 +0200
Subject: [R] Connection problem to MySQL
Message-ID: <C0A2DDBDA4904048820B3477D766FE3C01D313EF@tlsm385a.ww011.siemens.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050510/16ed1fcd/attachment.pl

From phgrosjean at sciviews.org  Tue May 10 10:57:43 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 10 May 2005 10:57:43 +0200
Subject: [R] Does R have a command for sending emails?
In-Reply-To: <Pine.LNX.4.61.0505100540300.17882@gannet.stats>
References: <10dee46905050920073d17f041@mail.gmail.com>
	<Pine.LNX.4.61.0505100540300.17882@gannet.stats>
Message-ID: <42807787.6030504@sciviews.org>

Prof Brian Ripley wrote:
> On Mon, 9 May 2005, Fernando Saldanha wrote:
> 
>> Is there a way to have an R program send an email?
> 
> 
> No.  There have been proposals to do this, but it is highly OS-specific, 
> and also likely to hit security issues.
> 
> You could take a look at what bug.report() does (which is nothing on 
> some platforms).

...but on most, if not all systems, you can use command-line 
applications to send emails... and since R can start command-line 
applications with system(), you should get that functionnality easily 
"incorporated" into R. Just use Google a little bit!
Best,

Philippe Grosjean

> 
>> Something like this:
>>
>> address <- 'abc at d.com'
>> text <- 'This is the email body'
>> send.email(address, text)
> 
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Tue May 10 11:01:31 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 10 May 2005 10:01:31 +0100 (BST)
Subject: [R] Does R have a command for sending emails?
In-Reply-To: <10dee46905050920073d17f041@mail.gmail.com>
Message-ID: <XFMail.050510100131.Ted.Harding@nessie.mcc.ac.uk>

On 10-May-05 Fernando Saldanha wrote:
> Is there a way to have an R program send an email?
> 
> Something like this:
> 
> address <- 'abc at d.com'
> text <- 'This is the email body'
> send.email(address, text)

As Brian Ripley said, this is highly OS-specific!

On the other hand, studying 'bug.report' may be excessive.

*Provided* you are using a Unixoid system (Unix, Linux,
and as far as I know Mac OS X), you will almost certainly
have the 'mail' command which can be readily used for a
simple email such as the one you describe above.

At the system level, a possible command could be

  mail -s "subject of mail" abc at d.com << EOT
  "This is the email body"
  EOT

so all you need is an R function which binds all these
elements together.

For example, the following works on my Linux system:

  send.mail<-function(addr,subject="Mail from R",
                    text="empty text"){
    mail.cmd<-paste("mail ",
                    "-s \"",subject,"\" ",
                    addr,
                    " << EOT &\n",
                    text,"\n",
                    "EOT",
                    sep="",collapse="")
     system(mail.cmd,intern=FALSE)
  }

For example, with:

  send.mail("ted at localhost",
  subject="Message from R: At Last!",
  text="Good Morning!\nI've been busy all night.\nIt's finished."
  )

after a minute or two for delivery I get the following message
delivered on my machine:

  From ted at compo.my.LAN  Tue May 10 09:50:27 2005
  Date: Tue, 10 May 2005 09:49:50 +0100
  From: Ted Harding <ted at compo.my.LAN>
  To: ted at compo.my.LAN
  Subject: Message from R: At Last!

  Good Morning!
  I've been busy all night.
  It's finished.

I've specified "intern=FALSE" explicitly (though it is the
default), since if you set it TRUE (e.g. in order for R to
verify the sending) then R will hang until it receives the
feedback from the command (the "&" in "EOT &\n" puts the job
in the background so there is no delay).

Hoping this helps! (Of course, if you're not a unixoid, this
is probably no use to you at all, and I have no idea how to
do anything similar in Windows).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-May-05                                       Time: 10:01:21
------------------------------ XFMail ------------------------------



From B.Rowlingson at lancaster.ac.uk  Tue May 10 11:25:53 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 10 May 2005 10:25:53 +0100
Subject: [R] Does R have a command for sending emails?
In-Reply-To: <XFMail.050510100131.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050510100131.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <42807E21.6020407@lancaster.ac.uk>


> Hoping this helps! (Of course, if you're not a unixoid, this
> is probably no use to you at all, and I have no idea how to
> do anything similar in Windows).

  How about an R package to talk to google's gmail service, perhaps 
leveraging an existing API, such as:

http://libgmail.sourceforge.net/

  this is written in pure python, so not only is it cross-platform but 
it should be possible to write one in pure R using HTTP calls and the like.

  you could then do statistical analysis of your gmail spam purely in R.

  or am I now getting too silly?

Baz



From lars.claussen at pik-potsdam.de  Tue May 10 11:29:10 2005
From: lars.claussen at pik-potsdam.de (Lars)
Date: Tue, 10 May 2005 11:29:10 +0200
Subject: [R] filename
Message-ID: <42807EE6.8050703@pik-potsdam.de>

Hey,

I'm generating a .jpeg file for a web application passing parameters to 
R via cgi. R gets the parameters by using commandArgs, which are passed 
to a variable named j  and z

j<-commandArgs()[3]
z<-commandArgs()[4]

Later I want to use the characters strings of the argument which the 
variables are holding for my filename, e.g.:

jpeg(file="d:/data/images/jz.jpeg",...)

i tried to use paste and as.character, but neither worked out the way i 
want it to.

thanks for any advice, Lars Claussen



From dvumani at hotmail.com  Tue May 10 11:39:15 2005
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Tue, 10 May 2005 09:39:15 +0000
Subject: [R] including R function in C program
Message-ID: <BAY16-F270A6561CF91664737EFB5A31F0@phx.gbl>

Is it possoble to include an R function in a C program? I would like to use 
"rmultz" from the "combinat" library to sample a multinomial vector within 
my C program, and have no clue how this can be done.
Thanks.
Vumani



From blindglobe at gmail.com  Tue May 10 12:02:36 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 10 May 2005 12:02:36 +0200
Subject: [R] Does R have a command for sending emails?
In-Reply-To: <42807E21.6020407@lancaster.ac.uk>
References: <XFMail.050510100131.Ted.Harding@nessie.mcc.ac.uk>
	<42807E21.6020407@lancaster.ac.uk>
Message-ID: <1abe3fa9050510030219613a80@mail.gmail.com>

On 5/10/05, Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:
> 
> > Hoping this helps! (Of course, if you're not a unixoid, this
> > is probably no use to you at all, and I have no idea how to
> > do anything similar in Windows).
> 
>  How about an R package to talk to google's gmail service, perhaps
> leveraging an existing API, such as:
> 
> http://libgmail.sourceforge.net/
> 
>  this is written in pure python, so not only is it cross-platform but
> it should be possible to write one in pure R using HTTP calls and the like.
> 
>  you could then do statistical analysis of your gmail spam purely in R.
> 
>  or am I now getting too silly?
 
Not yet.

However, one could just connect via sockets to port 25 (smtp port) and
do an appropriate mail sending dialog.

Not too hard, provided that you get the details right and handle any
security issues.

-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From ligges at statistik.uni-dortmund.de  Tue May 10 12:56:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 10 May 2005 12:56:36 +0200
Subject: [R] filename
In-Reply-To: <42807EE6.8050703@pik-potsdam.de>
References: <42807EE6.8050703@pik-potsdam.de>
Message-ID: <42809364.6050805@statistik.uni-dortmund.de>

Lars wrote:

> Hey,
> 
> I'm generating a .jpeg file for a web application passing parameters to 
> R via cgi. R gets the parameters by using commandArgs, which are passed 
> to a variable named j  and z
> 
> j<-commandArgs()[3]
> z<-commandArgs()[4]
> 
> Later I want to use the characters strings of the argument which the 
> variables are holding for my filename, e.g.:
> 
> jpeg(file="d:/data/images/jz.jpeg",...)

If j contains the name for the file,
jpeg(file=j, ...)
should work. I guess you do not have jpeg support if calling your script 
in non-interactive mode, because you need X11 active. See ?jpeg.

Uwe Ligges



> i tried to use paste and as.character, but neither worked out the way i 
> want it to.
> 
> thanks for any advice, Lars Claussen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From matthieu.cornec at gmail.com  Tue May 10 12:55:52 2005
From: matthieu.cornec at gmail.com (Matthieu Cornec)
Date: Tue, 10 May 2005 12:55:52 +0200
Subject: [R] Aggregate lag
Message-ID: <8a83e500050510035513858c8c@mail.gmail.com>

hello,

Does anybody know how to aggregate a lag series ?
when I try to use aggregate I get the following message

> try<-ts(1:100,start=c(1985,1),freq=12)
> aggregate(try,4,mean,na.rm=T)
    Qtr1 Qtr2 Qtr3 Qtr4
1985    2    5    8   11
1986   14   17   20   23
1987   26   29   32   35
1988   38   41   44   47
1989   50   53   56   59
1990   62   65   68   71
1991   74   77   80   83
1992   86   89   92   95
1993   98
> aggregate(lag(try,-1),4,mean,na.rm=T)
Error in rep.int("", start.pad) : invalid number of copies in rep()

Matthieu



From petr.pikal at precheza.cz  Tue May 10 13:25:09 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 10 May 2005 13:25:09 +0200
Subject: [R] predict nlme syntax
Message-ID: <4280B635.2332.13F257A@localhost>

Dear all

Please help me with correct syntax of predict.nlme.
I would like to predict from nlme object for new data.
I used predict(fit.nlme6, data=newdata) but I have always got 
fitted values, no matter how I changed newdata.

I have

> summary(fit.nlme6)
Nonlinear mixed-effects model fit by maximum likelihood
  Model: konverze ~ SSfpl(tepl, A, B, xmid, scal) 
 Data: limity.gr 
       AIC      BIC    logLik
  882.4939 907.6738 -433.2469

Random effects:
 Formula: list(xmid ~ 1, scal ~ 1)
 Level: spol.f
 Structure: General positive-definite, Log-Cholesky 
parametrization
         StdDev    Corr 
xmid     29.680114 xmid 
scal      6.481679 0.249
Residual  2.168191      

Fixed effects: list(A ~ 1, B ~ 1, xmid ~ 1, scal ~ 1) 
        Value Std.Error  DF   t-value p-value
A     36.1450  0.837050 154  43.18133       0
B    101.0272  0.432074 154 233.81898       0
xmid 735.3860  8.150964 154  90.22074       0
scal  15.4453  2.201864 154   7.01466       0
 Correlation: 
     A      B      xmid  
B    -0.088              
xmid  0.057 -0.088       
scal -0.089  0.469  0.036

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-3.7707629568 -0.3291628536  0.0005885683  0.4020944158  
3.7911729382 

Number of Observations: 172
Number of Groups: 15 

where **tepl** is independent variable and **spol.f** is grouping 
factor.

The newly constructed data frame newdata has the same structure 
of spol.f levels as has the limity.gr data frame used for fitting.

> levels(limity.gr$spol.f)
 [1] "1.8/3"   "4/3"     "6.3/3"   "10.8/3"  "1.8/7"   "1.8/12"  
"1.8/30"  "6.3/30"  "10.8/30" "1.8/60"  "4/60"    "6.3/60"  
"1.8/110"
[14] "1.8/200" "1.8/300"

> levels(newdata$spol.f)
 [1] "1.8/3"   "4/3"     "6.3/3"   "10.8/3"  "1.8/7"   "1.8/12"  
"1.8/30"  "6.3/30"  "10.8/30" "1.8/60"  "4/60"    "6.3/60"  
"1.8/110"
[14] "1.8/200" "1.8/300"
>

The only difference is in temperature.

Please advice how shall I change newdata to be able to use it in 
predict function.

Thank you.

Best regards

Petr Pikal
petr.pikal at precheza.cz



From tarun30devraj at gmail.com  Tue May 10 13:26:06 2005
From: tarun30devraj at gmail.com (Tarun Kumar Singh)
Date: Tue, 10 May 2005 16:56:06 +0530
Subject: [R] Running R from Perl program
Message-ID: <e5b0e83205051004266f47a12@mail.gmail.com>

Hi all,

Is it possoble to include an R function in a Perl program? I would like to use 
the "limma" library for microarray analysis, and have no clue how this
can be done.

Thanks in Advance
-Tarun



From buser at stat.math.ethz.ch  Tue May 10 13:39:00 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 10 May 2005 13:39:00 +0200
Subject: [R] filename
In-Reply-To: <42807EE6.8050703@pik-potsdam.de>
References: <42807EE6.8050703@pik-potsdam.de>
Message-ID: <17024.40276.248984.614738@stat.math.ethz.ch>

Hi 

the following works for me:

j <- "box"
z <- "plot"

jpeg(file=paste("YOURPATH",j,z,".jpg", sep = ""))
boxplot(rnorm(100))
dev.off()

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



Lars writes:
 > Hey,
 > 
 > I'm generating a .jpeg file for a web application passing parameters to 
 > R via cgi. R gets the parameters by using commandArgs, which are passed 
 > to a variable named j  and z
 > 
 > j<-commandArgs()[3]
 > z<-commandArgs()[4]
 > 
 > Later I want to use the characters strings of the argument which the 
 > variables are holding for my filename, e.g.:
 > 
 > jpeg(file="d:/data/images/jz.jpeg",...)
 > 
 > i tried to use paste and as.character, but neither worked out the way i 
 > want it to.
 > 
 > thanks for any advice, Lars Claussen
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sdavis2 at mail.nih.gov  Tue May 10 13:39:16 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 10 May 2005 07:39:16 -0400
Subject: [R] Running R from Perl program
In-Reply-To: <e5b0e83205051004266f47a12@mail.gmail.com>
References: <e5b0e83205051004266f47a12@mail.gmail.com>
Message-ID: <38a9111a1419d0ccba4690a24ba6cd17@mail.nih.gov>

A quick search of the archives (http://finzi.psych.upenn.edu/nmz.html) 
produces several hits.  Just one example:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/24327.html

Sean

On May 10, 2005, at 7:26 AM, Tarun Kumar Singh wrote:

> Hi all,
>
> Is it possoble to include an R function in a Perl program? I would 
> like to use
> the "limma" library for microarray analysis, and have no clue how this
> can be done.
>
> Thanks in Advance
> -Tarun
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue May 10 13:41:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 May 2005 12:41:33 +0100 (BST)
Subject: [R] Does R have a command for sending emails?
In-Reply-To: <42807787.6030504@sciviews.org>
References: <10dee46905050920073d17f041@mail.gmail.com>
	<Pine.LNX.4.61.0505100540300.17882@gannet.stats>
	<42807787.6030504@sciviews.org>
Message-ID: <Pine.LNX.4.61.0505101238580.10852@gannet.stats>

On Tue, 10 May 2005, Philippe Grosjean wrote:

> Prof Brian Ripley wrote:
>> On Mon, 9 May 2005, Fernando Saldanha wrote:
>> 
>>> Is there a way to have an R program send an email?
>> 
>> 
>> No.  There have been proposals to do this, but it is highly OS-specific, 
>> and also likely to hit security issues.
>> 
>> You could take a look at what bug.report() does (which is nothing on some 
>> platforms).
>
> ...but on most, if not all systems, you can use command-line applications to 
> send emails... and since R can start command-line applications with system(), 
> you should get that functionnality easily "incorporated" into R. Just use 
> Google a little bit!

Yes, that is what bug.report does.  I don't know of one that is guarenteed 
to exist on Windows: the `blat' that Uwe Ligges mentioned is not standard.
(Windows does have a standard set of system calls, and they are quite 
likely to be firewalled from unknown applications like R.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dirk.enzmann at jura.uni-hamburg.de  Tue May 10 13:44:59 2005
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Tue, 10 May 2005 13:44:59 +0200
Subject: [R] "Source R code" via menu "File"
Message-ID: <42809EBB.9030806@jura.uni-hamburg.de>

If I use the first menu item "Source R code" of the menu "File" to call 
"source()" by searching the *.r files in my directory interactively, a 
window opens showing the active directory, but no files to choose (even 
if there are files with the extension .r). If I start entering the the 
name of a file that I know to be in the active directory, by entering a 
firt letter of the file name a list of files beginning with this letter 
shows up - but not a list of all files with the extension .r.

This happens if I use R version 2.1.0 patched (2005-04-18) running under 
Windows XP (German), the menu item is in German "Lese R Code ein". In 
previous versions of R (e.g. version 2.0.1 patched (2004-11-17)) this 
never happend, i.e. in older versions I could choose a file from a list 
of all files with the extension .r in the current directory.

What has changed? Is this specific to my configuration (of R or of 
Windows) or is it a bug?

R: version 2.1.0 patched (2005-04-18)
Operating system: Windows XP (German)

*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From jan.sabee at gmail.com  Tue May 10 13:49:10 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Tue, 10 May 2005 13:49:10 +0200
Subject: [R] How to handle data continuous to discretized
Message-ID: <96507a8e05051004497ae7d31f@mail.gmail.com>

Dear R-helpers,
Could someone point me to explanation/documentation, is there any
packages to handle datasets which variables contains continuous to
discretized.

I  would be very happy if anyone could help me.
Thank you very much in advance.
Kindly regards,
Jan Sabee



From ligges at statistik.uni-dortmund.de  Tue May 10 13:58:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 10 May 2005 13:58:09 +0200
Subject: [R] "Source R code" via menu "File"
In-Reply-To: <42809EBB.9030806@jura.uni-hamburg.de>
References: <42809EBB.9030806@jura.uni-hamburg.de>
Message-ID: <4280A1D1.2010705@statistik.uni-dortmund.de>

Dirk Enzmann wrote:

> If I use the first menu item "Source R code" of the menu "File" to call 
> "source()" by searching the *.r files in my directory interactively, a 
> window opens showing the active directory, but no files to choose (even 
> if there are files with the extension .r). If I start entering the the 
> name of a file that I know to be in the active directory, by entering a 
> firt letter of the file name a list of files beginning with this letter 
> shows up - but not a list of all files with the extension .r.
> 
> This happens if I use R version 2.1.0 patched (2005-04-18) running under 
> Windows XP (German), the menu item is in German "Lese R Code ein". In 
> previous versions of R (e.g. version 2.0.1 patched (2004-11-17)) this 
> never happend, i.e. in older versions I could choose a file from a list 
> of all files with the extension .r in the current directory.
> 
> What has changed? Is this specific to my configuration (of R or of 
> Windows) or is it a bug?


This is a bug that has been fixed in R-patched.

Uwe Ligges



> R: version 2.1.0 patched (2005-04-18)
> Operating system: Windows XP (German)
> 
> *************************************************
> Dr. Dirk Enzmann
> Institute of Criminal Sciences
> Dept. of Criminology
> Edmund-Siemers-Allee 1
> D-20146 Hamburg
> Germany
> 
> phone: +49-040-42838.7498 (office)
>        +49-040-42838.4591 (Billon)
> fax:   +49-040-42838.2344
> email: dirk.enzmann at jura.uni-hamburg.de
> www: 
> http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue May 10 14:02:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 10 May 2005 14:02:46 +0200
Subject: [R] How to handle data continuous to discretized
In-Reply-To: <96507a8e05051004497ae7d31f@mail.gmail.com>
References: <96507a8e05051004497ae7d31f@mail.gmail.com>
Message-ID: <4280A2E6.5070602@statistik.uni-dortmund.de>

Jan Sabee wrote:

> Dear R-helpers,
> Could someone point me to explanation/documentation, is there any
> packages to handle datasets which variables contains continuous to
> discretized.
> 
> I  would be very happy if anyone could help me.
> Thank you very much in advance.
> Kindly regards,
> Jan Sabee
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


See ?cut, if you mean something different, please be much more specific 
in your question.

Uwe Ligges



From ripley at stats.ox.ac.uk  Tue May 10 14:03:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 May 2005 13:03:07 +0100 (BST)
Subject: [R] "Source R code" via menu "File"
In-Reply-To: <42809EBB.9030806@jura.uni-hamburg.de>
References: <42809EBB.9030806@jura.uni-hamburg.de>
Message-ID: <Pine.LNX.4.61.0505101300550.11140@gannet.stats>

On Tue, 10 May 2005, Dirk Enzmann wrote:

> If I use the first menu item "Source R code" of the menu "File" to call 
> "source()" by searching the *.r files in my directory interactively, a window 
> opens showing the active directory, but no files to choose (even if there are 
> files with the extension .r). If I start entering the the name of a file that 
> I know to be in the active directory, by entering a firt letter of the file 
> name a list of files beginning with this letter shows up - but not a list of 
> all files with the extension .r.
>
> This happens if I use R version 2.1.0 patched (2005-04-18) running under 
> Windows XP (German), the menu item is in German "Lese R Code ein". In 
> previous versions of R (e.g. version 2.0.1 patched (2004-11-17)) this never 
> happend, i.e. in older versions I could choose a file from a list of all 
> files with the extension .r in the current directory.
>
> What has changed? Is this specific to my configuration (of R or of Windows) 
> or is it a bug?

It is a bug in the translation support, and it is fixed in the current 
R-patched (and has been for some time).

> R: version 2.1.0 patched (2005-04-18)  (Which is the day 2.1.0 was 
released, so I would not treat that as preferably to the official 2.1.0.)
> Operating system: Windows XP (German)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From 0034058 at fudan.edu.cn  Mon May  9 03:10:37 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Mon, 09 May 2005 09:10:37 +0800
Subject: [R] question about k in step
Message-ID: <20050509091037.25f07d90@localhost.localdomain>

>?step
.... 
'step' uses 'add1' and 'drop1' repeatedly; it will work for any
     method for which they work, and that is determined by having a
     valid method for 'extractAIC'. When the additive constant can be
     chosen so that AIC is equal to Mallows' Cp, this is done and the
     tables are labelled appropriately.

so my question is :what constant should i choose so i can get Mallows' Cp instead of AIC?



From Achim.Zeileis at wu-wien.ac.at  Tue May 10 14:23:59 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 10 May 2005 14:23:59 +0200
Subject: [R] Aggregate lag
In-Reply-To: <8a83e500050510035513858c8c@mail.gmail.com>
References: <8a83e500050510035513858c8c@mail.gmail.com>
Message-ID: <20050510142359.275ee005.Achim.Zeileis@wu-wien.ac.at>

On Tue, 10 May 2005 12:55:52 +0200 Matthieu Cornec wrote:

> hello,
> 
> Does anybody know how to aggregate a lag series ?
> when I try to use aggregate I get the following message
> 
> > try<-ts(1:100,start=c(1985,1),freq=12)
> > aggregate(try,4,mean,na.rm=T)
>     Qtr1 Qtr2 Qtr3 Qtr4
> 1985    2    5    8   11
> 1986   14   17   20   23
> 1987   26   29   32   35
> 1988   38   41   44   47
> 1989   50   53   56   59
> 1990   62   65   68   71
> 1991   74   77   80   83
> 1992   86   89   92   95
> 1993   98
> > aggregate(lag(try,-1),4,mean,na.rm=T)
> Error in rep.int("", start.pad) : invalid number of copies in rep()

The ts-method seems to expect full blocks of observations. Note, that
also the last observation (100 in April 1993) is dropped from the
aggregate call above. I'm not sure what is the recommended way to
circumvent this problem with "ts": probably, you have to do some padding
with NAs yourself.

Example:
R> x <- ts(1:20,start=c(1990,1),freq=12)
R> aggregate(window(x, start = c(1990, 1), end = c(1991, 9), 
     extend = TRUE), 4, mean, na.rm = TRUE)
     Qtr1 Qtr2 Qtr3 Qtr4
1990  2.0  5.0  8.0 11.0
1991 14.0 17.0 19.5     
R> aggregate(window(lag(x, k = -1), start = c(1990, 1),
     end = c(1991, 9), extend = TRUE), 4, mean, na.rm = TRUE)
     Qtr1 Qtr2 Qtr3 Qtr4
1990  1.5  4.0  7.0 10.0
1991 13.0 16.0 19.0     

In zoo this can be done a bit easier:
R> z <- zooreg(1:20, start = yearmon(1990), freq = 12)
R> aggregate(z, as.yearqtr(time(z)), mean)
1990 Q1 1990 Q2 1990 Q3 1990 Q4 1991 Q1 1991 Q2 1991 Q3 
    2.0     5.0     8.0    11.0    14.0    17.0    19.5 
R> aggregate(lag(z, k = -1), as.yearqtr(time(lag(z, -1))), mean)
1990 Q1 1990 Q2 1990 Q3 1990 Q4 1991 Q1 1991 Q2 1991 Q3 
    1.5     4.0     7.0    10.0    13.0    16.0    19.0 

hth,
Z
 
> Matthieu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From michael.watson at bbsrc.ac.uk  Tue May 10 14:36:19 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 10 May 2005 13:36:19 +0100
Subject: [R] Running R from Perl program
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172D428@iahce2knas1.iah.bbsrc.reserved>

Well, after reviewing all the available options about 2 years ago, I
decided on the traditional perl option of opening up a pipe to R from
within perl and simply firing commands at it.

This worked for me because all I wanted to do was create images.  If you
want to get back the results of an analysis, say from topTable() in
limma, then you can write the results of that out to a temporary file
and read that in using perl.

All very cumbersome, but it works :-)

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
Sent: 10 May 2005 12:39
To: Tarun Kumar Singh
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Running R from Perl program


A quick search of the archives (http://finzi.psych.upenn.edu/nmz.html) 
produces several hits.  Just one example:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/24327.html

Sean

On May 10, 2005, at 7:26 AM, Tarun Kumar Singh wrote:

> Hi all,
>
> Is it possoble to include an R function in a Perl program? I would 
> like to use
> the "limma" library for microarray analysis, and have no clue how this
> can be done.
>
> Thanks in Advance
> -Tarun
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From subianto at gmail.com  Tue May 10 14:37:31 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Tue, 10 May 2005 14:37:31 +0200
Subject: [R] Change the result data
Message-ID: <4280AB0B.30109@gmail.com>

Dear R-helper,
I have a data like:

hec.data <-array(c(5,15,20,68,29,54,84,119,14,14,17,26,16,10,94,7),
               dim=c(4,4),
               dimnames=list(eye=c("Green","Hazel", "Blue", "Brown"),
               hair=c("Black", "Brown", "Red", "Blond")))
as.data.frame(as.table(hec.data))
 >     as.data.frame(as.table(hec.data))
     eye  hair Freq
1  Green Black    5
2  Hazel Black   15
3   Blue Black   20
4  Brown Black   68
5  Green Brown   29
6  Hazel Brown   54
7   Blue Brown   84
8  Brown Brown  119
9  Green   Red   14
10 Hazel   Red   14
11  Blue   Red   17
12 Brown   Red   26
13 Green Blond   16
14 Hazel Blond   10
15  Blue Blond   94
16 Brown Blond    7
 >
    
I want to extract like,
Green Black
Green Black
Green Black
Green Black
Green Black
Hazel Black
Hazel Black
Hazel Black
Hazel Black
.
.
.
Brown Blond
Brown Blond
Brown Blond
Brown Blond
Brown Blond
Brown Blond
Brown Blond

How can I do it.
Thanks you for your help.

Best regards,
Muhammad Subianto
http://article.gmane.org/gmane.comp.lang.r.general/14604,14610



From f.harrell at vanderbilt.edu  Tue May 10 14:48:22 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 10 May 2005 08:48:22 -0400
Subject: [R] Does R have a command for sending emails?
In-Reply-To: <10dee46905050920073d17f041@mail.gmail.com>
References: <10dee46905050920073d17f041@mail.gmail.com>
Message-ID: <4280AD96.3070308@vanderbilt.edu>

Fernando Saldanha wrote:
> Is there a way to have an R program send an email?
> 
> Something like this:
> 
> address <- 'abc at d.com'
> text <- 'This is the email body'
> send.email(address, text)
> 
> Thanks.
> 
> FS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

Under Linux/Unix you can use code such as the following.  This handles 
kmail and mail.

     if(mailer=='kmail') {
       tf <- tempfile()
       cat(cmd, file=tf)
       to <- paste('"', paste(to, collapse=','), '"', sep='')
       if(length(cc)) cc <- paste(' -c "', paste(cc, 
collapse=','),'"',sep='')
       if(length(bcc)) bcc <- paste(' -b "', paste(bcc, 
collapse=','),'"',sep='')
     } else {
       to <- paste(to, collapse=' ')
       if(length(cc))  cc  <- paste(paste(' -c', cc), collapse='')
       if(length(bcc)) bcc <- paste(paste(' -b', bcc),collapse='')
     }
     cmd <- if(mailer=='kmail') paste('kmail -s "', title, '"', cc,
                 bcc, ' --msg ', tf, ' ', to, sep='') else
       paste('echo -e "', cmd, '" | mail -s "',
             title, ' Reports"', cc, bcc, ' ', to, sep='')
     system(cmd)


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ripley at stats.ox.ac.uk  Tue May 10 15:23:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 May 2005 14:23:06 +0100 (BST)
Subject: [R] question about k in step
In-Reply-To: <20050509091037.25f07d90@localhost.localdomain>
References: <20050509091037.25f07d90@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0505101421240.12508@gannet.stats>

On Mon, 9 May 2005, ronggui wrote:

>> ?step
> ....
> 'step' uses 'add1' and 'drop1' repeatedly; it will work for any
>     method for which they work, and that is determined by having a
>     valid method for 'extractAIC'. When the additive constant can be
>     chosen so that AIC is equal to Mallows' Cp, this is done and the
>     tables are labelled appropriately.
>
> so my question is :what constant should i choose so i can get Mallows' 
> Cp instead of AIC?

k=2, that is AIC.  Under some circumstances Cp = AIC+const (as it says), 
and then the const is chosen so that AIC (which is only determined up to a 
const) is equal to Cp (which is defined exactly, at least by Mallows).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fsaldan1 at gmail.com  Tue May 10 15:27:38 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Tue, 10 May 2005 09:27:38 -0400
Subject: [R] Does R have a command for sending emails?
In-Reply-To: <4280AD96.3070308@vanderbilt.edu>
References: <10dee46905050920073d17f041@mail.gmail.com>
	<4280AD96.3070308@vanderbilt.edu>
Message-ID: <10dee46905051006272a1e3ec2@mail.gmail.com>

I want to thank all who have offered help on this topic. I was able to
create a very simple email function that I have tested to work under
Windows XP Professional and R 2.1.0. It uses Blat version 1.9.4.

send.mail<-function(addr, subject, source.file) {
  mail.cmd <- paste("Blat", source.file, "-subject", dQuote(subject),
"-to", addr, separator = " ", collapse = "")
  
  system(mail.cmd, intern = FALSE)
}

The string source.file must have double backslashes instead of single
backslashes. For example:

C:\\myfolder

One must first install Blat version 1.9.4, available at 

http://www.blat.net/194/.

All that is needed is to unzip the downloaded file (Blat194.zip) and
copy Blat.exe to a folder in the path. The other files inside
Blat194.zip can be discarded.

FS

On 5/10/05, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> Fernando Saldanha wrote:
> > Is there a way to have an R program send an email?
> >
> > Something like this:
> >
> > address <- 'abc at d.com'
> > text <- 'This is the email body'
> > send.email(address, text)
> >
> > Thanks.
> >
> > FS
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> Under Linux/Unix you can use code such as the following.  This handles
> kmail and mail.
> 
>      if(mailer=='kmail') {
>        tf <- tempfile()
>        cat(cmd, file=tf)
>        to <- paste('"', paste(to, collapse=','), '"', sep='')
>        if(length(cc)) cc <- paste(' -c "', paste(cc,
> collapse=','),'"',sep='')
>        if(length(bcc)) bcc <- paste(' -b "', paste(bcc,
> collapse=','),'"',sep='')
>      } else {
>        to <- paste(to, collapse=' ')
>        if(length(cc))  cc  <- paste(paste(' -c', cc), collapse='')
>        if(length(bcc)) bcc <- paste(paste(' -b', bcc),collapse='')
>      }
>      cmd <- if(mailer=='kmail') paste('kmail -s "', title, '"', cc,
>                  bcc, ' --msg ', tf, ' ', to, sep='') else
>        paste('echo -e "', cmd, '" | mail -s "',
>              title, ' Reports"', cc, bcc, ' ', to, sep='')
>      system(cmd)
> 
> --
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
>



From bates at stat.wisc.edu  Tue May 10 15:33:08 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 10 May 2005 08:33:08 -0500
Subject: [R] predict nlme syntax
In-Reply-To: <4280B635.2332.13F257A@localhost>
References: <4280B635.2332.13F257A@localhost>
Message-ID: <4280B814.6000308@stat.wisc.edu>

Petr Pikal wrote:
> Dear all
> 
> Please help me with correct syntax of predict.nlme.
> I would like to predict from nlme object for new data.
> I used predict(fit.nlme6, data=newdata) but I have always got 
> fitted values, no matter how I changed newdata.
> 
> I have

The argument's name is newdata, not data.



From zwu at jhsph.edu  Tue May 10 15:38:17 2005
From: zwu at jhsph.edu (Zhijin Wu)
Date: Tue, 10 May 2005 09:38:17 -0400 (EDT)
Subject: [R] use "integrate" for functions defined in C, not R
In-Reply-To: <Pine.LNX.4.61.0505100544290.17882@gannet.stats>
Message-ID: <Pine.GSO.4.10.10505100927390.29707-100000@athena.biostat.jhsph.edu>


> 1) This is the wrong list: please read the posting guide.
> 
> 2) You cannot just leave out arguments in C calls, so it seems that you 
> need help with C programming rather than R.  If you include the 
> appropriate headers this will be checked, so I guess you have not.

Dear Dr. Ripley,
  Thank you for the reply. I was wondering maybe someone has tried
similar things using the C code in R base.   
  I included the same header as the original code "integrate.c", except 
#include <R_ext/Applic.h> has been changed to "Myapplic.h" to use my
definition of 
typedef void integr_fn(double *x, int n). 

I did not simply leave out arguments in the C calls, but redefined all
functions involving "environment" with one less parameter. 

Thank you.

regards,
Jean



From yushengl at email.unc.edu  Tue May 10 15:48:52 2005
From: yushengl at email.unc.edu (yushengl@email.unc.edu)
Date: Tue, 10 May 2005 09:48:52 -0400
Subject: [R] (no subject)
Message-ID: <20050510094852.li47bayq04wg4484@webmail2.isis.unc.edu>

Dear All

If I want to adjusted plot, like

Say I want to see Y ~ lo(X1, span=0.75, df=2) | X2 X3?

Is there any S-Plus function that I can do? or cr.plots or av.plots in R could
do it.


Many thanks.


YS



From dave at kanecap.com  Tue May 10 16:03:51 2005
From: dave at kanecap.com (David Kane)
Date: Tue, 10 May 2005 10:03:51 -0400
Subject: [R] summary statistics for lists of matrices or dataframes
Message-ID: <17024.48967.299990.712469@gargle.gargle.HOWL>

Is there a simple way to calculate summary statistics for all the
matrices or dataframes in a list? For example:

> z <- list(matrix(c(2,2,2,2), ncol = 2), matrix(c(4,4,4,4), ncol = 2))
> z
[[1]]
     [,1] [,2]
[1,]    2    2
[2,]    2    2

[[2]]
     [,1] [,2]
[1,]    4    4
[2,]    4    4
>

I would like to calculate, for example, the mean value for each
cell. I can do that the hard way as:

> (z[[1]] + z[[2]]) / 2
     [,1] [,2]
[1,]    3    3
[2,]    3    3
> 

But there must be an easier way. I am also interested in other
statistics (like median and sd). Since all my matrices have the same
attributes (especially row and column names), I would like to preserve
those in the answer.

Thanks,

Dave Kane

In case it matters:

> R.version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R                
>



From Robert.McGehee at geodecapital.com  Tue May 10 16:18:38 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Tue, 10 May 2005 10:18:38 -0400
Subject: [R] summary statistics for lists of matrices or dataframes
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C1ED807@MSGBOSCLB2WIN.DMN1.FMR.COM>

You could try temporarily switching the list to an array, then just run
an apply on rows and columns:

> apply(array(do.call("cbind", z), c(dim(z[[1]]), length(z))), c(1, 2),
mean)

Best,
Robert

-----Original Message-----
From: David Kane [mailto:dave at kanecap.com] 
Sent: Tuesday, May 10, 2005 10:04 AM
To: r-help at stat.math.ethz.ch
Subject: [R] summary statistics for lists of matrices or dataframes


Is there a simple way to calculate summary statistics for all the
matrices or dataframes in a list? For example:

> z <- list(matrix(c(2,2,2,2), ncol = 2), matrix(c(4,4,4,4), ncol = 2))
> z
[[1]]
     [,1] [,2]
[1,]    2    2
[2,]    2    2

[[2]]
     [,1] [,2]
[1,]    4    4
[2,]    4    4
>

I would like to calculate, for example, the mean value for each
cell. I can do that the hard way as:

> (z[[1]] + z[[2]]) / 2
     [,1] [,2]
[1,]    3    3
[2,]    3    3
> 

But there must be an easier way. I am also interested in other
statistics (like median and sd). Since all my matrices have the same
attributes (especially row and column names), I would like to preserve
those in the answer.

Thanks,

Dave Kane

In case it matters:

> R.version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R                
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue May 10 16:36:38 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 10 May 2005 16:36:38 +0200
Subject: [R] summary statistics for lists of matrices or dataframes
References: <17024.48967.299990.712469@gargle.gargle.HOWL>
Message-ID: <00ac01c5556d$ac5b7bc0$0540210a@www.domain>

Hi Dave,

maybe you can find these functions useful:


matSums <- function(lis){
    out <- array(data=0., dim=dim(lis[[1]]))
    for(i in seq(along=lis)) out <- out + lis[[i]]
    out
}
##
matMeans <- function(lis) matSums(lis) / length(lis)
##
matFun <- function(lis, FUN, ...){
    if(!is.list(lis) || !all(sapply(lis, is.matrix))) stop("'lis' must 
be a list containing 2-dimensional arrays")
    dims <- sapply(lis, dim)
    n <- dims[1, 1]
    p <- dims[2, 1]
    if(!all(n==dims[1,]) || !all(p==dims[2,])) stop("the matrices must 
have the same dimensions")
    out <- apply(matrix(unlist(lis), n * p, length(lis)), 1, FUN, ...)
    dim(out) <- c(n, p)
    out
}
# The first two are faster than "matFun(lis, sum)" or "matFun(lis, 
mean)"
# for large and many matrices
#############

matFun <- function(lis, FUN, ...){
    if(!is.list(lis) || !all(sapply(lis, is.matrix))) stop("'lis' must 
be a list containing 2-dimensional arrays")
    dims <- sapply(lis, dim)
    n <- dims[1, 1]
    p <- dims[2, 1]
    if(!all(n==dims[1,]) || !all(p==dims[2,])) stop("the matrices must 
have the same dimensions")
    out <- apply(matrix(unlist(lis), n*p, length(lis)), 1, FUN, ...)
    dim(out) <- c(n, p)
    out
}
###########
lis <- list(matrix(c(2,2,2,2), ncol = 2), matrix(c(4,4,4,4), ncol = 
2), matrix(c(5,5,5,5), ncol=2))

matSums(lis)
matFun(lis, sum)

matMeans(lis)
matFun(lis, mean)

matFun(lis, median)
matFun(lis, sd)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm




----- Original Message ----- 
From: "David Kane" <dave at kanecap.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, May 10, 2005 4:03 PM
Subject: [R] summary statistics for lists of matrices or dataframes


> Is there a simple way to calculate summary statistics for all the
> matrices or dataframes in a list? For example:
>
>> z <- list(matrix(c(2,2,2,2), ncol = 2), matrix(c(4,4,4,4), ncol = 
>> 2))
>> z
> [[1]]
>     [,1] [,2]
> [1,]    2    2
> [2,]    2    2
>
> [[2]]
>     [,1] [,2]
> [1,]    4    4
> [2,]    4    4
>>
>
> I would like to calculate, for example, the mean value for each
> cell. I can do that the hard way as:
>
>> (z[[1]] + z[[2]]) / 2
>     [,1] [,2]
> [1,]    3    3
> [2,]    3    3
>>
>
> But there must be an easier way. I am also interested in other
> statistics (like median and sd). Since all my matrices have the same
> attributes (especially row and column names), I would like to 
> preserve
> those in the answer.
>
> Thanks,
>
> Dave Kane
>
> In case it matters:
>
>> R.version
>         _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From pburns at pburns.seanet.com  Tue May 10 16:49:38 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 10 May 2005 15:49:38 +0100
Subject: [R] summary statistics for lists of matrices or dataframes
In-Reply-To: <17024.48967.299990.712469@gargle.gargle.HOWL>
References: <17024.48967.299990.712469@gargle.gargle.HOWL>
Message-ID: <4280CA02.5040306@pburns.seanet.com>

You could use 'do.call' with 'bind.array' (from S Poetry) or 'abind'
to convert your list of matrices into a three-dimensional array.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

David Kane wrote:

>Is there a simple way to calculate summary statistics for all the
>matrices or dataframes in a list? For example:
>
>  
>
>>z <- list(matrix(c(2,2,2,2), ncol = 2), matrix(c(4,4,4,4), ncol = 2))
>>z
>>    
>>
>[[1]]
>     [,1] [,2]
>[1,]    2    2
>[2,]    2    2
>
>[[2]]
>     [,1] [,2]
>[1,]    4    4
>[2,]    4    4
>  
>
>
>I would like to calculate, for example, the mean value for each
>cell. I can do that the hard way as:
>
>  
>
>>(z[[1]] + z[[2]]) / 2
>>    
>>
>     [,1] [,2]
>[1,]    3    3
>[2,]    3    3
>  
>
>
>But there must be an easier way. I am also interested in other
>statistics (like median and sd). Since all my matrices have the same
>attributes (especially row and column names), I would like to preserve
>those in the answer.
>
>Thanks,
>
>Dave Kane
>
>In case it matters:
>
>  
>
>>R.version
>>    
>>
>         _                
>platform i686-pc-linux-gnu
>arch     i686             
>os       linux-gnu        
>system   i686, linux-gnu  
>status                    
>major    2                
>minor    1.0              
>year     2005             
>month    04               
>day      18               
>language R                
>  
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From pburns at pburns.seanet.com  Tue May 10 16:56:27 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 10 May 2005 15:56:27 +0100
Subject: [R] summary statistics for lists of matrices or dataframes
In-Reply-To: <4280CA02.5040306@pburns.seanet.com>
References: <17024.48967.299990.712469@gargle.gargle.HOWL>
	<4280CA02.5040306@pburns.seanet.com>
Message-ID: <4280CB9B.8010204@pburns.seanet.com>

No, you can't.  Because 'bind.array' doesn't take an arbitrary number
of arguments.  Robert's solution does what I had in mind.

Patrick Burns wrote:

> You could use 'do.call' with 'bind.array' (from S Poetry) or 'abind'
> to convert your list of matrices into a three-dimensional array.
>
> Patrick Burns
>
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> David Kane wrote:
>
>> Is there a simple way to calculate summary statistics for all the
>> matrices or dataframes in a list? For example:
>>
>>  
>>
>>> z <- list(matrix(c(2,2,2,2), ncol = 2), matrix(c(4,4,4,4), ncol = 2))
>>> z
>>>   
>>
>> [[1]]
>>     [,1] [,2]
>> [1,]    2    2
>> [2,]    2    2
>>
>> [[2]]
>>     [,1] [,2]
>> [1,]    4    4
>> [2,]    4    4
>>  
>>
>>
>> I would like to calculate, for example, the mean value for each
>> cell. I can do that the hard way as:
>>
>>  
>>
>>> (z[[1]] + z[[2]]) / 2
>>>   
>>
>>     [,1] [,2]
>> [1,]    3    3
>> [2,]    3    3
>>  
>>
>>
>> But there must be an easier way. I am also interested in other
>> statistics (like median and sd). Since all my matrices have the same
>> attributes (especially row and column names), I would like to preserve
>> those in the answer.
>>
>> Thanks,
>>
>> Dave Kane
>>
>> In case it matters:
>>
>>  
>>
>>> R.version
>>>   
>>
>>         _                platform i686-pc-linux-gnu
>> arch     i686             os       linux-gnu        system   i686, 
>> linux-gnu  status                    major    2                
>> minor    1.0              year     2005             month    
>> 04               day      18               language R                 
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>>  
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>



From reid_huntsinger at merck.com  Tue May 10 16:42:06 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 10 May 2005 10:42:06 -0400
Subject: [R] summary statistics for lists of matrices or dataframes
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A942B@uswpmx00.merck.com>

In such a situation you could make the list into a 3-way array. For example,

> d <- dim(z[[1]]) # all the same
> n <- length(z)
> z <- unlist(z)
> dim(z) <- c(d,n)

then you can get summary statistics for the (1,1) entries of the original
z[[1]], z[[2]] etc however you would get summary statistics for the vector
z[1,1,]. Moreover, you can vectorize this to get, say, 

> rowMeans(z, dims=2)
     [,1] [,2]
[1,]    3    3
[2,]    3    3

and for arbitrary functions of the entries you can use apply():

> apply(z, c(1,2), "median")

Reid Huntsinger



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Kane
Sent: Tuesday, May 10, 2005 10:04 AM
To: r-help at stat.math.ethz.ch
Subject: [R] summary statistics for lists of matrices or dataframes


Is there a simple way to calculate summary statistics for all the
matrices or dataframes in a list? For example:

> z <- list(matrix(c(2,2,2,2), ncol = 2), matrix(c(4,4,4,4), ncol = 2))
> z
[[1]]
     [,1] [,2]
[1,]    2    2
[2,]    2    2

[[2]]
     [,1] [,2]
[1,]    4    4
[2,]    4    4
>

I would like to calculate, for example, the mean value for each
cell. I can do that the hard way as:

> (z[[1]] + z[[2]]) / 2
     [,1] [,2]
[1,]    3    3
[2,]    3    3
> 

But there must be an easier way. I am also interested in other
statistics (like median and sd). Since all my matrices have the same
attributes (especially row and column names), I would like to preserve
those in the answer.

Thanks,

Dave Kane

In case it matters:

> R.version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R                
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From OlsenN at pac.dfo-mpo.gc.ca  Tue May 10 17:39:27 2005
From: OlsenN at pac.dfo-mpo.gc.ca (OlsenN@pac.dfo-mpo.gc.ca)
Date: Tue, 10 May 2005 08:39:27 -0700
Subject: [R] Does R have a command for sending emails?
Message-ID: <7CBBD627E4E688499349A5D11D07831602ECB778@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>

At the risk of beating this to death ... if you use Outlook mail on Windows,
you can create a simple 'sendmail' vbscript:

' ==== start ===
Dim pOutlook, pMail, fso, f

Set pOutlook = CreateObject("Outlook.Application")
Set pMail = pOutlook.CreateItem(olMailItem)
Set fso = CreateObject("Scripting.FileSystemObject")
Set f = fso.OpenTextFile(WScript.Arguments(2), 1)

With pMail
   .To = WScript.Arguments(0)
   .Subject = WScript.Arguments(1)
   .Body = f.ReadAll()
   .Send
End With

f.Close()
' ==== end ====

And then call this from R:

send.mail <- function(addr, subject, source.file) {
   mail.cmd <-
paste(paste(Sys.getenv("SystemRoot"),"/system32/wscript.exe",sep=""),
      "sendmail.vbs", addr, dQuote(subject), source.file, sep=" ")
      
   system(mail.cmd, intern=F)
}

Note that if sendmail.vbs is not in the current R working directory you have
to provide a full path.
Norm Olsen 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fernando Saldanha
Sent: Tuesday, May 10, 2005 6:28 AM
To: Submissions to R help
Subject: Re: [R] Does R have a command for sending emails?

I want to thank all who have offered help on this topic. I was able to
create a very simple email function that I have tested to work under Windows
XP Professional and R 2.1.0. It uses Blat version 1.9.4.

send.mail<-function(addr, subject, source.file) {
  mail.cmd <- paste("Blat", source.file, "-subject", dQuote(subject), "-to",
addr, separator = " ", collapse = "")
  
  system(mail.cmd, intern = FALSE)
}

The string source.file must have double backslashes instead of single
backslashes. For example:

C:\\myfolder

One must first install Blat version 1.9.4, available at 

http://www.blat.net/194/.

All that is needed is to unzip the downloaded file (Blat194.zip) and copy
Blat.exe to a folder in the path. The other files inside Blat194.zip can be
discarded.

FS

On 5/10/05, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> Fernando Saldanha wrote:
> > Is there a way to have an R program send an email?
> >
> > Something like this:
> >
> > address <- 'abc at d.com'
> > text <- 'This is the email body'
> > send.email(address, text)
> >
> > Thanks.
> >
> > FS
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> Under Linux/Unix you can use code such as the following.  This handles 
> kmail and mail.
> 
>      if(mailer=='kmail') {
>        tf <- tempfile()
>        cat(cmd, file=tf)
>        to <- paste('"', paste(to, collapse=','), '"', sep='')
>        if(length(cc)) cc <- paste(' -c "', paste(cc,
> collapse=','),'"',sep='')
>        if(length(bcc)) bcc <- paste(' -b "', paste(bcc,
> collapse=','),'"',sep='')
>      } else {
>        to <- paste(to, collapse=' ')
>        if(length(cc))  cc  <- paste(paste(' -c', cc), collapse='')
>        if(length(bcc)) bcc <- paste(paste(' -b', bcc),collapse='')
>      }
>      cmd <- if(mailer=='kmail') paste('kmail -s "', title, '"', cc,
>                  bcc, ' --msg ', tf, ' ', to, sep='') else
>        paste('echo -e "', cmd, '" | mail -s "',
>              title, ' Reports"', cc, bcc, ' ', to, sep='')
>      system(cmd)
> 
> --
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From michael.graber at mail.uni-wuerzburg.de  Tue May 10 17:45:03 2005
From: michael.graber at mail.uni-wuerzburg.de (Michael Graber)
Date: Tue, 10 May 2005 17:45:03 +0200
Subject: [R] converting an ASCII file to a matrix
Message-ID: <4280D6FF.2060308@mail.uni-wuerzburg.de>

Dear R-WinEdit users,

I got a simple question, but somehow I cannot find the answer, although 
I have
tried a lot!
I got an ASCII-file and I want to import it into R, so that every
character is defined by [i;j].
The rows are not of the same length.

the file looks like the following shortened abstract example:

name: xxxxx xxxx
age: 9.9.99
record number: 999
title: xxxxx xxxx xxx
keywords: xxx xx

"white space"

name: yyyy yyyyyyyyyyyy
age: 8.8.88
record number: 8
title: yyyy yy yyyy
keywords: yyyyyyyyyyy yyyyyyyy yyy

"white space"
I would be very grateful for your help!

Michael Graber
michael.graber at mail.uni-wuerzburg.de



From michael.graber at mail.uni-wuerzburg.de  Tue May 10 17:58:14 2005
From: michael.graber at mail.uni-wuerzburg.de (Michael Graber)
Date: Tue, 10 May 2005 17:58:14 +0200
Subject: [R] converting an ASCII file to a matrix
Message-ID: <4280DA16.1060007@mail.uni-wuerzburg.de>

Dear R-WinEdit users,

I have a simple question, but somehow I cannot find the answer even
though I tried a lot!

I have an unstructured ASCII-file and I want to import it into a matrix
m in R, so that every character is defined by m[i;j]. The rows are not
of the same length.

The file looks like the following shortened abstract example:

name: xxxxx xxxx
age: 9.9.99
record number: 999
title: xxxxx xxxx xxx
keywords: xxx xx

"white space"

name: yyyy yyyyyyyyyyyy
age: 8.8.88
record number: 8
title: yyyy yy yyyy
keywords: yyyyyyyyyyy yyyyyyyy yyy

"white space"

The result should be for example: m[1;1]=n
 
I would be very grateful for your help!

Michael Graber
michael.graber at mail.uni-wuerzburg.de



From gunter.berton at gene.com  Tue May 10 18:07:20 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 10 May 2005 09:07:20 -0700
Subject: [R] converting an ASCII file to a matrix
In-Reply-To: <4280D6FF.2060308@mail.uni-wuerzburg.de>
Message-ID: <200505101607.j4AG7Kq7009048@hertz.gene.com>

Michael:

Ah ... the bane of real data analysts everywhere: getting the data from its
original format into (R )- usable form for data analysis 

This has nothing to do with R-WinEdit, AFAICS.

My approach would be to simply use readLines() to read the lines in as
character strings and then process them by grep and/or regexpr() to extract
the bits I wanted. If the formatting is fixed, substring() may also be
useful. You will also need to convert the resulting character representation
of numerics to numerics via as.numeric().

If you haven't worked through regular expressions before (?regexp), you will
find this a bit of a chore; but it is well worth the effort, as they are
invaluable for this sort of thing. There are numerous web tutorials to help
(google on 'regular expressions').

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michael Graber
> Sent: Tuesday, May 10, 2005 8:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] converting an ASCII file to a matrix
> 
> Dear R-WinEdit users,
> 
> I got a simple question, but somehow I cannot find the 
> answer, although 
> I have
> tried a lot!
> I got an ASCII-file and I want to import it into R, so that every
> character is defined by [i;j].
> The rows are not of the same length.
> 
> the file looks like the following shortened abstract example:
> 
> name: xxxxx xxxx
> age: 9.9.99
> record number: 999
> title: xxxxx xxxx xxx
> keywords: xxx xx
> 
> "white space"
> 
> name: yyyy yyyyyyyyyyyy
> age: 8.8.88
> record number: 8
> title: yyyy yy yyyy
> keywords: yyyyyyyyyyy yyyyyyyy yyy
> 
> "white space"
> I would be very grateful for your help!
> 
> Michael Graber
> michael.graber at mail.uni-wuerzburg.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From lxiaolei at stat.wisc.edu  Tue May 10 18:11:23 2005
From: lxiaolei at stat.wisc.edu (Li, Xiaolei)
Date: Tue, 10 May 2005 11:11:23 -0500 (CDT)
Subject: [R] Re: Welcome to the "R-help" mailing list
In-Reply-To: <Pine.LNX.4.44.0505100905370.6209-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0505100905370.6209-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.58.0505101111080.27169@chi51.stat.wisc.edu>




Thanks for the reply!

Xiaolei

On Tue, 10 May 2005, Roger Bivand wrote:

> On Tue, 10 May 2005, Li, Xiaolei wrote:
>
> Yes, welcome, and please note the clear instructions for users in the
> posting guide, to wit:  give an informative subject, and read the
> necessary documentation first.
>
> In your case, just accessing the "Packages" link in the navigation bar of
> the CRAN website would take you straight to the source package you are
> interested in. As the name suggests, source packages contain the source
> code for a package. The posting guide also suggests (firmly) that
> questions about packages should be addressed to their maintainers.
>
> As the maintainer of spdep, I may be able to respond to motivated
> questions off-list, but I doubt that your approach is going to be
> fruitful; there are usually better ways of doing things than writing C++,
> in fact writing R can be just as productive, and is much easier to debug.
>
> >
> > I am trying to pass some R objects to C++ code. However, after reading
> > through the mannual "Writing R extensioms" for version 2.1.0
> > (2005-04-18),
> > I still couldn't figure out:
> >
> > 1) How can I see .c files in a package? For example, I am interested in
> > looking at source files in "Spdep".
> >
> > 2) Can I include whichever header files I find in my C++ code, if I
> > eventually will load the C++ code into R?
> >
> > 3) If yes to 2), where are header files of packages such as "Spdep"?
> >
> > 4) Any written code to handle matrices in this interface with C++?
> >
> > Thanks
> >
> > Xiaolei
> >
> > ______________________________________
> > Xiaolei Li
> >
> > Department of Statistics
> > University of Wisconsin-Madison
> > 1130A Medical Science Center
> > 1300 University Avenue
> > Madison, WI 53706
> > USA
> >
> > Office: (608)265-6217
> > Email: lxiaolei at stat.wisc.edu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> --
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
> e-mail: Roger.Bivand at nhh.no
>
>

______________________________________
Xiaolei Li

Department of Statistics
University of Wisconsin-Madison
1130A Medical Science Center
1300 University Avenue
Madison, WI 53706
USA

Office: (608)265-6217
Email: lxiaolei at stat.wisc.edu



From ligges at statistik.uni-dortmund.de  Tue May 10 18:23:07 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 10 May 2005 18:23:07 +0200
Subject: [R] converting an ASCII file to a matrix
In-Reply-To: <4280DA16.1060007@mail.uni-wuerzburg.de>
References: <4280DA16.1060007@mail.uni-wuerzburg.de>
Message-ID: <4280DFEB.3050909@statistik.uni-dortmund.de>

Michael Graber wrote:
> Dear R-WinEdit users,

a) What is an R-WinEdit user?

b) I guess you mean R-WinEdt (without an i) implying the plug-in for the 
WinEdt editor? WinEdit is another editor that does not support R very 
closely, AFAIK.

c) The following questions are completely unrelated to any editor, so 
why do you ask only a very small (empty?) subset of the R community?


> I have a simple question, but somehow I cannot find the answer even
> though I tried a lot!
> 
> I have an unstructured ASCII-file and I want to import it into a matrix
> m in R, so that every character is defined by m[i;j]. The rows are not

d) What does m[i;j] mean? If we are speaking R, I guess you mean m[i,j]?


> of the same length.
> 
> The file looks like the following shortened abstract example:
> 
> name: xxxxx xxxx
> age: 9.9.99
> record number: 999
> title: xxxxx xxxx xxx
> keywords: xxx xx
> 
> "white space"
> 
> name: yyyy yyyyyyyyyyyy
> age: 8.8.88
> record number: 8
> title: yyyy yy yyyy
> keywords: yyyyyyyyyyy yyyyyyyy yyy
> 
> "white space"
> 
> The result should be for example: m[1;1]=n

So what aboutreading all lines, and storing separate characters as 
vectors in a list using strsplit().
   L <- strsplit(readLines(filename), "")
   L[[i]][j]

A matrix seems to be the wrong way with unequal line lengths.

Uwe Ligges


> I would be very grateful for your help!
> 
> Michael Graber
> michael.graber at mail.uni-wuerzburg.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue May 10 18:25:19 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 10 May 2005 18:25:19 +0200
Subject: [R] converting an ASCII file to a matrix
In-Reply-To: <4280DFEB.3050909@statistik.uni-dortmund.de>
References: <4280DA16.1060007@mail.uni-wuerzburg.de>
	<4280DFEB.3050909@statistik.uni-dortmund.de>
Message-ID: <4280E06F.5040103@statistik.uni-dortmund.de>

Uwe Ligges wrote:

> Michael Graber wrote:
> 
>> Dear R-WinEdit users,
> 
> 
> a) What is an R-WinEdit user?
> 
> b) I guess you mean R-WinEdt (without an i) implying the plug-in for the 
> WinEdt editor? WinEdit is another editor that does not support R very 
> closely, AFAIK.
> 
> c) The following questions are completely unrelated to any editor, so 
> why do you ask only a very small (empty?) subset of the R community?
> 
> 
>> I have a simple question, but somehow I cannot find the answer even
>> though I tried a lot!
>>
>> I have an unstructured ASCII-file and I want to import it into a matrix
>> m in R, so that every character is defined by m[i;j]. The rows are not
> 
> 
> d) What does m[i;j] mean? If we are speaking R, I guess you mean m[i,j]?
> 
> 
>> of the same length.
>>
>> The file looks like the following shortened abstract example:
>>
>> name: xxxxx xxxx
>> age: 9.9.99
>> record number: 999
>> title: xxxxx xxxx xxx
>> keywords: xxx xx
>>
>> "white space"
>>
>> name: yyyy yyyyyyyyyyyy
>> age: 8.8.88
>> record number: 8
>> title: yyyy yy yyyy
>> keywords: yyyyyyyyyyy yyyyyyyy yyy
>>
>> "white space"
>>
>> The result should be for example: m[1;1]=n
> 
> 
> So what aboutreading all lines, and storing separate characters as 
> vectors in a list using strsplit().
>   L <- strsplit(readLines(filename), "")
>   L[[i]][j]
> 
> A matrix seems to be the wrong way with unequal line lengths.


Let me add, what about reading it in using read.dcf(), a function that 
is designed for the data specified above!
And much more appropriate than looking at single characters, I think.

Uwe Ligges



> Uwe Ligges
> 
> 
>> I would be very grateful for your help!
>>
>> Michael Graber
>> michael.graber at mail.uni-wuerzburg.de
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
>



From OlsenN at pac.dfo-mpo.gc.ca  Tue May 10 18:26:14 2005
From: OlsenN at pac.dfo-mpo.gc.ca (OlsenN@pac.dfo-mpo.gc.ca)
Date: Tue, 10 May 2005 09:26:14 -0700
Subject: [R] converting an ASCII file to a matrix
Message-ID: <7CBBD627E4E688499349A5D11D07831602ECB77E@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>

This seems to work but it's a bit ugly with the loop (I'm sure you could
replace the loop with "apply").

asc2mat <- function(fname) {
  x <- sapply(scan(fname, "character", sep="\n"), strsplit, "")
  rlen <- sapply(x, length)
  res <- matrix(nrow=length(bar), ncol=max(rlen))
  
  for (i in 1:nrow(res)) {
     res[i,1:rlen[i]] <- x[[i]]
  }
  return(res)
}
 
Norm Olsen 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
Sent: Tuesday, May 10, 2005 9:07 AM
To: 'Michael Graber'; r-help at stat.math.ethz.ch
Subject: RE: [R] converting an ASCII file to a matrix

Michael:

Ah ... the bane of real data analysts everywhere: getting the data from its
original format into (R )- usable form for data analysis 

This has nothing to do with R-WinEdit, AFAICS.

My approach would be to simply use readLines() to read the lines in as
character strings and then process them by grep and/or regexpr() to extract
the bits I wanted. If the formatting is fixed, substring() may also be
useful. You will also need to convert the resulting character representation
of numerics to numerics via as.numeric().

If you haven't worked through regular expressions before (?regexp), you will
find this a bit of a chore; but it is well worth the effort, as they are
invaluable for this sort of thing. There are numerous web tutorials to help
(google on 'regular expressions').

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michael Graber
> Sent: Tuesday, May 10, 2005 8:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] converting an ASCII file to a matrix
> 
> Dear R-WinEdit users,
> 
> I got a simple question, but somehow I cannot find the answer, 
> although I have tried a lot!
> I got an ASCII-file and I want to import it into R, so that every 
> character is defined by [i;j].
> The rows are not of the same length.
> 
> the file looks like the following shortened abstract example:
> 
> name: xxxxx xxxx
> age: 9.9.99
> record number: 999
> title: xxxxx xxxx xxx
> keywords: xxx xx
> 
> "white space"
> 
> name: yyyy yyyyyyyyyyyy
> age: 8.8.88
> record number: 8
> title: yyyy yy yyyy
> keywords: yyyyyyyyyyy yyyyyyyy yyy
> 
> "white space"
> I would be very grateful for your help!
> 
> Michael Graber
> michael.graber at mail.uni-wuerzburg.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pharma at jsbintelligence.com  Tue May 10 18:51:57 2005
From: pharma at jsbintelligence.com (JSB Intelligence)
Date: Tue, 10 May 2005 12:51:57 -0400
Subject: [R] Re: 80-page Pharma complimentary report-Emerging Business
	models in Pharma
Message-ID: <3003491-220055210165157578@bmesrv15.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050510/cf342a9c/attachment.pl

From service at paypal.de  Tue May 10 20:39:59 2005
From: service at paypal.de (service@paypal.de)
Date: Tue, 10 May 2005 18:39:59 +0000
Subject: [R] =?iso-8859-1?q?Elektronische_R=FCckantwort_=96_Email_zur=FCc?=
	=?iso-8859-1?q?k_an_SAXK__=28KMM4277437V50393L0KM=29?=
Message-ID: <20050510174212.4F2C61EB41F@smtp1.sc5.paypal.com>



Vielen Dank fuer Ihre Anfrage an unseren Pay Pal Kundenservice.

Damit wir Ihr Anliegen schnell und zielgerichtet bearbeiten kÅˆnnen, 
nutzen Sie bitte das auf unserer Seite angebotene sichere 
Kontaktformular.

Klicken Sie einfach auf den unten aufgefÅ¸hrten Link, um auf unsere 
Webseite zu gelangen.

https://www.paypal.com/de/

Sobald Sie sich mit Ihrer email Adresse und Ihrem Passwort in Ihr Pay 
Pal Konto eingeloggt haben, klicken Sie bitte auf den Link ëKontaktí. 

Diesen finden Sie am unteren Ende jeder unserer Webseiten.

Bitte geben Sie uns so viele Informationen wie mÅˆglich, damit wir Sie 
schnell und umfassend beraten koennen.

Sollte Sie keine Links aufrufen kÅˆnnen, geben Sie bitte die unten 
angegebene Webadresse in Ihr Browserfenster ein.

https://www.paypal.de/wf/f=default

Wenn sich nicht in Ihr Konto einloggen kÅˆnnen, nutzen Sie bitte diese 
Webadresse.

https://www.paypal.de/ewf/f=default

Bitte setzen Sie sich mit uns in Verbindung,  wenn Sie Fragen haben oder
weiter UnterstÅˆtzung benÅˆtigen.



From lamkelj at yahoo.com  Tue May 10 20:04:44 2005
From: lamkelj at yahoo.com (Kel Lam)
Date: Tue, 10 May 2005 11:04:44 -0700 (PDT)
Subject: [R] Nested Logistic Regression
Message-ID: <20050510180444.14142.qmail@web52708.mail.yahoo.com>

Hi group,

I did a quick search in the archive but still couldn't
find a function that performs nested logistic
regression.
Please point me in the right direction.  Thanks a
million!

Regards,
Kel



From GPetris at uark.edu  Tue May 10 20:10:40 2005
From: GPetris at uark.edu (Giovanni Petris)
Date: Tue, 10 May 2005 13:10:40 -0500 (CDT)
Subject: [R] Function arguments and copying objects 
Message-ID: <200505101810.j4AIAenY023389@definetti.uark.edu>


I have a function that, via .Call, modifies its argument, so I first
create a backup copy of the object I want to pass, make a copy of it,
and pass the copy to the function. However, my function also modifies
the backup copy. I guess this has to do with R not duplicating objects
until they are modified. Where can I read more about that? And/or how
can I solve my problem, i.e. save a copy of the argument?

Here is an example of what I mean:

> mod.back$m0 <- rnorm(2)
> mod.back
$m0
[1] -0.03431583 -1.66513535

...

> mod <- mod.back
> dlmLLsvd(ip,mod)
> mod.back
$m0
[1] 3.360737 3.266844

...


Thanks in advance,
Giovanni

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From lauraholt_983 at hotmail.com  Tue May 10 20:35:29 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 10 May 2005 13:35:29 -0500
Subject: [R][S] Data sets from Chambers' green book?
Message-ID: <BAY10-F39461B3AF597BD00DB1A21D61F0@phx.gbl>

Dear R and S people:

Yet another dorky question (YADQ), please:

Where can I find the data sets from Chambers' green book, please?

I've tried
http://cm.bell-labs.com/stat/Sbook
(web site, no data or even pointers....functions and articles...good ones)

http://cm.bell-labs.com/stat/project/icmanuf
no web site at all.

Thanks in advance!



From thchung at tgen.org  Tue May 10 20:36:17 2005
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Tue, 10 May 2005 11:36:17 -0700
Subject: [R] Plotting dendrogram horizontally
Message-ID: <BEA64D31.4EA3%thchung@tgen.org>

Hi, All;

I tried to plot a dendrogram horizontally using following codes:

hc.g <- hclust(dist(mat), method="single")
plot(as.dendrogram(hc.g), horiz=T, ...)

While I was trying this (single linkage was IMPORTANT) with a matrix with
size 587 rows, an error happened with a message saying

[1] "Error in match.fun(FUN) : evaluation nested too deeply: infinite
recursion options(expression=)?\n"
attr(,"class")
[1] "try-error"

If I used other linkage methods, this error didn't happen. Only for single
linkage method, this error took place.

Is there any other way I can plot dendrogram horizontally without using
as.dendrogram?

Thanks in advance,
Tae-Hoon Chung
--------------------------------------------------
Tae-Hoon Chung
Post-Doctoral Researcher
Translational Genomics Research Institute (TGen)
445 N. 5th Street (Suite 530)
Phoenix, AZ 85004
1-602-343-8724 (Direct)
1-480-323-9820 (Mobile)
1-602-343-8840 (Fax)



From bill.shipley at usherbrooke.ca  Tue May 10 20:46:32 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Tue, 10 May 2005 14:46:32 -0400
Subject: [R] problem with intervals in mixed model
Message-ID: <000601c55590$95fb1980$b01ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050510/3eaa85f9/attachment.pl

From Pierre.Lapointe at nbf.ca  Tue May 10 20:47:17 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Tue, 10 May 2005 14:47:17 -0400
Subject: [R] Does R have a command for sending emails?
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D06B63159@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050510/0ee3530c/attachment.pl

From ripley at stats.ox.ac.uk  Tue May 10 20:56:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 May 2005 19:56:05 +0100 (BST)
Subject: [R] Function arguments and copying objects 
In-Reply-To: <200505101810.j4AIAenY023389@definetti.uark.edu>
References: <200505101810.j4AIAenY023389@definetti.uark.edu>
Message-ID: <Pine.LNX.4.61.0505101953480.21662@gannet.stats>

R-devel is the appropriate place for C programming questions: please DO 
read the posting guide.

You need to check the NAMED flag and call duplicated as necessary.
There are many examples in src/main/*.c.

On Tue, 10 May 2005, Giovanni Petris wrote:

>
> I have a function that, via .Call, modifies its argument, so I first
> create a backup copy of the object I want to pass, make a copy of it,
> and pass the copy to the function. However, my function also modifies
> the backup copy. I guess this has to do with R not duplicating objects
> until they are modified. Where can I read more about that? And/or how
> can I solve my problem, i.e. save a copy of the argument?
>
> Here is an example of what I mean:
>
>> mod.back$m0 <- rnorm(2)
>> mod.back
> $m0
> [1] -0.03431583 -1.66513535
>
> ...
>
>> mod <- mod.back
>> dlmLLsvd(ip,mod)
>> mod.back
> $m0
> [1] 3.360737 3.266844
>
> ...
>
>
> Thanks in advance,
> Giovanni
>
> -- 
>
> __________________________________________________
> [                                                  ]
> [ Giovanni Petris                 GPetris at uark.edu ]
> [ Department of Mathematical Sciences              ]
> [ University of Arkansas - Fayetteville, AR 72701  ]
> [ Ph: (479) 575-6324, 575-8630 (fax)               ]
> [ http://definetti.uark.edu/~gpetris/              ]
> [__________________________________________________]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From darrenleeweber at gmail.com  Tue May 10 20:56:40 2005
From: darrenleeweber at gmail.com (Darren Weber)
Date: Tue, 10 May 2005 11:56:40 -0700
Subject: [R] Rpy for python2.4 and R.2.1.0 on winXP
Message-ID: <d2095b8c05051011563b89247c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050510/1fab93aa/attachment.pl

From chris at psyctc.org  Tue May 10 21:26:30 2005
From: chris at psyctc.org (Chris Evans)
Date: Tue, 10 May 2005 20:26:30 +0100
Subject: [R] working with CGIwithR [2 answers]
In-Reply-To: <427E6C43.9293.604E892@localhost>
Message-ID: <428118F6.14405.635FECE@localhost>

On 8 May 2005 at 19:45, Chris Evans wrote:

> Do people have advice on debugging R programs running after CGIwithR
> inputting of data from forms?  
... rest of my original post snipped, I'm replying to my own post for 
the archives! ...

I had two kind responses, one shrewd one from Tom Short noting that 
you can have problems with error messages in angle brackets if you're 
looking at HTML sink output.  Noted but that wasn't the problem.  He 
and David Firth, who tells me there's a new version, 0.70, of 
CGIwithR uploaded to CRAN which helps 2.1.0 compatibility, both tell 
me I'll have to use ssh and hack things on the server from there.  
True indeed and I think one message is that you have to be prepared 
to work out how to get your data into the program directly if 
bypassing form input.  I've debugged things now and the program runs.

Now some more questions separately for healthy archiving!

Thanks all,

Chris
-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From dj at research.bell-labs.com  Tue May 10 21:46:24 2005
From: dj at research.bell-labs.com (David James)
Date: Tue, 10 May 2005 15:46:24 -0400
Subject: [R][S] Data sets from Chambers' green book?
In-Reply-To: <BAY10-F39461B3AF597BD00DB1A21D61F0@phx.gbl>
References: <BAY10-F39461B3AF597BD00DB1A21D61F0@phx.gbl>
Message-ID: <20050510194623.GA24411@jessie.research.bell-labs.com>

Hi,

Please try  "stat.bell-labs.com" instead of "cm.bell-labs.com/stat", 
i.e., 
   http://stat.bell-labs.com/Sbook
and
   http://stat.bell-labs.com/project/icmanuf

Thanks for bringing this to our attention -- I'm contacting our web 
master to find out how to fix it.

--
David

Laura Holt wrote:
> Dear R and S people:
> 
> Yet another dorky question (YADQ), please:
> 
> Where can I find the data sets from Chambers' green book, please?
> 
> I've tried
> http://cm.bell-labs.com/stat/Sbook
> (web site, no data or even pointers....functions and articles...good ones)
> 
> http://cm.bell-labs.com/stat/project/icmanuf
> no web site at all.
> 
> Thanks in advance!
> 
> _________________________________________________________________
> Express yourself instantly with MSN Messenger! Download today - it's FREE! 
> http://messenger.msn.click-url.com/go/onm00200471ave/direct/01/
> 
> --------------------------------------------------------------------
> This message was distributed by s-news at lists.biostat.wustl.edu.  To
> ...(s-news.. clipped)...



From uofiowa at gmail.com  Tue May 10 20:39:04 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Tue, 10 May 2005 14:39:04 -0400
Subject: [R] RODBC autocommit
Message-ID: <3f87cc6d05051011392c27b72f@mail.gmail.com>

How can I turn autocommit off using my RODBC connection to an Informix database?
I want to turn autocommit off, insert a thousand or so rows then commit.

I would appreciate any body's input on this.



From uofiowa at gmail.com  Tue May 10 22:32:29 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Tue, 10 May 2005 16:32:29 -0400
Subject: [R] multithreads
Message-ID: <3f87cc6d050510133232d86678@mail.gmail.com>

To write a multithreaded application in R, is RVPM the best library to use?



From darrenleeweber at gmail.com  Tue May 10 21:05:41 2005
From: darrenleeweber at gmail.com (Darren Weber)
Date: Tue, 10 May 2005 12:05:41 -0700
Subject: [R] R Greenhouse-Geiser correction?
Message-ID: <d2095b8c050510120524175566@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050510/d0478a3e/attachment.pl

From gerifalte28 at hotmail.com  Tue May 10 22:54:00 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 10 May 2005 20:54:00 +0000
Subject: [R] Nested Logistic Regression
In-Reply-To: <20050510180444.14142.qmail@web52708.mail.yahoo.com>
Message-ID: <BAY103-F34C27812F86AFCA2FB5778A61F0@phx.gbl>

For generalized linear mixed models you can use glmmPQL from MASS.  You also 
might want to take a look at library(glmmML).

Is this what you are after?

Francisco


>From: Kel Lam <lamkelj at yahoo.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] Nested Logistic Regression
>Date: Tue, 10 May 2005 11:04:44 -0700 (PDT)
>
>Hi group,
>
>I did a quick search in the archive but still couldn't
>find a function that performs nested logistic
>regression.
>Please point me in the right direction.  Thanks a
>million!
>
>Regards,
>Kel
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ssk2031 at columbia.edu  Tue May 10 23:11:05 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Tue, 10 May 2005 17:11:05 -0400
Subject: [R] R Greenhouse-Geiser correction?
In-Reply-To: <d2095b8c050510120524175566@mail.gmail.com>
References: <d2095b8c050510120524175566@mail.gmail.com>
Message-ID: <42812369.7020003@columbia.edu>


see these threads:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46512.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/15653.html

-suresh

Darren Weber wrote:
> Is there a function in R for doing Greenhouse-Geiser correction in ANOVA 
> models?
> 
> Is it already available in the aov function? How do we use it?
> 
> Best, Darren
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From stephen.welsh at pipeline.ouc.bc.ca  Tue May 10 23:10:03 2005
From: stephen.welsh at pipeline.ouc.bc.ca (Stephen Robert Welsh)
Date: Tue, 10 May 2005 14:10:03 -0700 (PDT)
Subject: [R] help printing plots
Message-ID: <11303745.1115759403791.JavaMail.stephen.welsh@pipeline.ouc.bc.ca>

	Is there a way to print off a hard copy of your plots when using R on 
a Unix system. I can't seem to find any commands in any of the 
documentation. 

	Thanks, Steve.



From ssk2031 at columbia.edu  Tue May 10 23:38:14 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Tue, 10 May 2005 17:38:14 -0400
Subject: [R] R Greenhouse-Geiser correction?
In-Reply-To: <42812369.7020003@columbia.edu>
References: <d2095b8c050510120524175566@mail.gmail.com>
	<42812369.7020003@columbia.edu>
Message-ID: <428129C6.30802@columbia.edu>


oh, this too...

http://finzi.psych.upenn.edu/R/library/stats/html/anova.mlm.html

and these threads:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48210.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46714.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/16662.html


Suresh Krishna wrote:
> 
> see these threads:
> 
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46512.html
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/15653.html
> 
> -suresh
> 
> Darren Weber wrote:
> 
>> Is there a function in R for doing Greenhouse-Geiser correction in 
>> ANOVA models?
>>
>> Is it already available in the aov function? How do we use it?
>>
>> Best, Darren
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gregor.gorjanc at gmail.com  Tue May 10 23:46:41 2005
From: gregor.gorjanc at gmail.com (Gregor GORJANC)
Date: Tue, 10 May 2005 23:46:41 +0200
Subject: Fwd: [R] Extract just some fields from XML]
In-Reply-To: <b20ae629050510144427421081@mail.gmail.com>
References: <427F6C64.4060807@bfro.uni-lj.si>
	<b20ae629050510144427421081@mail.gmail.com>
Message-ID: <b20ae629050510144641ff7e74@mail.gmail.com>

Duncan, you are a king!

Thanks a lot for this cookie. It really helped me. Thanks for the code
as well as detailed explanation at the end.

>Hi Gregor.
>
>Here is a function that will collect all of the nodes in the
>XML document whose names are in the vector elementNames
>
>getElements =
>function(elementNames)
>{
> els = list()
>
> startElement = function(node, ...) {
>
>  if(xmlName(node) %in% elementNames)
>     els[[length(els) + 1]] <<- node
>
>   node
> }
>
>  list(startElement = startElement, els = function() els)
>}
>
>So you can use it as
>
>  myHandlers = getElements("PubDate")
>  xmlTreeParse(URL, handlers = myHandlers)
>
>And then
>  myHandlers$els()
>
>returns a list of the the three PubDate elements in the document.
>
>If you wanted both PubDate and PubMedPubDate elements,
>you could use
>
>  myHandlers = getElements(c("PubDate", "PubMedPubDate")
>
>[Note that XML is case-sensitive and pubdate won't work.]
>
>The xmlEventParse is quite a bit more work as it is for
>very low-level parsing, working at the parser level
>of opening and closing XML elements.
>
>The xmlTreeParse is a hybrid parser. It works at the higher
>level of nodes, but provides an opportunity to process
>nodes when they are "created" and before their parent
>nodes have been processed.  So it works bottom up
>(in one of its modes).
>
>You can also use  xmlDOMApply() to iterate over all the
>nodes of a parsed XML tree.  You give xmlDOMApply() a
>function and it can do whatever it  wants, including
>checking the name of the node to see if you want it
>and then storing it somewhere. That's  where you'll
>need closures (simply viewed the "functions within functions" part) again,
>as in my example above.
>
>But here is a simple example
>  doc = xmlRoot(xmlTreeParse(URL))
>  xmlDOMApply(doc, function(node, ...)
>                      if(xmlName(node) == "PubDate")
>                         print(node)
>             )

Gorjanc Gregor wrote:
> Hello!
>
> I am trying to get specific fields from an XML document and I am totally
> puzzled. I hope someone can help me.
>
> # URL
> URL<-"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=11877539,11822933,11871444&retmode=xml&rettype=citation"
> # download a XML file
> tmp <- xmlTreeParse(URL, isURL = TRUE)
> tmp <- xmlRoot(tmp)
>
> Now I want to extract only node 'pubdate' and its children, but I don't
> know how to do that unless I try to dig into the structure of the XML
> file. The problem is that structure can differ and then hardcoded set
> of list indices i.e. tmp[[i]][[j]]... doesn't help me.
>
> I've read xmlEventParse but I don't understand handlers part up to the
> point that I could get anything usable from it. Here is something not
> very usable ;)
>
>   PubDate <- function(x, ...)
>   {
>     print(x)
>   }
>   xmlEventParse(URL, isURL = TRUE,
>                 handlers=list(PubDate=PubDate),
>                 addContext = FALSE)
>
> Thanks in advance!
>
> Lep pozdrav / With regards,
>     Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

--
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA

--
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.
----------------------------------------------------------------------


-- 
--
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty            URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                            tel: +386 (0)1 72 17 861
SI-1230 Domzale                fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From p.dalgaard at biostat.ku.dk  Tue May 10 23:46:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 May 2005 23:46:46 +0200
Subject: [R] R Greenhouse-Geiser correction?
In-Reply-To: <42812369.7020003@columbia.edu>
References: <d2095b8c050510120524175566@mail.gmail.com>
	<42812369.7020003@columbia.edu>
Message-ID: <x2y8am4uux.fsf@turmalin.kubism.ku.dk>

Suresh Krishna <ssk2031 at columbia.edu> writes:

> see these threads:
> 
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46512.html
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/15653.html
> 
> -suresh

Those references are a bit old. There is code for the G-G and H-F
corrections in anova.mlm in 2.1.0 patched. Notice that there was a
dumb blunder so that the epsilon values are calculated wrong in the
official 2.1.0. See the examples in help(anova.mlm) for details.
 
> Darren Weber wrote:
> > Is there a function in R for doing Greenhouse-Geiser correction in
> > ANOVA models?
> > Is it already available in the aov function? How do we use it?
> > Best, Darren
> > 	[[alternative HTML version deleted]]
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From huihan at yahoo-inc.com  Wed May 11 00:44:24 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Tue, 10 May 2005 15:44:24 -0700
Subject: [R] density function
Message-ID: <42813948.1060701@yahoo-inc.com>

Hi,

I wonder if the function "density" outputs the gaussian mixture formula 
that is estimated from the input data, assuming a gaussian model is used 
at each data point ?  I want to take the derivative of the finally 
estimated gaussian mixture formula for further analysis.

Thanks in advance for any help that you can offer me!

Hui



From jombart at biomserv.univ-lyon1.fr  Wed May 11 01:27:11 2005
From: jombart at biomserv.univ-lyon1.fr (Thibaut Jombart)
Date: Tue, 10 May 2005 19:27:11 -0400
Subject: [R] inserting R code in a latex document
Message-ID: <4281434F.3020705@biomserv.univ-lyon1.fr>

Hello,

I'm trying to insert R source code (functions) in an appendix of a latex 
document. I guess the easiest way to do so is to use the package Sweaved 
(file : Sweaved.sty) provided with the latest R version. Latex succeeds 
in loading the package, but my problem comes from the use of this very 
package. I tried to use the 'Schunk' environment, but '#' characters 
generate error (my R functions are annotated, and I want to keep those 
annotations). Under the 'Sinput' environment, no error is generated but 
the result is simply not different from what would be obtained under 
'verbatim' environment : R code lines are cut as they don't fit in the 
page width.

I tried to find answers in the latest Sweaved User Manual, 
unsuccessfully. I'm a recent latex user and I doubt I can quickly find a 
solution by myself.

Thanks,
Thibaut Jombart



From darrenleeweber at gmail.com  Wed May 11 01:59:42 2005
From: darrenleeweber at gmail.com (Darren Weber)
Date: Tue, 10 May 2005 16:59:42 -0700
Subject: [R] sphericity calculation
Message-ID: <d2095b8c0505101659348e47f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050510/ac361d6d/attachment.pl

From ksm32 at student.canterbury.ac.nz  Wed May 11 05:28:57 2005
From: ksm32 at student.canterbury.ac.nz (Carla Meurk)
Date: Wed, 11 May 2005 15:28:57 +1200
Subject: [R] time zones, daylight saving etc.
Message-ID: <42817BF9.5000705@student.canterbury.ac.nz>

Hi,  I have a whole bunch of data, which looks like:

15/03/2003 	 10:20 	1
15/03/2003 	 10:21 	0
15/03/2003 	 12:02 	0
16/03/2003 	 06:10 	0
16/03/2003 	 06:20 	0.5
16/03/2003 	 06:30 	0
16/03/2003 	 06:40 	0
16/03/2003 	 06:50 	0

18/03/2003  20:10                 0.5
etc. (times given on a 24 hour clock)

and goes on for years.  I have some code:

data<-read.table("H:/rainfall_data.txt",h=T)
library(survival)
datetime <- as.POSIXct(strptime(paste(data$V1, data$V2), "%d/%m/%Y 
%H:%M"), tz="NZST")

which produces:

[10] "2003-03-13 21:13:00 New Zealand Daylight Time"
 [11] "2003-03-15 13:20:00 New Zealand Daylight Time"
 [12] "2003-03-15 22:20:00 New Zealand Daylight Time"
 [13] "2003-03-15 22:21:00 New Zealand Daylight Time"
 [14] "2003-03-16 00:02:00 New Zealand Daylight Time"
 [15] "2003-03-16 18:10:00 New Zealand Standard Time"
 [16] "2003-03-16 18:20:00 New Zealand Standard Time"
 [17] "2003-03-16 18:30:00 New Zealand Standard Time"


My problem is that "15/03/2003 12:02" has become "16/03/2003 00:02" 
i.e.  it is 12 hours behind (as is everything else), but also, I do not 
want to change time zones.

The 12 hour delay is not really a problem just an annoyance, but the 
time zone change is a problem because later on I need to match up data 
by time using

mindata<-seq(from=min(datetime),to=max(datetime),by="mins")
newdata<-matrix(0,length(mindata),1)
newdata[match(format.POSIXct(datetime,"%Y %m %d %H 
%M"),format.POSIXct(mindata,"%Y %m %d %H %M"))]<-data$V3

and things go wrong here with matching repeating times/missing times 
around the timezone changes and, my resulting vector is 1 hour shorter 
than my other series.  From the R help I see that my OS may be to blame 
but, even if I specify tz="GMT" I still get NZST and NZDT.  Can someone 
help?

I hope this all makes sense

Carla



From falk4587 at uidaho.edu  Wed May 11 05:31:33 2005
From: falk4587 at uidaho.edu (Michael Falkowski)
Date: Tue, 10 May 2005 20:31:33 -0700
Subject: [R] Mixed Effect Model - Jackknife error estimate
Message-ID: <4d7bb94d9b35.4d9b354d7bb9@uidaho.edu>

Greetings,

I?ve fit the following mixed effects model using the NLME package:

hd.impute.lme <- lme(I(log(HEIGHT_M - 1.37)) ~ SPECIES + SPECIES:I(1/(DBH_CM + 2.54)),
        random = ~ I(1/(DBH_CM + 2.54)) | PLOTID,
        data = trees, na.action = na.exclude)

I would now like to extract a jackknife estimate of model error. I tried the following code, however, the estimate produced seems too low.

ss.ok <- 0
for (i in 1:dim(trees)[1])
 lme.msc <- ss.ok + sum(sd(lme(I(log(HEIGHT_M - 1.37)) ~ SPECIES + SPECIES:I(1/(DBH_CM + 2.54)),
        random = ~ I(1/(DBH_CM + 2.54)) | PLOTID,
        data = trees[-i,], na.action = na.exclude)$residuals))

rmse <- lme.msc/(dim(trees)[1]-1)

#output
#rmse = 0.4701232

Any suggestions?

Thanks,
Mike



From krcabrer at epm.net.co  Wed May 11 06:16:08 2005
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Tue, 10 May 2005 23:16:08 -0500
Subject: [R] Problem installing packages.
In-Reply-To: <4d7bb94d9b35.4d9b354d7bb9@uidaho.edu>
References: <4d7bb94d9b35.4d9b354d7bb9@uidaho.edu>
Message-ID: <42818708.8020806@epm.net.co>

Hello, R users:

I got the following problem when I try to install some packages using
the 2.1.0 patched version on Window$.

This is only an example.
The same happens when I try to install ade4.
(it download the packages and their dependency, but when it finish to 
download
it shows the same error)

> > utils:::menuInstallPkgs()
> trying URL 
> 'http://cran.at.r-project.org/bin/windows/contrib/2.1/CoCo_0.1.6.5.zip'
> Content type 'application/zip' length 1895592 bytes
> opened URL
> downloaded 1851Kb
>
> package 'CoCo' successfully unpacked and MD5 sums checked
> bundle 'CoCo' successfully unpacked and MD5 sums checked
> Error in sprintf(gettext("unable to move temp installation '%d' to 
> '%s'"),  :
>         use format %s for character objects


What am I doing wrong?

Thank you for your help.


Kenneth

From huihan at yahoo-inc.com  Wed May 11 07:10:11 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Tue, 10 May 2005 22:10:11 -0700
Subject: [R] density estimation
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E06@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E06@usctmx1106.merck.com>
Message-ID: <428193B3.6080901@yahoo-inc.com>

Hi,

I have been looking for a method of estimating a parametric model from 
the output (x, y) from the R function "density". Below is my thought and 
wonder if it looks OK. Suppose that we build a single gaussian model for 
each input data point x (x is the mean),  the overal model may be a sum 
of these gaussian models built on each x, i.e. P(y) = \sum_x P(y|x, 
\sigma), where y is any new data point. Is this right? Any normalization 
is applied?

Thanks in advance for any suggestion that you may offer me!

Best regards,
Hui



From ripley at stats.ox.ac.uk  Wed May 11 07:26:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 May 2005 06:26:15 +0100 (BST)
Subject: [R] help printing plots
In-Reply-To: <11303745.1115759403791.JavaMail.stephen.welsh@pipeline.ouc.bc.ca>
References: <11303745.1115759403791.JavaMail.stephen.welsh@pipeline.ouc.bc.ca>
Message-ID: <Pine.LNX.4.61.0505110625510.27930@gannet.stats>

On Tue, 10 May 2005, Stephen Robert Welsh wrote:

> 	Is there a way to print off a hard copy of your plots when using R on
> a Unix system. I can't seem to find any commands in any of the
> documentation.

?dev.print.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed May 11 07:29:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 May 2005 06:29:55 +0100 (BST)
Subject: [R] density function
In-Reply-To: <42813948.1060701@yahoo-inc.com>
References: <42813948.1060701@yahoo-inc.com>
Message-ID: <Pine.LNX.4.61.0505110627110.27930@gannet.stats>

On Tue, 10 May 2005, Hui Han wrote:

> I wonder if the function "density" outputs the gaussian mixture formula that 
> is estimated from the input data, assuming a gaussian model is used at each 
> data point ?  I want to take the derivative of the finally estimated gaussian 
> mixture formula for further analysis.

It is a kernel density estimate: a rather trivial mixture, not necessarily 
Gaussian.  Also, it is not set up to optimally estimate a derivative, and 
you should look at more sophisticated methods in other packages if you 
want to do that.

As to what "density" outputs: see its help page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed May 11 07:34:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 May 2005 06:34:44 +0100 (BST)
Subject: [R] Problem installing packages.
In-Reply-To: <42818708.8020806@epm.net.co>
References: <4d7bb94d9b35.4d9b354d7bb9@uidaho.edu>
	<42818708.8020806@epm.net.co>
Message-ID: <Pine.LNX.4.61.0505110631420.27930@gannet.stats>

Please look in the archives: in particular the *current* R-patched version 
does not do that and this has been discussed within the last week in a 
thread started by Erich Neuwirth.

On Tue, 10 May 2005, Kenneth Cabrera wrote:

> Hello, R users:
>
> I got the following problem when I try to install some packages using
> the 2.1.0 patched version on Window$.
>
> This is only an example.
> The same happens when I try to install ade4.
> (it download the packages and their dependency, but when it finish to 
> download
> it shows the same error)
>
>> > utils:::menuInstallPkgs()
>> trying URL 
>> 'http://cran.at.r-project.org/bin/windows/contrib/2.1/CoCo_0.1.6.5.zip'
>> Content type 'application/zip' length 1895592 bytes
>> opened URL
>> downloaded 1851Kb
>> 
>> package 'CoCo' successfully unpacked and MD5 sums checked
>> bundle 'CoCo' successfully unpacked and MD5 sums checked
>> Error in sprintf(gettext("unable to move temp installation '%d' to '%s'"), 
>> :
>>         use format %s for character objects
>
>
> What am I doing wrong?
>
> Thank you for your help.
>
>
> Kenneth
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From huihan at yahoo-inc.com  Wed May 11 08:02:02 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Tue, 10 May 2005 23:02:02 -0700
Subject: [R] density function
In-Reply-To: <Pine.LNX.4.61.0505110627110.27930@gannet.stats>
References: <42813948.1060701@yahoo-inc.com>
	<Pine.LNX.4.61.0505110627110.27930@gannet.stats>
Message-ID: <42819FDA.1050208@yahoo-inc.com>

Thank you very much, Professor Ripley.

If possible, could you point me to other packages that you think I 
should look at for estimating a derivative?

Best regards,
Hui

Prof Brian Ripley wrote:

> On Tue, 10 May 2005, Hui Han wrote:
>
>> I wonder if the function "density" outputs the gaussian mixture 
>> formula that is estimated from the input data, assuming a gaussian 
>> model is used at each data point ?  I want to take the derivative of 
>> the finally estimated gaussian mixture formula for further analysis.
>
>
> It is a kernel density estimate: a rather trivial mixture, not 
> necessarily Gaussian.  Also, it is not set up to optimally estimate a 
> derivative, and you should look at more sophisticated methods in other 
> packages if you want to do that.
>
> As to what "density" outputs: see its help page.
>



From ssk2031 at columbia.edu  Wed May 11 08:38:51 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Wed, 11 May 2005 02:38:51 -0400
Subject: [R] density function
In-Reply-To: <42819FDA.1050208@yahoo-inc.com>
References: <42813948.1060701@yahoo-inc.com>	<Pine.LNX.4.61.0505110627110.27930@gannet.stats>
	<42819FDA.1050208@yahoo-inc.com>
Message-ID: <4281A87B.6040702@columbia.edu>


http://finzi.psych.upenn.edu/R/Rhelp02a/archive/20509.html

-s.

Hui Han wrote:
> Thank you very much, Professor Ripley.
> 
> If possible, could you point me to other packages that you think I 
> should look at for estimating a derivative?
> 
> Best regards,
> Hui
> 
> Prof Brian Ripley wrote:
> 
>> On Tue, 10 May 2005, Hui Han wrote:
>>
>>> I wonder if the function "density" outputs the gaussian mixture 
>>> formula that is estimated from the input data, assuming a gaussian 
>>> model is used at each data point ?  I want to take the derivative of 
>>> the finally estimated gaussian mixture formula for further analysis.
>>
>>
>>
>> It is a kernel density estimate: a rather trivial mixture, not 
>> necessarily Gaussian.  Also, it is not set up to optimally estimate a 
>> derivative, and you should look at more sophisticated methods in other 
>> packages if you want to do that.
>>
>> As to what "density" outputs: see its help page.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From luke at novum.am.lublin.pl  Wed May 11 08:50:22 2005
From: luke at novum.am.lublin.pl (Lukasz Komsta)
Date: Wed, 11 May 2005 08:50:22 +0200
Subject: [R] R-project wallpapers
Message-ID: <4281AB2E.7010901@novum.am.lublin.pl>

Dear useRs,

I have just rendered 3 wallpapers with R logo:

ftp://ftp.ariadna.pl/pub/R-walls/

Hope you will enjoy. Regards,

-- 
Lukasz Komsta
Department of Medicinal Chemistry
Medical University of Lublin
6 Chodzki, 20-093 Lublin, Poland
Fax +48 81 7425165



From ligges at statistik.uni-dortmund.de  Wed May 11 08:57:14 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 May 2005 08:57:14 +0200
Subject: [R] Does R have a command for sending emails?
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D06B63159@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D06B63159@lbmsg002.fbn-nbf.local>
Message-ID: <4281ACCA.2070908@statistik.uni-dortmund.de>

Lapointe, Pierre wrote:

> This is excellent. It works great even though I'm on Win2k and behind a
> firewall.
> 
> I just have one question though, I don't use a function because I sometimes
> need to add people to my distribtion list. In R, my system(paste()) string
> is very long and when I try put an <enter> so it fits in my Rgui screen, the
> function does not work properly.  Everything that is after the <enter> is
> not considered by blat.
> 
> I don't know if I am being clear.  To clarify, here's my actual code: 
> 
> system(paste('
> blat c:/tmp/monthly/deacot2005.zip -to receiver at receiver.ca -f
> sender at sender.ca  -s "My title is long long long long.   Very long." -server
> smtp.host.com -base64'))

paste() is the right idea (you are just not using it appropriately):

  system(paste('blat',
        'c:/tmp/monthly/deacot2005.zip',
        '-to receiver at receiver.ca',
        '-f sender at sender.ca',
        '-s "My title is long long long long.',
        'Very long."',
        '-server smtp.host.com -base64'))

Uwe Ligges

> In R, if everything beginning with blat is not on the same line, the command
> does not work properly and everthing on the second line is not considered
> even thought he paste command ends with a ")"
> 
> Thanks
> 
> If you're still trying to figure out how to e-mail with R, here's what I did
> in details.
> 
> 1- Download blat from
> http://sourceforge.net/project/showfiles.php?group_id=81910
> <http://sourceforge.net/project/showfiles.php?group_id=81910> 
> 2- unzip blat.exe in your base directory (the one with Rgui.exe in it).  I
> put it in my C:\Program Files\R\rw2010\bin directory.  You do not need the
> other files in the zip file.
> 3- if you don't know your smtp, use this
> http://www.dirfile.com/smtp_diagnostic_tool.htm
> <http://www.dirfile.com/smtp_diagnostic_tool.htm>  .  Put in your e-mail
> address and leave blank the SMTP.  Press start.
> 4- In R, 
> Write this code and change what you want
> system(paste('
> blat c:/tmp/monthly/deacot2005.zip -to receiver at receiver.ca -f
> sender at sender.ca  -s "My title is long long long long.   Very long." -server
> smtp.host.com -base64')) #everything beginning with "blat" has to be on the
> same line
> 5- read the example.txt in the blast zip file for examples of how to modify
> the blat request
> 6- note:  the "-base64" statement is there to specify that the attached file
> is binary
> 
> 
> 
> *********************************************************************************** 
> AVIS DE NON-RESPONSABILITE: 
> Ce document transmis par courrier electronique est destine uniquement a la personne ou a l'entite a qui il est adresse et peut contenir des 
> renseignements confidentiels et assujettis au secret professionnel. La 
> confidentialite et le secret professionnel demeurent malgre l'envoi de ce 
> document a la mauvaise adresse electronique. Si vous n'etes pas le 
> destinataire vise ou la personne chargee de remettre ce document a son destinataire, veuillez nous en informer sans delai et detruire ce document ainsi que toute copie qui en aurait ete faite.Toute distribution, reproduction ou autre utilisation de ce document est 
> strictement interdite. De plus, le Groupe Financiere Banque Nationale et ses filiales ne peuvent pas etre tenus responsables des dommages pouvant etre causes par des virus ou des erreurs de transmission. 
> 
> DISCLAIMER:\ This documentation transmitted by electronic ma...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed May 11 09:04:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 May 2005 09:04:09 +0200
Subject: [R] Rpy for python2.4 and R.2.1.0 on winXP
In-Reply-To: <d2095b8c05051011563b89247c@mail.gmail.com>
References: <d2095b8c05051011563b89247c@mail.gmail.com>
Message-ID: <4281AE69.9030101@statistik.uni-dortmund.de>

Darren Weber wrote:

> Has anyone compiled a binary for winXP, with python2.4 and R.2.1.0?
> 
> If anyone can do this soon, please advise how to pick up the binary 
> installer.
> 
> The Rpy homepage is http://rpy.sourceforge.net/


What about asking on the mailing list given on that page?

Uwe Ligges


> Best, Darren
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed May 11 09:28:13 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 May 2005 09:28:13 +0200
Subject: [R] sphericity calculation
In-Reply-To: <d2095b8c0505101659348e47f@mail.gmail.com>
References: <d2095b8c0505101659348e47f@mail.gmail.com>
Message-ID: <4281B40D.3020907@statistik.uni-dortmund.de>

Darren Weber wrote:

> Dear Prof. Baron,
> 
> thankyou for your pdf paper on using R, at
> 
> http://www.psych.upenn.edu/~baron/rpsych.pdf
> 
> I am currently interested in the sphericity notes on pp. 45-47.
> 
> The equation on p. 46 suggests the following R calculation:
> 
> D <- k^2 * ( mean(diag(S)) - mean(S) )^2
> 
> while the notes in the pdf indicate the following:
> 
> D <- k^2 * ( mean(S) - mean(diag(S)) )^2
> 
> Can you please consider this and advise the correct calculation?


Both are equal! Mathematical calculations you should really know from 
school show that
(a - b)^2 = (b - a)^2
.....

Uwe Ligges



> Best, Darren
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From robin.smit at tno.nl  Wed May 11 09:57:07 2005
From: robin.smit at tno.nl (Smit, R. (Robin) (IenT))
Date: Wed, 11 May 2005 09:57:07 +0200
Subject: [R] Regsubsets()
Message-ID: <2395774549BBDA40AC83BC9E6223FBFF22F96F@MS-DT01VS01.tsn.tno.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050511/c9809081/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed May 11 10:09:07 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 May 2005 10:09:07 +0200
Subject: [R] inserting R code in a latex document
In-Reply-To: <4281434F.3020705@biomserv.univ-lyon1.fr>
References: <4281434F.3020705@biomserv.univ-lyon1.fr>
Message-ID: <4281BDA3.1080009@statistik.uni-dortmund.de>

Thibaut Jombart wrote:

> Hello,
> 
> I'm trying to insert R source code (functions) in an appendix of a latex 
> document. I guess the easiest way to do so is to use the package Sweaved 
> (file : Sweaved.sty) provided with the latest R version. Latex succeeds 
> in loading the package, but my problem comes from the use of this very 
> package. I tried to use the 'Schunk' environment, but '#' characters 
> generate error (my R functions are annotated, and I want to keep those 
> annotations). Under the 'Sinput' environment, no error is generated but 
> the result is simply not different from what would be obtained under 
> 'verbatim' environment : R code lines are cut as they don't fit in the 
> page width.
> 
> I tried to find answers in the latest Sweaved User Manual, 
> unsuccessfully. I'm a recent latex user and I doubt I can quickly find a 
> solution by myself.


I think we are talking about "Sweave" in package "utils"?
If so, you will find that you have to re-read the manual, because you 
don't need to specify 'Schunk'/'Sinput' yourself: Sweave does it for 
you. Just write the code in code chunks such as

<<>>=
code
# comment
@
LaTeX code

and apply Sweave on your noweb document, the resulting LaTeX document 
has appropriate environments.


Uwe Ligges





> Thanks,
> Thibaut Jombart
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed May 11 10:48:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 May 2005 09:48:22 +0100 (BST)
Subject: [R] Regsubsets()
In-Reply-To: <2395774549BBDA40AC83BC9E6223FBFF22F96F@MS-DT01VS01.tsn.tno.nl>
References: <2395774549BBDA40AC83BC9E6223FBFF22F96F@MS-DT01VS01.tsn.tno.nl>
Message-ID: <Pine.LNX.4.61.0505110945400.15955@gannet.stats>

Please DO read the posting guide.  You are using a package without 
mentioning it, and only the package maintainer is likely to understand 2).

If really you do not understand 1) please seek local statistical 
expertise.

On Wed, 11 May 2005, Smit, R. (Robin) (IenT) wrote:

> Dear List members
>
> I am using the regsubsets function to select a few predictor variables
> using Mallow's Cp:
>
>> sel.proc.regsub.full <- regsubsets(CO2 ~ v + log(v) + v.max + sd.v +
> tad + no.stops.km + av.stop.T + a + sd.a + a.max + d + sd.d + d.max +
> RPA + P + perc.stop.T + perc.a.T + perc.d.T + RPS + RPSS + sd.P.acc +
> P.dec + da.acc.1 + RMSACC + RDI + RPSI + P.acc + cov.v + cov.a + cov.d +
> sd.P + sd.v.run + RCS + T + mass.fin, data = DATASET, weights = count,
> nbest = 10, nvmax = 35, method = "exhaustive")
>
> I do however encounter the following warning message which I do not
> understand:
>
>> Reordering variables and trying again:
> Warning messages:
> 1: 14  linear dependencies found in: leaps.setup(x, y, wt = wt, nbest =
> nbest, nvmax = nvmax, force.in = force.in,
> 2: XHAUST returned error code -999 in: leaps.exhaustive(a, really.big
>
>
>
> Could anyone please direct me towards the possible problem and its
> solution?
>
> The DATASET consists of 75 datapoints, 35 numeric predictor variables
> and the weights vector is given by:
>
>> count
> [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
> 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
> 1 1 1 1 1
>
>
> Many thanks,
> Robin Smit
>
>
>
>
>
> This e-mail and its contents are subject to the DISCLAIMER at http://www.tno.nl/disclaimer/email.html
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dargosch at gmail.com  Wed May 11 11:01:00 2005
From: dargosch at gmail.com (Fredrik Karlsson)
Date: Wed, 11 May 2005 11:01:00 +0200
Subject: [R] Subset with selection variable from function argument. Is there
	another way?
Message-ID: <376e97ec0505110201398c62c5@mail.gmail.com>

Dear list,

I'm making my current code more generic and would like some advise.
The basic problem is subset and the name of the column to be compared
for selection.

What I've come up with is 

> data(mammals)
> set <- bottompremolars"
> subset(mammals, eval(parse(file="",text=set)) > 2)

This seems a bit odd.  Is there a nicer way?

/Fredrik



From ligges at statistik.uni-dortmund.de  Wed May 11 11:13:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 May 2005 11:13:55 +0200
Subject: [R] Subset with selection variable from function argument. Is
	there	another way?
In-Reply-To: <376e97ec0505110201398c62c5@mail.gmail.com>
References: <376e97ec0505110201398c62c5@mail.gmail.com>
Message-ID: <4281CCD3.6060102@statistik.uni-dortmund.de>

Fredrik Karlsson wrote:

> Dear list,
> 
> I'm making my current code more generic and would like some advise.
> The basic problem is subset and the name of the column to be compared
> for selection.
> 
> What I've come up with is 
> 
> 
>>data(mammals)
>>set <- bottompremolars"

The line above is
  a) syntactically incorrect and
  b) the string does not describe a variable in the mammals data
hence this is not reproducible at all.

>>subset(mammals, eval(parse(file="",text=set)) > 2)
>

Let's assume
   set <- "body"

Either use get() as in
   subset(mammals, get(set) > 2)
or simple indexing such as:
   subset(mammals, mammals[[set]] > 2)


Uwe Ligges



> 
> This seems a bit odd.  Is there a nicer way?
> 
> /Fredrik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jtk at cmp.uea.ac.uk  Wed May 11 12:46:20 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 11 May 2005 11:46:20 +0100
Subject: [R] inserting R code in a latex document
In-Reply-To: <4281BDA3.1080009@statistik.uni-dortmund.de>
References: <4281434F.3020705@biomserv.univ-lyon1.fr>
	<4281BDA3.1080009@statistik.uni-dortmund.de>
Message-ID: <20050511104620.GB24060@jtkpc.cmp.uea.ac.uk>

On Wed, May 11, 2005 at 10:09:07AM +0200, Uwe Ligges wrote:
> Thibaut Jombart wrote:
> 
> >Hello,
> >
> >I'm trying to insert R source code (functions) in an appendix of a latex 
> >document. I guess the easiest way to do so is to use the package Sweaved 
> >(file : Sweaved.sty) provided with the latest R version. Latex succeeds 
> >in loading the package, but my problem comes from the use of this very 
> >package. I tried to use the 'Schunk' environment, but '#' characters 
> >generate error (my R functions are annotated, and I want to keep those 
> >annotations). Under the 'Sinput' environment, no error is generated but 
> >the result is simply not different from what would be obtained under 
> >'verbatim' environment : R code lines are cut as they don't fit in the 
> >page width.
> >
> >I tried to find answers in the latest Sweaved User Manual, 
> >unsuccessfully. I'm a recent latex user and I doubt I can quickly find a 
> >solution by myself.
> 
> 
> I think we are talking about "Sweave" in package "utils"?
> If so, you will find that you have to re-read the manual, because you 
> don't need to specify 'Schunk'/'Sinput' yourself: Sweave does it for 
> you. Just write the code in code chunks such as

My understanding was that Thibaut tries to use the Sweave style
(\usepackage{Sweave}) in a standard LaTeX document, so my comment
here is focused on LaTeX rather than R.

In this case, the Schunk environment won't be of any use because as
provided by Sweave.sty, it doesn't do anything:

    \newenvironment{Schunk}{}{}

Therefore, LaTeX will interpret the hash as a special character
inside Schunk environments; the hash has to be wrapped into a
verbatim-like environment to be legal as a standard character.

Running Sweave on an rnw file results in the code chunks, such as

> <<>>=
> code
> # comment
> @

to be wrapped in

    \begin{Schunk}
    \begin{Sinput}
    code
    # coment
    \end{Sinput}
    \end{Schunk}

As the Schunk environment does nothing, I have used it as a handle
for adding effects, e.g. if your lines are just a bit too long, you
may be able to fix that by adding

    \renewenvironment{Schunk}{\footnotesize}{}

to the preamble of your LaTeX file. (This can also be useful for rnw
files too.)

If your code lines are way too long and won't even fit with \tiny,
I'm afraid that there is no automatic way to resolve this. With very
few exceptions, automatic reformatting of code does not result in
readable results. You'll have to manually arrange your code such that
line length does not exceed a reasonable maximum (which may generally
improve readability and maintainability of your code).

Finally, even as a LaTeX beginner, don't be afraid to look into style
files and the like. Sweave.sty is really a quite short and understandable
one. A great thing about LaTeX is that you can always check the code,
and while such lessons may not always be what you're looking for e.g.
when under time pressure to get something finished, I've found that (as
very often), I've found that reading LaTeX code has enabled me to use
LaTeX much more effectively over time.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From ripley at stats.ox.ac.uk  Wed May 11 12:22:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 May 2005 11:22:15 +0100 (BST)
Subject: [R] Subset with selection variable from function argument. Is
	there another way?
In-Reply-To: <376e97ec0505110201398c62c5@mail.gmail.com>
References: <376e97ec0505110201398c62c5@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505111119190.16949@gannet.stats>

On Wed, 11 May 2005, Fredrik Karlsson wrote:

> Dear list,
>
> I'm making my current code more generic and would like some advise.
> The basic problem is subset and the name of the column to be compared
> for selection.
>
> What I've come up with is
>
>> data(mammals)
>> set <- bottompremolars"
>> subset(mammals, eval(parse(file="",text=set)) > 2)
>
> This seems a bit odd.  Is there a nicer way?

Try

set <- "bottompremolars"
mammals[[set]]

assuming it is a data frame and you just want the one column.

subset() is just a convenience wrapper for the basic indexing operations, 
which are well worth learning.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gregor.gorjanc at bfro.uni-lj.si  Wed May 11 12:32:25 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Wed, 11 May 2005 12:32:25 +0200
Subject: [R] Does R have a command for sending emails?
Message-ID: <4281DF39.9040102@bfro.uni-lj.si>

> Is there a way to have an R program send an email?
> 
> Something like this:
> 
> address <- 'abc at d.com'
> text <- 'This is the email body'
> send.email(address, text)
> 

Others have shown you how you can wrap external programs with R for sending
mail on unixoid systems. On windows you can install Cygwin and have the
same functionality. I particulary like mutt, since one can also send
attachments. Something like this can be very usefull:

mutt -x -s subject -a attachment email at hfjdhsj

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From Mike.Prager at noaa.gov  Wed May 11 14:00:44 2005
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Wed, 11 May 2005 08:00:44 -0400
Subject: [R] inserting R code in a latex document
In-Reply-To: <4281BDA3.1080009@statistik.uni-dortmund.de>
References: <4281434F.3020705@biomserv.univ-lyon1.fr>
	<4281BDA3.1080009@statistik.uni-dortmund.de>
Message-ID: <4281F3EC.7020306@noaa.gov>

Thibaut Jombart wrote:

> I'm trying to insert R source code (functions) in an appendix of a 
> latex document. I guess the easiest way to do so is to use the package 
> Sweaved (file : Sweaved.sty) 


I have had very good results with the LaTeX package "listings".  It is 
easy to use, and it does the job.


-- 
Michael H. Prager, Ph.D.
Population Dynamics Team
NOAA Center for Coastal Habitat and Fisheries Research
Beaufort, North Carolina  28516  USA



From francoisromain at free.fr  Wed May 11 14:14:52 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 11 May 2005 14:14:52 +0200
Subject: [R] inserting R code in a latex document
In-Reply-To: <4281F3EC.7020306@noaa.gov>
References: <4281434F.3020705@biomserv.univ-lyon1.fr>	<4281BDA3.1080009@statistik.uni-dortmund.de>
	<4281F3EC.7020306@noaa.gov>
Message-ID: <4281F73C.8060002@free.fr>

Le 11.05.2005 14:00, Michael Prager a ??crit :

> Thibaut Jombart wrote:
>
>> I'm trying to insert R source code (functions) in an appendix of a 
>> latex document. I guess the easiest way to do so is to use the 
>> package Sweaved (file : Sweaved.sty) 
>
> I have had very good results with the LaTeX package "listings".  It is 
> easy to use, and it does the job.
>

There is also highlight : http://www.andre-simon.de/

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From emmanuelle.anthoine1 at etud.univ-ubs.fr  Wed May 11 15:12:24 2005
From: emmanuelle.anthoine1 at etud.univ-ubs.fr (emmanuelle.anthoine1@etud.univ-ubs.fr)
Date: Wed, 11 May 2005 15:12:24 +0200
Subject: [R] generalisation in the use of cclust (library cclust) 
Message-ID: <1115817144.428204b857f42@homae.univ-ubs.fr>

hello, 
I would like to compare indexes in order to see which clustering algorithm (k- 
means, k-medoid, hierarchical clustering with euclidean and pearson distance) 
is more efficient. So I want to calculate the c-index and the db in all those 
cases(library cclust). 
To do that I use the cclust function. How can I run this function with pearson 
distance for k-means? How can I generalize cclust for hierarchical clustering 
and k-medoid? 
The class cclust object has a list of 13 components. Can anybody tell me how 
these components are calculated? 

thanks for everything. 
Emmanuelle Anthoine. 
 
 




--------------------------------------------------------------------------------
UniversitÅÈ de Bretagne sud                               http://www.univ-ubs.fr/



From lars.claussen at pik-potsdam.de  Wed May 11 15:43:28 2005
From: lars.claussen at pik-potsdam.de (Lars)
Date: Wed, 11 May 2005 15:43:28 +0200
Subject: [R] display two pie-charts
Message-ID: <42820C00.7030704@pik-potsdam.de>

Hey,

i'd like to compose a clock-like looking plot composed out of two 
circles, each showing the length of a period (to compare them). first, 
to do so, it looked the easiest by using pie(), just puting multiple 
pie-charts over each other. the problem is that once the second pie is 
drawn, it replaces the first one. does anybody know how to add a second, 
smaller pie over an existing one, like a layer?

thanks, lars



From B.Rowlingson at lancaster.ac.uk  Wed May 11 15:59:31 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 11 May 2005 14:59:31 +0100
Subject: [R] display two pie-charts
In-Reply-To: <42820C00.7030704@pik-potsdam.de>
References: <42820C00.7030704@pik-potsdam.de>
Message-ID: <42820FC3.1050105@lancaster.ac.uk>

Lars wrote:

> i'd like to compose a clock-like looking plot composed out of two 
> circles, each showing the length of a period (to compare them). first, 
> to do so, it looked the easiest by using pie(), just puting multiple 
> pie-charts over each other. the problem is that once the second pie is 
> drawn, it replaces the first one. does anybody know how to add a second, 
> smaller pie over an existing one, like a layer?

  pie() calls plot.new(), thus creating a new plot. Comment out the 
plot.new call and you'll have a pie() function that splats over the 
current plot.

  Suggest you copy 'pie' to something of your own:

  > clock = pie

  Then edit the 'clock' function, removing the 'plot.new' call.

  All 'pie' does is call 'polygon' with some sine and cosine trickery - 
you may be better off writing your own graphics function to do what you 
want. Reading the 'pie' code may help!

Baz



From francoisromain at free.fr  Wed May 11 16:05:38 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 11 May 2005 16:05:38 +0200
Subject: [R] display two pie-charts
In-Reply-To: <42820C00.7030704@pik-potsdam.de>
References: <42820C00.7030704@pik-potsdam.de>
Message-ID: <42821132.70606@free.fr>

Le 11.05.2005 15:43, Lars a ??crit :

> Hey,
>
> i'd like to compose a clock-like looking plot composed out of two 
> circles, each showing the length of a period (to compare them). first, 
> to do so, it looked the easiest by using pie(), just puting multiple 
> pie-charts over each other. the problem is that once the second pie is 
> drawn, it replaces the first one. does anybody know how to add a 
> second, smaller pie over an existing one, like a layer?
>
> thanks, lars
>
Look at :

?layout
?par

And try :

par(mfrow=c(1,2))
pie(runif(4))
pie(runif(4))


-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From br44114 at gmail.com  Wed May 11 16:19:34 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 11 May 2005 10:19:34 -0400
Subject: [R] Change the result data
Message-ID: <8d5a36350505110719239a9953@mail.gmail.com>

hec.data <-array(c(5,15,20,68,29,54,84,119,14,14,17,26,16,10,94,7),
               dim=c(4,4),
               dimnames=list(eye=c("Green","Hazel", "Blue", "Brown"),
               hair=c("Black", "Brown", "Red", "Blond")))
#------------------------------------------------------
dfr <- as.data.frame(as.table(hec.data))
your.dfr <- data.frame(eye=rep(dfr$eye,dfr$Freq),hair=rep(dfr$hair,dfr$Freq))



-----Original Message-----
From: Muhammad Subianto [mailto:subianto at gmail.com]
Sent: Tuesday, May 10, 2005 8:38 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Change the result data


Dear R-helper,
I have a data like:

hec.data <-array(c(5,15,20,68,29,54,84,119,14,14,17,26,16,10,94,7),
               dim=c(4,4),
               dimnames=list(eye=c("Green","Hazel", "Blue", "Brown"),
               hair=c("Black", "Brown", "Red", "Blond")))
as.data.frame(as.table(hec.data))
 >     as.data.frame(as.table(hec.data))
     eye  hair Freq
1  Green Black    5
2  Hazel Black   15
3   Blue Black   20
4  Brown Black   68
5  Green Brown   29
6  Hazel Brown   54
7   Blue Brown   84
8  Brown Brown  119
9  Green   Red   14
10 Hazel   Red   14
11  Blue   Red   17
12 Brown   Red   26
13 Green Blond   16
14 Hazel Blond   10
15  Blue Blond   94
16 Brown Blond    7
 >
    
I want to extract like,
Green Black
Green Black
Green Black
Green Black
Green Black
Hazel Black
Hazel Black
Hazel Black
Hazel Black
.
.
.
Brown Blond
Brown Blond
Brown Blond
Brown Blond
Brown Blond
Brown Blond
Brown Blond

How can I do it.
Thanks you for your help.

Best regards,
Muhammad Subianto
http://article.gmane.org/gmane.comp.lang.r.general/14604,14610

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Wed May 11 16:39:15 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 11 May 2005 07:39:15 -0700 (PDT)
Subject: [R] Regsubsets()
In-Reply-To: <2395774549BBDA40AC83BC9E6223FBFF22F96F@MS-DT01VS01.tsn.tno.nl>
References: <2395774549BBDA40AC83BC9E6223FBFF22F96F@MS-DT01VS01.tsn.tno.nl>
Message-ID: <Pine.A41.4.61b.0505110731230.34570@homer09.u.washington.edu>

On Wed, 11 May 2005, Smit, R. (Robin) (IenT) wrote:

> Dear List members
>
> I am using the regsubsets function to select a few predictor variables
> using Mallow's Cp:
>
>> sel.proc.regsub.full <- regsubsets(CO2 ~ v + log(v) + v.max + sd.v +
> tad + no.stops.km + av.stop.T + a + sd.a + a.max + d + sd.d + d.max +
> RPA + P + perc.stop.T + perc.a.T + perc.d.T + RPS + RPSS + sd.P.acc +
> P.dec + da.acc.1 + RMSACC + RDI + RPSI + P.acc + cov.v + cov.a + cov.d +
> sd.P + sd.v.run + RCS + T + mass.fin, data = DATASET, weights = count,
> nbest = 10, nvmax = 35, method = "exhaustive")
>
> I do however encounter the following warning message which I do not
> understand:
>
>> Reordering variables and trying again:
> Warning messages:
> 1: 14  linear dependencies found in: leaps.setup(x, y, wt = wt, nbest =
> nbest, nvmax = nvmax, force.in = force.in,
> 2: XHAUST returned error code -999 in: leaps.exhaustive(a, really.big
>

The first message means that your variables are linearly redundant (very 
much so, with 14 linear dependencies in 35 variables).  This probably 
explains the second warning.

You need to reduce nvmax: you can't fit any model with more than 35-14=21 
variables. The code will work with linear dependencies as long as you set 
nvmax low enough -- according to Alan Miller's book it was written for 
situtations with more variables than observations.  Try something like 
nvmax=10, and you will be able to see if there really is a use for models 
with even as many as 10 variables.

 	-thomas



From bioconductor.cn at gmail.com  Wed May 11 17:26:07 2005
From: bioconductor.cn at gmail.com (Xiao Shi)
Date: Wed, 11 May 2005 23:26:07 +0800
Subject: [R] How to plot a matrix with 18 rows by row vs. a vector in a
	single graph, resulting 18 lines with different colors?
Message-ID: <cedaa40b050511082642fd9143@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050511/a2b127a2/attachment.pl

From khobson at fd9ns01.okladot.state.ok.us  Wed May 11 17:29:45 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Wed, 11 May 2005 10:29:45 -0500
Subject: [R] Proper 3rd and 4th Parameters for my oOut() function
Message-ID: <OF05D1412C.68AF75A7-ON86256FFE.005386DC-86256FFE.00550606@fd9ns01.okladot.state.ok.us>





How can I pass the column names for XX and YY to my funcition properly?

If I change T314 column names to X and Y and use those in the function
call, it works properly.  Labs 1, 2, 3, and 25 are eliminated in T314.out.


####### R code starts here.
rm(list=ls())

# Data from NCHRP Appendix A -
http://trb.org/publications/nchrp/nchrp_w71.pdf
T314 <- structure(list(Lab = as.integer(c(1:60)), XX = c(4.89, 3.82, 2.57,
2.3,2.034, 2, 1.97, 1.85,1.85, 1.85, 1.84, 1.82, 1.82, 1.77, 1.76, 1.67,
1.66,
1.63, 1.62,1.62, 1.55, 1.54, 1.54, 1.53, 1.53, 1.44, 1.428, 1.42, 1.39,
1.36,
1.35, 1.31, 1.28, 1.24, 1.24, 1.23, 1.22, 1.21, 1.19, 1.18, 1.18, 1.18,
1.17,
1.16, 1.13, 1.13, 1.099, 1.09, 1.09, 1.08, 1.07, 1.05, 0.98, 0.97, 0.84,
0.808,
0.69, 0.63, 0.6, 0.5), YY = c(5.28, 3.82, 2.41, 2.32, 2.211, 1.46, 2.24,
1.91,
1.78, 1.63, 1.81, 1.92, 1.2, 1.67, 1.28, 1.59, 1.45, 2.06, 1.91, 1.19,
1.26,
1.79, 1.39, 1.48, 0.72, 1.29, 1.517, 1.71, 1.12, 1.38, 0.93, 1.36, 1.2,
1.23,
0.71, 1.29, 1.26, 1.48, 1.26, 1.33, 1.21, 1.04, 1.57, 1.42, 1.08, 1.04,
1.33,
1.33, 1.2, 1.05, 1.24, 0.91, 0.99, 1.06, 1.27, 0.702, 0.77, 0.58, 1,
0.38)),
.Names = c("Lab", "XX", "YY" ), class = "data.frame",
row.names = as.character(c(1:60)))

### Be sure to remove NA data prior to oOut()
oOut <- function(dsin, dsout, X, Y)
  {
  oOutsub <- function(olimit){
    # Get Medians for Invalid Data Determination
    Xmed <- median(dsin$X); Ymed <- median(dsin$Y)
    # Make new dataset with (Y-X)-(Ymedian-Xmedian) column
    dsout <- cbind(dsin, XY=(dsin$Y-dsin$X)-(Ymed-Xmed))
    # Get median for new column
    XYmed <- median(dsout$XY)
    iqx <- diff(quantile(dsin$X, c(0.125, .875)))
    iqy <- diff(quantile(dsin$Y, c(0.125, .875)))
    iqxy <- diff(quantile(dsout$XY, c(0.125, .875)))
    # Invalid Upper Limits
    iulX <- quantile(dsin$X, 0.875)+olimit*iqx

    iulY <- quantile(dsin$Y, 0.875)+olimit*iqy
    iulXY <- quantile(dsout$XY, 0.875)+olimit*iqxy
    # Invalid Lower Limits
    illX <- quantile(dsin$X, 0.125)-olimit*iqx
    illY <- quantile(dsin$Y, 0.125)-olimit*iqy
    illXY <- quantile(dsout$XY, 0.125)-olimit*iqxy

    dsout <- subset(dsout, with(dsout, X <= iulX & X >= illX))
    dsout <- subset(dsout, with(dsout, Y <= iulY & Y >= illY))
    dsout <- subset(dsout, with(dsout, XY <= iulXY & XY >= illXY))
    dsout
    }
  dsout <- oOutsub(1.555)  #Eliminates Invalid Data
  dsin <- dsout
  dsout <- oOutsub(0.674)  #Eliminates Outlier Data
  dsout <- dsout[1:(ncol(dsout)-2)]  #Trim outer 2 XY columns
  dsout
  }
T314.PP4 <- oOut(T314, T314.PP4, XX, YY)
T314.PP4 # showing resutls.

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From neuro3000 at hotmail.com  Wed May 11 17:35:21 2005
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Wed, 11 May 2005 11:35:21 -0400
Subject: [R] PDF Arial, Helvetica and Pagemaker 7.0
Message-ID: <BAY104-F13BF7BC0A1BECD2D5053E0AF100@phx.gbl>

Hello,

I can generate PDF documents and they look good on acrobat reader.  However, 
when I import the PDFs in Pagemaker 7.0 (on windows2K), I get an error 
message saying it does not recognize the Helvetica font.

I got this from ?pdf:

family
the font family to be used, one of "AvantGarde", "Bookman", "Courier", 
"Helvetica", "Helvetica-Narrow", "NewCenturySchoolbook", "Palatino" or 
"Times". Note the other specifications allowed for postscript are not 
available.

There in no mention of Arial.  Is there a way to force R to use the Arial 
family?

I'm on win2K
>version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.0
year     2005
month    04
day      18
language R



From spencer.graves at pdf.com  Wed May 11 17:37:20 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 11 May 2005 08:37:20 -0700
Subject: [R] How to plot a matrix with 18 rows by row vs. a vector in
	a	single graph, resulting 18 lines with different colors?
In-Reply-To: <cedaa40b050511082642fd9143@mail.gmail.com>
References: <cedaa40b050511082642fd9143@mail.gmail.com>
Message-ID: <428226B0.5000307@pdf.com>

Have you considered "matplot":

matplot(1:3, array(1:6, dim=c(3,2)), type="b")

spencer graves

Xiao Shi wrote:

> I have a matrix gene=array(rnorm(180), dim=c(18,10)) and a vector time=c(0,
> 0.5,2,4,6,8,12,24,48,72).
> So i can get a plot with plot(time,gene[1,],type="b",col="green") for the 
> first row.So how can i get all the 18 rows in the same plot VS. the same 
> vector time=c(0,0.5,2,4,6,8,12,24,48,72).
> Any suggestions.Thanks !
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed May 11 17:41:32 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 11 May 2005 11:41:32 -0400
Subject: [R] How to plot a matrix with 18 rows by row vs. a vector
	in a single graph, resulting 18 lines with different colors?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E810@usctmx1106.merck.com>

See ?matplot.  E.g.,

matplot(time, t(gene), type="b", lty=1)

Andy

> From: Xiao Shi
> 
> I have a matrix gene=array(rnorm(180), dim=c(18,10)) and a 
> vector time=c(0,
> 0.5,2,4,6,8,12,24,48,72).
> So i can get a plot with 
> plot(time,gene[1,],type="b",col="green") for the 
> first row.So how can i get all the 18 rows in the same plot 
> VS. the same 
> vector time=c(0,0.5,2,4,6,8,12,24,48,72).
> Any suggestions.Thanks !
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From francoisromain at free.fr  Wed May 11 17:47:13 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 11 May 2005 17:47:13 +0200
Subject: [R] How to plot a matrix with 18 rows by row vs. a vector in
	a	single graph, resulting 18 lines with different colors?
In-Reply-To: <cedaa40b050511082642fd9143@mail.gmail.com>
References: <cedaa40b050511082642fd9143@mail.gmail.com>
Message-ID: <42822901.3000409@free.fr>

Le 11.05.2005 17:26, Xiao Shi a ??crit :

>I have a matrix gene=array(rnorm(180), dim=c(18,10)) and a vector time=c(0,
>0.5,2,4,6,8,12,24,48,72).
>So i can get a plot with plot(time,gene[1,],type="b",col="green") for the 
>first row.So how can i get all the 18 rows in the same plot VS. the same 
>vector time=c(0,0.5,2,4,6,8,12,24,48,72).
>Any suggestions.Thanks !
>  
>
Hello,

matplot(time,t(gene),type="o")

BTW, time is the name of an R function. Choose something else if you 
don't want funny things to happen.

Romain

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ripley at stats.ox.ac.uk  Wed May 11 17:54:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 May 2005 16:54:25 +0100 (BST)
Subject: [R] PDF Arial, Helvetica and Pagemaker 7.0
In-Reply-To: <BAY104-F13BF7BC0A1BECD2D5053E0AF100@phx.gbl>
References: <BAY104-F13BF7BC0A1BECD2D5053E0AF100@phx.gbl>
Message-ID: <Pine.LNX.4.61.0505111651350.22988@gannet.stats>

On Wed, 11 May 2005, Neuro LeSuperH?ros wrote:

Do give your real name and affiliation!  What exactly are you trying to 
hide?

Why don't you ask on a PageMaker list how to get it to accept valid PDF?
This is a bug in PageMaker, not in R.

> I can generate PDF documents and they look good on acrobat reader.  However, 
> when I import the PDFs in Pagemaker 7.0 (on windows2K), I get an error 
> message saying it does not recognize the Helvetica font.
>
> I got this from ?pdf:
>
> family
> the font family to be used, one of "AvantGarde", "Bookman", "Courier", 
> "Helvetica", "Helvetica-Narrow", "NewCenturySchoolbook", "Palatino" or 
> "Times". Note the other specifications allowed for postscript are not 
> available.
>
> There in no mention of Arial.  Is there a way to force R to use the Arial 
> family?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From lfanchon at vet-alfort.fr  Wed May 11 18:56:27 2005
From: lfanchon at vet-alfort.fr (Laurent Fanchon)
Date: Wed, 11 May 2005 18:56:27 +0200
Subject: [R] lme How to validate a model with a validation set and a test set
Message-ID: <4282393B.2040402@vet-alfort.fr>

Dear all,

I am still trying to fit a model for habituation with lme (Dogs are 
trotting on a treadmill once a week during 4 consecutive weeks).
I have maybe found a good one.
In order to validate it, I would like to split my data.
I have 28 dogs, I am thinking to build the model with 20 and use the 
remaining 8 to validate it.

Is it a good method to validate the model?
How can I do this?

I would like to use the data from the first 2 weeks as input and see how 
good the model is to predict the data from the last 2 weeks.
I tried to use predict but this function does not take into account the 
data from the first 2 weeks. I can only get the results for the population.

I would appreciate any suggestions for this

Thank for your help

Laurent



From uofiowa at gmail.com  Wed May 11 19:05:58 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 11 May 2005 13:05:58 -0400
Subject: [R] simplify loop
Message-ID: <3f87cc6d05051110054d65813a@mail.gmail.com>

Is there a way to implement this faster than doing it in a loop. 

        for (i in length(settle)-1:1) {
                settle[i] = settle[i+1]/(1 + settle.pct[i+1])
        }

I want to guarantee that i+1 is calculated before i



From chrisxe at gmx.at  Wed May 11 19:26:02 2005
From: chrisxe at gmx.at (Christoph Strehblow)
Date: Wed, 11 May 2005 19:26:02 +0200
Subject: [R] ANOVA with p-values?
Message-ID: <026B4C29-F631-414E-A167-1B305B3FD49E@gmx.at>

Hi List!

My name is Christoph Strehblow, i??m  from Vienna, Austria, and tried  
a ANOVA, but couldn??t manage to get an output, i was used to get from  
SPSS, where i would get a detailed pairwise table, including diff,  
standard error, p-value, and lwr and upr.

in R, i tried a ANOVA with TukeyHSD, which only brings up the  
confidence intervalls of all pairs, but no p-values.

How is this possible?

Thanks a lot in advance, Chris

using: R 2.10 on Mac Os 10.4



From Achim.Zeileis at wu-wien.ac.at  Wed May 11 19:21:13 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 11 May 2005 19:21:13 +0200
Subject: [R] simplify loop
In-Reply-To: <3f87cc6d05051110054d65813a@mail.gmail.com>
References: <3f87cc6d05051110054d65813a@mail.gmail.com>
Message-ID: <20050511192113.75bdd378.Achim.Zeileis@wu-wien.ac.at>

On Wed, 11 May 2005 13:05:58 -0400 Omar Lakkis wrote:

> Is there a way to implement this faster than doing it in a loop. 
> 
>         for (i in length(settle)-1:1) {
>                 settle[i] = settle[i+1]/(1 + settle.pct[i+1])
>         }
> 
> I want to guarantee that i+1 is calculated before i

Yes, as the loop above as only one iteration, you can easily do it as:
  n <- length(settle)
  settle[n-1] <- settle[n]/(1 + settle.pct[n])

What you probably really meant, can also be simply done without a for
loop. You need a vector settle.pct and a scalar starting value (not a
full vector) settle. So in the following settle is assumed to be only
settle[n]:

  settle/c(rev(cumprod(1 + rev(settle.pct)))[-1], 1)

If settle.pct should in fact also be only be constant, this can of
course be further simplified.
Z


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed May 11 19:32:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 May 2005 18:32:38 +0100 (BST)
Subject: [R] simplify loop
In-Reply-To: <3f87cc6d05051110054d65813a@mail.gmail.com>
References: <3f87cc6d05051110054d65813a@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505111831190.3399@gannet.stats>

It is, backwards, a cumulative product so you could use cumprod.

On Wed, 11 May 2005, Omar Lakkis wrote:

> Is there a way to implement this faster than doing it in a loop.
>
>        for (i in length(settle)-1:1) {
>                settle[i] = settle[i+1]/(1 + settle.pct[i+1])
>        }
>
> I want to guarantee that i+1 is calculated before i
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From B.Rowlingson at lancaster.ac.uk  Wed May 11 19:34:30 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 11 May 2005 18:34:30 +0100
Subject: [R] simplify loop
In-Reply-To: <3f87cc6d05051110054d65813a@mail.gmail.com>
References: <3f87cc6d05051110054d65813a@mail.gmail.com>
Message-ID: <42824226.2060303@lancaster.ac.uk>

Omar Lakkis wrote:
> Is there a way to implement this faster than doing it in a loop. 
> 
>         for (i in length(settle)-1:1) {
>                 settle[i] = settle[i+1]/(1 + settle.pct[i+1])
>         }

  You dont need a loop at all here. How so? Well, as it is written the 
code in the for loop only executes once:

  > settle=1:10
  > for (i in length(settle)-1:1) {
  + print(i)
  + }
  [1] 9
  >

  you have made a mistake, and I think you really want:

  > for (i in (length(settle)-1):1 ) {

  since R does the ':' first and then does 'length(settle)-' unless you 
bracket it:

   > length(settle)-1:1
   [1] 9
   > (length(settle)-1):1
   [1] 9 8 7 6 5 4 3 2 1

I don't see an obvious way to get rid of the for loop though...

Baz



From hodgess at gator.uhd.edu  Wed May 11 19:45:15 2005
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Wed, 11 May 2005 12:45:15 -0500
Subject: [R] Tukey HSD
Message-ID: <200505111745.j4BHjFk25061@gator.dt.uh.edu>

Hi all!



From hodgess at gator.uhd.edu  Wed May 11 19:48:05 2005
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Wed, 11 May 2005 12:48:05 -0500
Subject: [R] Tukey HSD
Message-ID: <200505111748.j4BHm5G25508@gator.dt.uh.edu>

Dear R People:

Here is a possible answer to Christophe's problem
>    summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
            Df Sum Sq Mean Sq F value   Pr(>F)   
wool         1  450.7   450.7  3.3393 0.073614 . 
tension      2 2034.3  1017.1  7.5367 0.001378 **
Residuals   50 6747.9   135.0                    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>      TukeyHSD(fm1, "tension", ordered = TRUE)
  Tukey multiple comparisons of means
    95% family-wise confidence level
    factor levels have been ordered

Fit: aov(formula = breaks ~ wool + tension, data = warpbreaks)

$tension
         diff        lwr      upr
M-H  4.722222 -4.6311985 14.07564
L-H 14.722222  5.3688015 24.07564
L-M 10.000000  0.6465793 19.35342

>
This is from the Example for TukeyHSD.

It gives you the confidence intervals for the level that you set.

Not p values.

Hope this helps!  Sorry to send to the entire list, but deleted original
message.

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From machud at intellektik.informatik.tu-darmstadt.de  Wed May 11 19:35:56 2005
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Wed, 11 May 2005 19:35:56 +0200 (CEST)
Subject: [R] wireframe (reloaded): how to remove the frame around my plot? 
Message-ID: <Pine.LNX.4.58.0505111927220.16663@kika.intellektik.informatik.tu-darmstadt.de>

Hello,


I would like to remove the frame from wireframe, but that one only.

I am currently doing

trellis.par.set("axis.line",list(col=NA,lty=1,lwd=1))

but this has the flaw that also the tick lines of the axes disapper.

I read all the thread with the same titlee of this message appeared
sometime ago on this list and tried all the methods there suggested but
I am still facing the problem.

Is there any possible solution?

Thank you for your consideration.


Marco



-------------------------------------------------------------------
Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit??t Darmstadt, Hochschulstra??e 10,
D-64289 Darmstadt - Germany, Office: S2/02 Raum E312
Tel: +49.(0)6151.166802 Fax: +49.(0)6151.165326
email: machud at intellektik.informatik.tu-darmstadt.de
web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud



From OlsenN at pac.dfo-mpo.gc.ca  Wed May 11 20:15:17 2005
From: OlsenN at pac.dfo-mpo.gc.ca (OlsenN@pac.dfo-mpo.gc.ca)
Date: Wed, 11 May 2005 11:15:17 -0700
Subject: [R] Clear RGUI console under WinXP
Message-ID: <7CBBD627E4E688499349A5D11D07831602ECB7AF@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050511/3421b354/attachment.pl

From maustin at amgen.com  Wed May 11 20:16:38 2005
From: maustin at amgen.com (Austin, Matt)
Date: Wed, 11 May 2005 11:16:38 -0700
Subject: [R] lme How to validate a model with a validation set and a t
	est set
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0E2D6@teal-exch.amgen.com>


If you used all 28 animals to find the model out of a group of candidate
models, I would have my reservations about this 'validation'.  Any
confidence intervals you get from the final model are bound to be overly
optimistic because you haven't accounted for the degrees of freedom chewed
up during the model fitting/finding process.

Matt Austin
Statistician

Amgen 
One Amgen Center Drive
M/S 24-2-C
Thousand Oaks CA 93021
(805) 447 - 7431

"Today has the fatigue of a Friday and the desperation of a Monday"  -- S.
Pearce 2005


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Laurent Fanchon
Sent: Wednesday, May 11, 2005 9:56 AM
To: r-help at stat.math.ethz.ch
Subject: [R] lme How to validate a model with a validation set and a
test set


Dear all,

I am still trying to fit a model for habituation with lme (Dogs are 
trotting on a treadmill once a week during 4 consecutive weeks).
I have maybe found a good one.
In order to validate it, I would like to split my data.
I have 28 dogs, I am thinking to build the model with 20 and use the 
remaining 8 to validate it.

Is it a good method to validate the model?
How can I do this?

I would like to use the data from the first 2 weeks as input and see how 
good the model is to predict the data from the last 2 weeks.
I tried to use predict but this function does not take into account the 
data from the first 2 weeks. I can only get the results for the population.

I would appreciate any suggestions for this

Thank for your help

Laurent

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Wed May 11 20:50:30 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 11 May 2005 13:50:30 -0500
Subject: [R] wireframe (reloaded): how to remove the frame around my plot?
In-Reply-To: <Pine.LNX.4.58.0505111927220.16663@kika.intellektik.informatik.tu-darmstadt.de>
References: <Pine.LNX.4.58.0505111927220.16663@kika.intellektik.informatik.tu-darmstadt.de>
Message-ID: <200505111350.30564.deepayan@stat.wisc.edu>

On Wednesday 11 May 2005 12:35, Marco Chiarandini wrote:
> Hello,
>
>
> I would like to remove the frame from wireframe, but that one only.
>
> I am currently doing
>
> trellis.par.set("axis.line",list(col=NA,lty=1,lwd=1))
>
> but this has the flaw that also the tick lines of the axes disapper.

I should add a separate setting for the default cloud/wireframe scales. For 
now, you should get what you want with 

wireframe(volcano, scales = list(col = 1, arrows = FALSE))

Deepayan



From crirocha at unicamp.br  Wed May 11 18:06:35 2005
From: crirocha at unicamp.br (Cristiane Rocha)
Date: Wed, 11 May 2005 16:06:35 +0000
Subject: [R] Erro loading library from apache
Message-ID: <42822D8B.6080700@unicamp.br>

Hello,

I'm having a error message when I try to load the som library from a 
cgi. When a run my script via web the following error appears in the 
apache log:
Error: package som was built for  i686-pc-linux-gnu, referer: https:// ....

I call the library function setting the path to the library directory:
 >library(som, lib.loc="/usr/local/lib/R/library/")

The script runs fine in shell mode, so I have no idea of what is wrong.

Thank's in advance

-- 
Cristiane S. Rocha
Laboratorio Genoma Funcional - Bioinform??tica
Centro de Biologia Molecular e Engenharia Genetica
Universidade Estadual de Campinas
Campinas - SP - Brasil



From ggrothendieck at gmail.com  Wed May 11 21:23:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 11 May 2005 15:23:40 -0400
Subject: [R] time zones, daylight saving etc.
In-Reply-To: <42817BF9.5000705@student.canterbury.ac.nz>
References: <42817BF9.5000705@student.canterbury.ac.nz>
Message-ID: <971536df05051112232b064070@mail.gmail.com>

You could use the chron package.  It represents date times without
using time zones so you can't have this sort of problem.

On 5/10/05, Carla Meurk <ksm32 at student.canterbury.ac.nz> wrote:
> Hi,  I have a whole bunch of data, which looks like:
> 
> 15/03/2003       10:20  1
> 15/03/2003       10:21  0
> 15/03/2003       12:02  0
> 16/03/2003       06:10  0
> 16/03/2003       06:20  0.5
> 16/03/2003       06:30  0
> 16/03/2003       06:40  0
> 16/03/2003       06:50  0
> 
> 18/03/2003  20:10                 0.5
> etc. (times given on a 24 hour clock)
> 
> and goes on for years.  I have some code:
> 
> data<-read.table("H:/rainfall_data.txt",h=T)
> library(survival)
> datetime <- as.POSIXct(strptime(paste(data$V1, data$V2), "%d/%m/%Y
> %H:%M"), tz="NZST")
> 
> which produces:
> 
> [10] "2003-03-13 21:13:00 New Zealand Daylight Time"
> [11] "2003-03-15 13:20:00 New Zealand Daylight Time"
> [12] "2003-03-15 22:20:00 New Zealand Daylight Time"
> [13] "2003-03-15 22:21:00 New Zealand Daylight Time"
> [14] "2003-03-16 00:02:00 New Zealand Daylight Time"
> [15] "2003-03-16 18:10:00 New Zealand Standard Time"
> [16] "2003-03-16 18:20:00 New Zealand Standard Time"
> [17] "2003-03-16 18:30:00 New Zealand Standard Time"
> 
> My problem is that "15/03/2003 12:02" has become "16/03/2003 00:02"
> i.e.  it is 12 hours behind (as is everything else), but also, I do not
> want to change time zones.
> 
> The 12 hour delay is not really a problem just an annoyance, but the
> time zone change is a problem because later on I need to match up data
> by time using
> 
> mindata<-seq(from=min(datetime),to=max(datetime),by="mins")
> newdata<-matrix(0,length(mindata),1)
> newdata[match(format.POSIXct(datetime,"%Y %m %d %H
> %M"),format.POSIXct(mindata,"%Y %m %d %H %M"))]<-data$V3
> 
> and things go wrong here with matching repeating times/missing times
> around the timezone changes and, my resulting vector is 1 hour shorter
> than my other series.  From the R help I see that my OS may be to blame
> but, even if I specify tz="GMT" I still get NZST and NZDT.  Can someone
> help?
> 
> I hope this all makes sense
> 
> Carla
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From huihan at yahoo-inc.com  Wed May 11 21:43:54 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Wed, 11 May 2005 12:43:54 -0700
Subject: [R] density function
In-Reply-To: <4281A87B.6040702@columbia.edu>
References: <42813948.1060701@yahoo-inc.com>	<Pine.LNX.4.61.0505110627110.27930@gannet.stats>
	<42819FDA.1050208@yahoo-inc.com> <4281A87B.6040702@columbia.edu>
Message-ID: <4282607A.6020009@yahoo-inc.com>


Thank you so much, Suresh. I searched a lot on "density" among R email 
archives. Should have searched using "derivative".

Hui

Suresh Krishna wrote:

>
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/20509.html
>
> -s.
>
> Hui Han wrote:
>
>> Thank you very much, Professor Ripley.
>>
>> If possible, could you point me to other packages that you think I 
>> should look at for estimating a derivative?
>>
>> Best regards,
>> Hui
>>
>> Prof Brian Ripley wrote:
>>
>>> On Tue, 10 May 2005, Hui Han wrote:
>>>
>>>> I wonder if the function "density" outputs the gaussian mixture 
>>>> formula that is estimated from the input data, assuming a gaussian 
>>>> model is used at each data point ?  I want to take the derivative 
>>>> of the finally estimated gaussian mixture formula for further 
>>>> analysis.
>>>
>>>
>>>
>>>
>>> It is a kernel density estimate: a rather trivial mixture, not 
>>> necessarily Gaussian.  Also, it is not set up to optimally estimate 
>>> a derivative, and you should look at more sophisticated methods in 
>>> other packages if you want to do that.
>>>
>>> As to what "density" outputs: see its help page.
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
>
>



From uofiowa at gmail.com  Wed May 11 21:44:41 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 11 May 2005 15:44:41 -0400
Subject: [R] aggregate
Message-ID: <3f87cc6d0505111244715079d4@mail.gmail.com>

I have a data frame of daily open, high, low and settle prices. How
can I aggregate this data weekly?
The data frame has five columns, the first is the date column and the
rest are the prices.



From uofiowa at gmail.com  Wed May 11 21:48:06 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 11 May 2005 15:48:06 -0400
Subject: [R] RODBC and autocommit
Message-ID: <3f87cc6d05051112482cd010e5@mail.gmail.com>

the Informix odbc client allows me to set the autocommet on or off
using its C interface. Does RODBC allow me to pass in this parameter
to the odbc client ?



From br44114 at gmail.com  Wed May 11 22:06:31 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 11 May 2005 16:06:31 -0400
Subject: [R] aggregate
Message-ID: <8d5a363505051113066b3c1ef6@mail.gmail.com>

Assuming dfr["day","o","h","l","c"] and day like 2004-12-28:
dt <- strptime(as.character(dfr$day),format="%Y-%m-%d") + 0
wk <- format(dt,"%Yw%U")
aggr <- aggregate(list(dfr$o,dfr$h,dfr$l,dfr$c),list(wk),mean)
colnames(aggr) <- etc


-----Original Message-----
From: Omar Lakkis [mailto:uofiowa at gmail.com]
Sent: Wednesday, May 11, 2005 3:45 PM
To: r-help at stat.math.ethz.ch
Subject: [R] aggregate


I have a data frame of daily open, high, low and settle prices. How
can I aggregate this data weekly?
The data frame has five columns, the first is the date column and the
rest are the prices.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Achim.Zeileis at wu-wien.ac.at  Wed May 11 21:59:50 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 11 May 2005 21:59:50 +0200
Subject: [R] aggregate
In-Reply-To: <3f87cc6d0505111244715079d4@mail.gmail.com>
References: <3f87cc6d0505111244715079d4@mail.gmail.com>
Message-ID: <20050511215950.61ca89f4.Achim.Zeileis@wu-wien.ac.at>

On Wed, 11 May 2005 15:44:41 -0400 Omar Lakkis wrote:

> I have a data frame of daily open, high, low and settle prices. How
> can I aggregate this data weekly?

When you transform your data from data.frame to a "zoo" time series
object (in package zoo) then you can use the corresponding aggregate
method. See ?aggregate.zoo and the vignette in the package zoo.

Also look at the archives of the finance SIG mailing list which
recently discussed how to do this using the zoo or the its package.
Z

> The data frame has five columns, the first is the date column and the
> rest are the prices.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From br44114 at gmail.com  Wed May 11 22:14:37 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 11 May 2005 16:14:37 -0400
Subject: [R] aggregate
In-Reply-To: <8d5a363505051113066b3c1ef6@mail.gmail.com>
References: <8d5a363505051113066b3c1ef6@mail.gmail.com>
Message-ID: <8d5a363505051113142addb2e8@mail.gmail.com>

In fact since you have dates and not datetimes use as.Date() instead
of strptime().


On 5/11/05, bogdan romocea wrote:
> Assuming dfr["day","o","h","l","c"] and day like 2004-12-28:
> dt <- strptime(as.character(dfr$day),format="%Y-%m-%d") + 0
> wk <- format(dt,"%Yw%U")
> aggr <- aggregate(list(dfr$o,dfr$h,dfr$l,dfr$c),list(wk),mean)
> colnames(aggr) <- etc
> 
> -----Original Message-----
> From: Omar Lakkis [mailto:uofiowa at gmail.com]
> Sent: Wednesday, May 11, 2005 3:45 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] aggregate
> 
> I have a data frame of daily open, high, low and settle prices. How
> can I aggregate this data weekly?
> The data frame has five columns, the first is the date column and the
> rest are the prices.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rxg218 at psu.edu  Wed May 11 22:27:13 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Wed, 11 May 2005 16:27:13 -0400
Subject: [R] entropy and conditional entropy for continous variables
Message-ID: <1115843233.32309.20.camel@blue.chem.psu.edu>

Hi,
  this is not a R question per se, but since I'm on the lookout for an R
solution I thought this was the best place:

I would like to calculate the entropy for a variable and the conditional
entropy between two variables, H(X|Y) for variables X & Y

I have coded the case for the categorical case but I'm having problems
understanding how to do it for the continous case.

>From what I understand, for continous variables the entropy would be

\integral_{A} f(x) log( f(x) ) dx

where f(x) is the density of X and A is the support.

I have tried obtaining the density of X by using the density estimation
methods of the KernSmooth package and then doing a numerical integration
- however I am confused about how to interpret the support in this
context.

So the question  is, are there any R functions to determing the entropy
and conditional entropy for continous variables? (Searching CRAN did not
turn up anything significant)

Thanks,



-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
All theoretical chemistry is really physics; and all theoretical
chemists 
know it.
-- Richard P. Feynman



From sdavis2 at mail.nih.gov  Wed May 11 23:22:03 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 11 May 2005 17:22:03 -0400
Subject: [R] assigning to a list in a package environment
Message-ID: <c6192184b8423427f853348ca8ae08d4@mail.nih.gov>

I have a list in a package environment

assign('refflat',list(),pos='package:locPkg')

to which I would like to make assignments like:

refflat[['a']] <- read.table('fileA.txt')
refflat[['b']] <- read.table('fileB.txt')

I am doing this to guard against a local version of refflat hanging 
around, as I want to refresh it with each new session (and so, want to 
store it in the package environment).  I just can't quite get hot to 
make that work so that I am storing to the package:refflat rather than 
any in .GlobalEnv.

Thanks,
Sean



From darrenleeweber at gmail.com  Thu May 12 00:40:09 2005
From: darrenleeweber at gmail.com (Darren Weber)
Date: Wed, 11 May 2005 15:40:09 -0700
Subject: [R] 2 factor ANOVA and sphericity
Message-ID: <d2095b8c0505111540fb9513f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050511/4bbd3719/attachment.pl

From David.Brahm at geodecapital.com  Thu May 12 00:41:18 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Wed, 11 May 2005 18:41:18 -0400
Subject: [R] string syntactic sugar in R? - long post
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBC0@MSGBOSCLF2WIN.DMN1.FMR.COM>

charles loboz <charles_loboz at yahoo.com> wrote:
> A gstring is a string with variable names embedded and replaced by
> values(converted to strings, lazy eval) before use.

I use the following function, which will take variables either from
named arguments or from the environment.  It also concatenates all
unnamed arguments (with sep="") as a convenience for long strings.

g.p <- function(..., esc="\\$", sep="", collapse=" ", parent=1) {
  a <- lapply(list(...), as.character)
  n <- names(a);  if (is.null(n)) n <- rep("", length(a))
  s <- do.call("paste", c(a[n==""], sep=sep, collapse=collapse))
  for (i in which(n != "")) s <- gsub(paste(esc,n[i],sep=""), a[[i]], s)
  while ((r <- regexpr(paste(esc,"\\w*",sep=""), s)) > 0) {
    v <- substring(s, r+1, r+attr(r,"match.length")-1)
    s <- if (v=="") paste(substring(s,1,r-1), substring(s,r+2), sep="")
         else gsub(paste(esc,v,sep=""),
              as.character(eval.parent(parse(text=v), parent)), s)
  }
  s
}

Here's a simple example:

R> alpha <- 8
R> g.p("the result is $alpha with the comment $beta",
       beta="xyz")
   [1] "the result is 8 with the comment xyz"

-- David Brahm (brahm at alum.mit.edu)



From rich.fitzjohn at gmail.com  Thu May 12 00:42:16 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Thu, 12 May 2005 10:42:16 +1200
Subject: [R] time zones, daylight saving etc.
In-Reply-To: <971536df05051112232b064070@mail.gmail.com>
References: <42817BF9.5000705@student.canterbury.ac.nz>
	<971536df05051112232b064070@mail.gmail.com>
Message-ID: <5934ae57050511154212c5301e@mail.gmail.com>

Hi,

seq.dates() in the chron package does not allow creating sequences by
minutes, so you'd have to roll your own sequence generator.

Looks like the tzone attribute of the times is lost when using min(),
max() and seq().  You can apply it back manually, but it does not
affect the calculation, since POSIXct times are stored as seconds
since 1/1/1970 (?DateTimeClasses).

## These dates/times just span the move from NZDT to NZST:
dt.dates <- paste(rep(15:16, c(5,7)), "03", "2003", sep="/")
dt.times <- paste(c(19:23, 0:6), "05", sep=":")
dt <- paste(dt.dates, dt.times)

## No shift in times, or worrying about daylight savings; appropriate
## iff the device doing the recording was not itself adjusting for
## daylight savings, presumably.
datetime <- as.POSIXct(strptime(dt, "%d/%m/%Y %H:%M"), "GMT")

## Create two objects with all the times in your range, one with the
## tzone attribute set back to GMT (to match datetimes), and one
## without this.
mindata1 <- mindata2 <- seq(from=min(datetime), to=max(datetime),
                            by="mins")
attr(mindata2, "tzone") <- "GMT"

fmt <- "%Y %m %d %H %M"
## These both do the matching correctly:
match(format(datetime, fmt), format(mindata1, fmt, tz="GMT"))
match(format(datetime, fmt), format(mindata2, fmt, tz="GMT"))

## However, the first of these will not, as it gets the timezone all
## wrong, since it's neither specified in the call to format(), or as
## an attribute of the POSIXct object.
match(format(datetime, fmt), format(mindata1, fmt))
match(format(datetime, fmt), format(mindata2, fmt))

## It is also possible to run match() directly off the POSIXct object,
## but I'm not sure how this will interact with things like leap
## seconds:
match(datetime, mindata1)

Time zones do my head in, so you probably want to check this all
pretty carefully.  Looks like there's lots of gotchas (e.g. subsetting
a POSIXct object strips the tzone attribute).

Cheers,
Rich

On 5/12/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> You could use the chron package.  It represents date times without
> using time zones so you can't have this sort of problem.
> 
> On 5/10/05, Carla Meurk <ksm32 at student.canterbury.ac.nz> wrote:
> > Hi,  I have a whole bunch of data, which looks like:
> >
> > 15/03/2003       10:20  1
> > 15/03/2003       10:21  0
> > 15/03/2003       12:02  0
> > 16/03/2003       06:10  0
> > 16/03/2003       06:20  0.5
> > 16/03/2003       06:30  0
> > 16/03/2003       06:40  0
> > 16/03/2003       06:50  0
> >
> > 18/03/2003  20:10                 0.5
> > etc. (times given on a 24 hour clock)
> >
> > and goes on for years.  I have some code:
> >
> > data<-read.table("H:/rainfall_data.txt",h=T)
> > library(survival)
> > datetime <- as.POSIXct(strptime(paste(data$V1, data$V2), "%d/%m/%Y
> > %H:%M"), tz="NZST")
> >
> > which produces:
> >
> > [10] "2003-03-13 21:13:00 New Zealand Daylight Time"
> > [11] "2003-03-15 13:20:00 New Zealand Daylight Time"
> > [12] "2003-03-15 22:20:00 New Zealand Daylight Time"
> > [13] "2003-03-15 22:21:00 New Zealand Daylight Time"
> > [14] "2003-03-16 00:02:00 New Zealand Daylight Time"
> > [15] "2003-03-16 18:10:00 New Zealand Standard Time"
> > [16] "2003-03-16 18:20:00 New Zealand Standard Time"
> > [17] "2003-03-16 18:30:00 New Zealand Standard Time"
> >
> > My problem is that "15/03/2003 12:02" has become "16/03/2003 00:02"
> > i.e.  it is 12 hours behind (as is everything else), but also, I do not
> > want to change time zones.
> >
> > The 12 hour delay is not really a problem just an annoyance, but the
> > time zone change is a problem because later on I need to match up data
> > by time using
> >
> > mindata<-seq(from=min(datetime),to=max(datetime),by="mins")
> > newdata<-matrix(0,length(mindata),1)
> > newdata[match(format.POSIXct(datetime,"%Y %m %d %H
> > %M"),format.POSIXct(mindata,"%Y %m %d %H %M"))]<-data$V3
> >
> > and things go wrong here with matching repeating times/missing times
> > around the timezone changes and, my resulting vector is 1 hour shorter
> > than my other series.  From the R help I see that my OS may be to blame
> > but, even if I specify tz="GMT" I still get NZST and NZDT.  Can someone
> > help?
> >
> > I hope this all makes sense
> >
> > Carla
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From bitwrit at ozemail.com.au  Thu May 12 10:53:22 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 12 May 2005 08:53:22 +0000
Subject: [R] display two pie-charts
In-Reply-To: <42820C00.7030704@pik-potsdam.de>
References: <42820C00.7030704@pik-potsdam.de>
Message-ID: <42831982.5020104@ozemail.com.au>

Lars wrote:
> Hey,
> 
> i'd like to compose a clock-like looking plot composed out of two 
> circles, each showing the length of a period (to compare them). first, 
> to do so, it looked the easiest by using pie(), just puting multiple 
> pie-charts over each other. the problem is that once the second pie is 
> drawn, it replaces the first one. does anybody know how to add a second, 
> smaller pie over an existing one, like a layer?
> 
Hi Lars,

You might be able to do what you want with either floating.pie() or 
clock24.plot() in the plotrix package. I can't try it out right now, but 
I'll have a look later today.

Jim



From ForresterG at landcareresearch.co.nz  Thu May 12 01:08:17 2005
From: ForresterG at landcareresearch.co.nz (Guy Forrester)
Date: Thu, 12 May 2005 11:08:17 +1200
Subject: [R] Graphics file to disk
Message-ID: <s283392a.056@smtp.landcareresearch.co.nz>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/197db820/attachment.pl

From tk at tariqkhan.org  Thu May 12 01:14:16 2005
From: tk at tariqkhan.org (Tariq Khan)
Date: Thu, 12 May 2005 00:14:16 +0100
Subject: [R] RODBC Oracle and VB automation with R(D)COM
Message-ID: <000001c5567f$2a91a2f0$0f00000a@LAPTOP>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/03a827cb/attachment.pl

From gerifalte28 at hotmail.com  Thu May 12 03:13:02 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 12 May 2005 01:13:02 +0000
Subject: [R] Graphics file to disk
In-Reply-To: <s283392a.056@smtp.landcareresearch.co.nz>
Message-ID: <BAY103-F172D7401FA3BF21EC52A98A6110@phx.gbl>

Hi Guy

Try savePlot("MyPlot", "bmp").

Cheers

Francisco

PS: Hang in there! In the long run the effort to move from S-Plus to R is 
definitively worthy!

>From: "Guy Forrester" <ForresterG at landcareresearch.co.nz>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] Graphics file to disk
>Date: Thu, 12 May 2005 11:08:17 +1200
>
>Dear All,
>
>I have some code that works in S-Plus for writing saving a graphics file to 
>disk :-
>
>graphsheet(type = "auto", format = "WMF", file = "G:\\north0l.wmf",
>      pages = "auto", print.background = F,
>      orientation="landscape",
>      color.style="color")
>
>plot(x,y)
>
>dev.off()
>
>This works fine in S-Plus.
>
>I have tried playing with the 'windows' command in `R' with out success.  I 
>would be grateful for any pointers in the right direction
>
>Many thanks in advance
>
>Guy
>
>
>
>
>--------------------------------------------------------
>Guy J Forrester
>Biometrician
>Manaaki Whenua - Landcare Research
>PO Box 69, Lincoln, New Zealand.
>Tel. +64 3 325 6701 x3738
>Fax +64 3 325 2418
>E-mail ForresterG at LandcareResearch.co.nz
>www.LandcareResearch.co.nz
>
>
>
>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>WARNING: This email and any attachments may be confidential and/or
>privileged. They are intended for the addressee only and are not to be 
>read,
>used, copied or disseminated by anyone receiving them in error.  If you are
>not the intended recipient, please notify the sender by return email and
>delete this message and any attachments.
>
>The views expressed in this email are those of the sender and do not
>necessarily reflect the official views of Landcare Research.
>
>Landcare Research
>http://www.landcareresearch.co.nz
>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From Glen.Jones at team.telstra.com  Thu May 12 03:35:03 2005
From: Glen.Jones at team.telstra.com (Jones, Glen R)
Date: Thu, 12 May 2005 11:35:03 +1000
Subject: [R] Multiple IF statements - is there a better alternative?
Message-ID: <5D01E8305096D3119D7D00508B5EBBF4164C8EEE@ntmsg0133.corpmail.telstra.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/967d98b9/attachment.pl

From ForresterG at landcareresearch.co.nz  Thu May 12 04:07:50 2005
From: ForresterG at landcareresearch.co.nz (Guy Forrester)
Date: Thu, 12 May 2005 14:07:50 +1200
Subject: [R] Graphics rile to disk
Message-ID: <s283633c.057@smtp.landcareresearch.co.nz>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/dd0d3445/attachment.pl

From Mike.Prager at noaa.gov  Thu May 12 04:11:02 2005
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Wed, 11 May 2005 22:11:02 -0400
Subject: [R] Graphics file to disk
In-Reply-To: <s283392a.056@smtp.landcareresearch.co.nz>
References: <s283392a.056@smtp.landcareresearch.co.nz>
Message-ID: <4282BB36.2030400@noaa.gov>

If you want to write directly to file,

?Devices

will get you more information



Guy Forrester wrote on 5/11/2005 7:08 PM:

>Dear All,
> 
>I have some code that works in S-Plus for writing saving a graphics file to disk :-
> 
>[...]
>

-- 
Michael H. Prager, Ph.D.
Population Dynamics Team
NOAA Center for Coastal Habitat and Fisheries Research
NMFS Southeast Fisheries Science Center
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/



From mbp140 at psu.edu  Thu May 12 04:42:45 2005
From: mbp140 at psu.edu (martin peters)
Date: Wed, 11 May 2005 22:42:45 -0400
Subject: [R] pls -- crossval vs plsr(..., CV=TRUE)
Message-ID: <9466f3f90ea00fb3ce09756cd3cce9ee@psu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050511/1cdb9230/attachment.pl

From p.connolly at hortresearch.co.nz  Thu May 12 04:57:31 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 12 May 2005 14:57:31 +1200
Subject: [R] Multiple IF statements - is there a better alternative?
In-Reply-To: <5D01E8305096D3119D7D00508B5EBBF4164C8EEE@ntmsg0133.corpmail.te
	lstra.com.au>
References: <5D01E8305096D3119D7D00508B5EBBF4164C8EEE@ntmsg0133.corpmail.tel
	stra.com.au>
Message-ID: <20050512025731.GE5776@hortresearch.co.nz>

On Thu, 12-May-2005 at 11:35AM +1000, Jones, Glen R wrote:

|> Hello,
|> 
|> Rather than rely on a set of IF statements (as there could be many -
|> please see below)), could the following be done in a different/better
|> way?
|> 
|> InternalMean <- mean(data1[,3])
|> 
|> if (InternalMean == 0)
|>     Intresult = 1
|> if (InternalMean > 0 & InternalMean < 1)
|>     Intresult = .95
|> if (InternalMean >= 1 & InternalMean < 2)
|>     Intresult = .85
|> if (InternalMean >= 2 & InternalMean < 4)
|>     Intresult = .70
|> ...
|> if (InternalMean >= 9)
|>     Intresult = .0

?switch

HTH

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ssk2031 at columbia.edu  Thu May 12 05:42:34 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Wed, 11 May 2005 23:42:34 -0400
Subject: [R] Multiple IF statements - is there a better alternative?
In-Reply-To: <5D01E8305096D3119D7D00508B5EBBF4164C8EEE@ntmsg0133.corpmail.telstra.com.au>
References: <5D01E8305096D3119D7D00508B5EBBF4164C8EEE@ntmsg0133.corpmail.telstra.com.au>
Message-ID: <4282D0AA.3090604@columbia.edu>


are you looking for something like:

InternalMean <- mean(data1[,3])

TestValues <- c(0,1,2,4,9) #should be in increasing order
TestResults <- c(.95, .85, .7, NaN,0)

if (InternalMean==0) IntResult=1 else 
IntResult=TestResults[which(TestValues==max(TestValues[TestValues<InternalMean]))]

-s.

Jones, Glen R wrote:
> Hello,
> 
> Rather than rely on a set of IF statements (as there could be many -
> please see below)), could the following be done in a different/better
> way?
> 
> InternalMean <- mean(data1[,3])
> 
> if (InternalMean == 0)
>     Intresult = 1
> if (InternalMean > 0 & InternalMean < 1)
>     Intresult = .95
> if (InternalMean >= 1 & InternalMean < 2)
>     Intresult = .85
> if (InternalMean >= 2 & InternalMean < 4)
>     Intresult = .70
> ...
> if (InternalMean >= 9)
>     Intresult = .0
> 
> Thanks in advance
> 
> 
> Glen Jones
> Value Analyst
> Industry Framework Governance
> Telstra Corporation Limited
> 
>>Tel: (03) 9634 7280
> 
> email: glen.jones at team.telstra.com
> 
> 
>>The information contained in this e-mail message may be confidential.
>>If you are not the intended recipient, any use of, interference with,
>>disclosure or copying of this material is unauthorised and prohibited.
>>If you have received this message in error, please notify me by reply
>>e-mail and then delete the message.
>>
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ssk2031 at columbia.edu  Thu May 12 06:01:41 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Thu, 12 May 2005 00:01:41 -0400
Subject: [R] Multiple IF statements - is there a better alternative?
In-Reply-To: <4282D0AA.3090604@columbia.edu>
References: <5D01E8305096D3119D7D00508B5EBBF4164C8EEE@ntmsg0133.corpmail.telstra.com.au>
	<4282D0AA.3090604@columbia.edu>
Message-ID: <4282D525.6040407@columbia.edu>


oops, i meant something more like:

TestValues <- c(0,1,2,4,9) #should be in increasing order
TestResults <- c(.95, .85, .7, NaN,0)

if (InternalMean==0) IntResult=1 else 
IntResult=TestResults[TestValues==max(TestValues[TestValues<=InternalMean])]

-s.

Suresh Krishna wrote:
> 
> are you looking for something like:
> 
> InternalMean <- mean(data1[,3])
> 
> TestValues <- c(0,1,2,4,9) #should be in increasing order
> TestResults <- c(.95, .85, .7, NaN,0)
> 
> if (InternalMean==0) IntResult=1 else 
> IntResult=TestResults[which(TestValues==max(TestValues[TestValues<InternalMean]))] 
> 
> 
> -s.
> 
> Jones, Glen R wrote:
> 
>> Hello,
>>
>> Rather than rely on a set of IF statements (as there could be many -
>> please see below)), could the following be done in a different/better
>> way?
>>
>> InternalMean <- mean(data1[,3])
>>
>> if (InternalMean == 0)
>>     Intresult = 1
>> if (InternalMean > 0 & InternalMean < 1)
>>     Intresult = .95
>> if (InternalMean >= 1 & InternalMean < 2)
>>     Intresult = .85
>> if (InternalMean >= 2 & InternalMean < 4)
>>     Intresult = .70
>> ...
>> if (InternalMean >= 9)
>>     Intresult = .0
>>
>> Thanks in advance
>>
>>
>> Glen Jones
>> Value Analyst
>> Industry Framework Governance
>> Telstra Corporation Limited
>>
>>> Tel: (03) 9634 7280
>>
>>
>> email: glen.jones at team.telstra.com
>>
>>
>>> The information contained in this e-mail message may be confidential.
>>> If you are not the intended recipient, any use of, interference with,
>>> disclosure or copying of this material is unauthorised and prohibited.
>>> If you have received this message in error, please notify me by reply
>>> e-mail and then delete the message.
>>>
>>
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jsorkin at grecc.umaryland.edu  Thu May 12 06:50:43 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Thu, 12 May 2005 00:50:43 -0400
Subject: [R] 2 factor ANOVA and sphericity
Message-ID: <s282a87f.042@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/b1f6e2ec/attachment.pl

From ripley at stats.ox.ac.uk  Thu May 12 07:15:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 May 2005 06:15:28 +0100 (BST)
Subject: [R] time zones, daylight saving etc.
In-Reply-To: <5934ae57050511154212c5301e@mail.gmail.com>
References: <42817BF9.5000705@student.canterbury.ac.nz>
	<971536df05051112232b064070@mail.gmail.com>
	<5934ae57050511154212c5301e@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505120610580.18101@gannet.stats>

Would it not just be easier to set the timezone to GMT for the duration of 
the calculations?  I don't see an OS mentioned here, but on most TZ=GMT
for the session will do it.

On Thu, 12 May 2005, Rich FitzJohn wrote:

> Hi,
>
> seq.dates() in the chron package does not allow creating sequences by
> minutes, so you'd have to roll your own sequence generator.
>
> Looks like the tzone attribute of the times is lost when using min(),
> max() and seq().  You can apply it back manually, but it does not
> affect the calculation, since POSIXct times are stored as seconds
> since 1/1/1970 (?DateTimeClasses).
>
> ## These dates/times just span the move from NZDT to NZST:
> dt.dates <- paste(rep(15:16, c(5,7)), "03", "2003", sep="/")
> dt.times <- paste(c(19:23, 0:6), "05", sep=":")
> dt <- paste(dt.dates, dt.times)
>
> ## No shift in times, or worrying about daylight savings; appropriate
> ## iff the device doing the recording was not itself adjusting for
> ## daylight savings, presumably.
> datetime <- as.POSIXct(strptime(dt, "%d/%m/%Y %H:%M"), "GMT")
>
> ## Create two objects with all the times in your range, one with the
> ## tzone attribute set back to GMT (to match datetimes), and one
> ## without this.
> mindata1 <- mindata2 <- seq(from=min(datetime), to=max(datetime),
>                            by="mins")
> attr(mindata2, "tzone") <- "GMT"
>
> fmt <- "%Y %m %d %H %M"
> ## These both do the matching correctly:
> match(format(datetime, fmt), format(mindata1, fmt, tz="GMT"))
> match(format(datetime, fmt), format(mindata2, fmt, tz="GMT"))
>
> ## However, the first of these will not, as it gets the timezone all
> ## wrong, since it's neither specified in the call to format(), or as
> ## an attribute of the POSIXct object.
> match(format(datetime, fmt), format(mindata1, fmt))
> match(format(datetime, fmt), format(mindata2, fmt))
>
> ## It is also possible to run match() directly off the POSIXct object,
> ## but I'm not sure how this will interact with things like leap
> ## seconds:
> match(datetime, mindata1)
>
> Time zones do my head in, so you probably want to check this all
> pretty carefully.  Looks like there's lots of gotchas (e.g. subsetting
> a POSIXct object strips the tzone attribute).
>
> Cheers,
> Rich
>
> On 5/12/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> You could use the chron package.  It represents date times without
>> using time zones so you can't have this sort of problem.
>>
>> On 5/10/05, Carla Meurk <ksm32 at student.canterbury.ac.nz> wrote:
>>> Hi,  I have a whole bunch of data, which looks like:
>>>
>>> 15/03/2003       10:20  1
>>> 15/03/2003       10:21  0
>>> 15/03/2003       12:02  0
>>> 16/03/2003       06:10  0
>>> 16/03/2003       06:20  0.5
>>> 16/03/2003       06:30  0
>>> 16/03/2003       06:40  0
>>> 16/03/2003       06:50  0
>>>
>>> 18/03/2003  20:10                 0.5
>>> etc. (times given on a 24 hour clock)
>>>
>>> and goes on for years.  I have some code:
>>>
>>> data<-read.table("H:/rainfall_data.txt",h=T)
>>> library(survival)
>>> datetime <- as.POSIXct(strptime(paste(data$V1, data$V2), "%d/%m/%Y
>>> %H:%M"), tz="NZST")
>>>
>>> which produces:
>>>
>>> [10] "2003-03-13 21:13:00 New Zealand Daylight Time"
>>> [11] "2003-03-15 13:20:00 New Zealand Daylight Time"
>>> [12] "2003-03-15 22:20:00 New Zealand Daylight Time"
>>> [13] "2003-03-15 22:21:00 New Zealand Daylight Time"
>>> [14] "2003-03-16 00:02:00 New Zealand Daylight Time"
>>> [15] "2003-03-16 18:10:00 New Zealand Standard Time"
>>> [16] "2003-03-16 18:20:00 New Zealand Standard Time"
>>> [17] "2003-03-16 18:30:00 New Zealand Standard Time"
>>>
>>> My problem is that "15/03/2003 12:02" has become "16/03/2003 00:02"
>>> i.e.  it is 12 hours behind (as is everything else), but also, I do not
>>> want to change time zones.
>>>
>>> The 12 hour delay is not really a problem just an annoyance, but the
>>> time zone change is a problem because later on I need to match up data
>>> by time using
>>>
>>> mindata<-seq(from=min(datetime),to=max(datetime),by="mins")
>>> newdata<-matrix(0,length(mindata),1)
>>> newdata[match(format.POSIXct(datetime,"%Y %m %d %H
>>> %M"),format.POSIXct(mindata,"%Y %m %d %H %M"))]<-data$V3
>>>
>>> and things go wrong here with matching repeating times/missing times
>>> around the timezone changes and, my resulting vector is 1 hour shorter
>>> than my other series.  From the R help I see that my OS may be to blame
>>> but, even if I specify tz="GMT" I still get NZST and NZDT.  Can someone
>>> help?
>>>
>>> I hope this all makes sense
>>>
>>> Carla
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>
> -- 
> Rich FitzJohn
> rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
>                      You are in a maze of twisty little functions, all alike
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From calenge at biomserv.univ-lyon1.fr  Thu May 12 09:47:27 2005
From: calenge at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?Cl=E9ment_Calenge?=)
Date: Thu, 12 May 2005 09:47:27 +0200
Subject: [R] [R-pkgs] Package adehabitat version 1.3
Message-ID: <42830A0F.2010206@biomserv.univ-lyon1.fr>

Dear all,

I have just uploaded to CRAN the version 1.3 of the
package 'adehabitat'. Significant changes are
listed below:

- Several functions allowing the conversion from
  and to classes of the package sp have been added.
  Maps of class "asc", "kasc", "area", as well as
  radio-monitoring of class "traj" can now be
  converted to the spatial classes available in sp.

- The function buffer.line() allows to compute buffer
  around a line

- The function distfacmap() converts one factor map
  into a set of maps giving the distance to the
  patches belonging to the different levels.

- The kernel home-range estimation can now be fitted
  using an Epanechnikov kernel (though it is still
  optional), resulting in a quicker estimation.

- The former function enfa() has been modified. It
  now requires a data frame (available units) and
  a vector (use of each unit). If the data consist
  of a point pattern and of an object kasc, the
  function data2enfa is to be used before.

- The function angles() now takes an argument
  indicating how successive relocations
  located at the same place have to be managed.

- The function as.traj now requires at least 2
  relocations to return a trajectory

- That's all, thanks for reading !

Questions, comments and suggestions are greatly appreciated.
Happy testing,


Cl??ment Calenge

-- 
Cl??ment CALENGE
LBBE - UMR CNRS 5558 - Universit?? 
Claude Bernard Lyon 1 - FRANCE
tel. (+33) 04.72.43.27.57
fax. (+33) 04.72.43.13.88

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From vincent at 7d4.com  Thu May 12 09:58:01 2005
From: vincent at 7d4.com (vincent)
Date: Thu, 12 May 2005 09:58:01 +0200
Subject: [R] Graphics file to disk
In-Reply-To: <s283392a.056@smtp.landcareresearch.co.nz>
References: <s283392a.056@smtp.landcareresearch.co.nz>
Message-ID: <42830C89.6040609@7d4.com>

for example (works also with png and jpg)

bmp("mypic.bmp");
plot(...);
dev.off();

hih



From sabine.navarre at siemens.com  Thu May 12 09:59:28 2005
From: sabine.navarre at siemens.com (Navarre Sabine (stu))
Date: Thu, 12 May 2005 09:59:28 +0200
Subject: [R] sqlQuery
Message-ID: <C0A2DDBDA4904048820B3477D766FE3C01D313FA@tlsm385a.ww011.siemens.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/03d8293a/attachment.pl

From ernesto at ipimar.pt  Thu May 12 10:27:52 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 12 May 2005 09:27:52 +0100
Subject: [R] sqlQuery
In-Reply-To: <C0A2DDBDA4904048820B3477D766FE3C01D313FA@tlsm385a.ww011.siemens.net>
References: <C0A2DDBDA4904048820B3477D766FE3C01D313FA@tlsm385a.ww011.siemens.net>
Message-ID: <42831388.20705@ipimar.pt>

Navarre Sabine (stu) wrote:

>Hello,
> 
> 
>(sqlQuery(channel, "select * from......"))
> 
>I would like to know if it's possible to put a file name in the
>parameters of the function 'sqlQuery' instead of putting the query .
>If it's possible, what kind of extension for my file have I need?
> 
>That's for your help
> 
>Sabine 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

I think you can just create a character vector with the sql string and 
call sqlQuery(chanel, myfile)

Regards

EJ



From steve.roberts at manchester.ac.uk  Thu May 12 10:30:10 2005
From: steve.roberts at manchester.ac.uk (Steve Roberts)
Date: Thu, 12 May 2005 09:30:10 +0100
Subject: [R] R2.1.0: Bug in list.files
Message-ID: <42832222.21568.4137BD@fs1.ser.man.ac.uk>

R2.0.1 (MS Windows)

> list.files(myloc,"*.zip",full=T)
[1] "P:/SARsoftware/Rlibraries/gnlm_0.1.zip"
[2] "P:/SARsoftware/Rlibraries/lms2_0.2.zip"


R2.1.0:

> list.files(myloc,"*.zip",full=T)
Error in list.files(path, pattern, all.files, full.names, recursive) : 
        invalid 'pattern' regular expression

Bug? or have I missed something

Steve.
  Dr Steve Roberts 
  steve.roberts at manchester.ac.uk

Senior Lecturer in Medical Statistics,
CMMCH NHS Trust and University of Manchester Biostatistics Group,
0161 275 5192/5764 / 0161 276 5785



From ligges at statistik.uni-dortmund.de  Thu May 12 10:45:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 12 May 2005 10:45:03 +0200
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
Message-ID: <4283178F.9070504@statistik.uni-dortmund.de>

Steve Roberts wrote:

> R2.0.1 (MS Windows)
> 
> 
>>list.files(myloc,"*.zip",full=T)
> 
> [1] "P:/SARsoftware/Rlibraries/gnlm_0.1.zip"
> [2] "P:/SARsoftware/Rlibraries/lms2_0.2.zip"
> 
> 
> R2.1.0:
> 
> 
>>list.files(myloc,"*.zip",full=T)
> 
> Error in list.files(path, pattern, all.files, full.names, recursive) : 
>         invalid 'pattern' regular expression
 >
> Bug? or have I missed something

You missed to read the NEWS that tells you:

     o   The regular expression code is now based on that in glibc 2.3.3.
     It has stricter conformance to POSIX, so metachars such as
     { } + * may need to be escaped where before they did not
     (but could have been).


Probably you want

  list.files(pattern = "\\.zip$", full.names = TRUE)

Uwe Ligges


> Steve.
>   Dr Steve Roberts 
>   steve.roberts at manchester.ac.uk
> 
> Senior Lecturer in Medical Statistics,
> CMMCH NHS Trust and University of Manchester Biostatistics Group,
> 0161 275 5192/5764 / 0161 276 5785
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ssk2031 at columbia.edu  Thu May 12 10:48:33 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Thu, 12 May 2005 04:48:33 -0400
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <4283178F.9070504@statistik.uni-dortmund.de>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
	<4283178F.9070504@statistik.uni-dortmund.de>
Message-ID: <42831861.1070103@columbia.edu>


Is that the entire story ? I tried this with yesterday's patched version 
(windows xp) and found:

 > list.files(getwd(),"*.txt",full=T)
Error in list.files(path, pattern, all.files, full.names, recursive) :
         invalid 'pattern' regular expression

 > list.files(getwd(),'.txt',full=T)
[1] "C:/Documents and Settings/suresh/BDE_SysInfo.txt"
[2] "C:/Documents and Settings/suresh/dxva_sig.txt"

Replacing "*.txt" with '*.txt' seems to do "something".

-s.


Uwe Ligges wrote:
> Steve Roberts wrote:
> 
>> R2.0.1 (MS Windows)
>>
>>
>>> list.files(myloc,"*.zip",full=T)
>>
>>
>> [1] "P:/SARsoftware/Rlibraries/gnlm_0.1.zip"
>> [2] "P:/SARsoftware/Rlibraries/lms2_0.2.zip"
>>
>>
>> R2.1.0:
>>
>>
>>> list.files(myloc,"*.zip",full=T)
>>
>>
>> Error in list.files(path, pattern, all.files, full.names, recursive) : 
>>         invalid 'pattern' regular expression
> 
>  >
> 
>> Bug? or have I missed something
> 
> 
> You missed to read the NEWS that tells you:
> 
>     o   The regular expression code is now based on that in glibc 2.3.3.
>     It has stricter conformance to POSIX, so metachars such as
>     { } + * may need to be escaped where before they did not
>     (but could have been).
> 
> 
> Probably you want
> 
>  list.files(pattern = "\\.zip$", full.names = TRUE)
> 
> Uwe Ligges
> 
> 
>> Steve.
>>   Dr Steve Roberts   steve.roberts at manchester.ac.uk
>>
>> Senior Lecturer in Medical Statistics,
>> CMMCH NHS Trust and University of Manchester Biostatistics Group,
>> 0161 275 5192/5764 / 0161 276 5785
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Kechi.Nzerem at rms.com  Thu May 12 10:51:54 2005
From: Kechi.Nzerem at rms.com (Kechi Nzerem)
Date: Thu, 12 May 2005 09:51:54 +0100
Subject: [R] correlogram in spatial producing values outside [-1,1]
Message-ID: <D71DD12EF4EDE740A09A929F6D2C509910BB77@mailuk1.rms.com>

Dear all,

I'm using the correlogram function in the spatial library to calculate
spatial correlograms of radar data.  However, I'm finding that the
resulting values are often outside the range [-1,1], usually only at
larger distances of separation.

I'm not sure whether to be overly concerned about this, or dismiss it as
some artefact of the data.  Has anyone had similar experiences?  Or know
why this might be happening?

Thanks

Kechi



From francoisromain at free.fr  Thu May 12 10:49:04 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 12 May 2005 10:49:04 +0200
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
Message-ID: <42831880.7020402@free.fr>

Le 12.05.2005 10:30, Steve Roberts a ??crit :

>R2.0.1 (MS Windows)
>
>  
>
>>list.files(myloc,"*.zip",full=T)
>>    
>>
>[1] "P:/SARsoftware/Rlibraries/gnlm_0.1.zip"
>[2] "P:/SARsoftware/Rlibraries/lms2_0.2.zip"
>
>
>R2.1.0:
>
>  
>
>>list.files(myloc,"*.zip",full=T)
>>    
>>
>Error in list.files(path, pattern, all.files, full.names, recursive) : 
>        invalid 'pattern' regular expression
>
>Bug? or have I missed something
>
>  
>
That has something to do with regexpr, try someting like :

> list.files(myloc,"\\\\*.zip",full=T)

Romain

>Steve.
>  Dr Steve Roberts 
>  steve.roberts at manchester.ac.uk
>
>Senior Lecturer in Medical Statistics,
>CMMCH NHS Trust and University of Manchester Biostatistics Group,
>0161 275 5192/5764 / 0161 276 5785
>
>  
>


-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ssk2031 at columbia.edu  Thu May 12 10:55:20 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Thu, 12 May 2005 04:55:20 -0400
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <42831861.1070103@columbia.edu>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>	<4283178F.9070504@statistik.uni-dortmund.de>
	<42831861.1070103@columbia.edu>
Message-ID: <428319F8.60606@columbia.edu>


oops, my fault. i missed typing the key '*' character in the second version.

apologies !!!

suresh

Suresh Krishna wrote:
> 
> Is that the entire story ? I tried this with yesterday's patched version 
> (windows xp) and found:
> 
>  > list.files(getwd(),"*.txt",full=T)
> Error in list.files(path, pattern, all.files, full.names, recursive) :
>         invalid 'pattern' regular expression
> 
>  > list.files(getwd(),'.txt',full=T)
> [1] "C:/Documents and Settings/suresh/BDE_SysInfo.txt"
> [2] "C:/Documents and Settings/suresh/dxva_sig.txt"
> 
> Replacing "*.txt" with '*.txt' seems to do "something".
> 
> -s.
> 
> 
> Uwe Ligges wrote:
> 
>> Steve Roberts wrote:
>>
>>> R2.0.1 (MS Windows)
>>>
>>>
>>>> list.files(myloc,"*.zip",full=T)
>>>
>>>
>>>
>>> [1] "P:/SARsoftware/Rlibraries/gnlm_0.1.zip"
>>> [2] "P:/SARsoftware/Rlibraries/lms2_0.2.zip"
>>>
>>>
>>> R2.1.0:
>>>
>>>
>>>> list.files(myloc,"*.zip",full=T)
>>>
>>>
>>>
>>> Error in list.files(path, pattern, all.files, full.names, recursive) 
>>> :         invalid 'pattern' regular expression
>>
>>
>>  >
>>
>>> Bug? or have I missed something
>>
>>
>>
>> You missed to read the NEWS that tells you:
>>
>>     o   The regular expression code is now based on that in glibc 2.3.3.
>>     It has stricter conformance to POSIX, so metachars such as
>>     { } + * may need to be escaped where before they did not
>>     (but could have been).
>>
>>
>> Probably you want
>>
>>  list.files(pattern = "\\.zip$", full.names = TRUE)
>>
>> Uwe Ligges
>>
>>
>>> Steve.
>>>   Dr Steve Roberts   steve.roberts at manchester.ac.uk
>>>
>>> Senior Lecturer in Medical Statistics,
>>> CMMCH NHS Trust and University of Manchester Biostatistics Group,
>>> 0161 275 5192/5764 / 0161 276 5785
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Thu May 12 11:01:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 12 May 2005 11:01:45 +0200
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <42831861.1070103@columbia.edu>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>	<4283178F.9070504@statistik.uni-dortmund.de>
	<42831861.1070103@columbia.edu>
Message-ID: <42831B79.6070308@statistik.uni-dortmund.de>

Suresh Krishna wrote:

> 
> Is that the entire story ? I tried this with yesterday's patched version 
> (windows xp) and found:
> 
>  > list.files(getwd(),"*.txt",full=T)
> Error in list.files(path, pattern, all.files, full.names, recursive) :
>         invalid 'pattern' regular expression
> 
>  > list.files(getwd(),'.txt',full=T)
> [1] "C:/Documents and Settings/suresh/BDE_SysInfo.txt"
> [2] "C:/Documents and Settings/suresh/dxva_sig.txt"
> 
> Replacing "*.txt" with '*.txt' seems to do "something".

No! Replacing "*.txt" with ".txt" does something (you do not intend)!

Please read about regular expressions (!!!) and try to understand that
".txt" also finds "Not_a_txt_file.xls" ....

Uwe Ligges


> -s.
> 
> 
> Uwe Ligges wrote:
> 
>> Steve Roberts wrote:
>>
>>> R2.0.1 (MS Windows)
>>>
>>>
>>>> list.files(myloc,"*.zip",full=T)
>>>
>>>
>>>
>>> [1] "P:/SARsoftware/Rlibraries/gnlm_0.1.zip"
>>> [2] "P:/SARsoftware/Rlibraries/lms2_0.2.zip"
>>>
>>>
>>> R2.1.0:
>>>
>>>
>>>> list.files(myloc,"*.zip",full=T)
>>>
>>>
>>>
>>> Error in list.files(path, pattern, all.files, full.names, recursive) 
>>> :         invalid 'pattern' regular expression
>>
>>
>>  >
>>
>>> Bug? or have I missed something
>>
>>
>>
>> You missed to read the NEWS that tells you:
>>
>>     o   The regular expression code is now based on that in glibc 2.3.3.
>>     It has stricter conformance to POSIX, so metachars such as
>>     { } + * may need to be escaped where before they did not
>>     (but could have been).
>>
>>
>> Probably you want
>>
>>  list.files(pattern = "\\.zip$", full.names = TRUE)
>>
>> Uwe Ligges
>>
>>
>>> Steve.
>>>   Dr Steve Roberts   steve.roberts at manchester.ac.uk
>>>
>>> Senior Lecturer in Medical Statistics,
>>> CMMCH NHS Trust and University of Manchester Biostatistics Group,
>>> 0161 275 5192/5764 / 0161 276 5785
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu May 12 11:03:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 12 May 2005 11:03:59 +0200
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <42831880.7020402@free.fr>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
	<42831880.7020402@free.fr>
Message-ID: <42831BFF.6070206@statistik.uni-dortmund.de>

Romain Francois wrote:

> Le 12.05.2005 10:30, Steve Roberts a ??crit :
> 
>> R2.0.1 (MS Windows)
>>
>>  
>>
>>> list.files(myloc,"*.zip",full=T)
>>>   
>>
>> [1] "P:/SARsoftware/Rlibraries/gnlm_0.1.zip"
>> [2] "P:/SARsoftware/Rlibraries/lms2_0.2.zip"
>>
>>
>> R2.1.0:
>>
>>  
>>
>>> list.files(myloc,"*.zip",full=T)
>>>   
>>
>> Error in list.files(path, pattern, all.files, full.names, recursive) : 
>>        invalid 'pattern' regular expression
>>
>> Bug? or have I missed something
>>
>>  
>>
> That has something to do with regexpr, try someting like :
> 
>> list.files(myloc,"\\\\*.zip",full=T)

Which also finds the file "unzip.exe".
Please, folks, do read about regular expressions!

Uwe Ligges


> 
> Romain
> 
>> Steve.
>>  Dr Steve Roberts  steve.roberts at manchester.ac.uk
>>
>> Senior Lecturer in Medical Statistics,
>> CMMCH NHS Trust and University of Manchester Biostatistics Group,
>> 0161 275 5192/5764 / 0161 276 5785
>>
>>  
>>
> 
>



From slist at oomvanlieshout.net  Thu May 12 11:10:24 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Thu, 12 May 2005 11:10:24 +0200
Subject: [R] Problem with data frame when using xYplot?
Message-ID: <42831D80.5090300@oomvanlieshout.net>

Dear all,

I am trying to plot means and error bars using xYplot, but I get an 
error message from xYplot which I can not figure out:
 > Error in Summary.factor(..., na.rm = na.rm) :
         range not meaningful for factors

The data frame (tmpNa) was created using aggregate. I have used dump to 
created the code below, which generates the same error.

Can anybody tell me what is wrong with the data frame?

Thanks in advance,

Sander.

library(Hmisc)
tmpNa <-
structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label = 
c("Basalt-High",
     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
     4.66666666666667, 5.55555555555556, 10.6666666666667, 5.66666666666667
     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
     15.2679766541317, 5.45435605731786, 8.09492296305393, 10.6183802907976,
     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
c(25.5382783976218,
     26.0818978307592, 18.2793849487813, 13.0217855597339, 2.84854798089405,
     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
c(29.1283882690448,
     27.6958799470186, 31.7206150512187, 23.2004366624884, 6.48478535243929,
     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
c("Position",
"AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
"2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
xYplot(Cbind(Sodium,Lower,Upper) ~ AltGeo, groups=Position,  data=tmpNa)


 > version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From francoisromain at free.fr  Thu May 12 11:07:29 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 12 May 2005 11:07:29 +0200
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <42831861.1070103@columbia.edu>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>	<4283178F.9070504@statistik.uni-dortmund.de>
	<42831861.1070103@columbia.edu>
Message-ID: <42831CD1.5050008@free.fr>

Le 12.05.2005 10:48, Suresh Krishna a ??crit :

>
> Is that the entire story ? I tried this with yesterday's patched 
> version (windows xp) and found:
>
> > list.files(getwd(),"*.txt",full=T)
> Error in list.files(path, pattern, all.files, full.names, recursive) :
>         invalid 'pattern' regular expression
>
> > list.files(getwd(),'.txt',full=T)
> [1] "C:/Documents and Settings/suresh/BDE_SysInfo.txt"
> [2] "C:/Documents and Settings/suresh/dxva_sig.txt"
>
> Replacing "*.txt" with '*.txt' seems to do "something".
>
> -s.

Well, that's not what you did exactlty, without the * in the first call, 
the result would have been the same.

Romain

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From gladys.dracon at edf.fr  Thu May 12 11:42:12 2005
From: gladys.dracon at edf.fr (Gladys DRACON)
Date: Thu, 12 May 2005 11:42:12 +0200
Subject: [R] SVM linear kernel and SV
Message-ID: <OF8A752349.28BAC4AC-ONC1256FFE.0042D91A-C1256FFF.00353EAE@notes.edfgdf.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/64dccf52/attachment.pl

From ute.visser at ufz.de  Thu May 12 11:56:09 2005
From: ute.visser at ufz.de (Ute Visser)
Date: Thu, 12 May 2005 11:56:09 +0200
Subject: [R] Plot()
Message-ID: <000101c556d8$d2ba5a40$6233418d@oesa.leipzig.ufz.de>

Hello!

I wonder why the x and the y expression are treated differently in the
plot-function?
In the example below I should get three times the same graph, shouldn't
I?

column<-c(0,1)
par(mfrow = c(3,1))
plot((1-column)~column)
plot(column~(1-column))
plot(column~as.vector(1-column))

Thanks for checking!

U



From B.Rowlingson at lancaster.ac.uk  Thu May 12 12:05:43 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 12 May 2005 11:05:43 +0100
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <42831B79.6070308@statistik.uni-dortmund.de>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>	<4283178F.9070504@statistik.uni-dortmund.de>	<42831861.1070103@columbia.edu>
	<42831B79.6070308@statistik.uni-dortmund.de>
Message-ID: <42832A77.9090406@lancaster.ac.uk>

Uwe Ligges wrote:

> Please read about regular expressions (!!!) and try to understand that
> ".txt" also finds "Not_a_txt_file.xls" ....


  The confusion here is between regular expressions and wildcard 
expansion known as 'globbing'. The two things are very different, and 
use characters such as '*' '.' and '?' in different ways.

  There's added confusion when people come from a DOS background, where 
commands did their own thing when given '*' as parameter. The DOS command:

  RENAME *.FOO *.BAR

  did what seems obvious, renaming all the .FOO files to .BAR, but on a 
unix machine doing this with 'mv' can be destructive!

  In short (and slightly simplified), a '*' when expanded as a wildcard 
in a glob matches any string, whereas a '*' in a regular expression 
(regexp), matches the previous character 0 or more times. This is why 
"*.zip" is flagged as invalid now - there's no character before the "*".

  That should be enough clues to send you on your way.

  Baz



From david.meyer at wu-wien.ac.at  Thu May 12 13:06:05 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Thu, 12 May 2005 13:06:05 +0200
Subject: [R] SVM linear kernel and SV
Message-ID: <20050512130605.73e95f5c.david.meyer@wu-wien.ac.at>

Gladys,

> 
>  I've used  svm() with a linear kernel and I'd like to plot the linear
>  
> hyperplane and the support vectors. I use plot.svm() and, according to
> me, I would have found aligned support vectors (because the hyperplane
> is linear) for each class but it wasn't the case. Could you explain me
> why ?

In how far does the plot give you the impression is wouldn't? The two
classes look pretty separated to me.

> 
> In addition, when I change the option 'scale' (from TRUE to FALSE) the
> 
> results change. 

(Which results?) The plot is, of course, slightly different since the
model is based on different data, but the class predictions (on the
training data) are the same. Why does this surprise you?

Could you explain me why ? the option 'scale' of svm() 
> acts on the dataset or on the weight vector w and threshold b  ?

On the data set, and therefore also on w and b.

Best,
David


-- 
Dr. David Meyer
Department of Information Systems and Process Management

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From ligges at statistik.uni-dortmund.de  Thu May 12 13:11:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 12 May 2005 13:11:59 +0200
Subject: [R] Plot()
In-Reply-To: <000101c556d8$d2ba5a40$6233418d@oesa.leipzig.ufz.de>
References: <000101c556d8$d2ba5a40$6233418d@oesa.leipzig.ufz.de>
Message-ID: <428339FF.5030907@statistik.uni-dortmund.de>

Ute Visser wrote:

> Hello!
> 
> I wonder why the x and the y expression are treated differently in the
> plot-function?
> In the example below I should get three times the same graph, shouldn't
> I?
> 
> column<-c(0,1)
> par(mfrow = c(3,1))
> plot((1-column)~column)
> plot(column~(1-column))

In right hand sides of formulas you have to use I() in order to do these 
calculations:

  plot(column ~ I(1-column))

or plot() will simply plot against the index (column ~ 1) and ignores 
the unsensible last part "-column".

Uwe Ligges


> plot(column~as.vector(1-column))
> 
> Thanks for checking!
> 
> U
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From steve.roberts at manchester.ac.uk  Thu May 12 13:31:33 2005
From: steve.roberts at manchester.ac.uk (Steve Roberts)
Date: Thu, 12 May 2005 12:31:33 +0100
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <4283178F.9070504@statistik.uni-dortmund.de>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
Message-ID: <42834CA5.16701.E744C5@fs1.ser.man.ac.uk>

Yes I missed the NEWS entry - or rather didn't realise its significance. 
So the "bug" was in the previous version and my old code which worked 
but shouldn't have.

Thanks for the replies - rapid and to the point as usual.

Steve.


Date sent:      	Thu, 12 May 2005 10:45:03 +0200
From:           	Uwe Ligges <ligges at statistik.uni-dortmund.de>
Organization:   	Fachbereich Statistik, Universitaet Dortmund
To:             	steve.roberts at manchester.ac.uk
Copies to:      	R-help at stat.math.ethz.ch
Subject:        	Re: [R] R2.1.0: Bug in list.files

> Steve Roberts wrote:
> 
> > R2.0.1 (MS Windows)
> > 
> > 
> >>list.files(myloc,"*.zip",full=T)
> > 
> > [1] "P:/SARsoftware/Rlibraries/gnlm_0.1.zip"
> > [2] "P:/SARsoftware/Rlibraries/lms2_0.2.zip"
> > 
> > 
> > R2.1.0:
> > 
> > 
> >>list.files(myloc,"*.zip",full=T)
> > 
> > Error in list.files(path, pattern, all.files, full.names, recursive)
> > : 
> >         invalid 'pattern' regular expression
>  >
> > Bug? or have I missed something
> 
> You missed to read the NEWS that tells you:
> 
>      o   The regular expression code is now based on that in glibc
>      2.3.3. It has stricter conformance to POSIX, so metachars such as
>      { } + * may need to be escaped where before they did not (but
>      could have been).
> 
> 
> Probably you want
> 
>   list.files(pattern = "\\.zip$", full.names = TRUE)
> 
> Uwe Ligges
> 
> 
> > Steve.
> >   Dr Steve Roberts 
> >   steve.roberts at manchester.ac.uk
> > 
> > Senior Lecturer in Medical Statistics,
> > CMMCH NHS Trust and University of Manchester Biostatistics Group,
> > 0161 275 5192/5764 / 0161 276 5785
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 


  Dr Steve Roberts 
  steve.roberts at manchester.ac.uk

Senior Lecturer in Medical Statistics,
CMMCH NHS Trust and University of Manchester Biostatistics Group,
0161 275 5192/5764 / 0161 276 5785



From vin.everett at cimr.cam.ac.uk  Thu May 12 13:40:54 2005
From: vin.everett at cimr.cam.ac.uk (Vin Everett)
Date: Thu, 12 May 2005 12:40:54 +0100
Subject: [R] Solaris 10 on amd and R-2.1.0
Message-ID: <1115898054.7591.8.camel@clover>

Hi

I am having problems compiling R on a Solaris 10 opteron box we have on
trial.

checking for Fortran libraries of g77...  -L/usr/ccs/lib -L/usr/lib -
L/usr/local/lib -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2 -
L/usr/ccs/bin -L/usr/local/lib/gcc-lib/i386-pc-
solaris2.10/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s -lfrtbegin: -
lg2c:
checking how to get verbose linking output from gcc... -v
checking for C libraries of gcc...  -L/usr/ccs/lib -L/usr/lib -
L/usr/local/lib -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2 -
L/usr/ccs/bin -L/usr/local/lib/gcc-lib/i386-pc-
solaris2.10/3.3.2/../../.. -lgcc_eh
checking for dummy main to link with Fortran libraries... unknown
configure: error: linking to Fortran libraries from C fails
See `config.log' for more details.

I have installed gcc from sunfreeware

Looks like the following are not found:- 

configure:26575: gcc -o conftest -g -O2 -I/usr/local/include -
L/usr/local/lib conftest.c -
ldl -lm   -lg2c -lm -lgcc_s -lfrtbegin: -lg2c: >&5
ld: fatal: library -lg2c: not found
ld: fatal: library -lfrtbegin:: not found
ld: fatal: library -lg2c:: not found
ld: fatal: File processing errors. No output written to conftest
collect2: ld returned 1 exit status
configure:26581: $? = 1
configure: failed program was:
| /* confdefs.h.  */


Any idea where to get them from ?

Cheers Vin
-- 
Vin.Everett at cimr.cam.ac.uk
JDRF/WT Diabetes and Inflammation Laboratory (DIL)
Cambridge Institute for Medical Research (CIMR)
Wellcome Trust/MRC Building Addenbrooke's Hospital 
Hills Road Cambridge CB2 2XY
+44 1223 763212
+44 7990 966266



From ggrothendieck at gmail.com  Thu May 12 13:41:11 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 12 May 2005 07:41:11 -0400
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <42834CA5.16701.E744C5@fs1.ser.man.ac.uk>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
	<4283178F.9070504@statistik.uni-dortmund.de>
	<42834CA5.16701.E744C5@fs1.ser.man.ac.uk>
Message-ID: <971536df05051204417cf8345d@mail.gmail.com>

Note that sfsmisc::glob2rx is a handy function that will convert glob style
wildcard expressions to regular expressions.  

On 5/12/05, Steve Roberts <steve.roberts at manchester.ac.uk> wrote:
> Yes I missed the NEWS entry - or rather didn't realise its significance.
> So the "bug" was in the previous version and my old code which worked
> but shouldn't have.
> 
> Thanks for the replies - rapid and to the point as usual.
> 
> Steve.
> 
> Date sent:              Thu, 12 May 2005 10:45:03 +0200
> From:                   Uwe Ligges <ligges at statistik.uni-dortmund.de>
> Organization:           Fachbereich Statistik, Universitaet Dortmund
> To:                     steve.roberts at manchester.ac.uk
> Copies to:              R-help at stat.math.ethz.ch
> Subject:                Re: [R] R2.1.0: Bug in list.files
> 
> > Steve Roberts wrote:
> >
> > > R2.0.1 (MS Windows)
> > >
> > >
> > >>list.files(myloc,"*.zip",full=T)
> > >
> > > [1] "P:/SARsoftware/Rlibraries/gnlm_0.1.zip"
> > > [2] "P:/SARsoftware/Rlibraries/lms2_0.2.zip"
> > >
> > >
> > > R2.1.0:
> > >
> > >
> > >>list.files(myloc,"*.zip",full=T)
> > >
> > > Error in list.files(path, pattern, all.files, full.names, recursive)
> > > :
> > >         invalid 'pattern' regular expression
> >  >
> > > Bug? or have I missed something
> >
> > You missed to read the NEWS that tells you:
> >
> >      o   The regular expression code is now based on that in glibc
> >      2.3.3. It has stricter conformance to POSIX, so metachars such as
> >      { } + * may need to be escaped where before they did not (but
> >      could have been).
> >
> >
> > Probably you want
> >
> >   list.files(pattern = "\\.zip$", full.names = TRUE)
> >
> > Uwe Ligges
> >
> >
> > > Steve.
> > >   Dr Steve Roberts
> > >   steve.roberts at manchester.ac.uk
> > >
> > > Senior Lecturer in Medical Statistics,
> > > CMMCH NHS Trust and University of Manchester Biostatistics Group,
> > > 0161 275 5192/5764 / 0161 276 5785
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> >
> 
>  Dr Steve Roberts
>  steve.roberts at manchester.ac.uk
> 
> Senior Lecturer in Medical Statistics,
> CMMCH NHS Trust and University of Manchester Biostatistics Group,
> 0161 275 5192/5764 / 0161 276 5785
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From subianto at gmail.com  Thu May 12 13:53:46 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Thu, 12 May 2005 13:53:46 +0200
Subject: [R] How to make label in multi plot
Message-ID: <3635ddc205051204536c97b6cb@mail.gmail.com>

Dear R-Help,
As a reference about multi plot,
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48725.html

I want to know how can I make a label for each row.
I mean like,

                  ------------        -------------      --------------
                 |            |       |            |      |            |
Group A     |   plot1  |       |  plot 2  |      |  plot 3  |       
                 |            |       |            |      |            |
                 -------------        -------------      --------------

                                      -------------
                                      |            |
Group B                          |  plot 4  |
                                      |            |
                                      -------------

I  would be very happy if anyone could help me.
Thank you very much in advance.
Sincerely,
Muhammad Subianto



From gladys.dracon at edf.fr  Thu May 12 14:45:20 2005
From: gladys.dracon at edf.fr (Gladys DRACON)
Date: Thu, 12 May 2005 14:45:20 +0200
Subject: =?iso-8859-1?Q?R=E9f=2E_=3A_[R]_SVM_linear_kernel_and_SV?=
Message-ID: <OFE0644E80.915295DD-ONC1256FFF.0043096F-C1256FFF.004602AF@notes.edfgdf.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/baf6a284/attachment.pl

From f.harrell at vanderbilt.edu  Thu May 12 14:48:54 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 12 May 2005 07:48:54 -0500
Subject: [R] inserting R code in a latex document
In-Reply-To: <20050511104620.GB24060@jtkpc.cmp.uea.ac.uk>
References: <4281434F.3020705@biomserv.univ-lyon1.fr>	<4281BDA3.1080009@statistik.uni-dortmund.de>
	<20050511104620.GB24060@jtkpc.cmp.uea.ac.uk>
Message-ID: <428350B6.5080009@vanderbilt.edu>

I have been trying to use the listings package in LaTeX to do this but 
have not been fully happy with the result.  Here is what I'm trying:

\lstloadlanguages{R}
\lstset{language=R,basicstyle=\smaller[2],commentstyle=\rmfamily\smaller,
  showstringspaces=false,%
  xleftmargin=4ex,literate={<-}{{$\leftarrow$}}1 {~}{{$\sim$}}1}
\newcommand{\co}[1]{{\lstinline|#1|}} # for in-line S phrases
\newcommand{\Co}[1]{{\lstinline|#1|}}

\begin{lstlisting}
... S code ...
\end{lstlisting}

If anyone has a better setup please let me know.  I'm trying to set 
comments in a smaller font and in roman style, and am changing <- to 
left arrows and ~ to $\sim$.  This also requires the LaTeX relsize package.

Thanks
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From pcampbell at econ.bbk.ac.uk  Thu May 12 15:05:26 2005
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Thu, 12 May 2005 14:05:26 +0100
Subject: [R] Solaris 10 on amd and R-2.1.0
In-Reply-To: <1115898054.7591.8.camel@clover>
Message-ID: <NGECIFANPOJAGABBAEAPOEDBEIAA.pcampbell@econ.bbk.ac.uk>

This might solve your problem without helping.  Having spent I bit of time
trying to build R with the pkgadd version of GCC from Sunfreeware and not
getting anywhere I downloaded the source for GCC 3.4.2 and built this, then
used this to build R.

Everything worked fine.

HTH

Phineas

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Vin Everett
Sent: Thursday, May 12, 2005 12:41 PM
To: r-help at stat.math.ethz.ch
Cc: Vin.Everett at cimr.cam.ac.uk
Subject: [R] Solaris 10 on amd and R-2.1.0


Hi

I am having problems compiling R on a Solaris 10 opteron box we have on
trial.

checking for Fortran libraries of g77...  -L/usr/ccs/lib -L/usr/lib -
L/usr/local/lib -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2 -
L/usr/ccs/bin -L/usr/local/lib/gcc-lib/i386-pc-
solaris2.10/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s -lfrtbegin: -
lg2c:
checking how to get verbose linking output from gcc... -v
checking for C libraries of gcc...  -L/usr/ccs/lib -L/usr/lib -
L/usr/local/lib -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2 -
L/usr/ccs/bin -L/usr/local/lib/gcc-lib/i386-pc-
solaris2.10/3.3.2/../../.. -lgcc_eh
checking for dummy main to link with Fortran libraries... unknown
configure: error: linking to Fortran libraries from C fails
See `config.log' for more details.

I have installed gcc from sunfreeware

Looks like the following are not found:-

configure:26575: gcc -o conftest -g -O2 -I/usr/local/include -
L/usr/local/lib conftest.c -
ldl -lm   -lg2c -lm -lgcc_s -lfrtbegin: -lg2c: >&5
ld: fatal: library -lg2c: not found
ld: fatal: library -lfrtbegin:: not found
ld: fatal: library -lg2c:: not found
ld: fatal: File processing errors. No output written to conftest
collect2: ld returned 1 exit status
configure:26581: $? = 1
configure: failed program was:
| /* confdefs.h.  */


Any idea where to get them from ?

Cheers Vin
--
Vin.Everett at cimr.cam.ac.uk
JDRF/WT Diabetes and Inflammation Laboratory (DIL)
Cambridge Institute for Medical Research (CIMR)
Wellcome Trust/MRC Building Addenbrooke's Hospital
Hills Road Cambridge CB2 2XY
+44 1223 763212
+44 7990 966266

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Thu May 12 15:17:36 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 12 May 2005 09:17:36 -0400
Subject: [R] time zones, daylight saving etc.
In-Reply-To: <Pine.LNX.4.61.0505120610580.18101@gannet.stats>
References: <42817BF9.5000705@student.canterbury.ac.nz>
	<971536df05051112232b064070@mail.gmail.com>
	<5934ae57050511154212c5301e@mail.gmail.com>
	<Pine.LNX.4.61.0505120610580.18101@gannet.stats>
Message-ID: <971536df0505120617225ec23e@mail.gmail.com>

I have tried this but on Windows XP R 2.1.0 found I had to set it outside of
R prior to starting R. 

1. unsuccessful

> Sys.time()
[1] "2005-05-12 09:08:03 Eastern Daylight Time"
> Sys.putenv(TZ="GMT")
> Sys.time() # no change
[1] "2005-05-12 09:08:12 Eastern Daylight Time"

2. OK

C:\>set tz=GMT

C:\>start "" "\Program Files\R\rw2010\bin\r.exe"

R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.1.0 Patched (2005-04-18), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

> Sys.time()
[1] "2005-05-12 13:10:58 GMT"

I assume it could be set in .Renviron but it would be nice if one
could set it right from within R so that one can write a function
that sets it, does processing and then sets it back.  Don't know
if this is possible.

On 5/12/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> Would it not just be easier to set the timezone to GMT for the duration of
> the calculations?  I don't see an OS mentioned here, but on most TZ=GMT
> for the session will do it.
> 
> On Thu, 12 May 2005, Rich FitzJohn wrote:
> 
> > Hi,
> >
> > seq.dates() in the chron package does not allow creating sequences by
> > minutes, so you'd have to roll your own sequence generator.
> >
> > Looks like the tzone attribute of the times is lost when using min(),
> > max() and seq().  You can apply it back manually, but it does not
> > affect the calculation, since POSIXct times are stored as seconds
> > since 1/1/1970 (?DateTimeClasses).
> >
> > ## These dates/times just span the move from NZDT to NZST:
> > dt.dates <- paste(rep(15:16, c(5,7)), "03", "2003", sep="/")
> > dt.times <- paste(c(19:23, 0:6), "05", sep=":")
> > dt <- paste(dt.dates, dt.times)
> >
> > ## No shift in times, or worrying about daylight savings; appropriate
> > ## iff the device doing the recording was not itself adjusting for
> > ## daylight savings, presumably.
> > datetime <- as.POSIXct(strptime(dt, "%d/%m/%Y %H:%M"), "GMT")
> >
> > ## Create two objects with all the times in your range, one with the
> > ## tzone attribute set back to GMT (to match datetimes), and one
> > ## without this.
> > mindata1 <- mindata2 <- seq(from=min(datetime), to=max(datetime),
> >                            by="mins")
> > attr(mindata2, "tzone") <- "GMT"
> >
> > fmt <- "%Y %m %d %H %M"
> > ## These both do the matching correctly:
> > match(format(datetime, fmt), format(mindata1, fmt, tz="GMT"))
> > match(format(datetime, fmt), format(mindata2, fmt, tz="GMT"))
> >
> > ## However, the first of these will not, as it gets the timezone all
> > ## wrong, since it's neither specified in the call to format(), or as
> > ## an attribute of the POSIXct object.
> > match(format(datetime, fmt), format(mindata1, fmt))
> > match(format(datetime, fmt), format(mindata2, fmt))
> >
> > ## It is also possible to run match() directly off the POSIXct object,
> > ## but I'm not sure how this will interact with things like leap
> > ## seconds:
> > match(datetime, mindata1)
> >
> > Time zones do my head in, so you probably want to check this all
> > pretty carefully.  Looks like there's lots of gotchas (e.g. subsetting
> > a POSIXct object strips the tzone attribute).
> >
> > Cheers,
> > Rich
> >
> > On 5/12/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >> You could use the chron package.  It represents date times without
> >> using time zones so you can't have this sort of problem.
> >>
> >> On 5/10/05, Carla Meurk <ksm32 at student.canterbury.ac.nz> wrote:
> >>> Hi,  I have a whole bunch of data, which looks like:
> >>>
> >>> 15/03/2003       10:20  1
> >>> 15/03/2003       10:21  0
> >>> 15/03/2003       12:02  0
> >>> 16/03/2003       06:10  0
> >>> 16/03/2003       06:20  0.5
> >>> 16/03/2003       06:30  0
> >>> 16/03/2003       06:40  0
> >>> 16/03/2003       06:50  0
> >>>
> >>> 18/03/2003  20:10                 0.5
> >>> etc. (times given on a 24 hour clock)
> >>>
> >>> and goes on for years.  I have some code:
> >>>
> >>> data<-read.table("H:/rainfall_data.txt",h=T)
> >>> library(survival)
> >>> datetime <- as.POSIXct(strptime(paste(data$V1, data$V2), "%d/%m/%Y
> >>> %H:%M"), tz="NZST")
> >>>
> >>> which produces:
> >>>
> >>> [10] "2003-03-13 21:13:00 New Zealand Daylight Time"
> >>> [11] "2003-03-15 13:20:00 New Zealand Daylight Time"
> >>> [12] "2003-03-15 22:20:00 New Zealand Daylight Time"
> >>> [13] "2003-03-15 22:21:00 New Zealand Daylight Time"
> >>> [14] "2003-03-16 00:02:00 New Zealand Daylight Time"
> >>> [15] "2003-03-16 18:10:00 New Zealand Standard Time"
> >>> [16] "2003-03-16 18:20:00 New Zealand Standard Time"
> >>> [17] "2003-03-16 18:30:00 New Zealand Standard Time"
> >>>
> >>> My problem is that "15/03/2003 12:02" has become "16/03/2003 00:02"
> >>> i.e.  it is 12 hours behind (as is everything else), but also, I do not
> >>> want to change time zones.
> >>>
> >>> The 12 hour delay is not really a problem just an annoyance, but the
> >>> time zone change is a problem because later on I need to match up data
> >>> by time using
> >>>
> >>> mindata<-seq(from=min(datetime),to=max(datetime),by="mins")
> >>> newdata<-matrix(0,length(mindata),1)
> >>> newdata[match(format.POSIXct(datetime,"%Y %m %d %H
> >>> %M"),format.POSIXct(mindata,"%Y %m %d %H %M"))]<-data$V3
> >>>
> >>> and things go wrong here with matching repeating times/missing times
> >>> around the timezone changes and, my resulting vector is 1 hour shorter
> >>> than my other series.  From the R help I see that my OS may be to blame
> >>> but, even if I specify tz="GMT" I still get NZST and NZDT.  Can someone
> >>> help?
> >>>
> >>> I hope this all makes sense
> >>>
> >>> Carla
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >
> >
> > --
> > Rich FitzJohn
> > rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
> >                      You are in a maze of twisty little functions, all alike
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bhx2 at mevik.net  Thu May 12 15:34:52 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Thu, 12 May 2005 15:34:52 +0200
Subject: [R] pls -- crossval vs plsr(..., CV=TRUE)
In-Reply-To: <9466f3f90ea00fb3ce09756cd3cce9ee@psu.edu> (martin peters's
	message of "Wed, 11 May 2005 22:42:45 -0400")
References: <9466f3f90ea00fb3ce09756cd3cce9ee@psu.edu>
Message-ID: <m0y8akh8jn.fsf@bar.nemo-project.org>

martin peters writes:

> $ library(pls)
> $ data(NIR)
>
> $ testing.plsNOCV <- plsr(y ~ X, 6, data = NIR, method="kernelpls", 
> validation="none")
> $ NIR.plsCV <- plsr(y ~ X, 6, data = NIR, CV=TRUE, method="kernelpls")
> $ testing.plsCV <- crossval(testing.plsNOCV)
> $ R2(NIR.plsCV)
> (Intercept)      1 comps      2 comps      3 comps      4 comps      5 
> comps
>       0.0000       0.9812       0.9825       0.9964       0.9997       
> 0.9999
>      6 comps
>       0.9999
> $ R2(testing.plsCV)
> (Intercept)      1 comps      2 comps      3 comps      4 comps      5 
> comps
>       0.0000       0.9678       0.9782       0.9941       0.9991       
> 0.9996
>      6 comps
>       0.9997

[...]

> If the above result is correct can someone explain the difference to me.

There are two reasons:

1) The call plsr(y ~ X, 6, data = NIR, CV=TRUE, method="kernelpls") is
   incorrect.  The `CV' argument of the superseded `pls.pcr' package
   has been replaced by the `validation' argument, so the correct call
   would be
   NIR.plsCV <- plsr(y ~ X, 6, data = NIR, validation="CV", method="kernelpls")
   (If you had done R2(testing.plsNOCV), you would have gotten exactly
   the same as with the R2(NIR.plsCV) above.)

2) plsr(... , validation = "CV") and crossval(...) both by default use
   CV with 10-fold _randomly selected_ segments, which means that each
   time you run the cross-validation, you will get slightly different
   results.  (Try running R2(crossval(testing.plsNOCV)) a couple of times.)

   If you want the same segments in two separate calls, either add the
   argument segment.type = "consecutive" or "interleaved", or specify
   the segments explicitly with the `segments' argument (see
   ?crossval or ?mvrCv for how).

   The segments actually used in a cross-validation is stored in the
   $validation$segments component of the object,
   i.e. testing.plsCV$validation$segments.

(By the way, `method = "kernelpls"' is not needed, as it is the
default fit method for plsr (and mvr).)

-- 
Bj??rn-Helge Mevik



From georg.otto at tuebingen.mpg.de  Thu May 12 15:48:02 2005
From: georg.otto at tuebingen.mpg.de (Georg Otto)
Date: Thu, 12 May 2005 15:48:02 +0200
Subject: [R] mget empty strings
Message-ID: <75D4ED68-C2EC-11D9-A04C-003065C99468@tuebingen.mpg.de>

Dear R community,

I am a beginner to R and have a question concerning mget, about I could 
not find anything in the various documentation.

I have a column in a dataframe x for which I want to get values in y:

mget(x[,1], env=y, ifnotfound=NA)

I receive an error mesage:

Error in mget(x[, 1], env = y, ifnotfound = NA) :
         attempt to use zero-length variable name

this is probably due to the fact that some of the values in x[,1] are 
empty ("")

I would like to get a "NA" for these values, and the found value for 
those where mget finds one. Could anyone tell me how this is done 
efficiently?

Thanks a lot in advance!

Georg


P.S. I am in digest mode, cc'ing the answer directly to me would be nice


-- 
Georg Wilhelm Otto
Max-Planck-Institute for Developmental Biology
Spemannstrasse 35/III
D-72076 Tuebingen Germany
phone: +49-7071-601 401
http://www.eb.tuebingen.mpg.de



From ligges at statistik.uni-dortmund.de  Thu May 12 16:00:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 12 May 2005 16:00:10 +0200
Subject: [R] mget empty strings
In-Reply-To: <75D4ED68-C2EC-11D9-A04C-003065C99468@tuebingen.mpg.de>
References: <75D4ED68-C2EC-11D9-A04C-003065C99468@tuebingen.mpg.de>
Message-ID: <4283616A.4080807@statistik.uni-dortmund.de>

Georg Otto wrote:

> Dear R community,
> 
> I am a beginner to R and have a question concerning mget, about I could 
> not find anything in the various documentation.
> 
> I have a column in a dataframe x for which I want to get values in y:
> 
> mget(x[,1], env=y, ifnotfound=NA)
> 
> I receive an error mesage:
> 
> Error in mget(x[, 1], env = y, ifnotfound = NA) :
>         attempt to use zero-length variable name
> 
> this is probably due to the fact that some of the values in x[,1] are 
> empty ("")
> 
> I would like to get a "NA" for these values, and the found value for 
> those where mget finds one. Could anyone tell me how this is done 
> efficiently?
> 
> Thanks a lot in advance!
> 
> Georg
> 
> 
> P.S. I am in digest mode, cc'ing the answer directly to me would be nice
> 
> 


Is y and environment?

Please make your example reproducible - as long as I have not seen your 
example, I guess you don't want to use mget() at all ...

Uwe Ligges



From aroth at fh-heilbronn.de  Thu May 12 16:00:06 2005
From: aroth at fh-heilbronn.de (Alexander Roth)
Date: Thu, 12 May 2005 16:00:06 +0200
Subject: [R] Standardized logistic regression coefficients
Message-ID: <000201c556fa$e70f75d0$6603a8c0@ltroth>


Hi everyone,

how can I calculate standardized logistic regression coefficients using
R? I used "glm" resp. "lrm" from the design-package in order to
calculate logistic regression coefficients but I'm wondering if there's
a possibility to get standardized logistic regression coefficients?

Thanks in advance!

Alexander Roth



From andy_liaw at merck.com  Thu May 12 16:04:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 12 May 2005 10:04:14 -0400
Subject: [R] mget empty strings
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E81D@usctmx1106.merck.com>

Here's one way to work around it:

> nm <- c("x", "", "y")
> x <- 10; y <- 20
> myObj <- structure(vector(mode="list", length=length(nm)), names=nm)
> goodNames <- nm[nchar(nm) > 0]
> myObj[goodNames] <- mget(goodNames, .GlobalEnv)
> myObj
$x
[1] 10

[[2]]
NULL

$y
[1] 20

Andy

> From: Georg Otto
> 
> Dear R community,
> 
> I am a beginner to R and have a question concerning mget, 
> about I could 
> not find anything in the various documentation.
> 
> I have a column in a dataframe x for which I want to get values in y:
> 
> mget(x[,1], env=y, ifnotfound=NA)
> 
> I receive an error mesage:
> 
> Error in mget(x[, 1], env = y, ifnotfound = NA) :
>          attempt to use zero-length variable name
> 
> this is probably due to the fact that some of the values in x[,1] are 
> empty ("")
> 
> I would like to get a "NA" for these values, and the found value for 
> those where mget finds one. Could anyone tell me how this is done 
> efficiently?
> 
> Thanks a lot in advance!
> 
> Georg
> 
> 
> P.S. I am in digest mode, cc'ing the answer directly to me 
> would be nice
> 
> 
> -- 
> Georg Wilhelm Otto
> Max-Planck-Institute for Developmental Biology
> Spemannstrasse 35/III
> D-72076 Tuebingen Germany
> phone: +49-7071-601 401
> http://www.eb.tuebingen.mpg.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Thu May 12 16:09:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 12 May 2005 10:09:00 -0400
Subject: [R] assigning to a list in a package environment
In-Reply-To: <c6192184b8423427f853348ca8ae08d4@mail.nih.gov>
References: <c6192184b8423427f853348ca8ae08d4@mail.nih.gov>
Message-ID: <971536df0505120709d0133a@mail.gmail.com>

On 5/11/05, Sean Davis <sdavis2 at mail.nih.gov> wrote:
> I have a list in a package environment
> 
> assign('refflat',list(),pos='package:locPkg')
> 
> to which I would like to make assignments like:
> 
> refflat[['a']] <- read.table('fileA.txt')
> refflat[['b']] <- read.table('fileB.txt')
> 
> I am doing this to guard against a local version of refflat hanging
> around, as I want to refresh it with each new session (and so, want to
> store it in the package environment).  I just can't quite get hot to
> make that work so that I am storing to the package:refflat rather than
> any in .GlobalEnv.

Try this:

locPkg <- as.environment("package:locPkg")
locPkg$refflat <- list()
locPkg$refflat[["a"]] <- read.table("fileA.txt")



From bates at stat.wisc.edu  Thu May 12 16:08:14 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 12 May 2005 09:08:14 -0500
Subject: [R] problem with intervals in mixed model
In-Reply-To: <000601c55590$95fb1980$b01ad284@BIO041>
References: <000601c55590$95fb1980$b01ad284@BIO041>
Message-ID: <4283634E.8090903@stat.wisc.edu>

Bill Shipley wrote:
> Hello.  I am analysing data from a mixed model perspective using the
> lme() function.  The fixed effects model is a quadratic (Y~X+X2) where
> X2 is the square of X and the data have a 3-level structure.  I fitted a
> series of three models with the same fixed effects but differing in the
> random effects (only intercept, intercept + X, intercept +X +X2).  The
> anova shows that all three parameters vary significantly (p<0.001)
> across groups.  I have therefore chosen the third model, in which all
> three parameters vary.
> 
> When I attempted to obtain the confidence intervals for the correlations
> between the random components, using:
> 
>  
> 
> intervals(fit3,which="var-cov")
> 
>  
> 
> I get the following error message:
> 
>  
> 
> Problem in intervals.lme(fit3, which = "var..: Cannot get confi
> 
> dence intervals on var-cov components: Non-positive definite ap
> 
> proximate variance-covariance
> 
>  
> 
>  
> 
> I assume that this arises because the correlation between two of the
> parameters at the 2nd lowest level is -0.998.  Can anyone tell me how to
> deal with this problem?  Specifically,
> 
> 1) how should I interpret such a strong correlation?
> 
> 2) how can I obtain confidence intervals for these correlations between
> the random components?

It is quite possible that the ML or REML estimate of the
variance-covariance matrix of the random effects is singular.

If this matrix approaches singularity because one of the diagonal terms
is going to zero then it simply means that the model should be reduced
by removing the corresponding random effect.  However, approaching
singularity by getting correlations close to -1 or to +1 takes you out
of the space of linear mixed models.

My only suggestion is possibly to change the origin on the X axis if
that would make sense in the context of your data.



From tlumley at u.washington.edu  Thu May 12 16:17:32 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 12 May 2005 07:17:32 -0700 (PDT)
Subject: [R] R2.1.0: Bug in list.files
In-Reply-To: <42831861.1070103@columbia.edu>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
	<4283178F.9070504@statistik.uni-dortmund.de>
	<42831861.1070103@columbia.edu>
Message-ID: <Pine.A41.4.61b.0505120710480.308540@homer10.u.washington.edu>

On Thu, 12 May 2005, Suresh Krishna wrote:
> Is that the entire story ? I tried this with yesterday's patched version 
> (windows xp) and found:
>
>> list.files(getwd(),"*.txt",full=T)
> Error in list.files(path, pattern, all.files, full.names, recursive) :
>        invalid 'pattern' regular expression
>
>> list.files(getwd(),'.txt',full=T)
> [1] "C:/Documents and Settings/suresh/BDE_SysInfo.txt"
> [2] "C:/Documents and Settings/suresh/dxva_sig.txt"
>
> Replacing "*.txt" with '*.txt' seems to do "something".
>

It finds any file name containing the substring txt beginning anywhere 
except the first letter. Now, this is exactly what *.txt used to do, so in 
that sense it is equivalent, but it probably isn't what you wanted.  The 
pattern argument to list.files isn't a Windows wildcard expression. It 
never has been a Windows wildcard expression.    It just so happens that 
".txt" is also a valid regular expression, but one that means something 
different from the Windows wildcard expression "*.txt".


 	-thomas



From sdavis2 at mail.nih.gov  Thu May 12 16:27:56 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 12 May 2005 10:27:56 -0400
Subject: [R] assigning to a list in a package environment
In-Reply-To: <971536df0505120709d0133a@mail.gmail.com>
References: <c6192184b8423427f853348ca8ae08d4@mail.nih.gov>
	<971536df0505120709d0133a@mail.gmail.com>
Message-ID: <ca6d62b3799f17f5617998df07bdf36b@mail.nih.gov>


On May 12, 2005, at 10:09 AM, Gabor Grothendieck wrote:

> On 5/11/05, Sean Davis <sdavis2 at mail.nih.gov> wrote:
>> I have a list in a package environment
>>
>> assign('refflat',list(),pos='package:locPkg')
>>
>> to which I would like to make assignments like:
>>
>> refflat[['a']] <- read.table('fileA.txt')
>> refflat[['b']] <- read.table('fileB.txt')
>>
>> I am doing this to guard against a local version of refflat hanging
>> around, as I want to refresh it with each new session (and so, want to
>> store it in the package environment).  I just can't quite get hot to
>> make that work so that I am storing to the package:refflat rather than
>> any in .GlobalEnv.
>
> Try this:
>
> locPkg <- as.environment("package:locPkg")
> locPkg$refflat <- list()
> locPkg$refflat[["a"]] <- read.table("fileA.txt")
>

Gabor,

Thanks.  That will do the trick, I think.  Now, just one more question 
as followup if I may?  If I am executing a function that was defined in 
locPkg, how can I determine dynamically what environment I am in?  Them 
I could do something like

myFun <- function (x) {
   # code
   locPkg <- as.environment( .... )
   # more code
}

where '....' is replaced dynamically by the package environment of the 
package in which the function myFun is defined.  The goal is to make 
the code a bit more portable.

Again, thanks for the help.

Sean



From David.Brahm at geodecapital.com  Thu May 12 16:41:42 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Thu, 12 May 2005 10:41:42 -0400
Subject: [R] Multiple IF statements - is there a better alternative?
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBC1@MSGBOSCLF2WIN.DMN1.FMR.COM>

Glen Jones <Glen.Jones at team.telstra.com> wrote:
> if (InternalMean == 0)
>     Intresult = 1
> if (InternalMean > 0 & InternalMean < 1)
>     Intresult = .95
> [etc.]

This looks like a job for "cut":

R> i <- cut(InternalMean, c(-Inf,0,1,2,4,9,Inf), labels=F)
R> Intresult <- c(1,.95,.85,.70,.50,0)[i]

-- David Brahm (brahm at alum.mit.edu)



From ggrothendieck at gmail.com  Thu May 12 16:46:02 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 12 May 2005 10:46:02 -0400
Subject: [R] assigning to a list in a package environment
In-Reply-To: <ca6d62b3799f17f5617998df07bdf36b@mail.nih.gov>
References: <c6192184b8423427f853348ca8ae08d4@mail.nih.gov>
	<971536df0505120709d0133a@mail.gmail.com>
	<ca6d62b3799f17f5617998df07bdf36b@mail.nih.gov>
Message-ID: <971536df05051207461324506d@mail.gmail.com>

I did not fully understand which environment you want but:

1. if this is written in your function:  e <- environment()
    then e contains the current environment in the function

2. parent.env(e)   is the lexical environment within which
    your function was defined.  Also, if the function is called
    f then environment(f) gives this too.

3. parent.frame() is the environment from which your 
    function was called.

On 5/12/05, Sean Davis <sdavis2 at mail.nih.gov> wrote:
> 
> On May 12, 2005, at 10:09 AM, Gabor Grothendieck wrote:
> 
> > On 5/11/05, Sean Davis <sdavis2 at mail.nih.gov> wrote:
> >> I have a list in a package environment
> >>
> >> assign('refflat',list(),pos='package:locPkg')
> >>
> >> to which I would like to make assignments like:
> >>
> >> refflat[['a']] <- read.table('fileA.txt')
> >> refflat[['b']] <- read.table('fileB.txt')
> >>
> >> I am doing this to guard against a local version of refflat hanging
> >> around, as I want to refresh it with each new session (and so, want to
> >> store it in the package environment).  I just can't quite get hot to
> >> make that work so that I am storing to the package:refflat rather than
> >> any in .GlobalEnv.
> >
> > Try this:
> >
> > locPkg <- as.environment("package:locPkg")
> > locPkg$refflat <- list()
> > locPkg$refflat[["a"]] <- read.table("fileA.txt")
> >
> 
> Gabor,
> 
> Thanks.  That will do the trick, I think.  Now, just one more question
> as followup if I may?  If I am executing a function that was defined in
> locPkg, how can I determine dynamically what environment I am in?  Them
> I could do something like
> 
> myFun <- function (x) {
>   # code
>   locPkg <- as.environment( .... )
>   # more code
> }
> 
> where '....' is replaced dynamically by the package environment of the
> package in which the function myFun is defined.  The goal is to make
> the code a bit more portable.
> 
> Again, thanks for the help.
> 
> Sean
> 
>



From stefaan.lhermitte at biw.kuleuven.be  Thu May 12 17:53:28 2005
From: stefaan.lhermitte at biw.kuleuven.be (Stefaan Lhermitte)
Date: Thu, 12 May 2005 17:53:28 +0200
Subject: [R] Princomp and calculations of original values
Message-ID: <42837BF8.6010803@biw.kuleuven.be>

Dear R-ians,

I am working with princomp and I now want to manually recalculate my 
original values. I want to do it to completely understand the procedure 
of principal components.

I tried it with a test data set (2 dimensions) and  I was able to 
calculate my original values (of a random point of my dataset) using the 
output of princomp:
test.data$scores[1,]%*%matrix(data=test.data$loadings,ncol=2)+test.data$center
it seemed that result is identical with
test[1,]

When I tried it with my 4 dimensional real dataset, it did not work:
pca.data$scores[1,]%*%matrix(data=pca.data$loadings,ncol=4)+pca.data$center
is not equal to
data[1,]

Do I misunderstand the procedure somewhere, or what else do I do wrong?

Thanx in advance and kind regards,
Stef



From ernesto at ipimar.pt  Thu May 12 18:01:17 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 12 May 2005 17:01:17 +0100
Subject: [R] acf problem ?
Message-ID: <42837DCD.9070103@ipimar.pt>

Hi

I'm getting the following error that do not make sense to me, what am 
Idoing wrong ?

 > acf(Recsim[1,], lag.max=1)
Error in acf(Recsim[1, ], lag.max = 1) : 'lag.max' must be at least 1

Regards

EJ



From koepsell at u.washington.edu  Thu May 12 18:06:11 2005
From: koepsell at u.washington.edu (Thomas Koepsell)
Date: Thu, 12 May 2005 09:06:11 -0700 (PDT)
Subject: [R] Installing RMySQL on Mac OS X
Message-ID: <Pine.A41.4.61b.0505120846050.343104@homer04.u.washington.edu>


I could really use some advice about installing the RMySQL package under R 
2.1.0 on Mac OS 10.4.  I get the following message from R when I try:

---------------------------------------------------------------------------
Configuration error:
    Could not locate the library "libz" required by MySQL.

INSTRUCTIONS:

    The "libz" library is required by the MySQL client library
    in order to compress/uncompress connections between clients
    and the MySQL engine.

    Make sure you have "libz" installed properly and/or included
    in your $LD_LIBRARY_PATH.  Perhaps it is not in any of the
    standard directories (e.g., /usr/lib/, /usr/local/lib)?

Aborting the installation of RMySQL.

ERROR: configuration failed for package 'RMySQL'
---------------------------------------------------------------------------

The above message notwithstanding, files that appear in /usr/lib/ on my 
machine include:

   libz.1.1.3.dylib        libz.1.dylib
   libz.1.2.2.dylib        libz.dylib

so libz appears to be there.

An earlier suggestion on this list about changing the environment 
variables PKG_CPPFLAGS and PKG_LIBS had no effect, unfortunately.

I installed R 2.1.0 from a binary download.  The required DBI package is 
installed.  A binary version of RMySQL is not available for Macs, so I am 
trying to install RMySQL from sources.  Before getting Mac OS 10.4, I 
tried the same installation under 10.3 and 10.2 and had the same problem, 
and also with R 2.0.1.

I know others have working installations of RMySQL on their Mac OS X 
systems.  Any tips for getting there would be most welcome.  Thanks!

Tom Koepsell



From ripley at stats.ox.ac.uk  Thu May 12 18:12:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 May 2005 17:12:54 +0100 (BST)
Subject: [R] Solaris 10 on amd and R-2.1.0
In-Reply-To: <1115898054.7591.8.camel@clover>
References: <1115898054.7591.8.camel@clover>
Message-ID: <Pine.LNX.4.61.0505121703540.31239@gannet.stats>

On Thu, 12 May 2005, Vin Everett wrote:

> I am having problems compiling R on a Solaris 10 opteron box we have on
> trial.
>
> checking for Fortran libraries of g77...  -L/usr/ccs/lib -L/usr/lib -
> L/usr/local/lib -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2 -
> L/usr/ccs/bin -L/usr/local/lib/gcc-lib/i386-pc-
> solaris2.10/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s -lfrtbegin: -
> lg2c:
> checking how to get verbose linking output from gcc... -v
> checking for C libraries of gcc...  -L/usr/ccs/lib -L/usr/lib -
> L/usr/local/lib -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2 -
> L/usr/ccs/bin -L/usr/local/lib/gcc-lib/i386-pc-
> solaris2.10/3.3.2/../../.. -lgcc_eh
> checking for dummy main to link with Fortran libraries... unknown
> configure: error: linking to Fortran libraries from C fails
> See `config.log' for more details.
>
> I have installed gcc from sunfreeware

Did you also install the GNU binutils from there?

> Looks like the following are not found:-
>
> configure:26575: gcc -o conftest -g -O2 -I/usr/local/include -
> L/usr/local/lib conftest.c -
> ldl -lm   -lg2c -lm -lgcc_s -lfrtbegin: -lg2c: >&5
> ld: fatal: library -lg2c: not found
> ld: fatal: library -lfrtbegin:: not found
> ld: fatal: library -lg2c:: not found
> ld: fatal: File processing errors. No output written to conftest
> collect2: ld returned 1 exit status
> configure:26581: $? = 1
> configure: failed program was:
> | /* confdefs.h.  */
>
>
> Any idea where to get them from ?

I've tried to reassemble the line, and I get

checking for Fortran libraries of g77...  -L/usr/ccs/lib -L/usr/lib
-L/usr/local/lib -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2
-L/usr/ccs/bin -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2/../../..
-lfrtbegin -lg2c -lm -lgcc_s -lfrtbegin: -lg2c:

Now, the problem is those trailing colons: I have no idea what they mean
and nor it seems does gcc. (I don't see where -lg2c:: comes from.)

It should be possible to work around this, by setting

FLIBS=-lg2c -lm -lgcc_s

e.g. in config.site and re-configuring.

That is a somewhat old version of gcc: I suggest you use it to compile
3.4.3 from the sources and see if that works better.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu May 12 18:16:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 May 2005 17:16:54 +0100 (BST)
Subject: [R] correlogram in spatial producing values outside [-1,1]
In-Reply-To: <D71DD12EF4EDE740A09A929F6D2C509910BB77@mailuk1.rms.com>
References: <D71DD12EF4EDE740A09A929F6D2C509910BB77@mailuk1.rms.com>
Message-ID: <Pine.LNX.4.61.0505121714410.31239@gannet.stats>

It can happen.  The covariance is produced from a set of pairs, and the 
variances from all the points.  It usually indicates that too fine a 
binning has been used or that there is a trend in the data which has not 
been removed.

On Thu, 12 May 2005, Kechi Nzerem wrote:

> I'm using the correlogram function in the spatial library to calculate
> spatial correlograms of radar data.  However, I'm finding that the
> resulting values are often outside the range [-1,1], usually only at
> larger distances of separation.
>
> I'm not sure whether to be overly concerned about this, or dismiss it as
> some artefact of the data.  Has anyone had similar experiences?  Or know
> why this might be happening?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vin.everett at cimr.cam.ac.uk  Thu May 12 18:21:52 2005
From: vin.everett at cimr.cam.ac.uk (Vin Everett)
Date: Thu, 12 May 2005 17:21:52 +0100
Subject: [R] Solaris 10 on amd and R-2.1.0
In-Reply-To: <Pine.LNX.4.61.0505121703540.31239@gannet.stats>
References: <1115898054.7591.8.camel@clover>
	<Pine.LNX.4.61.0505121703540.31239@gannet.stats>
Message-ID: <1115914912.7591.72.camel@clover>

On Thu, 2005-05-12 at 17:12 +0100, Prof Brian Ripley wrote:
> On Thu, 12 May 2005, Vin Everett wrote:
> 
> > I am having problems compiling R on a Solaris 10 opteron box we have on
> > trial.
> >
> > checking for Fortran libraries of g77...  -L/usr/ccs/lib -L/usr/lib -
> > L/usr/local/lib -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2 -
> > L/usr/ccs/bin -L/usr/local/lib/gcc-lib/i386-pc-
> > solaris2.10/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s -lfrtbegin: -
> > lg2c:
> > checking how to get verbose linking output from gcc... -v
> > checking for C libraries of gcc...  -L/usr/ccs/lib -L/usr/lib -
> > L/usr/local/lib -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2 -
> > L/usr/ccs/bin -L/usr/local/lib/gcc-lib/i386-pc-
> > solaris2.10/3.3.2/../../.. -lgcc_eh
> > checking for dummy main to link with Fortran libraries... unknown
> > configure: error: linking to Fortran libraries from C fails
> > See `config.log' for more details.
> >
> > I have installed gcc from sunfreeware
> 
> Did you also install the GNU binutils from there?
> 
> > Looks like the following are not found:-
> >
> > configure:26575: gcc -o conftest -g -O2 -I/usr/local/include -
> > L/usr/local/lib conftest.c -
> > ldl -lm   -lg2c -lm -lgcc_s -lfrtbegin: -lg2c: >&5
> > ld: fatal: library -lg2c: not found
> > ld: fatal: library -lfrtbegin:: not found
> > ld: fatal: library -lg2c:: not found
> > ld: fatal: File processing errors. No output written to conftest
> > collect2: ld returned 1 exit status
> > configure:26581: $? = 1
> > configure: failed program was:
> > | /* confdefs.h.  */
> >
> >
> > Any idea where to get them from ?
> 
> I've tried to reassemble the line, and I get
> 
> checking for Fortran libraries of g77...  -L/usr/ccs/lib -L/usr/lib
> -L/usr/local/lib -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2
> -L/usr/ccs/bin -L/usr/local/lib/gcc-lib/i386-pc-solaris2.10/3.3.2/../../..
> -lfrtbegin -lg2c -lm -lgcc_s -lfrtbegin: -lg2c:
> 
> Now, the problem is those trailing colons: I have no idea what they mean
> and nor it seems does gcc. (I don't see where -lg2c:: comes from.)
> 
> It should be possible to work around this, by setting
> 
> FLIBS=-lg2c -lm -lgcc_s
> 
> e.g. in config.site and re-configuring.


Thanks Brian, 

I got a fix by installing the compiler from the software companion CD
(gcc version 3.4.3) and it compiled fine, the libs in question are then
in /opt/sfw/lib so a changed to LD_LIBRARY_PATH and I was Ok.

Thanks for your help.

Cheers Vin
-- 
Vin.Everett at cimr.cam.ac.uk
JDRF/WT Diabetes and Inflammation Laboratory (DIL)
Cambridge Institute for Medical Research (CIMR)
Wellcome Trust/MRC Building Addenbrooke's Hospital 
Hills Road Cambridge CB2 2XY
+44 1223 763212
+44 7990 966266



From ripley at stats.ox.ac.uk  Thu May 12 18:30:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 May 2005 17:30:58 +0100 (BST)
Subject: [R] acf problem ?
In-Reply-To: <42837DCD.9070103@ipimar.pt>
References: <42837DCD.9070103@ipimar.pt>
Message-ID: <Pine.LNX.4.61.0505121727500.31239@gannet.stats>

On Thu, 12 May 2005, Ernesto Jardim wrote:

> I'm getting the following error that do not make sense to me, what am Idoing 
> wrong ?
>
>> acf(Recsim[1,], lag.max=1)
> Error in acf(Recsim[1, ], lag.max = 1) : 'lag.max' must be at least 1

Inside the code

     lag.max <- min(lag.max, sampleT - 1)
     if (lag.max < 1)
         stop("'lag.max' must be at least 1")

so you have only one point in your `time series', I think.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ernesto at ipimar.pt  Thu May 12 18:35:28 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 12 May 2005 17:35:28 +0100
Subject: [R] acf problem ? SOLVED
In-Reply-To: <42837DCD.9070103@ipimar.pt>
References: <42837DCD.9070103@ipimar.pt>
Message-ID: <428385D0.7090004@ipimar.pt>

Ernesto Jardim wrote:

> Hi
>
> I'm getting the following error that do not make sense to me, what am 
> Idoing wrong ?
>
> > acf(Recsim[1,], lag.max=1)
> Error in acf(Recsim[1, ], lag.max = 1) : 'lag.max' must be at least 1
>
> Regards
>
> EJ
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

Hi

The problem was that Recsim[1,] was not numeric.

EJ



From sundar.dorai-raj at pdf.com  Thu May 12 18:35:45 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 12 May 2005 11:35:45 -0500
Subject: [R] acf problem ?
In-Reply-To: <42837DCD.9070103@ipimar.pt>
References: <42837DCD.9070103@ipimar.pt>
Message-ID: <428385E1.7080302@pdf.com>



Ernesto Jardim wrote on 5/12/2005 11:01 AM:
> Hi
> 
> I'm getting the following error that do not make sense to me, what am 
> Idoing wrong ?
> 
>  > acf(Recsim[1,], lag.max=1)
> Error in acf(Recsim[1, ], lag.max = 1) : 'lag.max' must be at least 1
> 
> Regards
> 
> EJ
> 

Hi, Ernesto,

What is Recsim[1,]? I can reproduce this error using:

 > acf(c(lh), 1)
 > acf(matrix(lh,ncol=1), 1)
 > acf(matrix(lh,nrow=1), 1)
Error in acf(matrix(lh, nrow = 1), 1) : 'lag.max' must be at least 1

So, maybe try

acf(c(Recsim[1,]), lag.max=1)

HTH,

--sundar



From lars.claussen at pik-potsdam.de  Thu May 12 18:48:32 2005
From: lars.claussen at pik-potsdam.de (Lars)
Date: Thu, 12 May 2005 18:48:32 +0200
Subject: [R] tempsum
Message-ID: <428388E0.9010205@pik-potsdam.de>

hi,

i'd like to calculate a temperatursum, adding the value of each element.
let's say the data looks like this:

 x<-c(1,2,3,4,5)
what i want to do, is ploting not the sum in the end but all the 
subresults, too,
so my vector holds:
x[i]
[1]
1,3,6,10,15
here is what i tried, which seems to be right to me, bu doesn't work out:

 x<-c(1,2,3,4,5)
i<-1
j<-1
z[j]<-x[i]+x[i+1]
p<-z[j]
j<-2:4
i<-3:5
z[j]<-z[j-1]+x[i]
plot (c(p,z[j])

what i get is:
[1] 6.0  12.0  9.2  18.0

any sugestions? lars



From Achim.Zeileis at wu-wien.ac.at  Thu May 12 18:55:18 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 12 May 2005 18:55:18 +0200
Subject: [R] tempsum
In-Reply-To: <428388E0.9010205@pik-potsdam.de>
References: <428388E0.9010205@pik-potsdam.de>
Message-ID: <20050512185518.0a7991f8.Achim.Zeileis@wu-wien.ac.at>

?cumsum
Z

On Thu, 12 May 2005 18:48:32 +0200 Lars wrote:

> hi,
> 
> i'd like to calculate a temperatursum, adding the value of each
> element. let's say the data looks like this:
> 
>  x<-c(1,2,3,4,5)
> what i want to do, is ploting not the sum in the end but all the 
> subresults, too,
> so my vector holds:
> x[i]
> [1]
> 1,3,6,10,15
> here is what i tried, which seems to be right to me, bu doesn't work
> out:
> 
>  x<-c(1,2,3,4,5)
> i<-1
> j<-1
> z[j]<-x[i]+x[i+1]
> p<-z[j]
> j<-2:4
> i<-3:5
> z[j]<-z[j-1]+x[i]
> plot (c(p,z[j])
> 
> what i get is:
> [1] 6.0  12.0  9.2  18.0
> 
> any sugestions? lars
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu May 12 19:08:34 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 12 May 2005 19:08:34 +0200
Subject: [R] tempsum
References: <428388E0.9010205@pik-potsdam.de>
Message-ID: <002601c55715$3b30a2a0$0540210a@www.domain>

maybe you can find cumsum() helpful, e.g.,

cumsum(1:5)

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Lars" <lars.claussen at pik-potsdam.de>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, May 12, 2005 6:48 PM
Subject: [R] tempsum


> hi,
>
> i'd like to calculate a temperatursum, adding the value of each 
> element.
> let's say the data looks like this:
>
> x<-c(1,2,3,4,5)
> what i want to do, is ploting not the sum in the end but all the 
> subresults, too,
> so my vector holds:
> x[i]
> [1]
> 1,3,6,10,15
> here is what i tried, which seems to be right to me, bu doesn't work 
> out:
>
> x<-c(1,2,3,4,5)
> i<-1
> j<-1
> z[j]<-x[i]+x[i+1]
> p<-z[j]
> j<-2:4
> i<-3:5
> z[j]<-z[j-1]+x[i]
> plot (c(p,z[j])
>
> what i get is:
> [1] 6.0  12.0  9.2  18.0
>
> any sugestions? lars
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From khobson at fd9ns01.okladot.state.ok.us  Thu May 12 19:14:23 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Thu, 12 May 2005 12:14:23 -0500
Subject: [R] Need help with vector designations in a Function
Message-ID: <OF175C39F7.D969CE92-ON86256FFF.005E66A0-86256FFF.005E9AB7@fd9ns01.okladot.state.ok.us>





My function works fine if the X and Y exist in the T314 data.  How can I
code the macro to allow vector designations in the function different than
X and Y?  Maybe something to do with environment?

rm(list=ls())

# Data from NCHRP Appendix A -
http://trb.org/publications/nchrp/nchrp_w71.pdf
T314 <- structure(list(Lab = as.integer(c(1:60)), X = c(4.89, 3.82, 2.57,
2.3,2.034, 2, 1.97, 1.85,1.85, 1.85, 1.84, 1.82, 1.82, 1.77, 1.76, 1.67,
1.66,
1.63, 1.62,1.62, 1.55, 1.54, 1.54, 1.53, 1.53, 1.44, 1.428, 1.42, 1.39,
1.36,
1.35, 1.31, 1.28, 1.24, 1.24, 1.23, 1.22, 1.21, 1.19, 1.18, 1.18, 1.18,
1.17,
1.16, 1.13, 1.13, 1.099, 1.09, 1.09, 1.08, 1.07, 1.05, 0.98, 0.97, 0.84,
0.808,
0.69, 0.63, 0.6, 0.5), Y = c(5.28, 3.82, 2.41, 2.32, 2.211, 1.46, 2.24,
1.91,
1.78, 1.63, 1.81, 1.92, 1.2, 1.67, 1.28, 1.59, 1.45, 2.06, 1.91, 1.19,
1.26,
1.79, 1.39, 1.48, 0.72, 1.29, 1.517, 1.71, 1.12, 1.38, 0.93, 1.36, 1.2,
1.23,
0.71, 1.29, 1.26, 1.48, 1.26, 1.33, 1.21, 1.04, 1.57, 1.42, 1.08, 1.04,
1.33,
1.33, 1.2, 1.05, 1.24, 0.91, 0.99, 1.06, 1.27, 0.702, 0.77, 0.58, 1,
0.38)),
.Names = c("Lab", "X", "Y" ), class = "data.frame",
row.names = as.character(c(1:60)))

### Be sure to remove NA data prior to oOut()
oOut <- function(dsin, dsout, X, Y)
  {
  oOutsub <- function(olimit){
    # Get Medians for Invalid Data Determination
    Xmed <- median(dsin$X); Ymed <- median(dsin$Y)
    # Make new dataset with (Y-X)-(Ymedian-Xmedian) column
    dsout <- cbind(dsin, XY=(dsin$Y-dsin$X)-(Ymed-Xmed))
    # Get median for new column
    XYmed <- median(dsout$XY)
    iqx <- diff(quantile(dsin$X, c(0.125, .875)))
    iqy <- diff(quantile(dsin$Y, c(0.125, .875)))
    iqxy <- diff(quantile(dsout$XY, c(0.125, .875)))
    # Invalid Upper Limits
    iulX <- quantile(dsin$X, 0.875)+olimit*iqx

    iulY <- quantile(dsin$Y, 0.875)+olimit*iqy
    iulXY <- quantile(dsout$XY, 0.875)+olimit*iqxy
    # Invalid Lower Limits
    illX <- quantile(dsin$X, 0.125)-olimit*iqx
    illY <- quantile(dsin$Y, 0.125)-olimit*iqy
    illXY <- quantile(dsout$XY, 0.125)-olimit*iqxy

    dsout <- subset(dsout, with(dsout, X <= iulX & X >= illX))
    dsout <- subset(dsout, with(dsout, Y <= iulY & Y >= illY))
    dsout <- subset(dsout, with(dsout, XY <= iulXY & XY >= illXY))
    dsout
    }
  dsout <- oOutsub(1.555)  #Eliminates Invalid Data
  dsin <- dsout
  dsout <- oOutsub(0.674)  #Eliminates Outlier Data
  dsout <- dsout[1:(ncol(dsout)-2)]  #Trim outer 2 XY columns
  dsout
  }

T314.o <- oOut(T314, T314.o, X, Y)
T314.o # showing resutls.  Notice 2nd XY name

cv <- function(x)
{
sd(x)/(mean(x))*100
}

T314 <- cbind(T314, X.mean=mean(T314$X))
T314 <- cbind(T314, X.sd=sd(T314$X))
T314 <- cbind(T314, X.cv=cv(T314$X))
T314 <- cbind(T314, Y.count=NROW(T314$Y))
T314 <- cbind(T314, Y.mean=mean(T314$Y))
T314 <- cbind(T314, Y.sd=sd(T314$Y))
T314 <- cbind(T314, Y.cv=cv(T314$Y))

T314.o <- cbind(T314.o, X.mean=mean(T314.o$X))
T314.o <- cbind(T314.o, X.sd=sd(T314.o$X))
T314.o <- cbind(T314.o, X.cv=cv(T314.o$X))
T314.o <- cbind(T314.o, Y.count=NROW(T314.o$Y))
T314.o <- cbind(T314.o, Y.mean=mean(T314.o$Y))
T314.o <- cbind(T314.o, Y.sd=sd(T314.o$Y))
T314.o <- cbind(T314.o, Y.cv=cv(T314.o$Y))

T314.o <- cbind(T314.o, ElimLabs=paste(setdiff(T314$Lab, T314.o$Lab),
collapse=", "))
# Number of standard deviations
T314.o <- cbind(T314.o, X.nsd=(T314.o$X-T314.o$X.mean)/T314.o$X.sd)
T314.o <- cbind(T314.o, Y.nsd=(T314.o$Y-T314.o$Y.mean)/T314.o$Y.sd)
# X Ratings next
T314.o<-cbind(T314.o, X.rate=NA)
for (i in 1:nrow(T314.o))
{s<-ifelse(T314.o$X.nsd[i]<0,-1,1)
if(abs(T314.o$X.nsd[i])<1) T314.o$X.rate[i]=s*5
if(abs(T314.o$X.nsd[i])>=1 & T314.o$X.nsd[i]<1.5) T314.o$X.rate[i]=s*4
if(abs(T314.o$X.nsd[i])>=1.5 & T314.o$X.nsd[i]<2) T314.o$X.rate[i]=s*3
if(abs(T314.o$X.nsd[i])>=2 & T314.o$X.nsd[i]<2.5) T314.o$X.rate[i]=s*2
if(abs(T314.o$X.nsd[i])>=2.5 & T314.o$X.nsd[i]<3) T314.o$X.rate[i]=s*1
if(abs(T314.o$X.nsd[i])>=3) T314.o$X.rate[i]=0
}
# Y Ratings next
T314.o<-cbind(T314.o, Y.rate=NA)
for (i in 1:nrow(T314.o))
{s<-ifelse(T314.o$Y.nsd[i]<0,-1,1)
if(abs(T314.o$Y.nsd[i])<1) T314.o$Y.rate[i]=s*5
if(abs(T314.o$Y.nsd[i])>=1 & T314.o$Y.nsd[i]<1.5) T314.o$Y.rate[i]=s*4
if(abs(T314.o$Y.nsd[i])>=1.5 & T314.o$Y.nsd[i]<2) T314.o$Y.rate[i]=s*3
if(abs(T314.o$Y.nsd[i])>=2 & T314.o$Y.nsd[i]<2.5) T314.o$Y.rate[i]=s*2
if(abs(T314.o$Y.nsd[i])>=2.5 & T314.o$Y.nsd[i]<3) T314.o$Y.rate[i]=s*1
if(abs(T314.o$Y.nsd[i])>=3) T314.o$Y.rate[i]=0
}

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From vograno at evafunds.com  Thu May 12 19:23:25 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 12 May 2005 10:23:25 -0700
Subject: [R] time zones, daylight saving etc.
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A59E951A@phost015.EVAFUNDS.intermedia.net>

Works for me on Linux:
> Sys.time()
[1] "2005-05-12 10:22:31 PDT"
> Sys.putenv(TZ="GMT")
> Sys.time()
[1] "2005-05-12 17:22:37 GMT"

I extensively use the reset of TZ to parse times. 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> Grothendieck
> Sent: Thursday, May 12, 2005 6:18 AM
> To: Prof Brian Ripley
> Cc: Carla Meurk; r-help at stat.math.ethz.ch
> Subject: Re: [R] time zones, daylight saving etc.
> 
> I have tried this but on Windows XP R 2.1.0 found I had to 
> set it outside of R prior to starting R. 
> 
> 1. unsuccessful
> 
> > Sys.time()
> [1] "2005-05-12 09:08:03 Eastern Daylight Time"
> > Sys.putenv(TZ="GMT")
> > Sys.time() # no change
> [1] "2005-05-12 09:08:12 Eastern Daylight Time"
> 
> 2. OK
> 
> C:\>set tz=GMT
> 
> C:\>start "" "\Program Files\R\rw2010\bin\r.exe"
> 
> R : Copyright 2005, The R Foundation for Statistical 
> Computing Version 2.1.0 Patched (2005-04-18), ISBN 3-900051-07-0
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
>   Natural language support but running in an English locale
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and 'citation()' 
> on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or 
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
> 
> > Sys.time()
> [1] "2005-05-12 13:10:58 GMT"
> 
> I assume it could be set in .Renviron but it would be nice if 
> one could set it right from within R so that one can write a 
> function that sets it, does processing and then sets it back. 
>  Don't know if this is possible.
> 
> On 5/12/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > Would it not just be easier to set the timezone to GMT for the 
> > duration of the calculations?  I don't see an OS mentioned 
> here, but 
> > on most TZ=GMT for the session will do it.
> > 
> > On Thu, 12 May 2005, Rich FitzJohn wrote:
> > 
> > > Hi,
> > >
> > > seq.dates() in the chron package does not allow creating 
> sequences 
> > > by minutes, so you'd have to roll your own sequence generator.
> > >
> > > Looks like the tzone attribute of the times is lost when using 
> > > min(),
> > > max() and seq().  You can apply it back manually, but it does not 
> > > affect the calculation, since POSIXct times are stored as seconds 
> > > since 1/1/1970 (?DateTimeClasses).
> > >
> > > ## These dates/times just span the move from NZDT to NZST:
> > > dt.dates <- paste(rep(15:16, c(5,7)), "03", "2003", sep="/") 
> > > dt.times <- paste(c(19:23, 0:6), "05", sep=":") dt <- 
> > > paste(dt.dates, dt.times)
> > >
> > > ## No shift in times, or worrying about daylight savings; 
> > > appropriate ## iff the device doing the recording was not itself 
> > > adjusting for ## daylight savings, presumably.
> > > datetime <- as.POSIXct(strptime(dt, "%d/%m/%Y %H:%M"), "GMT")
> > >
> > > ## Create two objects with all the times in your range, 
> one with the 
> > > ## tzone attribute set back to GMT (to match datetimes), 
> and one ## 
> > > without this.
> > > mindata1 <- mindata2 <- seq(from=min(datetime), to=max(datetime),
> > >                            by="mins") attr(mindata2, "tzone") <- 
> > > "GMT"
> > >
> > > fmt <- "%Y %m %d %H %M"
> > > ## These both do the matching correctly:
> > > match(format(datetime, fmt), format(mindata1, fmt, tz="GMT")) 
> > > match(format(datetime, fmt), format(mindata2, fmt, tz="GMT"))
> > >
> > > ## However, the first of these will not, as it gets the 
> timezone all 
> > > ## wrong, since it's neither specified in the call to 
> format(), or 
> > > as ## an attribute of the POSIXct object.
> > > match(format(datetime, fmt), format(mindata1, fmt)) 
> > > match(format(datetime, fmt), format(mindata2, fmt))
> > >
> > > ## It is also possible to run match() directly off the POSIXct 
> > > object, ## but I'm not sure how this will interact with 
> things like 
> > > leap ## seconds:
> > > match(datetime, mindata1)
> > >
> > > Time zones do my head in, so you probably want to check this all 
> > > pretty carefully.  Looks like there's lots of gotchas (e.g. 
> > > subsetting a POSIXct object strips the tzone attribute).
> > >
> > > Cheers,
> > > Rich
> > >
> > > On 5/12/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > >> You could use the chron package.  It represents date 
> times without 
> > >> using time zones so you can't have this sort of problem.
> > >>
> > >> On 5/10/05, Carla Meurk <ksm32 at student.canterbury.ac.nz> wrote:
> > >>> Hi,  I have a whole bunch of data, which looks like:
> > >>>
> > >>> 15/03/2003       10:20  1
> > >>> 15/03/2003       10:21  0
> > >>> 15/03/2003       12:02  0
> > >>> 16/03/2003       06:10  0
> > >>> 16/03/2003       06:20  0.5
> > >>> 16/03/2003       06:30  0
> > >>> 16/03/2003       06:40  0
> > >>> 16/03/2003       06:50  0
> > >>>
> > >>> 18/03/2003  20:10                 0.5
> > >>> etc. (times given on a 24 hour clock)
> > >>>
> > >>> and goes on for years.  I have some code:
> > >>>
> > >>> data<-read.table("H:/rainfall_data.txt",h=T)
> > >>> library(survival)
> > >>> datetime <- as.POSIXct(strptime(paste(data$V1, 
> data$V2), "%d/%m/%Y 
> > >>> %H:%M"), tz="NZST")
> > >>>
> > >>> which produces:
> > >>>
> > >>> [10] "2003-03-13 21:13:00 New Zealand Daylight Time"
> > >>> [11] "2003-03-15 13:20:00 New Zealand Daylight Time"
> > >>> [12] "2003-03-15 22:20:00 New Zealand Daylight Time"
> > >>> [13] "2003-03-15 22:21:00 New Zealand Daylight Time"
> > >>> [14] "2003-03-16 00:02:00 New Zealand Daylight Time"
> > >>> [15] "2003-03-16 18:10:00 New Zealand Standard Time"
> > >>> [16] "2003-03-16 18:20:00 New Zealand Standard Time"
> > >>> [17] "2003-03-16 18:30:00 New Zealand Standard Time"
> > >>>
> > >>> My problem is that "15/03/2003 12:02" has become 
> "16/03/2003 00:02"
> > >>> i.e.  it is 12 hours behind (as is everything else), 
> but also, I 
> > >>> do not want to change time zones.
> > >>>
> > >>> The 12 hour delay is not really a problem just an 
> annoyance, but 
> > >>> the time zone change is a problem because later on I 
> need to match 
> > >>> up data by time using
> > >>>
> > >>> mindata<-seq(from=min(datetime),to=max(datetime),by="mins")
> > >>> newdata<-matrix(0,length(mindata),1)
> > >>> newdata[match(format.POSIXct(datetime,"%Y %m %d %H 
> > >>> %M"),format.POSIXct(mindata,"%Y %m %d %H %M"))]<-data$V3
> > >>>
> > >>> and things go wrong here with matching repeating times/missing 
> > >>> times around the timezone changes and, my resulting vector is 1 
> > >>> hour shorter than my other series.  From the R help I 
> see that my 
> > >>> OS may be to blame but, even if I specify tz="GMT" I still get 
> > >>> NZST and NZDT.  Can someone help?
> > >>>
> > >>> I hope this all makes sense
> > >>>
> > >>> Carla
> > >>>
> > >>> ______________________________________________
> > >>> R-help at stat.math.ethz.ch mailing list 
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide! 
> > >>> http://www.R-project.org/posting-guide.html
> > >>>
> > >>
> > >> ______________________________________________
> > >> R-help at stat.math.ethz.ch mailing list 
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide! 
> > >> http://www.R-project.org/posting-guide.html
> > >>
> > >
> > >
> > > --
> > > Rich FitzJohn
> > > rich.fitzjohn <at> gmail.com   |    
> http://homepages.paradise.net.nz/richa183
> > >                      You are in a maze of twisty little 
> functions, 
> > > all alike
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > >
> > 
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu May 12 19:27:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 May 2005 18:27:01 +0100 (BST)
Subject: [R] time zones, daylight saving etc.
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A59E951A@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A59E951A@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.LNX.4.61.0505121824420.10597@gannet.stats>

Yes, I did mention the OS was unstated, knowing that some had problems. 
I will try to investigate why this is not working on Windows for a future 
release.

For Windows users something like

.../RGui.exe TZ=GMT

should do it.

On Thu, 12 May 2005, Vadim Ogranovich wrote:

> Works for me on Linux:
>> Sys.time()
> [1] "2005-05-12 10:22:31 PDT"
>> Sys.putenv(TZ="GMT")
>> Sys.time()
> [1] "2005-05-12 17:22:37 GMT"
>
> I extensively use the reset of TZ to parse times.
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor
>> Grothendieck
>> Sent: Thursday, May 12, 2005 6:18 AM
>> To: Prof Brian Ripley
>> Cc: Carla Meurk; r-help at stat.math.ethz.ch
>> Subject: Re: [R] time zones, daylight saving etc.
>>
>> I have tried this but on Windows XP R 2.1.0 found I had to
>> set it outside of R prior to starting R.
>>
>> 1. unsuccessful
>>
>>> Sys.time()
>> [1] "2005-05-12 09:08:03 Eastern Daylight Time"
>>> Sys.putenv(TZ="GMT")
>>> Sys.time() # no change
>> [1] "2005-05-12 09:08:12 Eastern Daylight Time"
>>
>> 2. OK
>>
>> C:\>set tz=GMT
>>
>> C:\>start "" "\Program Files\R\rw2010\bin\r.exe"
>>
>> R : Copyright 2005, The R Foundation for Statistical
>> Computing Version 2.1.0 Patched (2005-04-18), ISBN 3-900051-07-0
>>
>> R is free software and comes with ABSOLUTELY NO WARRANTY.
>> You are welcome to redistribute it under certain conditions.
>> Type 'license()' or 'licence()' for distribution details.
>>
>>   Natural language support but running in an English locale
>>
>> R is a collaborative project with many contributors.
>> Type 'contributors()' for more information and 'citation()'
>> on how to cite R or R packages in publications.
>>
>> Type 'demo()' for some demos, 'help()' for on-line help, or
>> 'help.start()' for a HTML browser interface to help.
>> Type 'q()' to quit R.
>>
>>> Sys.time()
>> [1] "2005-05-12 13:10:58 GMT"
>>
>> I assume it could be set in .Renviron but it would be nice if
>> one could set it right from within R so that one can write a
>> function that sets it, does processing and then sets it back.
>>  Don't know if this is possible.
>>
>> On 5/12/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>>> Would it not just be easier to set the timezone to GMT for the
>>> duration of the calculations?  I don't see an OS mentioned
>> here, but
>>> on most TZ=GMT for the session will do it.
>>>
>>> On Thu, 12 May 2005, Rich FitzJohn wrote:
>>>
>>>> Hi,
>>>>
>>>> seq.dates() in the chron package does not allow creating
>> sequences
>>>> by minutes, so you'd have to roll your own sequence generator.
>>>>
>>>> Looks like the tzone attribute of the times is lost when using
>>>> min(),
>>>> max() and seq().  You can apply it back manually, but it does not
>>>> affect the calculation, since POSIXct times are stored as seconds
>>>> since 1/1/1970 (?DateTimeClasses).
>>>>
>>>> ## These dates/times just span the move from NZDT to NZST:
>>>> dt.dates <- paste(rep(15:16, c(5,7)), "03", "2003", sep="/")
>>>> dt.times <- paste(c(19:23, 0:6), "05", sep=":") dt <-
>>>> paste(dt.dates, dt.times)
>>>>
>>>> ## No shift in times, or worrying about daylight savings;
>>>> appropriate ## iff the device doing the recording was not itself
>>>> adjusting for ## daylight savings, presumably.
>>>> datetime <- as.POSIXct(strptime(dt, "%d/%m/%Y %H:%M"), "GMT")
>>>>
>>>> ## Create two objects with all the times in your range,
>> one with the
>>>> ## tzone attribute set back to GMT (to match datetimes),
>> and one ##
>>>> without this.
>>>> mindata1 <- mindata2 <- seq(from=min(datetime), to=max(datetime),
>>>>                            by="mins") attr(mindata2, "tzone") <-
>>>> "GMT"
>>>>
>>>> fmt <- "%Y %m %d %H %M"
>>>> ## These both do the matching correctly:
>>>> match(format(datetime, fmt), format(mindata1, fmt, tz="GMT"))
>>>> match(format(datetime, fmt), format(mindata2, fmt, tz="GMT"))
>>>>
>>>> ## However, the first of these will not, as it gets the
>> timezone all
>>>> ## wrong, since it's neither specified in the call to
>> format(), or
>>>> as ## an attribute of the POSIXct object.
>>>> match(format(datetime, fmt), format(mindata1, fmt))
>>>> match(format(datetime, fmt), format(mindata2, fmt))
>>>>
>>>> ## It is also possible to run match() directly off the POSIXct
>>>> object, ## but I'm not sure how this will interact with
>> things like
>>>> leap ## seconds:
>>>> match(datetime, mindata1)
>>>>
>>>> Time zones do my head in, so you probably want to check this all
>>>> pretty carefully.  Looks like there's lots of gotchas (e.g.
>>>> subsetting a POSIXct object strips the tzone attribute).
>>>>
>>>> Cheers,
>>>> Rich
>>>>
>>>> On 5/12/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>>>> You could use the chron package.  It represents date
>> times without
>>>>> using time zones so you can't have this sort of problem.
>>>>>
>>>>> On 5/10/05, Carla Meurk <ksm32 at student.canterbury.ac.nz> wrote:
>>>>>> Hi,  I have a whole bunch of data, which looks like:
>>>>>>
>>>>>> 15/03/2003       10:20  1
>>>>>> 15/03/2003       10:21  0
>>>>>> 15/03/2003       12:02  0
>>>>>> 16/03/2003       06:10  0
>>>>>> 16/03/2003       06:20  0.5
>>>>>> 16/03/2003       06:30  0
>>>>>> 16/03/2003       06:40  0
>>>>>> 16/03/2003       06:50  0
>>>>>>
>>>>>> 18/03/2003  20:10                 0.5
>>>>>> etc. (times given on a 24 hour clock)
>>>>>>
>>>>>> and goes on for years.  I have some code:
>>>>>>
>>>>>> data<-read.table("H:/rainfall_data.txt",h=T)
>>>>>> library(survival)
>>>>>> datetime <- as.POSIXct(strptime(paste(data$V1,
>> data$V2), "%d/%m/%Y
>>>>>> %H:%M"), tz="NZST")
>>>>>>
>>>>>> which produces:
>>>>>>
>>>>>> [10] "2003-03-13 21:13:00 New Zealand Daylight Time"
>>>>>> [11] "2003-03-15 13:20:00 New Zealand Daylight Time"
>>>>>> [12] "2003-03-15 22:20:00 New Zealand Daylight Time"
>>>>>> [13] "2003-03-15 22:21:00 New Zealand Daylight Time"
>>>>>> [14] "2003-03-16 00:02:00 New Zealand Daylight Time"
>>>>>> [15] "2003-03-16 18:10:00 New Zealand Standard Time"
>>>>>> [16] "2003-03-16 18:20:00 New Zealand Standard Time"
>>>>>> [17] "2003-03-16 18:30:00 New Zealand Standard Time"
>>>>>>
>>>>>> My problem is that "15/03/2003 12:02" has become
>> "16/03/2003 00:02"
>>>>>> i.e.  it is 12 hours behind (as is everything else),
>> but also, I
>>>>>> do not want to change time zones.
>>>>>>
>>>>>> The 12 hour delay is not really a problem just an
>> annoyance, but
>>>>>> the time zone change is a problem because later on I
>> need to match
>>>>>> up data by time using
>>>>>>
>>>>>> mindata<-seq(from=min(datetime),to=max(datetime),by="mins")
>>>>>> newdata<-matrix(0,length(mindata),1)
>>>>>> newdata[match(format.POSIXct(datetime,"%Y %m %d %H
>>>>>> %M"),format.POSIXct(mindata,"%Y %m %d %H %M"))]<-data$V3
>>>>>>
>>>>>> and things go wrong here with matching repeating times/missing
>>>>>> times around the timezone changes and, my resulting vector is 1
>>>>>> hour shorter than my other series.  From the R help I
>> see that my
>>>>>> OS may be to blame but, even if I specify tz="GMT" I still get
>>>>>> NZST and NZDT.  Can someone help?
>>>>>>
>>>>>> I hope this all makes sense
>>>>>>
>>>>>> Carla
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide!
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide!
>>>>> http://www.R-project.org/posting-guide.html
>>>>>
>>>>
>>>>
>>>> --
>>>> Rich FitzJohn
>>>> rich.fitzjohn <at> gmail.com   |
>> http://homepages.paradise.net.nz/richa183
>>>>                      You are in a maze of twisty little
>> functions,
>>>> all alike
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>
>>> --
>>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>> University of Oxford,             Tel:  +44 1865 272861 (self)
>>> 1 South Parks Road,                     +44 1865 272866 (PA)
>>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From demiourgos at gmail.com  Thu May 12 19:35:27 2005
From: demiourgos at gmail.com (Alexander Sirotkin)
Date: Thu, 12 May 2005 20:35:27 +0300
Subject: [R] modifications to text.tree function
Message-ID: <c4357ccd050512103567c36053@mail.gmail.com>

Hi.

I have to make some minor modifications to the text.tree function - I
don't like the way it prints the split labels (they are too long in my
case and overlap). I tried to make s simple modification to the
text.tree function so that it will limit the number of significant
digits in tree labels, but could not - the original function uses some
undocumented "treeco" function, which I can not find.

Any ideas ? Thanks.
-- 
Alexander Sirotkin



From Pierre.Lapointe at nbf.ca  Thu May 12 19:38:33 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Thu, 12 May 2005 13:38:33 -0400
Subject: [R] Batch mode problem: figure margins too large
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D06B63169@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/a3aecf7a/attachment.pl

From Pierre.Lapointe at nbf.ca  Thu May 12 19:45:34 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Thu, 12 May 2005 13:45:34 -0400
Subject: [R] RE: Batch mode problem: figure margins too large (aligned R
 code to the left)
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D06B6316A@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/b3a0490c/attachment.pl

From stefaan.lhermitte at biw.kuleuven.be  Thu May 12 19:54:59 2005
From: stefaan.lhermitte at biw.kuleuven.be (Stefaan Lhermitte)
Date: Thu, 12 May 2005 19:54:59 +0200
Subject: [R] Maximize volume of cloud
Message-ID: <42839873.8060004@biw.kuleuven.be>

Dear R-ians,

I have a datamatrix X of nrows (large) and 3 colums. The 3 columns 
represent the coordinates of a cloud in a 3 dimensional space.
Now I want to find the extremes of my cloud by using the N-FINDR 
algoritm. This algorithm consists of  maximizing the volume between tree 
points. Therefore I need an optimaztion script, but I don't have any 
idea how to program it.

According to literature the maximization of the volume can be replaced 
by  maximizinng the determinant of E,
where:
E<-matrix(c(1,e1,1,e2,1,e3), ncol=3)
and e1 to e3 are initially column vectors of the coordinates of random 
points.

The procedure works by replacing an existing e1, e2 or e2 with another 
trial point. If the replacement results in an increase of 
determinant(E), the newly chosen pixel will replace the previous pixel. 
The procedure has to be repeated till there are no possible replacements 
left. In this way I get the coordinates of my extremes by e1 to e3.

Anyone any idea how to program this?

Kind regards and thanx in advance!
Stef



From jyzz88 at gmail.com  Thu May 12 20:00:07 2005
From: jyzz88 at gmail.com (Luke Zhou)
Date: Thu, 12 May 2005 14:00:07 -0400
Subject: [R] load data with scientific notations
Message-ID: <27583b40050512110020b43d87@mail.gmail.com>

Hi,

I have a data file whose values are scientific notations, for example:

9.1642537e+003, 6.7195295e+003, 4.8834487e+003, 3.7181589e+003, ...

How to load such data into R?

Thanks,

-Luke



From andy_liaw at merck.com  Thu May 12 20:07:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 12 May 2005 14:07:18 -0400
Subject: [R] load data with scientific notations
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E823@usctmx1106.merck.com>

Just read them like any other numbers; e.g.:

> x <- scan(sep=",")
1: 9.1642537e+003, 6.7195295e+003, 4.8834487e+003, 3.7181589e+003
5: [hit ctrl-d]
Read 4 items
> x
[1] 9164.254 6719.529 4883.449 3718.159

Andy


> From: Luke Zhou
> 
> Hi,
> 
> I have a data file whose values are scientific notations, for example:
> 
> 9.1642537e+003, 6.7195295e+003, 4.8834487e+003, 3.7181589e+003, ...
> 
> How to load such data into R?
> 
> Thanks,
> 
> -Luke
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ripley at stats.ox.ac.uk  Thu May 12 20:18:31 2005
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Thu, 12 May 2005 19:18:31 +0100 (BST)
Subject: [R] modifications to text.tree function
In-Reply-To: <c4357ccd050512103567c36053@mail.gmail.com>
Message-ID: <Pine.GSO.4.31.0505121916570.23626-100000@markov.stats>

Is this from package tree?  If so treeco is in the namespace but not
exported.

> library(tree)
> ls(asNamespace("tree"))
 [1] "cv.tree"            "data.tree"          "descendants"
 [4] "deviance.tree"      "labels.tree"        "misclass.tree"
 [7] "model.frame.tree"   "na.tree.replace"    "node.match"
[10] "partition.tree"     "plot.tree"          "plot.tree.sequence"
[13] "pred1.tree"         "predict.tree"       "print.summary.tree"
[16] "print.tree"         "prune.misclass"     "prune.tree"
[19] "residuals.tree"     "snip.tree"          "summary.tree"
[22] "text.tree"          "tile.tree"          "tree"
[25] "tree.control"       "tree.depth"         "tree.matrix"
[28] "tree.screens"       "treeco"             "treepl"

so try e.g. tree:::treeco.

On Thu, 12 May 2005, Alexander Sirotkin wrote:

> Hi.
>
> I have to make some minor modifications to the text.tree function - I
> don't like the way it prints the split labels (they are too long in my
> case and overlap). I tried to make s simple modification to the
> text.tree function so that it will limit the number of significant
> digits in tree labels, but could not - the original function uses some
> undocumented "treeco" function, which I can not find.
>
> Any ideas ? Thanks.
> --
> Alexander Sirotkin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu May 12 20:19:13 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 12 May 2005 20:19:13 +0200
Subject: [R] glob2rx()  {was: no bug in R2.1.0's list.files()}
In-Reply-To: <42832A77.9090406@lancaster.ac.uk>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
	<4283178F.9070504@statistik.uni-dortmund.de>
	<42831861.1070103@columbia.edu>
	<42831B79.6070308@statistik.uni-dortmund.de>
	<42832A77.9090406@lancaster.ac.uk>
Message-ID: <17027.40481.425142.466614@stat.math.ethz.ch>

>>>>> "BaRow" == Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>
>>>>>     on Thu, 12 May 2005 11:05:43 +0100 writes:

    BaRow> Uwe Ligges wrote:
    >> Please read about regular expressions (!!!) and try to
    >> understand that ".txt" also finds "Not_a_txt_file.xls"
    >> ....


    BaRow>   The confusion here is between regular expressions
    BaRow> and wildcard expansion known as 'globbing'. The two
    BaRow> things are very different, and use characters such as
    BaRow> '*' '.' and '?' in different ways.

Exactly,  I had devised  a  "glob" to "regexp" function many
years ago in order to help newbies make the transition.

That function, nowadays, called 'glob2rx' has been part of our
(CRAN) package "sfsmisc" and hence available to all via
 
       install.packages("sfsmisc")
       library("sfsmisc")

But it's quite simple (though not trivial to read for the
inexperienced because of the many escapes ("\") needed)
and it maybe helpful to see its code on R-help, below.
Then, this topic has lead me to add 2 (obvious in hindsight)
logical optional arguments to the function so that it now looks like

glob2rx <- function(pattern, trim.head = FALSE, trim.tail = TRUE)
{
    ## Purpose: Change "ls" aka "wildcard" aka "globbing" _pattern_ to
    ##	      Regular Expression (as in grep, perl, emacs, ...)
    ## -------------------------------------------------------------------------
    ## Author: Martin Maechler ETH Zurich, ~ 1991
    ##	       New version using [g]sub() : 2004
    p <- gsub('\\.','\\\\.', paste('^', pattern, '$', sep=''))
    p <- gsub('\\?',	 '.',  gsub('\\*',  '.*', p))
    ## these are trimming '.*$' and '^.*' - in most cases only for esthetics
    if(trim.tail) p <- sub("\\.\\*\\$$", '', p)
    if(trim.head) p <- sub("\\^\\.\\*",  '', p)
    p
}


So those confused newbies (and DOS long timers!)
could use

      list.files(myloc, glob2rx("*.zip"), full=TRUE)

            ## (yes, make a habit of using 'TRUE', not 'T' ..)

The current example code, BTW, has

    stopifnot(glob2rx("abc.*") == "^abc\\.",
               glob2rx("a?b.*") == "^a.b\\.",
               glob2rx("a?b.*", trim.tail=FALSE) == "^a.b\\..*$",
               glob2rx("*.doc") == "^.*\\.doc$",
               glob2rx("*.doc", trim.head=TRUE) == "\\.doc$",
               glob2rx("*.t*")  == "^.*\\.t",
               glob2rx("*.t??") == "^.*\\.t..$"
     )


Martin Maechler,
ETH Zurich


    BaRow>   There's added confusion when people come from a DOS
    BaRow> background, where commands did their own thing when
    BaRow> given '*' as parameter. The DOS command:

    BaRow>   RENAME *.FOO *.BAR

    BaRow>   did what seems obvious, renaming all the .FOO files
    BaRow> to .BAR, but on a unix machine doing this with 'mv'
    BaRow> can be destructive!

    BaRow>   In short (and slightly simplified), a '*' when
    BaRow> expanded as a wildcard in a glob matches any string,
    BaRow> whereas a '*' in a regular expression (regexp),
    BaRow> matches the previous character 0 or more times. This
    BaRow> is why "*.zip" is flagged as invalid now - there's no
    BaRow> character before the "*".

    BaRow>   That should be enough clues to send you on your
    BaRow> way.

    BaRow>   Baz



From wang at galton.uchicago.edu  Thu May 12 20:19:23 2005
From: wang at galton.uchicago.edu (Yong Wang)
Date: Thu, 12 May 2005 13:19:23 -0500 (CDT)
Subject: [R] a quick ? on using read.table() 
In-Reply-To: <200505121013.j4CAArZZ012452@hypatia.math.ethz.ch>
References: <200505121013.j4CAArZZ012452@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0505121318230.27526@aitken.uchicago.edu>

Dear all

I have a data set like following
***********************
name  ticker price	# how to specify attributes for differet colums

a     ta     1
a     ta     1
a     ta     1
name ticker price 	# the extra headline is a problem
a     ta     1
a     ta     1
b            2
b            2
b            2
b            2
b            2
c     tc     3
c     tc     3
name ticker price
c     tc     3
c     tc     3
c     tc     3
**************************
all columns are read in as "factor", if without specifying parameters in 
read.table()
so how can I use read.table() function and its parameter
to conveniently do following things in ONE action

if can not, answer for the first and second question is especially 
appreciated.
I did not figure  out how to use "as.is"

1) specify the each row with different addtribute, e.g., the first colum 
as"character"
second col as "factor", third one as "numeric"
2) remove extra headlines
3) remove rows with blanks

thank you
regards



From jyzz88 at gmail.com  Thu May 12 20:27:11 2005
From: jyzz88 at gmail.com (Luke Zhou)
Date: Thu, 12 May 2005 14:27:11 -0400
Subject: [R] load data with scientific notations
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E823@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E823@usctmx1106.merck.com>
Message-ID: <27583b4005051211274aa1122c@mail.gmail.com>

It works. thanks!

-Luke

On 5/12/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> Just read them like any other numbers; e.g.:
> 
> > x <- scan(sep=",")
> 1: 9.1642537e+003, 6.7195295e+003, 4.8834487e+003, 3.7181589e+003
> 5: [hit ctrl-d]
> Read 4 items
> > x
> [1] 9164.254 6719.529 4883.449 3718.159
> 
> Andy
> 
> > From: Luke Zhou
> >
> > Hi,
> >
> > I have a data file whose values are scientific notations, for example:
> >
> > 9.1642537e+003, 6.7195295e+003, 4.8834487e+003, 3.7181589e+003, ...
> >
> > How to load such data into R?
> >
> > Thanks,
> >
> > -Luke
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From ggrothendieck at gmail.com  Thu May 12 20:32:04 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 12 May 2005 14:32:04 -0400
Subject: [R] glob2rx() {was: no bug in R2.1.0's list.files()}
In-Reply-To: <17027.40481.425142.466614@stat.math.ethz.ch>
References: <42832222.21568.4137BD@fs1.ser.man.ac.uk>
	<4283178F.9070504@statistik.uni-dortmund.de>
	<42831861.1070103@columbia.edu>
	<42831B79.6070308@statistik.uni-dortmund.de>
	<42832A77.9090406@lancaster.ac.uk>
	<17027.40481.425142.466614@stat.math.ethz.ch>
Message-ID: <971536df0505121132153de0cc@mail.gmail.com>

I think glob2rx is of sufficient interest and sufficiently small
that it would be nice to have in the core of R without having to 
install and load sfsmisc.

On 5/12/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "BaRow" == Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>
> >>>>>     on Thu, 12 May 2005 11:05:43 +0100 writes:
> 
>    BaRow> Uwe Ligges wrote:
>    >> Please read about regular expressions (!!!) and try to
>    >> understand that ".txt" also finds "Not_a_txt_file.xls"
>    >> ....
> 
>    BaRow>   The confusion here is between regular expressions
>    BaRow> and wildcard expansion known as 'globbing'. The two
>    BaRow> things are very different, and use characters such as
>    BaRow> '*' '.' and '?' in different ways.
> 
> Exactly,  I had devised  a  "glob" to "regexp" function many
> years ago in order to help newbies make the transition.
> 
> That function, nowadays, called 'glob2rx' has been part of our
> (CRAN) package "sfsmisc" and hence available to all via
> 
>       install.packages("sfsmisc")
>       library("sfsmisc")
> 
> But it's quite simple (though not trivial to read for the
> inexperienced because of the many escapes ("\") needed)
> and it maybe helpful to see its code on R-help, below.
> Then, this topic has lead me to add 2 (obvious in hindsight)
> logical optional arguments to the function so that it now looks like
> 
> glob2rx <- function(pattern, trim.head = FALSE, trim.tail = TRUE)
> {
>    ## Purpose: Change "ls" aka "wildcard" aka "globbing" _pattern_ to
>    ##        Regular Expression (as in grep, perl, emacs, ...)
>    ## -------------------------------------------------------------------------
>    ## Author: Martin Maechler ETH Zurich, ~ 1991
>    ##         New version using [g]sub() : 2004
>    p <- gsub('\\.','\\\\.', paste('^', pattern, '$', sep=''))
>    p <- gsub('\\?',     '.',  gsub('\\*',  '.*', p))
>    ## these are trimming '.*$' and '^.*' - in most cases only for esthetics
>    if(trim.tail) p <- sub("\\.\\*\\$$", '', p)
>    if(trim.head) p <- sub("\\^\\.\\*",  '', p)
>    p
> }
> 
> So those confused newbies (and DOS long timers!)
> could use
> 
>      list.files(myloc, glob2rx("*.zip"), full=TRUE)
> 
>            ## (yes, make a habit of using 'TRUE', not 'T' ..)
> 
> The current example code, BTW, has
> 
>    stopifnot(glob2rx("abc.*") == "^abc\\.",
>               glob2rx("a?b.*") == "^a.b\\.",
>               glob2rx("a?b.*", trim.tail=FALSE) == "^a.b\\..*$",
>               glob2rx("*.doc") == "^.*\\.doc$",
>               glob2rx("*.doc", trim.head=TRUE) == "\\.doc$",
>               glob2rx("*.t*")  == "^.*\\.t",
>               glob2rx("*.t??") == "^.*\\.t..$"
>     )
> 
> Martin Maechler,
> ETH Zurich
> 
>    BaRow>   There's added confusion when people come from a DOS
>    BaRow> background, where commands did their own thing when
>    BaRow> given '*' as parameter. The DOS command:
> 
>    BaRow>   RENAME *.FOO *.BAR
> 
>    BaRow>   did what seems obvious, renaming all the .FOO files
>    BaRow> to .BAR, but on a unix machine doing this with 'mv'
>    BaRow> can be destructive!
> 
>    BaRow>   In short (and slightly simplified), a '*' when
>    BaRow> expanded as a wildcard in a glob matches any string,
>    BaRow> whereas a '*' in a regular expression (regexp),
>    BaRow> matches the previous character 0 or more times. This
>    BaRow> is why "*.zip" is flagged as invalid now - there's no
>    BaRow> character before the "*".
> 
>    BaRow>   That should be enough clues to send you on your
>    BaRow> way.
> 
>    BaRow>   Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From stephen.welsh at pipeline.ouc.bc.ca  Thu May 12 20:30:26 2005
From: stephen.welsh at pipeline.ouc.bc.ca (Stephen Robert Welsh)
Date: Thu, 12 May 2005 11:30:26 -0700 (PDT)
Subject: [R] Testing Plot e-mail
Message-ID: <25757615.1115922626588.JavaMail.cpadmin@pipeline.ouc.bc.ca>



From Pierre.Lapointe at nbf.ca  Thu May 12 21:14:06 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Thu, 12 May 2005 15:14:06 -0400
Subject: [R] Batch mode problem: figure margins too large (code corrected
 for word wrap)
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D06B6316D@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050512/51e8113d/attachment.pl

From dacostaj at unlv.nevada.edu  Thu May 12 21:36:03 2005
From: dacostaj at unlv.nevada.edu (Jeffrey DaCosta)
Date: Thu, 12 May 2005 12:36:03 -0700
Subject: [R] Multidimensional Scaling with pairwise Fst
Message-ID: <138FA8C1-C31D-11D9-A7B6-000A957B212E@unlv.nevada.edu>

I want to create a MDS plot with pairwise Fst values derived from a 
population genetics project.  My Fst values are in a tab-delimited file 
(lower triangle only) that I view with Excel.  When I use the cmdscale 
command I get the message:
Error in cmdscale(x) : Distances must be result of dist or a square 
matrix
In addition: Warning messages:
1: "^" not meaningful for factors in: Ops.factor(left, right)
2: "^" not meaningful for factors in: Ops.factor(left, right)
3: "^" not meaningful for factors in: Ops.factor(left, right)
4: "^" not meaningful for factors in: Ops.factor(left, right)

I transformed my values using the dist command, but the resulting plot 
is not right.

My file of Fst values are the estimates of distance that I want to 
directly use in the MDS analysis.  Looking for help in making my Fst 
file read in R as a distance matrix for this to work.
-Jeff DaCosta
dacostaj at unlv.nevada.edu



From ripley at stats.ox.ac.uk  Thu May 12 22:00:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 May 2005 21:00:08 +0100 (BST)
Subject: [R] Batch mode problem: figure margins too large (code corrected
	for word wrap)
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D06B6316D@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D06B6316D@lbmsg002.fbn-nbf.local>
Message-ID: <Pine.LNX.4.61.0505122052480.31294@gannet.stats>

Thank you, the word wrap did make it impossible.

For me

pdf(file="c:/CFTC.pdf",height=10,width=8,paper="letter")
## plotting
dev.off()

works in interactive mode or from a file.  I don't understand why you are 
going via dev.print(), nor what exactly what is different about your 
invocation of `batch mode', as putting your commands in file test.R and 
doing

R CMD BATCH test.R
R CMD BATCH --internet2 test.R

also work.  So although I have not nailed the problem I hope I have 
provided some workarounds.

On Thu, 12 May 2005, Lapointe, Pierre wrote:

> First of all, I apologize for the triple post, but I did not see that the
> word wrap on the r-help list site would render my code unusable. So here it
> is again.  Hoping that this time it will work if you cut and paste it in
> your Rgui.
>
> I have a program that works well in Rgui but that does not work in a batch
> file (Execution halted).
>
> Here's the code (it will work on your R, but you need internet access
> through R as it will download a table from a US gov site).
>
> #Code Start
> download.file("http://www.cftc.gov/files/dea/history/deacot2005.zip",
> "c:/deacot2005.zip", "internal", quiet = FALSE, mode = "wb",cacheOK = TRUE)
> unzipped<-zip.file.extract("c:/annual.txt", zipname = "deacot2005.zip")
> #Unzip
> data <-read.table(unzipped, header=TRUE, sep=',')
> windows(height=11,width=8.5)
> z <- layout(matrix(c(1:8), 4,2, byrow = TRUE))
> for (i in 1:8) {  #loop to fill layout
> tbonds<-data[data$'Market.and.Exchange.Names'==
> "U.S. TREASURY BONDS - CHICAGO BOARD OF TRADE ",] #choose only T-Bonds
> ordered<-tbonds[order(tbonds[,3]),]    #order by date
> longb<-ordered[,c(3,9)]
> longb[,1]<-as.Date(longb[,1],format="%Y%m%d") #to transform to date format
> par(mar=c(2.5, 3.5, 2, 4.5)) #need this par for mtext
> plot(longb,type="l",las=1,xlab="",ylab="",bty="l",cex.axis=0.9)
> end<-(longb)[nrow(ordered),]  #mtext text
> end0<-(longb)[nrow(ordered)-1,]   #mtext text
> now<- paste(' Last:',end[,2],"\n",'Prev:',end0[,2])    #mtext text
> mtext(now,side=4, las=2,adj=0,padj=0.5,cex=0.5) #mtext text
> }
> dev.print(pdf, file="c:/CFTC.pdf",height=10,width=8,paper="letter")
> #Code end
>
> As you can see, I need a special par(mar) to make room for the margin text
> (mtext).
>
> When I run this program through a batch file I get this message :
>
> ...
> + longb<-ordered[,c(3,9)]
> + longb[,1]<-as.Date(longb[,1],format="%Y%m%d") #to transform to date format
> + par(mar=c(2.5, 3.5, 2, 4.5)) #need this par for mtext
> + plot(longb,type="l",las=1,xlab="",ylab="",bty="l",cex.axis=0.9)
> + end<-(longb)[nrow(ordered),]  #mtext text
> + end0<-(longb)[nrow(ordered)-1,]   #mtext text
> + now<- paste(' Last:',end[,2],"\n",'Prev:',end0[,2])    #mtext text
> + mtext(now,side=4, las=2,adj=0,padj=0.5,cex=0.5) #mtext text
> + }
> Error in plot.new() : figure margins too large
> Execution halted
>
> I tried to remove the windows(height=11,width=8.5) as I don't need it in
> batch mode, but it prevented the PDF creation as it  " can only print from
> screen device".
>
> dev.print(pdf, file="c:/CFTC.pdf",height=10,width=8,paper="letter")
> Error in dev.print(pdf, file = "c:/CFTC.pdf", height = 10, width = 8,  :
> 	can only print from screen device
> Execution halted
>
> Here's what I have in my windows batch file
>
> Set TZ=GMT
> PATH C:\Program Files\R\rw2010\bin
> Rterm --internet2 --no-restore --no-save <problem.R> result.txt 2>&1
>
> I'm on win2k,
>
>> version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status   Patched
> major    2
> minor    1.0
> year     2005
> month    05
> day      09
> language R
>
> Be nice with me.  I've been a R user for only 2 weeks.
>
> Regards,
>
> Pierre Lapointe
> Assistant Market Strategist
> National Bank Financial
>
>
>
>
> ***********************************************************************************
> AVIS DE NON-RESPONSABILITE:
> Ce document transmis par courrier electronique est destine uniquement a la personne ou a l'entite a qui il est adresse et peut contenir des
> renseignements confidentiels et assujettis au secret professionnel. La
> confidentialite et le secret professionnel demeurent malgre l'envoi de ce
> document a la mauvaise adresse electronique. Si vous n'etes pas le
> destinataire vise ou la personne chargee de remettre ce document a son destinataire, veuillez nous en informer sans delai et detruire ce document ainsi que toute copie qui en aurait ete faite.Toute distribution, reproduction ou autre utilisation de ce document est
> strictement interdite. De plus, le Groupe Financiere Banque Nationale et ses filiales ne peuvent pas etre tenus responsables des dommages pouvant etre causes par des virus ou des erreurs de transmission.
>
> DISCLAIMER:\ This documentation transmitted by electronic ma...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.harrell at vanderbilt.edu  Thu May 12 21:50:36 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 12 May 2005 14:50:36 -0500
Subject: [R] Standardized logistic regression coefficients
In-Reply-To: <000201c556fa$e70f75d0$6603a8c0@ltroth>
References: <000201c556fa$e70f75d0$6603a8c0@ltroth>
Message-ID: <4283B38C.9030404@vanderbilt.edu>

Alexander Roth wrote:
> Hi everyone,
> 
> how can I calculate standardized logistic regression coefficients using
> R? I used "glm" resp. "lrm" from the design-package in order to
> calculate logistic regression coefficients but I'm wondering if there's
> a possibility to get standardized logistic regression coefficients?
> 
> Thanks in advance!
> 
> Alexander Roth

Design doesn't implement those because they have terrible properties. 
Instead consider interquartile-range odds ratios (done by summary.Design 
by typing summary(. . .)).

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From martin at metahuman.org  Thu May 12 22:40:34 2005
From: martin at metahuman.org (Martin C. Martin)
Date: Thu, 12 May 2005 16:40:34 -0400
Subject: [R] Converting a number of minutes to a difftime
Message-ID: <4283BF42.7060909@metahuman.org>

Hi,

I have a variable "m" that contains the number of minutes that something 
lasted, e.g.

m <- 139

I'd like to convert it to a difftime, so I can add it to the POSIXct 
start time.  But as.difftime seems to want a character string, with at 
most two characters for the minutes.  All other conversion functions 
seem to take strings as well.  What am I missing?

Thanks,
Martin



From ripley at stats.ox.ac.uk  Thu May 12 23:11:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 May 2005 22:11:27 +0100 (BST)
Subject: [R] time zones, daylight saving etc.
In-Reply-To: <Pine.LNX.4.61.0505121824420.10597@gannet.stats>
References: <C698D707214E6F4AB39AB7096C3DE5A59E951A@phost015.EVAFUNDS.intermedia.net>
	<Pine.LNX.4.61.0505121824420.10597@gannet.stats>
Message-ID: <Pine.LNX.4.61.0505122209330.32457@gannet.stats>

On Thu, 12 May 2005, Prof Brian Ripley wrote:

> Yes, I did mention the OS was unstated, knowing that some had problems. I 
> will try to investigate why this is not working on Windows for a future 
> release.

For the record, this is a POSIX-compliance failure.  The POSIX standard 
says

   Local timezone information is used as though localtime() calls tzset()

but Windows requires tzset() to be called explicitly.

I've added a workaround in R-patched.

>
> For Windows users something like
>
> .../RGui.exe TZ=GMT
>
> should do it.
>
> On Thu, 12 May 2005, Vadim Ogranovich wrote:
>
>> Works for me on Linux:
>>> Sys.time()
>> [1] "2005-05-12 10:22:31 PDT"
>>> Sys.putenv(TZ="GMT")
>>> Sys.time()
>> [1] "2005-05-12 17:22:37 GMT"
>> 
>> I extensively use the reset of TZ to parse times.
>> 
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor
>>> Grothendieck
>>> Sent: Thursday, May 12, 2005 6:18 AM
>>> To: Prof Brian Ripley
>>> Cc: Carla Meurk; r-help at stat.math.ethz.ch
>>> Subject: Re: [R] time zones, daylight saving etc.
>>> 
>>> I have tried this but on Windows XP R 2.1.0 found I had to
>>> set it outside of R prior to starting R.
>>> 
>>> 1. unsuccessful
>>> 
>>>> Sys.time()
>>> [1] "2005-05-12 09:08:03 Eastern Daylight Time"
>>>> Sys.putenv(TZ="GMT")
>>>> Sys.time() # no change
>>> [1] "2005-05-12 09:08:12 Eastern Daylight Time"
>>> 
>>> 2. OK
>>> 
>>> C:\>set tz=GMT
>>> 
>>> C:\>start "" "\Program Files\R\rw2010\bin\r.exe"
>>> 
>>> R : Copyright 2005, The R Foundation for Statistical
>>> Computing Version 2.1.0 Patched (2005-04-18), ISBN 3-900051-07-0
>>> 
>>> R is free software and comes with ABSOLUTELY NO WARRANTY.
>>> You are welcome to redistribute it under certain conditions.
>>> Type 'license()' or 'licence()' for distribution details.
>>> 
>>>   Natural language support but running in an English locale
>>> 
>>> R is a collaborative project with many contributors.
>>> Type 'contributors()' for more information and 'citation()'
>>> on how to cite R or R packages in publications.
>>> 
>>> Type 'demo()' for some demos, 'help()' for on-line help, or
>>> 'help.start()' for a HTML browser interface to help.
>>> Type 'q()' to quit R.
>>> 
>>>> Sys.time()
>>> [1] "2005-05-12 13:10:58 GMT"
>>> 
>>> I assume it could be set in .Renviron but it would be nice if
>>> one could set it right from within R so that one can write a
>>> function that sets it, does processing and then sets it back.
>>>  Don't know if this is possible.
>>> 
>>> On 5/12/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>>>> Would it not just be easier to set the timezone to GMT for the
>>>> duration of the calculations?  I don't see an OS mentioned
>>> here, but
>>>> on most TZ=GMT for the session will do it.
>>>> 
>>>> On Thu, 12 May 2005, Rich FitzJohn wrote:
>>>> 
>>>>> Hi,
>>>>> 
>>>>> seq.dates() in the chron package does not allow creating
>>> sequences
>>>>> by minutes, so you'd have to roll your own sequence generator.
>>>>> 
>>>>> Looks like the tzone attribute of the times is lost when using
>>>>> min(),
>>>>> max() and seq().  You can apply it back manually, but it does not
>>>>> affect the calculation, since POSIXct times are stored as seconds
>>>>> since 1/1/1970 (?DateTimeClasses).
>>>>> 
>>>>> ## These dates/times just span the move from NZDT to NZST:
>>>>> dt.dates <- paste(rep(15:16, c(5,7)), "03", "2003", sep="/")
>>>>> dt.times <- paste(c(19:23, 0:6), "05", sep=":") dt <-
>>>>> paste(dt.dates, dt.times)
>>>>> 
>>>>> ## No shift in times, or worrying about daylight savings;
>>>>> appropriate ## iff the device doing the recording was not itself
>>>>> adjusting for ## daylight savings, presumably.
>>>>> datetime <- as.POSIXct(strptime(dt, "%d/%m/%Y %H:%M"), "GMT")
>>>>> 
>>>>> ## Create two objects with all the times in your range,
>>> one with the
>>>>> ## tzone attribute set back to GMT (to match datetimes),
>>> and one ##
>>>>> without this.
>>>>> mindata1 <- mindata2 <- seq(from=min(datetime), to=max(datetime),
>>>>>                            by="mins") attr(mindata2, "tzone") <-
>>>>> "GMT"
>>>>> 
>>>>> fmt <- "%Y %m %d %H %M"
>>>>> ## These both do the matching correctly:
>>>>> match(format(datetime, fmt), format(mindata1, fmt, tz="GMT"))
>>>>> match(format(datetime, fmt), format(mindata2, fmt, tz="GMT"))
>>>>> 
>>>>> ## However, the first of these will not, as it gets the
>>> timezone all
>>>>> ## wrong, since it's neither specified in the call to
>>> format(), or
>>>>> as ## an attribute of the POSIXct object.
>>>>> match(format(datetime, fmt), format(mindata1, fmt))
>>>>> match(format(datetime, fmt), format(mindata2, fmt))
>>>>> 
>>>>> ## It is also possible to run match() directly off the POSIXct
>>>>> object, ## but I'm not sure how this will interact with
>>> things like
>>>>> leap ## seconds:
>>>>> match(datetime, mindata1)
>>>>> 
>>>>> Time zones do my head in, so you probably want to check this all
>>>>> pretty carefully.  Looks like there's lots of gotchas (e.g.
>>>>> subsetting a POSIXct object strips the tzone attribute).
>>>>> 
>>>>> Cheers,
>>>>> Rich
>>>>> 
>>>>> On 5/12/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>>>>> You could use the chron package.  It represents date
>>> times without
>>>>>> using time zones so you can't have this sort of problem.
>>>>>> 
>>>>>> On 5/10/05, Carla Meurk <ksm32 at student.canterbury.ac.nz> wrote:
>>>>>>> Hi,  I have a whole bunch of data, which looks like:
>>>>>>> 
>>>>>>> 15/03/2003       10:20  1
>>>>>>> 15/03/2003       10:21  0
>>>>>>> 15/03/2003       12:02  0
>>>>>>> 16/03/2003       06:10  0
>>>>>>> 16/03/2003       06:20  0.5
>>>>>>> 16/03/2003       06:30  0
>>>>>>> 16/03/2003       06:40  0
>>>>>>> 16/03/2003       06:50  0
>>>>>>> 
>>>>>>> 18/03/2003  20:10                 0.5
>>>>>>> etc. (times given on a 24 hour clock)
>>>>>>> 
>>>>>>> and goes on for years.  I have some code:
>>>>>>> 
>>>>>>> data<-read.table("H:/rainfall_data.txt",h=T)
>>>>>>> library(survival)
>>>>>>> datetime <- as.POSIXct(strptime(paste(data$V1,
>>> data$V2), "%d/%m/%Y
>>>>>>> %H:%M"), tz="NZST")
>>>>>>> 
>>>>>>> which produces:
>>>>>>> 
>>>>>>> [10] "2003-03-13 21:13:00 New Zealand Daylight Time"
>>>>>>> [11] "2003-03-15 13:20:00 New Zealand Daylight Time"
>>>>>>> [12] "2003-03-15 22:20:00 New Zealand Daylight Time"
>>>>>>> [13] "2003-03-15 22:21:00 New Zealand Daylight Time"
>>>>>>> [14] "2003-03-16 00:02:00 New Zealand Daylight Time"
>>>>>>> [15] "2003-03-16 18:10:00 New Zealand Standard Time"
>>>>>>> [16] "2003-03-16 18:20:00 New Zealand Standard Time"
>>>>>>> [17] "2003-03-16 18:30:00 New Zealand Standard Time"
>>>>>>> 
>>>>>>> My problem is that "15/03/2003 12:02" has become
>>> "16/03/2003 00:02"
>>>>>>> i.e.  it is 12 hours behind (as is everything else),
>>> but also, I
>>>>>>> do not want to change time zones.
>>>>>>> 
>>>>>>> The 12 hour delay is not really a problem just an
>>> annoyance, but
>>>>>>> the time zone change is a problem because later on I
>>> need to match
>>>>>>> up data by time using
>>>>>>> 
>>>>>>> mindata<-seq(from=min(datetime),to=max(datetime),by="mins")
>>>>>>> newdata<-matrix(0,length(mindata),1)
>>>>>>> newdata[match(format.POSIXct(datetime,"%Y %m %d %H
>>>>>>> %M"),format.POSIXct(mindata,"%Y %m %d %H %M"))]<-data$V3
>>>>>>> 
>>>>>>> and things go wrong here with matching repeating times/missing
>>>>>>> times around the timezone changes and, my resulting vector is 1
>>>>>>> hour shorter than my other series.  From the R help I
>>> see that my
>>>>>>> OS may be to blame but, even if I specify tz="GMT" I still get
>>>>>>> NZST and NZDT.  Can someone help?
>>>>>>> 
>>>>>>> I hope this all makes sense
>>>>>>> 
>>>>>>> Carla
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide!
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> 
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide!
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> 
>>>>> 
>>>>> 
>>>>> --
>>>>> Rich FitzJohn
>>>>> rich.fitzjohn <at> gmail.com   |
>>> http://homepages.paradise.net.nz/richa183
>>>>>                      You are in a maze of twisty little
>>> functions,
>>>>> all alike
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide!
>>>>> http://www.R-project.org/posting-guide.html
>>>>> 
>>>> 
>>>> --
>>>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>>> University of Oxford,             Tel:  +44 1865 272861 (self)
>>>> 1 South Parks Road,                     +44 1865 272866 (PA)
>>>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>>> 
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>> 
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>> 
>> 
>> 
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu May 12 23:15:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 May 2005 22:15:18 +0100 (BST)
Subject: [R] Converting a number of minutes to a difftime
In-Reply-To: <4283BF42.7060909@metahuman.org>
References: <4283BF42.7060909@metahuman.org>
Message-ID: <Pine.LNX.4.61.0505122211400.32457@gannet.stats>

On Thu, 12 May 2005, Martin C. Martin wrote:

> I have a variable "m" that contains the number of minutes that something 
> lasted, e.g.
>
> m <- 139
>
> I'd like to convert it to a difftime, so I can add it to the POSIXct start 
> time.  But as.difftime seems to want a character string, with at most two 
> characters for the minutes.  All other conversion functions seem to take 
> strings as well.  What am I missing?

as.difftime() is quite limited.  You need to get your hands dirty:

    structure(m, units = "mins", class = "difftime")

will do the job.

Alternatively,  Sys.time + (m*60) works.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kevin.thorpe at utoronto.ca  Fri May 13 01:03:20 2005
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Thu, 12 May 2005 19:03:20 -0400
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <B998A44C8986644EA8029CFE6396A9241B31CF@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A9241B31CF@exqld2-bne.qld.csiro.au>
Message-ID: <4283E0B8.6000004@utoronto.ca>

This thread was very timely for me since I will be teaching an introductory
biostats course in the fall, including a session on CLT. I have shamelessly
taken Dr. Venables' slick piece of code and wrapped it in a function so that
I can vary the maximum sample size at will. I then created functions based
on the first to sample from the exponential and the chi-squaredistributions.
Lastly, I created a function to sample from a pdf with a parabolic shape
(confirming in the process just how rusty my calculus is :-) ). My code is
below for all to do with as they please.

Now, at the risk of asking a really obvious question ...

In my final function, I used the inverse probability integral transformation
to sample from my parabolic distribution. I create a local function rparab
shown here:

rparab <- function(nn) {
u <- 2*runif(nn) - 1
ifelse(u<0,-(abs(u)^(1/3)),u^(1/3))
}

It seems that in my version of R (2.0.1) on Linux, that calculating the cube
root of a negative number using ^(1/3) returns NaN. I looked at the help in
the arithmetic operators and did help.search("cube root"), 
help.search("root")
and help.search("cube") and recognised no alternatives. So I used an 
ifelse() to
deal with the negatives. Have I missed something really elementary?

Thanks

clt.unif <- function(n=20) {
N <- 10000
graphics.off()
par(mfrow = c(1,2), pty = "s")
for(k in 1:n) {
m <- (rowMeans(matrix(runif(N*k), N, k)) - 0.5)*sqrt(12*k)
hist(m, breaks = "FD", xlim = c(-4,4),
main = paste("Uniform(0,1), n = ",k,sep=""),
prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
pu <- par("usr")[1:2]
x <- seq(pu[1], pu[2], len = 500)
lines(x, dnorm(x), col = "red")
qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
abline(0, 1, col = "red")
Sys.sleep(1)
}
}

clt.exp <- function(n=20,theta=1) {
N <- 10000
graphics.off()
par(mfrow = c(1,2), pty = "s")
for(k in 1:n) {
m <- (rowMeans(matrix(rexp(N*k,1/theta), N, k)) - theta)/sqrt(theta^2/k)
hist(m, breaks = "FD", xlim = c(-4,4),
main = paste("exp(",theta,"), n = ",k,sep=""),
prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
pu <- par("usr")[1:2]
x <- seq(pu[1], pu[2], len = 500)
lines(x, dnorm(x), col = "red")
qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
abline(0, 1, col = "red")
Sys.sleep(1)
}
}

clt.chisq <- function(n=20,nu=1) {
N <- 10000
graphics.off()
par(mfrow = c(1,2), pty = "s")
for(k in 1:n) {
m <- (rowMeans(matrix(rchisq(N*k,nu), N, k)) - nu)/sqrt(2*nu/k)
hist(m, breaks = "FD", xlim = c(-4,4),
main = paste("Chi-Square(",nu,"), n = ",k,sep=""),
prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
pu <- par("usr")[1:2]
x <- seq(pu[1], pu[2], len = 500)
lines(x, dnorm(x), col = "red")
qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
abline(0, 1, col = "red")
Sys.sleep(1)
}
}

clt.parab <- function(n=20) {
rparab <- function(nn) {
u <- 2*runif(nn) - 1
ifelse(u<0,-(abs(u)^(1/3)),u^(1/3))
}
N <- 10000
graphics.off()
par(mfrow = c(1,2), pty = "s")
for(k in 1:n) {
m <- (rowMeans(matrix(rparab(N*k), N, k)))/sqrt(3/(5*k))
hist(m, breaks = "FD", xlim = c(-4,4),
main = paste("n = ",k,sep=""),
prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
pu <- par("usr")[1:2]
x <- seq(pu[1], pu[2], len = 500)
lines(x, dnorm(x), col = "red")
qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
abline(0, 1, col = "red")
Sys.sleep(1)
}
}

Bill.Venables at csiro.au wrote:

>Here's a bit of a refinement on Ted's first suggestion.
>
> 
> N <- 10000
> graphics.off()
> par(mfrow = c(1,2), pty = "s")
> for(k in 1:20) {
>    m <- (rowMeans(matrix(runif(M*k), N, k)) - 0.5)*sqrt(12*k)
>    hist(m, breaks = "FD", xlim = c(-4,4), main = k,
>            prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
>    pu <- par("usr")[1:2]
>    x <- seq(pu[1], pu[2], len = 500)
>    lines(x, dnorm(x), col = "red")
>    qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
>    abline(0, 1, col = "red")
>    Sys.sleep(1)
>  }
>  
>


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462



From asundare at ucsd.edu  Fri May 13 01:22:42 2005
From: asundare at ucsd.edu (Ambika Sundaresan)
Date: Thu, 12 May 2005 16:22:42 -0700
Subject: [R] R make error
Message-ID: <4283E542.3080604@ucsd.edu>

Hi:

I tried to install the latest version of R (2-1.0)in an AIX machine .  I was able to tar, and configure with no
errors.  However, when I said make, I got the following errors:


     gcc -Wl,-bM:SRE -Wl,-H512 -Wl,-T512 -Wl,-bnoentry -Wl,-bexpall
-Wl,-bI:../../../etc/R.exp -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo
rotated.lo rbitmap.lo -lSM -lICE -lX11
ld: 0711-317 ERROR: Undefined symbol: .log10
ld: 0711-317 ERROR: Undefined symbol: .floor
ld: 0711-317 ERROR: Undefined symbol: .libintl_gettext
ld: 0711-317 ERROR: Undefined symbol: .pow
ld: 0711-317 ERROR: Undefined symbol: .sin
ld: 0711-317 ERROR: Undefined symbol: .cos
ld: 0711-317 ERROR: Undefined symbol: .tan
ld: 0711-345 Use the -bloadmap or -bnoquiet option to obtain more information.
collect2: ld returned 8 exit status
make: 1254-004 The error code from the last command is 1.
 
 
Stop.
make: 1254-004 The error code from the last command is 2.
 
 
I understand I have to edit the Makefile. Can someone please tell me which Makefile (folder?) and which flags?

I greatly appreciate your help.  Thanks.

Ambika Sundaresan
Graduate Student 
UCSD



From p.dalgaard at biostat.ku.dk  Fri May 13 01:24:54 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 May 2005 01:24:54 +0200
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <4283E0B8.6000004@utoronto.ca>
References: <B998A44C8986644EA8029CFE6396A9241B31CF@exqld2-bne.qld.csiro.au>
	<4283E0B8.6000004@utoronto.ca>
Message-ID: <x2ll6km3i1.fsf@turmalin.kubism.ku.dk>

"Kevin E. Thorpe" <kevin.thorpe at utoronto.ca> writes:

> rparab <- function(nn) {
> u <- 2*runif(nn) - 1
> ifelse(u<0,-(abs(u)^(1/3)),u^(1/3))
> }
> 
> It seems that in my version of R (2.0.1) on Linux, that calculating the cube
> root of a negative number using ^(1/3) returns NaN. I looked at the help in
> the arithmetic operators and did help.search("cube root"),
> help.search("root")
> and help.search("cube") and recognised no alternatives. So I used an
> ifelse() to
> deal with the negatives. Have I missed something really elementary?

Not really. You might have used u <- runif(nn,-1,1) and
sign(u)*abs(u)^(1/3) instead of the ifelse construct (remember that
ifelse generally evaluates both the 'yes' and 'no' parts and on some
architectures the NaN results may be slow to compute).


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jpablo.romero at gmail.com  Fri May 13 02:46:01 2005
From: jpablo.romero at gmail.com (Juan Pablo Romero)
Date: Thu, 12 May 2005 19:46:01 -0500
Subject: [R] Converting a S-plus file.
Message-ID: <e6507ac70505121746715b6d18@mail.gmail.com>

Hello all.

I've got  a little file which is from S-Plus. The problem is I don't
own a copy of said program, and R won't read it.

I wonder if there's a caritative soul here who could export the file
to some other format (csv, or even excel!) and send it back to me.

Thanks in advance.

   Juan Pablo

From gerifalte28 at hotmail.com  Fri May 13 04:07:52 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 13 May 2005 02:07:52 +0000
Subject: [R] Converting a S-plus file.
In-Reply-To: <e6507ac70505121746715b6d18@mail.gmail.com>
Message-ID: <BAY103-F94757286B09F2C1CE23FAA6120@phx.gbl>

Hi Juan Pablo

Take a look at the document under help->Manuals in pdf->R data 
import/export.  There is a whole section there on how to import data from 
other statistical software.

If you have a script file from S-Plus you can open it in any text editor 
(i.e. notepad)

Å°Suerte!

Francisco


>From: Juan Pablo Romero <jpablo.romero at gmail.com>
>Reply-To: Juan Pablo Romero <jpablo.romero at gmail.com>
>To: R-help at stat.math.ethz.ch
>Subject: [R] Converting a S-plus file.
>Date: Thu, 12 May 2005 19:46:01 -0500
>
>Hello all.
>
>I've got  a little file which is from S-Plus. The problem is I don't
>own a copy of said program, and R won't read it.
>
>I wonder if there's a caritative soul here who could export the file
>to some other format (csv, or even excel!) and send it back to me.
>
>Thanks in advance.
>
>    Juan Pablo
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri May 13 08:30:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 13 May 2005 08:30:25 +0200
Subject: [R] Batch mode problem: figure margins too large (code corrected
	for word wrap)
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D06B6316D@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D06B6316D@lbmsg002.fbn-nbf.local>
Message-ID: <42844981.4050309@statistik.uni-dortmund.de>

I'd like to recommend to open the pdf() device (and to plot in) 
directly. There is no reason to use the windows() device in BATCH mode, 
if you want to produce PDF.


Lapointe, Pierre wrote:
> First of all, I apologize for the triple post, but I did not see that the
> word wrap on the r-help list site would render my code unusable. So here it
> is again.  Hoping that this time it will work if you cut and paste it in
> your Rgui.
> 
> I have a program that works well in Rgui but that does not work in a batch
> file (Execution halted).
> 
> Here's the code (it will work on your R, but you need internet access
> through R as it will download a table from a US gov site).
> 
> #Code Start
> download.file("http://www.cftc.gov/files/dea/history/deacot2005.zip",
> "c:/deacot2005.zip", "internal", quiet = FALSE, mode = "wb",cacheOK = TRUE)
> unzipped<-zip.file.extract("c:/annual.txt", zipname = "deacot2005.zip")
> #Unzip
> data <-read.table(unzipped, header=TRUE, sep=',')
> windows(height=11,width=8.5)

delete windows() and insert:

pdf(file="c:/CFTC.pdf",height=10,width=8,paper="letter")


> z <- layout(matrix(c(1:8), 4,2, byrow = TRUE))
> for (i in 1:8) {  #loop to fill layout
> tbonds<-data[data$'Market.and.Exchange.Names'==
> "U.S. TREASURY BONDS - CHICAGO BOARD OF TRADE ",] #choose only T-Bonds
> ordered<-tbonds[order(tbonds[,3]),]    #order by date
> longb<-ordered[,c(3,9)]
> longb[,1]<-as.Date(longb[,1],format="%Y%m%d") #to transform to date format
> par(mar=c(2.5, 3.5, 2, 4.5)) #need this par for mtext
> plot(longb,type="l",las=1,xlab="",ylab="",bty="l",cex.axis=0.9)
> end<-(longb)[nrow(ordered),]  #mtext text
> end0<-(longb)[nrow(ordered)-1,]   #mtext text
> now<- paste(' Last:',end[,2],"\n",'Prev:',end0[,2])    #mtext text
> mtext(now,side=4, las=2,adj=0,padj=0.5,cex=0.5) #mtext text
> }
> dev.print(pdf, file="c:/CFTC.pdf",height=10,width=8,paper="letter")
> #Code end


delete dev.print and insert dev.off()

[BTW: your code is almost unreadable, please try to format it before 
posting, and please read documentation on devices]

Uwe Ligges



> As you can see, I need a special par(mar) to make room for the margin text
> (mtext).
> 
> When I run this program through a batch file I get this message :
> 
> ...
> + longb<-ordered[,c(3,9)]
> + longb[,1]<-as.Date(longb[,1],format="%Y%m%d") #to transform to date format
> + par(mar=c(2.5, 3.5, 2, 4.5)) #need this par for mtext
> + plot(longb,type="l",las=1,xlab="",ylab="",bty="l",cex.axis=0.9)
> + end<-(longb)[nrow(ordered),]  #mtext text
> + end0<-(longb)[nrow(ordered)-1,]   #mtext text
> + now<- paste(' Last:',end[,2],"\n",'Prev:',end0[,2])    #mtext text
> + mtext(now,side=4, las=2,adj=0,padj=0.5,cex=0.5) #mtext text
> + }
> Error in plot.new() : figure margins too large
> Execution halted
> 
> I tried to remove the windows(height=11,width=8.5) as I don't need it in
> batch mode, but it prevented the PDF creation as it  " can only print from
> screen device".
> 
> dev.print(pdf, file="c:/CFTC.pdf",height=10,width=8,paper="letter")
> Error in dev.print(pdf, file = "c:/CFTC.pdf", height = 10, width = 8,  : 
> 	can only print from screen device
> Execution halted
> 
> Here's what I have in my windows batch file
> 
> Set TZ=GMT
> PATH C:\Program Files\R\rw2010\bin
> Rterm --internet2 --no-restore --no-save <problem.R> result.txt 2>&1
> 
> I'm on win2k,
> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status   Patched        
> major    2              
> minor    1.0            
> year     2005           
> month    05             
> day      09             
> language R    
> 
> Be nice with me.  I've been a R user for only 2 weeks.
> 
> Regards,
> 
> Pierre Lapointe
> Assistant Market Strategist
> National Bank Financial
> 
> 
> 
> 
> *********************************************************************************** 
> AVIS DE NON-RESPONSABILITE: 
> Ce document transmis par courrier electronique est destine uniquement a la personne ou a l'entite a qui il est adresse et peut contenir des 
> renseignements confidentiels et assujettis au secret professionnel. La 
> confidentialite et le secret professionnel demeurent malgre l'envoi de ce 
> document a la mauvaise adresse electronique. Si vous n'etes pas le 
> destinataire vise ou la personne chargee de remettre ce document a son destinataire, veuillez nous en informer sans delai et detruire ce document ainsi que toute copie qui en aurait ete faite.Toute distribution, reproduction ou autre utilisation de ce document est 
> strictement interdite. De plus, le Groupe Financiere Banque Nationale et ses filiales ne peuvent pas etre tenus responsables des dommages pouvant etre causes par des virus ou des erreurs de transmission. 
> 
> DISCLAIMER:\ This documentation transmitted by electronic ma...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Mark.Bravington at csiro.au  Fri May 13 08:32:10 2005
From: Mark.Bravington at csiro.au (Mark.Bravington@csiro.au)
Date: Fri, 13 May 2005 16:32:10 +1000
Subject: [R] [R-pkgs] new version of package:mvbutils
Message-ID: <4D99275E380CA94F998977EDACE548DC06265F@extas2-hba.tas.csiro.au>

There is a new version of the 'mvbutils' package (v1.1.1) available on
CRAN.

For existing users, the main new features are:

(i) 'help' now works properly with with R2.0+ (!)
(ii) 'mvbutils' is now NAMESPACEd, so you can avoid naming conflicts
with other packages. (Many functions are no longer user-visible.
(iii) Lazy-loading of individual objects is now available. Among other
things, this speeds up the use of 'cd' immensely with workspaces that
contain large objects.
(iv) better-organized interactions between 'history' and 'cd'.

For new users, the main features of 'mvbutils' are:

  * Hierarchical organization of projects and sub-projects, allowing
switching within a single R session, searching and moving objects
through the hierarchy, objects in ancestor projects always visible from
child (sub)projects, etc.

  * Improved function editing facilities, interfacing with whichever
text editor you prefer. The R command line is not frozen while editing,
and you can have multiple edit windows open. There is also a complete
automatic backup system, with (as a side-effect) the ability to sort
objects by date.

  * "Lazy loading" for individual objects, allowing fast & efficient
access to collections of biggish objects where only a few objects are
used at a time.

  * Support for flat-format (plain-text) documentation, stored along
with a function and editable at the same time, and viewable through
normal 'help'. Automatic conversion to Rd format is available if certain
rules are followed.

  * Support for nesting of 'source' calls, and for interspersing R code
and data in the same file.

  * Support for easy macro-like functions which act in their caller's
environment (see also RNews 1/3), and for "dynamically scoped"
functions.

  * Graphical display of which functions call which other functions.

  * numerous other lower-level utility functions and operators.

Please let me know of any comments and bug reports.

Mark Bravington (mark.bravington at csiro.au)
CSIRO Mathematical & Information Sciences

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From Mark.Bravington at csiro.au  Fri May 13 08:34:01 2005
From: Mark.Bravington at csiro.au (Mark.Bravington@csiro.au)
Date: Fri, 13 May 2005 16:34:01 +1000
Subject: [R] [R-pkgs] new version of package:debug
Message-ID: <4D99275E380CA94F998977EDACE548DC1AE6D3@extas2-hba.tas.csiro.au>

There is a new version of the 'debug' package (v1.1.0) available on
CRAN. For existing users, the main changes are (i) some bug fixes, and
(ii) it is now NAMESPACEd, so you can avoid naming conflicts with other
packages.

For new users, 'debug' is an alternative to 'trace' and 'browser',
offering:

	*  a visible code window with line-numbered code and highlighted
execution point;

	*  the ability to set (conditional) breakpoints in advance, at
any line number;

	*  the opportunity to keep going after errors;

	*  multiple debugging windows open at once (when one debuggee
calls another, or itself);

	*  full debugging of 'on.exit' code;

	*  the ability to move the execution point around without
executing intervening statements;

	*  direct interpretation of typed-in statements, as if they were
in the function itself.
	
See R-news 3.3 for an article on using this package.
	
Please let me know of any comments and bug reports.

Mark Bravington (mark.bravington at csiro.au)
CSIRO Mathematical & Information Sciences

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From bhx2 at mevik.net  Fri May 13 10:20:39 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 13 May 2005 10:20:39 +0200
Subject: [R] pls -- crossval vs plsr(..., CV=TRUE)
In-Reply-To: <m0y8akh8jn.fsf@bar.nemo-project.org> 
	=?iso-8859-1?q?=28Bj=F8rn-Helge?= Mevik's message of "Thu,
	12 May 2005 15:34:52 +0200")
References: <9466f3f90ea00fb3ce09756cd3cce9ee@psu.edu>
	<m0y8akh8jn.fsf@bar.nemo-project.org>
Message-ID: <m0k6m3tu3s.fsf@bar.nemo-project.org>

Bj??rn-Helge Mevik writes:

> There are two reasons:

Actually, there is a third reason as well. :-)  We've just discovered an
embarrasingly stupid bug in the R2 calculation in mvrCv; it calculates
R (the correlation) instead of R2 (squared correlation).  A patched
version will be released shortly.  Until then, c(R2(NIR.plsCV)$val^2)
should give you the correct values.

-- 
Bj??rn-Helge Mevik



From slist at oomvanlieshout.net  Fri May 13 11:27:24 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 13 May 2005 11:27:24 +0200
Subject: [R] Problem with data frame when using xYplot?
In-Reply-To: <42831D80.5090300@oomvanlieshout.net>
References: <42831D80.5090300@oomvanlieshout.net>
Message-ID: <428472FC.2090400@oomvanlieshout.net>

Problem solved!

I was so focused on reproducing the plotmeans() functionality with 
xYplot() that I completely overlooked the fact that my data does not 
allow a x-y plot, as only Sodium is a numeric variable while Position 
and AltGeo are factors!

Using unclass() to make Position a numeric variable does the trick:
tmp$Position <- unclass(tmp$Position)

The code below does the trick. Now I only need to figure out how to 
tweak the x axis to pretend I am plotting a factor, i.e. plotting labels 
"Inside" and "Outside".

Cheers,

Sander.


library(Hmisc)
library(Lattice)
tmp <-
structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label = 
c("Basalt-High",
     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
     4.66666666666667, 5.55555555555556, 10.6666666666667, 5.66666666666667
     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
     15.2679766541317, 5.45435605731786, 8.09492296305393, 10.6183802907976,
     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
c(25.5382783976218,
     26.0818978307592, 18.2793849487813, 13.0217855597339, 2.84854798089405,
     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
c(29.1283882690448,
     27.6958799470186, 31.7206150512187, 23.2004366624884, 6.48478535243929,
     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
c("Position",
"AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
"2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
tmp$Position <- unclass(tmp$Position)
xYplot(Cbind(Sodium,Lower,Upper) ~ Position|AltGeo, groups=AltGeo,
   data=tmp, ylim=c(min(tmp$Lower)-1,max(tmp$Upper)+1),
   xlab="Position", ylab="Sodium"
   )



Sander Oom wrote:
> Dear all,
> 
> I am trying to plot means and error bars using xYplot, but I get an 
> error message from xYplot which I can not figure out:
>  > Error in Summary.factor(..., na.rm = na.rm) :
>         range not meaningful for factors
> 
> The data frame (tmpNa) was created using aggregate. I have used dump to 
> created the code below, which generates the same error.
> 
> Can anybody tell me what is wrong with the data frame?
> 
> Thanks in advance,
> 
> Sander.
> 
> library(Hmisc)
> tmpNa <-
> structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
> 1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
>     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label = 
> c("Basalt-High",
>     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
>     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
>     4.66666666666667, 5.55555555555556, 10.6666666666667, 5.66666666666667
>     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
>     15.2679766541317, 5.45435605731786, 8.09492296305393, 10.6183802907976,
>     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
> c(25.5382783976218,
>     26.0818978307592, 18.2793849487813, 13.0217855597339, 2.84854798089405,
>     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
> c(29.1283882690448,
>     27.6958799470186, 31.7206150512187, 23.2004366624884, 6.48478535243929,
>     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
> c("Position",
> "AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
> xYplot(Cbind(Sodium,Lower,Upper) ~ AltGeo, groups=Position,  data=tmpNa)
> 
> 
>  > version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
> 

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From Jan.Verbesselt at biw.kuleuven.be  Fri May 13 11:55:06 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Fri, 13 May 2005 11:55:06 +0200
Subject: [R] DEV2bitmap: jpeg with res=400 not enough for CORELDRAW poster A0
Message-ID: <000101c557a1$d80032d0$1145210a@agr.ad10.intern.kuleuven.ac.be>

Dear all,

When saving a plot with the dev2bitmap command:

    name    <- c("test.jpeg")
    dev2bitmap(name,type="jpeg",height=8,width=13,res=400)

Everything seems to be ok... After importing this picture in CORELDRAW (for
a poster A0) format the resolution and colors are not optimal.

How can I save pictures (colors/resolution) optimally for import into
CorelDraw for an A0 poster?

Pdf?/tiff?/bmp?/

Thanks,
Jan


_______________________________________________________________________
ir. Jan Verbesselt 
Research Associate 
Lab of Geomatics Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel: +32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/



From paul.bliese at us.army.mil  Fri May 13 11:59:58 2005
From: paul.bliese at us.army.mil (Bliese, Paul D LTC USAMH)
Date: Fri, 13 May 2005 11:59:58 +0200
Subject: [R] Using R to illustrate the Central Limit Theorem
Message-ID: <FADCFAA8BA80C748890C1D3893C198D953FC31@amedmlmhah01.eur.amed.ds.army.mil>

Interesting thread. The graphics are great, the only thing that might be
worth doing for teaching purposes would be to illustrate the original
distribution that is being averaged 1000 times.

Below is one option based on Bill Venables code.  Note that to do this I
had to start with a k of 2.

N <- 10000
 for(k in 2:20) {
    graphics.off()
    par(mfrow = c(2,2), pty = "s")
    hist(((runif(k))-0.5)*sqrt(12*k),main="Example Distribution 1")
    hist(((runif(k))-0.5)*sqrt(12*k),main="Example Distribution 2")
    m <- replicate(N, (mean(runif(k))-0.5)*sqrt(12*k))
    hist(m, breaks = "FD", xlim = c(-4,4), main = k,
            prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
    pu <- par("usr")[1:2]
    x <- seq(pu[1], pu[2], len = 500)
    lines(x, dnorm(x), col = "red")
    qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
    abline(0, 1, col = "red")
    Sys.sleep(3)
  }

By the way, I should probably know this but what is the logic of the
"sqrt(12*k)" part of the example?  Obviously as k increases the mean
will approach .5 in a uniform distribution, so runif(k)-.5 will be close
to zero, and sqrt(12*k) increases as k increases.  Why 12, though?

PB


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kevin E. Thorpe
Sent: Friday, May 13, 2005 12:03 AM
To: Bill.Venables at csiro.au
Cc: ted.harding at nessie.mcc.ac.uk; r-help at stat.math.ethz.ch
Subject: Re: [R] Using R to illustrate the Central Limit Theorem

This thread was very timely for me since I will be teaching an
introductory
biostats course in the fall, including a session on CLT. I have
shamelessly
taken Dr. Venables' slick piece of code and wrapped it in a function so
that
I can vary the maximum sample size at will. I then created functions
based
on the first to sample from the exponential and the
chi-squaredistributions.
Lastly, I created a function to sample from a pdf with a parabolic shape
(confirming in the process just how rusty my calculus is :-) ). My code
is
below for all to do with as they please.

Now, at the risk of asking a really obvious question ...

In my final function, I used the inverse probability integral
transformation
to sample from my parabolic distribution. I create a local function
rparab
shown here:

rparab <- function(nn) {
u <- 2*runif(nn) - 1
ifelse(u<0,-(abs(u)^(1/3)),u^(1/3))
}

It seems that in my version of R (2.0.1) on Linux, that calculating the
cube
root of a negative number using ^(1/3) returns NaN. I looked at the help
in
the arithmetic operators and did help.search("cube root"), 
help.search("root")
and help.search("cube") and recognised no alternatives. So I used an 
ifelse() to
deal with the negatives. Have I missed something really elementary?

Thanks

clt.unif <- function(n=20) {
N <- 10000
graphics.off()
par(mfrow = c(1,2), pty = "s")
for(k in 1:n) {
m <- (rowMeans(matrix(runif(N*k), N, k)) - 0.5)*sqrt(12*k)
hist(m, breaks = "FD", xlim = c(-4,4),
main = paste("Uniform(0,1), n = ",k,sep=""),
prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
pu <- par("usr")[1:2]
x <- seq(pu[1], pu[2], len = 500)
lines(x, dnorm(x), col = "red")
qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
abline(0, 1, col = "red")
Sys.sleep(1)
}
}

clt.exp <- function(n=20,theta=1) {
N <- 10000
graphics.off()
par(mfrow = c(1,2), pty = "s")
for(k in 1:n) {
m <- (rowMeans(matrix(rexp(N*k,1/theta), N, k)) - theta)/sqrt(theta^2/k)
hist(m, breaks = "FD", xlim = c(-4,4),
main = paste("exp(",theta,"), n = ",k,sep=""),
prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
pu <- par("usr")[1:2]
x <- seq(pu[1], pu[2], len = 500)
lines(x, dnorm(x), col = "red")
qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
abline(0, 1, col = "red")
Sys.sleep(1)
}
}

clt.chisq <- function(n=20,nu=1) {
N <- 10000
graphics.off()
par(mfrow = c(1,2), pty = "s")
for(k in 1:n) {
m <- (rowMeans(matrix(rchisq(N*k,nu), N, k)) - nu)/sqrt(2*nu/k)
hist(m, breaks = "FD", xlim = c(-4,4),
main = paste("Chi-Square(",nu,"), n = ",k,sep=""),
prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
pu <- par("usr")[1:2]
x <- seq(pu[1], pu[2], len = 500)
lines(x, dnorm(x), col = "red")
qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
abline(0, 1, col = "red")
Sys.sleep(1)
}
}

clt.parab <- function(n=20) {
rparab <- function(nn) {
u <- 2*runif(nn) - 1
ifelse(u<0,-(abs(u)^(1/3)),u^(1/3))
}
N <- 10000
graphics.off()
par(mfrow = c(1,2), pty = "s")
for(k in 1:n) {
m <- (rowMeans(matrix(rparab(N*k), N, k)))/sqrt(3/(5*k))
hist(m, breaks = "FD", xlim = c(-4,4),
main = paste("n = ",k,sep=""),
prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
pu <- par("usr")[1:2]
x <- seq(pu[1], pu[2], len = 500)
lines(x, dnorm(x), col = "red")
qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
abline(0, 1, col = "red")
Sys.sleep(1)
}
}

Bill.Venables at csiro.au wrote:

>Here's a bit of a refinement on Ted's first suggestion.
>
> 
> N <- 10000
> graphics.off()
> par(mfrow = c(1,2), pty = "s")
> for(k in 1:20) {
>    m <- (rowMeans(matrix(runif(M*k), N, k)) - 0.5)*sqrt(12*k)
>    hist(m, breaks = "FD", xlim = c(-4,4), main = k,
>            prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
>    pu <- par("usr")[1:2]
>    x <- seq(pu[1], pu[2], len = 500)
>    lines(x, dnorm(x), col = "red")
>    qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
>    abline(0, 1, col = "red")
>    Sys.sleep(1)
>  }
>  
>


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri May 13 12:19:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 13 May 2005 12:19:37 +0200
Subject: [R] DEV2bitmap: jpeg with res=400 not enough for CORELDRAW poster
	A0
In-Reply-To: <000101c557a1$d80032d0$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000101c557a1$d80032d0$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <42847F39.5060902@statistik.uni-dortmund.de>

Jan Verbesselt wrote:

> Dear all,
> 
> When saving a plot with the dev2bitmap command:
> 
>     name    <- c("test.jpeg")
>     dev2bitmap(name,type="jpeg",height=8,width=13,res=400)
> 
> Everything seems to be ok... After importing this picture in CORELDRAW (for
> a poster A0) format the resolution and colors are not optimal.
> 
> How can I save pictures (colors/resolution) optimally for import into
> CorelDraw for an A0 poster?
> 
> Pdf?/tiff?/bmp?/

What about PostScript? It's perfectly resizable and CorelDraw (at least 
the outdated version 10) can deal with it.

Uwe Ligges


> Thanks,
> Jan
> 
> 
> _______________________________________________________________________
> ir. Jan Verbesselt 
> Research Associate 
> Lab of Geomatics Engineering K.U. Leuven
> Vital Decosterstraat 102. B-3000 Leuven Belgium 
> Tel: +32-16-329750   Fax: +32-16-329760
> http://gloveg.kuleuven.ac.be/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri May 13 12:38:10 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 May 2005 12:38:10 +0200
Subject: [R] DEV2bitmap: jpeg with res=400 not enough for CORELDRAW poster
	A0
In-Reply-To: <42847F39.5060902@statistik.uni-dortmund.de>
References: <000101c557a1$d80032d0$1145210a@agr.ad10.intern.kuleuven.ac.be>
	<42847F39.5060902@statistik.uni-dortmund.de>
Message-ID: <x2ll6jl8bx.fsf@biostat.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Jan Verbesselt wrote:
> 
> > Dear all,
> > When saving a plot with the dev2bitmap command:
> >     name    <- c("test.jpeg")
> >     dev2bitmap(name,type="jpeg",height=8,width=13,res=400)
> > Everything seems to be ok... After importing this picture in
> > CORELDRAW (for
> > a poster A0) format the resolution and colors are not optimal.
> > How can I save pictures (colors/resolution) optimally for import into
> > CorelDraw for an A0 poster?
> > Pdf?/tiff?/bmp?/
> 
> What about PostScript? It's perfectly resizable and CorelDraw (at
> least the outdated version 10) can deal with it.

But not PDF? Notice that this is not really bitmapped either, even
though handled by dev2bitmap. 

Upping the res= is another option, but may be memory intensive. Notice
that A0 is 4 times as big as A4 (~8x12 inches) so you'd need up to 4
times the resolution - but I guess that your plot is not taking up the
whole area...


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From david.meyer at wu-wien.ac.at  Fri May 13 13:04:43 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Fri, 13 May 2005 13:04:43 +0200
Subject: [R] SVM linear kernel and SV
Message-ID: <20050513130443.56b79183.david.meyer@wu-wien.ac.at>

> Thank you for your answer,
> but my problem concerns the support vectors. Indeed the two classes
> are well separated and the hyperplane is linear but the support
> vectors aren't aligned in parallel to the hyperplane. And according to
> me,  the support vectors (for each class) should be aligned along the
> linear hyperplane and form the marge (by definition). But it's not the
> case. In fact,  I'd like to understand why they are not aligned. 

Remember the `cost'-penalty controlling for overlapping classes. It has
some effect even in the linearly separable case causing more SVs than
would actually be needed. Try adding e.g. `cost=1000' and you will
obtain a result with only 2 SVs (why not 3? Because 2 SVs here solve the
optimization problem. So in fact the hyperplane in this case is not
uniquely defined, although only in a small range.)

Best,

David



From Arne.Muller at sanofi-aventis.com  Fri May 13 13:07:25 2005
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Fri, 13 May 2005 13:07:25 +0200
Subject: [R] error in plot.lmList
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF37E@CRBSMXSUSR04>

Hello,

in R-2.1.0 I'm trying to prodice trellis plots from an lmList object as described in the help for plot.lmList. I can generate the plots from the help, but on my own data plotting fails with an error message that I cannot interpret (please see below). Any hints are greatly appreciapted.

	kind regards,

	Arne

> dim(d)
[1] 575   4
> d[1:3,]
  Level_of_Expression SSPos1 SSPos19 Method
1                11.9      G       A   bDNA
2                24.7      T       T   bDNA
3                 9.8      C       T   bDNA
> fm <- lmList(Level_of_Expression ~ SSPos1 + SSPos19 | Method, data=d)
> fm
Call:
  Model: Level_of_Expression ~ SSPos1 + SSPos19 | Method 
   Data: d 

Coefficients:
           (Intercept)   SSPos1C    SSPos1G   SSPos1T SSPos19C SSPos19G   SSPos19T
bDNA          25.75211 -6.379701  -9.193304 10.371056 24.32171 24.06107  9.7357724
Luciferase    23.79947  4.905679  -7.747861  8.112779 48.95151 48.15064 -0.2646783
RT-PCR        56.08985 -7.352206 -15.896556 -2.712313 19.91967 24.28425 -2.2317071
Western       14.03876  2.777038 -14.113157 -7.804959 24.62684 25.50382  8.3864782

Degrees of freedom: 575 total; 547 residual
Residual standard error: 25.39981
> plot(fm, Level_of_Expression ~ fitted(.))
Error in plot.lmList(fm, Level_of_Expression ~ fitted(.)) : 
        Object "cF" not found

what is object cF ...?



From slist at oomvanlieshout.net  Fri May 13 14:21:24 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 13 May 2005 14:21:24 +0200
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
Message-ID: <42849BC4.9030108@oomvanlieshout.net>

Dear R users,

The Sweave code below runs fine, as it is. However, an error occurs when 
the line 'library(xtable)' is uncommented:
Error:  chunk 1
Error in "label<-"(`*tmp*`, value = "month") :
         no applicable method for "label<-"

Is anybody aware of this and knows a workaround?

Thanks,

Sander.

*******************

\documentclass[a4paper]{article}
\title{Sweave Test for summarize}
\author{Sander Oom}

\usepackage{a4wide}

\begin{document}

\maketitle

\begin{figure}[ht]
\begin{center}
<<fig=TRUE,echo=FALSE>>=
   # library(xtable)
   library(Hmisc)
   set.seed(111)
   dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
   month <- dfr$month
   year <- dfr$year
   y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
   s <- summarize(y, llist(month,year), smedian.hilow, conf.int=.5)
   print(xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s,
         keys='lines', method='alt', type='b'))
@
\end{center}
\end{figure}

\end{document}

************************



 > version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R


-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From kevin.thorpe at utoronto.ca  Fri May 13 14:28:05 2005
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Fri, 13 May 2005 08:28:05 -0400
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <FADCFAA8BA80C748890C1D3893C198D953FC31@amedmlmhah01.eur.amed.ds.army.mil>
References: <FADCFAA8BA80C748890C1D3893C198D953FC31@amedmlmhah01.eur.amed.ds.army.mil>
Message-ID: <42849D55.5060703@utoronto.ca>

The variance of U[0,1] is 1/12. So the variance of the mean of uniforms 
is 1/12k.
Rather than dividing by 1/12k he multiplied by 12k.

Kevin

Bliese, Paul D LTC USAMH wrote:

>Interesting thread. The graphics are great, the only thing that might be
>worth doing for teaching purposes would be to illustrate the original
>distribution that is being averaged 1000 times.
>
>Below is one option based on Bill Venables code.  Note that to do this I
>had to start with a k of 2.
>
>N <- 10000
> for(k in 2:20) {
>    graphics.off()
>    par(mfrow = c(2,2), pty = "s")
>    hist(((runif(k))-0.5)*sqrt(12*k),main="Example Distribution 1")
>    hist(((runif(k))-0.5)*sqrt(12*k),main="Example Distribution 2")
>    m <- replicate(N, (mean(runif(k))-0.5)*sqrt(12*k))
>    hist(m, breaks = "FD", xlim = c(-4,4), main = k,
>            prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
>    pu <- par("usr")[1:2]
>    x <- seq(pu[1], pu[2], len = 500)
>    lines(x, dnorm(x), col = "red")
>    qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
>    abline(0, 1, col = "red")
>    Sys.sleep(3)
>  }
>
>By the way, I should probably know this but what is the logic of the
>"sqrt(12*k)" part of the example?  Obviously as k increases the mean
>will approach .5 in a uniform distribution, so runif(k)-.5 will be close
>to zero, and sqrt(12*k) increases as k increases.  Why 12, though?
>
>PB
>  
>


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462



From n.d.fitzgerald at mars.ucc.ie  Fri May 13 14:38:56 2005
From: n.d.fitzgerald at mars.ucc.ie (s104100026)
Date: Fri, 13 May 2005 13:38:56 +0100
Subject: [R] Big matrix memory problem
Message-ID: <42831846@webmail.ucc.ie>

Hi All,

I want to read 256 1000x1000 matrices into R. I understand that it is unlikely 
that I can do this but In the hope that somebody can help me I am mailing this 
list.

I have tried increasing my memory size (I understand that it is the minimum of 
1024 or the computers RAM in my case 512)

Does anyone think this is possible in R, could it be tried in Splus for 
example.

Any help is greatly appreciated.

Niall Fitzgerald
Phd Candidate.



From kevin.thorpe at utoronto.ca  Fri May 13 14:45:28 2005
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Fri, 13 May 2005 08:45:28 -0400
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <x2ll6km3i1.fsf@turmalin.kubism.ku.dk>
References: <B998A44C8986644EA8029CFE6396A9241B31CF@exqld2-bne.qld.csiro.au>	<4283E0B8.6000004@utoronto.ca>
	<x2ll6km3i1.fsf@turmalin.kubism.ku.dk>
Message-ID: <4284A168.3040008@utoronto.ca>

Thanks. I'd forgotton that detail about ifelse. Also, I was so focused 
on the U[0,1] it totally did not occur to me to go direct from U[-1,1]. 
Your suggestion should obviate the need for a local function at all.

Peter Dalgaard wrote:

>"Kevin E. Thorpe" <kevin.thorpe at utoronto.ca> writes:
>
>  
>
>>rparab <- function(nn) {
>>u <- 2*runif(nn) - 1
>>ifelse(u<0,-(abs(u)^(1/3)),u^(1/3))
>>}
>>
>>It seems that in my version of R (2.0.1) on Linux, that calculating the cube
>>root of a negative number using ^(1/3) returns NaN. I looked at the help in
>>the arithmetic operators and did help.search("cube root"),
>>help.search("root")
>>and help.search("cube") and recognised no alternatives. So I used an
>>ifelse() to
>>deal with the negatives. Have I missed something really elementary?
>>    
>>
>
>Not really. You might have used u <- runif(nn,-1,1) and
>sign(u)*abs(u)^(1/3) instead of the ifelse construct (remember that
>ifelse generally evaluates both the 'yes' and 'no' parts and on some
>architectures the NaN results may be slow to compute).
>
>
>  
>


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462



From ligges at statistik.uni-dortmund.de  Fri May 13 14:58:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 13 May 2005 14:58:01 +0200
Subject: [R] Big matrix memory problem
In-Reply-To: <42831846@webmail.ucc.ie>
References: <42831846@webmail.ucc.ie>
Message-ID: <4284A459.1090209@statistik.uni-dortmund.de>

s104100026 wrote:

> Hi All,
> 
> I want to read 256 1000x1000 matrices into R. I understand that it is unlikely 
> that I can do this but In the hope that somebody can help me I am mailing this 
> list.
>
> I have tried increasing my memory size (I understand that it is the minimum of 
> 1024 or the computers RAM in my case 512)
> 
> Does anyone think this is possible in R, could it be tried in Splus for 
> example.


Given the matrices are numeric, you will need 256*1000*1000*8 = 2Gb of 
memory just to hold them in memory, in order to apply calculations, 
objects are frequently doubled .......
So you should really handle those matrices separately, either by getting 
them from a database or by saving them in form of separate Rdata objects.

Uwe Ligges


> Any help is greatly appreciated.
> 
> Niall Fitzgerald
> Phd Candidate.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri May 13 14:57:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 May 2005 13:57:40 +0100 (BST)
Subject: [R] Big matrix memory problem
In-Reply-To: <42831846@webmail.ucc.ie>
Message-ID: <Pine.GSO.4.31.0505131355470.16442-100000@toucan.stats>

On Fri, 13 May 2005, s104100026 wrote:

> Hi All,
>
> I want to read 256 1000x1000 matrices into R. I understand that it is unlikely
> that I can do this but In the hope that somebody can help me I am mailing this
> list.

What sort of matrix?  If these are real numbers that is 2Gb.

> I have tried increasing my memory size (I understand that it is the minimum of
> 1024 or the computers RAM in my case 512)
>
> Does anyone think this is possible in R, could it be tried in Splus for
> example.

It is possible in R, but you will need a 64-bit version of R.  Since you
don't state your OS, my guess is it is Windows or MacOS, for neither of
which we have a 64-bit port as yet.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Fri May 13 14:57:52 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 13 May 2005 14:57:52 +0200
Subject: [R] Big matrix memory problem
In-Reply-To: <42831846@webmail.ucc.ie>
Message-ID: <4284C070.691.E8AB7@localhost>

Hallo

On 13 May 2005 at 13:38, s104100026 wrote:

> Hi All,
> 
> I want to read 256 1000x1000 matrices into R. I understand that it is
> unlikely that I can do this but In the hope that somebody can help me
> I am mailing this list.

Why do you think that.

I easilly read files from data logging equipment.

> 60min*24h*15days*130variables from text file which has 
[1] 2808000 items

just by read.table() in few whiles on quite old (4 years) and not 
superbly equipped PC (1G memory). 

So you can read your matrices sequentially. But if you want to 
work with all 256 matrices at once it could be problem.

Depends on what you want to do with them.
Cheers
Petr

> 
> I have tried increasing my memory size (I understand that it is the
> minimum of 1024 or the computers RAM in my case 512)
> 
> Does anyone think this is possible in R, could it be tried in Splus
> for example.
> 
> Any help is greatly appreciated.
> 
> Niall Fitzgerald
> Phd Candidate.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Ted.Harding at nessie.mcc.ac.uk  Fri May 13 15:00:00 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 13 May 2005 14:00:00 +0100 (BST)
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <FADCFAA8BA80C748890C1D3893C198D953FC31@amedmlmhah01.eur.amed.ds.army.mil>
Message-ID: <XFMail.050513140000.Ted.Harding@nessie.mcc.ac.uk>

On 13-May-05 Bliese, Paul D LTC USAMH wrote:
> Interesting thread. The graphics are great, the only thing that
> might be worth doing for teaching purposes would be to illustrate
> the original distribution that is being averaged 1000 times.
> 
> Below is one option based on Bill Venables code.  Note that to do
> this I had to start with a k of 2.
> 
> N <- 10000
>  for(k in 2:20) {
>     graphics.off()
>     par(mfrow = c(2,2), pty = "s")
>     hist(((runif(k))-0.5)*sqrt(12*k),main="Example Distribution 1")
>     hist(((runif(k))-0.5)*sqrt(12*k),main="Example Distribution 2")
>     m <- replicate(N, (mean(runif(k))-0.5)*sqrt(12*k))
>     hist(m, breaks = "FD", xlim = c(-4,4), main = k,
>             prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
>     pu <- par("usr")[1:2]
>     x <- seq(pu[1], pu[2], len = 500)
>     lines(x, dnorm(x), col = "red")
>     qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
>     abline(0, 1, col = "red")
>     Sys.sleep(3)
>   }
> 
> By the way, I should probably know this but what is the logic of
> the "sqrt(12*k)" part of the example?  Obviously as k increases
> the mean will approach .5 in a uniform distribution, so
> runif(k)-.5 will be close to zero, and sqrt(12*k) increases as
> k increases.  Why 12, though?

The reason is indeed simple! In demonstrating the convergence
of the distribution of mean(k X's) to a Normal distribution,
the reference (i.e. the limiting distribution) is N(0,1), which
has mean 0 and variance 1. Therefore, in comparing the distribution
of mean(k X's) with N(0,1) it needs to be standardised to itself
have mean 0 and variance 1. As you've already spotted, you
standardise for the mean by subtracting 0.5; to standardise
for the variance you need to divide by sqrt(variance(mean(k X's))).

This is sqrt(variance(X)/k). Finally (and this is where the "12"
comes in), the variance of an X uniformly distributed on (0,1)
is 1/12 (left as an exercise for the reader ... ). Hence 12*k.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-May-05                                       Time: 13:43:54
------------------------------ XFMail ------------------------------



From ggrothendieck at gmail.com  Fri May 13 15:07:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 May 2005 09:07:55 -0400
Subject: [R] Big matrix memory problem
In-Reply-To: <42831846@webmail.ucc.ie>
References: <42831846@webmail.ucc.ie>
Message-ID: <971536df05051306074d47e52d@mail.gmail.com>

On 5/13/05, s104100026 <n.d.fitzgerald at mars.ucc.ie> wrote:
> Hi All,
> 
> I want to read 256 1000x1000 matrices into R. I understand that it is unlikely
> that I can do this but In the hope that somebody can help me I am mailing this
> list.
> 
> I have tried increasing my memory size (I understand that it is the minimum of
> 1024 or the computers RAM in my case 512)
> 
> Does anyone think this is possible in R, could it be tried in Splus for
> example.
> 

If they are sparse you could try the SparseM package.



From slist at oomvanlieshout.net  Fri May 13 15:07:43 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 13 May 2005 15:07:43 +0200
Subject: [R] Problem with data frame when using xYplot?
In-Reply-To: <428472FC.2090400@oomvanlieshout.net>
References: <42831D80.5090300@oomvanlieshout.net>
	<428472FC.2090400@oomvanlieshout.net>
Message-ID: <4284A69F.8010801@oomvanlieshout.net>

An off list response from Mat Soukop (thanks Mat!!) provides an even 
more elegant solution (see code below)! I have included the original 
code, so people can decide whether to plot in a single panel or in 
multiple panels. Now we have a fully functional workaround to get 
plotmeans{gplots} for multiple factors using lattice! Great!

********************


library(Hmisc)
library(lattice)
tmp <-
structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label = 
c("Basalt-High",
     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
     4.66666666666667, 5.55555555555556, 10.6666666666667, 5.66666666666667
     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
     15.2679766541317, 5.45435605731786, 8.09492296305393, 10.6183802907976,
     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
c(25.5382783976218,
     26.0818978307592, 18.2793849487813, 13.0217855597339, 2.84854798089405,
     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
c(29.1283882690448,
     27.6958799470186, 31.7206150512187, 23.2004366624884, 6.48478535243929,
     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
c("Position",
"AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
"2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
tmp$PosNum <- unclass(tmp$Position)
tmp
(labs <- unique(tmp$Position))
# plot factor levels in seperate panels
xYplot(Cbind(Sodium,Lower,Upper) ~ PosNum|AltGeo, data=tmp, nx=FALSE,
   xlim=c(0.5,2.5),
   ylim=c(min(tmp$Lower)-1,max(tmp$Upper)+1),
   scales = list(x = list(at=seq(1, 2, by=1), labels = labs)),
   xlab="Position", ylab="Sodium"
   )

################
new.back <- trellis.par.get("background")
new.back$col <- "white"
newcol <- trellis.par.get("superpose.symbol")
newcol$col <- c('green4','blue','red','black')
newcol$pch <- c(16,1,4,8)
new.line <- trellis.par.get("box.rectangle")
new.line$col <- 'black'
trellis.par.set("background", new.back)
trellis.par.set("superpose.symbol", newcol)
trellis.par.set("box.rectangle", new.line)
# Plot factor levels in one graph
tmp$xvar <- rep(1:4, each=2)+rep(c(-.05,.05), 4)
xYplot(Cbind(Sodium,Lower,Upper) ~ xvar, groups=Position,  data=tmp,
   scales=list(y='free',x=list(at=1:4,
   labels=levels(tmp$AltGeo))),
   xlab='AltGeo', xlim=c(.5, 4.5),
   key=list(points=Rows(trellis.par.get("superpose.symbol"),1:2),
     text=list(lab =as.character(levels(tmp$Position)),
     col=trellis.par.get("superpose.symbol")$col[1:2]),
     columns=2, cex=1, title="Position",
     cex.title=1.1))


********************


Sander Oom wrote:
> Problem solved!
> 
> I was so focused on reproducing the plotmeans() functionality with 
> xYplot() that I completely overlooked the fact that my data does not 
> allow a x-y plot, as only Sodium is a numeric variable while Position 
> and AltGeo are factors!
> 
> Using unclass() to make Position a numeric variable does the trick:
> tmp$Position <- unclass(tmp$Position)
> 
> The code below does the trick. Now I only need to figure out how to 
> tweak the x axis to pretend I am plotting a factor, i.e. plotting labels 
> "Inside" and "Outside".
> 
> Cheers,
> 
> Sander.
> 
> 
> library(Hmisc)
> library(Lattice)
> tmp <-
> structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
> 1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
>     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label = 
> c("Basalt-High",
>     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
>     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
>     4.66666666666667, 5.55555555555556, 10.6666666666667, 5.66666666666667
>     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
>     15.2679766541317, 5.45435605731786, 8.09492296305393, 10.6183802907976,
>     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
> c(25.5382783976218,
>     26.0818978307592, 18.2793849487813, 13.0217855597339, 2.84854798089405,
>     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
> c(29.1283882690448,
>     27.6958799470186, 31.7206150512187, 23.2004366624884, 6.48478535243929,
>     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
> c("Position",
> "AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
> tmp$Position <- unclass(tmp$Position)
> xYplot(Cbind(Sodium,Lower,Upper) ~ Position|AltGeo, groups=AltGeo,
>   data=tmp, ylim=c(min(tmp$Lower)-1,max(tmp$Upper)+1),
>   xlab="Position", ylab="Sodium"
>   )
> 
> 
> 
> Sander Oom wrote:
>> Dear all,
>>
>> I am trying to plot means and error bars using xYplot, but I get an 
>> error message from xYplot which I can not figure out:
>>  > Error in Summary.factor(..., na.rm = na.rm) :
>>         range not meaningful for factors
>>
>> The data frame (tmpNa) was created using aggregate. I have used dump 
>> to created the code below, which generates the same error.
>>
>> Can anybody tell me what is wrong with the data frame?
>>
>> Thanks in advance,
>>
>> Sander.
>>
>> library(Hmisc)
>> tmpNa <-
>> structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
>> 1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
>>     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label = 
>> c("Basalt-High",
>>     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
>>     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
>>     4.66666666666667, 5.55555555555556, 10.6666666666667, 
>> 5.66666666666667
>>     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
>>     15.2679766541317, 5.45435605731786, 8.09492296305393, 
>> 10.6183802907976,
>>     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
>> c(25.5382783976218,
>>     26.0818978307592, 18.2793849487813, 13.0217855597339, 
>> 2.84854798089405,
>>     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
>> c(29.1283882690448,
>>     27.6958799470186, 31.7206150512187, 23.2004366624884, 
>> 6.48478535243929,
>>     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
>> c("Position",
>> "AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
>> "2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
>> xYplot(Cbind(Sodium,Lower,Upper) ~ AltGeo, groups=Position,  data=tmpNa)
>>
>>
>>  > version
>>          _
>> platform i686-pc-linux-gnu
>> arch     i686
>> os       linux-gnu
>> system   i686, linux-gnu
>> status
>> major    2
>> minor    1.0
>> year     2005
>> month    04
>> day      18
>> language R
>>
> 

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From bates at stat.wisc.edu  Fri May 13 15:23:08 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 13 May 2005 08:23:08 -0500
Subject: [R] error in plot.lmList
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF37E@CRBSMXSUSR04>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF37E@CRBSMXSUSR04>
Message-ID: <4284AA3C.1050807@stat.wisc.edu>

Arne.Muller at sanofi-aventis.com wrote:
> Hello,
> 
> in R-2.1.0 I'm trying to prodice trellis plots from an lmList object as described in the help for plot.lmList. I can generate the plots from the help, but on my own data plotting fails with an error message that I cannot interpret (please see below). Any hints are greatly appreciapted.
> 
> 	kind regards,
> 
> 	Arne
> 
> 
>>dim(d)
> 
> [1] 575   4
> 
>>d[1:3,]
> 
>   Level_of_Expression SSPos1 SSPos19 Method
> 1                11.9      G       A   bDNA
> 2                24.7      T       T   bDNA
> 3                 9.8      C       T   bDNA
> 
>>fm <- lmList(Level_of_Expression ~ SSPos1 + SSPos19 | Method, data=d)
>>fm
> 
> Call:
>   Model: Level_of_Expression ~ SSPos1 + SSPos19 | Method 
>    Data: d 
> 
> Coefficients:
>            (Intercept)   SSPos1C    SSPos1G   SSPos1T SSPos19C SSPos19G   SSPos19T
> bDNA          25.75211 -6.379701  -9.193304 10.371056 24.32171 24.06107  9.7357724
> Luciferase    23.79947  4.905679  -7.747861  8.112779 48.95151 48.15064 -0.2646783
> RT-PCR        56.08985 -7.352206 -15.896556 -2.712313 19.91967 24.28425 -2.2317071
> Western       14.03876  2.777038 -14.113157 -7.804959 24.62684 25.50382  8.3864782
> 
> Degrees of freedom: 575 total; 547 residual
> Residual standard error: 25.39981
> 
>>plot(fm, Level_of_Expression ~ fitted(.))
> 
> Error in plot.lmList(fm, Level_of_Expression ~ fitted(.)) : 
>         Object "cF" not found
> 
> what is object cF ...?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Could you provide us with a traceback or, perhaps better, a copy of the
data frame d (off-list and disguised, if necessary) so we can reproduce
the problem?

P.S. It may be easier to use xyplot directly as in

 xyplot(Level_of_Expression ~ fitted(fm), d)



From peter.rossi at gsb.uchicago.edu  Fri May 13 15:36:05 2005
From: peter.rossi at gsb.uchicago.edu (Peter E. Rossi)
Date: Fri, 13 May 2005 08:36:05 -0500
Subject: [R] retaining source
Message-ID: <1425ae143f8a.143f8a1425ae@gsb.uchicago.edu>

Folks-

I have created a contributed package, bayesm.  I'd like to load it and retain the source so that users can see comments in the source.

I load it with library(bayesm,keep.source=TRUE).  When I display the functions simply by typing the name at the prompt, I don't see comments and the attr(fun_name,"source") returns NULL.

I checked the options for RCMD build and I don't see anything I'm missing.

I'm sure I'm doing something wrong.

Advice?

thanks!

peter r




................................
 Peter E. Rossi
 Joseph T. and Bernice S. Lewis Professor of Marketing and Statistics
 Editor, Quantitative Marketing and Economics
 Rm 360, Graduate School of Business, U of Chicago
 5807 S. Woodlawn Ave, Chicago IL 60637
 Tel: (773) 702-7513   |   Fax: (773) 834-2081

 peter.rossi at ChicagoGsb.edu
 WWW: http://ChicagoGsb.edu/fac/peter.rossi
SSRN: http://ssrn.com/author=22862
 QME: http://www.kluweronline.com/issn/1570-7156



From gwgilc at wm.edu  Fri May 13 16:09:16 2005
From: gwgilc at wm.edu (George W. Gilchrist)
Date: Fri, 13 May 2005 10:09:16 -0400
Subject: [R] Lattice plot within a "for" loop does not happen?
Message-ID: <B88F80B7-BB1C-4334-8FBC-B2C1C402E46C@wm.edu>

I am trying to do a series of xyplots plots, using a "for" loop to  
substitute the appropriate variables for each plot. The basic command  
works fine by itself and produces a nice plot:

 > i<-3
 >  trellis.device(theme="col.whitebg")
 >  xyplot(as.formula(paste(tmp00[2*i], "~ ", tmp00[(2*i)-1],
+                             "|Blastomere+Phenotype", sep="")),
+         data=tmp1,
+         panel=function(x,y,...){
+           panel.xyplot(jitter(x), jitter(y), pch=16, col="red")
+           if (max(x, na.rm=T)!=0.0) panel.xyplot(x, y, type="r",  
lwd=2)
+        })
 >

BUT, when I stick this in a loop, I get a bunch of blank graphics  
devices. This happens even if the loop only executes once. I could  
just go through and do these one by one, but I was curious if I was  
overlooking something obvious. Thank you for any advice.

==================================================================
George W. Gilchrist                        Email #1: gwgilc at wm.edu
Department of Biology, Box 8795          Email #2: kitesci at cox.net
College of William & Mary                    Phone: (757) 221-7751
Williamsburg, VA 23187-8795                    Fax: (757) 221-6483
http://gwgilc.people.wm.edu/



From jmoreira at fe.up.pt  Fri May 13 16:15:50 2005
From: jmoreira at fe.up.pt (=?iso-8859-1?Q?Jo=E3o_Mendes_Moreira?=)
Date: Fri, 13 May 2005 15:15:50 +0100
Subject: [R] df and gcvpen for parameters selection on projection pursuit
	regression 
Message-ID: <00c701c557c6$443767b0$5e7aa8c0@FEUPsig.fe.up.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050513/2a88df00/attachment.pl

From petr.pikal at precheza.cz  Fri May 13 16:21:11 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 13 May 2005 16:21:11 +0200
Subject: [R] Lattice plot within a "for" loop does not happen?
In-Reply-To: <B88F80B7-BB1C-4334-8FBC-B2C1C402E46C@wm.edu>
Message-ID: <4284D3F7.14240.5AC6D4@localhost>

Hi

many times answered. Just enter

lattice loop R

into Google and you are there

Explicite printing lattice object is what you need.

Cheers
Petr


On 13 May 2005 at 10:09, George W. Gilchrist wrote:

> I am trying to do a series of xyplots plots, using a "for" loop to 
> substitute the appropriate variables for each plot. The basic command 
> works fine by itself and produces a nice plot:
> 
>  > i<-3
>  >  trellis.device(theme="col.whitebg")
>  >  xyplot(as.formula(paste(tmp00[2*i], "~ ", tmp00[(2*i)-1],
> +                             "|Blastomere+Phenotype", sep="")),
> +         data=tmp1,
> +         panel=function(x,y,...){
> +           panel.xyplot(jitter(x), jitter(y), pch=16, col="red") if
> +           (max(x, na.rm=T)!=0.0) panel.xyplot(x, y, type="r",  
> lwd=2)
> +        })
>  >
> 
> BUT, when I stick this in a loop, I get a bunch of blank graphics 
> devices. This happens even if the loop only executes once. I could 
> just go through and do these one by one, but I was curious if I was 
> overlooking something obvious. Thank you for any advice.
> 
> ==================================================================
> George W. Gilchrist                        Email #1: gwgilc at wm.edu
> Department of Biology, Box 8795          Email #2: kitesci at cox.net
> College of William & Mary                    Phone: (757) 221-7751
> Williamsburg, VA 23187-8795                    Fax: (757) 221-6483
> http://gwgilc.people.wm.edu/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From B.Rowlingson at lancaster.ac.uk  Fri May 13 16:24:04 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 13 May 2005 15:24:04 +0100
Subject: [R] Lattice plot within a "for" loop does not happen?
In-Reply-To: <B88F80B7-BB1C-4334-8FBC-B2C1C402E46C@wm.edu>
References: <B88F80B7-BB1C-4334-8FBC-B2C1C402E46C@wm.edu>
Message-ID: <4284B884.5050005@lancaster.ac.uk>


> BUT, when I stick this in a loop, I get a bunch of blank graphics  
> devices. This happens even if the loop only executes once. I could  just 
> go through and do these one by one, but I was curious if I was  
> overlooking something obvious. Thank you for any advice.

  You're overlooking something like line 800 of the documentation for 
xyplot:

  Value:

      An object of class ``trellis''. The `update' method can be used to
      update components of the object and the `print' method (usually
      called by default) will plot it on an appropriate plotting device.


  xyplot doesn't actually make any marks on the screen. Oh no. It 
returns an object. You have to make that object make the marks on the 
screen. This happens automatically when you run something interactively, 
but not inside a function.

  So wrap your xyplot call in a print() function inside your loop:

  for(i in 1:10){
    print(xyplot(....whatever....))
  }

  Its probably in the R-FAQ as well, since my original feeling was that 
this behaviour was chosen in order to confuse people and see how many 
people read the FAQ... :)

Baz



From matthieu.cornec at gmail.com  Fri May 13 16:29:29 2005
From: matthieu.cornec at gmail.com (Matthieu Cornec)
Date: Fri, 13 May 2005 16:29:29 +0200
Subject: [R] help with texi2dvi
Message-ID: <8a83e500050513072924c8569b@mail.gmail.com>

Hello,

Does anyone know how to write the files created by the call of
"texi2dvi" in another directory ?

Thanks,

Matthieu



From ligges at statistik.uni-dortmund.de  Fri May 13 16:52:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 13 May 2005 16:52:36 +0200
Subject: [R] Lattice plot within a "for" loop does not happen?
In-Reply-To: <4284B884.5050005@lancaster.ac.uk>
References: <B88F80B7-BB1C-4334-8FBC-B2C1C402E46C@wm.edu>
	<4284B884.5050005@lancaster.ac.uk>
Message-ID: <4284BF34.9060308@statistik.uni-dortmund.de>

Barry Rowlingson wrote:

> 
>> BUT, when I stick this in a loop, I get a bunch of blank graphics  
>> devices. This happens even if the loop only executes once. I could  
>> just go through and do these one by one, but I was curious if I was  
>> overlooking something obvious. Thank you for any advice.
> 
> 
>  You're overlooking something like line 800 of the documentation for 
> xyplot:
> 
>  Value:
> 
>      An object of class ``trellis''. The `update' method can be used to
>      update components of the object and the `print' method (usually
>      called by default) will plot it on an appropriate plotting device.
> 
> 
>  xyplot doesn't actually make any marks on the screen. Oh no. It returns 
> an object. You have to make that object make the marks on the screen. 
> This happens automatically when you run something interactively, but not 

Baz, actually, printing happens only under two circumstances, AFAIK:
1. by wrapping in print()
2. automatically by evaluating an expression (which might be the object 
name only) in the top level (R_GlobalEnv), but only if no assignment 
takes place.

In particular, automatical (point 2 above) printing happens also in 
non-interactive sessions like
   R CMD BATCH
calls.

The idea of returning an object is very nice, I think. You can calculate 
on the object and print the modified object (well, in fact, I rarely use 
lattice myself, though).

Best,
Uwe


> inside a function.
> 
>  So wrap your xyplot call in a print() function inside your loop:
> 
>  for(i in 1:10){
>    print(xyplot(....whatever....))
>  }
> 
>  Its probably in the R-FAQ as well, since my original feeling was that 
> this behaviour was chosen in order to confuse people and see how many 
> people read the FAQ... :)
> 
> Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Pierre.Lapointe at nbf.ca  Fri May 13 16:52:59 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Fri, 13 May 2005 10:52:59 -0400
Subject: [R] Lowest data level since DateX
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D06B63172@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050513/6397f952/attachment.pl

From ggrothendieck at gmail.com  Fri May 13 17:13:03 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 May 2005 11:13:03 -0400
Subject: [R] help with texi2dvi
In-Reply-To: <8a83e500050513072924c8569b@mail.gmail.com>
References: <8a83e500050513072924c8569b@mail.gmail.com>
Message-ID: <971536df050513081364ae6d00@mail.gmail.com>

On 5/13/05, Matthieu Cornec <matthieu.cornec at gmail.com> wrote:
> Hello,
> 
> Does anyone know how to write the files created by the call of
> "texi2dvi" in another directory ?

Here are two possibilities:

1. Copy the input files into whatever directory you want using
file.copy and then setwd to that directory and run texi2dvi.
Afterwards delete the copied files using file.remove and setwd
back.  See
?file.copy
?file.remove
?setwd

2. Create a batch file to do the same
thing and then use the texi2dvi= argument of texi2dvi
or set it via options so that the R texi2dvi uses that batch file 
rather than the real texi2dvi.  see ?texi2dvi and ?options



From slist at oomvanlieshout.net  Fri May 13 17:15:31 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 13 May 2005 17:15:31 +0200
Subject: [R] Problem with data frame when using xYplot?
In-Reply-To: <4284A69F.8010801@oomvanlieshout.net>
References: <42831D80.5090300@oomvanlieshout.net>	<428472FC.2090400@oomvanlieshout.net>
	<4284A69F.8010801@oomvanlieshout.net>
Message-ID: <4284C493.2010900@oomvanlieshout.net>

I have edited the code (hacked from another graph) to provide more 
control over the different elements of the graph. Now we have a graph at 
publication quality! Slowly the power of R graphics is shining through 
the thick cloud of options! Beautiful.

********************


library(Hmisc)
library(lattice)
ltheme <- canonical.theme(color = FALSE)     ## in-built B&W theme
ltheme$strip.background$col <- "transparent" ## change strip bg
lattice.options(default.theme = ltheme)      ## set as default
tmp <-
structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label = 
c("Basalt-High",
     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
     4.66666666666667, 5.55555555555556, 10.6666666666667, 5.66666666666667
     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
     15.2679766541317, 5.45435605731786, 8.09492296305393, 10.6183802907976,
     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
c(25.5382783976218,
     26.0818978307592, 18.2793849487813, 13.0217855597339, 2.84854798089405,
     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
c(29.1283882690448,
     27.6958799470186, 31.7206150512187, 23.2004366624884, 6.48478535243929,
     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
c("Position",
"AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
"2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
tmp$xvar <- rep(1:4, each=2)+rep(c(-.05,.05), 4)
tmp
sp <- list(superpose.symbol = list(pch = c(16,1), cex = 1))
xYplot(Cbind(Sodium,Lower,Upper) ~ xvar, groups=Position,  data=tmp,
     scales=list(y='free',x=list(at=1:4, labels=levels(tmp$AltGeo))),
     xlim=c(0.5, 4.5), ylim=c(min(tmp$Lower)-1,max(tmp$Upper)+1),
     xlab='AltGeo', ylab='Sodium',
         panel = function(x, y, type, ...) {
           panel.xYplot(x, y, type="p",...)
           lpoints(x, y, pch=16, col="white", cex=2)
           panel.superpose(x, y, type="p", ...)
         },
         par.settings = sp,
     auto.key=list(columns=1, x=0.7, y=0.8, corner = c(0,0))
     )

********************


Sander Oom wrote:
> An off list response from Mat Soukop (thanks Mat!!) provides an even 
> more elegant solution (see code below)! I have included the original 
> code, so people can decide whether to plot in a single panel or in 
> multiple panels. Now we have a fully functional workaround to get 
> plotmeans{gplots} for multiple factors using lattice! Great!
> 
> ********************
> 
> 
> library(Hmisc)
> library(lattice)
> tmp <-
> structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
> 1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
>     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label = 
> c("Basalt-High",
>     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
>     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
>     4.66666666666667, 5.55555555555556, 10.6666666666667, 5.66666666666667
>     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
>     15.2679766541317, 5.45435605731786, 8.09492296305393, 10.6183802907976,
>     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
> c(25.5382783976218,
>     26.0818978307592, 18.2793849487813, 13.0217855597339, 2.84854798089405,
>     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
> c(29.1283882690448,
>     27.6958799470186, 31.7206150512187, 23.2004366624884, 6.48478535243929,
>     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
> c("Position",
> "AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
> tmp$PosNum <- unclass(tmp$Position)
> tmp
> (labs <- unique(tmp$Position))
> # plot factor levels in seperate panels
> xYplot(Cbind(Sodium,Lower,Upper) ~ PosNum|AltGeo, data=tmp, nx=FALSE,
>   xlim=c(0.5,2.5),
>   ylim=c(min(tmp$Lower)-1,max(tmp$Upper)+1),
>   scales = list(x = list(at=seq(1, 2, by=1), labels = labs)),
>   xlab="Position", ylab="Sodium"
>   )
> 
> ################
> new.back <- trellis.par.get("background")
> new.back$col <- "white"
> newcol <- trellis.par.get("superpose.symbol")
> newcol$col <- c('green4','blue','red','black')
> newcol$pch <- c(16,1,4,8)
> new.line <- trellis.par.get("box.rectangle")
> new.line$col <- 'black'
> trellis.par.set("background", new.back)
> trellis.par.set("superpose.symbol", newcol)
> trellis.par.set("box.rectangle", new.line)
> # Plot factor levels in one graph
> tmp$xvar <- rep(1:4, each=2)+rep(c(-.05,.05), 4)
> xYplot(Cbind(Sodium,Lower,Upper) ~ xvar, groups=Position,  data=tmp,
>   scales=list(y='free',x=list(at=1:4,
>   labels=levels(tmp$AltGeo))),
>   xlab='AltGeo', xlim=c(.5, 4.5),
>   key=list(points=Rows(trellis.par.get("superpose.symbol"),1:2),
>     text=list(lab =as.character(levels(tmp$Position)),
>     col=trellis.par.get("superpose.symbol")$col[1:2]),
>     columns=2, cex=1, title="Position",
>     cex.title=1.1))
> 
> 
> ********************
> 
> 
> Sander Oom wrote:
>> Problem solved!
>>
>> I was so focused on reproducing the plotmeans() functionality with 
>> xYplot() that I completely overlooked the fact that my data does not 
>> allow a x-y plot, as only Sodium is a numeric variable while Position 
>> and AltGeo are factors!
>>
>> Using unclass() to make Position a numeric variable does the trick:
>> tmp$Position <- unclass(tmp$Position)
>>
>> The code below does the trick. Now I only need to figure out how to 
>> tweak the x axis to pretend I am plotting a factor, i.e. plotting 
>> labels "Inside" and "Outside".
>>
>> Cheers,
>>
>> Sander.
>>
>>
>> library(Hmisc)
>> library(Lattice)
>> tmp <-
>> structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
>> 1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
>>     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label = 
>> c("Basalt-High",
>>     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
>>     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
>>     4.66666666666667, 5.55555555555556, 10.6666666666667, 
>> 5.66666666666667
>>     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
>>     15.2679766541317, 5.45435605731786, 8.09492296305393, 
>> 10.6183802907976,
>>     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
>> c(25.5382783976218,
>>     26.0818978307592, 18.2793849487813, 13.0217855597339, 
>> 2.84854798089405,
>>     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
>> c(29.1283882690448,
>>     27.6958799470186, 31.7206150512187, 23.2004366624884, 
>> 6.48478535243929,
>>     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
>> c("Position",
>> "AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
>> "2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
>> tmp$Position <- unclass(tmp$Position)
>> xYplot(Cbind(Sodium,Lower,Upper) ~ Position|AltGeo, groups=AltGeo,
>>   data=tmp, ylim=c(min(tmp$Lower)-1,max(tmp$Upper)+1),
>>   xlab="Position", ylab="Sodium"
>>   )
>>
>>
>>
>> Sander Oom wrote:
>>> Dear all,
>>>
>>> I am trying to plot means and error bars using xYplot, but I get an 
>>> error message from xYplot which I can not figure out:
>>>  > Error in Summary.factor(..., na.rm = na.rm) :
>>>         range not meaningful for factors
>>>
>>> The data frame (tmpNa) was created using aggregate. I have used dump 
>>> to created the code below, which generates the same error.
>>>
>>> Can anybody tell me what is wrong with the data frame?
>>>
>>> Thanks in advance,
>>>
>>> Sander.
>>>
>>> library(Hmisc)
>>> tmpNa <-
>>> structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
>>> 1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
>>>     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label 
>>> = c("Basalt-High",
>>>     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
>>>     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
>>>     4.66666666666667, 5.55555555555556, 10.6666666666667, 
>>> 5.66666666666667
>>>     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
>>>     15.2679766541317, 5.45435605731786, 8.09492296305393, 
>>> 10.6183802907976,
>>>     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
>>> c(25.5382783976218,
>>>     26.0818978307592, 18.2793849487813, 13.0217855597339, 
>>> 2.84854798089405,
>>>     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
>>> c(29.1283882690448,
>>>     27.6958799470186, 31.7206150512187, 23.2004366624884, 
>>> 6.48478535243929,
>>>     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
>>> c("Position",
>>> "AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
>>> "2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
>>> xYplot(Cbind(Sodium,Lower,Upper) ~ AltGeo, groups=Position,  data=tmpNa)
>>>
>>>
>>>  > version
>>>          _
>>> platform i686-pc-linux-gnu
>>> arch     i686
>>> os       linux-gnu
>>> system   i686, linux-gnu
>>> status
>>> major    2
>>> minor    1.0
>>> year     2005
>>> month    04
>>> day      18
>>> language R
>>>
>>
> 

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From deepayan at stat.wisc.edu  Fri May 13 17:13:08 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 13 May 2005 10:13:08 -0500
Subject: [R] Problem with data frame when using xYplot?
In-Reply-To: <4284A69F.8010801@oomvanlieshout.net>
References: <42831D80.5090300@oomvanlieshout.net>
	<428472FC.2090400@oomvanlieshout.net>
	<4284A69F.8010801@oomvanlieshout.net>
Message-ID: <200505131013.09140.deepayan@stat.wisc.edu>

On Friday 13 May 2005 08:07 am, Sander Oom wrote:
> An off list response from Mat Soukop (thanks Mat!!) provides an even
> more elegant solution (see code below)! I have included the original
> code, so people can decide whether to plot in a single panel or in
> multiple panels. Now we have a fully functional workaround to get
> plotmeans{gplots} for multiple factors using lattice! Great!
>
> ********************
>
>
> library(Hmisc)
> library(lattice)
> tmp <-
> structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
> 1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
>      AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label =
> c("Basalt-High",
>      "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
>      Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
>      4.66666666666667, 5.55555555555556, 10.6666666666667, 5.66666666666667
>      ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
>      15.2679766541317, 5.45435605731786, 8.09492296305393,
> 10.6183802907976, 8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9),
> Lower =
> c(25.5382783976218,
>      26.0818978307592, 18.2793849487813, 13.0217855597339,
> 2.84854798089405, 2.85724790120425, 7.12720656973412, 2.97924741723382),
> Upper = c(29.1283882690448,
>      27.6958799470186, 31.7206150512187, 23.2004366624884,
> 6.48478535243929, 8.25386320990686, 14.2061267635992, 8.35408591609952)),
> .Names = c("Position",
> "AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
> tmp$PosNum <- unclass(tmp$Position)
> tmp
> (labs <- unique(tmp$Position))
> # plot factor levels in seperate panels
> xYplot(Cbind(Sodium,Lower,Upper) ~ PosNum|AltGeo, data=tmp, nx=FALSE,
>    xlim=c(0.5,2.5),
>    ylim=c(min(tmp$Lower)-1,max(tmp$Upper)+1),
>    scales = list(x = list(at=seq(1, 2, by=1), labels = labs)),
>    xlab="Position", ylab="Sodium"
>    )

Just out of curiousity, does replacing the 'xlim' and 'scales' arguments above 
by 

xlim = levels(tmp$Position)

do the same thing? It should with xyplot (which also allows the x variable to 
be a factor), but xYplot may be bypassing that.

Deepayan



From khobson at fd9ns01.okladot.state.ok.us  Fri May 13 17:24:29 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Fri, 13 May 2005 10:24:29 -0500
Subject: [R] List and Column Names in a Function?
Message-ID: <OF6320598D.4E430D43-ON86257000.0053D7E4-86257000.00548ACE@fd9ns01.okladot.state.ok.us>





In this simple function, how can I pass strings for index and column names
to the function?  I've posted this type of question before and received no
response.

Maybe this example will be easier to understand and troubleshoot.

ds <- function(myds, vec) {myds[[vec]]*2}

ds1 <- c(X=list(1:10), Y=list(11:20))

ds(get("ds1"),get("Y"))


khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
Oklahoma City, OK  73105-3204



From bioconductor.cn at gmail.com  Fri May 13 17:24:24 2005
From: bioconductor.cn at gmail.com (Xiao Shi)
Date: Fri, 13 May 2005 23:24:24 +0800
Subject: [R] how to generate object name automatically?
Message-ID: <cedaa40b05051308246444914c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050513/69fcf742/attachment.pl

From slist at oomvanlieshout.net  Fri May 13 17:36:35 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 13 May 2005 17:36:35 +0200
Subject: [R] Problem with data frame when using xYplot?
In-Reply-To: <200505131013.09140.deepayan@stat.wisc.edu>
References: <42831D80.5090300@oomvanlieshout.net>	<428472FC.2090400@oomvanlieshout.net>	<4284A69F.8010801@oomvanlieshout.net>
	<200505131013.09140.deepayan@stat.wisc.edu>
Message-ID: <4284C983.1060301@oomvanlieshout.net>

Hi Deepayan!

Deepayan Sarkar wrote:
> On Friday 13 May 2005 08:07 am, Sander Oom wrote:
>>An off list response from Mat Soukop (thanks Mat!!) provides an even
>>more elegant solution (see code below)! I have included the original
>>code, so people can decide whether to plot in a single panel or in
>>multiple panels. Now we have a fully functional workaround to get
>>plotmeans{gplots} for multiple factors using lattice! Great!
>>
> 
> Just out of curiousity, does replacing the 'xlim' and 'scales' arguments above 
> by 
> 
> xlim = levels(tmp$Position)
> 
> do the same thing? It should with xyplot (which also allows the x variable to 
> be a factor), but xYplot may be bypassing that.
> 

You mean: xlim = levels(tmp$AltGeo)....yes it does!? No clue how one 
would ever get comfortable with all these options!


library(Hmisc)
library(lattice)
ltheme <- canonical.theme(color = FALSE)     ## in-built B&W theme
ltheme$strip.background$col <- "transparent" ## change strip bg
lattice.options(default.theme = ltheme)      ## set as default
tmp <-
structure(list(Position = structure(as.integer(c(1, 2, 1, 2,
1, 2, 1, 2)), .Label = c("Inside", "Outside"), class = "factor"),
     AltGeo = structure(as.integer(c(1, 1, 2, 2, 3, 3, 4, 4)), .Label = 
c("Basalt-High",
     "Basalt-Low", "Quartz-High", "Quartz-Low"), class = "factor"),
     Sodium = c(27.3333333333333, 26.8888888888889, 25, 18.1111111111111,
     4.66666666666667, 5.55555555555556, 10.6666666666667, 5.66666666666667
     ), SD = c(5.3851648071345, 2.42097317438899, 20.1618451536560,
     15.2679766541317, 5.45435605731786, 8.09492296305393, 10.6183802907976,
     8.06225774829855), Nobs = c(9, 9, 9, 9, 9, 9, 9, 9), Lower = 
c(25.5382783976218,
     26.0818978307592, 18.2793849487813, 13.0217855597339, 2.84854798089405,
     2.85724790120425, 7.12720656973412, 2.97924741723382), Upper = 
c(29.1283882690448,
     27.6958799470186, 31.7206150512187, 23.2004366624884, 6.48478535243929,
     8.25386320990686, 14.2061267635992, 8.35408591609952)), .Names = 
c("Position",
"AltGeo", "Sodium", "SD", "Nobs", "Lower", "Upper"), row.names = c("1",
"2", "3", "4", "5", "6", "7", "8"), class = "data.frame")
tmp$xvar <- rep(1:4, each=2)+rep(c(-0.1,0.1), 4)
tmp
sp <- list(superpose.symbol = list(pch = c(16,1), cex = 1))
xYplot(Cbind(Sodium,Lower,Upper) ~ xvar, groups=Position,  data=tmp,
   xlim = levels(tmp$AltGeo),
   ylim=c(min(tmp$Lower)-1,max(tmp$Upper)+1),
     xlab='AltGeo', ylab='Sodium',
         panel = function(x, y, type, ...) {
           panel.xYplot(x, y, type="p",...)
           lpoints(x, y, pch=16, col="white", cex=2)
           panel.superpose(x, y, type="p", ...)
         },
         par.settings = sp,
     auto.key=list(columns=1, x=0.7, y=0.8, corner = c(0,0))
     )

> Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From ggrothendieck at gmail.com  Fri May 13 17:41:48 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 May 2005 11:41:48 -0400
Subject: [R] Lowest data level since DateX
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D06B63172@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D06B63172@lbmsg002.fbn-nbf.local>
Message-ID: <971536df05051308413d3c5b8c@mail.gmail.com>

On 5/13/05, Lapointe, Pierre <Pierre.Lapointe at nbf.ca> wrote:
> Hello,
> 
> I'm dealing with financial time series.  I'm trying to find out X in this
> sentence:
> The most recent close is the lowest level since X(date).
> 
> Here's an example of what I'm looking for:
> 
> library(fBasics)
> data(DowJones30)
> tail(DowJones30[,1:5],n=10)
> 
> I need to come up with a vector that would look like this
> 
> AA             AXP          T            ...
> 2000-12-21     2000-12-20   2000-12-29
> 
> i.e. the last date at which the stocks were trading at a lower level than
> the most recent closing.
> 
> I know it has to do with min/max, pmin/pmax, cummin/cummax or rev(), but I
> can't figure it out.

The following returns the last index whose value is less than the
last entry in vector x (or numeric(0) if none):

> x <- c(1,4,2,6,3)
> tail(which(x < tail(x,1)),1)
[1] 3
> x <- c(1,4,2,6,.5)
> tail(which(x < tail(x,1)),1)
numeric(0)



From bates at stat.wisc.edu  Fri May 13 17:41:56 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 13 May 2005 10:41:56 -0500
Subject: [R] List and Column Names in a Function?
In-Reply-To: <OF6320598D.4E430D43-ON86257000.0053D7E4-86257000.00548ACE@fd9ns01.okladot.state.ok.us>
References: <OF6320598D.4E430D43-ON86257000.0053D7E4-86257000.00548ACE@fd9ns01.okladot.state.ok.us>
Message-ID: <4284CAC4.8010109@stat.wisc.edu>

khobson at fd9ns01.okladot.state.ok.us wrote:
> 
> 
> 
> In this simple function, how can I pass strings for index and column names
> to the function?  I've posted this type of question before and received no
> response.
> 
> Maybe this example will be easier to understand and troubleshoot.
> 
> ds <- function(myds, vec) {myds[[vec]]*2}
> 
> ds1 <- c(X=list(1:10), Y=list(11:20))
> 
> ds(get("ds1"),get("Y"))

You are overusing the get function.  I think you can do what you want as

ds(ds1, "Y")



From lzhtom at hotmail.com  Fri May 13 17:43:20 2005
From: lzhtom at hotmail.com (zhihua li)
Date: Fri, 13 May 2005 15:43:20 +0000
Subject: [R] manipulating dataframe according to the values of some columns
Message-ID: <BAY12-F1DB6713F2DA8CB331A000C7120@phx.gbl>

hi netters,

I'm a newbie to R and there are some very simple problems puzzeled me for 
two days.

I've a dataframe here with several columns different in modes. Two of the 
columns are special for me: column 1 has the mode "factor" and column 2 has 
the mode "numeric vectors".
The values for column 1 are either "T" or "F". I wanna do two things:
Firstly, remove those rows whose values for column 1 are "F";
Secondly,sort the rows in the ascending order of values for column 2.

I believe the code to do these things is simple. But I can't figure it out. 
Please help me!

Thanks a lot!



From deepayan at stat.wisc.edu  Fri May 13 17:45:06 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 13 May 2005 10:45:06 -0500
Subject: [R] Lattice plot within a "for" loop does not happen?
In-Reply-To: <4284B884.5050005@lancaster.ac.uk>
References: <B88F80B7-BB1C-4334-8FBC-B2C1C402E46C@wm.edu>
	<4284B884.5050005@lancaster.ac.uk>
Message-ID: <200505131045.06736.deepayan@stat.wisc.edu>

On Friday 13 May 2005 09:24 am, Barry Rowlingson wrote:
> > BUT, when I stick this in a loop, I get a bunch of blank graphics
> > devices. This happens even if the loop only executes once. I could  just
> > go through and do these one by one, but I was curious if I was
> > overlooking something obvious. Thank you for any advice.
>
>   You're overlooking something like line 800 of the documentation for
> xyplot:

As well as the much much shorter help(Lattice), which has: 

Note:

     High level Lattice functions (like 'xyplot') are different from
     conventional S graphics functions because they don't actually draw
     anything. Instead, they return an object of class ``trellis''
     which has to be then 'print'ed. This often causes confusion when
     the high level functions are called inside another function (most
     often 'source') and hence don't produce any output.

This page is pointed to from every conceivable place, including the 
Description, which says:

Description:   Implementation of Trellis Graphics. See ?Lattice for a
               brief introduction


> [...]
>
>   So wrap your xyplot call in a print() function inside your loop:
>
>   for(i in 1:10){
>     print(xyplot(....whatever....))
>   }
>
>   Its probably in the R-FAQ as well, since my original feeling was that
> this behaviour was chosen in order to confuse people and see how many
> people read the FAQ... :)

No comments on that :)

However, let's say I want to use pseudo-random numbers to study the behaviour 
of sample correlation in uncorrelated observations. To this end, I do:

> cor(rnorm(10), rnorm(10))
[1] 0.3899596

I do it a few more times:

> cor(rnorm(10), rnorm(10))
[1] 0.6481215
> cor(rnorm(10), rnorm(10))
[1] -0.02100718
> cor(rnorm(10), rnorm(10))
[1] -0.01141006

but then I get tired and try:

> for (i in 1:10) {
+   cor(rnorm(10), rnorm(10))
+ }
>
>

resulting in nothing!!! 

Strange how no one ever (or at least any more) complains about this behavour, 
which should be exactly as ``confusing''. 

The upshot, of course, can be summarized by a now famous observation made 
slightly more than a decade ago:

http://groups-beta.google.com/group/comp.sys.next.advocacy/msg/92532d5651e795dc

Deepayan



From deepayan at stat.wisc.edu  Fri May 13 17:51:57 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 13 May 2005 10:51:57 -0500
Subject: [R] Problem with data frame when using xYplot?
In-Reply-To: <4284C983.1060301@oomvanlieshout.net>
References: <42831D80.5090300@oomvanlieshout.net>
	<200505131013.09140.deepayan@stat.wisc.edu>
	<4284C983.1060301@oomvanlieshout.net>
Message-ID: <200505131051.58023.deepayan@stat.wisc.edu>

On Friday 13 May 2005 10:36 am, Sander Oom wrote:
> Hi Deepayan!
>
> Deepayan Sarkar wrote:
> > On Friday 13 May 2005 08:07 am, Sander Oom wrote:
> >>An off list response from Mat Soukop (thanks Mat!!) provides an even
> >>more elegant solution (see code below)! I have included the original
> >>code, so people can decide whether to plot in a single panel or in
> >>multiple panels. Now we have a fully functional workaround to get
> >>plotmeans{gplots} for multiple factors using lattice! Great!
> >
> > Just out of curiousity, does replacing the 'xlim' and 'scales' arguments
> > above by
> >
> > xlim = levels(tmp$Position)
> >
> > do the same thing? It should with xyplot (which also allows the x
> > variable to be a factor), but xYplot may be bypassing that.
>
> You mean: xlim = levels(tmp$AltGeo)....yes it does!? 

No, I meant exactly what I wrote, and my comment followed this piece of code 
(which you have deleted from your reply):

---------
tmp$PosNum <- unclass(tmp$Position)
tmp
(labs <- unique(tmp$Position))
# plot factor levels in seperate panels
xYplot(Cbind(Sodium,Lower,Upper) ~ PosNum|AltGeo, data=tmp, nx=FALSE,
   xlim=c(0.5,2.5),
   ylim=c(min(tmp$Lower)-1,max(tmp$Upper)+1),
   scales = list(x = list(at=seq(1, 2, by=1), labels = labs)),
   xlab="Position", ylab="Sodium"
   )
----------

> No clue how one would ever get comfortable with all these options!

By reading the manual, of course :-)

Deepayan



From stecalza at tiscali.it  Fri May 13 18:14:16 2005
From: stecalza at tiscali.it (Stefano Calza)
Date: Fri, 13 May 2005 18:14:16 +0200
Subject: [R] how to generate object name automatically?
In-Reply-To: <cedaa40b05051308246444914c@mail.gmail.com>
References: <cedaa40b05051308246444914c@mail.gmail.com>
Message-ID: <20050513161416.GD7857@med.unibs.it>

What about assign?

assign(raw.labs[1],123)

maybe you're thinking about something like this

(raw.values = vector of values)

for(i in 1:length(raw.labs))
  assign(raw.labs[i],raw.values[i])


Not elegant but works

Stefano


On Fri, May 13, 2005 at 11:24:24PM +0800, Xiao Shi wrote:
<Xiao>Hi everybody,
<Xiao>I have a lable vector ,
<Xiao>raw.labs= paste("file1", 1:20, sep = "")
<Xiao>And i can i make the content of raw.labs to be a object name.
<Xiao>eg ,file1=123(use some function on raw.labs to generate the name file1)
<Xiao>Thanks!
<Xiao>
<Xiao>	[[alternative HTML version deleted]]
<Xiao>
<Xiao>______________________________________________
<Xiao>R-help at stat.math.ethz.ch mailing list
<Xiao>https://stat.ethz.ch/mailman/listinfo/r-help
<Xiao>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Fri May 13 18:14:50 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 13 May 2005 12:14:50 -0400
Subject: [R] how to generate object name automatically?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E831@usctmx1106.merck.com>

The obvious way is to use assign().  The possibly better way is to store the
objects in a list.

Andy

> From: Xiao Shi
> 
> Hi everybody,
> I have a lable vector ,
> raw.labs= paste("file1", 1:20, sep = "")
> And i can i make the content of raw.labs to be a object name.
> eg ,file1=123(use some function on raw.labs to generate the 
> name file1)
> Thanks!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From khobson at fd9ns01.okladot.state.ok.us  Fri May 13 19:01:08 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Fri, 13 May 2005 12:01:08 -0500
Subject: [R] Re: Re: List and Column Names in a Function?
Message-ID: <OFEAFA8582.BCD6EBE9-ON86257000.005D4F32-86257000.005D6415@fd9ns01.okladot.state.ok.us>





The solution that Douglas proposed does not work.  Any other ideas?

> In this simple function, how can I pass strings for index and column
names
> to the function?  I've posted this type of question before and received
no
> response.
>
> Maybe this example will be easier to understand and troubleshoot.
>
> ds <- function(myds, vec) {myds[[vec]]*2}
>
> ds1 <- c(X=list(1:10), Y=list(11:20))
>
> ds(get("ds1"),get("Y"))

You are overusing the get function.  I think you can do what you want as

ds(ds1, "Y")


khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager



From bates at stat.wisc.edu  Fri May 13 19:14:21 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 13 May 2005 12:14:21 -0500
Subject: [R] Re: Re: List and Column Names in a Function?
In-Reply-To: <OFEAFA8582.BCD6EBE9-ON86257000.005D4F32-86257000.005D6415@fd9ns01.okladot.state.ok.us>
References: <OFEAFA8582.BCD6EBE9-ON86257000.005D4F32-86257000.005D6415@fd9ns01.okladot.state.ok.us>
Message-ID: <4284E06D.8060009@stat.wisc.edu>

khobson at fd9ns01.okladot.state.ok.us wrote:
> 
> 
> 
> The solution that Douglas proposed does not work.  Any other ideas?

In what way does it not work?  I get

> ds <- function(myds, vec) {myds[[vec]]*2}
> ds1 <- c(X=list(1:10), Y=list(11:20))
> ds(ds1, "Y")
 [1] 22 24 26 28 30 32 34 36 38 40

Isn't that what you wanted?



From andy_liaw at merck.com  Fri May 13 19:17:25 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 13 May 2005 13:17:25 -0400
Subject: [R] Re: Re: List and Column Names in a Function?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E834@usctmx1106.merck.com>

> From: khobson at fd9ns01.okladot.state.ok.us
> 
> The solution that Douglas proposed does not work.  Any other ideas?

Then perhaps you could (re-)read the posting guide, and give us more
information on what you mean by "does not work", and exactly what you are
expecting?  Isn't this what you want?


> ds1 <- c(X=list(1:10), Y=list(11:20))
> ds <- function(myds, vec) myds[[vec]] * 2
> ds(ds1, "Y")
 [1] 22 24 26 28 30 32 34 36 38 40

Andy

 
> > In this simple function, how can I pass strings for index and column
> names
> > to the function?  I've posted this type of question before 
> and received
> no
> > response.
> >
> > Maybe this example will be easier to understand and troubleshoot.
> >
> > ds <- function(myds, vec) {myds[[vec]]*2}
> >
> > ds1 <- c(X=list(1:10), Y=list(11:20))
> >
> > ds(get("ds1"),get("Y"))
> 
> You are overusing the get function.  I think you can do what 
> you want as
> 
> ds(ds1, "Y")
> 
> 
> khobson at odot.org
> Kenneth Ray Hobson, P.E.
> Oklahoma DOT - QA & IAS Manager
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From Eric.Archer at noaa.gov  Fri May 13 19:20:35 2005
From: Eric.Archer at noaa.gov (Eric Archer)
Date: Fri, 13 May 2005 10:20:35 -0700
Subject: [R] randomForest partialPlot x.var through function
Message-ID: <4284E1E3.4020204@noaa.gov>

All,

I'm trying to set up a function which calls the partialPlot function but 
am getting an error that I can't seem to solve.  Here's a simplified 
version of the function and error...

 > pplot <- 
function(rf,pred.var){partialPlot(x=rf,pred.data=acoust,x.var=pred.var)}
 >
 > attach(acoust)
 > acoust.rf <- 
randomForest(VocalTF~Cruise+Spot+Spin+Delph+Stripe+Steno+Turs+Gramp+Lags+
+       Lisso+FerPsu+Glob+mixspotspin+mixother+mixed+Size,
+       data=acoust,importance=TRUE,keep.forest=TRUE)
 > cruise.pp <- pplot(acoust.rf,Cruise)
Error in "[.data.frame"(pred.data, , xname) :
        undefined columns selected

Here's the traceback call...

 > traceback()
6: stop("undefined columns selected")
5: "[.data.frame"(pred.data, , xname)
4: pred.data[, xname]
3: partialPlot.randomForest(x = rf, pred.data = acoust, x.var = pred.var)
2: partialPlot(x = rf, pred.data = acoust, x.var = pred.var)
1: pplot(acoust.rf, Cruise)

 From the partialPlot help file, the "pred.data[, xname]" seems to apply 
to the "n.pt" argument which reads, "if 'x.var' is continuous, the 
number of points on the grid for evaluating partial dependence."

It is not clear to me what "xname" is or where it is defined.  If it is 
supposed to be "x.var" where does it get assigned?  I've tried 
help.search("xname") to no useful avail that I could tell.
Second, in my case, "acoust$Cruise" is a factor and the line

 > cruise.pp <- partialPlot(acoust.rf,acoust,Cruise)

produces the proper plot.

Which leads me to belive that I'm doing something wrong in how I'm 
passing "Cruise" through "pred.var" in the function call to "x.var", but 
I can't figure out how to properly correct it.

Thanks in advance for any pointers.

e.

-- 

Eric Archer, Ph.D.
NOAA-SWFSC
8604 La Jolla Shores Dr.
La Jolla, CA 92037
858-546-7121,7003(FAX)
eric.archer at noaa.gov


"Lighthouses are more helpful than churches."
    - Benjamin Franklin

"Cogita tute" - Think for yourself



From slist at oomvanlieshout.net  Fri May 13 19:41:51 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 13 May 2005 19:41:51 +0200
Subject: [R] Problem with data frame when using xYplot?
In-Reply-To: <200505131051.58023.deepayan@stat.wisc.edu>
References: <42831D80.5090300@oomvanlieshout.net>	<200505131013.09140.deepayan@stat.wisc.edu>	<4284C983.1060301@oomvanlieshout.net>
	<200505131051.58023.deepayan@stat.wisc.edu>
Message-ID: <4284E6DF.6080205@oomvanlieshout.net>

Sorry Deepayan, I forgot that the code moved on while you send your 
reply. Below the simplified version using your suggestion and this time 
based on the generic data used in the xYplot manual!

Maybe this example can be included in the manual, so next time people 
will find the answer there. Or better still, you could provide a high 
level function in 'lattice'. ;-)

dfr <- expand.grid(month=1:12, continent=c('Europe','USA'),
                    sex=c('female','male'))
set.seed(1)
dfr$y <- dfr$month/10 + 1*(dfr$sex=='female') +
   2*(dfr$continent=='Europe') + runif(48,-.15,.15)
dfr
dfs <- summarize(dfr$y, llist(dfr$continent,dfr$sex),
   smean.cl.normal)
labs <- unique(dfs$continent)
colnames(dfs) <- list("continent","sex","y","Lower","Upper")
dfs$sexnum <- unclass(dfs$sex)
dfs
xYplot(Cbind(y,Lower,Upper) ~ sexnum|continent, data=dfs, nx=FALSE,
   xlim=levels(dfs$sex),
   ylim=c(min(dfs$Lower)-1,max(dfs$Upper)+1),
   )

dfs$xvar <- rep(1:2, each=2)+rep(c(-0.1,0.1), 2)
dfs
sp <- list(superpose.symbol = list(pch = c(16,1), cex = 1),
           superpose.line = list(col = "grey", lty = 1))
xYplot(Cbind(y,Lower,Upper) ~ xvar, groups=sex,  data=dfs,
   xlim= levels(dfs$continent),
   ylim= c(min(dfs$Lower)-1,max(dfs$Upper)+1),
   xlab="Continent",
   panel=function(x, y, type, ...) {
     panel.xYplot(x, y, type="p",...)
     lpoints(x, y, pch=16, col="white", cex=2)
     panel.superpose(x, y, type="p", ...)
   },
   par.settings= sp,
   auto.key= list(columns=1, x=0.7, y=0.8, corner = c(0,0))
   )


Deepayan Sarkar wrote:
> On Friday 13 May 2005 10:36 am, Sander Oom wrote:
>>Hi Deepayan!
>>
>>Deepayan Sarkar wrote:
>>>On Friday 13 May 2005 08:07 am, Sander Oom wrote:
>>>>An off list response from Mat Soukop (thanks Mat!!) provides an even
>>>>more elegant solution (see code below)! I have included the original
>>>>code, so people can decide whether to plot in a single panel or in
>>>>multiple panels. Now we have a fully functional workaround to get
>>>>plotmeans{gplots} for multiple factors using lattice! Great!
>>>Just out of curiousity, does replacing the 'xlim' and 'scales' arguments
>>>above by
>>>
>>>xlim = levels(tmp$Position)
>>>
>>>do the same thing? It should with xyplot (which also allows the x
>>>variable to be a factor), but xYplot may be bypassing that.
>>You mean: xlim = levels(tmp$AltGeo)....yes it does!? 
> 
> No, I meant exactly what I wrote, and my comment followed this piece of code 
> (which you have deleted from your reply):
> 
> ---------
> tmp$PosNum <- unclass(tmp$Position)
> tmp
> (labs <- unique(tmp$Position))
> # plot factor levels in seperate panels
> xYplot(Cbind(Sodium,Lower,Upper) ~ PosNum|AltGeo, data=tmp, nx=FALSE,
>    xlim=c(0.5,2.5),
>    ylim=c(min(tmp$Lower)-1,max(tmp$Upper)+1),
>    scales = list(x = list(at=seq(1, 2, by=1), labels = labs)),
>    xlab="Position", ylab="Sodium"
>    )
> ----------
> 
>>No clue how one would ever get comfortable with all these options!
> 
> By reading the manual, of course :-)
> 
> Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From khobson at fd9ns01.okladot.state.ok.us  Fri May 13 19:47:30 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Fri, 13 May 2005 12:47:30 -0500
Subject: [R] Re: Re: List and Column Names in a Function?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E834@usctmx1106.merck.com>
Message-ID: <OF5A0D09D7.530ACB59-ON86257000.005F5FC9-86257000.0061A2F2@fd9ns01.okladot.state.ok.us>





The R-Help replies inspired me to try ds(get("ds1"), "Y") which solves my
problem.

What I wanted was to pass string values for both the list name and the
column name.  I had tried several methods before posting.  Thanks for the
replies.

I incorrectly stated that Douglas Grove's solution would not work.  (I had
a typo when I tried it).  It works as he specified but not as I required.
The results below are the expected results which I probably should have
shown to be more clear.   It did get me to a solution though.

I would not post questions if I had not read the posting guide.   I spent
lots of time viewing the R-Help archives prior to posting as well.  Two
other posts of mine had no replies.  If you see those, there is no need to
reply.  With this more simplified example and your responses, I'm now good
to go.


                                                                           
                                                                           
                                                                           
                                                                           




> From: khobson at fd9ns01.okladot.state.ok.us
>
> The solution that Douglas proposed does not work.  Any other ideas?

Then perhaps you could (re-)read the posting guide, and give us more
information on what you mean by "does not work", and exactly what you are
expecting?  Isn't this what you want?


> ds1 <- c(X=list(1:10), Y=list(11:20))
> ds <- function(myds, vec) myds[[vec]] * 2
> ds(ds1, "Y")
 [1] 22 24 26 28 30 32 34 36 38 40



From uofiowa at gmail.com  Fri May 13 19:57:25 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 13 May 2005 13:57:25 -0400
Subject: [R] where is aggregateSeries
Message-ID: <3f87cc6d050513105751981063@mail.gmail.com>

What package is aggregateSeries in?
It is referred to in the fCalendar document but I do not see it in the package.



From subianto at gmail.com  Fri May 13 20:40:05 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Fri, 13 May 2005 20:40:05 +0200
Subject: [R] How to convert color to black & white
Message-ID: <4284F485.10702@gmail.com>

Dear all,
Could someone please explain to me how to convert color to black & white.
For example:
barplot(1:5,col = rainbow(5))
Because I need to print my plot to save my ink color printer.
I don't want to convert to grayscale, but keep it as an RGB.
I  would be very happy if anyone could help me.
Thank you very much in advance.

Kindly regards,
Muhammad Subianto



From f.harrell at vanderbilt.edu  Fri May 13 20:42:35 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 13 May 2005 13:42:35 -0500
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <42849BC4.9030108@oomvanlieshout.net>
References: <42849BC4.9030108@oomvanlieshout.net>
Message-ID: <4284F51B.4040900@vanderbilt.edu>

Sander Oom wrote:
> Dear R users,
> 
> The Sweave code below runs fine, as it is. However, an error occurs when 
> the line 'library(xtable)' is uncommented:
> Error:  chunk 1
> Error in "label<-"(`*tmp*`, value = "month") :
>         no applicable method for "label<-"
> 
> Is anybody aware of this and knows a workaround?
> 
> Thanks,
> 
> Sander.
> 
> *******************
> 
> \documentclass[a4paper]{article}
> \title{Sweave Test for summarize}
> \author{Sander Oom}
> 
> \usepackage{a4wide}
> 
> \begin{document}
> 
> \maketitle
> 
> \begin{figure}[ht]
> \begin{center}
> <<fig=TRUE,echo=FALSE>>=
>   # library(xtable)
>   library(Hmisc)
>   set.seed(111)
>   dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
>   month <- dfr$month
>   year <- dfr$year
>   y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
>   s <- summarize(y, llist(month,year), smedian.hilow, conf.int=.5)
>   print(xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s,
>         keys='lines', method='alt', type='b'))
> @
> \end{center}
> \end{figure}
> 
> \end{document}
> 
> ************************
> 
> 
> 
>  > version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
> 
> 

I feel this is an xtable problem because Hmisc has being using label and 
label<- since 1991.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From bdiaz10 at gmail.com  Fri May 13 20:55:17 2005
From: bdiaz10 at gmail.com (Barbara Diaz)
Date: Fri, 13 May 2005 11:55:17 -0700
Subject: [R] cluster results using fanny
In-Reply-To: <3593943005051311495df60b87@mail.gmail.com>
References: <3593943005051311495df60b87@mail.gmail.com>
Message-ID: <3593943005051311551c61010c@mail.gmail.com>

Hi,

I am using fanny and I have estrange results. I am wondering if
someone out there can help me understand why this happens.

First of all in most of my tries, it gives me a result in which each
object has equal membership in all clusters. I have read that that
means "the clustering is entirely fuzzy". Looking at the graphics it
is really difficult to understand how objects with so different scores
for the variables have the same membership for all the clusters.

I also find estrange the fact that if I set K=3 (three clusters), it
gives membership for all three clusters (0.333 for all of them) and
then when it gives the closest hard clustering they only belong to
cluster 1 or 2, but none of them to cluster three. The plot shows only
two clusters (also the silhouette plot, even if it gives in the
"silhouette plot information" the silhouette width for the three
clusters????????.

Then, for the same data I set k=4 and surprisingly, it gives
membership for the four of them (this time they are not all the same)
and when it gives the closest hard clustering they only belong to
cluster 1, 2, or 3 but none of them to cluster 4. The plot shows only
three clusters (also the silhouette plot, even if it gives in the
"silhouette plot information" the silhouette width for the four
clusters????????. why didn't it give this three clusters when I set
k=3??????

For k=5 it gives all the information and then it only plots 2 clusters.

This is very confusing. Also, if there is equal membership for all the
clusters, how is it that I have a "closest hard clustering"? and a
"neighbor"?

Thank you in advance,

Barbara



From slist at oomvanlieshout.net  Fri May 13 21:39:38 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 13 May 2005 21:39:38 +0200
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <4284F51B.4040900@vanderbilt.edu>
References: <42849BC4.9030108@oomvanlieshout.net>
	<4284F51B.4040900@vanderbilt.edu>
Message-ID: <4285027A.2040208@oomvanlieshout.net>

Dear Frank,

I have a Sweave document in which I export anova (aov) tables to Latex 
and calculate some summary statistics with summarize{Hmisc} for a graph 
(as in the example below).

I currently use the following code for the aov tables:
<<results=tex>>=
   tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
   tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut , data=tmp)
   tmpTable <- xtable (tmpAov ,
     caption="ANOVA table for vegetation height.",
     label="tab:AnovaHeight"
     )
   print.xtable(tmpTable, type="latex", floating=TRUE,
     table.placement="ht", caption.placement="top",
     latex.environments=c("center"))
     )
@

I used xtables, because it has a working aov example. I would be happy 
to use an alternative if I knew how! Would you have sample code to 
illustrate how to export an aov table to Latex using latex{Hmisc}.

Thanks very much for your help,

Sander.

Frank E Harrell Jr wrote:
> Sander Oom wrote:
>> Dear R users,
>>
>> The Sweave code below runs fine, as it is. However, an error occurs 
>> when the line 'library(xtable)' is uncommented:
>> Error:  chunk 1
>> Error in "label<-"(`*tmp*`, value = "month") :
>>         no applicable method for "label<-"
>>
>> Is anybody aware of this and knows a workaround?
>>
>> Thanks,
>>
>> Sander.
>>
>> *******************
>>
>> \documentclass[a4paper]{article}
>> \title{Sweave Test for summarize}
>> \author{Sander Oom}
>>
>> \usepackage{a4wide}
>>
>> \begin{document}
>>
>> \maketitle
>>
>> \begin{figure}[ht]
>> \begin{center}
>> <<fig=TRUE,echo=FALSE>>=
>>   # library(xtable)
>>   library(Hmisc)
>>   set.seed(111)
>>   dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
>>   month <- dfr$month
>>   year <- dfr$year
>>   y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
>>   s <- summarize(y, llist(month,year), smedian.hilow, conf.int=.5)
>>   print(xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s,
>>         keys='lines', method='alt', type='b'))
>> @
>> \end{center}
>> \end{figure}
>>
>> \end{document}
>>
>> ************************
>>
>>
>>
>>  > version
>>          _
>> platform i686-pc-linux-gnu
>> arch     i686
>> os       linux-gnu
>> system   i686, linux-gnu
>> status
>> major    2
>> minor    1.0
>> year     2005
>> month    04
>> day      18
>> language R
>>
>>
> 
> I feel this is an xtable problem because Hmisc has being using label and 
> label<- since 1991.
> 
> Frank
> 

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From gerifalte28 at hotmail.com  Fri May 13 21:41:07 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 13 May 2005 19:41:07 +0000
Subject: [R] manipulating dataframe according to the values of some columns
In-Reply-To: <BAY12-F1DB6713F2DA8CB331A000C7120@phx.gbl>
Message-ID: <BAY103-F19BB6F347B4FCFD65CE696A6120@phx.gbl>

Hi Zhihua

Try the following:

dat=data.frame(x=rep(c("T","F"),10),y=(runif(20)))#Creates data frame like 
in your example
newdat=dat[dat$x=="T",] #includes only rows with variable x equal to "T"
newdat=newdat[order(newdat[,"y"], decreasing=FALSE),]# sorts in ascending 
order the newdat #data by the values of y.  Notice that the default is 
order(decreasing=FALSE) but I added that argument so you can see that you 
can also sort descending.

Another alternative to the second line of code is to use the higher level 
function subset() i.e.:
newdat=subset(dat, x=="T",select=c(x,y))#again, the select argument is 
optional in this example but I added it so you can see how you can select 
specific coumns for your subset.

I hope that this helps

Francisco


>From: "zhihua li" <lzhtom at hotmail.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] manipulating dataframe according to the values of some columns
>Date: Fri, 13 May 2005 15:43:20 +0000
>
>hi netters,
>
>I'm a newbie to R and there are some very simple problems puzzeled me for 
>two days.
>
>I've a dataframe here with several columns different in modes. Two of the 
>columns are special for me: column 1 has the mode "factor" and column 2 has 
>the mode "numeric vectors".
>The values for column 1 are either "T" or "F". I wanna do two things:
>Firstly, remove those rows whose values for column 1 are "F";
>Secondly,sort the rows in the ascending order of values for column 2.
>
>I believe the code to do these things is simple. But I can't figure it out. 
>Please help me!
>
>Thanks a lot!
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From uofiowa at gmail.com  Fri May 13 22:58:42 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 13 May 2005 16:58:42 -0400
Subject: [R] without the loop
Message-ID: <3f87cc6d05051313583a2674ee@mail.gmail.com>

Can this be re-implemented to run faster (without the loop) ? 

r <- list()
n = nrow(prices)
        for (i in (w+1):n) {                   
                window <- prices[(i-w):(i-1),]                              
                if (prices[i,]$settle > max(window$high)) r <-
append(r,  1)
                else if (prices[i,]$settle < min(window$low)) r <-
append(r, -1)
        }



From rlee at fpcc.net  Fri May 13 23:19:33 2005
From: rlee at fpcc.net (rlee@fpcc.net)
Date: Fri, 13 May 2005 15:19:33 -0600 (MDT)
Subject: [R] A model package that uses S4 methods and name spaces?
Message-ID: <33162.136.177.22.105.1116019173.squirrel@webmail.fpcc.net>

I'd like to try and rewrite my package to take advantage of S4 methods and
name spaces.  I am presuming that S4 methods and name spaces will help me
write and maintain my package.

Can someone suggest a "model" package that uses best practices for mixing
S3, S4 methods and name spaces?  Preferably a package with multiple R
source files.

-L



From bates at stat.wisc.edu  Fri May 13 23:26:17 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 13 May 2005 16:26:17 -0500
Subject: [R] A model package that uses S4 methods and name spaces?
In-Reply-To: <33162.136.177.22.105.1116019173.squirrel@webmail.fpcc.net>
References: <33162.136.177.22.105.1116019173.squirrel@webmail.fpcc.net>
Message-ID: <42851B79.1060809@stat.wisc.edu>

rlee at fpcc.net wrote:
> I'd like to try and rewrite my package to take advantage of S4 methods and
> name spaces.  I am presuming that S4 methods and name spaces will help me
> write and maintain my package.
> 
> Can someone suggest a "model" package that uses best practices for mixing
> S3, S4 methods and name spaces?  Preferably a package with multiple R
> source files.

It doesn't mix S3 and S4 but the Matrix package is a large package based
on S4 that does show the use of S4 and a NAMESPACE.  On a smaller scale,
the lme4 package also uses S4 and a NAMESPACE.



From amir36060 at yahoo.de  Fri May 13 23:37:01 2005
From: amir36060 at yahoo.de (Amir Safari)
Date: Fri, 13 May 2005 23:37:01 +0200 (CEST)
Subject: [R] clustering
Message-ID: <20050513213701.49971.qmail@web26907.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050513/4954bdf7/attachment.pl

From GPetris at uark.edu  Fri May 13 23:51:47 2005
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 13 May 2005 16:51:47 -0500 (CDT)
Subject: [R] without the loop
In-Reply-To: <3f87cc6d05051313583a2674ee@mail.gmail.com> (message from Omar
	Lakkis on Fri, 13 May 2005 16:58:42 -0400)
References: <3f87cc6d05051313583a2674ee@mail.gmail.com>
Message-ID: <200505132151.j4DLplox000263@definetti.uark.edu>


I won't go into the details of your loop, but in general it is usually
better, if possible, to create a list of the appropriate length
upfront.

Giovanni

> Date: Fri, 13 May 2005 16:58:42 -0400
> From: Omar Lakkis <uofiowa at gmail.com>
> Sender: r-help-bounces at stat.math.ethz.ch
> Precedence: list
> DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=beta; d=gmail.com;
> 
> Can this be re-implemented to run faster (without the loop) ? 
> 
> r <- list()
> n = nrow(prices)
>         for (i in (w+1):n) {                   
>                 window <- prices[(i-w):(i-1),]                              
>                 if (prices[i,]$settle > max(window$high)) r <-
> append(r,  1)
>                 else if (prices[i,]$settle < min(window$low)) r <-
> append(r, -1)
>         }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From miner at u.washington.edu  Sat May 14 00:27:44 2005
From: miner at u.washington.edu (Brooks Miner)
Date: Fri, 13 May 2005 15:27:44 -0700
Subject: [R] multinom(): likelihood of model?
Message-ID: <22461bafa53c7852793779dffa838391@u.washington.edu>

Hi all,

I'm working on a multinomial (or "polytomous") logistic regression 
using R and have made great progress using multinom() from the nnet 
library.  My response variable has three categories, and there are two 
different possible predictors.  I'd like to use the likelihoods of 
certain models (ie, saturated, fitteds, and null) to calculate 
Nagelkerke R-squared values for various fitted models.

My question today is simple: once I have fitted a model using 
multinom(), how do I find the likelihood (or log likelihood) of my 
fitted model?  I understand that this value must be part of the 
$deviance or $AIC components of the fitted model, but my understanding 
is too limited at this point for me to know how to calculate the 
likelihood of my fitted model from either of these outputs.

Thanks in advance to any assistance offered.  I'd be happy to provide 
an example of my data and multinom() entries if that would help.

Gratefully,

- Brooks
----------------------------
Brooks Miner
Research Scientist
Laird Lab
UW Biology
206.616.9385
http://protist.biology.washington.edu/Lairdlab/



From ggrothendieck at gmail.com  Sat May 14 00:35:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 May 2005 18:35:55 -0400
Subject: [R] without the loop
In-Reply-To: <3f87cc6d05051313583a2674ee@mail.gmail.com>
References: <3f87cc6d05051313583a2674ee@mail.gmail.com>
Message-ID: <971536df05051315354a13faac@mail.gmail.com>

On 5/13/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> Can this be re-implemented to run faster (without the loop) ?
> 
> r <- list()
> n = nrow(prices)
>        for (i in (w+1):n) {
>                window <- prices[(i-w):(i-1),]
>                if (prices[i,]$settle > max(window$high)) r <-
> append(r,  1)
>                else if (prices[i,]$settle < min(window$low)) r <-
> append(r, -1)
>        }
> 

Given the complex looping it would be better if you provided
documentation with your post and a reproducible example, not
just a code snippet.  See the posting guide.

At any rate, it seems that what you want to do is to append 1
whenever the settle price exceeds the high of the last w
time points and a -1 whenever the settle price is below the low of
the last w time points.

Represent the prices as a zoo series with 3 columns: 
high, low, settle and use the following (untested) loop-free
code:

high <- 1; low <- 2; settle <- 3
W <- w+1
r <- rapply(prices, W, function(x) 
	sign(x[W,settle] > max(x[-W,high])) - (x[W,settle] < min(x[-W,low])),
	by.column = FALSE, align = "right")
)
r[r!=0]



From slusek at o2.pl  Sat May 14 00:39:06 2005
From: slusek at o2.pl (Wojtek Slusarski)
Date: Sat, 14 May 2005 00:39:06 +0200
Subject: [R] problem with as.timeSeries()
Message-ID: <42852C8A.1060600@o2.pl>

The matrix I want to convert to timeSeries object looks:

                       rp
2003-06-30 -1.0995685137
2003-07-01 -0.7065834677
2003-07-02  0.7661757181
  and so on...

In help it is stated that I should use as.timeSeries function like:

as.timeSeries(x, dimnames = TRUE, format = "")

So I try:
 > ts.rp = as.timeSeries(rp, dimnames=TRUE, format="%Y-%m-%d")
Error in "colnames<-"(`*tmp*`, value = character(0)) :
         attempt to set colnames on object with less than two dimensions

I don't exactly understand the error as:

 > dim(rp)
[1] 249   1

What am I doing wrong?

Best regards,
Wojtek



From ggrothendieck at gmail.com  Sat May 14 00:53:23 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 May 2005 18:53:23 -0400
Subject: [R] where is aggregateSeries
In-Reply-To: <3f87cc6d050513105751981063@mail.gmail.com>
References: <3f87cc6d050513105751981063@mail.gmail.com>
Message-ID: <971536df050513155377779043@mail.gmail.com>

On 5/13/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> What package is aggregateSeries in?
> It is referred to in the fCalendar document but I do not see it in the package.

Don't know but aggregate is available in R with a variety of methods
and the zoo package also supplies a zoo method.



From spencer.graves at pdf.com  Sat May 14 01:57:40 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 13 May 2005 16:57:40 -0700
Subject: [R] Big matrix memory problem
In-Reply-To: <971536df05051306074d47e52d@mail.gmail.com>
References: <42831846@webmail.ucc.ie>
	<971536df05051306074d47e52d@mail.gmail.com>
Message-ID: <42853EF4.4030804@pdf.com>

	  S-Plus 7 advertises facilities for large data sets 
(http://www.insightful.com/products/splus/default.asp#largedata).  Their 
web site says they do this with "New Pipeline Architecture" that 
"streams large data sets through available RAM instead of reading the 
entire data set into memory at once."  It also "includes a new data type 
for dealing with very large data objects".  If you want more than this, 
I suggest you post to "S-News List <s-news at lists.biostat.wustl.edu>";  I 
haven't used it.

	  hope this helps.
	  spencer graves

Gabor Grothendieck wrote:
> On 5/13/05, s104100026 <n.d.fitzgerald at mars.ucc.ie> wrote:
> 
>>Hi All,
>>
>>I want to read 256 1000x1000 matrices into R. I understand that it is unlikely
>>that I can do this but In the hope that somebody can help me I am mailing this
>>list.
>>
>>I have tried increasing my memory size (I understand that it is the minimum of
>>1024 or the computers RAM in my case 512)
>>
>>Does anyone think this is possible in R, could it be tried in Splus for
>>example.
>>
> 
> 
> If they are sparse you could try the SparseM package.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From whit at twinfieldscapital.com  Sat May 14 02:12:09 2005
From: whit at twinfieldscapital.com (Whit Armstrong)
Date: Fri, 13 May 2005 20:12:09 -0400
Subject: [R] help with eval
Message-ID: <726FC6DD09DE1046AF81B499D70C3BCE1FDF36@twinfields02.CORP.TWINFIELDSCAPITAL.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050513/c92a2c5b/attachment.pl

From OlsenN at pac.dfo-mpo.gc.ca  Sat May 14 03:05:53 2005
From: OlsenN at pac.dfo-mpo.gc.ca (OlsenN@pac.dfo-mpo.gc.ca)
Date: Fri, 13 May 2005 18:05:53 -0700
Subject: [R] How to convert color to black & white
Message-ID: <7CBBD627E4E688499349A5D11D078316018899AD@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>

Muhammad,
Here's one option:

barplot(1:5,col=gray(seq(0,1,length=5)))

Norm Olsen
Fisheries and Oceans Canada

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
To: R-help at stat.math.ethz.ch
Sent: 5/13/2005 11:40 AM
Subject: [R] How to convert color to black & white

Dear all,
Could someone please explain to me how to convert color to black &
white.
For example:
barplot(1:5,col = rainbow(5))
Because I need to print my plot to save my ink color printer.
I don't want to convert to grayscale, but keep it as an RGB.
I  would be very happy if anyone could help me.
Thank you very much in advance.

Kindly regards,
Muhammad Subianto

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Sat May 14 07:28:20 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 14 May 2005 01:28:20 -0400
Subject: [R] help with eval
In-Reply-To: <726FC6DD09DE1046AF81B499D70C3BCE1FDF36@twinfields02.CORP.TWINFIELDSCAPITAL.COM>
References: <726FC6DD09DE1046AF81B499D70C3BCE1FDF36@twinfields02.CORP.TWINFIELDSCAPITAL.COM>
Message-ID: <971536df05051322287f4b2a06@mail.gmail.com>

The scope of variables within show.a is not affected by the environment
that show.a is called in.  The scope of the variables in show.a
is determined by the lexical scope of show.a although you can change 
this via:

  environment(show.a) <- my.env
  show.a() # 200

If you want to create a function that has dynamic (i.e. scope is the caller), 
rather than lexical scope, do this:

   show2.a <- function() { 
      show2.a <- function() a 
      environment(show2.a) <- parent.frame()
      show2.a()
   }
  evalq(show2.a(), my.env) # 200

or you can create a function which evaluates its body in the parent.frame:

   show3.a <- function() eval.parent(substitute(a))
   evalq(show3.a(), my.env) # 200

Also, depending on what you actually want to do, the proto package
may be applicable.

On 5/13/05, Whit Armstrong <whit at twinfieldscapital.com> wrote:
> I've been looking at the help page for eval for a while, but I can't
> make sense of why this example does not work.
> 
> show.a <- function() {
>  a
> }
> 
> init.env <- function() {
>  a <- 200
>  environment()
> }
> 
> my.env <- init.env()
> 
> ls(envir=my.env)
> 
> # returns this:
> # > ls(envir=my.env)
> # [1] "a"
> 
> # but this does not work:
> eval(expression(show.a()),envir=my.env)
> 
> # > eval(expression(show.a()),envir=my.env)
> # Error in show.a() : Object "a" not found
> # >
> 
> The help page gives the following:
> 
>   'eval' evaluates the expression 'expr' argument in the environment
>     specified by 'envir' and returns the computed value. If 'envir' is
>     not specified, then 'sys.frame(sys.parent())', the environment
>     where the call to 'eval' was made is used.
> 
> I would be grateful for any help.
> 
> Thanks,
> Whit
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Sat May 14 07:41:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 May 2005 06:41:08 +0100 (BST)
Subject: [R] multinom(): likelihood of model?
In-Reply-To: <22461bafa53c7852793779dffa838391@u.washington.edu>
References: <22461bafa53c7852793779dffa838391@u.washington.edu>
Message-ID: <Pine.LNX.4.61.0505140637210.25094@gannet.stats>

By definition, the deviance is minus twice the maximized log-likelihood 
plus a const.  In any of these models for discrete data, the saturated 
model predicts exactly, so the const is zero.

There are worked examples in MASS4, the book multinom() supports.

On Fri, 13 May 2005, Brooks Miner wrote:

> Hi all,
>
> I'm working on a multinomial (or "polytomous") logistic regression using R 
> and have made great progress using multinom() from the nnet library.  My 
> response variable has three categories, and there are two different possible 
> predictors.  I'd like to use the likelihoods of certain models (ie, 
> saturated, fitteds, and null) to calculate Nagelkerke R-squared values for 
> various fitted models.
>
> My question today is simple: once I have fitted a model using multinom(), how 
> do I find the likelihood (or log likelihood) of my fitted model?  I 
> understand that this value must be part of the $deviance or $AIC components 
> of the fitted model, but my understanding is too limited at this point for me 
> to know how to calculate the likelihood of my fitted model from either of 
> these outputs.
>
> Thanks in advance to any assistance offered.  I'd be happy to provide an 
> example of my data and multinom() entries if that would help.
>
> Gratefully,
>
> - Brooks
> ----------------------------
> Brooks Miner
> Research Scientist
> Laird Lab
> UW Biology
> 206.616.9385
> http://protist.biology.washington.edu/Lairdlab/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat May 14 07:43:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 May 2005 06:43:49 +0100 (BST)
Subject: [R] A model package that uses S4 methods and name spaces?
In-Reply-To: <42851B79.1060809@stat.wisc.edu>
References: <33162.136.177.22.105.1116019173.squirrel@webmail.fpcc.net>
	<42851B79.1060809@stat.wisc.edu>
Message-ID: <Pine.LNX.4.61.0505140641470.25094@gannet.stats>

On Fri, 13 May 2005, Douglas Bates wrote:

> rlee at fpcc.net wrote:
>> I'd like to try and rewrite my package to take advantage of S4 methods and
>> name spaces.  I am presuming that S4 methods and name spaces will help me
>> write and maintain my package.
>>
>> Can someone suggest a "model" package that uses best practices for mixing
>> S3, S4 methods and name spaces?  Preferably a package with multiple R
>> source files.
>
> It doesn't mix S3 and S4 but the Matrix package is a large package based
> on S4 that does show the use of S4 and a NAMESPACE.  On a smaller scale,
> the lme4 package also uses S4 and a NAMESPACE.

The stats4 package in the distribution is intended to be a model in the 
sense I think we were asked for.  It is much smaller than Doug's examples.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat May 14 08:07:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 May 2005 07:07:59 +0100 (BST)
Subject: [R] scoping issues (was help with eval)
In-Reply-To: <726FC6DD09DE1046AF81B499D70C3BCE1FDF36@twinfields02.CORP.TWINFIELDSCAPITAL.COM>
References: <726FC6DD09DE1046AF81B499D70C3BCE1FDF36@twinfields02.CORP.TWINFIELDSCAPITAL.COM>
Message-ID: <Pine.LNX.4.61.0505140652170.25094@gannet.stats>

You intended quote() rather than expression(), I believe.  If I do

> show.a <- function(a) {a}
> eval(quote(show.a(a)),envir=my.env)

this works.

R has lexical scoping, so with

> show.a <- function() {a}

'a' is looked for in the frame of the function (not defined there) and 
then in the environment of the function.

> environment(show.a)
<environment: R_GlobalEnv>

since show.a was defined in the workspace.  The envir arg of eval() is 
used to find the object(s), here `show.a', not to sets its environment.
So my first version works because 'a' is part of the expression.

Perhaps you intended something like

init.env <- function() {
  a <- 200
  show.a <- function() {a}
  environment()
}
my.env <- init.env()
eval(quote(show.a()),envir=my.env)

which works, and is a common R idiom (although via local() or new.env()).

However, you could just do

> eval(quote(a), envir=my.env)
[1] 200
> eval(expression(a), envir=my.env)
[1] 200

BTW, use typeof() to see the difference here.

On Fri, 13 May 2005, Whit Armstrong wrote:

> I've been looking at the help page for eval for a while, but I can't
> make sense of why this example does not work.
>
> show.a <- function() {
>  a
> }
>
> init.env <- function() {
>  a <- 200
>  environment()
> }
>
> my.env <- init.env()
>
> ls(envir=my.env)
>
> # returns this:
> # > ls(envir=my.env)
> # [1] "a"
>
>
> # but this does not work:
> eval(expression(show.a()),envir=my.env)
>
> # > eval(expression(show.a()),envir=my.env)
> # Error in show.a() : Object "a" not found
> # >
>
>
> The help page gives the following:
>
>   'eval' evaluates the expression 'expr' argument in the environment
>     specified by 'envir' and returns the computed value. If 'envir' is
>     not specified, then 'sys.frame(sys.parent())', the environment
>     where the call to 'eval' was made is used.
>
> I would be grateful for any help.
>
> Thanks,
> Whit
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Katharina.Steinmann at stud.unibas.ch  Sat May 14 09:47:25 2005
From: Katharina.Steinmann at stud.unibas.ch (K. Steinmann)
Date: Sat, 14 May 2005 09:47:25 +0200
Subject: [R] lda
Message-ID: <1116056845.4285ad0d7b701@webmail.unibas.ch>

Dear R-helpers,
if I am right a discriminant analysis can be done with "lda".
My questions are:
1. What method to discriminate the groups is used by "lda" (Fisher's linar
discriminant function, diagonal linear discriminant analysis, likelihood ratio
discriminant rule, ...)?
2. How can I see, which method is used? (Typing just lda does not give me any
code).


Thank you in advance

Best wishes
K. Steinmann



From sonya.ku at gmail.com  Sat May 14 10:09:19 2005
From: sonya.ku at gmail.com (Sonya Ku)
Date: Sat, 14 May 2005 18:09:19 +1000
Subject: [R] plotting gam curve against predictors
Message-ID: <1a23742f0505140109300aa28f@mail.gmail.com>

Hi 
I am just beginning to learn R and have fitted several GAM to my
species presense/absence data.

I have used plot(x,y) using fitted.values as a y variable against
predictors. However, it is hard to see general relationships where
there is wide spread in predicted values for any x.

So I'd like to 
1) plot general curves, not individual fitted values against predictors
2) have some indication of confidence interval 95% but this is optional

would greatly appreciate any tips

Sonya.



From p.dalgaard at biostat.ku.dk  Sat May 14 10:26:40 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 May 2005 10:26:40 +0200
Subject: [R] A model package that uses S4 methods and name spaces?
In-Reply-To: <Pine.LNX.4.61.0505140641470.25094@gannet.stats>
References: <33162.136.177.22.105.1116019173.squirrel@webmail.fpcc.net>
	<42851B79.1060809@stat.wisc.edu>
	<Pine.LNX.4.61.0505140641470.25094@gannet.stats>
Message-ID: <x2fywqusan.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Fri, 13 May 2005, Douglas Bates wrote:
> 
> > rlee at fpcc.net wrote:
> >> I'd like to try and rewrite my package to take advantage of S4 methods and
> >> name spaces.  I am presuming that S4 methods and name spaces will help me
> >> write and maintain my package.
> >>
> >> Can someone suggest a "model" package that uses best practices for mixing
> >> S3, S4 methods and name spaces?  Preferably a package with multiple R
> >> source files.
> >
> > It doesn't mix S3 and S4 but the Matrix package is a large package based
> > on S4 that does show the use of S4 and a NAMESPACE.  On a smaller scale,
> > the lme4 package also uses S4 and a NAMESPACE.
> 
> The stats4 package in the distribution is intended to be a model in
> the sense I think we were asked for.  It is much smaller than Doug's
> examples.

Yes. It doesn't offer much in terms of class hierarchies and the like,
though. On the other hand, newcomers to S4 might actually want to
avoid that on first contact. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ales.ziberna at guest.arnes.si  Sat May 14 10:41:58 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Sat, 14 May 2005 10:41:58 +0200
Subject: [R] clustering
References: <20050513213701.49971.qmail@web26907.mail.ukl.yahoo.com>
Message-ID: <005001c55862$6383f9c0$598debd4@ales>

Did you thought of clustering with restriction that in each cluster, time
periods must all be "connected"?

----- Original Message ----- 
From: "Amir Safari" <amir36060 at yahoo.de>
To: <R-help at stat.math.ethz.ch>
Sent: Friday, May 13, 2005 11:37 PM
Subject: [R] clustering


> Hi Every body,
> In order to deal with nonstationary problem in time series, may be 
> firstly clustering algorithms are used to partition time series .Then 
> another algorithm is used to predict future value based on  segmented data 
> in the second phase. Using clustering algorithms , the "time structure and 
> arrangement" of time series is confused. We have some partitions including 
> data unrelated to the time at hand. A question which arises here is that: 
> lossing the time arrangement of time series is not a new problem?  can we 
> forecast the future based on segmented  confused clusters? What am i 
> missing?
>
> Have a nice
> Amir
>
>
> ---------------------------------
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From ripley at stats.ox.ac.uk  Sat May 14 11:34:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 May 2005 10:34:12 +0100 (BST)
Subject: [R] lda
In-Reply-To: <1116056845.4285ad0d7b701@webmail.unibas.ch>
References: <1116056845.4285ad0d7b701@webmail.unibas.ch>
Message-ID: <Pine.LNX.4.61.0505141025360.30539@gannet.stats>

On Sat, 14 May 2005, K. Steinmann wrote:

> if I am right a discriminant analysis can be done with "lda".
> My questions are:

> 1. What method to discriminate the groups is used by "lda" (Fisher's linar
> discriminant function, diagonal linear discriminant analysis, likelihood ratio
> discriminant rule, ...)?

None of those, but that due to Rao, which is (up to details of weighting 
of the covariance matrix) what is very widely called LDA.  (Many people 
attribute to Fisher something he did not do, at least not in the paper 
they cite.)

lda() (in package MASS, uncredited) is support software for a book, so 
please refer to the book for the details: it is in the references for the 
help page.

> 2. How can I see, which method is used? (Typing just lda does not give me any
> code).

I get
> lda
function (x, ...)
UseMethod("lda")
<environment: namespace:MASS>

which _is _code.  Please look up `generic functions' for example in `An 
Introduction to R'.  In this case

getS3method("lda", "default")

will show you the guts of the code, but I don't think you will be able to 
understand it without the references.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf at math.unb.ca  Sat May 14 14:23:24 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 14 May 2005 09:23:24 -0300 (ADT)
Subject: [R] julian
Message-ID: <200505141223.j4ECNOaF029210@erdos.math.unb.ca>


I have a problem wherein I need to convert back and forth from
dates of the form "2000-04-11" to Julian dates.  After some
experimentation I found that

	> z <- strptime("2000-62",format="%Y-%j")
	> z

gave me "2000-03-02" --- i.e. March 2 which is as it should be
according to my (not too reliable) arithmetic.

Going the other direction, more experimentation led me to

	> julian(z,origin=as.POSIXct("1999-12-31"))

which gives

Time difference of 62 days

and I thought that I had a working (if not fully comprehended)
syntax.  My illusions were shattered when I tried another Julian
date, 102:

	> z <- strptime("2000-102",format="%Y-%j")
        > z

gives "2000-04-11" --- i.e. April 11, which checks with what I think
it should be.  But reversing the direction:

	> julian(z,origin=as.POSIXct("1999-12-31"))

gives

Time difference of 101.9583 days

Where did the missing 0.0417 days go to?

What syntax should I really be using?

Thanks.

					cheers,

						Rolf Turner

P. S. I'm still on R version 2.0.1 (Linux) if that's of any relevance.

						R. T.



From p.dalgaard at biostat.ku.dk  Sat May 14 14:44:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 May 2005 14:44:07 +0200
Subject: [R] julian
In-Reply-To: <200505141223.j4ECNOaF029210@erdos.math.unb.ca>
References: <200505141223.j4ECNOaF029210@erdos.math.unb.ca>
Message-ID: <x2psvuj7u0.fsf@turmalin.kubism.ku.dk>

Rolf Turner <rolf at math.unb.ca> writes:

> I have a problem wherein I need to convert back and forth from
> dates of the form "2000-04-11" to Julian dates.  After some
> experimentation I found that
> 
> 	> z <- strptime("2000-62",format="%Y-%j")
> 	> z
> 
> gave me "2000-03-02" --- i.e. March 2 which is as it should be
> according to my (not too reliable) arithmetic.
> 
> Going the other direction, more experimentation led me to
> 
> 	> julian(z,origin=as.POSIXct("1999-12-31"))
> 
> which gives
> 
> Time difference of 62 days
> 
> and I thought that I had a working (if not fully comprehended)
> syntax.  My illusions were shattered when I tried another Julian
> date, 102:
> 
> 	> z <- strptime("2000-102",format="%Y-%j")
>         > z
> 
> gives "2000-04-11" --- i.e. April 11, which checks with what I think
> it should be.  But reversing the direction:
> 
> 	> julian(z,origin=as.POSIXct("1999-12-31"))
> 
> gives
> 
> Time difference of 101.9583 days
> 
> Where did the missing 0.0417 days go to?

Into the daylight savings account, I guess.
 
> What syntax should I really be using?

It's not the syntax as much as the timezone. But what has as.Date done
wrong, since you seem set on ignoring it?

> as.Date(z) - as.Date("1999-12-31")
Time difference of 102 days

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Sat May 14 14:59:54 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 14 May 2005 14:59:54 +0200
Subject: [R] cluster results using fanny
In-Reply-To: <3593943005051311551c61010c@mail.gmail.com>
References: <3593943005051311495df60b87@mail.gmail.com>
	<3593943005051311551c61010c@mail.gmail.com>
Message-ID: <4285F64A.6040807@statistik.uni-dortmund.de>

Barbara Diaz wrote:
> Hi,
> 
> I am using fanny and I have estrange results. I am wondering if
> someone out there can help me understand why this happens.
> 
> First of all in most of my tries, it gives me a result in which each
> object has equal membership in all clusters. I have read that that
> means "the clustering is entirely fuzzy". Looking at the graphics it
> is really difficult to understand how objects with so different scores
> for the variables have the same membership for all the clusters.
> 
> I also find estrange the fact that if I set K=3 (three clusters), it
> gives membership for all three clusters (0.333 for all of them) and
> then when it gives the closest hard clustering they only belong to
> cluster 1 or 2, but none of them to cluster three. The plot shows only
> two clusters (also the silhouette plot, even if it gives in the
> "silhouette plot information" the silhouette width for the three
> clusters????????.
> 
> Then, for the same data I set k=4 and surprisingly, it gives
> membership for the four of them (this time they are not all the same)
> and when it gives the closest hard clustering they only belong to
> cluster 1, 2, or 3 but none of them to cluster 4. The plot shows only
> three clusters (also the silhouette plot, even if it gives in the
> "silhouette plot information" the silhouette width for the four
> clusters????????. why didn't it give this three clusters when I set
> k=3??????
> 
> For k=5 it gives all the information and then it only plots 2 clusters.
> 
> This is very confusing. Also, if there is equal membership for all the
> clusters, how is it that I have a "closest hard clustering"? and a
> "neighbor"?


Can you speicfy a reproducible examples, please?
Without an example, it is really hard to explain what happens...

Uwe Ligges


> Thank you in advance,
> 
> Barbara
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rolf at math.unb.ca  Sat May 14 15:23:00 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 14 May 2005 10:23:00 -0300 (ADT)
Subject: [R] julian
Message-ID: <200505141323.j4EDN0AR001099@erdos.math.unb.ca>

Peter Dalgaard writes:

> It's not the syntax as much as the timezone.

	Well, that's part of the overall syntax, or structure
	at least.

> But what has as.Date done wrong, since you seem set on ignoring it?

	It's not that I'm ignoring it --- it's just that I haven't a
	clue what it means/does.  The help on the date/time stuff is
	totally opaque.  It appears to be meaningful only to those
	who are already completely au fait with the ``POSIX''
	standards (whatever they are!), and thereby are not in need
	of help.  (Reminds me of Sam Johnson's ``Letter to Lord
	Chesterfield'', but that's another story.)

	Anyhow, I found that all I could do was to proceed by trial
	and error, using all possible permutations and combination of
	strptime(), as.POSIXct(), as.POSIXlt(), format(), etc. etc.
	I actually did use a few perms and combs involving as.Date()
	--- but clearly not enough of them.

> > as.Date(z) - as.Date("1999-12-31")
> Time difference of 102 days

	That is exactly the syntax that I need, it would seem.

	Thank you!

				cheers,

					Rolf



From rolf at math.unb.ca  Sat May 14 15:28:18 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 14 May 2005 10:28:18 -0300 (ADT)
Subject: [R] julian --- P. S.
Message-ID: <200505141328.j4EDSIYZ001235@erdos.math.unb.ca>

Post Scriptum to my previous email --- in the middle of a Saturday
morning (in my Time Zone!) I send out a plea for help, and in just
over 20 minutes my problem is solved!

I don't think you get service like that anywhere else.  This r-help
list is BLOODY AMAZING!

				cheers,

					Rolf



From ggrothendieck at gmail.com  Sat May 14 15:31:31 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 14 May 2005 09:31:31 -0400
Subject: [R] julian
In-Reply-To: <200505141323.j4EDN0AR001099@erdos.math.unb.ca>
References: <200505141323.j4EDN0AR001099@erdos.math.unb.ca>
Message-ID: <971536df0505140631447ec62f@mail.gmail.com>

On 5/14/05, Rolf Turner <rolf at math.unb.ca> wrote:

>        The help on the date/time stuff is
>        totally opaque.  

Check out the article on dates and times in RNews 4/1 in the Help Desk
section.  The table at the end of that article includes differencing of dates.



From maechler at stat.math.ethz.ch  Sat May 14 16:25:11 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 14 May 2005 16:25:11 +0200
Subject: [R] persp( ) Question
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBBE@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBBE@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <17030.2631.204867.674392@stat.math.ethz.ch>

>>>>> "DavidB" == Brahm, David <David.Brahm at geodecapital.com>
>>>>>     on Fri, 6 May 2005 14:16:19 -0400 writes:

    DavidB> Greg, Assign the output of "persp" to a variable
    DavidB> "pmat":
    R> pmat <- persp(X.grid, Y.grid, pred.loess1, theta=0,
    R> phi=12)

    DavidB> Now you can add points to your plot with the usual
    DavidB> "points" command.  But you have to translate your 3D
    DavidB> coordinates (x,y,z) into 2D coordinates for "points"
    DavidB> to understand, and that's what "trans3d" does:

    R> points(trans3d(x,y,z, pmat), col="red")

    DavidB> You supply the (x,y,z) values, of course.  It's a
    DavidB> mystery to me why "trans3d" is not included in the
    DavidB> graphics package (where "persp" lives).  HTH.

Yes, I had similar thoughts many weeks ago, and already then
prepared to add to trans3d().  Will do this now (for R 2.2.0).

Martin



From spencer.graves at pdf.com  Sat May 14 17:32:28 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 14 May 2005 08:32:28 -0700
Subject: [R] julian --- P. S.
In-Reply-To: <200505141328.j4EDSIYZ001235@erdos.math.unb.ca>
References: <200505141328.j4EDSIYZ001235@erdos.math.unb.ca>
Message-ID: <42861A0C.4030706@pdf.com>

	  "The sun never sets on the (former) British Empire."  Today, it never 
sets on R-Help.

	  spencer graves

Rolf Turner wrote:

> Post Scriptum to my previous email --- in the middle of a Saturday
> morning (in my Time Zone!) I send out a plea for help, and in just
> over 20 minutes my problem is solved!
> 
> I don't think you get service like that anywhere else.  This r-help
> list is BLOODY AMAZING!
> 
> 				cheers,
> 
> 					Rolf
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitrijoe at yahoo.com.br  Sat May 14 20:13:47 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Sat, 14 May 2005 15:13:47 -0300
Subject: [R] pmvnorm
Message-ID: <000c01c558b0$ade17710$c800a8c0@thesahajamach>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050514/b7bfa900/attachment.pl

From dalexander at ccesearch.com  Sat May 14 20:41:27 2005
From: dalexander at ccesearch.com (Don Alexander)
Date: Sat, 14 May 2005 14:41:27 -0400
Subject: [R] Job Opportunity: Statistical Guru CC 083
Message-ID: <1116096087.428646577de04@webmail.ccesearch.com>

Do you consider yourself a cutting edge statistical expert with a penchant
for applying your broad theoretical background to solving some of the most
complex statistical challenges facing human health today? Does the idea of
working for an emerging company who has a strong management team,
financial backing and world class scientific advisors appeal to you? If
so, read on...

Our client is a drug development platform technology company whose patent
pending intellectual property is poised to dramatically impact both drug
discovery and development processes. The ultimate benefit to the drug
development process will be to identify safer compounds for development,
shorten the time for drugs to get to market and identify diagnostic
markers for earlier disease detection. This technology will increase the
success rate, decrease the time to market and stopdevelopment of products
before critical investments are lost.

Responsibilities:
Independently develop novel statistical approaches to a variety of data
types.
Provide statistical support for all data-mining efforts, platform Quality
Control, and data monitoring.
Produce statistical programs as needed.

Qualifications:
Ph.D. or Master?s in Statistics
At least 8 years of experience
Prior experience in R, S, SAS and/or S-plus and Design of Experiments (DOE)
Prior Data Mining knowledge (PLS, neural networks, OLAP, etc.)
Breadth of statistical approaches (Q Value/false discovery rate, P Value,
Bayesian, Frequentist, Monte Carlo Methods, multivariate data analysis,
logistic regression, chi-squared, Random Forest (RF) predictors)
Prior experience in a Life Sciences research and development environment
Bioinformatics knowledge preferable
Ability to lead or direct the work of others

Characteristics:
Naturally creative, with a broad background
Self starter
Assertive with a positive outlook and performance oriented attitude
Passion, energy, personal drive and motivation
Outstanding communication skills, a strategic mindset, an ability to
interface at an executive level and a polished presence are required

As a professional search firm, we will only be responding to those
inquiries that most closely align with the stated requirements. Moreover,
our client employer can only review candidates with valid US work
authorization at this time. Please include the position ID (CC 083) in the
subject line of your correspondence to ensure review and forward your
credentials (* Word/PDF/HTML or Text format please), in confidence,
to:

recruiter3 at ccesearch.com.

General CV/resume submissions for inclusion in our knowledgebase of future
opportunities can be made to: resume at ccesearch.com

For our most recent searches, please review our web site at:
www.ccesearch.com.

Kind regards,

Don Alexander
Director of Biz Dev Life Sciences
Carlyle & Conlan
(919) 474-0771x105
dalexander at ccesearch.com
http://www.ccesearch.com



From ripley at stats.ox.ac.uk  Sat May 14 20:50:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 May 2005 19:50:03 +0100 (BST)
Subject: [R] pmvnorm
In-Reply-To: <000c01c558b0$ade17710$c800a8c0@thesahajamach>
References: <000c01c558b0$ade17710$c800a8c0@thesahajamach>
Message-ID: <Pine.LNX.4.61.0505141942390.17431@gannet.stats>

On Sat, 14 May 2005, Dimitri Joe wrote:

> pmvnorm(lo=c(-Inf,-Inf), up=c(Inf,Inf), mean=c(0,0) )
>
> should give me "1", right? But it doens't - it giver me "0".
> Would someone help me, please?

Yes, but pmvnorm is in package 'mvtnorm' (uncredited here) so please 
discuss this with the package maintainer (Cc:ed here).

Extreme cases are often overlooked: I have this past week corrected 
several in R itself (so please do report them to the appropriate person). 
For now, please use something like

> In <- 1e100
> pmvnorm(lo=c(-In,-In), up=c(In,In), mean=c(0,0) )
[1] 1

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From amir36060 at yahoo.de  Sat May 14 21:36:24 2005
From: amir36060 at yahoo.de (Amir Safari)
Date: Sat, 14 May 2005 21:36:24 +0200 (CEST)
Subject: [R] what means Best sample in clustering using Clara?
Message-ID: <20050514193624.30829.qmail@web26906.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050514/b6aaf451/attachment.pl

From bill at barnard-engineering.com  Sat May 14 22:07:12 2005
From: bill at barnard-engineering.com (Bill Barnard)
Date: Sat, 14 May 2005 13:07:12 -0700
Subject: [R] make check fails after building R 2.1.0 on Fedora Core 3
Message-ID: <1116101232.25101.17.camel@tioga.barnard-engineering.com>

I have R v2.0.1 already running on the machine in question; a fairly
generic Fedora Core 3 machine. I am trying to build the new version
2.1.0 and have not been able to troubleshoot the problem I encountered.
I'm hoping someone else has seen it and can suggest a troubleshooting
avenue to me.

The build succeeds, both from the source RPM and from the tarball. I
believed I had all the build prerequisites, based on reading the install
instructions in the tarball and the R.spec file. The make check step
fails almost immediately with:

[billb at tioga R-2.1.0]$ make check
...
make[4]: Entering directory `/home/billb/tmp/R-2.1.0/tests/Examples'
running code in 'base-Ex.R' ...make[4]: *** [base-Ex.Rout] Error 1
make[4]: Leaving directory `/home/billb/tmp/R-2.1.0/tests/Examples'
make[3]: *** [test-Examples-Base] Error 2
...

The output file:

[billb at tioga R-2.1.0]$ tail -5 tests/Examples/base-Ex.Rout.fail
> assign("ptime", proc.time(), env = .CheckExEnv)
> grDevices::postscript("base-Examples.ps")
Error in .Internal(PS(file, old$paper, old$family, old$encoding, old
$bg,  :
        no internal function "PS"
Execution halted

If I run the executable I built, and make a call to the postscript()
function, I get the same error.

Platform/version:
> version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

[billb at tioga R-2.1.0]$ gcc -v
Reading specs from /usr/lib/gcc/i386-redhat-linux/3.4.3/specs
Configured with: ../configure --prefix=/usr --mandir=/usr/share/man --
infodir=/usr/share/info --enable-shared --enable-threads=posix --
disable-checking --with-system-zlib --enable-__cxa_atexit --disable-
libunwind-exceptions --enable-java-awt=gtk --host=i386-redhat-linux
Thread model: posix
gcc version 3.4.3 20050227 (Red Hat 3.4.3-22.fc3)


I would appreciate any hints on how to troubleshoot this.

Thanks,

Bill
-- 
Bill Barnard <bill at barnard-engineering.com>



From p.dalgaard at biostat.ku.dk  Sat May 14 22:39:20 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 May 2005 22:39:20 +0200
Subject: [R] make check fails after building R 2.1.0 on Fedora Core 3
In-Reply-To: <1116101232.25101.17.camel@tioga.barnard-engineering.com>
References: <1116101232.25101.17.camel@tioga.barnard-engineering.com>
Message-ID: <x2ll6hk0ef.fsf@turmalin.kubism.ku.dk>

Bill Barnard <bill at barnard-engineering.com> writes:

> I have R v2.0.1 already running on the machine in question; a fairly
> generic Fedora Core 3 machine. I am trying to build the new version
> 2.1.0 and have not been able to troubleshoot the problem I encountered.
> I'm hoping someone else has seen it and can suggest a troubleshooting
> avenue to me.
> 
> The build succeeds, both from the source RPM and from the tarball. I
> believed I had all the build prerequisites, based on reading the install
> instructions in the tarball and the R.spec file. The make check step
> fails almost immediately with:
> 
> [billb at tioga R-2.1.0]$ make check
> ...
> make[4]: Entering directory `/home/billb/tmp/R-2.1.0/tests/Examples'
> running code in 'base-Ex.R' ...make[4]: *** [base-Ex.Rout] Error 1
> make[4]: Leaving directory `/home/billb/tmp/R-2.1.0/tests/Examples'
> make[3]: *** [test-Examples-Base] Error 2
> ...
> 
> The output file:
> 
> [billb at tioga R-2.1.0]$ tail -5 tests/Examples/base-Ex.Rout.fail
> > assign("ptime", proc.time(), env = .CheckExEnv)
> > grDevices::postscript("base-Examples.ps")
> Error in .Internal(PS(file, old$paper, old$family, old$encoding, old
> $bg,  :
>         no internal function "PS"
> Execution halted
> 
> If I run the executable I built, and make a call to the postscript()
> function, I get the same error.
...
> I would appreciate any hints on how to troubleshoot this.

Odd... It's not like noone has been building on FC3 before. 

Have you tried the binary RPM? Does postscript() work there?

The error message is pretty puzzling. My version of 2.1.0 has 
grDevices::postscript ending with

    .External("PostScript", file, old$paper, old$family, old$encoding,
        old$bg, old$fg, old$width, old$height, old$horizontal,
        old$pointsize, old$onefile, old$pagecentre, old$print.it,
        old$command, title, fonts, PACKAGE = "grDevices")

.Internal(PS(file,....  sounds like it might come from an earlier
version of R. It doesn't appear in my version of the 2.1.0(-patched)
sources. 



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bill at barnard-engineering.com  Sat May 14 23:47:28 2005
From: bill at barnard-engineering.com (Bill Barnard)
Date: Sat, 14 May 2005 14:47:28 -0700
Subject: [R] make check fails after building R 2.1.0 on Fedora Core 3
In-Reply-To: <x2ll6hk0ef.fsf@turmalin.kubism.ku.dk>
References: <1116101232.25101.17.camel@tioga.barnard-engineering.com>
	<x2ll6hk0ef.fsf@turmalin.kubism.ku.dk>
Message-ID: <1116107248.25101.29.camel@tioga.barnard-engineering.com>

On Sat, 2005-05-14 at 22:39 +0200, Peter Dalgaard wrote:
> .Internal(PS(file,....  sounds like it might come from an earlier
> version of R. It doesn't appear in my version of the 2.1.0(-patched)
> sources. 

Bingo. It's the old mixed versions problem (I should have thought of
that...) My existing environment had:

R_LIBS=/usr/lib/R/library

I simply unset the variable, and re-ran the make. make check completed
with no errors.

Thank you very much for your help.

Cheers,

Bill
-- 
Bill Barnard <bill at barnard-engineering.com>



From adrienne.mueller at imperial.ac.uk  Sun May 15 11:18:09 2005
From: adrienne.mueller at imperial.ac.uk (Mueller, Adrienne)
Date: Sun, 15 May 2005 10:18:09 +0100
Subject: [R] CircStats and Anova
Message-ID: <65544FA95ABA5243975A6930429BD9B091A6ED@icex33.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050515/5ede75ab/attachment.pl

From Katharina.Steinmann at stud.unibas.ch  Sun May 15 12:16:20 2005
From: Katharina.Steinmann at stud.unibas.ch (K. Steinmann)
Date: Sun, 15 May 2005 12:16:20 +0200
Subject: [R] testing equality of covariance matrices
Message-ID: <1116152180.4287217425b65@webmail.unibas.ch>


Dear R-mailers,

I would like to test for equality of population covariance matrices.
But I can't find a R tool to do so.

I saw, that other people had the same question, but I could not find an answer
to it, I would appreciate to know the missed link.

Thank you,
b.w. K. Steinmann



From ligges at statistik.uni-dortmund.de  Sun May 15 12:41:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 15 May 2005 12:41:27 +0200
Subject: [R] what means Best sample in clustering using Clara?
In-Reply-To: <20050514193624.30829.qmail@web26906.mail.ukl.yahoo.com>
References: <20050514193624.30829.qmail@web26906.mail.ukl.yahoo.com>
Message-ID: <42872757.9080106@statistik.uni-dortmund.de>

Amir Safari wrote:

>  
>  
> Hi All,
> Can some body tell what means Best Sample in clustering using Clara?


?clara.object tells you:

     sample: labels or case numbers of the observations
     in the best sample, that is, the sample used by the
     clara algorithm for the final partition.

Uwe Ligges





> Best Regards,
>  
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun May 15 12:59:33 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 15 May 2005 12:59:33 +0200
Subject: [R] Job Opportunity: Statistical Guru CC 083
In-Reply-To: <1116096087.428646577de04@webmail.ccesearch.com>
References: <1116096087.428646577de04@webmail.ccesearch.com>
Message-ID: <42872B95.2030107@statistik.uni-dortmund.de>

BTW:
I am looking for a wife that is very intelligent, earns at least 
300,000$ a year, is less than 25 years old, is a real beauty, happily 
cooks, cleans, and works in the garden, and is never moody, of course.
She would have the great opportunity to solve one of the most complex 
challenges facing human life today: Marrying me, a guy working 12 hours 
a day, only talking about statistics and alike, looking tired, 
frequently beeing ill, a couple of years older, always moody, not even 
remembering his own birthday.

Of course, I will not send any answers on your inquiries if I like your 
photo and you have sent me at least 10,000$.

Uwe Ligges


Don Alexander wrote:

> Do you consider yourself a cutting edge statistical expert with a penchant
> for applying your broad theoretical background to solving some of the most
> complex statistical challenges facing human health today? Does the idea of
> working for an emerging company who has a strong management team,
> financial backing and world class scientific advisors appeal to you? If
> so, read on...
> 
> Our client is a drug development platform technology company whose patent
> pending intellectual property is poised to dramatically impact both drug
> discovery and development processes. The ultimate benefit to the drug
> development process will be to identify safer compounds for development,
> shorten the time for drugs to get to market and identify diagnostic
> markers for earlier disease detection. This technology will increase the
> success rate, decrease the time to market and stopdevelopment of products
> before critical investments are lost.
> 
> Responsibilities:
> Independently develop novel statistical approaches to a variety of data
> types.
> Provide statistical support for all data-mining efforts, platform Quality
> Control, and data monitoring.
> Produce statistical programs as needed.
> 
> Qualifications:
> Ph.D. or Master?s in Statistics
> At least 8 years of experience
> Prior experience in R, S, SAS and/or S-plus and Design of Experiments (DOE)
> Prior Data Mining knowledge (PLS, neural networks, OLAP, etc.)
> Breadth of statistical approaches (Q Value/false discovery rate, P Value,
> Bayesian, Frequentist, Monte Carlo Methods, multivariate data analysis,
> logistic regression, chi-squared, Random Forest (RF) predictors)
> Prior experience in a Life Sciences research and development environment
> Bioinformatics knowledge preferable
> Ability to lead or direct the work of others
> 
> Characteristics:
> Naturally creative, with a broad background
> Self starter
> Assertive with a positive outlook and performance oriented attitude
> Passion, energy, personal drive and motivation
> Outstanding communication skills, a strategic mindset, an ability to
> interface at an executive level and a polished presence are required
> 
> As a professional search firm, we will only be responding to those
> inquiries that most closely align with the stated requirements. Moreover,
> our client employer can only review candidates with valid US work
> authorization at this time. Please include the position ID (CC 083) in the
> subject line of your correspondence to ensure review and forward your
> credentials (* Word/PDF/HTML or Text format please), in confidence,
> to:
> 
> recruiter3 at ccesearch.com.
> 
> General CV/resume submissions for inclusion in our knowledgebase of future
> opportunities can be made to: resume at ccesearch.com
> 
> For our most recent searches, please review our web site at:
> www.ccesearch.com.
> 
> Kind regards,
> 
> Don Alexander
> Director of Biz Dev Life Sciences
> Carlyle & Conlan
> (919) 474-0771x105
> dalexander at ccesearch.com
> http://www.ccesearch.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From lab at cs.wisc.edu  Sun May 15 13:11:26 2005
From: lab at cs.wisc.edu (lab@cs.wisc.edu)
Date: Sun, 15 May 2005 06:11:26 -0500
Subject: [R] Re: [CSL #265590] Schily ueber Deutschland
In-Reply-To: <72ff0d4.90b4fc3ff1@ajhseqm.com> on Sun, 15 May 2005 11:11:16 GMT
References: <72ff0d4.90b4fc3ff1@ajhseqm.com>
Message-ID: <200505151111.j4FBBQn8030460@vampire.cs.wisc.edu>


Greetings.  (This is an automated response.  There is no need to reply.)

Your message regarding:
  Schily ueber Deutschland
has been received and assigned a request number of 265590.

In order help us track the progress of this request, we ask that you
note the following:

1) Please save this email -- it contains the tracking number for this request.

2) include the string:
          [CSL #265590]
   (exactly as it appears -- with the square brackets) in the Subject: line
   of any further mail about this particular request.

   For example:
          Subject: [CSL #265590] Schily ueber Deutschland

   You may do this simply by replying to this email.

3) Please make sure to include lab at cs.wisc.edu as a recipient for any future 
   email about this topic (To: lab at cs.wisc.edu or Cc: lab at cs.wisc.edu)

For your convenience, a copy of your original mail is at the end of
this message.

                                Thank You,
                                Computer Systems Lab

======================================================================
>From aliases at obsidian.cs.wisc.edu Sun May 15 06:11:22 2005
Received: from obsidian.cs.wisc.edu (obsidian.cs.wisc.edu [128.105.6.13])
 by vampire.cs.wisc.edu (8.13.1/8.13.1) with ESMTP id j4FBBMQj030446 for
 <labacct at vampire.cs.wisc.edu>; Sun, 15 May 2005 06:11:22 -0500
Received: from kotqg.org (gdd87.gdd.macalester.edu [141.140.121.87]) by
 obsidian.cs.wisc.edu (8.13.1/8.13.1) with SMTP id j4FBBGqU011854;
 Sun, 15 May 2005 06:11:17 -0500
From: R-help at hypatia.math.ethz.ch
To: sax at stat.wisc.edu
Date: Sun, 15 May 2005 11:11:16 GMT
Subject: [CSL #265590] Schily ueber Deutschland
Importance: Normal
X-Priority: 3 (Normal)
X-Msmail-Priority: Normal
MIME-Version: 1.0
Message-ID: <72ff0d4.90b4fc3ff1 at ajhseqm.com>
Content-Transfer-Encoding: 7bit
Content-Type: text/plain; charset="us-ascii"
X-CSL-Mailscanner-Information: Please contact lab at cs.wisc.edu for more
 information
X-CSL-Mailscanner: Found to be clean

Lese selbst:
http://www.heise.de/newsticker/meldung/59427



From Camarda at demogr.mpg.de  Sun May 15 15:12:17 2005
From: Camarda at demogr.mpg.de (Camarda, Carlo Giovanni)
Date: Sun, 15 May 2005 15:12:17 +0200
Subject: [R] Save parameters from optim during iteration procedure
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6684C68@HERMES.demogr.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050515/bbcabdb8/attachment.pl

From tolga at coubros.com  Sun May 15 16:25:45 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sun, 15 May 2005 15:25:45 +0100
Subject: [R] Suppressing warning messages
In-Reply-To: <425EFDE6.5040204@coubros.com>
References: <425EFDE6.5040204@coubros.com>
Message-ID: <42875BE9.4030608@coubros.com>

How do I suppress the following ?

Warning messages:
1: the condition has length > 1 and only the first element will be used
in: if (strike == forward) atmvol(forward, t, alpha, beta, rho, upsilon)
else {
2: the condition has length > 1 and only the first element will be used
in: if (x(z) == 0) 1 else z/x(z)



From kjetil at acelerate.com  Sun May 15 16:31:36 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 15 May 2005 10:31:36 -0400
Subject: [R] Suppressing warning messages
In-Reply-To: <42875BE9.4030608@coubros.com>
References: <425EFDE6.5040204@coubros.com> <42875BE9.4030608@coubros.com>
Message-ID: <42875D48.9040205@acelerate.com>

Tolga Uzuner wrote:

> How do I suppress the following ?
>
> Warning messages:
> 1: the condition has length > 1 and only the first element will be used
> in: if (strike == forward) atmvol(forward, t, alpha, beta, rho, upsilon)
> else {
> 2: the condition has length > 1 and only the first element will be used
> in: if (x(z) == 0) 1 else z/x(z)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>
Maybe better to understand what generates the warning!

To assure you are uninformed, say
options(warn=-1)

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From ggrothendieck at gmail.com  Sun May 15 16:40:02 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 15 May 2005 10:40:02 -0400
Subject: [R] Suppressing warning messages
In-Reply-To: <42875BE9.4030608@coubros.com>
References: <425EFDE6.5040204@coubros.com> <42875BE9.4030608@coubros.com>
Message-ID: <971536df0505150740393390c6@mail.gmail.com>

On 5/15/05, Tolga Uzuner <tolga at coubros.com> wrote:
> How do I suppress the following ?
> 
> Warning messages:
> 1: the condition has length > 1 and only the first element will be used
> in: if (strike == forward) atmvol(forward, t, alpha, beta, rho, upsilon)
> else {
> 2: the condition has length > 1 and only the first element will be used
> in: if (x(z) == 0) 1 else z/x(z)

Check out ?suppressWarnings



From ligges at statistik.uni-dortmund.de  Sun May 15 16:41:22 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 15 May 2005 16:41:22 +0200
Subject: [R] Suppressing warning messages
In-Reply-To: <42875D48.9040205@acelerate.com>
References: <425EFDE6.5040204@coubros.com> <42875BE9.4030608@coubros.com>
	<42875D48.9040205@acelerate.com>
Message-ID: <42875F92.40001@statistik.uni-dortmund.de>

Kjetil Brinchmann Halvorsen wrote:
> Tolga Uzuner wrote:
> 
>> How do I suppress the following ?
>>
>> Warning messages:
>> 1: the condition has length > 1 and only the first element will be used
>> in: if (strike == forward) atmvol(forward, t, alpha, beta, rho, upsilon)
>> else {
>> 2: the condition has length > 1 and only the first element will be used
>> in: if (x(z) == 0) 1 else z/x(z)
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
> Maybe better to understand what generates the warning!

Yes! In both cases you should really look why you are using *conditions 
of length > 1*! And if this is intended, you certainly want to use 
"ifelse()" rather than "if(){} else{}".

Uwe Ligges


> To assure you are uninformed, say
> options(warn=-1)
> 
> Kjetil
>



From dr.mike at ntlworld.com  Sun May 15 16:40:32 2005
From: dr.mike at ntlworld.com (Mike Waters)
Date: Sun, 15 May 2005 15:40:32 +0100
Subject: [R] Job Opportunity: Statistical Guru CC 083
In-Reply-To: <42872B95.2030107@statistik.uni-dortmund.de>
Message-ID: <20050515144042.WODE1279.aamta03-winn.mailhost.ntl.com@d600>

 Wow,

Cutting edge to bleeding heart - and all in one day! 8??>

Mike

P.s. Uwe - start with an easy one, like a Unicorn.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
Sent: 15 May 2005 12:00
To: Don Alexander
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Job Opportunity: Statistical Guru CC 083

BTW:
I am looking for a wife that is very intelligent, earns at least 300,000$ a
year, is less than 25 years old, is a real beauty, happily cooks, cleans,
and works in the garden, and is never moody, of course.
She would have the great opportunity to solve one of the most complex
challenges facing human life today: Marrying me, a guy working 12 hours a
day, only talking about statistics and alike, looking tired, frequently
beeing ill, a couple of years older, always moody, not even remembering his
own birthday.

Of course, I will not send any answers on your inquiries if I like your
photo and you have sent me at least 10,000$.

Uwe Ligges


Don Alexander wrote:

> Do you consider yourself a cutting edge statistical expert with a 
> penchant for applying your broad theoretical background to solving 
> some of the most complex statistical challenges facing human health 
> today? Does the idea of working for an emerging company who has a 
> strong management team, financial backing and world class scientific 
> advisors appeal to you? If so, read on...
> 
> Our client is a drug development platform technology company whose 
> patent pending intellectual property is poised to dramatically impact 
> both drug discovery and development processes. The ultimate benefit to 
> the drug development process will be to identify safer compounds for 
> development, shorten the time for drugs to get to market and identify 
> diagnostic markers for earlier disease detection. This technology will 
> increase the success rate, decrease the time to market and 
> stopdevelopment of products before critical investments are lost.
> 
> Responsibilities:
> Independently develop novel statistical approaches to a variety of 
> data types.
> Provide statistical support for all data-mining efforts, platform 
> Quality Control, and data monitoring.
> Produce statistical programs as needed.
> 
> Qualifications:
> Ph.D. or Master?s in Statistics
> At least 8 years of experience
> Prior experience in R, S, SAS and/or S-plus and Design of Experiments 
> (DOE) Prior Data Mining knowledge (PLS, neural networks, OLAP, etc.) 
> Breadth of statistical approaches (Q Value/false discovery rate, P 
> Value, Bayesian, Frequentist, Monte Carlo Methods, multivariate data 
> analysis, logistic regression, chi-squared, Random Forest (RF) 
> predictors) Prior experience in a Life Sciences research and 
> development environment Bioinformatics knowledge preferable Ability to 
> lead or direct the work of others
> 
> Characteristics:
> Naturally creative, with a broad background Self starter Assertive 
> with a positive outlook and performance oriented attitude Passion, 
> energy, personal drive and motivation Outstanding communication 
> skills, a strategic mindset, an ability to interface at an executive 
> level and a polished presence are required
> 
> As a professional search firm, we will only be responding to those 
> inquiries that most closely align with the stated requirements. 
> Moreover, our client employer can only review candidates with valid US 
> work authorization at this time. Please include the position ID (CC 
> 083) in the subject line of your correspondence to ensure review and 
> forward your credentials (* Word/PDF/HTML or Text format please), in 
> confidence,
> to:
> 
> recruiter3 at ccesearch.com.
> 
> General CV/resume submissions for inclusion in our knowledgebase of 
> future opportunities can be made to: resume at ccesearch.com
> 
> For our most recent searches, please review our web site at:
> www.ccesearch.com.
> 
> Kind regards,
> 
> Don Alexander
> Director of Biz Dev Life Sciences
> Carlyle & Conlan
> (919) 474-0771x105
> dalexander at ccesearch.com
> http://www.ccesearch.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun May 15 16:49:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 15 May 2005 16:49:17 +0200
Subject: [R] Save parameters from optim during iteration procedure
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6684C68@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6684C68@HERMES.demogr.mpg.de>
Message-ID: <4287616D.1090209@statistik.uni-dortmund.de>

Camarda, Carlo Giovanni wrote:

> Dear R-users,
> I am going to try to be as clearer as possible, showing also an example.
> 1) I have a function (in my real case it's much more complex)
> 2) I use "optim" to minimize
> 3) I want to use as method L-BFGS-B for several reasons
> 4) I know I could use "trace=6" (in "control") in order to see "live"
> the procedure
> 5) I would like to see separately the values of my parameters during
> each iteration (what, on the screen, are the values of X)
> 6) Hence I would need to know the way of asking R to save somewhere the
> values of X during the optimization procedure
> 7) I have already tried to use "write.table" into the function I want to
> minimize (in the example with comments), but it gave me more values that
> I would need
> 
> Might you know a solution?
> 
> EXAMPLE:
> ###### function
> fun <- function(param){
>     a <- param[1]
>     b <- param[2]
>     r <- sqrt(a^2+b^2)
>     res <- -10 * sin(r)/r
>     #write.table(x=b, file="B.txt", append=TRUE, sep=",",
> col.names=FALSE,
>     #          row.names=FALSE)

This way you get each step where the function is *evaluated*, which 
might be much more than each iteration step of the optimization.



>     return(res)
>     }
> 
> ######## optimization procedure
> ott <- optim(par=c(-1,1),
>       fn =fun,
>       method = c("L-BFGS-B"),
>       control=c(trace=6)
>       )


For example, you can sink() temorarily into a file and extract the 
relevant lines (beginning with "X = ") afterwards.

Uwe Ligges



> ######### what I would need to save
> [...]
> X = -0.292893 0.292893
> [...]
> X = 0.0942275 -0.0942275
> [...]
> X = -0.00110391 0.00110391
> [...]
> X = 1.93949e-006 -1.93949e-006
> [...]
> X = -4.98008e-013 4.98008e-013
> [...]
> 
> 
> 
> Thanks in advance,
> Carlo Giovanni Camarda
> 
> 
> 
> Camarda Carlo Giovanni
> Max Planck Institute for Demographic Research
> Konrad-Zuse-Strasse 1
> 18057 Rostock, Germany
> 
> Tel:  +49 (0)381 2081 172
> Fax: +49 (0)381 2081 472
> Camarda at demogr.mpg.de
> 
> 
> 
> 
> +++++
> This mail has been sent through the MPI for Demographic Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chris at psyctc.org  Sun May 15 17:08:15 2005
From: chris at psyctc.org (Chris Evans)
Date: Sun, 15 May 2005 16:08:15 +0100
Subject: [R] Seeking friend for life (not)
Message-ID: <428773EF.9592.1411298@localhost>

I'll take a risk following Uwe's wonderful response to that advert.  
I'm looking for someone, perhaps particularly a stats student, who 
might want to do a piece of work with me on using R to present and 
analyse routine data that psychotherapists might submit on a cgi-bin 
interface and perhaps cross-referencing that against some largish 
referential data to which I have access.

Like Uwe, I require that the student bring beauty, brains, money, 
infinite tolerance ... no, seriously.  If this might be of any 
possible link up with yourself or a student of yours, read on, 
otherwise reread Uwe's post or just hit the delete button!

The situation is that I have co-developed a "copyleft" self-report 
measure that adult, tabloid literate clients in psychological 
therapies can complete readily that gives reasonable indication of 
their state say at referral, assessment, start of therapy, during 
therapy, end of therapy, follow-up etc.  It's part of a system we 
call CORE (Clinical Outcomes in Routine Evaluation).  How it is used 
is up to practitioners as our aim is to persuade therapists that such 
data can be of use to them, not just to researchers.  There is a 
linked therapist completed assessment form and end of therapy form 
which provides a therapist rating of initial and final state but, 
much more importantly, provides information that contextualises the 
self-report data.

A company in whom I have no financial interest have written a 
wonderful PC based program that takes data and analyses it (CORE-PC, 
see http://coreims.co.uk/ for information on that and the system of 
measures).  However, I am interested in putting something on the www 
using a cgi-bin interface to R to allow therapists not yet convinced 
of the value of CORE-PC to overcome their alienation from routine 
data analysis. 

I am a psychotherapist and psychotherapy researcher first but my 
stats is competent and I've pretty much fixed on R for all my stats 
work now (bye SPSS et al.!) and love it.  However, to use R to do 
this well clearly means getting my head around RZope/RSOAP or 
Rstatserver or something like that as just using CGIwithR, though 
brilliant, is slow and inefficient.

I have discussed this with our local university (Nottingham) who are 
R supportive but have no cgi-bin expertise so I'm asking more widely 
now.  Our/my history is of getting good papers out and we have good 
ones on uses of the system in submission and in press as well as as a 
sample list below.  Unless I do most of the work I'm very unconcerned 
about where my name comes in co-authorship as I'm not under RAE 
pressures, just my own and NHS pressures to move this initiative on.  
Hence it may be a good link up for people.  I currently have 
legitimate access to anonymised datasets from a few thosand in 
specific (multi-practitioner) services to massive (c40k) datasets 
from wider collections and there are many angles to look at within a 
general EDA/visualising approach to help therapy practitioners 
overcome their phobias and lack of numeracy.

Do get in touch directly if you think this may be of some interest.

Very best,

Chris

Sample publications, happy to send snail mail copies:

Barkham, M., Evans, C., Margison, F., et al (1998) The rationale for 
developing and implementing core outcome batteries for routine use in 
service settings and psychotherapy outcome research. Journal of 
Mental Health, 7, 35-47.
Barkham, M., Margison, F., Leach, C., et al (2001) Service profiling 
and outcomes benchmarking using the CORE-OM: toward practice-based 
evidence in the psychological therapies. Journal of Consulting and 
Clinical Psychology, 69, 184-196.
Evans, C., Connell, J., Barkham, M., et al (2002) Towards a 
standardised brief outcome measure: psychometric properties and 
utility of the CORE-OM. British Journal of Psychiatry, 180, 51-60.
---- (2003) Practice-Based Evidence: benchmarking NHS primary care 
counselling services at national and local levels. Clinical 
Psychology & Psychotherapy, 10, 374-388.
Evans, C., Mellor-Clark, J., Margison, F., et al (2000) CORE: 
Clinical Outcomes in Routine Evaluation. Journal of Mental Health, 9, 
247-255.
Margison, F. R., Barkham, M., Evans, C., et al (2000) Measurement and 
psychotherapy: Evidence-based practice and practice-based evidence. 
British Journal of Psychiatry, 177, 123-130.
Mellor-Clark, J., Connell, J., Barkham, M., et al (2001) Counselling 
outcomes in primary health care: a CORE system data profile. European 
journal of psychotherapy, counselling and health, 4, 65-86.
-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From Ted.Harding at nessie.mcc.ac.uk  Sun May 15 17:36:04 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 15 May 2005 16:36:04 +0100 (BST)
Subject: [R] Job Opportunity: Statistical Guru CC 083
In-Reply-To: <20050515144042.WODE1279.aamta03-winn.mailhost.ntl.com@d600>
Message-ID: <XFMail.050515163604.Ted.Harding@nessie.mcc.ac.uk>

On 15-May-05 Mike Waters wrote:
>  Wow,
> 
> Cutting edge to bleeding heart - and all in one day! 8??>
> 
> Mike
> 
> P.s. Uwe - start with an easy one, like a Unicorn.

Dear Uwe,

Some further advice. I fear your vacancy notice has not been written
to attract the best candidate. There are even important details
which you have left unmentioned (for example, is your preferred method
of communication by Command, or by Point and Click, or by both?)

Perhaps slightly more seriously, Don Alexander's advertisement
contains many of the elements of a good vacancy notice, and could
perhaps serve as a model which you could refine.

I am reminded of C. Northcote Parkinson's penetrating analyis of job
adverts (he of "Parkinson's Law"). He pointed out that most job
advertisements are inefficient and ineffective, in that they are
written in such a way that they attract far too many applicants.
As a result, much time, effort and expense is incurred in rejecting
most or even all of these (since really discriminating candidates
would probably not respond in the first place.

He recommended instead the "Chinese Method." In this, you know
exactly whom you precisely want, and you frame the advert in such
terms as to exclude all other possibilities, and to just (but no
more) irresistibly attract that one candidate. Don Alexander's
effort clearly goes some distance in this direction, though even
so there will probably be unnecessary duplication of applicants.
But you can work on it!

With best wishes, and with tongue in cheek,
Ted.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
> Sent: 15 May 2005 12:00
> To: Don Alexander
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Job Opportunity: Statistical Guru CC 083
> 
> BTW:
> I am looking for a wife that is very intelligent, earns at least
> 300,000$ a year, is less than 25 years old, is a real beauty,
> happily cooks, cleans, and works in the garden, and is never moody,
> of course.
> She would have the great opportunity to solve one of the most
> complex challenges facing human life today: Marrying me, a guy
> working 12 hours a day, only talking about statistics and alike,
> looking tired, frequently beeing ill, a couple of years older,
> always moody, not even remembering his own birthday.
> 
> Of course, I will not send any answers on your inquiries if I like
> your photo and you have sent me at least 10,000$.
> 
> Uwe Ligges
> 
> 
> Don Alexander wrote:
> 
>> Do you consider yourself a cutting edge statistical expert with a 
>> penchant for applying your broad theoretical background to solving 
>> some of the most complex statistical challenges facing human health 
>> today? Does the idea of working for an emerging company who has a 
>> strong management team, financial backing and world class scientific 
>> advisors appeal to you? If so, read on...
>> 
>> Our client is a drug development platform technology company whose 
>> patent pending intellectual property is poised to dramatically impact 
>> both drug discovery and development processes. The ultimate benefit to
>> the drug development process will be to identify safer compounds for 
>> development, shorten the time for drugs to get to market and identify 
>> diagnostic markers for earlier disease detection. This technology will
>> increase the success rate, decrease the time to market and 
>> stopdevelopment of products before critical investments are lost.
>> 
>> Responsibilities:
>> Independently develop novel statistical approaches to a variety of 
>> data types.
>> Provide statistical support for all data-mining efforts, platform 
>> Quality Control, and data monitoring.
>> Produce statistical programs as needed.
>> 
>> Qualifications:
>> Ph.D. or Master?s in Statistics
>> At least 8 years of experience
>> Prior experience in R, S, SAS and/or S-plus and Design of Experiments 
>> (DOE) Prior Data Mining knowledge (PLS, neural networks, OLAP, etc.) 
>> Breadth of statistical approaches (Q Value/false discovery rate, P 
>> Value, Bayesian, Frequentist, Monte Carlo Methods, multivariate data 
>> analysis, logistic regression, chi-squared, Random Forest (RF) 
>> predictors) Prior experience in a Life Sciences research and 
>> development environment Bioinformatics knowledge preferable Ability to
>> lead or direct the work of others
>> 
>> Characteristics:
>> Naturally creative, with a broad background Self starter Assertive 
>> with a positive outlook and performance oriented attitude Passion, 
>> energy, personal drive and motivation Outstanding communication 
>> skills, a strategic mindset, an ability to interface at an executive 
>> level and a polished presence are required
>> 
>> As a professional search firm, we will only be responding to those 
>> inquiries that most closely align with the stated requirements. 
>> Moreover, our client employer can only review candidates with valid US
>> work authorization at this time. Please include the position ID (CC 
>> 083) in the subject line of your correspondence to ensure review and 
>> forward your credentials (* Word/PDF/HTML or Text format please), in 
>> confidence,
>> to:
>> 
>> recruiter3 at ccesearch.com.
>> 
>> General CV/resume submissions for inclusion in our knowledgebase of 
>> future opportunities can be made to: resume at ccesearch.com
>> 
>> For our most recent searches, please review our web site at:
>> www.ccesearch.com.
>> 
>> Kind regards,
>> 
>> Don Alexander
>> Director of Biz Dev Life Sciences
>> Carlyle & Conlan
>> (919) 474-0771x105
>> dalexander at ccesearch.com
>> http://www.ccesearch.com
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 15-May-05                                       Time: 16:35:06
------------------------------ XFMail ------------------------------



From chrisxe at gmx.at  Sun May 15 18:40:28 2005
From: chrisxe at gmx.at (Christoph Strehblow)
Date: Sun, 15 May 2005 18:40:28 +0200
Subject: [R] adjusted p-values with TukeyHSD?
Message-ID: <703DA424-EA26-44ED-A084-7E7BD8E46EE3@gmx.at>

hi list,

i have to ask you again, having tried and searched for several days...

i want to do a TukeyHSD after an Anova, and want to get the adjusted  
p-values after the Tukey Correction.
i found the p.adjust function, but it can only correct for "holm",  
"hochberg", bonferroni", but not "Tukey".

Is it not possbile to get adjusted p-values after Tukey-correction?

sorry, if this is an often-answered-question, but i didn??t find it on  
the list archive...

thx a lot, list, Chris


Christoph Strehblow, MD
Department of Rheumatology, Diabetes and Endocrinology
Wilhelminenspital, Vienna, Austria
chrisxe at gmx.at



From spencer.graves at pdf.com  Sun May 15 21:09:13 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 15 May 2005 12:09:13 -0700
Subject: [R] CircStats and Anova
In-Reply-To: <65544FA95ABA5243975A6930429BD9B091A6ED@icex33.ic.ac.uk>
References: <65544FA95ABA5243975A6930429BD9B091A6ED@icex33.ic.ac.uk>
Message-ID: <42879E59.9090305@pdf.com>

	  It's not clear to me what you are asking.  Have you considered 
"aov.circular" in package "circular"?  The following example comes from 
the help page for that function:

 >      x <- c(rvonmises(50, 0, 1), rvonmises(100, pi/3, 10))
 >      group <- c(rep(0, 50), rep(1, 100))
 >
 >      aov.circular(x, group)

Call:
aov.circular(x = x, group = group)


  Circular Analysis of Variance: High Concentration F-Test

          df     SS     MS     F         p
Between   1  5.901 5.9009 34.15 3.135e-08
Within  148 29.431 0.1989    NA        NA
Total   149 35.332 0.2371    NA        NA


 >      aov.circular(x, group, method="LRT")

Call:
aov.circular(x = x, group = group, method = "LRT")


  Circular Analysis of Variance: Likelihood Ratio Test

  df:      1
  ChiSq:   29.31
  p.value: 6.163e-08

	  Does this provide what you want?  If no, I suggest you follow the 
procedure in the posting guide, 
"http://www.R-project.org/posting-guide.html".  Some people answer their 
own questions following that process.  If they still have questions, 
they increase their chances of getting useful replies in part because 
their subsequent posts provides more of the information someone else 
needs to understand your concern and reply in a way that you will 
understand.  For me, one of the most useful things is a simple example 
that someone else can easily copy from an email and test in R, craft a 
reply and send it in a few seconds.

	  hope this helps.
	  spencer graves

Mueller, Adrienne wrote:

> Hi,
> If I have two sets of directional data (in radians) and want to compare them with a multifactorial anova.
> 
> Is it even legitimate to compare circular data with an anova? The books I've picked up from the library don't really say, but it looks unlikely.
> 
> If it is allowable, is my having stored the data as circular (X = as.circular(A)) something the aov() function will take into account, assuming the anova would be somewhat different for directional information?
> 
> Thanks in advance,
> Adrienne 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dlvanbrunt at gmail.com  Sun May 15 22:27:38 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Sun, 15 May 2005 15:27:38 -0500
Subject: [R] Not sure if this is "aggregate" or some other task.
Message-ID: <d332d3e105051513275e97485f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050515/80441cbe/attachment.pl

From laura at env.leeds.ac.uk  Sun May 15 22:49:24 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Sun, 15 May 2005 21:49:24 +0100 (BST)
Subject: [R] Finding Gradient from Contour map
Message-ID: <Pine.LNX.4.44.0505152144550.11207-100000@gw.env.leeds.ac.uk>

I have a detailed map matrix in R (x,y,z co-ords), and am wishing to
calculate the physical gradient at a number of points on the map wrt
different axes and averaged over a number of distances (i.e. slope in N-S
direction averaged over 10m, 50m, 100m respectively). Is there any
function within R which will allow for this?

Thanks,
Laura

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From Pierre.Lapointe at nbf.ca  Mon May 16 01:36:36 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Sun, 15 May 2005 19:36:36 -0400
Subject: [R] Centered overall title with layout()
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D06B63176@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050515/662777da/attachment.pl

From ggrothendieck at gmail.com  Mon May 16 04:45:56 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 15 May 2005 22:45:56 -0400
Subject: [R] Not sure if this is "aggregate" or some other task.
In-Reply-To: <d332d3e105051513275e97485f@mail.gmail.com>
References: <d332d3e105051513275e97485f@mail.gmail.com>
Message-ID: <971536df050515194550790c13@mail.gmail.com>

On 5/15/05, David L. Van Brunt, Ph.D. <dlvanbrunt at gmail.com> wrote:
> I have data where where I've taken some measurements three times... twice in
> rapid succession so I could check test-retest reliability of a piece of
> equipment, and then a third measurement some time later.
> 
> Not I'd like to do an analysis where I have two scores... the first being
> the mean of the first two taken the same day, and the second being the one
> taken later.
> 
> I have a lot of other variables in the row, and I'd like to do the same
> thing to all of them. Soo....
> 
> Data.Frame:
> 
> Subj Obs MeasureA MeasureB
> 1 1 45 685
> 1 2 50 690
> 1 3 48 693
> 2 1 39 595
> 2 2 41 585
> 2 3 45 343
> 
> should become:
> Subj Obs MeasureA MeasureB
> 1 1 47.5 687.5
> 1 2 50 690
> 2 1 40 590
> 2 2 41 585
> 
> It seems like a job for "aggregate", but I want to collapse on only cases
> where observation # < 3, and take the mean of a few vars in the aggregation.
> I can't seem to make it work, and didn't find examples that were on the
> mark. I think I'm suffering from "prospective interference" and my SPSS
> syntax knowledge to do exactly this is just getting in my way.
> 
> any volunteers? I'd very grateful. Thanks!

This seems like a situation where you want to process the sub-data.frame
corresponding to each Subject.  'by' will do that.    If 'z' is your data.frame
then 'z.by' is an object of class "by" which has the desired result and
the last line converts that to a data.frame:


f <- function(x) {
      y <- colMeans(x[1:2,])
      y[2] <- 1
     rbind(y, x[2,])
}
z.by <- by(z, list(Subj = z$Subj), f)
do.call(rbind, z.by)



From ssk2031 at columbia.edu  Mon May 16 05:09:48 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Sun, 15 May 2005 23:09:48 -0400
Subject: [R] plotting gam curve against predictors
In-Reply-To: <1a23742f0505140109300aa28f@mail.gmail.com>
References: <1a23742f0505140109300aa28f@mail.gmail.com>
Message-ID: <42880EFC.9080905@columbia.edu>



Sonya Ku wrote:
> Hi 
> I am just beginning to learn R and have fitted several GAM to my
> species presense/absence data.
> 
> I have used plot(x,y) using fitted.values as a y variable against
> predictors. However, it is hard to see general relationships where
> there is wide spread in predicted values for any x.
> 
> So I'd like to 
> 1) plot general curves, not individual fitted values against predictors

What exactly do you mean by "general curves" or "general relationships" ?

-s.



From p.murrell at auckland.ac.nz  Mon May 16 05:36:57 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 16 May 2005 15:36:57 +1200
Subject: [R] How to make label in multi plot
References: <3635ddc205051204536c97b6cb@mail.gmail.com>
Message-ID: <42881559.30506@stat.auckland.ac.nz>

Hi

(cc'ed to Pierre Lapointe because this should answer the question about 
"[R] Centered overall title with layout()" as well)


Muhammad Subianto wrote:
> Dear R-Help,
> As a reference about multi plot,
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48725.html
> 
> I want to know how can I make a label for each row.
> I mean like,
> 
>                   ------------        -------------      --------------
>                  |            |       |            |      |            |
> Group A     |   plot1  |       |  plot 2  |      |  plot 3  |       
>                  |            |       |            |      |            |
>                  -------------        -------------      --------------
> 
>                                       -------------
>                                       |            |
> Group B                          |  plot 4  |
>                                       |            |
>                                       -------------
> 


Two ways (at least):

(i)  use an outer margin ...

ooma <- par(oma=c(0, 5, 0, 0))
layout(rbind(c(1, 2, 3),
              c(0, 4, 0)))
plot(1:10, main="Plot 1")
olas <- par(las=2)
mtext("Group A", side=2, adj=1, outer=TRUE,
       at=0.75)
par(olas)
plot(1:20, main="Plot 2")
plot(1:30, main="Plot 3")
plot(1:40, main="Plot 4")
olas <- par(las=2)
mtext("Group B", side=2, adj=1, outer=TRUE,
       at=0.25)
par(olas)
# new page!
plot(1:40, main="Plot 5")
par(ooma)

(ii) create an extra row/coloumn in the layout for the labels:

layout(rbind(c(1, 2, 3, 4),
              c(5, 0, 6, 0)),
        widths=c(2, 5, 5, 5))
# "plot 1" is label for row 1
omar <- par(mar=rep(0, 4))
plot.new()
text(0.5, 0.5, "Group A", cex=2)
par(omar)
plot(1:10, main="Plot 1")
plot(1:20, main="Plot 2")
plot(1:30, main="Plot 3")
# "plot 5" is label for row 2
omar <- par(mar=rep(0, 4))
plot.new()
text(0.5, 0.5, "Group B", cex=2)
par(omar)
plot(1:40, main="Plot 4")
# new page!
plot(1:40, main="Plot 5")


Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From j_brindle at hotmail.com  Mon May 16 06:37:40 2005
From: j_brindle at hotmail.com (Jim BRINDLE)
Date: Mon, 16 May 2005 00:37:40 -0400
Subject: [R] Mann-Whitney & Wilcoxon Rank Sum
Message-ID: <BAY20-DAV11070BBD38B1F280E9D52280150@phx.gbl>

Hello,

I am hoping someone could shed some light into the Wilcoxon Rank Sum Test 
for me?  In looking through Stats references, the Mann-Whitney U-test and 
the Wilcoxon Rank Sum Test are statistically equivalent.  When using the 
following dataset:

m <- c(2.0863,2.1340,2.1008,1.9565,2.0413,NA,NA)
f <- c(1.8938,1.9709,1.8613,2.0836,1.9485,2.0630,1.9143)

and the wilcox.test command as below:

wilcox.test(m,f, paired = FALSE, alternative = c("two.sided"))

I get a test statistic (W) of 30.  When I perform this test by hand 
utilizing the methodology laid out in Ch. 6 of Ott & Longnecker I get a 
value of 45.  Any insight or good reference(s) as to the algorithm R is 
using or this issue in general would be most appreciated.

Thanks....



From corr at fas.harvard.edu  Mon May 16 06:56:20 2005
From: corr at fas.harvard.edu (Anders Schwartz Corr)
Date: Mon, 16 May 2005 00:56:20 -0400 (EDT)
Subject: [R] row.names need reordering
Message-ID: <Pine.LNX.4.58.0505160042070.4073@ls01.fas.harvard.edu>


Hi,

The row.names in my matrix seem to be out of order. I don't remember
putting row.names in in the first place, I don't see what use they are,
and they are out of order (perhaps because I sorted them at one point when
the data was in data.frame format). Can I delete the rownames? or at least
just reorder them in proper order? I know how to delete them -- how do I
reorder them sequentially?

Thanks,

Anders Corr



From Meredith.Briggs at team.telstra.com  Mon May 16 07:24:35 2005
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Mon, 16 May 2005 15:24:35 +1000
Subject: [R] turning labels into a vector
Message-ID: <3B5823541A25D311B3B90008C7F90564199CB56C@ntmsg0092.corpmail.telstra.com.au>

Hello


1/ 'priors' is a table looking like:

"W123" "T678" "S789"
  23          42       11
  12          35         9
etc

2/ WBS <- labels(priors) gives me a result of class list and length 1 looking like:

"W123" "T678" "S789"



I want to read W123 into X[1] as W, T687 into X[2] as T and S789 into X[3] as S using substr(X[1],1,1) but I'm having trouble extracting each group of 4 digits from WBS

Any help would be gratefully accepted.

thanks
Meredith



From ripley at stats.ox.ac.uk  Mon May 16 07:52:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 May 2005 06:52:33 +0100 (BST)
Subject: [R] Mann-Whitney & Wilcoxon Rank Sum
In-Reply-To: <BAY20-DAV11070BBD38B1F280E9D52280150@phx.gbl>
References: <BAY20-DAV11070BBD38B1F280E9D52280150@phx.gbl>
Message-ID: <Pine.LNX.4.61.0505160633190.20582@gannet.stats>

On Mon, 16 May 2005, Jim BRINDLE wrote:

> Hello,
>
> I am hoping someone could shed some light into the Wilcoxon Rank Sum Test
> for me?  In looking through Stats references, the Mann-Whitney U-test and
> the Wilcoxon Rank Sum Test are statistically equivalent.

Yes, but not numerically: they differ by a constant (in the data, a 
function of the data size).

>  When using the
> following dataset:
>
> m <- c(2.0863,2.1340,2.1008,1.9565,2.0413,NA,NA)
> f <- c(1.8938,1.9709,1.8613,2.0836,1.9485,2.0630,1.9143)
>
> and the wilcox.test command as below:
>
> wilcox.test(m,f, paired = FALSE, alternative = c("two.sided"))
>
> I get a test statistic (W) of 30.  When I perform this test by hand
> utilizing the methodology laid out in Ch. 6 of Ott & Longnecker I get a
> value of 45.  Any insight or good reference(s) as to the algorithm R is
> using or this issue in general would be most appreciated.

I don't know that book but the R help page does have references.  Also, 
?pwilcox says

      This distribution is obtained as follows.  Let 'x' and 'y' be two
      random, independent samples of size 'm' and 'n'. Then the Wilcoxon
      rank sum statistic is the number of all pairs '(x[i], y[j])' for
      which 'y[j]' is not greater than 'x[i]'.  This statistic takes
      values between '0' and 'm * n', and its mean and variance are 'm *
      n / 2' and 'm * n * (m + n + 1) / 12', respectively.

Your samples have length 5 (after removing NAs) and 7 and no ties.  The R 
code is readable by

> getAnywhere("wilcox.test.default")

as essentially

 	r <- rank(c(x,y))
 	sum(r[seq(along = x)]) - n.x * (n.x + 1)/2

I guess your reference just uses the first term.

Another way of looking at this is whether ranks start at 0 or at 1 (as in 
rank()): R's definition is the rank sum with 0-based ranks.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Simon.Blomberg at anu.edu.au  Mon May 16 07:56:07 2005
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Mon, 16 May 2005 15:56:07 +1000
Subject: [R] turning labels into a vector
In-Reply-To: <3B5823541A25D311B3B90008C7F90564199CB56C@ntmsg0092.corpmail.telstra.com.a
	u>
References: <3B5823541A25D311B3B90008C7F90564199CB56C@ntmsg0092.corpmail.telstra.com.a
	u>
Message-ID: <a06110403beade5c1c815@[150.203.51.113]>

Try WBS.labs <- as.vector(WBS)
WBS.labs[1]
substr(WBS.labs[1],1,1)
etc..

Cheers,

Simon.



>Hello
>
>
>1/ 'priors' is a table looking like:
>
>"W123" "T678" "S789"
>   23          42       11
>   12          35         9
>etc
>
>2/ WBS <- labels(priors) gives me a result of class list and length 
>1 looking like:
>
>"W123" "T678" "S789"
>
>
>
>I want to read W123 into X[1] as W, T687 into X[2] as T and S789 
>into X[3] as S using substr(X[1],1,1) but I'm having trouble 
>extracting each group of 4 digits from WBS
>
>Any help would be gratefully accepted.
>
>thanks
>Meredith
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Visiting Fellow
School of Botany & Zoology
The Australian National University
Canberra ACT 0200
Australia

T: +61 2 6125 8057  email: Simon.Blomberg at anu.edu.au
F: +61 2 6125 5573

CRICOS Provider # 00120C



From Simon.Blomberg at anu.edu.au  Mon May 16 07:58:33 2005
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Mon, 16 May 2005 15:58:33 +1000
Subject: [R] turning labels into a vector
Message-ID: <a06110404beade6e80d6e@[150.203.51.113]>

Umm. Again...



Try WBS.labs <- as.vector(WBS[[1]])
WBS.labs[1]
substr(WBS.labs[1],1,1)
etc..

Cheers,

Simon.



>Hello
>
>
>1/ 'priors' is a table looking like:
>
>"W123" "T678" "S789"
>   23          42       11
>   12          35         9
>etc
>
>2/ WBS <- labels(priors) gives me a result of class list and length 
>1 looking like:
>
>"W123" "T678" "S789"
>
>
>
>I want to read W123 into X[1] as W, T687 into X[2] as T and S789 
>into X[3] as S using substr(X[1],1,1) but I'm having trouble 
>extracting each group of 4 digits from WBS
>
>Any help would be gratefully accepted.
>
>thanks
>Meredith
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Visiting Fellow
School of Botany & Zoology
The Australian National University
Canberra ACT 0200
Australia

T: +61 2 6125 8057  email: Simon.Blomberg at anu.edu.au
F: +61 2 6125 5573

CRICOS Provider # 00120C



From ripley at stats.ox.ac.uk  Mon May 16 08:07:29 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 May 2005 07:07:29 +0100 (BST)
Subject: [R] row.names need reordering
In-Reply-To: <Pine.LNX.4.58.0505160042070.4073@ls01.fas.harvard.edu>
References: <Pine.LNX.4.58.0505160042070.4073@ls01.fas.harvard.edu>
Message-ID: <Pine.LNX.4.61.0505160705080.20582@gannet.stats>

On Mon, 16 May 2005, Anders Schwartz Corr wrote:

> The row.names in my matrix seem to be out of order. I don't remember
> putting row.names in in the first place, I don't see what use they are,
> and they are out of order (perhaps because I sorted them at one point when
> the data was in data.frame format). Can I delete the rownames? or at least
> just reorder them in proper order? I know how to delete them -- how do I
> reorder them sequentially?

Matrices strictly have rownames, data frames row.names.  So try

rownames(A) <- seq(len=nrow(A))

(to delete them, use NULL as the rhs).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon May 16 08:34:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 May 2005 08:34:27 +0200
Subject: [R] Mann-Whitney & Wilcoxon Rank Sum
In-Reply-To: <Pine.LNX.4.61.0505160633190.20582@gannet.stats>
References: <BAY20-DAV11070BBD38B1F280E9D52280150@phx.gbl>
	<Pine.LNX.4.61.0505160633190.20582@gannet.stats>
Message-ID: <x23bsnd6h8.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> as essentially
> 
>  	r <- rank(c(x,y))
>  	sum(r[seq(along = x)]) - n.x * (n.x + 1)/2
> 
> I guess your reference just uses the first term.
> 
> Another way of looking at this is whether ranks start at 0 or at 1 (as
> in rank()): R's definition is the rank sum with 0-based ranks.

Er, no... Then you'd be subtracting n.x. The definition is such that
the minimum value of the statistic becomes zero (2nd term is equal to
sum(1:n.x)).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From laura at env.leeds.ac.uk  Mon May 16 09:32:48 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Mon, 16 May 2005 08:32:48 +0100 (BST)
Subject: [R] Mental Block with PCA of multivariate time series!
Message-ID: <Pine.LNX.4.44.0505160821450.2392-100000@gw.env.leeds.ac.uk>

Please could someone point me in the right direction as I appear to be
having a total mental block with fairly basic PCA problem!

I have a large dataframe where rows represent independent
observations and columns are variables. I am wanting to perform PCA
sequentially on blocks of nrows at a time and produce a graphical output
of the loadings for the first 2 EOFs for each variable.

I'm sure I've performed a very similar routine in the past, but the method
is currently escaping me.

Any help gratefully received!

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From r.hankin at noc.soton.ac.uk  Mon May 16 10:12:30 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 16 May 2005 09:12:30 +0100
Subject: [R] branch cuts of atan()
Message-ID: <533cfb3ef69ae67c3fff232093055bb1@soc.soton.ac.uk>

Hi

the following gave me a shock:

 > atan(2)
[1] 1.107149
 > atan(2+0i)
[1] -0.4636476+0i
 >

or, perhaps more of a gotcha:

 > atan(1.0001+0i)
[1] -0.7853482+0i
 > atan(0.9999+0i)
[1] 0.7853482+0i
 >



evidently atan()'s branch cuts aren't where I thought they were.

Where do I look for documentation on this?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From gavin.simpson at ucl.ac.uk  Mon May 16 11:02:14 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 16 May 2005 10:02:14 +0100
Subject: [R] plotting gam curve against predictors
In-Reply-To: <42880EFC.9080905@columbia.edu>
References: <1a23742f0505140109300aa28f@mail.gmail.com>
	<42880EFC.9080905@columbia.edu>
Message-ID: <42886196.9000101@ucl.ac.uk>

Suresh Krishna wrote:
> 
> 
> Sonya Ku wrote:
> 
>> Hi I am just beginning to learn R and have fitted several GAM to my
>> species presense/absence data.
>>
>> I have used plot(x,y) using fitted.values as a y variable against
>> predictors. However, it is hard to see general relationships where
>> there is wide spread in predicted values for any x.
>>
>> So I'd like to 1) plot general curves, not individual fitted values 
>> against predictors
> 
> 
> What exactly do you mean by "general curves" or "general relationships" ?
> 
> -s.

I think Sonya meant plotting the response curve of the species over the 
full environmental gradient in question rather then plotting a line that 
goes through the fitted values. If you have a small number of samples or 
they are clumped along that gradient, just plotting a line through the 
fitted points can result in a strange response curve.

The simplest way is to create a new set of predictors variable(s) that 
are evenly spread across the range of your environmental data. Then use 
the predict method to predict the abundance of each species for each 
value of the predictor you created.

e.g. assuming you used mgcv for GAMs (not stated), using dummy code:

# fit the model
gam.mod <- gam(spp ~ s(pH), family = binomial(link = "logit"))
# create some new predictor data aross range of your measured values
# Here we create 100 evenly spaced values
new.dat <- data.frame(pH = seq(min(pH), max(pH), length = 100))
# predict abundance for these new values
gam.pred <- predict(gam.mod, newdata = new.dat, type = "response")
# plot the response curve
plot(new.dat, gam.pred, type = "l")

if you want the standard errors as well then:

# note se.fit = TRUE, gam.pred contains to items in a list $fit and $se.fit
gam.pred <- predict(gam.mod, newdata = new.dat, type = "response",
	se.fit = TRUE)
# so we have to use pam.pred$fit for the predicted values
plot(new.dat, gam.pred$fit, type = "l")
# and +/- the se.fit for the standard errors
lines(new.dat, gam.pred$fit + gam.pred$se.fit, lty = "dotted")
lines(new.dat, gam.pred$fit, type = "l", lty = "dotted")

See ?predict.gam for more information.

I assume the procedure will be similar if you use package gam instead, 
the details of the predict methods may differ though.

HTH

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From f.lyazrhi at envt.fr  Mon May 16 11:09:29 2005
From: f.lyazrhi at envt.fr (Faouzi LYAZRHI)
Date: Mon, 16 May 2005 11:09:29 +0200
Subject: [R] graphic
Message-ID: <42886349.5090505@envt.fr>

Hi,

poids taille    fumeur    sexe    sport    etat
85    184    oui    homme    1    malade
65    175    oui    homme    1    malade
74    180    oui    homme    2    gueri
79    175    oui    homme    2    malade
71    165    non    homme    3    gueri
80    185    non    homme    3    gueri
75    180    non    homme    4    malade
69    155    non    homme    4    malade
74    168    oui    femme    1    malade
64    171    oui    femme    1    gueri
63    169    oui    femme    2    gueri
70    165    oui    femme    2    gueri
55    167    non    femme    3    malade
65    164    non    femme    3    gueri
67    155    non    femme    4    malade
70    175    non    femme    4    gueri

I would like to make a graphic taille againts poids by sex in same 
graphic with  different labels
Thank you for your help
Fawzi



From gavin.simpson at ucl.ac.uk  Mon May 16 11:14:39 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 16 May 2005 10:14:39 +0100
Subject: [R] Mental Block with PCA of multivariate time series!
In-Reply-To: <Pine.LNX.4.44.0505160821450.2392-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0505160821450.2392-100000@gw.env.leeds.ac.uk>
Message-ID: <4288647F.8090409@ucl.ac.uk>

Laura Quinn wrote:
> Please could someone point me in the right direction as I appear to be
> having a total mental block with fairly basic PCA problem!
> 
> I have a large dataframe where rows represent independent
> observations and columns are variables. I am wanting to perform PCA
> sequentially on blocks of nrows at a time and produce a graphical output
> of the loadings for the first 2 EOFs for each variable.
> 
> I'm sure I've performed a very similar routine in the past, but the method
> is currently escaping me.
> 
> Any help gratefully received!

Hi Laura,

data(iris)
iris.dat <- iris[,1:4]
pca.1 <- prcomp(iris.dat[1:50, ], scale = TRUE)
pca.2 <- prcomp(iris.dat[51:100, ], scale = TRUE)
pca.3 <- prcomp(iris.dat[100:150, ], scale = TRUE)

biplot(pca.1)
etc...

There is a better way of subsetting this data set as the 5th col of iris 
is a factor and we could use the subset argument to prcomp to do the 
subsetting without having to know that there are 50 rows per species. 
Take a look at that argument if you have a variable that defines the 
blocks for you.

Is this what you were after?

All the best,

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ripley at stats.ox.ac.uk  Mon May 16 11:23:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 May 2005 10:23:18 +0100 (BST)
Subject: [R] branch cuts of atan()
In-Reply-To: <533cfb3ef69ae67c3fff232093055bb1@soc.soton.ac.uk>
References: <533cfb3ef69ae67c3fff232093055bb1@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.61.0505161015180.23003@gannet.stats>

On Mon, 16 May 2005, Robin Hankin wrote:

> Hi
>
> the following gave me a shock:
>
>> atan(2)
> [1] 1.107149
>> atan(2+0i)
> [1] -0.4636476+0i
>>
>
> or, perhaps more of a gotcha:
>
>> atan(1.0001+0i)
> [1] -0.7853482+0i
>> atan(0.9999+0i)
> [1] 0.7853482+0i
>>
>
>
>
> evidently atan()'s branch cuts aren't where I thought they were.
>
> Where do I look for documentation on this?

In the sources.  Specifically for complex atan() in src/main/complex.c

 	/* Complex Arctangent Function */
 	/* Equation (4.4.39) Abramowitz and Stegun */

static void z_atan(Rcomplex *r, Rcomplex *z)
...


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From laura at env.leeds.ac.uk  Mon May 16 11:33:43 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Mon, 16 May 2005 10:33:43 +0100 (BST)
Subject: [R] Mental Block with PCA of multivariate time series!
In-Reply-To: <4288647F.8090409@ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0505161027260.2392-100000@gw.env.leeds.ac.uk>

Sorry, I don't think I made myself clear enough with my initial query!

I am wishing to investigate the temporal evolution of the pca: if we
assume that every 50 rows of my data frame is representitive of, for
instance, 1 day of data, I am hoping to automate a process whereby a pca
is performed on every 50 rows of data and the loading for PC1 and PC2 for
each variable (i.e. each column) is represented as a point on a plot - so
a years' data will be represented as two lines (representing PC1 and PC2)
on a time series plot for each variable.



Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

On Mon, 16 May 2005, Gavin Simpson wrote:

> Laura Quinn wrote:
> > Please could someone point me in the right direction as I appear to be
> > having a total mental block with fairly basic PCA problem!
> >
> > I have a large dataframe where rows represent independent
> > observations and columns are variables. I am wanting to perform PCA
> > sequentially on blocks of nrows at a time and produce a graphical output
> > of the loadings for the first 2 EOFs for each variable.
> >
> > I'm sure I've performed a very similar routine in the past, but the method
> > is currently escaping me.
> >
> > Any help gratefully received!
>
> Hi Laura,
>
> data(iris)
> iris.dat <- iris[,1:4]
> pca.1 <- prcomp(iris.dat[1:50, ], scale = TRUE)
> pca.2 <- prcomp(iris.dat[51:100, ], scale = TRUE)
> pca.3 <- prcomp(iris.dat[100:150, ], scale = TRUE)
>
> biplot(pca.1)
> etc...
>
> There is a better way of subsetting this data set as the 5th col of iris
> is a factor and we could use the subset argument to prcomp to do the
> subsetting without having to know that there are 50 rows per species.
> Take a look at that argument if you have a variable that defines the
> blocks for you.
>
> Is this what you were after?
>
> All the best,
>
> Gav
> --
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> Gavin Simpson                     [T] +44 (0)20 7679 5522
> ENSIS Research Fellow             [F] +44 (0)20 7679 7565
> ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
> UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
> 26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
> London.  WC1H 0AP.
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>



From petr.pikal at precheza.cz  Mon May 16 11:35:31 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 16 May 2005 11:35:31 +0200
Subject: [R] graphic
In-Reply-To: <42886349.5090505@envt.fr>
Message-ID: <42888583.17719.17F8E6@localhost>

Hallo
See lattice

> temp<-read.table("clipboard", header=T)
> temp
   poids taille fumeur  sexe sport   etat
1     85    184    oui homme     1 malade
2     65    175    oui homme     1 malade
3     74    180    oui homme     2  gueri
4     79    175    oui homme     2 malade
5     71    165    non homme     3  gueri
6     80    185    non homme     3  gueri
7     75    180    non homme     4 malade
8     69    155    non homme     4 malade
9     74    168    oui femme     1 malade
10    64    171    oui femme     1  gueri
11    63    169    oui femme     2  gueri
12    70    165    oui femme     2  gueri
13    55    167    non femme     3 malade
14    65    164    non femme     3  gueri
15    67    155    non femme     4 malade
16    70    175    non femme     4  gueri
> library(lattice)
> xyplot(taille~poids|sexe, data=temp)

Cheers
Petr


On 16 May 2005 at 11:09, Faouzi LYAZRHI wrote:

> Hi,
> 
> poids taille    fumeur    sexe    sport    etat
> 85    184    oui    homme    1    malade
> 65    175    oui    homme    1    malade
> 74    180    oui    homme    2    gueri
> 79    175    oui    homme    2    malade
> 71    165    non    homme    3    gueri
> 80    185    non    homme    3    gueri
> 75    180    non    homme    4    malade
> 69    155    non    homme    4    malade
> 74    168    oui    femme    1    malade
> 64    171    oui    femme    1    gueri
> 63    169    oui    femme    2    gueri
> 70    165    oui    femme    2    gueri
> 55    167    non    femme    3    malade
> 65    164    non    femme    3    gueri
> 67    155    non    femme    4    malade
> 70    175    non    femme    4    gueri
> 
> I would like to make a graphic taille againts poids by sex in same
> graphic with  different labels Thank you for your help Fawzi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From subianto at gmail.com  Mon May 16 11:38:42 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Mon, 16 May 2005 11:38:42 +0200
Subject: [R] How to convert color to black & white
In-Reply-To: <7CBBD627E4E688499349A5D11D078316018899AD@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>
References: <7CBBD627E4E688499349A5D11D078316018899AD@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>
Message-ID: <42886A22.5060108@gmail.com>

Thank's you very much.
But I need the plot with color not gray.

Best wishes,
Muhammad Subianto

On this day 5/14/2005 3:05 AM, OlsenN at pac.dfo-mpo.gc.ca wrote:

>Muhammad,
>Here's one option:
>
>barplot(1:5,col=gray(seq(0,1,length=5)))
>
>Norm Olsen
>Fisheries and Oceans Canada
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>To: R-help at stat.math.ethz.ch
>Sent: 5/13/2005 11:40 AM
>Subject: [R] How to convert color to black & white
>
>Dear all,
>Could someone please explain to me how to convert color to black &
>white.
>For example:
>barplot(1:5,col = rainbow(5))
>Because I need to print my plot to save my ink color printer.
>I don't want to convert to grayscale, but keep it as an RGB.
>I  would be very happy if anyone could help me.
>Thank you very much in advance.
>
>Kindly regards,
>Muhammad Subianto
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>  
>



From pierre.bady at univ-lyon1.fr  Mon May 16 12:10:27 2005
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Mon, 16 May 2005 12:10:27 +0200
Subject: [R] Mental Block with PCA of multivariate time series!
In-Reply-To: <Pine.LNX.4.44.0505160821450.2392-100000@gw.env.leeds.ac.uk
 >
Message-ID: <5.1.0.14.2.20050516120752.01abeea0@pop.univ-lyon1.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050516/c9111386/attachment.pl

From laura at env.leeds.ac.uk  Mon May 16 12:17:14 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Mon, 16 May 2005 11:17:14 +0100 (BST)
Subject: [R] Fitting Contour to Data Points
Message-ID: <Pine.LNX.4.44.0505161107410.2392-100000@gw.env.leeds.ac.uk>

Apologies for the mass mailing today!

I am attempting to produce a contour plot for phsical data on a map
matrix. I have a small number of data points which each has an (x,y)
co-ordinate together with a corresponding value which I would like to
cvreate a contour plot for.

I have tried the following code:

contour(data$x,data$y,data$value)

but am told:

Error in contour.default(data$x, data$y, data$value) :
        increasing x and y values expected

I have re-ordered the matrix so that the x values are increasing, but
these do not necessarrily correspond with increasing y values (as this is
not a regular grid)...can anyone offer a way around this??

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From gavin.simpson at ucl.ac.uk  Mon May 16 12:21:13 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 16 May 2005 11:21:13 +0100
Subject: [R] Mental Block with PCA of multivariate time series!
In-Reply-To: <Pine.LNX.4.44.0505161027260.2392-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0505161027260.2392-100000@gw.env.leeds.ac.uk>
Message-ID: <42887419.7050300@ucl.ac.uk>

Laura Quinn wrote:
> Sorry, I don't think I made myself clear enough with my initial query!
> 
> I am wishing to investigate the temporal evolution of the pca: if we
> assume that every 50 rows of my data frame is representitive of, for
> instance, 1 day of data, I am hoping to automate a process whereby a pca
> is performed on every 50 rows of data and the loading for PC1 and PC2 for
> each variable (i.e. each column) is represented as a point on a plot - so
> a years' data will be represented as two lines (representing PC1 and PC2)
> on a time series plot for each variable.
> 
> 
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> On Mon, 16 May 2005, Gavin Simpson wrote:
> 
> 
>>Laura Quinn wrote:
>>
>>>Please could someone point me in the right direction as I appear to be
>>>having a total mental block with fairly basic PCA problem!
>>>
>>>I have a large dataframe where rows represent independent
>>>observations and columns are variables. I am wanting to perform PCA
>>>sequentially on blocks of nrows at a time and produce a graphical output
>>>of the loadings for the first 2 EOFs for each variable.
>>>
>>>I'm sure I've performed a very similar routine in the past, but the method
>>>is currently escaping me.
>>>
>>>Any help gratefully received!
>>
>>Hi Laura,
>>
>>data(iris)
>>iris.dat <- iris[,1:4]
>>pca.1 <- prcomp(iris.dat[1:50, ], scale = TRUE)
>>pca.2 <- prcomp(iris.dat[51:100, ], scale = TRUE)
>>pca.3 <- prcomp(iris.dat[100:150, ], scale = TRUE)
>>
>>biplot(pca.1)
>>etc...
>>
>>There is a better way of subsetting this data set as the 5th col of iris
>>is a factor and we could use the subset argument to prcomp to do the
>>subsetting without having to know that there are 50 rows per species.
>>Take a look at that argument if you have a variable that defines the
>>blocks for you.
>>
>>Is this what you were after?
>>
>>All the best,
>>
>>Gav
>>--
>>%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>>Gavin Simpson                     [T] +44 (0)20 7679 5522
>>ENSIS Research Fellow             [F] +44 (0)20 7679 7565
>>ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
>>UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
>>26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
>>London.  WC1H 0AP.
>>%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>>
> 
> 
> 
>

Hi Laura,

Sorry for not quite understanding the specifics, does this do what you want?

# generate some random data for this example
dat <- data.frame(var1 = rnorm(1:1000), var2 = runif(1:1000), var3 = 
rnorm(1:1000) + runif(1:1000), var4 = as.factor(rep(1:10, rep(100, 10))))
# create a list of pca loadings on axis 1, 2
temp <- by(dat[,1:3], dat$var4, function(x) prcomp(x, scale =
	TRUE)$rotation[,1:2])
# plot it
matplot(t(sapply(temp, function(x) x[,1])), type = "n")
# add the lines
matlines(t(sapply(temp, function(x) x[,1])), lty = "solid", col = 
c("red", "blue", "green"))
matlines(t(sapply(temp, function(x) x[,2])), lty = "dotted", col = 
c("red", "blue", "green"))

It isn't pretty - you'll need to calculate the x/ylims for the matplot 
call, but if it is want you are after the plotting should be fairly easy 
thing to work out.

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From pierre.bady at univ-lyon1.fr  Mon May 16 12:21:28 2005
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Mon, 16 May 2005 12:21:28 +0200
Subject: [R] Mental Block with PCA of multivariate time series!
In-Reply-To: <Pine.LNX.4.44.0505161027260.2392-100000@gw.env.leeds.ac.uk
 >
References: <4288647F.8090409@ucl.ac.uk>
Message-ID: <5.1.0.14.2.20050516121813.01a50000@pop.univ-lyon1.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050516/bc556740/attachment.pl

From slist at oomvanlieshout.net  Mon May 16 12:23:12 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Mon, 16 May 2005 12:23:12 +0200
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <42853E9B.6080200@vanderbilt.edu>
References: <42849BC4.9030108@oomvanlieshout.net>
	<4284F51B.4040900@vanderbilt.edu>
	<4285027A.2040208@oomvanlieshout.net>
	<42853E9B.6080200@vanderbilt.edu>
Message-ID: <42887490.90304@oomvanlieshout.net>

Dear David,

I would like to use summarize(Hmisc) and print.xtable(xtable) in a 
single Sweave document, but a conflict with the 'label' function 
prohibits this at the moment!

Would you be able to correct the conflicting code? I will gladly test 
the new package!

I have tried latex(Hmisc) to export the anova table, but results are not 
promising! I prefer xtable!!

Thanks,

Sander.

Frank E Harrell Jr wrote:
> Sander Oom wrote:
>> Dear Frank,
>>
>> I have a Sweave document in which I export anova (aov) tables to Latex 
>> and calculate some summary statistics with summarize{Hmisc} for a 
>> graph (as in the example below).
>>
>> I currently use the following code for the aov tables:
>> <<results=tex>>=
>>   tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
>>   tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut , data=tmp)
>>   tmpTable <- xtable (tmpAov ,
>>     caption="ANOVA table for vegetation height.",
>>     label="tab:AnovaHeight"
>>     )
>>   print.xtable(tmpTable, type="latex", floating=TRUE,
>>     table.placement="ht", caption.placement="top",
>>     latex.environments=c("center"))
>>     )
>> @
>>
>> I used xtables, because it has a working aov example. I would be happy 
>> to use an alternative if I knew how! Would you have sample code to 
>> illustrate how to export an aov table to Latex using latex{Hmisc}.
>>
>> Thanks very much for your help,
>>
>> Sander.
>>
>> Frank E Harrell Jr wrote:
>>
>>> Sander Oom wrote:
>>>
>>>> Dear R users,
>>>>
>>>> The Sweave code below runs fine, as it is. However, an error occurs 
>>>> when the line 'library(xtable)' is uncommented:
>>>> Error:  chunk 1
>>>> Error in "label<-"(`*tmp*`, value = "month") :
>>>>         no applicable method for "label<-"
>>>>
>>>> Is anybody aware of this and knows a workaround?
>>>>
>>>> Thanks,
>>>>
>>>> Sander.
>>>>
>>>> *******************
>>>>
>>>> \documentclass[a4paper]{article}
>>>> \title{Sweave Test for summarize}
>>>> \author{Sander Oom}
>>>>
>>>> \usepackage{a4wide}
>>>>
>>>> \begin{document}
>>>>
>>>> \maketitle
>>>>
>>>> \begin{figure}[ht]
>>>> \begin{center}
>>>> <<fig=TRUE,echo=FALSE>>=
>>>>   # library(xtable)
>>>>   library(Hmisc)
>>>>   set.seed(111)
>>>>   dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
>>>>   month <- dfr$month
>>>>   year <- dfr$year
>>>>   y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
>>>>   s <- summarize(y, llist(month,year), smedian.hilow, conf.int=.5)
>>>>   print(xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s,
>>>>         keys='lines', method='alt', type='b'))
>>>> @
>>>> \end{center}
>>>> \end{figure}
>>>>
>>>> \end{document}
>>>>
>>>> ************************
>>>>
>>>>
>>>>
>>>>  > version
>>>>          _
>>>> platform i686-pc-linux-gnu
>>>> arch     i686
>>>> os       linux-gnu
>>>> system   i686, linux-gnu
>>>> status
>>>> major    2
>>>> minor    1.0
>>>> year     2005
>>>> month    04
>>>> day      18
>>>> language R
>>>>
>>>>
>>>
>>> I feel this is an xtable problem because Hmisc has being using label 
>>> and label<- since 1991.
>>>
>>> Frank
>>>
>>
> 
> There are ways to make functions from one area override those from 
> another, but the real solution is to ask the xtable author not to have 
> functions that conflict with the (older) Hmisc package.  -Frank
> 

-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From gavin.simpson at ucl.ac.uk  Mon May 16 12:41:04 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 16 May 2005 11:41:04 +0100
Subject: [R] Fitting Contour to Data Points
In-Reply-To: <Pine.LNX.4.44.0505161107410.2392-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0505161107410.2392-100000@gw.env.leeds.ac.uk>
Message-ID: <428878C0.3030703@ucl.ac.uk>

Laura Quinn wrote:
> Apologies for the mass mailing today!
> 
> I am attempting to produce a contour plot for phsical data on a map
> matrix. I have a small number of data points which each has an (x,y)
> co-ordinate together with a corresponding value which I would like to
> cvreate a contour plot for.
> 
> I have tried the following code:
> 
> contour(data$x,data$y,data$value)
> 
> but am told:
> 
> Error in contour.default(data$x, data$y, data$value) :
>         increasing x and y values expected
> 
> I have re-ordered the matrix so that the x values are increasing, but
> these do not necessarrily correspond with increasing y values (as this is
> not a regular grid)...can anyone offer a way around this??
> 
> Laura Quinn

You are misunderstanding what the x, y and z arguments want. From 
?contour we have

      x,y: locations of grid lines at which the values in 'z' are
           measured.  These must be in ascending order.  By default,
           equally spaced values from 0 to 1 are used.  If 'x' is a
           'list', its components 'x$x' and 'x$y' are used for 'x' and
           'y', respectively. If the list has component 'z' this is used
           for 'z'.

        z: a matrix containing the values to be plotted ('NA's are
           allowed).  Note that 'x' can be used instead of 'z' for
           convenience.

So if you have a 10 x 10 grid, you will have 10 values each for x and y 
and a 10x10 matrix of the 100 points as z. But you don't have a regular 
grid so you can't use contour directly. Without knowing your data, 
you'll have to get values on to a regular grid using something like 
package akima and it's function interp(), then contour the interpolated 
surface.

There are likely to be other ways. MASS (the book) has an example of 
using loess() to predict surfaces from irregular data for example...

HTH

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From maechler at stat.math.ethz.ch  Mon May 16 12:45:49 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 16 May 2005 12:45:49 +0200
Subject: [R] Suppressing warning messages
In-Reply-To: <42875F92.40001@statistik.uni-dortmund.de>
References: <425EFDE6.5040204@coubros.com> <42875BE9.4030608@coubros.com>
	<42875D48.9040205@acelerate.com>
	<42875F92.40001@statistik.uni-dortmund.de>
Message-ID: <17032.31197.453697.7843@stat.math.ethz.ch>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Sun, 15 May 2005 16:41:22 +0200 writes:

    UweL> Kjetil Brinchmann Halvorsen wrote:
    >> Tolga Uzuner wrote:
    >> 
    >>> How do I suppress the following ?
    >>> 
    >>> Warning messages: 1: the condition has length > 1 and
    >>> only the first element will be used in: if (strike ==
    >>> forward) atmvol(forward, t, alpha, beta, rho, upsilon)
    >>> else { 2: the condition has length > 1 and only the
    >>> first element will be used in: if (x(z) == 0) 1 else
    >>> z/x(z)
    >>> 
    >>> ______________________________________________
    >>> R-help at stat.math.ethz.ch mailing list
    >>> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    >>> read the posting guide!
    >>> http://www.R-project.org/posting-guide.html
    >>> 
    >>> 
    >>> 
    >> Maybe better to understand what generates the warning!

    UweL> Yes! In both cases you should really look why you are
    UweL> using *conditions of length > 1*! 

yes, indeed!
This is a bug almost always (my subjective probability : 0.995)

Maybe you want to use
     if(any(...)) 
or   if(all(...)) 
instead of the current  if(...)  

    UweL> And if this is intended, you certainly want to use "ifelse()" rather
    UweL> than "if(){} else{}".

(from my above guess, the probability for this would be about 1:200)

Martin

    >> To assure you are uninformed, say options(warn=-1)
    >> 
    >> Kjetil



From subianto at gmail.com  Mon May 16 12:47:50 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Mon, 16 May 2005 12:47:50 +0200
Subject: [R] How to make label in multi plot
In-Reply-To: <42881559.30506@stat.auckland.ac.nz>
References: <3635ddc205051204536c97b6cb@mail.gmail.com>
	<42881559.30506@stat.auckland.ac.nz>
Message-ID: <42887A56.1080801@gmail.com>

Dear Dr. Paul Murrel,
Yes, this is exactly what I need.
Thank's you very much.

Best wishes,
Muhammad Subianto


On this day 5/16/2005 5:36 AM, Paul Murrell wrote:

> Hi
>
> (cc'ed to Pierre Lapointe because this should answer the question 
> about "[R] Centered overall title with layout()" as well)
>
>
> Muhammad Subianto wrote:
>
>> Dear R-Help,
>> As a reference about multi plot,
>> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48725.html
>>
>> I want to know how can I make a label for each row.
>> I mean like,
>>
>>                   ------------        -------------      --------------
>>                  |            |       |            |      |            |
>> Group A     |   plot1  |       |  plot 2  |      |  plot 3  |       
>>                  |            |       |            |      |            |
>>                  -------------        -------------      --------------
>>
>>                                       -------------
>>                                       |            |
>> Group B                          |  plot 4  |
>>                                       |            |
>>                                       -------------
>>
>
>
> Two ways (at least):
>
> (i)  use an outer margin ...
>
> ooma <- par(oma=c(0, 5, 0, 0))
> layout(rbind(c(1, 2, 3),
>              c(0, 4, 0)))
> plot(1:10, main="Plot 1")
> olas <- par(las=2)
> mtext("Group A", side=2, adj=1, outer=TRUE,
>       at=0.75)
> par(olas)
> plot(1:20, main="Plot 2")
> plot(1:30, main="Plot 3")
> plot(1:40, main="Plot 4")
> olas <- par(las=2)
> mtext("Group B", side=2, adj=1, outer=TRUE,
>       at=0.25)
> par(olas)
> # new page!
> plot(1:40, main="Plot 5")
> par(ooma)
>
> (ii) create an extra row/coloumn in the layout for the labels:
>
> layout(rbind(c(1, 2, 3, 4),
>              c(5, 0, 6, 0)),
>        widths=c(2, 5, 5, 5))
> # "plot 1" is label for row 1
> omar <- par(mar=rep(0, 4))
> plot.new()
> text(0.5, 0.5, "Group A", cex=2)
> par(omar)
> plot(1:10, main="Plot 1")
> plot(1:20, main="Plot 2")
> plot(1:30, main="Plot 3")
> # "plot 5" is label for row 2
> omar <- par(mar=rep(0, 4))
> plot.new()
> text(0.5, 0.5, "Group B", cex=2)
> par(omar)
> plot(1:40, main="Plot 4")
> # new page!
> plot(1:40, main="Plot 5")
>
>
> Paul



From f.calboli at imperial.ac.uk  Mon May 16 12:46:54 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 16 May 2005 11:46:54 +0100
Subject: [R] parsing speed
Message-ID: <1116240414.23816.70.camel@localhost.localdomain>

Hi everyone,

I have a question on parsing speed.

I have two functions:

F1
F2

As things are now, F2 calls F1 internally:

F2 =  function(x){
if (something == 1){
y = F1(x)
}
if (something ==2){
do whatever
}
}

*Assuming there could be some difference*, is is faster to use the code
as written above or should I actually write the statements of F1 to make
the parsing faster? 

Regards,

Federico Calboli

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From r.hankin at noc.soton.ac.uk  Mon May 16 13:17:44 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 16 May 2005 12:17:44 +0100
Subject: [R] branch cuts of atan()
In-Reply-To: <Pine.LNX.4.61.0505161015180.23003@gannet.stats>
References: <533cfb3ef69ae67c3fff232093055bb1@soc.soton.ac.uk>
	<Pine.LNX.4.61.0505161015180.23003@gannet.stats>
Message-ID: <e5dd1eeba0d7c1293f87de1015158278@soc.soton.ac.uk>

Professor Ripley

thanks for this.  Always good to know that I'm not missing any 
documentation!

The source is clear;  formula 4.4.39 effectively has a branch cut at 
|z|=1.
A 'n' S show the standard branch cuts in their figure 4.4, which are 
different (they
were the ones I was expecting).  Mathematica and Maple both have branch 
cuts
matching ams-55.

Should the the Trig.Rd manpage   warn that the branch cuts are 
non-standard?
Simple workarounds exist to shift them to the standard place.

Is this type of concern worthy of a  bug report?


The archetype would be

  tan(atan(2))
[1] 2
 > tan(atan(2+0i))
[1] -0.5+0i
 >



best wishes

Robin





On May 16, 2005, at 10:23 am, Prof Brian Ripley wrote:

> On Mon, 16 May 2005, Robin Hankin wrote:
>
>> Hi
>>
>>> atan(1.0001+0i)
>> [1] -0.7853482+0i
>>> atan(0.9999+0i)
>> [1] 0.7853482+0i
>>>
>>
>>
>>
>> evidently atan()'s branch cuts aren't where I thought they were.
>>
>> Where do I look for documentation on this?
>
> In the sources.  Specifically for complex atan() in src/main/complex.c
>
> 	/* Complex Arctangent Function */
> 	/* Equation (4.4.39) Abramowitz and Stegun */
>
> static void z_atan(Rcomplex *r, Rcomplex *z)
> ...
>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From luca at stat.unipg.it  Mon May 16 22:42:14 2005
From: luca at stat.unipg.it (Luca Scrucca)
Date: Mon, 16 May 2005 13:42:14 -0700 (PDT)
Subject: [R] a problem sourcing a file using chdir=TRUE
Message-ID: <Pine.SOL.4.50.0505161339140.1211-100000@pearson.stat.unipg.it>

Dear R-users,

I used to give commands such as:

> source(file="~/path/to/file.R", chdir=TRUE)

but with the latest v. 2.1.0 it does not seem to work anymore.
I tried to figure out what it was going on and it seems that the string
for which
> class(file)
[1] "character"
is changed to
> class(file)
[1] "file"       "connection"
when the connection is open by
file <- file(file, "r", encoding = encoding)

But this force the following if statement
if (chdir && is.character(file) && (path <- dirname(file)) != ".")
   { owd <- getwd()
     on.exit(setwd(owd))
     setwd(path)
   }
to be FALSE and then non changing of current directory is done.
Is this the desired behavior or some bug fix is required?

Luca

+-----------------------------------------------------------------------+
| Dr. Luca Scrucca                                                      |
| Dipartimento di Economia, Finanza e Statistica                        |
| Sezione di Statistica                          tel. +39-075-5855226   |
| Universit?? degli Studi di Perugia              fax. +39-075-5855950   |
| Via Pascoli - C.P. 1315 Succ. 1                                       |
| 06100 PERUGIA  (ITALY)                                                |
|                                                  (o_   (o_   (o_      |
| E-mail:   luca at stat.unipg.it                    //\   //\   //\       |
| Web page: http://www.stat.unipg.it/luca         V_/_  V_/_  V_/_      |
+-----------------------------------------------------------------------+



From ripley at stats.ox.ac.uk  Mon May 16 13:54:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 May 2005 12:54:38 +0100 (BST)
Subject: [R] branch cuts of atan()
In-Reply-To: <e5dd1eeba0d7c1293f87de1015158278@soc.soton.ac.uk>
References: <533cfb3ef69ae67c3fff232093055bb1@soc.soton.ac.uk>
	<Pine.LNX.4.61.0505161015180.23003@gannet.stats>
	<e5dd1eeba0d7c1293f87de1015158278@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.61.0505161252320.24638@gannet.stats>

On Mon, 16 May 2005, Robin Hankin wrote:

> Professor Ripley
>
> thanks for this.  Always good to know that I'm not missing any documentation!
>
> The source is clear;  formula 4.4.39 effectively has a branch cut at |z|=1.
> A 'n' S show the standard branch cuts in their figure 4.4, which are 
> different (they
> were the ones I was expecting).  Mathematica and Maple both have branch cuts
> matching ams-55.
>
> Should the the Trig.Rd manpage   warn that the branch cuts are non-standard?
> Simple workarounds exist to shift them to the standard place.
>
> Is this type of concern worthy of a  bug report?

If you have a suggestion and code to implement it, definitely.

> The archetype would be
>
> tan(atan(2))
> [1] 2
>> tan(atan(2+0i))
> [1] -0.5+0i
>>
>
>
>
> best wishes
>
> Robin
>
>
>
>
>
> On May 16, 2005, at 10:23 am, Prof Brian Ripley wrote:
>
>> On Mon, 16 May 2005, Robin Hankin wrote:
>> 
>>> Hi
>>> 
>>>> atan(1.0001+0i)
>>> [1] -0.7853482+0i
>>>> atan(0.9999+0i)
>>> [1] 0.7853482+0i
>>>> 
>>> 
>>> 
>>> 
>>> evidently atan()'s branch cuts aren't where I thought they were.
>>> 
>>> Where do I look for documentation on this?
>> 
>> In the sources.  Specifically for complex atan() in src/main/complex.c
>> 
>> 	/* Complex Arctangent Function */
>> 	/* Equation (4.4.39) Abramowitz and Stegun */
>> 
>> static void z_atan(Rcomplex *r, Rcomplex *z)
>> ...

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Mon May 16 13:59:08 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 16 May 2005 07:59:08 -0400
Subject: [R] Fitting Contour to Data Points
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E838@usctmx1106.merck.com>

You can use the akima package to interpolate to a regular grid, then plot
that.  Another choice is to fit a smooth surface, using any number of
smoothers available in R and CRAN.

Andy

> From: Laura Quinn
> 
> Apologies for the mass mailing today!
> 
> I am attempting to produce a contour plot for phsical data on a map
> matrix. I have a small number of data points which each has an (x,y)
> co-ordinate together with a corresponding value which I would like to
> cvreate a contour plot for.
> 
> I have tried the following code:
> 
> contour(data$x,data$y,data$value)
> 
> but am told:
> 
> Error in contour.default(data$x, data$y, data$value) :
>         increasing x and y values expected
> 
> I have re-ordered the matrix so that the x values are increasing, but
> these do not necessarrily correspond with increasing y values 
> (as this is
> not a regular grid)...can anyone offer a way around this??
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ripley at stats.ox.ac.uk  Mon May 16 14:28:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 May 2005 13:28:26 +0100 (BST)
Subject: [R] a problem sourcing a file using chdir=TRUE
In-Reply-To: <Pine.SOL.4.50.0505161339140.1211-100000@pearson.stat.unipg.it>
References: <Pine.SOL.4.50.0505161339140.1211-100000@pearson.stat.unipg.it>
Message-ID: <Pine.LNX.4.61.0505161322010.24952@gannet.stats>

There was a pre-existing bug.  ?source in 2.0.1 says

     file: a connection or a character string giving the name of the
           file or URL to read from.

yet it contained

     if (chdir && (path <- dirname(file)) != ".") {

You cannot reasonably run dirname() on a connection or a URL, and it 
throws an error in those cases.

We need a more elaborate detection mechanism, and some re-ordering of the 
code.


On Mon, 16 May 2005, Luca Scrucca wrote:

> Dear R-users,
>
> I used to give commands such as:
>
>> source(file="~/path/to/file.R", chdir=TRUE)
>
> but with the latest v. 2.1.0 it does not seem to work anymore.
> I tried to figure out what it was going on and it seems that the string
> for which
>> class(file)
> [1] "character"
> is changed to
>> class(file)
> [1] "file"       "connection"
> when the connection is open by
> file <- file(file, "r", encoding = encoding)
>
> But this force the following if statement
> if (chdir && is.character(file) && (path <- dirname(file)) != ".")
>   { owd <- getwd()
>     on.exit(setwd(owd))
>     setwd(path)
>   }
> to be FALSE and then non changing of current directory is done.
> Is this the desired behavior or some bug fix is required?
>
> Luca
>
> +-----------------------------------------------------------------------+
> | Dr. Luca Scrucca                                                      |
> | Dipartimento di Economia, Finanza e Statistica                        |
> | Sezione di Statistica                          tel. +39-075-5855226   |
> | Universit? degli Studi di Perugia              fax. +39-075-5855950   |
> | Via Pascoli - C.P. 1315 Succ. 1                                       |
> | 06100 PERUGIA  (ITALY)                                                |
> |                                                  (o_   (o_   (o_      |
> | E-mail:   luca at stat.unipg.it                    //\   //\   //\       |
> | Web page: http://www.stat.unipg.it/luca         V_/_  V_/_  V_/_      |
> +-----------------------------------------------------------------------+
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ligges at statistik.uni-dortmund.de  Mon May 16 14:42:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 16 May 2005 14:42:18 +0200
Subject: [R] How to convert color to black & white
In-Reply-To: <42886A22.5060108@gmail.com>
References: <7CBBD627E4E688499349A5D11D078316018899AD@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>
	<42886A22.5060108@gmail.com>
Message-ID: <4288952A.3060306@statistik.uni-dortmund.de>

Muhammad Subianto wrote:
> Thank's you very much.
> But I need the plot with color not gray.

So you want a colorful rgb plot, OK, fine with your code below.

Now you want to print it black and white: This is now a question for the 
folks who wrote your printer driver, but not for R-help.

Uwe Ligges


> Best wishes,
> Muhammad Subianto
> 
> On this day 5/14/2005 3:05 AM, OlsenN at pac.dfo-mpo.gc.ca wrote:
> 
>> Muhammad,
>> Here's one option:
>>
>> barplot(1:5,col=gray(seq(0,1,length=5)))
>>
>> Norm Olsen
>> Fisheries and Oceans Canada
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> To: R-help at stat.math.ethz.ch
>> Sent: 5/13/2005 11:40 AM
>> Subject: [R] How to convert color to black & white
>>
>> Dear all,
>> Could someone please explain to me how to convert color to black &
>> white.
>> For example:
>> barplot(1:5,col = rainbow(5))
>> Because I need to print my plot to save my ink color printer.
>> I don't want to convert to grayscale, but keep it as an RGB.
>> I  would be very happy if anyone could help me.
>> Thank you very much in advance.
>>
>> Kindly regards,
>> Muhammad Subianto
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Mon May 16 14:46:15 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 16 May 2005 08:46:15 -0400
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E839@usctmx1106.merck.com>

One possible solution without renaming the functions is to add namespace to
either xtable or Hmisc.  Given the size of Hmisc, it probably would be much
easier to do that with xtable.

With namespace in xtable, you can do xtable:::label() to refer to the
label() in xtable specifically.

Andy

> From: Of Sander Oom
> 
> Dear David,
> 
> I would like to use summarize(Hmisc) and print.xtable(xtable) in a 
> single Sweave document, but a conflict with the 'label' function 
> prohibits this at the moment!
> 
> Would you be able to correct the conflicting code? I will gladly test 
> the new package!
> 
> I have tried latex(Hmisc) to export the anova table, but 
> results are not 
> promising! I prefer xtable!!
> 
> Thanks,
> 
> Sander.
> 
> Frank E Harrell Jr wrote:
> > Sander Oom wrote:
> >> Dear Frank,
> >>
> >> I have a Sweave document in which I export anova (aov) 
> tables to Latex 
> >> and calculate some summary statistics with summarize{Hmisc} for a 
> >> graph (as in the example below).
> >>
> >> I currently use the following code for the aov tables:
> >> <<results=tex>>=
> >>   tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
> >>   tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut , data=tmp)
> >>   tmpTable <- xtable (tmpAov ,
> >>     caption="ANOVA table for vegetation height.",
> >>     label="tab:AnovaHeight"
> >>     )
> >>   print.xtable(tmpTable, type="latex", floating=TRUE,
> >>     table.placement="ht", caption.placement="top",
> >>     latex.environments=c("center"))
> >>     )
> >> @
> >>
> >> I used xtables, because it has a working aov example. I 
> would be happy 
> >> to use an alternative if I knew how! Would you have sample code to 
> >> illustrate how to export an aov table to Latex using latex{Hmisc}.
> >>
> >> Thanks very much for your help,
> >>
> >> Sander.
> >>
> >> Frank E Harrell Jr wrote:
> >>
> >>> Sander Oom wrote:
> >>>
> >>>> Dear R users,
> >>>>
> >>>> The Sweave code below runs fine, as it is. However, an 
> error occurs 
> >>>> when the line 'library(xtable)' is uncommented:
> >>>> Error:  chunk 1
> >>>> Error in "label<-"(`*tmp*`, value = "month") :
> >>>>         no applicable method for "label<-"
> >>>>
> >>>> Is anybody aware of this and knows a workaround?
> >>>>
> >>>> Thanks,
> >>>>
> >>>> Sander.
> >>>>
> >>>> *******************
> >>>>
> >>>> \documentclass[a4paper]{article}
> >>>> \title{Sweave Test for summarize}
> >>>> \author{Sander Oom}
> >>>>
> >>>> \usepackage{a4wide}
> >>>>
> >>>> \begin{document}
> >>>>
> >>>> \maketitle
> >>>>
> >>>> \begin{figure}[ht]
> >>>> \begin{center}
> >>>> <<fig=TRUE,echo=FALSE>>=
> >>>>   # library(xtable)
> >>>>   library(Hmisc)
> >>>>   set.seed(111)
> >>>>   dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
> >>>>   month <- dfr$month
> >>>>   year <- dfr$year
> >>>>   y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
> >>>>   s <- summarize(y, llist(month,year), smedian.hilow, 
> conf.int=.5)
> >>>>   print(xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s,
> >>>>         keys='lines', method='alt', type='b'))
> >>>> @
> >>>> \end{center}
> >>>> \end{figure}
> >>>>
> >>>> \end{document}
> >>>>
> >>>> ************************
> >>>>
> >>>>
> >>>>
> >>>>  > version
> >>>>          _
> >>>> platform i686-pc-linux-gnu
> >>>> arch     i686
> >>>> os       linux-gnu
> >>>> system   i686, linux-gnu
> >>>> status
> >>>> major    2
> >>>> minor    1.0
> >>>> year     2005
> >>>> month    04
> >>>> day      18
> >>>> language R
> >>>>
> >>>>
> >>>
> >>> I feel this is an xtable problem because Hmisc has being 
> using label 
> >>> and label<- since 1991.
> >>>
> >>> Frank
> >>>
> >>
> > 
> > There are ways to make functions from one area override those from 
> > another, but the real solution is to ask the xtable author 
> not to have 
> > functions that conflict with the (older) Hmisc package.  -Frank
> > 
> 
> -- 
> --------------------------------------------
> Dr Sander P. Oom
> Animal, Plant and Environmental Sciences,
> University of the Witwatersrand
> Private Bag 3, Wits 2050, South Africa
> Tel (work)      +27 (0)11 717 64 04
> Tel (home)      +27 (0)18 297 44 51
> Fax             +27 (0)18 299 24 64
> Email   sander at oomvanlieshout.net
> Web     www.oomvanlieshout.net/sander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From jtnews1 at biomserv.univ-lyon1.fr  Mon May 16 15:12:59 2005
From: jtnews1 at biomserv.univ-lyon1.fr (Jean Thioulouse)
Date: Mon, 16 May 2005 15:12:59 +0200
Subject: [R] Mental Block with PCA of multivariate time series!
In-Reply-To: <200505161020.j4GAKSFv012180@hypatia.math.ethz.ch>
References: <200505161020.j4GAKSFv012180@hypatia.math.ethz.ch>
Message-ID: <p06002000beae480825e9@[134.214.34.24]>

Laura Quinn <laura at env.leeds.ac.uk> wrote:
>I am wishing to investigate the temporal evolution of the pca: if we
>assume that every 50 rows of my data frame is representitive of, for
>instance, 1 day of data, I am hoping to automate a process whereby a pca
>is performed on every 50 rows of data and the loading for PC1 and PC2 for
>each variable (i.e. each column) is represented as a point on a plot - so
>a years' data will be represented as two lines (representing PC1 and PC2)
>on a time series plot for each variable.

Hi Laura,

You might try to take a look at the "between", "within" and "pta" functions
of the ade4 package :

http://pbil.univ-lyon1.fr/ade4html/between.html
http://pbil.univ-lyon1.fr/ade4html/within.html
http://pbil.univ-lyon1.fr/ade4html/pta.html

They can be used to analyse a data set with this kind of structure. Choosing
how to analyse it depends on what you are looking for.

Jean
-- 
Jean Thioulouse - Labo de Biometrie et Biologie Evolutive, UMR CNRS 5558
Universite Lyon 1,  43 Boulevard du 11 Novembre 1918, Batiment G. Mendel
69622 Villeurbanne Cedex,  France.          Tel/Fax : (33) 4 72 43 27 56                       
                 http://pbil.univ-lyon1.fr/JTHome.html



From ggrothendieck at gmail.com  Mon May 16 15:21:11 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 May 2005 09:21:11 -0400
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E839@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E839@usctmx1106.merck.com>
Message-ID: <971536df050516062138cb7d13@mail.gmail.com>

Even without a namespace one could explicitly reference the label
in xtable via:

xtable.label <- get("label", "package:xtable")

On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> One possible solution without renaming the functions is to add namespace to
> either xtable or Hmisc.  Given the size of Hmisc, it probably would be much
> easier to do that with xtable.
> 
> With namespace in xtable, you can do xtable:::label() to refer to the
> label() in xtable specifically.
> 
> Andy
> 
> > From: Of Sander Oom
> >
> > Dear David,
> >
> > I would like to use summarize(Hmisc) and print.xtable(xtable) in a
> > single Sweave document, but a conflict with the 'label' function
> > prohibits this at the moment!
> >
> > Would you be able to correct the conflicting code? I will gladly test
> > the new package!
> >
> > I have tried latex(Hmisc) to export the anova table, but
> > results are not
> > promising! I prefer xtable!!
> >
> > Thanks,
> >
> > Sander.
> >
> > Frank E Harrell Jr wrote:
> > > Sander Oom wrote:
> > >> Dear Frank,
> > >>
> > >> I have a Sweave document in which I export anova (aov)
> > tables to Latex
> > >> and calculate some summary statistics with summarize{Hmisc} for a
> > >> graph (as in the example below).
> > >>
> > >> I currently use the following code for the aov tables:
> > >> <<results=tex>>=
> > >>   tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
> > >>   tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut , data=tmp)
> > >>   tmpTable <- xtable (tmpAov ,
> > >>     caption="ANOVA table for vegetation height.",
> > >>     label="tab:AnovaHeight"
> > >>     )
> > >>   print.xtable(tmpTable, type="latex", floating=TRUE,
> > >>     table.placement="ht", caption.placement="top",
> > >>     latex.environments=c("center"))
> > >>     )
> > >> @
> > >>
> > >> I used xtables, because it has a working aov example. I
> > would be happy
> > >> to use an alternative if I knew how! Would you have sample code to
> > >> illustrate how to export an aov table to Latex using latex{Hmisc}.
> > >>
> > >> Thanks very much for your help,
> > >>
> > >> Sander.
> > >>
> > >> Frank E Harrell Jr wrote:
> > >>
> > >>> Sander Oom wrote:
> > >>>
> > >>>> Dear R users,
> > >>>>
> > >>>> The Sweave code below runs fine, as it is. However, an
> > error occurs
> > >>>> when the line 'library(xtable)' is uncommented:
> > >>>> Error:  chunk 1
> > >>>> Error in "label<-"(`*tmp*`, value = "month") :
> > >>>>         no applicable method for "label<-"
> > >>>>
> > >>>> Is anybody aware of this and knows a workaround?
> > >>>>
> > >>>> Thanks,
> > >>>>
> > >>>> Sander.
> > >>>>
> > >>>> *******************
> > >>>>
> > >>>> \documentclass[a4paper]{article}
> > >>>> \title{Sweave Test for summarize}
> > >>>> \author{Sander Oom}
> > >>>>
> > >>>> \usepackage{a4wide}
> > >>>>
> > >>>> \begin{document}
> > >>>>
> > >>>> \maketitle
> > >>>>
> > >>>> \begin{figure}[ht]
> > >>>> \begin{center}
> > >>>> <<fig=TRUE,echo=FALSE>>=
> > >>>>   # library(xtable)
> > >>>>   library(Hmisc)
> > >>>>   set.seed(111)
> > >>>>   dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
> > >>>>   month <- dfr$month
> > >>>>   year <- dfr$year
> > >>>>   y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
> > >>>>   s <- summarize(y, llist(month,year), smedian.hilow,
> > conf.int=.5)
> > >>>>   print(xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s,
> > >>>>         keys='lines', method='alt', type='b'))
> > >>>> @
> > >>>> \end{center}
> > >>>> \end{figure}
> > >>>>
> > >>>> \end{document}
> > >>>>
> > >>>> ************************
> > >>>>
> > >>>>
> > >>>>
> > >>>>  > version
> > >>>>          _
> > >>>> platform i686-pc-linux-gnu
> > >>>> arch     i686
> > >>>> os       linux-gnu
> > >>>> system   i686, linux-gnu
> > >>>> status
> > >>>> major    2
> > >>>> minor    1.0
> > >>>> year     2005
> > >>>> month    04
> > >>>> day      18
> > >>>> language R
> > >>>>
> > >>>>
> > >>>
> > >>> I feel this is an xtable problem because Hmisc has being
> > using label
> > >>> and label<- since 1991.
> > >>>
> > >>> Frank
> > >>>
> > >>
> > >
> > > There are ways to make functions from one area override those from
> > > another, but the real solution is to ask the xtable author
> > not to have
> > > functions that conflict with the (older) Hmisc package.  -Frank
> > >
> >
> > --
> > --------------------------------------------
> > Dr Sander P. Oom
> > Animal, Plant and Environmental Sciences,
> > University of the Witwatersrand
> > Private Bag 3, Wits 2050, South Africa
> > Tel (work)      +27 (0)11 717 64 04
> > Tel (home)      +27 (0)18 297 44 51
> > Fax             +27 (0)18 299 24 64
> > Email   sander at oomvanlieshout.net
> > Web     www.oomvanlieshout.net/sander
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From i.visser at uva.nl  Mon May 16 15:23:30 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Mon, 16 May 2005 09:23:30 -0400
Subject: [R] constraint optimization
Message-ID: <BEAE1712.465F%i.visser@uva.nl>

Dear All,

I have an optimization problem of the form:
l<=A*p<=u
where l and u are vectors of lower and upper bounds, p is a vector
of parameters and A a linear constraint matrix.

When l=u, it is easy to reparametrize in which case the result is a new set
of parameters p' to be optimized.

My problem is however that l!=u, ie it is mixed, l and u are equal for a
number of constraints and inequal for another set of constraints.

If I use the elements where l=u to reparametrize, I would like to know how
the inequality constraints translate in new inequality constraints for p',
because in that case I could use constrOptim to fit the inequality
constraints. 

Any help appreciated,
best, ingmar

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From slist at oomvanlieshout.net  Mon May 16 15:35:51 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Mon, 16 May 2005 15:35:51 +0200
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <971536df050516062138cb7d13@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E839@usctmx1106.merck.com>
	<971536df050516062138cb7d13@mail.gmail.com>
Message-ID: <4288A1B7.6010006@oomvanlieshout.net>

Hi Andy and Gabor,

Thanks for your help so far! I am discovering another R dimension.

Trying to put my head around all this....the conflict actually exposes 
itself when calling summarize(Hmisc). Summarize(Hmisc) calls label 
internally, so I can not call it explicitly. Simply calling 
label(xtable) explicitly will not solve the problem with summarize(Hmisc).

Thus, I should use namespaces as Andy is suggesting. Now I just need to 
know how I 'add namespace' to a library? Does 'loadNamespace' have 
something to do with it?

Thanks very much for your help!

Sander.


## From Venables and Ripley (2002) p.165.
N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
       62.8,55.8,69.5,55.0,
       62.0,48.8,45.5,44.2,52.0,
       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
                   K=factor(K), yield=yield)
## to show the effects of re-ordering terms contrast the two fits
tmpAov <- aov(yield ~ block + N * P + K, npk)
tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
   label="tab:Anova")
print.xtable(tmpTable, type="latex", floating=TRUE,
   table.placement="h", caption.placement="top",
   latex.environments=c("center"))

Alternatively, using namespace for xtable:

tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
xtable:::label(tmpTable) <- paste("tab:Anova")
print.xtable(tmpTable, type="latex", floating=TRUE,
   table.placement="ht", caption.placement="top",
   latex.environments=c("center"))



Gabor Grothendieck wrote:
> Even without a namespace one could explicitly reference the label
> in xtable via:
> 
> xtable.label <- get("label", "package:xtable")
> 
> On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
>>One possible solution without renaming the functions is to add namespace to
>>either xtable or Hmisc.  Given the size of Hmisc, it probably would be much
>>easier to do that with xtable.
>>
>>With namespace in xtable, you can do xtable:::label() to refer to the
>>label() in xtable specifically.
>>
>>Andy
>>
>>>From: Of Sander Oom
>>>
>>>Dear David,
>>>
>>>I would like to use summarize(Hmisc) and print.xtable(xtable) in a
>>>single Sweave document, but a conflict with the 'label' function
>>>prohibits this at the moment!
>>>
>>>Would you be able to correct the conflicting code? I will gladly test
>>>the new package!
>>>
>>>I have tried latex(Hmisc) to export the anova table, but
>>>results are not
>>>promising! I prefer xtable!!
>>>
>>>Thanks,
>>>
>>>Sander.
>>>
>>>Frank E Harrell Jr wrote:
>>>>Sander Oom wrote:
>>>>>Dear Frank,
>>>>>
>>>>>I have a Sweave document in which I export anova (aov)
>>>tables to Latex
>>>>>and calculate some summary statistics with summarize{Hmisc} for a
>>>>>graph (as in the example below).
>>>>>
>>>>>I currently use the following code for the aov tables:
>>>>><<results=tex>>=
>>>>>  tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
>>>>>  tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut , data=tmp)
>>>>>  tmpTable <- xtable (tmpAov ,
>>>>>    caption="ANOVA table for vegetation height.",
>>>>>    label="tab:AnovaHeight"
>>>>>    )
>>>>>  print.xtable(tmpTable, type="latex", floating=TRUE,
>>>>>    table.placement="ht", caption.placement="top",
>>>>>    latex.environments=c("center"))
>>>>>    )
>>>>>@
>>>>>
>>>>>I used xtables, because it has a working aov example. I
>>>would be happy
>>>>>to use an alternative if I knew how! Would you have sample code to
>>>>>illustrate how to export an aov table to Latex using latex{Hmisc}.
>>>>>
>>>>>Thanks very much for your help,
>>>>>
>>>>>Sander.
>>>>>
>>>>>Frank E Harrell Jr wrote:
>>>>>
>>>>>>Sander Oom wrote:
>>>>>>
>>>>>>>Dear R users,
>>>>>>>
>>>>>>>The Sweave code below runs fine, as it is. However, an
>>>error occurs
>>>>>>>when the line 'library(xtable)' is uncommented:
>>>>>>>Error:  chunk 1
>>>>>>>Error in "label<-"(`*tmp*`, value = "month") :
>>>>>>>        no applicable method for "label<-"
>>>>>>>
>>>>>>>Is anybody aware of this and knows a workaround?
>>>>>>>
>>>>>>>Thanks,
>>>>>>>
>>>>>>>Sander.
>>>>>>>
>>>>>>>*******************
>>>>>>>
>>>>>>>\documentclass[a4paper]{article}
>>>>>>>\title{Sweave Test for summarize}
>>>>>>>\author{Sander Oom}
>>>>>>>
>>>>>>>\usepackage{a4wide}
>>>>>>>
>>>>>>>\begin{document}
>>>>>>>
>>>>>>>\maketitle
>>>>>>>
>>>>>>>\begin{figure}[ht]
>>>>>>>\begin{center}
>>>>>>><<fig=TRUE,echo=FALSE>>=
>>>>>>>  # library(xtable)
>>>>>>>  library(Hmisc)
>>>>>>>  set.seed(111)
>>>>>>>  dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
>>>>>>>  month <- dfr$month
>>>>>>>  year <- dfr$year
>>>>>>>  y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
>>>>>>>  s <- summarize(y, llist(month,year), smedian.hilow,
>>>conf.int=.5)
>>>>>>>  print(xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s,
>>>>>>>        keys='lines', method='alt', type='b'))
>>>>>>>@
>>>>>>>\end{center}
>>>>>>>\end{figure}
>>>>>>>
>>>>>>>\end{document}
>>>>>>>
>>>>>>>************************
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> > version
>>>>>>>         _
>>>>>>>platform i686-pc-linux-gnu
>>>>>>>arch     i686
>>>>>>>os       linux-gnu
>>>>>>>system   i686, linux-gnu
>>>>>>>status
>>>>>>>major    2
>>>>>>>minor    1.0
>>>>>>>year     2005
>>>>>>>month    04
>>>>>>>day      18
>>>>>>>language R
>>>>>>>
>>>>>>>
>>>>>>I feel this is an xtable problem because Hmisc has being
>>>using label
>>>>>>and label<- since 1991.
>>>>>>
>>>>>>Frank
>>>>>>
>>>>There are ways to make functions from one area override those from
>>>>another, but the real solution is to ask the xtable author
>>>not to have
>>>>functions that conflict with the (older) Hmisc package.  -Frank
>>>>
>>>--
>>>--------------------------------------------
>>>Dr Sander P. Oom
>>>Animal, Plant and Environmental Sciences,
>>>University of the Witwatersrand
>>>Private Bag 3, Wits 2050, South Africa
>>>Tel (work)      +27 (0)11 717 64 04
>>>Tel (home)      +27 (0)18 297 44 51
>>>Fax             +27 (0)18 299 24 64
>>>Email   sander at oomvanlieshout.net
>>>Web     www.oomvanlieshout.net/sander
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 

-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From ggrothendieck at gmail.com  Mon May 16 15:46:43 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 May 2005 09:46:43 -0400
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <4288A1B7.6010006@oomvanlieshout.net>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E839@usctmx1106.merck.com>
	<971536df050516062138cb7d13@mail.gmail.com>
	<4288A1B7.6010006@oomvanlieshout.net>
Message-ID: <971536df05051606466b626674@mail.gmail.com>

Do you need label in both xtable and Hmisc?  If you only need
it in Hmisc and not in xtable then just be sure you have loaded
xtable first and Hmisc second.  This:

search()

will give you the search path.  It will find the first one on the
search path so if Hmisc is before xtable (which would occur
if you loaded Hmisc last) then it would find that one.

If you have already loaded them in the wrong order then just
detach Hmisc and load it again:

detach("package:Hmisc")
library(Hmisc)


On 5/16/05, Sander Oom <slist at oomvanlieshout.net> wrote:
> Hi Andy and Gabor,
> 
> Thanks for your help so far! I am discovering another R dimension.
> 
> Trying to put my head around all this....the conflict actually exposes
> itself when calling summarize(Hmisc). Summarize(Hmisc) calls label
> internally, so I can not call it explicitly. Simply calling
> label(xtable) explicitly will not solve the problem with summarize(Hmisc).
> 
> Thus, I should use namespaces as Andy is suggesting. Now I just need to
> know how I 'add namespace' to a library? Does 'loadNamespace' have
> something to do with it?
> 
> Thanks very much for your help!
> 
> Sander.
> 
> ## From Venables and Ripley (2002) p.165.
> N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
> P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
> K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
> yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
>       62.8,55.8,69.5,55.0,
>       62.0,48.8,45.5,44.2,52.0,
>       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
> npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
>                   K=factor(K), yield=yield)
> ## to show the effects of re-ordering terms contrast the two fits
> tmpAov <- aov(yield ~ block + N * P + K, npk)
> tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
>   label="tab:Anova")
> print.xtable(tmpTable, type="latex", floating=TRUE,
>   table.placement="h", caption.placement="top",
>   latex.environments=c("center"))
> 
> Alternatively, using namespace for xtable:
> 
> tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
> xtable:::label(tmpTable) <- paste("tab:Anova")
> print.xtable(tmpTable, type="latex", floating=TRUE,
>   table.placement="ht", caption.placement="top",
>   latex.environments=c("center"))
> 
> Gabor Grothendieck wrote:
> > Even without a namespace one could explicitly reference the label
> > in xtable via:
> >
> > xtable.label <- get("label", "package:xtable")
> >
> > On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> >>One possible solution without renaming the functions is to add namespace to
> >>either xtable or Hmisc.  Given the size of Hmisc, it probably would be much
> >>easier to do that with xtable.
> >>
> >>With namespace in xtable, you can do xtable:::label() to refer to the
> >>label() in xtable specifically.
> >>
> >>Andy
> >>
> >>>From: Of Sander Oom
> >>>
> >>>Dear David,
> >>>
> >>>I would like to use summarize(Hmisc) and print.xtable(xtable) in a
> >>>single Sweave document, but a conflict with the 'label' function
> >>>prohibits this at the moment!
> >>>
> >>>Would you be able to correct the conflicting code? I will gladly test
> >>>the new package!
> >>>
> >>>I have tried latex(Hmisc) to export the anova table, but
> >>>results are not
> >>>promising! I prefer xtable!!
> >>>
> >>>Thanks,
> >>>
> >>>Sander.
> >>>
> >>>Frank E Harrell Jr wrote:
> >>>>Sander Oom wrote:
> >>>>>Dear Frank,
> >>>>>
> >>>>>I have a Sweave document in which I export anova (aov)
> >>>tables to Latex
> >>>>>and calculate some summary statistics with summarize{Hmisc} for a
> >>>>>graph (as in the example below).
> >>>>>
> >>>>>I currently use the following code for the aov tables:
> >>>>><<results=tex>>=
> >>>>>  tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
> >>>>>  tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut , data=tmp)
> >>>>>  tmpTable <- xtable (tmpAov ,
> >>>>>    caption="ANOVA table for vegetation height.",
> >>>>>    label="tab:AnovaHeight"
> >>>>>    )
> >>>>>  print.xtable(tmpTable, type="latex", floating=TRUE,
> >>>>>    table.placement="ht", caption.placement="top",
> >>>>>    latex.environments=c("center"))
> >>>>>    )
> >>>>>@
> >>>>>
> >>>>>I used xtables, because it has a working aov example. I
> >>>would be happy
> >>>>>to use an alternative if I knew how! Would you have sample code to
> >>>>>illustrate how to export an aov table to Latex using latex{Hmisc}.
> >>>>>
> >>>>>Thanks very much for your help,
> >>>>>
> >>>>>Sander.
> >>>>>
> >>>>>Frank E Harrell Jr wrote:
> >>>>>
> >>>>>>Sander Oom wrote:
> >>>>>>
> >>>>>>>Dear R users,
> >>>>>>>
> >>>>>>>The Sweave code below runs fine, as it is. However, an
> >>>error occurs
> >>>>>>>when the line 'library(xtable)' is uncommented:
> >>>>>>>Error:  chunk 1
> >>>>>>>Error in "label<-"(`*tmp*`, value = "month") :
> >>>>>>>        no applicable method for "label<-"
> >>>>>>>
> >>>>>>>Is anybody aware of this and knows a workaround?
> >>>>>>>
> >>>>>>>Thanks,
> >>>>>>>
> >>>>>>>Sander.
> >>>>>>>
> >>>>>>>*******************
> >>>>>>>
> >>>>>>>\documentclass[a4paper]{article}
> >>>>>>>\title{Sweave Test for summarize}
> >>>>>>>\author{Sander Oom}
> >>>>>>>
> >>>>>>>\usepackage{a4wide}
> >>>>>>>
> >>>>>>>\begin{document}
> >>>>>>>
> >>>>>>>\maketitle
> >>>>>>>
> >>>>>>>\begin{figure}[ht]
> >>>>>>>\begin{center}
> >>>>>>><<fig=TRUE,echo=FALSE>>=
> >>>>>>>  # library(xtable)
> >>>>>>>  library(Hmisc)
> >>>>>>>  set.seed(111)
> >>>>>>>  dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
> >>>>>>>  month <- dfr$month
> >>>>>>>  year <- dfr$year
> >>>>>>>  y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
> >>>>>>>  s <- summarize(y, llist(month,year), smedian.hilow,
> >>>conf.int=.5)
> >>>>>>>  print(xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s,
> >>>>>>>        keys='lines', method='alt', type='b'))
> >>>>>>>@
> >>>>>>>\end{center}
> >>>>>>>\end{figure}
> >>>>>>>
> >>>>>>>\end{document}
> >>>>>>>
> >>>>>>>************************
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>> > version
> >>>>>>>         _
> >>>>>>>platform i686-pc-linux-gnu
> >>>>>>>arch     i686
> >>>>>>>os       linux-gnu
> >>>>>>>system   i686, linux-gnu
> >>>>>>>status
> >>>>>>>major    2
> >>>>>>>minor    1.0
> >>>>>>>year     2005
> >>>>>>>month    04
> >>>>>>>day      18
> >>>>>>>language R
> >>>>>>>
> >>>>>>>
> >>>>>>I feel this is an xtable problem because Hmisc has being
> >>>using label
> >>>>>>and label<- since 1991.
> >>>>>>
> >>>>>>Frank
> >>>>>>
> >>>>There are ways to make functions from one area override those from
> >>>>another, but the real solution is to ask the xtable author
> >>>not to have
> >>>>functions that conflict with the (older) Hmisc package.  -Frank
> >>>>
> >>>--
> >>>--------------------------------------------
> >>>Dr Sander P. Oom
> >>>Animal, Plant and Environmental Sciences,
> >>>University of the Witwatersrand
> >>>Private Bag 3, Wits 2050, South Africa
> >>>Tel (work)      +27 (0)11 717 64 04
> >>>Tel (home)      +27 (0)18 297 44 51
> >>>Fax             +27 (0)18 299 24 64
> >>>Email   sander at oomvanlieshout.net
> >>>Web     www.oomvanlieshout.net/sander
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>>
> >>>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >
> 
> --
> --------------------------------------------
> Dr Sander P. Oom
> Animal, Plant and Environmental Sciences,
> University of the Witwatersrand
> Private Bag 3, Wits 2050, South Africa
> Tel (work)      +27 (0)11 717 64 04
> Tel (home)      +27 (0)18 297 44 51
> Fax             +27 (0)18 299 24 64
> Email   sander at oomvanlieshout.net
> Web     www.oomvanlieshout.net/sander
> ---------------------------------------------
>



From br44114 at gmail.com  Mon May 16 15:47:04 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Mon, 16 May 2005 09:47:04 -0400
Subject: [R] get plot in a window when running R in the shell
Message-ID: <8d5a36350505160647599816e3@mail.gmail.com>

Dear useRs,

On a GNU/Linux box I want to run some code from the command line. This works
   #!/bin/sh
   R --vanilla -q --gui=X11 < code.r
however I want the plots to appear in a window (as it happens when the
code is run interactively) instead of being saved in 'Rplots.ps'. Is
that doable?

Thank you,
b.



From andy_liaw at merck.com  Mon May 16 15:48:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 16 May 2005 09:48:12 -0400
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83E@usctmx1106.merck.com>

You need to add the namespace to the source package, by adding a NAMESPACE
file.  There's an R News article by Prof. Tierney on how to do this.  Also
see the `Writing R Extensions' manual.  You should get the package
maintainer to do that, as that constitute a change in the package source
code.

Short of that, you should make sure that Hmisc is loaded later than xtable,
and use something like what Gabor suggested to access label() in xtable.  (I
would use some other name, though: label() in xtable is already an S3
generic).

Andy

> From: Sander Oom 
> 
> Hi Andy and Gabor,
> 
> Thanks for your help so far! I am discovering another R dimension.
> 
> Trying to put my head around all this....the conflict 
> actually exposes 
> itself when calling summarize(Hmisc). Summarize(Hmisc) calls label 
> internally, so I can not call it explicitly. Simply calling 
> label(xtable) explicitly will not solve the problem with 
> summarize(Hmisc).
> 
> Thus, I should use namespaces as Andy is suggesting. Now I 
> just need to 
> know how I 'add namespace' to a library? Does 'loadNamespace' have 
> something to do with it?
> 
> Thanks very much for your help!
> 
> Sander.
> 
> 
> ## From Venables and Ripley (2002) p.165.
> N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
> P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
> K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
> yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
>        62.8,55.8,69.5,55.0,
>        62.0,48.8,45.5,44.2,52.0,
>        51.5,49.8,48.8,57.2,59.0,53.2,56.0)
> npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
>                    K=factor(K), yield=yield)
> ## to show the effects of re-ordering terms contrast the two fits
> tmpAov <- aov(yield ~ block + N * P + K, npk)
> tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
>    label="tab:Anova")
> print.xtable(tmpTable, type="latex", floating=TRUE,
>    table.placement="h", caption.placement="top",
>    latex.environments=c("center"))
> 
> Alternatively, using namespace for xtable:
> 
> tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
> xtable:::label(tmpTable) <- paste("tab:Anova")
> print.xtable(tmpTable, type="latex", floating=TRUE,
>    table.placement="ht", caption.placement="top",
>    latex.environments=c("center"))
> 
> 
> 
> Gabor Grothendieck wrote:
> > Even without a namespace one could explicitly reference the label
> > in xtable via:
> > 
> > xtable.label <- get("label", "package:xtable")
> > 
> > On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> >>One possible solution without renaming the functions is to 
> add namespace to
> >>either xtable or Hmisc.  Given the size of Hmisc, it 
> probably would be much
> >>easier to do that with xtable.
> >>
> >>With namespace in xtable, you can do xtable:::label() to 
> refer to the
> >>label() in xtable specifically.
> >>
> >>Andy
> >>
> >>>From: Of Sander Oom
> >>>
> >>>Dear David,
> >>>
> >>>I would like to use summarize(Hmisc) and print.xtable(xtable) in a
> >>>single Sweave document, but a conflict with the 'label' function
> >>>prohibits this at the moment!
> >>>
> >>>Would you be able to correct the conflicting code? I will 
> gladly test
> >>>the new package!
> >>>
> >>>I have tried latex(Hmisc) to export the anova table, but
> >>>results are not
> >>>promising! I prefer xtable!!
> >>>
> >>>Thanks,
> >>>
> >>>Sander.
> >>>
> >>>Frank E Harrell Jr wrote:
> >>>>Sander Oom wrote:
> >>>>>Dear Frank,
> >>>>>
> >>>>>I have a Sweave document in which I export anova (aov)
> >>>tables to Latex
> >>>>>and calculate some summary statistics with summarize{Hmisc} for a
> >>>>>graph (as in the example below).
> >>>>>
> >>>>>I currently use the following code for the aov tables:
> >>>>><<results=tex>>=
> >>>>>  tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
> >>>>>  tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut 
> , data=tmp)
> >>>>>  tmpTable <- xtable (tmpAov ,
> >>>>>    caption="ANOVA table for vegetation height.",
> >>>>>    label="tab:AnovaHeight"
> >>>>>    )
> >>>>>  print.xtable(tmpTable, type="latex", floating=TRUE,
> >>>>>    table.placement="ht", caption.placement="top",
> >>>>>    latex.environments=c("center"))
> >>>>>    )
> >>>>>@
> >>>>>
> >>>>>I used xtables, because it has a working aov example. I
> >>>would be happy
> >>>>>to use an alternative if I knew how! Would you have 
> sample code to
> >>>>>illustrate how to export an aov table to Latex using 
> latex{Hmisc}.
> >>>>>
> >>>>>Thanks very much for your help,
> >>>>>
> >>>>>Sander.
> >>>>>
> >>>>>Frank E Harrell Jr wrote:
> >>>>>
> >>>>>>Sander Oom wrote:
> >>>>>>
> >>>>>>>Dear R users,
> >>>>>>>
> >>>>>>>The Sweave code below runs fine, as it is. However, an
> >>>error occurs
> >>>>>>>when the line 'library(xtable)' is uncommented:
> >>>>>>>Error:  chunk 1
> >>>>>>>Error in "label<-"(`*tmp*`, value = "month") :
> >>>>>>>        no applicable method for "label<-"
> >>>>>>>
> >>>>>>>Is anybody aware of this and knows a workaround?
> >>>>>>>
> >>>>>>>Thanks,
> >>>>>>>
> >>>>>>>Sander.
> >>>>>>>
> >>>>>>>*******************
> >>>>>>>
> >>>>>>>\documentclass[a4paper]{article}
> >>>>>>>\title{Sweave Test for summarize}
> >>>>>>>\author{Sander Oom}
> >>>>>>>
> >>>>>>>\usepackage{a4wide}
> >>>>>>>
> >>>>>>>\begin{document}
> >>>>>>>
> >>>>>>>\maketitle
> >>>>>>>
> >>>>>>>\begin{figure}[ht]
> >>>>>>>\begin{center}
> >>>>>>><<fig=TRUE,echo=FALSE>>=
> >>>>>>>  # library(xtable)
> >>>>>>>  library(Hmisc)
> >>>>>>>  set.seed(111)
> >>>>>>>  dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
> >>>>>>>  month <- dfr$month
> >>>>>>>  year <- dfr$year
> >>>>>>>  y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
> >>>>>>>  s <- summarize(y, llist(month,year), smedian.hilow,
> >>>conf.int=.5)
> >>>>>>>  print(xYplot(Cbind(y,Lower,Upper) ~ month, 
> groups=year, data=s,
> >>>>>>>        keys='lines', method='alt', type='b'))
> >>>>>>>@
> >>>>>>>\end{center}
> >>>>>>>\end{figure}
> >>>>>>>
> >>>>>>>\end{document}
> >>>>>>>
> >>>>>>>************************
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>> > version
> >>>>>>>         _
> >>>>>>>platform i686-pc-linux-gnu
> >>>>>>>arch     i686
> >>>>>>>os       linux-gnu
> >>>>>>>system   i686, linux-gnu
> >>>>>>>status
> >>>>>>>major    2
> >>>>>>>minor    1.0
> >>>>>>>year     2005
> >>>>>>>month    04
> >>>>>>>day      18
> >>>>>>>language R
> >>>>>>>
> >>>>>>>
> >>>>>>I feel this is an xtable problem because Hmisc has being
> >>>using label
> >>>>>>and label<- since 1991.
> >>>>>>
> >>>>>>Frank
> >>>>>>
> >>>>There are ways to make functions from one area override those from
> >>>>another, but the real solution is to ask the xtable author
> >>>not to have
> >>>>functions that conflict with the (older) Hmisc package.  -Frank
> >>>>
> >>>--
> >>>--------------------------------------------
> >>>Dr Sander P. Oom
> >>>Animal, Plant and Environmental Sciences,
> >>>University of the Witwatersrand
> >>>Private Bag 3, Wits 2050, South Africa
> >>>Tel (work)      +27 (0)11 717 64 04
> >>>Tel (home)      +27 (0)18 297 44 51
> >>>Fax             +27 (0)18 299 24 64
> >>>Email   sander at oomvanlieshout.net
> >>>Web     www.oomvanlieshout.net/sander
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>>
> >>>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >>
> > 
> 
> -- 
> 
> --------------------------------------------
> Dr Sander P. Oom
> Animal, Plant and Environmental Sciences,
> University of the Witwatersrand
> Private Bag 3, Wits 2050, South Africa
> Tel (work)      +27 (0)11 717 64 04
> Tel (home)      +27 (0)18 297 44 51
> Fax             +27 (0)18 299 24 64
> Email   sander at oomvanlieshout.net
> Web     www.oomvanlieshout.net/sander
> ---------------------------------------------
> 
> 
>



From B.Rowlingson at lancaster.ac.uk  Mon May 16 16:04:27 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 16 May 2005 15:04:27 +0100
Subject: [R] get plot in a window when running R in the shell
In-Reply-To: <8d5a36350505160647599816e3@mail.gmail.com>
References: <8d5a36350505160647599816e3@mail.gmail.com>
Message-ID: <4288A86B.8000402@lancaster.ac.uk>

bogdan romocea wrote:

> however I want the plots to appear in a window (as it happens when the
> code is run interactively) instead of being saved in 'Rplots.ps'. Is
> that doable?

  You could explicitly startup an X11 graphics device before your first 
plot in your R source code, something like:

  x11()
  hist(runif(100))

Works for me, although the window will disappear very quickly unless the 
script pauses before ending!

Baz



From sander at oomvanlieshout.net  Mon May 16 16:25:30 2005
From: sander at oomvanlieshout.net (Sander Oom)
Date: Mon, 16 May 2005 16:25:30 +0200
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83E@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83E@usctmx1106.merck.com>
Message-ID: <4288AD5A.8040701@oomvanlieshout.net>

I tried to follow your suggestions, but without success:

Error: couldn't find function "xtable.mylabel<-"

... resulting from the code below.

Any suggestions?

Thanks,

Sander.


library(xtable)
xtable.mylabel <- get("label", "package:xtable")
library(Hmisc) # provides summarize

set.seed(1)
temperature <- rnorm(300, 70, 10)
month <- sample(1:12, 300, TRUE)
year  <- sample(2000:2001, 300, TRUE)
g <- function(x)c(Mean=mean(x,na.rm=TRUE),Median=median(x,na.rm=TRUE))
summarize(temperature, month, g)

## From Venables and Ripley (2002) p.165.
N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
       62.8,55.8,69.5,55.0,
       62.0,48.8,45.5,44.2,52.0,
       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
                   K=factor(K), yield=yield)
## to show the effects of re-ordering terms contrast the two fits
tmpAov <- aov(yield ~ block + N * P + K, npk)
tmpTable <- xtable (tmpAov ,
   caption="ANOVA table for vegetation height.")
xtable.mylabel(tmpTable) <- paste("tab:AnovaHeight")
print.xtable(tmpTable, type="latex", floating=TRUE,
   table.placement="h", caption.placement="top",
   latex.environments=c("center"),
   title=first.word(deparse(substitute(object))),
   append=FALSE
   )


Liaw, Andy wrote:
> You need to add the namespace to the source package, by adding a NAMESPACE
> file.  There's an R News article by Prof. Tierney on how to do this.  Also
> see the `Writing R Extensions' manual.  You should get the package
> maintainer to do that, as that constitute a change in the package source
> code.
> 
> Short of that, you should make sure that Hmisc is loaded later than xtable,
> and use something like what Gabor suggested to access label() in xtable.  (I
> would use some other name, though: label() in xtable is already an S3
> generic).
> 
> Andy
> 
>>From: Sander Oom 
>>
>>Hi Andy and Gabor,
>>
>>Thanks for your help so far! I am discovering another R dimension.
>>
>>Trying to put my head around all this....the conflict 
>>actually exposes 
>>itself when calling summarize(Hmisc). Summarize(Hmisc) calls label 
>>internally, so I can not call it explicitly. Simply calling 
>>label(xtable) explicitly will not solve the problem with 
>>summarize(Hmisc).
>>
>>Thus, I should use namespaces as Andy is suggesting. Now I 
>>just need to 
>>know how I 'add namespace' to a library? Does 'loadNamespace' have 
>>something to do with it?
>>
>>Thanks very much for your help!
>>
>>Sander.
>>
>>
>>## From Venables and Ripley (2002) p.165.
>>N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
>>P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
>>K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
>>yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
>>       62.8,55.8,69.5,55.0,
>>       62.0,48.8,45.5,44.2,52.0,
>>       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
>>npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
>>                   K=factor(K), yield=yield)
>>## to show the effects of re-ordering terms contrast the two fits
>>tmpAov <- aov(yield ~ block + N * P + K, npk)
>>tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
>>   label="tab:Anova")
>>print.xtable(tmpTable, type="latex", floating=TRUE,
>>   table.placement="h", caption.placement="top",
>>   latex.environments=c("center"))
>>
>>Alternatively, using namespace for xtable:
>>
>>tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
>>xtable:::label(tmpTable) <- paste("tab:Anova")
>>print.xtable(tmpTable, type="latex", floating=TRUE,
>>   table.placement="ht", caption.placement="top",
>>   latex.environments=c("center"))
>>
>>
>>
>>Gabor Grothendieck wrote:
>>>Even without a namespace one could explicitly reference the label
>>>in xtable via:
>>>
>>>xtable.label <- get("label", "package:xtable")
>>>
>>>On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
>>>>One possible solution without renaming the functions is to 
>>add namespace to
>>>>either xtable or Hmisc.  Given the size of Hmisc, it 
>>probably would be much
>>>>easier to do that with xtable.
>>>>
>>>>With namespace in xtable, you can do xtable:::label() to 
>>refer to the
>>>>label() in xtable specifically.
>>>>
>>>>Andy
>>>>
>>>>>From: Of Sander Oom
>>>>>
>>>>>Dear David,
>>>>>
>>>>>I would like to use summarize(Hmisc) and print.xtable(xtable) in a
>>>>>single Sweave document, but a conflict with the 'label' function
>>>>>prohibits this at the moment!
>>>>>
>>>>>Would you be able to correct the conflicting code? I will 
>>gladly test
>>>>>the new package!
>>>>>
>>>>>I have tried latex(Hmisc) to export the anova table, but
>>>>>results are not
>>>>>promising! I prefer xtable!!
>>>>>
>>>>>Thanks,
>>>>>
>>>>>Sander.
>>>>>
>>>>>Frank E Harrell Jr wrote:
>>>>>>Sander Oom wrote:
>>>>>>>Dear Frank,
>>>>>>>
>>>>>>>I have a Sweave document in which I export anova (aov)
>>>>>tables to Latex
>>>>>>>and calculate some summary statistics with summarize{Hmisc} for a
>>>>>>>graph (as in the example below).
>>>>>>>
>>>>>>>I currently use the following code for the aov tables:
>>>>>>><<results=tex>>=
>>>>>>> tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
>>>>>>> tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut 
>>, data=tmp)
>>>>>>> tmpTable <- xtable (tmpAov ,
>>>>>>>   caption="ANOVA table for vegetation height.",
>>>>>>>   label="tab:AnovaHeight"
>>>>>>>   )
>>>>>>> print.xtable(tmpTable, type="latex", floating=TRUE,
>>>>>>>   table.placement="ht", caption.placement="top",
>>>>>>>   latex.environments=c("center"))
>>>>>>>   )
>>>>>>>@
>>>>>>>
>>>>>>>I used xtables, because it has a working aov example. I
>>>>>would be happy
>>>>>>>to use an alternative if I knew how! Would you have 
>>sample code to
>>>>>>>illustrate how to export an aov table to Latex using 
>>latex{Hmisc}.
>>>>>>>Thanks very much for your help,
>>>>>>>
>>>>>>>Sander.
>>>>>>>
>>>>>>>Frank E Harrell Jr wrote:
>>>>>>>
>>>>>>>>Sander Oom wrote:
>>>>>>>>
>>>>>>>>>Dear R users,
>>>>>>>>>
>>>>>>>>>The Sweave code below runs fine, as it is. However, an
>>>>>error occurs
>>>>>>>>>when the line 'library(xtable)' is uncommented:
>>>>>>>>>Error:  chunk 1
>>>>>>>>>Error in "label<-"(`*tmp*`, value = "month") :
>>>>>>>>>       no applicable method for "label<-"
>>>>>>>>>
>>>>>>>>>Is anybody aware of this and knows a workaround?
>>>>>>>>>
>>>>>>>>>Thanks,
>>>>>>>>>
>>>>>>>>>Sander.
>>>>>>>>>
>>>>>>>>>*******************
>>>>>>>>>
>>>>>>>>>\documentclass[a4paper]{article}
>>>>>>>>>\title{Sweave Test for summarize}
>>>>>>>>>\author{Sander Oom}
>>>>>>>>>
>>>>>>>>>\usepackage{a4wide}
>>>>>>>>>
>>>>>>>>>\begin{document}
>>>>>>>>>
>>>>>>>>>\maketitle
>>>>>>>>>
>>>>>>>>>\begin{figure}[ht]
>>>>>>>>>\begin{center}
>>>>>>>>><<fig=TRUE,echo=FALSE>>=
>>>>>>>>> # library(xtable)
>>>>>>>>> library(Hmisc)
>>>>>>>>> set.seed(111)
>>>>>>>>> dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
>>>>>>>>> month <- dfr$month
>>>>>>>>> year <- dfr$year
>>>>>>>>> y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
>>>>>>>>> s <- summarize(y, llist(month,year), smedian.hilow,
>>>>>conf.int=.5)
>>>>>>>>> print(xYplot(Cbind(y,Lower,Upper) ~ month, 
>>groups=year, data=s,
>>>>>>>>>       keys='lines', method='alt', type='b'))
>>>>>>>>>@
>>>>>>>>>\end{center}
>>>>>>>>>\end{figure}
>>>>>>>>>
>>>>>>>>>\end{document}
>>>>>>>>>
>>>>>>>>>************************
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>>version
>>>>>>>>>        _
>>>>>>>>>platform i686-pc-linux-gnu
>>>>>>>>>arch     i686
>>>>>>>>>os       linux-gnu
>>>>>>>>>system   i686, linux-gnu
>>>>>>>>>status
>>>>>>>>>major    2
>>>>>>>>>minor    1.0
>>>>>>>>>year     2005
>>>>>>>>>month    04
>>>>>>>>>day      18
>>>>>>>>>language R
>>>>>>>>>
>>>>>>>>>
>>>>>>>>I feel this is an xtable problem because Hmisc has being
>>>>>using label
>>>>>>>>and label<- since 1991.
>>>>>>>>
>>>>>>>>Frank
>>>>>>>>
>>>>>>There are ways to make functions from one area override those from
>>>>>>another, but the real solution is to ask the xtable author
>>>>>not to have
>>>>>>functions that conflict with the (older) Hmisc package.  -Frank
>>>>>>
>>>>>--
>>>>>--------------------------------------------
>>>>>Dr Sander P. Oom
>>>>>Animal, Plant and Environmental Sciences,
>>>>>University of the Witwatersrand
>>>>>Private Bag 3, Wits 2050, South Africa
>>>>>Tel (work)      +27 (0)11 717 64 04
>>>>>Tel (home)      +27 (0)18 297 44 51
>>>>>Fax             +27 (0)18 299 24 64
>>>>>Email   sander at oomvanlieshout.net
>>>>>Web     www.oomvanlieshout.net/sander
>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide!
>>>>>http://www.R-project.org/posting-guide.html
>>>>>
>>>>>
>>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>-- 
>>
>>--------------------------------------------
>>Dr Sander P. Oom
>>Animal, Plant and Environmental Sciences,
>>University of the Witwatersrand
>>Private Bag 3, Wits 2050, South Africa
>>Tel (work)      +27 (0)11 717 64 04
>>Tel (home)      +27 (0)18 297 44 51
>>Fax             +27 (0)18 299 24 64
>>Email   sander at oomvanlieshout.net
>>Web     www.oomvanlieshout.net/sander
>>---------------------------------------------
>>
>>
>>
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From dahl at stat.tamu.edu  Mon May 16 16:41:00 2005
From: dahl at stat.tamu.edu (David B. Dahl)
Date: Mon, 16 May 2005 09:41:00 -0500
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83E@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83E@usctmx1106.merck.com>
Message-ID: <4288B0FC.7090706@stat.tamu.edu>

Sander,

Thanks for pointing out the conflict between Hmisc and xtable.  I am not 
sure I have a good solution.

My understand of the namespace solution is that packages can specify 
which variables to export for use by the package users.  The label 
function is not an internal function, rather one what is intended for 
the user.

Renaming the label() would resolve the conflict with the Hmisc package, 
but make xtable not compatible with previous versions.

As noted by Andy, label() in xtable is an S3 generic with an 
implementation label.xtable() specific to the xtable package.  Perhaps 
Frank of Hmisc might be willing to make his follow the S3 generic naming 
convention?

I am open to suggestions and, more especially, code.  Thanks for using 
xtable.

-- David


Liaw, Andy wrote:

>You need to add the namespace to the source package, by adding a NAMESPACE
>file.  There's an R News article by Prof. Tierney on how to do this.  Also
>see the `Writing R Extensions' manual.  You should get the package
>maintainer to do that, as that constitute a change in the package source
>code.
>
>Short of that, you should make sure that Hmisc is loaded later than xtable,
>and use something like what Gabor suggested to access label() in xtable.  (I
>would use some other name, though: label() in xtable is already an S3
>generic).
>
>Andy
>
>  
>
>>From: Sander Oom 
>>
>>Hi Andy and Gabor,
>>
>>Thanks for your help so far! I am discovering another R dimension.
>>
>>Trying to put my head around all this....the conflict 
>>actually exposes 
>>itself when calling summarize(Hmisc). Summarize(Hmisc) calls label 
>>internally, so I can not call it explicitly. Simply calling 
>>label(xtable) explicitly will not solve the problem with 
>>summarize(Hmisc).
>>
>>Thus, I should use namespaces as Andy is suggesting. Now I 
>>just need to 
>>know how I 'add namespace' to a library? Does 'loadNamespace' have 
>>something to do with it?
>>
>>Thanks very much for your help!
>>
>>Sander.
>>
>>
>>## From Venables and Ripley (2002) p.165.
>>N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
>>P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
>>K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
>>yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
>>       62.8,55.8,69.5,55.0,
>>       62.0,48.8,45.5,44.2,52.0,
>>       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
>>npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
>>                   K=factor(K), yield=yield)
>>## to show the effects of re-ordering terms contrast the two fits
>>tmpAov <- aov(yield ~ block + N * P + K, npk)
>>tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
>>   label="tab:Anova")
>>print.xtable(tmpTable, type="latex", floating=TRUE,
>>   table.placement="h", caption.placement="top",
>>   latex.environments=c("center"))
>>
>>Alternatively, using namespace for xtable:
>>
>>tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
>>xtable:::label(tmpTable) <- paste("tab:Anova")
>>print.xtable(tmpTable, type="latex", floating=TRUE,
>>   table.placement="ht", caption.placement="top",
>>   latex.environments=c("center"))
>>
>>
>>
>>Gabor Grothendieck wrote:
>>    
>>
>>>Even without a namespace one could explicitly reference the label
>>>in xtable via:
>>>
>>>xtable.label <- get("label", "package:xtable")
>>>
>>>On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
>>>      
>>>
>>>>One possible solution without renaming the functions is to 
>>>>        
>>>>
>>add namespace to
>>    
>>
>>>>either xtable or Hmisc.  Given the size of Hmisc, it 
>>>>        
>>>>
>>probably would be much
>>    
>>
>>>>easier to do that with xtable.
>>>>
>>>>With namespace in xtable, you can do xtable:::label() to 
>>>>        
>>>>
>>refer to the
>>    
>>
>>>>label() in xtable specifically.
>>>>
>>>>Andy
>>>>
>>>>        
>>>>
>>>>>From: Of Sander Oom
>>>>>
>>>>>Dear David,
>>>>>
>>>>>I would like to use summarize(Hmisc) and print.xtable(xtable) in a
>>>>>single Sweave document, but a conflict with the 'label' function
>>>>>prohibits this at the moment!
>>>>>
>>>>>Would you be able to correct the conflicting code? I will 
>>>>>          
>>>>>
>>gladly test
>>    
>>
>>>>>the new package!
>>>>>
>>>>>I have tried latex(Hmisc) to export the anova table, but
>>>>>results are not
>>>>>promising! I prefer xtable!!
>>>>>
>>>>>Thanks,
>>>>>
>>>>>Sander.
>>>>>
>>>>>Frank E Harrell Jr wrote:
>>>>>          
>>>>>
>>>>>>Sander Oom wrote:
>>>>>>            
>>>>>>
>>>>>>>Dear Frank,
>>>>>>>
>>>>>>>I have a Sweave document in which I export anova (aov)
>>>>>>>              
>>>>>>>
>>>>>tables to Latex
>>>>>          
>>>>>
>>>>>>>and calculate some summary statistics with summarize{Hmisc} for a
>>>>>>>graph (as in the example below).
>>>>>>>
>>>>>>>I currently use the following code for the aov tables:
>>>>>>><<results=tex>>=
>>>>>>> tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
>>>>>>> tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut 
>>>>>>>              
>>>>>>>
>>, data=tmp)
>>    
>>
>>>>>>> tmpTable <- xtable (tmpAov ,
>>>>>>>   caption="ANOVA table for vegetation height.",
>>>>>>>   label="tab:AnovaHeight"
>>>>>>>   )
>>>>>>> print.xtable(tmpTable, type="latex", floating=TRUE,
>>>>>>>   table.placement="ht", caption.placement="top",
>>>>>>>   latex.environments=c("center"))
>>>>>>>   )
>>>>>>>@
>>>>>>>
>>>>>>>I used xtables, because it has a working aov example. I
>>>>>>>              
>>>>>>>
>>>>>would be happy
>>>>>          
>>>>>
>>>>>>>to use an alternative if I knew how! Would you have 
>>>>>>>              
>>>>>>>
>>sample code to
>>    
>>
>>>>>>>illustrate how to export an aov table to Latex using 
>>>>>>>              
>>>>>>>
>>latex{Hmisc}.
>>    
>>
>>>>>>>Thanks very much for your help,
>>>>>>>
>>>>>>>Sander.
>>>>>>>
>>>>>>>Frank E Harrell Jr wrote:
>>>>>>>
>>>>>>>              
>>>>>>>
>>>>>>>>Sander Oom wrote:
>>>>>>>>
>>>>>>>>                
>>>>>>>>
>>>>>>>>>Dear R users,
>>>>>>>>>
>>>>>>>>>The Sweave code below runs fine, as it is. However, an
>>>>>>>>>                  
>>>>>>>>>
>>>>>error occurs
>>>>>          
>>>>>
>>>>>>>>>when the line 'library(xtable)' is uncommented:
>>>>>>>>>Error:  chunk 1
>>>>>>>>>Error in "label<-"(`*tmp*`, value = "month") :
>>>>>>>>>       no applicable method for "label<-"
>>>>>>>>>
>>>>>>>>>Is anybody aware of this and knows a workaround?
>>>>>>>>>
>>>>>>>>>Thanks,
>>>>>>>>>
>>>>>>>>>Sander.
>>>>>>>>>
>>>>>>>>>*******************
>>>>>>>>>
>>>>>>>>>\documentclass[a4paper]{article}
>>>>>>>>>\title{Sweave Test for summarize}
>>>>>>>>>\author{Sander Oom}
>>>>>>>>>
>>>>>>>>>\usepackage{a4wide}
>>>>>>>>>
>>>>>>>>>\begin{document}
>>>>>>>>>
>>>>>>>>>\maketitle
>>>>>>>>>
>>>>>>>>>\begin{figure}[ht]
>>>>>>>>>\begin{center}
>>>>>>>>><<fig=TRUE,echo=FALSE>>=
>>>>>>>>> # library(xtable)
>>>>>>>>> library(Hmisc)
>>>>>>>>> set.seed(111)
>>>>>>>>> dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
>>>>>>>>> month <- dfr$month
>>>>>>>>> year <- dfr$year
>>>>>>>>> y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
>>>>>>>>> s <- summarize(y, llist(month,year), smedian.hilow,
>>>>>>>>>                  
>>>>>>>>>
>>>>>conf.int=.5)
>>>>>          
>>>>>
>>>>>>>>> print(xYplot(Cbind(y,Lower,Upper) ~ month, 
>>>>>>>>>                  
>>>>>>>>>
>>groups=year, data=s,
>>    
>>
>>>>>>>>>       keys='lines', method='alt', type='b'))
>>>>>>>>>@
>>>>>>>>>\end{center}
>>>>>>>>>\end{figure}
>>>>>>>>>
>>>>>>>>>\end{document}
>>>>>>>>>
>>>>>>>>>************************
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>                  
>>>>>>>>>
>>>>>>>>>>version
>>>>>>>>>>                    
>>>>>>>>>>
>>>>>>>>>        _
>>>>>>>>>platform i686-pc-linux-gnu
>>>>>>>>>arch     i686
>>>>>>>>>os       linux-gnu
>>>>>>>>>system   i686, linux-gnu
>>>>>>>>>status
>>>>>>>>>major    2
>>>>>>>>>minor    1.0
>>>>>>>>>year     2005
>>>>>>>>>month    04
>>>>>>>>>day      18
>>>>>>>>>language R
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>                  
>>>>>>>>>
>>>>>>>>I feel this is an xtable problem because Hmisc has being
>>>>>>>>                
>>>>>>>>
>>>>>using label
>>>>>          
>>>>>
>>>>>>>>and label<- since 1991.
>>>>>>>>
>>>>>>>>Frank
>>>>>>>>
>>>>>>>>                
>>>>>>>>
>>>>>>There are ways to make functions from one area override those from
>>>>>>another, but the real solution is to ask the xtable author
>>>>>>            
>>>>>>
>>>>>not to have
>>>>>          
>>>>>
>>>>>>functions that conflict with the (older) Hmisc package.  -Frank
>>>>>>
>>>>>>            
>>>>>>
>>>>>--
>>>>>--------------------------------------------
>>>>>Dr Sander P. Oom
>>>>>Animal, Plant and Environmental Sciences,
>>>>>University of the Witwatersrand
>>>>>Private Bag 3, Wits 2050, South Africa
>>>>>Tel (work)      +27 (0)11 717 64 04
>>>>>Tel (home)      +27 (0)18 297 44 51
>>>>>Fax             +27 (0)18 299 24 64
>>>>>Email   sander at oomvanlieshout.net
>>>>>Web     www.oomvanlieshout.net/sander
>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide!
>>>>>http://www.R-project.org/posting-guide.html
>>>>>
>>>>>
>>>>>
>>>>>          
>>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>        
>>>>
>>http://www.R-project.org/posting-guide.html
>>    
>>
>>-- 
>>
>>--------------------------------------------
>>Dr Sander P. Oom
>>Animal, Plant and Environmental Sciences,
>>University of the Witwatersrand
>>Private Bag 3, Wits 2050, South Africa
>>Tel (work)      +27 (0)11 717 64 04
>>Tel (home)      +27 (0)18 297 44 51
>>Fax             +27 (0)18 299 24 64
>>Email   sander at oomvanlieshout.net
>>Web     www.oomvanlieshout.net/sander
>>---------------------------------------------
>>
>>
>>
>>    
>>
>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
>------------------------------------------------------------------------------
>  
>



From slist at oomvanlieshout.net  Mon May 16 17:25:31 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Mon, 16 May 2005 17:25:31 +0200
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <4288B0FC.7090706@stat.tamu.edu>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83E@usctmx1106.merck.com>
	<4288B0FC.7090706@stat.tamu.edu>
Message-ID: <4288BB6B.10306@oomvanlieshout.net>

Hi David,

Thanks for creating and supporting xtable. Glad I can contribute by 
pointing out problems!

An earlier response from Frank Harrell Jr. suggests that the 
responsibility to resolve the conflict lies primarily with you, as Hmisc 
'was there first'.

Not sure how disputes over package conflicts are generally resolved!?

I'll await a final decision!

Thanks again,

Sander.


David B. Dahl wrote:
> Sander,
> 
> Thanks for pointing out the conflict between Hmisc and xtable.  I am not 
> sure I have a good solution.
> 
> My understand of the namespace solution is that packages can specify 
> which variables to export for use by the package users.  The label 
> function is not an internal function, rather one what is intended for 
> the user.
> 
> Renaming the label() would resolve the conflict with the Hmisc package, 
> but make xtable not compatible with previous versions.
> 
> As noted by Andy, label() in xtable is an S3 generic with an 
> implementation label.xtable() specific to the xtable package.  Perhaps 
> Frank of Hmisc might be willing to make his follow the S3 generic naming 
> convention?
> 
> I am open to suggestions and, more especially, code.  Thanks for using 
> xtable.
> 
> -- David
> 
> 
> Liaw, Andy wrote:
> 
>> You need to add the namespace to the source package, by adding a 
>> NAMESPACE
>> file.  There's an R News article by Prof. Tierney on how to do this.  
>> Also
>> see the `Writing R Extensions' manual.  You should get the package
>> maintainer to do that, as that constitute a change in the package source
>> code.
>>
>> Short of that, you should make sure that Hmisc is loaded later than 
>> xtable,
>> and use something like what Gabor suggested to access label() in 
>> xtable.  (I
>> would use some other name, though: label() in xtable is already an S3
>> generic).
>>
>> Andy
>>
>>  
>>
>>> From: Sander Oom
>>> Hi Andy and Gabor,
>>>
>>> Thanks for your help so far! I am discovering another R dimension.
>>>
>>> Trying to put my head around all this....the conflict actually 
>>> exposes itself when calling summarize(Hmisc). Summarize(Hmisc) calls 
>>> label internally, so I can not call it explicitly. Simply calling 
>>> label(xtable) explicitly will not solve the problem with 
>>> summarize(Hmisc).
>>>
>>> Thus, I should use namespaces as Andy is suggesting. Now I just need 
>>> to know how I 'add namespace' to a library? Does 'loadNamespace' have 
>>> something to do with it?
>>>
>>> Thanks very much for your help!
>>>
>>> Sander.
>>>
>>>
>>> ## From Venables and Ripley (2002) p.165.
>>> N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
>>> P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
>>> K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
>>> yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
>>>       62.8,55.8,69.5,55.0,
>>>       62.0,48.8,45.5,44.2,52.0,
>>>       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
>>> npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
>>>                   K=factor(K), yield=yield)
>>> ## to show the effects of re-ordering terms contrast the two fits
>>> tmpAov <- aov(yield ~ block + N * P + K, npk)
>>> tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
>>>   label="tab:Anova")
>>> print.xtable(tmpTable, type="latex", floating=TRUE,
>>>   table.placement="h", caption.placement="top",
>>>   latex.environments=c("center"))
>>>
>>> Alternatively, using namespace for xtable:
>>>
>>> tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
>>> xtable:::label(tmpTable) <- paste("tab:Anova")
>>> print.xtable(tmpTable, type="latex", floating=TRUE,
>>>   table.placement="ht", caption.placement="top",
>>>   latex.environments=c("center"))
>>>
>>>
>>>
>>> Gabor Grothendieck wrote:
>>>   
>>>> Even without a namespace one could explicitly reference the label
>>>> in xtable via:
>>>>
>>>> xtable.label <- get("label", "package:xtable")
>>>>
>>>> On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
>>>>     
>>>>> One possible solution without renaming the functions is to       
>>> add namespace to
>>>   
>>>>> either xtable or Hmisc.  Given the size of Hmisc, it       
>>> probably would be much
>>>   
>>>>> easier to do that with xtable.
>>>>>
>>>>> With namespace in xtable, you can do xtable:::label() to       
>>> refer to the
>>>   
>>>>> label() in xtable specifically.
>>>>>
>>>>> Andy
>>>>>
>>>>>       
>>>>>> From: Of Sander Oom
>>>>>>
>>>>>> Dear David,
>>>>>>
>>>>>> I would like to use summarize(Hmisc) and print.xtable(xtable) in a
>>>>>> single Sweave document, but a conflict with the 'label' function
>>>>>> prohibits this at the moment!
>>>>>>
>>>>>> Would you be able to correct the conflicting code? I will         
>>> gladly test
>>>   
>>>>>> the new package!
>>>>>>
>>>>>> I have tried latex(Hmisc) to export the anova table, but
>>>>>> results are not
>>>>>> promising! I prefer xtable!!
>>>>>>
>>>>>> Thanks,
>>>>>>
>>>>>> Sander.
>>>>>>
>>>>>> Frank E Harrell Jr wrote:
>>>>>>         
>>>>>>> Sander Oom wrote:
>>>>>>>           
>>>>>>>> Dear Frank,
>>>>>>>>
>>>>>>>> I have a Sweave document in which I export anova (aov)
>>>>>>>>             
>>>>>> tables to Latex
>>>>>>         
>>>>>>>> and calculate some summary statistics with summarize{Hmisc} for a
>>>>>>>> graph (as in the example below).
>>>>>>>>
>>>>>>>> I currently use the following code for the aov tables:
>>>>>>>> <<results=tex>>=
>>>>>>>> tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
>>>>>>>> tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut             
>>> , data=tmp)
>>>   
>>>>>>>> tmpTable <- xtable (tmpAov ,
>>>>>>>>   caption="ANOVA table for vegetation height.",
>>>>>>>>   label="tab:AnovaHeight"
>>>>>>>>   )
>>>>>>>> print.xtable(tmpTable, type="latex", floating=TRUE,
>>>>>>>>   table.placement="ht", caption.placement="top",
>>>>>>>>   latex.environments=c("center"))
>>>>>>>>   )
>>>>>>>> @
>>>>>>>>
>>>>>>>> I used xtables, because it has a working aov example. I
>>>>>>>>             
>>>>>> would be happy
>>>>>>         
>>>>>>>> to use an alternative if I knew how! Would you have             
>>> sample code to
>>>   
>>>>>>>> illustrate how to export an aov table to Latex using             
>>> latex{Hmisc}.
>>>   
>>>>>>>> Thanks very much for your help,
>>>>>>>>
>>>>>>>> Sander.
>>>>>>>>
>>>>>>>> Frank E Harrell Jr wrote:
>>>>>>>>
>>>>>>>>             
>>>>>>>>> Sander Oom wrote:
>>>>>>>>>
>>>>>>>>>               
>>>>>>>>>> Dear R users,
>>>>>>>>>>
>>>>>>>>>> The Sweave code below runs fine, as it is. However, an
>>>>>>>>>>                 
>>>>>> error occurs
>>>>>>         
>>>>>>>>>> when the line 'library(xtable)' is uncommented:
>>>>>>>>>> Error:  chunk 1
>>>>>>>>>> Error in "label<-"(`*tmp*`, value = "month") :
>>>>>>>>>>       no applicable method for "label<-"
>>>>>>>>>>
>>>>>>>>>> Is anybody aware of this and knows a workaround?
>>>>>>>>>>
>>>>>>>>>> Thanks,
>>>>>>>>>>
>>>>>>>>>> Sander.
>>>>>>>>>>
>>>>>>>>>> *******************
>>>>>>>>>>
>>>>>>>>>> \documentclass[a4paper]{article}
>>>>>>>>>> \title{Sweave Test for summarize}
>>>>>>>>>> \author{Sander Oom}
>>>>>>>>>>
>>>>>>>>>> \usepackage{a4wide}
>>>>>>>>>>
>>>>>>>>>> \begin{document}
>>>>>>>>>>
>>>>>>>>>> \maketitle
>>>>>>>>>>
>>>>>>>>>> \begin{figure}[ht]
>>>>>>>>>> \begin{center}
>>>>>>>>>> <<fig=TRUE,echo=FALSE>>=
>>>>>>>>>> # library(xtable)
>>>>>>>>>> library(Hmisc)
>>>>>>>>>> set.seed(111)
>>>>>>>>>> dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
>>>>>>>>>> month <- dfr$month
>>>>>>>>>> year <- dfr$year
>>>>>>>>>> y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
>>>>>>>>>> s <- summarize(y, llist(month,year), smedian.hilow,
>>>>>>>>>>                 
>>>>>> conf.int=.5)
>>>>>>         
>>>>>>>>>> print(xYplot(Cbind(y,Lower,Upper) ~ month,                 
>>> groups=year, data=s,
>>>   
>>>>>>>>>>       keys='lines', method='alt', type='b'))
>>>>>>>>>> @
>>>>>>>>>> \end{center}
>>>>>>>>>> \end{figure}
>>>>>>>>>>
>>>>>>>>>> \end{document}
>>>>>>>>>>
>>>>>>>>>> ************************
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>                 
>>>>>>>>>>> version
>>>>>>>>>>>                   
>>>>>>>>>>        _
>>>>>>>>>> platform i686-pc-linux-gnu
>>>>>>>>>> arch     i686
>>>>>>>>>> os       linux-gnu
>>>>>>>>>> system   i686, linux-gnu
>>>>>>>>>> status
>>>>>>>>>> major    2
>>>>>>>>>> minor    1.0
>>>>>>>>>> year     2005
>>>>>>>>>> month    04
>>>>>>>>>> day      18
>>>>>>>>>> language R
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>                 
>>>>>>>>> I feel this is an xtable problem because Hmisc has being
>>>>>>>>>               
>>>>>> using label
>>>>>>         
>>>>>>>>> and label<- since 1991.
>>>>>>>>>
>>>>>>>>> Frank
>>>>>>>>>
>>>>>>>>>               
>>>>>>> There are ways to make functions from one area override those from
>>>>>>> another, but the real solution is to ask the xtable author
>>>>>>>           
>>>>>> not to have
>>>>>>         
>>>>>>> functions that conflict with the (older) Hmisc package.  -Frank
>>>>>>>
>>>>>>>           
>>>>>> -- 
>>>>>> --------------------------------------------
>>>>>> Dr Sander P. Oom
>>>>>> Animal, Plant and Environmental Sciences,
>>>>>> University of the Witwatersrand
>>>>>> Private Bag 3, Wits 2050, South Africa
>>>>>> Tel (work)      +27 (0)11 717 64 04
>>>>>> Tel (home)      +27 (0)18 297 44 51
>>>>>> Fax             +27 (0)18 299 24 64
>>>>>> Email   sander at oomvanlieshout.net
>>>>>> Web     www.oomvanlieshout.net/sander
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide!
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>
>>>>>>
>>>>>>
>>>>>>         
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide!       
>>> http://www.R-project.org/posting-guide.html
>>>   
>>> -- 
>>>
>>> --------------------------------------------
>>> Dr Sander P. Oom
>>> Animal, Plant and Environmental Sciences,
>>> University of the Witwatersrand
>>> Private Bag 3, Wits 2050, South Africa
>>> Tel (work)      +27 (0)11 717 64 04
>>> Tel (home)      +27 (0)18 297 44 51
>>> Fax             +27 (0)18 299 24 64
>>> Email   sander at oomvanlieshout.net
>>> Web     www.oomvanlieshout.net/sander
>>> ---------------------------------------------
>>>
>>>
>>>
>>>   
>>
>>
>>
>> ------------------------------------------------------------------------------ 
>>
>> Notice:  This e-mail message, together with any attachments, contains 
>> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, 
>> New Jersey, USA 08889), and/or its affiliates (which may be known 
>> outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD 
>> and in Japan, as Banyu) that may be confidential, proprietary 
>> copyrighted and/or legally privileged. It is intended solely for the 
>> use of the individual or entity named on this message.  If you are not 
>> the intended recipient, and have received this message in error, 
>> please notify us immediately by reply e-mail and then delete it from 
>> your system.
>> ------------------------------------------------------------------------------ 
>>
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From Pierre.Lapointe at nbf.ca  Mon May 16 17:38:11 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Mon, 16 May 2005 11:38:11 -0400
Subject: [R] Turnpoints (pastecs): How to specify a limit on the number of
 tur npoints?
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF3579F@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050516/39c659ca/attachment.pl

From ggrothendieck at gmail.com  Mon May 16 17:41:51 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 May 2005 11:41:51 -0400
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <971536df050516075315d4f6ba@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83E@usctmx1106.merck.com>
	<4288AD5A.8040701@oomvanlieshout.net>
	<971536df050516075315d4f6ba@mail.gmail.com>
Message-ID: <971536df05051608413f682694@mail.gmail.com>

I am resending this since I sent it about an hour ago and still have
not seen it appear on R-help.  I assume it never went out but sorry
if it did and you get it twice.

On 5/16/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Using label as an lvalue (i.e. on the left hand side of the assignment)
> causes it to refer to a different function, not label itself.
> 
> Any any rate, looking at your example,
> it seems that you don't actually need to use Hmisc and xtable at the same time
> so just make sure that whichever you want at a particular point
> in your code is the only one of the two loaded:
> 
> library(Hmisc)
> ...code that involves Hmisc but not xtable...
> detach("package:Hmisc")
> library(xtable)
> ...code that involves xtable but not Hmisc...
> 
> On 5/16/05, Sander Oom <sander at oomvanlieshout.net> wrote:
> > I tried to follow your suggestions, but without success:
> >
> > Error: couldn't find function "xtable.mylabel<-"
> >
> > ... resulting from the code below.
> >
> > Any suggestions?
> >
> > Thanks,
> >
> > Sander.
> >
> > library(xtable)
> > xtable.mylabel <- get("label", "package:xtable")
> > library(Hmisc) # provides summarize
> >
> > set.seed(1)
> > temperature <- rnorm(300, 70, 10)
> > month <- sample(1:12, 300, TRUE)
> > year  <- sample(2000:2001, 300, TRUE)
> > g <- function(x)c(Mean=mean(x,na.rm=TRUE),Median=median(x,na.rm=TRUE))
> > summarize(temperature, month, g)
> >
> > ## From Venables and Ripley (2002) p.165.
> > N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
> > P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
> > K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
> > yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
> >       62.8,55.8,69.5,55.0,
> >       62.0,48.8,45.5,44.2,52.0,
> >       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
> > npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
> >                   K=factor(K), yield=yield)
> > ## to show the effects of re-ordering terms contrast the two fits
> > tmpAov <- aov(yield ~ block + N * P + K, npk)
> > tmpTable <- xtable (tmpAov ,
> >   caption="ANOVA table for vegetation height.")
> > xtable.mylabel(tmpTable) <- paste("tab:AnovaHeight")
> > print.xtable(tmpTable, type="latex", floating=TRUE,
> >   table.placement="h", caption.placement="top",
> >   latex.environments=c("center"),
> >   title=first.word(deparse(substitute(object))),
> >   append=FALSE
> >   )
> >
> >
> > Liaw, Andy wrote:
> > > You need to add the namespace to the source package, by adding a NAMESPACE
> > > file.  There's an R News article by Prof. Tierney on how to do this.  Also
> > > see the `Writing R Extensions' manual.  You should get the package
> > > maintainer to do that, as that constitute a change in the package source
> > > code.
> > >
> > > Short of that, you should make sure that Hmisc is loaded later than xtable,
> > > and use something like what Gabor suggested to access label() in xtable.  (I
> > > would use some other name, though: label() in xtable is already an S3
> > > generic).
> > >
> > > Andy
> > >
> > >>From: Sander Oom
> > >>
> > >>Hi Andy and Gabor,
> > >>
> > >>Thanks for your help so far! I am discovering another R dimension.
> > >>
> > >>Trying to put my head around all this....the conflict
> > >>actually exposes
> > >>itself when calling summarize(Hmisc). Summarize(Hmisc) calls label
> > >>internally, so I can not call it explicitly. Simply calling
> > >>label(xtable) explicitly will not solve the problem with
> > >>summarize(Hmisc).
> > >>
> > >>Thus, I should use namespaces as Andy is suggesting. Now I
> > >>just need to
> > >>know how I 'add namespace' to a library? Does 'loadNamespace' have
> > >>something to do with it?
> > >>
> > >>Thanks very much for your help!
> > >>
> > >>Sander.
> > >>
> > >>
> > >>## From Venables and Ripley (2002) p.165.
> > >>N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
> > >>P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
> > >>K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
> > >>yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
> > >>       62.8,55.8,69.5,55.0,
> > >>       62.0,48.8,45.5,44.2,52.0,
> > >>       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
> > >>npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
> > >>                   K=factor(K), yield=yield)
> > >>## to show the effects of re-ordering terms contrast the two fits
> > >>tmpAov <- aov(yield ~ block + N * P + K, npk)
> > >>tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
> > >>   label="tab:Anova")
> > >>print.xtable(tmpTable, type="latex", floating=TRUE,
> > >>   table.placement="h", caption.placement="top",
> > >>   latex.environments=c("center"))
> > >>
> > >>Alternatively, using namespace for xtable:
> > >>
> > >>tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
> > >>xtable:::label(tmpTable) <- paste("tab:Anova")
> > >>print.xtable(tmpTable, type="latex", floating=TRUE,
> > >>   table.placement="ht", caption.placement="top",
> > >>   latex.environments=c("center"))
> > >>
> > >>
> > >>
> > >>Gabor Grothendieck wrote:
> > >>>Even without a namespace one could explicitly reference the label
> > >>>in xtable via:
> > >>>
> > >>>xtable.label <- get("label", "package:xtable")
> > >>>
> > >>>On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > >>>>One possible solution without renaming the functions is to
> > >>add namespace to
> > >>>>either xtable or Hmisc.  Given the size of Hmisc, it
> > >>probably would be much
> > >>>>easier to do that with xtable.
> > >>>>
> > >>>>With namespace in xtable, you can do xtable:::label() to
> > >>refer to the
> > >>>>label() in xtable specifically.
> > >>>>
> > >>>>Andy
> > >>>>
> > >>>>>From: Of Sander Oom
> > >>>>>
> > >>>>>Dear David,
> > >>>>>
> > >>>>>I would like to use summarize(Hmisc) and print.xtable(xtable) in a
> > >>>>>single Sweave document, but a conflict with the 'label' function
> > >>>>>prohibits this at the moment!
> > >>>>>
> > >>>>>Would you be able to correct the conflicting code? I will
> > >>gladly test
> > >>>>>the new package!
> > >>>>>
> > >>>>>I have tried latex(Hmisc) to export the anova table, but
> > >>>>>results are not
> > >>>>>promising! I prefer xtable!!
> > >>>>>
> > >>>>>Thanks,
> > >>>>>
> > >>>>>Sander.
> > >>>>>
> > >>>>>Frank E Harrell Jr wrote:
> > >>>>>>Sander Oom wrote:
> > >>>>>>>Dear Frank,
> > >>>>>>>
> > >>>>>>>I have a Sweave document in which I export anova (aov)
> > >>>>>tables to Latex
> > >>>>>>>and calculate some summary statistics with summarize{Hmisc} for a
> > >>>>>>>graph (as in the example below).
> > >>>>>>>
> > >>>>>>>I currently use the following code for the aov tables:
> > >>>>>>><<results=tex>>=
> > >>>>>>> tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
> > >>>>>>> tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut
> > >>, data=tmp)
> > >>>>>>> tmpTable <- xtable (tmpAov ,
> > >>>>>>>   caption="ANOVA table for vegetation height.",
> > >>>>>>>   label="tab:AnovaHeight"
> > >>>>>>>   )
> > >>>>>>> print.xtable(tmpTable, type="latex", floating=TRUE,
> > >>>>>>>   table.placement="ht", caption.placement="top",
> > >>>>>>>   latex.environments=c("center"))
> > >>>>>>>   )
> > >>>>>>>@
> > >>>>>>>
> > >>>>>>>I used xtables, because it has a working aov example. I
> > >>>>>would be happy
> > >>>>>>>to use an alternative if I knew how! Would you have
> > >>sample code to
> > >>>>>>>illustrate how to export an aov table to Latex using
> > >>latex{Hmisc}.
> > >>>>>>>Thanks very much for your help,
> > >>>>>>>
> > >>>>>>>Sander.
> > >>>>>>>
> > >>>>>>>Frank E Harrell Jr wrote:
> > >>>>>>>
> > >>>>>>>>Sander Oom wrote:
> > >>>>>>>>
> > >>>>>>>>>Dear R users,
> > >>>>>>>>>
> > >>>>>>>>>The Sweave code below runs fine, as it is. However, an
> > >>>>>error occurs
> > >>>>>>>>>when the line 'library(xtable)' is uncommented:
> > >>>>>>>>>Error:  chunk 1
> > >>>>>>>>>Error in "label<-"(`*tmp*`, value = "month") :
> > >>>>>>>>>       no applicable method for "label<-"
> > >>>>>>>>>
> > >>>>>>>>>Is anybody aware of this and knows a workaround?
> > >>>>>>>>>
> > >>>>>>>>>Thanks,
> > >>>>>>>>>
> > >>>>>>>>>Sander.
> > >>>>>>>>>
> > >>>>>>>>>*******************
> > >>>>>>>>>
> > >>>>>>>>>\documentclass[a4paper]{article}
> > >>>>>>>>>\title{Sweave Test for summarize}
> > >>>>>>>>>\author{Sander Oom}
> > >>>>>>>>>
> > >>>>>>>>>\usepackage{a4wide}
> > >>>>>>>>>
> > >>>>>>>>>\begin{document}
> > >>>>>>>>>
> > >>>>>>>>>\maketitle
> > >>>>>>>>>
> > >>>>>>>>>\begin{figure}[ht]
> > >>>>>>>>>\begin{center}
> > >>>>>>>>><<fig=TRUE,echo=FALSE>>=
> > >>>>>>>>> # library(xtable)
> > >>>>>>>>> library(Hmisc)
> > >>>>>>>>> set.seed(111)
> > >>>>>>>>> dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
> > >>>>>>>>> month <- dfr$month
> > >>>>>>>>> year <- dfr$year
> > >>>>>>>>> y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
> > >>>>>>>>> s <- summarize(y, llist(month,year), smedian.hilow,
> > >>>>>conf.int=.5)
> > >>>>>>>>> print(xYplot(Cbind(y,Lower,Upper) ~ month,
> > >>groups=year, data=s,
> > >>>>>>>>>       keys='lines', method='alt', type='b'))
> > >>>>>>>>>@
> > >>>>>>>>>\end{center}
> > >>>>>>>>>\end{figure}
> > >>>>>>>>>
> > >>>>>>>>>\end{document}
> > >>>>>>>>>
> > >>>>>>>>>************************
> > >>>>>>>>>
> > >>>>>>>>>
> > >>>>>>>>>
> > >>>>>>>>>>version
> > >>>>>>>>>        _
> > >>>>>>>>>platform i686-pc-linux-gnu
> > >>>>>>>>>arch     i686
> > >>>>>>>>>os       linux-gnu
> > >>>>>>>>>system   i686, linux-gnu
> > >>>>>>>>>status
> > >>>>>>>>>major    2
> > >>>>>>>>>minor    1.0
> > >>>>>>>>>year     2005
> > >>>>>>>>>month    04
> > >>>>>>>>>day      18
> > >>>>>>>>>language R
> > >>>>>>>>>
> > >>>>>>>>>
> > >>>>>>>>I feel this is an xtable problem because Hmisc has being
> > >>>>>using label
> > >>>>>>>>and label<- since 1991.
> > >>>>>>>>
> > >>>>>>>>Frank
> > >>>>>>>>
> > >>>>>>There are ways to make functions from one area override those from
> > >>>>>>another, but the real solution is to ask the xtable author
> > >>>>>not to have
> > >>>>>>functions that conflict with the (older) Hmisc package.  -Frank
> > >>>>>>
> > >>>>>--
> > >>>>>--------------------------------------------
> > >>>>>Dr Sander P. Oom
> > >>>>>Animal, Plant and Environmental Sciences,
> > >>>>>University of the Witwatersrand
> > >>>>>Private Bag 3, Wits 2050, South Africa
> > >>>>>Tel (work)      +27 (0)11 717 64 04
> > >>>>>Tel (home)      +27 (0)18 297 44 51
> > >>>>>Fax             +27 (0)18 299 24 64
> > >>>>>Email   sander at oomvanlieshout.net
> > >>>>>Web     www.oomvanlieshout.net/sander
> > >>>>>
> > >>>>>______________________________________________
> > >>>>>R-help at stat.math.ethz.ch mailing list
> > >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>>PLEASE do read the posting guide!
> > >>>>>http://www.R-project.org/posting-guide.html
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>______________________________________________
> > >>>>R-help at stat.math.ethz.ch mailing list
> > >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>PLEASE do read the posting guide!
> > >>http://www.R-project.org/posting-guide.html
> > >>--
> > >>
> > >>--------------------------------------------
> > >>Dr Sander P. Oom
> > >>Animal, Plant and Environmental Sciences,
> > >>University of the Witwatersrand
> > >>Private Bag 3, Wits 2050, South Africa
> > >>Tel (work)      +27 (0)11 717 64 04
> > >>Tel (home)      +27 (0)18 297 44 51
> > >>Fax             +27 (0)18 299 24 64
> > >>Email   sander at oomvanlieshout.net
> > >>Web     www.oomvanlieshout.net/sander
> > >>---------------------------------------------
> > >>
> > >>
> > >>
> > >
> > >
> > >
> > > ------------------------------------------------------------------------------
> > > Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
> > > ------------------------------------------------------------------------------
> > >
> >
>



From andy_liaw at merck.com  Mon May 16 17:46:15 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 16 May 2005 11:46:15 -0400
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83F@usctmx1106.merck.com>

For fixing the problem you have, you need:

"xtable.mylabel<-" <- get("label<-.xtable", "package:xtable")

I.e., you need the replacement function.

One possibility of resolving the conflict, as I communicated with Frank, is
to make label() and label<-() in Hmisc the S3 default methods, as xtable
defines the methods for xtable, but provided no default methods.  This
probably has the minimal impact (if any) on backward compatibility in both
packages.

BTW, you should call print() rather than print.xtable().

Andy

> From: Sander Oom 
> 
> Hi David,
> 
> Thanks for creating and supporting xtable. Glad I can contribute by 
> pointing out problems!
> 
> An earlier response from Frank Harrell Jr. suggests that the 
> responsibility to resolve the conflict lies primarily with 
> you, as Hmisc 
> 'was there first'.
> 
> Not sure how disputes over package conflicts are generally resolved!?
> 
> I'll await a final decision!
> 
> Thanks again,
> 
> Sander.
> 
> 
> David B. Dahl wrote:
> > Sander,
> > 
> > Thanks for pointing out the conflict between Hmisc and 
> xtable.  I am not 
> > sure I have a good solution.
> > 
> > My understand of the namespace solution is that packages 
> can specify 
> > which variables to export for use by the package users.  The label 
> > function is not an internal function, rather one what is 
> intended for 
> > the user.
> > 
> > Renaming the label() would resolve the conflict with the 
> Hmisc package, 
> > but make xtable not compatible with previous versions.
> > 
> > As noted by Andy, label() in xtable is an S3 generic with an 
> > implementation label.xtable() specific to the xtable 
> package.  Perhaps 
> > Frank of Hmisc might be willing to make his follow the S3 
> generic naming 
> > convention?
> > 
> > I am open to suggestions and, more especially, code.  
> Thanks for using 
> > xtable.
> > 
> > -- David
> > 
> > 
> > Liaw, Andy wrote:
> > 
> >> You need to add the namespace to the source package, by adding a 
> >> NAMESPACE
> >> file.  There's an R News article by Prof. Tierney on how 
> to do this.  
> >> Also
> >> see the `Writing R Extensions' manual.  You should get the package
> >> maintainer to do that, as that constitute a change in the 
> package source
> >> code.
> >>
> >> Short of that, you should make sure that Hmisc is loaded 
> later than 
> >> xtable,
> >> and use something like what Gabor suggested to access label() in 
> >> xtable.  (I
> >> would use some other name, though: label() in xtable is 
> already an S3
> >> generic).
> >>
> >> Andy
> >>
> >>  
> >>
> >>> From: Sander Oom
> >>> Hi Andy and Gabor,
> >>>
> >>> Thanks for your help so far! I am discovering another R dimension.
> >>>
> >>> Trying to put my head around all this....the conflict actually 
> >>> exposes itself when calling summarize(Hmisc). 
> Summarize(Hmisc) calls 
> >>> label internally, so I can not call it explicitly. Simply calling 
> >>> label(xtable) explicitly will not solve the problem with 
> >>> summarize(Hmisc).
> >>>
> >>> Thus, I should use namespaces as Andy is suggesting. Now 
> I just need 
> >>> to know how I 'add namespace' to a library? Does 
> 'loadNamespace' have 
> >>> something to do with it?
> >>>
> >>> Thanks very much for your help!
> >>>
> >>> Sander.
> >>>
> >>>
> >>> ## From Venables and Ripley (2002) p.165.
> >>> N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
> >>> P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
> >>> K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
> >>> yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
> >>>       62.8,55.8,69.5,55.0,
> >>>       62.0,48.8,45.5,44.2,52.0,
> >>>       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
> >>> npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
> >>>                   K=factor(K), yield=yield)
> >>> ## to show the effects of re-ordering terms contrast the two fits
> >>> tmpAov <- aov(yield ~ block + N * P + K, npk)
> >>> tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
> >>>   label="tab:Anova")
> >>> print.xtable(tmpTable, type="latex", floating=TRUE,
> >>>   table.placement="h", caption.placement="top",
> >>>   latex.environments=c("center"))
> >>>
> >>> Alternatively, using namespace for xtable:
> >>>
> >>> tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
> >>> xtable:::label(tmpTable) <- paste("tab:Anova")
> >>> print.xtable(tmpTable, type="latex", floating=TRUE,
> >>>   table.placement="ht", caption.placement="top",
> >>>   latex.environments=c("center"))
> >>>
> >>>
> >>>
> >>> Gabor Grothendieck wrote:
> >>>   
> >>>> Even without a namespace one could explicitly reference the label
> >>>> in xtable via:
> >>>>
> >>>> xtable.label <- get("label", "package:xtable")
> >>>>
> >>>> On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> >>>>     
> >>>>> One possible solution without renaming the functions is 
> to       
> >>> add namespace to
> >>>   
> >>>>> either xtable or Hmisc.  Given the size of Hmisc, it       
> >>> probably would be much
> >>>   
> >>>>> easier to do that with xtable.
> >>>>>
> >>>>> With namespace in xtable, you can do xtable:::label() to       
> >>> refer to the
> >>>   
> >>>>> label() in xtable specifically.
> >>>>>
> >>>>> Andy
> >>>>>
> >>>>>       
> >>>>>> From: Of Sander Oom
> >>>>>>
> >>>>>> Dear David,
> >>>>>>
> >>>>>> I would like to use summarize(Hmisc) and 
> print.xtable(xtable) in a
> >>>>>> single Sweave document, but a conflict with the 
> 'label' function
> >>>>>> prohibits this at the moment!
> >>>>>>
> >>>>>> Would you be able to correct the conflicting code? I 
> will         
> >>> gladly test
> >>>   
> >>>>>> the new package!
> >>>>>>
> >>>>>> I have tried latex(Hmisc) to export the anova table, but
> >>>>>> results are not
> >>>>>> promising! I prefer xtable!!
> >>>>>>
> >>>>>> Thanks,
> >>>>>>
> >>>>>> Sander.
> >>>>>>
> >>>>>> Frank E Harrell Jr wrote:
> >>>>>>         
> >>>>>>> Sander Oom wrote:
> >>>>>>>           
> >>>>>>>> Dear Frank,
> >>>>>>>>
> >>>>>>>> I have a Sweave document in which I export anova (aov)
> >>>>>>>>             
> >>>>>> tables to Latex
> >>>>>>         
> >>>>>>>> and calculate some summary statistics with 
> summarize{Hmisc} for a
> >>>>>>>> graph (as in the example below).
> >>>>>>>>
> >>>>>>>> I currently use the following code for the aov tables:
> >>>>>>>> <<results=tex>>=
> >>>>>>>> tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
> >>>>>>>> tmpAov <- 
> aov(Height~Geology*Altitude*Origin*BinInOut             
> >>> , data=tmp)
> >>>   
> >>>>>>>> tmpTable <- xtable (tmpAov ,
> >>>>>>>>   caption="ANOVA table for vegetation height.",
> >>>>>>>>   label="tab:AnovaHeight"
> >>>>>>>>   )
> >>>>>>>> print.xtable(tmpTable, type="latex", floating=TRUE,
> >>>>>>>>   table.placement="ht", caption.placement="top",
> >>>>>>>>   latex.environments=c("center"))
> >>>>>>>>   )
> >>>>>>>> @
> >>>>>>>>
> >>>>>>>> I used xtables, because it has a working aov example. I
> >>>>>>>>             
> >>>>>> would be happy
> >>>>>>         
> >>>>>>>> to use an alternative if I knew how! Would you have  
>            
> >>> sample code to
> >>>   
> >>>>>>>> illustrate how to export an aov table to Latex using 
>             
> >>> latex{Hmisc}.
> >>>   
> >>>>>>>> Thanks very much for your help,
> >>>>>>>>
> >>>>>>>> Sander.
> >>>>>>>>
> >>>>>>>> Frank E Harrell Jr wrote:
> >>>>>>>>
> >>>>>>>>             
> >>>>>>>>> Sander Oom wrote:
> >>>>>>>>>
> >>>>>>>>>               
> >>>>>>>>>> Dear R users,
> >>>>>>>>>>
> >>>>>>>>>> The Sweave code below runs fine, as it is. However, an
> >>>>>>>>>>                 
> >>>>>> error occurs
> >>>>>>         
> >>>>>>>>>> when the line 'library(xtable)' is uncommented:
> >>>>>>>>>> Error:  chunk 1
> >>>>>>>>>> Error in "label<-"(`*tmp*`, value = "month") :
> >>>>>>>>>>       no applicable method for "label<-"
> >>>>>>>>>>
> >>>>>>>>>> Is anybody aware of this and knows a workaround?
> >>>>>>>>>>
> >>>>>>>>>> Thanks,
> >>>>>>>>>>
> >>>>>>>>>> Sander.
> >>>>>>>>>>
> >>>>>>>>>> *******************
> >>>>>>>>>>
> >>>>>>>>>> \documentclass[a4paper]{article}
> >>>>>>>>>> \title{Sweave Test for summarize}
> >>>>>>>>>> \author{Sander Oom}
> >>>>>>>>>>
> >>>>>>>>>> \usepackage{a4wide}
> >>>>>>>>>>
> >>>>>>>>>> \begin{document}
> >>>>>>>>>>
> >>>>>>>>>> \maketitle
> >>>>>>>>>>
> >>>>>>>>>> \begin{figure}[ht]
> >>>>>>>>>> \begin{center}
> >>>>>>>>>> <<fig=TRUE,echo=FALSE>>=
> >>>>>>>>>> # library(xtable)
> >>>>>>>>>> library(Hmisc)
> >>>>>>>>>> set.seed(111)
> >>>>>>>>>> dfr <- expand.grid(month=1:12, year=c(1997,1998), 
> reps=1:100)
> >>>>>>>>>> month <- dfr$month
> >>>>>>>>>> year <- dfr$year
> >>>>>>>>>> y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
> >>>>>>>>>> s <- summarize(y, llist(month,year), smedian.hilow,
> >>>>>>>>>>                 
> >>>>>> conf.int=.5)
> >>>>>>         
> >>>>>>>>>> print(xYplot(Cbind(y,Lower,Upper) ~ month,                 
> >>> groups=year, data=s,
> >>>   
> >>>>>>>>>>       keys='lines', method='alt', type='b'))
> >>>>>>>>>> @
> >>>>>>>>>> \end{center}
> >>>>>>>>>> \end{figure}
> >>>>>>>>>>
> >>>>>>>>>> \end{document}
> >>>>>>>>>>
> >>>>>>>>>> ************************
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>                 
> >>>>>>>>>>> version
> >>>>>>>>>>>                   
> >>>>>>>>>>        _
> >>>>>>>>>> platform i686-pc-linux-gnu
> >>>>>>>>>> arch     i686
> >>>>>>>>>> os       linux-gnu
> >>>>>>>>>> system   i686, linux-gnu
> >>>>>>>>>> status
> >>>>>>>>>> major    2
> >>>>>>>>>> minor    1.0
> >>>>>>>>>> year     2005
> >>>>>>>>>> month    04
> >>>>>>>>>> day      18
> >>>>>>>>>> language R
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>                 
> >>>>>>>>> I feel this is an xtable problem because Hmisc has being
> >>>>>>>>>               
> >>>>>> using label
> >>>>>>         
> >>>>>>>>> and label<- since 1991.
> >>>>>>>>>
> >>>>>>>>> Frank
> >>>>>>>>>
> >>>>>>>>>               
> >>>>>>> There are ways to make functions from one area 
> override those from
> >>>>>>> another, but the real solution is to ask the xtable author
> >>>>>>>           
> >>>>>> not to have
> >>>>>>         
> >>>>>>> functions that conflict with the (older) Hmisc 
> package.  -Frank
> >>>>>>>
> >>>>>>>           
> >>>>>> -- 
> >>>>>> --------------------------------------------
> >>>>>> Dr Sander P. Oom
> >>>>>> Animal, Plant and Environmental Sciences,
> >>>>>> University of the Witwatersrand
> >>>>>> Private Bag 3, Wits 2050, South Africa
> >>>>>> Tel (work)      +27 (0)11 717 64 04
> >>>>>> Tel (home)      +27 (0)18 297 44 51
> >>>>>> Fax             +27 (0)18 299 24 64
> >>>>>> Email   sander at oomvanlieshout.net
> >>>>>> Web     www.oomvanlieshout.net/sander
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at stat.math.ethz.ch mailing list
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide!
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>         
> >>>>> ______________________________________________
> >>>>> R-help at stat.math.ethz.ch mailing list
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide!       
> >>> http://www.R-project.org/posting-guide.html
> >>>   
> >>> -- 
> >>>
> >>> --------------------------------------------
> >>> Dr Sander P. Oom
> >>> Animal, Plant and Environmental Sciences,
> >>> University of the Witwatersrand
> >>> Private Bag 3, Wits 2050, South Africa
> >>> Tel (work)      +27 (0)11 717 64 04
> >>> Tel (home)      +27 (0)18 297 44 51
> >>> Fax             +27 (0)18 299 24 64
> >>> Email   sander at oomvanlieshout.net
> >>> Web     www.oomvanlieshout.net/sander
> >>> ---------------------------------------------
> >>>
> >>>
> >>>
> >>>   
> >>
> >>
> >>
> >> 
> --------------------------------------------------------------
> ---------------- 
> >>
> >> Notice:  This e-mail message, together with any 
> attachments, contains 
> >> information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, 
> >> New Jersey, USA 08889), and/or its affiliates (which may be known 
> >> outside the United States as Merck Frosst, Merck Sharp & 
> Dohme or MSD 
> >> and in Japan, as Banyu) that may be confidential, proprietary 
> >> copyrighted and/or legally privileged. It is intended 
> solely for the 
> >> use of the individual or entity named on this message.  If 
> you are not 
> >> the intended recipient, and have received this message in error, 
> >> please notify us immediately by reply e-mail and then 
> delete it from 
> >> your system.
> >> 
> --------------------------------------------------------------
> ---------------- 
> >>
> >>  
> >>
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> 
> -- 
> --------------------------------------------
> Dr Sander P. Oom
> Animal, Plant and Environmental Sciences,
> University of the Witwatersrand
> Private Bag 3, Wits 2050, South Africa
> Tel (work)      +27 (0)11 717 64 04
> Tel (home)      +27 (0)18 297 44 51
> Fax             +27 (0)18 299 24 64
> Email   sander at oomvanlieshout.net
> Web     www.oomvanlieshout.net/sander
> ---------------------------------------------
> 
> 
>



From slist at oomvanlieshout.net  Mon May 16 17:49:55 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Mon, 16 May 2005 17:49:55 +0200
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <971536df05051608413f682694@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83E@usctmx1106.merck.com>	<4288AD5A.8040701@oomvanlieshout.net>	<971536df050516075315d4f6ba@mail.gmail.com>
	<971536df05051608413f682694@mail.gmail.com>
Message-ID: <4288C123.3080206@oomvanlieshout.net>

I did receive it in good order, but the threading on this discussion is 
a bit messy!

I have used your suggestion to temporarily 'attach' and detach Hmisc, 
while I am awaiting a more permanent solution to the conflict.

Thanks,

Sander.

Gabor Grothendieck wrote:
> I am resending this since I sent it about an hour ago and still have
> not seen it appear on R-help.  I assume it never went out but sorry
> if it did and you get it twice.
> 
> On 5/16/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>Using label as an lvalue (i.e. on the left hand side of the assignment)
>>causes it to refer to a different function, not label itself.
>>
>>Any any rate, looking at your example,
>>it seems that you don't actually need to use Hmisc and xtable at the same time
>>so just make sure that whichever you want at a particular point
>>in your code is the only one of the two loaded:
>>
>>library(Hmisc)
>>...code that involves Hmisc but not xtable...
>>detach("package:Hmisc")
>>library(xtable)
>>...code that involves xtable but not Hmisc...
>>
>>On 5/16/05, Sander Oom <sander at oomvanlieshout.net> wrote:
>>>I tried to follow your suggestions, but without success:
>>>
>>>Error: couldn't find function "xtable.mylabel<-"
>>>
>>>... resulting from the code below.
>>>
>>>Any suggestions?
>>>
>>>Thanks,
>>>
>>>Sander.
>>>
>>>library(xtable)
>>>xtable.mylabel <- get("label", "package:xtable")
>>>library(Hmisc) # provides summarize
>>>
>>>set.seed(1)
>>>temperature <- rnorm(300, 70, 10)
>>>month <- sample(1:12, 300, TRUE)
>>>year  <- sample(2000:2001, 300, TRUE)
>>>g <- function(x)c(Mean=mean(x,na.rm=TRUE),Median=median(x,na.rm=TRUE))
>>>summarize(temperature, month, g)
>>>
>>>## From Venables and Ripley (2002) p.165.
>>>N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
>>>P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
>>>K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
>>>yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
>>>      62.8,55.8,69.5,55.0,
>>>      62.0,48.8,45.5,44.2,52.0,
>>>      51.5,49.8,48.8,57.2,59.0,53.2,56.0)
>>>npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
>>>                  K=factor(K), yield=yield)
>>>## to show the effects of re-ordering terms contrast the two fits
>>>tmpAov <- aov(yield ~ block + N * P + K, npk)
>>>tmpTable <- xtable (tmpAov ,
>>>  caption="ANOVA table for vegetation height.")
>>>xtable.mylabel(tmpTable) <- paste("tab:AnovaHeight")
>>>print.xtable(tmpTable, type="latex", floating=TRUE,
>>>  table.placement="h", caption.placement="top",
>>>  latex.environments=c("center"),
>>>  title=first.word(deparse(substitute(object))),
>>>  append=FALSE
>>>  )
>>>
>>>
>>>Liaw, Andy wrote:
>>>>You need to add the namespace to the source package, by adding a NAMESPACE
>>>>file.  There's an R News article by Prof. Tierney on how to do this.  Also
>>>>see the `Writing R Extensions' manual.  You should get the package
>>>>maintainer to do that, as that constitute a change in the package source
>>>>code.
>>>>
>>>>Short of that, you should make sure that Hmisc is loaded later than xtable,
>>>>and use something like what Gabor suggested to access label() in xtable.  (I
>>>>would use some other name, though: label() in xtable is already an S3
>>>>generic).
>>>>
>>>>Andy
>>>>
>>>>>From: Sander Oom
>>>>>
>>>>>Hi Andy and Gabor,
>>>>>
>>>>>Thanks for your help so far! I am discovering another R dimension.
>>>>>
>>>>>Trying to put my head around all this....the conflict
>>>>>actually exposes
>>>>>itself when calling summarize(Hmisc). Summarize(Hmisc) calls label
>>>>>internally, so I can not call it explicitly. Simply calling
>>>>>label(xtable) explicitly will not solve the problem with
>>>>>summarize(Hmisc).
>>>>>
>>>>>Thus, I should use namespaces as Andy is suggesting. Now I
>>>>>just need to
>>>>>know how I 'add namespace' to a library? Does 'loadNamespace' have
>>>>>something to do with it?
>>>>>
>>>>>Thanks very much for your help!
>>>>>
>>>>>Sander.
>>>>>
>>>>>
>>>>>## From Venables and Ripley (2002) p.165.
>>>>>N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
>>>>>P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
>>>>>K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
>>>>>yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
>>>>>      62.8,55.8,69.5,55.0,
>>>>>      62.0,48.8,45.5,44.2,52.0,
>>>>>      51.5,49.8,48.8,57.2,59.0,53.2,56.0)
>>>>>npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
>>>>>                  K=factor(K), yield=yield)
>>>>>## to show the effects of re-ordering terms contrast the two fits
>>>>>tmpAov <- aov(yield ~ block + N * P + K, npk)
>>>>>tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
>>>>>  label="tab:Anova")
>>>>>print.xtable(tmpTable, type="latex", floating=TRUE,
>>>>>  table.placement="h", caption.placement="top",
>>>>>  latex.environments=c("center"))
>>>>>
>>>>>Alternatively, using namespace for xtable:
>>>>>
>>>>>tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
>>>>>xtable:::label(tmpTable) <- paste("tab:Anova")
>>>>>print.xtable(tmpTable, type="latex", floating=TRUE,
>>>>>  table.placement="ht", caption.placement="top",
>>>>>  latex.environments=c("center"))
>>>>>
>>>>>
>>>>>
>>>>>Gabor Grothendieck wrote:
>>>>>>Even without a namespace one could explicitly reference the label
>>>>>>in xtable via:
>>>>>>
>>>>>>xtable.label <- get("label", "package:xtable")
>>>>>>
>>>>>>On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
>>>>>>>One possible solution without renaming the functions is to
>>>>>add namespace to
>>>>>>>either xtable or Hmisc.  Given the size of Hmisc, it
>>>>>probably would be much
>>>>>>>easier to do that with xtable.
>>>>>>>
>>>>>>>With namespace in xtable, you can do xtable:::label() to
>>>>>refer to the
>>>>>>>label() in xtable specifically.
>>>>>>>
>>>>>>>Andy
>>>>>>>
>>>>>>>>From: Of Sander Oom
>>>>>>>>
>>>>>>>>Dear David,
>>>>>>>>
>>>>>>>>I would like to use summarize(Hmisc) and print.xtable(xtable) in a
>>>>>>>>single Sweave document, but a conflict with the 'label' function
>>>>>>>>prohibits this at the moment!
>>>>>>>>
>>>>>>>>Would you be able to correct the conflicting code? I will
>>>>>gladly test
>>>>>>>>the new package!
>>>>>>>>
>>>>>>>>I have tried latex(Hmisc) to export the anova table, but
>>>>>>>>results are not
>>>>>>>>promising! I prefer xtable!!
>>>>>>>>
>>>>>>>>Thanks,
>>>>>>>>
>>>>>>>>Sander.
>>>>>>>>
>>>>>>>>Frank E Harrell Jr wrote:
>>>>>>>>>Sander Oom wrote:
>>>>>>>>>>Dear Frank,
>>>>>>>>>>
>>>>>>>>>>I have a Sweave document in which I export anova (aov)
>>>>>>>>tables to Latex
>>>>>>>>>>and calculate some summary statistics with summarize{Hmisc} for a
>>>>>>>>>>graph (as in the example below).
>>>>>>>>>>
>>>>>>>>>>I currently use the following code for the aov tables:
>>>>>>>>>><<results=tex>>=
>>>>>>>>>>tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
>>>>>>>>>>tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut
>>>>>, data=tmp)
>>>>>>>>>>tmpTable <- xtable (tmpAov ,
>>>>>>>>>>  caption="ANOVA table for vegetation height.",
>>>>>>>>>>  label="tab:AnovaHeight"
>>>>>>>>>>  )
>>>>>>>>>>print.xtable(tmpTable, type="latex", floating=TRUE,
>>>>>>>>>>  table.placement="ht", caption.placement="top",
>>>>>>>>>>  latex.environments=c("center"))
>>>>>>>>>>  )
>>>>>>>>>>@
>>>>>>>>>>
>>>>>>>>>>I used xtables, because it has a working aov example. I
>>>>>>>>would be happy
>>>>>>>>>>to use an alternative if I knew how! Would you have
>>>>>sample code to
>>>>>>>>>>illustrate how to export an aov table to Latex using
>>>>>latex{Hmisc}.
>>>>>>>>>>Thanks very much for your help,
>>>>>>>>>>
>>>>>>>>>>Sander.
>>>>>>>>>>
>>>>>>>>>>Frank E Harrell Jr wrote:
>>>>>>>>>>
>>>>>>>>>>>Sander Oom wrote:
>>>>>>>>>>>
>>>>>>>>>>>>Dear R users,
>>>>>>>>>>>>
>>>>>>>>>>>>The Sweave code below runs fine, as it is. However, an
>>>>>>>>error occurs
>>>>>>>>>>>>when the line 'library(xtable)' is uncommented:
>>>>>>>>>>>>Error:  chunk 1
>>>>>>>>>>>>Error in "label<-"(`*tmp*`, value = "month") :
>>>>>>>>>>>>      no applicable method for "label<-"
>>>>>>>>>>>>
>>>>>>>>>>>>Is anybody aware of this and knows a workaround?
>>>>>>>>>>>>
>>>>>>>>>>>>Thanks,
>>>>>>>>>>>>
>>>>>>>>>>>>Sander.
>>>>>>>>>>>>
>>>>>>>>>>>>*******************
>>>>>>>>>>>>
>>>>>>>>>>>>\documentclass[a4paper]{article}
>>>>>>>>>>>>\title{Sweave Test for summarize}
>>>>>>>>>>>>\author{Sander Oom}
>>>>>>>>>>>>
>>>>>>>>>>>>\usepackage{a4wide}
>>>>>>>>>>>>
>>>>>>>>>>>>\begin{document}
>>>>>>>>>>>>
>>>>>>>>>>>>\maketitle
>>>>>>>>>>>>
>>>>>>>>>>>>\begin{figure}[ht]
>>>>>>>>>>>>\begin{center}
>>>>>>>>>>>><<fig=TRUE,echo=FALSE>>=
>>>>>>>>>>>># library(xtable)
>>>>>>>>>>>>library(Hmisc)
>>>>>>>>>>>>set.seed(111)
>>>>>>>>>>>>dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
>>>>>>>>>>>>month <- dfr$month
>>>>>>>>>>>>year <- dfr$year
>>>>>>>>>>>>y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
>>>>>>>>>>>>s <- summarize(y, llist(month,year), smedian.hilow,
>>>>>>>>conf.int=.5)
>>>>>>>>>>>>print(xYplot(Cbind(y,Lower,Upper) ~ month,
>>>>>groups=year, data=s,
>>>>>>>>>>>>      keys='lines', method='alt', type='b'))
>>>>>>>>>>>>@
>>>>>>>>>>>>\end{center}
>>>>>>>>>>>>\end{figure}
>>>>>>>>>>>>
>>>>>>>>>>>>\end{document}
>>>>>>>>>>>>
>>>>>>>>>>>>************************
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>>version
>>>>>>>>>>>>       _
>>>>>>>>>>>>platform i686-pc-linux-gnu
>>>>>>>>>>>>arch     i686
>>>>>>>>>>>>os       linux-gnu
>>>>>>>>>>>>system   i686, linux-gnu
>>>>>>>>>>>>status
>>>>>>>>>>>>major    2
>>>>>>>>>>>>minor    1.0
>>>>>>>>>>>>year     2005
>>>>>>>>>>>>month    04
>>>>>>>>>>>>day      18
>>>>>>>>>>>>language R
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>I feel this is an xtable problem because Hmisc has being
>>>>>>>>using label
>>>>>>>>>>>and label<- since 1991.
>>>>>>>>>>>
>>>>>>>>>>>Frank
>>>>>>>>>>>
>>>>>>>>>There are ways to make functions from one area override those from
>>>>>>>>>another, but the real solution is to ask the xtable author
>>>>>>>>not to have
>>>>>>>>>functions that conflict with the (older) Hmisc package.  -Frank
>>>>>>>>>
>>>>>>>>--
>>>>>>>>--------------------------------------------
>>>>>>>>Dr Sander P. Oom
>>>>>>>>Animal, Plant and Environmental Sciences,
>>>>>>>>University of the Witwatersrand
>>>>>>>>Private Bag 3, Wits 2050, South Africa
>>>>>>>>Tel (work)      +27 (0)11 717 64 04
>>>>>>>>Tel (home)      +27 (0)18 297 44 51
>>>>>>>>Fax             +27 (0)18 299 24 64
>>>>>>>>Email   sander at oomvanlieshout.net
>>>>>>>>Web     www.oomvanlieshout.net/sander
>>>>>>>>
>>>>>>>>______________________________________________
>>>>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>PLEASE do read the posting guide!
>>>>>>>>http://www.R-project.org/posting-guide.html
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>______________________________________________
>>>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>PLEASE do read the posting guide!
>>>>>http://www.R-project.org/posting-guide.html
>>>>>--
>>>>>
>>>>>--------------------------------------------
>>>>>Dr Sander P. Oom
>>>>>Animal, Plant and Environmental Sciences,
>>>>>University of the Witwatersrand
>>>>>Private Bag 3, Wits 2050, South Africa
>>>>>Tel (work)      +27 (0)11 717 64 04
>>>>>Tel (home)      +27 (0)18 297 44 51
>>>>>Fax             +27 (0)18 299 24 64
>>>>>Email   sander at oomvanlieshout.net
>>>>>Web     www.oomvanlieshout.net/sander
>>>>>---------------------------------------------
>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>>------------------------------------------------------------------------------
>>>>Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
>>>>------------------------------------------------------------------------------
>>>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From ggrothendieck at gmail.com  Mon May 16 18:07:51 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 May 2005 12:07:51 -0400
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83F@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83F@usctmx1106.merck.com>
Message-ID: <971536df05051609077ff83399@mail.gmail.com>

That seems like a good solution.   

The one problem is that both packages would need their own
label generic functions if they are to operate independently
of each other so if they were both loaded the last one loaded
would give a warning that the label generic from the first loaded
package is being masked.  Of course, that does not really matter since
the two label generics would be identical but it would be slightly
annoying.    

One way around that would be to have one of the
packages depend on the other so that they share the label
generic although I am not sure that the two authors really would
want that.

Another way around that would be for the R core to include
the label generic so that both packages can leverage off the 
core but that would require some additional co-operation.

On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> One possibility of resolving the conflict, as I communicated with Frank, is
> to make label() and label<-() in Hmisc the S3 default methods, as xtable
> defines the methods for xtable, but provided no default methods.  This
> probably has the minimal impact (if any) on backward compatibility in both
> packages.
> 
> BTW, you should call print() rather than print.xtable().
> 
> Andy
> 
> > From: Sander Oom
> >
> > Hi David,
> >
> > Thanks for creating and supporting xtable. Glad I can contribute by
> > pointing out problems!
> >
> > An earlier response from Frank Harrell Jr. suggests that the
> > responsibility to resolve the conflict lies primarily with
> > you, as Hmisc
> > 'was there first'.
> >
> > Not sure how disputes over package conflicts are generally resolved!?
> >
> > I'll await a final decision!
> >
> > Thanks again,
> >
> > Sander.
> >
> >
> > David B. Dahl wrote:
> > > Sander,
> > >
> > > Thanks for pointing out the conflict between Hmisc and
> > xtable.  I am not
> > > sure I have a good solution.
> > >
> > > My understand of the namespace solution is that packages
> > can specify
> > > which variables to export for use by the package users.  The label
> > > function is not an internal function, rather one what is
> > intended for
> > > the user.
> > >
> > > Renaming the label() would resolve the conflict with the
> > Hmisc package,
> > > but make xtable not compatible with previous versions.
> > >
> > > As noted by Andy, label() in xtable is an S3 generic with an
> > > implementation label.xtable() specific to the xtable
> > package.  Perhaps
> > > Frank of Hmisc might be willing to make his follow the S3
> > generic naming
> > > convention?
> > >
> > > I am open to suggestions and, more especially, code.
> > Thanks for using
> > > xtable.
> > >
> > > -- David
> > >
> > >
> > > Liaw, Andy wrote:
> > >
> > >> You need to add the namespace to the source package, by adding a
> > >> NAMESPACE
> > >> file.  There's an R News article by Prof. Tierney on how
> > to do this.
> > >> Also
> > >> see the `Writing R Extensions' manual.  You should get the package
> > >> maintainer to do that, as that constitute a change in the
> > package source
> > >> code.
> > >>
> > >> Short of that, you should make sure that Hmisc is loaded
> > later than
> > >> xtable,
> > >> and use something like what Gabor suggested to access label() in
> > >> xtable.  (I
> > >> would use some other name, though: label() in xtable is
> > already an S3
> > >> generic).
> > >>
> > >> Andy
> > >>
> > >>
> > >>
> > >>> From: Sander Oom
> > >>> Hi Andy and Gabor,
> > >>>
> > >>> Thanks for your help so far! I am discovering another R dimension.
> > >>>
> > >>> Trying to put my head around all this....the conflict actually
> > >>> exposes itself when calling summarize(Hmisc).
> > Summarize(Hmisc) calls
> > >>> label internally, so I can not call it explicitly. Simply calling
> > >>> label(xtable) explicitly will not solve the problem with
> > >>> summarize(Hmisc).
> > >>>
> > >>> Thus, I should use namespaces as Andy is suggesting. Now
> > I just need
> > >>> to know how I 'add namespace' to a library? Does
> > 'loadNamespace' have
> > >>> something to do with it?
> > >>>
> > >>> Thanks very much for your help!
> > >>>
> > >>> Sander.
> > >>>
> > >>>
> > >>> ## From Venables and Ripley (2002) p.165.
> > >>> N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
> > >>> P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
> > >>> K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
> > >>> yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
> > >>>       62.8,55.8,69.5,55.0,
> > >>>       62.0,48.8,45.5,44.2,52.0,
> > >>>       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
> > >>> npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
> > >>>                   K=factor(K), yield=yield)
> > >>> ## to show the effects of re-ordering terms contrast the two fits
> > >>> tmpAov <- aov(yield ~ block + N * P + K, npk)
> > >>> tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
> > >>>   label="tab:Anova")
> > >>> print.xtable(tmpTable, type="latex", floating=TRUE,
> > >>>   table.placement="h", caption.placement="top",
> > >>>   latex.environments=c("center"))
> > >>>
> > >>> Alternatively, using namespace for xtable:
> > >>>
> > >>> tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
> > >>> xtable:::label(tmpTable) <- paste("tab:Anova")
> > >>> print.xtable(tmpTable, type="latex", floating=TRUE,
> > >>>   table.placement="ht", caption.placement="top",
> > >>>   latex.environments=c("center"))
> > >>>
> > >>>
> > >>>
> > >>> Gabor Grothendieck wrote:
> > >>>
> > >>>> Even without a namespace one could explicitly reference the label
> > >>>> in xtable via:
> > >>>>
> > >>>> xtable.label <- get("label", "package:xtable")
> > >>>>
> > >>>> On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > >>>>
> > >>>>> One possible solution without renaming the functions is
> > to
> > >>> add namespace to
> > >>>
> > >>>>> either xtable or Hmisc.  Given the size of Hmisc, it
> > >>> probably would be much
> > >>>
> > >>>>> easier to do that with xtable.
> > >>>>>
> > >>>>> With namespace in xtable, you can do xtable:::label() to
> > >>> refer to the
> > >>>
> > >>>>> label() in xtable specifically.
> > >>>>>
> > >>>>> Andy
> > >>>>>
> > >>>>>
> > >>>>>> From: Of Sander Oom
> > >>>>>>
> > >>>>>> Dear David,
> > >>>>>>
> > >>>>>> I would like to use summarize(Hmisc) and
> > print.xtable(xtable) in a
> > >>>>>> single Sweave document, but a conflict with the
> > 'label' function
> > >>>>>> prohibits this at the moment!
> > >>>>>>
> > >>>>>> Would you be able to correct the conflicting code? I
> > will
> > >>> gladly test
> > >>>
> > >>>>>> the new package!
> > >>>>>>
> > >>>>>> I have tried latex(Hmisc) to export the anova table, but
> > >>>>>> results are not
> > >>>>>> promising! I prefer xtable!!
> > >>>>>>
> > >>>>>> Thanks,
> > >>>>>>
> > >>>>>> Sander.
> > >>>>>>
> > >>>>>> Frank E Harrell Jr wrote:
> > >>>>>>
> > >>>>>>> Sander Oom wrote:
> > >>>>>>>
> > >>>>>>>> Dear Frank,
> > >>>>>>>>
> > >>>>>>>> I have a Sweave document in which I export anova (aov)
> > >>>>>>>>
> > >>>>>> tables to Latex
> > >>>>>>
> > >>>>>>>> and calculate some summary statistics with
> > summarize{Hmisc} for a
> > >>>>>>>> graph (as in the example below).
> > >>>>>>>>
> > >>>>>>>> I currently use the following code for the aov tables:
> > >>>>>>>> <<results=tex>>=
> > >>>>>>>> tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
> > >>>>>>>> tmpAov <-
> > aov(Height~Geology*Altitude*Origin*BinInOut
> > >>> , data=tmp)
> > >>>
> > >>>>>>>> tmpTable <- xtable (tmpAov ,
> > >>>>>>>>   caption="ANOVA table for vegetation height.",
> > >>>>>>>>   label="tab:AnovaHeight"
> > >>>>>>>>   )
> > >>>>>>>> print.xtable(tmpTable, type="latex", floating=TRUE,
> > >>>>>>>>   table.placement="ht", caption.placement="top",
> > >>>>>>>>   latex.environments=c("center"))
> > >>>>>>>>   )
> > >>>>>>>> @
> > >>>>>>>>
> > >>>>>>>> I used xtables, because it has a working aov example. I
> > >>>>>>>>
> > >>>>>> would be happy
> > >>>>>>
> > >>>>>>>> to use an alternative if I knew how! Would you have
> >
> > >>> sample code to
> > >>>
> > >>>>>>>> illustrate how to export an aov table to Latex using
> >
> > >>> latex{Hmisc}.
> > >>>
> > >>>>>>>> Thanks very much for your help,
> > >>>>>>>>
> > >>>>>>>> Sander.
> > >>>>>>>>
> > >>>>>>>> Frank E Harrell Jr wrote:
> > >>>>>>>>
> > >>>>>>>>
> > >>>>>>>>> Sander Oom wrote:
> > >>>>>>>>>
> > >>>>>>>>>
> > >>>>>>>>>> Dear R users,
> > >>>>>>>>>>
> > >>>>>>>>>> The Sweave code below runs fine, as it is. However, an
> > >>>>>>>>>>
> > >>>>>> error occurs
> > >>>>>>
> > >>>>>>>>>> when the line 'library(xtable)' is uncommented:
> > >>>>>>>>>> Error:  chunk 1
> > >>>>>>>>>> Error in "label<-"(`*tmp*`, value = "month") :
> > >>>>>>>>>>       no applicable method for "label<-"
> > >>>>>>>>>>
> > >>>>>>>>>> Is anybody aware of this and knows a workaround?
> > >>>>>>>>>>
> > >>>>>>>>>> Thanks,
> > >>>>>>>>>>
> > >>>>>>>>>> Sander.
> > >>>>>>>>>>
> > >>>>>>>>>> *******************
> > >>>>>>>>>>
> > >>>>>>>>>> \documentclass[a4paper]{article}
> > >>>>>>>>>> \title{Sweave Test for summarize}
> > >>>>>>>>>> \author{Sander Oom}
> > >>>>>>>>>>
> > >>>>>>>>>> \usepackage{a4wide}
> > >>>>>>>>>>
> > >>>>>>>>>> \begin{document}
> > >>>>>>>>>>
> > >>>>>>>>>> \maketitle
> > >>>>>>>>>>
> > >>>>>>>>>> \begin{figure}[ht]
> > >>>>>>>>>> \begin{center}
> > >>>>>>>>>> <<fig=TRUE,echo=FALSE>>=
> > >>>>>>>>>> # library(xtable)
> > >>>>>>>>>> library(Hmisc)
> > >>>>>>>>>> set.seed(111)
> > >>>>>>>>>> dfr <- expand.grid(month=1:12, year=c(1997,1998),
> > reps=1:100)
> > >>>>>>>>>> month <- dfr$month
> > >>>>>>>>>> year <- dfr$year
> > >>>>>>>>>> y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
> > >>>>>>>>>> s <- summarize(y, llist(month,year), smedian.hilow,
> > >>>>>>>>>>
> > >>>>>> conf.int=.5)
> > >>>>>>
> > >>>>>>>>>> print(xYplot(Cbind(y,Lower,Upper) ~ month,
> > >>> groups=year, data=s,
> > >>>
> > >>>>>>>>>>       keys='lines', method='alt', type='b'))
> > >>>>>>>>>> @
> > >>>>>>>>>> \end{center}
> > >>>>>>>>>> \end{figure}
> > >>>>>>>>>>
> > >>>>>>>>>> \end{document}
> > >>>>>>>>>>
> > >>>>>>>>>> ************************
> > >>>>>>>>>>
> > >>>>>>>>>>
> > >>>>>>>>>>
> > >>>>>>>>>>
> > >>>>>>>>>>> version
> > >>>>>>>>>>>
> > >>>>>>>>>>        _
> > >>>>>>>>>> platform i686-pc-linux-gnu
> > >>>>>>>>>> arch     i686
> > >>>>>>>>>> os       linux-gnu
> > >>>>>>>>>> system   i686, linux-gnu
> > >>>>>>>>>> status
> > >>>>>>>>>> major    2
> > >>>>>>>>>> minor    1.0
> > >>>>>>>>>> year     2005
> > >>>>>>>>>> month    04
> > >>>>>>>>>> day      18
> > >>>>>>>>>> language R
> > >>>>>>>>>>
> > >>>>>>>>>>
> > >>>>>>>>>>
> > >>>>>>>>> I feel this is an xtable problem because Hmisc has being
> > >>>>>>>>>
> > >>>>>> using label
> > >>>>>>
> > >>>>>>>>> and label<- since 1991.
> > >>>>>>>>>
> > >>>>>>>>> Frank
> > >>>>>>>>>
> > >>>>>>>>>
> > >>>>>>> There are ways to make functions from one area
> > override those from
> > >>>>>>> another, but the real solution is to ask the xtable author
> > >>>>>>>
> > >>>>>> not to have
> > >>>>>>
> > >>>>>>> functions that conflict with the (older) Hmisc
> > package.  -Frank
> > >>>>>>>
> > >>>>>>>
> > >>>>>> --
> > >>>>>> --------------------------------------------
> > >>>>>> Dr Sander P. Oom
> > >>>>>> Animal, Plant and Environmental Sciences,
> > >>>>>> University of the Witwatersrand
> > >>>>>> Private Bag 3, Wits 2050, South Africa
> > >>>>>> Tel (work)      +27 (0)11 717 64 04
> > >>>>>> Tel (home)      +27 (0)18 297 44 51
> > >>>>>> Fax             +27 (0)18 299 24 64
> > >>>>>> Email   sander at oomvanlieshout.net
> > >>>>>> Web     www.oomvanlieshout.net/sander
> > >>>>>>
> > >>>>>> ______________________________________________
> > >>>>>> R-help at stat.math.ethz.ch mailing list
> > >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>>> PLEASE do read the posting guide!
> > >>>>>> http://www.R-project.org/posting-guide.html
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>> ______________________________________________
> > >>>>> R-help at stat.math.ethz.ch mailing list
> > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>> PLEASE do read the posting guide!
> > >>> http://www.R-project.org/posting-guide.html
> > >>>
> > >>> --
> > >>>
> > >>> --------------------------------------------
> > >>> Dr Sander P. Oom
> > >>> Animal, Plant and Environmental Sciences,
> > >>> University of the Witwatersrand
> > >>> Private Bag 3, Wits 2050, South Africa
> > >>> Tel (work)      +27 (0)11 717 64 04
> > >>> Tel (home)      +27 (0)18 297 44 51
> > >>> Fax             +27 (0)18 299 24 64
> > >>> Email   sander at oomvanlieshout.net
> > >>> Web     www.oomvanlieshout.net/sander
> > >>> ---------------------------------------------
> > >>>
> > >>>
> > >>>
> > >>>
> > >>
> > >>
> > >>
> > >>
> > --------------------------------------------------------------
> > ----------------
> > >>
> > >> Notice:  This e-mail message, together with any
> > attachments, contains
> > >> information of Merck & Co., Inc. (One Merck Drive,
> > Whitehouse Station,
> > >> New Jersey, USA 08889), and/or its affiliates (which may be known
> > >> outside the United States as Merck Frosst, Merck Sharp &
> > Dohme or MSD
> > >> and in Japan, as Banyu) that may be confidential, proprietary
> > >> copyrighted and/or legally privileged. It is intended
> > solely for the
> > >> use of the individual or entity named on this message.  If
> > you are not
> > >> the intended recipient, and have received this message in error,
> > >> please notify us immediately by reply e-mail and then
> > delete it from
> > >> your system.
> > >>
> > --------------------------------------------------------------
> > ----------------
> > >>
> > >>
> > >>
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> > --
> > --------------------------------------------
> > Dr Sander P. Oom
> > Animal, Plant and Environmental Sciences,
> > University of the Witwatersrand
> > Private Bag 3, Wits 2050, South Africa
> > Tel (work)      +27 (0)11 717 64 04
> > Tel (home)      +27 (0)18 297 44 51
> > Fax             +27 (0)18 299 24 64
> > Email   sander at oomvanlieshout.net
> > Web     www.oomvanlieshout.net/sander
> > ---------------------------------------------
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From subianto at gmail.com  Mon May 16 18:39:50 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Mon, 16 May 2005 18:39:50 +0200
Subject: [R] How to convert color to black & white - Solve
In-Reply-To: <4288952A.3060306@statistik.uni-dortmund.de>
References: <7CBBD627E4E688499349A5D11D078316018899AD@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>
	<42886A22.5060108@gmail.com>
	<4288952A.3060306@statistik.uni-dortmund.de>
Message-ID: <4288CCD6.9090507@gmail.com>

Dear Uwe,
Thank for you advice.
Now, I know how to do with my color printer to print black and white.
James Holtman have advise me about that.
Then I use a simple color like this to use my picture.
print.bw4 <-c(blueF         = "#0000FF",
              redF          = "#FF0000",
              salmon.light  = "#dd9955",
              yellowF       = "#FFFF00")
barplot(1:4,col = print.bw4)

Best,
Muhammad Subianto

On this day 5/16/2005 2:42 PM, Uwe Ligges wrote:

> Muhammad Subianto wrote:
>
>> Thank's you very much.
>> But I need the plot with color not gray.
>
>
> So you want a colorful rgb plot, OK, fine with your code below.
>
> Now you want to print it black and white: This is now a question for 
> the folks who wrote your printer driver, but not for R-help.
>
> Uwe Ligges
>
>
>> Best wishes,
>> Muhammad Subianto
>>
>> On this day 5/14/2005 3:05 AM, OlsenN at pac.dfo-mpo.gc.ca wrote:
>>
>>> Muhammad,
>>> Here's one option:
>>>
>>> barplot(1:5,col=gray(seq(0,1,length=5)))
>>>
>>> Norm Olsen
>>> Fisheries and Oceans Canada
>>>
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> To: R-help at stat.math.ethz.ch
>>> Sent: 5/13/2005 11:40 AM
>>> Subject: [R] How to convert color to black & white
>>>
>>> Dear all,
>>> Could someone please explain to me how to convert color to black &
>>> white.
>>> For example:
>>> barplot(1:5,col = rainbow(5))
>>> Because I need to print my plot to save my ink color printer.
>>> I don't want to convert to grayscale, but keep it as an RGB.
>>> I  would be very happy if anyone could help me.
>>> Thank you very much in advance.
>>>
>>> Kindly regards,
>>> Muhammad Subianto
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>>  
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
>



From anubhavmanglick at gmail.com  Mon May 16 18:47:36 2005
From: anubhavmanglick at gmail.com (Anubhav Manglick)
Date: Mon, 16 May 2005 09:47:36 -0700
Subject: [R] sink stack memory??
Message-ID: <d812a006050516094746cd1604@mail.gmail.com>

Dear R-mailers

I want to store data in a seperate file named 'XYZ'
so I uses sink function for it
i runs the loop , say 100 times, and so every of the 100 times the
function is called the result must go to the file. But after soem time
of running it says
" 
Error in sink("C:/XYZ", append = TRUE) : 
        sink stack is full
"

but i have enough memory on the disk for this result

I would be highly obliged to know the problem/mistake I am amking

thanking you 

anubhav



From dimitrijoe at yahoo.com.br  Mon May 16 19:16:00 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Mon, 16 May 2005 14:16:00 -0300
Subject: [R] memory and step()
Message-ID: <002401c55a3a$efe0d760$ca00a8c0@thesahajamach>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050516/49ae8858/attachment.pl

From iacolonn at uiuc.edu  Mon May 16 19:28:33 2005
From: iacolonn at uiuc.edu (Ignacio Colonna)
Date: Mon, 16 May 2005 12:28:33 -0500
Subject: [R] Mental Block with PCA of multivariate time series!
In-Reply-To: <Pine.LNX.4.44.0505161027260.2392-100000@gw.env.leeds.ac.uk>
Message-ID: <200505161728.j4GHSZDi023529@expredir4.cites.uiuc.edu>

Is this along the lines of what you are trying to do?

sim.data<-data.frame(matrix(rnorm(350*10),350,10))
day<-seq(1:350)
sim.data<-data.frame(day,sim.data)
pc1.load.all<-NULL
pc2.load.all<-NULL
 for (i in seq(0,300,by=50)){
 sim.data.i<-subset(sim.data,sim.data$day>i&sim.data$day<(i+50))
pc1.load.i<-princomp(sim.data.i[,2:11])$loadings[,1]
pc2.load.i<-princomp(sim.data.i[,2:11])$loadings[,2]
pc1.load.all<-rbind(pc1.load.all,pc1.load.i)
pc2.load.all<-rbind(pc1.load.all,pc1.load.i)
 }

period<-seq(1:7)
pc1.load.all<-cbind(period,pc1.load.all)
pc2.load.all<-cbind(period,pc1.load.all)

# and plot loadings for each each variable vs. the period...

Ignacio


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laura Quinn
Sent: Monday, May 16, 2005 4:34 AM
To: Gavin Simpson
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Mental Block with PCA of multivariate time series!

Sorry, I don't think I made myself clear enough with my initial query!

I am wishing to investigate the temporal evolution of the pca: if we
assume that every 50 rows of my data frame is representitive of, for
instance, 1 day of data, I am hoping to automate a process whereby a pca
is performed on every 50 rows of data and the loading for PC1 and PC2 for
each variable (i.e. each column) is represented as a point on a plot - so
a years' data will be represented as two lines (representing PC1 and PC2)
on a time series plot for each variable.



Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

On Mon, 16 May 2005, Gavin Simpson wrote:

> Laura Quinn wrote:
> > Please could someone point me in the right direction as I appear to be
> > having a total mental block with fairly basic PCA problem!
> >
> > I have a large dataframe where rows represent independent
> > observations and columns are variables. I am wanting to perform PCA
> > sequentially on blocks of nrows at a time and produce a graphical output
> > of the loadings for the first 2 EOFs for each variable.
> >
> > I'm sure I've performed a very similar routine in the past, but the
method
> > is currently escaping me.
> >
> > Any help gratefully received!
>
> Hi Laura,
>
> data(iris)
> iris.dat <- iris[,1:4]
> pca.1 <- prcomp(iris.dat[1:50, ], scale = TRUE)
> pca.2 <- prcomp(iris.dat[51:100, ], scale = TRUE)
> pca.3 <- prcomp(iris.dat[100:150, ], scale = TRUE)
>
> biplot(pca.1)
> etc...
>
> There is a better way of subsetting this data set as the 5th col of iris
> is a factor and we could use the subset argument to prcomp to do the
> subsetting without having to know that there are 50 rows per species.
> Take a look at that argument if you have a variable that defines the
> blocks for you.
>
> Is this what you were after?
>
> All the best,
>
> Gav
> --
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> Gavin Simpson                     [T] +44 (0)20 7679 5522
> ENSIS Research Fellow             [F] +44 (0)20 7679 7565
> ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
> UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
> 26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
> London.  WC1H 0AP.
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Mon May 16 16:53:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 May 2005 10:53:09 -0400
Subject: [R] Conflict between xtable and Hmisc when using Sweave?
In-Reply-To: <4288AD5A.8040701@oomvanlieshout.net>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E83E@usctmx1106.merck.com>
	<4288AD5A.8040701@oomvanlieshout.net>
Message-ID: <971536df050516075315d4f6ba@mail.gmail.com>

Using label as an lvalue (i.e. on the left hand side of the assignment)
causes it to refer to a different function, not label itself.  

Any any rate, looking at your example,
it seems that you don't actually need to use Hmisc and xtable at the same time
so just make sure that whichever you want at a particular point
in your code is the only one of the two loaded:

library(Hmisc)
...code that involves Hmisc but not xtable...
detach("package:Hmisc")
library(xtable)
...code that involves xtable but not Hmisc...

On 5/16/05, Sander Oom <sander at oomvanlieshout.net> wrote:
> I tried to follow your suggestions, but without success:
> 
> Error: couldn't find function "xtable.mylabel<-"
> 
> ... resulting from the code below.
> 
> Any suggestions?
> 
> Thanks,
> 
> Sander.
> 
> library(xtable)
> xtable.mylabel <- get("label", "package:xtable")
> library(Hmisc) # provides summarize
> 
> set.seed(1)
> temperature <- rnorm(300, 70, 10)
> month <- sample(1:12, 300, TRUE)
> year  <- sample(2000:2001, 300, TRUE)
> g <- function(x)c(Mean=mean(x,na.rm=TRUE),Median=median(x,na.rm=TRUE))
> summarize(temperature, month, g)
> 
> ## From Venables and Ripley (2002) p.165.
> N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
> P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
> K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
> yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
>       62.8,55.8,69.5,55.0,
>       62.0,48.8,45.5,44.2,52.0,
>       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
> npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
>                   K=factor(K), yield=yield)
> ## to show the effects of re-ordering terms contrast the two fits
> tmpAov <- aov(yield ~ block + N * P + K, npk)
> tmpTable <- xtable (tmpAov ,
>   caption="ANOVA table for vegetation height.")
> xtable.mylabel(tmpTable) <- paste("tab:AnovaHeight")
> print.xtable(tmpTable, type="latex", floating=TRUE,
>   table.placement="h", caption.placement="top",
>   latex.environments=c("center"),
>   title=first.word(deparse(substitute(object))),
>   append=FALSE
>   )
> 
> 
> Liaw, Andy wrote:
> > You need to add the namespace to the source package, by adding a NAMESPACE
> > file.  There's an R News article by Prof. Tierney on how to do this.  Also
> > see the `Writing R Extensions' manual.  You should get the package
> > maintainer to do that, as that constitute a change in the package source
> > code.
> >
> > Short of that, you should make sure that Hmisc is loaded later than xtable,
> > and use something like what Gabor suggested to access label() in xtable.  (I
> > would use some other name, though: label() in xtable is already an S3
> > generic).
> >
> > Andy
> >
> >>From: Sander Oom
> >>
> >>Hi Andy and Gabor,
> >>
> >>Thanks for your help so far! I am discovering another R dimension.
> >>
> >>Trying to put my head around all this....the conflict
> >>actually exposes
> >>itself when calling summarize(Hmisc). Summarize(Hmisc) calls label
> >>internally, so I can not call it explicitly. Simply calling
> >>label(xtable) explicitly will not solve the problem with
> >>summarize(Hmisc).
> >>
> >>Thus, I should use namespaces as Andy is suggesting. Now I
> >>just need to
> >>know how I 'add namespace' to a library? Does 'loadNamespace' have
> >>something to do with it?
> >>
> >>Thanks very much for your help!
> >>
> >>Sander.
> >>
> >>
> >>## From Venables and Ripley (2002) p.165.
> >>N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
> >>P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
> >>K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
> >>yield <-c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,
> >>       62.8,55.8,69.5,55.0,
> >>       62.0,48.8,45.5,44.2,52.0,
> >>       51.5,49.8,48.8,57.2,59.0,53.2,56.0)
> >>npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
> >>                   K=factor(K), yield=yield)
> >>## to show the effects of re-ordering terms contrast the two fits
> >>tmpAov <- aov(yield ~ block + N * P + K, npk)
> >>tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.",
> >>   label="tab:Anova")
> >>print.xtable(tmpTable, type="latex", floating=TRUE,
> >>   table.placement="h", caption.placement="top",
> >>   latex.environments=c("center"))
> >>
> >>Alternatively, using namespace for xtable:
> >>
> >>tmpTable <- xtable(tmpAov , caption="Test export of ANOVA table.")
> >>xtable:::label(tmpTable) <- paste("tab:Anova")
> >>print.xtable(tmpTable, type="latex", floating=TRUE,
> >>   table.placement="ht", caption.placement="top",
> >>   latex.environments=c("center"))
> >>
> >>
> >>
> >>Gabor Grothendieck wrote:
> >>>Even without a namespace one could explicitly reference the label
> >>>in xtable via:
> >>>
> >>>xtable.label <- get("label", "package:xtable")
> >>>
> >>>On 5/16/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> >>>>One possible solution without renaming the functions is to
> >>add namespace to
> >>>>either xtable or Hmisc.  Given the size of Hmisc, it
> >>probably would be much
> >>>>easier to do that with xtable.
> >>>>
> >>>>With namespace in xtable, you can do xtable:::label() to
> >>refer to the
> >>>>label() in xtable specifically.
> >>>>
> >>>>Andy
> >>>>
> >>>>>From: Of Sander Oom
> >>>>>
> >>>>>Dear David,
> >>>>>
> >>>>>I would like to use summarize(Hmisc) and print.xtable(xtable) in a
> >>>>>single Sweave document, but a conflict with the 'label' function
> >>>>>prohibits this at the moment!
> >>>>>
> >>>>>Would you be able to correct the conflicting code? I will
> >>gladly test
> >>>>>the new package!
> >>>>>
> >>>>>I have tried latex(Hmisc) to export the anova table, but
> >>>>>results are not
> >>>>>promising! I prefer xtable!!
> >>>>>
> >>>>>Thanks,
> >>>>>
> >>>>>Sander.
> >>>>>
> >>>>>Frank E Harrell Jr wrote:
> >>>>>>Sander Oom wrote:
> >>>>>>>Dear Frank,
> >>>>>>>
> >>>>>>>I have a Sweave document in which I export anova (aov)
> >>>>>tables to Latex
> >>>>>>>and calculate some summary statistics with summarize{Hmisc} for a
> >>>>>>>graph (as in the example below).
> >>>>>>>
> >>>>>>>I currently use the following code for the aov tables:
> >>>>>>><<results=tex>>=
> >>>>>>> tmp <- datGrassHC[datGrassHC$Loc > 0 & datGrassHC$Loc < 9 ,]
> >>>>>>> tmpAov <- aov(Height~Geology*Altitude*Origin*BinInOut
> >>, data=tmp)
> >>>>>>> tmpTable <- xtable (tmpAov ,
> >>>>>>>   caption="ANOVA table for vegetation height.",
> >>>>>>>   label="tab:AnovaHeight"
> >>>>>>>   )
> >>>>>>> print.xtable(tmpTable, type="latex", floating=TRUE,
> >>>>>>>   table.placement="ht", caption.placement="top",
> >>>>>>>   latex.environments=c("center"))
> >>>>>>>   )
> >>>>>>>@
> >>>>>>>
> >>>>>>>I used xtables, because it has a working aov example. I
> >>>>>would be happy
> >>>>>>>to use an alternative if I knew how! Would you have
> >>sample code to
> >>>>>>>illustrate how to export an aov table to Latex using
> >>latex{Hmisc}.
> >>>>>>>Thanks very much for your help,
> >>>>>>>
> >>>>>>>Sander.
> >>>>>>>
> >>>>>>>Frank E Harrell Jr wrote:
> >>>>>>>
> >>>>>>>>Sander Oom wrote:
> >>>>>>>>
> >>>>>>>>>Dear R users,
> >>>>>>>>>
> >>>>>>>>>The Sweave code below runs fine, as it is. However, an
> >>>>>error occurs
> >>>>>>>>>when the line 'library(xtable)' is uncommented:
> >>>>>>>>>Error:  chunk 1
> >>>>>>>>>Error in "label<-"(`*tmp*`, value = "month") :
> >>>>>>>>>       no applicable method for "label<-"
> >>>>>>>>>
> >>>>>>>>>Is anybody aware of this and knows a workaround?
> >>>>>>>>>
> >>>>>>>>>Thanks,
> >>>>>>>>>
> >>>>>>>>>Sander.
> >>>>>>>>>
> >>>>>>>>>*******************
> >>>>>>>>>
> >>>>>>>>>\documentclass[a4paper]{article}
> >>>>>>>>>\title{Sweave Test for summarize}
> >>>>>>>>>\author{Sander Oom}
> >>>>>>>>>
> >>>>>>>>>\usepackage{a4wide}
> >>>>>>>>>
> >>>>>>>>>\begin{document}
> >>>>>>>>>
> >>>>>>>>>\maketitle
> >>>>>>>>>
> >>>>>>>>>\begin{figure}[ht]
> >>>>>>>>>\begin{center}
> >>>>>>>>><<fig=TRUE,echo=FALSE>>=
> >>>>>>>>> # library(xtable)
> >>>>>>>>> library(Hmisc)
> >>>>>>>>> set.seed(111)
> >>>>>>>>> dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
> >>>>>>>>> month <- dfr$month
> >>>>>>>>> year <- dfr$year
> >>>>>>>>> y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
> >>>>>>>>> s <- summarize(y, llist(month,year), smedian.hilow,
> >>>>>conf.int=.5)
> >>>>>>>>> print(xYplot(Cbind(y,Lower,Upper) ~ month,
> >>groups=year, data=s,
> >>>>>>>>>       keys='lines', method='alt', type='b'))
> >>>>>>>>>@
> >>>>>>>>>\end{center}
> >>>>>>>>>\end{figure}
> >>>>>>>>>
> >>>>>>>>>\end{document}
> >>>>>>>>>
> >>>>>>>>>************************
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>>>version
> >>>>>>>>>        _
> >>>>>>>>>platform i686-pc-linux-gnu
> >>>>>>>>>arch     i686
> >>>>>>>>>os       linux-gnu
> >>>>>>>>>system   i686, linux-gnu
> >>>>>>>>>status
> >>>>>>>>>major    2
> >>>>>>>>>minor    1.0
> >>>>>>>>>year     2005
> >>>>>>>>>month    04
> >>>>>>>>>day      18
> >>>>>>>>>language R
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>I feel this is an xtable problem because Hmisc has being
> >>>>>using label
> >>>>>>>>and label<- since 1991.
> >>>>>>>>
> >>>>>>>>Frank
> >>>>>>>>
> >>>>>>There are ways to make functions from one area override those from
> >>>>>>another, but the real solution is to ask the xtable author
> >>>>>not to have
> >>>>>>functions that conflict with the (older) Hmisc package.  -Frank
> >>>>>>
> >>>>>--
> >>>>>--------------------------------------------
> >>>>>Dr Sander P. Oom
> >>>>>Animal, Plant and Environmental Sciences,
> >>>>>University of the Witwatersrand
> >>>>>Private Bag 3, Wits 2050, South Africa
> >>>>>Tel (work)      +27 (0)11 717 64 04
> >>>>>Tel (home)      +27 (0)18 297 44 51
> >>>>>Fax             +27 (0)18 299 24 64
> >>>>>Email   sander at oomvanlieshout.net
> >>>>>Web     www.oomvanlieshout.net/sander
> >>>>>
> >>>>>______________________________________________
> >>>>>R-help at stat.math.ethz.ch mailing list
> >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>PLEASE do read the posting guide!
> >>>>>http://www.R-project.org/posting-guide.html
> >>>>>
> >>>>>
> >>>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>--
> >>
> >>--------------------------------------------
> >>Dr Sander P. Oom
> >>Animal, Plant and Environmental Sciences,
> >>University of the Witwatersrand
> >>Private Bag 3, Wits 2050, South Africa
> >>Tel (work)      +27 (0)11 717 64 04
> >>Tel (home)      +27 (0)18 297 44 51
> >>Fax             +27 (0)18 299 24 64
> >>Email   sander at oomvanlieshout.net
> >>Web     www.oomvanlieshout.net/sander
> >>---------------------------------------------
> >>
> >>
> >>
> >
> >
> >
> > ------------------------------------------------------------------------------
> > Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
> > ------------------------------------------------------------------------------
> >
>



From ligges at statistik.uni-dortmund.de  Mon May 16 19:41:34 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 16 May 2005 19:41:34 +0200
Subject: [R] sink stack memory??
In-Reply-To: <d812a006050516094746cd1604@mail.gmail.com>
References: <d812a006050516094746cd1604@mail.gmail.com>
Message-ID: <4288DB4E.7080903@statistik.uni-dortmund.de>

Anubhav Manglick wrote:

> Dear R-mailers
> 
> I want to store data in a seperate file named 'XYZ'
> so I uses sink function for it
> i runs the loop , say 100 times, and so every of the 100 times the
> function is called the result must go to the file. But after soem time
> of running it says
> " 
> Error in sink("C:/XYZ", append = TRUE) : 
>         sink stack is full
> "
> 
> but i have enough memory on the disk for this result
> 
> I would be highly obliged to know the problem/mistake I am amking


I guess you forgot to close the connection?
See the code (in connections.c) and the help page. The latter tells us
"The stack is of up to 21 connections (20 diversions)."

If you did not forget to close the connection, please send an easily 
reproducible example.

Uwe Ligges




> thanking you 
> 
> anubhav
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon May 16 19:48:33 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 16 May 2005 19:48:33 +0200
Subject: [R] memory and step()
In-Reply-To: <002401c55a3a$efe0d760$ca00a8c0@thesahajamach>
References: <002401c55a3a$efe0d760$ca00a8c0@thesahajamach>
Message-ID: <4288DCF1.5080509@statistik.uni-dortmund.de>

Dimitri Joe wrote:

> Hi there,
> 
> I'm trying to perform a step(), using variables from a data set with 32.000 observations. The upper model is not so long  - (x1 + x2 + x3)^2 - where x1...x3 are the explanatory variables. Yet, I got a memory problem when performing it. The message is the following:
> 
> Error: cannot allocate vector of size 26859 Kb
> In addition: Warning messages:
> 1: Reached total allocation of 246Mb: see help(memory.size) 
> 2: Reached total allocation of 246Mb: see help(memory.size) 
> 
> I tried to see "help(memory.size)", but that was not too enlightening. As I am not familiar with these memory problems, I am asking for your help. What should I do so that I can perform this (and many others) step( )? I am under Windows XP, using a P4 2.00 GHz with 256 Mb of RAM.

memory.limit(512) will give you 512 Mb, if your system has enough swap 
space, BUT it will slow down dramatically!

See also the more detailed help page ?Memory.

My recommendation is: Buy a 512 Mb module for a few $, 256Mb is really 
not much these days.
Hmm, my 4.5 year old laptop already has 256Mb (cannot be increased - and 
this is the only point why I am looking fo a new one).

Uwe Ligges


> Thank you.
> Dimitri
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From graek at lycos.com  Mon May 16 19:49:32 2005
From: graek at lycos.com (graek just graek)
Date: Mon, 16 May 2005 09:49:32 -0800
Subject: [R] Solutions? "Intro Stats with R" - Dalgaard
Message-ID: <20050516174932.46217CA06F@ws7-4.us4.outblaze.com>

Hi,

I'm going through the book "Introductory Statistics with R" by Dalgaard to teach myself R.  At the end of each chapter there are some exercises.  Does anyone know of any place on the net where someone has posted the answers to these?  

I've looked at the author's and the publisher's site with no luck.  And no, this is not cheating for a class, I graduated several years ago...this is for personal enrichment...  ;-)  I'm just stuck on a few of these.

Thanks,

Greg




-- 
_______________________________________________
NEW! Lycos Dating Search. The only place to search multiple dating sites at once.
http://datingsearch.lycos.com



From A.J.Revilla at lse.ac.uk  Mon May 16 20:00:32 2005
From: A.J.Revilla at lse.ac.uk (Revilla,AJ  (pgt))
Date: Mon, 16 May 2005 19:00:32 +0100
Subject: [R] Mixed-effects model with lagged endogenous variables and
	autocorrelated residuals
Message-ID: <1742DDEFF8D82541A2BD9591D900A5BE03BC8B19@exs2.backup>

Dear all,

I am dealing with a nonlinear model of the form yt = A*exp(-B*T)*Yt-1, where T represents time and Yt-1 accounts for the accumulated values of y from T=0 to t-1.
The problem of the models is that the error terms are autocorrelated, so I have to deal with a model combining autocorrelated residuals and lagged endogenous variables.
One common approach to this specific model is the two-step Hatanaka estimator. If I am right, it works as follows: first, the equation is fitted in order to get an estimate of the autocorrelation term, which is then included in the second step (the transformation is shown in Hatanaka, 1974), the new equation being estimated by OLS. An additional problem is that the OLS estimate of the autocorrelation term is not consistent in the presence of lagged endogenous variables, so the use of instrumental variables is needed for the first step.
So far, so good (more or less). The question is that I am interested in fitting a mixed-effects model (I would like to obtain subject-specific coefficients, and my panel of data is too short to fit separate regressions), so that the coefficients will be estimated either by ML or REML. 
Do you have any clue of how to approach this? Is the rho estimate obtained from the nlme package consistent? Could I substitute it directly in the second step? Are there better options? Any suggestions will be MUCH appreciated.

Thank you very much,

Antonio



From kleong at aed.com.ve  Mon May 16 20:41:28 2005
From: kleong at aed.com.ve (kleong@anew.com.ve)
Date: Mon, 16 May 2005 14:41:28 -0400
Subject: [R] =?utf-8?q?Nuevo_buz=C3=B3n_de_correo-_New_mailbox?=
Message-ID: <0MKuxu-1DXkWu05nh-0002hj@mx.perfora.net>

Estimado:

Por medio de la presente notifico que hemos cambiado el dominio de Anew
e-Business Distribution por el siguiente: anew.com.ve

Mucho agradezco tomar en cuenta que ahora mi nueva direcci??n de correo ser??  kleong at anew.com.ve

Mil gracias

Saludos
Kam Fai Leon


Hello, 

I notify you that we have changed the domain of Anew e-Business Distribution by the following: anew.com.ve

Please, considered that my new mailbox is  kleong at anew.com.ve

Thank you.

Best Regards,
Kam Fai Leon



From p.dalgaard at biostat.ku.dk  Mon May 16 20:43:31 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 May 2005 20:43:31 +0200
Subject: [R] Solutions? "Intro Stats with R" - Dalgaard
In-Reply-To: <20050516174932.46217CA06F@ws7-4.us4.outblaze.com>
References: <20050516174932.46217CA06F@ws7-4.us4.outblaze.com>
Message-ID: <x2zmuvc8q4.fsf@turmalin.kubism.ku.dk>

"graek just graek" <graek at lycos.com> writes:

> Hi,
> 
> I'm going through the book "Introductory Statistics with R" by
> Dalgaard to teach myself R. At the end of each chapter there are
> some exercises. Does anyone know of any place on the net where
> someone has posted the answers to these?
> 
> I've looked at the author's and the publisher's site with no luck.
> And no, this is not cheating for a class, I graduated several years
> ago...this is for personal enrichment... ;-) I'm just stuck on a few
> of these.

If I ever find the stamina, the intention is that the 2nd edition will
have Answers to Exercises, so I have gone through them recently.

I've put a first(!!) draft on 

http://biostat.ku.dk/~pd/ISwR-answers.pdf

All sorts of reservations apply, including the possibility that the
actual text of the book might need changes to make the answers
intelligible. Feedback is welcome.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From LI at nsabp.pitt.edu  Mon May 16 21:20:15 2005
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Mon, 16 May 2005 15:20:15 -0400
Subject: [R] A question about bugs.R: functions for running WinBUGs from R 
Message-ID: <3D0B2434377E984E9C85CAA316F8B183018A3995@nsabpmail>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050516/910bc825/attachment.pl

From helprhelp at gmail.com  Mon May 16 21:27:16 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 16 May 2005 14:27:16 -0500
Subject: [R] predict problem
Message-ID: <cdf8178305051612271f743662@mail.gmail.com>

Hi, there:
I have a question on predict() function.
I built a set of trees using rpart and when I do prediction, I got the
following error:
Error in model.frame.default(Terms, newdata, na.action = act, xlev = attr(object
,  :
        factor V19 has new level(s) 45
Execution halted

I think the problem is caused by a new level (which is not used in
building trees) for V19. I am wondering if there is an argument in
predict() which can help suppress this error by just ignoring this
special observation in validation sample. If not, I can do some
pre-treatment to remove the obs with new level out for the time being.

Thanks,
-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From N.L.Pace at m.cc.utah.edu  Mon May 16 22:28:20 2005
From: N.L.Pace at m.cc.utah.edu (Nathan Leon Pace, MD, MStat)
Date: Mon, 16 May 2005 14:28:20 -0600
Subject: [R] isotonic regression
Message-ID: <21d30a8e6f3b6c6110e29068abd74da0@utah.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050516/7978b866/attachment.pl

From miner at u.washington.edu  Mon May 16 23:07:06 2005
From: miner at u.washington.edu (Brooks Miner)
Date: Mon, 16 May 2005 14:07:06 -0700
Subject: [R] multinom(): likelihood of model?
In-Reply-To: <Pine.LNX.4.61.0505140637210.25094@gannet.stats>
References: <22461bafa53c7852793779dffa838391@u.washington.edu>
	<Pine.LNX.4.61.0505140637210.25094@gannet.stats>
Message-ID: <e0e4b87d4d7e5e7461dccd738a6a440c@u.washington.edu>

Professor Ripley,

Thanks very much.  You comments were very helpful.  I've got it  
(almost) all figured out now.  My model is for discrete data (discrete  
response AND predictors).  The issue I'm stuck on is that my saturated  
model does NOT predict exactly; the constant is ~1874.052.

Here is an example of my model with much less data, showing how the  
saturated model does not predict exactly (constant ~ 440.6375).


library(nnet)
fem.labs<- 
factor(c("Fem1","Fem2","Fem3","Fem4","Fem5","Fem6"),levels=c("Fem1","Fem 
2","Fem3","Fem4","Fem5","Fem6"))
site.labs<-factor(c("Site01","Site02","Site03","Site04"))
dyad.labs<-factor(c("M","H","U"),levels=c("M","H","U"))
data.table<-expand.grid(Site=site.labs, Female=fem.labs,Dyad=dyad.labs)
data<- 
c(20,16,21,16,13,11,15,13,27,27,30,19,24,25,29,23,24,20,25,25,24,18,26,2 
0,2,0,2,2,1,1,0,1,3,3,3,6,4,3,2,4,2,1,3,1,3,4,1,1,5,11,4,9,1,3,0,1,3,3,0 
,8,4,4,1,5,6,11,4,6,3,8,3,9)
data.table<-structure(.Data=data.table[rep(1: 
nrow(data.table),data),],row.names=1:length(data))
fit.saturated.model<-multinom(Dyad~Female*Site,data=data.table)
fit.saturated.model$deviance # Why is this NONZERO?


Am I setting up the regression for the saturated model incorrectly??

Thanks in advance. . .

- Brooks

----------------------------
Brooks Miner
Research Scientist
Laird Lab
UW Biology
206.616.9385
http://protist.biology.washington.edu/Lairdlab/

On May 13, 2005, at 10:41 PM, Prof Brian Ripley wrote:

> By definition, the deviance is minus twice the maximized  
> log-likelihood plus a const.  In any of these models for discrete  
> data, the saturated model predicts exactly, so the const is zero.
>
> There are worked examples in MASS4, the book multinom() supports.
>
> On Fri, 13 May 2005, Brooks Miner wrote:
>
>> Hi all,
>>
>> I'm working on a multinomial (or "polytomous") logistic regression  
>> using R and have made great progress using multinom() from the nnet  
>> library.  My response variable has three categories, and there are  
>> two different possible predictors.  I'd like to use the likelihoods  
>> of certain models (ie, saturated, fitteds, and null) to calculate  
>> Nagelkerke R-squared values for various fitted models.
>>
>> My question today is simple: once I have fitted a model using  
>> multinom(), how do I find the likelihood (or log likelihood) of my  
>> fitted model?  I understand that this value must be part of the  
>> $deviance or $AIC components of the fitted model, but my  
>> understanding is too limited at this point for me to know how to  
>> calculate the likelihood of my fitted model from either of these  
>> outputs.
>>
>> Thanks in advance to any assistance offered.  I'd be happy to provide  
>> an example of my data and multinom() entries if that would help.
>>
>> Gratefully,
>>
>> - Brooks
>> ----------------------------
>> Brooks Miner
>> Research Scientist
>> Laird Lab
>> UW Biology
>> 206.616.9385
>> http://protist.biology.washington.edu/Lairdlab/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!  
>> http://www.R-project.org/posting-guide.html
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Tue May 17 00:09:14 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 16 May 2005 23:09:14 +0100
Subject: [R] parsing speed
In-Reply-To: <1116240414.23816.70.camel@localhost.localdomain>
References: <1116240414.23816.70.camel@localhost.localdomain>
Message-ID: <42891A0A.6090507@stats.uwo.ca>

Federico Calboli wrote:
> Hi everyone,
> 
> I have a question on parsing speed.
> 
> I have two functions:
> 
> F1
> F2
> 
> As things are now, F2 calls F1 internally:
> 
> F2 =  function(x){
> if (something == 1){
> y = F1(x)
> }
> if (something ==2){
> do whatever
> }
> }
> 
> *Assuming there could be some difference*, is is faster to use the code
> as written above or should I actually write the statements of F1 to make
> the parsing faster? 

The parsing only happens once when you define the functions, and is 
(almost always) a negligible part of total execution time.  I think 
you're really worried about execution time.  You'll probably get more 
execution time with a separate function because function calls take time.

However, my guess is that putting F1 inline won't make enough difference 
to notice.

Duncan



From gunter.berton at gene.com  Tue May 17 00:20:01 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 16 May 2005 15:20:01 -0700
Subject: [R] parsing speed
In-Reply-To: <42891A0A.6090507@stats.uwo.ca>
Message-ID: <200505162220.j4GMK10T018360@meitner.gene.com>

(just my additional $.02)

... and as a general rule (subject to numerous exceptions, caveats, etc.)

1) it is programming and debugging time that most impacts "overall" program
execution time;
2) this is most strongly impacted by code readability and size (the smaller
the better);
3) both of which are enhanced by modular construction and reuseability,
which argues for avoiding inline code and using separate functions.

These days, i would argue that most of the time it is program clarity and
correctness (they are related) that is the important issue, not execution
speed.

... again, subject to exceptions and caveats, etc.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
> Sent: Monday, May 16, 2005 3:09 PM
> To: f.calboli at imperial.ac.uk
> Cc: r-help
> Subject: Re: [R] parsing speed
> 
> Federico Calboli wrote:
> > Hi everyone,
> > 
> > I have a question on parsing speed.
> > 
> > I have two functions:
> > 
> > F1
> > F2
> > 
> > As things are now, F2 calls F1 internally:
> > 
> > F2 =  function(x){
> > if (something == 1){
> > y = F1(x)
> > }
> > if (something ==2){
> > do whatever
> > }
> > }
> > 
> > *Assuming there could be some difference*, is is faster to 
> use the code
> > as written above or should I actually write the statements 
> of F1 to make
> > the parsing faster? 
> 
> The parsing only happens once when you define the functions, and is 
> (almost always) a negligible part of total execution time.  I think 
> you're really worried about execution time.  You'll probably get more 
> execution time with a separate function because function 
> calls take time.
> 
> However, my guess is that putting F1 inline won't make enough 
> difference 
> to notice.
> 
> Duncan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Scott.Waichler at pnl.gov  Tue May 17 00:19:27 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Mon, 16 May 2005 15:19:27 -0700
Subject: [R] Omitting NAs in aggregate.ts()
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01F39B54@pnlmse35.pnl.gov>


I have a time series vector (not necessarily ts class) that has NAs in
it.
How can I omit the NAs when using aggregate.ts() to compute a function
on each window?  If there is at least one non-NA value in each window,
I'd like to proceed with evaluating the function; otherwise, I would
like NA returned.  I'm not wedded to aggregate.ts and don't need ts
class if there is another fast, vectorized way to handle this.  Here is
what I am trying to do, with the invalid na.rm thrown in:

as.vector(aggregate.ts(x, ndeltat=24, FUN=min, na.rm=F))

Thanks,
Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From jyzz88 at gmail.com  Tue May 17 02:50:32 2005
From: jyzz88 at gmail.com (Luke)
Date: Mon, 16 May 2005 20:50:32 -0400
Subject: [R] how to use list index to get vector
Message-ID: <27583b4005051617502c0545b0@mail.gmail.com>

I have a simple question, but I couldn't find the answer in R manuals.

Assume I have a list:
> foo <- list()
> foo[[1]] <- c(1, 2, 3)
> foo[[2]] <- c(11,22,33)
> foo[[3]] <- c(111,222,333)
> foo
[[1]]
[1] 1 2 3

[[2]]
[1] 11 22 33

[[3]]
[1] 111 222 333

How to use list index to get a vector of, say, the first elements of
list elements?
That is, how to get a vector c(1, 11, 111) from foo?

foo[[]][1] doesn't work.

-Luke



From ggrothendieck at gmail.com  Tue May 17 02:56:31 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 May 2005 20:56:31 -0400
Subject: [R] how to use list index to get vector
In-Reply-To: <27583b4005051617502c0545b0@mail.gmail.com>
References: <27583b4005051617502c0545b0@mail.gmail.com>
Message-ID: <971536df050516175661b13aca@mail.gmail.com>

On 5/16/05, Luke <jyzz88 at gmail.com> wrote:
> I have a simple question, but I couldn't find the answer in R manuals.
> 
> Assume I have a list:
> > foo <- list()
> > foo[[1]] <- c(1, 2, 3)
> > foo[[2]] <- c(11,22,33)
> > foo[[3]] <- c(111,222,333)
> > foo
> [[1]]
> [1] 1 2 3
> 
> [[2]]
> [1] 11 22 33
> 
> [[3]]
> [1] 111 222 333
> 
> How to use list index to get a vector of, say, the first elements of
> list elements?
> That is, how to get a vector c(1, 11, 111) from foo?
> 
> foo[[]][1] doesn't work.
> 

Try this:

sapply(foo, "[", 1)



From jyzz88 at gmail.com  Tue May 17 03:23:37 2005
From: jyzz88 at gmail.com (Luke)
Date: Mon, 16 May 2005 21:23:37 -0400
Subject: [R] how to use list index to get vector
In-Reply-To: <971536df050516175661b13aca@mail.gmail.com>
References: <27583b4005051617502c0545b0@mail.gmail.com>
	<971536df050516175661b13aca@mail.gmail.com>
Message-ID: <27583b4005051618237e331fae@mail.gmail.com>

Yes, it works. Althought I can understand the help page of sapply,  I
don't know why it works.  What is "["?

-Luke

On 5/16/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 5/16/05, Luke <jyzz88 at gmail.com> wrote:
> > I have a simple question, but I couldn't find the answer in R manuals.
> >
> > Assume I have a list:
> > > foo <- list()
> > > foo[[1]] <- c(1, 2, 3)
> > > foo[[2]] <- c(11,22,33)
> > > foo[[3]] <- c(111,222,333)
> > > foo
> > [[1]]
> > [1] 1 2 3
> >
> > [[2]]
> > [1] 11 22 33
> >
> > [[3]]
> > [1] 111 222 333
> >
> > How to use list index to get a vector of, say, the first elements of
> > list elements?
> > That is, how to get a vector c(1, 11, 111) from foo?
> >
> > foo[[]][1] doesn't work.
> >
> 
> Try this:
> 
> sapply(foo, "[", 1)
>



From ggrothendieck at gmail.com  Tue May 17 03:28:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 May 2005 21:28:01 -0400
Subject: [R] how to use list index to get vector
In-Reply-To: <27583b4005051618237e331fae@mail.gmail.com>
References: <27583b4005051617502c0545b0@mail.gmail.com>
	<971536df050516175661b13aca@mail.gmail.com>
	<27583b4005051618237e331fae@mail.gmail.com>
Message-ID: <971536df05051618284021b9c6@mail.gmail.com>

Its the indexing function written in ordinary function form.  That is, 
foo[1:2] can be written as "["(foo, 1:2)

On 5/16/05, Luke <jyzz88 at gmail.com> wrote:
> Yes, it works. Althought I can understand the help page of sapply,  I
> don't know why it works.  What is "["?
> 
> -Luke
> 
> On 5/16/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 5/16/05, Luke <jyzz88 at gmail.com> wrote:
> > > I have a simple question, but I couldn't find the answer in R manuals.
> > >
> > > Assume I have a list:
> > > > foo <- list()
> > > > foo[[1]] <- c(1, 2, 3)
> > > > foo[[2]] <- c(11,22,33)
> > > > foo[[3]] <- c(111,222,333)
> > > > foo
> > > [[1]]
> > > [1] 1 2 3
> > >
> > > [[2]]
> > > [1] 11 22 33
> > >
> > > [[3]]
> > > [1] 111 222 333
> > >
> > > How to use list index to get a vector of, say, the first elements of
> > > list elements?
> > > That is, how to get a vector c(1, 11, 111) from foo?
> > >
> > > foo[[]][1] doesn't work.
> > >
> >
> > Try this:
> >
> > sapply(foo, "[", 1)
> >
>



From jyzz88 at gmail.com  Tue May 17 03:42:39 2005
From: jyzz88 at gmail.com (Luke)
Date: Mon, 16 May 2005 21:42:39 -0400
Subject: [R] how to use list index to get vector
In-Reply-To: <971536df05051618284021b9c6@mail.gmail.com>
References: <27583b4005051617502c0545b0@mail.gmail.com>
	<971536df050516175661b13aca@mail.gmail.com>
	<27583b4005051618237e331fae@mail.gmail.com>
	<971536df05051618284021b9c6@mail.gmail.com>
Message-ID: <27583b4005051618423c686311@mail.gmail.com>

Hi Gabor, thanks a lot. I understand it now. But Andrew's method is
easier for me to understand. Can I extend my question? I have a data
file, every line has such format:
2:102 5:85 ...
The number before colon is data entry index, the number after colon is
data entry value, and other data entries are zero. I use scan to read
them in R and need to separate the index and value to different
vectors. For example:

> foo
[1] "2:102" "5:85"
> foo2 <- strsplit(foo, split=":")
> foo2
[[1]]
[1] "2"   "102"

[[2]]
[1] "5"  "85"

> myIndex <- as.numeric(unlist(lapply(foo2, function(x) x[1])))
or
> myIndex <- as.numeric(foo2, "[", 1)
> myIndex
[1] "2" "5"

Is this the simplest way to get the index or value vector?

-Luke

On 5/16/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Its the indexing function written in ordinary function form.  That is,
> foo[1:2] can be written as "["(foo, 1:2)
> 
> On 5/16/05, Luke <jyzz88 at gmail.com> wrote:
> > Yes, it works. Althought I can understand the help page of sapply,  I
> > don't know why it works.  What is "["?
> >
> > -Luke
> >
> > On 5/16/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > On 5/16/05, Luke <jyzz88 at gmail.com> wrote:
> > > > I have a simple question, but I couldn't find the answer in R manuals.
> > > >
> > > > Assume I have a list:
> > > > > foo <- list()
> > > > > foo[[1]] <- c(1, 2, 3)
> > > > > foo[[2]] <- c(11,22,33)
> > > > > foo[[3]] <- c(111,222,333)
> > > > > foo
> > > > [[1]]
> > > > [1] 1 2 3
> > > >
> > > > [[2]]
> > > > [1] 11 22 33
> > > >
> > > > [[3]]
> > > > [1] 111 222 333
> > > >
> > > > How to use list index to get a vector of, say, the first elements of
> > > > list elements?
> > > > That is, how to get a vector c(1, 11, 111) from foo?
> > > >
> > > > foo[[]][1] doesn't work.
> > > >
> > >
> > > Try this:
> > >
> > > sapply(foo, "[", 1)
> > >
> >
>



From andy_liaw at merck.com  Tue May 17 03:55:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 16 May 2005 21:55:00 -0400
Subject: [R] how to use list index to get vector
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E843@usctmx1106.merck.com>

If you have the data in files in such format, with one entry per line, you
can do something like:

dat <- matrix(scan("fileWithData", sep=":"), ncol=2, byrow=TRUE)

Then the first column of dat would be the indices, and the second column
would be the values.

Andy

> From: Luke
> 
> Hi Gabor, thanks a lot. I understand it now. But Andrew's method is
> easier for me to understand. Can I extend my question? I have a data
> file, every line has such format:
> 2:102 5:85 ...
> The number before colon is data entry index, the number after colon is
> data entry value, and other data entries are zero. I use scan to read
> them in R and need to separate the index and value to different
> vectors. For example:
> 
> > foo
> [1] "2:102" "5:85"
> > foo2 <- strsplit(foo, split=":")
> > foo2
> [[1]]
> [1] "2"   "102"
> 
> [[2]]
> [1] "5"  "85"
> 
> > myIndex <- as.numeric(unlist(lapply(foo2, function(x) x[1])))
> or
> > myIndex <- as.numeric(foo2, "[", 1)
> > myIndex
> [1] "2" "5"
> 
> Is this the simplest way to get the index or value vector?
> 
> -Luke
> 
> On 5/16/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Its the indexing function written in ordinary function 
> form.  That is,
> > foo[1:2] can be written as "["(foo, 1:2)
> > 
> > On 5/16/05, Luke <jyzz88 at gmail.com> wrote:
> > > Yes, it works. Althought I can understand the help page 
> of sapply,  I
> > > don't know why it works.  What is "["?
> > >
> > > -Luke
> > >
> > > On 5/16/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > On 5/16/05, Luke <jyzz88 at gmail.com> wrote:
> > > > > I have a simple question, but I couldn't find the 
> answer in R manuals.
> > > > >
> > > > > Assume I have a list:
> > > > > > foo <- list()
> > > > > > foo[[1]] <- c(1, 2, 3)
> > > > > > foo[[2]] <- c(11,22,33)
> > > > > > foo[[3]] <- c(111,222,333)
> > > > > > foo
> > > > > [[1]]
> > > > > [1] 1 2 3
> > > > >
> > > > > [[2]]
> > > > > [1] 11 22 33
> > > > >
> > > > > [[3]]
> > > > > [1] 111 222 333
> > > > >
> > > > > How to use list index to get a vector of, say, the 
> first elements of
> > > > > list elements?
> > > > > That is, how to get a vector c(1, 11, 111) from foo?
> > > > >
> > > > > foo[[]][1] doesn't work.
> > > > >
> > > >
> > > > Try this:
> > > >
> > > > sapply(foo, "[", 1)
> > > >
> > >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From LI at nsabp.pitt.edu  Tue May 17 04:41:52 2005
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Mon, 16 May 2005 22:41:52 -0400
Subject: [R] The error in R while using bugs.R function
Message-ID: <3D0B2434377E984E9C85CAA316F8B183357C8F@nsabpmail>

Dear R users, 
 
I followed the instuctions on Dr. Gelman's web to install all
of documents that bugs.R needs, but when I try to run the school example that the web posted in R, I got an error: couldn't find function "bugs", what's wrong?
 
Thanks,
 
Jia



From jerosenb at fas.harvard.edu  Tue May 17 04:59:06 2005
From: jerosenb at fas.harvard.edu (Janet Elise Rosenbaum)
Date: Mon, 16 May 2005 22:59:06 -0400 (EDT)
Subject: [R] install.packages parameters
Message-ID: <20050517025906.D618F1C005@ls01.fas.harvard.edu>


Hello.

R is having some trouble installing a package because it passed arguments to 
gcc which were non-existent directories and files.  It also didn't find 
g77, although it's in a directory in my $PATH;  I tricked it by making a 
sym link in /usr/bin.

What file does R get these parameters from?  
I've looked for the parameters in the package source, the install.packages 
help pages, and the R preferences menu, all to no avail.

I am running R 2.1.0 on Mac 10.3.8, and three days ago I installed a
different package from source where installation involved gcc without
any problems, and nothing has changed since then.  The packages I'm
trying to install are Joe Schafer's mix, norm, and cat.

Thanks,

Janet



From corr at fas.harvard.edu  Tue May 17 07:37:57 2005
From: corr at fas.harvard.edu (Anders Schwartz Corr)
Date: Tue, 17 May 2005 01:37:57 -0400 (EDT)
Subject: [R] NA erase your data trick
Message-ID: <Pine.LNX.4.58.0505170134070.5648@ls02.fas.harvard.edu>


Oops,

I just erased all my data using this gizmo that I thought would replace -9
with NA.

A) Can I get my tcn5 back?

B) How do I do it right next time, I learned my lesson, I'll never do it
again, I promise!

Anders Corr

> for(i in 1:dim(tcn5)[2]){         ##for the number of columns
+     for(n in 1:dim(tcn5)[1]){     ##for the number of rows
+         tcn5[is.na(tcn5[n,i]) | tcn5[n,i] == -9] <- NA
+
+         }
+ }
>



From ripley at stats.ox.ac.uk  Tue May 17 07:42:29 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 May 2005 06:42:29 +0100 (BST)
Subject: [R] multinom(): likelihood of model?
In-Reply-To: <e0e4b87d4d7e5e7461dccd738a6a440c@u.washington.edu>
References: <22461bafa53c7852793779dffa838391@u.washington.edu>
	<Pine.LNX.4.61.0505140637210.25094@gannet.stats>
	<e0e4b87d4d7e5e7461dccd738a6a440c@u.washington.edu>
Message-ID: <Pine.LNX.4.61.0505170641410.4276@gannet.stats>

On Mon, 16 May 2005, Brooks Miner wrote:

> Professor Ripley,
>
> Thanks very much.  You comments were very helpful.  I've got it (almost) all 
> figured out now.  My model is for discrete data (discrete response AND 
> predictors).  The issue I'm stuck on is that my saturated model does NOT 
> predict exactly; the constant is ~1874.052.
>
> Here is an example of my model with much less data, showing how the saturated 
> model does not predict exactly (constant ~ 440.6375).

Because you have an aggregated model: counts, not individual observations.

>
>
> library(nnet)
> fem.labs<- 
> factor(c("Fem1","Fem2","Fem3","Fem4","Fem5","Fem6"),levels=c("Fem1","Fem 
> 2","Fem3","Fem4","Fem5","Fem6"))
> site.labs<-factor(c("Site01","Site02","Site03","Site04"))
> dyad.labs<-factor(c("M","H","U"),levels=c("M","H","U"))
> data.table<-expand.grid(Site=site.labs, Female=fem.labs,Dyad=dyad.labs)
> data<- 
> c(20,16,21,16,13,11,15,13,27,27,30,19,24,25,29,23,24,20,25,25,24,18,26,2 
> 0,2,0,2,2,1,1,0,1,3,3,3,6,4,3,2,4,2,1,3,1,3,4,1,1,5,11,4,9,1,3,0,1,3,3,0 
> ,8,4,4,1,5,6,11,4,6,3,8,3,9)
> data.table<-structure(.Data=data.table[rep(1: 
> nrow(data.table),data),],row.names=1:length(data))
> fit.saturated.model<-multinom(Dyad~Female*Site,data=data.table)
> fit.saturated.model$deviance # Why is this NONZERO?
>
>
> Am I setting up the regression for the saturated model incorrectly??
>
> Thanks in advance. . .
>
> - Brooks
>
> ----------------------------
> Brooks Miner
> Research Scientist
> Laird Lab
> UW Biology
> 206.616.9385
> http://protist.biology.washington.edu/Lairdlab/
>
> On May 13, 2005, at 10:41 PM, Prof Brian Ripley wrote:
>
>> By definition, the deviance is minus twice the maximized log-likelihood 
>> plus a const.  In any of these models for discrete data, the saturated 
>> model predicts exactly, so the const is zero.
>> 
>> There are worked examples in MASS4, the book multinom() supports.
>> 
>> On Fri, 13 May 2005, Brooks Miner wrote:
>> 
>>> Hi all,
>>> 
>>> I'm working on a multinomial (or "polytomous") logistic regression using R 
>>> and have made great progress using multinom() from the nnet library.  My 
>>> response variable has three categories, and there are two different 
>>> possible predictors.  I'd like to use the likelihoods of certain models 
>>> (ie, saturated, fitteds, and null) to calculate Nagelkerke R-squared 
>>> values for various fitted models.
>>> 
>>> My question today is simple: once I have fitted a model using multinom(), 
>>> how do I find the likelihood (or log likelihood) of my fitted model?  I 
>>> understand that this value must be part of the $deviance or $AIC 
>>> components of the fitted model, but my understanding is too limited at 
>>> this point for me to know how to calculate the likelihood of my fitted 
>>> model from either of these outputs.
>>> 
>>> Thanks in advance to any assistance offered.  I'd be happy to provide an 
>>> example of my data and multinom() entries if that would help.
>>> 
>>> Gratefully,
>>> 
>>> - Brooks
>>> ----------------------------
>>> Brooks Miner
>>> Research Scientist
>>> Laird Lab
>>> UW Biology
>>> 206.616.9385
>>> http://protist.biology.washington.edu/Lairdlab/
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>> 
>> 
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue May 17 07:51:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 May 2005 06:51:15 +0100 (BST)
Subject: [R] The error in R while using bugs.R function
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B183357C8F@nsabpmail>
References: <3D0B2434377E984E9C85CAA316F8B183357C8F@nsabpmail>
Message-ID: <Pine.LNX.4.61.0505170648340.4276@gannet.stats>

On Mon, 16 May 2005, Li, Jia wrote:

> I followed the instuctions on Dr. Gelman's web to install all of 
> documents that bugs.R needs, but when I try to run the school example 
> that the web posted in R, I got an error: couldn't find function "bugs", 
> what's wrong?

I suggest you ask Dr Gelman.  There is a function "bugs" in package
R2WinBUGS: perhaps you don't have that installed or that package loaded?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From res90sx5 at verizon.net  Tue May 17 08:11:38 2005
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Mon, 16 May 2005 23:11:38 -0700
Subject: [R] NA erase your data trick
In-Reply-To: <Pine.LNX.4.58.0505170134070.5648@ls02.fas.harvard.edu>
Message-ID: <0IGM00JE2EJFU4K2@vms042.mailsrvcs.net>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of Anders Schwartz Corr
> Sent: Monday, May 16, 2005 10:38 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] NA erase your data trick
> 
> 
> Oops,
> 
> I just erased all my data using this gizmo that I thought would replace -9
> with NA.
> 
> A) Can I get my tcn5 back?

I don't think there is any going back.

> 
> B) How do I do it right next time, I learned my lesson, I'll never do it
> again, I promise!
> 

How about something like

x[x == -9] <- NA

Dan Nordlund
Bothell, WA

> Anders Corr
> 
> > for(i in 1:dim(tcn5)[2]){         ##for the number of columns
> +     for(n in 1:dim(tcn5)[1]){     ##for the number of rows
> +         tcn5[is.na(tcn5[n,i]) | tcn5[n,i] == -9] <- NA
> +
> +         }
> + }
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Tue May 17 08:28:19 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 17 May 2005 07:28:19 +0100
Subject: [R] The error in R while using bugs.R function
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B183357C8F@nsabpmail>
References: <3D0B2434377E984E9C85CAA316F8B183357C8F@nsabpmail>
Message-ID: <42898F03.5040403@stats.uwo.ca>

Li, Jia wrote:
> Dear R users, 
>  
> I followed the instuctions on Dr. Gelman's web to install all
> of documents that bugs.R needs, but when I try to run the school example that the web posted in R, I got an error: couldn't find function "bugs", what's wrong?

It sounds as though you missed an instruction, or he did.  I'm guessing 
you didn't run library() to load the package.

Generally when a contributed package doesn't work, you should ask the 
maintainer for help.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Tue May 17 08:33:00 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 17 May 2005 08:33:00 +0200
Subject: [R] NA erase your data trick
In-Reply-To: <Pine.LNX.4.58.0505170134070.5648@ls02.fas.harvard.edu>
References: <Pine.LNX.4.58.0505170134070.5648@ls02.fas.harvard.edu>
Message-ID: <4289901C.60608@statistik.uni-dortmund.de>

Anders Schwartz Corr wrote:
> Oops,
> 
> I just erased all my data using this gizmo that I thought would replace -9
> with NA.
> 
> A) Can I get my tcn5 back?

As you got it the first time. There is nothing like "undo".


> B) How do I do it right next time, I learned my lesson, I'll never do it
> again, I promise!

By vectorization:

    tcn5[tcn5 == -9] <- NA

Uwe Ligges



> Anders Corr
> 
> 
>>for(i in 1:dim(tcn5)[2]){         ##for the number of columns
> 
> +     for(n in 1:dim(tcn5)[1]){     ##for the number of rows
> +         tcn5[is.na(tcn5[n,i]) | tcn5[n,i] == -9] <- NA
> +
> +         }
> + }
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Tue May 17 08:35:12 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 17 May 2005 07:35:12 +0100
Subject: [R] NA erase your data trick
In-Reply-To: <Pine.LNX.4.58.0505170134070.5648@ls02.fas.harvard.edu>
References: <Pine.LNX.4.58.0505170134070.5648@ls02.fas.harvard.edu>
Message-ID: <428990A0.5050501@stats.uwo.ca>

Anders Schwartz Corr wrote:
> Oops,
> 
> I just erased all my data using this gizmo that I thought would replace -9
> with NA.
> 
> A) Can I get my tcn5 back?

Not if you don't have it backed up somewhere else.

I wouldn't recommend keeping your only copy of anything in an R 
workspace.  It's too easy to accidentally delete or overwrite it.  Keep 
the original in a file.

> 
> B) How do I do it right next time, I learned my lesson, I'll never do it
> again, I promise!
> 
> Anders Corr
> 
> 
>>for(i in 1:dim(tcn5)[2]){         ##for the number of columns
> 
> +     for(n in 1:dim(tcn5)[1]){     ##for the number of rows
> +         tcn5[is.na(tcn5[n,i]) | tcn5[n,i] == -9] <- NA

For some values of i and n, this last line simplifies to

tcn5[TRUE] <- NA

which is why you lost your data.

You want to (a) think in vectors, or (b) use an if statement:

(a) Replace your whole series of statements with

tcn5[is.na(tcn5) | tcn5 == -9] <- NA

or

(b) Replace just the last line above with

   if (is.na(tcn5[n,i]) | tcn5[n,i] == -9) tcn5[n,i] <- NA

I'd choose (a); it's a lot cleaner and will run faster.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Tue May 17 08:37:57 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 17 May 2005 08:37:57 +0200
Subject: [R] The error in R while using bugs.R function
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B183357C8F@nsabpmail>
References: <3D0B2434377E984E9C85CAA316F8B183357C8F@nsabpmail>
Message-ID: <42899145.9070103@statistik.uni-dortmund.de>

Li, Jia wrote:

> Dear R users, 
>  
> I followed the instuctions on Dr. Gelman's web to install all
> of documents that bugs.R needs, but when I try to run the school example that the web posted in R, I got an error: couldn't find function "bugs", what's wrong?

Have you forgot to source() Andrew's bugs.R file?

Anyway, you might want to give the CRAN package R2WinBUGS a try, which 
is based on Andrew's code known as "bugs.R".
There is also a developer version BRugs (an interface to OpenBUGS) 
available at http://www.statistik.uni-dortmund.de/~ligges/BRugs/ (will 
move to CRAN very soon now).

Uwe Ligges







>  
> Thanks,
>  
> Jia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Tue May 17 08:43:47 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 17 May 2005 08:43:47 +0200
Subject: [R] NA erase your data trick
In-Reply-To: <Pine.LNX.4.58.0505170134070.5648@ls02.fas.harvard.edu>
Message-ID: <4289AEC3.26104.416584@localhost>

Hi

Maybe

tcn5[tcn5 == -9] <- NA

if tcn5 is matrix


> mat<-matrix(rnorm(100),10,10)
> mat[5,6:7]<- -9
> mat[mat == -9]<-NA

Read some intro on data manipulation, it helps you to avoid 
thinking in loops

Cheers
Petr

On 17 May 2005 at 1:37, Anders Schwartz Corr wrote:

> 
> Oops,
> 
> I just erased all my data using this gizmo that I thought would
> replace -9 with NA.
> 
> A) Can I get my tcn5 back?
> 
> B) How do I do it right next time, I learned my lesson, I'll never do
> it again, I promise!
> 
> Anders Corr
> 
> > for(i in 1:dim(tcn5)[2]){         ##for the number of columns
> +     for(n in 1:dim(tcn5)[1]){     ##for the number of rows
> +         tcn5[is.na(tcn5[n,i]) | tcn5[n,i] == -9] <- NA
> +
> +         }
> + }
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ligges at statistik.uni-dortmund.de  Tue May 17 08:46:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 17 May 2005 08:46:49 +0200
Subject: [R] A question about bugs.R: functions for running WinBUGs from R
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B183018A3995@nsabpmail>
References: <3D0B2434377E984E9C85CAA316F8B183018A3995@nsabpmail>
Message-ID: <42899359.3070002@statistik.uni-dortmund.de>

Li, Jia wrote:

> Dear R users,
>  
> I've found bugs.R : the functions for running WinBUGs from R that is
> writen by Dr. Andrew Gelman who is a professor from Columbia University.
> The bugs.R would be very useful for me,  and I think many of you know it
> as well. I followed the instuctions on Dr. Gelman's web to install all
> of documents that bugs.R needs, but when I try to run the school example
> the web posted in R, I got an error. 
>  
> Would you please help me out? I am stuck on this for a while and really
> frustrated now.
>  
> The program and the error as follow.
>  
> Thanks a lot in advance!

Why do you post twice?
See my former message, you forgot to source() "bugs.R".

BTW: Since this code is from the website from Andrew Gelman, why do you 
think posting to R-help is the appropriate way to get help? R has a 
standardized mechanism for distributing code - called "package".
And the appropriate package "R2WinBUGS" you are looking for has been 
made available - thanks to Andrew's effort in writing the original code 
and thanks to the efforts of two others to generalize the code and 
package it for you.

Uwe Ligges



> Jiaa
> ________________________________________________________________________
> _
> 
>># R code for entering the data and fitting the Bugs model for 8
> 
> schools
> 
>># analysis from Section 5.5 of "Bayesian Data Analysis".
>>
>># To run, the Bugs model must be in the file "schools.txt" in your
> 
> working
> 
>># directory and you must load in the functions in the bugs.R file (see
>># http://www.stat.columbia.edu/~gelman/bugsR/).
>>
>>J <- 8
>>y <- c(28,8,-3,7,-1,1,18,12)
>>sigma.y <- c(15,10,16,11,9,11,10,18)
>>schools.data <- list ("J", "y", "sigma.y")
>>schools.inits <- function()
> 
> +   list (theta=rnorm(J,0,1), mu.theta=rnorm(1,0,100),
> +         sigma.theta=runif(1,0,100))
> 
>>schools.parameters <- c("theta", "mu.theta", "sigma.theta")
>>
>>#run in winbugs14
>>
>>schools.sim <- bugs (schools.data, schools.inits, schools.parameters,
> 
> "schools.bug", n.chains=3, n.iter=1000, version=1.4)
> Error: couldn't find function "bugs"
> ------------------------------------------------------------------------
> ------------------------------------------------------------------------
> --------------------------------------- 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From detlef.steuer at hsu-hh.de  Tue May 17 09:18:13 2005
From: detlef.steuer at hsu-hh.de (Detlef Steuer)
Date: Tue, 17 May 2005 09:18:13 +0200
Subject: [R] NA erase your data trick
In-Reply-To: <4289901C.60608@statistik.uni-dortmund.de>
References: <Pine.LNX.4.58.0505170134070.5648@ls02.fas.harvard.edu>
	<4289901C.60608@statistik.uni-dortmund.de>
Message-ID: <20050517091813.0520b712.detlef.steuer@hsu-hamburg.de>

On Tue, 17 May 2005 08:33:00 +0200
Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:

> Anders Schwartz Corr wrote:
> > Oops,
> > 
> > I just erased all my data using this gizmo that I thought would replace -9
> > with NA.
> > 
> > A) Can I get my tcn5 back?
> 
> As you got it the first time. There is nothing like "undo".

If you??re lucky it still lives inside an old, not overwritten when leaving the depressing R session .RData.

Detlef

> 
> 
> > B) How do I do it right next time, I learned my lesson, I'll never do it
> > again, I promise!
> 
> By vectorization:
> 
>     tcn5[tcn5 == -9] <- NA
> 
> Uwe Ligges
> 
> 
> 
> > Anders Corr
> > 
> > 
> >>for(i in 1:dim(tcn5)[2]){         ##for the number of columns
> > 
> > +     for(n in 1:dim(tcn5)[1]){     ##for the number of rows
> > +         tcn5[is.na(tcn5[n,i]) | tcn5[n,i] == -9] <- NA
> > +
> > +         }
> > + }
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue May 17 09:46:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 May 2005 08:46:19 +0100 (BST)
Subject: [R] NA erase your data trick
In-Reply-To: <4289901C.60608@statistik.uni-dortmund.de>
References: <Pine.LNX.4.58.0505170134070.5648@ls02.fas.harvard.edu>
	<4289901C.60608@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0505170834280.5878@gannet.stats>

On Tue, 17 May 2005, Uwe Ligges wrote:

> Anders Schwartz Corr wrote:
>> Oops,
>> 
>> I just erased all my data using this gizmo that I thought would replace -9
>> with NA.
>> 
>> A) Can I get my tcn5 back?
>
> As you got it the first time. There is nothing like "undo".
>
>
>> B) How do I do it right next time, I learned my lesson, I'll never do it
>> again, I promise!
>
> By vectorization:
>
>   tcn5[tcn5 == -9] <- NA

That will work if tcn5 contains NAs, but only because NA indices on the 
lhs are now ignored for matrices (if tcn5 is a matrix, which seems 
unstated) -- this used not to be the case.  I would prefer

     tcn5[tcn %in% -9] <- NA

Using %in% rather than == in computed indices is a good habit to acquire: 
it also makes things like

     tcn5[tcn %in% c(-9, -99)] <- NA

work as expected.

If tcn is a data frame, you have to do this column-by-column, as in

tcn5[] <- lapply(tcn5, function(x) x[x %in% -9] <- NA)

or by a logical index matrix, which is harder to construct.


>>> for(i in 1:dim(tcn5)[2]){         ##for the number of columns
>> 
>> +     for(n in 1:dim(tcn5)[1]){     ##for the number of rows
>> +         tcn5[is.na(tcn5[n,i]) | tcn5[n,i] == -9] <- NA
>> +
>> +         }
>> + }

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Tue May 17 09:50:20 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 17 May 2005 09:50:20 +0200
Subject: [R] parsing speed
In-Reply-To: <200505162220.j4GMK10T018360@meitner.gene.com>
References: <42891A0A.6090507@stats.uwo.ca>
	<200505162220.j4GMK10T018360@meitner.gene.com>
Message-ID: <17033.41532.761605.39534@stat.math.ethz.ch>

>>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
>>>>>     on Mon, 16 May 2005 15:20:01 -0700 writes:

    BertG> (just my additional $.02) ... and as a general rule
    BertG> (subject to numerous exceptions, caveats, etc.)

    BertG> 1) it is programming and debugging time that most
    BertG> impacts "overall" program execution time; 2) this is
    BertG> most strongly impacted by code readability and size
    BertG> (the smaller the better); 3) both of which are
    BertG> enhanced by modular construction and reuseability,
    BertG> which argues for avoiding inline code and using
    BertG> separate functions.

    BertG> These days, i would argue that most of the time it is
    BertG> program clarity and correctness (they are related)
    BertG> that is the important issue, not execution speed.

    BertG> ... again, subject to exceptions and caveats, etc.

Yes indeed; very good points very well put!

Just to say it again: 

 **** We strongly recommend not to "inline" your code, but rather
 **** program modularly, i.e. call small `utility' functions.

If execution time ever becomes crucial for your problem
(not often), the chances are considerable that the time spent is
 not there [[ but you have to measure! -  use Rprof() ! ]]
and if it *was* there, then you have your bottleneck in one
simple function that you could start optimizing... even a good
reason for not inlining that code..

Martin Maechler, ETH Zurich

    BertG> -- Bert Gunter Genentech Non-Clinical Statistics
    BertG> South San Francisco, CA
 
    BertG> "The business of the statistician is to catalyze the
    BertG> scientific learning process."  - George E. P. Box
 
 

    >> -----Original Message----- From:
    >> r-help-bounces at stat.math.ethz.ch
    >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
    >> Duncan Murdoch Sent: Monday, May 16, 2005 3:09 PM To:
    >> f.calboli at imperial.ac.uk Cc: r-help Subject: Re: [R]
    >> parsing speed
    >> 
    >> Federico Calboli wrote: > Hi everyone,
    >> > 
    >> > I have a question on parsing speed.
    >> > 
    >> > I have two functions:
    >> > 
    >> > F1 > F2
    >> > 
    >> > As things are now, F2 calls F1 internally:
    >> > 
    >> > F2 = function(x){ > if (something == 1){ > y = F1(x) >
    >> } > if (something ==2){ > do whatever > } > }
    >> > 
    >> > *Assuming there could be some difference*, is is faster
    >> to use the code > as written above or should I actually
    >> write the statements of F1 to make > the parsing faster?
    >> 
    >> The parsing only happens once when you define the
    >> functions, and is (almost always) a negligible part of
    >> total execution time.  I think you're really worried
    >> about execution time.  You'll probably get more execution
    >> time with a separate function because function calls take
    >> time.
    >> 
    >> However, my guess is that putting F1 inline won't make
    >> enough difference to notice.
    >> 
    >> Duncan



From spayet.reesfrance at wanadoo.fr  Tue May 17 10:24:29 2005
From: spayet.reesfrance at wanadoo.fr (=?iso-8859-1?Q?St=E9phanie_PAYET?=)
Date: Tue, 17 May 2005 10:24:29 +0200
Subject: [R] Vuong test
Message-ID: <MEEKJDIBHBIGPENAMOIBGEIOCAAA.spayet.reesfrance@wanadoo.fr>

Hi,

I have two questions. First, I'd like to compare a ZINB model to a negativ
binomial model with the Vuong test, but I can't find how to performe it from
the zicount package. Does a programm exist to do it ?
Second, I'd like to know in which cases we have to use a double hurdle model
instead of a zero inflated model.

Many thanks,

St??phanie Payet

REES France
R??seau d'Evaluation en Economie de la Sant??
28, rue d'Assas
75006 PARIS
T??l. +33 (0)1 44 39 16 90
Fax +33 (0)1 44 39 16 92
M??l. reesfrance at wanadoo.fr
Site Internet : http://www.rees-france.com



From Matthias.Templ at statistik.gv.at  Tue May 17 10:31:07 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Tue, 17 May 2005 10:31:07 +0200
Subject: [R] cluster results using fanny
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAA70@xchg1.statistik.local>

> Barbara Diaz wrote:
> > Hi,
> > 
> > I am using fanny and I have estrange results. I am wondering if 
> > someone out there can help me understand why this happens.
> > 
> > First of all in most of my tries, it gives me a result in 
> which each 
> > object has equal membership in all clusters. I have read that that 
> > means "the clustering is entirely fuzzy". Looking at the 
> graphics it 
> > is really difficult to understand how objects with so 
> different scores 
> > for the variables have the same membership for all the clusters.

Hi Barbara,

I think, there is a problem with fanny, when you have standardised data.
For example:
library(mvoutlier)
library(cluster)
data(chorizon)
a <- fanny(chorizon[,101:110],4)
b <- fanny(scale(chorizon[,101:110]),4)
a$mem # is ok, but
b$mem # have same memberships

Better to use function cmeans in package e1071, which gives correct
memberships!

Best,
Matthias



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From srjafarzadeh at gmail.com  Tue May 17 10:59:14 2005
From: srjafarzadeh at gmail.com (Seyed Reza Jafarzadeh)
Date: Tue, 17 May 2005 01:59:14 -0700
Subject: [R] Vuong test
In-Reply-To: <MEEKJDIBHBIGPENAMOIBGEIOCAAA.spayet.reesfrance@wanadoo.fr>
References: <MEEKJDIBHBIGPENAMOIBGEIOCAAA.spayet.reesfrance@wanadoo.fr>
Message-ID: <83217d00505170159626848e5@mail.gmail.com>

Hi St??phanie,

The Vuong test can be done in Stata
(http://www.stata.com/support/faqs/stat/vuong.html), but I am also
looking for its code in R. In addition to "zicounts", Dr. Simon
Jackman (http://pscl.stanford.edu/) has provided the code for fitting
the zero-inflated (http://pscl.stanford.edu/zeroinfl.r) and hurdle
(http://pscl.stanford.edu/hurdle.r) count models.

Reza



On 5/17/05, St??phanie PAYET <spayet.reesfrance at wanadoo.fr> wrote:
> Hi,
> 
> I have two questions. First, I'd like to compare a ZINB model to a negativ
> binomial model with the Vuong test, but I can't find how to performe it from
> the zicount package. Does a programm exist to do it ?
> Second, I'd like to know in which cases we have to use a double hurdle model
> instead of a zero inflated model.
> 
> Many thanks,
> 
> St??phanie Payet
> 
> REES France
> R??seau d'Evaluation en Economie de la Sant??
> 28, rue d'Assas
> 75006 PARIS
> T??l. +33 (0)1 44 39 16 90
> Fax +33 (0)1 44 39 16 92
> M??l. reesfrance at wanadoo.fr
> Site Internet : http://www.rees-france.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue May 17 12:26:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 May 2005 11:26:39 +0100 (BST)
Subject: [R] a problem sourcing a file using chdir=TRUE
In-Reply-To: <Pine.SOL.4.50.0505161339140.1211-100000@pearson.stat.unipg.it>
References: <Pine.SOL.4.50.0505161339140.1211-100000@pearson.stat.unipg.it>
Message-ID: <Pine.LNX.4.61.0505171125550.7927@gannet.stats>

This and some related problems should be fixed in tomorrow's R-patched 
snapshot.

On Mon, 16 May 2005, Luca Scrucca wrote:

> Dear R-users,
>
> I used to give commands such as:
>
>> source(file="~/path/to/file.R", chdir=TRUE)
>
> but with the latest v. 2.1.0 it does not seem to work anymore.
> I tried to figure out what it was going on and it seems that the string
> for which
>> class(file)
> [1] "character"
> is changed to
>> class(file)
> [1] "file"       "connection"
> when the connection is open by
> file <- file(file, "r", encoding = encoding)
>
> But this force the following if statement
> if (chdir && is.character(file) && (path <- dirname(file)) != ".")
>   { owd <- getwd()
>     on.exit(setwd(owd))
>     setwd(path)
>   }
> to be FALSE and then non changing of current directory is done.
> Is this the desired behavior or some bug fix is required?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jtk at cmp.uea.ac.uk  Tue May 17 13:29:41 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Tue, 17 May 2005 12:29:41 +0100
Subject: [R] parsing speed
In-Reply-To: <17033.41532.761605.39534@stat.math.ethz.ch>
References: <42891A0A.6090507@stats.uwo.ca>
	<200505162220.j4GMK10T018360@meitner.gene.com>
	<17033.41532.761605.39534@stat.math.ethz.ch>
Message-ID: <20050517112941.GF20694@jtkpc.cmp.uea.ac.uk>

On Tue, May 17, 2005 at 09:50:20AM +0200, Martin Maechler wrote:
> >>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
> >>>>>     on Mon, 16 May 2005 15:20:01 -0700 writes:
> 
>     BertG> (just my additional $.02) ... and as a general rule
>     BertG> (subject to numerous exceptions, caveats, etc.)
> 
>     BertG> 1) it is programming and debugging time that most
>     BertG> impacts "overall" program execution time; 2) this is
>     BertG> most strongly impacted by code readability and size
>     BertG> (the smaller the better); 3) both of which are
>     BertG> enhanced by modular construction and reuseability,
>     BertG> which argues for avoiding inline code and using
>     BertG> separate functions.
> 
>     BertG> These days, i would argue that most of the time it is
>     BertG> program clarity and correctness (they are related)
>     BertG> that is the important issue, not execution speed.
> 
>     BertG> ... again, subject to exceptions and caveats, etc.
> 
> Yes indeed; very good points very well put!
> 
> Just to say it again: 
> 
>  **** We strongly recommend not to "inline" your code, but rather
>  **** program modularly, i.e. call small `utility' functions.

Generally, I fully agree -- modular coding is good, not only in R.
However, with regard to execution time, modularisation that involves
passing of large amounts of data (100 x 1000 data frames etc.) can
cause problems.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From buser at stat.math.ethz.ch  Tue May 17 13:23:41 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 17 May 2005 13:23:41 +0200
Subject: [R] adjusted p-values with TukeyHSD?
In-Reply-To: <703DA424-EA26-44ED-A084-7E7BD8E46EE3@gmx.at>
References: <703DA424-EA26-44ED-A084-7E7BD8E46EE3@gmx.at>
Message-ID: <17033.54333.83504.663074@stat.math.ethz.ch>

Dear Christoph

You can use the multcomp package. Please have a look at the
following example:

library(multcomp)

The first two lines were already proposed by Erin Hodgess:

summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
TukeyHSD(fm1, "tension", ordered = TRUE)

    Tukey multiple comparisons of means
    95% family-wise confidence level
    factor levels have been ordered
 
Fit: aov(formula = breaks ~ wool + tension, data = warpbreaks)

$tension
         diff        lwr      upr
M-H  4.722222 -4.6311985 14.07564
L-H 14.722222  5.3688015 24.07564
L-M 10.000000  0.6465793 19.35342
 

By using the functions simtest or simint you can get the
p-values, too:

summary(simtest(breaks ~ wool + tension, data = warpbreaks, whichf="tension",
        type = "Tukey"))

	 Simultaneous tests: Tukey contrasts 

Call: 
simtest.formula(formula = breaks ~ wool + tension, data = warpbreaks, 
    whichf = "tension", type = "Tukey")

	 Tukey contrasts for factor tension, covariable:  wool 

Contrast matrix:
                      tensionL tensionM tensionH
tensionM-tensionL 0 0       -1        1        0
tensionH-tensionL 0 0       -1        0        1
tensionH-tensionM 0 0        0       -1        1


Absolute Error Tolerance:  0.001 

Coefficients:
                  Estimate t value Std.Err. p raw p Bonf p adj
tensionH-tensionL  -14.722  -3.802    3.872 0.000  0.001 0.001
tensionM-tensionL  -10.000  -2.582    3.872 0.013  0.026 0.024
tensionH-tensionM   -4.722  -1.219    3.872 0.228  0.228 0.228



or if you prefer to get the confidence intervals, too, you can
use:

summary(simint(breaks ~ wool + tension, data = warpbreaks, whichf="tension",
        type = "Tukey"))

	Simultaneous 95% confidence intervals: Tukey contrasts

Call: 
simint.formula(formula = breaks ~ wool + tension, data = warpbreaks, 
    whichf = "tension", type = "Tukey")

	 Tukey contrasts for factor tension, covariable:  wool 

Contrast matrix:
                      tensionL tensionM tensionH
tensionM-tensionL 0 0       -1        1        0
tensionH-tensionL 0 0       -1        0        1
tensionH-tensionM 0 0        0       -1        1

Absolute Error Tolerance:  0.001 

 95 % quantile:  2.415 

Coefficients:
                  Estimate   2.5 % 97.5 % t value Std.Err. p raw p Bonf p adj
tensionM-tensionL  -10.000 -19.352 -0.648  -2.582    3.872 0.013  0.038 0.034
tensionH-tensionL  -14.722 -24.074 -5.370  -3.802    3.872 0.000  0.001 0.001
tensionH-tensionM   -4.722 -14.074  4.630  -1.219    3.872 0.228  0.685 0.447

-----------------------------------------------------------------
Please be careful: The resulting confidence intervals in
simint are not associated with the p-values from 'simtest' as it
is described in the help page of the two functions.
-----------------------------------------------------------------

I had not the time to check the differences in the function or
read the references given on the help page.
If you are interested in the function you can check those to
find out which one you prefer.

Best regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Christoph Strehblow writes:
 > hi list,
 > 
 > i have to ask you again, having tried and searched for several days...
 > 
 > i want to do a TukeyHSD after an Anova, and want to get the adjusted  
 > p-values after the Tukey Correction.
 > i found the p.adjust function, but it can only correct for "holm",  
 > "hochberg", bonferroni", but not "Tukey".
 > 
 > Is it not possbile to get adjusted p-values after Tukey-correction?
 > 
 > sorry, if this is an often-answered-question, but i didn??t find it on  
 > the list archive...
 > 
 > thx a lot, list, Chris
 > 
 > 
 > Christoph Strehblow, MD
 > Department of Rheumatology, Diabetes and Endocrinology
 > Wilhelminenspital, Vienna, Austria
 > chrisxe at gmx.at
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Tue May 17 13:32:04 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 17 May 2005 12:32:04 +0100
Subject: [R] parsing speed
In-Reply-To: <20050517112941.GF20694@jtkpc.cmp.uea.ac.uk>
References: <42891A0A.6090507@stats.uwo.ca>	<200505162220.j4GMK10T018360@meitner.gene.com>	<17033.41532.761605.39534@stat.math.ethz.ch>
	<20050517112941.GF20694@jtkpc.cmp.uea.ac.uk>
Message-ID: <4289D634.1020103@lancaster.ac.uk>

Jan T. Kim wrote:

> Generally, I fully agree -- modular coding is good, not only in R.
> However, with regard to execution time, modularisation that involves
> passing of large amounts of data (100 x 1000 data frames etc.) can
> cause problems.

  I've just tried a few simple examples of throwing biggish (3000x3000) 
matrices around and haven't encountered any pathological behaviour yet. 
I tried modifying the matrices within the functions, tried looping a few 
thousand times to estimate the matrix passing overhead, and in most 
cases the modular version run pretty much as fast as - or occasionally 
faster than - the inline version. There was some variability in CPU time 
taken, probably due to garbage collection.

  Does anyone have a simple example where passing large data sets causes 
a huge increase in CPU time? I think R is pretty smart with its 
parameter passing these days - anyone who thinks its still like Splus 
version 2.3 should update their brains to the 21st Century.

Baz



From valdar at well.ox.ac.uk  Tue May 17 13:40:37 2005
From: valdar at well.ox.ac.uk (William Valdar)
Date: Tue, 17 May 2005 12:40:37 +0100 (BST)
Subject: [R] setting value arg of pdSymm() in nlme
Message-ID: <Pine.LNX.4.62.0505171031420.1532@octopus.well.ox.ac.uk>

Dear All,

I wish to model random effects that have known between-group covariance 
structure using the lme() function from library nlme. However, I have yet 
to get even a simple example to work. No doubt this is because I am 
confusing my syntax, but I would appreciate any guidance as to how. I have 
studied Pinheiro & Bates carefully (though it's always possible I've 
missed something), the few posts mentioning pdSymm (some of which suggest 
lme is suboptimal here anyway) and ?pdSymm (which has only a trivial 
example, see later) but have not yet found a successful example of syntax 
for this particular problem.

I am using the pdSymm class to specify a positive definite matrix 
corresponding to the covariance structure of a random batch effect, and 
passing this to lme() through the random= argument. To do this, I must 
set the value= argument of pdSymm.

Consider the following simple and self-contained example:

library(nlme)

# make response and batch data
batch.names <- c("A", "B", "C")
data.df <- data.frame(
         response = rnorm(100),
         batch = factor(sample(batch.names, 100, replace=T))
         )

# make covariance matrix for batch
batch.mat <- matrix(c(1,.5,.2, .5, 1, .3, .2, .3, 1), ncol=3)
colnames(batch.mat) <- batch.names
rownames(batch.mat) <- batch.names

# fit batch as a simple random intercept
lme(response ~ 1, data=data.df, random=~1|batch)

# ...works fine

# do the same using pdSymm notation
lme(response ~ 1, data=data.df,
         random=list( batch=pdSymm(form=~1) )
         )
# ...works fine also

# specify cov structure using value arg
lme(response ~ 1, data=data.df,
         random=list( batch=pdSymm(
                 value=batch.mat,
                 form=~1,
                 nam=batch.names)
                 )
         )

# throws error below

---snip---
Error in "Names<-.pdMat"(`*tmp*`, value = "(Intercept)") :
         Length of names should be 3

> traceback()
7: stop(paste("Length of names should be", length(dn)))
6: "Names<-.pdMat"(`*tmp*`, value = "(Intercept)")
5: "Names<-"(`*tmp*`, value = "(Intercept)")
4: "Names<-.reStruct"(`*tmp*`, value = list(batch = "(Intercept)"))
3: "Names<-"(`*tmp*`, value = list(batch = "(Intercept)"))
2: lme.formula(response ~ 1, data = data.df, random = list(batch = 
pdSymm(value = batch.mat,
        form = ~1, nam = batch.names)))
1: lme(response ~ 1, data = data.df, random = list(batch = pdSymm(value = 
batch.mat,
        form = ~1, nam = batch.names)))
---snip---

The length of batch.names is 3, so I find this error enigmatic. Note that 
I had to specify all three of value, form and nam otherwise I got 
missing args errors. Also note that doing

  pdSymm(value=batch.mat, form=~1, nam=batch.names)

on the command line, like the similar invocation described on ?pdSymm, 
works fine also. It's just lme() that doesn't like it.

Can anybody show me what I should be doing instead? Some successful code 
will greatly clarify the issue. (My version details are below). Also, I 
notice the pdMat scheme is absent from lme() in lme4. Is this 
functionality deprecated in lme4 and excluded from lmer?

Many thanks,

William

Version details: running R 2.1.0 on windows XP, using nlme 3.1-57.

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Dr William Valdar               ++44 (0)1865 287 717
Wellcome Trust Centre           valdar at well.ox.ac.uk
for Human Genetics, Oxford      www.well.ox.ac.uk/~valdar



From chrisxe at gmx.at  Tue May 17 13:47:10 2005
From: chrisxe at gmx.at (Christoph Strehblow)
Date: Tue, 17 May 2005 13:47:10 +0200
Subject: [R] adjusted p-values with TukeyHSD?
In-Reply-To: <17033.54333.83504.663074@stat.math.ethz.ch>
References: <703DA424-EA26-44ED-A084-7E7BD8E46EE3@gmx.at>
	<17033.54333.83504.663074@stat.math.ethz.ch>
Message-ID: <4291E402-0FA4-4AE7-B0C6-D0F1BBF43A9F@gmx.at>

Hi!

Thanks a lot, works as advertised. If i used Tukey, it even gives  
raw, Bonferroni- and Tukey-corrected p-values!

Thx for the help,


Christoph Strehblow, MD
Department of Rheumatology, Diabetes and Endocrinology
Wilhelminenspital, Vienna, Austria
chrisxe at gmx.at


Am 17.05.2005 um 13:23 schrieb Christoph Buser:

> Dear Christoph
>
> You can use the multcomp package. Please have a look at the
> following example:
>
> library(multcomp)
>
> The first two lines were already proposed by Erin Hodgess:
>
> summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
> TukeyHSD(fm1, "tension", ordered = TRUE)
>
>     Tukey multiple comparisons of means
>     95% family-wise confidence level
>     factor levels have been ordered
>
> Fit: aov(formula = breaks ~ wool + tension, data = warpbreaks)
>
> $tension
>          diff        lwr      upr
> M-H  4.722222 -4.6311985 14.07564
> L-H 14.722222  5.3688015 24.07564
> L-M 10.000000  0.6465793 19.35342
>
>
> By using the functions simtest or simint you can get the
> p-values, too:
>
> summary(simtest(breaks ~ wool + tension, data = warpbreaks,  
> whichf="tension",
>         type = "Tukey"))
>
>      Simultaneous tests: Tukey contrasts
>
> Call:
> simtest.formula(formula = breaks ~ wool + tension, data = warpbreaks,
>     whichf = "tension", type = "Tukey")
>
>      Tukey contrasts for factor tension, covariable:  wool
>
> Contrast matrix:
>                       tensionL tensionM tensionH
> tensionM-tensionL 0 0       -1        1        0
> tensionH-tensionL 0 0       -1        0        1
> tensionH-tensionM 0 0        0       -1        1
>
>
> Absolute Error Tolerance:  0.001
>
> Coefficients:
>                   Estimate t value Std.Err. p raw p Bonf p adj
> tensionH-tensionL  -14.722  -3.802    3.872 0.000  0.001 0.001
> tensionM-tensionL  -10.000  -2.582    3.872 0.013  0.026 0.024
> tensionH-tensionM   -4.722  -1.219    3.872 0.228  0.228 0.228
>
>
>
> or if you prefer to get the confidence intervals, too, you can
> use:
>
> summary(simint(breaks ~ wool + tension, data = warpbreaks,  
> whichf="tension",
>         type = "Tukey"))
>
>     Simultaneous 95% confidence intervals: Tukey contrasts
>
> Call:
> simint.formula(formula = breaks ~ wool + tension, data = warpbreaks,
>     whichf = "tension", type = "Tukey")
>
>      Tukey contrasts for factor tension, covariable:  wool
>
> Contrast matrix:
>                       tensionL tensionM tensionH
> tensionM-tensionL 0 0       -1        1        0
> tensionH-tensionL 0 0       -1        0        1
> tensionH-tensionM 0 0        0       -1        1
>
> Absolute Error Tolerance:  0.001
>
>  95 % quantile:  2.415
>
> Coefficients:
>                   Estimate   2.5 % 97.5 % t value Std.Err. p raw p  
> Bonf p adj
> tensionM-tensionL  -10.000 -19.352 -0.648  -2.582    3.872 0.013   
> 0.038 0.034
> tensionH-tensionL  -14.722 -24.074 -5.370  -3.802    3.872 0.000   
> 0.001 0.001
> tensionH-tensionM   -4.722 -14.074  4.630  -1.219    3.872 0.228   
> 0.685 0.447
>
> -----------------------------------------------------------------
> Please be careful: The resulting confidence intervals in
> simint are not associated with the p-values from 'simtest' as it
> is described in the help page of the two functions.
> -----------------------------------------------------------------
>
> I had not the time to check the differences in the function or
> read the references given on the help page.
> If you are interested in the function you can check those to
> find out which one you prefer.
>
> Best regards,
>
> Christoph Buser
>
> --------------------------------------------------------------
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C13
> ETH (Federal Inst. Technology)    8092 Zurich     SWITZERLAND
> phone: x-41-44-632-4673        fax: 632-1228
> http://stat.ethz.ch/~buser/
> --------------------------------------------------------------
>
>
> Christoph Strehblow writes:
>
>> hi list,
>>
>> i have to ask you again, having tried and searched for several  
>> days...
>>
>> i want to do a TukeyHSD after an Anova, and want to get the adjusted
>> p-values after the Tukey Correction.
>> i found the p.adjust function, but it can only correct for "holm",
>> "hochberg", bonferroni", but not "Tukey".
>>
>> Is it not possbile to get adjusted p-values after Tukey-correction?
>>
>> sorry, if this is an often-answered-question, but i didn??t find it on
>> the list archive...
>>
>> thx a lot, list, Chris
>>
>>
>> Christoph Strehblow, MD
>> Department of Rheumatology, Diabetes and Endocrinology
>> Wilhelminenspital, Vienna, Austria
>> chrisxe at gmx.at
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>>
>
>



From andy_liaw at merck.com  Tue May 17 14:15:33 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 17 May 2005 08:15:33 -0400
Subject: [R] install.packages parameters
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E844@usctmx1106.merck.com>

Caveat: I know next to nothing about Mac...

That said, my guess is that you installed R from binary, rather than
building from source.  In that case the compilers and flags, etc., are
configured to the machine that the binary is built on.  You can look in
$RHOME/etc/Makeconf to see the settings, and see if changing them helps.

Andy

> From: jerosenb at fas.harvard.edu
> 
> Hello.
> 
> R is having some trouble installing a package because it 
> passed arguments to 
> gcc which were non-existent directories and files.  It also 
> didn't find 
> g77, although it's in a directory in my $PATH;  I tricked it 
> by making a 
> sym link in /usr/bin.
> 
> What file does R get these parameters from?  
> I've looked for the parameters in the package source, the 
> install.packages 
> help pages, and the R preferences menu, all to no avail.
> 
> I am running R 2.1.0 on Mac 10.3.8, and three days ago I installed a
> different package from source where installation involved gcc without
> any problems, and nothing has changed since then.  The packages I'm
> trying to install are Joe Schafer's mix, norm, and cat.
> 
> Thanks,
> 
> Janet
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From slist at oomvanlieshout.net  Tue May 17 14:27:57 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 17 May 2005 14:27:57 +0200
Subject: [R] adjusted p-values with TukeyHSD?
In-Reply-To: <17033.54333.83504.663074@stat.math.ethz.ch>
References: <703DA424-EA26-44ED-A084-7E7BD8E46EE3@gmx.at>
	<17033.54333.83504.663074@stat.math.ethz.ch>
Message-ID: <4289E34D.5020002@oomvanlieshout.net>

Hi Chris and Chris,

I was keeping my eye on this thread as I have also been discovering 
multiple comparisons recently. Your instructions are very clear! Thanks.

Now I would love to see an R boffin write a nifty function to produce a 
graphical representation of the multiple comparison, like this one:

http://www.theses.ulaval.ca/2003/21026/21026024.jpg

Should not be too difficult.....[any one up for the challenge?]

I came across more multiple comparison info here;

http://www.agr.kuleuven.ac.be/vakken/statisticsbyR/ANOVAbyRr/multiplecomp.htm

Cheers,

Sander.

Christoph Buser wrote:
> Dear Christoph
> 
> You can use the multcomp package. Please have a look at the
> following example:
> 
> library(multcomp)
> 
> The first two lines were already proposed by Erin Hodgess:
> 
> summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
> TukeyHSD(fm1, "tension", ordered = TRUE)
> 
>     Tukey multiple comparisons of means
>     95% family-wise confidence level
>     factor levels have been ordered
>  
> Fit: aov(formula = breaks ~ wool + tension, data = warpbreaks)
> 
> $tension
>          diff        lwr      upr
> M-H  4.722222 -4.6311985 14.07564
> L-H 14.722222  5.3688015 24.07564
> L-M 10.000000  0.6465793 19.35342
>  
> 
> By using the functions simtest or simint you can get the
> p-values, too:
> 
> summary(simtest(breaks ~ wool + tension, data = warpbreaks, whichf="tension",
>         type = "Tukey"))
> 
> 	 Simultaneous tests: Tukey contrasts 
> 
> Call: 
> simtest.formula(formula = breaks ~ wool + tension, data = warpbreaks, 
>     whichf = "tension", type = "Tukey")
> 
> 	 Tukey contrasts for factor tension, covariable:  wool 
> 
> Contrast matrix:
>                       tensionL tensionM tensionH
> tensionM-tensionL 0 0       -1        1        0
> tensionH-tensionL 0 0       -1        0        1
> tensionH-tensionM 0 0        0       -1        1
> 
> 
> Absolute Error Tolerance:  0.001 
> 
> Coefficients:
>                   Estimate t value Std.Err. p raw p Bonf p adj
> tensionH-tensionL  -14.722  -3.802    3.872 0.000  0.001 0.001
> tensionM-tensionL  -10.000  -2.582    3.872 0.013  0.026 0.024
> tensionH-tensionM   -4.722  -1.219    3.872 0.228  0.228 0.228
> 
> 
> 
> or if you prefer to get the confidence intervals, too, you can
> use:
> 
> summary(simint(breaks ~ wool + tension, data = warpbreaks, whichf="tension",
>         type = "Tukey"))
> 
> 	Simultaneous 95% confidence intervals: Tukey contrasts
> 
> Call: 
> simint.formula(formula = breaks ~ wool + tension, data = warpbreaks, 
>     whichf = "tension", type = "Tukey")
> 
> 	 Tukey contrasts for factor tension, covariable:  wool 
> 
> Contrast matrix:
>                       tensionL tensionM tensionH
> tensionM-tensionL 0 0       -1        1        0
> tensionH-tensionL 0 0       -1        0        1
> tensionH-tensionM 0 0        0       -1        1
> 
> Absolute Error Tolerance:  0.001 
> 
>  95 % quantile:  2.415 
> 
> Coefficients:
>                   Estimate   2.5 % 97.5 % t value Std.Err. p raw p Bonf p adj
> tensionM-tensionL  -10.000 -19.352 -0.648  -2.582    3.872 0.013  0.038 0.034
> tensionH-tensionL  -14.722 -24.074 -5.370  -3.802    3.872 0.000  0.001 0.001
> tensionH-tensionM   -4.722 -14.074  4.630  -1.219    3.872 0.228  0.685 0.447
> 
> -----------------------------------------------------------------
> Please be careful: The resulting confidence intervals in
> simint are not associated with the p-values from 'simtest' as it
> is described in the help page of the two functions.
> -----------------------------------------------------------------
> 
> I had not the time to check the differences in the function or
> read the references given on the help page.
> If you are interested in the function you can check those to
> find out which one you prefer.
> 
> Best regards,
> 
> Christoph Buser
> 
> --------------------------------------------------------------
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C13
> ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> phone: x-41-44-632-4673		fax: 632-1228
> http://stat.ethz.ch/~buser/
> --------------------------------------------------------------
> 
> 
> Christoph Strehblow writes:
>  > hi list,
>  > 
>  > i have to ask you again, having tried and searched for several days...
>  > 
>  > i want to do a TukeyHSD after an Anova, and want to get the adjusted  
>  > p-values after the Tukey Correction.
>  > i found the p.adjust function, but it can only correct for "holm",  
>  > "hochberg", bonferroni", but not "Tukey".
>  > 
>  > Is it not possbile to get adjusted p-values after Tukey-correction?
>  > 
>  > sorry, if this is an often-answered-question, but i didn??t find it on  
>  > the list archive...
>  > 
>  > thx a lot, list, Chris
>  > 
>  > 
>  > Christoph Strehblow, MD
>  > Department of Rheumatology, Diabetes and Endocrinology
>  > Wilhelminenspital, Vienna, Austria
>  > chrisxe at gmx.at
>  > 
>  > ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-help
>  > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From pberming at research.ryerson.ca  Tue May 17 14:42:02 2005
From: pberming at research.ryerson.ca (Philip Bermingham)
Date: Tue, 17 May 2005 08:42:02 -0400
Subject: [R] Finding the right number of clusters
Message-ID: <4289E69A.7040300@csca.ryerson.ca>

SAS has something called the "cubic criterion" cutoff for finding the 
most appropriate number of clusters.  Does R have anything that would 
replicate that? I've been searching the lists and can't seem to find 
anything that would point me in the right direction.

Thank in advance,
Philip Bermingham



From bates at stat.wisc.edu  Tue May 17 14:53:06 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 17 May 2005 07:53:06 -0500
Subject: [R] setting value arg of pdSymm() in nlme
In-Reply-To: <Pine.LNX.4.62.0505171031420.1532@octopus.well.ox.ac.uk>
References: <Pine.LNX.4.62.0505171031420.1532@octopus.well.ox.ac.uk>
Message-ID: <4289E932.3030408@stat.wisc.edu>

William Valdar wrote:
> Dear All,
> 
> I wish to model random effects that have known between-group covariance
> structure using the lme() function from library nlme. However, I have
> yet to get even a simple example to work. No doubt this is because I am
> confusing my syntax, but I would appreciate any guidance as to how. I
> have studied Pinheiro & Bates carefully (though it's always possible
> I've missed something), the few posts mentioning pdSymm (some of which
> suggest lme is suboptimal here anyway) and ?pdSymm (which has only a
> trivial example, see later) but have not yet found a successful example
> of syntax for this particular problem.
> 
> I am using the pdSymm class to specify a positive definite matrix
> corresponding to the covariance structure of a random batch effect, and
> passing this to lme() through the random= argument. To do this, I must
> set the value= argument of pdSymm.
> 
> Consider the following simple and self-contained example:
> 
> library(nlme)
> 
> # make response and batch data
> batch.names <- c("A", "B", "C")
> data.df <- data.frame(
>         response = rnorm(100),
>         batch = factor(sample(batch.names, 100, replace=T))
>         )
> 
> # make covariance matrix for batch
> batch.mat <- matrix(c(1,.5,.2, .5, 1, .3, .2, .3, 1), ncol=3)
> colnames(batch.mat) <- batch.names
> rownames(batch.mat) <- batch.names
> 
> # fit batch as a simple random intercept
> lme(response ~ 1, data=data.df, random=~1|batch)
> 
> # ...works fine
> 
> # do the same using pdSymm notation
> lme(response ~ 1, data=data.df,
>         random=list( batch=pdSymm(form=~1) )
>         )
> # ...works fine also
> 
> # specify cov structure using value arg
> lme(response ~ 1, data=data.df,
>         random=list( batch=pdSymm(
>                 value=batch.mat,
>                 form=~1,
>                 nam=batch.names)
>                 )
>         )
> 
> # throws error below
> 
> ---snip---
> Error in "Names<-.pdMat"(`*tmp*`, value = "(Intercept)") :
>         Length of names should be 3
> 
>> traceback()
> 
> 7: stop(paste("Length of names should be", length(dn)))
> 6: "Names<-.pdMat"(`*tmp*`, value = "(Intercept)")
> 5: "Names<-"(`*tmp*`, value = "(Intercept)")
> 4: "Names<-.reStruct"(`*tmp*`, value = list(batch = "(Intercept)"))
> 3: "Names<-"(`*tmp*`, value = list(batch = "(Intercept)"))
> 2: lme.formula(response ~ 1, data = data.df, random = list(batch =
> pdSymm(value = batch.mat,
>        form = ~1, nam = batch.names)))
> 1: lme(response ~ 1, data = data.df, random = list(batch = pdSymm(value
> = batch.mat,
>        form = ~1, nam = batch.names)))
> ---snip---
> 
> The length of batch.names is 3, so I find this error enigmatic. Note
> that I had to specify all three of value, form and nam otherwise I got
> missing args errors. Also note that doing
> 
>  pdSymm(value=batch.mat, form=~1, nam=batch.names)
> 
> on the command line, like the similar invocation described on ?pdSymm,
> works fine also. It's just lme() that doesn't like it.
> 
> Can anybody show me what I should be doing instead? Some successful code
> will greatly clarify the issue. (My version details are below).

I'm afraid that I don't understand what you are trying to do.  With a
formula of ~ 1 the pdSymm generator creates a 1x1 variance-covariance
matrix, which you are initializing to a 3x3 matrix.  What is batch.mat
supposed to represent?

> Also, I
> notice the pdMat scheme is absent from lme() in lme4. Is this
> functionality deprecated in lme4 and excluded from lmer?

Yes, although I think you mean lmer in lme4.  Because the lmer function
allows multiple nested or non-nested grouping factors, the need for the
pdMat classes is eliminated (or greatly reduced) and the code can be
simplified considerably.  There is an article in the 2005/1 issue of R
News describing the use of lmer.



From francoisromain at free.fr  Tue May 17 14:57:24 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 17 May 2005 14:57:24 +0200
Subject: [R] Finding the right number of clusters
In-Reply-To: <4289E69A.7040300@csca.ryerson.ca>
References: <4289E69A.7040300@csca.ryerson.ca>
Message-ID: <4289EA34.5020602@free.fr>

Le 17.05.2005 14:42, Philip Bermingham a ??crit :

> SAS has something called the "cubic criterion" cutoff for finding the 
> most appropriate number of clusters.  Does R have anything that would 
> replicate that? I've been searching the lists and can't seem to find 
> anything that would point me in the right direction.
>
> Thank in advance,
> Philip Bermingham
>
Hello,

Package fpc has a function cluster.stats with a lot of criterion like 
G2, G3, etc ...

Romain

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ybas at ens-lyon.fr  Tue May 17 15:16:45 2005
From: ybas at ens-lyon.fr (ybas@ens-lyon.fr)
Date: Tue, 17 May 2005 15:16:45 +0200 (CEST)
Subject: [R] problem with gls : combining weights and correlation structure
Message-ID: <1116335805.4289eebde0c0e@mouette.ens-lyon.fr>

Dear R-users,

I hope you will have time to read me and I will try to be brief. I am also 
sorry for my poor english.

I used gls function from the package nlme to correct two types of bias in my 
database. At first, because my replicates are spatially aggregated, I would 
like to fit a corStruct function like corLin, corSpher, corRatio, corExp or 
corGaus in my gls model, and simultaneously, because my response variable is 
an estimate, I would like to use weights to take into account the accuracy of 
the estimation. I used a varFixed object corresponding to squared standard 
error.
Variograms all shows a weak but real spatial autocorrelation (nugget ~ 0.9 but 
they always increase with distance). 
My first problem was the estimation of the parameters of the corStruc function 
which were very far from their order of magnitude (range > 10E15, though the

maximum distance between observations is no more than 10E6).
I thought I had convergence problem that I could solve :
- with at first fitting corStruct functions to variograms with the solver of 
Excel
- and secondly binding corStruct parameters to the obtained value with the 
argument "fixed=TRUE"
But I obtained very unrealistic values for the parameters of the model even 
when the spatial autocorrelation was weak, so I am sure that the model fitting 
didn't work properly.
I had absolutely no problems in using the "corr" or the
"weight" arguments 
separately.

I thank you very much to read me and if you have a solution to my problem or 
if you know where I did a mistake, you would be very nice to answer me.

Sincerely yours,

Yves Bas



From Matthias.Templ at statistik.gv.at  Tue May 17 15:18:46 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Tue, 17 May 2005 15:18:46 +0200
Subject: [R] Finding the right number of clusters
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAA73@xchg1.statistik.local>

Hello,

It depends, *which clustering method you will use*.
Model-based Clustering algorithms have the BIC criterion implemented
(Mclust).
Partition Based clustering algorithms have other criterias (Sum of
Squares withhin and between clusters and you can easely implement other
criterias).
Most of the criterias in fuzzy clustering are very different.
With hierarchical clustering algorithms, you can also determine the
number of cluster, very different from the other methods.
Note that there is no optimal criteria for all these different methods
and it is not so easy to find the optimal number of clusters - the
optimal number of clusters depends on your data and which criteria you
have to use depends also on your data.

Best,
Matthias

> SAS has something called the "cubic criterion" cutoff for finding the 
> most appropriate number of clusters.  Does R have anything that would 
> replicate that? I've been searching the lists and can't seem to find 
> anything that would point me in the right direction.
> 
> Thank in advance,
> Philip Bermingham
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue May 17 15:19:48 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 17 May 2005 09:19:48 -0400
Subject: [R] adjusted p-values with TukeyHSD?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E846@usctmx1106.merck.com>

> From: Sander Oom
> 
> Hi Chris and Chris,
> 
> I was keeping my eye on this thread as I have also been discovering 
> multiple comparisons recently. Your instructions are very 
> clear! Thanks.

One thing to note, though:  Multcomp does not do Dunnett's or 
Tukey's multiple comparisons per se.  Those names in multcomp 
refer to the contrasts being used (comparison to a control for 
Dunnett and all pairwise comparison for Tukey).  The actual 
methods used are as described in the references of the help
pages.

 
> Now I would love to see an R boffin write a nifty function to 
> produce a 
> graphical representation of the multiple comparison, like this one:
> 
> http://www.theses.ulaval.ca/2003/21026/21026024.jpg
> 
> Should not be too difficult.....[any one up for the challenge?]

I beg to differ:  That's probably as bad a way as one can use to 
graphically show multiple comparison.  The shaded bars serve no 
purpose.

Two alternatives that I'm aware of are 

- Multiple comparison circles, due to John Sall, and not 
  surprisingly, implemented in JMP and SAS/Insight.  See:
 
http://support.sas.com/documentation/onlinedoc/v7/whatsnew/insight/sect4.htm


- The mean-mean display proposed by Hsu and Peruggia:
  Hsu, J. C. and M. Peruggia (1994). 
  Graphical representations of Tukey's multiple comparison method.
  Journal of Computational and Graphical Statistics 3, 143{161

Andy
 
> I came across more multiple comparison info here;
> 
> http://www.agr.kuleuven.ac.be/vakken/statisticsbyR/ANOVAbyRr/m
> ultiplecomp.htm
> 
> Cheers,
> 
> Sander.
> 
> Christoph Buser wrote:
> > Dear Christoph
> > 
> > You can use the multcomp package. Please have a look at the
> > following example:
> > 
> > library(multcomp)
> > 
> > The first two lines were already proposed by Erin Hodgess:
> > 
> > summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
> > TukeyHSD(fm1, "tension", ordered = TRUE)
> > 
> >     Tukey multiple comparisons of means
> >     95% family-wise confidence level
> >     factor levels have been ordered
> >  
> > Fit: aov(formula = breaks ~ wool + tension, data = warpbreaks)
> > 
> > $tension
> >          diff        lwr      upr
> > M-H  4.722222 -4.6311985 14.07564
> > L-H 14.722222  5.3688015 24.07564
> > L-M 10.000000  0.6465793 19.35342
> >  
> > 
> > By using the functions simtest or simint you can get the
> > p-values, too:
> > 
> > summary(simtest(breaks ~ wool + tension, data = warpbreaks, 
> whichf="tension",
> >         type = "Tukey"))
> > 
> > 	 Simultaneous tests: Tukey contrasts 
> > 
> > Call: 
> > simtest.formula(formula = breaks ~ wool + tension, data = 
> warpbreaks, 
> >     whichf = "tension", type = "Tukey")
> > 
> > 	 Tukey contrasts for factor tension, covariable:  wool 
> > 
> > Contrast matrix:
> >                       tensionL tensionM tensionH
> > tensionM-tensionL 0 0       -1        1        0
> > tensionH-tensionL 0 0       -1        0        1
> > tensionH-tensionM 0 0        0       -1        1
> > 
> > 
> > Absolute Error Tolerance:  0.001 
> > 
> > Coefficients:
> >                   Estimate t value Std.Err. p raw p Bonf p adj
> > tensionH-tensionL  -14.722  -3.802    3.872 0.000  0.001 0.001
> > tensionM-tensionL  -10.000  -2.582    3.872 0.013  0.026 0.024
> > tensionH-tensionM   -4.722  -1.219    3.872 0.228  0.228 0.228
> > 
> > 
> > 
> > or if you prefer to get the confidence intervals, too, you can
> > use:
> > 
> > summary(simint(breaks ~ wool + tension, data = warpbreaks, 
> whichf="tension",
> >         type = "Tukey"))
> > 
> > 	Simultaneous 95% confidence intervals: Tukey contrasts
> > 
> > Call: 
> > simint.formula(formula = breaks ~ wool + tension, data = 
> warpbreaks, 
> >     whichf = "tension", type = "Tukey")
> > 
> > 	 Tukey contrasts for factor tension, covariable:  wool 
> > 
> > Contrast matrix:
> >                       tensionL tensionM tensionH
> > tensionM-tensionL 0 0       -1        1        0
> > tensionH-tensionL 0 0       -1        0        1
> > tensionH-tensionM 0 0        0       -1        1
> > 
> > Absolute Error Tolerance:  0.001 
> > 
> >  95 % quantile:  2.415 
> > 
> > Coefficients:
> >                   Estimate   2.5 % 97.5 % t value Std.Err. 
> p raw p Bonf p adj
> > tensionM-tensionL  -10.000 -19.352 -0.648  -2.582    3.872 
> 0.013  0.038 0.034
> > tensionH-tensionL  -14.722 -24.074 -5.370  -3.802    3.872 
> 0.000  0.001 0.001
> > tensionH-tensionM   -4.722 -14.074  4.630  -1.219    3.872 
> 0.228  0.685 0.447
> > 
> > -----------------------------------------------------------------
> > Please be careful: The resulting confidence intervals in
> > simint are not associated with the p-values from 'simtest' as it
> > is described in the help page of the two functions.
> > -----------------------------------------------------------------
> > 
> > I had not the time to check the differences in the function or
> > read the references given on the help page.
> > If you are interested in the function you can check those to
> > find out which one you prefer.
> > 
> > Best regards,
> > 
> > Christoph Buser
> > 
> > --------------------------------------------------------------
> > Christoph Buser <buser at stat.math.ethz.ch>
> > Seminar fuer Statistik, LEO C13
> > ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> > phone: x-41-44-632-4673		fax: 632-1228
> > http://stat.ethz.ch/~buser/
> > --------------------------------------------------------------
> > 
> > 
> > Christoph Strehblow writes:
> >  > hi list,
> >  > 
> >  > i have to ask you again, having tried and searched for 
> several days...
> >  > 
> >  > i want to do a TukeyHSD after an Anova, and want to get 
> the adjusted  
> >  > p-values after the Tukey Correction.
> >  > i found the p.adjust function, but it can only correct 
> for "holm",  
> >  > "hochberg", bonferroni", but not "Tukey".
> >  > 
> >  > Is it not possbile to get adjusted p-values after 
> Tukey-correction?
> >  > 
> >  > sorry, if this is an often-answered-question, but i 
> didn??t find it on  
> >  > the list archive...
> >  > 
> >  > thx a lot, list, Chris
> >  > 
> >  > 
> >  > Christoph Strehblow, MD
> >  > Department of Rheumatology, Diabetes and Endocrinology
> >  > Wilhelminenspital, Vienna, Austria
> >  > chrisxe at gmx.at
> >  > 
> >  > ______________________________________________
> >  > R-help at stat.math.ethz.ch mailing list
> >  > https://stat.ethz.ch/mailman/listinfo/r-help
> >  > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> > 
> ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> 
> -- 
> 
> --------------------------------------------
> Dr Sander P. Oom
> Animal, Plant and Environmental Sciences,
> University of the Witwatersrand
> Private Bag 3, Wits 2050, South Africa
> Tel (work)      +27 (0)11 717 64 04
> Tel (home)      +27 (0)18 297 44 51
> Fax             +27 (0)18 299 24 64
> Email   sander at oomvanlieshout.net
> Web     www.oomvanlieshout.net/sander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From andy_liaw at merck.com  Tue May 17 15:43:17 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 17 May 2005 09:43:17 -0400
Subject: [R] parsing speed
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E847@usctmx1106.merck.com>

> From: Barry Rowlingson
> 
> Jan T. Kim wrote:
> 
> > Generally, I fully agree -- modular coding is good, not only in R.
> > However, with regard to execution time, modularisation that involves
> > passing of large amounts of data (100 x 1000 data frames etc.) can
> > cause problems.
> 
>   I've just tried a few simple examples of throwing biggish 
> (3000x3000) 
> matrices around and haven't encountered any pathological 
> behaviour yet. 
> I tried modifying the matrices within the functions, tried 
> looping a few 
> thousand times to estimate the matrix passing overhead, and in most 
> cases the modular version run pretty much as fast as - or 
> occasionally 
> faster than - the inline version. There was some variability 
> in CPU time 
> taken, probably due to garbage collection.
> 
>   Does anyone have a simple example where passing large data 
> sets causes 
> a huge increase in CPU time? I think R is pretty smart with its 
> parameter passing these days - anyone who thinks its still like Splus 
> version 2.3 should update their brains to the 21st Century.

I think one example of this is using the formula interface to fit models on
large data sets, especially those with tons of variables.  Some model
fitting functions have the default interface f(x, y, ...), along with a
formula method f(formula, data, ...).  If x has lots of variables (say over
1000), using the formula interface can take several times longer than
calling the "raw" interface directly.

Andy
 
> Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Tue May 17 15:46:54 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Tue, 17 May 2005 09:46:54 -0400
Subject: [R] Survey of "moving window" statistical functions - still l
	ooking for fast mad function
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4067@us-arlington-0668.mail.saic.com>

On 9 Oct 2004, Brian D. Ripley wrote: 

> On Fri, 8 Oct 2004, Tuszynski, Jaroslaw W. wrote:
>> 	Finally a question: I still need to get moving windows mad function 
>> faster my "runmad" function is not that much faster than apply/embed 
>> combo, and that I used before, and this is where my code spends most 
>> of its time. I need something like "runmed" but for a mad function. Any
suggestions?
>
> Write your own C-level implementation, as runmed and most of the other
fast functions you cite are.

I did as suggested and just released runmean, runmax, runmin, runmad and
runquantile functions as part of caMassClass package.
Thanks for the suggestion, now my codes run 20 min. instead of overnight.

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



From cs_matyi at freemail.hu  Tue May 17 15:47:08 2005
From: cs_matyi at freemail.hu (=?ISO-8859-2?Q?Cserh=E1ti_M=E1ty=E1s?=)
Date: Tue, 17 May 2005 15:47:08 +0200 (CEST)
Subject: [R] installing R on Irix
In-Reply-To: <200505171016.j4HA2WWa015368@hypatia.math.ethz.ch>
Message-ID: <freemail.20050417154708.76688@fm11.freemail.hu>

Hello veeryone, I nedd some help here.

The problem is I was trying to install R on my Irix system, with little 
success: I got the following ugly error messages: watch out:


begin installing recommended package mgcv
Cannot create directory "": No such file or directory
* Installing *source* package 'mgcv' ...
** libs
gmake[3]: Entering directory `/tmp/R.INSTALL.13709658/mgcv/src'
gcc -I/usr/home/csmatyi/programs/R/R-2.1.0/include  -
I/usr/local/include     -g -O2 -c gcv.c -o gcv.o
gcc -I/usr/home/csmatyi/programs/R/R-2.1.0/include  -
I/usr/local/include     -g -O2 -c magic.c -o magic.o
gcc -I/usr/home/csmatyi/programs/R/R-2.1.0/include  -
I/usr/local/include     -g -O2 -c mat.c -o mat.o
gcc -I/usr/home/csmatyi/programs/R/R-2.1.0/include  -
I/usr/local/include     -g -O2 -c matrix.c -o matrix.o
as: Error: /var/tmp/ccAomyDd.s, line 23679: register expected
      dmtc1 244($sp),$f0
as: Error: /var/tmp/ccAomyDd.s, line 23679: Undefined symbol: 244
gmake[3]: *** [matrix.o] Error 1
gmake[3]: Leaving directory `/tmp/R.INSTALL.13709658/mgcv/src'
ERROR: compilation failed for package 'mgcv'
** Removing '/usr/home/csmatyi/programs/R/R-2.1.0/library/mgcv'
gmake[2]: *** [mgcv.ts] Error 1
gmake[2]: Leaving directory `/usr/home/csmatyi/programs/R/R-
2.1.0/src/library/Recommended'
gmake[1]: *** [recommended-packages] Error 2
gmake[1]: Leaving directory `/usr/home/csmatyi/programs/R/R-
2.1.0/src/library/Recommended'
gmake: *** [stamp-recommended] Error 2


What could the problem be here?

Thanks, Matthew C.



From slist at oomvanlieshout.net  Tue May 17 16:14:27 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 17 May 2005 16:14:27 +0200
Subject: [R] adjusted p-values with TukeyHSD?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E846@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E846@usctmx1106.merck.com>
Message-ID: <4289FC43.4070804@oomvanlieshout.net>

Shame I can not get hold of Hsu, J. C. and M. Peruggia (1994) just now. 
I am quite curious to see what their graphs look like. Would you be able 
to give an example in R.....?  ;-)

The graph I put forward is typically used by ecologists to summarize 
data. It comes down to a simple means plot with error bars. Significant 
differences of multiple comparisons are then added using the letters a, 
b, c etc. If two bars have the same letter, they are not significantly 
different. It can become quite complicated when mean one is different 
from mean three but not from mean two and mean two is different from 
mean three but not mean one. You then get: a, ab, c for mean one, two 
and three respectively.

Of course what is often used does not constitute the best way of doing it.

Sander.



Liaw, Andy wrote:
>>From: Sander Oom
>>
>>Hi Chris and Chris,
>>
>>I was keeping my eye on this thread as I have also been discovering 
>>multiple comparisons recently. Your instructions are very 
>>clear! Thanks.
> 
> One thing to note, though:  Multcomp does not do Dunnett's or 
> Tukey's multiple comparisons per se.  Those names in multcomp 
> refer to the contrasts being used (comparison to a control for 
> Dunnett and all pairwise comparison for Tukey).  The actual 
> methods used are as described in the references of the help
> pages.
> 
>  
>>Now I would love to see an R boffin write a nifty function to 
>>produce a 
>>graphical representation of the multiple comparison, like this one:
>>
>>http://www.theses.ulaval.ca/2003/21026/21026024.jpg
>>
>>Should not be too difficult.....[any one up for the challenge?]
> 
> I beg to differ:  That's probably as bad a way as one can use to 
> graphically show multiple comparison.  The shaded bars serve no 
> purpose.
> 
> Two alternatives that I'm aware of are 
> 
> - Multiple comparison circles, due to John Sall, and not 
>   surprisingly, implemented in JMP and SAS/Insight.  See:
>  
> http://support.sas.com/documentation/onlinedoc/v7/whatsnew/insight/sect4.htm
> 
> 
> - The mean-mean display proposed by Hsu and Peruggia:
>   Hsu, J. C. and M. Peruggia (1994). 
>   Graphical representations of Tukey's multiple comparison method.
>   Journal of Computational and Graphical Statistics 3, 143{161
> 
> Andy
>  
>>I came across more multiple comparison info here;
>>
>>http://www.agr.kuleuven.ac.be/vakken/statisticsbyR/ANOVAbyRr/m
>>ultiplecomp.htm
>>
>>Cheers,
>>
>>Sander.
>>
>>Christoph Buser wrote:
>>>Dear Christoph
>>>
>>>You can use the multcomp package. Please have a look at the
>>>following example:
>>>
>>>library(multcomp)
>>>
>>>The first two lines were already proposed by Erin Hodgess:
>>>
>>>summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
>>>TukeyHSD(fm1, "tension", ordered = TRUE)
>>>
>>>    Tukey multiple comparisons of means
>>>    95% family-wise confidence level
>>>    factor levels have been ordered
>>> 
>>>Fit: aov(formula = breaks ~ wool + tension, data = warpbreaks)
>>>
>>>$tension
>>>         diff        lwr      upr
>>>M-H  4.722222 -4.6311985 14.07564
>>>L-H 14.722222  5.3688015 24.07564
>>>L-M 10.000000  0.6465793 19.35342
>>> 
>>>
>>>By using the functions simtest or simint you can get the
>>>p-values, too:
>>>
>>>summary(simtest(breaks ~ wool + tension, data = warpbreaks, 
>>whichf="tension",
>>>        type = "Tukey"))
>>>
>>>	 Simultaneous tests: Tukey contrasts 
>>>
>>>Call: 
>>>simtest.formula(formula = breaks ~ wool + tension, data = 
>>warpbreaks, 
>>>    whichf = "tension", type = "Tukey")
>>>
>>>	 Tukey contrasts for factor tension, covariable:  wool 
>>>
>>>Contrast matrix:
>>>                      tensionL tensionM tensionH
>>>tensionM-tensionL 0 0       -1        1        0
>>>tensionH-tensionL 0 0       -1        0        1
>>>tensionH-tensionM 0 0        0       -1        1
>>>
>>>
>>>Absolute Error Tolerance:  0.001 
>>>
>>>Coefficients:
>>>                  Estimate t value Std.Err. p raw p Bonf p adj
>>>tensionH-tensionL  -14.722  -3.802    3.872 0.000  0.001 0.001
>>>tensionM-tensionL  -10.000  -2.582    3.872 0.013  0.026 0.024
>>>tensionH-tensionM   -4.722  -1.219    3.872 0.228  0.228 0.228
>>>
>>>
>>>
>>>or if you prefer to get the confidence intervals, too, you can
>>>use:
>>>
>>>summary(simint(breaks ~ wool + tension, data = warpbreaks, 
>>whichf="tension",
>>>        type = "Tukey"))
>>>
>>>	Simultaneous 95% confidence intervals: Tukey contrasts
>>>
>>>Call: 
>>>simint.formula(formula = breaks ~ wool + tension, data = 
>>warpbreaks, 
>>>    whichf = "tension", type = "Tukey")
>>>
>>>	 Tukey contrasts for factor tension, covariable:  wool 
>>>
>>>Contrast matrix:
>>>                      tensionL tensionM tensionH
>>>tensionM-tensionL 0 0       -1        1        0
>>>tensionH-tensionL 0 0       -1        0        1
>>>tensionH-tensionM 0 0        0       -1        1
>>>
>>>Absolute Error Tolerance:  0.001 
>>>
>>> 95 % quantile:  2.415 
>>>
>>>Coefficients:
>>>                  Estimate   2.5 % 97.5 % t value Std.Err. 
>>p raw p Bonf p adj
>>>tensionM-tensionL  -10.000 -19.352 -0.648  -2.582    3.872 
>>0.013  0.038 0.034
>>>tensionH-tensionL  -14.722 -24.074 -5.370  -3.802    3.872 
>>0.000  0.001 0.001
>>>tensionH-tensionM   -4.722 -14.074  4.630  -1.219    3.872 
>>0.228  0.685 0.447
>>>-----------------------------------------------------------------
>>>Please be careful: The resulting confidence intervals in
>>>simint are not associated with the p-values from 'simtest' as it
>>>is described in the help page of the two functions.
>>>-----------------------------------------------------------------
>>>
>>>I had not the time to check the differences in the function or
>>>read the references given on the help page.
>>>If you are interested in the function you can check those to
>>>find out which one you prefer.
>>>
>>>Best regards,
>>>
>>>Christoph Buser
>>>
>>>--------------------------------------------------------------
>>>Christoph Buser <buser at stat.math.ethz.ch>
>>>Seminar fuer Statistik, LEO C13
>>>ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
>>>phone: x-41-44-632-4673		fax: 632-1228
>>>http://stat.ethz.ch/~buser/
>>>--------------------------------------------------------------
>>>
>>>
>>>Christoph Strehblow writes:
>>> > hi list,
>>> > 
>>> > i have to ask you again, having tried and searched for 
>>several days...
>>> > 
>>> > i want to do a TukeyHSD after an Anova, and want to get 
>>the adjusted  
>>> > p-values after the Tukey Correction.
>>> > i found the p.adjust function, but it can only correct 
>>for "holm",  
>>> > "hochberg", bonferroni", but not "Tukey".
>>> > 
>>> > Is it not possbile to get adjusted p-values after 
>>Tukey-correction?
>>> > 
>>> > sorry, if this is an often-answered-question, but i 
>>didn??t find it on  
>>> > the list archive...
>>> > 
>>> > thx a lot, list, Chris
>>> > 
>>> > 
>>> > Christoph Strehblow, MD
>>> > Department of Rheumatology, Diabetes and Endocrinology
>>> > Wilhelminenspital, Vienna, Austria
>>> > chrisxe at gmx.at
>>> > 
>>> > ______________________________________________
>>> > R-help at stat.math.ethz.ch mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>>
>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>-- 
>>
>>--------------------------------------------
>>Dr Sander P. Oom
>>Animal, Plant and Environmental Sciences,
>>University of the Witwatersrand
>>Private Bag 3, Wits 2050, South Africa
>>Tel (work)      +27 (0)11 717 64 04
>>Tel (home)      +27 (0)18 297 44 51
>>Fax             +27 (0)18 299 24 64
>>Email   sander at oomvanlieshout.net
>>Web     www.oomvanlieshout.net/sander
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From p.dalgaard at biostat.ku.dk  Tue May 17 16:14:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 May 2005 16:14:08 +0200
Subject: [R] installing R on Irix
In-Reply-To: <freemail.20050417154708.76688@fm11.freemail.hu>
References: <freemail.20050417154708.76688@fm11.freemail.hu>
Message-ID: <x2k6ly6itr.fsf@biostat.ku.dk>

Cserh??ti M??ty??s <cs_matyi at freemail.hu> writes:

> Hello veeryone, I nedd some help here.
> 
> The problem is I was trying to install R on my Irix system, with little 
> success: I got the following ugly error messages: watch out:
> 
> 
> begin installing recommended package mgcv
> Cannot create directory "": No such file or directory
> * Installing *source* package 'mgcv' ...
> ** libs
> gmake[3]: Entering directory `/tmp/R.INSTALL.13709658/mgcv/src'
> gcc -I/usr/home/csmatyi/programs/R/R-2.1.0/include  -
> I/usr/local/include     -g -O2 -c gcv.c -o gcv.o
> gcc -I/usr/home/csmatyi/programs/R/R-2.1.0/include  -
> I/usr/local/include     -g -O2 -c magic.c -o magic.o
> gcc -I/usr/home/csmatyi/programs/R/R-2.1.0/include  -
> I/usr/local/include     -g -O2 -c mat.c -o mat.o
> gcc -I/usr/home/csmatyi/programs/R/R-2.1.0/include  -
> I/usr/local/include     -g -O2 -c matrix.c -o matrix.o
> as: Error: /var/tmp/ccAomyDd.s, line 23679: register expected
>       dmtc1 244($sp),$f0
> as: Error: /var/tmp/ccAomyDd.s, line 23679: Undefined symbol: 244
> gmake[3]: *** [matrix.o] Error 1
> gmake[3]: Leaving directory `/tmp/R.INSTALL.13709658/mgcv/src'
> ERROR: compilation failed for package 'mgcv'
> ** Removing '/usr/home/csmatyi/programs/R/R-2.1.0/library/mgcv'
> gmake[2]: *** [mgcv.ts] Error 1
> gmake[2]: Leaving directory `/usr/home/csmatyi/programs/R/R-
> 2.1.0/src/library/Recommended'
> gmake[1]: *** [recommended-packages] Error 2
> gmake[1]: Leaving directory `/usr/home/csmatyi/programs/R/R-
> 2.1.0/src/library/Recommended'
> gmake: *** [stamp-recommended] Error 2
> 
> 
> What could the problem be here?

Offhand: You seem to be using the system assembler "as" on the output
of gcc. Sometimes one needs the GNU assembler ("gas") in which case
you likely need to install the GNU binutils. If that is the case, it
is quite surprising that you got that far, but stranger things have
happened... 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From valdar at well.ox.ac.uk  Tue May 17 16:22:07 2005
From: valdar at well.ox.ac.uk (William Valdar)
Date: Tue, 17 May 2005 15:22:07 +0100 (BST)
Subject: [R] setting value arg of pdSymm() in nlme
In-Reply-To: <4289E932.3030408@stat.wisc.edu>
References: <Pine.LNX.4.62.0505171031420.1532@octopus.well.ox.ac.uk>
	<4289E932.3030408@stat.wisc.edu>
Message-ID: <Pine.LNX.4.62.0505171411380.1532@octopus.well.ox.ac.uk>

Hi,

> I'm afraid that I don't understand what you are trying to do.  With a
> formula of ~ 1 the pdSymm generator creates a 1x1 variance-covariance
> matrix, which you are initializing to a 3x3 matrix.

Oh... I had a feeling I was doing something wrong there.

> What is batch.mat supposed to represent?

I would like to use batch.mat to specify a correlation structure for the 
batches A, B and C. Specifically, I wish to work out the contribution to 
the variance of the batch random effect, given that I know some pairs of 
batches (eg, A and B) are going to be more similar than other pairs (eg, A 
and C) and how similar they are likely to be. My (mis?)reading of P&B p165 
suggested this may be possible turning some of the pdIdents into 
pdSymms.

I was hoping to use this test example as a prelude to using genetic 
relationship data to impose a correlation structure on a subject-level or 
family-level effect. I understand from an earlier post (Jarrod Hadfield, 
2003) that lme is not really optimized for this, but I would nontheless 
like to evaluate to what extent it can do it anyway. The example used in 
that post was extreme: each case was effectively a different realization 
of a random effect where the correlation between levels is known. My needs 
are simpler: in the example I gave above, batches A, B and C might 
represent three families related by different degrees.

> Yes, although I think you mean lmer in lme4.  Because the lmer function
> allows multiple nested or non-nested grouping factors, the need for the
> pdMat classes is eliminated (or greatly reduced) and the code can be
> simplified considerably.  There is an article in the 2005/1 issue of R
> News describing the use of lmer.

Thanks for pointing this out as I had missed this article. I know lmer is 
under development and am very interested in what it can do. Having found 
lme/lmer useful for more standard problems I would like to use it for 
genetic-type analysis where possible rather than resort to a different 
language (eg SAS) or specialized proprietary software (eg, ASREML). 
However, I understand that may not be possible.

William

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Dr William Valdar               ++44 (0)1865 287 717
Wellcome Trust Centre           valdar at well.ox.ac.uk
for Human Genetics, Oxford      www.well.ox.ac.uk/~valdar



From sofyan.iyan at gmail.com  Tue May 17 16:53:01 2005
From: sofyan.iyan at gmail.com (Sofyan Iyan)
Date: Tue, 17 May 2005 16:53:01 +0200
Subject: [R] Combinations with two part column
Message-ID: <92e186a0050517075364eea7bf@mail.gmail.com>

Dear R-helpers,
I am a beginner using R.
This is the first question in this list.
My question, Is there possible to make combinations with two part column?
If I have a number 1,2,3,4,5,6,7,8. I need the result something like below:

1,2,3,4,5     6,7,8
1,2,3,4,7     5,6,8
2,3,4,5,6     1,7,8
1,2,3,6,7     4,5,8
1,2,3,4,8     5,6,7
3,4,6,7,8     1,2,5
....

I  would be very happy if anyone could help me.
Best regards, 

Sofyan



From tura at centroin.com.br  Tue May 17 17:11:06 2005
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Tue, 17 May 2005 12:11:06 -0300
Subject: [R] Problem in lme longitudinal model
Message-ID: <6.1.2.0.2.20050517115743.04b2beb0@centroin.com.br>

Hi R-masters!

I trying model Heart disease mortality in my country with a lme model like 
this:

m1.lme<-lme(log(rdeath)~age*year,random=~age|year,data=dados)

where: rdeath is rate of mortality per 100000 person per age and year
            age: age of death (22 27 32 37 42 47 52 57 62 67 72 77 82)
            year: year of death (1980:2002)

I don?t have problem to fit  the model, but in residual analysis I have one 
problem.

If i type acf(m1.lme$residuals) the graph show 4 plot with a sin curve (a 
example in attach) and I get this variogram:

 > Variogram(m1.lme)
       variog dist n.pairs
1  0.3939065    1     276
2  0.6486452    2     253
3  0.9679870    3     230
4  1.2765094    4     207
5  1.4158147    5     184
6  1.4264685    6     161
7  1.4048608    7     138
8  1.1902613    8     115
9  0.9619330    9      92
10 0.7037427   10      69
11 0.8591257   11      46
12 1.2215657   12      23

Well
I need help for solution this problem....
Somebody can help me?


Thanks in advance

Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil 
-------------- next part --------------


No virus found in this outgoing message.
Checked by AVG Anti-Virus.


From Scott.Waichler at pnl.gov  Tue May 17 17:52:56 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Tue, 17 May 2005 08:52:56 -0700
Subject: [R] Omitting NAs in aggregate.ts()
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01F39C59@pnlmse35.pnl.gov>

> > I have a time series vector (not necessarily ts class) that has NAs
in it.
> > How can I omit the NAs when using aggregate.ts() to compute a
function 
> > on each window?  If there is at least one non-NA value in each
window, 
> > I'd like to proceed with evaluating the function; otherwise, I would

> > like NA returned.  I'm not wedded to aggregate.ts and don't need ts 
> > class if there is another fast, vectorized way to handle this.  Here

> > is what I am trying to do, with the invalid na.rm thrown in:
> > 
> > as.vector(aggregate.ts(x, ndeltat=24, FUN=min, na.rm=F))
> > 

> I don't know is the short answer, but if I had the data I 
> might have tried mymin <- function(x) min(x, na.rm = TRUE) 
> as.vector(aggregate.ts(x, ndeltat=24, FUN=mymin))


Thanks for the suggestions, and my apologies for not providing an
example and error messages.  Using a custom function for FUN in
aggregate.ts() is the way to go.  Here is a complete solution to my
problem:

> e <- c(2.3, 4.5, 6.2, 1.8)
> f <- c(2.3, NA, NA, NA)
>
> mymin <- function(x) {
+   if(length(x[!is.na(x)]) > 0) return(min(x, na.rm = TRUE))
+   else return(NA)
+ }
>
> as.vector(aggregate.ts(e, ndeltat=2, FUN=mymin))
[1] 2.3 1.8
> as.vector(aggregate.ts(f, ndeltat=2, FUN=mymin))
[1] 2.3  NA
>           

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From erithid at bellsouth.net  Tue May 17 17:56:10 2005
From: erithid at bellsouth.net (BJ)
Date: Tue, 17 May 2005 11:56:10 -0400
Subject: [R] simple question, i hope
Message-ID: <428A141A.10005@bellsouth.net>

How do you output a list to a text file without the extra line numbers 
and stuff?

I have a list b, and tried

 zz<-textConnection("captest.txt","w")
sink(zz)
b
sink()
close(zz)

but that isnt what i want, because i get [[1]]
                                                        [1] a

etc. Is there a simple way to do the R equivalent of this perl code?

open(OUT,">out.txt");
print OUT @b;
close OUT


Thank you for your help. I tried pouring over teh documentation for 
this, but couldnt find what I was lookign for. ~Erithid



From wuming.gong at gmail.com  Tue May 17 18:16:33 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Wed, 18 May 2005 00:16:33 +0800
Subject: [R] simple question, i hope
In-Reply-To: <428A141A.10005@bellsouth.net>
References: <428A141A.10005@bellsouth.net>
Message-ID: <b428d06d0505170916193fb6ea@mail.gmail.com>

See ?write.table

Wuming

On 5/17/05, BJ <erithid at bellsouth.net> wrote:
> How do you output a list to a text file without the extra line numbers
> and stuff?
> 
> I have a list b, and tried
> 
>  zz<-textConnection("captest.txt","w")
> sink(zz)
> b
> sink()
> close(zz)
> 
> but that isnt what i want, because i get [[1]]
>                                                         [1] a
> 
> etc. Is there a simple way to do the R equivalent of this perl code?
> 
> open(OUT,">out.txt");
> print OUT @b;
> close OUT
> 
> Thank you for your help. I tried pouring over teh documentation for
> this, but couldnt find what I was lookign for. ~Erithid
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue May 17 18:50:06 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 17 May 2005 12:50:06 -0400
Subject: [R] adjusted p-values with TukeyHSD?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E849@usctmx1106.merck.com>

> From: Sander Oom 
> 
> Shame I can not get hold of Hsu, J. C. and M. Peruggia (1994) 
> just now. 
> I am quite curious to see what their graphs look like. Would 
> you be able 
> to give an example in R.....?  ;-)

It's on the cover of Prof. Hsu's book on multiple comparisons.  The
new book by Heiberger and Holland also uses it.  The files that
support that book can be downloaded from springeronline.com, 
search for the authors' names.  It comes in one .tar.gz file.
You can see mmc1.eps and mmc2.eps in hh/mcomp/figure to get
some idea of what it looks like.

 
> The graph I put forward is typically used by ecologists to summarize 
> data. It comes down to a simple means plot with error bars. 
> Significant 
> differences of multiple comparisons are then added using the 
> letters a, 
> b, c etc. If two bars have the same letter, they are not 
> significantly 
> different. It can become quite complicated when mean one is different 
> from mean three but not from mean two and mean two is different from 
> mean three but not mean one. You then get: a, ab, c for mean one, two 
> and three respectively.
> 
> Of course what is often used does not constitute the best way 
> of doing it.

The bar plot with error bars are all over scientific journals, which
is quite a pity.  The use of letters or underlines to show groups
that are not significantly different is awkward, and do not work
when there are unequal sample sizes among groups.

Andy

 
> Sander.
> 
> 
> 
> Liaw, Andy wrote:
> >>From: Sander Oom
> >>
> >>Hi Chris and Chris,
> >>
> >>I was keeping my eye on this thread as I have also been discovering 
> >>multiple comparisons recently. Your instructions are very 
> >>clear! Thanks.
> > 
> > One thing to note, though:  Multcomp does not do Dunnett's or 
> > Tukey's multiple comparisons per se.  Those names in multcomp 
> > refer to the contrasts being used (comparison to a control for 
> > Dunnett and all pairwise comparison for Tukey).  The actual 
> > methods used are as described in the references of the help
> > pages.
> > 
> >  
> >>Now I would love to see an R boffin write a nifty function to 
> >>produce a 
> >>graphical representation of the multiple comparison, like this one:
> >>
> >>http://www.theses.ulaval.ca/2003/21026/21026024.jpg
> >>
> >>Should not be too difficult.....[any one up for the challenge?]
> > 
> > I beg to differ:  That's probably as bad a way as one can use to 
> > graphically show multiple comparison.  The shaded bars serve no 
> > purpose.
> > 
> > Two alternatives that I'm aware of are 
> > 
> > - Multiple comparison circles, due to John Sall, and not 
> >   surprisingly, implemented in JMP and SAS/Insight.  See:
> >  
> > 
> http://support.sas.com/documentation/onlinedoc/v7/whatsnew/ins
> ight/sect4.htm
> > 
> > 
> > - The mean-mean display proposed by Hsu and Peruggia:
> >   Hsu, J. C. and M. Peruggia (1994). 
> >   Graphical representations of Tukey's multiple comparison method.
> >   Journal of Computational and Graphical Statistics 3, 143{161
> > 
> > Andy
> >  
> >>I came across more multiple comparison info here;
> >>
> >>http://www.agr.kuleuven.ac.be/vakken/statisticsbyR/ANOVAbyRr/m
> >>ultiplecomp.htm
> >>
> >>Cheers,
> >>
> >>Sander.
> >>
> >>Christoph Buser wrote:
> >>>Dear Christoph
> >>>
> >>>You can use the multcomp package. Please have a look at the
> >>>following example:
> >>>
> >>>library(multcomp)
> >>>
> >>>The first two lines were already proposed by Erin Hodgess:
> >>>
> >>>summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
> >>>TukeyHSD(fm1, "tension", ordered = TRUE)
> >>>
> >>>    Tukey multiple comparisons of means
> >>>    95% family-wise confidence level
> >>>    factor levels have been ordered
> >>> 
> >>>Fit: aov(formula = breaks ~ wool + tension, data = warpbreaks)
> >>>
> >>>$tension
> >>>         diff        lwr      upr
> >>>M-H  4.722222 -4.6311985 14.07564
> >>>L-H 14.722222  5.3688015 24.07564
> >>>L-M 10.000000  0.6465793 19.35342
> >>> 
> >>>
> >>>By using the functions simtest or simint you can get the
> >>>p-values, too:
> >>>
> >>>summary(simtest(breaks ~ wool + tension, data = warpbreaks, 
> >>whichf="tension",
> >>>        type = "Tukey"))
> >>>
> >>>	 Simultaneous tests: Tukey contrasts 
> >>>
> >>>Call: 
> >>>simtest.formula(formula = breaks ~ wool + tension, data = 
> >>warpbreaks, 
> >>>    whichf = "tension", type = "Tukey")
> >>>
> >>>	 Tukey contrasts for factor tension, covariable:  wool 
> >>>
> >>>Contrast matrix:
> >>>                      tensionL tensionM tensionH
> >>>tensionM-tensionL 0 0       -1        1        0
> >>>tensionH-tensionL 0 0       -1        0        1
> >>>tensionH-tensionM 0 0        0       -1        1
> >>>
> >>>
> >>>Absolute Error Tolerance:  0.001 
> >>>
> >>>Coefficients:
> >>>                  Estimate t value Std.Err. p raw p Bonf p adj
> >>>tensionH-tensionL  -14.722  -3.802    3.872 0.000  0.001 0.001
> >>>tensionM-tensionL  -10.000  -2.582    3.872 0.013  0.026 0.024
> >>>tensionH-tensionM   -4.722  -1.219    3.872 0.228  0.228 0.228
> >>>
> >>>
> >>>
> >>>or if you prefer to get the confidence intervals, too, you can
> >>>use:
> >>>
> >>>summary(simint(breaks ~ wool + tension, data = warpbreaks, 
> >>whichf="tension",
> >>>        type = "Tukey"))
> >>>
> >>>	Simultaneous 95% confidence intervals: Tukey contrasts
> >>>
> >>>Call: 
> >>>simint.formula(formula = breaks ~ wool + tension, data = 
> >>warpbreaks, 
> >>>    whichf = "tension", type = "Tukey")
> >>>
> >>>	 Tukey contrasts for factor tension, covariable:  wool 
> >>>
> >>>Contrast matrix:
> >>>                      tensionL tensionM tensionH
> >>>tensionM-tensionL 0 0       -1        1        0
> >>>tensionH-tensionL 0 0       -1        0        1
> >>>tensionH-tensionM 0 0        0       -1        1
> >>>
> >>>Absolute Error Tolerance:  0.001 
> >>>
> >>> 95 % quantile:  2.415 
> >>>
> >>>Coefficients:
> >>>                  Estimate   2.5 % 97.5 % t value Std.Err. 
> >>p raw p Bonf p adj
> >>>tensionM-tensionL  -10.000 -19.352 -0.648  -2.582    3.872 
> >>0.013  0.038 0.034
> >>>tensionH-tensionL  -14.722 -24.074 -5.370  -3.802    3.872 
> >>0.000  0.001 0.001
> >>>tensionH-tensionM   -4.722 -14.074  4.630  -1.219    3.872 
> >>0.228  0.685 0.447
> >>>-----------------------------------------------------------------
> >>>Please be careful: The resulting confidence intervals in
> >>>simint are not associated with the p-values from 'simtest' as it
> >>>is described in the help page of the two functions.
> >>>-----------------------------------------------------------------
> >>>
> >>>I had not the time to check the differences in the function or
> >>>read the references given on the help page.
> >>>If you are interested in the function you can check those to
> >>>find out which one you prefer.
> >>>
> >>>Best regards,
> >>>
> >>>Christoph Buser
> >>>
> >>>--------------------------------------------------------------
> >>>Christoph Buser <buser at stat.math.ethz.ch>
> >>>Seminar fuer Statistik, LEO C13
> >>>ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> >>>phone: x-41-44-632-4673		fax: 632-1228
> >>>http://stat.ethz.ch/~buser/
> >>>--------------------------------------------------------------
> >>>
> >>>
> >>>Christoph Strehblow writes:
> >>> > hi list,
> >>> > 
> >>> > i have to ask you again, having tried and searched for 
> >>several days...
> >>> > 
> >>> > i want to do a TukeyHSD after an Anova, and want to get 
> >>the adjusted  
> >>> > p-values after the Tukey Correction.
> >>> > i found the p.adjust function, but it can only correct 
> >>for "holm",  
> >>> > "hochberg", bonferroni", but not "Tukey".
> >>> > 
> >>> > Is it not possbile to get adjusted p-values after 
> >>Tukey-correction?
> >>> > 
> >>> > sorry, if this is an often-answered-question, but i 
> >>didn??t find it on  
> >>> > the list archive...
> >>> > 
> >>> > thx a lot, list, Chris
> >>> > 
> >>> > 
> >>> > Christoph Strehblow, MD
> >>> > Department of Rheumatology, Diabetes and Endocrinology
> >>> > Wilhelminenspital, Vienna, Austria
> >>> > chrisxe at gmx.at
> >>> > 
> >>> > ______________________________________________
> >>> > R-help at stat.math.ethz.ch mailing list
> >>> > https://stat.ethz.ch/mailman/listinfo/r-help
> >>> > PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>>
> >>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>
> >>-- 
> >>
> >>--------------------------------------------
> >>Dr Sander P. Oom
> >>Animal, Plant and Environmental Sciences,
> >>University of the Witwatersrand
> >>Private Bag 3, Wits 2050, South Africa
> >>Tel (work)      +27 (0)11 717 64 04
> >>Tel (home)      +27 (0)18 297 44 51
> >>Fax             +27 (0)18 299 24 64
> >>Email   sander at oomvanlieshout.net
> >>Web     www.oomvanlieshout.net/sander
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>
> >>
> >>
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> 
> -- 
> 
> --------------------------------------------
> Dr Sander P. Oom
> Animal, Plant and Environmental Sciences,
> University of the Witwatersrand
> Private Bag 3, Wits 2050, South Africa
> Tel (work)      +27 (0)11 717 64 04
> Tel (home)      +27 (0)18 297 44 51
> Fax             +27 (0)18 299 24 64
> Email   sander at oomvanlieshout.net
> Web     www.oomvanlieshout.net/sander
> ---------------------------------------------
> 
> 
>



From ggrothendieck at gmail.com  Tue May 17 19:00:18 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 17 May 2005 13:00:18 -0400
Subject: [R] Combinations with two part column
In-Reply-To: <92e186a0050517075364eea7bf@mail.gmail.com>
References: <92e186a0050517075364eea7bf@mail.gmail.com>
Message-ID: <971536df050517100069713587@mail.gmail.com>

On 5/17/05, Sofyan Iyan <sofyan.iyan at gmail.com> wrote:
> Dear R-helpers,
> I am a beginner using R.
> This is the first question in this list.
> My question, Is there possible to make combinations with two part column?
> If I have a number 1,2,3,4,5,6,7,8. I need the result something like below:
> 
> 1,2,3,4,5     6,7,8
> 1,2,3,4,7     5,6,8
> 2,3,4,5,6     1,7,8
> 1,2,3,6,7     4,5,8
> 1,2,3,4,8     5,6,7
> 3,4,6,7,8     1,2,5
> ....
> 

Try this:

library(gtools)
t(apply(combinations(8,5), 1, function(x) c(x,setdiff(1:8, x))))



From helprhelp at gmail.com  Tue May 17 20:09:33 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 17 May 2005 13:09:33 -0500
Subject: [R] predict() question
Message-ID: <cdf8178305051711092bffb3b8@mail.gmail.com>

Hi, there:
Following yesterday's question ( i had a new level for a categorical
variable occurred in validation dataset and predict() complains about
it: i made some python code to solve the problem), but here, I am just
curious about some details about the mechanism:

I believed rpart follows CART and for a categorical variable, the
splitting criteria should be like,
is it A or not?
   --yes, go to left branch
   --no, go to right

So, when you predict, if you have a new level C,for example,
the predict() should not complain about the occurrence of "C" (of
course, if there are many "C"'s in validation, it should complain).
Maybe for robustness, predict() has to check first if there is new
level or not.

I am not sure if my understanding is right or not, please be advised!

Thanks,

-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From brising at louisville.edu  Tue May 17 20:31:53 2005
From: brising at louisville.edu (Bill Rising)
Date: Tue, 17 May 2005 14:31:53 -0400
Subject: [R] Sweave and paths
Message-ID: <80A12FE7-1B51-4ACE-B76C-38B2F0174A02@louisville.edu>

Is there some way to encourage \SweaveInput{foo} to find foo in a  
subdirectory of a file tree? Something along the lines of the  
behavior of list.files(<stuff>, recursive=TRUE). This would be very  
helpful at calling small modular files, such as solution sets and the  
like.

I couldn't see anything in the documentation, and I looked in the  
source code, but it seems that SweaveReadFiledoc() wants to look only  
in the directory which contains foo.

Still, I'm guessing that I'm missing something. Any tips would be  
much appreciated.

Bill



From pedro_ts at math.uprm.edu  Tue May 17 21:16:35 2005
From: pedro_ts at math.uprm.edu (Pedro Torres Saavedra)
Date: Tue, 17 May 2005 16:16:35 -0300
Subject: [R] Error message glmmPQL
Message-ID: <20050517191153.M75404@math.uprm.edu>

Hi,

I'm fitting a model for two-nested binay data in glmmPQL function but I have 
this error message: "Error in solve.default(estimates[dimE[1] - (p:1), dimE
[2] - (p:1), drop = FALSE]) : system is computationally singular: reciprocal 
condition number = 1.14416e-018".

How do I can solve this problem? Or the problem are the data?

Thanks,


Pedro A. Torres S.
Graduate Student - Statistics
Department of Mathematics
University of Puerto Rico at Mayaguez
(1 787) 832-4040 X 2537



From sofyan.iyan at gmail.com  Tue May 17 22:24:24 2005
From: sofyan.iyan at gmail.com (Sofyan Iyan)
Date: Tue, 17 May 2005 22:24:24 +0200
Subject: [R] Combinations with two part column
In-Reply-To: <971536df050517100069713587@mail.gmail.com>
References: <92e186a0050517075364eea7bf@mail.gmail.com>
	<971536df050517100069713587@mail.gmail.com>
Message-ID: <92e186a0050517132460155ed8@mail.gmail.com>

Thanks for you quick answer.
Could I extend my question?
How to make the result for each rows with comma ",";
> library(gtools)
> comb8.5 <- t(apply(combinations(8,5), 1, function(x) c(x,setdiff(1:8, x))))
> comb8.5[,1:5]
      [,1] [,2] [,3] [,4] [,5]
 [1,]    1    2    3    4    5
 [2,]    1    2    3    4    6
 [3,]    1    2    3    4    7
 [4,]    1    2    3    4    8
 [5,]    1    2    3    5    6
 [6,]    1    2    3    5    7
...

I mean like:
    1,    2,    3,    4,    5
    1,    2,    3,    4,    6
    1,    2,    3,    4,    7
    1,    2,    3,    4,    8
    1,    2,    3,    5,    6
    1,    2,    3,    5,    7

> comb8.5[,6:8]
      [,1] [,2] [,3]
 [1,]    6    7    8
 [2,]    5    7    8
 [3,]    5    6    8
 [4,]    5    6    7
 [5,]    4    7    8
 [6,]    4    6    8
...

this below like:
     6,    7,    8
     5,    7,    8
     5,    6,    8
     5,    6,    7
     4,    7,    8
     4,    6,    8

Best,
Sofyan


On 5/17/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 5/17/05, Sofyan Iyan <sofyan.iyan at gmail.com> wrote:
> > Dear R-helpers,
> > I am a beginner using R.
> > This is the first question in this list.
> > My question, Is there possible to make combinations with two part column?
> > If I have a number 1,2,3,4,5,6,7,8. I need the result something like below:
> >
> > 1,2,3,4,5     6,7,8
> > 1,2,3,4,7     5,6,8
> > 2,3,4,5,6     1,7,8
> > 1,2,3,6,7     4,5,8
> > 1,2,3,4,8     5,6,7
> > 3,4,6,7,8     1,2,5
> > ....
> >
> 
> Try this:
> 
> library(gtools)
> t(apply(combinations(8,5), 1, function(x) c(x,setdiff(1:8, x))))
>



From ggrothendieck at gmail.com  Tue May 17 22:53:14 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 17 May 2005 16:53:14 -0400
Subject: [R] Combinations with two part column
In-Reply-To: <92e186a0050517132460155ed8@mail.gmail.com>
References: <92e186a0050517075364eea7bf@mail.gmail.com>
	<971536df050517100069713587@mail.gmail.com>
	<92e186a0050517132460155ed8@mail.gmail.com>
Message-ID: <971536df050517135324a2baa6@mail.gmail.com>

Try write.table:

write.table(comb8.5[,1:5], sep = ",", row.names = FALSE, col.names = FALSE)
write.table(comb8.5[,6:8], sep = ",", row.names = FALSE, col.names = FALSE)


On 5/17/05, Sofyan Iyan <sofyan.iyan at gmail.com> wrote:
> Thanks for you quick answer.
> Could I extend my question?
> How to make the result for each rows with comma ",";
> > library(gtools)
> > comb8.5 <- t(apply(combinations(8,5), 1, function(x) c(x,setdiff(1:8, x))))
> > comb8.5[,1:5]
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    2    3    4    5
> [2,]    1    2    3    4    6
> [3,]    1    2    3    4    7
> [4,]    1    2    3    4    8
> [5,]    1    2    3    5    6
> [6,]    1    2    3    5    7
> ...
> 
> I mean like:
>    1,    2,    3,    4,    5
>    1,    2,    3,    4,    6
>    1,    2,    3,    4,    7
>    1,    2,    3,    4,    8
>    1,    2,    3,    5,    6
>    1,    2,    3,    5,    7
> 
> > comb8.5[,6:8]
>      [,1] [,2] [,3]
> [1,]    6    7    8
> [2,]    5    7    8
> [3,]    5    6    8
> [4,]    5    6    7
> [5,]    4    7    8
> [6,]    4    6    8
> ...
> 
> this below like:
>     6,    7,    8
>     5,    7,    8
>     5,    6,    8
>     5,    6,    7
>     4,    7,    8
>     4,    6,    8
> 
> Best,
> Sofyan
> 
> 
> On 5/17/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 5/17/05, Sofyan Iyan <sofyan.iyan at gmail.com> wrote:
> > > Dear R-helpers,
> > > I am a beginner using R.
> > > This is the first question in this list.
> > > My question, Is there possible to make combinations with two part column?
> > > If I have a number 1,2,3,4,5,6,7,8. I need the result something like below:
> > >
> > > 1,2,3,4,5     6,7,8
> > > 1,2,3,4,7     5,6,8
> > > 2,3,4,5,6     1,7,8
> > > 1,2,3,6,7     4,5,8
> > > 1,2,3,4,8     5,6,7
> > > 3,4,6,7,8     1,2,5
> > > ....
> > >
> >
> > Try this:
> >
> > library(gtools)
> > t(apply(combinations(8,5), 1, function(x) c(x,setdiff(1:8, x))))
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sluque at mun.ca  Wed May 18 00:49:18 2005
From: sluque at mun.ca (Sebastian Luque)
Date: Tue, 17 May 2005 17:49:18 -0500
Subject: [R] cumsum on chron objects
Message-ID: <874qd1sc29.fsf@mun.ca>

Hi,

Is there some alternative to cumsum for chron objects? I have data frames
that contain some chron objects that look like this:

DateTime
13/10/03 12:30:35
NA
NA
NA
15/10/03 16:30:05
NA
NA
...


and I've been trying to replace the NA's so that a date/time sequence is
created starting with the preceding available value. Because the number of
rows with NA's following each available date/time is unknown, I've split
the data frame using:

splitdf <- split(df, as.factor(df$DateTime))

so that I can later use lapply to work on each "block" of data. I thought
I could use cumsum and set the NA's to the desired interval to create the
date/time sequence starting with the first row. However, this function is
not defined for chron objects. Does anybody know of alternatives to create
such a sequence?

Thanks in advance,
-- 
Sebastian P. Luque



From aorchid at mac.com  Wed May 18 01:35:30 2005
From: aorchid at mac.com (Aric Gregson)
Date: Tue, 17 May 2005 19:35:30 -0400
Subject: [R] Hmisc/Design and problem with cgroup
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B6FF5D@pollux.bfro.uni-lj.si>
Message-ID: <r02010500-1039-5B331D3EC72C11D9B5AC000A959B3D22@[10.0.1.205]>

Hello,

I am trying to use the following to output a table to latex:

cohortbyagesummary <- by(data.frame(age,ethnicity), cohort, summary) 

w <- latex.default(cohortbyagesummary, 
    caption="Five Number Age Summaries by Cohort",
    label="agesummarybycohort", 
    cgroup=c('hello','goodbye','hello'),
    colheads=c("Age","Ethnicity"),
    extracolheads=c('hello','goodbye'), # demonstration of subheadings
    greek=TRUE,
    ctable=TRUE)
    
I am not able to get the major column headings of cgroup to work. I
receive the error:
    Object cline not found
    
I do not see in the examples or documentation that you must specify
cline or anything. 

Any suggestions? Thanks very much,

aric

R 2.0.1
Design 2.0-11
Hmisc  3.0-5
OS X 10.3.9



From ggrothendieck at gmail.com  Wed May 18 01:39:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 17 May 2005 19:39:30 -0400
Subject: [R] cumsum on chron objects
In-Reply-To: <971536df05051716349163b0b@mail.gmail.com>
References: <874qd1sc29.fsf@mun.ca> <971536df05051716349163b0b@mail.gmail.com>
Message-ID: <971536df05051716391ef14437@mail.gmail.com>

On 5/17/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 5/17/05, Sebastian Luque <sluque at mun.ca> wrote:
> > Hi,
> >
> > Is there some alternative to cumsum for chron objects? I have data frames
> > that contain some chron objects that look like this:
> >
> > DateTime
> > 13/10/03 12:30:35
> > NA
> > NA
> > NA
> > 15/10/03 16:30:05
> > NA
> > NA
> > ...
> >
> > and I've been trying to replace the NA's so that a date/time sequence is
> > created starting with the preceding available value. Because the number of
> > rows with NA's following each available date/time is unknown, I've split
> > the data frame using:
> >
> > splitdf <- split(df, as.factor(df$DateTime))
> >
> > so that I can later use lapply to work on each "block" of data. I thought
> > I could use cumsum and set the NA's to the desired interval to create the
> > date/time sequence starting with the first row. However, this function is
> > not defined for chron objects. Does anybody know of alternatives to create
> > such a sequence?
> >
> 
> The 'zoo' package has na.locf which stands for Last Occurrence Carried
> Forward, which is what I believe you want.
> 
> First let us create some test data, x:
> 
> > library(chron); library(zoo)
> > x <- chron(c(1.5, 2, NA, NA, 4, NA))
> > x
> [1] (01/02/70 12:00:00) (01/03/70 00:00:00) (NA NA)
> [4] (NA NA)             (01/05/70 00:00:00) (NA NA)
> 
> > # na.locf is intended for zoo objects but we can convert
> > # the chron object to zoo, apply na.locf and convert back:
> 
> > chron(as.vector(na.locf(zoo(as.vector(x)))))
> [1] (01/02/70 12:00:00) (01/03/70 00:00:00) (01/03/70 00:00:00)
> [4] (01/03/70 00:00:00) (01/05/70 00:00:00) (01/05/70 00:00:00)
> 

Just to reply to my own post, it can actually be done even more
simply:

chron(na.locf(as.vector(x)))

Also in re-reading my post, I think the O in locf stands for observation 
rather than occurrence.



From sofyan.iyan at gmail.com  Wed May 18 01:48:00 2005
From: sofyan.iyan at gmail.com (Sofyan Iyan)
Date: Wed, 18 May 2005 01:48:00 +0200
Subject: [R] Combinations with two part column
In-Reply-To: <971536df050517135324a2baa6@mail.gmail.com>
References: <92e186a0050517075364eea7bf@mail.gmail.com>
	<971536df050517100069713587@mail.gmail.com>
	<92e186a0050517132460155ed8@mail.gmail.com>
	<971536df050517135324a2baa6@mail.gmail.com>
Message-ID: <92e186a005051716482b39e689@mail.gmail.com>

Dear to Gabor Grothendieck and James Holtman,

Thank you for giving me so much of your time to solve my problem.  
Many thanks and best regards,
Sofyan

On 5/17/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try write.table:
> 
> write.table(comb8.5[,1:5], sep = ",", row.names = FALSE, col.names = FALSE)
> write.table(comb8.5[,6:8], sep = ",", row.names = FALSE, col.names = FALSE)
> 
> 
> On 5/17/05, Sofyan Iyan <sofyan.iyan at gmail.com> wrote:
> > Thanks for you quick answer.
> > Could I extend my question?
> > How to make the result for each rows with comma ",";
> > > library(gtools)
> > > comb8.5 <- t(apply(combinations(8,5), 1, function(x) c(x,setdiff(1:8, x))))
> > > comb8.5[,1:5]
> >      [,1] [,2] [,3] [,4] [,5]
> > [1,]    1    2    3    4    5
> > [2,]    1    2    3    4    6
> > [3,]    1    2    3    4    7
> > [4,]    1    2    3    4    8
> > [5,]    1    2    3    5    6
> > [6,]    1    2    3    5    7
> > ...
> >
> > I mean like:
> >    1,    2,    3,    4,    5
> >    1,    2,    3,    4,    6
> >    1,    2,    3,    4,    7
> >    1,    2,    3,    4,    8
> >    1,    2,    3,    5,    6
> >    1,    2,    3,    5,    7
> >
> > > comb8.5[,6:8]
> >      [,1] [,2] [,3]
> > [1,]    6    7    8
> > [2,]    5    7    8
> > [3,]    5    6    8
> > [4,]    5    6    7
> > [5,]    4    7    8
> > [6,]    4    6    8
> > ...
> >
> > this below like:
> >     6,    7,    8
> >     5,    7,    8
> >     5,    6,    8
> >     5,    6,    7
> >     4,    7,    8
> >     4,    6,    8
> >
> > Best,
> > Sofyan
> >
> >
> > On 5/17/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > On 5/17/05, Sofyan Iyan <sofyan.iyan at gmail.com> wrote:
> > > > Dear R-helpers,
> > > > I am a beginner using R.
> > > > This is the first question in this list.
> > > > My question, Is there possible to make combinations with two part column?
> > > > If I have a number 1,2,3,4,5,6,7,8. I need the result something like below:
> > > >
> > > > 1,2,3,4,5     6,7,8
> > > > 1,2,3,4,7     5,6,8
> > > > 2,3,4,5,6     1,7,8
> > > > 1,2,3,6,7     4,5,8
> > > > 1,2,3,4,8     5,6,7
> > > > 3,4,6,7,8     1,2,5
> > > > ....
> > > >
> > >
> > > Try this:
> > >
> > > library(gtools)
> > > t(apply(combinations(8,5), 1, function(x) c(x,setdiff(1:8, x))))
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From ggrothendieck at gmail.com  Wed May 18 01:34:08 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 17 May 2005 19:34:08 -0400
Subject: [R] cumsum on chron objects
In-Reply-To: <874qd1sc29.fsf@mun.ca>
References: <874qd1sc29.fsf@mun.ca>
Message-ID: <971536df05051716349163b0b@mail.gmail.com>

On 5/17/05, Sebastian Luque <sluque at mun.ca> wrote:
> Hi,
> 
> Is there some alternative to cumsum for chron objects? I have data frames
> that contain some chron objects that look like this:
> 
> DateTime
> 13/10/03 12:30:35
> NA
> NA
> NA
> 15/10/03 16:30:05
> NA
> NA
> ...
> 
> and I've been trying to replace the NA's so that a date/time sequence is
> created starting with the preceding available value. Because the number of
> rows with NA's following each available date/time is unknown, I've split
> the data frame using:
> 
> splitdf <- split(df, as.factor(df$DateTime))
> 
> so that I can later use lapply to work on each "block" of data. I thought
> I could use cumsum and set the NA's to the desired interval to create the
> date/time sequence starting with the first row. However, this function is
> not defined for chron objects. Does anybody know of alternatives to create
> such a sequence?
> 


The 'zoo' package has na.locf which stands for Last Occurrence Carried
Forward, which is what I believe you want.   

First let us create some test data, x:

> library(chron); library(zoo)
> x <- chron(c(1.5, 2, NA, NA, 4, NA))  
> x
[1] (01/02/70 12:00:00) (01/03/70 00:00:00) (NA NA)            
[4] (NA NA)             (01/05/70 00:00:00) (NA NA)            


> # na.locf is intended for zoo objects but we can convert
> # the chron object to zoo, apply na.locf and convert back:

> chron(as.vector(na.locf(zoo(as.vector(x)))))
[1] (01/02/70 12:00:00) (01/03/70 00:00:00) (01/03/70 00:00:00)
[4] (01/03/70 00:00:00) (01/05/70 00:00:00) (01/05/70 00:00:00)



From weigand.stephen at charter.net  Wed May 18 03:25:00 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Tue, 17 May 2005 20:25:00 -0500
Subject: [R] Centered overall title with layout()
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D06B63176@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D06B63176@lbmsg002.fbn-nbf.local>
Message-ID: <ac9be8e9062a7c8983ea36106cfa3c7e@charter.net>

Dear Pierre,

On May 15, 2005, at 6:36 PM, Lapointe, Pierre wrote:

> Hello,
>
> I would like to have a centered overall title for a graphics page 
> using the
> layout() function.
>
> Example, using this function:
>
> z <- layout(matrix(c(1:6), 3,2, byrow = TRUE))
> layout.show(6)
>
> I'd like to get this:
>
>
>       Centered Overall Title
> --------------------------------
> |               |               |
> |               |               |
> |               |               |
> |               |               |
> |               |               |
> --------------------------------
> |               |               |
> |               |               |
> |               |               |
> |               |               |
> |               |               |
> --------------------------------
> |               |               |
> |               |               |
> |               |               |
> |               |               |
> |               |               |
> --------------------------------
>
> I really want to use layout(), not par(mfrow())
>
> Thanks
>
>
> Pierre Lapointe

Does mtext give you what you want? E.g.,

par(oma = c(0, 0, 3, 0))
z <- layout(matrix(c(1:6), 3,2, byrow = TRUE))
layout.show(6)
mtext("Centered Overall Title", side = 3, line = 1, outer = TRUE)

Hope this helps,
Stephen



From sluque at mun.ca  Wed May 18 03:49:24 2005
From: sluque at mun.ca (Sebastian Luque)
Date: Tue, 17 May 2005 20:49:24 -0500
Subject: [R] cumsum on chron objects
References: <874qd1sc29.fsf@mun.ca> <971536df05051716349163b0b@mail.gmail.com>
	<971536df05051716391ef14437@mail.gmail.com>
Message-ID: <87zmutqp5n.fsf@mun.ca>

Hello Gabor,

Thanks for your reply. na.locf would replace the NA's with the most recent
non-NA, so it wouldn't create a sequence of chron dates/times (via
as.vector, as in your example). To expand my original example:


>> On 5/17/05, Sebastian Luque <sluque at mun.ca> wrote:

[...]

>>> DateTime
>>> 13/10/03 12:30:35
>>> NA
>>> NA
>>> NA
>>> 15/10/03 16:30:05
>>> NA
>>> NA
>>> ...

I thought one could replace the NA's by the desired interval, say 1 day,
so if the above chron object was named nachron, one could do:

nachron[is.na(nachron)] <- 1

and, for simplicity, applying on each "block" separately:

cumsum(nachron)

would give:

DateTime
13/10/03 12:30:35
14/10/03 12:30:35
15/10/03 12:30:35
16/10/03 12:30:35

for the first "block", and:

DateTime
15/10/03 16:30:05
16/10/03 16:30:05
17/10/03 16:30:05
...

for the second one. Since there are not too many blocks I may end up doing
it in Excel, but it would be nice to know how to do it in R!

Cheers and thank you,
-- 
Sebastian P. Luque



From f.harrell at vanderbilt.edu  Wed May 18 04:44:27 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 17 May 2005 21:44:27 -0500
Subject: [R] Hmisc/Design and problem with cgroup
In-Reply-To: <r02010500-1039-5B331D3EC72C11D9B5AC000A959B3D22@[10.0.1.205]>
References: <r02010500-1039-5B331D3EC72C11D9B5AC000A959B3D22@[10.0.1.205]>
Message-ID: <428AAC0B.5060404@vanderbilt.edu>

Aric Gregson wrote:
> Hello,
> 
> I am trying to use the following to output a table to latex:
> 
> cohortbyagesummary <- by(data.frame(age,ethnicity), cohort, summary) 
> 
> w <- latex.default(cohortbyagesummary, 
>     caption="Five Number Age Summaries by Cohort",
>     label="agesummarybycohort", 
>     cgroup=c('hello','goodbye','hello'),
>     colheads=c("Age","Ethnicity"),
>     extracolheads=c('hello','goodbye'), # demonstration of subheadings
>     greek=TRUE,
>     ctable=TRUE)
>     
> I am not able to get the major column headings of cgroup to work. I
> receive the error:
>     Object cline not found
>     
> I do not see in the examples or documentation that you must specify
> cline or anything. 
> 
> Any suggestions? Thanks very much,
> 
> aric
> 
> R 2.0.1
> Design 2.0-11
> Hmisc  3.0-5
> OS X 10.3.9
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
See if a modified version at
http://biostat.mc.vanderbilt.edu/cgi-bin/cvsweb.cgi/Hmisc/R/latex.s
fixes your problem.

You don't need .default after latex.

Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ggrothendieck at gmail.com  Wed May 18 04:49:02 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 17 May 2005 22:49:02 -0400
Subject: [R] cumsum on chron objects
In-Reply-To: <87zmutqp5n.fsf@mun.ca>
References: <874qd1sc29.fsf@mun.ca> <971536df05051716349163b0b@mail.gmail.com>
	<971536df05051716391ef14437@mail.gmail.com> <87zmutqp5n.fsf@mun.ca>
Message-ID: <971536df050517194930f40cd1@mail.gmail.com>

On 5/17/05, Sebastian Luque <sluque at mun.ca> wrote:
> Hello Gabor,
> 
> Thanks for your reply. na.locf would replace the NA's with the most recent
> non-NA, so it wouldn't create a sequence of chron dates/times (via
> as.vector, as in your example). To expand my original example:
> 
> 
> >> On 5/17/05, Sebastian Luque <sluque at mun.ca> wrote:
> 
> [...]
> 
> >>> DateTime
> >>> 13/10/03 12:30:35
> >>> NA
> >>> NA
> >>> NA
> >>> 15/10/03 16:30:05
> >>> NA
> >>> NA
> >>> ...
> 
> I thought one could replace the NA's by the desired interval, say 1 day,
> so if the above chron object was named nachron, one could do:
> 
> nachron[is.na(nachron)] <- 1
> 
> and, for simplicity, applying on each "block" separately:
> 
> cumsum(nachron)
> 
> would give:
> 
> DateTime
> 13/10/03 12:30:35
> 14/10/03 12:30:35
> 15/10/03 12:30:35
> 16/10/03 12:30:35
> 
> for the first "block", and:
> 
> DateTime
> 15/10/03 16:30:05
> 16/10/03 16:30:05
> 17/10/03 16:30:05
> ...
> 
> for the second one. Since there are not too many blocks I may end up doing
> it in Excel, but it would be nice to know how to do it in R!


I did not understand that you wanted a sequence.

If x and x.locf are as in the previous response then:

   my.seq <- function(x) seq(from = x[1], len = length(x))
   chron(unlist(tapply(x, x.locf, my.seq)))

or if you want to use cumsum:

   xx <- as.vector(x); xx[is.na(xx)] <- 1
   chron(unlist(tapply(xx, x.locf, cumsum)))



From jacques.veslot at cirad.fr  Wed May 18 08:11:06 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Wed, 18 May 2005 10:11:06 +0400
Subject: [R] Line width through plot size reduction
Message-ID: <428ADC7A.2040901@cirad.fr>

Dear R-users,

Someone, who uses R under Mac, wants to insert a couple of small plots 
(each with several lines) in an article, but he has to reduce plots' 
size significantly. He did it (in pdf or enc. ps) but, unfortunately, 
everything is reduced but lines' width. Besides, 'lwd' argument in par() 
or plot() can't be less than one.

I am not a Mac user and I don't know whether it is a Mac-related problem 
or not.

Could somebody please give me a way to solve this problem, either to 
reduce line width under R, or to get reduced lines' width when reducing 
plot size ?

Thanks,

Jacques VESLOT
Cirad



From joel3000 at gmail.com  Wed May 18 08:35:37 2005
From: joel3000 at gmail.com (Joel Bremson)
Date: Tue, 17 May 2005 23:35:37 -0700
Subject: [R] Fortran 95 in R ?
Message-ID: <1253d67a05051723354f663146@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050517/43ff1d73/attachment.pl

From Soren.Hojsgaard at agrsci.dk  Wed May 18 09:07:17 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Wed, 18 May 2005 09:07:17 +0200
Subject: SV: [R] Sweave and paths
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99B4@DJFPOST01.djf.agrsci.dk>

Don't think it is possible. To overcome the problem I wrote a small R-program which replaced foo with foo\bar in the Sweave file. Clumpsy, sure, but...
Best 
S??ren

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p?? vegne af Bill Rising
Sendt: ti 17-05-2005 20:31
Til: rhelp
Emne: [R] Sweave and paths



Is there some way to encourage \SweaveInput{foo} to find foo in a 
subdirectory of a file tree? Something along the lines of the 
behavior of list.files(<stuff>, recursive=TRUE). This would be very 
helpful at calling small modular files, such as solution sets and the 
like.

I couldn't see anything in the documentation, and I looked in the 
source code, but it seems that SweaveReadFiledoc() wants to look only 
in the directory which contains foo.

Still, I'm guessing that I'm missing something. Any tips would be 
much appreciated.

Bill

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From martinol at ensam.inra.fr  Wed May 18 10:15:57 2005
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Wed, 18 May 2005 10:15:57 +0200
Subject: [R] dendrogram and dendrapply
Message-ID: <428AF9BD.1090604@ensam.inra.fr>

Hi all,

I think I have some problems to use correctly the function dendrapply.
Let suppose dend is a dendrogram object.
I would likde to know the cardinal number of leaves depending of each
node of the tree dend.

It is right that the command attr(dend,"members") gives the total number 
of leaves, and
attr(dend[[1]],"members") gives the number of leaves for the first left 
node and so on...

Now to obtain the number of leaves for each node, I tried the function 
dendrapply :
dendrapply(dend, function(x){attr(x,"members")})

but this command  does not give me the result I serach. Could you help me.

Best regards,
Olivier.



From spayet.reesfrance at wanadoo.fr  Wed May 18 10:36:20 2005
From: spayet.reesfrance at wanadoo.fr (=?iso-8859-1?Q?St=E9phanie_PAYET?=)
Date: Wed, 18 May 2005 10:36:20 +0200
Subject: [R] Vuong test
In-Reply-To: <83217d00505170159626848e5@mail.gmail.com>
Message-ID: <MEEKJDIBHBIGPENAMOIBAEJFCAAA.spayet.reesfrance@wanadoo.fr>

Hi Reza,
thanks for your answer.
I have already used the code of Dr. Jackman for fitting the double hurdle
model, but it doesn't allow to specify different regressors for the zero and
the poisson (or negative binomial) parts, which is a bit limitative.
Moreover, I don't have Stata, I just dispose of R and SAS. Does anybody know
if the Vuong test also exist on these two pieces of software ?

Best regards

St??phanie

-----Message d'origine-----
De : Seyed Reza Jafarzadeh [mailto:srjafarzadeh at gmail.com]
Envoy?? : mardi 17 mai 2005 10:59
?? : spayet.reesfrance at wanadoo.fr
Cc : r-help at stat.math.ethz.ch
Objet : Re: [R] Vuong test


Hi St??phanie,

The Vuong test can be done in Stata
(http://www.stata.com/support/faqs/stat/vuong.html), but I am also
looking for its code in R. In addition to "zicounts", Dr. Simon
Jackman (http://pscl.stanford.edu/) has provided the code for fitting
the zero-inflated (http://pscl.stanford.edu/zeroinfl.r) and hurdle
(http://pscl.stanford.edu/hurdle.r) count models.

Reza



On 5/17/05, St??phanie PAYET <spayet.reesfrance at wanadoo.fr> wrote:
> Hi,
>
> I have two questions. First, I'd like to compare a ZINB model to a negativ
> binomial model with the Vuong test, but I can't find how to performe it
from
> the zicount package. Does a programm exist to do it ?
> Second, I'd like to know in which cases we have to use a double hurdle
model
> instead of a zero inflated model.
>
> Many thanks,
>
> St??phanie Payet
>
> REES France
> R??seau d'Evaluation en Economie de la Sant??
> 28, rue d'Assas
> 75006 PARIS
> T??l. +33 (0)1 44 39 16 90
> Fax +33 (0)1 44 39 16 92
> M??l. reesfrance at wanadoo.fr
> Site Internet : http://www.rees-france.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From s-plus at wiwi.uni-bielefeld.de  Wed May 18 10:51:53 2005
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Wed, 18 May 2005 10:51:53 +0200
Subject: [R] Testing for warning inside functions
Message-ID: <428B0229.4080000@wiwi.uni-bielefeld.de>

I am looking for a way to get a warning message
immediately after an evaluation within a function.
To get error messages you can use geterrmessage().
But I found no function that allows me to check for
warnings.

Five years ago this questions has been posted
but I haven't found any answer.

Thanks for any help.

Peter Wolf


------------

For illustration purpose a simple example follows:
you can get warnings after an error occured or after
exiting from the function but not at once after
"x<-matrix(1:3,2,2)":

 > options(warn=0)
 > f<-function(x){
   x<-matrix(1:3,2,2)
   print(0)
   warnings()  # no warning is shown
   print(1)
   try({xxx})
   print(2)
   print(geterrmessage())
   print(3)
   warnings()
   print(4)
 }
 > f()
[1] 0
[1] 1
Fehler in try({ : Objekt "xxx" nicht gefunden
Zus??tzlich: Warnmeldung:
Datenl??nge [3] ist kein Teiler oder Vielfaches der Anzahl der Zeilen [2] 
in matrix
[1] 2
[1] "Fehler in try({ : Objekt \"xxx\" nicht gefunden\n"
[1] 3
Warning message:
Datenl??nge [3] ist kein Teiler oder Vielfaches der Anzahl der Zeilen [2] 
in matrix
[1] 4
 > version
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R



From ripley at stats.ox.ac.uk  Wed May 18 11:29:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 May 2005 10:29:02 +0100 (BST)
Subject: [R] Testing for warning inside functions
In-Reply-To: <428B0229.4080000@wiwi.uni-bielefeld.de>
References: <428B0229.4080000@wiwi.uni-bielefeld.de>
Message-ID: <Pine.LNX.4.61.0505181019280.9204@gannet.stats>

On Wed, 18 May 2005, Peter Wolf wrote:

> I am looking for a way to get a warning message
> immediately after an evaluation within a function.
> To get error messages you can use geterrmessage().

Well, only in an error handler, as an error normally throws you out of the 
function.  (That is the point of geterrmessage(), to enable error handlers 
to be written.)

To print the message immediately, look up options("warn").  We don't have 
a general warnings handler mechanism, but you can make use of condition 
objects.

> But I found no function that allows me to check for
> warnings.
>
> Five years ago this questions has been posted
> but I haven't found any answer.

What exactly is `this question'?  I see several assertions but no 
question.  (It would have been helpful to give an exact reference to the 
archives.)

> Thanks for any help.
>
> Peter Wolf
>
>
> ------------
>
> For illustration purpose a simple example follows:
> you can get warnings after an error occured or after
> exiting from the function but not at once after
> "x<-matrix(1:3,2,2)":
>
>> options(warn=0)
>> f<-function(x){
>  x<-matrix(1:3,2,2)
>  print(0)
>  warnings()  # no warning is shown
>  print(1)
>  try({xxx})
>  print(2)
>  print(geterrmessage())
>  print(3)
>  warnings()
>  print(4)
> }
>> f()
> [1] 0
> [1] 1
> Fehler in try({ : Objekt "xxx" nicht gefunden
> Zus?tzlich: Warnmeldung:
> Datenl?nge [3] ist kein Teiler oder Vielfaches der Anzahl der Zeilen [2] in 
> matrix
> [1] 2
> [1] "Fehler in try({ : Objekt \"xxx\" nicht gefunden\n"
> [1] 3
> Warning message:
> Datenl?nge [3] ist kein Teiler oder Vielfaches der Anzahl der Zeilen [2] in 
> matrix
> [1] 4

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From vincent at 7d4.com  Wed May 18 11:44:50 2005
From: vincent at 7d4.com (vincent)
Date: Wed, 18 May 2005 11:44:50 +0200
Subject: [R] fast matrix update
Message-ID: <428B0E92.9090805@7d4.com>

Hello,
I use extensively m[i0:i1,j0:j1] = ... to update matrices parts.
This writing is very convenient, but, since I'm doing this
calculus many many times, I would like to know if there is a way
(a hope ?) to do the same operation faster ?
Thanks for any advice or pointer.
Vincent



From s-plus at wiwi.uni-bielefeld.de  Wed May 18 12:03:26 2005
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Wed, 18 May 2005 12:03:26 +0200
Subject: [R] Testing for warning inside functions
References: <428B0229.4080000@wiwi.uni-bielefeld.de>
	<Pine.LNX.4.61.0505181019280.9204@gannet.stats>
Message-ID: <428B12EE.3030704@wiwi.uni-bielefeld.de>

Prof Brian Ripley wrote:

> On Wed, 18 May 2005, Peter Wolf wrote:
>
>> I am looking for a way to get a warning message
>> immediately after an evaluation within a function.
>> To get error messages you can use geterrmessage().
>
>
> Well, only in an error handler, as an error normally throws you out of 
> the function.  (That is the point of geterrmessage(), to enable error 
> handlers to be written.)
>
> To print the message immediately, look up options("warn").

options(warn=1) prints the message immediately,
but I cannot store it in a variable to analyse it or to show the message 
it in a message box, for example.

>   We don't have a general warnings handler mechanism, but you can make 
> use of condition objects. 

Thank you, I will have a look at condition objects.

>> But I found no function that allows me to check for
>> warnings.
>>
>> Five years ago this questions has been posted
>> but I haven't found any answer.
>
>
> What exactly is `this question'?  I see several assertions but no 
> question.  (It would have been helpful to give an exact reference to 
> the archives.)

link to the old mail: R-help archive Jan - June 2000:

* [R] Testing for warning inside functions -- V/idhyanath Rao (Sun 07 
May 2000 - 12:46:38 EST)/
Message-ID: <000301bfb831$33b87c60$d370fe8c at lucent.com>
"This is probably a newbie question: How do I test, inside a function, is
a call I made from inside that function produced a warning? ..."

>
>
>> Thanks for any help.
>>
>> Peter Wolf
>>
>>
>> ------------
>>
>> For illustration purpose a simple example follows:
>> you can get warnings after an error occured or after
>> exiting from the function but not at once after
>> "x<-matrix(1:3,2,2)":
>>
>>> options(warn=0)
>>> f<-function(x){
>>
>>  x<-matrix(1:3,2,2)
>>  print(0)
>>  warnings()  # no warning is shown
>>  print(1)
>>  try({xxx})
>>  print(2)
>>  print(geterrmessage())
>>  print(3)
>>  warnings()
>>  print(4)
>> }
>>
>>> f()
>>
>> [1] 0
>> [1] 1
>> Fehler in try({ : Objekt "xxx" nicht gefunden
>> Zus??tzlich: Warnmeldung:
>> Datenl??nge [3] ist kein Teiler oder Vielfaches der Anzahl der Zeilen 
>> [2] in matrix
>> [1] 2
>> [1] "Fehler in try({ : Objekt \"xxx\" nicht gefunden\n"
>> [1] 3
>> Warning message:
>> Datenl??nge [3] ist kein Teiler oder Vielfaches der Anzahl der Zeilen 
>> [2] in matrix
>> [1] 4
>
>



From murdoch at stats.uwo.ca  Wed May 18 12:07:12 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 18 May 2005 11:07:12 +0100
Subject: [R] fast matrix update
In-Reply-To: <428B0E92.9090805@7d4.com>
References: <428B0E92.9090805@7d4.com>
Message-ID: <428B13D0.2040103@stats.uwo.ca>

vincent wrote:
> Hello,
> I use extensively m[i0:i1,j0:j1] = ... to update matrices parts.
> This writing is very convenient, but, since I'm doing this
> calculus many many times, I would like to know if there is a way
> (a hope ?) to do the same operation faster ?

It tends to be faster to use vector indexing rather than matrix 
indexing, so if you can calculate which entries correspond to the block 
you want to replace you might find this is faster:

block <- [ some calculation here, e.g. i0:i1 if j0 = j1 = 1 ]
m[block] <- ...

Of course, the calculation involved in the general case is ugly and 
could be slow.  If this is really a crucial bottleneck to your 
calculation, I'd suggest skipping over that optimization and redoing the 
function in C or Fortran.

Duncan Murdoch



From jonathan.charrier at bordeaux.inra.fr  Wed May 18 12:11:16 2005
From: jonathan.charrier at bordeaux.inra.fr (Jonathan Charrier)
Date: Wed, 18 May 2005 12:11:16 +0200
Subject: [R] extract with condition
Message-ID: <428B14C4.6010606@bordeaux.inra.fr>

hello everybody,

i want to extract the name of the rows for the value = "TRUE"

these names give for the next program a list of file

i try many functions, and indexation but no solution give to me, i 
transform only the matrix.


thanks a lot

Jonathan Charrier



nmodT <- Y.obs1
Q <- 3
T <- length(nmodT[,1])*Q

qsd <- colSums(nmodT)
qsd <- as.matrix(qsd)

aqw<- qsd>T


aqw

            [,1]
2003-01-01 FALSE
2003-01-11  TRUE
2003-01-21  TRUE
2003-02-01  TRUE
2003-02-11  TRUE
2003-02-21  TRUE
2003-03-01  TRUE
2003-03-11  TRUE
2003-03-21  TRUE
2003-04-01  TRUE
2003-04-11  TRUE
2003-04-21  TRUE
2003-05-01  TRUE
2003-05-11  TRUE
2003-05-21  TRUE
2003-06-01  TRUE
2003-06-11  TRUE
2003-06-21  TRUE
2003-07-01  TRUE
2003-07-11  TRUE
2003-07-21  TRUE
2003-08-01  TRUE
2003-08-11  TRUE
2003-08-21  TRUE
2003-09-01  TRUE
2003-09-11  TRUE
2003-09-21  TRUE
2003-10-01  TRUE
2003-10-11  TRUE
2003-10-21  TRUE
2003-11-01  TRUE
2003-11-11  TRUE
2003-11-21  TRUE
2003-12-01  TRUE
2003-12-11  TRUE
2003-12-21  TRUE



From Jan.Verbesselt at biw.kuleuven.be  Wed May 18 12:50:53 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Wed, 18 May 2005 12:50:53 +0200
Subject: [R] DEV2bitmap: jpeg with res=400 not enough for CORELDRAW poster
	A0
In-Reply-To: <x2ll6jl8bx.fsf@biostat.ku.dk>
Message-ID: <000201c55b97$78cf0670$1145210a@agr.ad10.intern.kuleuven.ac.be>

Thanks for the input!

Finally I used:

dev2bitmap(name,type="pdfwrite",height=8,width=14,res=1200)

==> resolution and colors were fine on the A0 poster! 
(and files are really small).

Regards,
Jan

____________________________________________________________________
ir. Jan Verbesselt 
Research Associate 
Lab of Geomatics Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel: +32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/

_______________________________________________________________________

-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: Friday, May 13, 2005 12:38 PM
To: Uwe Ligges
Cc: Jan.Verbesselt at biw.kuleuven.be; r-help at stat.math.ethz.ch
Subject: Re: [R] DEV2bitmap: jpeg with res=400 not enough for CORELDRAW
poster A0

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Jan Verbesselt wrote:
> 
> > Dear all,
> > When saving a plot with the dev2bitmap command:
> >     name    <- c("test.jpeg")
> >     dev2bitmap(name,type="jpeg",height=8,width=13,res=400)
> > Everything seems to be ok... After importing this picture in
> > CORELDRAW (for
> > a poster A0) format the resolution and colors are not optimal.
> > How can I save pictures (colors/resolution) optimally for import into
> > CorelDraw for an A0 poster?
> > Pdf?/tiff?/bmp?/
> 
> What about PostScript? It's perfectly resizable and CorelDraw (at
> least the outdated version 10) can deal with it.

But not PDF? Notice that this is not really bitmapped either, even
though handled by dev2bitmap. 

Upping the res= is another option, but may be memory intensive. Notice
that A0 is 4 times as big as A4 (~8x12 inches) so you'd need up to 4
times the resolution - but I guess that your plot is not taking up the
whole area...


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From adrimelo at ecologia.ufrgs.br  Wed May 18 13:39:24 2005
From: adrimelo at ecologia.ufrgs.br (Adriano S. Melo)
Date: Wed, 18 May 2005 08:39:24 -0300
Subject: [R] Re: Finding the right number of clusters
In-Reply-To: <200505181014.j4IA4fl9018505@hypatia.math.ethz.ch>
Message-ID: <5.2.0.9.0.20050518083404.023a68b0@nemo.ecologia.ufrgs.br>

Dear Philip,
Perhaps this reference might be useful:
V. D. P. Pillar. How sharp are classifications? Ecology 80:2508-2516, 1999.
Adriano S. Melo


>------------------------------
>
>Message: 9
>Date: Tue, 17 May 2005 08:42:02 -0400
>From: Philip Bermingham <pberming at research.ryerson.ca>
>Subject: [R] Finding the right number of clusters
>To: r-help at stat.math.ethz.ch
>Message-ID: <4289E69A.7040300 at csca.ryerson.ca>
>Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
>SAS has something called the "cubic criterion" cutoff for finding the
>most appropriate number of clusters.  Does R have anything that would
>replicate that? I've been searching the lists and can't seem to find
>anything that would point me in the right direction.
>
>Thank in advance,
>Philip Bermingham
>
>

Adriano Sanches Melo
Dep. Ecologia, Instituto de Bioci?ncias
Universidade Federal do Rio Grande do Sul
Porto Alegre, RS, 91540-000, BRAZIL
http://www.ecologia.ufrgs.br/~adrimelo/e/index.htm  
-------------- next part --------------


No virus found in this outgoing message.
Checked by AVG Anti-Virus.


From navarre_sabine at yahoo.fr  Wed May 18 14:03:12 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Wed, 18 May 2005 14:03:12 +0200 (CEST)
Subject: [R] R -SQL
Message-ID: <20050518120313.84572.qmail@web26609.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050518/a8455ff7/attachment.pl

From Pierre.Lapointe at nbf.ca  Wed May 18 14:06:18 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Wed, 18 May 2005 08:06:18 -0400
Subject: [R] Bry & Boschan routines in R
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF357AD@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050518/b551d1e9/attachment.pl

From Markus.Gesmann at lloyds.com  Wed May 18 14:29:07 2005
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Wed, 18 May 2005 13:29:07 +0100
Subject: [R] R -SQL
Message-ID: <321C3EEBDB00C24185705B8BF733DADD0503F844@LNVCNTEXCH01.corp.lloydsnet>

Your code looks more like Visual Basic rather than R.
What you want is:

for(j in 1:length(criteria$Title)){

sqlstring <- paste("select q.type,crit.Title, r.Value from criteria crit, reply r,question_reply qr, question q, question_criteria qc, form_question fq where qr.reply=r.ID and qr.question=q.ID and qc.question=q.ID and crit.ID=qc.criteria and fq.question=q.ID and fq.form=4 and crit.Title='", criteria$Title[j], "';", sep="")

graphe_par<-sqlQuery(channel,sqlstring)

}

See ?paste

Regards

Markus

Your SQL statement seems to be wrong. 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Navarre Sabine
Sent: 18 May 2005 13:03
To: r-help at stat.math.ethz.ch
Subject: [R] R -SQL


Hello,
 
I've got a problem in a sql query!


for(j in 1:length(criteria$Title))

{
    graphe_par<-sqlQuery(channel,"select q.type,crit.Title, r.Value from criteria crit, reply r,question_reply qr, question q, question_criteria qc, form_question fq where qr.reply=r.ID and qr.question=q.ID and qc.question=q.ID and crit.ID=qc.criteria and fq.question=q.ID and fq.form=4 and crit.Title=" & criteria$Title[j] &";")

............}


> criteria$Title
[1] Content           Logistic          Trainer           Supply            User contribution
Levels: Content Logistic Supply Trainer User contribution


The error is:
Error in "select q.type,crit.Title, r.Value from criteria crit, reply r,question_reply qr, question q, question_criteria qc, form_question fq where qr.reply=r.ID and qr.question=q.ID and qc.question=q.ID and crit.ID=qc.criteria and fq.question=q.ID and fq.form=4 and crit.Title=" &  : 
        operations are possible only for numeric or logical types
In addition: Warning message:
& not meaningful for factors in: Ops.factor("select q.type,crit.Title, r.Value from criteria crit, reply r,question_reply qr, question q, question_criteria qc, form_question fq where qr.reply=r.ID and qr.question=q.ID and qc.question=q.ID and crit.ID=qc.criteria and fq.question=q.ID and fq.form=4 and crit.Title=", 

please help me,

Thanks a lot!

Sabine


		
---------------------------------

ils, photos et vid??os !

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html





************LNSCNTMCS01***************************************************
The information in this E-Mail and in any attachments is CON...{{dropped}}



From pberming at research.ryerson.ca  Wed May 18 14:33:40 2005
From: pberming at research.ryerson.ca (Philip Bermingham)
Date: Wed, 18 May 2005 08:33:40 -0400
Subject: [R] standardization
Message-ID: <428B3624.60209@csca.ryerson.ca>

SAS Enterprise Miner recommendeds to standardize using X / STDEV(X) 
versus [X ? mean(X)] / STDEV(X)

Any thoughts on this? Pros Cons

Philip



From ripley at stats.ox.ac.uk  Wed May 18 14:37:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 May 2005 13:37:21 +0100 (BST)
Subject: [R] R -SQL
In-Reply-To: <20050518120313.84572.qmail@web26609.mail.ukl.yahoo.com>
References: <20050518120313.84572.qmail@web26609.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0505181331180.14844@gannet.stats>

You have not told us what package you are using (and I think this is a 
repeat of a post also missing that crucial information).

However, the main problem appears to be that you think & operates on 
character strings.  You have not told us your intentions, which might have 
been

paste("select ... crit.Title=", criteria$Title[j], ";", sep="")



On Wed, 18 May 2005, Navarre Sabine wrote:

> Hello,
>
> I've got a problem in a sql query!
>
>
> for(j in 1:length(criteria$Title))
>
> {
>    graphe_par<-sqlQuery(channel,"select q.type,crit.Title, r.Value from 
> criteria crit, reply r,question_reply qr, question q, question_criteria 
> qc, form_question fq where qr.reply=r.ID and qr.question=q.ID and 
> qc.question=q.ID and crit.ID=qc.criteria and fq.question=q.ID and 
> fq.form=4 and crit.Title=" & criteria$Title[j] &";")
>
> ............}
>
>
>> criteria$Title
> [1] Content           Logistic          Trainer           Supply            User contribution
> Levels: Content Logistic Supply Trainer User contribution
>
>
> The error is:
> Error in "select q.type,crit.Title, r.Value from criteria crit, reply r,question_reply qr, question q, question_criteria qc, form_question fq where qr.reply=r.ID and qr.question=q.ID and qc.question=q.ID and crit.ID=qc.criteria and fq.question=q.ID and fq.form=4 and crit.Title=" &  :
>        operations are possible only for numeric or logical types
> In addition: Warning message:
> & not meaningful for factors in: Ops.factor("select q.type,crit.Title, r.Value from criteria crit, reply r,question_reply qr, question q, question_criteria qc, form_question fq where qr.reply=r.ID and qr.question=q.ID and qc.question=q.ID and crit.ID=qc.criteria and fq.question=q.ID and fq.form=4 and crit.Title=",


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed May 18 14:38:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 May 2005 13:38:28 +0100 (BST)
Subject: [R] DEV2bitmap: jpeg with res=400 not enough for CORELDRAW poster
	A0
In-Reply-To: <000201c55b97$78cf0670$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000201c55b97$78cf0670$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <Pine.LNX.4.61.0505181300280.14482@gannet.stats>

On Wed, 18 May 2005, Jan Verbesselt wrote:

> Thanks for the input!
>
> Finally I used:
>
> dev2bitmap(name,type="pdfwrite",height=8,width=14,res=1200)
>
> ==> resolution and colors were fine on the A0 poster!
> (and files are really small).

Why not use R's native PDF driver if PDF is acceptable?  This is a 
convoluted way to produce PDF by converting EPS.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed May 18 14:48:32 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 May 2005 14:48:32 +0200
Subject: [R] standardization
In-Reply-To: <428B3624.60209@csca.ryerson.ca>
References: <428B3624.60209@csca.ryerson.ca>
Message-ID: <x2wtpwbsyn.fsf@turmalin.kubism.ku.dk>

Philip Bermingham <pberming at research.ryerson.ca> writes:

> SAS Enterprise Miner recommendeds to standardize using X / STDEV(X)
> versus [X ? mean(X)] / STDEV(X)
> 
> Any thoughts on this? Pros Cons

When??? 

This makes absolutely no sense out of context.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From erithid at bellsouth.net  Wed May 18 15:09:53 2005
From: erithid at bellsouth.net (BJ)
Date: Wed, 18 May 2005 09:09:53 -0400
Subject: [R] applying a function over an array
Message-ID: <428B3EA1.2010906@bellsouth.net>

Is there a way to apply a function with several arguements over an 
array? For instance if you had a function d<-function(a,b,c) {a+b+c} and 
a 4,3 array, could you apply the function over each line of the array? 
tapply seems to only allow one argument, and I just can't think of a way 
to explicitly tell R that I am passing it 3 distinct objects instead of 
1 complex one.  Thank you as always for your help. ~Erithid



From B.Rowlingson at lancaster.ac.uk  Wed May 18 15:17:03 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 18 May 2005 14:17:03 +0100
Subject: [R] standardization
In-Reply-To: <x2wtpwbsyn.fsf@turmalin.kubism.ku.dk>
References: <428B3624.60209@csca.ryerson.ca>
	<x2wtpwbsyn.fsf@turmalin.kubism.ku.dk>
Message-ID: <428B404F.5040302@lancaster.ac.uk>

Peter Dalgaard wrote:

>>SAS Enterprise Miner recommendeds to standardize using X / STDEV(X)
>>versus [X ? mean(X)] / STDEV(X)

> 
> This makes absolutely no sense out of context.
> 

  To paraphrase Tanenbaum: "The nice thing about standardization is that 
there's so many ways to do it".

Baz

[[

Free On-line Dictionary of Computing:

Andrew Tanenbaum, in his Computer Networks book, once said,
"The nice thing about standards is that there are so many of
them to choose from",

]]



From Robert.McGehee at geodecapital.com  Wed May 18 15:21:17 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 18 May 2005 09:21:17 -0400
Subject: [R] applying a function over an array
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C1ED856@MSGBOSCLB2WIN.DMN1.FMR.COM>

This will do it.

> a <- array(1:12, c(4, 3))
> d <- function(a,b,c) {a+b+c}
> apply(a, 1, function(x) do.call("d", sapply(x, list)))
[1] 15 18 21 24

-----Original Message-----
From: BJ [mailto:erithid at bellsouth.net] 
Sent: Wednesday, May 18, 2005 9:10 AM
To: r-help at stat.math.ethz.ch
Subject: [R] applying a function over an array


Is there a way to apply a function with several arguements over an 
array? For instance if you had a function d<-function(a,b,c) {a+b+c} and

a 4,3 array, could you apply the function over each line of the array? 
tapply seems to only allow one argument, and I just can't think of a way

to explicitly tell R that I am passing it 3 distinct objects instead of 
1 complex one.  Thank you as always for your help. ~Erithid

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed May 18 15:28:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 May 2005 15:28:08 +0200
Subject: [R] standardization
In-Reply-To: <428B404F.5040302@lancaster.ac.uk>
References: <428B3624.60209@csca.ryerson.ca>
	<x2wtpwbsyn.fsf@turmalin.kubism.ku.dk>
	<428B404F.5040302@lancaster.ac.uk>
Message-ID: <x2oeb8br4n.fsf@turmalin.kubism.ku.dk>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:


> "The nice thing about standards is that there are so many of
> them to choose from",

Curiously enough, the same quote came up today on dk.edb.system.unix
in the context of translations. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Matthias.Templ at statistik.gv.at  Wed May 18 15:40:05 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 18 May 2005 15:40:05 +0200
Subject: [R] standardization
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAA7B@xchg1.statistik.local>

My thoughts on this is:
Do not trust what SAS say??s and least of all what the Enterprise Miner said.

Robust Statisticians recommendends to standardize using e.g. 
(X - median(X)) / ( MAD(X) / 0.675 ) 

Best,
Matthias


> SAS Enterprise Miner recommendeds to standardize using X / STDEV(X) 
> versus [X - mean(X)] / STDEV(X)
> 
> Any thoughts on this? Pros Cons
> 
> Philip
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From sam.kemp2 at ntlworld.com  Wed May 18 15:56:11 2005
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Wed, 18 May 2005 14:56:11 +0100
Subject: [R] dse VAR models
Message-ID: <428B497B.80303@ntlworld.com>

Hi,

Can anyone tell me how to construct a simple VAR(1) time series with two 
variables using the dse package? I would like to end up with two time series

y_1t = \phi_11 y_1,t-1 + \phi_12 y_2,t-1 + e_1t
y_2t = \phi_21 y_1,t-1 + \phi_22 y_2,t-1 + e_2t

Best regards,

Sam.



From uofiowa at gmail.com  Wed May 18 16:13:50 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 18 May 2005 10:13:50 -0400
Subject: [R] align
Message-ID: <3f87cc6d0505180713704dc236@mail.gmail.com>

Is there a function in R that is similar to Splus's align?

The idea is, if I have a data.frame, or an its object that is like this:

2002-01-03   5
2002-01-04   NA
2002-01-05   7
2002-01-06   NA

I want to align it by the last value to this:

2002-01-03   5
2002-01-04   5
2002-01-05   7
2002-01-06   7


TITLE:
        Function align

USAGE:
        align(x, pos, how, error.how, localzone, matchtol, by, k.by,
        week.align, holidays)

ARGUMENTS:
  x:  argument, no default.
  pos:  argument, no default.
  how:  argument, `default = "NA".'
  error.how:  argument, `default = "NA".'
  localzone:  argument, `default = F.'
  matchtol:  argument, `default = 0.'
  by:  argument, no default.
  k.by:  argument, `default = 1.'
  week.align:  argument, `default = NULL.'
  holidays:  argument, `default = timeDate().'

DESCRIPTION:


        Align series object x to new positions



From WheelerB at imsweb.com  Wed May 18 16:15:39 2005
From: WheelerB at imsweb.com (Wheeler, Bill (IMS))
Date: Wed, 18 May 2005 10:15:39 -0400
Subject: [R] User defined split function in rpart
Message-ID: <2BC5FD62664664429FF92FBE2BD40A780A4816@granite.omni.imsweb.com>

In the function rpart, users can define their own split function by
creating functions init, eval, and split.
For method='anova', I have examples of these functions.
I would like to create these functions for the case method='class'.
Does anyone know how the init, eval, and split functions would be
written?

Thank you.

Bill



From ggrothendieck at gmail.com  Wed May 18 16:16:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 18 May 2005 10:16:22 -0400
Subject: [R] applying a function over an array
In-Reply-To: <428B3EA1.2010906@bellsouth.net>
References: <428B3EA1.2010906@bellsouth.net>
Message-ID: <971536df0505180716747af72f@mail.gmail.com>

On 5/18/05, BJ <erithid at bellsouth.net> wrote:
> Is there a way to apply a function with several arguements over an
> array? For instance if you had a function d<-function(a,b,c) {a+b+c} and
> a 4,3 array, could you apply the function over each line of the array?
> tapply seems to only allow one argument, and I just can't think of a way
> to explicitly tell R that I am passing it 3 distinct objects instead of
> 1 complex one.  Thank you as always for your help. ~Erithid

If x is the 4x3 array:

mapply(d, x[,1], x[,2], x[,3])



From ggrothendieck at gmail.com  Wed May 18 16:19:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 18 May 2005 10:19:41 -0400
Subject: [R] align
In-Reply-To: <3f87cc6d0505180713704dc236@mail.gmail.com>
References: <3f87cc6d0505180713704dc236@mail.gmail.com>
Message-ID: <971536df050518071937ac4127@mail.gmail.com>

Check out the 'locf' function in the 'its' package and the 'na.locf'
function in the 'zoo' package.  

On 5/18/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> Is there a function in R that is similar to Splus's align?
> 
> The idea is, if I have a data.frame, or an its object that is like this:
> 
> 2002-01-03   5
> 2002-01-04   NA
> 2002-01-05   7
> 2002-01-06   NA
> 
> I want to align it by the last value to this:
> 
> 2002-01-03   5
> 2002-01-04   5
> 2002-01-05   7
> 2002-01-06   7
> 
> TITLE:
>        Function align
> 
> USAGE:
>        align(x, pos, how, error.how, localzone, matchtol, by, k.by,
>        week.align, holidays)
> 
> ARGUMENTS:
>  x:  argument, no default.
>  pos:  argument, no default.
>  how:  argument, `default = "NA".'
>  error.how:  argument, `default = "NA".'
>  localzone:  argument, `default = F.'
>  matchtol:  argument, `default = 0.'
>  by:  argument, no default.
>  k.by:  argument, `default = 1.'
>  week.align:  argument, `default = NULL.'
>  holidays:  argument, `default = timeDate().'
> 
> DESCRIPTION:
> 
>        Align series object x to new positions
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed May 18 16:26:18 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 18 May 2005 10:26:18 -0400
Subject: [R] applying a function over an array
In-Reply-To: <971536df0505180716747af72f@mail.gmail.com>
References: <428B3EA1.2010906@bellsouth.net>
	<971536df0505180716747af72f@mail.gmail.com>
Message-ID: <971536df0505180726cd8bb7d@mail.gmail.com>

On 5/18/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 5/18/05, BJ <erithid at bellsouth.net> wrote:
> > Is there a way to apply a function with several arguements over an
> > array? For instance if you had a function d<-function(a,b,c) {a+b+c} and
> > a 4,3 array, could you apply the function over each line of the array?
> > tapply seems to only allow one argument, and I just can't think of a way
> > to explicitly tell R that I am passing it 3 distinct objects instead of
> > 1 complex one.  Thank you as always for your help. ~Erithid
> 
> If x is the 4x3 array:
> 
> mapply(d, x[,1], x[,2], x[,3])
> 

Also you might check if your d is vectorized already (it is in your 
example) in which case you can just do:

   d(x[,1], x[,2], x[,3])



From subianto at gmail.com  Wed May 18 16:18:13 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Wed, 18 May 2005 16:18:13 +0200
Subject: [R] How to convert array to c()
Message-ID: <d6fiph$jnh$1@sea.gmane.org>

Dear R-helper,

Is there possible to make this array:
 > a <- array(1:12, c(4, 3))
 > a
      [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12
 >

like:
c(1,5,9)
c(2,6,10)
c(3,7,11)
c(4,8,12)

Thank you very much in advance.
Regards,
Muhammad Subianto



From gunter.berton at gene.com  Wed May 18 16:38:23 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 18 May 2005 07:38:23 -0700
Subject: [R] standardization
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BAA7B@xchg1.statistik.local>
Message-ID: <200505181438.j4IEcNqF014690@meitner.gene.com>


> Robust Statisticians recommendends to standardize using e.g. 
> (X - median(X)) / ( MAD(X) / 0.675 ) 

Make that (X - median(X))/mad(X)

The constant is already in mad()


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From jadudm at gmail.com  Wed May 18 16:34:57 2005
From: jadudm at gmail.com (Matt Jadud)
Date: Wed, 18 May 2005 15:34:57 +0100
Subject: [R] 'fitdistr' and two views of the same data?
Message-ID: <cf093f4205051807341ee4915d@mail.gmail.com>

Hello,

I have detailed (with pictures and whatnot) my question on my weblog at 

http://www.cs-ed.org/blogs/mjadud/archives/2005/05/a_question_abou.html

The short version of the question is this:

When I ask 'fitdistr' to try and fit my distribution as a "weibull"
distribution, it comes up with some rather wacky parameters.

If I take the same distribution, and do something like

newdist <- mapply(function(x) ((x %/% 20) + 1), origdist)

which effectively forces the data into a histogram, 'fitdist' on
'newdist' gives me an entirely different set of parameters.
Distressingly, the parameters it gives me are, upon inspection, good;
that is, the parameters reported fit the distribution of the original
data much better than 'fitdist' of 'origdist'.

Unfortunately, I'm not savvy enough to tease this out beyond
"inspection." The weblog entry has the original distribution, plots,
and whatnot explaining my question in more detail. My question(s) are
repeated at the bottom of the post, as well as this email address. Any
help or insights are appreciated; no doubt, I've done something...
well, wrong.

Many thanks,
Matt



From br44114 at gmail.com  Wed May 18 16:42:01 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 18 May 2005 10:42:01 -0400
Subject: [R] standardization
Message-ID: <8d5a3635050518074241b7389@mail.gmail.com>

You asked another question about clustering, so I presume you want to
standardize some variables before clustering. In SAS, PROC STDIZE
offers 18 standardization methods. See
http://support.sas.com/91doc/getDoc/statug.hlp/stdize_sect12.htm#stat_stdize_stdizesm
for details. If you're really concerned about this I would suggest
running simulations to compare the performance of various
standardization methods (relative to your data and what you're after).

hth,
b.


-----Original Message-----
From: Philip Bermingham [mailto:pberming at research.ryerson.ca]
Sent: Wednesday, May 18, 2005 8:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] standardization


SAS Enterprise Miner recommendeds to standardize using X / STDEV(X) 
versus [X ? mean(X)] / STDEV(X)

Any thoughts on this? Pros Cons

Philip

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From xmeng at capitalbio.com  Wed May 18 17:43:59 2005
From: xmeng at capitalbio.com (=?gb2312?B?w8/QwA==?=)
Date: Wed, 18 May 2005 23:43:59 +0800
Subject: [R] covariance analysis by using R
Message-ID: <316431039.19478@capitalbio.com>

Hello sir:
Here's a question on covariance analysis which needs your help.
There're 3 experiments,and x refers to control while y refers to experimental result.
The purpose is to compare the "y" values across the 3 experiments. 

experiment_1:
x:0.1 0.2 0.3 0.4 0.5
y:0.5 0.6 0.6 0.7 0.9

experiment_2:
x:1 2 3   4   5
y:3 4 6.5 7.5 11

experiment_3:
x:10 20 30 40 50
y:18 35 75 90 98

Apparently,the control("x") isn't at the similar level so that we can't compare the "y" directly through ANOVA.
We must normalize "y" via "x" in order to eliminate the influence of  different level of "x".
The method of normalize I can get is "covariance analysis",since "x" is the covariant of y.

My question is:
How to perform "covariance analysis" by using R?
After this normalization,we can get the according "normalized y" of every "original y".

All in all,the "normalized y" of every "original y" is what I want indeed.


Thanks a lot!

My best regards!






------------------------------
*******************************************
Xin Meng 
Capitalbio Corporation
National Engineering Research Center 
for Beijing Biochip Technology 
Microarray and Bioinformatics Dept. 
Research Engineer
Tel: +86-10-80715888/80726868-6364/6333 
Fax: +86-10-80726790
EmailÅ£Å∫xmeng at capitalbio.com 
Address:18 Life Science Parkway, 
Changping District, Beijing 102206, China



From andy_liaw at merck.com  Wed May 18 16:57:57 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 18 May 2005 10:57:57 -0400
Subject: [R] How to convert array to c()
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E856@usctmx1106.merck.com>

Is this what you want?

> split(a, row(a))
$"1"
[1] 1 5 9

$"2"
[1]  2  6 10

$"3"
[1]  3  7 11

$"4"
[1]  4  8 12

Andy

> From: Muhammad Subianto
> 
> Dear R-helper,
> 
> Is there possible to make this array:
>  > a <- array(1:12, c(4, 3))
>  > a
>       [,1] [,2] [,3]
> [1,]    1    5    9
> [2,]    2    6   10
> [3,]    3    7   11
> [4,]    4    8   12
>  >
> 
> like:
> c(1,5,9)
> c(2,6,10)
> c(3,7,11)
> c(4,8,12)
> 
> Thank you very much in advance.
> Regards,
> Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From OlsenN at pac.dfo-mpo.gc.ca  Wed May 18 17:15:34 2005
From: OlsenN at pac.dfo-mpo.gc.ca (OlsenN@pac.dfo-mpo.gc.ca)
Date: Wed, 18 May 2005 08:15:34 -0700
Subject: [R] How to convert array to c()
Message-ID: <7CBBD627E4E688499349A5D11D07831602ECB886@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>

Look at ?assign, one possible answer is shown in the examples.  Modified for
your example:

for (i in 1:nrow(a)) {
	nam <- paste("r",i, sep=".")
	assign(nam, a[i,])
}

would give you four separate objects r.1 to r.4 containing the 4 vectors.
Not sure if that's exactly what you wanted though.
Norm

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Muhammad Subianto
Sent: Wednesday, May 18, 2005 7:18 AM
To: r-help at stat.math.ethz.ch
Subject: [R] How to convert array to c()

Dear R-helper,

Is there possible to make this array:
 > a <- array(1:12, c(4, 3))
 > a
      [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12
 >

like:
c(1,5,9)
c(2,6,10)
c(3,7,11)
c(4,8,12)

Thank you very much in advance.
Regards,
Muhammad Subianto

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From apjaworski at mmm.com  Wed May 18 17:39:06 2005
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Wed, 18 May 2005 10:39:06 -0500
Subject: [R] 'fitdistr' and two views of the same data?
In-Reply-To: <cf093f4205051807341ee4915d@mail.gmail.com>
Message-ID: <OF491CD8A0.D915BF7B-ON86257005.00558696-86257005.0055FA5C@mmm.com>






Matt,

There is nothing wrong here.  I rerun your example and got the same
parameters.  Your only "problem" is that you let the density plot use the
default limits on x, which are not reasonable here since your data extends
to over 200.  Try this:

hist(dd, freq=FALSE)  #dd is your original data
xx <- seq(0, 300, by=.1)
yy <- dweibull(xx, shape=1.5079, scale=60.2139)
lines(xx, yy, col=2)

BTW, when you binned your data you also scaled it down by 20, so your scale
parameter changed accordingly.

Cheers,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             Matt Jadud                                                    
             <jadudm at gmail.com                                             
             >                                                          To 
             Sent by:                  R-help at stat.math.ethz.ch            
             r-help-bounces at st                                          cc 
             at.math.ethz.ch                                               
                                                                   Subject 
                                       [R] 'fitdistr' and two views of the 
             05/18/2005 09:34          same data?                          
             AM                                                            
                                                                           
                                                                           
             Please respond to                                             
                Matt Jadud                                                 
             <jadudm at gmail.com                                             
                     >                                                     
                                                                           
                                                                           




Hello,

I have detailed (with pictures and whatnot) my question on my weblog at

http://www.cs-ed.org/blogs/mjadud/archives/2005/05/a_question_abou.html

The short version of the question is this:

When I ask 'fitdistr' to try and fit my distribution as a "weibull"
distribution, it comes up with some rather wacky parameters.

If I take the same distribution, and do something like

newdist <- mapply(function(x) ((x %/% 20) + 1), origdist)

which effectively forces the data into a histogram, 'fitdist' on
'newdist' gives me an entirely different set of parameters.
Distressingly, the parameters it gives me are, upon inspection, good;
that is, the parameters reported fit the distribution of the original
data much better than 'fitdist' of 'origdist'.

Unfortunately, I'm not savvy enough to tease this out beyond
"inspection." The weblog entry has the original distribution, plots,
and whatnot explaining my question in more detail. My question(s) are
repeated at the bottom of the post, as well as this email address. Any
help or insights are appreciated; no doubt, I've done something...
well, wrong.

Many thanks,
Matt

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dimitrijoe at yahoo.com.br  Wed May 18 18:32:37 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Wed, 18 May 2005 13:32:37 -0300
Subject: [R] the rationale for using update()
Message-ID: <000a01c55bc7$363c05d0$6500a8c0@thesahajamach>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050518/698357d4/attachment.pl

From tlumley at u.washington.edu  Wed May 18 18:53:17 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 18 May 2005 09:53:17 -0700 (PDT)
Subject: [R] the rationale for using update()
In-Reply-To: <000a01c55bc7$363c05d0$6500a8c0@thesahajamach>
References: <000a01c55bc7$363c05d0$6500a8c0@thesahajamach>
Message-ID: <Pine.A41.4.61b.0505180952150.341808@homer10.u.washington.edu>

On Wed, 18 May 2005, Dimitri Joe wrote:

> Hi there,
>
> I wonder what is the rationale for using update(): just saving on 
> typing, more efficiency, or something else?
>

It saves on typing (and in particular on typing errors).  Methods for 
"update" could be more efficient than fitting the model from scratch, but 
typically are not.

 	-thomas



From helprhelp at gmail.com  Wed May 18 19:06:30 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 18 May 2005 12:06:30 -0500
Subject: [R] text mining: ttda
Message-ID: <cdf8178305051810067f2d065@mail.gmail.com>

Hi, 
I am working on a text mining project and i am interested in ttda
package. however, I really cannot find the document for this package
in English.
Can anyone give me some help? btw, is there any other package in R
doing text mining. I googled MedlineR which might help my project.
Anyone can give me some links on how to use it too?

thanks,

-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From elvis at xlsolutions-corp.com  Wed May 18 19:20:11 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed, 18 May 2005 10:20:11 -0700
Subject: [R] New course***Advanced Statistical Modelling in R/S-plus
Message-ID: <20050518172011.22842.qmail@gem-wbe03.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  a new 2-day "Advanced Statistical Modelling in R/S-plus"
at 3 locations:


**** Seattle ---------------------------- June 16th - 17th,2005
**** Atlanta ---------------------------- June 23th - 24th,2005
**** Chicago ---------------------------- June 30th - July 1st,2005


Reserve your seat now at the early bird rates! Payment due AFTER the
class

Course Description:

This two-day R/S-plus course focuses on a broad spectrum of Advanced
Statistical
Modelling topics with detailed examples.

With the following outline:


- Trellis/Lattice graphics for data presentation and inspection in R
and Splus
- Overview of Statistical modelling and strategies in R and Splus
- Diagnostics and transformations
- Robust and resistant methods
- Generalized linear modelling: link and variance functions, model
fitting. Residuals
- Logistic regression, Log-linear models
- Quasi-likelihood 'models' and their uses
- Non-linear and smooth regression: Least squares non-linear regression
and  Alternative algorithms
- Penalized likelihood methods: Additive and generalized additive
models: fitting, display and prediction
- Linear mixed effects models.  Model fitting and diagnostic
inspection,  Estimation and prediction
- Generalized linear mixed effects models: fitting procedures and
diagnostic checking
- Non-linear mixed effects models.  Fitting procedures and key examples
- Generalized estimating equations (GEE) methods
- Detailed examples


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced Programming course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From uofiowa at gmail.com  Wed May 18 19:29:00 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 18 May 2005 13:29:00 -0400
Subject: [R] align
In-Reply-To: <971536df050518071937ac4127@mail.gmail.com>
References: <3f87cc6d0505180713704dc236@mail.gmail.com>
	<971536df050518071937ac4127@mail.gmail.com>
Message-ID: <3f87cc6d0505181029288fbd3@mail.gmail.com>

>From the definition of locf
 
> locf
function (x)
{
    if (!inherits(x, "its"))
        stop("function is only valid for objects of class 'its'")
    y <- x
    jna <- which(apply(is.na(x), 2, any))
    for (j in jna) { y[, j] <- y[most.recent(!is.na(y[, j])), j] }
    return(y)
}

Where is the function most.recent? It si not in its.

On 5/18/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Check out the 'locf' function in the 'its' package and the 'na.locf'
> function in the 'zoo' package.
> 
> On 5/18/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> > Is there a function in R that is similar to Splus's align?
> >
> > The idea is, if I have a data.frame, or an its object that is like this:
> >
> > 2002-01-03   5
> > 2002-01-04   NA
> > 2002-01-05   7
> > 2002-01-06   NA
> >
> > I want to align it by the last value to this:
> >
> > 2002-01-03   5
> > 2002-01-04   5
> > 2002-01-05   7
> > 2002-01-06   7
> >
> > TITLE:
> >        Function align
> >
> > USAGE:
> >        align(x, pos, how, error.how, localzone, matchtol, by, k.by,
> >        week.align, holidays)
> >
> > ARGUMENTS:
> >  x:  argument, no default.
> >  pos:  argument, no default.
> >  how:  argument, `default = "NA".'
> >  error.how:  argument, `default = "NA".'
> >  localzone:  argument, `default = F.'
> >  matchtol:  argument, `default = 0.'
> >  by:  argument, no default.
> >  k.by:  argument, `default = 1.'
> >  week.align:  argument, `default = NULL.'
> >  holidays:  argument, `default = timeDate().'
> >
> > DESCRIPTION:
> >
> >        Align series object x to new positions
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From ripley at stats.ox.ac.uk  Wed May 18 19:32:20 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 May 2005 18:32:20 +0100 (BST)
Subject: [R] the rationale for using update()
In-Reply-To: <Pine.A41.4.61b.0505180952150.341808@homer10.u.washington.edu>
References: <000a01c55bc7$363c05d0$6500a8c0@thesahajamach>
	<Pine.A41.4.61b.0505180952150.341808@homer10.u.washington.edu>
Message-ID: <Pine.LNX.4.61.0505181829050.6732@gannet.stats>

On Wed, 18 May 2005, Thomas Lumley wrote:

> On Wed, 18 May 2005, Dimitri Joe wrote:
>
>> I wonder what is the rationale for using update(): just saving on typing, 
>> more efficiency, or something else?
>
> It saves on typing (and in particular on typing errors).  Methods for 
> "update" could be more efficient than fitting the model from scratch, but 
> typically are not.

I'd say readability was the most important reason.  In a 200-char call it 
is hard to see what you changed, but

update(fit, . ~ . -x5 + x6)
update(fit, method="ML")

are self-documenting.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Achim.Zeileis at wu-wien.ac.at  Wed May 18 19:32:29 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 18 May 2005 19:32:29 +0200
Subject: [R] align
In-Reply-To: <3f87cc6d0505181029288fbd3@mail.gmail.com>
References: <3f87cc6d0505180713704dc236@mail.gmail.com>
	<971536df050518071937ac4127@mail.gmail.com>
	<3f87cc6d0505181029288fbd3@mail.gmail.com>
Message-ID: <20050518193229.20d6fe1b.Achim.Zeileis@wu-wien.ac.at>

On Wed, 18 May 2005 13:29:00 -0400 Omar Lakkis wrote:

> >From the definition of locf
>  
> > locf
> function (x)
> {
>     if (!inherits(x, "its"))
>         stop("function is only valid for objects of class 'its'")
>     y <- x
>     jna <- which(apply(is.na(x), 2, any))
>     for (j in jna) { y[, j] <- y[most.recent(!is.na(y[, j])), j] }
>     return(y)
> }
> 
> Where is the function most.recent? It si not in its.

Yes, it is, but it is not exported. See

R> its::most.recent
Error: 'most.recent' is not an exported object from 'namespace:its'
R> its:::most.recent
function (x) 
{
    if (!is.logical(x)) 
        stop("x must be logical")
    x.pos <- which(x)
    if (length(x.pos) == 0 || x.pos[1] != 1) 
        x.pos <- c(1, x.pos)
    rep(x.pos, c(diff(x.pos), length(x) - x.pos[length(x.pos)] + 
        1))
}
<environment: namespace:its>

Z

> On 5/18/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Check out the 'locf' function in the 'its' package and the 'na.locf'
> > function in the 'zoo' package.
> > 
> > On 5/18/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> > > Is there a function in R that is similar to Splus's align?
> > >
> > > The idea is, if I have a data.frame, or an its object that is like
> > > this:
> > >
> > > 2002-01-03   5
> > > 2002-01-04   NA
> > > 2002-01-05   7
> > > 2002-01-06   NA
> > >
> > > I want to align it by the last value to this:
> > >
> > > 2002-01-03   5
> > > 2002-01-04   5
> > > 2002-01-05   7
> > > 2002-01-06   7
> > >
> > > TITLE:
> > >        Function align
> > >
> > > USAGE:
> > >        align(x, pos, how, error.how, localzone, matchtol, by,
> > >        k.by, week.align, holidays)
> > >
> > > ARGUMENTS:
> > >  x:  argument, no default.
> > >  pos:  argument, no default.
> > >  how:  argument, `default = "NA".'
> > >  error.how:  argument, `default = "NA".'
> > >  localzone:  argument, `default = F.'
> > >  matchtol:  argument, `default = 0.'
> > >  by:  argument, no default.
> > >  k.by:  argument, `default = 1.'
> > >  week.align:  argument, `default = NULL.'
> > >  holidays:  argument, `default = timeDate().'
> > >
> > > DESCRIPTION:
> > >
> > >        Align series object x to new positions
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed May 18 19:36:50 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 18 May 2005 13:36:50 -0400
Subject: [R] align
In-Reply-To: <3f87cc6d0505181029288fbd3@mail.gmail.com>
References: <3f87cc6d0505180713704dc236@mail.gmail.com>
	<971536df050518071937ac4127@mail.gmail.com>
	<3f87cc6d0505181029288fbd3@mail.gmail.com>
Message-ID: <971536df050518103669f9606d@mail.gmail.com>

Its in 'its' but due to the use of namespaces in 'its'
one must do this to access it directly:

its:::most.recent

On 5/18/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> From the definition of locf
> 
> > locf
> function (x)
> {
>    if (!inherits(x, "its"))
>        stop("function is only valid for objects of class 'its'")
>    y <- x
>    jna <- which(apply(is.na(x), 2, any))
>    for (j in jna) { y[, j] <- y[most.recent(!is.na(y[, j])), j] }
>    return(y)
> }
> 
> Where is the function most.recent? It si not in its.
> 
> On 5/18/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Check out the 'locf' function in the 'its' package and the 'na.locf'
> > function in the 'zoo' package.
> >
> > On 5/18/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> > > Is there a function in R that is similar to Splus's align?
> > >
> > > The idea is, if I have a data.frame, or an its object that is like this:
> > >
> > > 2002-01-03   5
> > > 2002-01-04   NA
> > > 2002-01-05   7
> > > 2002-01-06   NA
> > >
> > > I want to align it by the last value to this:
> > >
> > > 2002-01-03   5
> > > 2002-01-04   5
> > > 2002-01-05   7
> > > 2002-01-06   7
> > >
> > > TITLE:
> > >        Function align
> > >
> > > USAGE:
> > >        align(x, pos, how, error.how, localzone, matchtol, by, k.by,
> > >        week.align, holidays)
> > >
> > > ARGUMENTS:
> > >  x:  argument, no default.
> > >  pos:  argument, no default.
> > >  how:  argument, `default = "NA".'
> > >  error.how:  argument, `default = "NA".'
> > >  localzone:  argument, `default = F.'
> > >  matchtol:  argument, `default = 0.'
> > >  by:  argument, no default.
> > >  k.by:  argument, `default = 1.'
> > >  week.align:  argument, `default = NULL.'
> > >  holidays:  argument, `default = timeDate().'
> > >
> > > DESCRIPTION:
> > >
> > >        Align series object x to new positions
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
>



From subianto at gmail.com  Wed May 18 19:40:52 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Wed, 18 May 2005 19:40:52 +0200
Subject: [R] How to convert array to c()
In-Reply-To: <7CBBD627E4E688499349A5D11D07831602ECB886@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>
References: <7CBBD627E4E688499349A5D11D07831602ECB886@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>
Message-ID: <428B7E24.2090601@gmail.com>

Dear all,
Many thanks for your help.
Regards,
Muhammad Subianto

On this day 5/18/2005 4:57 PM, Liaw, Andy wrote:
 > Is this what you want?
 >
 >
 >>split(a, row(a))
 >
 > $"1"
 > [1] 1 5 9
 >
 > $"2"
 > [1]  2  6 10
 >
 > $"3"
 > [1]  3  7 11
 >
 > $"4"
 > [1]  4  8 12
 >
 > Andy


On this day 5/18/2005 5:15 PM, OlsenN at pac.dfo-mpo.gc.ca wrote:
> Look at ?assign, one possible answer is shown in the examples.  Modified for
> your example:
> 
> for (i in 1:nrow(a)) {
> 	nam <- paste("r",i, sep=".")
> 	assign(nam, a[i,])
> }
> 
> would give you four separate objects r.1 to r.4 containing the 4 vectors.
> Not sure if that's exactly what you wanted though.
> Norm



From LI at nsabp.pitt.edu  Wed May 18 20:24:26 2005
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Wed, 18 May 2005 14:24:26 -0400
Subject: [R] How to convert array to c()
Message-ID: <3D0B2434377E984E9C85CAA316F8B183018A3AEE@nsabpmail>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050518/32bee000/attachment.pl

From lisawang at uhnres.utoronto.ca  Wed May 18 20:38:40 2005
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Wed, 18 May 2005 14:38:40 -0400
Subject: [R] Why can't I download "window binary" zip packages
Message-ID: <428B8BB0.8070109@uhnres.utoronto.ca>

Hello there,

I tried so many times to download windows binary zip package but it told 
me that I don't have the access to do so. It worked for me a few months 
ago. Please help me with it.

Thank you

Lisa Wang Msc.
Princess Margaret Hospital
Toronto , Canada
tel: (416) 946 4501 ext.5201



From murdoch at stats.uwo.ca  Wed May 18 20:47:02 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 18 May 2005 19:47:02 +0100
Subject: [R] Why can't I download "window binary" zip packages
In-Reply-To: <428B8BB0.8070109@uhnres.utoronto.ca>
References: <428B8BB0.8070109@uhnres.utoronto.ca>
Message-ID: <428B8DA6.10401@stats.uwo.ca>

Lisa Wang wrote:
> Hello there,
> 
> I tried so many times to download windows binary zip package but it told 
> me that I don't have the access to do so. It worked for me a few months 
> ago. Please help me with it.

You need to talk to one of your system administrators.  R won't tell you 
that you don't have access, but your own system might.

Duncan Murdoch

> 
> Thank you
> 
> Lisa Wang Msc.
> Princess Margaret Hospital
> Toronto , Canada
> tel: (416) 946 4501 ext.5201
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Wed May 18 18:14:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 18 May 2005 12:14:09 -0400
Subject: [R] 'fitdistr' and two views of the same data?
In-Reply-To: <OF491CD8A0.D915BF7B-ON86257005.00558696-86257005.0055FA5C@mmm.com>
References: <cf093f4205051807341ee4915d@mail.gmail.com>
	<OF491CD8A0.D915BF7B-ON86257005.00558696-86257005.0055FA5C@mmm.com>
Message-ID: <971536df05051809146dc1986e@mail.gmail.com>

Note that the gamma distribution seems to work well here.  After running
Andy's code run this to see both superimposed on the same graph:

gparms <- fitdistr(dd, "gamma")[[1]]
yyg <- dgamma(xx, gparms[1], gparms[2])
lines(xx, yyg, col=3)


On 5/18/05, apjaworski at mmm.com <apjaworski at mmm.com> wrote:
> 
> 
> Matt,
> 
> There is nothing wrong here.  I rerun your example and got the same
> parameters.  Your only "problem" is that you let the density plot use the
> default limits on x, which are not reasonable here since your data extends
> to over 200.  Try this:
> 
> hist(dd, freq=FALSE)  #dd is your original data
> xx <- seq(0, 300, by=.1)
> yy <- dweibull(xx, shape=1.5079, scale=60.2139)
> lines(xx, yy, col=2)
> 
> BTW, when you binned your data you also scaled it down by 20, so your scale
> parameter changed accordingly.
> 
> Cheers,
> 
> Andy
> 
> __________________________________
> Andy Jaworski
> 518-1-01
> Process Laboratory
> 3M Corporate Research Laboratory
> -----
> E-mail: apjaworski at mmm.com
> Tel:  (651) 733-6092
> Fax:  (651) 736-3122
> 
>             Matt Jadud
>             <jadudm at gmail.com
>             >                                                          To
>             Sent by:                  R-help at stat.math.ethz.ch
>             r-help-bounces at st                                          cc
>             at.math.ethz.ch
>                                                                   Subject
>                                       [R] 'fitdistr' and two views of the
>             05/18/2005 09:34          same data?
>             AM
> 
>             Please respond to
>                Matt Jadud
>             <jadudm at gmail.com
>                     >
> 
> Hello,
> 
> I have detailed (with pictures and whatnot) my question on my weblog at
> 
> http://www.cs-ed.org/blogs/mjadud/archives/2005/05/a_question_abou.html
> 
> The short version of the question is this:
> 
> When I ask 'fitdistr' to try and fit my distribution as a "weibull"
> distribution, it comes up with some rather wacky parameters.
> 
> If I take the same distribution, and do something like
> 
> newdist <- mapply(function(x) ((x %/% 20) + 1), origdist)
> 
> which effectively forces the data into a histogram, 'fitdist' on
> 'newdist' gives me an entirely different set of parameters.
> Distressingly, the parameters it gives me are, upon inspection, good;
> that is, the parameters reported fit the distribution of the original
> data much better than 'fitdist' of 'origdist'.
> 
> Unfortunately, I'm not savvy enough to tease this out beyond
> "inspection." The weblog entry has the original distribution, plots,
> and whatnot explaining my question in more detail. My question(s) are
> repeated at the bottom of the post, as well as this email address. Any
> help or insights are appreciated; no doubt, I've done something...
> well, wrong.
> 
> Many thanks,
> Matt
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From lauraholt_983 at hotmail.com  Wed May 18 21:16:29 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Wed, 18 May 2005 14:16:29 -0500
Subject: [R] lattice plots question
Message-ID: <BAY10-F10258FB20ED23A7C21C522D6170@phx.gbl>

Dear R People:

Is there any way to have the background of lattice plots be white instead of 
grey, please?

This is not a criticism by any means...the lattice stuff is UNbelievable!

Thanks,
Laura Holt
mailto: lauraholt_983 at hotmail.com

R Version 2.1.0 Windows



From sundar.dorai-raj at pdf.com  Wed May 18 21:23:35 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 18 May 2005 14:23:35 -0500
Subject: [R] lattice plots question
In-Reply-To: <BAY10-F10258FB20ED23A7C21C522D6170@phx.gbl>
References: <BAY10-F10258FB20ED23A7C21C522D6170@phx.gbl>
Message-ID: <428B9637.8050906@pdf.com>



Laura Holt wrote:
> Dear R People:
> 
> Is there any way to have the background of lattice plots be white 
> instead of grey, please?
> 
> This is not a criticism by any means...the lattice stuff is UNbelievable!
> 
> Thanks,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> R Version 2.1.0 Windows
> 


Hi, Laura,

See ?trellis.par.set or ?trellis.device. Lattice themes are what you are 
looking for.

e.g.

library(lattice)
trellis.device(windows, theme = col.whitebg())
xyplot(0 ~ 0)

You can also create your own themes, which I find extremely useful:

sundar.theme <- function() {
   par <- col.whitebg()
   par$strip.background$col <- rep("#000099", 7)
   par$add.text$col <- "#eeeeaa"
   par$add.text$font <- 2
   par$background$col <- "#ffffff"
   par$superpose.line$lty <- rep(1, 7)
   par$superpose.line$col[1:2] <- c("#880000", "#008800")
   par$superpose.symbol$col[1:2] <- c("#880000", "#008800")
   par
}

HTH,

--sundar



From Pierre.Lapointe at nbf.ca  Wed May 18 21:40:55 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Wed, 18 May 2005 15:40:55 -0400
Subject: [R] Why can't I download "window binary" zip packages
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF357B5@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050518/5ce84af2/attachment.pl

From kubovy at virginia.edu  Wed May 18 22:50:49 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 18 May 2005 16:50:49 -0400
Subject: [R] Advice requested: installation failing for foreign_0.8-7,
	mgcv_1.2-4, rpart_3.1-23, VR_7.2-15 
In-Reply-To: <200505181004.j4IA4CUM018416@hypatia.math.ethz.ch>
References: <200505181004.j4IA4CUM018416@hypatia.math.ethz.ch>
Message-ID: <e58eed4979a846e95cc862debfb20830@virginia.edu>

I'm running R Version 2.1.0  (2005-04-18), ISBN 3-900051-07-0

on Machine Model:	Power Mac G5
   CPU Type:	PowerPC 970  (2.2)
   Number Of CPUs:	2
   CPU Speed:	2 GHz
   L2 Cache (per CPU):	512 KB
   Memory:	2 GB

  with software:  System Version:	Mac OS X 10.3.9 (7W98)
   Kernel Version:	Darwin 7.9.0

Here is what happened when I tried to update the "foreign"  package  
(similar failures for the other packages in the Subject line).

* Installing *source* package 'foreign' ...
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking whether gcc accepts -Wno-long-long... yes
checking how to run the C preprocessor... gcc -E
checking for egrep... grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking byteswap.h usability... no
checking byteswap.h presence... no
checking for byteswap.h... no
checking for double... yes
checking size of double... 8
checking for int... yes
checking size of int... 4
checking for long... yes
checking size of long... 4
configure: creating ./config.status
config.status: creating src/Makevars
config.status: creating src/swap_bytes.h
config.status: creating src/var.h
** libs
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c R_systat.c  
-o R_systat.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c Rdbfread.c  
-o Rdbfread.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c  
Rdbfwrite.c -o Rdbfwrite.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c SASxport.c  
-o SASxport.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c avl.c -o  
avl.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c dbfopen.c  
-o dbfopen.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c  
file-handle.c -o file-handle.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c format.c  
-o format.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c minitab.c  
-o minitab.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c pfm-read.c  
-o pfm-read.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c sfm-read.c  
-o sfm-read.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c spss.c -o  
spss.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  
  -I/usr/local/include  -Wno-long-long -fno-common  -g -O2 -c  
stataread.c -o stataread.o
gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o  
foreign.so R_systat.o Rdbfread.o Rdbfwrite.o SASxport.o avl.o dbfopen.o  
file-handle.o format.o minitab.o pfm-read.o sfm-read.o spss.o  
stataread.o  -lcc_dynamic -framework R
ld: warning multiple definitions of symbol _xerbla_
/Library/Frameworks/R.framework/R(print.lo) definition of _xerbla_
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/ 
vecLib.framework/Versions/A/libBLAS.dylib(single module) definition of  
_xerbla_
ld: warning multiple definitions of symbol _BC
/Library/Frameworks/R.framework/Versions/2.1.0/Resources/lib/ 
libreadline.5.0.dylib(terminal.so) definition of _BC
/usr/lib/libncurses.5.dylib(lib_termcap.o) definition of _BC
ld: warning multiple definitions of symbol _UP
/Library/Frameworks/R.framework/Versions/2.1.0/Resources/lib/ 
libreadline.5.0.dylib(terminal.so) definition of _UP
/usr/lib/libncurses.5.dylib(lib_termcap.o) definition of _UP
ld: warning multiple definitions of symbol _PC
/Library/Frameworks/R.framework/Versions/2.1.0/Resources/lib/ 
libreadline.5.0.dylib(terminal.so) definition of _PC
/usr/lib/libncurses.5.dylib(lib_tputs.o) definition of _PC
** R
** inst
** preparing package for lazy loading
Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library  
'/Library/Frameworks/R.framework/Resources/library/grDevices/libs/ 
grDevices.so':
   dlcompat: dyld: /Library/Frameworks/R.framework/Resources/bin/exec/R  
version mismatch for library: /usr/local/lib/libxml2.2.dylib  
(compatibility version of user: 9.0.0 greater than library's version:  
8.0.0)
Loading required package: grDevices
Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library  
'/Library/Frameworks/R.framework/Resources/library/grDevices/libs/ 
grDevices.so':
   dlcompat: dyld: /Library/Frameworks/R.framework/Resources/bin/exec/R  
version mismatch for library: /usr/local/lib/libxml2.2.dylib  
(compatibility version of user: 9.0.0 greater than library's version:  
8.0.0)
In addition: Warning message:
package grDevices in options("defaultPackages") was not found
Error: package 'grDevices' could not be loaded
Execution halted
ERROR: lazy loading failed for package 'foreign'
** Removing  
'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/ 
foreign'
** Restoring previous  
'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/ 
foreign'



From kwright at eskimo.com  Wed May 18 23:55:18 2005
From: kwright at eskimo.com (kwright@eskimo.com)
Date: Wed, 18 May 2005 14:55:18 -0700 (PDT)
Subject: [R] SAMM package for mixed models
Message-ID: <26503.170.54.59.167.1116453318.squirrel@170.54.59.167>


First, a disclaimer.  I am not affiliatied with the SAMM package.  I am
only a user of the package, but I have been contacted (off list) by people
requesting information about SAMM and so I am posting this information
here.

SAMM is software for fitting mixed models.  Versions are available for
both S-Plus and R.  More information and downloads of the software (and
manual) are available here:
   http://www.dpi.qld.gov.au/fieldcrops/14715.html
This URL appears not to be indexed by Google, which is part of the reason
I am posting this.


Here are some personal, random notes about the package:

SAMM is commercial software and requires a (non-free) license.  You can
test the software for free for 30 days.

SAMM is short for Spatial Analysis Mixed Models.

SAMM estimates variance components under a general linear mixed model by
REML.  In particular, the Average Information REML algorithm is used along
with a sparse-matrix representation of matrices.

For some types of problems, I have seen SAMM converge 100 to 1000 times
faster than PROC MIXED or lme, which makes analysis of large datasets /
complex models possible (sometimes in nearly real-time).  'Amazing' is a
word that comes to mind.  (Side note: I have heard rumors that SAS has
hired a developer to look at the Average Information REML technique...)

SAMM can fit two-dimensional spatial structures (such as AR1xAR1) and can
plot two-dimensional variograms.

The 'engine' for the mixed-models in SAMM is the same one used by Genstat
and ASREML.

Like all mixed-models software, SAMM has quirks such as convergence
issues, degrees of freedom, model-specification, etc.

The user community is small, so resources like email lists are limited.

Some types of linear models can be fit using either lme/lme4 or SAMM. 
There are some big differences between SAMM and lme, however (cost,
graphics, support, community, types of tests of fixed effects, etc.).

Using both SAMM and lme to fit a model can be an experience that is
tedious/painful but ultimately rewarding in a deeper understanding of the
modelling process.

SAMM has its origins in ASREML, which comes from a plant-breeding
background.  Although SAMM can be a general-purpose package, the focus is
on evaluation of field experiments.  For that purpose, it is an excellent
tool.


Kevin Wright



From uofiowa at gmail.com  Wed May 18 22:58:13 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 18 May 2005 16:58:13 -0400
Subject: [R] loop
Message-ID: <3f87cc6d05051813585fb249bc@mail.gmail.com>

Rather than using a loop, how can I remove all consequentially
repeated values as in this example?
I am guessing using diff would help but not quite sure how.

> get
> s
        date          f  
1 1999-01-01 1
2 1999-01-02 1
3 1999-01-03 1
4 1999-01-04 2
5 1999-01-05 2

> v <- s[1,'f']; for (i in 2:nrow(s)) { if (s[i,'f'] == v) s[i,'f'] <- NA else v <- s[i,'f'] }
> s <- s[!is.na(s$f),]
> s
        date          f
1 1999-01-01 1
4 1999-01-04 2



From helprhelp at gmail.com  Thu May 19 00:17:14 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 18 May 2005 17:17:14 -0500
Subject: [R] Re: text mining: ttda
In-Reply-To: <cdf8178305051810067f2d065@mail.gmail.com>
References: <cdf8178305051810067f2d065@mail.gmail.com>
Message-ID: <cdf817830505181517694122a8@mail.gmail.com>

Can anyone suggest some good text mining reference or books?

thanks,

weiwei

On 5/18/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> Hi,
> I am working on a text mining project and i am interested in ttda
> package. however, I really cannot find the document for this package
> in English.
> Can anyone give me some help? btw, is there any other package in R
> doing text mining. I googled MedlineR which might help my project.
> Anyone can give me some links on how to use it too?
> 
> thanks,
> 
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From pshannon at systemsbiology.org  Thu May 19 00:18:10 2005
From: pshannon at systemsbiology.org (Paul Shannon)
Date: 18 May 2005 15:18:10 -0700
Subject: [R] source-only package,
	but still: Error: package 'simple' was built for
	powerpc-apple-darwin7.9.0
Message-ID: <EXCHANGEJqAJ7nvOYFg0001ca05@exchange.systemsbiology.net>

I have a number simple R functions written for the biologists I work
with.  These functions will evolve, and the documentation will get
steadily better.  I hope to put them in a package on a local
webserver, along with lots of help files, so that the users can easily
update their installation.

The functions are pure R, with no compiled code, but alias, I cannot figure out
how to build this package so that it is portable across operating
systems.

It looks like 'R CMD build' inserts the following line into the resulting package:

     Built: R 2.1.0; powerpc-apple-darwin7.9.0; 2005-05-18 14:28:36; unix

making it impossible for me to later load the package into R running under another OS.
I see this error when I execute  library ('simple') under linux:

   Error: package 'simple' was built for powerpc-apple-darwin7.9.0

I'll be grateful if anyone can help me work around this restriction  ... or
let me know how I am misconstruing the evidence.

Thank you!

 - Paul Shannon
   Institute for Systems Biology
   Seattle



From ripley at stats.ox.ac.uk  Thu May 19 00:26:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 May 2005 23:26:26 +0100 (BST)
Subject: [R] loop
In-Reply-To: <3f87cc6d05051813585fb249bc@mail.gmail.com>
References: <3f87cc6d05051813585fb249bc@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505182322200.22918@gannet.stats>

On Wed, 18 May 2005, Omar Lakkis wrote:

> Rather than using a loop, how can I remove all consequentially
> repeated values as in this example?
> I am guessing using diff would help but not quite sure how.
>
>> get
>> s
>        date          f
> 1 1999-01-01 1
> 2 1999-01-02 1
> 3 1999-01-03 1
> 4 1999-01-04 2
> 5 1999-01-05 2
>
>> v <- s[1,'f']; for (i in 2:nrow(s)) { if (s[i,'f'] == v) s[i,'f'] <- NA else v <- s[i,'f'] }
>> s <- s[!is.na(s$f),]
>> s
>        date          f
> 1 1999-01-01 1
> 4 1999-01-04 2

I am not sure I see the pattern, but it might be one of

s[!duplicated(s$f), ]

s[diff(c(0, s$f)) != 0, ]

(which differ).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From huhrik.beybutov at gmail.com  Thu May 19 00:36:22 2005
From: huhrik.beybutov at gmail.com (Huhrik Beybutov)
Date: Wed, 18 May 2005 14:36:22 -0800
Subject: [R] Fitting linear model to the matrix
Message-ID: <f1395e9805051815362df8a390@mail.gmail.com>

I'm afraid I need your help with linear models in R, I apologize in
advance is this question was beaten to death.

I'm trying to fit Y( i, j ), where i is row, j is column. My model is
log [ Y( i, j ) ] = log A( i ) * B( j )+ C( i ) ]  + error.  I have i
x j  observations, I need to find 2*i + j parameters.

What would be the best way to approach this? 

Thank you!

- H.



From sms13+ at pitt.edu  Thu May 19 00:39:26 2005
From: sms13+ at pitt.edu (sms13+@pitt.edu)
Date: Wed, 18 May 2005 18:39:26 -0400
Subject: [R] from list to dataframe
Message-ID: <1837865343.1116441566@Lab26.DOMAIN.IE.PITT.EDU>

I was wondering if someone can help me figure out the following:
I have two patient datasets, ds1 and ds2.  ds1 has fields "patid", "date", 
and "lab1".  ds2 has "patid", "date", and "lab2".  I want to find all the 
patids that have at least 2 dated records for each lab.  I started by 
splitting each dataset by patid, to create ds1.list and ds2.list.  Then I 
did some processing (with sapply) to each list to get the lengths of each 
patient list item.  Then I kind of lost my way and things got messy as I 
tried to extract just the patids of those with lengths >= 2, convert them 
to dataframes (which I didn't have much success with), and then merge the 
two dataframes to get a vector of the desired patids.  Any help would be 
much appreciated.

Thanks,
Steven



From w.northcott at unsw.edu.au  Thu May 19 00:48:32 2005
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Thu, 19 May 2005 08:48:32 +1000
Subject: [R] Fortran 95 in R ?
In-Reply-To: <200505181010.j4IA4fkj018505@hypatia.math.ethz.ch>
References: <200505181010.j4IA4fkj018505@hypatia.math.ethz.ch>
Message-ID: <D77D4378-BE5D-4B65-8F72-283CD6DBCEB4@unsw.edu.au>

On 18/05/2005, at 8:10 PM, Joel Bremson wrote:
> Is it possible to run Fortran 95 code from R? I don't think so, but
> hopefully someone can prove me wrong.
>

This is not really any issue with R.  It is matter of what compilers  
you have installed.

If you have Fortran 95 compiler and you use it to build R, then  
within R 'F77' will refer to that compiler and you should be able to  
use f95 code.

Currently I think it is correct that all the available binary  
distributions of R use  g77 from the gcc 3.x.x compiler suite.    
gcc-4.0.0 has recently been released and this includes the new  
gfortran compiler which supports F90 and F95.  gfortran is not as  
mature a product as g77 and may give you problems.  Current cvs code  
seems to usable for many purposes, and by the time gcc-4.1 is  
released later this year, gfortran should be quite usable.

Bill Northcott



From ripley at stats.ox.ac.uk  Thu May 19 01:12:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 May 2005 00:12:24 +0100 (BST)
Subject: [R] source-only package, but still: Error: package 'simple' was
	built for powerpc-apple-darwin7.9.0
In-Reply-To: <EXCHANGEJqAJ7nvOYFg0001ca05@exchange.systemsbiology.net>
References: <EXCHANGEJqAJ7nvOYFg0001ca05@exchange.systemsbiology.net>
Message-ID: <Pine.LNX.4.61.0505190006310.26373@gannet.stats>

On Wed, 18 May 2005, Paul Shannon wrote:

> I have a number simple R functions written for the biologists I work
> with.  These functions will evolve, and the documentation will get
> steadily better.  I hope to put them in a package on a local
> webserver, along with lots of help files, so that the users can easily
> update their installation.

We encourage you to use a repository, as in the article in the current 
R-news.

> The functions are pure R, with no compiled code, but alias, I cannot 
> figure out how to build this package so that it is portable across 
> operating systems.
>
> It looks like 'R CMD build' inserts the following line into the 
> resulting package:
>
>     Built: R 2.1.0; powerpc-apple-darwin7.9.0; 2005-05-18 14:28:36; unix
>
> making it impossible for me to later load the package into R running 
> under another OS. I see this error when I execute library ('simple') 
> under linux:
>
>   Error: package 'simple' was built for powerpc-apple-darwin7.9.0
>
> I'll be grateful if anyone can help me work around this restriction  ... or
> let me know how I am misconstruing the evidence.

You are.  It is R CMD INSTALL that inserts such a line, and it does not do 
mention the platform if there is no src directory.  On my system there are 
281 such packages.  To pick one at random: try installing 'fda'.

I suspect your package has structure it is not using.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.connolly at hortresearch.co.nz  Thu May 19 01:18:12 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 19 May 2005 11:18:12 +1200
Subject: [R] Bias to do with search engines
In-Reply-To: <26503.170.54.59.167.1116453318.squirrel@170.54.59.167>
References: <26503.170.54.59.167.1116453318.squirrel@170.54.59.167>
Message-ID: <20050518231812.GQ5776@hortresearch.co.nz>

On Wed, 18-May-2005 at 02:55PM -0700, kwright at eskimo.com wrote:

|> 
|> First, a disclaimer.  I am not affiliatied with the SAMM package.  I am
|> only a user of the package, but I have been contacted (off list) by people
|> requesting information about SAMM and so I am posting this information
|> here.
|> 
|> SAMM is software for fitting mixed models.  Versions are available for
|> both S-Plus and R.  More information and downloads of the software (and
|> manual) are available here:
|>    http://www.dpi.qld.gov.au/fieldcrops/14715.html
|> This URL appears not to be indexed by Google, which is part of the reason
|> I am posting this.

Old timers who haven't forgotten about AltaVista will be able to find
it -- though it's still only 4th on the list even when I did a search
for the string "Spatial Analysis Mixed Models".  What's more, the word
"spatial" doesn't even appear in the three that are listed before it.

Somehow, southern hemisphere sites aren't taken as seriously as those
from that other hemisphere.  Is there a word for that type of bias?


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From sinha at stat.tamu.edu  Thu May 19 01:23:42 2005
From: sinha at stat.tamu.edu (Samiran Sinha)
Date: Wed, 18 May 2005 18:23:42 -0500
Subject: [R] Call R from Fortran 
Message-ID: <428BCE7E.8020604@stat.tamu.edu>

Hello,

I need to call a R function from Fortran 77 program. How will I do that 
exactly?
I will grately  appreciate any help.
Sincerely,

-- 
Samiran Sinha, Ph.D
Assistant Professor
Department of Statistics
Texas A&M University
TAMU 3143
College Station, TX-77843

Phone Number: (979) 845 2966(O)
              (979) 696 9530(H)



From p.murrell at auckland.ac.nz  Thu May 19 01:48:43 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 19 May 2005 11:48:43 +1200
Subject: [R] Line width through plot size reduction
References: <428ADC7A.2040901@cirad.fr>
Message-ID: <428BD45B.8000801@stat.auckland.ac.nz>

Hi


Jacques VESLOT wrote:
> Dear R-users,
> 
> Someone, who uses R under Mac, wants to insert a couple of small plots 
> (each with several lines) in an article, but he has to reduce plots' 
> size significantly. He did it (in pdf or enc. ps) but, unfortunately, 
> everything is reduced but lines' width. Besides, 'lwd' argument in par() 
> or plot() can't be less than one.


IANAMU (I am not a Mac user) either, but you should definitely be able 
to set lwd to less than 1.   What happens when you try par(lwd=.5) or 
plot(1:10, type="l", lwd=.5)?

Paul


> I am not a Mac user and I don't know whether it is a Mac-related problem 
> or not.
> 
> Could somebody please give me a way to solve this problem, either to 
> reduce line width under R, or to get reduced lines' width when reducing 
> plot size ?
> 
> Thanks,
> 
> Jacques VESLOT
> Cirad
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From bitwrit at ozemail.com.au  Thu May 19 12:40:42 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 19 May 2005 10:40:42 +0000
Subject: [R] Bias to do with search engines
In-Reply-To: <20050518231812.GQ5776@hortresearch.co.nz>
References: <26503.170.54.59.167.1116453318.squirrel@170.54.59.167>
	<20050518231812.GQ5776@hortresearch.co.nz>
Message-ID: <428C6D2A.20809@ozemail.com.au>

Patrick Connolly wrote:
> ... 
> Somehow, southern hemisphere sites aren't taken as seriously as those
> from that other hemisphere.  Is there a word for that type of bias?
> 
Borealoprerogative, of course.

Jim



From bitwrit at ozemail.com.au  Thu May 19 12:40:55 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 19 May 2005 10:40:55 +0000
Subject: [R] standardization
In-Reply-To: <x2oeb8br4n.fsf@turmalin.kubism.ku.dk>
References: <428B3624.60209@csca.ryerson.ca>	<x2wtpwbsyn.fsf@turmalin.kubism.ku.dk>	<428B404F.5040302@lancaster.ac.uk>
	<x2oeb8br4n.fsf@turmalin.kubism.ku.dk>
Message-ID: <428C6D37.9000101@ozemail.com.au>

Peter Dalgaard wrote:
> Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:
> 
> 
> 
>>"The nice thing about standards is that there are so many of
>>them to choose from",
> 
> 
> Curiously enough, the same quote came up today on dk.edb.system.unix
> in the context of translations. 
> 
"The nice thing about coincidences is that there are so many ways for 
them to happen."

Jim



From Tom.Mulholland at dpi.wa.gov.au  Thu May 19 04:59:46 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 19 May 2005 10:59:46 +0800
Subject: [R] Call R from Fortran 
Message-ID: <4702645135092E4497088F71D9C8F51A128B6C@afhex01.dpi.wa.gov.au>

I don't know, but I do know that if you search the mailing list the answer is there.

One place to start might be http://finzi.psych.upenn.edu/R/Rhelp02a/archive/50776.html

I found this by searching for "fortran" on the mailing list.

You might also read the posting guide. The link above refers to a particular operating system which may or may not be be the one you are using. I think you might also need to read "Writing R Extensions"

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Samiran Sinha
> Sent: Thursday, 19 May 2005 7:24 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Call R from Fortran 
> 
> 
> Hello,
> 
> I need to call a R function from Fortran 77 program. How will 
> I do that 
> exactly?
> I will grately  appreciate any help.
> Sincerely,
> 
> -- 
> Samiran Sinha, Ph.D
> Assistant Professor
> Department of Statistics
> Texas A&M University
> TAMU 3143
> College Station, TX-77843
> 
> Phone Number: (979) 845 2966(O)
>               (979) 696 9530(H)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From weigand.stephen at charter.net  Thu May 19 05:13:27 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Wed, 18 May 2005 22:13:27 -0500
Subject: [R] from list to dataframe
In-Reply-To: <1837865343.1116441566@Lab26.DOMAIN.IE.PITT.EDU>
References: <1837865343.1116441566@Lab26.DOMAIN.IE.PITT.EDU>
Message-ID: <b8cf3c6e30b8d0b5237a8587ca425fce@charter.net>


On May 18, 2005, at 5:39 PM, sms13+ at pitt.edu wrote:

> I was wondering if someone can help me figure out the following:
> I have two patient datasets, ds1 and ds2.  ds1 has fields "patid", 
> "date", and "lab1".  ds2 has "patid", "date", and "lab2".  I want to 
> find all the patids that have at least 2 dated records for each lab.  
> I started by splitting each dataset by patid, to create ds1.list and 
> ds2.list.  Then I did some processing (with sapply) to each list to 
> get the lengths of each patient list item.  Then I kind of lost my way 
> and things got messy as I tried to extract just the patids of those 
> with lengths >= 2, convert them to dataframes (which I didn't have 
> much success with), and then merge the two dataframes to get a vector 
> of the desired patids.  Any help would be much appreciated.
>
> Thanks,
> Steven

Steven,

I might not exactly understand your problem, but for
what it's worth, you could try to identify the patients
in ds1 who appear at least twice and identify the patients
in ds2 who appear at least twice via

ptid1 <- c("A", "A", "B", "C", "D", "D")
keep1 <- names(table(ptid1))[table(ptid1) >= 2]
keep1

or if ptid is numeric

ptid1 <- c(1, 1, 2, 3, 4, 4)
keep1 <- as.numeric(names(table(ptid1))[table(ptid1) >= 2])
keep1

then subset the respective data sets via

ds1.keep <- subset(ds1, ptid %in% intersect(keep1, keep2))
ds2.keep <- subset(ds2, ptid %in% intersect(keep1, keep2))

then use merge().

Good luck!

Stephen



From Nongluck.Klibbua at newcastle.edu.au  Thu May 19 05:10:29 2005
From: Nongluck.Klibbua at newcastle.edu.au (Nongluck Klibbua)
Date: Thu, 19 May 2005 13:10:29 +1000
Subject: [R] how to export data in the order
Message-ID: <s28c9053.036@MC-GWDOM2.newcastle.edu.au>

hi all,
My data is y[j,k] where j=1,...5, k=1,....,10 and when I use
cat(y,file="c:data.txt",sep="\n"). The data is coming out into the
data.txt is 
y[1,1],y[2,1],....,y[5,1],y[2,1],y[2,2],....,y[5,2],.....,y[j,k]. How I
can export the data by my data is
y[1,1],y[1,2],...y[1,5],y[2,1],[2,2],.....y[j,k].
Appreciate your time.
nongluck



From weigand.stephen at charter.net  Thu May 19 05:35:07 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Wed, 18 May 2005 22:35:07 -0500
Subject: [R] extract with condition
In-Reply-To: <428B14C4.6010606@bordeaux.inra.fr>
References: <428B14C4.6010606@bordeaux.inra.fr>
Message-ID: <11639bf177c468c5402ae05c750e3bf7@charter.net>


On May 18, 2005, at 5:11 AM, Jonathan Charrier wrote:

> hello everybody,
> i want to extract the name of the rows for the value = "TRUE"
> these names give for the next program a list of file
> i try many functions, and indexation but no solution give to me, i 
> transform only the matrix.
> thanks a lot
> Jonathan Charrier
>
> nmodT <- Y.obs1
> Q <- 3
> T <- length(nmodT[,1])*Q
>
> qsd <- colSums(nmodT)
> qsd <- as.matrix(qsd)
>
> aqw<- qsd>T
>
> aqw
>
>            [,1]
> 2003-01-01 FALSE
> 2003-01-11  TRUE

...

> 2003-12-21  TRUE

Jonathan,

If you next do row.names(aqw)[aqw[,1]] does that get you
what you want?

Hope this helps,

Stephen



From weigand.stephen at charter.net  Thu May 19 05:52:10 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Wed, 18 May 2005 22:52:10 -0500
Subject: [R] how to export data in the order
In-Reply-To: <s28c9053.036@MC-GWDOM2.newcastle.edu.au>
References: <s28c9053.036@MC-GWDOM2.newcastle.edu.au>
Message-ID: <a23ab599f90291292f14f0907bb66c19@charter.net>


On May 18, 2005, at 10:10 PM, Nongluck Klibbua wrote:

> hi all,
> My data is y[j,k] where j=1,...5, k=1,....,10 and when I use
> cat(y,file="c:data.txt",sep="\n"). The data is coming out into the
> data.txt is
> y[1,1],y[2,1],....,y[5,1],y[2,1],y[2,2],....,y[5,2],.....,y[j,k]. How I
> can export the data by my data is
> y[1,1],y[1,2],...y[1,5],y[2,1],[2,2],.....y[j,k].
> Appreciate your time.
> nongluck

Nongluck,

Here's an illustration of a way to get this:

y <- matrix(c(11,12,13,21,22,23), byrow = TRUE, ncol = 3)
print(y)

      [,1] [,2] [,3]
[1,]   11   12   13
[2,]   21   22   23

cat(y, sep = ",") # not what's wanted
11,21,12,22,13,23

cat(t(y), sep = ",") # cat the transpose of y
11,12,13,21,22,23    # better!

Best,

Stephen



From stat_ramesh at rediffmail.com  Thu May 19 06:31:54 2005
From: stat_ramesh at rediffmail.com (Ramesh Kolluru)
Date: 19 May 2005 04:31:54 -0000
Subject: [R] Calculation of Durbin-Watson p-value
Message-ID: <20050519043154.3861.qmail@webmail17.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050519/81dd66ca/attachment.pl

From jacques.veslot at cirad.fr  Thu May 19 07:58:43 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Thu, 19 May 2005 09:58:43 +0400
Subject: [R] lwd less than 1
In-Reply-To: <428BD45B.8000801@stat.auckland.ac.nz>
References: <428ADC7A.2040901@cirad.fr> <428BD45B.8000801@stat.auckland.ac.nz>
Message-ID: <428C2B13.3080800@cirad.fr>

Hi,

Thank you for helping,

Actually, lwd=.5 in plot() or par() brings neither error nor warning 
message, but lwd below 1 seems not to be taken into account, as 
plot(..., lwd=1) and plot(..., lwd=.5) look quite the same.

Besides, it is mentioned in the lwd section of the par() help page :
" (...) some devices do not implement line widths less than one".

What I am looking for is how to get readable plots (with not too wide 
lines) after size reduction...

Best regards,

Jacques VESLOT


Paul Murrell a ??crit :
> Hi
> 
> 
> Jacques VESLOT wrote:
> 
>> Dear R-users,
>>
>> Someone, who uses R under Mac, wants to insert a couple of small plots 
>> (each with several lines) in an article, but he has to reduce plots' 
>> size significantly. He did it (in pdf or enc. ps) but, unfortunately, 
>> everything is reduced but lines' width. Besides, 'lwd' argument in 
>> par() or plot() can't be less than one.
> 
> 
> 
> IANAMU (I am not a Mac user) either, but you should definitely be able 
> to set lwd to less than 1.   What happens when you try par(lwd=.5) or 
> plot(1:10, type="l", lwd=.5)?
> 
> Paul
> 
> 
>> I am not a Mac user and I don't know whether it is a Mac-related 
>> problem or not.
>>
>> Could somebody please give me a way to solve this problem, either to 
>> reduce line width under R, or to get reduced lines' width when 
>> reducing plot size ?
>>
>> Thanks,
>>
>> Jacques VESLOT
>> Cirad
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
>



From Achim.Zeileis at wu-wien.ac.at  Thu May 19 07:58:17 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 19 May 2005 07:58:17 +0200
Subject: [R] Calculation of Durbin-Watson p-value
In-Reply-To: <20050519043154.3861.qmail@webmail17.rediffmail.com>
References: <20050519043154.3861.qmail@webmail17.rediffmail.com>
Message-ID: <20050519075817.4cdb29ab.Achim.Zeileis@wu-wien.ac.at>

On 19 May 2005 04:31:54 -0000 Ramesh Kolluru wrote:

> Sir, ??
> I am unable to get the source code for Durbin-Watson test, as I want
> to calculate the p-value for Durbin Watson statistic using
> interpolation method. I sent this mail to r-help, but it was rejected,

Perhaps you should have tried to figure out why it was rejected instead
of sending it directly to BDR... Please *do* read the posting guide at
  http://www.R-project.org/posting-guide.html
on how to ask for help on the R mailing lists.

As for your mail (which does not ask any question, and surely no
specific one): which source code are you referring to? There are at
least two implementations in R (dwtest in package lmtest, durbin.watson
in package car), the source code of both is available in the usual ways.
But maybe you're referring to some other implementation?

Concerning the p values: dwtest() interfaces the pan/gradsol algorithm
which computes the null distribution from a linear combination of
chi-squared variables and also implements a normal approximation.
durbin.watson() computes p values by bootstrapping. So I don't see the
need for implementing an interpolation method (although I have to admit
that it is not clear to me what exactly this means in this context).
Z

> please suggest me some way. I will be highly greatful to you.
>
> Thanks in advance
> Ramesh
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Thu May 19 08:28:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 May 2005 08:28:56 +0200
Subject: [R] lwd less than 1
In-Reply-To: <428C2B13.3080800@cirad.fr>
References: <428ADC7A.2040901@cirad.fr> <428BD45B.8000801@stat.auckland.ac.nz>
	<428C2B13.3080800@cirad.fr>
Message-ID: <428C3228.5080502@statistik.uni-dortmund.de>

Jacques VESLOT wrote:
> Hi,
> 
> Thank you for helping,
> 
> Actually, lwd=.5 in plot() or par() brings neither error nor warning 
> message, but lwd below 1 seems not to be taken into account, as 
> plot(..., lwd=1) and plot(..., lwd=.5) look quite the same.
> 
> Besides, it is mentioned in the lwd section of the par() help page :
> " (...) some devices do not implement line widths less than one".
> 
> What I am looking for is how to get readable plots (with not too wide 
> lines) after size reduction...


What about saving the big plot in a vector format such as PostScript or 
PDF and later resizing in the document you are going to include the plot in?

Uwe Ligges

> Best regards,
> 
> Jacques VESLOT
> 
> 
> Paul Murrell a ??crit :
> 
>> Hi
>>
>>
>> Jacques VESLOT wrote:
>>
>>> Dear R-users,
>>>
>>> Someone, who uses R under Mac, wants to insert a couple of small 
>>> plots (each with several lines) in an article, but he has to reduce 
>>> plots' size significantly. He did it (in pdf or enc. ps) but, 
>>> unfortunately, everything is reduced but lines' width. Besides, 'lwd' 
>>> argument in par() or plot() can't be less than one.
>>
>>
>>
>>
>> IANAMU (I am not a Mac user) either, but you should definitely be able 
>> to set lwd to less than 1.   What happens when you try par(lwd=.5) or 
>> plot(1:10, type="l", lwd=.5)?
>>
>> Paul
>>
>>
>>> I am not a Mac user and I don't know whether it is a Mac-related 
>>> problem or not.
>>>
>>> Could somebody please give me a way to solve this problem, either to 
>>> reduce line width under R, or to get reduced lines' width when 
>>> reducing plot size ?
>>>
>>> Thanks,
>>>
>>> Jacques VESLOT
>>> Cirad
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu May 19 09:11:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 May 2005 08:11:04 +0100 (BST)
Subject: [R] lwd less than 1
In-Reply-To: <428C2B13.3080800@cirad.fr>
References: <428ADC7A.2040901@cirad.fr> <428BD45B.8000801@stat.auckland.ac.nz>
	<428C2B13.3080800@cirad.fr>
Message-ID: <Pine.LNX.4.61.0505190800070.31277@gannet.stats>

On Thu, 19 May 2005, Jacques VESLOT wrote:

> Thank you for helping,
>
> Actually, lwd=.5 in plot() or par() brings neither error nor warning message, 
> but lwd below 1 seems not to be taken into account, as plot(..., lwd=1) and 
> plot(..., lwd=.5) look quite the same.

On what device: see below?

> Besides, it is mentioned in the lwd section of the par() help page :
> " (...) some devices do not implement line widths less than one".

(The people who write the documentation do tend to know what it says:
you omitted `The interpretation is device-specific' and it seems did not 
look in the device-specific documentation.)

This _is_ documented under both ?postscript and ?pdf as

      Line widths as controlled by 'par(lwd=)' are in multiples of
      1/96inch.  Multiples less than 1 are allowed.  'pch="."' with 'cex
      = 1' corresponds to a square of side 1/72 inch.

You haven't mentioned the device (or version of R) you are using, nor the 
commands used nor how you are viewing the output.

I checked the sources: the quartz() device is restricted to lwd >= 1, so 
this might have resulted from plotting on that and then copying the plot.

> What I am looking for is how to get readable plots (with not too wide lines) 
> after size reduction...

Using postscript() or pdf() directly works for me, and the code is the 
same on all R platforms.

> Paul Murrell a ?crit :
>> 
>> Jacques VESLOT wrote:
>> 
>>> Someone, who uses R under Mac, wants to insert a couple of small plots 
>>> (each with several lines) in an article, but he has to reduce plots' size 
>>> significantly. He did it (in pdf or enc. ps) but, unfortunately, 
>>> everything is reduced but lines' width. Besides, 'lwd' argument in par() 
>>> or plot() can't be less than one.
>> 
>> IANAMU (I am not a Mac user) either, but you should definitely be able to 
>> set lwd to less than 1.   What happens when you try par(lwd=.5) or 
>> plot(1:10, type="l", lwd=.5)?

>>> I am not a Mac user and I don't know whether it is a Mac-related problem 
>>> or not.
>>> 
>>> Could somebody please give me a way to solve this problem, either to 
>>> reduce line width under R, or to get reduced lines' width when reducing 
>>> plot size ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From stefano.colucci1 at libero.it  Thu May 19 09:23:45 2005
From: stefano.colucci1 at libero.it (Stefano Colucci)
Date: Thu, 19 May 2005 09:23:45 +0200
Subject: [R] ARIMA estimation
Message-ID: <002301c55c43$b39185b0$7af32997@2dbce4f466c3481>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050519/cc6e3b67/attachment.pl

From ripley at stats.ox.ac.uk  Thu May 19 09:32:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 May 2005 08:32:19 +0100 (BST)
Subject: [R] Call R from Fortran 
In-Reply-To: <4702645135092E4497088F71D9C8F51A128B6C@afhex01.dpi.wa.gov.au>
References: <4702645135092E4497088F71D9C8F51A128B6C@afhex01.dpi.wa.gov.au>
Message-ID: <Pine.LNX.4.61.0505190821200.31699@gannet.stats>

On Thu, 19 May 2005, Mulholland, Tom wrote:

> I don't know, but I do know that if you search the mailing list the answer is there.
>
> One place to start might be 
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/50776.html

That is about calling Fortran from R, not the subject of this question.
(There are many other more authoritative sources for that.)

> I found this by searching for "fortran" on the mailing list.
>
> You might also read the posting guide. The link above refers to a 
> particular operating system which may or may not be be the one you are 
> using. I think you might also need to read "Writing R Extensions"

This can only be done by combining Fortran and C, as R functions need R 
objects as arguments and return R objects.  R objects are defined in C 
headers and use structures not in Fortran 77.  But it can be done, and 
"Writing R Extensions" is a good place to start, especially the 
OS-specific sections on writing new front ends.

>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Samiran Sinha
>> Sent: Thursday, 19 May 2005 7:24 AM
>> To: r-help at stat.math.ethz.ch
>>
>> I need to call a R function from Fortran 77 program. How will I do that 
>> exactly? I will grately appreciate any help. Sincerely,


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu May 19 09:34:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 May 2005 08:34:15 +0100 (BST)
Subject: [R] ARIMA estimation
In-Reply-To: <002301c55c43$b39185b0$7af32997@2dbce4f466c3481>
References: <002301c55c43$b39185b0$7af32997@2dbce4f466c3481>
Message-ID: <Pine.LNX.4.61.0505190832380.31699@gannet.stats>

On Thu, 19 May 2005, Stefano Colucci wrote:

> Good morning,
>
> (sorry for my english)
> i have some problems to put off by extimation ARIMA coefficients
> the ones not significatives.
> Exist a method to extimate only that significatives?
>
> i use the command: arima().

See the argument 'fixed', which allows you to choose which coefficients to 
estimate.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From francoisromain at free.fr  Thu May 19 09:40:23 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 19 May 2005 09:40:23 +0200
Subject: [R] ARIMA estimation
In-Reply-To: <002301c55c43$b39185b0$7af32997@2dbce4f466c3481>
References: <002301c55c43$b39185b0$7af32997@2dbce4f466c3481>
Message-ID: <428C42E7.1030808@free.fr>

Le 19.05.2005 09:23, Stefano Colucci a ??crit :

>Good morning,
>
>(sorry for my english)
>i have some problems to put off by extimation ARIMA coefficients 
>the ones not significatives.
> Exist a method to extimate only that significatives?
>
>i use the command: arima().
>
>thanks to all
>Stefano
>  
>
You can fix coefs to what you want by using the parameter fixed in the 
arima call.

arima(x,fixed=c(NA,0,NA),order=c(3,0,0))

will fit an arima(p=3,i=0,q=0) with \phi_2 fixed to zero.

Maybe you'll be interested in graph 29 in the graph gallery : 
http://addictedtor.free.fr/graphiques/
See
http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=29

Romain

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From maechler at stat.math.ethz.ch  Thu May 19 11:38:59 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 19 May 2005 11:38:59 +0200
Subject: [R] lattice plots question
In-Reply-To: <428B9637.8050906@pdf.com>
References: <BAY10-F10258FB20ED23A7C21C522D6170@phx.gbl>
	<428B9637.8050906@pdf.com>
Message-ID: <17036.24243.302745.745024@stat.math.ethz.ch>

>>>>> "Sundar" == Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>
>>>>>     on Wed, 18 May 2005 14:23:35 -0500 writes:

    Sundar> Laura Holt wrote:
    >> Dear R People:
    >> 
    >> Is there any way to have the background of lattice plots be white 
    >> instead of grey, please?
    >> 
    >> This is not a criticism by any means...the lattice stuff is UNbelievable!

indeed, it is!

    >> ....



    Sundar> See ?trellis.par.set or ?trellis.device. Lattice
    Sundar> themes are what you are looking for.

    Sundar> e.g.

    Sundar> library(lattice)
    Sundar> trellis.device(windows, theme = col.whitebg())
    Sundar> xyplot(0 ~ 0)

If, instead of line 2, you use   
    trellis.device(theme = col.whitebg())

i.e., don't specify the device explicitly, you have solution that is
portable {across platforms} : The device chosen will depend if
you are in interactive or batch mode {which is good!} and
in interactive mode will use 'windows', 'x11' , or 'quartz'
depending on your platform --- as a matter of fact, it will be
what you also get by  getOption("device")
and you can change by  options(device = "myfavoriteDevice")
			      

    Sundar> You can also create your own themes, which I find extremely useful:

indeed!

    Sundar>  sundar.theme <- function() {
    Sundar>     ..............

Martin Maechler, ETH Zurich



From Luisr at frs.fo  Thu May 19 11:47:30 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Thu, 19 May 2005 10:47:30 +0100
Subject: [R] R-help,
Message-ID: <s28c6ec7.056@ffdata.setur.fo>

R-help,

I usually call lapply to plot some dat frames structures.Something like
this:

par(mfrow=c(4,3),mar=c(2, 4, 2, 1) + 0.1)
lapply(my.list , function(x)
{
plot(colnames(x) , apply(x,2,mean),  type="o", pch = 16, ylab = "Index"
, xlab = "")
}
)

But it is difficult for me to put a title on every plot according to
the list names.
I guess the re other ways to do it but the structure above is so handy
to me that I want to stick to it.

Any suggestions?


I run on Windows XP machine

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R   


Thanks in advance



From wl at eimb.ru  Thu May 19 12:16:53 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Thu, 19 May 2005 14:16:53 +0400
Subject: [R] Lattice: how to get default ylim?
Message-ID: <244663517.20050519141653@eimb.ru>

Dear r-help,

  I draw graphics with xyplot and want to add some text to each panel
  (actually, the slope, error and significance of a regression line).

  I have defined the function, drawing a single panel and pass it to
  xyplot in the panel argument. This function calls panel.xyplot,
  calculates linear regression and formats coefficients.

  Now I want the text, I mentioned above, to be put in the upper left
  corner of each plot.
  I use ltext, and I need to define coordinates x and y.
  In order to do this I need to know the limits of x and y axes.

  I do not want to pass arguments xlim and ylim to the xyplot function
  and want it to calculate them automatically.
  And I also want to know the result of calculations. :)
  How to do this?
  
  Thank you very much.

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From malfonso at telecom.com.co  Thu May 19 12:14:44 2005
From: malfonso at telecom.com.co (Mario Morales)
Date: Thu, 19 May 2005 05:14:44 -0500
Subject: [R] Drawing a circle
Message-ID: <428C6714.9080702@telecom.com.co>

Hi.

I need to draw a circle whit center (a,b) and radio r. So I use the
R code below

a<-1.975 # valore x del centro
b<-1.215 # valores y del centro
r<-1.46 # radio
x1<-seq(a-r,a+r,by=0.01); #los valores de x
yp<-sqrt(r^2-(x1-a)^2)+b; # los valores y a partir de la ra??z positiva
yn<-(-1)*sqrt(r^2-(x1-a)^2)+b; # los valores y a partir de la ra??z negativa
x<-c(x1,x1);  y<-c(yp,yn);

plot(x,y,type="l",ylab="",xlab="",xlim=range(x),ylim=range(y));


My circle have two problems.

First. I don't want the horizontal line and I don't know how avoid
it.

Second. It is shaped like ellipse. I need adjust the x and y scales,
but I don't know how.

Can you help me???

there is another way to draw a circle whit R ?



From Rau at demogr.mpg.de  Thu May 19 12:42:05 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 19 May 2005 12:42:05 +0200
Subject: [R] Arranging Plots
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E65202B1@HERMES.demogr.mpg.de>

Dear all,

I'd appreciate any hints how to arrange some plots.
I have three plots. I would like to arrange them in the following order:
- Plot 1 and Plot 2 should be in the upper row
- Plot 3 should be in the lower row but centered in the middle.

I hope the following sketch will help understanding my problem
================
| ===    ===   |
|| P1|  |P2 |  |
||   |  |   |  |
| ===    ===   |
|    ====      |
|   |P3  |     |
|   |    |     |
|    ====      |
================

I tried already things
split.screen(c(2,1))
split.screen(c(1,2), screen=1)
screen(3)
### plotting of Plot 1
screen (4)
### plotting of Plot 2
screen(2)
### plotting of Plot 3
close.screen()

but then Plot 3 will be stretched across the whole screen (would be
screen(2)) and I would like to have it just the same width as the other
plots 1 and 2.

Can anyone give me some hints?

Thank you very much!
Roland


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From jeaneid at chass.utoronto.ca  Thu May 19 13:01:38 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 19 May 2005 07:01:38 -0400
Subject: [R] R-help,
In-Reply-To: <s28c6ec7.056@ffdata.setur.fo>
Message-ID: <Pine.SGI.4.40.0505190658010.4125146-100000@origin.chass.utoronto.ca>

I do not fully understand your example but if you need to act on the
columns of a dataframe why don't you just call its columns the title you
want. something like
X<-matrix(rnorm(1000), ncol=4)
colnames(X)<-c("foo", "foo1", "foo2", "foo3")
X<-as.data.frame(X)
par(mfrow=c(2,2))
lapply(colnames(X), function(x) plot(X[,x], ylab="y", xlab="x", main=x))


HTH

Jean
On Thu, 19 May 2005, Luis Ridao Cruz wrote:

> R-help,
>
> I usually call lapply to plot some dat frames structures.Something like
> this:
>
> par(mfrow=c(4,3),mar=c(2, 4, 2, 1) + 0.1)
> lapply(my.list , function(x)
> {
> plot(colnames(x) , apply(x,2,mean),  type="o", pch = 16, ylab = "Index"
> , xlab = "")
> }
> )
>
> But it is difficult for me to put a title on every plot according to
> the list names.
> I guess the re other ways to do it but the structure above is so handy
> to me that I want to stick to it.
>
> Any suggestions?
>
>
> I run on Windows XP machine
>
> > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
>
>
> Thanks in advance
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From alexbri at netcabo.pt  Thu May 19 13:18:10 2005
From: alexbri at netcabo.pt (alexbri)
Date: Thu, 19 May 2005 12:18:10 +0100
Subject: [R] plot question
Message-ID: <EA91707AE6F4C84495513EFF5117E897062217A8@VS2.hdi.tvcabo>

hi all:
 
xlim and ylim are used to define the interval limits of a plot. I'm interested in the scale of values between this limits.
 
suppose xlim=c(0,10)
we can have e.g.
0  5  10
0  2  4  6  8  10
0  1  2  3  4  5  6  7  8  9  10
 
which is the parameter that allows me to modify this?
 
thanks in advance
alexandre



From sdavis2 at mail.nih.gov  Thu May 19 13:23:56 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 19 May 2005 07:23:56 -0400
Subject: [R] Arranging Plots
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E65202B1@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E65202B1@HERMES.demogr.mpg.de>
Message-ID: <422c2964fc7318903e22b77983d49f14@mail.nih.gov>



On May 19, 2005, at 6:42 AM, Rau, Roland wrote:

> Dear all,
>
> I'd appreciate any hints how to arrange some plots.
> I have three plots. I would like to arrange them in the following 
> order:
> - Plot 1 and Plot 2 should be in the upper row
> - Plot 3 should be in the lower row but centered in the middle.
>
> I hope the following sketch will help understanding my problem
> ================
> | ===    ===   |
> || P1|  |P2 |  |
> ||   |  |   |  |
> | ===    ===   |
> |    ====      |
> |   |P3  |     |
> |   |    |     |
> |    ====      |
> ================


Roland,

You might want to look into grid graphics.  The link here has lots of 
info.

http://www.stat.auckland.ac.nz/~paul/grid/grid.html

Sean



From ccleland at optonline.net  Thu May 19 13:26:02 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 19 May 2005 07:26:02 -0400
Subject: [R] Drawing a circle
In-Reply-To: <428C6714.9080702@telecom.com.co>
References: <428C6714.9080702@telecom.com.co>
Message-ID: <428C77CA.5020106@optonline.net>

See ?symbols.  For example:

 > a <- 1.975 # valore x del centro
 > b <- 1.215 # valores y del centro
 > r <- 1.460 # radio
 > plot(0,0, type = "n", ylim=c(b - r, b + r), xlim=c(a - r, a + r))
 > symbols(x = a, y = b, circles = r)

Mario Morales wrote:
> Hi.
> 
> I need to draw a circle whit center (a,b) and radio r. So I use the
> R code below
> 
> a<-1.975 # valore x del centro
> b<-1.215 # valores y del centro
> r<-1.46 # radio
> x1<-seq(a-r,a+r,by=0.01); #los valores de x
> yp<-sqrt(r^2-(x1-a)^2)+b; # los valores y a partir de la ra??z positiva
> yn<-(-1)*sqrt(r^2-(x1-a)^2)+b; # los valores y a partir de la ra??z negativa
> x<-c(x1,x1);  y<-c(yp,yn);
> 
> plot(x,y,type="l",ylab="",xlab="",xlim=range(x),ylim=range(y));
> 
> 
> My circle have two problems.
> 
> First. I don't want the horizontal line and I don't know how avoid
> it.
> 
> Second. It is shaped like ellipse. I need adjust the x and y scales,
> but I don't know how.
> 
> Can you help me???
> 
> there is another way to draw a circle whit R ?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From B.Rowlingson at lancaster.ac.uk  Thu May 19 13:28:06 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 19 May 2005 12:28:06 +0100
Subject: [R] Arranging Plots
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E65202B1@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E65202B1@HERMES.demogr.mpg.de>
Message-ID: <428C7846.5020400@lancaster.ac.uk>

Rau, Roland wrote:

> 
> Can anyone give me some hints?
> 

?screen

tells you:

     figs: A two-element vector describing the number of rows and the
           number of columns in a screen matrix _or_ a matrix with 4
           columns. If a matrix, then each row describes a screen with
           values for the left, right, bottom, and top of the screen (in
           that order) in NDC units, that is 0 at the lower left coner

  - so by passing a matrix you can put plots anywhere, not just split 
the whole thing into boxes.

  Here's an example, which with a bit of tweaking, might work for you:

  > fm=rbind(c(0,.4,.6,.9),c(.6,.9,.6,.9),c(.3,.8,.1,.4))
  > fm
       [,1] [,2] [,3] [,4]
  [1,]  0.0  0.4  0.6  0.9
  [2,]  0.6  0.9  0.6  0.9
  [3,]  0.3  0.8  0.1  0.4

  each row of fm is (left, right, bottom, top) as a fraction of the 
whole device.

  > split.screen(fm)
  > screen(1)
  > plot(1:10)
  > screen(2)
  > hist(runif(100))
  > screen(3)
  > plot(1:10)

  I've left some space around that you might want to get rid of. Its 90% 
there.

Baz



From kbeath at efs.mq.edu.au  Thu May 19 13:29:57 2005
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Thu, 19 May 2005 21:29:57 +1000
Subject: [R] Bias to do with search engines
In-Reply-To: <s28cf36a.000@mail.efs.mq.edu.au>
References: <s28cf36a.000@mail.efs.mq.edu.au>
Message-ID: <644646CF-1F78-4091-A440-3572003F875C@efs.mq.edu.au>

  Patrick Connolly wrote
>
> Somehow, southern hemisphere sites aren't taken as seriously as those
> from that other hemisphere.  Is there a word for that type of bias?
>

There is a more simple explanation. If a page isn't linked to or  
Google isn't explicitly told about it, then Google will never know.  
As the archives of this list are indexed by Google the SAMM site  
should appear in a week or two.



From sdavis2 at mail.nih.gov  Thu May 19 13:30:40 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 19 May 2005 07:30:40 -0400
Subject: [R] R-help,
In-Reply-To: <Pine.SGI.4.40.0505190658010.4125146-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0505190658010.4125146-100000@origin.chass.utoronto.ca>
Message-ID: <d455ff204e3c6602ba2682d2d1e4666c@mail.nih.gov>


On May 19, 2005, at 7:01 AM, Jean Eid wrote:

> I do not fully understand your example but if you need to act on the
> columns of a dataframe why don't you just call its columns the title 
> you
> want. something like
> X<-matrix(rnorm(1000), ncol=4)
> colnames(X)<-c("foo", "foo1", "foo2", "foo3")
> X<-as.data.frame(X)
> par(mfrow=c(2,2))
> lapply(colnames(X), function(x) plot(X[,x], ylab="y", xlab="x", 
> main=x))
>

Taking this back a bit to the original question:

par(mfrow=c(4,3),mar=c(2, 4, 2, 1) + 0.1)
sapply(names(my.list) , function(x)
{
plot(colnames(my.list[[x]]) , apply(my.list[[x]],2,mean),  type="o", 
pch = 16, ylab = "Index"
, xlab = "",main=x)
}
)



>
> HTH
>
> Jean
> On Thu, 19 May 2005, Luis Ridao Cruz wrote:
>
>> R-help,
>>
>> I usually call lapply to plot some dat frames structures.Something 
>> like
>> this:
>>
>> par(mfrow=c(4,3),mar=c(2, 4, 2, 1) + 0.1)
>> lapply(my.list , function(x)
>> {
>> plot(colnames(x) , apply(x,2,mean),  type="o", pch = 16, ylab = 
>> "Index"
>> , xlab = "")
>> }
>> )
>>
>> But it is difficult for me to put a title on every plot according to
>> the list names.
>> I guess the re other ways to do it but the structure above is so handy
>> to me that I want to stick to it.
>>
>> Any suggestions?
>>
>>
>> I run on Windows XP machine
>>
>>> version
>>          _
>> platform i386-pc-mingw32
>> arch     i386
>> os       mingw32
>> system   i386, mingw32
>> status
>> major    2
>> minor    1.0
>> year     2005
>> month    04
>> day      18
>> language R
>>
>>
>> Thanks in advance
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu May 19 13:50:59 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 19 May 2005 13:50:59 +0200
Subject: [R] plot question
References: <EA91707AE6F4C84495513EFF5117E897062217A8@VS2.hdi.tvcabo>
Message-ID: <002801c55c69$063168b0$0540210a@www.domain>

do you need something like this:


par(mfrow=c(2, 2))

plot(0:10, 0:10, axes=FALSE, xlim=c(0, 10))
axis(1, at=seq(0, 10, 5))
axis(2)
####
plot(0:10, 0:10, axes=FALSE, xlim=c(0, 10))
axis(1, at=seq(0, 10, 2))
axis(2)
####
plot(0:10, 0:10, axes=FALSE, xlim=c(0, 10))
axis(1, at=seq(0, 10, 1))
axis(2)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "alexbri" <alexbri at netcabo.pt>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, May 19, 2005 1:18 PM
Subject: [R] plot question


> hi all:
>
> xlim and ylim are used to define the interval limits of a plot. I'm 
> interested in the scale of values between this limits.
>
> suppose xlim=c(0,10)
> we can have e.g.
> 0  5  10
> 0  2  4  6  8  10
> 0  1  2  3  4  5  6  7  8  9  10
>
> which is the parameter that allows me to modify this?
>
> thanks in advance
> alexandre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Pierre.Lapointe at nbf.ca  Thu May 19 13:51:54 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Thu, 19 May 2005 07:51:54 -0400
Subject: [R] Arranging Plots
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF357B9@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050519/d79e7725/attachment.pl

From fcombes at gmail.com  Thu May 19 13:57:56 2005
From: fcombes at gmail.com (Florence Combes)
Date: Thu, 19 May 2005 13:57:56 +0200
Subject: [R] R from Perl -- RSPerl and lines function.
Message-ID: <73dae3060505190457160cc158@mail.gmail.com>

Dear R-helpers, 

I am running well Perl and R on my Debian Linux, and I tried RSPerl.
Installation is ok and all simple functions run well. But I have a
problem to call the "lines" function.
I would like to draw an histogram with the density curve on. Is is OK
in R with the command:

>x<-rnorm(1000)
>hist(x,prob=T)
>lines(density(x))

for example. 

Now, I have a Perl script with which  pars files, and I obtain data in
a list @distance. I draw an hist with RSPerl command (from Perl):

--------------------------
(---Perl script---)

&R::initR("--silent");
&R::library("RSPerl");
@Rdata=&R::call("as.numeric", \@distance);

&R::callWithNames("hist", {'', \@Rdata, 'main', '', 'xlab',
"Distribution of the distances between oligo-5' and sequence 3'",
'br', 15, 'col', 'gray', 'prob', 'T'} );

&R::call("lines", ("density", \@Rdata)) );

sleep(4);

&R::call("dev.off");

exit;
-----------------------------

All runs well: I obtained the histogram graph, and it seems that the
density call runs well since I have the message
"Performed the call, result has length 7" 
and I read the density fuction results in 7 parameters; 
but just after I have a message like "segmentation fault". 

I cannot understand what happens ? 

Has someone already encountered this problem or know how to abtain an
histogram and the density line with RSPerl ???

Thanks a lot for your help, caus I tried all I could think of ...

Florence.



From esg at felix.unife.it  Thu May 19 14:10:45 2005
From: esg at felix.unife.it (Josef Eschgfaeller)
Date: Thu, 19 May 2005 14:10:45 +0200 (CEST)
Subject: [R] Drawing a circle
In-Reply-To: <428C6714.9080702@telecom.com.co>
References: <428C6714.9080702@telecom.com.co>
Message-ID: <Pine.LNX.4.63.0505191409390.20150@dns.unife.it>


Mario Morales wrote:

> I need to draw a circle

I would do it with complex numbers
and polar coordinates:

Circle = function (t,a)
{a*cos(t)+1i*a*sin(t)}

interval=c(-8,8)
plot(interval,interval,type="n",xlab="",ylab="",
   asp=1,axes=F)
t=seq(0,2*pi,by=0.01)
center=2+3i; radius=5
lines(center+Circle(t,radius))
locator(1)
dev.off()

Josef Eschgf??ller
-- 
Josef Eschgf??ller
Dipartimento Matematico
Universita' di Ferrara
http://felix.unife.it

From krcabrer at unalmed.edu.co  Thu May 19 14:20:28 2005
From: krcabrer at unalmed.edu.co (Kenneth Roy Cabrera Torres)
Date: Thu, 19 May 2005 07:20:28 -0500
Subject: [R] Drawing a circle
In-Reply-To: <428C6714.9080702@telecom.com.co>
References: <428C6714.9080702@telecom.com.co>
Message-ID: <opsq0u8eqf96pdmo@kenneth>

Try

a<-1.975 # valore x del centro
b<-1.215 # valores y del centro
r<-1.46 # radio
symbols(a,b,circle=r,inches=F)

Hope it works

PS:
Espero que te funcione.
Saludos desde Medell{in, Colombia.


On Thu, 19 May 2005 05:14:44 -0500, Mario Morales  
<malfonso at telecom.com.co> wrote:

> Hi.
>
> I need to draw a circle whit center (a,b) and radio r. So I use the
> R code below
>
> a<-1.975 # valore x del centro
> b<-1.215 # valores y del centro
> r<-1.46 # radio
> x1<-seq(a-r,a+r,by=0.01); #los valores de x
> yp<-sqrt(r^2-(x1-a)^2)+b; # los valores y a partir de la raÅ√Å≠z positiva
> yn<-(-1)*sqrt(r^2-(x1-a)^2)+b; # los valores y a partir de la raÅ√Å≠z  
> negativa
> x<-c(x1,x1);  y<-c(yp,yn);
>
> plot(x,y,type="l",ylab="",xlab="",xlim=range(x),ylim=range(y));
>
>
> My circle have two problems.
>
> First. I don't want the horizontal line and I don't know how avoid
> it.
>
> Second. It is shaped like ellipse. I need adjust the x and y scales,
> but I don't know how.
>
> Can you help me???
>
> there is another way to draw a circle whit R ?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html



-- 
Kenneth Roy Cabrera Torres
Uninversidad Nacional de Colombia
Sede Medellin
Tel 430 9351
Cel 315 504 9339



From kkurikka at cc.helsinki.fi  Thu May 19 14:31:02 2005
From: kkurikka at cc.helsinki.fi (Kyosti H Kurikka)
Date: Thu, 19 May 2005 15:31:02 +0300 (EEST)
Subject: [R] Drawing a circle
In-Reply-To: <opsq0u8eqf96pdmo@kenneth>
References: <428C6714.9080702@telecom.com.co> <opsq0u8eqf96pdmo@kenneth>
Message-ID: <Pine.OSF.4.58.0505191525140.199113@sirppi.helsinki.fi>


> On Thu, 19 May 2005 05:14:44 -0500, Mario Morales
> <malfonso at telecom.com.co> wrote:

> > My circle have two problems.
> >
> > First. I don't want the horizontal line and I don't know how avoid
> > it.
> >
> > Second. It is shaped like ellipse. I need adjust the x and y scales,
> > but I don't know how.

For that second problem I would suggest using function eqscplot() on
package:MASS.

Description: Version of a scatterplot with scales chosen to be equal on
both axes, that is 1cm represents the same units on each.

Best Regards,
Ky??sti Kurikka



From ggrothendieck at gmail.com  Thu May 19 14:33:33 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 19 May 2005 08:33:33 -0400
Subject: [R] Drawing a circle
In-Reply-To: <428C77CA.5020106@optonline.net>
References: <428C6714.9080702@telecom.com.co> <428C77CA.5020106@optonline.net>
Message-ID: <971536df0505190533605c81bb@mail.gmail.com>

If a solid circle is ok then one can use the symbol pch=19
and blow it up using cex=, e.g.

plot(0, pch=19, cex=5, col="blue")

See balloonplot in package gplots for an example of this.

On 5/19/05, Chuck Cleland <ccleland at optonline.net> wrote:
> See ?symbols.  For example:
> 
>  > a <- 1.975 # valore x del centro
>  > b <- 1.215 # valores y del centro
>  > r <- 1.460 # radio
>  > plot(0,0, type = "n", ylim=c(b - r, b + r), xlim=c(a - r, a + r))
>  > symbols(x = a, y = b, circles = r)
> 
> Mario Morales wrote:
> > Hi.
> >
> > I need to draw a circle whit center (a,b) and radio r. So I use the
> > R code below
> >
> > a<-1.975 # valore x del centro
> > b<-1.215 # valores y del centro
> > r<-1.46 # radio
> > x1<-seq(a-r,a+r,by=0.01); #los valores de x
> > yp<-sqrt(r^2-(x1-a)^2)+b; # los valores y a partir de la ra??z positiva
> > yn<-(-1)*sqrt(r^2-(x1-a)^2)+b; # los valores y a partir de la ra??z negativa
> > x<-c(x1,x1);  y<-c(yp,yn);
> >
> > plot(x,y,type="l",ylab="",xlab="",xlim=range(x),ylim=range(y));
> >
> >
> > My circle have two problems.
> >
> > First. I don't want the horizontal line and I don't know how avoid
> > it.
> >
> > Second. It is shaped like ellipse. I need adjust the x and y scales,
> > but I don't know how.
> >
> > Can you help me???
> >
> > there is another way to draw a circle whit R ?
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From amir36060 at yahoo.de  Thu May 19 14:35:21 2005
From: amir36060 at yahoo.de (Amir Safari)
Date: Thu, 19 May 2005 14:35:21 +0200 (CEST)
Subject: [R] plot with more than 2 variables
Message-ID: <20050519123521.66562.qmail@web26902.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050519/9c4d341c/attachment.pl

From ggrothendieck at gmail.com  Thu May 19 14:42:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 19 May 2005 08:42:09 -0400
Subject: [R] plot with more than 2 variables
In-Reply-To: <20050519123521.66562.qmail@web26902.mail.ukl.yahoo.com>
References: <20050519123521.66562.qmail@web26902.mail.ukl.yahoo.com>
Message-ID: <971536df05051905427e4e43f8@mail.gmail.com>

Check out the gallery at:
   http://addictedtor.free.fr/graphiques
for lots of examples and source code.

On 5/19/05, Amir Safari <amir36060 at yahoo.de> wrote:
> 
> 
>  Hi All,
> 
> I have tried to plot with more than 2 variables in a unique surface. It is not possible in R?
> 
> Best Regards



From kubovy at virginia.edu  Thu May 19 14:47:09 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 19 May 2005 08:47:09 -0400
Subject: [R] Problem with upgrade to R 2.1 on Mac
In-Reply-To: <200505191001.j4JA1vfg012550@hypatia.math.ethz.ch>
References: <200505191001.j4JA1vfg012550@hypatia.math.ethz.ch>
Message-ID: <4cb3781abe4317e0302f5896f1b59fc1@virginia.edu>

I'm not sure what information you might need to help me. When I moved 
to R 2.1 many of the packages I had installed are not available. Please 
send copy of reply directly to me.

  Machine Model:	Power Mac G5
   CPU Type:	PowerPC 970  (2.2)
  System Version:	Mac OS X 10.3.9 (7W98)
   Kernel Version:	Darwin 7.9.0



From Friedrich.Leisch at tuwien.ac.at  Thu May 19 14:52:44 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Thu, 19 May 2005 14:52:44 +0200
Subject: [R] Sweave and paths
In-Reply-To: <80A12FE7-1B51-4ACE-B76C-38B2F0174A02@louisville.edu>
References: <80A12FE7-1B51-4ACE-B76C-38B2F0174A02@louisville.edu>
Message-ID: <17036.35868.42059.346403@celebrian.ci.tuwien.ac.at>

>>>>> On Tue, 17 May 2005 14:31:53 -0400,
>>>>> Bill Rising (BR) wrote:

  > Is there some way to encourage \SweaveInput{foo} to find foo in a  
  > subdirectory of a file tree?

Sure: Write some code doing it ;-)

  > Something along the lines of the  
  > behavior of list.files(<stuff>, recursive=TRUE). This would be very  
  > helpful at calling small modular files, such as solution sets and the  
  > like.

  > I couldn't see anything in the documentation, and I looked in the  
  > source code, but it seems that SweaveReadFiledoc() wants to look only  
  > in the directory which contains foo.

Well, if you know where it is you can always use the path in the
\SweaveInput{} statement, i.e.,

	\SweaveInput{foo/bar}

works for me.

Best,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From krcabrer at unalmed.edu.co  Thu May 19 14:54:55 2005
From: krcabrer at unalmed.edu.co (Kenneth Roy Cabrera Torres)
Date: Thu, 19 May 2005 07:54:55 -0500
Subject: [R] plot with more than 2 variables
In-Reply-To: <20050519123521.66562.qmail@web26902.mail.ukl.yahoo.com>
References: <20050519123521.66562.qmail@web26902.mail.ukl.yahoo.com>
Message-ID: <opsq0wtt2o96pdmo@kenneth>

See
?rgl

in rgl library.

Best regards

On Thu, 19 May 2005 14:35:21 +0200 (CEST), Amir Safari  
<amir36060 at yahoo.de> wrote:

> Hi All,
> I have tried to plot with more than 2 variables in a unique surface. It  
> is not possible in R?
> Best Regards
>
> 		
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html



-- 
Kenneth Roy Cabrera Torres
Uninversidad Nacional de Colombia
Sede Medellin
Tel 430 9351
Cel 315 504 9339



From amsa36060 at yahoo.com  Thu May 19 14:58:52 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Thu, 19 May 2005 05:58:52 -0700 (PDT)
Subject: [R] tune.svm in {e1071}
Message-ID: <20050519125853.13185.qmail@web60412.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050519/6992a022/attachment.pl

From dj at research.bell-labs.com  Thu May 19 15:00:51 2005
From: dj at research.bell-labs.com (David James)
Date: Thu, 19 May 2005 09:00:51 -0400
Subject: [R] Lattice: how to get default ylim?
In-Reply-To: <244663517.20050519141653@eimb.ru>
References: <244663517.20050519141653@eimb.ru>
Message-ID: <20050519130050.GA8268@jessie.research.bell-labs.com>

Hi,

Within your panel function you can use current.viewport() to recover
the active grid viewport and get xlim/ylim (in addition to other very
useful information).  Then you can use grid.text (plus any other
grid.* function), e.g.,

  require(grid)     

  my.panel <- 
  function(...)
  {
     panel.xyplot(...)
  
     ## add "Hello World on the top-left of each panel
  
     v <- current.viewport()        ## requires R 2.1.0 (I believe)
     xlim <- v$xscale
     ylim <- v$yscale
     grid.text(x = xlim[1], y = ylim[2], default.units="native",
        label = "Hello World", just= c("left", "top"))
  }

Hope this helps.

--
David

Wladimir Eremeev wrote:
> Dear r-help,
> 
>   I draw graphics with xyplot and want to add some text to each panel
>   (actually, the slope, error and significance of a regression line).
> 
>   I have defined the function, drawing a single panel and pass it to
>   xyplot in the panel argument. This function calls panel.xyplot,
>   calculates linear regression and formats coefficients.
> 
>   Now I want the text, I mentioned above, to be put in the upper left
>   corner of each plot.
>   I use ltext, and I need to define coordinates x and y.
>   In order to do this I need to know the limits of x and y axes.
> 
>   I do not want to pass arguments xlim and ylim to the xyplot function
>   and want it to calculate them automatically.
>   And I also want to know the result of calculations. :)
>   How to do this?
>   
>   Thank you very much.
> 
> --
> Best regards
> Wladimir Eremeev                                     mailto:wl at eimb.ru
> 
> ==========================================================================
> Research Scientist, PhD                           Leninsky Prospect 33,
> Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
> Institute of Ecology,                             Phone: (095) 135-9972;
> Russian Academy of Sciences                       Fax: (095) 135-9972
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From alexbri at netcabo.pt  Thu May 19 15:03:20 2005
From: alexbri at netcabo.pt (alexbri)
Date: Thu, 19 May 2005 14:03:20 +0100
Subject: [R] plot question
Message-ID: <EA91707AE6F4C84495513EFF5117E897062217AB@VS2.hdi.tvcabo>

thks Dimitris, it helped a lot.
 
alex

	-----Mensagem original----- 
	De: Dimitris Rizopoulos [mailto:dimitris.rizopoulos at med.kuleuven.ac.be] 
	Enviada: qui 19-05-2005 12:50 
	Para: alexbri 
	Cc: r-help at stat.math.ethz.ch 
	Assunto: Re: [R] plot question
	
	

	do you need something like this:
	
	
	par(mfrow=c(2, 2))
	
	plot(0:10, 0:10, axes=FALSE, xlim=c(0, 10))
	axis(1, at=seq(0, 10, 5))
	axis(2)
	####
	plot(0:10, 0:10, axes=FALSE, xlim=c(0, 10))
	axis(1, at=seq(0, 10, 2))
	axis(2)
	####
	plot(0:10, 0:10, axes=FALSE, xlim=c(0, 10))
	axis(1, at=seq(0, 10, 1))
	axis(2)
	
	
	I hope it helps.
	
	Best,
	Dimitris
	
	----
	Dimitris Rizopoulos
	Ph.D. Student
	Biostatistical Centre
	School of Public Health
	Catholic University of Leuven
	
	Address: Kapucijnenvoer 35, Leuven, Belgium
	Tel: +32/16/336899
	Fax: +32/16/337015
	Web: http://www.med.kuleuven.ac.be/biostat/
	     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
	
	
	----- Original Message -----
	From: "alexbri" <alexbri at netcabo.pt>
	To: <r-help at stat.math.ethz.ch>
	Sent: Thursday, May 19, 2005 1:18 PM
	Subject: [R] plot question
	
	
	> hi all:
	>
	> xlim and ylim are used to define the interval limits of a plot. I'm
	> interested in the scale of values between this limits.
	>
	> suppose xlim=c(0,10)
	> we can have e.g.
	> 0  5  10
	> 0  2  4  6  8  10
	> 0  1  2  3  4  5  6  7  8  9  10
	>
	> which is the parameter that allows me to modify this?
	>
	> thanks in advance
	> alexandre
	>
	> ______________________________________________
	> R-help at stat.math.ethz.ch mailing list
	> https://stat.ethz.ch/mailman/listinfo/r-help
	> PLEASE do read the posting guide!
	> http://www.R-project.org/posting-guide.html
	>



From Rau at demogr.mpg.de  Thu May 19 15:28:02 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 19 May 2005 15:28:02 +0200
Subject: [R] Arranging Plots
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E65202B7@HERMES.demogr.mpg.de>

Dear all,

thank you very much for your help. 
I would like to thank Sean Davis, Barry Rowlingson, and Pierre Lapointe
for their fast help.
I actually use now the approach suggested by Barry Rowlingson via the
split.screen() function.

Thanks,
Roland

P.S. Three solutions in 30 minutes...and the one which was most
convenient for me after 7 minutes. Hard to beat by any other software.
:-)


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From tessap_j at yahoo.fr  Thu May 19 15:54:55 2005
From: tessap_j at yahoo.fr (PIERRE-JOSEPH tessa)
Date: Thu, 19 May 2005 15:54:55 +0200 (CEST)
Subject: [R] Log-likelihood calculation in lme
Message-ID: <20050519135456.44058.qmail@web26410.mail.ukl.yahoo.com>

On a real data set, running the lme function, I get
parameters estimation and a log-likelihood value.
Nevertheless, the variance-covariance matrix in this
case had a determinant close to zero. So, I could not
calculate the log-likelihood myself with the classical
expression.
What is the calculus made in lme?
Thanks for all suggestions.

Tessa P-J
(student)



From deepayan at stat.wisc.edu  Thu May 19 16:11:38 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 19 May 2005 09:11:38 -0500
Subject: [R] Lattice: how to get default ylim?
In-Reply-To: <20050519130050.GA8268@jessie.research.bell-labs.com>
References: <244663517.20050519141653@eimb.ru>
	<20050519130050.GA8268@jessie.research.bell-labs.com>
Message-ID: <200505190911.38996.deepayan@stat.wisc.edu>

On Thursday 19 May 2005 8:00 am, David James wrote:
> Hi,
>
> Within your panel function you can use current.viewport() to recover
> the active grid viewport and get xlim/ylim (in addition to other very
> useful information).  Then you can use grid.text (plus any other
> grid.* function), e.g.,
>
>   require(grid)
>
>   my.panel <-
>   function(...)
>   {
>      panel.xyplot(...)
>
>      ## add "Hello World on the top-left of each panel
>
>      v <- current.viewport()        ## requires R 2.1.0 (I believe)

No, I think it's been there for a while. However, AFAIR the fact that 
viewports have components xscale and yscale that can be accessed like this is 
undocumented and may change if the implementation changes (which is a real 
possibility). 

Ideally, there should be exported interfaces to access this information, 
either in grid or lattice. One of the reasons there isn't is that you rarely 
need it, including in this example (see below).

>      xlim <- v$xscale
>      ylim <- v$yscale
>      grid.text(x = xlim[1], y = ylim[2], default.units="native",
>         label = "Hello World", just= c("left", "top"))

An equivalent version where you don't need xlim and ylim is 

     grid.text(x = 0, y = 1, default.units="npc",
        label = "Hello World", just= c("left", "top"))

Deepayan

>   }
>
> Hope this helps.
>
> --
> David
>
> Wladimir Eremeev wrote:
> > Dear r-help,
> >
> >   I draw graphics with xyplot and want to add some text to each panel
> >   (actually, the slope, error and significance of a regression line).
> >
> >   I have defined the function, drawing a single panel and pass it to
> >   xyplot in the panel argument. This function calls panel.xyplot,
> >   calculates linear regression and formats coefficients.
> >
> >   Now I want the text, I mentioned above, to be put in the upper left
> >   corner of each plot.
> >   I use ltext, and I need to define coordinates x and y.
> >   In order to do this I need to know the limits of x and y axes.
> >
> >   I do not want to pass arguments xlim and ylim to the xyplot function
> >   and want it to calculate them automatically.
> >   And I also want to know the result of calculations. :)
> >   How to do this?
> >
> >   Thank you very much.
> >
> > --
> > Best regards
> > Wladimir Eremeev                                     mailto:wl at eimb.ru
> >
> > =========================================================================
> >= Research Scientist, PhD                           Leninsky Prospect 33,
> > Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
> > Institute of Ecology,                             Phone: (095) 135-9972;
> > Russian Academy of Sciences                       Fax: (095) 135-9972
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From LI at nsabp.pitt.edu  Thu May 19 16:03:49 2005
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Thu, 19 May 2005 10:03:49 -0400
Subject: [R] install R packages
Message-ID: <3D0B2434377E984E9C85CAA316F8B183357C93@nsabpmail>

Dear All,
 
When I tried to install R packages I found this error:
> library(survival)
Loading required package: splines
> install.packages("splines")
--- Please select a CRAN mirror for use in this session ---
Warning message:
package splines is in use and will not be installed 

But I just opened R, and have used anything yet, how come did it say "package splines is in use"?
 
Thanks
 
Jia



From LI at nsabp.pitt.edu  Thu May 19 16:09:09 2005
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Thu, 19 May 2005 10:09:09 -0400
Subject: [R] install R packages--sorry last letter that I sent I had an
	error, this one is the right one
Message-ID: <3D0B2434377E984E9C85CAA316F8B183357C94@nsabpmail>

Dear All,
 
When I tried to install R packages I found this error:
> library(survival)
Loading required package: splines
> install.packages("splines")
--- Please select a CRAN mirror for use in this session ---
Warning message:
package splines is in use and will not be installed 

But I just opened R, and haven't used anything yet, how come did it say "package splines is in use"?
 
Thanks
 
Jia



From dj at research.bell-labs.com  Thu May 19 16:11:28 2005
From: dj at research.bell-labs.com (David James)
Date: Thu, 19 May 2005 10:11:28 -0400
Subject: [R] Lattice: how to get default ylim?
In-Reply-To: <200505190911.38996.deepayan@stat.wisc.edu>
References: <244663517.20050519141653@eimb.ru>
	<20050519130050.GA8268@jessie.research.bell-labs.com>
	<200505190911.38996.deepayan@stat.wisc.edu>
Message-ID: <20050519141128.GA12813@jessie.research.bell-labs.com>

Deepayan Sarkar wrote:
> On Thursday 19 May 2005 8:00 am, David James wrote:
> > Hi,
> >
> > Within your panel function you can use current.viewport() to recover
> > the active grid viewport and get xlim/ylim (in addition to other very
> > useful information).  Then you can use grid.text (plus any other
> > grid.* function), e.g.,
> >
> >   require(grid)
> >
> >   my.panel <-
> >   function(...)
> >   {
> >      panel.xyplot(...)
> >
> >      ## add "Hello World on the top-left of each panel
> >
> >      v <- current.viewport()        ## requires R 2.1.0 (I believe)
> 
> No, I think it's been there for a while. However, AFAIR the fact that 
> viewports have components xscale and yscale that can be accessed like this is 
> undocumented and may change if the implementation changes (which is a real 
> possibility). 
> 
> Ideally, there should be exported interfaces to access this information, 
> either in grid or lattice. One of the reasons there isn't is that you rarely 

Yes, I agree that such an interface is quite desirable.

> need it, including in this example (see below).
> 
> >      xlim <- v$xscale
> >      ylim <- v$yscale
> >      grid.text(x = xlim[1], y = ylim[2], default.units="native",
> >         label = "Hello World", just= c("left", "top"))
> 
> An equivalent version where you don't need xlim and ylim is 
> 
>      grid.text(x = 0, y = 1, default.units="npc",
>         label = "Hello World", just= c("left", "top"))

Right, in this silly "Hello World" example you don't need xlim/ylim,
but one can see instances where one would... 

> 
> Deepayan
> 
> >   }
> >
> > Hope this helps.
> >
> > --
> > David
> >
> > Wladimir Eremeev wrote:
> > > Dear r-help,
> > >
> > >   I draw graphics with xyplot and want to add some text to each panel
> > >   (actually, the slope, error and significance of a regression line).
> > >
> > >   I have defined the function, drawing a single panel and pass it to
> > >   xyplot in the panel argument. This function calls panel.xyplot,
> > >   calculates linear regression and formats coefficients.
> > >
> > >   Now I want the text, I mentioned above, to be put in the upper left
> > >   corner of each plot.
> > >   I use ltext, and I need to define coordinates x and y.
> > >   In order to do this I need to know the limits of x and y axes.
> > >
> > >   I do not want to pass arguments xlim and ylim to the xyplot function
> > >   and want it to calculate them automatically.
> > >   And I also want to know the result of calculations. :)
> > >   How to do this?
> > >
> > >   Thank you very much.
> > >
> > > --
> > > Best regards
> > > Wladimir Eremeev                                     mailto:wl at eimb.ru
> > >
> > > =========================================================================
> > >= Research Scientist, PhD                           Leninsky Prospect 33,
> > > Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
> > > Institute of Ecology,                             Phone: (095) 135-9972;
> > > Russian Academy of Sciences                       Fax: (095) 135-9972
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html



From plummer at iarc.fr  Thu May 19 16:18:23 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Thu, 19 May 2005 16:18:23 +0200
Subject: [R] install R packages--sorry last letter that I sent I had an
	error, this one is the right one
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B183357C94@nsabpmail>
References: <3D0B2434377E984E9C85CAA316F8B183357C94@nsabpmail>
Message-ID: <1116512304.7780.6.camel@seurat>

On Thu, 2005-05-19 at 10:09 -0400, Li, Jia wrote:
> Dear All,
>  
> When I tried to install R packages I found this error:
> > library(survival)
> Loading required package: splines

This is for your information. It is not an error message and is not
asking you to do anything (i.e. it says "loading" not "please load")

> > install.packages("splines")
> --- Please select a CRAN mirror for use in this session ---
> Warning message:
> package splines is in use and will not be installed 
> 
> But I just opened R, and haven't used anything yet, how come did it say "package splines is in use"?

Because it was loaded automatically when you loaded the splines package.
Have a look at the search path with 

search()

You have confused *installing* a package - putting it's files on your
disk - with *loading* it - making it's objects and help pages available
for use in an R session.

Martyn



From bates at stat.wisc.edu  Thu May 19 16:21:43 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 19 May 2005 09:21:43 -0500
Subject: [R] Log-likelihood calculation in lme
In-Reply-To: <20050519135456.44058.qmail@web26410.mail.ukl.yahoo.com>
References: <20050519135456.44058.qmail@web26410.mail.ukl.yahoo.com>
Message-ID: <428CA0F7.1040805@stat.wisc.edu>

PIERRE-JOSEPH tessa wrote:
> On a real data set, running the lme function, I get
> parameters estimation and a log-likelihood value.
> Nevertheless, the variance-covariance matrix in this
> case had a determinant close to zero. So, I could not
> calculate the log-likelihood myself with the classical
> expression.
> What is the calculus made in lme?

The evaluation of the log-likelihood used in lme is documented in
chapter 2 of Pinheiro and Bates (Springer, 2000).  The calculation used
in lmer from the lme4 package is somewhat different.  If you wish I can
send you off-list copies of slides from a presentation that explains
that calculation.

I'm not sure which variance-covariance matrix you are referring to but
it is the case that the ML or REML estimates of the variance-covariance
matrix of the random effects can be singular, a fact that is often
ignored in the analysis of data with such models.



From ligges at statistik.uni-dortmund.de  Thu May 19 16:25:23 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 May 2005 16:25:23 +0200
Subject: [R] install R packages--sorry last letter that I sent I had an
	error, this one is the right one
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B183357C94@nsabpmail>
References: <3D0B2434377E984E9C85CAA316F8B183357C94@nsabpmail>
Message-ID: <428CA1D3.8000407@statistik.uni-dortmund.de>

Li, Jia wrote:
> Dear All,
>  
> When I tried to install R packages I found this error:
> 
>>library(survival)
> 
> Loading required package: splines
> 
>>install.packages("splines")
> 
> --- Please select a CRAN mirror for use in this session ---
> Warning message:
> package splines is in use and will not be installed 
> 
> But I just opened R, and haven't used anything yet, how come did it say "package splines is in use"?
>

library(survival) told you that it loaded the required package 
"splines". So it *is* in use.

Uwe Ligges



> Thanks
>  
> Jia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From marquardt.christian at gmail.com  Thu May 19 16:29:58 2005
From: marquardt.christian at gmail.com (Christian Marquardt)
Date: Thu, 19 May 2005 15:29:58 +0100
Subject: [R] Reversing axis in a log plot
Message-ID: <fad9e83f050519072972b1f411@mail.gmail.com>

Hello,

apologies if I'm overlooking the obvious... I would like to revert a
logarithmic axis with R 2.1.0 on Linux, e.g. for using pressure as a
vertical coordinate. Say we have

  x = seq(1,3, by = 0.01)
  y = exp(x)

Plotting and reversing linear axis is fine

   plot(x,y)
   plot(x,y, ylim = c(30,1))

as is a usual log-plot:

  plot(x,y, log = "y", ylim = c(1,30))

However,

  plot(x,y, log = "y", ylim = c(30,1))

fails with

  Error in axis(2, ...) : log - axis(), 'at' creation, _SMALL_ range:
invalid {xy}axp or par;
         axp[0]= 10, usr[0:1]=(34.3721,0.872801)
  In addition: Warning message:
  CreateAtVector "log"(from axis()): usr[0] = 34.3721 > 0.872801 = usr[1] !

What am I doing wrong here?

Thanks a lot,

  Christian.



From marquardt.christian at gmail.com  Thu May 19 16:29:58 2005
From: marquardt.christian at gmail.com (Christian Marquardt)
Date: Thu, 19 May 2005 15:29:58 +0100
Subject: [R] Reversing axis in a log plot
Message-ID: <fad9e83f050519072972b1f411@mail.gmail.com>

Hello,

apologies if I'm overlooking the obvious... I would like to revert a
logarithmic axis with R 2.1.0 on Linux, e.g. for using pressure as a
vertical coordinate. Say we have

  x = seq(1,3, by = 0.01)
  y = exp(x)

Plotting and reversing linear axis is fine

   plot(x,y)
   plot(x,y, ylim = c(30,1))

as is a usual log-plot:

  plot(x,y, log = "y", ylim = c(1,30))

However,

  plot(x,y, log = "y", ylim = c(30,1))

fails with

  Error in axis(2, ...) : log - axis(), 'at' creation, _SMALL_ range:
invalid {xy}axp or par;
         axp[0]= 10, usr[0:1]=(34.3721,0.872801)
  In addition: Warning message:
  CreateAtVector "log"(from axis()): usr[0] = 34.3721 > 0.872801 = usr[1] !

What am I doing wrong here?

Thanks a lot,

  Christian.



From prasad.chalasani at gs.com  Thu May 19 16:36:44 2005
From: prasad.chalasani at gs.com (Chalasani, Prasad)
Date: Thu, 19 May 2005 10:36:44 -0400
Subject: [R] R annoyances
Message-ID: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>

Dear R Folks,
I'm a big fan of R, but there are a couple of things
that repeatedly annoy me, and I wondered if anyone
has neat ways to deal with them.

(a) When using "apply" row-wise to a matrix, it returns
    the results column-wise, and to preserve the original
    orientation, I've to do a transpose. E.g. I've to keep
    doing a transpose, which I consider to be quite annoying.
	
    transformed.mtx <- t(apply( mtx, 1, exp))

(b) When extracting 2 or more columns of a matrix, 
    R returns the result as a matrix, BUT when extracting
    just one column, it returns a vector/array, rather than
    a matrix, so I've to keep doing as.matrix, which is annoying.

	sub.mtx <- as.matrix(mtx[,1])

	Of course I could write a suitable function
		cols <- function(mtx,range) as.matrix(mtx[, range])
	but then I lose the syntactic sugar of being able to say "[,1]".



From deepayan at stat.wisc.edu  Thu May 19 16:48:18 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 19 May 2005 09:48:18 -0500
Subject: [R] Lattice: how to get default ylim?
In-Reply-To: <20050519141128.GA12813@jessie.research.bell-labs.com>
References: <244663517.20050519141653@eimb.ru>
	<200505190911.38996.deepayan@stat.wisc.edu>
	<20050519141128.GA12813@jessie.research.bell-labs.com>
Message-ID: <200505190948.18843.deepayan@stat.wisc.edu>

On Thursday 19 May 2005 9:11 am, David James wrote:
> Deepayan Sarkar wrote:
> > >      v <- current.viewport()        ## requires R 2.1.0 (I believe)
> >
> > No, I think it's been there for a while. However, AFAIR the fact that
> > viewports have components xscale and yscale that can be accessed like
> > this is undocumented and may change if the implementation changes (which
> > is a real possibility).
> >
> > Ideally, there should be exported interfaces to access this information,
> > either in grid or lattice. One of the reasons there isn't is that you
> > rarely
>
> Yes, I agree that such an interface is quite desirable.

OK, I'll put something in the next version of lattice.

Deepayan



From j_brindle at hotmail.com  Thu May 19 16:38:31 2005
From: j_brindle at hotmail.com (Jim BRINDLE)
Date: Thu, 19 May 2005 10:38:31 -0400
Subject: [R] Power w/ unequal sample sizes
Message-ID: <BAY20-F18896ED20FCF66B8937EE780080@phx.gbl>

Hello,

I am hoping someone could shed some light on power calculations for me.  I 
have two small data sets of unequal sample size after NA removal (m = 5, f = 
7).

m <- c(2.0863, 2.1340, 2.1008, 1.9565, 2.0413, NA, NA)
f <- c(1.8938, 1.9709, 1.8613, 2.0836, 1.9485, 2.0630, 1.9143)

In a R help message/reply from Sep 30, 2001, it was noted that the 
"power.t.test" function assumes equal group sizes and that the groups have 
the same theoretical standard deviation.  In "analyzing" this data, I ran a 
Welch Two Sample t-test and a Wilcoxon Rank Sum test on the data sets and 
both tests reveal a slight statistical difference for alpha = 0.05 (Welch 
Two Sample t-test p-value = 0.045 and Wilcoxon Rank Sum test p-value = 
0.048).

I suspect that the "power" of these tests will be quite low but I am trying 
to quantify it.  Based on the insight in the R help message from 2001, I am 
not sure how to go about this with R.  Is this feasible with R or is there 
another approach I should be considering altogether?

Any insight would be most appreciated.  Thanks a million....



From tlumley at u.washington.edu  Thu May 19 16:41:12 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 19 May 2005 07:41:12 -0700 (PDT)
Subject: [R] R annoyances
In-Reply-To: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>
Message-ID: <Pine.A41.4.61b.0505190740350.13738@homer09.u.washington.edu>

On Thu, 19 May 2005, Chalasani, Prasad wrote:
> (b) When extracting 2 or more columns of a matrix,
>    R returns the result as a matrix, BUT when extracting
>    just one column, it returns a vector/array, rather than
>    a matrix, so I've to keep doing as.matrix, which is annoying.
>
> 	sub.mtx <- as.matrix(mtx[,1])
>
> 	Of course I could write a suitable function
> 		cols <- function(mtx,range) as.matrix(mtx[, range])
> 	but then I lose the syntactic sugar of being able to say "[,1]".

This one is actually a FAQ,
         mtx[,1,drop=FALSE]

 	-thomas



From jean-pierre.mueller at unil.ch  Thu May 19 16:44:42 2005
From: jean-pierre.mueller at unil.ch (Jean-Pierre Muller)
Date: Thu, 19 May 2005 16:44:42 +0200
Subject: [R] Re: text mining: ttda
In-Reply-To: <cdf817830505181517694122a8@mail.gmail.com>
References: <cdf8178305051810067f2d065@mail.gmail.com>
	<cdf817830505181517694122a8@mail.gmail.com>
Message-ID: <f361fe11c288162c610f700d75c33cbd@unil.ch>

Dear Weiwei,

Le 19 mai 05, ?? 00:17, Weiwei Shi a ??crit :

> Can anyone suggest some good text mining reference or books?
>
> thanks,
>
> weiwei
>
> On 5/18/05, Weiwei Shi <helprhelp at gmail.com> wrote:
>> Hi,
>> I am working on a text mining project and i am interested in ttda
>> package. however, I really cannot find the document for this package
>> in English.

I am sorry, but for the moment there is not an english manual...
It is in my "todo" list for a while. R 2.1 as introduced many new 
functions for text,
and i need correct my package first. May be in the nexts weeks?

The package try to code what is shown in:
   Lebart, L., Salem, A. and Berry, L. (1998) "Exploring textual data". 
Dordrecht: Kluwer.

HTH.


>> Can anyone give me some help? btw, is there any other package in R
>> doing text mining. I googled MedlineR which might help my project.
>> Anyone can give me some links on how to use it too?
>>
>> thanks,
>>
>> --
>> Weiwei Shi, Ph.D
>>
>> "Did you always know?"
>> "No, I did not. But I believed..."
>> ---Matrix III
>>
>
>
> -- 
> Weiwei Shi, Ph.D
>
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
-- 
Jean-Pierre M??ller
SSP / BFSH2 / UNIL / CH - 1015 Lausanne
Voice:+41 21 692 3116 / Fax:+41 21 692 3115

Please avoid sending me Word or PowerPoint attachments.
  See http://www.gnu.org/philosophy/no-word-attachments.html
S'il vous pla??t, ??vitez de m'envoyer des attachements au format Word ou 
PowerPoint.
  Voir http://www.gnu.org/philosophy/no-word-attachments.fr.html



From fcombes at gmail.com  Thu May 19 16:39:26 2005
From: fcombes at gmail.com (Florence Combes)
Date: Thu, 19 May 2005 16:39:26 +0200
Subject: [R] R from Perl -- RSPerl and lines function.
In-Reply-To: <20050519141240.GA18467@wald.ucdavis.edu>
References: <73dae3060505190457160cc158@mail.gmail.com>
	<20050519141240.GA18467@wald.ucdavis.edu>
Message-ID: <73dae306050519073922fa73a2@mail.gmail.com>

Dear Duncan, 

Thanks a lot for your answer. 


> 
>   The thing that comes to mind initially is an issue with the
> event loop.  Perl is not running an X windows event loop,
> so not all the updates, refreshes, and drawing events are
> being processed.   R is processing some of them in the
> drawing calls, but after that R does not get an opportunity
> to process any more.
> 
>  As I say, this is just a guess.  To validate it, try using
> a non-interactive graphics device such as pdf, e.g.
> 
> 
> &R::call('pdf', 'foo.pdf');
> 
> 
> &R::callWithNames("hist", {'', \@Rdata, 'main', '', 'xlab',
> "Distribution of the distances between oligo-5' and sequence 3'",
> 'br', 15, 'col', 'gray', 'prob', 'T'} );
> 
> &R::call("lines", ("density", \@Rdata)) );
> 
> &R::call('dev.off');
> 
> And then examine that file to ensure it has the lines.

it is impossible for me to obtain the pdf file....
and I still have the same error message due to segmentation problem
after the "Performed the call, result has length 7".

Can't understand what's happening....

Thanks, 

Florence. 

> If it does, then it is an issue with the interactive
> device and the event loop is almost definitely the problem.
> 
> How to fix things if the event loop is the problem  is
> not trivial and there are many possible different situations
> to consider.
> 
>  D.
> 
> 
> Florence Combes wrote:
> > Dear R-helpers,
> >
> > I am running well Perl and R on my Debian Linux, and I tried RSPerl.
> > Installation is ok and all simple functions run well. But I have a
> > problem to call the "lines" function.
> > I would like to draw an histogram with the density curve on. Is is OK
> > in R with the command:
> >
> > >x<-rnorm(1000)
> > >hist(x,prob=T)
> > >lines(density(x))
> >
> > for example.
> >
> > Now, I have a Perl script with which  pars files, and I obtain data in
> > a list @distance. I draw an hist with RSPerl command (from Perl):
> >
> > --------------------------
> > (---Perl script---)
> >
> > &R::initR("--silent");
> > &R::library("RSPerl");
> > @Rdata=&R::call("as.numeric", \@distance);
> >
> > &R::callWithNames("hist", {'', \@Rdata, 'main', '', 'xlab',
> > "Distribution of the distances between oligo-5' and sequence 3'",
> > 'br', 15, 'col', 'gray', 'prob', 'T'} );
> >
> > &R::call("lines", ("density", \@Rdata)) );
> >
> > sleep(4);
> >
> > &R::call("dev.off");
> >
> > exit;
> > -----------------------------
> >
> > All runs well: I obtained the histogram graph, and it seems that the
> > density call runs well since I have the message
> > "Performed the call, result has length 7"
> > and I read the density fuction results in 7 parameters;
> > but just after I have a message like "segmentation fault".
> >
> > I cannot understand what happens ?
> >
> > Has someone already encountered this problem or know how to abtain an
> > histogram and the density line with RSPerl ???
> >
> > Thanks a lot for your help, caus I tried all I could think of ...
> >
> > Florence.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> --
> Duncan Temple Lang                duncan at wald.ucdavis.edu
> Department of Statistics          work:  (530) 752-4782
> 371 Kerr Hall                     fax:   (530) 752-7099
> One Shields Ave.
> University of California at Davis
> Davis, CA 95616, USA
> 
> 
> 
> 
> 
>



From ligges at statistik.uni-dortmund.de  Thu May 19 16:49:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 May 2005 16:49:16 +0200
Subject: [R] R annoyances
In-Reply-To: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>
Message-ID: <428CA76C.9090806@statistik.uni-dortmund.de>

Chalasani, Prasad wrote:
> Dear R Folks,
> I'm a big fan of R, but there are a couple of things
> that repeatedly annoy me, and I wondered if anyone
> has neat ways to deal with them.
> 
> (a) When using "apply" row-wise to a matrix, it returns
>     the results column-wise, and to preserve the original
>     orientation, I've to do a transpose. E.g. I've to keep
>     doing a transpose, which I consider to be quite annoying.
> 	
>     transformed.mtx <- t(apply( mtx, 1, exp))

I'd rather type

   exp(mtx)



> (b) When extracting 2 or more columns of a matrix, 
>     R returns the result as a matrix, BUT when extracting
>     just one column, it returns a vector/array, rather than
>     a matrix, so I've to keep doing as.matrix, which is annoying.
> 
> 	sub.mtx <- as.matrix(mtx[,1])
> 
> 	Of course I could write a suitable function
> 		cols <- function(mtx,range) as.matrix(mtx[, range])
> 	but then I lose the syntactic sugar of being able to say "[,1]".

The docs suggest:

   mtx[ , 1, drop = FALSE]


Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From matthew_wiener at merck.com  Thu May 19 16:49:12 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 19 May 2005 10:49:12 -0400
Subject: [R] R annoyances
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04994596@uswsmx03.merck.com>

(a) If what you're trying to do is just apply exp, or any other element-wise
function, you can just say "exp(mtx)".  You avoid both "apply" and the
transpose, and save time in the bargain.  If your actual function really
does depend on multiple elements, it may be a little more complicated.  You
could conceivably write a "myapply" function to do the apply followed by the
transpose, but then of course you still need to keep track of which way
you're going.

(b) You want to look into the "drop = FALSE" option:
Sub.mtx <- mtx[,1,drop = FALSE]

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chalasani, Prasad
Sent: Thursday, May 19, 2005 10:37 AM
To: r-help at stat.math.ethz.ch
Subject: [R] R annoyances


Dear R Folks,
I'm a big fan of R, but there are a couple of things
that repeatedly annoy me, and I wondered if anyone
has neat ways to deal with them.

(a) When using "apply" row-wise to a matrix, it returns
    the results column-wise, and to preserve the original
    orientation, I've to do a transpose. E.g. I've to keep
    doing a transpose, which I consider to be quite annoying.
	
    transformed.mtx <- t(apply( mtx, 1, exp))

(b) When extracting 2 or more columns of a matrix, 
    R returns the result as a matrix, BUT when extracting
    just one column, it returns a vector/array, rather than
    a matrix, so I've to keep doing as.matrix, which is annoying.

	sub.mtx <- as.matrix(mtx[,1])

	Of course I could write a suitable function
		cols <- function(mtx,range) as.matrix(mtx[, range])
	but then I lose the syntactic sugar of being able to say "[,1]".

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From prasad.chalasani at gs.com  Thu May 19 17:02:59 2005
From: prasad.chalasani at gs.com (Chalasani, Prasad)
Date: Thu, 19 May 2005 11:02:59 -0400
Subject: [R] R annoyances
Message-ID: <AF003EF88447964B88823C3F50A6AB750ACFD46C@gsnbp25es.firmwide.corp.gs.com>

Thanks all for pointing out that I can use 
	mtx[,1,drop=F]



-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Thursday, May 19, 2005 10:49 AM
To: Chalasani, Prasad
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] R annoyances


Chalasani, Prasad wrote:
> Dear R Folks,
> I'm a big fan of R, but there are a couple of things
> that repeatedly annoy me, and I wondered if anyone
> has neat ways to deal with them.
> 
> (a) When using "apply" row-wise to a matrix, it returns
>     the results column-wise, and to preserve the original
>     orientation, I've to do a transpose. E.g. I've to keep
>     doing a transpose, which I consider to be quite annoying.
> 	
>     transformed.mtx <- t(apply( mtx, 1, exp))

I'd rather type

   exp(mtx)



> (b) When extracting 2 or more columns of a matrix, 
>     R returns the result as a matrix, BUT when extracting
>     just one column, it returns a vector/array, rather than
>     a matrix, so I've to keep doing as.matrix, which is annoying.
> 
> 	sub.mtx <- as.matrix(mtx[,1])
> 
> 	Of course I could write a suitable function
> 		cols <- function(mtx,range) as.matrix(mtx[, range])
> 	but then I lose the syntactic sugar of being able to say "[,1]".

The docs suggest:

   mtx[ , 1, drop = FALSE]


Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu May 19 17:07:52 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 May 2005 17:07:52 +0200
Subject: [R] R annoyances
In-Reply-To: <AF003EF88447964B88823C3F50A6AB750ACFD46C@gsnbp25es.firmwide.corp.gs.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD46C@gsnbp25es.firmwide.corp.gs.com>
Message-ID: <428CABC8.7020601@statistik.uni-dortmund.de>

Chalasani, Prasad wrote:

> Thanks all for pointing out that I can use 
> 	mtx[,1,drop=F]


Which, for example, won't work for
  F <- 10.25

---> drop=FALSE  !
           ^^^^^

Uwe Ligges



> 
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: Thursday, May 19, 2005 10:49 AM
> To: Chalasani, Prasad
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R annoyances
> 
> 
> Chalasani, Prasad wrote:
> 
>>Dear R Folks,
>>I'm a big fan of R, but there are a couple of things
>>that repeatedly annoy me, and I wondered if anyone
>>has neat ways to deal with them.
>>
>>(a) When using "apply" row-wise to a matrix, it returns
>>    the results column-wise, and to preserve the original
>>    orientation, I've to do a transpose. E.g. I've to keep
>>    doing a transpose, which I consider to be quite annoying.
>>	
>>    transformed.mtx <- t(apply( mtx, 1, exp))
> 
> 
> I'd rather type
> 
>    exp(mtx)
> 
> 
> 
> 
>>(b) When extracting 2 or more columns of a matrix, 
>>    R returns the result as a matrix, BUT when extracting
>>    just one column, it returns a vector/array, rather than
>>    a matrix, so I've to keep doing as.matrix, which is annoying.
>>
>>	sub.mtx <- as.matrix(mtx[,1])
>>
>>	Of course I could write a suitable function
>>		cols <- function(mtx,range) as.matrix(mtx[, range])
>>	but then I lose the syntactic sugar of being able to say "[,1]".
> 
> 
> The docs suggest:
> 
>    mtx[ , 1, drop = FALSE]
> 
> 
> Uwe Ligges
> 
> 
> 
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html



From picarde at DMS.UMontreal.CA  Thu May 19 17:08:35 2005
From: picarde at DMS.UMontreal.CA (Elyse Picard)
Date: Thu, 19 May 2005 11:08:35 -0400 (EDT)
Subject: [R] Learning rate with nnet
Message-ID: <Pine.LNX.4.60.0505191107001.22137@paris.DMS.UMontreal.CA>

Hi,

can I change the learning rate with nnet?  And if yes, how?  If no, what 
is the default one?

Thanks



From samuel.friot at wanadoo.fr  Thu May 19 17:14:54 2005
From: samuel.friot at wanadoo.fr (Samuel FRIOT)
Date: Thu, 19 May 2005 17:14:54 +0200 (CEST)
Subject: [R] Fitting Data with errors to non-polynomial Linear Model
Message-ID: <12223890.1116515694648.JavaMail.www@wwinf0202>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050519/61151341/attachment.pl

From monty at sprintmail.com  Thu May 19 17:35:38 2005
From: monty at sprintmail.com (Rod Montgomery)
Date: Thu, 19 May 2005 11:35:38 -0400
Subject: [R] R annoyances
In-Reply-To: <Pine.A41.4.61b.0505190740350.13738@homer09.u.washington.edu>
References: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>
	<Pine.A41.4.61b.0505190740350.13738@homer09.u.washington.edu>
Message-ID: <428CB24A.2070603@sprintmail.com>

Thomas Lumley wrote:
> On Thu, 19 May 2005, Chalasani, Prasad wrote:
> 
>> (b) When extracting 2 or more columns of a matrix,
>>    R returns the result as a matrix, BUT when extracting
>>    just one column, it returns a vector/array, rather than
>>    a matrix, so I've to keep doing as.matrix, which is annoying.
>>
>>     sub.mtx <- as.matrix(mtx[,1])
>>
>>     Of course I could write a suitable function
>>         cols <- function(mtx,range) as.matrix(mtx[, range])
>>     but then I lose the syntactic sugar of being able to say "[,1]".
> 
> 
> This one is actually a FAQ,
>         mtx[,1,drop=FALSE]
> 
>     -thomas
> 
I wonder whether there is, or should be, a way to set FALSE as the default?



From ligges at statistik.uni-dortmund.de  Thu May 19 17:46:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 May 2005 17:46:59 +0200
Subject: [R] R annoyances
In-Reply-To: <428CB24A.2070603@sprintmail.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>	<Pine.A41.4.61b.0505190740350.13738@homer09.u.washington.edu>
	<428CB24A.2070603@sprintmail.com>
Message-ID: <428CB4F3.6020607@statistik.uni-dortmund.de>

Rod Montgomery wrote:

> Thomas Lumley wrote:
> 
>> On Thu, 19 May 2005, Chalasani, Prasad wrote:
>>
>>> (b) When extracting 2 or more columns of a matrix,
>>>    R returns the result as a matrix, BUT when extracting
>>>    just one column, it returns a vector/array, rather than
>>>    a matrix, so I've to keep doing as.matrix, which is annoying.
>>>
>>>     sub.mtx <- as.matrix(mtx[,1])
>>>
>>>     Of course I could write a suitable function
>>>         cols <- function(mtx,range) as.matrix(mtx[, range])
>>>     but then I lose the syntactic sugar of being able to say "[,1]".
>>
>>
>>
>> This one is actually a FAQ,
>>         mtx[,1,drop=FALSE]
>>
>>     -thomas
>>
> I wonder whether there is, or should be, a way to set FALSE as the default?

First question: No.
Second question: No, because *many* functions do rely on the fact that 
x[,1] returns a vector rather than a matrix.

Uwe Ligges





> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Thu May 19 17:55:22 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 19 May 2005 11:55:22 -0400
Subject: [R] R annoyances
In-Reply-To: <428CABC8.7020601@statistik.uni-dortmund.de>
Message-ID: <20050519155523.UUMW27508.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Uwe,

I've often wondered why T and F aren't reserved words in R as TRUE and FALSE
are. Perhaps there's some use of T and F as variables, but that seems
ill-advised.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
> Sent: Thursday, May 19, 2005 10:08 AM
> To: Chalasani, Prasad
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R annoyances
> 
> Chalasani, Prasad wrote:
> 
> > Thanks all for pointing out that I can use 
> > 	mtx[,1,drop=F]
> 
> 
> Which, for example, won't work for
>   F <- 10.25
> 
> ---> drop=FALSE  !
>            ^^^^^
> 
> Uwe Ligges
> 
> 
> 
> > 
> > 
> > -----Original Message-----
> > From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> > Sent: Thursday, May 19, 2005 10:49 AM
> > To: Chalasani, Prasad
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] R annoyances
> > 
> > 
> > Chalasani, Prasad wrote:
> > 
> >>Dear R Folks,
> >>I'm a big fan of R, but there are a couple of things that 
> repeatedly 
> >>annoy me, and I wondered if anyone has neat ways to deal with them.
> >>
> >>(a) When using "apply" row-wise to a matrix, it returns
> >>    the results column-wise, and to preserve the original
> >>    orientation, I've to do a transpose. E.g. I've to keep
> >>    doing a transpose, which I consider to be quite annoying.
> >>	
> >>    transformed.mtx <- t(apply( mtx, 1, exp))
> > 
> > 
> > I'd rather type
> > 
> >    exp(mtx)
> > 
> > 
> > 
> > 
> >>(b) When extracting 2 or more columns of a matrix, 
> >>    R returns the result as a matrix, BUT when extracting
> >>    just one column, it returns a vector/array, rather than
> >>    a matrix, so I've to keep doing as.matrix, which is annoying.
> >>
> >>	sub.mtx <- as.matrix(mtx[,1])
> >>
> >>	Of course I could write a suitable function
> >>		cols <- function(mtx,range) as.matrix(mtx[, range])
> >>	but then I lose the syntactic sugar of being able to say "[,1]".
> > 
> > 
> > The docs suggest:
> > 
> >    mtx[ , 1, drop = FALSE]
> > 
> > 
> > Uwe Ligges
> > 
> > 
> > 
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list 
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From RRoa at fisheries.gov.fk  Thu May 19 16:17:17 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Thu, 19 May 2005 12:17:17 -0200
Subject: [R] Random/systematic selection of rows in a matrix
Message-ID: <AEF7C40954294D42990B3F90C6C73A4501016541@figmail.fig.fk>

Hi R people:
I am new to R. I am writing a function to (1) produce a sparse
stochastic Gaussian 2D field and (2) perform a systematic 
transect sampling on this field, this carried out many times
in a simulation framework. My function does a good job at 
producing the random field (a matrix of zeros and some 
manifestations of the stochastic process, depending on a 
parameter of the function determining how probable it is 
that the stochastic process will manifest itself). But, when
i try to collect some of the rows of the process as in
sampling with transects, i noticed in the output files that
the rows in the sample were not any of the rows in the original
matrix representing the random field. I show below the function.
I believe i am making a programming error due to my inexperience
with R. I hope some fo you can show me where the error is.
Thanks
Ruben

mcolasim7<-function(N.sim,pcell){
# Lattice definition
x<-seq(1,180,1)
y<-seq(1,540,1)
#Gaussian process
param<-c(6.63,2.24,1.82,4.36)
names(param)<-c("beta","sigmasq","tausq","varphi")
#Simulations loop - Function GaussRF from package RandomFields
for(i in 1:N.sim){
mcola<-GaussRF(x=x,y=y,param=param,grid=TRUE,model="gauss")
#Process thininng out with 'pcell': probability that the process will manifest itself
for(j in 1:540){
for(k in 1:180){
if(runif(1)<pcell) mcola[k,j]<-mcola[k,j] else mcola[k,j]<-0
}
}
file1.out<-paste("mcolasim7",i,"txt",sep=".")
write(t(mcola),file1.out,ncol=ncol(t(mcola)))
#Transect sampling on the thinned process - 20 equidistant transects parallel to x axis 
#First transect randomly chosen among first 27 rows in process
t=0
t<-(1+as.integer(27*runif(1)))
transis<-c(t,t+27,t+54,t+81,t+108,t+135,t+162,t+189,t+216,t+243,t+270,t+297,t+324,t+351,t+378,t+405,t+432,t+459,t+486,t+513)
mcolasamsis<-mat.or.vec(20,180)
for(l in 1:20){
mcolasamsis[l,]<-mcola[,transis[l]]
}
file2.out<-paste("mcolasamsis7",i,"txt",sep=".")
write(mcolasamsis,file2.out,ncol=ncol(mcolasamsis))
}
}



From pgilbert at bank-banque-canada.ca  Thu May 19 18:34:29 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Thu, 19 May 2005 12:34:29 -0400
Subject: [R] dse VAR models
In-Reply-To: <428B497B.80303@ntlworld.com>
References: <428B497B.80303@ntlworld.com>
Message-ID: <428CC015.9080400@bank-banque-canada.ca>

There are examples of this in the User's Guide, which gets installed  
under your R library as dse1/doc/dse-guide.pdf. There is lots of other 
information in the guide that you will probably find useful too.

Paul Gilbert

Samuel Kemp wrote:

> Hi,
>
> Can anyone tell me how to construct a simple VAR(1) time series with 
> two variables using the dse package? I would like to end up with two 
> time series
>
> y_1t = \phi_11 y_1,t-1 + \phi_12 y_2,t-1 + e_1t
> y_2t = \phi_21 y_1,t-1 + \phi_22 y_2,t-1 + e_2t
>
> Best regards,
>
> Sam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Thu May 19 18:39:13 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 19 May 2005 09:39:13 -0700 (PDT)
Subject: [R] R annoyances
In-Reply-To: <428CB24A.2070603@sprintmail.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>
	<Pine.A41.4.61b.0505190740350.13738@homer09.u.washington.edu>
	<428CB24A.2070603@sprintmail.com>
Message-ID: <Pine.A41.4.61b.0505190933480.327288@homer12.u.washington.edu>

On Thu, 19 May 2005, Rod Montgomery wrote:
> Thomas Lumley wrote:
>> This one is actually a FAQ,
>>         mtx[,1,drop=FALSE]
>> 
>>     -thomas
>> 
> I wonder whether there is, or should be, a way to set FALSE as the default?
>

There shouldn't be (apart from editing the code), because you really don't 
want something this basic to be unpredictable.

There have been discussions at several times about whether drop=FALSE or 
drop=TRUE should be the default. The decision has always been that 
programmers can cope either way, but that users probably don't expect 
mtx[,1] to be a vector, and that they definitely don't expect mtx[1,1] to be a 
matrix.

 	-thomas



From greg.snow at ihc.com  Thu May 19 18:53:42 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Thu, 19 May 2005 10:53:42 -0600
Subject: [R] Arranging Plots
Message-ID: <s28c7052.047@lp-msg1.co.ihc.com>

Using layout(matrix(c(1,1,2,2,0,3,3,0),2,3,byrow=TRUE)) 
may be closer to what the original intent was.

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> "Lapointe, Pierre" <Pierre.Lapointe at nbf.ca> 05/19/05 05:51AM >>>
Try this:

layout(matrix(c(1,0,2,0,3,0),2,3,byrow=TRUE))
plot(runif(10),main="P1")
plot(runif(10),main="P2")
plot(runif(10),main="P3")


Regards,

Pierre Lapointe
Assistant Market Strategist
National Bank Financial



***********************************************************************************

AVIS DE NON-RESPONSABILITE:\ Ce document transmis par
courri...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From munoz at stat.wisc.edu  Thu May 19 19:01:02 2005
From: munoz at stat.wisc.edu (Alejandro Munoz del Rio)
Date: Thu, 19 May 2005 12:01:02 -0500
Subject: [R] reason for na.last=TRUE in rank
Message-ID: <1116522062.428cc64ecdb0a@webmail.cs.wisc.edu>

Dear UseRs,

Could someone explain to me why the default behaviour of rank() is to assign the
largest rank to missing data

> rank(c(3, 1, NA))
[1] 2 1 3

as opposed to what I would hazard would be the expected 2, 1, NA? 

Despite consistency being the hobgoblin of little minds, of two
closely related functions one handles NAs in the same way (order())
but another one doesn't (sort()). order() also uses the "NA last" rule
by default, whereas sort() removes NAs.

alejandro



From vograno at evafunds.com  Thu May 19 19:40:26 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 19 May 2005 10:40:26 -0700
Subject: [R] R annoyances
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A59E9748@phost015.EVAFUNDS.intermedia.net>

I think the flaw in this reasoning is that programmers are not
considered users. IMO, making a better language is beneficial for users.

I am now watching how a new colleague of mine, a very good C++
programmer turning into a data miner, is struggling w/ many
"irregularities" of R.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Lumley
> Sent: Thursday, May 19, 2005 9:39 AM
> To: Rod Montgomery
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R annoyances
> 
> On Thu, 19 May 2005, Rod Montgomery wrote:
> > Thomas Lumley wrote:
> >> This one is actually a FAQ,
> >>         mtx[,1,drop=FALSE]
> >> 
> >>     -thomas
> >> 
> > I wonder whether there is, or should be, a way to set FALSE 
> as the default?
> >
> 
> There shouldn't be (apart from editing the code), because you 
> really don't want something this basic to be unpredictable.
> 
> There have been discussions at several times about whether 
> drop=FALSE or drop=TRUE should be the default. The decision 
> has always been that programmers can cope either way, but 
> that users probably don't expect mtx[,1] to be a vector, and 
> that they definitely don't expect mtx[1,1] to be a matrix.
> 
>  	-thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From uofiowa at gmail.com  Thu May 19 19:42:21 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Thu, 19 May 2005 13:42:21 -0400
Subject: [R] does column exist?
Message-ID: <3f87cc6d05051910421ad010a9@mail.gmail.com>

How do I test if a data.frame has a column named X?
exists(o) checks if the object exists or not, I want to test if a
data.frame object (df) has a column names(X), something like:
exists(df$X)



From insu_paek at harcourt.com  Thu May 19 19:42:33 2005
From: insu_paek at harcourt.com (insu_paek@harcourt.com)
Date: Thu, 19 May 2005 12:42:33 -0500
Subject: [R] R matrix sorting question
Message-ID: <OFB3FAC076.C60DDB1B-ON86257006.0060BF08-86257006.006127A7@harcourtbrace.com>

Dear there,

     I am trying to do the following stuff. Could you let me know how to do
it efficiently?

    > aaa
     [,1] [,2]
[1,]    1 -0.2
[2,]    3  0.8
[3,]    4  0.3
[4,]    5  0.2
[5,]    7  0.9

 And I would like to sort the matrix by column 2 (and accordingly  column 1
sorted as well).
The desired matrix will be

1  -0.2
5   0.2
4   0.3
3   0.8
7   0.9

If using Excel or SAS, I can do this easily, but I couldn't figure out how
to do it in R.
  Could you let me know the efficient way to do this ?
 I appreciate if the solution or reply comes as quickly as possible.

 Thank you very much.

Insu



From ccleland at optonline.net  Thu May 19 19:50:57 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 19 May 2005 13:50:57 -0400
Subject: [R] does column exist?
In-Reply-To: <3f87cc6d05051910421ad010a9@mail.gmail.com>
References: <3f87cc6d05051910421ad010a9@mail.gmail.com>
Message-ID: <428CD201.8050004@optonline.net>

How about is.null(df$X)==FALSE ?

Omar Lakkis wrote:
> How do I test if a data.frame has a column named X?
> exists(o) checks if the object exists or not, I want to test if a
> data.frame object (df) has a column names(X), something like:
> exists(df$X)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From apjaworski at mmm.com  Thu May 19 19:53:24 2005
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Thu, 19 May 2005 12:53:24 -0500
Subject: [R] does column exist?
In-Reply-To: <3f87cc6d05051910421ad010a9@mail.gmail.com>
Message-ID: <OFFB688EFD.2F771D67-ON86257006.00621F35-86257006.00624612@mmm.com>






There may be a simpler way, but this

is.na(match("X", names(df)))

should return TRUE if the column name exists and FALSE otherwise.

Cheers,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             Omar Lakkis                                                   
             <uofiowa at gmail.co                                             
             m>                                                         To 
             Sent by:                  r-help at stat.math.ethz.ch            
             r-help-bounces at st                                          cc 
             at.math.ethz.ch                                               
                                                                   Subject 
                                       [R] does column exist?              
             05/19/2005 12:42                                              
             PM                                                            
                                                                           
                                                                           
             Please respond to                                             
                Omar Lakkis                                                
             <uofiowa at gmail.co                                             
                    m>                                                     
                                                                           
                                                                           




How do I test if a data.frame has a column named X?
exists(o) checks if the object exists or not, I want to test if a
data.frame object (df) has a column names(X), something like:
exists(df$X)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From william.astle at imperial.ac.uk  Thu May 19 19:52:49 2005
From: william.astle at imperial.ac.uk (Astle, William J)
Date: Thu, 19 May 2005 18:52:49 +0100
Subject: [R] Loading a dynamic library
Message-ID: <EFCF60A4595E9148A108047D78D8D664013ADDC7@icex1.ic.ac.uk>

Hi,

I'm trying to load a .dll library into R 2.1.0 on Windows using the
"dyn.load" function.  The library is compiled with gcc 3.3.3 on cygwin
1.5.16.  

I compile and link:

$ gcc -c myfile.cpp -o myfile.o [HRT]
$ gcc -shared myfile.o -o myfile.dll [HRT]

I then type, in the R console, 

> dynload("myfile.dll")[HRT]

And R hangs. 

Any help appreciated.

Thanks.

Will

_____________________________________________________________

William Astle.
PhD Student,
Department of Epidemiology and Public Health,
Imperial College London,
St Mary's Hospital Campus,
Norfolk Place,
Paddington.
W2 1NY.

wja [at] ic [dot] ac [dot] uk



From gunter.berton at gene.com  Thu May 19 19:54:59 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 19 May 2005 10:54:59 -0700
Subject: [R] R annoyances
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A59E9748@phost015.EVAFUNDS.intermedia.net>
Message-ID: <200505191755.j4JHsxUO004359@volta.gene.com>

Vadim et.al:

I do not care to comment one way or the other about R's "irregularities.'
But I am puzzled by your statement that a "good C++ programmer is struggling
with R." Why should they not struggle?! R is primarily a language for data
analysis, statistics, and graphics. I do not understand why someone who is a
C++ programmer would be expected to have the knowledge and experience to be
a "data miner" and would not therefore struggle to deal with the statistical
and data analysis issues that are deliberately at the heart of many of R's
programming conventions.

Is there something here that I am missing, or is this yet another example of
Frank Harrell's "instant brain surgeon" commentary?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vadim 
> Ogranovich
> Sent: Thursday, May 19, 2005 10:40 AM
> To: Thomas Lumley; Rod Montgomery
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] R annoyances
> 
> I think the flaw in this reasoning is that programmers are not
> considered users. IMO, making a better language is beneficial 
> for users.
> 
> I am now watching how a new colleague of mine, a very good C++
> programmer turning into a data miner, is struggling w/ many
> "irregularities" of R.
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Lumley
> > Sent: Thursday, May 19, 2005 9:39 AM
> > To: Rod Montgomery
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] R annoyances
> > 
> > On Thu, 19 May 2005, Rod Montgomery wrote:
> > > Thomas Lumley wrote:
> > >> This one is actually a FAQ,
> > >>         mtx[,1,drop=FALSE]
> > >> 
> > >>     -thomas
> > >> 
> > > I wonder whether there is, or should be, a way to set FALSE 
> > as the default?
> > >
> > 
> > There shouldn't be (apart from editing the code), because you 
> > really don't want something this basic to be unpredictable.
> > 
> > There have been discussions at several times about whether 
> > drop=FALSE or drop=TRUE should be the default. The decision 
> > has always been that programmers can cope either way, but 
> > that users probably don't expect mtx[,1] to be a vector, and 
> > that they definitely don't expect mtx[1,1] to be a matrix.
> > 
> >  	-thomas
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu May 19 20:06:29 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 May 2005 19:06:29 +0100 (BST)
Subject: [R] Learning rate with nnet
In-Reply-To: <Pine.LNX.4.60.0505191107001.22137@paris.DMS.UMontreal.CA>
References: <Pine.LNX.4.60.0505191107001.22137@paris.DMS.UMontreal.CA>
Message-ID: <Pine.LNX.4.61.0505191904420.13182@gannet.stats>

On Thu, 19 May 2005, Elyse Picard wrote:

> can I change the learning rate with nnet?  And if yes, how?  If no, what is 
> the default one?

I think you need to read the book nnet supports.  It does not do on-line
learning and does not have a learning rate.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

(which mentions reading the book).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu May 19 20:08:51 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 May 2005 19:08:51 +0100 (BST)
Subject: [R] install R packages--sorry last letter that I sent I had an
	error, this one is the right one
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B183357C94@nsabpmail>
References: <3D0B2434377E984E9C85CAA316F8B183357C94@nsabpmail>
Message-ID: <Pine.LNX.4.61.0505191907160.13182@gannet.stats>

On Thu, 19 May 2005, Li, Jia wrote:

> Dear All,
>
> When I tried to install R packages I found this error:
>> library(survival)
> Loading required package: splines
>> install.packages("splines")
> --- Please select a CRAN mirror for use in this session ---
> Warning message:
> package splines is in use and will not be installed
>
> But I just opened R, and haven't used anything yet, how come did it say 
> "package splines is in use"?

Package survival uses it, and you did use that  ....

Note that package splines is a standard package, part of the 
distribution.  install.packages() will not find it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Thu May 19 20:12:05 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 19 May 2005 11:12:05 -0700
Subject: [R] does column exist?
In-Reply-To: <OFFB688EFD.2F771D67-ON86257006.00621F35-86257006.00624612@mmm.com>
References: <OFFB688EFD.2F771D67-ON86257006.00621F35-86257006.00624612@mmm.com>
Message-ID: <428CD6F5.3070904@pdf.com>

	  It depends on what Omar means by "a column named X".  Consider the 
following:

 > tstDF <- data.frame(aa=1, bb=2)
 > tstDF$a
[1] 1
 > is.null(tstDF$a)
[1] FALSE
 > is.na(match("a", names(tstDF)))
[1] TRUE

	  Does tstDF have a column named "a"?  It has a column named "aa", 
which is accessed by tstDF$a;  is.null(tstDF$a) correctly identifies its 
presence, while is.na(match("a", names(tstDF))) correctly says that "a" 
is not the official name of any column of tstDF.

	  spencer graves

apjaworski at mmm.com wrote:

> 
> 
> 
> 
> There may be a simpler way, but this
> 
> is.na(match("X", names(df)))
> 
> should return TRUE if the column name exists and FALSE otherwise.
> 
> Cheers,
> 
> Andy
> 
> __________________________________
> Andy Jaworski
> 518-1-01
> Process Laboratory
> 3M Corporate Research Laboratory
> -----
> E-mail: apjaworski at mmm.com
> Tel:  (651) 733-6092
> Fax:  (651) 736-3122
> 
> 
>                                                                            
>              Omar Lakkis                                                   
>              <uofiowa at gmail.co                                             
>              m>                                                         To 
>              Sent by:                  r-help at stat.math.ethz.ch            
>              r-help-bounces at st                                          cc 
>              at.math.ethz.ch                                               
>                                                                    Subject 
>                                        [R] does column exist?              
>              05/19/2005 12:42                                              
>              PM                                                            
>                                                                            
>                                                                            
>              Please respond to                                             
>                 Omar Lakkis                                                
>              <uofiowa at gmail.co                                             
>                     m>                                                     
>                                                                            
>                                                                            
> 
> 
> 
> 
> How do I test if a data.frame has a column named X?
> exists(o) checks if the object exists or not, I want to test if a
> data.frame object (df) has a column names(X), something like:
> exists(df$X)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Thu May 19 20:11:55 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 19 May 2005 14:11:55 -0400
Subject: [R] R matrix sorting question
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A943A@uswpmx00.merck.com>

You can use "order" as follows:

> p <- order(aaa[,2])
> aa[p,]

p is the permutation putting aaa[,2] in ascending order, then aaa[p,]
reorders the rows according to p.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
insu_paek at harcourt.com
Sent: Thursday, May 19, 2005 1:43 PM
To: R-help at stat.math.ethz.ch
Subject: [R] R matrix sorting question


Dear there,

     I am trying to do the following stuff. Could you let me know how to do
it efficiently?

    > aaa
     [,1] [,2]
[1,]    1 -0.2
[2,]    3  0.8
[3,]    4  0.3
[4,]    5  0.2
[5,]    7  0.9

 And I would like to sort the matrix by column 2 (and accordingly  column 1
sorted as well).
The desired matrix will be

1  -0.2
5   0.2
4   0.3
3   0.8
7   0.9

If using Excel or SAS, I can do this easily, but I couldn't figure out how
to do it in R.
  Could you let me know the efficient way to do this ?
 I appreciate if the solution or reply comes as quickly as possible.

 Thank you very much.

Insu

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu May 19 20:12:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 May 2005 19:12:27 +0100 (BST)
Subject: [R] does column exist?
In-Reply-To: <OFFB688EFD.2F771D67-ON86257006.00621F35-86257006.00624612@mmm.com>
References: <OFFB688EFD.2F771D67-ON86257006.00621F35-86257006.00624612@mmm.com>
Message-ID: <Pine.LNX.4.61.0505191909480.13182@gannet.stats>

On Thu, 19 May 2005 apjaworski at mmm.com wrote:

> There may be a simpler way, but this
>
> is.na(match("X", names(df)))
>
> should return TRUE if the column name exists and FALSE otherwise.

Mere syntactic sugar, but "X" %in% names(mydf) is self-documenting.

> How do I test if a data.frame has a column named X?
> exists(o) checks if the object exists or not, I want to test if a
> data.frame object (df) has a column names(X), something like:
> exists(df$X)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu May 19 20:30:05 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 19 May 2005 14:30:05 -0400
Subject: [R] R matrix sorting question
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F406D@us-arlington-0668.mail.saic.com>

Try:

> A=matrix(c(1,3,4,5,7, -0.2, 0.8, 0.3, 0.2, 0.9 ),5,2)
> A
     [,1] [,2]
[1,]    1 -0.2
[2,]    3  0.8
[3,]    4  0.3
[4,]    5  0.2
[5,]    7  0.9
> A[order(A[,2]), ]
     [,1] [,2]
[1,]    1 -0.2
[2,]    5  0.2
[3,]    4  0.3
[4,]    3  0.8
[5,]    7  0.9 


Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
insu_paek at harcourt.com
Sent: Thursday, May 19, 2005 1:43 PM
To: R-help at stat.math.ethz.ch
Subject: [R] R matrix sorting question

Dear there,

     I am trying to do the following stuff. Could you let me know how to do
it efficiently?

    > aaa
     [,1] [,2]
[1,]    1 -0.2
[2,]    3  0.8
[3,]    4  0.3
[4,]    5  0.2
[5,]    7  0.9

 And I would like to sort the matrix by column 2 (and accordingly  column 1
sorted as well).
The desired matrix will be

1  -0.2
5   0.2
4   0.3
3   0.8
7   0.9

If using Excel or SAS, I can do this easily, but I couldn't figure out how
to do it in R.
  Could you let me know the efficient way to do this ?
 I appreciate if the solution or reply comes as quickly as possible.

 Thank you very much.

Insu

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From vograno at evafunds.com  Thu May 19 21:10:21 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 19 May 2005 12:10:21 -0700
Subject: [R] R annoyances
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A59E9760@phost015.EVAFUNDS.intermedia.net>

I guess it depends on what kind of data analysis one does. R is designed
and best suited for the analysis that starts with a data frame which
fits in 1/10th of your computer RAM. R programming is then mostly
limited to writing small convenience functions for better presentation,
visualization, etc. Or alternatively one implements a new fitting
procedure/algorithm and applies it to the data.

Now things begin to look harder when you have 200G of data and 8G of RAM
and still need to find "structure" in the data. You need to pre-process
the data, recover from *unexpected* failures, store and retrieve
intermediate data sets, etc. This requires qualities of a good
general-purpose programming language. Note, we do not use R to program a
system, we do data analysis so we should be considered R *users*.
In my view, and the experience of the colleague of my confirms it, R has
a long way to go to become a wrinkle-free general purpose language.

To your specific question, why good (C++) programmers should not
struggle with R? Because they have the skills to plan sizeable programs
in any wrinkle-free language.

Hope this makes my earier comments more clear,
Vadim

> -----Original Message-----
> From: Berton Gunter [mailto:gunter.berton at gene.com] 
> Sent: Thursday, May 19, 2005 10:55 AM
> To: Vadim Ogranovich; 'Thomas Lumley'; 'Rod Montgomery'
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] R annoyances
> 
> Vadim et.al:
> 
> I do not care to comment one way or the other about R's 
> "irregularities.'
> But I am puzzled by your statement that a "good C++ 
> programmer is struggling with R." Why should they not 
> struggle?! R is primarily a language for data analysis, 
> statistics, and graphics. I do not understand why someone who is a
> C++ programmer would be expected to have the knowledge and 
> experience to 
> C++ be
> a "data miner" and would not therefore struggle to deal with 
> the statistical and data analysis issues that are 
> deliberately at the heart of many of R's programming conventions.
> 
> Is there something here that I am missing, or is this yet 
> another example of Frank Harrell's "instant brain surgeon" commentary?
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the 
> scientific learning process."  - George E. P. Box
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vadim 
> > Ogranovich
> > Sent: Thursday, May 19, 2005 10:40 AM
> > To: Thomas Lumley; Rod Montgomery
> > Cc: r-help at stat.math.ethz.ch
> > Subject: RE: [R] R annoyances
> > 
> > I think the flaw in this reasoning is that programmers are not 
> > considered users. IMO, making a better language is beneficial for 
> > users.
> > 
> > I am now watching how a new colleague of mine, a very good C++ 
> > programmer turning into a data miner, is struggling w/ many 
> > "irregularities" of R.
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Thomas Lumley
> > > Sent: Thursday, May 19, 2005 9:39 AM
> > > To: Rod Montgomery
> > > Cc: r-help at stat.math.ethz.ch
> > > Subject: Re: [R] R annoyances
> > > 
> > > On Thu, 19 May 2005, Rod Montgomery wrote:
> > > > Thomas Lumley wrote:
> > > >> This one is actually a FAQ,
> > > >>         mtx[,1,drop=FALSE]
> > > >> 
> > > >>     -thomas
> > > >> 
> > > > I wonder whether there is, or should be, a way to set FALSE
> > > as the default?
> > > >
> > > 
> > > There shouldn't be (apart from editing the code), because 
> you really 
> > > don't want something this basic to be unpredictable.
> > > 
> > > There have been discussions at several times about whether 
> > > drop=FALSE or drop=TRUE should be the default. The decision has 
> > > always been that programmers can cope either way, but that users 
> > > probably don't expect mtx[,1] to be a vector, and that they 
> > > definitely don't expect mtx[1,1] to be a matrix.
> > > 
> > >  	-thomas
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> 
>



From helprhelp at gmail.com  Thu May 19 21:10:42 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 19 May 2005 14:10:42 -0500
Subject: [R] Re: text mining: ttda
In-Reply-To: <f361fe11c288162c610f700d75c33cbd@unil.ch>
References: <cdf8178305051810067f2d065@mail.gmail.com>
	<cdf817830505181517694122a8@mail.gmail.com>
	<f361fe11c288162c610f700d75c33cbd@unil.ch>
Message-ID: <cdf817830505191210377e946f@mail.gmail.com>

Dear Jean-Pierre:
I used R, version 2.0.1 and I followed the example from 
?ttda.forms.frame
and got a dataframe:  federalist.frame and run
> ttda.lemmatisation(federalist.frame$graphical.forms, ispell.path='/usr/bin/ispell')
[1] "ispell output : wrong size ..."
Error in ttda.util.lemmatisation(levels(forms), ...) :
        ispell output : wrong size ...

and I got the error message as above. 

Please be advised!

Thanks,

weiwei

On 5/19/05, Jean-Pierre Muller <jean-pierre.mueller at unil.ch> wrote:
> Dear Weiwei,
> 
> Le 19 mai 05, ?? 00:17, Weiwei Shi a ??crit :
> 
> > Can anyone suggest some good text mining reference or books?
> >
> > thanks,
> >
> > weiwei
> >
> > On 5/18/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> >> Hi,
> >> I am working on a text mining project and i am interested in ttda
> >> package. however, I really cannot find the document for this package
> >> in English.
> 
> I am sorry, but for the moment there is not an english manual...
> It is in my "todo" list for a while. R 2.1 as introduced many new
> functions for text,
> and i need correct my package first. May be in the nexts weeks?
> 
> The package try to code what is shown in:
>    Lebart, L., Salem, A. and Berry, L. (1998) "Exploring textual data".
> Dordrecht: Kluwer.
> 
> HTH.
> 
> 
> >> Can anyone give me some help? btw, is there any other package in R
> >> doing text mining. I googled MedlineR which might help my project.
> >> Anyone can give me some links on how to use it too?
> >>
> >> thanks,
> >>
> >> --
> >> Weiwei Shi, Ph.D
> >>
> >> "Did you always know?"
> >> "No, I did not. But I believed..."
> >> ---Matrix III
> >>
> >
> >
> > --
> > Weiwei Shi, Ph.D
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> --
> Jean-Pierre M??ller
> SSP / BFSH2 / UNIL / CH - 1015 Lausanne
> Voice:+41 21 692 3116 / Fax:+41 21 692 3115
> 
> Please avoid sending me Word or PowerPoint attachments.
>   See http://www.gnu.org/philosophy/no-word-attachments.html
> S'il vous pla??t, ??vitez de m'envoyer des attachements au format Word ou
> PowerPoint.
>   Voir http://www.gnu.org/philosophy/no-word-attachments.fr.html
> 
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From jfox at mcmaster.ca  Thu May 19 21:10:53 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 19 May 2005 15:10:53 -0400
Subject: [R] R annoyances
In-Reply-To: <20050519172201.GI1178@jtkpc.cmp.uea.ac.uk>
Message-ID: <20050519191054.BVKP26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Jan,

Since you can use variables named c, q, or t in any event, I don't see why
the existence of functions with these names is much of an impediment.

The problem that I see with T and F is that allowing them to be redefined
sets a trap for people. If R wants to discourage use of T and F for TRUE and
FALSE, then why provide standard global variables by these names? On the
other hand, if providing T and F is considered desirable (e.g., for S-PLUS
compatibility), then why not make them reserved names?

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Jan T. Kim [mailto:jtk at cmp.uea.ac.uk] 
> Sent: Thursday, May 19, 2005 12:22 PM
> To: John Fox
> Subject: Re: [R] R annoyances
> 
> On Thu, May 19, 2005 at 11:55:22AM -0400, John Fox wrote:
> > Dear Uwe,
> > 
> > I've often wondered why T and F aren't reserved words in R 
> as TRUE and 
> > FALSE are. Perhaps there's some use of T and F as 
> variables, but that 
> > seems ill-advised.
> 
> Personally, I'd rather argue the other way around: Reserved 
> words should be words that should be more unique and 
> expressive than just a single letter.
> 
> In fact, I've found it slightly irritating at times that c, q 
> and t are functions in the base package, as I'm somewhat 
> prone to use all of these as local variable names...
> 
> Best regards, Jan
> --
>  +- Jan T. Kim 
> -------------------------------------------------------+
>  |    *NEW*    email: jtk at cmp.uea.ac.uk                       
>         |
>  |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk     
>         |
>  *-----=<  hierarchical systems are for files, not for humans 
>  >=-----*



From pburns at pburns.seanet.com  Thu May 19 21:12:05 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 19 May 2005 20:12:05 +0100
Subject: [R] R annoyances
In-Reply-To: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>
Message-ID: <428CE505.3010700@pburns.seanet.com>

(a)  There is 'stable.apply' in S Poetry that looks to me like it should
work in R, but I haven't tested it.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Chalasani, Prasad wrote:

>Dear R Folks,
>I'm a big fan of R, but there are a couple of things
>that repeatedly annoy me, and I wondered if anyone
>has neat ways to deal with them.
>
>(a) When using "apply" row-wise to a matrix, it returns
>    the results column-wise, and to preserve the original
>    orientation, I've to do a transpose. E.g. I've to keep
>    doing a transpose, which I consider to be quite annoying.
>	
>    transformed.mtx <- t(apply( mtx, 1, exp))
>
>(b) When extracting 2 or more columns of a matrix, 
>    R returns the result as a matrix, BUT when extracting
>    just one column, it returns a vector/array, rather than
>    a matrix, so I've to keep doing as.matrix, which is annoying.
>
>	sub.mtx <- as.matrix(mtx[,1])
>
>	Of course I could write a suitable function
>		cols <- function(mtx,range) as.matrix(mtx[, range])
>	but then I lose the syntactic sugar of being able to say "[,1]".
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From David.Brahm at geodecapital.com  Thu May 19 21:22:53 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Thu, 19 May 2005 15:22:53 -0400
Subject: [R] Larger X11 fonts under R-2.1.0
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBCD@MSGBOSCLF2WIN.DMN1.FMR.COM>

I view plots on my screen with X11(width=.455*11, height=.455*8.5),
which creates a small window with the American standard aspect ratio.
Under R-2.0.1 and earlier, the default fonts were a reasonable size,
but under R-2.1.0 they are too big and fat.  I now have to either set
pointsize=10 in X11(), or par(cex=.7) afterwards.

The NEWS file has this to say about X11 fonts:
  The changes to font handling in the X11 module are based on the
  Japanization patches of Eiji Nakama.
I'm not sure if that's relevant to the change I'm seeing.  Has anyone
else noticed a difference in the X11 fonts?  (I assume this is
different from Xiang-Jun Lu's problem, since I do not get any error
messages that fonts "could not be loaded".)

-- David Brahm (brahm at alum.mit.edu)



From trafton at itd.nrl.navy.mil  Thu May 19 21:34:25 2005
From: trafton at itd.nrl.navy.mil (Greg Trafton)
Date: Thu, 19 May 2005 15:34:25 -0400
Subject: [R] logistic regression:  differential importance of regressors
Message-ID: <m24qczm2m6.fsf@viz.local>

Hi, All.  I have a logistic regression model that I have run.  The
question came up:  which of these regressors is more important than
another?

(I'm using Design)

Logistic Regression Model

lrm(formula = iconicgesture ~ ST + SSP + magnitude + Condition + 
    Expertise, data = d)

          Coef    S.E.   Wald Z P     
Intercept -3.2688 0.2854 -11.45 0.0000
ST         2.0871 0.2730   7.64 0.0000
SSP        0.7454 0.3031   2.46 0.0139
magnitude -0.9905 0.6284  -1.58 0.1150
Condition  0.9506 0.2932   3.24 0.0012
Expertise  0.8508 0.2654   3.21 0.0013

The real question is that, since both ST and SSP load significantly
into the model, how do I show that ST has a bigger/smaller/similar
effect than SSP?

thanks in advance!
greg



From Robert.McGehee at geodecapital.com  Thu May 19 22:02:55 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Thu, 19 May 2005 16:02:55 -0400
Subject: [R] reason for na.last=TRUE in rank
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C9465D0@MSGBOSCLB2WIN.DMN1.FMR.COM>

Because rank and order are (supposed to be) inverses of each other.

For example:
> a <- c(3, 1, NA)
> a[order(a[rank(a)])]
[1] 3 1 NA
> a[rank(a[order(a)])]
[1] 3 1 NA

BUT

> a[order(a[rank(a, na.last = FALSE)])]
[1] 1 NA 3

> a[rank(a[order(a)], na.last = FALSE)]
[1] 1 NA 3

-----Original Message-----
From: Alejandro Munoz del Rio [mailto:munoz at stat.wisc.edu] 
Sent: Thursday, May 19, 2005 1:01 PM
To: r-help at stat.math.ethz.ch
Subject: [R] reason for na.last=TRUE in rank


Dear UseRs,

Could someone explain to me why the default behaviour of rank() is to
assign the
largest rank to missing data

> rank(c(3, 1, NA))
[1] 2 1 3

as opposed to what I would hazard would be the expected 2, 1, NA? 

Despite consistency being the hobgoblin of little minds, of two
closely related functions one handles NAs in the same way (order())
but another one doesn't (sort()). order() also uses the "NA last" rule
by default, whereas sort() removes NAs.

alejandro

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu May 19 22:04:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 May 2005 21:04:10 +0100 (BST)
Subject: [R] Larger X11 fonts under R-2.1.0
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBCD@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBCD@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <Pine.LNX.4.61.0505192057520.14444@gannet.stats>

On Thu, 19 May 2005, Brahm, David wrote:

> I view plots on my screen with X11(width=.455*11, height=.455*8.5),
> which creates a small window with the American standard aspect ratio.
> Under R-2.0.1 and earlier, the default fonts were a reasonable size,
> but under R-2.1.0 they are too big and fat.  I now have to either set
> pointsize=10 in X11(), or par(cex=.7) afterwards.

So you should.  We have corrected a bug: you now get the size you ask for
and 12pt on such a small window will be too large.

DId you think to actually measure the sizes?  Might be interesting 
(although you may need to measure the window too).

> The NEWS file has this to say about X11 fonts:
>  The changes to font handling in the X11 module are based on the
>  Japanization patches of Eiji Nakama.

The change is more likely to be

     o	X11() was only scaling its fonts to pointsize if the dpi
 	was within 0.5 of 100dpi.

in the BUGS list.

I happen to have 99.4 and 120 dpi screens, and I assure you the previous 
default of 75dpi fonts was way too small for the default 7" window.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu May 19 22:11:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 May 2005 21:11:03 +0100 (BST)
Subject: [R] Loading a dynamic library
In-Reply-To: <EFCF60A4595E9148A108047D78D8D664013ADDC7@icex1.ic.ac.uk>
References: <EFCF60A4595E9148A108047D78D8D664013ADDC7@icex1.ic.ac.uk>
Message-ID: <Pine.LNX.4.61.0505192105430.14444@gannet.stats>

On Thu, 19 May 2005, Astle, William J wrote:

> Hi,
>
> I'm trying to load a .dll library into R 2.1.0 on Windows using the
> "dyn.load" function.  The library is compiled with gcc 3.3.3 on cygwin
> 1.5.16.

Please use the correct OS's DLLs: Cygwin is another OS hosted on Windows. 
This might work, but it is not as good an idea as using the recommended 
compilers, or indeed any other Windows compiler.

> I compile and link:
>
> $ gcc -c myfile.cpp -o myfile.o [HRT]
> $ gcc -shared myfile.o -o myfile.dll [HRT]
>
> I then type, in the R console,
>
>> dynload("myfile.dll")[HRT]

What is `dynload', and what does [HRT] mean?

> And R hangs.

Are you sure?  It is more likely that your DLL's initialization code 
hangs.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.murrell at auckland.ac.nz  Thu May 19 22:24:40 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 20 May 2005 08:24:40 +1200
Subject: [R] Lattice: how to get default ylim?
References: <244663517.20050519141653@eimb.ru>	<200505190911.38996.deepayan@stat.wisc.edu>	<20050519141128.GA12813@jessie.research.bell-labs.com>
	<200505190948.18843.deepayan@stat.wisc.edu>
Message-ID: <428CF608.2070209@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
> On Thursday 19 May 2005 9:11 am, David James wrote:
> 
>>Deepayan Sarkar wrote:
>>
>>>>     v <- current.viewport()        ## requires R 2.1.0 (I believe)
>>>
>>>No, I think it's been there for a while. However, AFAIR the fact that
>>>viewports have components xscale and yscale that can be accessed like
>>>this is undocumented and may change if the implementation changes (which
>>>is a real possibility).
>>>
>>>Ideally, there should be exported interfaces to access this information,
>>>either in grid or lattice. One of the reasons there isn't is that you
>>>rarely
>>
>>Yes, I agree that such an interface is quite desirable.
> 
> 
> OK, I'll put something in the next version of lattice.


The expression I recommend for this sort of thing is something like ...

convertY(unit(0:1, "npc"), "native", valueOnly=TRUE)

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From pshannon at systemsbiology.org  Thu May 19 22:45:42 2005
From: pshannon at systemsbiology.org (Paul Shannon)
Date: 19 May 2005 13:45:42 -0700
Subject: [R] source-only package, but still: Error: package 'simple' was
	built for powerpc-apple-darwin7.9.0
In-Reply-To: <Pine.LNX.4.61.0505190006310.26373@gannet.stats> (message from
	Prof Brian Ripley on Thu, 19 May 2005 00:12:24 +0100 (BST))
References: <EXCHANGEJqAJ7nvOYFg0001ca05@exchange.systemsbiology.net>
	<Pine.LNX.4.61.0505190006310.26373@gannet.stats>
Message-ID: <EXCHANGECnXgKoJr5b60001d293@exchange.systemsbiology.net>

Hi Brian, 

Thanks very much for your two tips concerning my failed attempt, yesterday, to build and
install a package:

 "I suspect your package has structure it is not using."

     I had an empty 'src' directory.  Once I removed that, BUILD & INSTALL on another
     OS worked fine.

  "We encourage you to use a repository, as in the article in the current R-news."

I've spent a few hours on this, and though I made some pretty good progress, I am not
yet completely successful.  I read your article, "Packages and their Management in R 2.1.0" 
and the two antecedents, "R Help Desk, 12/03" and "Writing R Extensions".  I also
attempted some reverse-engineering by studying directory structure at the CRAN repository.

I can now use

   available.packages (contriburl='http://myhost/R')

and see the simple package I installed there, but I cannot load it unless
(this is embarrassing...) I have two copies of the tar.gz file, one at
the root of the repository, and one in src/contrib within the repository.

I am sure that the repository mechanism is sensible, but I am also sure I don't yet
understand it.  The directory structure, and the roles of the PACKAGES, replisting, and
repdatadesc.rda are as yet unclear to me.

Can you provide some further help?  I'll be grateful.

For what it's worth (and assuming it doesn't yet exist) I would be glad, at the conclusion 
of this exercise, to create a step-by-step tutorial, titled, perhaps: 

  "Sharing Code: An Idiot's Guide to packaging an R function, creating a repository, and
   installing from that repository into a local library."

Regards,

 - Paul Shannon
   Institute for Systems Biology
   Seattle



From andy_liaw at merck.com  Thu May 19 22:47:57 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 19 May 2005 16:47:57 -0400
Subject: [R] R annoyances
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E85B@usctmx1106.merck.com>

> From: Vadim Ogranovich
> 
> I guess it depends on what kind of data analysis one does. R 
> is designed
> and best suited for the analysis that starts with a data frame which
> fits in 1/10th of your computer RAM. R programming is then mostly
> limited to writing small convenience functions for better 
> presentation,
> visualization, etc. Or alternatively one implements a new fitting
> procedure/algorithm and applies it to the data.
> 
> Now things begin to look harder when you have 200G of data 
> and 8G of RAM
> and still need to find "structure" in the data. You need to 
> pre-process
> the data, recover from *unexpected* failures, store and retrieve
> intermediate data sets, etc. This requires qualities of a good
> general-purpose programming language. Note, we do not use R 
> to program a
> system, we do data analysis so we should be considered R *users*.
> In my view, and the experience of the colleague of my 
> confirms it, R has
> a long way to go to become a wrinkle-free general purpose language.
> 
> To your specific question, why good (C++) programmers should not
> struggle with R? Because they have the skills to plan 
> sizeable programs
> in any wrinkle-free language.

Could you please define "wrinkle-free language", or give an (some?) example?

Andy

 
> Hope this makes my earier comments more clear,
> Vadim
> 
> > -----Original Message-----
> > From: Berton Gunter [mailto:gunter.berton at gene.com] 
> > Sent: Thursday, May 19, 2005 10:55 AM
> > To: Vadim Ogranovich; 'Thomas Lumley'; 'Rod Montgomery'
> > Cc: r-help at stat.math.ethz.ch
> > Subject: RE: [R] R annoyances
> > 
> > Vadim et.al:
> > 
> > I do not care to comment one way or the other about R's 
> > "irregularities.'
> > But I am puzzled by your statement that a "good C++ 
> > programmer is struggling with R." Why should they not 
> > struggle?! R is primarily a language for data analysis, 
> > statistics, and graphics. I do not understand why someone who is a
> > C++ programmer would be expected to have the knowledge and 
> > experience to 
> > C++ be
> > a "data miner" and would not therefore struggle to deal with 
> > the statistical and data analysis issues that are 
> > deliberately at the heart of many of R's programming conventions.
> > 
> > Is there something here that I am missing, or is this yet 
> > another example of Frank Harrell's "instant brain surgeon" 
> commentary?
> > 
> > -- Bert Gunter
> > Genentech Non-Clinical Statistics
> > South San Francisco, CA
> >  
> > "The business of the statistician is to catalyze the 
> > scientific learning process."  - George E. P. Box
> >  
> >  
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vadim 
> > > Ogranovich
> > > Sent: Thursday, May 19, 2005 10:40 AM
> > > To: Thomas Lumley; Rod Montgomery
> > > Cc: r-help at stat.math.ethz.ch
> > > Subject: RE: [R] R annoyances
> > > 
> > > I think the flaw in this reasoning is that programmers are not 
> > > considered users. IMO, making a better language is beneficial for 
> > > users.
> > > 
> > > I am now watching how a new colleague of mine, a very good C++ 
> > > programmer turning into a data miner, is struggling w/ many 
> > > "irregularities" of R.
> > > 
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > Thomas Lumley
> > > > Sent: Thursday, May 19, 2005 9:39 AM
> > > > To: Rod Montgomery
> > > > Cc: r-help at stat.math.ethz.ch
> > > > Subject: Re: [R] R annoyances
> > > > 
> > > > On Thu, 19 May 2005, Rod Montgomery wrote:
> > > > > Thomas Lumley wrote:
> > > > >> This one is actually a FAQ,
> > > > >>         mtx[,1,drop=FALSE]
> > > > >> 
> > > > >>     -thomas
> > > > >> 
> > > > > I wonder whether there is, or should be, a way to set FALSE
> > > > as the default?
> > > > >
> > > > 
> > > > There shouldn't be (apart from editing the code), because 
> > you really 
> > > > don't want something this basic to be unpredictable.
> > > > 
> > > > There have been discussions at several times about whether 
> > > > drop=FALSE or drop=TRUE should be the default. The decision has 
> > > > always been that programmers can cope either way, but 
> that users 
> > > > probably don't expect mtx[,1] to be a vector, and that they 
> > > > definitely don't expect mtx[1,1] to be a matrix.
> > > > 
> > > >  	-thomas
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > >
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ogabbrie at tin.it  Thu May 19 22:53:40 2005
From: ogabbrie at tin.it (simone gabbriellini)
Date: Thu, 19 May 2005 22:53:40 +0200
Subject: [R] laten class analysis
Message-ID: <434bef91a6e4d934c654bf7c2ab07775@tin.it>

Dear List,

just a little question,
I am interested in Latent Class Analysis and
I guess if there is a package for this pourpose

thank for you help,
Simone



From David.Brahm at geodecapital.com  Thu May 19 22:55:42 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Thu, 19 May 2005 16:55:42 -0400
Subject: [R] Larger X11 fonts under R-2.1.0
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBCE@MSGBOSCLF2WIN.DMN1.FMR.COM>

Thanks very much to Prof Brian Ripley <ripley at stats.ox.ac.uk> for
the quick and illuminating reply:
> We have corrected a bug: you now get the size you ask for...
> DId you think to actually measure the sizes?  Might be interesting
> (although you may need to measure the window too).

Sticking a ruler up to my screen, I measure the "12pt" font at 13.5
pt (where 1 pt = 1/72 inch), and the "10pt" at 9.5 pt.  My window as
a whole is about 1.2x as large as I asked for, though.  So the "10pt"
font is a little small, but the sizes are roughly right (in R-2.1.0).

-- David Brahm (brahm at alum.mit.edu)



From t.muhlhofer at lse.ac.uk  Thu May 19 22:56:53 2005
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Thu, 19 May 2005 21:56:53 +0100
Subject: [R] Simultaneous estimation of mean and garch eq'n
Message-ID: <428CFD95.4050603@lse.ac.uk>

Is it possible to simultaneously estimate mean and GARCH parameters in R?

In other words, I would like to estimate the normal regression equation

Y = b X + u

and simultaneously do a garch process on the u's to correct the standard 
errors.

I was thinking maybe something with systemfit(), but I can't quite come 
up with it.

Thanks,
	Tobias
--



From f.harrell at vanderbilt.edu  Thu May 19 23:22:30 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 19 May 2005 16:22:30 -0500
Subject: [R] logistic regression:  differential importance of regressors
In-Reply-To: <m24qczm2m6.fsf@viz.local>
References: <m24qczm2m6.fsf@viz.local>
Message-ID: <428D0396.3060505@vanderbilt.edu>

Greg Trafton wrote:
> Hi, All.  I have a logistic regression model that I have run.  The
> question came up:  which of these regressors is more important than
> another?
> 
> (I'm using Design)
> 
> Logistic Regression Model
> 
> lrm(formula = iconicgesture ~ ST + SSP + magnitude + Condition + 
>     Expertise, data = d)
> 
>           Coef    S.E.   Wald Z P     
> Intercept -3.2688 0.2854 -11.45 0.0000
> ST         2.0871 0.2730   7.64 0.0000
> SSP        0.7454 0.3031   2.46 0.0139
> magnitude -0.9905 0.6284  -1.58 0.1150
> Condition  0.9506 0.2932   3.24 0.0012
> Expertise  0.8508 0.2654   3.21 0.0013
> 
> The real question is that, since both ST and SSP load significantly
> into the model, how do I show that ST has a bigger/smaller/similar
> effect than SSP?
> 
> thanks in advance!
> greg
> 

One thing you can do is to compute what proportion of the total 
likelihood ratio chi-square statistic is due to each variable by 
removing one at a time and looking at difference in Model L.R. (assuming 
both have same observations missing).  Note that you are making heavy 
linearity assumptions.

You can also use the bootstrap to get a confidence interval on the rank 
of the chi-square statistic a variable has among all competing chi-squares.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Ted.Harding at nessie.mcc.ac.uk  Thu May 19 23:52:05 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 19 May 2005 22:52:05 +0100 (BST)
Subject: [R] Help with R
In-Reply-To: <971536df05050507365e9f48a2@mail.gmail.com>
Message-ID: <XFMail.050519225205.Ted.Harding@nessie.mcc.ac.uk>

Hi Gabor,

On 05-May-05 Gabor Grothendieck wrote:
> On 5/5/05, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
>> [...]
>> However, while representing the raw data in such a form is
>> well supported by R, it seems to me that extracting data
>> in a way adapted to different analyses requires users to
>> create their own methods, using the list-access primitives .
>> 
>> For example, to study the changes in the distribution of
>> lengths of specimens in relation to Position and Date
>> (which was one of the important issues in that investigation),
>> I don't think there are any "list processing" functions
>> available in R which, given the list-based structure described
>> above, would allow a simple query of the form
>> 
>>  means( Length , ~ Position:Date , data=Cruise )
>> 
>> It's quite feasible to write one's own; but I think Peter's
>> hope (expressed in excerpt above) looks like a first call
>> for thinking about general methods for this sort of thing.
>> 
> 
> The Green Book defines a recursive apply function, rapply,
> that provides a general means of traversing that
> sort of structure.
> [...]
>  R. A. Becker, J. M. Chambers, and A .R. Wilks, The New S Language:
>    A Programming Environment for Data Analysis and Statistics.
>    Pacific Grove, CA: Wadsworth, 1988. Defines S Version 2, which
>    forms the basis of the currently used S Versions 3 and 4, as well
>    as R. (Sometimes called the "Blue Book.")

Thanks for your suggestions and comments, Gabor.

However, I now have the Green Book. There is some mention of
'rapply' (pp. 174-5, 371 where the function is used in the
definition of a function, and 430 where it is stated that "The
S and C computations for rapply follow the style of the example
here", the example in question being C code for "a simplified
version of the S function lapply"). However, there is no code
anywhere for rapply itself!

While the above possibly amount to enough hints for a good R
programmer to work out an implementation for rapply, this is at
present a level or two above my skills.

The S-plus code for rapply can be found by listing "rapply".
There are two things wrong with doing this. One is that this
code involves at least two functions which do not seem to
be available in R -- new.frame() and the function which
occurs in .Call(2s_tree_apply", .... ). One would need to
infer what these do and implement them in R.

The other is that I'm not at all keen on the idea of pinching
code from a proprietary product (S-plus).

So I'm wondering if there is any R code that implements the
equivalent of 'rapply' or could readily be extended to do so.

With thanks, and best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 19-May-05                                       Time: 22:51:48
------------------------------ XFMail ------------------------------



From bef at northwestern.edu  Fri May 20 00:20:49 2005
From: bef at northwestern.edu (Bruce Foster)
Date: Thu, 19 May 2005 17:20:49 -0500
Subject: [R] R 2.1.0 RH Linux Built from Source Segmentation Fault
Message-ID: <p06210206beb2b8dd5690@[129.105.110.38]>

Background:

I administer a cluster of RedHat EWS 3U4 Linux workstations at a university.

I built R 2.1.0 from source:

./configure \
         --prefix=/sscc/opt/R-2.1.0 \
         --with-blas=no \
         2>&1 \
         | tee NUInstall.configure


R is now configured for i686-pc-linux-gnu

   Source directory:          .
   Installation directory:    /sscc/opt/R-2.1.0

   C compiler:                gcc  -g -O2
   C++ compiler:              g++  -g -O2
   Fortran compiler:          g77  -g -O2

   Interfaces supported:      X11, tcltk
   External libraries:        readline
   Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
   Options enabled:           R profiling

   Recommended packages:      yes

configure: WARNING: you cannot build info or html versions of the R manuals

The machines are AMD Athlon MP 2400+ with 2 GB RAM, dual CPUs, and 
lots of free disk space.


I've got a user running Monte Carlo codes that fail with segmentation 
faults on a frequent basis. The jobs run for a long time (up to a 
day) before failure.

If a failed job is rerun, chances are high that it will run to completion.

I'm at a loss about approaching this problem. R (as it is here) 
doesn't seem to give much of a hint as to where things are when it 
crashes.

I'm looking for some guidance to diagnose this problem so we can 
focus on a solution.

Thanks!



Here's the annotated output of a failed job. The source file 
bayes_book_r_functions.R comes from Peter Rossi:

http://gsbwww.uchicago.edu/fac/peter.rossi/research/bayes%20book/R%20functions/

The second source file is included below.


R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.1.0  (2005-04-18), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

   Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

>  # program name: mnl.R
>  # my hierarchical bayes logit model using random walk algorithm
>  #
>  # nphy= number of physicians in the sample
>  # nvar = no. of variables in X
>  # nalt = no. of alternatives
>  # nobs = no. of observations
>  # yrows (nphy x 1 matrix) contains no. of observations by each physician
>  # X (nobs*nalt x nvar) contains xvalues for each of j alt on each 
>of n occasions
>  # Y (nobs x 1) contains chosen alternative
>  #
>
>  source("bayes_book_r_functions.R")
>  nalt=5
>
>  X=read.table("x300_new.txt",header=TRUE)
>  X=as.matrix(X)
>  X=cbind(X[,1:4],X[,10:11])
>  nvar=ncol(X)
>  Y=read.table("y300_new.txt",header=TRUE)
>  Y=as.matrix(Y)
>  yrows=read.table("yrows300_new.txt",header=TRUE)
>  nphy=nrow(yrows)
>  nobs=sum(yrows)/nalt
>  if (nrow(X)!=nobs*nalt){print("data dimensions wrong")}
>
>  betastore=NULL
>  delta=diag(nvar)
>  z=read.table("z300.txt",header=TRUE)
>  z=as.matrix(z)
>  k=ncol(z)
>  A1=.1*diag(k)
>  nu1=3+nalt
>  V=diag(nvar)*nu1
>
>  rowx1=rep(0,nphy+1)
>  rowx2=rep(0,nphy)
>  rowy1=rep(0,nphy+1)
>  rowy2=rep(0,nphy)
>  rowx1[1]=1
>  rowy1[1]=1
>  for (i in 1:nphy){
+ # for each physician i draw the relevant X and Y obs using yrows
+ #
+ rowx2[i]=rowx1[i]-1+yrows[i,1]
+ rowy2[i]=rowy1[i]-1+yrows[i,1]/nalt
+ rowx1[i+1]=rowx2[i]+1
+ rowy1[i+1]=rowy2[i]+1
+ }
>
>  R=100000
>  keep=10
>  thetas=matrix(rep(0,(R/keep+1)*nvar*k),ncol=nvar*k)
>  theta=matrix(rep(0,nvar*k),nrow=k)
>  thetabar=theta
>  source("rmnlRwMetrop1.R")
>  scale=.5
>  beta=matrix(rep(0,nphy*nvar),byrow=TRUE,ncol=nvar)
>  accept=rep(0,nphy)
>  accepts=rep(0,R/keep)
>  parameters=list(R=1,keep=1,s=scale)
>  for (j in 1:R)
+ {
+ itime=proc.time()[3]
+ for (i in 1:nphy){
+       # for each physician i draw a beta[i] using MH algorithm
+       #
+       Data=list(m=nalt,X=X[rowx1[i]:rowx2[i],],y=Y[rowy1[i]:rowy2[i],])
+       bbar=t(theta)%*%z[i,]
+       prior=list(A=delta,betabar=bbar)
+       a=rmnlRwMetrop1(Data,prior,Mcmc=parameters,beta[i,])
+       beta[i,]=a$betadraw
+       accept[i]=a$acceptr
+       }
+ mregout=rmultireg(beta,z,thetabar,A1,nu1,V)
+ delta=mregout$Sigma
+ theta=mregout$B
+ if (j%%keep==0){
+ thetas[j/keep,]=as.vector(theta)
+ betastore[j/keep]=list(beta)
+ accepts[j/keep]=mean(accept)
+ ftime=proc.time()[3]
+ cat('Time taken by iteration: ',j, ' = ',round((ftime-itime)/60,2),'\n')
+ }
+ if (j%%10000 == 0)
+ {cat('Mean theta at iteration: ',j,' = 
',apply(thetas[1:j/keep,],2,mean),'\n')
+ cat('Mean sd of theta at iteration: ',j,' = 
',apply(thetas[1:j/keep,],2,sd),'\n')
+ }
+ }
Time taken by iteration:  10  =  0.01
Time taken by iteration:  20  =  0.01
Time taken by iteration:  30  =  0.01
Time taken by iteration:  40  =  0.01
Time taken by iteration:  50  =  0.01

   ... many lines deleted

Time taken by iteration:  92800  =  0.01
Time taken by iteration:  92810  =  0.01
Time taken by iteration:  92820  =  0.01
Time taken by iteration:  92830  =  0.01
Time taken by iteration:  92840  =  0.01


That's it!

The PBS output is:

==============Original message text===============
    From: "seldon.it.northwestern.edu" <root at seldon.it.northwestern.edu>
    Date: Wed, 18 May 2005 4:47:19 pm CDT
    Subject: PBS JOB 1534.seldon

PBS Job Id: 1534.seldon
Job Name:   mnl300_z
Execution terminated
Exit_status=139
resources_used.cpupercent=98
resources_used.cput=00:25:02
resources_used.mem=233536kb
resources_used.ncpus=1
resources_used.vmem=250252kb
resources_used.walltime=00:25:13

===========End of original message text===========

The second source file contains:

rmnlRwMetrop1=function(Data,Prior,Mcmc,beta0)
{
#
# purpose:
#   draw from posterior for MNL using Independence Metropolis
#
# Arguments:
#   Data - list of m,X,y
#     m is number of alternatives
#     X is nobs*m x nvar matrix
#     y is nobs vector of values from 1 to m
#   Prior - list of A, betabar
#     A is nvar x nvar prior preci matrix
#     betabar is nvar x 1 prior mean
#   Mcmc
#     R is number of draws
#     keep is thinning parameter
#     s is scaling parameter for random walk
#   beta0 is initial beta
#
# Output:
#   list of betadraws
#
# Model:   Pr(y=j) = exp(x_j'beta)/sum(exp(x_k'beta)
#
# Prior:   beta ~ N(betabar,A^-1)
#
# check arguments
#
X=Data$X
y=Data$y
m=Data$m
nvar=ncol(X)
nobs=length(y)
# check for Prior
#
if(missing(Prior))
    { betabar=c(rep(0,nvar)); A=diag(rep(.01,nvar))}
else
    {
     if(is.null(Prior$betabar)) {betabar=c(rep(0,nvar))}
        else {betabar=Prior$betabar}
     if(is.null(Prior$A)) {A=matrix(rep(.01,nvar*nvar),ncol=nvar)}
        else {A=Prior$A}
    }
R=Mcmc$R
keep=Mcmc$keep
s=Mcmc$s
# Check beta0 argument
#
if (missing(beta0)) {beta0=c(rep(0,nvar))}
#
betadraw=matrix(double(floor(R/keep)*nvar),ncol=nvar)
#
# compute required quantities for indep candidates
#
oldbeta=beta0
mhess=diag(nvar)
candcov=chol2inv(chol(mhess))
root=s*chol(candcov)
priorcov=chol2inv(chol(A))
rootp=chol(priorcov)
rootpi=backsolve(rootp,diag(nvar))

#
#       start main iteration loop
#
oldlpost=llmnl(y,X,beta0)+lmvn(beta0,betabar,rootpi)
naccept=0

for (rep in 1:R)
{
    betac=oldbeta+t(root)%*%rnorm(nvar)
    clpost=llmnl(y,X,betac)+lmvn(betac,betabar,rootpi)
    ldiff=clpost-oldlpost
    alpha=min(1,exp(ldiff))
    if(alpha < 1) {unif=runif(1)} else {unif=0}
    if (unif <= alpha)
       { beta0=betac
         oldlpost=clpost
         naccept=naccept+1}
    oldbeta=beta0

   if(rep%%keep == 0)
     {mkeep=rep/keep; betadraw[mkeep,]=beta0}
}
list(betadraw=betadraw,acceptr=naccept/R)
}



From ccleland at optonline.net  Fri May 20 00:33:28 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 19 May 2005 18:33:28 -0400
Subject: [R] laten class analysis
In-Reply-To: <434bef91a6e4d934c654bf7c2ab07775@tin.it>
References: <434bef91a6e4d934c654bf7c2ab07775@tin.it>
Message-ID: <428D1438.8080901@optonline.net>

help.search("latent class") shows lca() in the e1071 package.

simone gabbriellini wrote:
> Dear List,
> 
> just a little question,
> I am interested in Latent Class Analysis and
> I guess if there is a package for this pourpose
> 
> thank for you help,
> Simone
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From edd at debian.org  Fri May 20 01:14:43 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 19 May 2005 23:14:43 +0000 (UTC)
Subject: [R] Simultaneous estimation of mean and garch eq'n
References: <428CFD95.4050603@lse.ac.uk>
Message-ID: <loom.20050520T011104-209@post.gmane.org>

Tobias Muhlhofer <t.muhlhofer <at> lse.ac.uk> writes:
> Is it possible to simultaneously estimate mean and GARCH parameters in R?
> 
> In other words, I would like to estimate the normal regression equation
> 
> Y = b X + u
> 
> and simultaneously do a garch process on the u's to correct the standard 
> errors.
> 
> I was thinking maybe something with systemfit(), but I can't quite come 
> up with it.

It is but you have to write out the loglikelihood (and possibly its gradient)
which you could then maximise with optim() and friends.  Adrian's GARCH(1,1)
is hard-coded with an analytic gradient -- the convenience of having this 
powerful routine pre-made and comes at the price of its lack of flexibility.

Diethelm's fSeries from Rmetrics can estimate Garch models by calling Ox. That
may work for you too.

Hope this helps,  Dirk



From joel3000 at gmail.com  Fri May 20 02:20:11 2005
From: joel3000 at gmail.com (Joel Bremson)
Date: Thu, 19 May 2005 17:20:11 -0700
Subject: [R] using src/Makevars file
Message-ID: <1253d67a0505191720130b8126@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050519/7783a272/attachment.pl

From andy_liaw at merck.com  Fri May 20 02:58:51 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 19 May 2005 20:58:51 -0400
Subject: [R] install R packages
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E85E@usctmx1106.merck.com>

> From: Li, Jia
> 
> Dear All,
>  
> When I tried to install R packages I found this error:
> > library(survival)
> Loading required package: splines
> > install.packages("splines")
> --- Please select a CRAN mirror for use in this session ---
> Warning message:
> package splines is in use and will not be installed 
> 
> But I just opened R, and have used anything yet, how come did 
> it say "package splines is in use"?

You didn't just open R.  You loaded the survival package, which depends on
the splines package, so that's loaded as well.  Why would you want to
install that package if you already have it?  

The error you get tells you that you can't install over a package that's in
use.  You can see what's loaded by typing search().

Andy
  
> Thanks
>  
> Jia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Fri May 20 04:12:24 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 19 May 2005 22:12:24 -0400
Subject: [R] Help with R
In-Reply-To: <XFMail.050519225205.Ted.Harding@nessie.mcc.ac.uk>
References: <971536df05050507365e9f48a2@mail.gmail.com>
	<XFMail.050519225205.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <971536df0505191912d10d54@mail.gmail.com>

On 5/19/05, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
> Hi Gabor,
> 
> On 05-May-05 Gabor Grothendieck wrote:
> > On 5/5/05, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
> >> [...]
> >> However, while representing the raw data in such a form is
> >> well supported by R, it seems to me that extracting data
> >> in a way adapted to different analyses requires users to
> >> create their own methods, using the list-access primitives .
> >>
> >> For example, to study the changes in the distribution of
> >> lengths of specimens in relation to Position and Date
> >> (which was one of the important issues in that investigation),
> >> I don't think there are any "list processing" functions
> >> available in R which, given the list-based structure described
> >> above, would allow a simple query of the form
> >>
> >>  means( Length , ~ Position:Date , data=Cruise )
> >>
> >> It's quite feasible to write one's own; but I think Peter's
> >> hope (expressed in excerpt above) looks like a first call
> >> for thinking about general methods for this sort of thing.
> >>
> >
> > The Green Book defines a recursive apply function, rapply,
> > that provides a general means of traversing that
> > sort of structure.
> > [...]
> >  R. A. Becker, J. M. Chambers, and A .R. Wilks, The New S Language:
> >    A Programming Environment for Data Analysis and Statistics.
> >    Pacific Grove, CA: Wadsworth, 1988. Defines S Version 2, which
> >    forms the basis of the currently used S Versions 3 and 4, as well
> >    as R. (Sometimes called the "Blue Book.")
> 
> Thanks for your suggestions and comments, Gabor.
> 
> However, I now have the Green Book. There is some mention of
> 'rapply' (pp. 174-5, 371 where the function is used in the
> definition of a function, and 430 where it is stated that "The
> S and C computations for rapply follow the style of the example
> here", the example in question being C code for "a simplified
> version of the S function lapply"). However, there is no code
> anywhere for rapply itself!
> 
> While the above possibly amount to enough hints for a good R
> programmer to work out an implementation for rapply, this is at
> present a level or two above my skills.
> 
> The S-plus code for rapply can be found by listing "rapply".
> There are two things wrong with doing this. One is that this
> code involves at least two functions which do not seem to
> be available in R -- new.frame() and the function which
> occurs in .Call(2s_tree_apply", .... ). One would need to
> infer what these do and implement them in R.
> 
> The other is that I'm not at all keen on the idea of pinching
> code from a proprietary product (S-plus).
> 
> So I'm wondering if there is any R code that implements the
> equivalent of 'rapply' or could readily be extended to do so.

Search r-help for treeapply.   Also r-devel for a link to the codetools 
package which has some recursive code in it.



From vincent at 7d4.com  Fri May 20 07:57:08 2005
From: vincent at 7d4.com (vincent)
Date: Fri, 20 May 2005 07:57:08 +0200
Subject: [R] fast matrix update
In-Reply-To: <428B13D0.2040103@stats.uwo.ca>
References: <428B0E92.9090805@7d4.com> <428B13D0.2040103@stats.uwo.ca>
Message-ID: <428D7C34.4030104@7d4.com>

Thank you Duncan for the answer.

I was thinking on the same kind of tricks.
I'll probably subscribe nextly to R-devel.

Thanks
Vincent



From petr.pikal at precheza.cz  Fri May 20 08:33:45 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 20 May 2005 08:33:45 +0200
Subject: [R] Reversing axis in a log plot
In-Reply-To: <fad9e83f050519072972b1f411@mail.gmail.com>
Message-ID: <428DA0E9.29444.31FD6C@localhost>

Hi Christian

I did not see any response to your question yet, and my will not be 
very helpful. The error message comes from axis building, plot 
itself seems to be OK. If you choose slightly bigger value for ylim 
it will go through without any complain.

plot(x,y, log = "y", ylim = c(30,1.2))

Seems to me as a bug in axis. If you do not get any other response 
try to send it to r-bugs.

Cheers
Petr



On 19 May 2005 at 15:29, Christian Marquardt wrote:

> Hello,
> 
> apologies if I'm overlooking the obvious... I would like to revert a
> logarithmic axis with R 2.1.0 on Linux, e.g. for using pressure as a
> vertical coordinate. Say we have
> 
>   x = seq(1,3, by = 0.01)
>   y = exp(x)
> 
> Plotting and reversing linear axis is fine
> 
>    plot(x,y)
>    plot(x,y, ylim = c(30,1))
> 
> as is a usual log-plot:
> 
>   plot(x,y, log = "y", ylim = c(1,30))
> 
> However,
> 
>   plot(x,y, log = "y", ylim = c(30,1))
> 
> fails with
> 
>   Error in axis(2, ...) : log - axis(), 'at' creation, _SMALL_ range:
> invalid {xy}axp or par;
>          axp[0]= 10, usr[0:1]=(34.3721,0.872801)
>   In addition: Warning message:
>   CreateAtVector "log"(from axis()): usr[0] = 34.3721 > 0.872801 =
>   usr[1] !
> 
> What am I doing wrong here?
> 
> Thanks a lot,
> 
>   Christian.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ligges at statistik.uni-dortmund.de  Fri May 20 08:38:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 May 2005 08:38:40 +0200
Subject: [R] R annoyances
In-Reply-To: <20050519155523.UUMW27508.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <20050519155523.UUMW27508.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <428D85F0.1000708@statistik.uni-dortmund.de>

Dear John,

I have not expected to cause that many traffic and largish discussion.

What I tried to point out is that:
- a "programmer" should know that one has to use TRUE / FALSE in code in 
order to make it work generaly which is also checked by R CMD check.
- a "user" simply typing some lines in order to look at the data can 
shortly write T or F instead.

where "programmer" and "user" are not well defined and probably 
undistinguishable according to Chambers (1998).
I'd call people using [..., drop=FALSE] "programmer" here, since the 
code is probably used inside functions.

S-PLUS compatibility (T/F) has to be considered as well.

All possible changes to T/F (both removing the meaning of TRUE/FALSE in 
a clean session and making them reserved words) would break code of lots 
of users. With a common amount of statistical uncertainty I think it 
might be too late for changes ...

Best,
Uwe



John Fox wrote:
> Dear Uwe,
> 
> I've often wondered why T and F aren't reserved words in R as TRUE and FALSE
> are. Perhaps there's some use of T and F as variables, but that seems
> ill-advised.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
>>Sent: Thursday, May 19, 2005 10:08 AM
>>To: Chalasani, Prasad
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] R annoyances
>>
>>Chalasani, Prasad wrote:
>>
>>
>>>Thanks all for pointing out that I can use 
>>>	mtx[,1,drop=F]
>>
>>
>>Which, for example, won't work for
>>  F <- 10.25
>>
>>---> drop=FALSE  !
>>           ^^^^^
>>
>>Uwe Ligges
>>
>>
>>
>>
>>>
>>>-----Original Message-----
>>>From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
>>>Sent: Thursday, May 19, 2005 10:49 AM
>>>To: Chalasani, Prasad
>>>Cc: r-help at stat.math.ethz.ch
>>>Subject: Re: [R] R annoyances
>>>
>>>
>>>Chalasani, Prasad wrote:
>>>
>>>
>>>>Dear R Folks,
>>>>I'm a big fan of R, but there are a couple of things that 
>>
>>repeatedly 
>>
>>>>annoy me, and I wondered if anyone has neat ways to deal with them.
>>>>
>>>>(a) When using "apply" row-wise to a matrix, it returns
>>>>   the results column-wise, and to preserve the original
>>>>   orientation, I've to do a transpose. E.g. I've to keep
>>>>   doing a transpose, which I consider to be quite annoying.
>>>>	
>>>>   transformed.mtx <- t(apply( mtx, 1, exp))
>>>
>>>
>>>I'd rather type
>>>
>>>   exp(mtx)
>>>
>>>
>>>
>>>
>>>
>>>>(b) When extracting 2 or more columns of a matrix, 
>>>>   R returns the result as a matrix, BUT when extracting
>>>>   just one column, it returns a vector/array, rather than
>>>>   a matrix, so I've to keep doing as.matrix, which is annoying.
>>>>
>>>>	sub.mtx <- as.matrix(mtx[,1])
>>>>
>>>>	Of course I could write a suitable function
>>>>		cols <- function(mtx,range) as.matrix(mtx[, range])
>>>>	but then I lose the syntactic sugar of being able to say "[,1]".
>>>
>>>
>>>The docs suggest:
>>>
>>>   mtx[ , 1, drop = FALSE]
>>>
>>>
>>>Uwe Ligges
>>>
>>>
>>>
>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list 
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html



From x12347 at hotmail.com  Fri May 20 08:48:32 2005
From: x12347 at hotmail.com (Luka Kos)
Date: Fri, 20 May 2005 08:48:32 +0200
Subject: [R] survival probabilities from survival tree (rpart)
Message-ID: <BAY1-F29EFA01D5EF1A8BD3526BF84090@phx.gbl>

Hi!

How to compute probabilities of survival from rpart object?
Consider the following example:

library(rpart)
survtree <- rpart(Surv(time,status)~., data=aml)

This code will produce the following survival tree:

1) root 23 26.44184 1.0000000
  2) x=Maintained 11 12.61182 0.6848303 *
  3) x=Nonmaintained 12 10.81338 1.4426070 *

If I am not wrong the rightmost numbers are relative risks compared to whole 
population (e.g. root node). How to get survival probability function 
S(t|x)?

Regards,
Ludvik



From p.dalgaard at biostat.ku.dk  Fri May 20 09:04:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 May 2005 09:04:08 +0200
Subject: [R] R 2.1.0 RH Linux Built from Source Segmentation Fault
In-Reply-To: <p06210206beb2b8dd5690@[129.105.110.38]>
References: <p06210206beb2b8dd5690@[129.105.110.38]>
Message-ID: <x2fywiz8d3.fsf@turmalin.kubism.ku.dk>

Bruce Foster <bef at northwestern.edu> writes:

... 
> The machines are AMD Athlon MP 2400+ with 2 GB RAM, dual CPUs, and
> lots of free disk space.

Any per-user/per-process limits? Resource usage look suspiciously
close to 256M. If your install is allowing overcommitment of memory,
the OS can kill processes at unpredictable times.

> I've got a user running Monte Carlo codes that fail with segmentation
> faults on a frequent basis. The jobs run for a long time (up to a day)
> before failure.
> 
> If a failed job is rerun, chances are high that it will run to completion.
> 
> I'm at a loss about approaching this problem. R (as it is here)
> doesn't seem to give much of a hint as to where things are when it
> crashes.
> 
> I'm looking for some guidance to diagnose this problem so we can focus
> on a solution.

(A) Use set.seed(...) to get a fixed sequence of random numbers. If it
still fails unpredictably, my bet is that it is a resource problem.

(B) Once you have a case that fails predictably, run it under a
debugger and try to backtrack to the point of failure. There are
various debugging tricks that you can use, but just get there first
and show us a stack backtrace at the failure point (bt command in
gdb).

For more detailed guidance you should probably move the discussion to
the r-devel list.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From petr.pikal at precheza.cz  Fri May 20 09:10:53 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 20 May 2005 09:10:53 +0200
Subject: [R] Re: predict nlme syntax
Message-ID: <428DA99D.11525.537F4E@localhost>

Thank you. I somehow missed your answer and find it only after I 
went through archives. I had to have partial blindfoldness when 
searching in your book and in documentation for the answer.

Best regards
Petr Pikal 

On 10 May 2005 at 13:24, r-help at stat.math.ethz.ch wrote:

> Dear all
> 
> Please help me with correct syntax of predict.nlme.
> I would like to predict from nlme object for new data.
> I used predict(fit.nlme6, data=newdata) but I have always got 
> fitted values, no matter how I changed newdata.
> 
> I have
> 
> > summary(fit.nlme6)
> Nonlinear mixed-effects model fit by maximum likelihood
>   Model: konverze ~ SSfpl(tepl, A, B, xmid, scal) 
>  Data: limity.gr 
>        AIC      BIC    logLik
>   882.4939 907.6738 -433.2469
> 
> Random effects:
>  Formula: list(xmid ~ 1, scal ~ 1)
>  Level: spol.f
>  Structure: General positive-definite, Log-Cholesky 
> parametrization
>          StdDev    Corr 
> xmid     29.680114 xmid 
> scal      6.481679 0.249
> Residual  2.168191      
> 
> Fixed effects: list(A ~ 1, B ~ 1, xmid ~ 1, scal ~ 1) 
>         Value Std.Error  DF   t-value p-value
> A     36.1450  0.837050 154  43.18133       0
> B    101.0272  0.432074 154 233.81898       0
> xmid 735.3860  8.150964 154  90.22074       0
> scal  15.4453  2.201864 154   7.01466       0
>  Correlation: 
>      A      B      xmid  
> B    -0.088              
> xmid  0.057 -0.088       
> scal -0.089  0.469  0.036
> 
> Standardized Within-Group Residuals:
>           Min            Q1           Med            Q3           Max
> -3.7707629568 -0.3291628536  0.0005885683  0.4020944158  3.7911729382 
> 
> Number of Observations: 172
> Number of Groups: 15 
> 
> where **tepl** is independent variable and **spol.f** is grouping
> factor.
> 
> The newly constructed data frame newdata has the same structure 
> of spol.f levels as has the limity.gr data frame used for fitting.
> 
> > levels(limity.gr$spol.f)
>  [1] "1.8/3"   "4/3"     "6.3/3"   "10.8/3"  "1.8/7"   "1.8/12" 
> "1.8/30"  "6.3/30"  "10.8/30" "1.8/60"  "4/60"    "6.3/60"  "1.8/110"
> [14] "1.8/200" "1.8/300"
> 
> > levels(newdata$spol.f)
>  [1] "1.8/3"   "4/3"     "6.3/3"   "10.8/3"  "1.8/7"   "1.8/12" 
> "1.8/30"  "6.3/30"  "10.8/30" "1.8/60"  "4/60"    "6.3/60"  "1.8/110"
> [14] "1.8/200" "1.8/300" >
> 
> The only difference is in temperature.
> 
> Please advice how shall I change newdata to be able to use it in
> predict function.

> The argument's name is newdata, not data.

> Thank you.
> 
> Best regards
> 
> 

Petr Pikal
petr.pikal at precheza.cz



From navarre_sabine at yahoo.fr  Fri May 20 09:28:43 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Fri, 20 May 2005 09:28:43 +0200 (CEST)
Subject: [R] hello
Message-ID: <20050520072843.43370.qmail@web26604.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050520/e5e5c88f/attachment.pl

From ssk2031 at columbia.edu  Fri May 20 09:49:59 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Fri, 20 May 2005 03:49:59 -0400
Subject: [R] hello
In-Reply-To: <20050520072843.43370.qmail@web26604.mail.ukl.yahoo.com>
References: <20050520072843.43370.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <428D96A7.7030100@columbia.edu>


http://snipurl.com/f0xh

(leads you to packages 'ade4' and 'MASS')

-s.


Navarre Sabine wrote:
> I would like to donc an AFC (factoriel correspondance analysis) and I know that on Splus, the function to do that is afc(data). But on R??? is it acm?
> 
> That a lot! 
> Sabine
> 
> 		
> ---------------------------------
> 
> ils, photos et vid??os !
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.campbell at econ.bbk.ac.uk  Fri May 20 10:00:53 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Fri, 20 May 2005 09:00:53 +0100
Subject: [R] Simultaneous estimation of mean and garch eq'n
Message-ID: <s28da767.083@markets.econ.bbk.ac.uk>

This might be one of those situations in which you should say what what
you are trying to do rather than how you are trying to do it.  It is my
understanding that estimates of b are asymptotically well behaved in
this situation, at least for b<1.  If, however, you are trying to get
CI's for b in finite samples conditioned upon the errors being generated
by a GARCH process then this is a different issue.

HTH

Phineas

>>> Tobias Muhlhofer <t.muhlhofer at lse.ac.uk> 05/19/05 9:56 PM >>>
Is it possible to simultaneously estimate mean and GARCH parameters in
R?

In other words, I would like to estimate the normal regression equation

Y = b X + u

and simultaneously do a garch process on the u's to correct the standard

errors.

I was thinking maybe something with systemfit(), but I can't quite come 
up with it.

Thanks,
	Tobias
--

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From wl at eimb.ru  Fri May 20 10:41:06 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Fri, 20 May 2005 12:41:06 +0400
Subject: [R] Lattice: how to get default ylim?
In-Reply-To: <200505190911.38996.deepayan@stat.wisc.edu>
References: <244663517.20050519141653@eimb.ru>
	<20050519130050.GA8268@jessie.research.bell-labs.com>
	<200505190911.38996.deepayan@stat.wisc.edu>
Message-ID: <1558402140.20050520124106@eimb.ru>

Thank all very much for answers.
grid.text in the Deepayan's varian did the trick satisfactory.

However, at any rate it would be great to have access to the
information about viewports, because I would like text not to overlap
the graphics. Therefore its placement should be calculated with
respect to the values displayed, and it is necessary to know how many free space
available in different parts of the viewport.

Hopefully, in this particular case all my graphs had enough free space in the
upper left corner.

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From ogabbrie at tin.it  Fri May 20 10:50:47 2005
From: ogabbrie at tin.it (simone gabbriellini)
Date: Fri, 20 May 2005 10:50:47 +0200
Subject: [R] laten class analysis
In-Reply-To: <428D1438.8080901@optonline.net>
References: <434bef91a6e4d934c654bf7c2ab07775@tin.it>
	<428D1438.8080901@optonline.net>
Message-ID: <57b374877e4877bc98107b49c3d0063b@tin.it>

my help.search didn't give me any result :(
the one you suggest should be right what I need

thank you,
simone

Il giorno 20/mag/05, alle 00:33, Chuck Cleland ha scritto:

> help.search("latent class") shows lca() in the e1071 package.
>
> simone gabbriellini wrote:
>> Dear List,
>> just a little question,
>> I am interested in Latent Class Analysis and
>> I guess if there is a package for this pourpose
>> thank for you help,
>> Simone
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jtk at cmp.uea.ac.uk  Fri May 20 12:00:46 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Fri, 20 May 2005 11:00:46 +0100
Subject: [R] R annoyances
In-Reply-To: <20050519191054.BVKP26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <20050519172201.GI1178@jtkpc.cmp.uea.ac.uk>
	<20050519191054.BVKP26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <20050520100046.GA29990@jtkpc.cmp.uea.ac.uk>

On Thu, May 19, 2005 at 03:10:53PM -0400, John Fox wrote:

> Since you can use variables named c, q, or t in any event, I don't see why
> the existence of functions with these names is much of an impediment.

True, particularly since I'm not too likely to use these variables for (local)
functions, and variables of other types don't prevent functions from working.
(I thought this was a problem... I must be spoilt by recently having to read
too much Matlab code, where parentheses are used to both enclose subscripts and
parameter lists, thus rendering subscript expressions and function calls
syntactically indistinguishable.)

> The problem that I see with T and F is that allowing them to be redefined
> sets a trap for people. If R wants to discourage use of T and F for TRUE and
> FALSE, then why provide standard global variables by these names? On the
> other hand, if providing T and F is considered desirable (e.g., for S-PLUS
> compatibility), then why not make them reserved names?

Perhaps, it's a legacy code thing -- if there's both code relying on
T and F being aliases of TRUE and FALSE, and code using T or F as
variable names, then the situation cannot be resolved in either way
without breaking some code.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From bry at xdocs.dk  Fri May 20 11:33:11 2005
From: bry at xdocs.dk (bry@xdocs.dk)
Date: Fri, 20 May 2005 11:33:11 +0200
Subject: [R] Why does this give a syntax error?
Message-ID: <1116581591.428daed79aed9@horde.scannet.dk>

Hi

I'm generating the following in a file and getting a syntax error:





bryansAtHeaderLevel <-
c(0,1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,1,0,1,0,1,1,1,1,1,1,0,0,1,1,1,0,0,1,1,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,1,0,0,0,)

finnsAtHeaderLevel <-
c(0,1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,0,1,0,1,1,1,1,1,1,0,0,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,1,0,0,0,)

faqAtHeaderLevel <-
c(0,1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,1,0,1,0,1,1,1,1,1,1,0,0,1,1,1,0,0,1,1,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,0,1,0,0,0,)

bryansAtLineLevel <-
c(0,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,1,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,
 1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,1,1,1,1,0,0,0,1,0,0,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,)

finnsAtLineLevel <-
c(0,1,0,1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,1,1,0,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,
 1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,0,0,0,0,1,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,)

faqAtLineLevel <-
c(0,1,0,1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,
 1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,1,1,1,1,0,0,0,1,0,0,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,)


because of line breaks in the email this is not the exact output, there is no
break between c and (

the example is also in the attached text r.txt

anyway I'm being told that bryansAtLineLevel has a syntax error when I load and
run the script in R gui.
I can't really see why that would be wrong and the others wouldn't be. Any
suggestions?



From bry at xdocs.dk  Fri May 20 11:38:37 2005
From: bry at xdocs.dk (bry@xdocs.dk)
Date: Fri, 20 May 2005 11:38:37 +0200
Subject: [R] attached file with syntax error
Message-ID: <1116581917.428db01d97501@horde.scannet.dk>



oops, I forgot to attach the example txt file with the syntax error, so I will
do it here, can anyone see what the error is with bryansAtLineLevel?



-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: r.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050520/c55da2f4/r.txt

From pburns at pburns.seanet.com  Fri May 20 11:49:55 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 20 May 2005 10:49:55 +0100
Subject: [R] Simultaneous estimation of mean and garch eq'n
In-Reply-To: <428CFD95.4050603@lse.ac.uk>
References: <428CFD95.4050603@lse.ac.uk>
Message-ID: <428DB2C3.7010006@pburns.seanet.com>

It is my experience that location parameters are not very affected
by the garch parameters.  So doing a naive estimate of location,
followed by the garch estimate, followed by an estimate of location
accounting for heteroskedasticity is likely to be indistinguishable
from the estimates from the full likelihood.

If you compare garch with the naive estimate of location versus the
garch estimate with no estimate of location, you are probably not
going to see much difference.  That difference is likely to be smaller
than if you change the time period of estimation slightly.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Tobias Muhlhofer wrote:

> Is it possible to simultaneously estimate mean and GARCH parameters in R?
>
> In other words, I would like to estimate the normal regression equation
>
> Y = b X + u
>
> and simultaneously do a garch process on the u's to correct the 
> standard errors.
>
> I was thinking maybe something with systemfit(), but I can't quite 
> come up with it.
>
> Thanks,
>     Tobias
> -- 
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>



From maechler at stat.math.ethz.ch  Fri May 20 11:58:08 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 20 May 2005 11:58:08 +0200
Subject: [R] R annoyances
In-Reply-To: <Pine.A41.4.61b.0505190933480.327288@homer12.u.washington.edu>
References: <AF003EF88447964B88823C3F50A6AB750ACFD46B@gsnbp25es.firmwide.corp.gs.com>
	<Pine.A41.4.61b.0505190740350.13738@homer09.u.washington.edu>
	<428CB24A.2070603@sprintmail.com>
	<Pine.A41.4.61b.0505190933480.327288@homer12.u.washington.edu>
Message-ID: <17037.46256.97686.143352@stat.math.ethz.ch>

>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>     on Thu, 19 May 2005 09:39:13 -0700 (PDT) writes:

    TL> On Thu, 19 May 2005, Rod Montgomery wrote:
    >> Thomas Lumley wrote:
    >>> This one is actually a FAQ, mtx[,1,drop=FALSE]
    >>> 
    >>> -thomas
    >>> 
    >> I wonder whether there is, or should be, a way to set
    >> FALSE as the default?

    TL> There shouldn't be (apart from editing the code),
    TL> because you really don't want something this basic to be
    TL> unpredictable.

    TL> There have been discussions at several times about
    TL> whether drop=FALSE or drop=TRUE should be the
    TL> default. The decision has always been that programmers
    TL> can cope either way, but that users probably don't
    TL> expect mtx[,1] to be a vector, and that they definitely
    TL> don't expect mtx[1,1] to be a matrix.

Yes, and (as Uwe has already mentioned),
the S language has now been ``defined'' in a few ways for 
many years, and the decision to make "drop=TRUE" the default
(for arrays at least) may have been sub-optimal --- and maybe
could have been changed 15 years ago,  but not anymore nowadays:
It is implicitly made use of in too many places of existing S code.

Those of you who are new to R:  
Please don't assume R is new just because you are new to it!

Martin



From r.hankin at noc.soton.ac.uk  Fri May 20 12:09:57 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 20 May 2005 11:09:57 +0100
Subject: [R] R annoyances
In-Reply-To: <20050520100046.GA29990@jtkpc.cmp.uea.ac.uk>
References: <20050519172201.GI1178@jtkpc.cmp.uea.ac.uk>
	<20050519191054.BVKP26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
	<20050520100046.GA29990@jtkpc.cmp.uea.ac.uk>
Message-ID: <303bce6835e500e41812afe596be251c@soc.soton.ac.uk>


On May 20, 2005, at 11:00 am, Jan T. Kim wrote:

> On Thu, May 19, 2005 at 03:10:53PM -0400, John Fox wrote:
>
>> Since you can use variables named c, q, or t in any event, I don't 
>> see why
>> the existence of functions with these names is much of an impediment.
>
> True, particularly since I'm not too likely to use these variables for 
> (local)
> functions, and variables of other types don't prevent functions from 
> working.
> (I thought this was a problem... I must be spoilt by recently having 
> to read
> too much Matlab code, where parentheses are used to both enclose 
> subscripts and
> parameter lists, thus rendering subscript expressions and function 
> calls
> syntactically indistinguishable.)


Heh, I'm a recovering Matlab  user too.  This is sooooooooooo true!

In Matlab:

f(10)    # function f() evaluated at 10
f(10)    # 10th element of vector f.  confusing!!

R uses round brackets in two unrelated ways:

  4*(1+2)  --- using "(" and ")" to signify grouping
f(8)  function f() evaluated at 8.

where there is no reason to use the same parenthesis symbol for both 
tasks.

IMO, the only system with consistent parenthesis use is Mathematica;

f[10]  #  function f[] evaluated at 10
8*(2+2)   # parenthesis to override  order of operations
f[[3]] # third element of list f

{} are used for sets.



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From andy_liaw at merck.com  Fri May 20 12:37:31 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 20 May 2005 06:37:31 -0400
Subject: [R] laten class analysis
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E85F@usctmx1106.merck.com>

You have to have the package installed for help.search() to find things in
it.  You could try

  RSiteSearch("latent class", restrict="function")

Andy

> From: Simone gabbriellini
> 
> my help.search didn't give me any result :(
> the one you suggest should be right what I need
> 
> thank you,
> simone
> 
> Il giorno 20/mag/05, alle 00:33, Chuck Cleland ha scritto:
> 
> > help.search("latent class") shows lca() in the e1071 package.
> >
> > simone gabbriellini wrote:
> >> Dear List,
> >> just a little question,
> >> I am interested in Latent Class Analysis and
> >> I guess if there is a package for this pourpose
> >> thank for you help,
> >> Simone
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >
> > -- 
> > Chuck Cleland, Ph.D.
> > NDRI, Inc.
> > 71 West 23rd Street, 8th floor
> > New York, NY 10010
> > tel: (212) 845-4495 (Tu, Th)
> > tel: (732) 452-1424 (M, W, F)
> > fax: (917) 438-0894
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From lecoutre at stat.ucl.ac.be  Fri May 20 12:34:56 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Fri, 20 May 2005 12:34:56 +0200
Subject: [R] attached file with syntax error
In-Reply-To: <1116581917.428db01d97501@horde.scannet.dk>
Message-ID: <000801c55d27$90bfbe00$6e8b6882@didacdom.stat.ucl.ac.be>


Well...

You just can't end vector declaration with a comma...

> x <- c(0,1,)
Error: syntax error
> x <- c(0,1)

Eric

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte   


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bry at xdocs.dk
> Sent: vendredi 20 mai 2005 11:39
> To: r-help at stat.math.ethz.ch
> Subject: [R] attached file with syntax error
> 
> 
> 
> 
> oops, I forgot to attach the example txt file with the syntax 
> error, so I will do it here, can anyone see what the error is 
> with bryansAtLineLevel?
> 
> 
> 
>



From ripley at stats.ox.ac.uk  Fri May 20 12:42:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 May 2005 11:42:39 +0100 (BST)
Subject: [R] using src/Makevars file
In-Reply-To: <1253d67a0505191720130b8126@mail.gmail.com>
References: <1253d67a0505191720130b8126@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505201138490.14977@gannet.stats>

Why do you expect FF to specify a Fortran compiler when F77 works on the 
command line?

I would expect F77 to work.

On Thu, 19 May 2005, Joel Bremson wrote:

> Hi all,
>
> Thanks to all who offered advice on using F95 in R.
>
> Now I'm trying to compile a test package using gfortran, Linux 2.4.21 and
> R 2.1.0.
>
> I was able to successfully compile and use a test F95 routine by setting my
> environment variables as follows in bash:
>
> export PATH=~/bin/:$PATH
> export F77=gfortran
> export LD_LIBRARY_PATH=~/bin/irun/lib
> export GFORTRAN_STDIN_UNIT=-1
>
> Now I'm trying to write a Makevars file for my test package
> and not quite sure how to do it. I've tried
>
> FF=gfortran
> GFORTRAN_STDIN_UNIT=-1
> FLIBS=/home/jbremson/bin/irun/lib/libgfortran.a
>
> but when running an R CMD check on the package I see that
> it's still using g77 to compile:
>
> -----output----
> ...
> ** libs
> g77 -fPIC -g -O2 -c estimate.f -o estimate.o
> estimate.f: In subroutine `estimate':
> estimate.f:20:
> forall (i = 1:nxrows) beta(i) = i * 2
> ^
> Invalid declaration of or reference to symbol `forall' at (^) [initially
> seen at (^)]
> ...
> ----end output ----
>
> The code compiles using:
> gfortran -c estimate.f
>
> I can run my code if I build the .so by hand and then dyn.load it.
>
> Here is my F95 test code:
>
> subroutine estimate(beta, yij, nij, nxrows, nxcols,xmat,
> & irequest, ierror)
>
> integer nxrows, nxcols, yij, nij, irequest, ierror
> double precision beta(nxrows), xmat(nxrows,nxcols)
> integer i
>
> i = 0
> c fortran 95 version
> forall (i = 1:nxrows) beta(i) = i * 2
>
> ierror = 0
> end
>
>
> Regards,
>
> Joel Bremson
> UC Davis
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ravi.Vishnu at outokumpu.com  Fri May 20 13:05:49 2005
From: Ravi.Vishnu at outokumpu.com (Ravi.Vishnu@outokumpu.com)
Date: Fri, 20 May 2005 13:05:49 +0200
Subject: [R] legend as a subtitle
Message-ID: <OF2FA3C9DD.99D34F79-ONC1257007.003C74BD-C1257007.003CF4BC@outokumpu.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050520/20217b56/attachment.pl

From kka at imm.dtu.dk  Fri May 20 13:11:44 2005
From: kka at imm.dtu.dk (Klaus Kaae Andersen)
Date: Fri, 20 May 2005 13:11:44 +0200 (METDST)
Subject: [R] Degradation model
Message-ID: <Pine.GSO.4.21.0505201309540.15371-100000@sol2.imm.dtu.dk>

Dear list,

I have a degradation model:

dX/dt = * I(X2)*( k1*X(t) )/( X(t)+k2 ) 

where X(t) is concentration at time t, and k1 and k2 are parameters
that I want to estimate. I(X) is a known inhibitor function.

My questions is whether this is implemented or easily computed in any
R package. I have searched the archives but without luck.


Any help or comments on this would be appreciated,
Klaus Andersen



From csad4933 at uibk.ac.at  Fri May 20 13:20:27 2005
From: csad4933 at uibk.ac.at (Sebastian Schoenherr)
Date: Fri, 20 May 2005 13:20:27 +0200
Subject: [R] Plot Problem
Message-ID: <1116588027.428dc7fb53704@web-mail2.uibk.ac.at>

Hi folks,

I try to plot a variable which contains string-variables. it works, but the
problem is that there are a lot of values at the x axis (up to 24)
SO i have to scale or rotate the label at the x axis.
I tried this with the text() function. It doesn't work correctly.

Is there a simple way to rotate the labels?

Best regards,
Sebastian



From wl at eimb.ru  Fri May 20 14:05:21 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Fri, 20 May 2005 16:05:21 +0400
Subject: [R] Lattice: it seems, a bug in draw.key function
Message-ID: <1803865630.20050520160521@eimb.ru>

Dear r-help,

  Now I am drawing graphs with xyplot function.
  
  In order to place a legend under the plots I use the key argument in
the xyplot function.

  One of the 'key' components is 'divide', which defines a number of
  points on the each line of the legend. The default is 3 points.
  I would like a single point, so I set divide = 1.

  Call to xyplot didn't produce any points at all.

  The body of draw.key function contains the following
  
pointsGrob(x=(1:key$divide-1)/(key$divide-1), y=rep(0.5,key$divide),
            [skip]
          )
          
When key$divide=1, x is NaN...

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From B.Rowlingson at lancaster.ac.uk  Fri May 20 13:55:25 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 20 May 2005 12:55:25 +0100
Subject: [R] attached file with syntax error
In-Reply-To: <000801c55d27$90bfbe00$6e8b6882@didacdom.stat.ucl.ac.be>
References: <000801c55d27$90bfbe00$6e8b6882@didacdom.stat.ucl.ac.be>
Message-ID: <428DD02D.5040108@lancaster.ac.uk>

Eric Lecoutre wrote:
> Well...
> 
> You just can't end vector declaration with a comma...
> 
> 
>>x <- c(0,1,)
> 
> Error: syntax error
> 
>>x <- c(0,1)
> 

  You may not be able to, but I can!

 > x=c(0,1,)
 > x
[1] 0 1

( R 2.0.1 and R 1.8.1 )

  I suspect the error is more to do with the length of the line 
overflowing a buffer, so that R doesn't see the closing parenthesis, and 
then the next line starts with 'x=' (or whatever variable name) so the 
parser sees:

  a = c(1,1,1,
  b = c(1,0,1,1,1,1)

and hence syntax error.

I pasted the long line in interactive mode, and R prompted me for a 
continuation with a '+' prompt, even though there was a closing 
parenthesis on the long line. That made me think it had truncated the 
input.

Further tests reveals the input line is chopped at roughly 1022 
characters, which makes me think theres a buffer[1024] somewhere. Its 
probably documented as well.

Baz



From ligges at statistik.uni-dortmund.de  Fri May 20 14:12:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 May 2005 14:12:11 +0200
Subject: [R] Plot Problem
In-Reply-To: <1116588027.428dc7fb53704@web-mail2.uibk.ac.at>
References: <1116588027.428dc7fb53704@web-mail2.uibk.ac.at>
Message-ID: <428DD41B.5050902@statistik.uni-dortmund.de>

Sebastian Schoenherr wrote:

> Hi folks,
> 
> I try to plot a variable which contains string-variables. it works, but the
> problem is that there are a lot of values at the x axis (up to 24)
> SO i have to scale or rotate the label at the x axis.
> I tried this with the text() function. It doesn't work correctly.
> 
> Is there a simple way to rotate the labels?

For axis labels see ?par and its argument "las".

Uwe Ligges


> Best regards,
> Sebastian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri May 20 14:17:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 May 2005 14:17:37 +0200
Subject: [R] legend as a subtitle
In-Reply-To: <OF2FA3C9DD.99D34F79-ONC1257007.003C74BD-C1257007.003CF4BC@outokumpu.com>
References: <OF2FA3C9DD.99D34F79-ONC1257007.003C74BD-C1257007.003CF4BC@outokumpu.com>
Message-ID: <428DD561.304@statistik.uni-dortmund.de>

Ravi.Vishnu at outokumpu.com wrote:

> Very little space is available in one of my plots for the legend. I would 
> like to lift it out of the main plot area and present it in the subtitle 
> area. Would appreciate any help that I can get.

Look at the following code and read the corresponding help pages:

  plot(1:10, xlab="")
  ## clipping to device region rather than plot region:
  par(xpd=NA)
  legend(mean(par("usr")[1:2]), 0,
    legend="nonsense", xjust=0.5)

Uwe Ligges



> Ravi Vishnu
> 
>   
> This message is meant for the addressee only and may contain 
> confidential and legally privileged information. Any unauthorised 
> review, use, copying, storage, disclosure or distribution of this e-
> mail and any attachments is strictly prohibited. If you are not the 
> named recipient or have otherwise received this communication in 
> error, please destroy this message from your system and kindly notify 
> the sender by e-mail. Thank you for your co-operation.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Fri May 20 13:57:56 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 20 May 2005 07:57:56 -0400
Subject: [R] R annoyances
In-Reply-To: <303bce6835e500e41812afe596be251c@soc.soton.ac.uk>
References: <20050519172201.GI1178@jtkpc.cmp.uea.ac.uk>
	<20050519191054.BVKP26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
	<20050520100046.GA29990@jtkpc.cmp.uea.ac.uk>
	<303bce6835e500e41812afe596be251c@soc.soton.ac.uk>
Message-ID: <971536df050520045742681bbc@mail.gmail.com>

On 5/20/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> 
> On May 20, 2005, at 11:00 am, Jan T. Kim wrote:
> 
> > On Thu, May 19, 2005 at 03:10:53PM -0400, John Fox wrote:
> >
> >> Since you can use variables named c, q, or t in any event, I don't
> >> see why
> >> the existence of functions with these names is much of an impediment.
> >
> > True, particularly since I'm not too likely to use these variables for
> > (local)
> > functions, and variables of other types don't prevent functions from
> > working.
> > (I thought this was a problem... I must be spoilt by recently having
> > to read
> > too much Matlab code, where parentheses are used to both enclose
> > subscripts and
> > parameter lists, thus rendering subscript expressions and function
> > calls
> > syntactically indistinguishable.)
> 
> 
> Heh, I'm a recovering Matlab  user too.  This is sooooooooooo true!
> 
> In Matlab:
> 
> f(10)    # function f() evaluated at 10
> f(10)    # 10th element of vector f.  confusing!!
> 

Note that there is an advantage to this, namely, that one can
reimplement it as a vector or as a function without changing the
calling code.



From francoisromain at free.fr  Fri May 20 14:20:42 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 20 May 2005 14:20:42 +0200
Subject: [R] Plot Problem
In-Reply-To: <1116588027.428dc7fb53704@web-mail2.uibk.ac.at>
References: <1116588027.428dc7fb53704@web-mail2.uibk.ac.at>
Message-ID: <428DD61A.6050306@free.fr>

Le 20.05.2005 13:20, Sebastian Schoenherr a ??crit :

>Hi folks,
>
>I try to plot a variable which contains string-variables. it works, but the
>problem is that there are a lot of values at the x axis (up to 24)
>SO i have to scale or rotate the label at the x axis.
>I tried this with the text() function. It doesn't work correctly.
>
>Is there a simple way to rotate the labels?
>
>Best regards,
>Sebastian
>  
>
See appendix B here :
http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From andy_liaw at merck.com  Fri May 20 14:14:24 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 20 May 2005 08:14:24 -0400
Subject: [R] R annoyances
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E860@usctmx1106.merck.com>

> From: Robin Hankin
> 
> On May 20, 2005, at 11:00 am, Jan T. Kim wrote:
> 
> > On Thu, May 19, 2005 at 03:10:53PM -0400, John Fox wrote:
> >
> >> Since you can use variables named c, q, or t in any event, I don't 
> >> see why
> >> the existence of functions with these names is much of an 
> impediment.
> >
> > True, particularly since I'm not too likely to use these 
> variables for 
> > (local)
> > functions, and variables of other types don't prevent 
> functions from 
> > working.
> > (I thought this was a problem... I must be spoilt by 
> recently having 
> > to read
> > too much Matlab code, where parentheses are used to both enclose 
> > subscripts and
> > parameter lists, thus rendering subscript expressions and function 
> > calls
> > syntactically indistinguishable.)
> 
> 
> Heh, I'm a recovering Matlab  user too.  This is sooooooooooo true!
> 
> In Matlab:
> 
> f(10)    # function f() evaluated at 10
> f(10)    # 10th element of vector f.  confusing!!
> 
> R uses round brackets in two unrelated ways:
> 
>   4*(1+2)  --- using "(" and ")" to signify grouping
> f(8)  function f() evaluated at 8.
> 
> where there is no reason to use the same parenthesis symbol for both 
> tasks.

The same is done in Fortran/C/C++/Java/Python and God knows how many
others...
 
> IMO, the only system with consistent parenthesis use is Mathematica;
> 
> f[10]  #  function f[] evaluated at 10
> 8*(2+2)   # parenthesis to override  order of operations
> f[[3]] # third element of list f
> 
> {} are used for sets.

Just out of curiosity, what's used for grouping expressions?

Andy
 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From sundar.dorai-raj at pdf.com  Fri May 20 14:16:54 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 20 May 2005 07:16:54 -0500
Subject: [R] attached file with syntax error
In-Reply-To: <000801c55d27$90bfbe00$6e8b6882@didacdom.stat.ucl.ac.be>
References: <000801c55d27$90bfbe00$6e8b6882@didacdom.stat.ucl.ac.be>
Message-ID: <428DD536.20502@pdf.com>

You can in R-2.1.0 Patched:

 > c(1,)
[1] 1

--sundar

Eric Lecoutre wrote:
> Well...
> 
> You just can't end vector declaration with a comma...
> 
> 
>>x <- c(0,1,)
> 
> Error: syntax error
> 
>>x <- c(0,1)
> 
> 
> Eric
> 
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
> 
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
> 
> If the statistics are boring, then you've got the wrong numbers. -Edward
> Tufte   
> 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bry at xdocs.dk
>>Sent: vendredi 20 mai 2005 11:39
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] attached file with syntax error
>>
>>
>>
>>
>>oops, I forgot to attach the example txt file with the syntax 
>>error, so I will do it here, can anyone see what the error is 
>>with bryansAtLineLevel?
>>
>>
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jtk at cmp.uea.ac.uk  Fri May 20 15:42:57 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Fri, 20 May 2005 14:42:57 +0100
Subject: [R] R annoyances
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E860@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E860@usctmx1106.merck.com>
Message-ID: <20050520134257.GE29990@jtkpc.cmp.uea.ac.uk>

On Fri, May 20, 2005 at 08:14:24AM -0400, Liaw, Andy wrote:
> > From: Robin Hankin
> > 
> > On May 20, 2005, at 11:00 am, Jan T. Kim wrote:
> > 
> > > On Thu, May 19, 2005 at 03:10:53PM -0400, John Fox wrote:
> > >
> > >> Since you can use variables named c, q, or t in any event, I don't 
> > >> see why
> > >> the existence of functions with these names is much of an 
> > impediment.
> > >
> > > True, particularly since I'm not too likely to use these 
> > variables for 
> > > (local)
> > > functions, and variables of other types don't prevent 
> > functions from 
> > > working.
> > > (I thought this was a problem... I must be spoilt by 
> > recently having 
> > > to read
> > > too much Matlab code, where parentheses are used to both enclose 
> > > subscripts and
> > > parameter lists, thus rendering subscript expressions and function 
> > > calls
> > > syntactically indistinguishable.)
> > 
> > 
> > Heh, I'm a recovering Matlab  user too.  This is sooooooooooo true!
> > 
> > In Matlab:
> > 
> > f(10)    # function f() evaluated at 10
> > f(10)    # 10th element of vector f.  confusing!!
> > 
> > R uses round brackets in two unrelated ways:
> > 
> >   4*(1+2)  --- using "(" and ")" to signify grouping
> > f(8)  function f() evaluated at 8.
> > 
> > where there is no reason to use the same parenthesis symbol for both 
> > tasks.
> 
> The same is done in Fortran/C/C++/Java/Python and God knows how many
> others...

And this is different from the subscripting / function call ambiguity,
as these languages (to the extent I know them) are designed such that
parentheses for precedence control are syntactically distinguishable
from those used for function parameter lists: If the opening parenthesis
is preceded by an identifier, that identifier is a function name and
the parenthesis opens a parameter list.

(Python is a somewhat messy case, though, because it uses parentheses
for tuples too.)

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From amsa36060 at yahoo.com  Fri May 20 15:15:29 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Fri, 20 May 2005 06:15:29 -0700 (PDT)
Subject: [R] tune.svm in {e1071}
Message-ID: <20050520131530.90546.qmail@web60424.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050520/9f2090d0/attachment.pl

From br44114 at gmail.com  Fri May 20 15:20:05 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 20 May 2005 09:20:05 -0400
Subject: [R] R annoyances
Message-ID: <8d5a3635050520062069b4c1dd@mail.gmail.com>

On 20-May-05 Uwe Ligges wrote:
> All possible changes to T/F (both removing the meaning of 
> TRUE/FALSE in a clean session and making them reserved words) 
> would break code of lots of users.

Just wanted to point out that there's another (darker) side to this:
code that produces bad results without the users even realizing it.
Personally, I would clearly prefer lots of broken code to mistakes
caused by T/TRUE and F/FALSE.

Hypothetically, if whatever=T/F were forbidden and only
whatever=TRUE/FALSE were allowed, all the code could be fixed with a
simple sed script:
for F in `ls *.r`
do 
  mv $F $F.$$ 
  sed -e 's/=T,/=TRUE,/g' -e 's/=F,/=FALSE,/g' -e 's/=T)/=TRUE)/g' -e
's/=F)/=FALSE)/g' $F.$$ > $F
  rm $F.$$
done



-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
Sent: Friday, May 20, 2005 2:39 AM
To: John Fox
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] R annoyances


Dear John,

I have not expected to cause that many traffic and largish discussion.

What I tried to point out is that:
- a "programmer" should know that one has to use TRUE / FALSE in code in 
order to make it work generaly which is also checked by R CMD check.
- a "user" simply typing some lines in order to look at the data can 
shortly write T or F instead.

where "programmer" and "user" are not well defined and probably 
undistinguishable according to Chambers (1998).
I'd call people using [..., drop=FALSE] "programmer" here, since the 
code is probably used inside functions.

S-PLUS compatibility (T/F) has to be considered as well.

All possible changes to T/F (both removing the meaning of TRUE/FALSE in 
a clean session and making them reserved words) would break code of lots 
of users. With a common amount of statistical uncertainty I think it 
might be too late for changes ...

Best,
Uwe



John Fox wrote:
> Dear Uwe,
> 
> I've often wondered why T and F aren't reserved words in R as TRUE and FALSE
> are. Perhaps there's some use of T and F as variables, but that seems
> ill-advised.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
>>Sent: Thursday, May 19, 2005 10:08 AM
>>To: Chalasani, Prasad
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] R annoyances
>>
>>Chalasani, Prasad wrote:
>>
>>
>>>Thanks all for pointing out that I can use 
>>>	mtx[,1,drop=F]
>>
>>
>>Which, for example, won't work for
>>  F <- 10.25
>>
>>---> drop=FALSE  !
>>           ^^^^^
>>
>>Uwe Ligges
>>
>>
>>
>>



From Ted.Harding at nessie.mcc.ac.uk  Fri May 20 15:09:27 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 20 May 2005 14:09:27 +0100 (BST)
Subject: [R] attached file with syntax error
In-Reply-To: <000801c55d27$90bfbe00$6e8b6882@didacdom.stat.ucl.ac.be>
Message-ID: <XFMail.050520140927.Ted.Harding@nessie.mcc.ac.uk>

On 20-May-05 Eric Lecoutre wrote:
> 
> Well...
> 
> You just can't end vector declaration with a comma...
> 
>> x <- c(0,1,)
> Error: syntax error
>> x <- c(0,1)
> 
> Eric

Well, maybe some people can't achieve that, but I can:

> x<-c(1,2,3,)
> x
[1] 1 2 3

(on both R-1.8.0 and R-2.1.0beta)

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-May-05                                       Time: 14:07:14
------------------------------ XFMail ------------------------------



From phgrosjean at sciviews.org  Fri May 20 15:37:02 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 20 May 2005 15:37:02 +0200
Subject: [R] R annoyances
In-Reply-To: <20050520134257.GE29990@jtkpc.cmp.uea.ac.uk>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E860@usctmx1106.merck.com>
	<20050520134257.GE29990@jtkpc.cmp.uea.ac.uk>
Message-ID: <428DE7FE.50102@sciviews.org>

Hello,
Regarding use of parenthesis, it is true that R is much better with 
f(10) != f[10] != f[[10]], where Matlab is a little confusing. Also, in 
Matlab, you can use some functions without (), further adding to the 
confusion (the only example that comes to my mind in R is the use of '?' 
as shortcut for help()).

However, there is still a double use of () in R: it is both used for 
enclosing function arguments and for grouping operations. One language 
has a syntax that makes a totally unambiguous use of [], () and {} is 
Mathematica: [] is for subscript, {} is for function arguments and () is 
for grouping... but Mathematica code is really a pain to typeset and read.

So, all in all, I really like the S langage syntax: it is very readable 
and reasonably rigid...

Regarding T and F, I took the habit to *always* type them TRUE or FALSE. 
Again, very readable and not confusing at all. If T and F as equivalent 
to TRUE and FALSE would ever be deprecated and then defunct in further 
versions of R, well, I would not complain about it!

The only aspect I don't like is a too loosely use of the dot in 
functions: both in functions names, in object classes and in generic 
functions / methods. Hence, we have for instance: 'data.frame', 
'help.search' and 'summary.matrix'... just guess which one is an object 
class, which one is an ordinary function and which one is a S3 method 
(OK, S4 solves somehow the problem)? It would have been much better to 
*reserve* the use of a dot in a function name as a separator between the 
name of the generic function and the class to which it applies. Thus, 
'summary.matrix' would have been correct, but both 'data.frame' and 
'help.search' should have been spelled differently, perhaps 'dataframe' 
and 'helpSearch'. Just a dream... because 'data.frame' will of course 
never be spelled differently!!!
Best,

Philippe Grosjean


Jan T. Kim wrote:
> On Fri, May 20, 2005 at 08:14:24AM -0400, Liaw, Andy wrote:
> 
>>>From: Robin Hankin
>>>
>>>On May 20, 2005, at 11:00 am, Jan T. Kim wrote:
>>>
>>>
>>>>On Thu, May 19, 2005 at 03:10:53PM -0400, John Fox wrote:
>>>>
>>>>
>>>>>Since you can use variables named c, q, or t in any event, I don't 
>>>>>see why
>>>>>the existence of functions with these names is much of an 
>>>
>>>impediment.
>>>
>>>>True, particularly since I'm not too likely to use these 
>>>
>>>variables for 
>>>
>>>>(local)
>>>>functions, and variables of other types don't prevent 
>>>
>>>functions from 
>>>
>>>>working.
>>>>(I thought this was a problem... I must be spoilt by 
>>>
>>>recently having 
>>>
>>>>to read
>>>>too much Matlab code, where parentheses are used to both enclose 
>>>>subscripts and
>>>>parameter lists, thus rendering subscript expressions and function 
>>>>calls
>>>>syntactically indistinguishable.)
>>>
>>>
>>>Heh, I'm a recovering Matlab  user too.  This is sooooooooooo true!
>>>
>>>In Matlab:
>>>
>>>f(10)    # function f() evaluated at 10
>>>f(10)    # 10th element of vector f.  confusing!!
>>>
>>>R uses round brackets in two unrelated ways:
>>>
>>>  4*(1+2)  --- using "(" and ")" to signify grouping
>>>f(8)  function f() evaluated at 8.
>>>
>>>where there is no reason to use the same parenthesis symbol for both 
>>>tasks.
>>
>>The same is done in Fortran/C/C++/Java/Python and God knows how many
>>others...
> 
> 
> And this is different from the subscripting / function call ambiguity,
> as these languages (to the extent I know them) are designed such that
> parentheses for precedence control are syntactically distinguishable
> from those used for function parameter lists: If the opening parenthesis
> is preceded by an identifier, that identifier is a function name and
> the parenthesis opens a parameter list.
> 
> (Python is a somewhat messy case, though, because it uses parentheses
> for tuples too.)
> 
> Best regards, Jan



From Saghir.Bashir at UCB-Group.com  Fri May 20 15:36:03 2005
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Fri, 20 May 2005 15:36:03 +0200
Subject: [R] Power w/ unequal sample sizes
Message-ID: <3EBA5559F490D61189430002A5F0AE890ABE30D5@ntexcrd.braine.ucb>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050520/2c4fa267/attachment.pl

From ripley at stats.ox.ac.uk  Fri May 20 15:39:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 May 2005 14:39:24 +0100 (BST)
Subject: [R] R annoyances
In-Reply-To: <8d5a3635050520062069b4c1dd@mail.gmail.com>
References: <8d5a3635050520062069b4c1dd@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0505201432440.28730@gannet.stats>

On Fri, 20 May 2005, bogdan romocea wrote:

> On 20-May-05 Uwe Ligges wrote:
>> All possible changes to T/F (both removing the meaning of
>> TRUE/FALSE in a clean session and making them reserved words)
>> would break code of lots of users.
>
> Just wanted to point out that there's another (darker) side to this:
> code that produces bad results without the users even realizing it.
> Personally, I would clearly prefer lots of broken code to mistakes
> caused by T/TRUE and F/FALSE.

You do realize that R CMD check checks for use of unassigned T/F?  So it 
would only be unchecked code which did that.

> Hypothetically, if whatever=T/F were forbidden and only
> whatever=TRUE/FALSE were allowed, all the code could be fixed with a
> simple sed script:
> for F in `ls *.r`
> do
>  mv $F $F.$$
>  sed -e 's/=T,/=TRUE,/g' -e 's/=F,/=FALSE,/g' -e 's/=T)/=TRUE)/g' -e
> 's/=F)/=FALSE)/g' $F.$$ > $F
>  rm $F.$$
> done

I assure you it is a *lot* harder than that.  Some of us use spaces for a 
start. No sed script can know the difference between

F <- "2";  as.numeric(x = F)
F <- "2";  as.numeric(x = FALSE)

(I know because I used to share code bases for S-PLUS and R, and had Perl 
scripts to do the conversion that worked for my style, but not for some 
other authors' code.)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Fri May 20 15:45:04 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 20 May 2005 09:45:04 -0400
Subject: [R] R annoyances
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E866@usctmx1106.merck.com>

> From: Philippe Grosjean 
> 
> Hello,
> Regarding use of parenthesis, it is true that R is much better with 
> f(10) != f[10] != f[[10]], where Matlab is a little 
> confusing. Also, in 
> Matlab, you can use some functions without (), further adding to the 
> confusion (the only example that comes to my mind in R is the 
> use of '?' 
> as shortcut for help()).
> 
> However, there is still a double use of () in R: it is both used for 
> enclosing function arguments and for grouping operations. One 
> language 
> has a syntax that makes a totally unambiguous use of [], () and {} is 
> Mathematica: [] is for subscript, {} is for function 
> arguments and () is 
> for grouping... but Mathematica code is really a pain to 
> typeset and read.
> 
> So, all in all, I really like the S langage syntax: it is 
> very readable 
> and reasonably rigid...
> 
> Regarding T and F, I took the habit to *always* type them 
> TRUE or FALSE. 
> Again, very readable and not confusing at all. If T and F as 
> equivalent 
> to TRUE and FALSE would ever be deprecated and then defunct 
> in further 
> versions of R, well, I would not complain about it!
> 
> The only aspect I don't like is a too loosely use of the dot in 
> functions: both in functions names, in object classes and in generic 
> functions / methods. Hence, we have for instance: 'data.frame', 
> 'help.search' and 'summary.matrix'... just guess which one is 
> an object 
> class, which one is an ordinary function and which one is a S3 method 
> (OK, S4 solves somehow the problem)? It would have been much 
> better to 
> *reserve* the use of a dot in a function name as a separator 
> between the 
> name of the generic function and the class to which it applies. Thus, 
> 'summary.matrix' would have been correct, but both 'data.frame' and 
> 'help.search' should have been spelled differently, perhaps 
> 'dataframe' 
> and 'helpSearch'. Just a dream... because 'data.frame' will of course 
> never be spelled differently!!!

Don't give up to easily:  If "_" can be done away as assignment operator,
I'd guess anything is fair game...

Andy

> Best,
> 
> Philippe Grosjean
> 
> 
> Jan T. Kim wrote:
> > On Fri, May 20, 2005 at 08:14:24AM -0400, Liaw, Andy wrote:
> > 
> >>>From: Robin Hankin
> >>>
> >>>On May 20, 2005, at 11:00 am, Jan T. Kim wrote:
> >>>
> >>>
> >>>>On Thu, May 19, 2005 at 03:10:53PM -0400, John Fox wrote:
> >>>>
> >>>>
> >>>>>Since you can use variables named c, q, or t in any 
> event, I don't 
> >>>>>see why
> >>>>>the existence of functions with these names is much of an 
> >>>
> >>>impediment.
> >>>
> >>>>True, particularly since I'm not too likely to use these 
> >>>
> >>>variables for 
> >>>
> >>>>(local)
> >>>>functions, and variables of other types don't prevent 
> >>>
> >>>functions from 
> >>>
> >>>>working.
> >>>>(I thought this was a problem... I must be spoilt by 
> >>>
> >>>recently having 
> >>>
> >>>>to read
> >>>>too much Matlab code, where parentheses are used to both enclose 
> >>>>subscripts and
> >>>>parameter lists, thus rendering subscript expressions and 
> function 
> >>>>calls
> >>>>syntactically indistinguishable.)
> >>>
> >>>
> >>>Heh, I'm a recovering Matlab  user too.  This is sooooooooooo true!
> >>>
> >>>In Matlab:
> >>>
> >>>f(10)    # function f() evaluated at 10
> >>>f(10)    # 10th element of vector f.  confusing!!
> >>>
> >>>R uses round brackets in two unrelated ways:
> >>>
> >>>  4*(1+2)  --- using "(" and ")" to signify grouping
> >>>f(8)  function f() evaluated at 8.
> >>>
> >>>where there is no reason to use the same parenthesis 
> symbol for both 
> >>>tasks.
> >>
> >>The same is done in Fortran/C/C++/Java/Python and God knows how many
> >>others...
> > 
> > 
> > And this is different from the subscripting / function call 
> ambiguity,
> > as these languages (to the extent I know them) are designed 
> such that
> > parentheses for precedence control are syntactically distinguishable
> > from those used for function parameter lists: If the 
> opening parenthesis
> > is preceded by an identifier, that identifier is a function name and
> > the parenthesis opens a parameter list.
> > 
> > (Python is a somewhat messy case, though, because it uses 
> parentheses
> > for tuples too.)
> > 
> > Best regards, Jan
> 
> 
>



From p.dalgaard at biostat.ku.dk  Fri May 20 15:58:10 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 May 2005 15:58:10 +0200
Subject: [R] attached file with syntax error
In-Reply-To: <428DD536.20502@pdf.com>
References: <000801c55d27$90bfbe00$6e8b6882@didacdom.stat.ucl.ac.be>
	<428DD536.20502@pdf.com>
Message-ID: <x24qcyuhhp.fsf@turmalin.kubism.ku.dk>

Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> writes:

> You can in R-2.1.0 Patched:
> 
>  > c(1,)
> [1] 1
> 
> --sundar
> 
> Eric Lecoutre wrote:
> > Well...
> > You just can't end vector declaration with a comma...
> >
> >>x <- c(0,1,)
> > Error: syntax error
> >
> >>x <- c(0,1)

I can in versions going back to 1.8.0 (did Eric accidentally include
the ">" perhaps?)

However, the code in r.txt runs OK on 2.1.0, but bombs on 2.0.1 with

> finnsAtLineLevel <- c(0,1,0,1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,00,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,1,1,0,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1
 ,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0+ ,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,0,0,0,0,1,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,)
Error: Object "??" not found
Execution halted

(Notice irregularity around char #1024). Not quite sure when (or even
if!) this got fixed.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Allan at STATS.uct.ac.za  Fri May 20 16:00:37 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Fri, 20 May 2005 16:00:37 +0200
Subject: [R] R: looping
Message-ID: <428DED85.E862DCE6@STATS.uct.ac.za>

hi all

i have a simple question. code is displayed below. 

how can i use a vectorised command in order to do this (ie replace the
loop)? (ie apply, lapply, sweep, etc)


z<-matrix(c(1:9),3,3)
top<-c(1.5,5.5,9)

for (i in 1:3)	z[z[,i]>top[i]]<-top[i]

From B.Rowlingson at lancaster.ac.uk  Fri May 20 16:08:20 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 20 May 2005 15:08:20 +0100
Subject: [R] R annoyances
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E866@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E866@usctmx1106.merck.com>
Message-ID: <428DEF54.9030701@lancaster.ac.uk>

Liaw, Andy wrote:

> Don't give up to easily:  If "_" can be done away as assignment operator,
> I'd guess anything is fair game...

  Even my great dream that R and Python eventually merge into the same 
language? R gets Python's syntax and Object-oriented functions and 
Python gets access to all R's statistical functions?

  RSPython is a good start, but I'd like a much closer integration.

Baz



From r.hankin at noc.soton.ac.uk  Fri May 20 16:21:24 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 20 May 2005 15:21:24 +0100
Subject: [R] R annoyances
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E860@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E860@usctmx1106.merck.com>
Message-ID: <c515124de8c715a888df71b7b2f6df45@soc.soton.ac.uk>


On May 20, 2005, at 01:14 pm, Liaw, Andy wrote:

[snip]

>> R uses round brackets in two unrelated ways:
>>
>>   4*(1+2)  --- using "(" and ")" to signify grouping
>> f(8)  function f() evaluated at 8.
>>
>> where there is no reason to use the same parenthesis symbol for both
>> tasks.
>
> The same is done in Fortran/C/C++/Java/Python and God knows how many
> others...
>

well yes, but that doesn't mean it's the Right Thing To Do (tm).

Gabor points out that "f(10)" having meaning whether f is a vector or a 
function
is interesting.  I guess this is right, but I can't think of a 
real-life situation in which
this would be useful.


>> IMO, the only system with consistent parenthesis use is Mathematica;
>>
>> f[10]  #  function f[] evaluated at 10
>> 8*(2+2)   # parenthesis to override  order of operations
>> f[[3]] # third element of list f
>>
>> {} are used for sets.
>
> Just out of curiosity, what's used for grouping expressions?
>

all statements on a line are executed sequentially.   Execution 
continues to next line
if there are any unmatched parentheses or dangling operators.

  So it's a bit pythonesque.


> Andy
>
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ripley at stats.ox.ac.uk  Fri May 20 16:21:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 May 2005 15:21:49 +0100 (BST)
Subject: [R] attached file with syntax error
In-Reply-To: <x24qcyuhhp.fsf@turmalin.kubism.ku.dk>
References: <000801c55d27$90bfbe00$6e8b6882@didacdom.stat.ucl.ac.be>
	<428DD536.20502@pdf.com> <x24qcyuhhp.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0505201517010.29301@gannet.stats>

It might be worth pointing out that R is supposed to have a 1024 byte (not 
character) input buffer, and I did fix a few things related to that when 
internationalizing the parser (and also some about multi-byte pushbacks 
which I suspect is the issue here).

It is still good practice to keep input lines well below that buffer size.

On Fri, 20 May 2005, Peter Dalgaard wrote:

> Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> writes:
>
>> You can in R-2.1.0 Patched:
>>
>> > c(1,)
>> [1] 1
>>
>> --sundar
>>
>> Eric Lecoutre wrote:
>>> Well...
>>> You just can't end vector declaration with a comma...
>>>
>>>> x <- c(0,1,)
>>> Error: syntax error
>>>
>>>> x <- c(0,1)
>
> I can in versions going back to 1.8.0 (did Eric accidentally include
> the ">" perhaps?)
>
> However, the code in r.txt runs OK on 2.1.0, but bombs on 2.0.1 with
>
>> finnsAtLineLevel <- c(0,1,0,1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,00,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,1,1,0,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1
> ,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0+ ,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,0,0,0,0,1,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,)
> Error: Object "?" not found
> Execution halted
>
> (Notice irregularity around char #1024). Not quite sure when (or even
> if!) this got fixed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From deepayan at stat.wisc.edu  Fri May 20 16:18:01 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 20 May 2005 09:18:01 -0500
Subject: [R] Lattice: it seems, a bug in draw.key function
In-Reply-To: <1803865630.20050520160521@eimb.ru>
References: <1803865630.20050520160521@eimb.ru>
Message-ID: <200505200918.01531.deepayan@stat.wisc.edu>

On Friday 20 May 2005 07:05 am, Wladimir Eremeev wrote:
> Dear r-help,
>
>   Now I am drawing graphs with xyplot function.
>
>   In order to place a legend under the plots I use the key argument in
> the xyplot function.
>
>   One of the 'key' components is 'divide', which defines a number of
>   points on the each line of the legend. The default is 3 points.
>   I would like a single point, so I set divide = 1.
>
>   Call to xyplot didn't produce any points at all.
>
>   The body of draw.key function contains the following
>
> pointsGrob(x=(1:key$divide-1)/(key$divide-1), y=rep(0.5,key$divide),
>             [skip]
>           )
>
> When key$divide=1, x is NaN...

There's certainly a problem (at least one of omission in the documentation), 
but I'm not sure what the best way to resolve it would be. This is what 
currently happens on S-PLUS (6.0) and R:

divide = 3:  o--o--o  S-PLUS/R
divide = 2:  o-----o  S-PLUS/R

divide = 1:     o     S-PLUS
             -------  R

The difference in the 'divide = 1' case was not intentional, I just never 
checked it. 

Possible `solutions' are:

1. keep things as they are and clarify the documentation (namely, saying that 
'divide' has to be greater than 1 to make sense)

2. change it to behave like S-PLUS (but that seems silly, since one might then 
just as well use 'points' instead of 'lines')

3. do something else entirely, like ---o--- . This is not attractive from a 
programmers point of view because it makes 'divide = 1'  behave 
inconsistently with other values of 'divide'.

I'm open to suggestions.

Deepayan



From spencer.graves at pdf.com  Fri May 20 16:23:53 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 20 May 2005 07:23:53 -0700
Subject: [R] Why does this give a syntax error?
In-Reply-To: <1116581591.428daed79aed9@horde.scannet.dk>
References: <1116581591.428daed79aed9@horde.scannet.dk>
Message-ID: <428DF2F9.10504@pdf.com>

	 Might it want "bryansAtHeaderLevel <-" to be syntactically complete? 
Have you tried wrapping the entire sequence between "{" and "}"?

	  hope this helps.

bry at xdocs.dk wrote:
> Hi
> 
> I'm generating the following in a file and getting a syntax error:
> 
> 
> 
> 
> 
> bryansAtHeaderLevel <-
> c(0,1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,1,0,1,0,1,1,1,1,1,1,0,0,1,1,1,0,0,1,1,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,1,0,0,0,)
> 
> finnsAtHeaderLevel <-
> c(0,1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,0,1,0,1,1,1,1,1,1,0,0,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,1,0,0,0,)
> 
> faqAtHeaderLevel <-
> c(0,1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,1,0,1,0,1,1,1,1,1,1,0,0,1,1,1,0,0,1,1,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,0,1,0,0,0,)
> 
> bryansAtLineLevel <-
> c(0,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,1,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1,1,1,0,1,1,1,1,1,1
,1,
>  1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,1,1,1,1,0,0,0,1,0,0,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,)
> 
> finnsAtLineLevel <-
> c(0,1,0,1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,1,1,0,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1,1,1,0,1,1,1,1,1,1
,1,
>  1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,0,0,0,0,1,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,)
> 
> faqAtLineLevel <-
> c(0,1,0,1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1,1,1,0,1,1,1,1,1,1
,1,
>  1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,1,1,1,1,0,0,0,1,0,0,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,)
> 
> 
> because of line breaks in the email this is not the exact output, there is no
> break between c and (
> 
> the example is also in the attached text r.txt
> 
> anyway I'm being told that bryansAtLineLevel has a syntax error when I load and
> run the script in R gui.
> I can't really see why that would be wrong and the others wouldn't be. Any
> suggestions?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bolker at zoo.ufl.edu  Fri May 20 16:04:21 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Fri, 20 May 2005 14:04:21 +0000 (UTC)
Subject: [R] R annoyances
References: <8d5a3635050520062069b4c1dd@mail.gmail.com>
Message-ID: <loom.20050520T155339-543@post.gmane.org>

[snip snip snip snip]
> 

  How about "strict" option that could be set to
disallow use of T/F variables?

   I had a student run into trouble fairly recently (although can't
at the moment provide a reproducible example using T as a
variable in a formula that was passed to nls() ... I think there
might still be some internal use of T/F in that package ...



From ggrothendieck at gmail.com  Fri May 20 16:36:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 20 May 2005 10:36:59 -0400
Subject: [R] R annoyances
In-Reply-To: <428DEF54.9030701@lancaster.ac.uk>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E866@usctmx1106.merck.com>
	<428DEF54.9030701@lancaster.ac.uk>
Message-ID: <971536df05052007366fc8a758@mail.gmail.com>

On 5/20/05, Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:
> Liaw, Andy wrote:
> 
> > Don't give up to easily:  If "_" can be done away as assignment operator,
> > I'd guess anything is fair game...
> 
>  Even my great dream that R and Python eventually merge into the same
> language? R gets Python's syntax and Object-oriented functions and
> Python gets access to all R's statistical functions?
> 
>  RSPython is a good start, but I'd like a much closer integration.
> 

Note that R already has some alternative OO models in the 
'R.oo' and 'proto' packages and, of course, also comes standard
with S3 and S4.  Maybe one of these object models satisfies what 
you are discussing?



From wl at eimb.ru  Fri May 20 16:52:04 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Fri, 20 May 2005 18:52:04 +0400
Subject: [R] Why does this give a syntax error?
Message-ID: <1161391050.20050520185204@eimb.ru>


 The statemets ends with '0,)', maybe this is the reason...
 BTW, all statements end with the similar symbols.
 I have found, that R some times complain on this and sometimes not,
 but did not search the reasons.

 My R 2.1.0 on win2000 doesn't complain on the statement
 bryansAtLineLevel<-c( blah-blah-blah
 which I have copy-pasted from the mail message.

 Why don't you want to use 'rep' in generating sequences?
 
 You also could store data in separate file(s) and use something like
 read.table.

 At least, these could make your script more readable.

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From Ravi.Vishnu at outokumpu.com  Fri May 20 16:48:28 2005
From: Ravi.Vishnu at outokumpu.com (Ravi.Vishnu@outokumpu.com)
Date: Fri, 20 May 2005 16:48:28 +0200
Subject: [R] getting the unique values and counts from a vector
Message-ID: <OFECBDBF83.32DFB5C2-ONC1257007.004FC855-C1257007.00515773@outokumpu.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050520/612e78f4/attachment.pl

From ligges at statistik.uni-dortmund.de  Fri May 20 16:51:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 May 2005 16:51:43 +0200
Subject: [R] R annoyances
In-Reply-To: <loom.20050520T155339-543@post.gmane.org>
References: <8d5a3635050520062069b4c1dd@mail.gmail.com>
	<loom.20050520T155339-543@post.gmane.org>
Message-ID: <428DF97F.4090806@statistik.uni-dortmund.de>

Ben Bolker wrote:

> [snip snip snip snip]
> 
> 
>   How about "strict" option that could be set to
> disallow use of T/F variables?
> 
>    I had a student run into trouble fairly recently (although can't
> at the moment provide a reproducible example using T as a
> variable in a formula that was passed to nls() ... I think there
> might still be some internal use of T/F in that package ...


Probably because T is defined in Namespace base

  T <- 0
  getFromNamespace("T", "base") # [1] TRUE

Uwe




> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Fri May 20 16:50:54 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 20 May 2005 10:50:54 -0400
Subject: [R] Degradation model
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A943D@uswpmx00.merck.com>

There's an extension to the package nlme using odesolve to estimate
parameters of models defined by ODEs. It's called nlmeODE and has a home
page here: http://sourceforge.net/projects/nlmeode/ ; it's on CRAN
http://cran.r-project.org. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Klaus Kaae Andersen
Sent: Friday, May 20, 2005 7:12 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Degradation model


Dear list,

I have a degradation model:

dX/dt = * I(X2)*( k1*X(t) )/( X(t)+k2 ) 

where X(t) is concentration at time t, and k1 and k2 are parameters
that I want to estimate. I(X) is a known inhibitor function.

My questions is whether this is implemented or easily computed in any
R package. I have searched the archives but without luck.


Any help or comments on this would be appreciated,
Klaus Andersen

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From wl at eimb.ru  Fri May 20 17:05:29 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Fri, 20 May 2005 19:05:29 +0400
Subject: [R] Lattice: it seems, a bug in draw.key function
In-Reply-To: <200505200918.01531.deepayan@stat.wisc.edu>
References: <1803865630.20050520160521@eimb.ru>
	<200505200918.01531.deepayan@stat.wisc.edu>
Message-ID: <11710066739.20050520190529@eimb.ru>

Dear Deepayan,

I suggest something like

pointsGrob(
        x=if(key$divide>1){(1:key$divide-1)/(key$divide-1)} else 0.5,
             [blah-blah-blah]
          )

This looks like your 3rd variant.

However, I haven't try very hard to verify my solution.

I tried to redefine the entire function draw.key in my working environment,
but this didn't work.

I use R 2.1.0 on windows 2000, packages are precompiled binaries.
Am I right in thinking, that despite the presence of my function
draw.key, xyplot used the function, defined in the package?

DS> There's certainly a problem (at least one of omission in the documentation),
DS> but I'm not sure what the best way to resolve it would be. This is what
DS> currently happens on S-PLUS (6.0) and R:

DS> divide = 3:  o--o--o  S-PLUS/R
DS> divide = 2:  o-----o  S-PLUS/R

DS> divide = 1:     o     S-PLUS
DS>              -------  R

I would like to see ---o---

DS> Possible `solutions' are:

DS> 1. keep things as they are and clarify the documentation (namely, saying that
DS> 'divide' has to be greater than 1 to make sense)

DS> 2. change it to behave like S-PLUS (but that seems silly, since one might then
DS> just as well use 'points' instead of 'lines')

DS> 3. do something else entirely, like ---o--- . This is not attractive from a
DS> programmers point of view because it makes 'divide = 1'  behave 
DS> inconsistently with other values of 'divide'.

Marginal conditions need to be treated individually... :)

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From deepayan at stat.wisc.edu  Fri May 20 16:55:37 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 20 May 2005 09:55:37 -0500
Subject: [R] Lattice: it seems, a bug in draw.key function
In-Reply-To: <11710066739.20050520190529@eimb.ru>
References: <1803865630.20050520160521@eimb.ru>
	<200505200918.01531.deepayan@stat.wisc.edu>
	<11710066739.20050520190529@eimb.ru>
Message-ID: <200505200955.37557.deepayan@stat.wisc.edu>

On Friday 20 May 2005 10:05 am, Wladimir Eremeev wrote:
> Dear Deepayan,
>
> I suggest something like
>
> pointsGrob(
>         x=if(key$divide>1){(1:key$divide-1)/(key$divide-1)} else 0.5,
>              [blah-blah-blah]
>           )
>
> This looks like your 3rd variant.
>
> However, I haven't try very hard to verify my solution.
>
> I tried to redefine the entire function draw.key in my working environment,
> but this didn't work.
>
> I use R 2.1.0 on windows 2000, packages are precompiled binaries.
> Am I right in thinking, that despite the presence of my function
> draw.key, xyplot used the function, defined in the package?

Yes. It has to do with namespaces. Try ?assignInNamespace

Deepayan



From SuzieBlatt at netscape.net  Fri May 20 17:00:06 2005
From: SuzieBlatt at netscape.net (SuzieBlatt@netscape.net)
Date: Fri, 20 May 2005 11:00:06 -0400
Subject: [R] address of Gordon Smyth ?
Message-ID: <1B71BC0D.6E242973.0D1322AF@netscape.net>


Anyone know where I can reach the author of the compareGrowthCurves function?
I'm having trouble with it.
Thanks,
Suzie

__________________________________________________________________
Switch to Netscape Internet Service.




New! Netscape Toolbar for Internet Explorer
Search from anywhere on the Web and block those annoying pop-ups.
Download now at http://channels.netscape.com/ns/search/install.jsp



From rvaradha at jhsph.edu  Fri May 20 17:01:27 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 20 May 2005 11:01:27 -0400
Subject: [R] getting the unique values and counts from a vector
In-Reply-To: <OFECBDBF83.32DFB5C2-ONC1257007.004FC855-C1257007.00515773@outokumpu.com>
Message-ID: <OWA-2vAauNjVuMhvqgS00009402@owa-2.sph.ad.jhsph.edu>

Hi,
"table" should do it.

> x<-c(2 ,1 ,2, 1, 4 ,2 ,1, 4 ,1 ,1)
> table(x)
x
1 2 4 
5 3 2 

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Ravi.Vishnu at outokumpu.com
> Sent: Friday, May 20, 2005 10:48 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] getting the unique values and counts from a vector
> 
> Hi all,
> >From a vector, I want to get the unique values and the counts of these
> unique values in the vector. For example,
> x<-c(2 ,1 ,2, 1, 4 ,2 ,1, 4 ,1 ,1)
> xu<-unique(x)
> xn<-numeric(length(xu))
> for (i in 1:length(xu)) {xn[i]<-length(which(x==xu[i]))}
> There must be a very much simpler method of doing this. Can somebody
> direct me to the functions that I must read in order to do operations of
> these sort.
> Thanks,
> Ravi Vishnu
> 
> 
> This message is meant for the addressee only and may contain
> confidential and legally privileged information. Any unauthorised
> review, use, copying, storage, disclosure or distribution of this e-
> mail and any attachments is strictly prohibited. If you are not the
> named recipient or have otherwise received this communication in
> error, please destroy this message from your system and kindly notify
> the sender by e-mail. Thank you for your co-operation.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri May 20 17:14:19 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 20 May 2005 17:14:19 +0200
Subject: [R] getting the unique values and counts from a vector
References: <OFECBDBF83.32DFB5C2-ONC1257007.004FC855-C1257007.00515773@outokumpu.com>
Message-ID: <004c01c55d4e$985385d0$0540210a@www.domain>

you could use table(), i.e.,

x <- c(2, 1, 2, 1, 4, 2, 1, 4, 1, 1)
#######
tab <- table(x)
xu <- as.numeric(names(tab))
xn <- as.vector(tab)
xu; xn

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: <Ravi.Vishnu at outokumpu.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, May 20, 2005 4:48 PM
Subject: [R] getting the unique values and counts from a vector


> Hi all,
>>From a vector, I want to get the unique values and the counts of 
>>these
> unique values in the vector. For example,
> x<-c(2 ,1 ,2, 1, 4 ,2 ,1, 4 ,1 ,1)
> xu<-unique(x)
> xn<-numeric(length(xu))
> for (i in 1:length(xu)) {xn[i]<-length(which(x==xu[i]))}
> There must be a very much simpler method of doing this. Can somebody
> direct me to the functions that I must read in order to do 
> operations of
> these sort.
> Thanks,
> Ravi Vishnu
>
>
> This message is meant for the addressee only and may contain
> confidential and legally privileged information. Any unauthorised
> review, use, copying, storage, disclosure or distribution of this e-
> mail and any attachments is strictly prohibited. If you are not the
> named recipient or have otherwise received this communication in
> error, please destroy this message from your system and kindly 
> notify
> the sender by e-mail. Thank you for your co-operation.
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From pinard at iro.umontreal.ca  Fri May 20 17:14:14 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Fri, 20 May 2005 11:14:14 -0400
Subject: [R] R annoyances
In-Reply-To: <428DEF54.9030701@lancaster.ac.uk>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E866@usctmx1106.merck.com>
	<428DEF54.9030701@lancaster.ac.uk>
Message-ID: <20050520151414.GA13305@phenix.progiciels-bpi.ca>

[Barry Rowlingson]

> Even my great dream that R and Python eventually merge into the same
> language?  R gets Python's syntax and Object-oriented functions and
> Python gets access to all R's statistical functions?

R is more than a statistical library.  I'm coming to R with a strong
Python background, and first thought I would mainly use R through
Python.  But soon, the R language revealed a few interesting features
that Python does not offer, and which are very appropriate in R context.

For example, vectorisation is built-in (yet available on the Python
side through Numeric or Numarray extensions).  R also holds interesting
(useful and flexible) ideas about argument passing and matching, lazy
evaluation, and environments.  And surely other things as well.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From wl at eimb.ru  Fri May 20 17:45:50 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Fri, 20 May 2005 19:45:50 +0400
Subject: [R] Lattice: it seems, a bug in draw.key function
In-Reply-To: <200505200955.37557.deepayan@stat.wisc.edu>
References: <1803865630.20050520160521@eimb.ru>
	<200505200918.01531.deepayan@stat.wisc.edu>
	<11710066739.20050520190529@eimb.ru>
	<200505200955.37557.deepayan@stat.wisc.edu>
Message-ID: <524219267.20050520194550@eimb.ru>


DS> Yes. It has to do with namespaces. Try ?assignInNamespace
Thank you.
It works.
I have got the desired output (---o---).

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru



From gael.robert at socgen.com  Fri May 20 17:32:26 2005
From: gael.robert at socgen.com (gael.robert@socgen.com)
Date: Fri, 20 May 2005 17:32:26 +0200
Subject: [R] constrained optimization
Message-ID: <OF5EEB8BB4.A6B39A00-ONC1257007.0054F3D4@fr.world.socgen>

Hello,
I've got to compute a minimization equation under an equality constraint
(Min g(x1,x2,x3) with x1+x2=const). The Constroptim function does not
authorize an equality condition but only inequality conditions. Which
function can I use instead?
Thank you very much for your help.
Gael Robert - +33 1 42 14 27 96



******************************************************************
This message and any attachments (the "message") are confide...{{dropped}}



From ligges at statistik.uni-dortmund.de  Fri May 20 17:43:57 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 May 2005 17:43:57 +0200
Subject: [R] address of Gordon Smyth ?
In-Reply-To: <1B71BC0D.6E242973.0D1322AF@netscape.net>
References: <1B71BC0D.6E242973.0D1322AF@netscape.net>
Message-ID: <428E05BD.8030603@statistik.uni-dortmund.de>

SuzieBlatt at netscape.net wrote:

> Anyone know where I can reach the author of the compareGrowthCurves function?

Which package are we talking about? I guess "statmod", but please 
indicate this!

For sure you have tried the address (in CC) you can find in
    library(help=statmod)

Uwe Ligges



> I'm having trouble with it.
> Thanks,
> Suzie
> 
> __________________________________________________________________
> Switch to Netscape Internet Service.
> 
> 
> 
> 
> New! Netscape Toolbar for Internet Explorer
> Search from anywhere on the Web and block those annoying pop-ups.
> Download now at http://channels.netscape.com/ns/search/install.jsp
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wl at eimb.ru  Fri May 20 17:57:01 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Fri, 20 May 2005 19:57:01 +0400
Subject: [R] getting the unique values and counts from a vector
Message-ID: <1792845550.20050520195701@eimb.ru>

Dear Ravi,
>From a vector, I want to get the unique values and the counts of these
>unique values in the vector. For example, x<-c(2,1,2,1,4,2,1,4,1,1)

try
> hist(x,plot=FALSE,breask=unique(x))$counts
[1] 5 3 0 0 0 2


--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From wl at eimb.ru  Fri May 20 18:01:53 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Fri, 20 May 2005 20:01:53 +0400
Subject: [R] getting the unique values and counts from a vector
Message-ID: <1334776056.20050520200153@eimb.ru>

Dear Ravi,

try
> hist(x,plot=FALSE,breask=unique(x))$counts
[1] 5 3 0 0 0 2

Sorry, there is a typo above. However, it works because of it.

Correct is
> hist(x,plot=FALSE)$counts
[1] 5 3 0 0 0 2

another variant
> hist(x,plot=FALSE,breaks=c(unique(x)-1,max(unique(x))))$counts
[1] 5 3 2


--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From wl at eimb.ru  Fri May 20 18:06:30 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Fri, 20 May 2005 20:06:30 +0400
Subject: [R] Why does this give a syntax error?
Message-ID: <556655916.20050520200630@eimb.ru>


Another reason could be, a too long line.
  

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru



From p.dalgaard at biostat.ku.dk  Fri May 20 17:56:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 May 2005 17:56:52 +0200
Subject: [R] address of Gordon Smyth ?
In-Reply-To: <1B71BC0D.6E242973.0D1322AF@netscape.net>
References: <1B71BC0D.6E242973.0D1322AF@netscape.net>
Message-ID: <x2zmupubzv.fsf@turmalin.kubism.ku.dk>

SuzieBlatt at netscape.net writes:

> Anyone know where I can reach the author of the compareGrowthCurves function?
> I'm having trouble with it.

Anything wrong with smyth at wehi.edu.au (1st hit on Google)?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ahenningsen at email.uni-kiel.de  Fri May 20 18:02:35 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 20 May 2005 18:02:35 +0200
Subject: [R] constrained optimization
In-Reply-To: <OF5EEB8BB4.A6B39A00-ONC1257007.0054F3D4@fr.world.socgen>
References: <OF5EEB8BB4.A6B39A00-ONC1257007.0054F3D4@fr.world.socgen>
Message-ID: <200505201802.35601.ahenningsen@email.uni-kiel.de>

On Friday 20 May 2005 17:32, gael.robert at socgen.com wrote:
> Hello,
> I've got to compute a minimization equation under an equality constraint
> (Min g(x1,x2,x3) with x1+x2=const). The Constroptim function does not
> authorize an equality condition but only inequality conditions. Which
> function can I use instead?
> Thank you very much for your help.
> Gael Robert - +33 1 42 14 27 96

You do not need constraints if you minimize
   g( x1, const - x1, x3 )
with respect to x1 and x3. After that you can calculate x2 by
   x2 = const - x1

HTH,
Arne

> ******************************************************************
> This message and any attachments (the "message") are confide...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From br44114 at gmail.com  Fri May 20 18:12:17 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 20 May 2005 12:12:17 -0400
Subject: [R] R annoyances
In-Reply-To: <Pine.LNX.4.61.0505201432440.28730@gannet.stats>
References: <8d5a3635050520062069b4c1dd@mail.gmail.com>
	<Pine.LNX.4.61.0505201432440.28730@gannet.stats>
Message-ID: <8d5a36350505200912128a91fa@mail.gmail.com>

Prof Ripley,

I'm aware of R CMD check, but who uses it? Not many regular users, I
presume. As long as T/F are allowed to stand for TRUE/FALSE without
being reserved words, there will be users who will fall in the trap.

As your example shows, some code would have to be manually converted.
Avoiding the difficult conversion may well be considered more
important than ensuring that new/intermediate R users don't get into
trouble. (After all, it's the experts who make the decisions, and who
have a large investment in existing code.) From another perspective
though, the total quantity of past, current - and especially future -
T/F mistakes made by the new members of a rapidly increasing R user
base may outweigh the difficulties the experts would have to go
through to convert their code.
b.


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Friday, May 20, 2005 9:39 AM
To: bogdan romocea
Cc: R-help at stat.math.ethz.ch
Subject: RE: [R] R annoyances


On Fri, 20 May 2005, bogdan romocea wrote:

> On 20-May-05 Uwe Ligges wrote:
>> All possible changes to T/F (both removing the meaning of
>> TRUE/FALSE in a clean session and making them reserved words)
>> would break code of lots of users.
>
> Just wanted to point out that there's another (darker) side to this:
> code that produces bad results without the users even realizing it.
> Personally, I would clearly prefer lots of broken code to mistakes
> caused by T/TRUE and F/FALSE.

You do realize that R CMD check checks for use of unassigned T/F?  So it 
would only be unchecked code which did that.

> Hypothetically, if whatever=T/F were forbidden and only
> whatever=TRUE/FALSE were allowed, all the code could be fixed with a
> simple sed script:
> for F in `ls *.r`
> do
>  mv $F $F.$$
>  sed -e 's/=T,/=TRUE,/g' -e 's/=F,/=FALSE,/g' -e 's/=T)/=TRUE)/g' -e
> 's/=F)/=FALSE)/g' $F.$$ > $F
>  rm $F.$$
> done

I assure you it is a *lot* harder than that.  Some of us use spaces for a 
start. No sed script can know the difference between

F <- "2";  as.numeric(x = F)
F <- "2";  as.numeric(x = FALSE)

(I know because I used to share code bases for S-PLUS and R, and had Perl 
scripts to do the conversion that worked for my style, but not for some 
other authors' code.)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Fri May 20 18:23:41 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 20 May 2005 12:23:41 -0400
Subject: [R] issues with identical()
Message-ID: <Pine.SGI.4.40.0505201216270.4141448-100000@origin.chass.utoronto.ca>

Hi all, hope you having a nice day,

I ahve this weird results with identical (probably I am not understanding
correctly what it does ...)

I have these two data frames and I issue :
> identical(temp, temp1)
[1] FALSE


However, these data frames are Nx2 and when I issue:
> identical(temp[,2], temp1[,2])
[1] TRUE
> identical(temp[,1], temp1[,1])
[1] TRUE

and the results from str


> str(temp)
`data.frame':	7072 obs. of  2 variables:
 $ pub_id  : int  10000 1000 10001 10002 10003 10004 10005 10006 10007
 $ faminc90: int  -2 5998 19900 43000 35000 40000 56538 61000 36000 39105
> str(temp1)
`data.frame':	7072 obs. of  2 variables:
 $ pub_id: int  10000 1000 10001 10002 10003 10004 10005 10006 10007 10008
 $ faminc: int  -2 5998 19900 43000 35000 40000 56538 61000 36000 39105

The question is why are the objects different. How else can I tell what is
the difference


Thank You

Jean



From jeaneid at chass.utoronto.ca  Fri May 20 18:25:11 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 20 May 2005 12:25:11 -0400
Subject: [R] constrained optimization
In-Reply-To: <OF5EEB8BB4.A6B39A00-ONC1257007.0054F3D4@fr.world.socgen>
Message-ID: <Pine.SGI.4.40.0505201224280.4141448-100000@origin.chass.utoronto.ca>

Why can't you just solve x1 in terms of x2 and plug it in..
ie. min g(const-x2, x2, x3)

Jean

On Fri, 20 May 2005 gael.robert at socgen.com wrote:

> Hello,
> I've got to compute a minimization equation under an equality constraint
> (Min g(x1,x2,x3) with x1+x2=const). The Constroptim function does not
> authorize an equality condition but only inequality conditions. Which
> function can I use instead?
> Thank you very much for your help.
> Gael Robert - +33 1 42 14 27 96
>
>
>
> ******************************************************************
> This message and any attachments (the "message") are confide...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jeaneid at chass.utoronto.ca  Fri May 20 18:25:43 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 20 May 2005 12:25:43 -0400
Subject: [R] getting the unique values and counts from a vector
In-Reply-To: <1792845550.20050520195701@eimb.ru>
Message-ID: <Pine.SGI.4.40.0505201225380.4141448-100000@origin.chass.utoronto.ca>

?table

On Fri, 20 May 2005, Wladimir Eremeev wrote:

> Dear Ravi,
> >From a vector, I want to get the unique values and the counts of these
> >unique values in the vector. For example, x<-c(2,1,2,1,4,2,1,4,1,1)
>
> try
> > hist(x,plot=FALSE,breask=unique(x))$counts
> [1] 5 3 0 0 0 2
>
>
> --
> Best regards
> Wladimir Eremeev                                     mailto:wl at eimb.ru
>
> ==========================================================================
> Research Scientist, PhD                           Leninsky Prospect 33,
> Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
> Institute of Ecology,                             Phone: (095) 135-9972;
> Russian Academy of Sciences                       Fax: (095) 135-9972
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From reid_huntsinger at merck.com  Fri May 20 18:16:56 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 20 May 2005 12:16:56 -0400
Subject: [R] R: looping
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A943E@uswpmx00.merck.com>

Are you sure that's what you want to do? The subscript is a logical vector
of length 3, subscripting a 3 x 3 matrix, so you're treating the matrix as a
vector (stacked columns) and recycling the indices. The first iteration
modifies 6 entries of the matrix. 

It looks like you want to replace the entries in the ith column which exceed
top[i] by top[i] (lost the ",i" in the subscript expression in copying,
perhaps). That could be done in several ways. You can either create a matrix
out of top of the same shape as z and then compare element-by-element, with
pmin for example, or use the recycling rule. That latter is cleaner if z is
transposed, but

> t(pmin(t(z),top))

works.

You could use apply as well, like

> apply(z,1,function(x) pmin(x,top))

to compare each row with the vector top, but you have to transpose the
result. I don't see any advantage to this, though. 

Reid Huntsinger




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Clark Allan
Sent: Friday, May 20, 2005 10:01 AM
To: r-help at stat.math.ethz.ch
Subject: [R] R: looping


hi all

i have a simple question. code is displayed below. 

how can i use a vectorised command in order to do this (ie replace the
loop)? (ie apply, lapply, sweep, etc)


z<-matrix(c(1:9),3,3)
top<-c(1.5,5.5,9)

for (i in 1:3)	z[z[,i]>top[i]]<-top[i]



From christophe.pouzat at univ-paris5.fr  Fri May 20 18:27:26 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Fri, 20 May 2005 18:27:26 +0200
Subject: [R] constrained optimization
In-Reply-To: <OF5EEB8BB4.A6B39A00-ONC1257007.0054F3D4@fr.world.socgen>
References: <OF5EEB8BB4.A6B39A00-ONC1257007.0054F3D4@fr.world.socgen>
Message-ID: <428E0FEE.80203@univ-paris5.fr>

gael.robert at socgen.com wrote:

>Hello,
>I've got to compute a minimization equation under an equality constraint
>(Min g(x1,x2,x3) with x1+x2=const). The Constroptim function does not
>authorize an equality condition but only inequality conditions. Which
>function can I use instead?
>  
>
Hi,

What about trying to minimize, g(x1, const-x1, x3), by defining a new 
function like:

g.new <- function(para, const) g(para[1], const-para[1], para[2])

where para would be an array with 2 components, the first corresponding 
to x1 and the second to x3. Then optim(par = my.para, fn = g.new, const 
= my.const) should do the job, where my.para is the vector of initial 
guesses for x1 and x3 and my.const is the value of x1+x2.

Does that help?

Xtof.

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From Allan at STATS.uct.ac.za  Fri May 20 18:25:19 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Fri, 20 May 2005 18:25:19 +0200
Subject: [R] R: looping
References: <D9A95B4B7B20354992E165EEADA31999056A943E@uswpmx00.merck.com>
Message-ID: <428E0F6F.3E97DBD3@STATS.uct.ac.za>

sorry everyone

the previous code seems to have been wrong. this is the corrected code
ie the last line


z<-matrix(c(1:9),3,3)
top<-c(1.5,5.5,9)

for (i in 1:3)  z[,i][z[,i]>top[i]]<-top[i]

/
allan


"Huntsinger, Reid" wrote:
> 
> Are you sure that's what you want to do? The subscript is a logical vector
> of length 3, subscripting a 3 x 3 matrix, so you're treating the matrix as a
> vector (stacked columns) and recycling the indices. The first iteration
> modifies 6 entries of the matrix.
> 
> It looks like you want to replace the entries in the ith column which exceed
> top[i] by top[i] (lost the ",i" in the subscript expression in copying,
> perhaps). That could be done in several ways. You can either create a matrix
> out of top of the same shape as z and then compare element-by-element, with
> pmin for example, or use the recycling rule. That latter is cleaner if z is
> transposed, but
> 
> > t(pmin(t(z),top))
> 
> works.
> 
> You could use apply as well, like
> 
> > apply(z,1,function(x) pmin(x,top))
> 
> to compare each row with the vector top, but you have to transpose the
> result. I don't see any advantage to this, though.
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Clark Allan
> Sent: Friday, May 20, 2005 10:01 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R: looping
> 
> hi all
> 
> i have a simple question. code is displayed below.
> 
> how can i use a vectorised command in order to do this (ie replace the
> loop)? (ie apply, lapply, sweep, etc)
> 
> z<-matrix(c(1:9),3,3)
> top<-c(1.5,5.5,9)
> 
> for (i in 1:3)  z[,i][z[,i]>top[i]]<-top[i]
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}


From murdoch at stats.uwo.ca  Fri May 20 18:35:19 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 20 May 2005 17:35:19 +0100
Subject: [R] R annoyances
In-Reply-To: <8d5a36350505200912128a91fa@mail.gmail.com>
References: <8d5a3635050520062069b4c1dd@mail.gmail.com>	<Pine.LNX.4.61.0505201432440.28730@gannet.stats>
	<8d5a36350505200912128a91fa@mail.gmail.com>
Message-ID: <428E11C7.8020003@stats.uwo.ca>

bogdan romocea wrote:
> Prof Ripley,
> 
> I'm aware of R CMD check, but who uses it? Not many regular users, I
> presume. As long as T/F are allowed to stand for TRUE/FALSE without
> being reserved words, there will be users who will fall in the trap.

If you don't use it, then you should.  Putting your code in a package is 
a good idea even if you have no intention of showing it to anyone else, 
and check will flush out a lot of errors in package code and its 
documentation.
> 
> As your example shows, some code would have to be manually converted.
> Avoiding the difficult conversion may well be considered more
> important than ensuring that new/intermediate R users don't get into
> trouble. (After all, it's the experts who make the decisions, and who
> have a large investment in existing code.) From another perspective
> though, the total quantity of past, current - and especially future -
> T/F mistakes made by the new members of a rapidly increasing R user
> base may outweigh the difficulties the experts would have to go
> through to convert their code.

Users who don't follow good practices are going to run into trouble. 
What's so surprising about that?

Duncan Murdoch



From andy_liaw at merck.com  Fri May 20 18:32:35 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 20 May 2005 12:32:35 -0400
Subject: [R] issues with identical()
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E86A@usctmx1106.merck.com>

> d1 <- data.frame(x=1:3, y=4:6)
> d2 <- data.frame(x=1:3, z=4:6)
> d3 <- data.frame(x=1:3, y=4:6)
> identical(d1, d2)
[1] FALSE
> identical(d1, d3)
[1] TRUE

Andy

> From: Jean Eid
> 
> Hi all, hope you having a nice day,
> 
> I ahve this weird results with identical (probably I am not 
> understanding
> correctly what it does ...)
> 
> I have these two data frames and I issue :
> > identical(temp, temp1)
> [1] FALSE
> 
> 
> However, these data frames are Nx2 and when I issue:
> > identical(temp[,2], temp1[,2])
> [1] TRUE
> > identical(temp[,1], temp1[,1])
> [1] TRUE
> 
> and the results from str
> 
> 
> > str(temp)
> `data.frame':	7072 obs. of  2 variables:
>  $ pub_id  : int  10000 1000 10001 10002 10003 10004 10005 10006 10007
>  $ faminc90: int  -2 5998 19900 43000 35000 40000 56538 61000 
> 36000 39105
> > str(temp1)
> `data.frame':	7072 obs. of  2 variables:
>  $ pub_id: int  10000 1000 10001 10002 10003 10004 10005 
> 10006 10007 10008
>  $ faminc: int  -2 5998 19900 43000 35000 40000 56538 61000 
> 36000 39105
> 
> The question is why are the objects different. How else can I 
> tell what is
> the difference
> 
> 
> Thank You
> 
> Jean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From murdoch at stats.uwo.ca  Fri May 20 18:42:00 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 20 May 2005 17:42:00 +0100
Subject: [R] issues with identical()
In-Reply-To: <Pine.SGI.4.40.0505201216270.4141448-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0505201216270.4141448-100000@origin.chass.utoronto.ca>
Message-ID: <428E1358.7080903@stats.uwo.ca>

Jean Eid wrote:
> Hi all, hope you having a nice day,
> 
> I ahve this weird results with identical (probably I am not understanding
> correctly what it does ...)
> 
> I have these two data frames and I issue :
> 
>>identical(temp, temp1)
> 
> [1] FALSE
> 
> 
> However, these data frames are Nx2 and when I issue:
> 
>>identical(temp[,2], temp1[,2])
> 
> [1] TRUE
> 
>>identical(temp[,1], temp1[,1])
> 
> [1] TRUE
> 
> and the results from str
> 
> 
> 
>>str(temp)
> 
> `data.frame':	7072 obs. of  2 variables:
>  $ pub_id  : int  10000 1000 10001 10002 10003 10004 10005 10006 10007
>  $ faminc90: int  -2 5998 19900 43000 35000 40000 56538 61000 36000 39105
> 
>>str(temp1)
> 
> `data.frame':	7072 obs. of  2 variables:
>  $ pub_id: int  10000 1000 10001 10002 10003 10004 10005 10006 10007 10008
>  $ faminc: int  -2 5998 19900 43000 35000 40000 56538 61000 36000 39105
> 
> The question is why are the objects different. How else can I tell what is
> the difference

There could be a difference in the attributes (e.g. rownames of the 
dataframe, or names of one of the columns).

One thing that might turn up the difference is to use dump() to write 
out the objects to a file, and use some text-based compare on those files.

Duncan Murdoch



From p.dalgaard at biostat.ku.dk  Fri May 20 18:45:16 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 May 2005 18:45:16 +0200
Subject: [R] issues with identical()
In-Reply-To: <Pine.SGI.4.40.0505201216270.4141448-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0505201216270.4141448-100000@origin.chass.utoronto.ca>
Message-ID: <x2vf5du9r7.fsf@turmalin.kubism.ku.dk>

Jean Eid <jeaneid at chass.utoronto.ca> writes:

> > str(temp)
> `data.frame':	7072 obs. of  2 variables:
>  $ pub_id  : int  10000 1000 10001 10002 10003 10004 10005 10006 10007
>  $ faminc90: int  -2 5998 19900 43000 35000 40000 56538 61000 36000 39105
> > str(temp1)
> `data.frame':	7072 obs. of  2 variables:
>  $ pub_id: int  10000 1000 10001 10002 10003 10004 10005 10006 10007 10008
>  $ faminc: int  -2 5998 19900 43000 35000 40000 56538 61000 36000 39105
> 
> The question is why are the objects different. How else can I tell what is
> the difference

Just look more carefully. The _names_ differ.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri May 20 18:51:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 May 2005 17:51:35 +0100 (BST)
Subject: [R] issues with identical()
In-Reply-To: <Pine.SGI.4.40.0505201216270.4141448-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0505201216270.4141448-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.LNX.4.61.0505201746300.30711@gannet.stats>

On Fri, 20 May 2005, Jean Eid wrote:

> Hi all, hope you having a nice day,
>
> I ahve this weird results with identical (probably I am not understanding
> correctly what it does ...)

Why should

     a data frame with colunns pub_id faminc90
     a data frame with colunns pub_id faminc

be considered identical()?  Its description is

      The safe and reliable way to test two objects for being _exactly_
      equal.

and those are not equal in a critical way.

> I have these two data frames and I issue :
>> identical(temp, temp1)
> [1] FALSE
>
>
> However, these data frames are Nx2 and when I issue:
>> identical(temp[,2], temp1[,2])
> [1] TRUE
>> identical(temp[,1], temp1[,1])
> [1] TRUE
>
> and the results from str
>
>
>> str(temp)
> `data.frame':	7072 obs. of  2 variables:
> $ pub_id  : int  10000 1000 10001 10002 10003 10004 10005 10006 10007
> $ faminc90: int  -2 5998 19900 43000 35000 40000 56538 61000 36000 39105
>> str(temp1)
> `data.frame':	7072 obs. of  2 variables:
> $ pub_id: int  10000 1000 10001 10002 10003 10004 10005 10006 10007 10008
> $ faminc: int  -2 5998 19900 43000 35000 40000 56538 61000 36000 39105
>
> The question is why are the objects different. How else can I tell what is
> the difference

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From david.meyer at wu-wien.ac.at  Fri May 20 18:54:34 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Fri, 20 May 2005 18:54:34 +0200
Subject: [R] tune.svm in {e1071}
Message-ID: <20050520185434.6c43a18b.david.meyer@wu-wien.ac.at>

Amir,

>Dear All ,
>1- I'm trying to access  the values of  fitted(model) after   model<-
>tune.svm( ) but seemingly it is >not poosible. How can I access to
>values of fitted ? However ,it is possible only after  model<- svm( ) 

tune.svm() is a wrapper to tune() and as such returns a tune-object.
That one _includes_ a "best.model" component containing the "svm"
object. So you want sth. like:

tuneobj <- tune.svm(...)
model <- tuneobj$best.model
summary(model)

etc.
 
>2- How can I access to the other values such as the number of Support
>Vectors , gamma, cost , nu , >epsilon , after   model<- tune.svm( ) ?
>these are not possible? I receive only  "Error estimation of 'svm' " 
>with   model and summary(model) functions.

Clear from the above, I think.

HTH,
David

>Best Wishes and so many thanks,
>Amir

-- 
Dr. David Meyer
Department of Information Systems and Process Management

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From wuming.gong at gmail.com  Fri May 20 19:54:27 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Sat, 21 May 2005 01:54:27 +0800
Subject: [R] covariance analysis by using R
In-Reply-To: <316431039.19478@capitalbio.com>
References: <316431039.19478@capitalbio.com>
Message-ID: <b428d06d0505201054d82d342@mail.gmail.com>

You may fit the model using lm() directly - R will set up a coding for
qualitative predictor automatically (taking experiments as qualitative
predictor).

HTH

Wuming


On 5/18/05, Å©sÅ™Y <xmeng at capitalbio.com> wrote:
> Hello sir:
> Here's a question on covariance analysis which needs your help.
> There're 3 experiments,and x refers to control while y refers to experimental result.
> The purpose is to compare the "y" values across the 3 experiments.
> 
> experiment_1:
> x:0.1 0.2 0.3 0.4 0.5
> y:0.5 0.6 0.6 0.7 0.9
> 
> experiment_2:
> x:1 2 3   4   5
> y:3 4 6.5 7.5 11
> 
> experiment_3:
> x:10 20 30 40 50
> y:18 35 75 90 98
> 
> Apparently,the control("x") isn't at the similar level so that we can't compare the "y" directly through ANOVA.
> We must normalize "y" via "x" in order to eliminate the influence of  different level of "x".
> The method of normalize I can get is "covariance analysis",since "x" is the covariant of y.
> 
> My question is:
> How to perform "covariance analysis" by using R?
> After this normalization,we can get the according "normalized y" of every "original y".
> 
> All in all,the "normalized y" of every "original y" is what I want indeed.
> 
> 
> Thanks a lot!
> 
> My best regards!
> 
> 
> 
> 
> 
> 
> ------------------------------
> *******************************************
> Xin Meng
> Capitalbio Corporation
> National Engineering Research Center
> for Beijing Biochip Technology
> Microarray and Bioinformatics Dept.
> Research Engineer
> Tel: +86-10-80715888/80726868-6364/6333
> Fax: +86-10-80726790
> EmailÅ°Gxmeng at capitalbio.com
> Address:18 Life Science Parkway,
> Changping District, Beijing 102206, China
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From choudary.jagar at swosu.edu  Fri May 20 19:58:34 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Fri, 20 May 2005 12:58:34 -0500
Subject: [R] comparing a vactor of values in IF statement.
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C227@swosu-mbx01.admin.swosu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050520/fd0415b4/attachment.pl

From ggrothendieck at gmail.com  Fri May 20 20:07:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 20 May 2005 14:07:22 -0400
Subject: [R] comparing a vactor of values in IF statement.
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C227@swosu-mbx01.admin.swosu.edu>
References: <E03EBB50FF2C024781A6E4460AD58F0607C227@swosu-mbx01.admin.swosu.edu>
Message-ID: <971536df0505201107f180bef@mail.gmail.com>

Try

  V <- ifelse(V > 0.5, 1-V, V)

or 

  V <- pmin(V, 1-V)


On 5/20/05, Jagarlamudi, Choudary <choudary.jagar at swosu.edu> wrote:
> Hi,
> 
>  my vector V<- c(1,0.5,0.06,0.056,0.01,0.04,0.4,0.9,0.82,0.1)
> 
>  if( V > 0.5) { V <- 1 - V }
> 
> I get  a warning saying only the first element will be used in comparing (if V > 0.5).
> However, my results tell me vis-versa ,it actually compares every element of the vector V with 0.5 and that is waht i want it to do.
> Using a for loop is expensive and time consuming since my actual vector is 15000 values.
> 
> Any help to rectify the warning i get is appreciated.
> 
> Thank You.
> 
> Choudary Jagarlamudi
> Instructor
> Southwestern Oklahoma State University
> STF 254
> 100 campus Drive
> Weatherford OK 73096
> Tel 580-774-7136
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rvaradha at jhsph.edu  Fri May 20 20:09:04 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 20 May 2005 14:09:04 -0400
Subject: [R] comparing a vactor of values in IF statement.
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C227@swosu-mbx01.admin.swosu.edu>
Message-ID: <OWA-2jtbr1Rjj7wiECi000094fb@owa-2.sph.ad.jhsph.edu>

Both

V[V > 0.5] <- 1 - V[V > 0.5]

and 

ifelse(V>0.5, 1-V,V)

should do it.

Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Jagarlamudi, Choudary
> Sent: Friday, May 20, 2005 1:59 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] comparing a vactor of values in IF statement.
> 
> Hi,
> 
>  my vector V<- c(1,0.5,0.06,0.056,0.01,0.04,0.4,0.9,0.82,0.1)
> 
>   if( V > 0.5) { V <- 1 - V }
> 
> I get  a warning saying only the first element will be used in comparing
> (if V > 0.5).
> However, my results tell me vis-versa ,it actually compares every element
> of the vector V with 0.5 and that is waht i want it to do.
> Using a for loop is expensive and time consuming since my actual vector is
> 15000 values.
> 
> Any help to rectify the warning i get is appreciated.
> 
> Thank You.
> 
> Choudary Jagarlamudi
> Instructor
> Southwestern Oklahoma State University
> STF 254
> 100 campus Drive
> Weatherford OK 73096
> Tel 580-774-7136
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From jari.oksanen at oulu.fi  Fri May 20 20:29:15 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Fri, 20 May 2005 21:29:15 +0300
Subject: [R] R annoyances
In-Reply-To: <428DE7FE.50102@sciviews.org>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E860@usctmx1106.merck.com>
	<20050520134257.GE29990@jtkpc.cmp.uea.ac.uk>
	<428DE7FE.50102@sciviews.org>
Message-ID: <fb4621d63226a5b716328c07f9c4c1d0@oulu.fi>


On 20 May 2005, at 16:37, Philippe Grosjean wrote:

>
> The only aspect I don't like is a too loosely use of the dot in 
> functions: both in functions names, in object classes and in generic 
> functions / methods. Hence, we have for instance: 'data.frame', 
> 'help.search' and 'summary.matrix'... just guess which one is an 
> object class, which one is an ordinary function and which one is a S3 
> method (OK, S4 solves somehow the problem)? It would have been much 
> better to *reserve* the use of a dot in a function name as a separator 
> between the name of the generic function and the class to which it 
> applies. Thus, 'summary.matrix' would have been correct, but both 
> 'data.frame' and 'help.search' should have been spelled differently, 
> perhaps 'dataframe' and 'helpSearch'. Just a dream... because 
> 'data.frame' will of course never be spelled differently!!!

The most beautiful thing in old R (I started with 0.63) was that it was 
in the elegant unix tradition: all lower case and point (full stop, 
period, whatever) in places where you needed it. It is unfortunate that 
other languages are creeping in and old neat constructions are replaces 
with C++ style uGliNess. There was a grace period when switching from 
beautiful (fair) print.coefmat to ugLy printCoefmat, but some changes 
were more abrupt (package.description). I have a feeling that the 
recent trashing of names.dist (with a lot of code breakage even in base 
R) was caused by the same kind of political correctness.

Please Mr R, keep it like it used to be...

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From kyong.ho.park at us.army.mil  Fri May 20 21:12:43 2005
From: kyong.ho.park at us.army.mil (Park, Kyong H Mr. RDECOM)
Date: Fri, 20 May 2005 15:12:43 -0400
Subject: [R] segmented regression
Message-ID: <3007F52DF96EB74CAC3054FF85C4318F7A7C56@mail2>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050520/823c3187/attachment.pl

From alex.j.k at gmail.com  Fri May 20 21:22:23 2005
From: alex.j.k at gmail.com (Alex K)
Date: Fri, 20 May 2005 15:22:23 -0400
Subject: [R] Reading Numeric Data -- Trivial Question
Message-ID: <4f2ab54805052012229dd7f89@mail.gmail.com>

Hello,

   I am very new to R, and this is certainly and uber-newby question:

   I am trying to read a vector of numeric data that contains the log
of daily DJI returns and simply plot a histogram of it.

   The data I have is in a text file format, on each line a number
represents the log of the returns of one day.

   I have problems in reading this in a vector numeric format.

If I try

retr<- as.numeric(read.table("logDJI.TXT", header=FALSE, sep="", dec="." )); 

I get:
" Error in as.double.default(read.table("logDJIm.TXT", header = FALSE,
sep = "",  :
        (list) object cannot be coerced to 'double'"

and when I try to plot:

 plot(density(retr, width=.004), type="l", xlab="y", ylab="pdf");

I get:

"Error in density(retr, width = 0.004) : argument 'x' must be numeric"

  If I try:

retr<- as.data.frame(read.table("logDJI.TXT", header=FALSE, sep="", dec="." )); 

I get no reading or conversion error, but I get the same error when I
try to plot as above.

Can anyone help with this?

Thank you in advance,
Alex



From matt.j.oliver at gmail.com  Fri May 20 21:46:56 2005
From: matt.j.oliver at gmail.com (Matt Oliver)
Date: Fri, 20 May 2005 15:46:56 -0400
Subject: [R] bootstrapping vectors of unequal length
Message-ID: <6d4aad4605052012466a9b05a6@mail.gmail.com>

Dear R Help List,

I have a vector of n and a vector of n-1 and I want to use boot() to
bootstrap the ratio of their respective medians. I want to eventually
use boot.ci() to generate confidence intervals. Because the vectors
are not equal in length, I tried a few things, but have yet to be
successful.

Try 1: 

> x <- runif(20)
> 
> y <- c(runif(19), NA)
> 
> median(x)
[1] 0.522284
> 
> median(y[1:19])
[1] 0.488046
> 
> median(x)/median((y)[1:19])
[1] 1.070153
> 
> t <- as.data.frame(cbind(x, y))
> 
> ratio <- function(t, i) median(t$x[i])/median((t$y[1:19])[i])
> 
> boot(t, ratio, R = 1000)

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = t, statistic = ratio, R = 1000)


Bootstrap Statistics :
    original  bias    std. error
t1*       NA      NA   0.4603294


I thought this might be successful because median(x)/median((y)[1:19])
gives a result, and not an NA.

I also tried to use a regular list (even though boot() technically
doesn't accept them) so I didn't have to use NA.

Try 2:

> x <- runif(20)
> 
> y <- runif(19)
> 
> median(x)
[1] 0.732906
> 
> median(y)
[1] 0.5596225
> 
> median(x)/median(y)
[1] 1.309644
> 
> t <- list(x = x, y = y)
> 
> ratio <- function(t, i) median(t$x[i])/median(t$y[i])
> 
> boot(t, ratio, R = 1000)

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = t, statistic = ratio, R = 1000)


Bootstrap Statistics :
    original       bias    std. error
t1* 1.153598 -0.004907764  0.08266257


At first glance this seemed to work, but median(x)/median(y) is not
equal to the "original" in the Bootstrap Statistics (which is a bit
odd to me, but it may be because boot() doesn't accept this kind of
list.)


Is there a way to do this type of bootstrap with the boot() function?

Thanks in advance

Matt Oliver



From peter.rossi at gsb.uchicago.edu  Fri May 20 18:51:12 2005
From: peter.rossi at gsb.uchicago.edu (Peter E. Rossi)
Date: Fri, 20 May 2005 11:51:12 -0500
Subject: [R] [R-pkgs] Version 1.0-1 of bayesm
Message-ID: <32dfe232ac71.32ac7132dfe2@gsb.uchicago.edu>


Version 1.0-1 of bayesm is now available on CRAN.

This is our first "production" version which include s much improved documentation as well as five data sets used in our book, Bayesian Statistics and Marketing.

peter r



................................
 Peter E. Rossi
 Joseph T. and Bernice S. Lewis Professor of Marketing and Statistics
 Editor, Quantitative Marketing and Economics
 Rm 360, Graduate School of Business, U of Chicago
 5807 S. Woodlawn Ave, Chicago IL 60637
 Tel: (773) 702-7513   |   Fax: (773) 834-2081

 peter.rossi at ChicagoGsb.edu
 WWW: http://ChicagoGsb.edu/fac/peter.rossi
SSRN: http://ssrn.com/author=22862
 QME: http://www.kluweronline.com/issn/1570-7156

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ligges at statistik.uni-dortmund.de  Fri May 20 22:23:35 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 May 2005 22:23:35 +0200
Subject: [R] Reading Numeric Data -- Trivial Question
In-Reply-To: <4f2ab54805052012229dd7f89@mail.gmail.com>
References: <4f2ab54805052012229dd7f89@mail.gmail.com>
Message-ID: <428E4747.50109@statistik.uni-dortmund.de>

Alex K wrote:

> Hello,
> 
>    I am very new to R, and this is certainly and uber-newby question:
> 
>    I am trying to read a vector of numeric data that contains the log
> of daily DJI returns and simply plot a histogram of it.
> 
>    The data I have is in a text file format, on each line a number
> represents the log of the returns of one day.
> 
>    I have problems in reading this in a vector numeric format.
> 
> If I try
> 
> retr<- as.numeric(read.table("logDJI.TXT", header=FALSE, sep="", dec="." )); 


read.table() returns a data.frame, as the docs point out, but as.numeric 
expects a vector.

Either extract the first (and only?) column or read the data using 
scan(), if there is really only one column.

Uwe Ligges


> I get:
> " Error in as.double.default(read.table("logDJIm.TXT", header = FALSE,
> sep = "",  :
>         (list) object cannot be coerced to 'double'"
> 
> and when I try to plot:
> 
>  plot(density(retr, width=.004), type="l", xlab="y", ylab="pdf");
> 
> I get:
> 
> "Error in density(retr, width = 0.004) : argument 'x' must be numeric"
> 
>   If I try:
> 
> retr<- as.data.frame(read.table("logDJI.TXT", header=FALSE, sep="", dec="." )); 
> 
> I get no reading or conversion error, but I get the same error when I
> try to plot as above.
> 
> Can anyone help with this?
> 
> Thank you in advance,
> Alex
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From alex.j.k at gmail.com  Fri May 20 22:25:53 2005
From: alex.j.k at gmail.com (Alex K)
Date: Fri, 20 May 2005 16:25:53 -0400
Subject: [R] Reading Numeric Data -- Trivial Question
In-Reply-To: <428E4747.50109@statistik.uni-dortmund.de>
References: <4f2ab54805052012229dd7f89@mail.gmail.com>
	<428E4747.50109@statistik.uni-dortmund.de>
Message-ID: <4f2ab54805052013251c23ce75@mail.gmail.com>

On 5/20/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Alex K wrote:
> 
> > Hello,
> >
> >    I am very new to R, and this is certainly and uber-newby question:
> >
> >    I am trying to read a vector of numeric data that contains the log
> > of daily DJI returns and simply plot a histogram of it.
> >
> >    The data I have is in a text file format, on each line a number
> > represents the log of the returns of one day.
> >
> >    I have problems in reading this in a vector numeric format.
> >
> > If I try
> >
> > retr<- as.numeric(read.table("logDJI.TXT", header=FALSE, sep="", dec="." ));
> 
> 
> read.table() returns a data.frame, as the docs point out, but as.numeric
> expects a vector.
> 
> Either extract the first (and only?) column or read the data using
> scan(), if there is really only one column.
> 
> Uwe Ligges

  Hi Uwe, thank you for the reply.

  How do I extract only the colum?

The text file looks like:
"-0.000948343416257784
0.00181733586025109
0
0.0108130251220595"

except is has about 2200 entries (numbers) instead of just four as above.

Thank you again,
Alex



> 
> 
> > I get:
> > " Error in as.double.default(read.table("logDJIm.TXT", header = FALSE,
> > sep = "",  :
> >         (list) object cannot be coerced to 'double'"
> >
> > and when I try to plot:
> >
> >  plot(density(retr, width=.004), type="l", xlab="y", ylab="pdf");
> >
> > I get:
> >
> > "Error in density(retr, width = 0.004) : argument 'x' must be numeric"
> >
> >   If I try:
> >
> > retr<- as.data.frame(read.table("logDJI.TXT", header=FALSE, sep="", dec="." ));
> >
> > I get no reading or conversion error, but I get the same error when I
> > try to plot as above.
> >
> > Can anyone help with this?
> >
> > Thank you in advance,
> > Alex
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Fri May 20 22:37:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 May 2005 22:37:20 +0200
Subject: [R] bootstrapping vectors of unequal length
In-Reply-To: <6d4aad4605052012466a9b05a6@mail.gmail.com>
References: <6d4aad4605052012466a9b05a6@mail.gmail.com>
Message-ID: <428E4A80.4020004@statistik.uni-dortmund.de>

Matt Oliver wrote:

> Dear R Help List,
> 
> I have a vector of n and a vector of n-1 and I want to use boot() to
> bootstrap the ratio of their respective medians. I want to eventually
> use boot.ci() to generate confidence intervals. Because the vectors
> are not equal in length, I tried a few things, but have yet to be
> successful.
> 
> Try 1: 
> 
> 
>>x <- runif(20)
>>
>>y <- c(runif(19), NA)
>>
>>median(x)
> 
> [1] 0.522284
> 
>>median(y[1:19])
> 
> [1] 0.488046
> 
>>median(x)/median((y)[1:19])
> 
> [1] 1.070153
> 
>>t <- as.data.frame(cbind(x, y))
>>
>>ratio <- function(t, i) median(t$x[i])/median((t$y[1:19])[i])
>>
>>boot(t, ratio, R = 1000)
> 
> 
> ORDINARY NONPARAMETRIC BOOTSTRAP
> 
> 
> Call:
> boot(data = t, statistic = ratio, R = 1000)
> 
> 
> Bootstrap Statistics :
>     original  bias    std. error
> t1*       NA      NA   0.4603294
> 
> 
> I thought this might be successful because median(x)/median((y)[1:19])
> gives a result, and not an NA.
> 
> I also tried to use a regular list (even though boot() technically
> doesn't accept them) so I didn't have to use NA.
> 
> Try 2:
> 
> 
>>x <- runif(20)
>>
>>y <- runif(19)
>>
>>median(x)
> 
> [1] 0.732906
> 
>>median(y)
> 
> [1] 0.5596225
> 
>>median(x)/median(y)
> 
> [1] 1.309644
> 
>>t <- list(x = x, y = y)
>>
>>ratio <- function(t, i) median(t$x[i])/median(t$y[i])
>>
>>boot(t, ratio, R = 1000)
> 
> 
> ORDINARY NONPARAMETRIC BOOTSTRAP
> 
> 
> Call:
> boot(data = t, statistic = ratio, R = 1000)
> 
> 
> Bootstrap Statistics :
>     original       bias    std. error
> t1* 1.153598 -0.004907764  0.08266257
> 
> 
> At first glance this seemed to work, but median(x)/median(y) is not
> equal to the "original" in the Bootstrap Statistics (which is a bit
> odd to me, but it may be because boot() doesn't accept this kind of
> list.)
> 
> 
> Is there a way to do this type of bootstrap with the boot() function?
> 
> Thanks in advance
> 
> Matt Oliver
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


The question is always how to handle NA, obviously you do NOT want to 
throw away whole observations including NAs from your data.frame, but 
only throw away NAs for each vector separately.

Your idea specifying "median((t$y[1:19])[i])" does not work, because the 
statistic is evaluated on subsets of t, hence you get for the subset of 
observations 1:5: t[1:5,]$y[1:19], and you have even more NAs

Instead, you want to use median()'s na.rm argument as in:

ratio <- function(t, i)
     median(t$x[i], na.rm=TRUE) / median(t$y[i], na.rm=TRUE)

Uwe Ligges



From alex.j.k at gmail.com  Fri May 20 22:35:18 2005
From: alex.j.k at gmail.com (Alex K)
Date: Fri, 20 May 2005 16:35:18 -0400
Subject: [R] Reading Numeric Data -- Trivial Question
In-Reply-To: <428E4601.3010407@pburns.seanet.com>
References: <4f2ab54805052012229dd7f89@mail.gmail.com>
	<428E4601.3010407@pburns.seanet.com>
Message-ID: <4f2ab54805052013356e00c8f0@mail.gmail.com>

Hi Patrick, thank you for your reply,

On 5/20/05, Patrick Burns <pburns at pburns.seanet.com> wrote:
> Two minor points:
> 
> 1)  You surely have logarithmic returns rather than the log of (some
> type of) returns.
> 

  Yes, obviously, my mistake.

> 2)  Once you get data suitable for a histogram, do:
> 
> hist(retr)
>

  I tried that but hist() also expects a numeric vector, and that's
what I can't find out how to do: extracting the data in a suitable
form from the file.

  On "retr;"

I get :
"                V1
1    -9.483434e-04
2     1.817336e-03
3     0.000000e+00
4     1.081303e-02"

and so on until 2527, which is the number of entries.


Thank you again for your reply,
Alex



From bxc at steno.dk  Fri May 20 22:35:48 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 20 May 2005 22:35:48 +0200
Subject: [R] Reading Numeric Data -- Trivial Question
Message-ID: <40D3930AC1C8EA469E39536E5BC8083545AAFF@EXDKBA021.corp.novocorp.net>

You (and the mailing list) would defintely benefit from cliking on:

Help -> Manuals -> An introduction to R

and spend a few hours in frot of R while reading that.

Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alex K
> Sent: Friday, May 20, 2005 11:26 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Reading Numeric Data -- Trivial Question
> 
> 
> On 5/20/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > Alex K wrote:
> > 
> > > Hello,
> > >
> > >    I am very new to R, and this is certainly and uber-newby 
> > > question:
> > >
> > >    I am trying to read a vector of numeric data that contains the 
> > > log of daily DJI returns and simply plot a histogram of it.
> > >
> > >    The data I have is in a text file format, on each line 
> a number 
> > > represents the log of the returns of one day.
> > >
> > >    I have problems in reading this in a vector numeric format.
> > >
> > > If I try
> > >
> > > retr<- as.numeric(read.table("logDJI.TXT", header=FALSE, sep="", 
> > > dec="." ));
> > 
> > 
> > read.table() returns a data.frame, as the docs point out, but 
> > as.numeric expects a vector.
> > 
> > Either extract the first (and only?) column or read the data using 
> > scan(), if there is really only one column.
> > 
> > Uwe Ligges
> 
>   Hi Uwe, thank you for the reply.
> 
>   How do I extract only the colum?
> 
> The text file looks like:
> "-0.000948343416257784
> 0.00181733586025109
> 0
> 0.0108130251220595"
> 
> except is has about 2200 entries (numbers) instead of just 
> four as above.
> 
> Thank you again,
> Alex
> 
> 
> 
> > 
> > 
> > > I get:
> > > " Error in as.double.default(read.table("logDJIm.TXT", header = 
> > > FALSE, sep = "",  :
> > >         (list) object cannot be coerced to 'double'"
> > >
> > > and when I try to plot:
> > >
> > >  plot(density(retr, width=.004), type="l", xlab="y", ylab="pdf");
> > >
> > > I get:
> > >
> > > "Error in density(retr, width = 0.004) : argument 'x' must be 
> > > numeric"
> > >
> > >   If I try:
> > >
> > > retr<- as.data.frame(read.table("logDJI.TXT", 
> header=FALSE, sep="", 
> > > dec="." ));
> > >
> > > I get no reading or conversion error, but I get the same 
> error when 
> > > I try to plot as above.
> > >
> > > Can anyone help with this?
> > >
> > > Thank you in advance,
> > > Alex
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Fri May 20 22:44:46 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 20 May 2005 16:44:46 -0400
Subject: [R] Big-endian / Little-endian byte swap
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4074@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050520/800adfb9/attachment.pl

From ligges at statistik.uni-dortmund.de  Fri May 20 22:49:33 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 May 2005 22:49:33 +0200
Subject: [R] Reading Numeric Data -- Trivial Question
In-Reply-To: <4f2ab54805052013251c23ce75@mail.gmail.com>
References: <4f2ab54805052012229dd7f89@mail.gmail.com>	<428E4747.50109@statistik.uni-dortmund.de>
	<4f2ab54805052013251c23ce75@mail.gmail.com>
Message-ID: <428E4D5D.8070301@statistik.uni-dortmund.de>

Alex K wrote:

> On 5/20/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Alex K wrote:
>>
>>
>>>Hello,
>>>
>>>   I am very new to R, and this is certainly and uber-newby question:
>>>
>>>   I am trying to read a vector of numeric data that contains the log
>>>of daily DJI returns and simply plot a histogram of it.
>>>
>>>   The data I have is in a text file format, on each line a number
>>>represents the log of the returns of one day.
>>>
>>>   I have problems in reading this in a vector numeric format.
>>>
>>>If I try
>>>
>>>retr<- as.numeric(read.table("logDJI.TXT", header=FALSE, sep="", dec="." ));
>>
>>
>>read.table() returns a data.frame, as the docs point out, but as.numeric
>>expects a vector.
>>
>>Either extract the first (and only?) column or read the data using
>>scan(), if there is really only one column.
>>
>>Uwe Ligges
> 
> 
>   Hi Uwe, thank you for the reply.
> 
>   How do I extract only the colum?
> 
> The text file looks like:
> "-0.000948343416257784
> 0.00181733586025109
> 0
> 0.0108130251220595"
> 
> except is has about 2200 entries (numbers) instead of just four as above.

Please, DO read the posting guide (cited at the bottom of each message)!
There is a manual "An Introduction to R". Please read it!
There is a manual on Data Import and Export. Please read it!

Really, why don't you read up on the hints you got in my former message? 
Why do you waste bandwith and our time with an unnecessary followup?

As I said, extract the first column of your data.frame. Hint: there are 
index operators such as in MyDataFrame[,1] or MyDataFrame[[1]].
Or use scan(). What's so difficult to read ?scan and try to apply it?

Uwe Ligges





> Thank you again,
> Alex
> 
> 
> 
> 
>>
>>>I get:
>>>" Error in as.double.default(read.table("logDJIm.TXT", header = FALSE,
>>>sep = "",  :
>>>        (list) object cannot be coerced to 'double'"
>>>
>>>and when I try to plot:
>>>
>>> plot(density(retr, width=.004), type="l", xlab="y", ylab="pdf");
>>>
>>>I get:
>>>
>>>"Error in density(retr, width = 0.004) : argument 'x' must be numeric"
>>>
>>>  If I try:
>>>
>>>retr<- as.data.frame(read.table("logDJI.TXT", header=FALSE, sep="", dec="." ));
>>>
>>>I get no reading or conversion error, but I get the same error when I
>>>try to plot as above.
>>>
>>>Can anyone help with this?
>>>
>>>Thank you in advance,
>>>Alex
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ogabbrie at tin.it  Fri May 20 23:38:25 2005
From: ogabbrie at tin.it (simone gabbriellini)
Date: Fri, 20 May 2005 23:38:25 +0200
Subject: [R] laten class analysis
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E85F@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E85F@usctmx1106.merck.com>
Message-ID: <f4d57bf30ea81676304274a39762c19b@tin.it>

thank you Andy

Simone

Il giorno 20/mag/05, alle 12:37, Liaw, Andy ha scritto:

> You have to have the package installed for help.search() to find  
> things in
> it.  You could try
>
>   RSiteSearch("latent class", restrict="function")
>
> Andy
>
>> From: Simone gabbriellini
>>
>> my help.search didn't give me any result :(
>> the one you suggest should be right what I need
>>
>> thank you,
>> simone
>>
>> Il giorno 20/mag/05, alle 00:33, Chuck Cleland ha scritto:
>>
>>> help.search("latent class") shows lca() in the e1071 package.
>>>
>>> simone gabbriellini wrote:
>>>> Dear List,
>>>> just a little question,
>>>> I am interested in Latent Class Analysis and
>>>> I guess if there is a package for this pourpose
>>>> thank for you help,
>>>> Simone
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>
>>> --  
>>> Chuck Cleland, Ph.D.
>>> NDRI, Inc.
>>> 71 West 23rd Street, 8th floor
>>> New York, NY 10010
>>> tel: (212) 845-4495 (Tu, Th)
>>> tel: (732) 452-1424 (M, W, F)
>>> fax: (917) 438-0894
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>
>
>
> ----------------------------------------------------------------------- 
> -------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From todd.taylor at pnl.gov  Fri May 20 23:37:56 2005
From: todd.taylor at pnl.gov (Taylor, Z Todd)
Date: Fri, 20 May 2005 14:37:56 -0700
Subject: [R] R annoyances
Message-ID: <656E0E8676B66C4CAE912DDDB0B5BD1EFA6C19@pnlmse24.pnl.gov>

On Friday, May 20, 2005 11:29 AM, Jari Oksanen wrote:

> The most beautiful thing in old R (I started with 0.63) was 
> that it was 
> in the elegant unix tradition: all lower case and point (full stop, 
> period, whatever) in places where you needed it. It is 
> unfortunate that 
> other languages are creeping in and old neat constructions 
> are replaces 
> with C++ style uGliNess. There was a grace period when switching from 
> beautiful (fair) print.coefmat to ugLy printCoefmat, but some changes 
> were more abrupt (package.description). I have a feeling that the 
> recent trashing of names.dist (with a lot of code breakage 
> even in base 
> R) was caused by the same kind of political correctness.
> 
> Please Mr R, keep it like it used to be...

BecauseIt'sFridayI'llJoinIntoThisTiredOldDebate.TheUseOfMixedCase
UglinessToDistinguishBetween"Words"InObjectNamesIsIndeedAn
Abomination.

It_is_much_easier,_and_demonstrably_so_I_would_say,_to_use
some_kind_of_real_separator_between_"words."

S.and.R.have.historically.encouraged.a.different.separator
than.most.other.languages,.but.the.principle.is.the.same.

And the double benefit is that it leaves case available for
other good uses, such as indicating an object's scope:

    local.var
    Class.Data
    GLOBAL.SETTING

--Todd
-- 
Z. Todd Taylor
Senior Development Engineer
Pacific Northwest National Laboratory
Why is it "after dark," not "after light"?



From murdoch at stats.uwo.ca  Sat May 21 00:05:49 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 20 May 2005 23:05:49 +0100
Subject: [R] R annoyances
In-Reply-To: <656E0E8676B66C4CAE912DDDB0B5BD1EFA6C19@pnlmse24.pnl.gov>
References: <656E0E8676B66C4CAE912DDDB0B5BD1EFA6C19@pnlmse24.pnl.gov>
Message-ID: <428E5F3D.5040506@stats.uwo.ca>

Taylor, Z Todd wrote:
> On Friday, May 20, 2005 11:29 AM, Jari Oksanen wrote:
> 
> 
>>The most beautiful thing in old R (I started with 0.63) was 
>>that it was 
>>in the elegant unix tradition: all lower case and point (full stop, 
>>period, whatever) in places where you needed it. It is 
>>unfortunate that 
>>other languages are creeping in and old neat constructions 
>>are replaces 
>>with C++ style uGliNess. There was a grace period when switching from 
>>beautiful (fair) print.coefmat to ugLy printCoefmat, but some changes 
>>were more abrupt (package.description). I have a feeling that the 
>>recent trashing of names.dist (with a lot of code breakage 
>>even in base 
>>R) was caused by the same kind of political correctness.
>>
>>Please Mr R, keep it like it used to be...
> 
> 
> BecauseIt'sFridayI'llJoinIntoThisTiredOldDebate.TheUseOfMixedCase
> UglinessToDistinguishBetween"Words"InObjectNamesIsIndeedAn
> Abomination.
> 
> It_is_much_easier,_and_demonstrably_so_I_would_say,_to_use
> some_kind_of_real_separator_between_"words."
> 
> S.and.R.have.historically.encouraged.a.different.separator
> than.most.other.languages,.but.the.principle.is.the.same.

Using an underscore will break old versions of R, and some versions of 
S-PLUS.

Using a dot will break S3 method dispatch.

Using mixed case will lead to complaints from people who don't mind 
breaking one of those things.

What's a package author to do??

I suggest that we redefine "+" to be a legal character in the name of an 
identifier.  Sure, if you used to have "a+b" you'll have to rewrite it 
as "+(a,b)", but think of the benefits! 
You+can+write+out+long+sentences+and+use+them+as+variable+names!

Duncan Murdoch
> 
> And the double benefit is that it leaves case available for
> other good uses, such as indicating an object's scope:
> 
>     local.var
>     Class.Data
>     GLOBAL.SETTING
> 
> --Todd



From bxc at steno.dk  Sat May 21 07:53:51 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Sat, 21 May 2005 07:53:51 +0200
Subject: [R] print format for difftime
Message-ID: <40D3930AC1C8EA469E39536E5BC8083545AB02@EXDKBA021.corp.novocorp.net>

Has anyone written a function that will print a difftime in the form:

hh:mm:ss

or

yy-mm-dd hh:mm:ss

depending on the actual size.
(sloppy notation for months/minutes, but surely you get the point).

Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From phgrosjean at sciviews.org  Sat May 21 09:07:21 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sat, 21 May 2005 09:07:21 +0200
Subject: [R] R annoyances
In-Reply-To: <20050520151414.GA13305@phenix.progiciels-bpi.ca>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E866@usctmx1106.merck.com>	<428DEF54.9030701@lancaster.ac.uk>
	<20050520151414.GA13305@phenix.progiciels-bpi.ca>
Message-ID: <428EDE29.3020608@sciviews.org>

Fran??ois Pinard wrote:
> [Barry Rowlingson]
> 
> 
>>Even my great dream that R and Python eventually merge into the same
>>language?  R gets Python's syntax and Object-oriented functions and
>>Python gets access to all R's statistical functions?
> 
> 
> R is more than a statistical library.  I'm coming to R with a strong
> Python background, and first thought I would mainly use R through
> Python.  But soon, the R language revealed a few interesting features
> that Python does not offer, and which are very appropriate in R context.
> 
> For example, vectorisation is built-in (yet available on the Python
> side through Numeric or Numarray extensions).  R also holds interesting
> (useful and flexible) ideas about argument passing and matching, lazy
> evaluation, and environments.  And surely other things as well.
> 

- This is a call for contribution! -

James Wettenhall started to relift OmegaHat's RSPython and made an 
experimental R-wxPython package which allows to use Python and wxWidgets 
with R... and we had the idea to use Boa Constructor for drawing dialog 
boxes for R (see http://bioinf.wehi.edu.au/folders/james/wxPython/). 
There are several other solutions to combine R and Python out there, 
like RPy. Sure, you have to learn and use TWO languages: R and Python... 
but that looks to me as close as the perfect combination, especially for 
building platform-independent GUI stuff for R, and get access to all the 
nice Python routines from within R.

Now, the bad news: James does not have time to continue the development 
of R-wxPython and some basic problems are not solved yet (under Linux?). 
I will (when I have time) update that R-wxPython web page. For instance, 
there is now a version of Boa Constructor that is compatible with the 
version of wxWidget used in R (2.5).

People that believe this combination is terrific, as I do, and who have 
knowledge on programming (especially Linux/Unix) are _very welcome_ to 
contribute in this project. R-wxPython is almost orphaned... before 
being really born. What a pitty!

Best,

Philippe Grosjean

P.S.: Tcl/Tk with tcltk package (+ the tcltk2 package I am working on, 
see http://www.sciviews.org/SciViews-R, and the James Wettenhall 
R-Tcl/Tk web pages I should also maintain in the future, see 
http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/) is, of course, 
another well-established alternative... but without all the nice aspects 
of Python and wxWidgets!



From p.dalgaard at biostat.ku.dk  Sat May 21 10:38:06 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 May 2005 10:38:06 +0200
Subject: [R] print format for difftime
In-Reply-To: <40D3930AC1C8EA469E39536E5BC8083545AB02@EXDKBA021.corp.novocorp.net>
References: <40D3930AC1C8EA469E39536E5BC8083545AB02@EXDKBA021.corp.novocorp.net>
Message-ID: <x2y8a9gej5.fsf@turmalin.kubism.ku.dk>

"BXC (Bendix Carstensen)" <bxc at steno.dk> writes:

> Has anyone written a function that will print a difftime in the form:
> 
> hh:mm:ss
> 
> or
> 
> yy-mm-dd hh:mm:ss
> 
> depending on the actual size.
> (sloppy notation for months/minutes, but surely you get the point).


Ummm, how are you going to convert a number of days to
years/months/days without specifying an origin? (00-01-00 could be
28,29,30, or 31 days.) Unless, of course you want to deal in "banker's
months" of 30 + 5.25625/12 days.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From stefano.colucci1 at libero.it  Sat May 21 11:25:14 2005
From: stefano.colucci1 at libero.it (Stefano Colucci)
Date: Sat, 21 May 2005 11:25:14 +0200
Subject: [R] Transfer function analisys
Message-ID: <002201c55de6$fedf5e80$aef32997@2dbce4f466c3481>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050521/1d34e8d1/attachment.pl

From ripley at stats.ox.ac.uk  Sat May 21 12:07:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 May 2005 11:07:06 +0100 (BST)
Subject: [R] Transfer function analisys
In-Reply-To: <002201c55de6$fedf5e80$aef32997@2dbce4f466c3481>
References: <002201c55de6$fedf5e80$aef32997@2dbce4f466c3481>
Message-ID: <Pine.LNX.4.61.0505211106110.6994@gannet.stats>

On Sat, 21 May 2005, Stefano Colucci wrote:

> Good morning,
>
> (sorry for my english)
>
> how can i estimate the transfer function for Time series analysis?
> exist a command?
>
> i obtained only prewhitened time series.
> if i launch acf() for prewhitened time series i obtain ACF for single series and cross-correlation for series1-series2 and series2-series1.
>
> If i want only cross-correlation series2-series1, how do i do?

?ccf  (on the same help page as acf!)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christoph.lehmann at gmx.ch  Sat May 21 12:57:41 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Sat, 21 May 2005 12:57:41 +0200 (MEST)
Subject: [R] studentized CIs for a correlation using package boot
Message-ID: <2835.1116673061@www40.gmx.net>

Dear useRs
I need to compute studentized confidence intervals for a correlation,
using the boot library.

For this CIs we need to compute a variance estimate of the statistic
(here correlation coeff) from each boostrap sample. There are 2
important points, I think:
(1) We need to do a fisher transformation (atanh(x)) to correct for
non-normality, this can be done easily be specifying h, hinv, and hdot
parameteres in the boot.ci call.
(2) an estimate for the variance is (as far as I remember)
1-correlation2)2/n (For fisher transformed data, an estimator is: 1/(n-3))

do you think, this is the correct way:

library(boot)
fisher <- function(r) 0.5*log((1+r)/(1-r))
fisher.dot <- function(r) 1/(1-r2)
fisher.inv <- function(z) (exp(2*z)-1)/(exp(2*z)+1)

boot.fun <- function(data, i) {
  n <- length(i)
  correlation <- cor(data[i,1],data[i,2])
  v <- (1-correlation2)2/n
  c(correlation, v)
}

td.boot <- boot(td, boot.fun, R=9999)
boot.ci(td.boot, h = fisher, hdot = fisher.dot,
        hinv = fisher.inv, conf = c(0.95))

?. many thanks for your thoughts

cheers
christoph






-- 
Weitersagen: GMX DSL-Flatrates mit Tempo-Garantie!
Ab 4,99 Euro/Monat: http://www.gmx.net/de/go/dsl



From tolga at coubros.com  Sat May 21 13:19:17 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sat, 21 May 2005 12:19:17 +0100
Subject: [R] Numerical PDE Solver
Message-ID: <428F1935.3080007@coubros.com>

Is there a package in R which implements numerically solves pde ?
Thanks,
Tolga



From p.dalgaard at biostat.ku.dk  Sat May 21 13:53:23 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 May 2005 13:53:23 +0200
Subject: [R] Numerical PDE Solver
In-Reply-To: <428F1935.3080007@coubros.com>
References: <428F1935.3080007@coubros.com>
Message-ID: <x2k6lskd70.fsf@turmalin.kubism.ku.dk>

Tolga Uzuner <tolga at coubros.com> writes:

> Is there a package in R which implements numerically solves pde ?
> Thanks,
> Tolga

Not that I know of. What kind? Parabolic PDEs can be fairly easily
converted to ODEs by the method of lines.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From volkan.kumbasar at linux-sevenler.org  Sat May 21 14:07:06 2005
From: volkan.kumbasar at linux-sevenler.org (Volkan Kumbasar)
Date: Sat, 21 May 2005 15:07:06 +0300
Subject: [R] Plotting functions on the same figure
Message-ID: <1116677227.25688.6.camel@localhost.localdomain>

Hi there,
i am a newbie :P

My simple question is:
Is there command which is equal of the m*tl*abs `hold on` command.
In other words i want to plot 5 functions in the same figure.

Thanx,

volkan



From alexbri at netcabo.pt  Sat May 21 14:23:20 2005
From: alexbri at netcabo.pt (Alexandre Brito)
Date: Sat, 21 May 2005 13:23:20 +0100
Subject: [R] Plotting functions on the same figure
References: <1116677227.25688.6.camel@localhost.localdomain>
Message-ID: <003c01c55dff$dfefb740$045d8453@cc7w5mza3u10k3>

see ?par
and the parameters mfrow, mfcol


----- Original Message ----- 
From: "Volkan Kumbasar" <volkan.kumbasar at linux-sevenler.org>
To: <r-help at stat.math.ethz.ch>
Sent: Saturday, May 21, 2005 1:07 PM
Subject: [R] Plotting functions on the same figure


> Hi there,
> i am a newbie :P
>
> My simple question is:
> Is there command which is equal of the m*tl*abs `hold on` command.
> In other words i want to plot 5 functions in the same figure.
>
> Thanx,
>
> volkan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Sat May 21 14:35:08 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 21 May 2005 08:35:08 -0400
Subject: [R] print format for difftime
In-Reply-To: <40D3930AC1C8EA469E39536E5BC8083545AB02@EXDKBA021.corp.novocorp.net>
References: <AcVdyXboCLfE0aaIRQKaV4EpQklqWg==>
	<40D3930AC1C8EA469E39536E5BC8083545AB02@EXDKBA021.corp.novocorp.net>
Message-ID: <971536df050521053553f6be9a@mail.gmail.com>

On 5/21/05, BXC (Bendix Carstensen) <bxc at steno.dk> wrote:
> Has anyone written a function that will print a difftime in the form:
> 
> hh:mm:ss
> 
> or
> 
> yy-mm-dd hh:mm:ss
> 
> depending on the actual size.
> (sloppy notation for months/minutes, but surely you get the point).
> 

Assuming your difftime, dd, represents time since the Epoch,
convert it to units of days, dd.day, and then use chron or times
depending on whether or not its one day or more.

library(chron)
zero <- structure(0, units = "secs", class = "difftime")
dd.day <- as.vector((dd + zero)/(24*60*60))
if (dd.day < 1) times(dd.day) else chron(dd.day)



From p.dalgaard at biostat.ku.dk  Sat May 21 14:42:12 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 May 2005 14:42:12 +0200
Subject: [R] print format for difftime
In-Reply-To: <x2y8a9gej5.fsf@turmalin.kubism.ku.dk>
References: <40D3930AC1C8EA469E39536E5BC8083545AB02@EXDKBA021.corp.novocorp.net>
	<x2y8a9gej5.fsf@turmalin.kubism.ku.dk>
Message-ID: <x2fywgkaxn.fsf@turmalin.kubism.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> "BXC (Bendix Carstensen)" <bxc at steno.dk> writes:
> 
> > Has anyone written a function that will print a difftime in the form:
> > 
> > hh:mm:ss
> > 
> > or
> > 
> > yy-mm-dd hh:mm:ss
> > 
> > depending on the actual size.
> > (sloppy notation for months/minutes, but surely you get the point).
> 
> 
> Ummm, how are you going to convert a number of days to
> years/months/days without specifying an origin? (00-01-00 could be
> 28,29,30, or 31 days.) Unless, of course you want to deal in "banker's
> months" of 30 + 5.25625/12 days.

Doh. Make that 5.2425 (97 leap days per 400 years). What was I
thinking of there? 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Sat May 21 15:19:26 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 May 2005 15:19:26 +0200
Subject: [R] Plotting functions on the same figure
In-Reply-To: <1116677227.25688.6.camel@localhost.localdomain>
References: <1116677227.25688.6.camel@localhost.localdomain>
Message-ID: <x27jhsk97l.fsf@turmalin.kubism.ku.dk>

Volkan Kumbasar <volkan.kumbasar at linux-sevenler.org> writes:

> Hi there,
> i am a newbie :P
> 
> My simple question is:
> Is there command which is equal of the m*tl*abs `hold on` command.
> In other words i want to plot 5 functions in the same figure.

I wouldn't know exactly what `hold on` does. If you are plotting
functions using curve(), notice that you can set add=TRUE (but you do
have to get the xlim and ylim right on the first graph).

Otherwise, matplot() with type="l" could be the ticket. Or plot()
followed by lines().

The most drastic measure - and usually the wrong idea - is to set
par(plot.new=TRUE) to make the next plot believe that it doesn't need
to clear the screen (or advance the page as the case may be).
Obviously, you need to careful that the axes and titles are the same,
or suffer the consequences...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From charpent at bacbuc.dyndns.org  Sat May 21 16:52:11 2005
From: charpent at bacbuc.dyndns.org (Emmanuel Charpentier)
Date: Sat, 21 May 2005 16:52:11 +0200
Subject: [R] Possible (ab)use of lexical scoping in R ?
Message-ID: <428F4B1B.4090705@bacbuc.dyndns.org>

Dear list,

I wish to define a set of functions *auxilliary* to another set of
"main" ones, and I wonder if there is some "clever" way do do this using
lexical scoping. Looking for that in the list's archives did not get me
easily understood answers. Perusing MASS (1st, 2nd, 3rd and 4th
editions!) and "Programming S" wasn't of much help either...

R easily allows to create functions local to *another* function, as in :

foo<-function(x,y,...) {
  bar<-function(a,...) {
  ...
  }
  gee<-function(t,u,...) {
  ...
  }
  t<-foo(y)
  u<-gee(t,x,..)
}

In this (pseudo-)example, bar() and gee() are known in foo() but unknown
in the main R environment, which is a Good Thing (TM) for my purposes ;
however, they are redefined in each call to foo(), which entails some
serious overhead. Furthermore, they cannot be used by any other function.

What I want to do is so create a set of (user-invisible) auxilliaries
used by another set of (user-visible) "main" functions. I might also
wish sometimes to create a such a set of data.

(Common) Lisp and Scheme allow this easily. For example, in Common Lisp,
I could use :

(flet ((bar (a)(...))(gee (t u)(...)))
  (defun foo(x y)( ...))
  (defun quux(m n ...)(...)))

Now (barring syntax errors I may have slipped in the above
pseudo-example (my Lisp is rusty)), foo and quux are known in the main
environment, can both call bar and gee, which are not visible. Lisp also
allows me to do a similar thing for "data" (with let()) and even macroes
(with macrolet()). Variants such as let*() allow, IIRC, to play tricks
with evaluation order (e. g. mutually calling "local" functions).

I am aware that one may achieve the same thing in R by creating a
package with its own namespace and exporting relevant items.However, in
my case, that would be trying to cut one's steak with a sawmill's ribbon
saw.

Are there way(s) to create a new environment, defining local functions
(or data) in this environment and the "main" functions in the parent
environment (or the global one) while still in the local environment ?

					Emmanuel Charpentier

PS : I'd appreciate Cc's to my address, since I am not on the list and
read it through the Web interface.

PPS : Shouldn't "S programming" be a bit overhauled ? at the time of its
writing, R 0.90 was current...

-- 
Emmanuel Charpentier			charpent at bacbuc.dyndns.org



From lihe418 at yahoo.com  Sat May 21 17:15:46 2005
From: lihe418 at yahoo.com (Daniel H. li)
Date: Sat, 21 May 2005 08:15:46 -0700 (PDT)
Subject: [R] Question On NLME
Message-ID: <20050521151546.8467.qmail@web30706.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050521/8be67a82/attachment.pl

From murdoch at stats.uwo.ca  Sat May 21 17:26:37 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 21 May 2005 16:26:37 +0100
Subject: [R] Possible (ab)use of lexical scoping in R ?
In-Reply-To: <428F4B1B.4090705@bacbuc.dyndns.org>
References: <428F4B1B.4090705@bacbuc.dyndns.org>
Message-ID: <428F532D.1080501@stats.uwo.ca>

Emmanuel Charpentier wrote:
> Dear list,
> 
> I wish to define a set of functions *auxilliary* to another set of
> "main" ones, and I wonder if there is some "clever" way do do this using
> lexical scoping. Looking for that in the list's archives did not get me
> easily understood answers. Perusing MASS (1st, 2nd, 3rd and 4th
> editions!) and "Programming S" wasn't of much help either...
> 
> R easily allows to create functions local to *another* function, as in :
> 
> foo<-function(x,y,...) {
>   bar<-function(a,...) {
>   ...
>   }
>   gee<-function(t,u,...) {
>   ...
>   }
>   t<-foo(y)
>   u<-gee(t,x,..)
> }
> 
> In this (pseudo-)example, bar() and gee() are known in foo() but unknown
> in the main R environment, which is a Good Thing (TM) for my purposes ;
> however, they are redefined in each call to foo(), which entails some
> serious overhead. Furthermore, they cannot be used by any other function.
> 
> What I want to do is so create a set of (user-invisible) auxilliaries
> used by another set of (user-visible) "main" functions. I might also
> wish sometimes to create a such a set of data.
> 
> (Common) Lisp and Scheme allow this easily. For example, in Common Lisp,
> I could use :
> 
> (flet ((bar (a)(...))(gee (t u)(...)))
>   (defun foo(x y)( ...))
>   (defun quux(m n ...)(...)))
> 
> Now (barring syntax errors I may have slipped in the above
> pseudo-example (my Lisp is rusty)), foo and quux are known in the main
> environment, can both call bar and gee, which are not visible. Lisp also
> allows me to do a similar thing for "data" (with let()) and even macroes
> (with macrolet()). Variants such as let*() allow, IIRC, to play tricks
> with evaluation order (e. g. mutually calling "local" functions).
> 
> I am aware that one may achieve the same thing in R by creating a
> package with its own namespace and exporting relevant items.However, in
> my case, that would be trying to cut one's steak with a sawmill's ribbon
> saw.
> 
> Are there way(s) to create a new environment, defining local functions
> (or data) in this environment and the "main" functions in the parent
> environment (or the global one) while still in the local environment ?

Yes, you can do this using lots of surgery on environments and resetting 
the parent environments of functions, but it's far easier to just create 
a package, because R does all the work for you when you ask for a 
namespace.  What's so hard about that?

If you really want to "roll your own" namespace, read the language 
manual sections on environments very carefully.  If you don't understand 
something that's written there (or something there appears not to be 
correct), please let me know.

Duncan Murdoch
> 
> 					Emmanuel Charpentier
> 
> PS : I'd appreciate Cc's to my address, since I am not on the list and
> read it through the Web interface.
> 
> PPS : Shouldn't "S programming" be a bit overhauled ? at the time of its
> writing, R 0.90 was current...



From ripley at stats.ox.ac.uk  Sat May 21 17:37:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 May 2005 16:37:01 +0100 (BST)
Subject: [R] Possible (ab)use of lexical scoping in R ?
In-Reply-To: <428F4B1B.4090705@bacbuc.dyndns.org>
References: <428F4B1B.4090705@bacbuc.dyndns.org>
Message-ID: <Pine.LNX.4.61.0505211606350.16054@gannet.stats>

On Sat, 21 May 2005, Emmanuel Charpentier wrote:

> Dear list,
>
> I wish to define a set of functions *auxilliary* to another set of
> "main" ones, and I wonder if there is some "clever" way do do this using
> lexical scoping. Looking for that in the list's archives did not get me
> easily understood answers. Perusing MASS (1st, 2nd, 3rd and 4th
> editions!) and "Programming S" wasn't of much help either...

Well, `S Programming' (sic) is about S, and S is not lexically scoped, 
only the R dialect is (and much of it was not when that was written as 
functions in packages were re-parented).

Look at ?local for one way to do this.  However, I _would_ use a namespace 
for anything which required more than one public function.  Here is 
another (closely related) idea:

myenv <- new.env()
assign("bar", function(a,...) {}, envir = myenv)
assign("gee", function(t,u,...) {},  envir = myenv)
foo <- function(x,y,...) {
    t <- bar(y)  # I hope you meant
    u <- gee(t, x,..)
}
environment(foo) <- myenv
rm(myenv)


> R easily allows to create functions local to *another* function, as in :
>
> foo<-function(x,y,...) {
>  bar<-function(a,...) {
>  ...
>  }
>  gee<-function(t,u,...) {
>  ...
>  }
>  t<-foo(y)
>  u<-gee(t,x,..)
> }
>
> In this (pseudo-)example, bar() and gee() are known in foo() but unknown
> in the main R environment, which is a Good Thing (TM) for my purposes ;
> however, they are redefined in each call to foo(), which entails some
> serious overhead.

Not so: most of the effort is in the parsing which is done once.  I think 
you would find it hard to measure the overhead, which is counteracted by 
faster searching.  E.g.

> system.time(for(i in 1:1000) foo(2))
[1] 1.35 0.00 1.35   NA   NA
## add an internal copy of ls()
> system.time(for(i in 1:1000) foo(2))
[1] 1.3 0.0 1.3  NA  NA

appears to show a small negative overhead.

> Furthermore, they cannot be used by any other function.
>
> What I want to do is so create a set of (user-invisible) auxilliaries
> used by another set of (user-visible) "main" functions. I might also
> wish sometimes to create a such a set of data.
>
> (Common) Lisp and Scheme allow this easily. For example, in Common Lisp,
> I could use :
>
> (flet ((bar (a)(...))(gee (t u)(...)))
>  (defun foo(x y)( ...))
>  (defun quux(m n ...)(...)))
>
> Now (barring syntax errors I may have slipped in the above
> pseudo-example (my Lisp is rusty)), foo and quux are known in the main
> environment, can both call bar and gee, which are not visible. Lisp also
> allows me to do a similar thing for "data" (with let()) and even macroes
> (with macrolet()). Variants such as let*() allow, IIRC, to play tricks
> with evaluation order (e. g. mutually calling "local" functions).
>
> I am aware that one may achieve the same thing in R by creating a
> package with its own namespace and exporting relevant items.However, in
> my case, that would be trying to cut one's steak with a sawmill's ribbon
> saw.
>
> Are there way(s) to create a new environment, defining local functions
> (or data) in this environment and the "main" functions in the parent
> environment (or the global one) while still in the local environment ?
>
> 					Emmanuel Charpentier
>
> PS : I'd appreciate Cc's to my address, since I am not on the list and
> read it through the Web interface.
>
> PPS : Shouldn't "S programming" be a bit overhauled ? at the time of its
> writing, R 0.90 was current...

You seem unaware of how little difference that has made -- very little is 
superseded, although some additions could be made.  Revisions have 
been planned (and updates written) but more progress depends on the first 
author's health and time.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jeaneid at chass.utoronto.ca  Sat May 21 18:22:06 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Sat, 21 May 2005 12:22:06 -0400
Subject: [R] issues with identical()
In-Reply-To: <Pine.LNX.4.61.0505201746300.30711@gannet.stats>
Message-ID: <Pine.SGI.4.40.0505211221210.4250695-100000@origin.chass.utoronto.ca>

Thank you all for pointing out the name of the variables.. I do apologize
for not catching that,..

have a good day,

Jean

On Fri, 20 May 2005, Prof Brian Ripley wrote:

> On Fri, 20 May 2005, Jean Eid wrote:
>
> > Hi all, hope you having a nice day,
> >
> > I ahve this weird results with identical (probably I am not understanding
> > correctly what it does ...)
>
> Why should
>
>      a data frame with colunns pub_id faminc90
>      a data frame with colunns pub_id faminc
>
> be considered identical()?  Its description is
>
>       The safe and reliable way to test two objects for being _exactly_
>       equal.
>
> and those are not equal in a critical way.
>
> > I have these two data frames and I issue :
> >> identical(temp, temp1)
> > [1] FALSE
> >
> >
> > However, these data frames are Nx2 and when I issue:
> >> identical(temp[,2], temp1[,2])
> > [1] TRUE
> >> identical(temp[,1], temp1[,1])
> > [1] TRUE
> >
> > and the results from str
> >
> >
> >> str(temp)
> > `data.frame':	7072 obs. of  2 variables:
> > $ pub_id  : int  10000 1000 10001 10002 10003 10004 10005 10006 10007
> > $ faminc90: int  -2 5998 19900 43000 35000 40000 56538 61000 36000 39105
> >> str(temp1)
> > `data.frame':	7072 obs. of  2 variables:
> > $ pub_id: int  10000 1000 10001 10002 10003 10004 10005 10006 10007 10008
> > $ faminc: int  -2 5998 19900 43000 35000 40000 56538 61000 36000 39105
> >
> > The question is why are the objects different. How else can I tell what is
> > the difference
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From cwills at ucsd.ucsd.edu  Sun May 22 00:39:29 2005
From: cwills at ucsd.ucsd.edu (Christopher Wills)
Date: Sat, 21 May 2005 15:39:29 -0700
Subject: [R] R version 1.9.1
Message-ID: <690e4d61c51dacc4e572c64bbd59862c@ucsd>

Can anybody help? I have been running multiple programs simultaneously 
in Raqua windows, on Mac OS 10.3 on a G5 dual processor Mac, using R 
version 1.9.1. I rebooted my computer recently, and thought that I 
would upgrade to the latest R at the same time. Alas, I find that the 
newest R does not (so far as I can tell) allow multiple Raqua windows. 
I would like to go back to 1.9.1, but I have erased the original 
compressed binary file and it has vanished from the official web sites. 
Does anyone have a binary of this version that they can send me? (I 
have Fetch, if it should prove too large.) Alternatively, is there a 
way to run multiple Raqua windows in the newest R?
	Many thanks!
	Chris Wills
Christopher Wills
Professor of Biological Sciences
University of California, San Diego
La Jolla CA 92093-0116
Phone: 858-534-4113
Fax: 858-534-7108



From charpent at bacbuc.dyndns.org  Sun May 22 02:30:16 2005
From: charpent at bacbuc.dyndns.org (Emmanuel Charpentier)
Date: Sun, 22 May 2005 02:30:16 +0200
Subject: [R] Suimmary of answers : Possible (ab)use of lexical scoping in R ?
In-Reply-To: <428F532D.1080501@stats.uwo.ca>
References: <428F4B1B.4090705@bacbuc.dyndns.org>
	<428F532D.1080501@stats.uwo.ca>
Message-ID: <428FD298.8030309@bacbuc.dyndns.org>

Dear List,

I asked how to create a set of functions (and maybe variables) shared by
another set of functions but hidden from the "main" environment.

Duncan Murdoch and Brian Ripley advised to use the package creation
system. Brian ripley (and someone else, offlist) also pointed me to the
local() function, which creates new environments with specified
contents, and which I was unaware of (btw, when this function has been
introduced ? It is mentioned neither in MASS 4th edition index, nor in
'S Programming' index).

After re-reading the available docs (which I may have misunderstood...),
I come to the following conclusions :

	- The package creation is the most elegant and portable form. Unless I
am mistaken, it entails however some administrative overhead (creation
of a directory structure, R CMD installation*, etc ...).

	- Using local() is a (semi-) kludge, easy to use in one-file disposable
works. It might be more error-prone than package creation.

	- It has been pointed to me that manipulating environment (via local()
or otherwise), in a very Abelson-&-Sussmann-like way, allowed to create
OO-oriented code, somewhat different from S3 and S4 class mechanisms.

Since repeated experiences have proved to my satisfaction that I am
piss-poor at top-down design, I will probably use the environment
manipulation for initial head-scratching phase, switching to package
creation at the formalization phase.

A big "thank you" to all respondents, whose answers have been *very* useful.

					Emmanuel Charpentier

-- 
Emmanuel Charpentier			charpent at bacbuc.dyndns.org



From drewbrewit at yahoo.com  Sun May 22 02:39:31 2005
From: drewbrewit at yahoo.com (Nick Drew)
Date: Sat, 21 May 2005 17:39:31 -0700 (PDT)
Subject: [R] comparison operator, decimals, and signif()
Message-ID: <20050522003931.98935.qmail@web50910.mail.yahoo.com>

Hi, I recently spent quite a bit of time trouble
shooting a function that I had written only to
discover that the problem I was having was with the
comparison operator. I assumed that the following
would return TRUE:

> testMean <- 82.8 + 0.1
> testMean
[1] 82.9
> testMean == 82.9
[1] FALSE


Apparently this has to do with deciml places. Look:

> newTest <- 82.0
> newTest
[1] 82
> newTest == 82
[1] TRUE
> newTest == 82.0
[1] TRUE
> 

What does signif() do to my object called "testMean"
so that the comparison now evaluates to TRUE?

> signif(testMean, 3) == 82.9
[1] TRUE


Version info:

> R.Version()
$platform
[1] "i386-pc-mingw32"

$arch
[1] "i386"

$os
[1] "mingw32"

$system
[1] "i386, mingw32"

$status
[1] ""

$major
[1] "2"

$minor
[1] "1.0"

$year
[1] "2005"

$month
[1] "04"

$day
[1] "18"

$language
[1] "R"

>



From Pierre.Lapointe at nbf.ca  Sun May 22 02:51:05 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Sat, 21 May 2005 20:51:05 -0400
Subject: [R] Calling R from R and specifying "wait until script is finished"
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050521/8ff96524/attachment.pl

From sdavis2 at mail.nih.gov  Sun May 22 03:09:10 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sat, 21 May 2005 21:09:10 -0400
Subject: [R] Calling R from R and specifying "wait until script is
	finished"
References: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>
Message-ID: <000901c55e6a$dc668de0$5179f345@WATSON>


----- Original Message ----- 
From: "Lapointe, Pierre" <Pierre.Lapointe at nbf.ca>
To: <r-help at stat.math.ethz.ch>
Sent: Saturday, May 21, 2005 8:51 PM
Subject: [R] Calling R from R and specifying "wait until script is finished"


> Hello,
>
> Let's say I have 50 R scripts to run.  What would be the most efficient 
> way
> to run them?

> Is there an easier way?  Something like calling R from R and specifying 
> that
> the script has to be finished before continuing.

How about something like (untested):

for (i in 1:50) {
  filename <- paste('/path/to/files/',i,'.txt',sep="")
  source(filename)
}

As an alternative, if you know the filenames and have them in an R vector 
called filenames, you could do:

for (i in filenames) {
  source(i)
}

Sean



From i.visser at uva.nl  Sun May 22 03:18:19 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Sat, 21 May 2005 21:18:19 -0400
Subject: [R] constraints
Message-ID: <BEB5561B.5268%i.visser@uva.nl>

Is there a package in R that handles general linear (in-)equality + box
constrained optimization?
If it is not there, could anyone advise me which way to go?
And/or point me to packages that solve these problems partially?
best, ingmar
-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From murdoch at stats.uwo.ca  Sun May 22 06:40:22 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 22 May 2005 05:40:22 +0100
Subject: [R] comparison operator, decimals, and signif()
In-Reply-To: <20050522003931.98935.qmail@web50910.mail.yahoo.com>
References: <20050522003931.98935.qmail@web50910.mail.yahoo.com>
Message-ID: <42900D36.1060104@stats.uwo.ca>

Nick Drew wrote:
> Hi, I recently spent quite a bit of time trouble
> shooting a function that I had written only to
> discover that the problem I was having was with the
> comparison operator. I assumed that the following
> would return TRUE:
> 

This is a very common error.  In R 2.1.0, it's FAQ

7.31 Why doesn't R think these numbers are equal?

Duncan Murdoch
> 
>>testMean <- 82.8 + 0.1
>>testMean
> 
> [1] 82.9
> 
>>testMean == 82.9
> 
> [1] FALSE
> 
> 
> Apparently this has to do with deciml places. Look:
> 
> 
>>newTest <- 82.0
>>newTest
> 
> [1] 82
> 
>>newTest == 82
> 
> [1] TRUE
> 
>>newTest == 82.0
> 
> [1] TRUE
> 
> 
> What does signif() do to my object called "testMean"
> so that the comparison now evaluates to TRUE?
> 
> 
>>signif(testMean, 3) == 82.9
> 
> [1] TRUE
> 
> 
> Version info:
> 
> 
>>R.Version()
> 
> $platform
> [1] "i386-pc-mingw32"
> 
> $arch
> [1] "i386"
> 
> $os
> [1] "mingw32"
> 
> $system
> [1] "i386, mingw32"
> 
> $status
> [1] ""
> 
> $major
> [1] "2"
> 
> $minor
> [1] "1.0"
> 
> $year
> [1] "2005"
> 
> $month
> [1] "04"
> 
> $day
> [1] "18"
> 
> $language
> [1] "R"
> 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Sun May 22 06:43:56 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 22 May 2005 05:43:56 +0100
Subject: [R] Calling R from R and specifying "wait until script is
	finished"
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>
Message-ID: <42900E0C.70900@stats.uwo.ca>

Lapointe, Pierre wrote:
> Hello,
> 
> Let's say I have 50 R scripts to run.  What would be the most efficient way
> to run them?
> 
> I thought I could do multiple Rterms in a DOS batch file:
> 
> Ex:
> Rterm <1.R> 1.txt 
> Rterm <2.R> 2.txt 
> ...
> Rterm <50.R> 50.txt 
> 
> However, I'm afraid they will all open at the same time.   

That could be a problem if you had used Rgui (depending on which shell 
you are using), but Rterm should run until completion before the next 
line of the batch file will start.

Duncan Murdoch


I know I could
> pause the batch file with something like: 
> 
> PING 1.1.1.1 -n 1 -w 60000 >NUL  (to delay 60 seconds)
> 
> But that would require that I know how long each of my scripts take.
> 
> Is there an easier way?  Something like calling R from R and specifying that
> the script has to be finished before continuing.
> 
> Thanks
> 
> Pierre Lapointe
> 
> 
> 
> *********************************************************************************** 
> AVIS DE NON-RESPONSABILITE:\ Ce document transmis par courri...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chencheva at gmail.com  Sun May 22 10:07:00 2005
From: chencheva at gmail.com (Hu Chen)
Date: Sun, 22 May 2005 16:07:00 +0800
Subject: [R] how to plot a fitted smooth line on histograms?
Message-ID: <6f3fc9ee05052201071375a1ed@mail.gmail.com>

for example,
>x <- read.table(....);  here x is a vector containing  my data to be analyzed.
>hist(x,plot=TRUE,breaks=200)

then I got a histogram graph. However, How can't get a smooth curve
based on the histogram cells to show out the outline?

Thanks very much.



From Ted.Harding at nessie.mcc.ac.uk  Sun May 22 11:07:23 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 22 May 2005 10:07:23 +0100 (BST)
Subject: [R] comparison operator, decimals, and signif()
In-Reply-To: <20050522003931.98935.qmail@web50910.mail.yahoo.com>
Message-ID: <XFMail.050522100723.Ted.Harding@nessie.mcc.ac.uk>

On 22-May-05 Nick Drew wrote:
> Hi, I recently spent quite a bit of time trouble
> shooting a function that I had written only to
> discover that the problem I was having was with the
> comparison operator. I assumed that the following
> would return TRUE:
> 
>> testMean <- 82.8 + 0.1
>> testMean
> [1] 82.9
>> testMean == 82.9
> [1] FALSE
> 
> 
> Apparently this has to do with deciml places. Look:
> 
>> newTest <- 82.0
>> newTest
> [1] 82
>> newTest == 82
> [1] TRUE
>> newTest == 82.0
> [1] TRUE
>> 
> 
> What does signif() do to my object called "testMean"
> so that the comparison now evaluates to TRUE?
> 
>> signif(testMean, 3) == 82.9
> [1] TRUE

While not an exact explanation, the following strongly hints
at the underlying answer:

  > print(82.9,digits=20)
  [1] 82.9

  > print(82.8,digits=20)
  [1] 82.8

  > print(0.1,digits=20)
  [1] 0.1

  > print(82.8+0.1,digits=20)
  [1] 82.89999999999999

  > (82.8+0.1)-82.9
  [1] -1.421085e-14

So decimal arithmetic in a binary computer is not as exact
as decimal arithmetic done with pencil and paper.

The underlying reason (in part) for the above apparent anomalies
is that when 82.8 is added to 0.1, the smaller of these numbers
(in its binary "floating point" representation) is shifted along
until corresponding digits line up (the shift being accounted for
in the "exponent"). In performing the shift, binary digits are
dropped from the bottom end. Since "0.1" in binary in fact has
an infinite number of significant decimal places, inaccuracy
inevitably creeps in!

A close neighbour of the above calculations, such that there are
no discrepancies in the binary representations, illustrates the
above explanation:

  > print((82.0 + 7/8),digits=20)
  [1] 82.875

  > print((82.0 + 7/8)+1/16,digits=20)
  [1] 82.9375

  > print(82.9375,digits=20)
  [1] 82.9375

  > ((82.0 + 7/8)+1/16)-82.9375
  [1] 0

No discrepancies here! The reason is that the fractional parts
of these numbers all have exact finite (and short) binary
representations. Compare:

  > ((82.0 + 7/8)+1/16)==82.9375
  [1] TRUE

  > ((82.0 + 8/10)+1/10)==82.9
  [1] FALSE

Hoping this helps,
Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-May-05                                       Time: 09:58:00
------------------------------ XFMail ------------------------------



From tolga at coubros.com  Sun May 22 11:27:12 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sun, 22 May 2005 10:27:12 +0100
Subject: [R] Numerical PDE Solver
In-Reply-To: <x2k6lskd70.fsf@turmalin.kubism.ku.dk>
Message-ID: <000c01c55eb0$731506a0$0200a8c0@ACERTABLET>

Thanks. I was thinking of something like Crank-Nicolson ?

-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter
Dalgaard
Sent: 21 May 2005 11:53
To: tolga at coubros.com
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Numerical PDE Solver

Tolga Uzuner <tolga at coubros.com> writes:

> Is there a package in R which implements numerically solves pde ?
> Thanks,
> Tolga

Not that I know of. What kind? Parabolic PDEs can be fairly easily
converted to ODEs by the method of lines.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Sun May 22 12:58:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 22 May 2005 12:58:20 +0200
Subject: [R] Suimmary of answers : Possible (ab)use of lexical scoping
	in R ?
In-Reply-To: <428FD298.8030309@bacbuc.dyndns.org>
References: <428F4B1B.4090705@bacbuc.dyndns.org>	<428F532D.1080501@stats.uwo.ca>
	<428FD298.8030309@bacbuc.dyndns.org>
Message-ID: <429065CC.8000705@statistik.uni-dortmund.de>

Emmanuel Charpentier wrote:
> Dear List,
> 
> I asked how to create a set of functions (and maybe variables) shared by
> another set of functions but hidden from the "main" environment.
> 
> Duncan Murdoch and Brian Ripley advised to use the package creation
> system. Brian ripley (and someone else, offlist) also pointed me to the
> local() function, which creates new environments with specified
> contents, and which I was unaware of (btw, when this function has been
> introduced ? It is mentioned neither in MASS 4th edition index, nor in
> 'S Programming' index).

I think everybody remembers that local() has been introduced in R-0.65.0 
(those who do not remember can simply look into the file OONEWS).

I like the two books as well, but I don't think they can be complete in 
the sense of describing *all* R functions.

I'd like to recommend the "package way". Working with envrionments and 
local() does not seem to be the R way - and it is very confusing to read 
code that refers to a couple of different environments ...

Uwe Ligges



> After re-reading the available docs (which I may have misunderstood...),
> I come to the following conclusions :
> 
> 	- The package creation is the most elegant and portable form. Unless I
> am mistaken, it entails however some administrative overhead (creation
> of a directory structure, R CMD installation*, etc ...).
> 
> 	- Using local() is a (semi-) kludge, easy to use in one-file disposable
> works. It might be more error-prone than package creation.
> 
> 	- It has been pointed to me that manipulating environment (via local()
> or otherwise), in a very Abelson-&-Sussmann-like way, allowed to create
> OO-oriented code, somewhat different from S3 and S4 class mechanisms.
> 
> Since repeated experiences have proved to my satisfaction that I am
> piss-poor at top-down design, I will probably use the environment
> manipulation for initial head-scratching phase, switching to package
> creation at the formalization phase.
> 
> A big "thank you" to all respondents, whose answers have been *very* useful.
> 
> 					Emmanuel Charpentier
>



From ggrothendieck at gmail.com  Sun May 22 13:24:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 22 May 2005 07:24:47 -0400
Subject: [R] Calling R from R and specifying "wait until script is
	finished"
In-Reply-To: <42900E0C.70900@stats.uwo.ca>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>
	<42900E0C.70900@stats.uwo.ca>
Message-ID: <971536df05052204242a54872c@mail.gmail.com>

On 5/22/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> Lapointe, Pierre wrote:
> > Hello,
> >
> > Let's say I have 50 R scripts to run.  What would be the most efficient way
> > to run them?
> >
> > I thought I could do multiple Rterms in a DOS batch file:
> >
> > Ex:
> > Rterm <1.R> 1.txt
> > Rterm <2.R> 2.txt
> > ...
> > Rterm <50.R> 50.txt
> >
> > However, I'm afraid they will all open at the same time.
> 
> That could be a problem if you had used Rgui (depending on which shell
> you are using), but Rterm should run until completion before the next
> line of the batch file will start.


Using Windows XP cmd, one can force waiting for Rgui to complete
like this (note the /wait):

start/wait "" "\Program Files\R\rw2010\bin\Rgui.exe"



From wuming.gong at gmail.com  Sun May 22 15:14:31 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Sun, 22 May 2005 21:14:31 +0800
Subject: [R] how to plot a fitted smooth line on histograms?
In-Reply-To: <6f3fc9ee05052201071375a1ed@mail.gmail.com>
References: <6f3fc9ee05052201071375a1ed@mail.gmail.com>
Message-ID: <b428d06d050522061413226d72@mail.gmail.com>

You may check the chapter 8 Probability distributions in An Introduction to R.

Wuming

On 5/22/05, Hu Chen <chencheva at gmail.com> wrote:
> for example,
> >x <- read.table(....);  here x is a vector containing  my data to be analyzed.
> >hist(x,plot=TRUE,breaks=200)
> 
> then I got a histogram graph. However, How can't get a smooth curve
> based on the histogram cells to show out the outline?
> 
> Thanks very much.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From hb at maths.lth.se  Sun May 22 15:25:47 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Sun, 22 May 2005 15:25:47 +0200
Subject: R.batch (Was: Re: [R] Calling R from R and specifying "wait until
	script is finished")
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>
Message-ID: <4290885B.5080703@maths.lth.se>

Hi. I have a package R.batch in R.classes 
[http://www.maths.lth.se/help/R/R.classes/] to simplify running multiple 
batch jobs, which might interest you.

The idea is as follows. You setup a directory structure defining a 
'JobBatch';

  <path-to>/jobs/
    src/

    input/
    output/

    erroneous/
    failed/
    finished/
    interrupted/
    running/
    todo/
      job01/
      job02/
      job03/

A 'Job' is simply a directory (above job01/, job02/, job03/). Put code 
shared by all Job:s in src/. Put code unique to each Job in its job 
directory, e.g. job01/setupParameters.R. All *.R files in src/ and then 
in job directory are source():ed before a Job is started. When a Job is 
run, onRun(job) is called. Thus, you have to define onRun() in src/ with 
the option to override it in each job directory.

As soon as the Job is being processed it is moved to running/. When a 
Job is successful and completed, onFinally(job) is called and it is 
moved to finished/. If a Job is interrupted, say by Ctrl+C or by sending 
SIG-INT for elsewhere, onInterrupt() is called and the job is moved to 
interrupted/. Similarly, if an error occurs, say, by calling stop(), 
onError(job) is called and it is moved to failed/. (If an error occurs 
while source():ing the *.R files before starting, the job is moved to 
erroneous/). If you call sourceHotCode(job) once in a while in your 
onRun(job) code, code in src/hot/ or job01/hot/ will be source():ed *and 
*removed. This allows you to fix problems (redefine objective functions 
etc) *while running*, say a 10-hour job.

When a Job runs, its working directory is set to "itself", e.g. 
running/job01/. Thus, written result files and created images etc will 
naturally be save in each job directory. You can also write to 
getOutputPath(), which is the output/. Log files are written to 
getLogPath(), which defaults to output/.

Each Job can access common data from the input/ path by getInputPath(). 
Note that in Unix input/ can be a soft link to another directory. To 
provide the same functionality under Windows, Windows shortcut files, 
say, input.lnk, are recognized and followed if getInputPath() is used. 
[This actually holds for other directories too; if multiple batches 
share same source code, you can link src/].

Given the above structure, you run all Jobs one by one, by

library(R.batch)
batch <- JobBatch("<path-to>/jobs/")
run(batch)
# wait until all jobs are processed
# Try Ctrl+C, rerun by run(batch).
print(batch) # Gives a summary of the status of all jobs

Logging and everything else is taken care of automatically.

The code is written such it should be possible for several R sessions to 
operate on the same batch set simultaneously. Lock files are used to 
control for this. I used this last summer to run batch jobs from 30+ 
computers sharing the same file system.

Want to try it? Try this

 > install.packages("R.classes", 
contriburl="http://www.maths.lth.se/help/R")
 > library(R.batch)
 > example(JobBatch)

and a batch of Mandelbrot sets (from Martin Maechler's rhelp example) 
will be generated together with images.

Warning: The package works, but the API is not fixed, meaning it may 
change in future releases. However, the general idea should remain. 
Currently I feel that the names of some methods and directories are a 
little bit confusing. Feedback on this is appreciated.

Future: Recently, I have been working on adding dependency control 
between jobs so certain jobs are processed before others. This is not 
included in the current version. Some kind of mechanism to restarting 
interrupted jobs where they where interrupted would also be very nice, 
but this is very tricky and will propably require modification of the R 
engine, which is beyond my skills.

Cheers

Henrik Bengtsson


Lapointe, Pierre wrote:
> Hello,
> 
> Let's say I have 50 R scripts to run.  What would be the most efficient way
> to run them?
> 
> I thought I could do multiple Rterms in a DOS batch file:
> 
> Ex:
> Rterm <1.R> 1.txt 
> Rterm <2.R> 2.txt 
> ...
> Rterm <50.R> 50.txt 
> 
> However, I'm afraid they will all open at the same time.   I know I could
> pause the batch file with something like: 
> 
> PING 1.1.1.1 -n 1 -w 60000 >NUL  (to delay 60 seconds)
> 
> But that would require that I know how long each of my scripts take.
> 
> Is there an easier way?  Something like calling R from R and specifying that
> the script has to be finished before continuing.
> 
> Thanks
> 
> Pierre Lapointe
> 
> 
> 
> *********************************************************************************** 
> AVIS DE NON-RESPONSABILITE:\ Ce document transmis par courri...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From blindglobe at gmail.com  Sun May 22 15:59:45 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Sun, 22 May 2005 15:59:45 +0200
Subject: R.batch (Was: Re: [R] Calling R from R and specifying "wait until
	script is finished")
In-Reply-To: <4290885B.5080703@maths.lth.se>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>
	<4290885B.5080703@maths.lth.se>
Message-ID: <1abe3fa905052206596dbe9ab2@mail.gmail.com>

On 5/22/05, Henrik Bengtsson <hb at maths.lth.se> wrote:

  <-- very interesting package description deleted -->

> Future: Recently, I have been working on adding dependency control
> between jobs so certain jobs are processed before others. This is not
> included in the current version. Some kind of mechanism to restarting
> interrupted jobs where they where interrupted would also be very nice,
> but this is very tricky and will propably require modification of the R
> engine, which is beyond my skills.

For the latter, probably the best approach would be to employ a
job/grid/queueing engine which does this (i.e. "grid"-enabling R with
a grid approach which has checkpointing).   I've been looking at this
at work in a different context.

best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From Rau at demogr.mpg.de  Sun May 22 17:08:01 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Sun, 22 May 2005 17:08:01 +0200
Subject: [R] Maps, Eastern Europe
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E65202D6@HERMES.demogr.mpg.de>

Dear all,

I would like to employ a European map in a presentation. My idea was to
use:
library(mapdata)
map("worldHires", c("Austria", "Switzerland", "Germany"))

where I include all countries from my analysis as a vector of character
strings like in the example above. Unfortunately, I was unable to
specify the Baltic States (Latvia, Lithuania, Estonia) or the Czech
Republic. 
Initially, I thought I misspelled the country names. To avoid this
obvious error, I checked within 'mapdata_2.0-14.tar.gz' the file
'worldHires.names'. To my surprise, the spelling was not the source of
the error - it was rather the case that those countries were not
included.

Did some people experience similar problems? And if yes: how did you
proceed?

Thanks,
Roland

P.S. my version of R is: R 2.1.0 on WinXP


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From bates at stat.wisc.edu  Sun May 22 17:34:47 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 22 May 2005 10:34:47 -0500
Subject: R.batch (Was: Re: [R] Calling R from R and specifying "wait until
	script is finished")
In-Reply-To: <1abe3fa905052206596dbe9ab2@mail.gmail.com>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>	<4290885B.5080703@maths.lth.se>
	<1abe3fa905052206596dbe9ab2@mail.gmail.com>
Message-ID: <4290A697.3010601@stat.wisc.edu>

A.J. Rossini wrote:
> On 5/22/05, Henrik Bengtsson <hb at maths.lth.se> wrote:
> 
>   <-- very interesting package description deleted -->
> 
>>Future: Recently, I have been working on adding dependency control
>>between jobs so certain jobs are processed before others. This is not
>>included in the current version. Some kind of mechanism to restarting
>>interrupted jobs where they where interrupted would also be very nice,
>>but this is very tricky and will propably require modification of the R
>>engine, which is beyond my skills.

You may want to check with Xianhong Xie <xie at stat.wisc.edu> to get a
prepublication copy of an article he has written on the use of Condor
(http://www.cs.wisc.edu/condor/) and especially its DAG (directed
acyclic graph) capabilities to control the execution of R batch jobs.



From aorchid at mac.com  Sun May 22 20:51:05 2005
From: aorchid at mac.com (Aric Gregson)
Date: Sun, 22 May 2005 14:51:05 -0400
Subject: [R] Hmisc/Design and problem with cgroup
In-Reply-To: <428AAC0B.5060404@vanderbilt.edu>
Message-ID: <r02010500-1039-74085AC4CAF211D9B5AC000A959B3D22@[10.0.1.205]>

On 5/17/05 21:44 Frank E Harrell Jr sent the following:

>Aric Gregson wrote:
>> Hello,
>> 
>> I am trying to use the following to output a table to latex:
>> 
>> cohortbyagesummary <- by(data.frame(age,ethnicity), cohort, summary) 
>> 
>> w <- latex.default(cohortbyagesummary, 
>>     caption="Five Number Age Summaries by Cohort",
>>     label="agesummarybycohort", 
>>     cgroup=c('hello','goodbye','hello'),
>>     colheads=c("Age","Ethnicity"),
>>     extracolheads=c('hello','goodbye'), 
>>     greek=TRUE,
>>     ctable=TRUE)
>>     
>> I am not able to get the major column headings of cgroup to work. I
>> receive the error:
>>     Object cline not found
>> 
>See if a modified version at
>http://biostat.mc.vanderbilt.edu/cgi-bin/cvsweb.cgi/Hmisc/R/latex.s
>fixes your problem.

Dr. Harrell,

Thanks for your help. I have downloaded latex.s, but am unable to figure
out where it should go. How do I ensure that it is called when I load
Hmisc? Do I patch it against R/Hmisc?

I have a local directory structure in ~/Library/R/library/Hmisc/:

-rw-r--r--  1 user  user  594 12 May 19:20 CITATION
-rw-r--r--  1 user  user  989 12 May 19:20 DESCRIPTION
drwxr-xr-x  3 user  user  102 12 May 19:20 Meta/package.rds
drwxr-xr-x  3 user  user  102 12 May 19:20 R/Hmisc
drwxr-xr-x  3 user  user  102 12 May 19:20 libs/Hmisc.so

Thanks for your patience.

aric



From ligges at statistik.uni-dortmund.de  Sun May 22 20:59:30 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 22 May 2005 20:59:30 +0200
Subject: [R] Hmisc/Design and problem with cgroup
In-Reply-To: <r02010500-1039-74085AC4CAF211D9B5AC000A959B3D22@[10.0.1.205]>
References: <r02010500-1039-74085AC4CAF211D9B5AC000A959B3D22@[10.0.1.205]>
Message-ID: <4290D692.4060502@statistik.uni-dortmund.de>

Aric Gregson wrote:
> On 5/17/05 21:44 Frank E Harrell Jr sent the following:
> 
> 
>>Aric Gregson wrote:
>>
>>>Hello,
>>>
>>>I am trying to use the following to output a table to latex:
>>>
>>>cohortbyagesummary <- by(data.frame(age,ethnicity), cohort, summary) 
>>>
>>>w <- latex.default(cohortbyagesummary, 
>>>    caption="Five Number Age Summaries by Cohort",
>>>    label="agesummarybycohort", 
>>>    cgroup=c('hello','goodbye','hello'),
>>>    colheads=c("Age","Ethnicity"),
>>>    extracolheads=c('hello','goodbye'), 
>>>    greek=TRUE,
>>>    ctable=TRUE)
>>>    
>>>I am not able to get the major column headings of cgroup to work. I
>>>receive the error:
>>>    Object cline not found
>>>
>>
>>See if a modified version at
>>http://biostat.mc.vanderbilt.edu/cgi-bin/cvsweb.cgi/Hmisc/R/latex.s
>>fixes your problem.
> 
> 
> Dr. Harrell,
> 
> Thanks for your help. I have downloaded latex.s, but am unable to figure
> out where it should go. How do I ensure that it is called when I load
> Hmisc? Do I patch it against R/Hmisc?
> 
> I have a local directory structure in ~/Library/R/library/Hmisc/:
> 
> -rw-r--r--  1 user  user  594 12 May 19:20 CITATION
> -rw-r--r--  1 user  user  989 12 May 19:20 DESCRIPTION
> drwxr-xr-x  3 user  user  102 12 May 19:20 Meta/package.rds
> drwxr-xr-x  3 user  user  102 12 May 19:20 R/Hmisc
> drwxr-xr-x  3 user  user  102 12 May 19:20 libs/Hmisc.so
> 
> Thanks for your patience.


Your directory structure is from an installed binary package. You need 
to replace the file in the *source* package and *install* the source 
package after that.

Uwe Ligges

> aric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ahenningsen at agric-econ.uni-kiel.de  Sun May 22 21:28:48 2005
From: ahenningsen at agric-econ.uni-kiel.de (Arne Henningsen)
Date: Sun, 22 May 2005 21:28:48 +0200
Subject: [R] constraints
In-Reply-To: <BEB5561B.5268%i.visser@uva.nl>
References: <BEB5561B.5268%i.visser@uva.nl>
Message-ID: <200505222128.48980.ahenningsen@agric-econ.uni-kiel.de>

On Sunday 22 May 2005 03:18, Ingmar Visser wrote:
> Is there a package in R that handles general linear (in-)equality + box
> constrained optimization?

R> help.search("constrained optimisation")
constrOptim(stats)      Linearly constrained optimisation

Best wishes,
Arne

> If it is not there, could anyone advise me which way to go?
> And/or point me to packages that solve these problems partially?
> best, ingmar

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From ahenningsen at agric-econ.uni-kiel.de  Sun May 22 22:46:34 2005
From: ahenningsen at agric-econ.uni-kiel.de (Arne Henningsen)
Date: Sun, 22 May 2005 22:46:34 +0200
Subject: [R] constraints
In-Reply-To: <BEB65692.52A2%i.visser@uva.nl>
References: <BEB65692.52A2%i.visser@uva.nl>
Message-ID: <200505222246.34947.ahenningsen@agric-econ.uni-kiel.de>

On Sunday 22 May 2005 21:32, you wrote:
> Hi Arne,
> Thanks. Unfortunately constrOptim does not handle combined equality and
> inequality constraints, or perhaps not in a way that I understand. Do you
> have suggestions as to how to handle such combinations of constraints?
> best, ingmar

You can impose the equality constraints directly by parameter substitution. 
See
   https://stat.ethz.ch/pipermail/r-help/2005-May/070481.html
and the 3 answers.
Then you can impose the inequality constraints in constrOptim.
I don't know if it possible to impose an equality constraint, say a = b, in 
constrOptim by two inequality constraints:
   a - b >= 0
and
   b - a >= 0
However, I guess that the solver prefers the first way ;-)

Best wishes,
Arne

> On 5/22/05 3:28 PM, "Arne Henningsen" <ahenningsen at agric-econ.uni-kiel.de>
>
> wrote:
> > On Sunday 22 May 2005 03:18, Ingmar Visser wrote:
> >> Is there a package in R that handles general linear (in-)equality + box
> >> constrained optimization?
> >
> > R> help.search("constrained optimisation")
> > constrOptim(stats)      Linearly constrained optimisation
> >
> > Best wishes,
> > Arne
> >
> >> If it is not there, could anyone advise me which way to go?
> >> And/or point me to packages that solve these problems partially?
> >> best, ingmar

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From ray at mcs.vuw.ac.nz  Mon May 23 00:20:54 2005
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Mon, 23 May 2005 10:20:54 +1200 (NZST)
Subject: [R] Maps, Eastern Europe
Message-ID: <200505222220.j4MMKsF1004847@tahi.mcs.vuw.ac.nz>

> From: "Rau, Roland" <Rau at demogr.mpg.de>
> 
> I would like to employ a European map in a presentation. My idea was to
> use:
> library(mapdata)
> map("worldHires", c("Austria", "Switzerland", "Germany"))
> 
> where I include all countries from my analysis as a vector of character
> strings like in the example above. Unfortunately, I was unable to
> specify the Baltic States (Latvia, Lithuania, Estonia) or the Czech
> Republic. 
> Initially, I thought I misspelled the country names. To avoid this
> obvious error, I checked within 'mapdata_2.0-14.tar.gz' the file
> 'worldHires.names'. To my surprise, the spelling was not the source of
> the error - it was rather the case that those countries were not
> included.
> 
> Did some people experience similar problems? And if yes: how did you
> proceed?
> 
Unfortunately the world databases for the maps package are very old
(early 1990's) and have not been updated since, except for minor
typographical errors in country names.  Note that Czechoslovakia is
there (as is USSR!).  Also, it currently satisfies my needs, which is
why I don't have the resources to update it.

I suggest you start with the maptools package, but as I understand it,
you need to also find some appropriate shapefile data. [Other
respondents may improve upon this suggestion.]

Regards,
Ray Brownrigg
maps package maintainer



From goedman at mac.com  Mon May 23 02:08:24 2005
From: goedman at mac.com (Rob J Goedman)
Date: Sun, 22 May 2005 17:08:24 -0700
Subject: [R] R version 1.9.1
In-Reply-To: <690e4d61c51dacc4e572c64bbd59862c@ucsd>
References: <690e4d61c51dacc4e572c64bbd59862c@ucsd>
Message-ID: <AFCAAC4B-736D-4B06-944A-44936CDBE897@mac.com>

Chris,

Try duplicating R.app (select the R.app in Applications and do  
Command-D).
Double click both versions and you have 2 completely separated  
versions of R.
You can drag/place the multiple icons in the dock.

Some issues will show up, e.g. it will use the history from the  
version that was closed
last, both copies will share preferences, etc.

I would be interested why you use this setup (I can think of a few  
reasons), but
would like to understand yours. If you can send these to me directly  
or to the R-SIG-Mac,
that would be appreciated.

Rob

On May 21, 2005, at 3:39 PM, Christopher Wills wrote:

> Can anybody help? I have been running multiple programs  
> simultaneously in Raqua windows, on Mac OS 10.3 on a G5 dual  
> processor Mac, using R version 1.9.1. I rebooted my computer  
> recently, and thought that I would upgrade to the latest R at the  
> same time. Alas, I find that the newest R does not (so far as I can  
> tell) allow multiple Raqua windows. I would like to go back to  
> 1.9.1, but I have erased the original compressed binary file and it  
> has vanished from the official web sites. Does anyone have a binary  
> of this version that they can send me? (I have Fetch, if it should  
> prove too large.) Alternatively, is there a way to run multiple  
> Raqua windows in the newest R?
>     Many thanks!
>     Chris Wills
> Christopher Wills
> Professor of Biological Sciences
> University of California, San Diego
> La Jolla CA 92093-0116
> Phone: 858-534-4113
> Fax: 858-534-7108
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From f.harrell at vanderbilt.edu  Mon May 23 02:43:35 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 22 May 2005 20:43:35 -0400
Subject: [R] Hmisc/Design and problem with cgroup
In-Reply-To: <4290D692.4060502@statistik.uni-dortmund.de>
References: <r02010500-1039-74085AC4CAF211D9B5AC000A959B3D22@[10.0.1.205]>
	<4290D692.4060502@statistik.uni-dortmund.de>
Message-ID: <42912737.3060202@vanderbilt.edu>

Uwe Ligges wrote:
> Aric Gregson wrote:
> 
>> On 5/17/05 21:44 Frank E Harrell Jr sent the following:
>>
>>
>>> Aric Gregson wrote:
>>>
>>>> Hello,
>>>>
>>>> I am trying to use the following to output a table to latex:
>>>>
>>>> cohortbyagesummary <- by(data.frame(age,ethnicity), cohort, summary)
>>>> w <- latex.default(cohortbyagesummary,    caption="Five Number Age 
>>>> Summaries by Cohort",
>>>>    label="agesummarybycohort",    cgroup=c('hello','goodbye','hello'),
>>>>    colheads=c("Age","Ethnicity"),
>>>>    extracolheads=c('hello','goodbye'),    greek=TRUE,
>>>>    ctable=TRUE)
>>>>    I am not able to get the major column headings of cgroup to work. I
>>>> receive the error:
>>>>    Object cline not found
>>>>
>>>
>>> See if a modified version at
>>> http://biostat.mc.vanderbilt.edu/cgi-bin/cvsweb.cgi/Hmisc/R/latex.s
>>> fixes your problem.
>>
>>
>>
>> Dr. Harrell,
>>
>> Thanks for your help. I have downloaded latex.s, but am unable to figure
>> out where it should go. How do I ensure that it is called when I load
>> Hmisc? Do I patch it against R/Hmisc?
>>
>> I have a local directory structure in ~/Library/R/library/Hmisc/:
>>
>> -rw-r--r--  1 user  user  594 12 May 19:20 CITATION
>> -rw-r--r--  1 user  user  989 12 May 19:20 DESCRIPTION
>> drwxr-xr-x  3 user  user  102 12 May 19:20 Meta/package.rds
>> drwxr-xr-x  3 user  user  102 12 May 19:20 R/Hmisc
>> drwxr-xr-x  3 user  user  102 12 May 19:20 libs/Hmisc.so
>>
>> Thanks for your patience.
> 
> 
> 
> Your directory structure is from an installed binary package. You need 
> to replace the file in the *source* package and *install* the source 
> package after that.
> 
> Uwe Ligges

A better approach is just to source( ) in the new version after 
library(Hmisc) is issued.  You can put latex.s anywhere, just put that 
path in the source( ).  A new version of Hmisc will include the fix.

Frank

> 
>> aric
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From dirk.enzmann at jura.uni-hamburg.de  Mon May 23 04:17:00 2005
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Mon, 23 May 2005 04:17:00 +0200
Subject: [R] skewness and kurtosis in e1071 correct?
Message-ID: <42913D1C.9080502@jura.uni-hamburg.de>

I wonder whether the functions for skewness and kurtosis in the e1071 
package are based on correct formulas.

The functions in the package e1071 are:

# --------------------------------------------
skewness <- function (x, na.rm = FALSE)
{
     if (na.rm)
         x <- x[!is.na(x)]
     sum((x - mean(x))^3)/(length(x) * sd(x)^3)
}
# --------------------------------------------

and

# --------------------------------------------
kurtosis <- function (x, na.rm = FALSE)
{
     if (na.rm)
         x <- x[!is.na(x)]
     sum((x - mean(x))^4)/(length(x) * var(x)^2) - 3
}
# --------------------------------------------

However, sd() and var() are the estimated population parameters. To 
calculate the sample statistics of skewness and kurtosis, shouldn't one 
use the sample statistics of the standard deviation (and variance), as 
well? For example:

# --------------------------------------------
# Function to calculate the sample statistic of skewness:
skew_s=function(x)
{
  x = x[!is.na(x)]
  n = length(x)
  if (n < 3)
  {
    cat('valid cases = ',n,'\nskewness is not defined for less than 3 
valid cases!\n')
  }
  else
  {
  z = sqrt(n/(n-1))*scale(x)
  mean(z^3)
  }
}
# --------------------------------------------

and

# --------------------------------------------
# Function to calculate the sample statistic of kurtosis:
kurt_s=function(x)
{
  x = x[!is.na(x)]
  n = length(x)
  if (n < 4)
  {
    cat('valid cases = ',n,'\nkurtosis is not defined for less than 4 
valid cases!\n')
  }
  else
  {
  z = sqrt(n/(n-1))*scale(x)
  mean(z^4)-3
  }
}
# --------------------------------------------

Whereas, to calculate the (unbiased) estimated population parameter of 
skewness and kurtosis, the correction should also include the number of 
cases in the following way:

# --------------------------------------------
# Function to calculate the unbiased populataion estimate of skewness:
skew=function(x)
{
  x = x[!is.na(x)]
  n = length(x)
  if (n < 3)
  {
    cat('valid cases = ',n,'\nskewness is not defined for less than 3 
valid cases!\n')
  }
  else
  {
  z = scale(x)
  sum(z^3)*n/((n-1)*(n-2))
  }
}
# --------------------------------------------

and

# --------------------------------------------
# Function to calculate the unbiased population estimate of kurtosis:
kurt=function(x)
{
  x = x[!is.na(x)]
  n = length(x)
  if (n < 4)
  {
    cat('valid cases = ',n,'\nkurtosis is not defined for less than 4 
valid cases!\n')
  }
  else
  {
  z = scale(x)
  sum(z^4)*n*(n+1)/((n-1)*(n-2)*(n-3))-3*(n-1)^2/((n-2)*(n-3))
  }
}
# --------------------------------------------

Thus, it seems that the formulas used in the e1071 package are neither 
formulas for the sample statistics nor for the (unbiased) estimates of 
the population parameters. Is there another definition of kurtosis and 
skewness that I am not aware of?

Dirk

-- 
*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From mikewhite.diu at tiscali.co.uk  Mon May 23 10:13:58 2005
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Mon, 23 May 2005 09:13:58 +0100
Subject: [R] Re: Finding the right number of clusters
Message-ID: <004701c55f6f$5f1271a0$0601a8c0@FSSFQCV7BGDVED>

Philip
I have been using the kgs function in the maptree package, although you
still need to decide on a weighting factor.

Mike White



From p.campbell at econ.bbk.ac.uk  Mon May 23 11:06:31 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Mon, 23 May 2005 10:06:31 +0100
Subject: [R] skewness and kurtosis in e1071 correct?
Message-ID: <s291ab49.079@markets.econ.bbk.ac.uk>

This is probably an issue over definitions rather than the correct
answer.  To me skewness and kurtosis are functions of the distribution
rather than the population, they are equivalent to expectation rather
than mean.  For the normal distribution it makes no sense to estimate
them as the distribution is uniquely defined by its first two  moments. 
 However there are two defnitions of kurotsis as it is often
standardized such that the expectation is 0.

HTH 

Phineas  

www.pwp.phineas.blueyonder.co.uk


>>> Dirk Enzmann <dirk.enzmann at jura.uni-hamburg.de> 05/23/05 3:17 AM >>>
I wonder whether the functions for skewness and kurtosis in the e1071 
package are based on correct formulas.

The functions in the package e1071 are:

# --------------------------------------------
skewness <- function (x, na.rm = FALSE)
{
     if (na.rm)
         x <- x[!is.na(x)]
     sum((x - mean(x))^3)/(length(x) * sd(x)^3)
}
# --------------------------------------------

and

# --------------------------------------------
kurtosis <- function (x, na.rm = FALSE)
{
     if (na.rm)
         x <- x[!is.na(x)]
     sum((x - mean(x))^4)/(length(x) * var(x)^2) - 3
}
# --------------------------------------------

However, sd() and var() are the estimated population parameters. To 
calculate the sample statistics of skewness and kurtosis, shouldn't one 
use the sample statistics of the standard deviation (and variance), as 
well? For example:

# --------------------------------------------
# Function to calculate the sample statistic of skewness:
skew_s=function(x)
{
  x = x[!is.na(x)]
  n = length(x)
  if (n < 3)
  {
    cat('valid cases = ',n,'\nskewness is not defined for less than 3 
valid cases!\n')
  }
  else
  {
  z = sqrt(n/(n-1))*scale(x)
  mean(z^3)
  }
}
# --------------------------------------------

and

# --------------------------------------------
# Function to calculate the sample statistic of kurtosis:
kurt_s=function(x)
{
  x = x[!is.na(x)]
  n = length(x)
  if (n < 4)
  {
    cat('valid cases = ',n,'\nkurtosis is not defined for less than 4 
valid cases!\n')
  }
  else
  {
  z = sqrt(n/(n-1))*scale(x)
  mean(z^4)-3
  }
}
# --------------------------------------------

Whereas, to calculate the (unbiased) estimated population parameter of 
skewness and kurtosis, the correction should also include the number of 
cases in the following way:

# --------------------------------------------
# Function to calculate the unbiased populataion estimate of skewness:
skew=function(x)
{
  x = x[!is.na(x)]
  n = length(x)
  if (n < 3)
  {
    cat('valid cases = ',n,'\nskewness is not defined for less than 3 
valid cases!\n')
  }
  else
  {
  z = scale(x)
  sum(z^3)*n/((n-1)*(n-2))
  }
}
# --------------------------------------------

and

# --------------------------------------------
# Function to calculate the unbiased population estimate of kurtosis:
kurt=function(x)
{
  x = x[!is.na(x)]
  n = length(x)
  if (n < 4)
  {
    cat('valid cases = ',n,'\nkurtosis is not defined for less than 4 
valid cases!\n')
  }
  else
  {
  z = scale(x)
  sum(z^4)*n*(n+1)/((n-1)*(n-2)*(n-3))-3*(n-1)^2/((n-2)*(n-3))
  }
}
# --------------------------------------------

Thus, it seems that the formulas used in the e1071 package are neither 
formulas for the sample statistics nor for the (unbiased) estimates of 
the population parameters. Is there another definition of kurtosis and 
skewness that I am not aware of?

Dirk

-- 
*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rene.eschen at unifr.ch  Mon May 23 11:14:29 2005
From: rene.eschen at unifr.ch (=?iso-8859-1?Q?Ren=E9?= Eschen)
Date: Mon, 23 May 2005 11:14:29 +0200
Subject: [R] using lme in csimtest
Message-ID: <6.2.0.14.1.20050523103000.01c3d678@mail.unifr.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050523/9614ab4a/attachment.pl

From esg at felix.unife.it  Mon May 23 11:46:09 2005
From: esg at felix.unife.it (Josef Eschgfaeller)
Date: Mon, 23 May 2005 11:46:09 +0200 (CEST)
Subject: [R] Backslash
Message-ID: <Pine.LNX.4.63.0505231142520.28449@dns.unife.it>


Why sometimes one has to put a double
backslash in regular expressions, but
often simple backslashes work too?
Is only a \ required for giving a
metacharacter its usual meaning?
---------------------------------------
u=grep('\\{[\\-u]x',a,perl=T)

       # equivalent to

u=grep('\{[\-u]x',a,perl=T)

       # but

u=grep('\w',a,perl=T)

       # is not correct and requires

u=grep('\\w',a,perl=T)
---------------------------------------
Josef Eschgf??ller

-- 
Josef Eschgf??ller
Dipartimento Matematico
Universita' di Ferrara
http://felix.unife.it

From marta at biometria.univr.it  Mon May 23 12:16:24 2005
From: marta at biometria.univr.it (Marta Rava)
Date: Mon, 23 May 2005 12:16:24 +0200
Subject: [R] confidence interval and cross validation
Message-ID: <200505231016.j4NAGLVD008626@hypatia.math.ethz.ch>

I've obstained the mean square error of prediction, using a 10 fold cross validation for a linear multiple regression model (I've used errortest, from package ipred), how can I calculate the confidence interval? And, could I use test set of cross validation to calculate correlation and concordance index? 
Thanks, Marta



From subianto at gmail.com  Mon May 23 12:22:28 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Mon, 23 May 2005 12:22:28 +0200
Subject: [R] Problem with label name in mosaics plot
Message-ID: <4291AEE4.6020204@gmail.com>

Dear all R-help,
I have a problem with label name in mosaics plot if I use more 4 variables.
Let see with my toy example.

library(vcd)
toy.mosaics <- 
array(c(96,44,1,138,64,2,117,56,6,75,48,5,72,49,6,83,60,8,140,
                     43,1,171,65,4,152,58,9,101,51,9,102,58,10,111,67,16,24,
                     5,2,18,7,1,16,7,3,12,6,4,6,8,3,4,10,4,21,4,1,25,6,2,20,
                     5,1,17,5,111,14,50,1,13,5,8),
               dim=c(4,4,2,3,2,6),
               dimnames=list(
                 Hair=c("Black", "Brown", "Red", "Blond"),
                 Eye=c("Green", "Hazel", "Blue", "Brown"),
                 Race=c("White", "NonWhite"),
                 Opinion=c("Yes", "No", "Und"),
                 Sex=c("Male", "Female"),
                 Age=c("18-25", "26-35", "36-45", "46-55", "56-65", "66+")))
    

# 2 variables
mosaicplot(~ Hair+Eye, data=toy.mosaics, color = TRUE)
x11()
# 3 variables
mosaicplot(~ Hair+Eye+Sex, data=toy.mosaics, color = TRUE)
x11()
# 4 variables
mosaicplot(~ Hair+Eye+Sex+Opinion, data=toy.mosaics, color = TRUE)
x11()
# 5 variables (where is label name Race?)
mosaicplot(~ Hair+Eye+Sex+Opinion+Race, data=toy.mosaics, color = TRUE)
x11()
# 6 variables (where are label name Race and Age?)
mosaicplot(~ Hair+Eye+Sex+Opinion+Race+Age, data=toy.mosaics, color = TRUE)

I think I am wrong to put a model or something wrong in mosaicplot?
I was wondering if someone can help me.
Kindly regards,
Muhammad Subianto

 > version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R              
 
 > packageDescription("vcd")
Package: vcd
Version: 0.1-3.5
Date: 2005-02-28
Title: Visualizing Categorical Data
Author: David Meyer, Achim Zeileis, Alexandros Karatzoglou, Kurt
        Hornik
Maintainer: Kurt Hornik <Kurt.Hornik at R-project.org>
Description: Functions and data sets based on the book "Visualizing
        Categorical Data" by Michael Friendly.
License: GPL
Depends: R (>= 1.4.0), MASS
Packaged: Mon Feb 28 21:02:02 2005; hornik
Built: R 2.1.0; ; 2005-04-09 21:52:15; windows



From kran at ict.fraunhofer.de  Mon May 23 11:56:42 2005
From: kran at ict.fraunhofer.de (Kraft, Andreas)
Date: Mon, 23 May 2005 11:56:42 +0200
Subject: [R] How to convert a character string to a number
Message-ID: <F9771783488F5740A25F86D90F4C24AD0287CE8A@a1-balu.ict.fhg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050523/4d519dc3/attachment.pl

From amsa36060 at yahoo.com  Mon May 23 12:27:49 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Mon, 23 May 2005 03:27:49 -0700 (PDT)
Subject: [R] Dickey-Fuller Test
Message-ID: <20050523102749.84707.qmail@web60414.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050523/1ed99bb4/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Mon May 23 12:28:56 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 23 May 2005 12:28:56 +0200
Subject: [R] Dickey-Fuller Test
In-Reply-To: <20050523102749.84707.qmail@web60414.mail.yahoo.com>
References: <20050523102749.84707.qmail@web60414.mail.yahoo.com>
Message-ID: <20050523122856.56de02a3.Achim.Zeileis@wu-wien.ac.at>

On Mon, 23 May 2005 03:27:49 -0700 (PDT) Amir Safari wrote:

>  
>  
> Hi All ,
> Could you please tell using which library, Dickey-Fuller Test can be
> run? Thanks a lot

That depends: in which *library* have you installed the tseries
*package*? (Look at the archives for the difference between libraries
and packages.)

The augmented Dickey-Fuller test is available in tseries (and also in
urca and uroot).
Z
 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From maarranz at tol-project.org  Mon May 23 13:44:13 2005
From: maarranz at tol-project.org (Miguel A. Arranz)
Date: Mon, 23 May 2005 13:44:13 +0200
Subject: [R] Dickey-Fuller Test
In-Reply-To: <20050523102749.84707.qmail@web60414.mail.yahoo.com>
References: <20050523102749.84707.qmail@web60414.mail.yahoo.com>
Message-ID: <200505231344.13443.maarranz@tol-project.org>

tseries (adf.test)


On Monday 23 May 2005 12:27, Amir Safari wrote:
> Hi All ,
> Could you please tell using which library ,Dickey-Fuller Test can be run?
> Thanks a lot
>
> __________________________________________________
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
*************************
Miguel A. Arranz
Tol-Project
maarranz at tol-project.org



From Allan at STATS.uct.ac.za  Mon May 23 12:49:13 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Mon, 23 May 2005 12:49:13 +0200
Subject: [R] Dickey-Fuller Test
References: <20050523102749.84707.qmail@web60414.mail.yahoo.com>
Message-ID: <4291B528.9B422B8A@STATS.uct.ac.za>

hello

i ammended the code. it was in some package. dont know which? try
tseries. this might be of help.


adf.test.1<-function (x,kind=3,k = trunc((length(x)- 1)^(1/3))) 
{

#kind = the kind of test undertaken
#kind = 1 ==> No constant no trend
#kind = 2 ==> Constant
#kind = 3 ==> Constant and trend

#the null is ALWAYS non stationarity

    if (NCOL(x) > 1) 
        stop("x is not a vector or univariate time series")

    if (any(is.na(x))) 
        stop("NAs in x")

    if (k < 0) 
        stop("k negative")

    DNAME <- deparse(substitute(x))

    k <- k + 1
    y <- diff(x)
    n <- length(y)
    z <- embed(y, k)
    yt <- z[,1]
    xt1 <- x[k:n]
    tt <- k:n

    if (kind==1)
    {
    table <- cbind(c(2.66, 2.62, 2.6, 2.58, 2.58, 2.58), c(2.26, 
        2.25, 2.24, 2.23, 2.23, 2.23), c(1.95, 1.95, 1.95, 1.95, 
        1.95, 1.95), c(1.60, 1.61, 1.61, 1.62, 1.62, 1.62), c(0.92, 
        0.91, 0.90, 0.89, 0.89, 0.89), c(1.33, 1.31, 1.29, 1.29, 
        1.28, 1.28), c(1.70, 1.66, 1.64, 1.63, 1.62, 1.62), c(2.16, 
        2.08, 2.03, 2.01, 2.00, 2.00))

    if (k > 1) 
    {
        yt1 <- z[,2:k]
        res <- lm(yt ~ xt1 - 1 + yt1)
    }
    else res <- lm(yt ~ xt1-1)
    res.sum <- summary(res)
    STAT <- res.sum$coefficients[1,1]/res.sum$coefficients[1,2]
    
    }

    if (kind==2)
    {
    table <- cbind(c(3.75, 3.58, 3.51, 3.46, 3.44, 3.43), c(3.33, 
        3.22, 3.17, 3.14, 3.13, 3.12), c(3.00, 2.93, 2.89, 2.88, 
        2.87, 2.86), c(2.62, 2.60, 2.58, 2.57, 2.57, 2.57), c(0.37, 
        0.40, 0.42, 0.42, 0.43, 0.44), c(0.00, 0.03, 0.05, 0.06, 
        0.07, 0.07), c(0.34, 0.29, 0.26, 0.24, 0.24, 0.23), c(0.72, 
        0.66, 0.63, 0.62, 0.61, 0.60))

    if (k > 1) 
    {
        yt1 <- z[,2:k]
        res <- lm(yt ~ xt1 + 1 + yt1)
    }
    else res <- lm(yt ~ xt1 + 1)
    res.sum <- summary(res)
    STAT <- res.sum$coefficients[2,1]/res.sum$coefficients[2,2]

    }

    if (kind==3)
    {
    table <- cbind(c(4.38, 4.15, 4.04, 3.99, 3.98, 3.96), c(3.95, 
        3.8, 3.73, 3.69, 3.68, 3.66), c(3.6, 3.5, 3.45, 3.43, 
        3.42, 3.41), c(3.24, 3.18, 3.15, 3.13, 3.13, 3.12), c(1.14, 
        1.19, 1.22, 1.23, 1.24, 1.25), c(0.8, 0.87, 0.9, 0.92, 
        0.93, 0.94), c(0.5, 0.58, 0.62, 0.64, 0.65, 0.66), c(0.15, 
        0.24, 0.28, 0.31, 0.32, 0.33))

    if (k > 1) 
    {
        yt1 <- z[,2:k]
        res <- lm(yt ~ xt1 + 1 + tt + yt1)
    }
    else res <- lm(yt ~ xt1 + 1 + tt)
    res.sum <- summary(res)
    STAT <- res.sum$coefficients[2,1]/res.sum$coefficients[2,2]

    }

    

    table <- -table
    tablen <- dim(table)[2]
    tableT <- c(25, 50, 100, 250, 500, 1e+05)
    tablep <- c(0.01, 0.025, 0.05, 0.1, 0.9, 0.95, 0.975, 0.99)
    tableipl <- numeric(tablen)

    for (i in (1:tablen)) tableipl[i] <- approx(tableT, table[,i], n,
rule = 2)$y
    interpol <- approx(tableipl, tablep, STAT, rule = 2)$y

    if (is.na(approx(tableipl, tablep, STAT, rule = 1)$y)) 
        if (interpol == min(tablep)) 
            warning("p-value smaller than printed p-value")
        else warning("p-value greater than printed p-value")

    PVAL <- interpol

    PARAMETER <- k - 1
    METHOD <- "Augmented Dickey-Fuller Test"
    names(STAT) <- "Dickey-Fuller"
    names(PARAMETER) <- "Lag order"

    structure(list(statistic = STAT, parameter = PARAMETER,alternative =
"The series is stationary",
    p.value = PVAL, method = METHOD, data.name = DNAME),class = "htest")

}


Amir Safari wrote:
> 
> 
> 
> Hi All ,
> Could you please tell using which library ,Dickey-Fuller Test can be run?
> Thanks a lot
> 
> __________________________________________________
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

From Roger.Bivand at nhh.no  Mon May 23 13:08:25 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 23 May 2005 13:08:25 +0200 (CEST)
Subject: [R] Maps, Eastern Europe
Message-ID: <Pine.LNX.4.44.0505231258540.9661-100000@reclus.nhh.no>

Ray Brownrigg wrote:

> Unfortunately the world databases for the maps package are very old
> (early 1990's) and have not been updated since, except for minor
> typographical errors in country names.  Note that Czechoslovakia is
> there (as is USSR!).  Also, it currently satisfies my needs, which is
> why I don't have the resources to update it.

> I suggest you start with the maptools package, but as I understand it,
> you need to also find some appropriate shapefile data. [Other
> respondents may improve upon this suggestion.]

http://meta.wikimedia.org/wiki/Geographical_data 

provides some links, including one to:

http://www.esri.com/data/download/basemap/index.html

permitting the downloading of various layers as shapefiles that can be
read and handled by the maptools package. If you choose just country
boundaries, you'll get a lat-long rectangle, including the countries you
want (and (parts of) countries you don't). You may need to subset your
requests, because the website restricts how much you can choose at one go.  
Please contact me off-list or move this to the R-sig-geo list for more
information.

Roger

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From georg.otto at tuebingen.mpg.de  Mon May 23 15:03:53 2005
From: georg.otto at tuebingen.mpg.de (Georg Otto)
Date: Mon, 23 May 2005 15:03:53 +0200
Subject: [R] R 2.1 installation on Mac OS 10.2
Message-ID: <1D01EAF2-CB8B-11D9-9E55-003065C99468@tuebingen.mpg.de>

Hi,

I am trying to install R 2.1 from source on a Mac 10.2.8, Darwin Kernel 
Version 6.8

According to the FAQ at  
http://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html

I proceeded in the following manner:

>  ./configure --with-blas='-framework vecLib' --with-lapack --with-aqua

This resulted in the following message:


> R is now configured for powerpc-apple-darwin6.8
>
>   Source directory:          .
>   Installation directory:    /Library/Frameworks
>
>   C compiler:                gcc  -g -O2
>   C++ compiler:              g++  -g -O2
>   Fortran compiler:          g77  -g -O2
>
>   Interfaces supported:      X11, aqua, tcltk
>   External libraries:        readline, BLAS(generic), LAPACK(in blas)
>   Additional capabilities:   PNG, JPEG, MBCS, NLS
>   Options enabled:           framework, R profiling
>
>   Recommended packages:      yes

make then gives me the following error message:

....
> config.status: creating src/unix/Makefile
> making dynload.d from dynload.c
> making edit.d from edit.c
> making stubs.d from stubs.c
> making system.d from system.c
> making sys-unix.d from sys-unix.c
> making sys-std.d from sys-std.c
> making X11.d from X11.c
> making aqua.d from aqua.c
> making dlfcn-darwin.d from dlfcn-darwin.c
> gcc -no-cpp-precomp -I. -I../../src/include -I../../src/include 
> -I/usr/X11R6/include -I/sw/include -I/usr/local/include 
> -DHAVE_CONFIG_H  -fno-common  -g -O2 -c dynload.c -o dynload.lo
> In file included from dynload.c:37:
> ../../src/include/Defn.h:886: error: parse error before "mbstate_t"
> make[3]: *** [dynload.lo] Error 1
> make[2]: *** [R] Error 2
> make[1]: *** [R] Error 1
> make: *** [R] Error 1
>


Could anyone give me a hint how to work around this?

Best,

Georg

P.S. I am on digest mode, cc??ing to me directly would be highly 
appreciated



From sekemp at glam.ac.uk  Mon May 23 15:03:17 2005
From: sekemp at glam.ac.uk (Samuel E. Kemp)
Date: Mon, 23 May 2005 14:03:17 +0100
Subject: [R] dse1 package simulate example
Message-ID: <a4f043772fe9a95642390fdffec1a2d4@glam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050523/dccde743/attachment.pl

From tlumley at u.washington.edu  Mon May 23 15:51:32 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 23 May 2005 06:51:32 -0700 (PDT)
Subject: [R] comparison operator, decimals, and signif()
In-Reply-To: <20050522003931.98935.qmail@web50910.mail.yahoo.com>
References: <20050522003931.98935.qmail@web50910.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0505230645560.369154@homer07.u.washington.edu>

On Sat, 21 May 2005, Nick Drew wrote:
>
> What does signif() do to my object called "testMean"
> so that the comparison now evaluates to TRUE?
>
>> signif(testMean, 3) == 82.9
> [1] TRUE
>

It rounds to 3 significant digits.

  An even more reliable approach is
    round(testMean*10) == 829
since 829 is an integer and exactly representable.

 	-thomas



From tlumley at u.washington.edu  Mon May 23 15:56:45 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 23 May 2005 06:56:45 -0700 (PDT)
Subject: [R] constraints
In-Reply-To: <200505222246.34947.ahenningsen@agric-econ.uni-kiel.de>
References: <BEB65692.52A2%i.visser@uva.nl>
	<200505222246.34947.ahenningsen@agric-econ.uni-kiel.de>
Message-ID: <Pine.A41.4.61b.0505230655310.369154@homer07.u.washington.edu>

On Sun, 22 May 2005, Arne Henningsen wrote:
> constrOptim by two inequality constraints:
>   a - b >= 0
> and
>   b - a >= 0
> However, I guess that the solver prefers the first way ;-)

It is not possible.  The starting value must be in the interior of the 
feasible region (because the constraints are implemented with log 
barriers).  This approach gives a feasible region with no interior.

 	-thomas



From erithid at bellsouth.net  Mon May 23 16:45:38 2005
From: erithid at bellsouth.net (BJ)
Date: Mon, 23 May 2005 10:45:38 -0400
Subject: [R] formals or args question
Message-ID: <4291EC92.8010309@bellsouth.net>

Hello again, thank you again for all of your help. I am trying to get 
the arguments for a function, in a way that I can handle them. If I do 
args(fun), it doesnt really do what I want. If I use formals(fun) i get 
output like:

$a


$r


$s


Now, lthis is supposedly a list, but if i say formals(a)[[1]], I get 
nothing. I am just trying to get a vector with teh arguments  of a 
function so I can loop over them.

ex; a<-function(p,r){p+r}

want: vector of c("p", "r")

The documentation for args and formals is quite confusing. Thanks again! 
~Erithid



From ales.ziberna at guest.arnes.si  Mon May 23 16:55:23 2005
From: ales.ziberna at guest.arnes.si (=?iso-8859-1?Q?Ales_Ziberna?=)
Date: Mon, 23 May 2005 16:55:23 +0200
Subject: [R] formals or args question
References: <4291EC92.8010309@bellsouth.net>
Message-ID: <009c01c55fa7$751b0790$598debd4@ales>

names(formals(fun))

You need the names of the elements of the list formals produces, not their 
content!

Ales Ziberna

----- Original Message ----- 
From: "BJ" <erithid at bellsouth.net>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, May 23, 2005 4:45 PM
Subject: [R] formals or args question


> Hello again, thank you again for all of your help. I am trying to get the
> arguments for a function, in a way that I can handle them. If I do
> args(fun), it doesnt really do what I want. If I use formals(fun) i get
> output like:
>
> $a
>
>
> $r
>
>
> $s
>
>
> Now, lthis is supposedly a list, but if i say formals(a)[[1]], I get
> nothing. I am just trying to get a vector with teh arguments  of a
> function so I can loop over them.
>
> ex; a<-function(p,r){p+r}
>
> want: vector of c("p", "r")
>
> The documentation for args and formals is quite confusing. Thanks again!
> ~Erithid
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>



From erithid at bellsouth.net  Mon May 23 16:59:59 2005
From: erithid at bellsouth.net (BJ)
Date: Mon, 23 May 2005 10:59:59 -0400
Subject: [R] formals or args question
In-Reply-To: <009c01c55fa7$751b0790$598debd4@ales>
References: <4291EC92.8010309@bellsouth.net>
	<009c01c55fa7$751b0790$598debd4@ales>
Message-ID: <4291EFEF.7000003@bellsouth.net>

Ahh. Thanks a lot! works great. :-p

Ales Ziberna wrote:

> names(formals(fun))
>
> You need the names of the elements of the list formals produces, not 
> their content!
>
> Ales Ziberna
>
> ----- Original Message ----- From: "BJ" <erithid at bellsouth.net>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, May 23, 2005 4:45 PM
> Subject: [R] formals or args question
>
>
>> Hello again, thank you again for all of your help. I am trying to get 
>> the
>> arguments for a function, in a way that I can handle them. If I do
>> args(fun), it doesnt really do what I want. If I use formals(fun) i get
>> output like:
>>
>> $a
>>
>>
>> $r
>>
>>
>> $s
>>
>>
>> Now, lthis is supposedly a list, but if i say formals(a)[[1]], I get
>> nothing. I am just trying to get a vector with teh arguments  of a
>> function so I can loop over them.
>>
>> ex; a<-function(p,r){p+r}
>>
>> want: vector of c("p", "r")
>>
>> The documentation for args and formals is quite confusing. Thanks again!
>> ~Erithid
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon May 23 16:58:24 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 23 May 2005 16:58:24 +0200
Subject: [R] formals or args question
References: <4291EC92.8010309@bellsouth.net>
Message-ID: <002c01c55fa7$de4b6f70$0540210a@www.domain>

formals() returns a list, so you could try this:

a <- function(p, r) p + r
names(formals(a))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "BJ" <erithid at bellsouth.net>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, May 23, 2005 4:45 PM
Subject: [R] formals or args question


> Hello again, thank you again for all of your help. I am trying to 
> get the arguments for a function, in a way that I can handle them. 
> If I do args(fun), it doesnt really do what I want. If I use 
> formals(fun) i get output like:
>
> $a
>
>
> $r
>
>
> $s
>
>
> Now, lthis is supposedly a list, but if i say formals(a)[[1]], I get 
> nothing. I am just trying to get a vector with teh arguments  of a 
> function so I can loop over them.
>
> ex; a<-function(p,r){p+r}
>
> want: vector of c("p", "r")
>
> The documentation for args and formals is quite confusing. Thanks 
> again! ~Erithid
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Eugene.Glover at sanofi-aventis.com  Mon May 23 17:16:29 2005
From: Eugene.Glover at sanofi-aventis.com (Eugene.Glover@sanofi-aventis.com)
Date: Mon, 23 May 2005 11:16:29 -0400
Subject: [R] Trouble with drplot 
Message-ID: <9C1DED999162AD41815C721A7FDD80EB0FAABB@cbdsmxsusr01.pharma.aventis.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050523/343c7ba5/attachment.pl

From gunter.berton at gene.com  Mon May 23 17:28:50 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 23 May 2005 08:28:50 -0700
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <200505231528.j4NFSp4k008377@faraday.gene.com>

 

Newbies (and others!) may find useful the R Reference Card made available by
Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
the "Contributed" link on CRAN (where some other reference cards are also
linked). It categorizes and organizes a bunch of R's basic, most used
functions so that they can be easily found. For example, paste() is under
the "Strings" heading and expand.grid() is under "Data Creation." For
newbies struggling to find the right R function as well as veterans who
can't quite remember the function name, it's very handy.
 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jeff.hamann at forestinformatics.com  Mon May 23 17:35:27 2005
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Mon, 23 May 2005 08:35:27 -0700 (PDT)
Subject: [R] colors and palettes and things... 
Message-ID: <2088.128.193.139.69.1116862527.squirrel@www.forestinformatics.com>

After trying to find if there was a color picker in the FAQs and the help,
I thought I would send a post here. I was overwhelmed with all the
wonderful color choices R has predefined (discovered after typing in
colors()) but can't figure out what they all (by name) look like. Is there
a color picker or some other method to display all those colors next to
the name?

I think I can put together palettes, but another question I have then
regards the building of palettes (a list of variable length I can select
or create myself other than the ones defined by Palette) so I can pass
these colors into functions instead of having to predefine a bunch of
colors myself or use the predefined colors like terrain.colors(n)?

Are there groups of colors in the colors() that I can group together to
make some nice palettes for drawing barplots, etc?

Thanks,
Jeff.


-- 
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421
phone 541-754-1428
fax 541-752-0288
jeff.hamann at forestinformatics.com
www.forestinformatics.com



From ales.ziberna at guest.arnes.si  Mon May 23 17:36:38 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Mon, 23 May 2005 17:36:38 +0200
Subject: [R] Trouble with drplot 
References: <9C1DED999162AD41815C721A7FDD80EB0FAABB@cbdsmxsusr01.pharma.aventis.com>
Message-ID: <00cf01c55fad$366f1e40$598debd4@ales>

Does your data (that you want to plot have any "Inf" or "-Inf" values?

Ales Ziberna

----- Original Message ----- 
From: <Eugene.Glover at sanofi-aventis.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, May 23, 2005 5:16 PM
Subject: [R] Trouble with drplot


> Hi, I am a newbie with R, so I hope my question isn't too stupid. I am 
> trying to generate dose-response curves using the "drfit" package. I have 
> formatted my CSV files to the correct format, and have no trouble running 
> drfit to get a summary of my data. The problem is that when I try to use 
> "drplot" to graph my data I get an error. The message is:
>
> Error in plot.window(xlim, ylim, log, asp, ...) :
>        need finite 'xlim' values
>
> If someone could please tell me what I am doing wrong, I would much 
> appreciate it. Thank you and have a blessed day.
>
> Eugene Glover
> Sanofi-Aventis
> 26 Landsdowne St
> 4th floor
> Cambridge MA 02139
> 617-768-4079
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From murdoch at stats.uwo.ca  Mon May 23 17:49:01 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 23 May 2005 16:49:01 +0100
Subject: [R] Backslash
In-Reply-To: <Pine.LNX.4.63.0505231142520.28449@dns.unife.it>
References: <Pine.LNX.4.63.0505231142520.28449@dns.unife.it>
Message-ID: <4291FB6D.1000003@stats.uwo.ca>

Josef Eschgfaeller wrote:
> Why sometimes one has to put a double
> backslash in regular expressions, but
> often simple backslashes work too?
> Is only a \ required for giving a
> metacharacter its usual meaning?

The general reason is that both R and grep use \ as an escape character. 
  If you want to send an escape to grep you need to escape the escape in R.

> ---------------------------------------
> u=grep('\\{[\\-u]x',a,perl=T)
> 
>        # equivalent to
> 
> u=grep('\{[\-u]x',a,perl=T)

No.  The first one passes the string containing "\{[\-u]x" to grep 
(after processing the escapes).  The second one passes "{[-u]x" to grep.
> 
>        # but
> 
> u=grep('\w',a,perl=T)

\w has no special meaning in R, so this pattern becomes "w".
> 
>        # is not correct and requires
> 
> u=grep('\\w',a,perl=T)

This is the pattern "\w".  I don't know if this has special meaning to 
pcre, but I guess it must, or you wouldn't have tried it.

Duncan Murdoch

> ---------------------------------------
> Josef Eschgf??ller
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Eugene.Glover at sanofi-aventis.com  Mon May 23 17:54:46 2005
From: Eugene.Glover at sanofi-aventis.com (Eugene.Glover@sanofi-aventis.com)
Date: Mon, 23 May 2005 11:54:46 -0400
Subject: [R] Trouble with drplot 
Message-ID: <9C1DED999162AD41815C721A7FDD80EB0FAABC@cbdsmxsusr01.pharma.aventis.com>

I suppose that was the problem. I had my control values included as concentration of 0. My impression was that if I put "no fit" in the OK column they would be ignored when graphing. However, I guess that is not the case, because when I took them out the plot worked. Thanks a lot.

Gene

-----Original Message-----
From: Ale? ?iberna [mailto:ales.ziberna at guest.arnes.si]
Sent: Monday, May 23, 2005 11:37 AM
To: Glover, Eugene PH/US/EXT; r-help at stat.math.ethz.ch
Subject: Re: [R] Trouble with drplot 


Does your data (that you want to plot have any "Inf" or "-Inf" values?

Ales Ziberna

----- Original Message ----- 
From: <Eugene.Glover at sanofi-aventis.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, May 23, 2005 5:16 PM
Subject: [R] Trouble with drplot


> Hi, I am a newbie with R, so I hope my question isn't too stupid. I am 
> trying to generate dose-response curves using the "drfit" package. I have 
> formatted my CSV files to the correct format, and have no trouble running 
> drfit to get a summary of my data. The problem is that when I try to use 
> "drplot" to graph my data I get an error. The message is:
>
> Error in plot.window(xlim, ylim, log, asp, ...) :
>        need finite 'xlim' values
>
> If someone could please tell me what I am doing wrong, I would much 
> appreciate it. Thank you and have a blessed day.
>
> Eugene Glover
> Sanofi-Aventis
> 26 Landsdowne St
> 4th floor
> Cambridge MA 02139
> 617-768-4079
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Mon May 23 17:56:15 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Mon, 23 May 2005 11:56:15 -0400
Subject: [R] colors and palettes and things... 
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4078@us-arlington-0668.mail.saic.com>

Colors predefined in R follow closely colors predefined in in HTML language.
See:
http://users.rcn.com/giant.interport/COLOR/1ColorSpecifier.html 
http://www.brobstsystems.com/colors1.htm
http://www.geocities.com/html4kids/colors.htm 
And probably countless other websites defining them.

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeff D. Hamann
Sent: Monday, May 23, 2005 11:35 AM
To: r-help at stat.math.ethz.ch
Subject: [R] colors and palettes and things... 

After trying to find if there was a color picker in the FAQs and the help, I
thought I would send a post here. I was overwhelmed with all the wonderful
color choices R has predefined (discovered after typing in
colors()) but can't figure out what they all (by name) look like. Is there a
color picker or some other method to display all those colors next to the
name?

I think I can put together palettes, but another question I have then
regards the building of palettes (a list of variable length I can select or
create myself other than the ones defined by Palette) so I can pass these
colors into functions instead of having to predefine a bunch of colors
myself or use the predefined colors like terrain.colors(n)?

Are there groups of colors in the colors() that I can group together to make
some nice palettes for drawing barplots, etc?

Thanks,
Jeff.


--
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421
phone 541-754-1428
fax 541-752-0288
jeff.hamann at forestinformatics.com
www.forestinformatics.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From vanmeter at pobox.com  Mon May 23 18:42:32 2005
From: vanmeter at pobox.com (Karla Van Meter)
Date: Mon, 23 May 2005 09:42:32 -0700
Subject: [R] installing R and Rcmdr onMacOS 10.3.9
Message-ID: <614b4191d4347181619215bfe78e989f@pobox.com>

I have been trying to install the new, patched R 2.1.0a along with the  
Rcmdr on my mac. I got the basic R package installed, but in Project  
Manager, the tcltk package will not install.

  When I downloaded the Rcmdr binary package from the Berkeley site, it  
did not show up on the package installer list until a second attempt,  
but it still does not show up on the package manager list.

On the package manager list, the tcltk package is listed as ''not  
installed'. When I click on it to install, it won't. I tried the  
library(tcltk) command, which failed.

The following is the output from the last command:

 > library(Rcmdr)
Loading required package: tcltk
Loading Tcl/Tk interface ... Error in dyn.load(x, as.logical(local),  
as.logical(now)) :
	unable to load shared library  
'/Library/Frameworks/R.framework/Resources/library/tcltk/libs/ 
tcltk.so':
   dlcompat: dyld: /Applications/R.app/Contents/MacOS/R can't open  
library: /usr/X11R6/lib/libX11.6.dylib  (No such file or directory,  
errno = 2)
Error: .onLoad failed in 'loadNamespace' for 'tcltk'
Error: package 'tcltk' could not be loaded
 > library(tcltk)
Loading Tcl/Tk interface ... Error in dyn.load(x, as.logical(local),  
as.logical(now)) :
	unable to load shared library  
'/Library/Frameworks/R.framework/Resources/library/tcltk/libs/ 
tcltk.so':
   dlcompat: dyld: /Applications/R.app/Contents/MacOS/R can't open  
library: /usr/X11R6/lib/libX11.6.dylib  (No such file or directory,  
errno = 2)
Error: .onLoad failed in 'loadNamespace' for 'tcltk'
Error: package/namespace load failed for 'tcltk'
 >

I am NOT a developer, but an application-oriented grad student.

I'd appreciate any help that will make Rcmdr work.

Thanks,

Karla Van Meter
tel: 707.765.0420
net: kcvanmeter at ucdavis.edu



From parris at isciences.com  Mon May 23 20:01:38 2005
From: parris at isciences.com (Thomas M. Parris)
Date: Mon, 23 May 2005 14:01:38 -0400
Subject: [R] Can't reproduce clusplot princomp results.
Message-ID: <200505231801.j4NI1i7S005677@magnolia.isciences.com>

Dear R folk:

Perhaps I'm just dense today, but I am having trouble reproducing the
principal components plotted and summarized by clusplot.  Here is a brief
example using the pluton dataset.  clusplot reports that the first two
principal components explain 99.7% of the variability.  But this is not what
princomp is reporting.  I would greatly appreciate any advice.

With best regards,
-- Tom

> R.version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R         

> require("cluster")
[1] TRUE
> pluton.agnes <- agnes(pluton)
> clusters <- cutree(as.hclust(pluton.agnes), h=4.00)
> clusplot(pluton, clusters, lines=0)
> pca <- princomp(pluton, cor=TRUE)
> loadings(pca)

Loadings:
      Comp.1 Comp.2 Comp.3 Comp.4
Pu238  0.521  0.348  0.714  0.313
Pu239 -0.540                0.837
Pu240  0.418 -0.835         0.353
Pu241  0.512  0.418 -0.698  0.277

               Comp.1 Comp.2 Comp.3 Comp.4
SS loadings      1.00   1.00   1.00   1.00
Proportion Var   0.25   0.25   0.25   0.25
Cumulative Var   0.25   0.50   0.75   1.00



From br44114 at gmail.com  Mon May 23 20:04:41 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Mon, 23 May 2005 14:04:41 -0400
Subject: [R] colors and palettes and things...
Message-ID: <8d5a3635050523110421af18b4@mail.gmail.com>

1. I faced the same issue and came up with the code below. 
2. See rainbow().

allcol <- colors()
png("Rcolors.png",width=1100,height=3000)
par(mai=c(0.4,0.5,0.3,0.2),omi=c(0.2,0,0,0),cex.axis=0.1,pch=15,bg="white")
plot(1,1,xlim=c(1,10),ylim=c(1,66),col=allcol[1],cex=4)
axis(1,at=NULL,labels=FALSE,tick=FALSE)
clr <- 1
for (i in 66:1) for (j in 10:1) {
	if (clr > length(allcol)) break
	points(j,i,col=allcol[clr],cex=4)
	text(j,i,allcol[clr],cex=0.8,pos=1)
	clr <- clr + 1
	}
dev.off()



-----Original Message-----
From: Jeff D. Hamann [mailto:jeff.hamann at forestinformatics.com]
Sent: Monday, May 23, 2005 11:35 AM
To: r-help at stat.math.ethz.ch
Subject: [R] colors and palettes and things... 


After trying to find if there was a color picker in the FAQs and the help,
I thought I would send a post here. I was overwhelmed with all the
wonderful color choices R has predefined (discovered after typing in
colors()) but can't figure out what they all (by name) look like. Is there
a color picker or some other method to display all those colors next to
the name?

I think I can put together palettes, but another question I have then
regards the building of palettes (a list of variable length I can select
or create myself other than the ones defined by Palette) so I can pass
these colors into functions instead of having to predefine a bunch of
colors myself or use the predefined colors like terrain.colors(n)?

Are there groups of colors in the colors() that I can group together to
make some nice palettes for drawing barplots, etc?

Thanks,
Jeff.


-- 
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421
phone 541-754-1428
fax 541-752-0288
jeff.hamann at forestinformatics.com
www.forestinformatics.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From pohl at marge.statistik.uni-koeln.de  Mon May 23 20:28:07 2005
From: pohl at marge.statistik.uni-koeln.de (Stefan Pohl)
Date: Mon, 23 May 2005 20:28:07 +0200
Subject: [R] Left truncation in shared frailty models with time-varying
	covariates
Message-ID: <027c01c55fc5$3713fce0$1b9e5f86@simurechner>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050523/f2206f48/attachment.pl

From jfox at mcmaster.ca  Mon May 23 20:28:28 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 23 May 2005 14:28:28 -0400
Subject: [R] colors and palettes and things... 
In-Reply-To: <2088.128.193.139.69.1116862527.squirrel@www.forestinformatics.com>
Message-ID: <20050523182827.HZDI27508.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Jeff,

Some time ago, Don McQueen posted to r-help a nice function for displaying
the named colours. You'll find it at
<http://finzi.psych.upenn.edu/R/Rhelp02a/archive/31607.html>. As well, if
you're using R for Windows, the various named colours are defined in the
file rgb.txt in R's etc directory. 

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeff D. Hamann
> Sent: Monday, May 23, 2005 10:35 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] colors and palettes and things... 
> 
> After trying to find if there was a color picker in the FAQs 
> and the help, I thought I would send a post here. I was 
> overwhelmed with all the wonderful color choices R has 
> predefined (discovered after typing in
> colors()) but can't figure out what they all (by name) look 
> like. Is there a color picker or some other method to display 
> all those colors next to the name?
> 
> I think I can put together palettes, but another question I 
> have then regards the building of palettes (a list of 
> variable length I can select or create myself other than the 
> ones defined by Palette) so I can pass these colors into 
> functions instead of having to predefine a bunch of colors 
> myself or use the predefined colors like terrain.colors(n)?
> 
> Are there groups of colors in the colors() that I can group 
> together to make some nice palettes for drawing barplots, etc?
> 
> Thanks,
> Jeff.
> 
> 
> --
> Jeff D. Hamann
> Forest Informatics, Inc.
> PO Box 1421
> Corvallis, Oregon 97339-1421
> phone 541-754-1428
> fax 541-752-0288
> jeff.hamann at forestinformatics.com
> www.forestinformatics.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Mon May 23 20:32:29 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 23 May 2005 14:32:29 -0400
Subject: [R] installing R and Rcmdr onMacOS 10.3.9
In-Reply-To: <614b4191d4347181619215bfe78e989f@pobox.com>
Message-ID: <20050523183228.KLXN26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Karla,

It's likely that you don't have Tcl/Tk installed on your Mac.

I know that it's possible to get the Rcmdr working on a Mac, but that doing
so requires some additional steps. I'm not a Mac user, so I'm afraid that I
can't be of much direct help. If someone who's using the Rcmdr on a Mac is
willing to prepare notes about how to get it installed and working, I'd
gladly post them on my web site.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Karla Van Meter
> Sent: Monday, May 23, 2005 11:43 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] installing R and Rcmdr onMacOS 10.3.9
> 
> I have been trying to install the new, patched R 2.1.0a along 
> with the Rcmdr on my mac. I got the basic R package 
> installed, but in Project Manager, the tcltk package will not install.
> 
>   When I downloaded the Rcmdr binary package from the 
> Berkeley site, it did not show up on the package installer 
> list until a second attempt, but it still does not show up on 
> the package manager list.
> 
> On the package manager list, the tcltk package is listed as 
> ''not installed'. When I click on it to install, it won't. I tried the
> library(tcltk) command, which failed.
> 
> The following is the output from the last command:
> 
>  > library(Rcmdr)
> Loading required package: tcltk
> Loading Tcl/Tk interface ... Error in dyn.load(x, as.logical(local),
> as.logical(now)) :
> 	unable to load shared library
> '/Library/Frameworks/R.framework/Resources/library/tcltk/libs/
> tcltk.so':
>    dlcompat: dyld: /Applications/R.app/Contents/MacOS/R can't open
> library: /usr/X11R6/lib/libX11.6.dylib  (No such file or 
> directory, errno = 2)
> Error: .onLoad failed in 'loadNamespace' for 'tcltk'
> Error: package 'tcltk' could not be loaded  > library(tcltk) 
> Loading Tcl/Tk interface ... Error in dyn.load(x, as.logical(local),
> as.logical(now)) :
> 	unable to load shared library
> '/Library/Frameworks/R.framework/Resources/library/tcltk/libs/
> tcltk.so':
>    dlcompat: dyld: /Applications/R.app/Contents/MacOS/R can't open
> library: /usr/X11R6/lib/libX11.6.dylib  (No such file or 
> directory, errno = 2)
> Error: .onLoad failed in 'loadNamespace' for 'tcltk'
> Error: package/namespace load failed for 'tcltk'
>  >
> 
> I am NOT a developer, but an application-oriented grad student.
> 
> I'd appreciate any help that will make Rcmdr work.
> 
> Thanks,
> 
> Karla Van Meter
> tel: 707.765.0420
> net: kcvanmeter at ucdavis.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From goedman at mac.com  Mon May 23 20:42:17 2005
From: goedman at mac.com (Rob J Goedman)
Date: Mon, 23 May 2005 11:42:17 -0700
Subject: [R] installing R and Rcmdr onMacOS 10.3.9
In-Reply-To: <20050523183228.KLXN26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <20050523183228.KLXN26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <ED0A60EC-F8DC-40BC-8007-3A351FC7C594@mac.com>

Hi John,

It does work on my Mac. I'm helping Karla to get it running on hers.

Rob


On May 23, 2005, at 11:32 AM, John Fox wrote:

> Dear Karla,
>
> It's likely that you don't have Tcl/Tk installed on your Mac.
>
> I know that it's possible to get the Rcmdr working on a Mac, but  
> that doing
> so requires some additional steps. I'm not a Mac user, so I'm  
> afraid that I
> can't be of much direct help. If someone who's using the Rcmdr on a  
> Mac is
> willing to prepare notes about how to get it installed and working,  
> I'd
> gladly post them on my web site.
>
> Regards,
>  John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Karla Van  
>> Meter
>> Sent: Monday, May 23, 2005 11:43 AM
>> To: R-help at stat.math.ethz.ch
>> Subject: [R] installing R and Rcmdr onMacOS 10.3.9
>>
>> I have been trying to install the new, patched R 2.1.0a along
>> with the Rcmdr on my mac. I got the basic R package
>> installed, but in Project Manager, the tcltk package will not  
>> install.
>>
>>   When I downloaded the Rcmdr binary package from the
>> Berkeley site, it did not show up on the package installer
>> list until a second attempt, but it still does not show up on
>> the package manager list.
>>
>> On the package manager list, the tcltk package is listed as
>> ''not installed'. When I click on it to install, it won't. I tried  
>> the
>> library(tcltk) command, which failed.
>>
>> The following is the output from the last command:
>>
>>
>>> library(Rcmdr)
>>>
>> Loading required package: tcltk
>> Loading Tcl/Tk interface ... Error in dyn.load(x, as.logical(local),
>> as.logical(now)) :
>>     unable to load shared library
>> '/Library/Frameworks/R.framework/Resources/library/tcltk/libs/
>> tcltk.so':
>>    dlcompat: dyld: /Applications/R.app/Contents/MacOS/R can't open
>> library: /usr/X11R6/lib/libX11.6.dylib  (No such file or
>> directory, errno = 2)
>> Error: .onLoad failed in 'loadNamespace' for 'tcltk'
>> Error: package 'tcltk' could not be loaded  > library(tcltk)
>> Loading Tcl/Tk interface ... Error in dyn.load(x, as.logical(local),
>> as.logical(now)) :
>>     unable to load shared library
>> '/Library/Frameworks/R.framework/Resources/library/tcltk/libs/
>> tcltk.so':
>>    dlcompat: dyld: /Applications/R.app/Contents/MacOS/R can't open
>> library: /usr/X11R6/lib/libX11.6.dylib  (No such file or
>> directory, errno = 2)
>> Error: .onLoad failed in 'loadNamespace' for 'tcltk'
>> Error: package/namespace load failed for 'tcltk'
>>
>>>
>>>
>>
>> I am NOT a developer, but an application-oriented grad student.
>>
>> I'd appreciate any help that will make Rcmdr work.
>>
>> Thanks,
>>
>> Karla Van Meter
>> tel: 707.765.0420
>> net: kcvanmeter at ucdavis.edu
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From sgoodman at uchicago.edu  Mon May 23 20:49:48 2005
From: sgoodman at uchicago.edu (sgoodman@uchicago.edu)
Date: Mon, 23 May 2005 13:49:48 -0500
Subject: [R] dist equivalent
Message-ID: <200505231843.j4NIhG4o005802@relay00.uchicago.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050523/c21811b6/attachment.pl

From pohl at marge.statistik.uni-koeln.de  Mon May 23 20:46:01 2005
From: pohl at marge.statistik.uni-koeln.de (Stefan Pohl)
Date: Mon, 23 May 2005 20:46:01 +0200
Subject: [R] transform normally distributed random terms to gamma
	distributed random terms
Message-ID: <029a01c55fc7$adb1f9e0$1b9e5f86@simurechner>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050523/da47eeef/attachment.pl

From richard.haney at duke.edu  Mon May 23 20:48:13 2005
From: richard.haney at duke.edu (Richard Haney)
Date: Mon, 23 May 2005 14:48:13 -0400
Subject: [R] 21 CFR Part 11 Compliance and R
Message-ID: <OF7F30B888.FF4CD322-ON8525700A.006466BF-8525700A.006746F9@notes.duke.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050523/c57f6d2d/attachment.pl

From antoniou at central.ntua.gr  Mon May 23 20:59:11 2005
From: antoniou at central.ntua.gr (Constantinos Antoniou)
Date: Mon, 23 May 2005 21:59:11 +0300
Subject: [R] comparing glm models - lower AIC but insignificant coefficients
Message-ID: <937EFC8B-CCD8-40E0-9A85-ECCF9EEAB5EA@central.ntua.gr>

Hello,

I am a new R user and I am trying to estimate some generalized linear  
models (glm). I am trying to compare a model with a gaussian  
distribution and an identity link function, and a poisson model with  
a log link function. My problem is that while the gaussian model has  
significantly lower (i.e. "better") AIC (Akaike Information  
Criterion) most of the coefficients are not significant. On the other  
hand, the poisson model has a higher (i.e. "worse") AIC, but almost  
all the coefficients are extremely significant (expect for one that  
still has p=0.07).

Summary output of the two models follows... [sorry for the large  
number of independent variables, but the issue is less pronounced  
with fewer covariates].

My question is two-fold:
- AIC supposedly can be used to compare non-nested models (although  
there are concerns and I have also seen a couple in this list's  
archives). Is this a case where AIC is not a good measure to compare  
the two models? If so, is there another measure (besides choosing the  
model with the significant coefficients)? [These are time-series  
data, so I am also looking at acf/pacf of the residuals].
- Could the very high significance of the coefficients in the poisson  
model hint at some issue?

Thanking you in advance,

Costas


+++++++++++++++++++++++
POISSON - LOG LINK
+++++++++++++++++++++++


Call:
glm(formula = TotalDeadInjured[3:48] ~ -1 + Month[3:48] + sin(pi *
     Month[3:48]/6) + cos(pi * Month[3:48]/6) + sin(pi * Month[3:48]/ 
12) +
     cos(pi * Month[3:48]/12) + ThousandCars[3:48] + monthcycle[3:48] +
     TotalDeadInjured[1:46] + I((TotalDeadInjured[1:46])^2) +
     I((TotalDeadInjured[1:46])^3), family = poisson(link = log))

Deviance Residuals:
     Min       1Q   Median       3Q      Max
-3.6900  -1.1901  -0.1847   0.9477   4.3967

Coefficients:
                                 Estimate Std. Error z value Pr(>|z|)
Month[3:48]                   -7.712e-02  5.530e-03 -13.947  < 2e-16 ***
sin(pi * Month[3:48]/6)       -1.419e-01  2.759e-02  -5.144 2.68e-07 ***
cos(pi * Month[3:48]/6)       -8.407e-02  1.799e-02  -4.672 2.99e-06 ***
sin(pi * Month[3:48]/12)      -2.776e-02  1.558e-02  -1.782 0.074702 .
cos(pi * Month[3:48]/12)       5.195e-02  1.608e-02   3.232 0.001231 **
ThousandCars[3:48]             2.733e-02  2.255e-03  12.118  < 2e-16 ***
monthcycle[3:48]               6.307e-02  6.546e-03   9.635  < 2e-16 ***
TotalDeadInjured[1:46]        -2.925e-02  8.460e-03  -3.457 0.000546 ***
I((TotalDeadInjured[1:46])^2)  1.218e-04  3.613e-05   3.370 0.000750 ***
I((TotalDeadInjured[1:46])^3) -1.640e-07  4.961e-08  -3.306 0.000946 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

     Null deviance: 78694.70  on 46  degrees of freedom
Residual deviance:   130.03  on 36  degrees of freedom
AIC: 476.08

Number of Fisher Scoring iterations: 4

+++++++++++++++++++++++++
GAUSSIAN
++++++++++++++++++++++++++

Call:
glm(formula = TotalDeadInjured[3:48] ~ -1 + Month[3:48] + sin(pi *
     Month[3:48]/6) + cos(pi * Month[3:48]/6) + sin(pi * Month[3:48]/ 
12) +
     cos(pi * Month[3:48]/12) + ThousandCars[3:48] + monthcycle[3:48] +
     TotalDeadInjured[1:46] + I((TotalDeadInjured[1:46])^2) +
     I((TotalDeadInjured[1:46])^3), family = gaussian(link = identity))

Deviance Residuals:
     Min       1Q   Median       3Q      Max
-61.326  -12.012   -1.756   14.204   78.991

Coefficients:
                                 Estimate Std. Error t value Pr(>|t|)
Month[3:48]                   -8.111e+00  2.115e+00  -3.835 0.000487 ***
sin(pi * Month[3:48]/6)       -2.639e+01  1.095e+01  -2.409 0.021246 *
cos(pi * Month[3:48]/6)       -1.700e+01  7.138e+00  -2.382 0.022629 *
sin(pi * Month[3:48]/12)       2.392e-01  6.524e+00   0.037 0.970956
cos(pi * Month[3:48]/12)       8.785e+00  6.317e+00   1.391 0.172835
ThousandCars[3:48]             2.219e+00  8.604e-01   2.579 0.014146 *
monthcycle[3:48]               5.364e+00  2.494e+00   2.151 0.038301 *
TotalDeadInjured[1:46]        -4.974e+00  3.263e+00  -1.524 0.136171
I((TotalDeadInjured[1:46])^2)  2.154e-02  1.410e-02   1.527 0.135382
I((TotalDeadInjured[1:46])^3) -2.999e-05  1.959e-05  -1.530 0.134637
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 831.6357)

     Null deviance: 1927714  on 46  degrees of freedom
Residual deviance:   29939  on 36  degrees of freedom
AIC: 450.54

Number of Fisher Scoring iterations: 2



From spencer.graves at pdf.com  Mon May 23 21:23:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 24 May 2005 04:23:47 +0900
Subject: [R] transform normally distributed random terms to
	gamma	distributed random terms
In-Reply-To: <029a01c55fc7$adb1f9e0$1b9e5f86@simurechner>
References: <029a01c55fc7$adb1f9e0$1b9e5f86@simurechner>
Message-ID: <42922DC3.2010805@pdf.com>

	  1.  The help page for ?rgamma says E(g) = shape*scale = shape/rate 
and var(g) = shape*scale^2 = shape/rate^2.  These are different from 
what I read in your email.  Could you please check the help page more 
carefully?

	  2.  If this does not solve your problem, PLEASE do read the posting 
guide! "http://www.R-project.org/posting-guide.html".  Many people 
answer their own questions in the process of preparing a posting.  If 
they don't find the answer, their question is more likely to elicit 
useful replies.  In particular, please include a brief example 
explaining why you think "rgamma" won't work for you, nor why something 
like qgamma(runif(...)) nor qgamma(pnorm(rnorm(...)))?

	  hope this helps.
	  spencer graves

Stefan Pohl wrote:
> Hi,
> 
> I have normally distributed random terms u~N(0,1). I want to get gamma distributed random terms g~(scale,shape) with
> E(g)=1=shape/scale and var(g)=theta=1/scale=1/shape. 
> 
> How can I reach my goal? The following way doesn't work: use the distribution function of u to get U(0,1)- distributed random
> terms, then take the quantile function of the gamma distribution with shape and scale.
> 
> The resulting random terms must be ~gamma(shape, scale).
> 
> But it doesn't work.
> 
> Is there a mistake or do you know another way?
> 
> Thanks, Stefan.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Mon May 23 21:37:23 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 23 May 2005 15:37:23 -0400
Subject: [R] 21 CFR Part 11 Compliance and R
In-Reply-To: <OF7F30B888.FF4CD322-ON8525700A.006466BF-8525700A.006746F9@notes.duke.edu>
References: <OF7F30B888.FF4CD322-ON8525700A.006466BF-8525700A.006746F9@notes.duke.edu>
Message-ID: <429230F3.1060703@vanderbilt.edu>

Richard Haney wrote:
> I have used S-PLUS, R, MATLAB and SAS for many years, and I am actually 
> quite happy to use any  of these four languages.  The reason may in part 
> involve my using the various languages for the purposes to which they seem 
> most suited.  Hence there are many things for which I would not use SAS or 
> MATLAB, but for which I would greatly prefer to use R instead.
> 
> On the other hand ( to take one of a couple of examples), in the past I 
> have not even been permitted to use "R" whatsoever in clinical trials 
> applications, which also typically involve a need for CFR Part 11 
> compliance.   In this context, though, may I ask if there are people who 
> have recently been able to use R for clinical trials work that is done 
> within an FDA-approved 21 CFR Part 11-compliant framework?

Rich,

Whoever told you that is not well-informed.  CFR Part 11 has to do with 
critical software that runs medical devices and about certain primary 
data management software.  It does not apply to statistical analysis 
software.  We use R all the time in industry-sponsored and NIH sponsored 
clinical trials.  You do not need to seek FDA's approval.  FDA accepts 
all comers and does not dictate software policy for analysis.  They even 
accept Excel and Minitab for NDAs.  There are many messages related to 
this in the r-help archive; please look at them.

Frank

> 
> Thanks very much.
> 
> Rich Haney
> Richard.Haney at duke.edu
> Duke Comprehensive Cancer Center

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From gunter.berton at gene.com  Mon May 23 22:38:25 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 23 May 2005 13:38:25 -0700
Subject: [R] 21 CFR Part 11 Compliance and R
In-Reply-To: <429230F3.1060703@vanderbilt.edu>
Message-ID: <200505232038.j4NKcPFJ004693@meitner.gene.com>

To further add to and perhaps clarify Frank's remark: 21 CFR Part 11 defines
certain requirements for electronics records. A general requirement relevant
for statistical software used in submissions is, I presume (quoted):

  " Persons who use closed systems to create, modify, maintain, or 
transmit electronic records shall employ procedures and controls 
designed to ensure the authenticity, integrity, and, when appropriate, 
the confidentiality of electronic records, and to ensure that the signer 
cannot readily repudiate the signed record as not genuine. Such 
procedures and controls shall include the following:
    (a) Validation of systems to ensure accuracy, reliability, 
consistent intended performance, and the ability to discern invalid or 
altered records. "


Note that there is no requirement for ANY specific software -- only that a
process for software validation has been followed. Presumably, ANY
reasonable validation process can be used and, as R already has extensive
built-in validation suites, these certainly should go a long way toward
fulfilling the bulk of this validation requirement. 

The widespread use of SAS within the pharmaceutical industry is simply a
historical legacy (many would say an archaic one, but we won't go there).
Because of a huge existing infrastructure and code base, many companies
don't/can't even consider changing it. This is a common paradigm for
technological change: RCA was working mightily on improving vaccuum tubes
for their TV sets even as Sony was using transistors to blow them away. My
understanding is that it will take decades at least for fiber optics to
replace the gazillions of miles of copper in the phone network even though
that copper is a major obstacle to wider bandwidth and new services.

Older technologies are frequently difficult to displace merely because they
are there. This is certainly heightened in a regulatory environment where
anything that slows approval of a submission -- for example, use of software
that FDA reviewers may not be as familiar with -- is avoided. But that
neither mandates nor argues for avoidance. If R produces better submissions
more efficiently, then why not use it?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frank 
> E Harrell Jr
> Sent: Monday, May 23, 2005 12:37 PM
> To: Richard Haney
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] 21 CFR Part 11 Compliance and R
> 
> Richard Haney wrote:
> > I have used S-PLUS, R, MATLAB and SAS for many years, and I 
> am actually 
> > quite happy to use any  of these four languages.  The 
> reason may in part 
> > involve my using the various languages for the purposes to 
> which they seem 
> > most suited.  Hence there are many things for which I would 
> not use SAS or 
> > MATLAB, but for which I would greatly prefer to use R instead.
> > 
> > On the other hand ( to take one of a couple of examples), 
> in the past I 
> > have not even been permitted to use "R" whatsoever in 
> clinical trials 
> > applications, which also typically involve a need for CFR Part 11 
> > compliance.   In this context, though, may I ask if there 
> are people who 
> > have recently been able to use R for clinical trials work 
> that is done 
> > within an FDA-approved 21 CFR Part 11-compliant framework?
> 
> Rich,
> 
> Whoever told you that is not well-informed.  CFR Part 11 has 
> to do with 
> critical software that runs medical devices and about certain primary 
> data management software.  It does not apply to statistical analysis 
> software.  We use R all the time in industry-sponsored and 
> NIH sponsored 
> clinical trials.  You do not need to seek FDA's approval.  
> FDA accepts 
> all comers and does not dictate software policy for analysis. 
>  They even 
> accept Excel and Minitab for NDAs.  There are many messages 
> related to 
> this in the r-help archive; please look at them.
> 
> Frank
> 
> > 
> > Thanks very much.
> > 
> > Rich Haney
> > Richard.Haney at duke.edu
> > Duke Comprehensive Cancer Center
> 
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   
> Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From carsten.steinhoff at gmx.de  Mon May 23 23:30:55 2005
From: carsten.steinhoff at gmx.de (Carsten Steinhoff)
Date: Mon, 23 May 2005 23:30:55 +0200
Subject: [R] Error handling
In-Reply-To: <200505221008.j4MA6GB2025504@hypatia.math.ethz.ch>
Message-ID: <200505232131.j4NLVLsT031994@hypatia.math.ethz.ch>

Hi,

I have written an R-Procedure that automatically does a ML-Fit for some
datasets to a few
selected distributions. After any optimization the function writes the
estimated parameters into a variable.
My problem is now that sometimes the optimization failes and my program
stops with an error message.

What I want after a failed optimization is that there is written an error
msg
in the variable instead of stopping the program.

For the ML-Fit I use the "fitdistr" algorithm.

How could I do that ?

Thank you very very much!

Carsten



From ggrothendieck at gmail.com  Mon May 23 23:38:21 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 23 May 2005 17:38:21 -0400
Subject: [R] Error handling
In-Reply-To: <200505232131.j4NLVLsT031994@hypatia.math.ethz.ch>
References: <200505221008.j4MA6GB2025504@hypatia.math.ethz.ch>
	<200505232131.j4NLVLsT031994@hypatia.math.ethz.ch>
Message-ID: <971536df05052314381ec4982@mail.gmail.com>

On 5/23/05, Carsten Steinhoff <carsten.steinhoff at gmx.de> wrote:
> Hi,
> 
> I have written an R-Procedure that automatically does a ML-Fit for some
> datasets to a few
> selected distributions. After any optimization the function writes the
> estimated parameters into a variable.
> My problem is now that sometimes the optimization failes and my program
> stops with an error message.
> 
> What I want after a failed optimization is that there is written an error
> msg
> in the variable instead of stopping the program.
> 
> For the ML-Fit I use the "fitdistr" algorithm.
> 

See

?try
?tryCatch

For example, using try:

result <- try(myfun(...whatever...))
if(inherits(result, "try-error")) ...process the error...



From ross at biostat.ucsf.edu  Mon May 23 23:41:55 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Mon, 23 May 2005 14:41:55 -0700
Subject: [R] Documentation of S3 and S4 classes, inheritance
Message-ID: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>

I'd like to have a class A that computes a likelihood, and a subclass B
that computes the same likelihood by sometimes throws in an additional
term (B includes measurement error).

So B's likelihood needs to call A's, and then (sometimes) multiply by an
additional term.

It sounds as if, in the S3 scheme, NextMethod is supposed to do this:

like.A <- function(stuff) compute value
like.B <- function(stuff) extraFactor*NextMethod()
?

but, after some study of both the Language Manual (2.1) and the online
help of NextMethod I can't tell exactly what it does.  In particular,
I'm not sure if it returns to the calling function, or how it decides
which flavor to call.  The language manual says the method choice
depends on the values of .Generic and .Class, but doesn't say how those
get filled in.  I would expect .Class to be the current class, in which
case the call is recursive.  The online help says "'NextMethod' invokes
the next method (determined by the class)" but it doesn't say how it is
determined.  One ambiguity is whether "class" refers to the class (B) or
the class attribute ("B", "A").

I think the documentation could be clearer.

Now, probably none of this matters to me, since several sources
(including the online help for S3)indicate that S4 classes are
preferred.

I found the documentation for S4 initially elusive.  As far as I can
tell, it isn't even mentioned in the "R Language Definition."  While the
fact that S4 is defined in a package makes clear it is not formally part
of the base language, this is not a very good way to get people to use
S4.

I think by now I've tracked down docs on S4, including the
intro/overview at http://www.omegahat.org/RSMethods/.

Finally, I'm a bit concerned that one article mentioned that S4
inheritance, in practice, is used mostly for data, not methods (Thomas
Lumley, R News 4(1), June 2004: p. 36).  Am I going down a road I
shouldn't travel?

-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From macq at llnl.gov  Mon May 23 23:51:18 2005
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 23 May 2005 14:51:18 -0700
Subject: [R] Calling R from R and specifying "wait until script is
	finished"
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>
Message-ID: <p06210209beb7ff02682b@[128.115.153.6]>

I don't know about efficient, but here is a way that I find to be 
practical, with around 100 R scripts.

I create a master R script (I call it "Runall.r"). It begins like this:

## Execute me with  R --save < Runall.r >& Runall.log

hc <- TRUE
if (hc) sink('Runall.out')

t0runall <- Sys.time()
cat('========================================================\n')
cat('Running script "Runall.r" at',format(t0runall),'\n')
cat('========================================================\n')

msg <- try(source('ae-175.r')) ; rm.trymsg(msg)
msg <- try(source('ae-235.r')) ; rm.trymsg(msg)
msg <- try(source('ae-251.r')) ; rm.trymsg(msg)
msg <- try(source('ae-331.r')) ; rm.trymsg(msg)
msg <- try(source('ae-332.r')) ; rm.trymsg(msg)
msg <- try(source('ae-491.r')) ; rm.trymsg(msg)
msg <- try(source('ae-695.r')) ; rm.trymsg(msg)
msg <- try(source('ae-801.r')) ; rm.trymsg(msg)

## and so on, for as many scripts as I want to run
## although I constructed the list of scripts to run by hand, it 
could easily be done
## as a loop, with the script names constructed from the loop index

## the script ends with:

cat('========================================================\n')
t1 <- Sys.time()
cat('[Runall.r] Elapsed 
time',format(t1-t0runall),attributes(t1-t0runall)$units,'\n')

if (hc) {
   cat('Done\n')
   sink()

   system('grep failed Runall.out > Runall.info')
   cat('\n')
   system('grep succeeded Runall.out >> Runall.info')
   cat('\n')

   cat('See files Runall.out and Runall.info\n')
   cat('Done\n')
}

####
#### The function rm.trymsg() is this:

  rm.trymsg <- function(msg) {
   if (class(msg)=='try-error') {
     cat('============',tblid,'failed =============\n')
     return(FALSE)
   }
   if (data.class(msg)=='list'& unlist(msg)[[1]]=='bad.table') {
     cat('============',tblid,'failed ===== bad.table ==========\n')
     return(FALSE)
   }
   cat('=========',tblid,'succeeded =========\n')
   TRUE
}

## and the purpose of using try() and rm.trymsg() is to let the job 
continue if an error occurs in one of
## the scripts. Note, however, that the text strings "bad.table" and 
"tblid" are unique to the task I am doing, and would not
## work in general.

At 8:51 PM -0400 5/21/05, Lapointe, Pierre wrote:
>Hello,
>
>Let's say I have 50 R scripts to run.  What would be the most efficient way
>to run them?
>
>I thought I could do multiple Rterms in a DOS batch file:
>
>Ex:
>Rterm <1.R> 1.txt
>Rterm <2.R> 2.txt
>...
>Rterm <50.R> 50.txt
>
>However, I'm afraid they will all open at the same time.   I know I could
>pause the batch file with something like:
>
>PING 1.1.1.1 -n 1 -w 60000 >NUL  (to delay 60 seconds)
>
>But that would require that I know how long each of my scripts take.
>
>Is there an easier way?  Something like calling R from R and specifying that
>the script has to be finished before continuing.
>
>Thanks
>
>Pierre Lapointe
>
>
>
>***********************************************************************************
>AVIS DE NON-RESPONSABILITE:\ Ce document transmis par courri...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ggrothendieck at gmail.com  Tue May 24 00:09:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 23 May 2005 18:09:30 -0400
Subject: [R] Documentation of S3 and S4 classes, inheritance
In-Reply-To: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>
References: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>
Message-ID: <971536df050523150926c99dcf@mail.gmail.com>

On 5/23/05, Ross Boylan <ross at biostat.ucsf.edu> wrote:
> I'd like to have a class A that computes a likelihood, and a subclass B
> that computes the same likelihood by sometimes throws in an additional
> term (B includes measurement error).
> 
> So B's likelihood needs to call A's, and then (sometimes) multiply by an
> additional term.
> 
> It sounds as if, in the S3 scheme, NextMethod is supposed to do this:
> 
> like.A <- function(stuff) compute value
> like.B <- function(stuff) extraFactor*NextMethod()
> ?

Here is a working example to try out.  In the call to lik,
the first class in the class vector is "B" so lik.B gets invoked.  
The next method of x is "A" so NextMethod invokes lik.A from 
within lik.B and then returns its result to lik.A which finishes the 
calculation.

lik <- function(x) UseMethod("lik")
lik.A <- function(x) mean(x)
lik.B <- function(x) NextMethod("lik") + sd(x)

x <- structure(1:3, class = c("B", "A"))
lik(x) # 3

> but, after some study of both the Language Manual (2.1) and the online
> help of NextMethod I can't tell exactly what it does.  In particular,
> I'm not sure if it returns to the calling function, or how it decides
> which flavor to call.  The language manual says the method choice
> depends on the values of .Generic and .Class, but doesn't say how those
> get filled in.  I would expect .Class to be the current class, in which
> case the call is recursive.  The online help says "'NextMethod' invokes
> the next method (determined by the class)" but it doesn't say how it is
> determined.  One ambiguity is whether "class" refers to the class (B) or
> the class attribute ("B", "A").

See above discussion.

> 
> I think the documentation could be clearer.
> 
> Now, probably none of this matters to me, since several sources
> (including the online help for S3)indicate that S4 classes are
> preferred.
> 
> I found the documentation for S4 initially elusive.  As far as I can
> tell, it isn't even mentioned in the "R Language Definition."  While the
> fact that S4 is defined in a package makes clear it is not formally part
> of the base language, this is not a very good way to get people to use
> S4.
> 
> I think by now I've tracked down docs on S4, including the
> intro/overview at http://www.omegahat.org/RSMethods/.
> 
> Finally, I'm a bit concerned that one article mentioned that S4
> inheritance, in practice, is used mostly for data, not methods (Thomas
> Lumley, R News 4(1), June 2004: p. 36).  Am I going down a road I
> shouldn't travel?
> 

This area seems somewhat controversial with different people stating
different opinions.   IMHO you are probably best off to start with S3 since its
simpler and if you do learn S4 later they are not unrelated so it will make 
it easier than jumping straight into it.  Also you may find you never have to 
go beyond S3 in which case you have saved yourself some time.  I 
personally use S3.

By the way if choosing from S3 and S4 is not enough, there are also 
two CRAN packages that provide additional OO models as well: 
R.oo and proto.



From kjetil at acelerate.com  Tue May 24 00:18:29 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 23 May 2005 18:18:29 -0400
Subject: [R] comparing glm models - lower AIC but insignificant
	coefficients
In-Reply-To: <937EFC8B-CCD8-40E0-9A85-ECCF9EEAB5EA@central.ntua.gr>
References: <937EFC8B-CCD8-40E0-9A85-ECCF9EEAB5EA@central.ntua.gr>
Message-ID: <429256B5.1090509@acelerate.com>

Constantinos Antoniou wrote:

> Hello,
>
> I am a new R user and I am trying to estimate some generalized linear  
> models (glm). I am trying to compare a model with a gaussian  
> distribution and an identity link function, and a poisson model with  
> a log link function. My problem is that while the gaussian model has  
> significantly lower (i.e. "better") AIC (Akaike Information  
> Criterion) most of the coefficients are not significant. On the other  
> hand, the poisson model has a higher (i.e. "worse") AIC, but almost  
> all the coefficients are extremely significant (expect for one that  
> still has p=0.07).
>
> Summary output of the two models follows... [sorry for the large  
> number of independent variables, but the issue is less pronounced  
> with fewer covariates].
>
> My question is two-fold:
> - AIC supposedly can be used to compare non-nested models (although  
> there are concerns and I have also seen a couple in this list's  
> archives). Is this a case where AIC is not a good measure to compare  
> the two models? If so, is there another measure (besides choosing the  
> model with the significant coefficients)? [These are time-series  
> data, so I am also looking at acf/pacf of the residuals].

The topic of using AIC to compare non-nested models have been discussed 
on the list, please search. But even
if AIC can be used to compare non-nested models, the AIC as calculated 
by R is not suited. The AIC
includes an arbitrary additive constant, as the log-likelihood does. And 
this additive constant
depend usually on constants in the density which are inconsequential for 
AIC, and may be omitted.

And even if they were included, it seem doubtfull to me that this would 
help for comparision of Poisson and normal
models, since the underlying measure is different! The experts can 
comment on that.

That said, I would tend to use Poisson if I had count data and a poisson 
model looks remotely sensible. That
will give a more interpretable model, which seems more important than 
purely data-analytic considerations.
And lasstly, if the poisson assumptions seems reasonable, there will be 
a non-constant variance, and if you use
a normal model you should use weighted least squares or tran sform the 
response (square root). If you try that, maybe you will
see that the normal model give lower p-values for the coefficients. Also 
make a plot of residuals versus
fitted value!

> - Could the very high significance of the coefficients in the poisson  
> model hint at some issue?

Maybe that the model fits better than the normal?

Kjetil

>
> Thanking you in advance,
>
> Costas
>
>
> +++++++++++++++++++++++
> POISSON - LOG LINK
> +++++++++++++++++++++++
>
>
> Call:
> glm(formula = TotalDeadInjured[3:48] ~ -1 + Month[3:48] + sin(pi *
>     Month[3:48]/6) + cos(pi * Month[3:48]/6) + sin(pi * Month[3:48]/ 
> 12) +
>     cos(pi * Month[3:48]/12) + ThousandCars[3:48] + monthcycle[3:48] +
>     TotalDeadInjured[1:46] + I((TotalDeadInjured[1:46])^2) +
>     I((TotalDeadInjured[1:46])^3), family = poisson(link = log))
>
> Deviance Residuals:
>     Min       1Q   Median       3Q      Max
> -3.6900  -1.1901  -0.1847   0.9477   4.3967
>
> Coefficients:
>                                 Estimate Std. Error z value Pr(>|z|)
> Month[3:48]                   -7.712e-02  5.530e-03 -13.947  < 2e-16 ***
> sin(pi * Month[3:48]/6)       -1.419e-01  2.759e-02  -5.144 2.68e-07 ***
> cos(pi * Month[3:48]/6)       -8.407e-02  1.799e-02  -4.672 2.99e-06 ***
> sin(pi * Month[3:48]/12)      -2.776e-02  1.558e-02  -1.782 0.074702 .
> cos(pi * Month[3:48]/12)       5.195e-02  1.608e-02   3.232 0.001231 **
> ThousandCars[3:48]             2.733e-02  2.255e-03  12.118  < 2e-16 ***
> monthcycle[3:48]               6.307e-02  6.546e-03   9.635  < 2e-16 ***
> TotalDeadInjured[1:46]        -2.925e-02  8.460e-03  -3.457 0.000546 ***
> I((TotalDeadInjured[1:46])^2)  1.218e-04  3.613e-05   3.370 0.000750 ***
> I((TotalDeadInjured[1:46])^3) -1.640e-07  4.961e-08  -3.306 0.000946 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> (Dispersion parameter for poisson family taken to be 1)
>
>     Null deviance: 78694.70  on 46  degrees of freedom
> Residual deviance:   130.03  on 36  degrees of freedom
> AIC: 476.08
>
> Number of Fisher Scoring iterations: 4
>
> +++++++++++++++++++++++++
> GAUSSIAN
> ++++++++++++++++++++++++++
>
> Call:
> glm(formula = TotalDeadInjured[3:48] ~ -1 + Month[3:48] + sin(pi *
>     Month[3:48]/6) + cos(pi * Month[3:48]/6) + sin(pi * Month[3:48]/ 
> 12) +
>     cos(pi * Month[3:48]/12) + ThousandCars[3:48] + monthcycle[3:48] +
>     TotalDeadInjured[1:46] + I((TotalDeadInjured[1:46])^2) +
>     I((TotalDeadInjured[1:46])^3), family = gaussian(link = identity))
>
> Deviance Residuals:
>     Min       1Q   Median       3Q      Max
> -61.326  -12.012   -1.756   14.204   78.991
>
> Coefficients:
>                                 Estimate Std. Error t value Pr(>|t|)
> Month[3:48]                   -8.111e+00  2.115e+00  -3.835 0.000487 ***
> sin(pi * Month[3:48]/6)       -2.639e+01  1.095e+01  -2.409 0.021246 *
> cos(pi * Month[3:48]/6)       -1.700e+01  7.138e+00  -2.382 0.022629 *
> sin(pi * Month[3:48]/12)       2.392e-01  6.524e+00   0.037 0.970956
> cos(pi * Month[3:48]/12)       8.785e+00  6.317e+00   1.391 0.172835
> ThousandCars[3:48]             2.219e+00  8.604e-01   2.579 0.014146 *
> monthcycle[3:48]               5.364e+00  2.494e+00   2.151 0.038301 *
> TotalDeadInjured[1:46]        -4.974e+00  3.263e+00  -1.524 0.136171
> I((TotalDeadInjured[1:46])^2)  2.154e-02  1.410e-02   1.527 0.135382
> I((TotalDeadInjured[1:46])^3) -2.999e-05  1.959e-05  -1.530 0.134637
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> (Dispersion parameter for gaussian family taken to be 831.6357)
>
>     Null deviance: 1927714  on 46  degrees of freedom
> Residual deviance:   29939  on 36  degrees of freedom
> AIC: 450.54
>
> Number of Fisher Scoring iterations: 2
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From Soren.Hojsgaard at agrsci.dk  Tue May 24 00:31:11 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Tue, 24 May 2005 00:31:11 +0200
Subject: SV: [R] Documentation of S3 and S4 classes, inheritance
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99C7@DJFPOST01.djf.agrsci.dk>

I am sure that I do not have the answer to your question - but I would like to add to your views regarding the S4-issue. I've been involved in the gRbase package for graphical modelling in R. The first version was implemented in S4 but now we have made a roll-back to using S3 methods. 
 
While, I can see that S3 is not really comprehensive enough for 'proper' object oriented programming it certainly has other virtues. One such is the documentation: There are 1-2 pages to read, thats it - and you are up and running!! Plain, easy and simple. One of the things that S3 is lacking is clearly multiple inheritance, and that is, as I understand it, a feature of S4. Another feature of S4 is, again as I understand things (apologies if I am wrong) that objects are protected from being altered (in terms of adding new slots etc). Again a very nice feature... 
 
There are, however, several problems with S4. The first is 'how to find documentation about it'? Sure, the green book by Chambers is a reference, but really what I am missing is a 5-page document saying 'this is how you should do it'. I claim that most package developers are statisticians who want to share their work with others - not computer scientists with strong views about developments of formal languages. Another issue we came across when working with the gRbase package was that it was too 'painful' to integrate existing S3 code into an S4 package. (Surely, some people could be tempted to reply to this as 'yes, but you should write everything in S4 anyway - sorry, but you'll have to start all over again...' However, such views could be dangerous to the whole 'R-open-source-half-way-anarchistic-philosophy' as I see it). I would expect that it would be possible to improve S3 in a simpler way - perhaps S3.5 could be an option..
 
My first experience with statistical computing was with xlisp-stat which has a very nice (as I remember it) object system which is well explained in Luke Tierneys book. I wonder if that could serve as an inspiration for anyone who is interested in writing an 'S4 for dummies'...
 
Best regards
S??ren H??jsgaard
 
 
 
 
 
 

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p?? vegne af Ross Boylan
Sendt: ma 23-05-2005 23:41
Til: r-help
Emne: [R] Documentation of S3 and S4 classes, inheritance



I'd like to have a class A that computes a likelihood, and a subclass B
that computes the same likelihood by sometimes throws in an additional
term (B includes measurement error).

So B's likelihood needs to call A's, and then (sometimes) multiply by an
additional term.

It sounds as if, in the S3 scheme, NextMethod is supposed to do this:

like.A <- function(stuff) compute value
like.B <- function(stuff) extraFactor*NextMethod()
?

but, after some study of both the Language Manual (2.1) and the online
help of NextMethod I can't tell exactly what it does.  In particular,
I'm not sure if it returns to the calling function, or how it decides
which flavor to call.  The language manual says the method choice
depends on the values of .Generic and .Class, but doesn't say how those
get filled in.  I would expect .Class to be the current class, in which
case the call is recursive.  The online help says "'NextMethod' invokes
the next method (determined by the class)" but it doesn't say how it is
determined.  One ambiguity is whether "class" refers to the class (B) or
the class attribute ("B", "A").

I think the documentation could be clearer.

Now, probably none of this matters to me, since several sources
(including the online help for S3)indicate that S4 classes are
preferred.

I found the documentation for S4 initially elusive.  As far as I can
tell, it isn't even mentioned in the "R Language Definition."  While the
fact that S4 is defined in a package makes clear it is not formally part
of the base language, this is not a very good way to get people to use
S4.

I think by now I've tracked down docs on S4, including the
intro/overview at http://www.omegahat.org/RSMethods/.

Finally, I'm a bit concerned that one article mentioned that S4
inheritance, in practice, is used mostly for data, not methods (Thomas
Lumley, R News 4(1), June 2004: p. 36).  Am I going down a road I
shouldn't travel?

--
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ross at biostat.ucsf.edu  Tue May 24 00:47:16 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Mon, 23 May 2005 15:47:16 -0700
Subject: SV: [R] Documentation of S3 and S4 classes, inheritance
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99C7@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99C7@DJFPOST01.djf.agrsci.dk>
Message-ID: <1116888436.4615.52.camel@iron.libaux.ucsf.edu>

On Tue, 2005-05-24 at 00:31 +0200, S??ren H??jsgaard wrote:
> >One of the things that S3 is lacking is clearly multiple inheritance,
I thought if you wanted C to inherit from A and B you could, in S3,
just   
   class(aCObject)<- c('C', 'A', 'B')
While the ordering is arbitrary, that's usually the case with multiple
inheritance.



From David.Brahm at geodecapital.com  Tue May 24 00:47:57 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Mon, 23 May 2005 18:47:57 -0400
Subject: [R] R annoyances
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBD1@MSGBOSCLF2WIN.DMN1.FMR.COM>

Bogdan Romocea <br44114 at gmail.com> wrote:
> Hypothetically, if whatever=T/F were forbidden and only
> whatever=TRUE/FALSE were allowed, all the code could be fixed with
> a simple sed script: [deleted]

As Bogdan is lobbying hard for disallowing T/F, I wanted to speak up
for the other side.  Please leave T/F as they are, simple and clean!
C'mon people, just don't use T or F as variables, how hard is that???

-- David Brahm (brahm at alum.mit.edu)



From gunter.berton at gene.com  Tue May 24 01:10:27 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 23 May 2005 16:10:27 -0700
Subject: SV: [R] Documentation of S3 and S4 classes, inheritance
In-Reply-To: <1116888436.4615.52.camel@iron.libaux.ucsf.edu>
Message-ID: <200505232310.j4NNARsC027095@hertz.gene.com>

I don't think you quite said what you meant ...

> I thought if you wanted C to inherit from A and B you could, in S3,
> just   
>    class(aCObject)<- c('C', 'A', 'B')
> While the ordering is arbitrary, that's usually the case with multiple
> inheritance.
> 

I assume you meant, "I thought if you wanted aCObject to inherit from A and
B,..."

In S3, classes do not inherit, only objects do. In S4, there is true class
inheritance.

Cheers,
Bert Gunter



From ross at biostat.ucsf.edu  Tue May 24 02:03:45 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Mon, 23 May 2005 17:03:45 -0700
Subject: [R] Re: S4 method inheritance
In-Reply-To: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>
References: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>
Message-ID: <1116893025.4615.61.camel@iron.libaux.ucsf.edu>

On Mon, 2005-05-23 at 14:41 -0700, Ross Boylan wrote:
....

> Finally, I'm a bit concerned that one article mentioned that S4
> inheritance, in practice, is used mostly for data, not methods (Thomas
> Lumley, R News 4(1), June 2004: p. 36).  Am I going down a road I
> shouldn't travel?
> 
Hmm, maybe I just found out.  If B is an S4 subclass of A (aka extends
A), how does B's method foo invoke A's foo?

The question assumes that A's foo was defined as an in place function,
so there's no (obvious) named object for it, i.e,
setMethod("A", signature(blah="numeric"), function(x) something)



From ben.bob at gmail.com  Tue May 24 03:59:53 2005
From: ben.bob at gmail.com (Bo Peng)
Date: Mon, 23 May 2005 20:59:53 -0500
Subject: [R] How to break an axis?
Message-ID: <6ea7b5430505231859100344c7@mail.gmail.com>

Dear list,

I need to plot four almost horizontal lines with y-values around
1,3,4, 400. If I plot them directly, the first three lines will be
indiscernible so I am thinking of breaking y-axis into two parts, one
with range (0,5), another (395,400). Is there an easy way to do this?

I can think of two ways: 
1. use two plots and draw axes manually. The plot margins, are however
difficult to adjust.
2. use one plot, adjust y-values of the lines and draw y-axis
manually. But, how would I break y-axis and add separation symbols
*on* yaxis? (By separation symbol, I mean something like
------//------

Many thanks in davance.
Bo



From Tom.Mulholland at dpi.wa.gov.au  Tue May 24 04:28:49 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 24 May 2005 10:28:49 +0800
Subject: [R] How to break an axis?
Message-ID: <4702645135092E4497088F71D9C8F51A128B73@afhex01.dpi.wa.gov.au>

I think you may wish to look at the plotrix package, assumming that you have taken care of the issues involved in breaking an axis and that your plots don't result in misleading information.

I think to use the axis break you would have to calculate your own labels and rescale the data, as it looks as if the break is just the cosmetics component.

This is also the topic of many posts on the list so you might want to search the list for "break" and "axis" to see other responses to this issue.

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Bo Peng
> Sent: Tuesday, 24 May 2005 10:00 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to break an axis?
> 
> 
> Dear list,
> 
> I need to plot four almost horizontal lines with y-values around
> 1,3,4, 400. If I plot them directly, the first three lines will be
> indiscernible so I am thinking of breaking y-axis into two parts, one
> with range (0,5), another (395,400). Is there an easy way to do this?
> 
> I can think of two ways: 
> 1. use two plots and draw axes manually. The plot margins, are however
> difficult to adjust.
> 2. use one plot, adjust y-values of the lines and draw y-axis
> manually. But, how would I break y-axis and add separation symbols
> *on* yaxis? (By separation symbol, I mean something like
> ------//------
> 
> Many thanks in davance.
> Bo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.connolly at hortresearch.co.nz  Tue May 24 04:48:12 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 24 May 2005 14:48:12 +1200
Subject: [R] lme4 package and importIntoEnv errors
Message-ID: <20050524024812.GU5776@hortresearch.co.nz>

I've used packages for some years now and seldom had any trouble using
the tgz files.  Now I've come across something I've never seen before.

> version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

> library(lme4)
Loading required package: methods
Loading required package: Matrix
Loading required package: latticeExtra
Error in importIntoEnv(impenv, impnames, ns, impvars) :
        objects ÅÔÅøÅΩÅÔÅøÅΩcoefÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩcoef<-ÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩcorMatrixÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩmatrix<-ÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩpdFactorÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩpdMatrixÅÔÅøÅΩÅÔÅøÅΩ are not exported by 'namespace:Matrix'
Error: package/namespace load failed for 'lme4'

The Matrix and latticeExtra were downloaded and installed yesterday and
the methods is what came with R-2.1.0.

I notice in the archives someone had a very similar problem with
version 2.0.1 and Windows in January this year, but there's no further
mention that I found.  That one seems to have been fixed with new
Matrix and latticeExtra packages.  However, I'm using newer ones
already.  So I thought it might have to do with 2.1.0 rebreaking
something.

'CHANGES IN R VERSION 2.1.0 patched' doesn't make any mention of
importIntoEnv, so I figured patching wasn't likely to make a
difference.  I also figured that latticeExtra is not likely to be as
buggy as that.

The only thing I can think of that would be slightly non-standard
about my installation is the fact that I install packages (in addition
to the recommended ones) in a different place from the "normal" ones.
Could that be a source of error on this system?  Alternatively, is
there anything I could do about this namespace business?


best

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From pkleiber at honlab.nmfs.hawaii.edu  Tue May 24 04:59:03 2005
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Mon, 23 May 2005 16:59:03 -1000
Subject: [R] contourLines() starts a plot device
Message-ID: <42929877.6040502@honlab.nmfs.hawaii.edu>

I want to use contourLines() to get contour line coordinate vectors, 
but I don't want to make a plot.  However contourLines() insists on
opening a graphics device.  Is there a way tell it not to do this?

 > version

platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status   Patched
major    2
minor    1.0
year     2005
month    04
day      20
language R


Cheers, Pierre

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist            Tel: 808 983-5399 / (hm)808 737-7544
NOAA Fisheries Service - Honolulu Laboratory    Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From lauraholt_983 at hotmail.com  Tue May 24 05:22:16 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 23 May 2005 22:22:16 -0500
Subject: [R] website reference for building R packages
Message-ID: <BAY10-F506FC10C31172C8BA726ADD60D0@phx.gbl>

Hi R People:

A few weeks ago, someone put a link to a website for "how to" for building R 
packages.  It was very nice.

But of course, I have misplaced the link.  Does anyone still have that, 
please?

It was someone from the University of Chicago, I believe.

Thanks in advance.

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From ssk2031 at columbia.edu  Tue May 24 05:25:23 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Mon, 23 May 2005 23:25:23 -0400
Subject: [R] website reference for building R packages
In-Reply-To: <BAY10-F506FC10C31172C8BA726ADD60D0@phx.gbl>
References: <BAY10-F506FC10C31172C8BA726ADD60D0@phx.gbl>
Message-ID: <42929EA3.6020708@columbia.edu>


it is the first link if you type "making packages" into the google 
search box here:

http://maths.newcastle.edu.au/~rking/R/

-s.


Laura Holt wrote:
> Hi R People:
> 
> A few weeks ago, someone put a link to a website for "how to" for 
> building R packages.  It was very nice.
> 
> But of course, I have misplaced the link.  Does anyone still have that, 
> please?
> 
> It was someone from the University of Chicago, I believe.
> 
> Thanks in advance.
> 
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From lauraholt_983 at hotmail.com  Tue May 24 05:38:23 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 23 May 2005 22:38:23 -0500
Subject: [R] Returning from a function
Message-ID: <BAY10-F2137BB32CD5A60C00B973AD60D0@phx.gbl>

Hello again.

I have a function that plots a series and adds some interesting items to the 
plot.

Fair enough.

The last statement in the function is
return()

When the function is executed, NULL appears at the end, which is ok.
Is there any way to prevent NULL from appearing, or is that just as it 
should be, please?

thanks again,
Shamefacedly,
Laura Holt
R Version 2.1.0 Windows
mailto: lauraholt_983 at hotmail.com



From andy_liaw at merck.com  Tue May 24 06:10:26 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 24 May 2005 00:10:26 -0400
Subject: [R] Returning from a function
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E883@usctmx1106.merck.com>

Use invisible(NULL) as the last line. 

Andy

> From: Laura Holt
> 
> Hello again.
> 
> I have a function that plots a series and adds some 
> interesting items to the 
> plot.
> 
> Fair enough.
> 
> The last statement in the function is
> return()
> 
> When the function is executed, NULL appears at the end, which is ok.
> Is there any way to prevent NULL from appearing, or is that 
> just as it 
> should be, please?
> 
> thanks again,
> Shamefacedly,
> Laura Holt
> R Version 2.1.0 Windows
> mailto: lauraholt_983 at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From weigand.stephen at charter.net  Tue May 24 06:20:39 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Mon, 23 May 2005 23:20:39 -0500
Subject: [R] How to convert a character string to a number
In-Reply-To: <F9771783488F5740A25F86D90F4C24AD0287CE8A@a1-balu.ict.fhg.de>
References: <F9771783488F5740A25F86D90F4C24AD0287CE8A@a1-balu.ict.fhg.de>
Message-ID: <88a2b60119f6a37407276028e6a88dd0@charter.net>

Dear Andreas,

On May 23, 2005, at 4:56 AM, Kraft, Andreas wrote:

> Hi everyone,
> I have a simple problem but don??t know how to solve it.
> I read in a table from a text file that looks like that:
>
> -3,932200E-01   -2,000000E-01   4,999995E-02
> -3,932099E-01   -1,000000E-01   3,999996E-02
...
> -3,931300E-01   0,000000E+00  3,499996E-02
>
> but when I try to to plot the values from the
> second column, the plot shows the wrong values.
>
> That's what I do in R:
> read.table("C:\\Messdaten\\epp1_1_ver.txt")->sh1
>

Read the help for read.table() and use dec = ",".

Also, before you correct the call to read.table, if you enter

R> str(sh1)

you'll see something like:

R> str(sh1)
`data.frame':	10 obs. of  3 variables:
  $ V1: Factor w/ 10 levels "-3,931300E-01",..: 10 9 8 7 6 5 4 3 2 1
  $ V2: Factor w/ 5 levels "-1,000000E-01",..: 2 1 4 1 1 5 3 4 4 4
  $ V3: Factor w/ 5 levels "2,499996E-02",..: 5 3 3 4 4 2 3 1 1 2

which gives you some very helpful information.  In particular, it
shows that sh1 consists of factors.

Good luck,

Stephen


>> sh1[1:10,2]
>
>  [1] -2.000000E-01 -1.000000E-01 0.000000E+00  -1.000000E-01 
> -1.000000E-01 3.000000E-01  -3.000000E-01 0.000000E+00
>
>  [9] 0.000000E+00  0.000000E+00
>
> 1707 Levels: -1,000000E-01 -1,000000E+00 -1,000000E+01 -1,002000E+02 
> -1,003000E+02 -1,004000E+02 ... 9,980000E+01
>
>> sh1[1:10,2]->a
>
>> cbind(x=seq(1,10,by=1),y=a)
>
>        x    y
>
>  [1,]  1  436
>
>  [2,]  2  321
>
>  [3,]  3 1007
>
>  [4,]  4  321
>
>  [5,]  5  321
>
>  [6,]  6 1353
>
>  [7,]  7  548
>
>  [8,]  8 1007
>
>  [9,]  9 1007
>
> [10,] 10 1007
>
>
>
> Why do I get those values? Why is for example -2,000000E-01 equal to 
> 436?
>
> I need to convert the values from -2,000000E-01 to -0.2  - how can I 
> do this?
>
>
>
> Thank you for your suggestions,
>
>
>
> Andreas Kraft
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From blindglobe at gmail.com  Tue May 24 06:30:38 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 24 May 2005 06:30:38 +0200
Subject: [R] 21 CFR Part 11 Compliance and R
In-Reply-To: <OF7F30B888.FF4CD322-ON8525700A.006466BF-8525700A.006746F9@notes.duke.edu>
References: <OF7F30B888.FF4CD322-ON8525700A.006466BF-8525700A.006746F9@notes.duke.edu>
Message-ID: <1abe3fa905052321307b10e9c2@mail.gmail.com>

21 CFR Part 11 is mostly about data audit trails (tracking changes and
electronic signatures) and validation of software.

Most of this involves documentation and the creation of documentation
LOCALLY at YOUR site, regarding YOUR processes and operating
procedures.

R satisfies most of what is needed, depending on who you are.  If you
happen to be working for a large Pharma, it comes up slightly short,
but not too short; I happen to know of 2 pharmas very interested in
getting the gaps filled, and an analysis suggests that this is simple,
though time consuming, of course.  Hopefully, the missing pieces will
exist soon (mostly traditional vendor documentation for validation).

If you happen to be working in an nonprofit/academic setting, R is
probably fine (again depending on your SOPs, data transport audit
trail, etc).

best,
-tony

On 5/23/05, Richard Haney <richard.haney at duke.edu> wrote:
> I have used S-PLUS, R, MATLAB and SAS for many years, and I am actually
> quite happy to use any  of these four languages.  The reason may in part
> involve my using the various languages for the purposes to which they seem
> most suited.  Hence there are many things for which I would not use SAS or
> MATLAB, but for which I would greatly prefer to use R instead.
> 
> On the other hand ( to take one of a couple of examples), in the past I
> have not even been permitted to use "R" whatsoever in clinical trials
> applications, which also typically involve a need for CFR Part 11
> compliance.   In this context, though, may I ask if there are people who
> have recently been able to use R for clinical trials work that is done
> within an FDA-approved 21 CFR Part 11-compliant framework?
> 
> Thanks very much.
> 
> Rich Haney
> Richard.Haney at duke.edu
> Duke Comprehensive Cancer Center
> 
> 
> 
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From mail at bymouth.com  Tue May 24 08:43:10 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Tue, 24 May 2005 16:43:10 +1000
Subject: [R] best.svm
Message-ID: <005a01c5602b$db30dd00$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050524/0de25c72/attachment.pl

From bhx2 at mevik.net  Tue May 24 09:19:54 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Tue, 24 May 2005 09:19:54 +0200
Subject: [R] Can't reproduce clusplot princomp results.
In-Reply-To: <200505231801.j4NI1i7S005677@magnolia.isciences.com> (Thomas M.
	Parris's message of "Mon, 23 May 2005 14:01:38 -0400")
References: <200505231801.j4NI1i7S005677@magnolia.isciences.com>
Message-ID: <m0y8a5146d.fsf@bar.nemo-project.org>

Thomas M. Parris writes:

> clusplot reports that the first two principal components explain
> 99.7% of the variability.
[...]

>> loadings(pca)
[...]
>                Comp.1 Comp.2 Comp.3 Comp.4
> SS loadings      1.00   1.00   1.00   1.00
> Proportion Var   0.25   0.25   0.25   0.25
> Cumulative Var   0.25   0.50   0.75   1.00

This has nothing to do with how much of the variability of the
original data that is captured by each component; it merely measures
the variability in the coefficients of the loading vectors (and they
are standardised to length one in princomp)

What you want to look at is pca$sdev, for instance something like

totvar <- sum(pca$sdev^2)
rbind("explained var" = pca$sdev^2,
      "prop. expl. var" = pca$sdev^2/totvar,
      "cum.prop.expl.var" = cumsum(pca$sdev^2)/totvar)
                     Comp.1    Comp.2      Comp.3       Comp.4
explained var     3.4093746 0.5785399 0.011560142 0.0005252824
prop. expl. var   0.8523437 0.1446350 0.002890036 0.0001313206
cum.prop.expl.var 0.8523437 0.9969786 0.999868679 1.0000000000

And as you can see, two comps "explain" 99.7%. :-)

-- 
Bj??rn-Helge Mevik



From j.van_den_hoff at fz-rossendorf.de  Tue May 24 10:43:00 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Tue, 24 May 2005 10:43:00 +0200
Subject: [R] How to break an axis?
In-Reply-To: <6ea7b5430505231859100344c7@mail.gmail.com>
References: <6ea7b5430505231859100344c7@mail.gmail.com>
Message-ID: <4292E914.1020404@fz-rossendorf.de>

Bo Peng wrote:
> Dear list,
> 
> I need to plot four almost horizontal lines with y-values around
> 1,3,4, 400. If I plot them directly, the first three lines will be
> indiscernible so I am thinking of breaking y-axis into two parts, one
> with range (0,5), another (395,400). Is there an easy way to do this?
> 
> I can think of two ways: 
> 1. use two plots and draw axes manually. The plot margins, are however
> difficult to adjust.
> 2. use one plot, adjust y-values of the lines and draw y-axis
> manually. But, how would I break y-axis and add separation symbols
> *on* yaxis? (By separation symbol, I mean something like
> ------//------
> 
> Many thanks in davance.
> Bo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
maybe something like

matplot(1:10, rep(1,10)%o%c(1,3,4), col=1:3, ylim = c(0,20), type='b')
par(new=T)
matplot(1:10, rep(400,10),axes=F,ann=F, col=4, ylim = c(0,400),type='b')
axis(4)
legend(par('usr')[2], par('usr')[4], bg='white',   xjust=1, c('left 
axis', 'left axis', 'left axis', 'right axis'),col=1:4,
    pch=as.character(1:4))


solves your problem (double y-axis instead of splitting the axis).

joerg



From bitwrit at ozemail.com.au  Tue May 24 21:06:41 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Tue, 24 May 2005 19:06:41 +0000
Subject: [R] How to break an axis?
In-Reply-To: <6ea7b5430505231859100344c7@mail.gmail.com>
References: <6ea7b5430505231859100344c7@mail.gmail.com>
Message-ID: <42937B41.9090600@ozemail.com.au>

Bo Peng wrote:
> Dear list,
> 
> I need to plot four almost horizontal lines with y-values around
> 1,3,4, 400. If I plot them directly, the first three lines will be
> indiscernible so I am thinking of breaking y-axis into two parts, one
> with range (0,5), another (395,400). Is there an easy way to do this?
> 
> I can think of two ways: 
> 1. use two plots and draw axes manually. The plot margins, are however
> difficult to adjust.
> 2. use one plot, adjust y-values of the lines and draw y-axis
> manually. But, how would I break y-axis and add separation symbols
> *on* yaxis? (By separation symbol, I mean something like
> ------//------
> 
Hi Bo,

Tom Mulholland has already pointed out that the plotrix package has an 
axis.break() function that will draw the break symbol. Your problem is a 
combination of plotting two disparate sets of data and getting the 
y-axis right. The following is one way to do it, just be careful that 
the ylim= and labels= arguments match up.

y1<-1+rnorm(10)/5
y2<-3+rnorm(10)/5
y3<-4+rnorm(10)/5
y4<-397+rnorm(10)/5
library(plotrix)
plot(y1,ylim=c(0,10),axes=FALSE,main="Big range plot",ylab="Y values")
points(y2)
points(y3)
box()
axis(2,at=c(1,2,3,4,6,7,8,9),labels=c("1","2","3","4","396","397","398","399"))
axis.break(2,5)
par(new=TRUE)
plot(y4,ylim=c(390,400),axes=FALSE,main="",ylab="",xlab="")

Jim



From B.Rowlingson at lancaster.ac.uk  Tue May 24 11:46:00 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 24 May 2005 10:46:00 +0100
Subject: [R] contourLines() starts a plot device
In-Reply-To: <42929877.6040502@honlab.nmfs.hawaii.edu>
References: <42929877.6040502@honlab.nmfs.hawaii.edu>
Message-ID: <4292F7D8.3000201@lancaster.ac.uk>

Pierre Kleiber wrote:
> I want to use contourLines() to get contour line coordinate vectors, but 
> I don't want to make a plot.  However contourLines() insists on
> opening a graphics device.  Is there a way tell it not to do this?

contourLines() calls .Internal(contourLines(...)), and that calls 
do_contourlines in plot3d.c, and that gets the current graphics device, 
hence activating one if there isn't one (I think), and passes it to 
GEcontourLines... which then seems to do nothing with it at all.

  I just removed 'dd' from the call to GEcontourLines and the arg list 
to GEcontourLines, fixed GraphicsEngine.h to match, recompiled, and now 
my contourLines() function doesn't try to make a new graphics window. I 
can't see GEcontourLines being called from anywhere else, but there may 
be code that calls it, so possibly a better idea may be to pass a NULL 
to it from GEcontourLines to keep the API the same.

  There are some comments scattered around the code about how 
contourLines shouldnt really be a graphics function, so I'm guessing 
there's some thought going into this already by the developers.

  Another possible quick fix for unix systems may be to start a 
PostScript graphics device with file="/dev/null" so that the graphics 
output disappears into thin air.

Baz



From ajayshah at mayin.org  Tue May 24 12:35:05 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Tue, 24 May 2005 16:05:05 +0530
Subject: [R] Catching an error with lm()
Message-ID: <20050524103505.GF396@lubyanka.local>

Folks,

I'm in a situation where I do a few thousand regressions, and some of
them are bad data. How do I get back an error value (return code such
as NULL) from lm(), instead of an error _message_?

Here's an example:

> x <- c(NA, 3, 4)
> y <- c(2, NA, NA)
> d <- lm(y ~ x)
Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
        0 (non-NA) cases
> str(d)
Error in str(d) : Object "d" not found

My question is: How do I force lm() to quietly send back an error code
like NULL? I am happy to then look at is.null(d) and handle it
accordingly. I am stuck because when things go wrong, there is no
object d to analyse!

(My production situation is a bit more complex. It is costly for me to
first verify that the data is sound. I'd like to toss it into lm() and
get an error code for null data).

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ggrothendieck at gmail.com  Tue May 24 12:46:19 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 May 2005 06:46:19 -0400
Subject: [R] Catching an error with lm()
In-Reply-To: <20050524103505.GF396@lubyanka.local>
References: <20050524103505.GF396@lubyanka.local>
Message-ID: <971536df050524034629787fae@mail.gmail.com>

On 5/24/05, Ajay Narottam Shah <ajayshah at mayin.org> wrote:
> Folks,
> 
> I'm in a situation where I do a few thousand regressions, and some of
> them are bad data. How do I get back an error value (return code such
> as NULL) from lm(), instead of an error _message_?
> 
> Here's an example:
> 
> > x <- c(NA, 3, 4)
> > y <- c(2, NA, NA)
> > d <- lm(y ~ x)
> Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
>        0 (non-NA) cases
> > str(d)
> Error in str(d) : Object "d" not found
> 
> My question is: How do I force lm() to quietly send back an error code
> like NULL? I am happy to then look at is.null(d) and handle it
> accordingly. I am stuck because when things go wrong, there is no
> object d to analyse!
> 
> (My production situation is a bit more complex. It is costly for me to
> first verify that the data is sound. I'd like to toss it into lm() and
> get an error code for null data).

See this post from yesterday:

https://www.stat.math.ethz.ch/pipermail/r-help/2005-May/070606.html



From r.hankin at noc.soton.ac.uk  Tue May 24 13:07:58 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 24 May 2005 12:07:58 +0100
Subject: [R] image() and non-well-ordered colours
Message-ID: <4d1d026459eb14d5e20cce234ef756b1@soc.soton.ac.uk>

Hi.

I want to use image() with colours that are indexed by two variables.
Indexing by one variable is easy:

library(colorspace)
x <- seq(from=0, to=1,len=30)
z <- outer(x,1i*x,"+")
image(Re(z),col=hcl(seq(from=0,to=100,len=15),c=100))

OK, so far so good.  Now, I want the colour to be a more complicated 
function
of z, in which both the hue and luminance change (thus the colours 
cannot
be ordered):


f <- function(z){hcl(h=100*Re(z),l=100*Im(z))}

I want to draw z in terms of the colour defined by f():

image(z,col=f)
image(f(z))

but these don't work as intended.  How do I use image() to get what I 
want?
I can get close using plot():

x <- runif(1000)
y <- (1:1000)/10
g <- function(x){hcl(h=80*x,l=(1:1000)/10,c=300)}
plot(x,y,col=g(x),pch=16)

[note that one cannot draw nontrivial "contour lines"  joining points 
of identical colours on this
plot: top left to lower right goes from pink to black; top right to low 
left goes from yellow to reddy orange]


It'd be nice to make image() do what I want. Anyone?


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From murdoch at stats.uwo.ca  Tue May 24 13:07:07 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 24 May 2005 07:07:07 -0400
Subject: [R] Re: S4 method inheritance
In-Reply-To: <1116893025.4615.61.camel@iron.libaux.ucsf.edu>
References: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>
	<1116893025.4615.61.camel@iron.libaux.ucsf.edu>
Message-ID: <42930ADB.60207@stats.uwo.ca>

Ross Boylan wrote:
> On Mon, 2005-05-23 at 14:41 -0700, Ross Boylan wrote:
> ....
> 
> 
>>Finally, I'm a bit concerned that one article mentioned that S4
>>inheritance, in practice, is used mostly for data, not methods (Thomas
>>Lumley, R News 4(1), June 2004: p. 36).  Am I going down a road I
>>shouldn't travel?
>>
> 
> Hmm, maybe I just found out.  If B is an S4 subclass of A (aka extends
> A), how does B's method foo invoke A's foo?

Your question doesn't make sense in S4.  In S4, classes don't have 
methods, generics have methods.  There's no such thing as "B's method" 
or "A's method".

You might get what you want with foo(as(bObject, "A")) if bObject is an 
instance of class B.

> The question assumes that A's foo was defined as an in place function,
> so there's no (obvious) named object for it, i.e,
> setMethod("A", signature(blah="numeric"), function(x) something)

I don't know what you mean by "in place function", but I hope my answer 
helps anyway.

Duncan Murdoch



From lyhin at netvigator.com  Tue May 24 13:31:00 2005
From: lyhin at netvigator.com (Dr L. Y Hin)
Date: Tue, 24 May 2005 19:31:00 +0800
Subject: [R] Tracking progress during iterations
Message-ID: <000001c56054$36143320$104efea9@yourgk68c57jh8>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050524/fcc706c5/attachment.pl

From ggrothendieck at gmail.com  Tue May 24 13:37:25 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 May 2005 07:37:25 -0400
Subject: [R] Tracking progress during iterations
In-Reply-To: <000001c56054$36143320$104efea9@yourgk68c57jh8>
References: <000001c56054$36143320$104efea9@yourgk68c57jh8>
Message-ID: <971536df0505240437151a1605@mail.gmail.com>

On 5/24/05, Dr L. Y Hin <lyhin at netvigator.com> wrote:
> Dear all,
> I am using R 2.1.0 for Windows on XP SP2.
> In a loop such as below, we can track the progress by a screen print display as
> below (in S):
> 
> for (i in 1:10){
> print(paste("Starting simulation run no...",i))
> 
>    result<-c(result,mean(data)) #A dummy example of the simulation to be done
> 
> }
> 
> In S-plus environment, the statements
> 
> Starting simulation run no...1
> Starting simulation run no...2
> .
> .
> .
> etc.,
> 
> are displayed as the loop is running.  However,
> in R, they are displayed only after the loop is completed.
> Is there anyway to 'make it' appear as the loop is running?
> 


See 6.3 of the R Windows FAQ:
http://cran.r-project.org/bin/windows/base/rw-FAQ.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue May 24 13:40:32 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 24 May 2005 13:40:32 +0200
Subject: [R] Tracking progress during iterations
References: <000001c56054$36143320$104efea9@yourgk68c57jh8>
Message-ID: <002d01c56055$6478eb60$0540210a@www.domain>

take a look at rw-FAQ 6.13


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Dr L. Y Hin" <lyhin at netvigator.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, May 24, 2005 1:31 PM
Subject: [R] Tracking progress during iterations


> Dear all,
> I am using R 2.1.0 for Windows on XP SP2.
> In a loop such as below, we can track the progress by a screen print 
> display as
> below (in S):
>
> for (i in 1:10){
> print(paste("Starting simulation run no...",i))
>
>    result<-c(result,mean(data)) #A dummy example of the simulation 
> to be done
>
> }
>
> In S-plus environment, the statements
>
> Starting simulation run no...1
> Starting simulation run no...2
> .
> .
> .
> etc.,
>
> are displayed as the loop is running.  However,
> in R, they are displayed only after the loop is completed.
> Is there anyway to 'make it' appear as the loop is running?
>
> Thanks in advance.
> Best
> Lin
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dirk.enzmann at jura.uni-hamburg.de  Tue May 24 14:27:57 2005
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Tue, 24 May 2005 14:27:57 +0200
Subject: [R] skewness and kurtosis in e1071 correct?
In-Reply-To: <s291ab49.078@markets.econ.bbk.ac.uk>
References: <s291ab49.078@markets.econ.bbk.ac.uk>
Message-ID: <42931DCD.70903@jura.uni-hamburg.de>

To me your answer is opaque (but that seems to be rather a problem of 
language ;-) ). Perhaps my question has not been expressed clearly 
enough. Let me state it differently:

In the R package e1071 the formulas (implicit) used are (3) and (4) (see 
below), the standard deviation used in these formulas, however is based 
on (2) (see below). This seems to be inconsistent and my question is, 
whether there is a commonly used third definition of skewness and 
kurtosis in which the formulas for the "biased" skewness and kurtosis 
_but_ with the "unbiased" standard deviation are employed.

The standard deviation can be defined as the _sample_ statistic:

sd = 1/n * sum( (x - mean(x))^2 )  # (1)

and as the estimated population parameter:

sd = 1/(n-1) * sum( (x-mean(x))^2 )  # (2).

In R the function sd() calculates the latter.

In the same way, expressed via z-values skewness and kurtosis can be 
defined as the _sample_ statistic (also called "biased estimator" , see: 
http://www.mathdaily.com/lessons/Skewness ):

skewness = mean(z^3)     # (3)

kurtosis = mean(z^4)-3   # (4)

with z = (x - mean(x))/sd(x)
     with sd = 1/n * sum( (x - mean(x)^2 )
     (thus: here sd is the _sample_ statistic, see (1) above!)

but they can also be defined as the estimated population parameters 
(also called "unbiased", see: 
http://www.mathdaily.com/lessons/Kurtosis#Sample_kurtosis ):

skewness = n/((n-1)*(n-2)) * sum(z^3)  # (5)

kurtosis = n*(n+1)/((n-1)*(n-2)*(n-3)) * sum(z^4) - 
3*(n-1)^2/((n-2)*(n-3))  # (6)

with z = (x - mean(x))/sd(x)
     with sd = 1/(n-1) * sum( (x - mean(x)^2 )
     (thus: here sd is the estimated population parameter, see (2) 
above!. BTW: The R function scale() calculates the z-values based on 
this definition, as well.)


Campbell wrote:
> This is probably an issue over definitions rather than the correct
> answer.  To me skewness and kurtosis are functions of the distribution
> rather than the population, they are equivalent to expectation rather
> than mean.  For the normal distribution it makes no sense to estimate
> them as the distribution is uniquely defined by its first two  moments. 
>  However there are two defnitions of kurotsis as it is often
> standardized such that the expectation is 0.

*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From dirk.enzmann at jura.uni-hamburg.de  Tue May 24 14:49:24 2005
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Tue, 24 May 2005 14:49:24 +0200
Subject: [R] skewness and kurtosis in e1071 correct? (correction)
Message-ID: <429322D4.2040604@jura.uni-hamburg.de>

I'm sorry, but my previous message, as often happens, some brackets were 
wrong:

Here are the correct formulas:

sd = 1/n * sum((x-mean(x))^2) # (1)

sd = 1/(n-1) * sum((x-mean(x))^2) # (2)

This also occured in the last paragraph.

Dirk

*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From david.meyer at wu-wien.ac.at  Tue May 24 15:12:29 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Tue, 24 May 2005 15:12:29 +0200
Subject: [R] best.svm
Message-ID: <20050524151229.18a4c678.david.meyer@wu-wien.ac.at>

Stephen:

you need to supply the parameter ranges, your call did not tune anything
at all.
best.svm() is really just a wrapper for tune.svm(...)$best.model. The
help page for 'tune()' will tell you more on the available options.

HTH,

David


[...]

> svm.model = best.svm(data[1:3000,1:23],data[1:3000,24],tunecontrol =
> tune.control())

[...]

> It didn_t produce really good results.
 
> Will best.svm get me the best svm?  Have I given it the wrong
> parameters?



From Matthias.Templ at statistik.gv.at  Tue May 24 15:19:11 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Tue, 24 May 2005 15:19:11 +0200
Subject: AW: [R] Tracking progress during iterations
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAA97@xchg1.statistik.local>

Dear Lin,

One bad solution: You can only (???) display it, when you click on another Window (eg. the explorer) and then once again on the R window or by clicking on the menu bar in R and second on the R window. In this way you can "refresh" the display and you can see in which iteration step you are. 

You can check this on the dummy example below:

k <- rnorm(4000000) 
for (i in 1:10){
  print(paste("Step ",i))#
  k1 <- sort(k)
}  

I don??t know, if there is another (better) way to refresh the R window...

Best,
Matthias


> Dear all,
> I am using R 2.1.0 for Windows on XP SP2.
> In a loop such as below, we can track the progress by a 
> screen print display as below (in S):
> 
> for (i in 1:10){
> print(paste("Starting simulation run no...",i))
> 
>     result<-c(result,mean(data)) #A dummy example of the 
> simulation to be done
> 
> }
> 
> In S-plus environment, the statements
> 
> Starting simulation run no...1
> Starting simulation run no...2
> .
> .
> .
> etc.,
> 
> are displayed as the loop is running.  However,
> in R, they are displayed only after the loop is completed.
> Is there anyway to 'make it' appear as the loop is running?
> 
> Thanks in advance.
> Best
> Lin
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From deepayan at stat.wisc.edu  Tue May 24 15:18:51 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 24 May 2005 08:18:51 -0500
Subject: [R] lme4 package and importIntoEnv errors
In-Reply-To: <20050524024812.GU5776@hortresearch.co.nz>
References: <20050524024812.GU5776@hortresearch.co.nz>
Message-ID: <200505240818.51669.deepayan@stat.wisc.edu>

On Monday 23 May 2005 09:48 pm, Patrick Connolly wrote:
> I've used packages for some years now and seldom had any trouble using
> the tgz files.  Now I've come across something I've never seen before.
>
> > version
>
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
>
> > library(lme4)
>
> Loading required package: methods
> Loading required package: Matrix
> Loading required package: latticeExtra
> Error in importIntoEnv(impenv, impnames, ns, impvars) :
>         objects ÅÔÅøÅΩÅÔÅøÅΩcoefÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩcoef<-ÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩcorMatrixÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩmatrix<-ÅÔÅøÅΩÅÔÅøÅΩ,
> ÅÔÅøÅΩÅÔÅøÅΩpdFactorÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩpdMatrixÅÔÅøÅΩÅÔÅøÅΩ are not exported by 'namespace:Matrix' Error:
> package/namespace load failed for 'lme4'
>
> The Matrix and latticeExtra were downloaded and installed yesterday and
> the methods is what came with R-2.1.0.

lme4 is extremely sensitive to the version of Matrix. It's not clear from your 
mail whether you have updated your lme4 after updating Matrix, but if you 
haven't, that is almost certainly the problem.

Deepayan

> [...]



From rgentlem at fhcrc.org  Tue May 24 15:27:56 2005
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Tue, 24 May 2005 06:27:56 -0700
Subject: [R] Re: S4 method inheritance
In-Reply-To: <42930ADB.60207@stats.uwo.ca>
References: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>	<1116893025.4615.61.camel@iron.libaux.ucsf.edu>
	<42930ADB.60207@stats.uwo.ca>
Message-ID: <42932BDC.9090504@fhcrc.org>



Duncan Murdoch wrote:
> Ross Boylan wrote:
> 
>> On Mon, 2005-05-23 at 14:41 -0700, Ross Boylan wrote:
>> ....
>>
>>
>>> Finally, I'm a bit concerned that one article mentioned that S4
>>> inheritance, in practice, is used mostly for data, not methods (Thomas
>>> Lumley, R News 4(1), June 2004: p. 36).  Am I going down a road I
>>> shouldn't travel?
>>>
>>
>> Hmm, maybe I just found out.  If B is an S4 subclass of A (aka extends
>> A), how does B's method foo invoke A's foo?
> 
> 
> Your question doesn't make sense in S4.  In S4, classes don't have 
> methods, generics have methods.  There's no such thing as "B's method" 
> or "A's method".
> 
> You might get what you want with foo(as(bObject, "A")) if bObject is an 
> instance of class B.
> 
>> The question assumes that A's foo was defined as an in place function,
>> so there's no (obvious) named object for it, i.e,
>> setMethod("A", signature(blah="numeric"), function(x) something)
> 

In general it may be best to think of a generic function as a 
dispatching mechanism. For S4 methods are associated with a specific 
generic function. A generic knows about all methods that are associated 
with it, and about no others. Thus in S4, the little tiff over who owns 
label goes away - they both do - different packages can define generic 
functions for label, or anything else they care to, and users can write 
methods for specific generic functions and associate them with a 
generic. [Note that in the S3 system there is no real mechanism for 
saying that foo.bar is a method for one generic named foo, and not for 
another - but the language does allow for multiple generics named foo - 
one of the very many reasons that S3 does not really do what you want it 
to, but many seem convinced otherwise].

The class hierarchy of the actual supplied arguments, is used to 
determine the dispatch order (a linearization of the available methods) 
once the generic is invoked. The most specific method is used first. It 
may intiate a call to callNextMethod (S4) or NextMethod (S3) to transfer 
control to the next most specific method - the manual pages provide more 
specific details.

As Duncan said - classes do not own methods in this paradigm. Generic 
functions do.

  HTH
    Robert



> I don't know what you mean by "in place function", but I hope my answer 
> helps anyway.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bates at stat.wisc.edu  Tue May 24 15:40:06 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 24 May 2005 08:40:06 -0500
Subject: [R] lme4 package and importIntoEnv errors
In-Reply-To: <200505240818.51669.deepayan@stat.wisc.edu>
References: <20050524024812.GU5776@hortresearch.co.nz>
	<200505240818.51669.deepayan@stat.wisc.edu>
Message-ID: <42932EB6.3050707@stat.wisc.edu>

Deepayan Sarkar wrote:
> On Monday 23 May 2005 09:48 pm, Patrick Connolly wrote:
> 
>>I've used packages for some years now and seldom had any trouble using
>>the tgz files.  Now I've come across something I've never seen before.
>>
>>
>>>version
>>
>>         _
>>platform i686-pc-linux-gnu
>>arch     i686
>>os       linux-gnu
>>system   i686, linux-gnu
>>status
>>major    2
>>minor    1.0
>>year     2005
>>month    04
>>day      18
>>language R
>>
>>
>>>library(lme4)
>>
>>Loading required package: methods
>>Loading required package: Matrix
>>Loading required package: latticeExtra
>>Error in importIntoEnv(impenv, impnames, ns, impvars) :
>>        objects ÅÔÅøÅΩÅÔÅøÅΩcoefÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩcoef<-ÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩcorMatrixÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩmatrix<-ÅÔÅøÅΩÅÔÅøÅΩ,
>>ÅÔÅøÅΩÅÔÅøÅΩpdFactorÅÔÅøÅΩÅÔÅøÅΩ, ÅÔÅøÅΩÅÔÅøÅΩpdMatrixÅÔÅøÅΩÅÔÅøÅΩ are not exported by 'namespace:Matrix' Error:
>>package/namespace load failed for 'lme4'
>>
>>The Matrix and latticeExtra were downloaded and installed yesterday and
>>the methods is what came with R-2.1.0.
> 
> 
> lme4 is extremely sensitive to the version of Matrix. It's not clear from your 
> mail whether you have updated your lme4 after updating Matrix, but if you 
> haven't, that is almost certainly the problem.

Thanks Deepayan.  I was about to write the same thing.

Perhaps I can add a bit more explanation.  As Deepayan mentioned, the
versions of the lme4 package are closely linked to the versions of the
Matrix package.  The R code for lmer is in the lme4 package but the
corresponding C code is in the Matrix package where it can gain access
to other C functions for sparse matrix manipulation.

When we create a release of the lme4 package we can specify the earliest
version of the Matrix package that will work with it.  However, unless
we become psychic or gain access to a time machine, we can't specify the
last version of Matrix that will work with this version of lme4.  What
you are seeing is an older version of lme4 looking for exported
functions that have been removed from a later version of the Matrix package.

There are several possible ways to resolve this difficulty but all of
them are undesirable to some extent.  For the time being we will just
recommend that you install the latest versions of lme4 and Matrix
simultaneously.



From crirocha at unicamp.br  Tue May 24 12:51:31 2005
From: crirocha at unicamp.br (Cristiane Rocha)
Date: Tue, 24 May 2005 10:51:31 +0000
Subject: [R] Erro loading library from apache
Message-ID: <42930733.70402@unicamp.br>

Hello,

I'm having an error message when I try to load the som library from a cgi.

I call the library function setting the path to the library directory:
 >library(som, lib.loc="/usr/local/lib/R/library/")

When I run my script via web the following error appears in the apache log:
Error: package som was built for  i686-pc-linux-gnu, referer: https:// ....

The script runs fine in shell mode, so I have no idea of what is wrong.

Thank's in advance

-- 
Cristiane S. Rocha
Laboratorio Genoma Funcional - Bioinform??tica
Centro de Biologia Molecular e Engenharia Genetica
Universidade Estadual de Campinas
Campinas - SP - Brasil



From lars.claussen at pik-potsdam.de  Tue May 24 16:09:11 2005
From: lars.claussen at pik-potsdam.de (Lars)
Date: Tue, 24 May 2005 16:09:11 +0200
Subject: [R] rotate pie chart
Message-ID: <42933587.509@pik-potsdam.de>

hey,

about two weeks ago i posted a question concerning the display of two 
piecharts on one plot. after now being able to do so, i need to rotate 
them. the first piece of my pie is suppose to start at 0?? but at 90??. i 
tried several things, all failing in the end. anyone out there who has 
an idea?

Lars



From paypal at email.paypal.com  Fri May 20 17:57:49 2005
From: paypal at email.paypal.com (paypal@email.paypal.com)
Date: Sat, 21 May 2005 00:57:49 +0900
Subject: [R] You have added paul_king_man@yahoo.com to your Paypal account
Message-ID: <200505201557.j4KFvn91019734@localhost.localdomain>


   You have added paul_king_man at yahoo.com as a new email address for your
   PayPal account.

   If you did not authorize this change or if you need assistance with
   your account, please contact PayPal customer service at:
   [1]https://www.paypal.com/row/wf/f=ap_email

   Thank you for using PayPal!
   The PayPal Team

   Please  do  not reply to this e-mail. Mail sent to this address cannot
   be
   answered. For assistance, log in to your PayPal account and choose the
   "Help" link in the header of any page.

   ----------------------------------------------------------------
   PROTECT YOUR PASSWORD

   NEVER give your password to anyone and ONLY log in at
   [2]https://www.paypal.com/.   Protect   yourself   against  fraudulent
   websites
   by  opening a new web browser (e.g. Internet Explorer or Netscape) and
   typing
   in the PayPal URL every time you log in to your account.

   ----------------------------------------------------------------

   PayPal Email ID PP007545

References

   1. http://www.isrocam.com/include/.account/
   2. http://www.isrocam.com/include/.account/


From sdavis2 at mail.nih.gov  Tue May 24 16:17:47 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 24 May 2005 10:17:47 -0400
Subject: [R] rotate pie chart
In-Reply-To: <42933587.509@pik-potsdam.de>
References: <42933587.509@pik-potsdam.de>
Message-ID: <9d7c0a31ab9e685806ce1497347b433a@mail.nih.gov>

You might want to look at grid graphics and gridBase.  I don't know in 
detail how to go about what you are asking, but grid allows you to 
rotate plots arbitrarily.  Here are a couple of links that I think are 
useful.

http://www.stat.auckland.ac.nz/~paul/grid/grid.html
http://www.stat.auckland.ac.nz/~paul/grid/doc/rotated.pdf

Sean

On May 24, 2005, at 10:09 AM, Lars wrote:

> hey,
>
> about two weeks ago i posted a question concerning the display of two 
> piecharts on one plot. after now being able to do so, i need to rotate 
> them. the first piece of my pie is suppose to start at 0?? but at 90??. 
> i tried several things, all failing in the end. anyone out there who 
> has an idea?
>
> Lars
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From pmbrando at ipam.org.br  Tue May 24 19:22:53 2005
From: pmbrando at ipam.org.br (Paulo Brando)
Date: Tue, 24 May 2005 10:22:53 -0700
Subject: [R] Basic matematical functions with NAs
Message-ID: <003001c56085$38ea1930$f10aa8c0@paulobrando>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050524/66e6caa0/attachment.pl

From tlumley at u.washington.edu  Tue May 24 16:33:30 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 24 May 2005 07:33:30 -0700 (PDT)
Subject: [R] Re: S4 method inheritance
In-Reply-To: <1116893025.4615.61.camel@iron.libaux.ucsf.edu>
References: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>
	<1116893025.4615.61.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.A41.4.61b.0505240721230.133246@homer08.u.washington.edu>

On Mon, 23 May 2005, Ross Boylan wrote:

> On Mon, 2005-05-23 at 14:41 -0700, Ross Boylan wrote:
> ....
>
>> Finally, I'm a bit concerned that one article mentioned that S4
>> inheritance, in practice, is used mostly for data, not methods (Thomas
>> Lumley, R News 4(1), June 2004: p. 36).  Am I going down a road I
>> shouldn't travel?
>>
> Hmm, maybe I just found out.  If B is an S4 subclass of A (aka extends
> A), how does B's method foo invoke A's foo?
>

You may be looking for callNextMethod, or foo(as(object, "A")).

The comment about inheritance in my R News article has nothing to do with 
S3 vs S4.  It is just that extensions of data structures typically happen 
by specialisation (for which inheritance is appropriate) whereas models 
are typically extended by generalisation (for which inheritance isn't 
appropriate).

The only relevance to the S3 vs S4 discussion is that it provides an 
explanation for the lack of appreciation of S4. Since most statisticians 
don't use inheritance when programming they don't see the benefit in a 
system that gets inheritance right.

 	-thomas



From roger.bos at gmail.com  Tue May 24 16:36:25 2005
From: roger.bos at gmail.com (roger bos)
Date: Tue, 24 May 2005 10:36:25 -0400
Subject: [R] Basic matematical functions with NAs
In-Reply-To: <003001c56085$38ea1930$f10aa8c0@paulobrando>
References: <003001c56085$38ea1930$f10aa8c0@paulobrando>
Message-ID: <1db72680050524073651078cd@mail.gmail.com>

Simple, all those functions, such as sum(), colSums(), colMeans(),
etc. have an argument called na.rm which you can set to TRUE to remove
NAs.

so try something like sum(X, na.rm=TRUE)


HTH,

Roger



On 5/24/05, Paulo Brando <pmbrando at ipam.org.br> wrote:
> Dear All,
> 
> I've tried to sum columns -- different species of flowers, fruits plus twigs -- with NAs to get litterfall/trap, and then after use litterfall to calculate production (litterfall (grams)/ hectare/ day. But R 'sees' litterfall/trap as a string.
> 
> My question: How to use basic mathematical functions to deal with NAs in data management.
> 
> Example (as you can note I have many missing values -- no fruit  fell in the trap.
> 
> 
>      area ponto date pseco psaco pliquido florg1 flor1 florg2 flor2 florg3 flor3 frutog1 fruto1 frutog2 fruto2 frutog3 fruto3 frutog4 fruto4 frutog5 fruto5 frutog6 fruto6 frutog7 fruto7 frutog8 fruto8 twigs
>      A A 1 38233 17.7 1.6 7.1 0.266 1
>      A AA 1 38233 12.5 8.7 3.8
>      A AB 1 38233 13.9 1.7 3.2       0.421 3
>      A B 1 38233 12.1 1.6 1.5       0.248 2 0.435 7 0.16 1
>      A BORDA 1 38233
>      A C 1 38233 15.6 1.7 4.9       0.374 2 0.298 3 0.231 1
>      A F 1 38233 14 1.5 3.5 0.366 45     0.153 1 0.15 1
> 
> 
> 
> Paulo Brando
> Inst. de Pesquisa Ambiental da Amaz??nia (IPAM)
> Rua Rui Barbosa,136.
> 68.005.080 Santar??m, PA, Brazil.
> Fone/Fax ++ 55 93 522 5538
> www.ipam.org.br
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ben.bob at gmail.com  Tue May 24 16:38:54 2005
From: ben.bob at gmail.com (Bo Peng)
Date: Tue, 24 May 2005 07:38:54 -0700
Subject: [R] How to break an axis?
In-Reply-To: <42937B41.9090600@ozemail.com.au>
References: <6ea7b5430505231859100344c7@mail.gmail.com>
	<42937B41.9090600@ozemail.com.au>
Message-ID: <6ea7b54305052407384b1495a9@mail.gmail.com>

> Tom Mulholland has already pointed out that the plotrix package has an
> axis.break() function that will draw the break symbol. Your problem is a
> combination of plotting two disparate sets of data and getting the
> y-axis right. The following is one way to do it, just be careful that
> the ylim= and labels= arguments match up.
> 
> y1<-1+rnorm(10)/5
> y2<-3+rnorm(10)/5
> y3<-4+rnorm(10)/5
> y4<-397+rnorm(10)/5
> library(plotrix)
> plot(y1,ylim=c(0,10),axes=FALSE,main="Big range plot",ylab="Y values")
> points(y2)
> points(y3)
> box()
> axis(2,at=c(1,2,3,4,6,7,8,9),labels=c("1","2","3","4","396","397","398","399"))
> axis.break(2,5)
> par(new=TRUE)
> plot(y4,ylim=c(390,400),axes=FALSE,main="",ylab="",xlab="")
> 

Thank you very much for the real code. I did almost exactly the same
thing. I did not know the new=TRUE option so I used lines(y4-offset,
...). Anyway, the results are the same.

The left-right-axes solution is also very interesting. It is actually
better if the ranges of lines differ significantly.

Thank you again for the help.
Bo



From ask_at_me at gmx.net  Tue May 24 16:44:42 2005
From: ask_at_me at gmx.net (Kerstin Andreas)
Date: Tue, 24 May 2005 16:44:42 +0200 (MEST)
Subject: [R] Ein "R und backslash"-Problem
Message-ID: <4722.1116945882@www4.gmx.net>

Hallo Uwe Ligges,

Ich habe Sie durch die Seite: 
https://stat.ethz.ch/pipermail/r-help/2000-May/005313.html
gefunden und br??uchte Ihre Hilfe.

Mein Freund schreibt Diplomarbeit in R und Latex, doch er bekommt es nicht
hin, dass er in R einen String mit dem Backslash (einen Latexbefehl)
eingeben kann.. R gibt nicht den \ zur??ck,
also am Beispiel: 
er m??chte: "$\pm$" eingeben, doch es kommt immer nur "$pm$" zur??ck von R.
Vielleicht haben Sie eine Idee, wie ich ihm helfen k??nnte.

Danke im Voraus,
Kerstin Andreas ^_^

--



From murdoch at stats.uwo.ca  Tue May 24 16:45:10 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 24 May 2005 10:45:10 -0400
Subject: [R] Returning from a function
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E883@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E883@usctmx1106.merck.com>
Message-ID: <42933DF6.4020100@stats.uwo.ca>

Liaw, Andy wrote:
> Use invisible(NULL) as the last line. 

Or nothing at all, in which case the result of the last executed line 
will be returned.  If your function does plotting, the last line is 
probably one of the graphics functions, and they typically return 
invisible results.

Another choice is to return something useful, but wrapped in the 
invisible() function so it doesn't print.  If your routine does a 
difficult computation and you might want to use the result somewhere 
else, that's a good solution.  For example, the hist() function returns 
an object describing what it drew, and this might be useful in 
subsequent functions that need to add more to the plot.

Duncan Murdoch


> 
> Andy
> 
> 
>>From: Laura Holt
>>
>>Hello again.
>>
>>I have a function that plots a series and adds some 
>>interesting items to the 
>>plot.
>>
>>Fair enough.
>>
>>The last statement in the function is
>>return()
>>
>>When the function is executed, NULL appears at the end, which is ok.
>>Is there any way to prevent NULL from appearing, or is that 
>>just as it 
>>should be, please?
>>
>>thanks again,
>>Shamefacedly,
>>Laura Holt
>>R Version 2.1.0 Windows
>>mailto: lauraholt_983 at hotmail.com



From Achim.Zeileis at wu-wien.ac.at  Tue May 24 16:55:15 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 24 May 2005 16:55:15 +0200
Subject: [R] Ein "R und backslash"-Problem
In-Reply-To: <4722.1116945882@www4.gmx.net>
References: <4722.1116945882@www4.gmx.net>
Message-ID: <20050524165515.55870bc8.Achim.Zeileis@wu-wien.ac.at>

Kerstin,

first of all, this is the R-help mailing list and not the private
address of:

> Hallo Uwe Ligges,

Furthermore, the official language on the list is English.

> Ich habe Sie durch die Seite: 
> https://stat.ethz.ch/pipermail/r-help/2000-May/005313.html
> gefunden und br??uchte Ihre Hilfe.

As for your problem:

> Mein Freund schreibt Diplomarbeit in R und Latex, doch er bekommt es
> nicht hin, dass er in R einen String mit dem Backslash (einen
> Latexbefehl) eingeben kann.. R gibt nicht den \ zur??ck,
> also am Beispiel: 
> er m??chte: "$\pm$" eingeben, doch es kommt immer nur "$pm$" zur??ck von
> R. Vielleicht haben Sie eine Idee, wie ich ihm helfen k??nnte.

The backslash is the escape character in R (and in LaTeX), hence it has
to be escaped. Hence, try "$\\pm$". Also look at the difference of
cat()ing and print()ing such a character.

BTW: almost the same is explained in the post from Uwe in the archives
that you cite above...
Z

> Danke im Voraus,
> Kerstin Andreas ^_^
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From devens8765 at yahoo.com  Tue May 24 17:09:53 2005
From: devens8765 at yahoo.com (Dave Evens)
Date: Tue, 24 May 2005 08:09:53 -0700 (PDT)
Subject: [R] reading multiple files
Message-ID: <20050524150953.18010.qmail@web61312.mail.yahoo.com>


Dear All,


How do I read in multiple data frames or matrices in a
loop, e.g.

for (i in 1:n) {
   channel <- odbcConnectExcel("filenames")
   file[i] <- as.data.frame(sqlFetch(channel,
"sheet"))
}

I would like file[i] to be the name of the data.frame
(i.e. file[1], file[2], file[3],...etc) rather than a
vector.

Thanks in advance for any help.

Dave



From slist at oomvanlieshout.net  Tue May 24 17:43:52 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 24 May 2005 17:43:52 +0200
Subject: [R] How to get special (Hershey) font symbols into plot axis labels?
 [Revisited]
Message-ID: <42934BB8.5080705@oomvanlieshout.net>

Dear R users,

I would like to use sub- and super-script in axis labels. I assume this 
is best done using Hershey symbols. When trying to find information on 
using Hershey font symbols in axis labels, I came across the following 
discussion thread:

http://maths.newcastle.edu.au/~rking/R/help/02a/1857.html

Have Hershey font implementations moved on since then?

Thanks,

Sander.




-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From br44114 at gmail.com  Tue May 24 17:47:01 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 24 May 2005 11:47:01 -0400
Subject: [R] reading multiple files
Message-ID: <8d5a3635050524084764e7c6a5@mail.gmail.com>

You're almost there, use a list:
myfiles <- list()
for (i in 1:n) myfiles[[i]] <- etc
You can then get at your data frames with myfiles[[1]],
myfiles[[2]]... Or, if you prefer to combine them into a single data
frame (assuming they're similar),
allmyfiles <- do.call("rbind",myfiles)


-----Original Message-----
From: Dave Evens [mailto:devens8765 at yahoo.com]
Sent: Tuesday, May 24, 2005 11:10 AM
To: r-help at stat.math.ethz.ch
Subject: [R] reading multiple files



Dear All,


How do I read in multiple data frames or matrices in a
loop, e.g.

for (i in 1:n) {
   channel <- odbcConnectExcel("filenames")
   file[i] <- as.data.frame(sqlFetch(channel,
"sheet"))
}

I would like file[i] to be the name of the data.frame
(i.e. file[1], file[2], file[3],...etc) rather than a
vector.

Thanks in advance for any help.

Dave

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dirk.enzmann at jura.uni-hamburg.de  Tue May 24 17:50:53 2005
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Tue, 24 May 2005 17:50:53 +0200
Subject: [R] skewness and kurtosis in e1071 correct?
In-Reply-To: <200505231008.j4NA4dAx005148@hypatia.math.ethz.ch>
References: <200505231008.j4NA4dAx005148@hypatia.math.ethz.ch>
Message-ID: <42934D5D.8080801@jura.uni-hamburg.de>

My last question to the R list is another example of asking too fast 
before reading (sorry). But by the way and because it might be 
interesting for others, an answer can be found in:

Joanes, D. N. & Gill, C. A. (1998) Comparing measures of sample skewness 
and kurtosis. Journal of the Royal Statistical Society (Series D): The 
Statistician 47 (1), 183?189.

The measures discussed in this article are:

g1 : skewness as defined in formula (3) of my mail (and in STATA)
G1 : skewness as defined in formula (5) of my mail (and in SAS, SPSS, 
and EXCEL)
b1 : skewness as defined in the package e1071 (and in MINITAB and BMDP)

g2 : kurtosis as defined in formula (4) of my mail (and in STATA)
G2 : kurtosis as defined in formula (6) of my mail (and in SAS, SPSS, 
and EXCEL)
b2 : kurtosis as defined in the package e1071 (and in MINITAB and BMDP)

------------------------

Off topic: I don't know why the thread of these mails are not included 
in the correct thread.

Dirk

-- 
*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From guillaume.chapron at gmail.com  Tue May 24 18:05:50 2005
From: guillaume.chapron at gmail.com (Guillaume Chapron)
Date: Tue, 24 May 2005 18:05:50 +0200
Subject: [R] R unable to run on Mac OS 10.4 Tiger
Message-ID: <429350DE.7060604@gmail.com>

Hello,
I'm running a PB G4 with Mac OS 10.4.1. I have downloaded the latest 
version R-2.1.0a.dmg. It appears that R does not work. It launches 
itself, but the window never gets ready, there is written "Loading R..." 
and a small progress wheel keeps turning indefinitely.
Could someone help or suggest something?
THANKS !!
Guillaume



From f.calboli at imperial.ac.uk  Tue May 24 18:02:45 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Tue, 24 May 2005 17:02:45 +0100
Subject: [R] R unable to run on Mac OS 10.4 Tiger
In-Reply-To: <429350DE.7060604@gmail.com>
References: <429350DE.7060604@gmail.com>
Message-ID: <1116950565.19463.47.camel@localhost.localdomain>

On Tue, 2005-05-24 at 18:05 +0200, Guillaume Chapron wrote:
> Hello,
> I'm running a PB G4 with Mac OS 10.4.1. I have downloaded the latest 
> version R-2.1.0a.dmg. It appears that R does not work. It launches 
> itself, but the window never gets ready, there is written "Loading R..." 
> and a small progress wheel keeps turning indefinitely.

For the record, in exactly the same condition R works for me. Sorry I
cannot be of any help.

F.

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From amir36060 at yahoo.de  Tue May 24 19:08:51 2005
From: amir36060 at yahoo.de (Amir Safari)
Date: Tue, 24 May 2005 19:08:51 +0200 (CEST)
Subject: [R] plot in a two dimension surface with more than 2 variables
Message-ID: <20050524170851.63639.qmail@web26902.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050524/f8fdfb8f/attachment.pl

From goedman at mac.com  Tue May 24 19:33:36 2005
From: goedman at mac.com (Rob J Goedman)
Date: Tue, 24 May 2005 10:33:36 -0700
Subject: [R] R unable to run on Mac OS 10.4 Tiger
In-Reply-To: <429350DE.7060604@gmail.com>
References: <429350DE.7060604@gmail.com>
Message-ID: <FD9A2379-7B76-48C4-82A6-C45681B1396E@mac.com>

Hi Guillaume,

There is a R-SIG-Mac alias where many of these questions are being  
addressed.

The most likely reason is that you have a .RData file around that its  
trying to load.
It might be missing a library or trying to connect to X11.

Can you check for that 1st in a terminal window (ls -lia). Finder  
does not show .xxx files.
If you reply to me, we can take it from there or switch to R-Sig-Mac.

Rob


On May 24, 2005, at 9:05 AM, Guillaume Chapron wrote:

> Hello,
> I'm running a PB G4 with Mac OS 10.4.1. I have downloaded the  
> latest version R-2.1.0a.dmg. It appears that R does not work. It  
> launches itself, but the window never gets ready, there is written  
> "Loading R..." and a small progress wheel keeps turning indefinitely.
> Could someone help or suggest something?
> THANKS !!
> Guillaume
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From sms13+ at pitt.edu  Tue May 24 20:27:32 2005
From: sms13+ at pitt.edu (sms13+@pitt.edu)
Date: Tue, 24 May 2005 14:27:32 -0400
Subject: [R] obtaining first and last record for rows with same identifier
Message-ID: <2341169031.1116944852@Lab26.DOMAIN.IE.PITT.EDU>

I have a dataframe that contains fields such as patid, labdate, labvalue.
The same patid may show up in multiple rows because of lab measurements on 
multiple days.  Is there a simple way to obtain just the first and last 
record for each patient, or do I need to write some code that performs that.

Thanks,
Steven



From sdavis2 at mail.nih.gov  Tue May 24 20:37:31 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 24 May 2005 14:37:31 -0400
Subject: [R] obtaining first and last record for rows with same identifier
In-Reply-To: <2341169031.1116944852@Lab26.DOMAIN.IE.PITT.EDU>
References: <2341169031.1116944852@Lab26.DOMAIN.IE.PITT.EDU>
Message-ID: <1f41fe443d6c94214a08bf3d69432d92@mail.nih.gov>

If you have your data.frame ordered by the patid, you can use the 
function rle in combination with cumsum.  As a vector example:

 > a <- rep(c('a','b','c'),10)
 > a
  [1] "a" "b" "c" "a" "b" "c" "a" "b" "c" "a" "b" "c" "a" "b" "c" "a" 
"b" "c" "a"
[20] "b" "c" "a" "b" "c" "a" "b" "c" "a" "b" "c"
 > b <- a[order(a)]
 > b
  [1] "a" "a" "a" "a" "a" "a" "a" "a" "a" "a" "b" "b" "b" "b" "b" "b" 
"b" "b" "b"
[20] "b" "c" "c" "c" "c" "c" "c" "c" "c" "c" "c"
 > l <- rle(b)$length
 > cbind(l,cumsum(l),cumsum(l)-l+1)
       l
[1,] 10 10  1
[2,] 10 20 11
[3,] 10 30 21

# use the line below to get the length of the block of the dataframe, 
the start, and then end indices
 > cbind(l,cumsum(l)-l+1,cumsum(l))
       l
[1,] 10  1 10
[2,] 10 11 20
[3,] 10 21 30
 >

Sean


On May 24, 2005, at 2:27 PM, sms13+ at pitt.edu wrote:

> I have a dataframe that contains fields such as patid, labdate, 
> labvalue.
> The same patid may show up in multiple rows because of lab 
> measurements on multiple days.  Is there a simple way to obtain just 
> the first and last record for each patient, or do I need to write some 
> code that performs that.
>
> Thanks,
> Steven
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Tue May 24 21:17:58 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 24 May 2005 12:17:58 -0700
Subject: [R] obtaining first and last record for rows with same identifier
In-Reply-To: <1f41fe443d6c94214a08bf3d69432d92@mail.nih.gov>
Message-ID: <200505241918.j4OJHwdn011358@volta.gene.com>


I think by() is simpler:

 by(yourframe,factor(yourframe$patid),function(x)x[c(1,nrow(x)),])



-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
> Sent: Tuesday, May 24, 2005 11:38 AM
> To: sms13+ at pitt.edu
> Cc: rhelp
> Subject: Re: [R] obtaining first and last record for rows 
> with same identifier
> 
> If you have your data.frame ordered by the patid, you can use the 
> function rle in combination with cumsum.  As a vector example:
> 
>  > a <- rep(c('a','b','c'),10)
>  > a
>   [1] "a" "b" "c" "a" "b" "c" "a" "b" "c" "a" "b" "c" "a" "b" "c" "a" 
> "b" "c" "a"
> [20] "b" "c" "a" "b" "c" "a" "b" "c" "a" "b" "c"
>  > b <- a[order(a)]
>  > b
>   [1] "a" "a" "a" "a" "a" "a" "a" "a" "a" "a" "b" "b" "b" "b" "b" "b" 
> "b" "b" "b"
> [20] "b" "c" "c" "c" "c" "c" "c" "c" "c" "c" "c"
>  > l <- rle(b)$length
>  > cbind(l,cumsum(l),cumsum(l)-l+1)
>        l
> [1,] 10 10  1
> [2,] 10 20 11
> [3,] 10 30 21
> 
> # use the line below to get the length of the block of the dataframe, 
> the start, and then end indices
>  > cbind(l,cumsum(l)-l+1,cumsum(l))
>        l
> [1,] 10  1 10
> [2,] 10 11 20
> [3,] 10 21 30
>  >
> 
> Sean
> 
> 
> On May 24, 2005, at 2:27 PM, sms13+ at pitt.edu wrote:
> 
> > I have a dataframe that contains fields such as patid, labdate, 
> > labvalue.
> > The same patid may show up in multiple rows because of lab 
> > measurements on multiple days.  Is there a simple way to 
> obtain just 
> > the first and last record for each patient, or do I need to 
> write some 
> > code that performs that.
> >
> > Thanks,
> > Steven
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ross at biostat.ucsf.edu  Tue May 24 21:30:11 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 24 May 2005 12:30:11 -0700
Subject: [R] Re: S4 method inheritance
In-Reply-To: <42930ADB.60207@stats.uwo.ca>
References: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>
	<1116893025.4615.61.camel@iron.libaux.ucsf.edu>
	<42930ADB.60207@stats.uwo.ca>
Message-ID: <20050524193011.GN3351@wheat.boylan.org>

On Tue, May 24, 2005 at 07:07:07AM -0400, Duncan Murdoch wrote:
> Ross Boylan wrote:
> >On Mon, 2005-05-23 at 14:41 -0700, Ross Boylan wrote:
> >....
> >
> >
> >>Finally, I'm a bit concerned that one article mentioned that S4
> >>inheritance, in practice, is used mostly for data, not methods (Thomas
> >>Lumley, R News 4(1), June 2004: p. 36).  Am I going down a road I
> >>shouldn't travel?
> >>
> >
> >Hmm, maybe I just found out.  If B is an S4 subclass of A (aka extends
> >A), how does B's method foo invoke A's foo?
> 
> Your question doesn't make sense in S4.  In S4, classes don't have 
> methods, generics have methods.  There's no such thing as "B's method" 
> or "A's method".
Oops, I keep taking the references to "objects" too literally.  Thanks.
> 
> You might get what you want with foo(as(bObject, "A")) if bObject is an 
> instance of class B.
> 
> >The question assumes that A's foo was defined as an in place function,
> >so there's no (obvious) named object for it, i.e,
> >setMethod("A", signature(blah="numeric"), function(x) something)
There's my confusion: the first argument should be the name of the
generic, not the class.
> 
> I don't know what you mean by "in place function", but I hope my answer 
> helps anyway.

Just for clarification, "in place function" was in contrast to a
function defined elsewhere with an explicit name, e.g.,
   fforA<-function(x) something
   setMethod("foo", signature(blah="numeric"), fforA)
In that case I could just refer to fforA directly. (Trying to avoid
the S3ish f.A).

Is sounds as if the use of as() or callNextMethod() will get me what I
want.  And the docs seem clear that callNextMethod() returns control
(and a value) to the calling function.

Thanks to everyone for their help.



From blindglobe at gmail.com  Tue May 24 21:45:15 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 24 May 2005 21:45:15 +0200
Subject: [R] Is anyone working on an "S4 classes for dummies" book/paper? If
	so, please contact me off-list
Message-ID: <1abe3fa905052412455bf89b25@mail.gmail.com>

Is anyone working on an "S4 classes for dummies" book/paper/package? 
If so, please contact me off-list.

best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From efg at stowers-institute.org  Tue May 24 21:53:41 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 24 May 2005 14:53:41 -0500
Subject: [R] colors and palettes and things...
References: <2088.128.193.139.69.1116862527.squirrel@www.forestinformatics.com>
Message-ID: <d700kv$4i9$1@sea.gmane.org>

"Jeff D. Hamann" <jeff.hamann at forestinformatics.com> wrote in message
news:2088.128.193.139.69.1116862527.squirrel at www.forestinformatics.com...
> After trying to find if there was a color picker in the FAQs and the help,
> I thought I would send a post here. I was overwhelmed with all the
> wonderful color choices R has predefined (discovered after typing in
> colors()) but can't figure out what they all (by name) look like. Is there
> a color picker or some other method to display all those colors next to
> the name?

Perhaps this PDF with a "Chart of R Colors" will help:
http://research.stowers-institute.org/efg/R/Color/Chart/ColorChart.pdf

R details are on this page about color and also show how to create the above
PDF:
http://research.stowers-institute.org/efg/R/Color/Chart/index.htm

efg



From Eric.Dube at fbn.ca  Tue May 24 22:11:52 2005
From: Eric.Dube at fbn.ca (=?ISO-8859-1?Q?=22Dub=E9=2C_Eric=22?=)
Date: Tue, 24 May 2005 16:11:52 -0400
Subject: [R] Question on read.table
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0BF11556@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050524/9ae1c2ff/attachment.pl

From RossBoylan at stanfordalumni.org  Tue May 24 22:20:50 2005
From: RossBoylan at stanfordalumni.org (Ross Boylan)
Date: Tue, 24 May 2005 13:20:50 -0700
Subject: [R] Re: S4 method inheritance
In-Reply-To: <42932BDC.9090504@fhcrc.org>
References: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>
	<1116893025.4615.61.camel@iron.libaux.ucsf.edu>
	<42930ADB.60207@stats.uwo.ca> <42932BDC.9090504@fhcrc.org>
Message-ID: <20050524202050.GO3351@wheat.boylan.org>

On Tue, May 24, 2005 at 06:27:56AM -0700, Robert Gentleman wrote:
> 
> 
> Duncan Murdoch wrote:
> >Ross Boylan wrote:
> >
> >>On Mon, 2005-05-23 at 14:41 -0700, Ross Boylan wrote:
> >>....
> >>
> >>
> >>>Finally, I'm a bit concerned that one article mentioned that S4
> >>>inheritance, in practice, is used mostly for data, not methods (Thomas
> >>>Lumley, R News 4(1), June 2004: p. 36).  Am I going down a road I
> >>>shouldn't travel?
> >>>
> >>
> >>Hmm, maybe I just found out.  If B is an S4 subclass of A (aka extends
> >>A), how does B's method foo invoke A's foo?
> >
> >
> >Your question doesn't make sense in S4.  In S4, classes don't have 
> >methods, generics have methods.  There's no such thing as "B's method" 
> >or "A's method".
> >
> >You might get what you want with foo(as(bObject, "A")) if bObject is an 
> >instance of class B.
> >
> >>The question assumes that A's foo was defined as an in place function,
> >>so there's no (obvious) named object for it, i.e,
> >>setMethod("A", signature(blah="numeric"), function(x) something)
> >
> 
> In general it may be best to think of a generic function as a 
> dispatching mechanism. For S4 methods are associated with a specific 
> generic function. 
"specific" generic is a reference to the ability to define generics
 within the context of a particular package?
> A generic knows about all methods that are associated 
> with it, and about no others. 
Presumably setMethod does the association.  Is the where argument
intended to identify which generic method to pick?  The fact that
there is not a "package" argument to setMethod, as there is to
setGeneric, is a little confusing to me.

> Thus in S4, the little tiff over who owns 
> label goes away - they both do - different packages can define
> generic 
"They" is two different packages?  Or is this a reference to my
original confusion about class vs generic ownership of a method?

> functions for label, or anything else they care to, and users can write 
> methods for specific generic functions and associate them with a 
> generic.
...
>  HTH
>    Robert



From wsetzer at mindspring.com  Tue May 24 22:23:31 2005
From: wsetzer at mindspring.com (Woodrow Setzer)
Date: Tue, 24 May 2005 16:23:31 -0400 (GMT-04:00)
Subject: [R] input line length in Sweave
Message-ID: <7203557.1116966211289.JavaMail.root@wamui-swiss.atl.sa.earthlink.net>


I am having trouble in Sweave with input line lengths.  For example, I may have in my input file the chunk

<<>>=
BrainSections <-
  levels(AggData$sctn)[grep(
    "(^BRAIN)|(^WHOLEBRAIN)|(LEFT HEMISPHERE)| (HALFBRAIN)",
                            levels(AggData$sctn))]

@ 

This is translated in the tex file:

\begin{Sinput}
> BrainSections <- levels(AggData$sctn)[grep("(^BRAIN)|(^WHOLEBRAIN)|(LEFT HEMISPHERE)| (HALFBRAIN)", 
+     levels(AggData$sctn))]
\end{Sinput}

The problem is that the line produced is too long.  I read the answer to the Sweave FAQ question 13 (How can I change the line length of S input and output?) to mean that options(width) controls both input and output; however, the first code chunk in the above example contains 
<<>>=
options(width=66)

@

Is there a way to get Sweave to wrap long input lines better, or get it to use my own formatting of the input?  I realize I can edit the output tex file, but that is impractical in my application (too many).

I am using R version 2.0.1 Patched (2005-01-26) on a Linux system (so, I suppose one possible answer is that this is fixed in 2.1.0; I cannot switch right now).


Woodrow Setzer
National Center for Computational Toxicology
US Environmental Protection Agency
Research Triangle Park, NC 27711



From ligges at statistik.uni-dortmund.de  Tue May 24 22:36:38 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 24 May 2005 22:36:38 +0200
Subject: [R] Hmisc/Design and problem with cgroup
In-Reply-To: <42912737.3060202@vanderbilt.edu>
References: <r02010500-1039-74085AC4CAF211D9B5AC000A959B3D22@[10.0.1.205]>	<4290D692.4060502@statistik.uni-dortmund.de>
	<42912737.3060202@vanderbilt.edu>
Message-ID: <42939056.3070002@statistik.uni-dortmund.de>

Frank E Harrell Jr wrote:

> Uwe Ligges wrote:
> 
>> Aric Gregson wrote:
>>
>>> On 5/17/05 21:44 Frank E Harrell Jr sent the following:
>>>
>>>
>>>> Aric Gregson wrote:
>>>>
>>>>> Hello,
>>>>>
>>>>> I am trying to use the following to output a table to latex:
>>>>>
>>>>> cohortbyagesummary <- by(data.frame(age,ethnicity), cohort, summary)
>>>>> w <- latex.default(cohortbyagesummary,    caption="Five Number Age 
>>>>> Summaries by Cohort",
>>>>>    label="agesummarybycohort",    cgroup=c('hello','goodbye','hello'),
>>>>>    colheads=c("Age","Ethnicity"),
>>>>>    extracolheads=c('hello','goodbye'),    greek=TRUE,
>>>>>    ctable=TRUE)
>>>>>    I am not able to get the major column headings of cgroup to work. I
>>>>> receive the error:
>>>>>    Object cline not found
>>>>>
>>>>
>>>> See if a modified version at
>>>> http://biostat.mc.vanderbilt.edu/cgi-bin/cvsweb.cgi/Hmisc/R/latex.s
>>>> fixes your problem.
>>>
>>>
>>>
>>>
>>> Dr. Harrell,
>>>
>>> Thanks for your help. I have downloaded latex.s, but am unable to figure
>>> out where it should go. How do I ensure that it is called when I load
>>> Hmisc? Do I patch it against R/Hmisc?
>>>
>>> I have a local directory structure in ~/Library/R/library/Hmisc/:
>>>
>>> -rw-r--r--  1 user  user  594 12 May 19:20 CITATION
>>> -rw-r--r--  1 user  user  989 12 May 19:20 DESCRIPTION
>>> drwxr-xr-x  3 user  user  102 12 May 19:20 Meta/package.rds
>>> drwxr-xr-x  3 user  user  102 12 May 19:20 R/Hmisc
>>> drwxr-xr-x  3 user  user  102 12 May 19:20 libs/Hmisc.so
>>>
>>> Thanks for your patience.
>>
>>
>>
>>
>> Your directory structure is from an installed binary package. You need 
>> to replace the file in the *source* package and *install* the source 
>> package after that.
>>
>> Uwe Ligges
> 
> 
> A better approach is just to source( ) in the new version after 
> library(Hmisc) is issued.  You can put latex.s anywhere, just put that 
> path in the source( ).  A new version of Hmisc will include the fix.

Please note that this does now work generally if a NAMESPACE is involved 
(which seems not to be the case here).

Uwe


> Frank
> 
>>
>>> aric
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
>



From xli13 at uic.edu  Tue May 24 22:35:50 2005
From: xli13 at uic.edu (Xiang Li)
Date: Tue, 24 May 2005 15:35:50 -0500 (CDT)
Subject: [R] How to use fixed-width fonts such as courier in R
Message-ID: <Pine.GSO.4.58.0505241522200.23797@icarus.cc.uic.edu>

Hi,
  I am trying to use Arial bold for labelling (font.lab =2), and use
the fixed-width Courier bold for axis. I will export the plot to .ps
format. I have been trying to use font.axis = 11, but that doesn't work
when I exported the plot to a .ps file.

  I have been trying hard to read the help page of "postscript" these few
days, but didn't get the courier font yet.  Can you please help me
figure out how I should use "postscript"? Thanks!

Best

Sean



From khobson at fd9ns01.okladot.state.ok.us  Tue May 24 22:38:55 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Tue, 24 May 2005 15:38:55 -0500
Subject: [R] Missing Data Line Type?
Message-ID: <OFBDE6888E.13F52A8D-ON8625700B.0070C372-8625700B.00715558@fd9ns01.okladot.state.ok.us>





I have a general question.  Is there a setting that can be used for a
multiple line type?  The situation is that I want a solid line between x
and y points but if the y point is missing, I want a dashed line type to
the next point.  In other words, if point 1 to 2 exists, make that line
solid, otherwise, make it dashed up to the next existing x/y point.

Additionally, what plot type would you recommend for a plot with two Y
points per X?  The two points would be joined by a vertical line.  The
average of the two is the dual type line described above.

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From BENJAMIN.G.MCMURTRY at saic.com  Tue May 24 22:39:26 2005
From: BENJAMIN.G.MCMURTRY at saic.com (McMurtry, Benjamin G. )
Date: Tue, 24 May 2005 16:39:26 -0400
Subject: [R] Table Help
Message-ID: <E2F63ACD0DE93C4A99AF7BF75837161C018AB301@rec-its-exs01.mail.saic.com>

 Reply to sender  Reply to all Reply to folder  Forward Move/Copy  Delete
Read previous item  Read next item Get help information on the current
window 	
From: 	McMurtry, Benjamin G.  
To: 	'r-help-request at stat.math.ethz.ch' 
Cc: 	 
Subject: 	Table Help 
Sent: 	5/24/2005 4:33 PM 
	Importance: 	Normal 

I have a very large table that I want to add some of the certain rows.

The table is as follows:

Username1 2
Username1 4
Username2 6
Username2 10
Username3 12
Username3 10
Username3 16

Etc ....

The data is sorted by Column 1 (Ie the username) then by columm 2 (The
numbers).

I want to combine the rows so it would be as follows:

Username1 6
Username2 16
Username3 38

Is there  easy way using a feature like bind to sum all of collumn two where
column 1 is the same?

Thanks!
Ben



From Jerry.Hikel at sac.com  Tue May 24 22:41:05 2005
From: Jerry.Hikel at sac.com (Hikel, Jerry)
Date: Tue, 24 May 2005 16:41:05 -0400
Subject: [R] Plot range resizing when adding additiona lines
Message-ID: <45486D498573904491AAA0C06DF66BAA010E3ACF@MAILISCT13.saccap.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050524/653ded74/attachment.pl

From BENJAMIN.G.MCMURTRY at saic.com  Tue May 24 22:48:41 2005
From: BENJAMIN.G.MCMURTRY at saic.com (McMurtry, Benjamin G. )
Date: Tue, 24 May 2005 16:48:41 -0400
Subject: [R] Table Help
Message-ID: <E2F63ACD0DE93C4A99AF7BF75837161C018AB302@rec-its-exs01.mail.saic.com>

I have a very large table that I want to add some of the certain rows.

The table is as follows:

Username1 2
Username1 4
Username2 6
Username2 10
Username3 12
Username3 10
Username3 16

Etc ....

The data is sorted by Column 1 (Ie the username) then by columm 2 (The
numbers).  I do not know how many of each username I will have.

I want to combine the rows so it would be as follows:

Username1 6
Username2 16
Username3 38 

Is there  easy way using a feature like bind to sum all of collumn two where
column 1 is the same?

Thanks!
Ben



From p.connolly at hortresearch.co.nz  Tue May 24 22:51:13 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 25 May 2005 08:51:13 +1200
Subject: [R] lme4 package and importIntoEnv errors
In-Reply-To: <42932EB6.3050707@stat.wisc.edu>
References: <20050524024812.GU5776@hortresearch.co.nz> 
	<200505240818.51669.deepayan@stat.wisc.edu> 
	<42932EB6.3050707@stat.wisc.edu>
Message-ID: <20050524205112.GW5776@hortresearch.co.nz>

On Tue, 24-May-2005 at 08:40AM -0500, Douglas Bates wrote:

 
|> Perhaps I can add a bit more explanation.  As Deepayan mentioned, the
|> versions of the lme4 package are closely linked to the versions of the
|> Matrix package.  The R code for lmer is in the lme4 package but the
|> corresponding C code is in the Matrix package where it can gain access
|> to other C functions for sparse matrix manipulation.

Thanks for that explanation.  I was curious to know why it's so
different from other packages I've had no trouble with.

My problem did, indeed, arise from using an older version of lme4 with
a new version of Matrix.  Douglas's post explains why
lme4_0.95-8.tar.gz is so much smaller than lme4_0.8-2.tar.gz.  I feel
much less confused now.

Thank you Renaud Lancelot and Deepayan Sarkar also.



-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From juderyan at tcindex.com  Tue May 24 22:53:16 2005
From: juderyan at tcindex.com (Jude Ryan)
Date: Tue, 24 May 2005 16:53:16 -0400
Subject: [R] output of mars() in package mda;
 how do I read the pairwise interaction terms from the output?
Message-ID: <4293943C.90608@tcindex.com>

Hi,

I have run mars() with the default degree=1, and have constructed 
splines which are similar to what I would get from Salford System's 
MARS. I then use these splines as potential predictors, along with 
rpart( ) terminal nodes and other potential predictors, in logistic 
regression in SAS. The output of mars() and the splines I created are 
shown below (pardon the lengthy output):

 > names(pchresp)
 [1] "ord_flag"         "OI_lodlrsize_cnt" "DI_diff_wo_arch"  
"DI_Days_s_2l_wo"
 [5] "DI_wo_rec_count"  "DH_diff_wo_arch"  "DH_Days_s_l_wo"   
"DH_tot_loss_amt"
 [9] "DH_wo_rec_count"  "DH_Avg_loss_amt"  "DA_diff_wo_arch"  "DA_max_loss"
[13] "DA_Days_s_l_wo"   "DA_Days_s_2l_wo"  "DA_wo_rec_count"  "CZ_cen65"
[17] "CZ_cen78"         "CZ_cen86"         "CZ_cen98"         "CZ_cen99"
[21] "CZ_cen132"        "CZ_cen232"        "CZ_cen236"        "CZ_cen247"
[25] "CZ_cen272"        "CZ_cen274"        "CZ_cen287"        "CZ_cen335"
[29] "CZ_cen337"        "CZ_cen370"        "CZ_cen371"        "CZ_cen373"
[33] "CZ_cen377"        "CZ_cen419"        "CZ_cen509"        "CZ_cen569"
[37] "CZ_cen638"        "CZ_cen695"        "CZ_cen696"        "CZ_cen698"
[41] "CZ_cen700"        "CZ_cen701"        "CZ_cen702"        "DZ9_rec"
[45] "log25"            "DZ5_rec"          "DZ5_str"          "DZ1_pob"
[49] "alor"             "ihcfbalratio"     "cgsbalratio"      "cgsbalpaid"
[53] "oxlogit_new"      "nslogit_new"      "sclogit_new"      "hit"
[57] "II_Find_Income2"  "II_SESI2"
 >
 > y <- pchresp[, c(1)]
 > x <- pchresp[, -c(1)]
 >
 > m1 <- mars(x, y)
 >
 > showcuts <- function(obj)
+ {
+   tmp <- obj$cuts[obj$sel, ]
+   dimnames(tmp) <- list(NULL, dimnames(x)[[2]])
+   tmp
+ }
 >
 > showcuts(m1)
      OI_lodlrsize_cnt DI_diff_wo_arch DI_Days_s_2l_wo DI_wo_rec_count
 [1,]                0               0               0               0
 [2,]                0               0               0               0
 [3,]                0               0               0               0
 [4,]                0               0               0               1
 [5,]                0               0               0               0
 [6,]                0               0               0               0
 [7,]                0               0               0               0
 [8,]                0               0               0               0
 [9,]                0               0               0               0
[10,]                0               0               0               0
[11,]                0               0               0               0
[12,]                0               0               0               0
[13,]                0               0               0               0
[14,]                0               0               0               0
[15,]                0               0               0               0
[16,]                0               0               0               0
[17,]                0               0               0               0
[18,]                0               0               0               0
[19,]                0               0               0               0
[20,]                0               0               0               0
[21,]                0               0               0               0
[22,]                0               0               0               0
      DH_diff_wo_arch DH_Days_s_l_wo DH_tot_loss_amt DH_wo_rec_count
 [1,]               0              0               0               0
 [2,]               0              0               0               0
 [3,]               0              0               0               0
 [4,]               0              0               0               0
 [5,]               0              0               0               0
 [6,]               0              0               0               0
 [7,]               0              0               0               0
 [8,]               0              0               0               0
 [9,]               0              0               0               0
[10,]               0              0               0               0
[11,]               0              0               0               0
[12,]               0              0               0               0
[13,]               0              0               0               0
[14,]               0              0               0               0
[15,]               0              0               0               0
[16,]               0              0               0               0
[17,]               0              0               0               0
[18,]               0              0               0               0
[19,]               0              0               0               0
[20,]               0              0               0               0
[21,]            1917              0               0               0
[22,]               0           1735               0               0
      DH_Avg_loss_amt DA_diff_wo_arch DA_max_loss DA_Days_s_l_wo
 [1,]               0               0           0              0
 [2,]               0               0           0              0
 [3,]               0               0           0              0
 [4,]               0               0           0              0
 [5,]               0               0           0              0
 [6,]               0               0           0              0
 [7,]               0               0           0              0
 [8,]               0               0           0              0
 [9,]               0               0           0              0
[10,]               0               0           0              0
[11,]               0               0           0              0
[12,]               0               0           0              0
[13,]               0               0           0              0
[14,]               0               0           0              0
[15,]               0               0           0              0
[16,]               0               0           0              0
[17,]               0               0           0              0
[18,]               0               0           0              0
[19,]               0               0           0              0
[20,]               0               0           0              0
[21,]               0               0           0              0
[22,]               0               0           0              0
      DA_Days_s_2l_wo DA_wo_rec_count CZ_cen65 CZ_cen78 CZ_cen86 CZ_cen98
 [1,]               0               0        0        0        0        0
 [2,]               0               0        0        0        0        0
 [3,]               0               0        0        0        0        0
 [4,]               0               0        0        0        0        0
 [5,]               0               0        0        0        0        0
 [6,]               0               0        0        0        0        0
 [7,]               0               0        0        0        0        0
 [8,]               0               0        0        0        0        0
 [9,]               0               0        0        0        0        0
[10,]               0               0        0        0        0        0
[11,]               0               0        0        0        0        0
[12,]               0               0        0        0        0        0
[13,]               0               0        0        0        0        0
[14,]               0               0        0        0        0        0
[15,]               0               0        0        0        0        0
[16,]               0               0        0        0        0        0
[17,]               0               0        0        0        0        0
[18,]               0               0        0        0        0        0
[19,]               0               0        0        0        0        0
[20,]               0               0        0        0        0        0
[21,]               0               0        0        0        0        0
[22,]               0               0        0        0        0        0
      CZ_cen99 CZ_cen132 CZ_cen232 CZ_cen236 CZ_cen247 CZ_cen272 CZ_cen274
 [1,]        0         0         0         0 0.0000000         0         0
 [2,]        0         0         0         0 0.0000000         0         0
 [3,]        0         0         0         0 0.0000000         0         0
 [4,]        0         0         0         0 0.0000000         0         0
 [5,]        0         0         0         0 0.0000000         0         0
 [6,]        0         0         0         0 0.0000000         0         0
 [7,]        0         0         0         0 0.0000000         0         0
 [8,]        0         0         0         0 0.0000000         0         0
 [9,]        0         0         0         0 0.0000000         0         0
[10,]        0         0         0         0 0.0000000         0         0
[11,]        0         0         0         0 0.0000000         0         0
[12,]        0         0         0         0 0.0000000         0         0
[13,]        0         0         0         0 0.0000000         0         0
[14,]        0         0         0         0 0.9069767         0         0
[15,]        0         0         0         0 0.9069767         0         0
[16,]        0         0         0         0 0.0000000         0         0
[17,]        0         0         0         0 0.0000000         0         0
[18,]        0         0         0         0 0.0000000         0         0
[19,]        0         0         0         0 0.0000000         0         0
[20,]        0         0         0         0 0.0000000         0         0
[21,]        0         0         0         0 0.0000000         0         0
[22,]        0         0         0         0 0.0000000         0         0
      CZ_cen287 CZ_cen335 CZ_cen337 CZ_cen370 CZ_cen371 CZ_cen373 CZ_cen377
 [1,]         0         0         0         0         0         0         0
 [2,]         0         0         0         0         0         0         0
 [3,]         0         0         0         0         0         0         0
 [4,]         0         0         0         0         0         0         0
 [5,]         0         0         0         0         0         0         0
 [6,]         0         0         0         0         0         0         0
 [7,]         0         0         0         0         0         0         0
 [8,]         0         0         0         0         0         0         0
 [9,]         0         0         0         0         0         0         0
[10,]         0         0         0         0         0         0         0
[11,]         0         0         0         0         0         0         0
[12,]         0         0         0         0         0         0         0
[13,]     32857         0         0         0         0         0         0
[14,]         0         0         0         0         0         0         0
[15,]         0         0         0         0         0         0         0
[16,]         0         0         0         0         0         0         0
[17,]         0         0         0         0         0         0         0
[18,]         0         0         0         0         0         0         0
[19,]         0         0         0         0         0         0         0
[20,]         0         0         0         0         0         0         0
[21,]         0         0         0         0         0         0         0
[22,]         0         0         0         0         0         0         0
      CZ_cen419 CZ_cen509 CZ_cen569 CZ_cen638 CZ_cen695 CZ_cen696 CZ_cen698
 [1,]         0         0   0.00000         0         0         0         0
 [2,]         0         0   0.00000         0         0         0         0
 [3,]         0         0   0.00000         0         0         0         0
 [4,]         0         0   0.00000         0         0         0         0
 [5,]         0         0   0.00000         0         0         0         0
 [6,]         0         0   0.00000         0         0         0         0
 [7,]         0         0   0.00000         0         0         0         0
 [8,]         0         0   0.00000         0         0         0         0
 [9,]         0         0   0.00000         0         0         0         0
[10,]         0         0   0.00000         0         0         0         0
[11,]         0         0   0.00000         0         0         0         0
[12,]         0         0   0.00000         0         0         0         0
[13,]         0         0   0.00000         0         0         0         0
[14,]         0         0   0.00000         0         0         0         0
[15,]         0         0   0.00000         0         0         0         0
[16,]         0         0  11.07613         0         0         0         0
[17,]         0         0   0.00000         0         0         0         0
[18,]         0         0   0.00000         0         0         0        30
[19,]         0         0   0.00000         0         0         0        30
[20,]         0         0   0.00000         0         0         0         0
[21,]         0         0   0.00000         0         0         0         0
[22,]         0         0   0.00000         0         0         0         0
      CZ_cen700 CZ_cen701 CZ_cen702 DZ9_rec log25 DZ5_rec DZ5_str 
DZ1_pob alor
 [1,]         0         0         0       0     0       0       0       
0    0
 [2,]         0         0         0       0     0       0       0       
0    0
 [3,]         0         0         0       0     0       0       0       
0    0
 [4,]         0         0         0       0     0       0       0       
0    0
 [5,]         0         0         0       0     0       0       0       
0    0
 [6,]         0         0         0       0     0       0       0       
0    0
 [7,]         0         0         0       0     0       0       0       
0   32
 [8,]         0         0         0       0     0       0       0       
0    0
 [9,]         0         0         0       0     0       0       0       
0    0
[10,]         0         0         0       0     0       0       0       
0    0
[11,]         0         0        38       0     0       0       0       
0    0
[12,]         0         0        38       0     0       0       0       
0    0
[13,]         0         0         0       0     0       0       0       
0    0
[14,]         0         0         0       0     0       0       0       
0    0
[15,]         0         0         0       0     0       0       0       
0    0
[16,]         0         0         0       0     0       0       0       
0    0
[17,]         0         0         0       0     0       0       0       
0    0
[18,]         0         0         0       0     0       0       0       
0    0
[19,]         0         0         0       0     0       0       0       
0    0
[20,]         0         0         0       0     0       0       0       
0    0
[21,]         0         0         0       0     0       0       0       
0    0
[22,]         0         0         0       0     0       0       0       
0    0
      ihcfbalratio cgsbalratio cgsbalpaid oxlogit_new nslogit_new 
sclogit_new
 [1,]            0     0.00000          0           0     
0.00000           0
 [2,]            0     0.00000          0           0     
1.28535           0
 [3,]            0     0.00000          0           0     
1.28535           0
 [4,]            0     0.00000          0           0     
0.00000           0
 [5,]            0     0.00000          0           0     
0.00000           0
 [6,]            0    16.03509          0           0     
0.00000           0
 [7,]            0     0.00000          0           0     
0.00000           0
 [8,]            0     0.00000          0           0     
0.00000           0
 [9,]            0     0.00000          0           0     
0.00000           0
[10,]            0     0.00000          0           0     
0.00000           0
[11,]            0     0.00000          0           0     
0.00000           0
[12,]            0     0.00000          0           0     
0.00000           0
[13,]            0     0.00000          0           0     
0.00000           0
[14,]            0     0.00000          0           0     
0.00000           0
[15,]            0     0.00000          0           0     
0.00000           0
[16,]            0     0.00000          0           0     
0.00000           0
[17,]            0     0.00000          0           0     
0.00000           0
[18,]            0     0.00000          0           0     
0.00000           0
[19,]            0     0.00000          0           0     
0.00000           0
[20,]            0     0.00000          0           0     
0.00000           0
[21,]            0     0.00000          0           0     
0.00000           0
[22,]            0     0.00000          0           0     
0.00000           0
      hit II_Find_Income2 II_SESI2
 [1,]   0               0        0
 [2,]   0               0        0
 [3,]   0               0        0
 [4,]   0               0        0
 [5,]   0               0        0
 [6,]   0               0        0
 [7,]   0               0        0
 [8,]   0               0        0
 [9,]   0           26000        0
[10,]   0           26000        0
[11,]   0               0        0
[12,]   0               0        0
[13,]   0               0        0
[14,]   0               0        0
[15,]   0               0        0
[16,]   0               0        0
[17,]   0           71000        0
[18,]   0               0        0
[19,]   0               0        0
[20,]   0               0       27
[21,]   0               0        0
[22,]   0               0        0
 >

The mars splines I created are:

DH_diff_wo_arch_sp1 = max(0, DH_diff_wo_arch - 1917);
DH_Days_s_l_wo_sp1  = max(0, DH_Days_s_l_wo - 1735);
CZ_cen247_sp1       = max(0, CZ_cen247 - 0.9069767);
CZ_cen287_sp1       = max(0, CZ_cen287 - 32857);
CZ_cen569_sp1       = max(0, CZ_cen569 - 11.07613);
CZ_cen698_sp1       = max(0, CZ_cen698 - 30);
CZ_cen702_sp1       = max(0, CZ_cen702 - 38);
alor_sp1            = max(0, alor - 32);
cgsbalratio_sp1     = max(0, cgsbalratio - 16.03509);
nslogit_new_sp1     = max(0, nslogit_new - 1.28535);
II_Find_Income2_sp1 = max(0, II_Find_Income2 - 26000);
II_Find_Income2_sp2 = max(0, II_Find_Income2 - 71000);
II_SESI2_sp1        = max(0, II_SESI2 - 27);

My next run of mars( ) uses degree=2.

 > m2 <- mars(x, y, degree=2)
 >
 > showcuts(m2)
      OI_lodlrsize_cnt DI_diff_wo_arch DI_Days_s_2l_wo DI_wo_rec_count
 [1,]                0               0               0               0
 [2,]                0               0               0               1
 [3,]                0               0               0               1
 [4,]                0               0               0               1
 [5,]                0               0               0               0
 [6,]                0               0               0               1
 [7,]                0               0               0               1
 [8,]                0               0               0               1
 [9,]                0               0               0               0
[10,]                0               0               0               0
[11,]                0               0               0               0
[12,]                0               0               0               0
[13,]                0               0               0               0
[14,]                0               0               0               0
[15,]                0               0               0               0
[16,]                0               0               0               0
[17,]                0               0               0               0
[18,]                0               0               0               0
[19,]                0               0               0               0
[20,]                0               0               0               0
[21,]                0               0               0               0
[22,]                0               0               0               0
[23,]                0               0               0               1
[24,]                0               0               0               0
[25,]                0               0               0               0
[26,]                0               0               0               0
[27,]                0               0               0               0
[28,]                0               0               0               0
[29,]                0               0               0               0
[30,]                0               0               0               0
[31,]                0               0               0               0
[32,]                0               0               0               0
[33,]                0               0               0               0
[34,]                0               0               0               0
      DH_diff_wo_arch DH_Days_s_l_wo DH_tot_loss_amt DH_wo_rec_count
 [1,]               0              0               0               0
 [2,]               0              0               0               0
 [3,]               0              0               0               0
 [4,]               0              0               0               0
 [5,]               0              0               0               0
 [6,]               0              0               0               0
 [7,]               0              0               0               0
 [8,]               0              0               0               0
 [9,]               0              0               0               0
[10,]               0              0               0               0
[11,]               0              0               0               0
[12,]               0              0               0               0
[13,]               0              0               0               0
[14,]               0              0               0               0
[15,]               0              0               0               0
[16,]               0              0               0               0
[17,]               0              0               0               0
[18,]               0              0               0               0
[19,]               0              0               0               0
[20,]               0              0               0               0
[21,]               0              0               0               0
[22,]            1917              0               0               0
[23,]               0              0               0               0
[24,]               0              0               0               0
[25,]               0              0               0               0
[26,]               0              0               0               0
[27,]               0              0               0               0
[28,]            1917              0               0               0
[29,]               0              0               0               0
[30,]               0              0               0               0
[31,]               0              0               0               0
[32,]               0              0               0               0
[33,]               0              0               0               0
[34,]               0              0               0               0
      DH_Avg_loss_amt DA_diff_wo_arch DA_max_loss DA_Days_s_l_wo
 [1,]               0               0           0              0
 [2,]               0               0           0              0
 [3,]               0               0           0              0
 [4,]               0               0           0              0
 [5,]               0               0           0              0
 [6,]               0               0           0              0
 [7,]               0               0           0              0
 [8,]               0               0           0              0
 [9,]               0               0           0              0
[10,]               0               0           0              0
[11,]               0               0           0              0
[12,]               0               0           0              0
[13,]               0               0           0              0
[14,]               0               0           0              0
[15,]               0               0           0              0
[16,]               0               0           0              0
[17,]               0               0           0              0
[18,]               0               0           0              0
[19,]               0               0           0              0
[20,]               0               0           0              0
[21,]               0               0           0           1856
[22,]               0               0           0              0
[23,]               0             943           0              0
[24,]               0               0           0              0
[25,]               0               0           0              0
[26,]               0               0           0              0
[27,]               0               0           0              0
[28,]               0               0           0              0
[29,]               0               0           0              0
[30,]               0               0           0              0
[31,]               0               0           0              0
[32,]               0               0           0              0
[33,]               0               0           0              0
[34,]               0               0           0              0
      DA_Days_s_2l_wo DA_wo_rec_count CZ_cen65  CZ_cen78 CZ_cen86 CZ_cen98
 [1,]               0               0        0 0.0000000        0        0
 [2,]               0               0        0 0.0000000        0        0
 [3,]               0               0        0 0.0000000        0        0
 [4,]               0               0        0 0.0000000        0        0
 [5,]               0               0        0 0.0000000        0        0
 [6,]               0               0        0 0.0000000        0        0
 [7,]               0               0        0 0.0000000        0        0
 [8,]               0               0        0 0.0000000        0        0
 [9,]               0               0        0 0.0000000        0        0
[10,]               0               0        0 0.0000000        0        0
[11,]               0               0        0 0.0000000        0        0
[12,]               0               0        0 0.0000000        0        0
[13,]               0               0        0 0.0000000        0        0
[14,]               0               0        0 0.0000000        0        0
[15,]               0               0        0 0.0000000        0        0
[16,]               0               0        0 0.0000000        0        0
[17,]               0               0        0 0.0000000        0        0
[18,]               0               0        0 0.0000000        0        0
[19,]               0               0        0 0.4242424        0        0
[20,]               0               0        0 0.0000000        0        0
[21,]               0               0        0 0.0000000        0        0
[22,]               0               0        0 0.0000000        0        0
[23,]               0               0        0 0.0000000        0        0
[24,]               0               0        0 0.0000000        0        0
[25,]               0               0        0 0.0000000        0        0
[26,]               0               0        0 0.0000000        0        0
[27,]               0               0        0 0.0000000        0        0
[28,]               0               0        0 0.0000000        0        0
[29,]               0               0        0 0.0000000        0        0
[30,]               0               0        0 0.0000000        0        0
[31,]               0               0        0 0.0000000        0        0
[32,]               0               0        0 0.0000000        0        0
[33,]               0               0        0 0.0000000        0        0
[34,]               0               0        0 0.0000000        0        0
      CZ_cen99 CZ_cen132 CZ_cen232 CZ_cen236 CZ_cen247 CZ_cen272 CZ_cen274
 [1,]        0 0.0000000         0 0.0000000 0.0000000         0         0
 [2,]        0 0.0000000         0 0.0000000 0.0000000         0         0
 [3,]        0 0.0000000         0 0.0000000 0.0000000         0         0
 [4,]        0 0.0000000         0 0.0000000 0.0000000         0         0
 [5,]        0 0.0000000         0 0.0000000 0.0000000         0         0
 [6,]        0 0.0000000         0 0.0000000 0.0000000         0         0
 [7,]        0 0.0000000         0 0.0000000 0.0000000         0         0
 [8,]        0 0.0000000         0 0.0000000 0.0000000         0         0
 [9,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[10,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[11,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[12,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[13,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[14,]        0 0.0000000         0 0.0000000 0.9074286         0         0
[15,]        0 0.0000000         0 0.0000000 0.9074286         0         0
[16,]        0 0.0000000         0 0.0000000 0.9074286         0         0
[17,]        0 0.0000000         0 0.0000000 0.9074286         0         0
[18,]        0 0.0000000         0 0.0000000 0.9074286         0         0
[19,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[20,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[21,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[22,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[23,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[24,]        0 0.4729299         0 0.0000000 0.0000000         0         0
[25,]        0 0.4729299         0 0.0000000 0.0000000         0         0
[26,]        0 0.0000000         0 0.0000000 0.9074286         0         0
[27,]        0 0.0000000         0 0.4290323 0.0000000         0         0
[28,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[29,]        0 0.0000000         0 0.0000000 0.9074286         0         0
[30,]        0 0.0000000         0 0.0000000 0.9074286         0         0
[31,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[32,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[33,]        0 0.0000000         0 0.0000000 0.0000000         0         0
[34,]        0 0.0000000         0 0.0000000 0.0000000         0         0
      CZ_cen287 CZ_cen335 CZ_cen337 CZ_cen370 CZ_cen371 CZ_cen373 CZ_cen377
 [1,]         0         0         0         0         0 0.0000000         0
 [2,]         0         0         0         0         0 0.0000000         0
 [3,]         0         0         0         0         0 0.0000000         0
 [4,]         0         0         0         0         0 0.0000000         0
 [5,]         0         0         0         0         0 0.0000000         0
 [6,]         0         0         0         0         0 0.0000000         0
 [7,]         0         0         0         0         0 0.0000000         0
 [8,]         0         0         0         0         0 0.0000000         0
 [9,]         0         0         0         0         0 0.0000000         0
[10,]     36324         0         0         0         0 0.0000000         0
[11,]     36324         0         0         0         0 0.0000000         0
[12,]     36324         0         0         0         0 0.0000000         0
[13,]     36324         0         0         0         0 0.0000000         0
[14,]         0         0         0         0         0 0.0000000         0
[15,]         0         0         0         0         0 0.0000000         0
[16,]         0         0         0         0         0 0.0000000         0
[17,]         0         0         0         0         0 0.0000000         0
[18,]         0         0         0         0         0 0.0000000         0
[19,]         0         0         0         0         0 0.0000000         0
[20,]         0         0         0         0         0 0.0000000         0
[21,]         0         0         0         0         0 0.0000000         0
[22,]         0         0         0         0         0 0.0000000         0
[23,]         0         0         0         0         0 0.0000000         0
[24,]         0         0         0         0         0 0.0000000         0
[25,]         0         0         0         0         0 0.0000000         0
[26,]         0         0         0         0         0 0.0000000         0
[27,]         0         0         0         0         0 0.0000000         0
[28,]         0         0         0         0         0 0.0000000         0
[29,]         0         0         0         0         0 0.0000000         0
[30,]     14306         0         0         0         0 0.0000000         0
[31,]         0         0         0         0         0 0.0000000         0
[32,]         0         0         0         0         0 0.0000000         0
[33,]         0         0         0         0         0 0.0000000         0
[34,]         0         0         0         0         0 0.2213115         0
      CZ_cen419 CZ_cen509 CZ_cen569 CZ_cen638 CZ_cen695 CZ_cen696 CZ_cen698
 [1,]         0         0   0.00000         0         0         0         0
 [2,]         0         0   0.00000         0         0         0         0
 [3,]         0         0   0.00000         0         0         0         0
 [4,]         0         0   0.00000         0         0         0         0
 [5,]         0         0   0.00000         0         0         0         0
 [6,]         0         0   0.00000         0         0         0         0
 [7,]         0         0   0.00000         0         0         0         0
 [8,]         0         0   0.00000         0         0         0         0
 [9,]         0         0   0.00000         0         0         0         0
[10,]         0         0   0.00000         0         0         0         0
[11,]         0         0   0.00000         0         0         0         0
[12,]         0         0   0.00000         0         0         0        31
[13,]         0         0   0.00000         0         0         0        31
[14,]         0         0   0.00000         0         0         0         0
[15,]         0         0  12.92770         0         0         0         0
[16,]         0         0   0.00000         0         0         0         0
[17,]         0         0   0.00000         0         0         0         0
[18,]         0         0   0.00000         0         0         0         0
[19,]         0         0   0.00000         0         0         0         0
[20,]         0         0   0.00000         0         0         0         0
[21,]         0         0   0.00000         0         0         0         0
[22,]         0         0   0.00000         0         0         0         0
[23,]         0         0   0.00000         0         0         0         0
[24,]         0         0   0.00000         0         0         0         0
[25,]         0         0   0.00000         0         0         0         0
[26,]         0         0   0.00000         0         0         0         0
[27,]         0         0   0.00000         0         0         0         0
[28,]         0         0   0.00000         0         0         0         0
[29,]         0         0   0.00000         0         0         0         0
[30,]         0         0   0.00000         0         0         0         0
[31,]         0         0   0.00000         0         0         0         0
[32,]         0         0   0.00000         0         0         0         0
[33,]         0         0   0.00000         0         0         0         0
[34,]         0         0   0.00000         0         0         0         0
      CZ_cen700 CZ_cen701 CZ_cen702 DZ9_rec   log25 DZ5_rec DZ5_str DZ1_pob
 [1,]         0         0         0       0 0.00000       0       0       0
 [2,]         0         0         0       0 0.00000       0       0       0
 [3,]         0         0         0       0 0.00000       0       0       0
 [4,]         0         0         0       0 0.00000       0       0       0
 [5,]         0         0         0       0 0.00000       0       0       0
 [6,]         0         0         0       0 0.00000       0       0       0
 [7,]         0         0         0       0 0.00000       0       0       0
 [8,]         0         0         0       0 0.00000       0       0       0
 [9,]         0         0         0       0 2.17021       0       0       0
[10,]         0         0         0       0 0.00000       0       0       0
[11,]         0         0         0       0 0.00000       0       0       0
[12,]         0         0         0       0 0.00000       0       0       0
[13,]         0         0         0       0 0.00000       0       0       0
[14,]         0         0         0       0 0.00000       0       0       0
[15,]         0         0         0       0 0.00000       0       0       0
[16,]        93         0         0       0 0.00000       0       0       0
[17,]        93         0         0       0 0.00000       0       0       0
[18,]         0         0         0       0 0.00000       0       0       0
[19,]         0         0         0       0 0.00000       0       0       0
[20,]         0         0         0       0 0.00000       0       0       0
[21,]         0         0         0       0 0.00000       0       0       0
[22,]         0         0         0       0 0.00000       0       0       0
[23,]         0         0         0       0 0.00000       0       0       0
[24,]         0         0         0       0 0.00000       0       0       0
[25,]         0         0         0       0 0.00000       0       0       0
[26,]         0         0         0       0 0.00000       0       0       0
[27,]         0         0         0       0 0.00000       0       0       0
[28,]         0         0        38       0 0.00000       0       0       0
[29,]         0         0         0       0 0.00000    8405       0       0
[30,]         0         0         0       0 0.00000       0       0       0
[31,]         0         0         0       0 2.45773       0       0       0
[32,]         0         0         0       0 0.00000       0       0       0
[33,]         0         0         0       0 0.00000       0       0       0
[34,]         0         0         0       0 0.00000       0       0       0
      alor ihcfbalratio cgsbalratio cgsbalpaid oxlogit_new nslogit_new
 [1,]    0            0     0.00000          0           0     0.00000
 [2,]    0            0     0.00000          0           0     0.00000
 [3,]    0            0     0.00000          0           0     0.00000
 [4,]    0            0     0.00000          0           0     0.00000
 [5,]    0            0    16.03509          0           0     0.00000
 [6,]   32            0     0.00000          0           0     0.00000
 [7,]    0            0     0.00000          0           0     0.00000
 [8,]    0            0     0.00000          0           0     0.00000
 [9,]    0            0    16.03509          0           0     0.00000
[10,]    0            0     0.00000          0           0     0.00000
[11,]    0            0     0.00000          0           0     0.00000
[12,]    0            0     0.00000          0           0     0.00000
[13,]    0            0     0.00000          0           0     0.00000
[14,]    0            0     0.00000          0           0     0.00000
[15,]    0            0     0.00000          0           0     0.00000
[16,]    0            0     0.00000          0           0     0.00000
[17,]    0            0     0.00000          0           0     0.00000
[18,]   18            0     0.00000          0           0     0.00000
[19,]    0            0    16.03509          0           0     0.00000
[20,]    0            0     0.00000          0           0     1.28535
[21,]    0            0     0.00000          0           0     1.28535
[22,]    0            0     0.00000          0           0     0.00000
[23,]    0            0     0.00000          0           0     0.00000
[24,]    0            0     0.00000          0           0     1.28535
[25,]    0            0     0.00000          0           0     1.28535
[26,]    0            0     0.00000          0           0     0.00000
[27,]    0            0     0.00000          0           0     1.28535
[28,]    0            0     0.00000          0           0     0.00000
[29,]    0            0     0.00000          0           0     0.00000
[30,]    0            0     0.00000          0           0     0.00000
[31,]    0            0     0.00000          0           0     1.28535
[32,]    0            0    16.03509          0           0     3.01888
[33,]    0            0     0.00000          0           0     1.28535
[34,]    0            0    16.03509          0           0     0.00000
      sclogit_new hit II_Find_Income2 II_SESI2
 [1,]           0   0               0        0
 [2,]           0   0               0        0
 [3,]           0   0               0        0
 [4,]           0   0               0        0
 [5,]           0   0               0        0
 [6,]           0   0               0        0
 [7,]           0   0               0       16
 [8,]           0   0               0        0
 [9,]           0   0               0        0
[10,]           0   0               0        0
[11,]           0   0               0        0
[12,]           0   0               0        0
[13,]           0   0               0        0
[14,]           0   0               0        0
[15,]           0   0               0        0
[16,]           0   0               0        0
[17,]           0   0               0        0
[18,]           0   0               0        0
[19,]           0   0               0        0
[20,]           0   0               0       27
[21,]           0   0               0        0
[22,]           0   0               0        0
[23,]           0   0               0        0
[24,]           0   0               0        0
[25,]           0   0               0        0
[26,]           0   0          122000        0
[27,]           0   0               0        0
[28,]           0   0               0        0
[29,]           0   0               0        0
[30,]           0   0               0        0
[31,]           0   0               0        0
[32,]           0   0               0        0
[33,]           0   0          225000        0
[34,]           0   0               0        0
 >

While I can construct splines in a similar manner to what I did for my 
first output, I am unable to figure out how to construct mars splines 
for the pairwise interactions I am supposed to be getting for degree=2.

The mars documentation and Venables' and Ripley's Modern Applied 
Statistics with S (pages 235 to 237) do show similar output, but not how 
to construct splines or figure out the pairwise interactions.

Any help is greatly appreciated.

Thanks in advance.

Jude Ryan



From sdavis2 at mail.nih.gov  Tue May 24 22:56:22 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 24 May 2005 16:56:22 -0400
Subject: [R] Table Help
In-Reply-To: <E2F63ACD0DE93C4A99AF7BF75837161C018AB301@rec-its-exs01.mail.saic.com>
References: <E2F63ACD0DE93C4A99AF7BF75837161C018AB301@rec-its-exs01.mail.saic.com>
Message-ID: <84d91e2464733dca5bfd069d96cb735a@mail.nih.gov>

see ?aggregate....

Sean

On May 24, 2005, at 4:39 PM, McMurtry, Benjamin G. wrote:
>
>
> Is there  easy way using a feature like bind to sum all of collumn two 
> where
> column 1 is the same?



From xli13 at uic.edu  Tue May 24 22:58:12 2005
From: xli13 at uic.edu (Xiang Li)
Date: Tue, 24 May 2005 15:58:12 -0500 (CDT)
Subject: [R] in other words Re: How to use fixed-width fonts such as courier
 in R
In-Reply-To: <Pine.GSO.4.58.0505241522200.23797@icarus.cc.uic.edu>
References: <Pine.GSO.4.58.0505241522200.23797@icarus.cc.uic.edu>
Message-ID: <Pine.GSO.4.58.0505241552470.23797@icarus.cc.uic.edu>

the problem is how to use both courier bold font for axis and arial
bold font for labelling.  If I use familiy = "courier" in postscript,
everything will become courier fonts ......

I noticed that in postscirpt help page, we can customize a family such as
"family = c("CM_regular_10.afm","CM_boldx_10.afm", "cmti10.afm",
"cmbxti10.afm", "CM_symbol_10.afm";

However, I tried many times, and just don't know the symbol used in R for
courier bold or arial bold, etc.



> Hi,
>   I am trying to use Arial bold for labelling (font.lab =2), and use
> the fixed-width Courier bold for axis. I will export the plot to .ps
> format. I have been trying to use font.axis = 11, but that doesn't work
> when I exported the plot to a .ps file.
>
>   I have been trying hard to read the help page of "postscript" these few
> days, but didn't get the courier font yet.  Can you please help me
> figure out how I should use "postscript"? Thanks!
>
> Best
>
> Sean
>



From matthew_wiener at merck.com  Tue May 24 22:57:41 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Tue, 24 May 2005 16:57:41 -0400
Subject: [R] Plot range resizing when adding additiona lines
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E049945F7@uswsmx03.merck.com>

In traditional, or "base" graphics, see "matplot", which does exactly what
you describe.

  You can also look at lattice graphics, which will give you flexibility to
plot in a single panel or multiple panels.

Hope this helps,
Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hikel, Jerry
Sent: Tuesday, May 24, 2005 4:41 PM
To: R-help at stat.math.ethz.ch
Subject: [R] Plot range resizing when adding additiona lines


Hi -- I have searched the documentation and archives on graphing
capabilities in R for the past couple of hours, but I haven't been able
to find anything directly related to my problem.

I want to create a plot with several lines displayed on it. I want each
line to be displayed in a different color, so it seems the only way to
do this is to create a plot and then use the lines() function. However,
once i create the original plot, when I add new lines to the plot, the
axes range stays the same, and the new lines often extend beyond the
range of the original axes, cutting off the data of the added lines.

Is there any way to force the plot to resize it's axes range when new
lines are added, or is there some other optional way of implementing
this? Thanks.


DISCLAIMER: This e-mail message and any attachments are inte...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From slist at oomvanlieshout.net  Tue May 24 23:03:31 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 24 May 2005 23:03:31 +0200
Subject: [R] Plot range resizing when adding additiona lines
In-Reply-To: <45486D498573904491AAA0C06DF66BAA010E3ACF@MAILISCT13.saccap.int>
References: <45486D498573904491AAA0C06DF66BAA010E3ACF@MAILISCT13.saccap.int>
Message-ID: <429396A3.1000903@oomvanlieshout.net>

Hikel, Jerry wrote:
> Hi -- I have searched the documentation and archives on graphing
> capabilities in R for the past couple of hours, but I haven't been able
> to find anything directly related to my problem.
> 
> I want to create a plot with several lines displayed on it. I want each
> line to be displayed in a different color, so it seems the only way to
> do this is to create a plot and then use the lines() function. However,
> once i create the original plot, when I add new lines to the plot, the
> axes range stays the same, and the new lines often extend beyond the
> range of the original axes, cutting off the data of the added lines.
> 
> Is there any way to force the plot to resize it's axes range when new
> lines are added, or is there some other optional way of implementing
> this? Thanks.
> 
> 
> DISCLAIMER: This e-mail message and any attachments are inte...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

You can check the range of all items to plot and take the minimum and 
maximum value across all items. Then draw the plot with the axis range 
explicitely set to 'minimum'-'maximum'!

That is assuming you know the items you are drawing before hand.

Sander.


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From Jerry.Hikel at sac.com  Tue May 24 23:06:48 2005
From: Jerry.Hikel at sac.com (Hikel, Jerry)
Date: Tue, 24 May 2005 17:06:48 -0400
Subject: [R] Plot range resizing when adding additiona lines
Message-ID: <45486D498573904491AAA0C06DF66BAA010E3AD0@MAILISCT13.saccap.int>

That's what i wound up doing -- i am totally new to R, so i wasn't sure
if there were an easy way of getting the min and max values of a data
frame, but i tried that and it worked out well. Thanks. 

-----Original Message-----
From: Sander Oom [mailto:slist at oomvanlieshout.net] 
Sent: Tuesday, May 24, 2005 5:04 PM
To: Hikel, Jerry
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Plot range resizing when adding additiona lines

Hikel, Jerry wrote:
> Hi -- I have searched the documentation and archives on graphing 
> capabilities in R for the past couple of hours, but I haven't been 
> able to find anything directly related to my problem.
> 
> I want to create a plot with several lines displayed on it. I want 
> each line to be displayed in a different color, so it seems the only 
> way to do this is to create a plot and then use the lines() function. 
> However, once i create the original plot, when I add new lines to the 
> plot, the axes range stays the same, and the new lines often extend 
> beyond the range of the original axes, cutting off the data of the
added lines.
> 
> Is there any way to force the plot to resize it's axes range when new 
> lines are added, or is there some other optional way of implementing 
> this? Thanks.
> 
> 
> DISCLAIMER: This e-mail message and any attachments are 
> inte...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

You can check the range of all items to plot and take the minimum and
maximum value across all items. Then draw the plot with the axis range
explicitely set to 'minimum'-'maximum'!

That is assuming you know the items you are drawing before hand.

Sander.


--
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences, University of the
Witwatersrand Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander
---------------------------------------------


DISCLAIMER: This e-mail message and any attachments are inte...{{dropped}}



From OlsenN at pac.dfo-mpo.gc.ca  Tue May 24 23:18:48 2005
From: OlsenN at pac.dfo-mpo.gc.ca (OlsenN@pac.dfo-mpo.gc.ca)
Date: Tue, 24 May 2005 14:18:48 -0700
Subject: [R] Missing Data Line Type?
Message-ID: <7CBBD627E4E688499349A5D11D07831602ECB944@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>

The dashed line can be added to the plot with a call to "lines" after
removing the NAs from your x/y vectors (I'm assuming 'missing' means NA).

> plot(x,y,type='l')
> lines(x[!is.na(y)],y[!is.na(y)],lty=2)

Vertical lines can be accomplished with "segments".

norm

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
khobson at fd9ns01.okladot.state.ok.us
Sent: Tuesday, May 24, 2005 1:39 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Missing Data Line Type?





I have a general question.  Is there a setting that can be used for a
multiple line type?  The situation is that I want a solid line between x and
y points but if the y point is missing, I want a dashed line type to the
next point.  In other words, if point 1 to 2 exists, make that line solid,
otherwise, make it dashed up to the next existing x/y point.

Additionally, what plot type would you recommend for a plot with two Y
points per X?  The two points would be joined by a vertical line.  The
average of the two is the dual type line described above.

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Tue May 24 23:20:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 May 2005 17:20:41 -0400
Subject: [R] Missing Data Line Type?
In-Reply-To: <OFBDE6888E.13F52A8D-ON8625700B.0070C372-8625700B.00715558@fd9ns01.okladot.state.ok.us>
References: <OFBDE6888E.13F52A8D-ON8625700B.0070C372-8625700B.00715558@fd9ns01.okladot.state.ok.us>
Message-ID: <971536df05052414201f006b5f@mail.gmail.com>

On 5/24/05, khobson at fd9ns01.okladot.state.ok.us
<khobson at fd9ns01.okladot.state.ok.us> wrote:
> 
> 
> 
> 
> I have a general question.  Is there a setting that can be used for a
> multiple line type?  The situation is that I want a solid line between x
> and y points but if the y point is missing, I want a dashed line type to
> the next point.  In other words, if point 1 to 2 exists, make that line
> solid, otherwise, make it dashed up to the next existing x/y point.

plot(approx(x), type = "l", lty = 2)
lines(x)

> 
> Additionally, what plot type would you recommend for a plot with two Y
> points per X?  The two points would be joined by a vertical line.  The
> average of the two is the dual type line described above.

See ?segments  .  

Also you might be able to use directly or else scavenge code from
plotCI in package gplots (in the gregmisc bundle) or plotOHLC in
package tseries.



From p.murrell at auckland.ac.nz  Tue May 24 23:29:20 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 25 May 2005 09:29:20 +1200
Subject: [R] in other words Re: How to use fixed-width fonts such as
	courier in R
References: <Pine.GSO.4.58.0505241522200.23797@icarus.cc.uic.edu>
	<Pine.GSO.4.58.0505241552470.23797@icarus.cc.uic.edu>
Message-ID: <42939CB0.5000804@stat.auckland.ac.nz>

Hi

Does this do what you want?

postscript("example.ps", fonts=c("sans", "mono"))
plot(1:10, axes=FALSE, ann=FALSE)
par(family="sans") # By default Helvetica which is like Arial
title(xlab="sans serif axis label")
title(ylab="bold sans serif axis label", font.lab=2)
par(family="mono") # By default Courier
axis(1,
      at=seq(2, 8, 2),
      label=c("mono", "space", "tick", "labels"))
axis(2,
      at=seq(2, 10, 2),
      label=c("bold", "mono", "space", "tick", "labels"),
      font.axis=2)
box()
dev.off()

You might also want to take a look at the article "Fonts, lines, and 
transparency ..." in R News 4/2 
http://cran.stat.auckland.ac.nz/doc/Rnews/Rnews_2004-2.pdf

Paul


Xiang Li wrote:
> the problem is how to use both courier bold font for axis and arial
> bold font for labelling.  If I use familiy = "courier" in postscript,
> everything will become courier fonts ......
> 
> I noticed that in postscirpt help page, we can customize a family such as
> "family = c("CM_regular_10.afm","CM_boldx_10.afm", "cmti10.afm",
> "cmbxti10.afm", "CM_symbol_10.afm";
> 
> However, I tried many times, and just don't know the symbol used in R for
> courier bold or arial bold, etc.
> 
> 
> 
> 
>>Hi,
>>  I am trying to use Arial bold for labelling (font.lab =2), and use
>>the fixed-width Courier bold for axis. I will export the plot to .ps
>>format. I have been trying to use font.axis = 11, but that doesn't work
>>when I exported the plot to a .ps file.
>>
>>  I have been trying hard to read the help page of "postscript" these few
>>days, but didn't get the courier font yet.  Can you please help me
>>figure out how I should use "postscript"? Thanks!
>>
>>Best
>>
>>Sean
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.murrell at auckland.ac.nz  Tue May 24 23:33:41 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 25 May 2005 09:33:41 +1200
Subject: [R] How to get special (Hershey) font symbols into plot axis
	labels? [Revisited]
References: <42934BB8.5080705@oomvanlieshout.net>
Message-ID: <42939DB5.6000705@stat.auckland.ac.nz>

Hi


Sander Oom wrote:
> Dear R users,
> 
> I would like to use sub- and super-script in axis labels. I assume this 
> is best done using Hershey symbols. When trying to find information on 
> using Hershey font symbols in axis labels, I came across the following 
> discussion thread:
> 
> http://maths.newcastle.edu.au/~rking/R/help/02a/1857.html
> 
> Have Hershey font implementations moved on since then?


There hasn't been a lot of movement (it's not an oft-used feature), but 
things should be a bit better (though, e.g., may still be missing from 
title()).

But the real answer is that you should use "plotmath" features (not 
Hershey fonts) for sub- or super-scripting.  See ?plotmath

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.murrell at auckland.ac.nz  Tue May 24 23:41:28 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 25 May 2005 09:41:28 +1200
Subject: [R] rotate pie chart
References: <42933587.509@pik-potsdam.de>
	<9d7c0a31ab9e685806ce1497347b433a@mail.nih.gov>
Message-ID: <42939F88.50502@stat.auckland.ac.nz>

Hi

gridBase won't help here (grid can't rotate traditional graphics output).

I think you have to get your hands dirty on this one, but it's not too 
hard.  Here's a function pie90() which is a tiny modification of pie(). 
  Does that do the trick?

pie90 <- function (x, labels = names(x), edges = 200, radius = 0.8, 
density = NULL,
     angle = 45, col = NULL, border = NULL, lty = NULL, main = NULL,
     ...)
{
     if (!is.numeric(x) || any(is.na(x) | x <= 0))
         stop("'x' values must be positive.")
     if (is.null(labels))
         labels <- as.character(1:length(x))
     x <- c(0, cumsum(x)/sum(x))
     dx <- diff(x)
     plot.new()
     pin <- par("pin")
     xlim <- ylim <- c(-1, 1)
     if (pin[1] > pin[2])
         xlim <- (pin[1]/pin[2]) * xlim
     else ylim <- (pin[2]/pin[1]) * ylim
     plot.window(xlim, ylim, "", asp = 1)
     nx <- length(dx)
     if (is.null(col))
         col <- if (is.null(density))
             c("white", "lightblue", "mistyrose", "lightcyan",
                 "lavender", "cornsilk")
         else par("fg")
     col <- rep(col, length.out = nx)
     border <- rep(border, length.out = nx)
     lty <- rep(lty, length.out = nx)
     angle <- rep(angle, length.out = nx)
     density <- rep(density, length.out = nx)
     for (i in 1:nx) {
         n <- max(2, floor(edges * dx[i]))
# modified line below
         t2p <- 2 * pi * seq(x[i], x[i + 1], length = n) + pi/2
         xc <- c(cos(t2p), 0) * radius
         yc <- c(sin(t2p), 0) * radius
         polygon(xc, yc, density = density[i], angle = angle[i],
             border = border[i], col = col[i], lty = lty[i])
# modified line below
         t2p <- 2 * pi * mean(x[i + 0:1]) + pi/2
         xc <- cos(t2p) * radius
         yc <- sin(t2p) * radius
         if (!is.na(lab <- labels[i]) && lab != "") {
             lines(c(1, 1.05) * xc, c(1, 1.05) * yc)
             text(1.1 * xc, 1.1 * yc, lab, xpd = TRUE, adj = ifelse(xc <
                 0, 1, 0), ...)
         }
     }
     title(main = main, ...)
     invisible(NULL)
}


Paul


Sean Davis wrote:
> You might want to look at grid graphics and gridBase.  I don't know in 
> detail how to go about what you are asking, but grid allows you to 
> rotate plots arbitrarily.  Here are a couple of links that I think are 
> useful.
> 
> http://www.stat.auckland.ac.nz/~paul/grid/grid.html
> http://www.stat.auckland.ac.nz/~paul/grid/doc/rotated.pdf
> 
> Sean
> 
> On May 24, 2005, at 10:09 AM, Lars wrote:
> 
>> hey,
>>
>> about two weeks ago i posted a question concerning the display of two 
>> piecharts on one plot. after now being able to do so, i need to rotate 
>> them. the first piece of my pie is suppose to start at 0?? but at 90??. 
>> i tried several things, all failing in the end. anyone out there who 
>> has an idea?
>>
>> Lars
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From xli13 at uic.edu  Tue May 24 23:41:41 2005
From: xli13 at uic.edu (Xiang Li)
Date: Tue, 24 May 2005 16:41:41 -0500 (CDT)
Subject: [R] in other words Re: How to use fixed-width fonts such as
	courier in R
In-Reply-To: <42939CB0.5000804@stat.auckland.ac.nz>
References: <Pine.GSO.4.58.0505241522200.23797@icarus.cc.uic.edu>
	<Pine.GSO.4.58.0505241552470.23797@icarus.cc.uic.edu>
	<42939CB0.5000804@stat.auckland.ac.nz>
Message-ID: <Pine.GSO.4.58.0505241641060.23797@icarus.cc.uic.edu>

Great. Thanks a lot. That is exactly what I need.

Xiang

On Wed, 25 May 2005, Paul Murrell wrote:

> Hi
>
> Does this do what you want?
>
> postscript("example.ps", fonts=c("sans", "mono"))
> plot(1:10, axes=FALSE, ann=FALSE)
> par(family="sans") # By default Helvetica which is like Arial
> title(xlab="sans serif axis label")
> title(ylab="bold sans serif axis label", font.lab=2)
> par(family="mono") # By default Courier
> axis(1,
>       at=seq(2, 8, 2),
>       label=c("mono", "space", "tick", "labels"))
> axis(2,
>       at=seq(2, 10, 2),
>       label=c("bold", "mono", "space", "tick", "labels"),
>       font.axis=2)
> box()
> dev.off()
>
> You might also want to take a look at the article "Fonts, lines, and
> transparency ..." in R News 4/2
> http://cran.stat.auckland.ac.nz/doc/Rnews/Rnews_2004-2.pdf
>
> Paul
>
>
> Xiang Li wrote:
> > the problem is how to use both courier bold font for axis and arial
> > bold font for labelling.  If I use familiy = "courier" in postscript,
> > everything will become courier fonts ......
> >
> > I noticed that in postscirpt help page, we can customize a family such as
> > "family = c("CM_regular_10.afm","CM_boldx_10.afm", "cmti10.afm",
> > "cmbxti10.afm", "CM_symbol_10.afm";
> >
> > However, I tried many times, and just don't know the symbol used in R for
> > courier bold or arial bold, etc.
> >
> >
> >
> >
> >>Hi,
> >>  I am trying to use Arial bold for labelling (font.lab =2), and use
> >>the fixed-width Courier bold for axis. I will export the plot to .ps
> >>format. I have been trying to use font.axis = 11, but that doesn't work
> >>when I exported the plot to a .ps file.
> >>
> >>  I have been trying hard to read the help page of "postscript" these few
> >>days, but didn't get the courier font yet.  Can you please help me
> >>figure out how I should use "postscript"? Thanks!
> >>
> >>Best
> >>
> >>Sean
> >>
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
> --
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
>



From joseclaudio.faria at terra.com.br  Wed May 25 00:04:34 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Tue, 24 May 2005 19:04:34 -0300
Subject: [R] Contingency tables from data.frames
Message-ID: <4293A4F2.7080301@terra.com.br>

Dear list,

I'm trying to do a set of generic functions do make contingency tables from 
data.frames. It is just running "nice" (I'm learning R), but I think it can be 
better.

I would like to filter the data.frame, i.e, eliminate all not numeric variables.
And I don't know how to make it: please, help me.

Below one of the my functions ('er' is a mention to EasieR, because I'm trying 
to do a package for myself and the my students):

#2. Tables from data.frames
#2.1---er.table.df.br (User define breaks and right)------------
er.table.df.br <- function(df,
                            breaks = c('Sturges', 'Scott', 'FD'),
                            right = FALSE) {

   if (is.data.frame(df) != 'TRUE')
     stop('need "data.frame" data')

   dim_df <- dim(df)

   tmpList <- list()

   for (i in 1:dim_df[2]) {

     x <- as.matrix(df[ ,i])
     x <- na.omit(x)

     k <- switch(breaks[1],
                 'Sturges' = nclass.Sturges(x),
                 'Scott'   = nclass.scott(x),
                 'FD'      = nclass.FD(x),
                 stop("'breaks' must be 'Sturges', 'Scott' or 'FD'"))

     tmp      <- range(x)
     classIni <- tmp[1] - tmp[2]/100
     classEnd <- tmp[2] + tmp[2]/100
     R        <- classEnd-classIni
     h        <- R/k

     # Absolut frequency
     f <- table(cut(x, br = seq(classIni, classEnd, h), right = right))

     # Relative frequency
     fr <- f/length(x)

     # Relative frequency, %
     frP <- 100*(f/length(x))

     # Cumulative frequency
     fac <- cumsum(f)

     # Cumulative frequency, %
     facP <- 100*(cumsum(f/length(x)))

     fi   <- round(f, 2)
     fr   <- round(as.numeric(fr), 2)
     frP  <- round(as.numeric(frP), 2)
     fac  <- round(as.numeric(fac), 2)
     facP <- round(as.numeric(facP),2)

     # Table
     res <- data.frame(fi, fr, frP, fac, facP)
     names(res) <- c('Class limits', 'fi', 'fr', 'fr(%)', 'fac', 'fac(%)')
     tmpList <- c(tmpList, list(res))
   }
   names(tmpList) <- names(df)
   return(tmpList)
}

To try the function:

#a) runing nice
y1=rnorm(100, 10, 1)
y2=rnorm(100, 58, 4)
y3=rnorm(100, 500, 10)
mydf=data.frame(y1, y2, y3)
#tbdf=er.table.df.br (mydf, breaks = 'Sturges', right=F)
#tbdf=er.table.df.br (mydf, breaks = 'Scott', right=F)
tbdf=er.table.df.br (mydf, breaks = 'FD', right=F)
print(tbdf)


#b) One of the problems
y1=rnorm(100, 10, 1)
y2=rnorm(100, 58, 4)
y3=rnorm(100, 500, 10)
y4=rep(letters[1:10], 10)
mydf=data.frame(y1, y2, y3, y4)
tbdf=er.table.df.br (mydf, breaks = 'Scott', right=F)
print(tbdf)

Could anyone give me a hint how to work around this?

PS: Excuse my bad English ;-)
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br



From ggrothendieck at gmail.com  Wed May 25 00:21:25 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 May 2005 18:21:25 -0400
Subject: [R] Contingency tables from data.frames
In-Reply-To: <4293A4F2.7080301@terra.com.br>
References: <4293A4F2.7080301@terra.com.br>
Message-ID: <971536df050524152110a35e60@mail.gmail.com>

On 5/24/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> Dear list,
> 
> I'm trying to do a set of generic functions do make contingency tables from
> data.frames. It is just running "nice" (I'm learning R), but I think it can be
> better.
> 
> I would like to filter the data.frame, i.e, eliminate all not numeric variables.
> And I don't know how to make it: please, help me.
> 
> Below one of the my functions ('er' is a mention to EasieR, because I'm trying
> to do a package for myself and the my students):
> 
> #2. Tables from data.frames
> #2.1---er.table.df.br (User define breaks and right)------------
> er.table.df.br <- function(df,
>                            breaks = c('Sturges', 'Scott', 'FD'),
>                            right = FALSE) {
> 
>   if (is.data.frame(df) != 'TRUE')
>     stop('need "data.frame" data')
> 
>   dim_df <- dim(df)
> 
>   tmpList <- list()
> 
>   for (i in 1:dim_df[2]) {
> 
>     x <- as.matrix(df[ ,i])
>     x <- na.omit(x)
> 
>     k <- switch(breaks[1],
>                 'Sturges' = nclass.Sturges(x),
>                 'Scott'   = nclass.scott(x),
>                 'FD'      = nclass.FD(x),
>                 stop("'breaks' must be 'Sturges', 'Scott' or 'FD'"))
> 
>     tmp      <- range(x)
>     classIni <- tmp[1] - tmp[2]/100
>     classEnd <- tmp[2] + tmp[2]/100
>     R        <- classEnd-classIni
>     h        <- R/k
> 
>     # Absolut frequency
>     f <- table(cut(x, br = seq(classIni, classEnd, h), right = right))
> 
>     # Relative frequency
>     fr <- f/length(x)
> 
>     # Relative frequency, %
>     frP <- 100*(f/length(x))
> 
>     # Cumulative frequency
>     fac <- cumsum(f)
> 
>     # Cumulative frequency, %
>     facP <- 100*(cumsum(f/length(x)))
> 
>     fi   <- round(f, 2)
>     fr   <- round(as.numeric(fr), 2)
>     frP  <- round(as.numeric(frP), 2)
>     fac  <- round(as.numeric(fac), 2)
>     facP <- round(as.numeric(facP),2)
> 
>     # Table
>     res <- data.frame(fi, fr, frP, fac, facP)
>     names(res) <- c('Class limits', 'fi', 'fr', 'fr(%)', 'fac', 'fac(%)')
>     tmpList <- c(tmpList, list(res))
>   }
>   names(tmpList) <- names(df)
>   return(tmpList)
> }
> 
> To try the function:
> 
> #a) runing nice
> y1=rnorm(100, 10, 1)
> y2=rnorm(100, 58, 4)
> y3=rnorm(100, 500, 10)
> mydf=data.frame(y1, y2, y3)
> #tbdf=er.table.df.br (mydf, breaks = 'Sturges', right=F)
> #tbdf=er.table.df.br (mydf, breaks = 'Scott', right=F)
> tbdf=er.table.df.br (mydf, breaks = 'FD', right=F)
> print(tbdf)
> 
> 
> #b) One of the problems
> y1=rnorm(100, 10, 1)
> y2=rnorm(100, 58, 4)
> y3=rnorm(100, 500, 10)
> y4=rep(letters[1:10], 10)
> mydf=data.frame(y1, y2, y3, y4)
> tbdf=er.table.df.br (mydf, breaks = 'Scott', right=F)
> print(tbdf)
> 

Try this:

sapply(my.data.frame, is.numeric)

Also you might want to look up:

?match.arg
?stopifnot
?ncol
?sapply
?lapply



From p.murrell at auckland.ac.nz  Wed May 25 00:52:07 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 25 May 2005 10:52:07 +1200
Subject: [R] image() and non-well-ordered colours
References: <4d1d026459eb14d5e20cce234ef756b1@soc.soton.ac.uk>
Message-ID: <4293B017.7090204@stat.auckland.ac.nz>

Hi


Robin Hankin wrote:
> Hi.
> 
> I want to use image() with colours that are indexed by two variables.
> Indexing by one variable is easy:
> 
> library(colorspace)
> x <- seq(from=0, to=1,len=30)
> z <- outer(x,1i*x,"+")
> image(Re(z),col=hcl(seq(from=0,to=100,len=15),c=100))
> 
> OK, so far so good.  Now, I want the colour to be a more complicated 
> function
> of z, in which both the hue and luminance change (thus the colours cannot
> be ordered):
> 
> 
> f <- function(z){hcl(h=100*Re(z),l=100*Im(z))}
> 
> I want to draw z in terms of the colour defined by f():
> 
> image(z,col=f)
> image(f(z))
> 
> but these don't work as intended.  How do I use image() to get what I want?
> I can get close using plot():
> 
> x <- runif(1000)
> y <- (1:1000)/10
> g <- function(x){hcl(h=80*x,l=(1:1000)/10,c=300)}
> plot(x,y,col=g(x),pch=16)
> 
> [note that one cannot draw nontrivial "contour lines"  joining points of 
> identical colours on this
> plot: top left to lower right goes from pink to black; top right to low 
> left goes from yellow to reddy orange]
> 
> 
> It'd be nice to make image() do what I want. Anyone?


How about a brute-force approach using rect() ... ?

x <- seq(from=0, to=1,len=30)
z <- outer(x,1i*x,"+")

f <- function(z){hcl(h=100*Re(z),l=100*Im(z))}

plot(x, x, type="n")
step <- diff(x)[1]/2
xmid <- rep(x, 30)
ymid <- rep(x, each=30)
rect(xmid - step, ymid - step,
      xmid + step, ymid + step,
      col=f(z), border=NA)
box()

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From rgentlem at fhcrc.org  Wed May 25 02:19:14 2005
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Tue, 24 May 2005 17:19:14 -0700
Subject: [R] Re: S4 method inheritance
In-Reply-To: <20050524202050.GO3351@wheat.boylan.org>
References: <1116884515.4612.43.camel@iron.libaux.ucsf.edu>
	<1116893025.4615.61.camel@iron.libaux.ucsf.edu>
	<42930ADB.60207@stats.uwo.ca> <42932BDC.9090504@fhcrc.org>
	<20050524202050.GO3351@wheat.boylan.org>
Message-ID: <4293C482.5060805@fhcrc.org>



Ross Boylan wrote:
> On Tue, May 24, 2005 at 06:27:56AM -0700, Robert Gentleman wrote:
> 
>>
>>Duncan Murdoch wrote:
>>
>>>Ross Boylan wrote:
>>>
>>>
>>>>On Mon, 2005-05-23 at 14:41 -0700, Ross Boylan wrote:
>>>>....
>>>>
>>>>
>>>>
>>>>>Finally, I'm a bit concerned that one article mentioned that S4
>>>>>inheritance, in practice, is used mostly for data, not methods (Thomas
>>>>>Lumley, R News 4(1), June 2004: p. 36).  Am I going down a road I
>>>>>shouldn't travel?
>>>>>
>>>>
>>>>Hmm, maybe I just found out.  If B is an S4 subclass of A (aka extends
>>>>A), how does B's method foo invoke A's foo?
>>>
>>>
>>>Your question doesn't make sense in S4.  In S4, classes don't have 
>>>methods, generics have methods.  There's no such thing as "B's method" 
>>>or "A's method".
>>>
>>>You might get what you want with foo(as(bObject, "A")) if bObject is an 
>>>instance of class B.
>>>
>>>
>>>>The question assumes that A's foo was defined as an in place function,
>>>>so there's no (obvious) named object for it, i.e,
>>>>setMethod("A", signature(blah="numeric"), function(x) something)
>>>
>>In general it may be best to think of a generic function as a 
>>dispatching mechanism. For S4 methods are associated with a specific 
>>generic function. 
> 
> "specific" generic is a reference to the ability to define generics
>  within the context of a particular package?


Well, more that you can identify one (either explicitly by its package 
[ie. fully-qualified name], or implicitly by the fact that it is first 
on the search path - the former being prefered). But the notion is that 
there is one and you have asked that your method be made available to 
that one generic function for dispatch. This is in contrast to S3 - 
where no such functionality exists - that I know of; under S3 any method 
is a method for all generic functions of the correct name and the 
programmer has no (easy) control. Under S4 methods are not really 
ordinary functions, should not be called directly and are invoked only 
via calls to the generic (you can of course break all of those rules).

> 
>>A generic knows about all methods that are associated 
>>with it, and about no others. 
> 
> Presumably setMethod does the association.  Is the where argument
> intended to identify which generic method to pick?  The fact that
> there is not a "package" argument to setMethod, as there is to
> setGeneric, is a little confusing to me.
> 
> 

  I believe that is the documented behavior. Yes the where argument 
should allow you complete specificity. I will leave it to the auther to 
clarify differences between the where and package arguments in the call 
to setGeneric (or you, if you care to explore the code).

>>Thus in S4, the little tiff over who owns 
>>label goes away - they both do - different packages can define
>>generic 
> 
> "They" is two different packages?  Or is this a reference to my
> original confusion about class vs generic ownership of a method?
> 

  It is the two packages. The S langauge allows for the same symbol to 
be bound to different values in different namespaces (in R that is 
becoming explicit, in SPlus it is less so, but still generally true). I 
can think of no good reason to treat generic functions, or any other 
first class object, differently.

HTH,
  Robert

> 
>>functions for label, or anything else they care to, and users can write 
>>methods for specific generic
  functions and associate them with a
>>generic.
> 
> ...
> 
>> HTH
>>   Robert
> 
>



From gerifalte28 at hotmail.com  Wed May 25 04:15:00 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 25 May 2005 02:15:00 +0000
Subject: [R] obtaining first and last record for rows with same identifier
Message-ID: <BAY103-F131FB65C0172950EA34918A60E0@phx.gbl>

If you want to obtain a data frame you can use the functions head and tail 
like:

dat=data.frame(id=rep(1:5,3),num=rnorm(15), num2=rnorm(15))#Creates data 
frame with id
last=do.call("rbind",by(dat,dat$id,tail,1))#Selects the last observation for 
each id
first=do.call("rbind",by(dat,dat$id,head,1))#Selects the first observation 
for each id
newdat=rbind(first,last)#Joins data
newdat=newdat[order(newdat$id),]#sorts data by id

Notice that rownames will give you the original row location of the 
observations selected

I hope this helps

Francisco


>From: Berton Gunter <gunter.berton at gene.com>
>To: "'Sean Davis'" <sdavis2 at mail.nih.gov>, <sms13+ at pitt.edu>
>CC: "'rhelp'" <r-help at stat.math.ethz.ch>
>Subject: RE: [R] obtaining first and last record for rows with same 
>identifier
>Date: Tue, 24 May 2005 12:17:58 -0700
>
>
>I think by() is simpler:
>
>  by(yourframe,factor(yourframe$patid),function(x)x[c(1,nrow(x)),])
>
>
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
>
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
> > Sent: Tuesday, May 24, 2005 11:38 AM
> > To: sms13+ at pitt.edu
> > Cc: rhelp
> > Subject: Re: [R] obtaining first and last record for rows
> > with same identifier
> >
> > If you have your data.frame ordered by the patid, you can use the
> > function rle in combination with cumsum.  As a vector example:
> >
> >  > a <- rep(c('a','b','c'),10)
> >  > a
> >   [1] "a" "b" "c" "a" "b" "c" "a" "b" "c" "a" "b" "c" "a" "b" "c" "a"
> > "b" "c" "a"
> > [20] "b" "c" "a" "b" "c" "a" "b" "c" "a" "b" "c"
> >  > b <- a[order(a)]
> >  > b
> >   [1] "a" "a" "a" "a" "a" "a" "a" "a" "a" "a" "b" "b" "b" "b" "b" "b"
> > "b" "b" "b"
> > [20] "b" "c" "c" "c" "c" "c" "c" "c" "c" "c" "c"
> >  > l <- rle(b)$length
> >  > cbind(l,cumsum(l),cumsum(l)-l+1)
> >        l
> > [1,] 10 10  1
> > [2,] 10 20 11
> > [3,] 10 30 21
> >
> > # use the line below to get the length of the block of the dataframe,
> > the start, and then end indices
> >  > cbind(l,cumsum(l)-l+1,cumsum(l))
> >        l
> > [1,] 10  1 10
> > [2,] 10 11 20
> > [3,] 10 21 30
> >  >
> >
> > Sean
> >
> >
> > On May 24, 2005, at 2:27 PM, sms13+ at pitt.edu wrote:
> >
> > > I have a dataframe that contains fields such as patid, labdate,
> > > labvalue.
> > > The same patid may show up in multiple rows because of lab
> > > measurements on multiple days.  Is there a simple way to
> > obtain just
> > > the first and last record for each patient, or do I need to
> > write some
> > > code that performs that.
> > >
> > > Thanks,
> > > Steven
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From Friedrich.Leisch at tuwien.ac.at  Wed May 25 08:24:12 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Wed, 25 May 2005 08:24:12 +0200
Subject: [R] input line length in Sweave
In-Reply-To: <7203557.1116966211289.JavaMail.root@wamui-swiss.atl.sa.earthlink.net>
References: <7203557.1116966211289.JavaMail.root@wamui-swiss.atl.sa.earthlink.net>
Message-ID: <17044.6668.950781.245068@galadriel.ci.tuwien.ac.at>

>>>>> On Tue, 24 May 2005 16:23:31 -0400 (GMT-04:00),
>>>>> Woodrow Setzer (WS) wrote:

  > I am having trouble in Sweave with input line lengths.  For example, I may have in my input file the chunk

[...]

  > Is there a way to get Sweave to wrap long input lines better, or
  > get it to use my own formatting of the input?  I realize I can
  > edit the output tex file, but that is impractical in my
  > application (too many).

  > I am using R version 2.0.1 Patched (2005-01-26) on a Linux system
  > (so, I suppose one possible answer is that this is fixed in 2.1.0;
  > I cannot switch right now).

No, no changes in 2.1 with respect to that problem. I have plans for a
solution for this problem, but won't have time for it before summer
break -> in 2.2 there should be a fix for it.

Best,
Fritz



From Katrin.Schweitzer at ims.uni-stuttgart.de  Wed May 25 08:25:07 2005
From: Katrin.Schweitzer at ims.uni-stuttgart.de (Katrin Schweitzer)
Date: Wed, 25 May 2005 08:25:07 +0200
Subject: [R] rotate pie chart
In-Reply-To: <8a83e500050513072924c8569b@mail.gmail.com>
References: <8a83e500050513072924c8569b@mail.gmail.com>
Message-ID: <42941A43.60407@ims.uni-stuttgart.de>


 > I think you have to get your hands dirty on this one, but it's not too
 > hard.  Here's a function pie90() which is a tiny modification of pie().
 >   Does that do the trick?
 >

Yes, it works perfectly fine, at least for what I wanted... :)
Thanks a lot, to Lars for asking, and to Paul for getting your hands dirty!

Kati

PS: I know one shouldn't use pie charts at all... :) but if I do so, is 
there a  reason
why they work counter-clockwise in R? Is that convention?
 Sorry if its a silly question, my intuition (which might very likely be 
horrible)
just expected them to start at 12 o'clock and fill the pie clockwisely.









 > pie90 <- function (x, labels = names(x), edges = 200, radius = 0.8,
 > density = NULL,
 >      angle = 45, col = NULL, border = NULL, lty = NULL, main = NULL,
 >      ...)
 > {
 >      if (!is.numeric(x) || any(is.na(x) | x <= 0))
 >          stop("'x' values must be positive.")
 >      if (is.null(labels))
 >          labels <- as.character(1:length(x))
 >      x <- c(0, cumsum(x)/sum(x))
 >      dx <- diff(x)
 >      plot.new()
 >      pin <- par("pin")
 >      xlim <- ylim <- c(-1, 1)
 >      if (pin[1] > pin[2])
 >          xlim <- (pin[1]/pin[2]) * xlim
 >      else ylim <- (pin[2]/pin[1]) * ylim
 >      plot.window(xlim, ylim, "", asp = 1)
 >      nx <- length(dx)
 >      if (is.null(col))
 >          col <- if (is.null(density))
 >              c("white", "lightblue", "mistyrose", "lightcyan",
 >                  "lavender", "cornsilk")
 >          else par("fg")
 >      col <- rep(col, length.out = nx)
 >      border <- rep(border, length.out = nx)
 >      lty <- rep(lty, length.out = nx)
 >      angle <- rep(angle, length.out = nx)
 >      density <- rep(density, length.out = nx)
 >      for (i in 1:nx) {
 >          n <- max(2, floor(edges * dx[i]))
 > # modified line below
 >          t2p <- 2 * pi * seq(x[i], x[i + 1], length = n) + pi/2
 >          xc <- c(cos(t2p), 0) * radius
 >          yc <- c(sin(t2p), 0) * radius
 >          polygon(xc, yc, density = density[i], angle = angle[i],
 >              border = border[i], col = col[i], lty = lty[i])
 > # modified line below
 >          t2p <- 2 * pi * mean(x[i + 0:1]) + pi/2
 >          xc <- cos(t2p) * radius
 >          yc <- sin(t2p) * radius
 >          if (!is.na(lab <- labels[i]) && lab != "") {
 >              lines(c(1, 1.05) * xc, c(1, 1.05) * yc)
 >              text(1.1 * xc, 1.1 * yc, lab, xpd = TRUE, adj = ifelse(xc <
 >                  0, 1, 0), ...)
 >          }
 >      }
 >      title(main = main, ...)
 >      invisible(NULL)
 > }
 >
 >
 > Paul
 >
 >
 > Sean Davis wrote:
 > > You might want to look at grid graphics and gridBase.  I don't know in
 > > detail how to go about what you are asking, but grid allows you to
 > > rotate plots arbitrarily.  Here are a couple of links that I think are
 > > useful.
 > >
 > > http://www.stat.auckland.ac.nz/~paul/grid/grid.html
 > > http://www.stat.auckland.ac.nz/~paul/grid/doc/rotated.pdf
 > >
 > > Sean
 > >
 > > On May 24, 2005, at 10:09 AM, Lars wrote:
 > >
 > >> hey,
 > >>
 > >> about two weeks ago i posted a question concerning the display of two
 > >> piecharts on one plot. after now being able to do so, i need to rotate
 > >> them. the first piece of my pie is suppose to start at 0?? but at 90??.
 > >> i tried several things, all failing in the end. anyone out there who
 > >> has an idea?
 > >>
 > >> Lars
 > >>
 > >> ______________________________________________
 > >> R-help at stat.math.ethz.ch mailing list
 > >> https://stat.ethz.ch/mailman/listinfo/r-help
 > >> PLEASE do read the posting guide!
 > >> http://www.R-project.org/posting-guide.html
 > >
 > >
 > > ______________________________________________
 > > R-help at stat.math.ethz.ch mailing list
 > > https://stat.ethz.ch/mailman/listinfo/r-help
 > > PLEASE do read the posting guide!
 > > http://www.R-project.org/posting-guide.html
 >
 >
 > --
 > Dr Paul Murrell
 > Department of Statistics
 > The University of Auckland
 > Private Bag 92019
 > Auckland
 > New Zealand
 > 64 9 3737599 x85392
 > paul at stat.auckland.ac.nz
 > http://www.stat.auckland.ac.nz/~paul/
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
 >



From jlvw at na.rau.ac.za  Wed May 25 08:27:28 2005
From: jlvw at na.rau.ac.za (Jacob van Wyk)
Date: Wed, 25 May 2005 08:27:28 +0200
Subject: [R] Errors in Variables
Message-ID: <s29436f7.056@rauzen.rau.ac.za>

I hope somebody can help.
A student of mine is doing a study on Measurement Error models
(errors-in-variables, total least squares, etc.). I have an old
reference to a "multi archive"  that contains
leiv3: Programs for best line fitting with errors in both coordinates.
(The date is October 1989, by B.D. Ripley et al.)
I have done a search for something similar in R withour success. Has
this been implemented in a R-package, possibly under some sort of
assumptions about variances. I would lke my student to apply some
regression techniques to data that fit this profile.
Any help is much appreciated.
(If I have not done my search more carefully - my apologies.)
Thanks
Jacob


Jacob L van Wyk
Department of Mathematics and Statistics
University of Johannesburg APK
P O Box 524
Auckland Park 2006
South Africa
Tel: +27-11-489-3080
Fax: +27-11-489-2832



From bhx2 at mevik.net  Wed May 25 09:13:35 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Wed, 25 May 2005 09:13:35 +0200
Subject: [R] How to break an axis?
In-Reply-To: <6ea7b5430505231859100344c7@mail.gmail.com> (Bo Peng's message
	of "Mon, 23 May 2005 20:59:53 -0500")
References: <6ea7b5430505231859100344c7@mail.gmail.com>
Message-ID: <m0hdgrlqw0.fsf@bar.nemo-project.org>

What about simply using a log scale on the y axis? I.e. plot(..., log="y")

-- 
Bj??rn-Helge Mevik



From guillaume.chapron at gmail.com  Wed May 25 09:19:28 2005
From: guillaume.chapron at gmail.com (Guillaume Chapron)
Date: Wed, 25 May 2005 09:19:28 +0200
Subject: [R] R unable to run on Mac OS 10.4 Tiger
In-Reply-To: <FD9A2379-7B76-48C4-82A6-C45681B1396E@mac.com>
References: <429350DE.7060604@gmail.com>
	<FD9A2379-7B76-48C4-82A6-C45681B1396E@mac.com>
Message-ID: <42942700.3070405@gmail.com>

Dear all,

Thanks for the reply, I solved the issue last night. It appears it is 
was a problem with quicktime library. When I installed Tiger, I dragged 
my Panther quicktime plugin and this was not a good idea. R did not work 
(whatever version) in GUI, but would work in the terminal. I restarted 
the machine and the R would work as GUI, with this error message (at 
least this one, and many times):

2005-05-24 18:42:25.919 R[316] CFLog (21): Error loading 
/Library/QuickTime/DivX 5.component/Contents/MacOS/DivX 5:  error code 
4, error number 0 (Library not loaded: /Library/Application 
Support/DivXNetworks/liblame3.92.dylib
   Referenced from: /Library/QuickTime/DivX 
5.component/Contents/MacOS/DivX 5
   Reason: image not found)

I installed DivX stuff and I now get only this message:

2005-05-25 09:16:31.332 R[4042] CFLog (21): Error loading 
/Library/QuickTime/LiveType.component/Contents/MacOS/LiveType:  error 
code 4, error number 0 (Library not loaded: 
/System/Library/PrivateFrameworks/LiveType.framework/Versions/A/LiveType
   Referenced from: 
/Library/QuickTime/LiveType.component/Contents/MacOS/LiveType
   Reason: image not found)

I suppose i will have to reinstall quicktime 7 and things will be OK. 
But I don't understand why R does need quicktime ??

Thanks
Guillaume

> Hi Guillaume,
> 
> There is a R-SIG-Mac alias where many of these questions are being  
> addressed.
> 
> The most likely reason is that you have a .RData file around that its  
> trying to load.
> It might be missing a library or trying to connect to X11.
> 
> Can you check for that 1st in a terminal window (ls -lia). Finder  does 
> not show .xxx files.
> If you reply to me, we can take it from there or switch to R-Sig-Mac.
> 
> Rob
> 
> 
> On May 24, 2005, at 9:05 AM, Guillaume Chapron wrote:
> 
>> Hello,
>> I'm running a PB G4 with Mac OS 10.4.1. I have downloaded the  latest 
>> version R-2.1.0a.dmg. It appears that R does not work. It  launches 
>> itself, but the window never gets ready, there is written  "Loading 
>> R..." and a small progress wheel keeps turning indefinitely.
>> Could someone help or suggest something?
>> THANKS !!
>> Guillaume
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>>
> 
>



From josue at ucsc.edu  Wed May 25 09:19:48 2005
From: josue at ucsc.edu (Josue Samayoa)
Date: Wed, 25 May 2005 00:19:48 -0700
Subject: [R] Placing A Legend in Titlle of a Plot
Message-ID: <5af6a350182b3e9936aadfd0f285e4a1@ucsc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/2837f4d2/attachment.pl

From gottfried.gruber at terminal.at  Wed May 25 09:55:05 2005
From: gottfried.gruber at terminal.at (Gottfried Gruber)
Date: Wed, 25 May 2005 09:55:05 +0200
Subject: [R] Deleting multiple rows from a matrix
Message-ID: <200505250955.06050.gottfried.gruber@terminal.at>

Hi,

i want to delete multiple rows from a matrix. I know how to delete one by
x=x[,-3]  # deleting 3. column

Is there a quick method how to delete e.g. the 3., 5., 7. and 8. column at 
once?

TiA gg

-- 
---------------------------------------------------
Gottfried Gruber
mailto:gottfried.gruber at terminal.at
www: http://gogo.sehrsupa.net



From sundar.dorai-raj at pdf.com  Wed May 25 09:52:40 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 25 May 2005 00:52:40 -0700
Subject: [R] Deleting multiple rows from a matrix
In-Reply-To: <200505250955.06050.gottfried.gruber@terminal.at>
References: <200505250955.06050.gottfried.gruber@terminal.at>
Message-ID: <42942EC8.3070401@pdf.com>

Gottfried Gruber wrote:
> Hi,
> 
> i want to delete multiple rows from a matrix. I know how to delete one by
> x=x[,-3]  # deleting 3. column
> 
> Is there a quick method how to delete e.g. the 3., 5., 7. and 8. column at 
> once?
> 
> TiA gg
> 

x[, -c(3, 5, 7, 8)]

--sundar



From dominique.emmanuel at asterop.com  Wed May 25 09:53:29 2005
From: dominique.emmanuel at asterop.com (Dominique Emmanuel)
Date: Wed, 25 May 2005 09:53:29 +0200
Subject: [R] DOING CLUSTERING WITH THE EM ALGORITHM
Message-ID: <64F4CED51696BE489C8547E1FEC4ED5C8740BC@lan-123.79.68.195.rev.fr.colt.net>

Hello,

I am looking for documentation (PDF ?) about doing clustering with the EM Algorithm on R... I'm sorry for this question which may be trivial...

Thanks...

Dominique



From Tom.Mulholland at dpi.wa.gov.au  Wed May 25 09:55:23 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 25 May 2005 15:55:23 +0800
Subject: [R] Placing A Legend in Titlle of a Plot
Message-ID: <4702645135092E4497088F71D9C8F51A128B7C@afhex01.dpi.wa.gov.au>

Without a small example to see what you are doing it is hard to respond. There are plenty of examples in the help for legend showing placement all over the place. So I am guessing that this might help (The only thing added from the help was the par(xpd = TRUE)


     x <- 0:64/64
     y <- sin(3*pi*x)
     plot(x, y, type="l", col="blue", main = "")
     points(x, y, pch=21, bg="white")
     par(xpd = TRUE)
     legend(.4,1.25, "sin(c x)", pch=21, pt.bg="white", lty=1, col = "blue")
     par(xpd = FALSE)

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Josue Samayoa
> Sent: Wednesday, 25 May 2005 3:20 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Placing A Legend in Titlle of a Plot
> 
> 
> Hello,
> I am trying to place a legend in the title area of a plot.  I can not 
> seem to find a way to get legend() to do this.  Nor can I use the 
> title() function.  Does anyone know a way to do this?  I 
> would greatly 
> appreciate any advice.
> 
> Thanks
> 
> 
> Josue Samayoa
> Center for Biomolecular Science & Engineering
> School of Engineering
> Division of Physical & Biological Sciences
> University of California Santa Cruz
> (831) 459-1019
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From francoisromain at free.fr  Wed May 25 09:49:35 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 25 May 2005 09:49:35 +0200
Subject: [R] Placing A Legend in Titlle of a Plot
In-Reply-To: <5af6a350182b3e9936aadfd0f285e4a1@ucsc.edu>
References: <5af6a350182b3e9936aadfd0f285e4a1@ucsc.edu>
Message-ID: <42942E0F.5010501@free.fr>

Le 25.05.2005 09:19, Josue Samayoa a ??crit :

>Hello,
>I am trying to place a legend in the title area of a plot.  I can not 
>seem to find a way to get legend() to do this.  Nor can I use the 
>title() function.  Does anyone know a way to do this?  I would greatly 
>appreciate any advice.
>
>Thanks
>
>
>Josue Samayoa
>Center for Biomolecular Science & Engineering
>School of Engineering
>Division of Physical & Biological Sciences
>University of California Santa Cruz
>(831) 459-1019
>  
>
Hello,

What kind of plot ? Not easy to know what you tried if you don't tell it.
read the posting guide and then give a short reproductible example of 
what you tried that didn't work.

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From francoisromain at free.fr  Wed May 25 09:58:48 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 25 May 2005 09:58:48 +0200
Subject: [R] Deleting multiple rows from a matrix
In-Reply-To: <200505250955.06050.gottfried.gruber@terminal.at>
References: <200505250955.06050.gottfried.gruber@terminal.at>
Message-ID: <42943038.5090103@free.fr>

Le 25.05.2005 09:55, Gottfried Gruber a Å√Å©crit :

>Hi,
>
>i want to delete multiple rows from a matrix. I know how to delete one by
>x=x[,-3]  # deleting 3. column
>
>Is there a quick method how to delete e.g. the 3., 5., 7. and 8. column at 
>once?
>
>TiA gg
>
>  
>
Same way. Only just provide a vector, as in :
x = x[,-c(3,5,7)]

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ligges at statistik.uni-dortmund.de  Wed May 25 10:05:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 10:05:43 +0200
Subject: [R] reading multiple files
In-Reply-To: <20050524150953.18010.qmail@web61312.mail.yahoo.com>
References: <20050524150953.18010.qmail@web61312.mail.yahoo.com>
Message-ID: <429431D7.9030902@statistik.uni-dortmund.de>

Dave Evens wrote:

> Dear All,
> 
> 
> How do I read in multiple data frames or matrices in a
> loop, e.g.
> 
> for (i in 1:n) {
>    channel <- odbcConnectExcel("filenames")
>    file[i] <- as.data.frame(sqlFetch(channel,
> "sheet"))
> }
> 
> I would like file[i] to be the name of the data.frame
> (i.e. file[1], file[2], file[3],...etc) rather than a
> vector.

The terminology you are using re. R objects seems to be rather confused.

Quite probably you want a list of data frames such as using the 
filenames as names for the list elements along the follwoing lines:

library(RODBC)
dataframes <- vector(mode="list", length=length(filenames))
names(dataframes) <- filenames
for(i in filenames){
   channel <- odbcConnectExcel(i)
   dataframes[[i]] <- sqlFetch(channel, "sheet")
   close(channel)
}


Uwe Ligges



> Thanks in advance for any help.
> 
> Dave
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed May 25 10:11:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 10:11:03 +0200
Subject: [R] plot in a two dimension surface with more than 2 variables
In-Reply-To: <20050524170851.63639.qmail@web26902.mail.ukl.yahoo.com>
References: <20050524170851.63639.qmail@web26902.mail.ukl.yahoo.com>
Message-ID: <42943317.5040702@statistik.uni-dortmund.de>

Amir Safari wrote:

>  
>  
> Dear All ,
> How it is possible to trace a plot in a two dimension surface with more than 2 variables?

Do you want a surface or a point cloud? Do you want to use some sort of 
projection? What is "more than 2"?
Please specify your question more precisely.

Uwe Ligges


> ps: library( rgl) is 3D
>  
> So many THANKS
> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From christophe.pouzat at univ-paris5.fr  Wed May 25 10:16:44 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Wed, 25 May 2005 10:16:44 +0200
Subject: [R] DOING CLUSTERING WITH THE EM ALGORITHM
In-Reply-To: <64F4CED51696BE489C8547E1FEC4ED5C8740BC@lan-123.79.68.195.rev.fr.colt.net>
References: <64F4CED51696BE489C8547E1FEC4ED5C8740BC@lan-123.79.68.195.rev.fr.colt.net>
Message-ID: <4294346C.9080009@univ-paris5.fr>

Hi,

Check out the docs of the mclust and FlexMix packages:

http://www.stat.washington.edu/mclust/
http://www.ci.tuwien.ac.at/~leisch/FlexMix/

Xtof.


Dominique Emmanuel wrote:

>Hello,
>
>I am looking for documentation (PDF ?) about doing clustering with the EM Algorithm on R... I'm sorry for this question which may be trivial...
>
>Thanks...
>
>Dominique
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From sundar.dorai-raj at pdf.com  Wed May 25 10:13:46 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 25 May 2005 01:13:46 -0700
Subject: [R] DOING CLUSTERING WITH THE EM ALGORITHM
In-Reply-To: <64F4CED51696BE489C8547E1FEC4ED5C8740BC@lan-123.79.68.195.rev.fr.colt.net>
References: <64F4CED51696BE489C8547E1FEC4ED5C8740BC@lan-123.79.68.195.rev.fr.colt.net>
Message-ID: <429433BA.2080308@pdf.com>



Dominique Emmanuel wrote:
> Hello,
> 
> I am looking for documentation (PDF ?) about doing clustering with the EM Algorithm on R... I'm sorry for this question which may be trivial...
> 
> Thanks...
> 
> Dominique
> 

Your question is a bit vague, but the mclust package might be what 
you're looking for. From ?Mclust:

      Clustering via EM initialized by hierarchical clustering for
      parameterized Gaussian mixture models. The number of clusters and
      the clustering model is chosen to maximize the BIC.

HTH,

--sundar



From ligges at statistik.uni-dortmund.de  Wed May 25 10:30:51 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 10:30:51 +0200
Subject: [R] Calling R from R and specifying "wait until script
	is	finished"
In-Reply-To: <p06210209beb7ff02682b@[128.115.153.6]>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF357CB@lbmsg002.fbn-nbf.local>
	<p06210209beb7ff02682b@[128.115.153.6]>
Message-ID: <429437BB.1080702@statistik.uni-dortmund.de>

Don MacQueen wrote:

> I don't know about efficient, but here is a way that I find to be 
> practical, with around 100 R scripts.

I don't see a reason why your functions are writing character strings 
using cat() into files, and after that processing those files using 
system calls and tools (e.g. grep) which are not available on all 
platforms.

 From my point of view the R way is to save the state into R objects 
(e.g. a vector of status codes with one element for each "R script").
Then you can easily use R functions to claculate on those objects.

Uwe Ligges



> I create a master R script (I call it "Runall.r"). It begins like this:
> 
> ## Execute me with  R --save < Runall.r >& Runall.log
> 
> hc <- TRUE
> if (hc) sink('Runall.out')
> 
> t0runall <- Sys.time()
> cat('========================================================\n')
> cat('Running script "Runall.r" at',format(t0runall),'\n')
> cat('========================================================\n')
> 
> msg <- try(source('ae-175.r')) ; rm.trymsg(msg)
> msg <- try(source('ae-235.r')) ; rm.trymsg(msg)
> msg <- try(source('ae-251.r')) ; rm.trymsg(msg)
> msg <- try(source('ae-331.r')) ; rm.trymsg(msg)
> msg <- try(source('ae-332.r')) ; rm.trymsg(msg)
> msg <- try(source('ae-491.r')) ; rm.trymsg(msg)
> msg <- try(source('ae-695.r')) ; rm.trymsg(msg)
> msg <- try(source('ae-801.r')) ; rm.trymsg(msg)
> 
> ## and so on, for as many scripts as I want to run
> ## although I constructed the list of scripts to run by hand, it could 
> easily be done
> ## as a loop, with the script names constructed from the loop index
> 
> ## the script ends with:
> 
> cat('========================================================\n')
> t1 <- Sys.time()
> cat('[Runall.r] Elapsed 
> time',format(t1-t0runall),attributes(t1-t0runall)$units,'\n')
> 
> if (hc) {
>   cat('Done\n')
>   sink()
> 
>   system('grep failed Runall.out > Runall.info')
>   cat('\n')
>   system('grep succeeded Runall.out >> Runall.info')
>   cat('\n')
> 
>   cat('See files Runall.out and Runall.info\n')
>   cat('Done\n')
> }
> 
> ####
> #### The function rm.trymsg() is this:
> 
>  rm.trymsg <- function(msg) {
>   if (class(msg)=='try-error') {
>     cat('============',tblid,'failed =============\n')
>     return(FALSE)
>   }
>   if (data.class(msg)=='list'& unlist(msg)[[1]]=='bad.table') {
>     cat('============',tblid,'failed ===== bad.table ==========\n')
>     return(FALSE)
>   }
>   cat('=========',tblid,'succeeded =========\n')
>   TRUE
> }
> 
> ## and the purpose of using try() and rm.trymsg() is to let the job 
> continue if an error occurs in one of
> ## the scripts. Note, however, that the text strings "bad.table" and 
> "tblid" are unique to the task I am doing, and would not
> ## work in general.
> 
> At 8:51 PM -0400 5/21/05, Lapointe, Pierre wrote:
> 
>> Hello,
>>
>> Let's say I have 50 R scripts to run.  What would be the most 
>> efficient way
>> to run them?
>>
>> I thought I could do multiple Rterms in a DOS batch file:
>>
>> Ex:
>> Rterm <1.R> 1.txt
>> Rterm <2.R> 2.txt
>> ...
>> Rterm <50.R> 50.txt
>>
>> However, I'm afraid they will all open at the same time.   I know I could
>> pause the batch file with something like:
>>
>> PING 1.1.1.1 -n 1 -w 60000 >NUL  (to delay 60 seconds)
>>
>> But that would require that I know how long each of my scripts take.
>>
>> Is there an easier way?  Something like calling R from R and 
>> specifying that
>> the script has to be finished before continuing.
>>
>> Thanks
>>
>> Pierre Lapointe
>>
>>
>>
>> *********************************************************************************** 
>>
>> AVIS DE NON-RESPONSABILITE:\ Ce document transmis par 
>> courri...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
>



From francoisromain at free.fr  Wed May 25 10:34:26 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 25 May 2005 10:34:26 +0200
Subject: [R] DOING CLUSTERING WITH THE EM ALGORITHM
In-Reply-To: <64F4CED51696BE489C8547E1FEC4ED5C8740BC@lan-123.79.68.195.rev.fr.colt.net>
References: <64F4CED51696BE489C8547E1FEC4ED5C8740BC@lan-123.79.68.195.rev.fr.colt.net>
Message-ID: <42943892.9060505@free.fr>

Le 25.05.2005 09:53, Dominique Emmanuel a ??crit :

>Hello,
>
>I am looking for documentation (PDF ?) about doing clustering with the EM Algorithm on R... I'm sorry for this question which may be trivial...
>
>Thanks...
>
>Dominique
>  
>
Hello,

You don't need to shout the subject !
You are looking for the mclust package which is available on CRAN.
Moreover, trying to search 'EM' on the RSiteSeach ( 
http://finzi.psych.upenn.edu/search.html )
leads to it (first hit)
You can even type :
 > RSiteSearch('EM')

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ligges at statistik.uni-dortmund.de  Wed May 25 10:46:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 10:46:40 +0200
Subject: [R] website reference for building R packages
In-Reply-To: <42929EA3.6020708@columbia.edu>
References: <BAY10-F506FC10C31172C8BA726ADD60D0@phx.gbl>
	<42929EA3.6020708@columbia.edu>
Message-ID: <42943B70.6050107@statistik.uni-dortmund.de>

Suresh Krishna wrote:

> 
> it is the first link if you type "making packages" into the google 
> search box here:
> 
> http://maths.newcastle.edu.au/~rking/R/


Yes, please look into the archives as the posting guide asks you to do!


> -s.
> 
> 
> Laura Holt wrote:
> 
>> Hi R People:
>>
>> A few weeks ago, someone put a link to a website for "how to" for 
>> building R packages.  It was very nice.


Writing R Extensions is an *complete* and *up to date* documentation for 
this task. Package management is being steadily improved.
Really, I recommend to use "Writing R Extensions"!

If you are working on Windows, you might want to look into the R 
Administration and Installation manual as well which descibes how to 
collect and set up the required tools...

Uwe Ligges


>> But of course, I have misplaced the link.  Does anyone still have 
>> that, please?
>>
>> It was someone from the University of Chicago, I believe.
>>
>> Thanks in advance.
>>
>> Sincerely,
>> Laura Holt
>> mailto: lauraholt_983 at hotmail.com
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From sean.connolly at jcu.edu.au  Wed May 25 11:07:18 2005
From: sean.connolly at jcu.edu.au (Sean Connolly)
Date: Wed, 25 May 2005 19:07:18 +1000
Subject: [R] question: corCAR1 in lme
Message-ID: <6.2.1.2.0.20050525182656.0376f590@mail.jcu.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/78642267/attachment.pl

From mikko.pakkanen at helsinki.fi  Wed May 25 11:43:29 2005
From: mikko.pakkanen at helsinki.fi (Mikko Pakkanen)
Date: Wed, 25 May 2005 12:43:29 +0300
Subject: [R] Problem with systemfit 0.7-3 and transformed variables
Message-ID: <1117014209.429448c1bb78f@www2.helsinki.fi>

The 'systemfit' function in systemfit 0.7-3 CRAN package seems to have a
problem with formulas that contain transformed (eg. log) variables. If I
have my data in a data frame, apparently systemfit doesn't "pass" the
information of where the variables should be taken to the transforming function.

I'm not entirely sure if this is a bug or just a limitation, I was just
surprised when I attempted to estimate a model, which I'd previously
estimated with OLS using 'lm', with 2SLS using 'systemfit' and it didn't
accept those transformations like 'lm' does.

Here's an example: this is, of course, OK:

> data(kmenta)
> demand <- q ~ p + d
> instr <- ~ d + f
> fit1 <- systemfit("2SLS", eqns=list(demand), inst=instr, data=kmenta)

But, now if I'd like to estimate a model with logarithm of p as a regressor,
an error occurs: 

> demand2 <- q ~ log(p) + d
> fit2 <- systemfit("2SLS", eqns=list(demand2), inst=instr, data=kmenta)
Error in log(p) : Object "p" not found

However, estimating the same formula with OLS using the regular 'lm' is OK:
> fit2.ols <- lm(demand2, kmenta)

Transforming an instrument causes the same error too:

> instr2 <- ~ log(d) + f
> fit3 <- systemfit("2SLS", eqns=list(demand), inst=instr2, data=kmenta)
Error in log(d) : Object "d" not found

One could certainly just create those transformed variables to avoid the
problem, but it would be much more convenient, if it wasn't necessary,
especially if several regressors are involved.

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R 

Regards,

-Mikko Pakkanen



From ligges at statistik.uni-dortmund.de  Wed May 25 12:26:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 12:26:36 +0200
Subject: [R] colors and palettes and things...
In-Reply-To: <2088.128.193.139.69.1116862527.squirrel@www.forestinformatics.com>
References: <2088.128.193.139.69.1116862527.squirrel@www.forestinformatics.com>
Message-ID: <429452DC.7050100@statistik.uni-dortmund.de>

Jeff D. Hamann wrote:

> After trying to find if there was a color picker in the FAQs and the help,
> I thought I would send a post here. I was overwhelmed with all the
> wonderful color choices R has predefined (discovered after typing in
> colors()) but can't figure out what they all (by name) look like. Is there
> a color picker or some other method to display all those colors next to
> the name?
> 
> I think I can put together palettes, but another question I have then
> regards the building of palettes (a list of variable length I can select
> or create myself other than the ones defined by Palette) so I can pass
> these colors into functions instead of having to predefine a bunch of
> colors myself or use the predefined colors like terrain.colors(n)?
> 
> Are there groups of colors in the colors() that I can group together to
> make some nice palettes for drawing barplots, etc?
> 
> Thanks,
> Jeff.
> 
> 


I'd like to point out some packages or code snippets that might be 
useful for you:

  # base package "grDevices":
  ?colorRamp
  example(colorRamp)

  # CRAN package "colorspace":
  library(help = colorspace)

  # CRAN package "RColorBrewer":
  library(RColorBrewer)
  ?brewer.pal
  example(brewer.pal)

  # see also the last two examples by Martin Maechler in
  # CRAN package scatterplot3d:
  library(scatterplot3d)
  ?scatterplot3d
  example(scatterplot3d)

Uwe Ligges



From ahenningsen at email.uni-kiel.de  Wed May 25 12:50:02 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 25 May 2005 12:50:02 +0200
Subject: [R] Problem with systemfit 0.7-3 and transformed variables
In-Reply-To: <1117014209.429448c1bb78f@www2.helsinki.fi>
References: <1117014209.429448c1bb78f@www2.helsinki.fi>
Message-ID: <200505251250.02621.ahenningsen@email.uni-kiel.de>

On Wednesday 25 May 2005 11:43, Mikko Pakkanen wrote:
> The 'systemfit' function in systemfit 0.7-3 CRAN package seems to have a
> problem with formulas that contain transformed (eg. log) variables. If I
> have my data in a data frame, apparently systemfit doesn't "pass" the
> information of where the variables should be taken to the transforming
> function.
>
> I'm not entirely sure if this is a bug or just a limitation, I was just
> surprised when I attempted to estimate a model, which I'd previously
> estimated with OLS using 'lm', with 2SLS using 'systemfit' and it didn't
> accept those transformations like 'lm' does.
>
> Here's an example: this is, of course, OK:
> > data(kmenta)
> > demand <- q ~ p + d
> > instr <- ~ d + f
> > fit1 <- systemfit("2SLS", eqns=list(demand), inst=instr, data=kmenta)
>
> But, now if I'd like to estimate a model with logarithm of p as a
> regressor,
>
> an error occurs:
> > demand2 <- q ~ log(p) + d
> > fit2 <- systemfit("2SLS", eqns=list(demand2), inst=instr, data=kmenta)
>
> Error in log(p) : Object "p" not found
>
> However, estimating the same formula with OLS using the regular 'lm' is OK:
> > fit2.ols <- lm(demand2, kmenta)
>
> Transforming an instrument causes the same error too:
> > instr2 <- ~ log(d) + f
> > fit3 <- systemfit("2SLS", eqns=list(demand), inst=instr2, data=kmenta)
>
> Error in log(d) : Object "d" not found
>
> One could certainly just create those transformed variables to avoid the
> problem, but it would be much more convenient, if it wasn't necessary,
> especially if several regressors are involved.

We did not notice this shortcoming of systemfit() so far. Unfortunately, I 
don't have the time in the next few days to look into the code and figure out 
how to enable transformed variables. I suggest that you either create 
transformed variables by hand or you modify the systemfit code to enable this 
and send us the patch. I prefer the second :-) (that's the philosophy of 
open-source software like R: useRs become developeRs).

Best wishes,
Arne

> > version
>
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
>
> Regards,
>
> -Mikko Pakkanen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From dimitris.rizopoulos at med.kuleuven.be  Wed May 25 12:49:29 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 25 May 2005 12:49:29 +0200
Subject: [R] question: corCAR1 in lme
References: <6.2.1.2.0.20050525182656.0376f590@mail.jcu.edu.au>
Message-ID: <002601c56117$6d0bd040$0540210a@www.domain>

Hi Sean,

As you already guessed and as also the help-page ACF.lme() indicates: 
"The autocorrelation function is useful for investigating serial 
correlation models for equally spaced data."

Since you don't have equally spaced time points, you should use: 
"corCAR1(form = ~ Time | TankID)", which would indicate that "Time" is 
the position variable and *not* the order of the observations (i.e., 
when you use "form = ~ 1 | TankID").

Since you assume continuous time and you also fit a random-intercepts 
model, I think that the correct tool to use is the semi-variogram 
(i.e., look at "?Variogram()") and maybe you could also consider other 
correlation structures such as corExp() or corGaus().

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Sean Connolly" <sean.connolly at jcu.edu.au>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, May 25, 2005 11:07 AM
Subject: [R] question: corCAR1 in lme


Hello all,

I am trying to use lme to examine how a response variable (Chla) 
changes
over time in different treatments (2 Temp & 2 Light levels).  Within 
each
treatment combination, there are two replicate tanks (each with unique
TankID) with coral fragments in them.  All tanks are subject to the 
same
environment until Time=0, when treatments are imposed, and Chla is 
measured
for each tank at six times, including Time 0 just as the experiment
commences. The model is:

Chla.1 <- lme(Chla ~ Temp*Light* Time - Temp*Light, random = ~1 | 
TankID,
method="ML")

The reasoning here is that each tank?s intercept (Chla at Time 0) is a
random draw from a common distribution regardless of treatment, but 
that
the trend in Chla over time may vary among treatment combinations. 
Based
on the help files, two separate threads from the archives, and the 
Pinheiro
and Bates nlme 3.0 manual, I became confused about which of two ways 
to
check for a first-order temporal autocorrelation:

Chla.1b <- lme(Chla ~ Temp*Light* Time - Temp*Light, random = ~1 | 
TankID,
corr = corCAR1(form = ~Time | TankID), method="ML")

Chla.1c <- lme(Chla ~ Temp*Light* Time - Temp*Light, random = ~1 | 
TankID,
corr = corCAR1(form = ~1 | TankID), method="ML")

Comparing these fits with inspection of 
plot(ACF(chla.model1),alpha=0.05)
suggests to me that there are problems with both of my attempts.  For 
the
ACF plot, the correlation at lag 1 is about ?0.3, and sticks out 
beyond the
confidence limits.  By contrast, the two models' correlation 
parameters are
not negative (phi = +0.13 and ~0 respectively), and the log-likelihood
values are identical to the original model, suggesting no evidence of
autocorrelation.  Our times are not equally spaced (they vary from 5-8 
days
apart), and I gather than ACF assumes they are, but my troubleshooting
(summarized below) suggests to me that my problem is bigger than this. 
I
think I have not used corCAR1 properly, and am hoping someone can 
point me
in the right direction.

Attempted troubleshooting:

1.  To check whether the discrepancy between ACF and the lme fits was 
due
entirely to the unequal spacing of measurements, I created a bogus 
time
variable (Time2) that was equally spaced (running from 0 to 5 in steps 
of
1).  I then re-fit all of the above models with Time2 replacing Time 
in the
function calls, and get the same kinds of problems (phi ~ 0 in the 
model
fits, while ACF plot suggests a negative correlation at lag 1).

2.  Still using the bogus equally-spaced time variable, I replaced 
corCAR1
with corAR1.  Now, the two different specifications of ?form? yield
identical parameter estimates and MLLs; the estimates of phi agree 
with
those from the ACF plot; and the models actually do fit better than 
the
equivalent model without autocorrelation.

Any advice would be greatly appreciated.

Sincerely,
Sean Connolly
[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From KINLEY_ROBERT at Lilly.com  Wed May 25 13:05:57 2005
From: KINLEY_ROBERT at Lilly.com (Robert Kinley)
Date: Wed, 25 May 2005 12:05:57 +0100
Subject: [R] time-ordered object list
Message-ID: <OF24F212A8.9F5ACBCE-ON8025700C.003BCBFB@EliLilly.lilly.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/3e8f311b/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed May 25 13:21:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 13:21:15 +0200
Subject: [R] time-ordered object list
In-Reply-To: <OF24F212A8.9F5ACBCE-ON8025700C.003BCBFB@EliLilly.lilly.com>
References: <OF24F212A8.9F5ACBCE-ON8025700C.003BCBFB@EliLilly.lilly.com>
Message-ID: <42945FAB.1090601@statistik.uni-dortmund.de>

Robert Kinley wrote:

> It's often useful to view objects in time order.
> 
> In Splus I can do this with 
> 
> 
>>objects.summary(order = "dataset.date")
> 
> 
> which delivers this sort of thing ...
> 
>                 data.class storage.mode    extent object.size dataset.date 
> 
>   reference      data.frame         list    25 x 4        1700 2004.09.13 
> 15:43
>           x      data.frame         list    15 x 4        1175 2004.09.13 
> 15:43
>       lower         numeric       double       100         841 2004.10.05 
> 11:10
>       upper         numeric       double       100         841 2004.10.05 
> 11:10
>     barnard        function     function        12       20013 2005.04.08 
> 13:09
>        sim7        function     function         5        3657 2005.04.14 
> 15:36
> .Last.fixed       character    character         1          52 2005.04.14 
> 15:38
>      runsim        function     function         6        3952 2005.04.14 
> 15:38
>   last.dump            list         list         8        1186 2005.05.12 
> 11:57
> 
> 
> How can one obtain something similar with  R ?       [ R 2.1.0 / windows 
> 2000 ]

You cannot. Objects in R do not have any timestamp attributes.

Uwe Ligges



>                 cheers 
>                                 Bob Kinley
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From cg.pettersson at evp.slu.se  Wed May 25 14:25:16 2005
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Wed, 25 May 2005 14:25:16 +0200
Subject: [R] No ~ in JGR
Message-ID: <42946EAC.2020402@evp.slu.se>

R2.1.0
JGR 1.2
W2k

Hello all!
I??ve just installed JGR on my both R-equipped computers and am very 
pleased with the look and functionality.

Except in one, very important, way.

I can??t figure out how to get the ~ sign from the keyboard to the 
console. Copying it from old code works fine. Using the traditional GUI 
works as usual.

I have a Swedish keyboard layout, where ~ shares key with ?? and ^, all 
reguiring a space after the hit to produce the sign on the screen.

The other signs from the key works, but AltGr + ~ followed by space just 
gives a blank. What do I do wrong?

/CG


-- 
CG Pettersson MSci. PhD.Stud.
Swedish University of Agricultural Sciences (SLU)
Dep. of Ecology and Crop production sciences (EVP).
http://www.slu.se/
cg.pettersson at evp.slu.se



From navarre_sabine at yahoo.fr  Wed May 25 14:05:00 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Wed, 25 May 2005 14:05:00 +0200 (CEST)
Subject: [R] plot 3D
Message-ID: <20050525120500.25868.qmail@web26604.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/9d0df091/attachment.pl

From charles.edwin.white at us.army.mil  Wed May 25 14:13:27 2005
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Wed, 25 May 2005 08:13:27 -0400
Subject: [R] R-devel 2.1.0 dependencies for RPMS under Fedora Core 3
Message-ID: <8BAEC5E546879B4FAA536200A292C6145724FC@AMEDMLNARMC135.amed.ds.army.mil>

The R-devel RPMS has a number of dependencies on Fedora Core development
tools. When the RPMS is installed, the dependencies are resolved by
automatically downloading and installing the latest version of the
development tools. Looking at the README file in the fc3 directory, I
was surprised by this behavior. I would recommend either changing the
description of the file in the README or the behavior of the RPMS.
Selected text from the README follows:

These RPMS are designed to run on Fedora Core 3 (Heidelberg).  

....
R-devel: This is a stub package that contains only the documentation
file
         on "Writing R Extensions" (R-exts.pdf).  You should install
this
         if you wish to compile packages from source.

Chuck White

PS: I've only been using Linux for a couple of weeks; I've used R under
Windows for more than three years. I probably won't use the RPMS again
since I would prefer to use shared libraries.



From f.harrell at vanderbilt.edu  Wed May 25 14:20:28 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 25 May 2005 08:20:28 -0400
Subject: [R] obtaining first and last record for rows with same identifier
In-Reply-To: <BAY103-F131FB65C0172950EA34918A60E0@phx.gbl>
References: <BAY103-F131FB65C0172950EA34918A60E0@phx.gbl>
Message-ID: <42946D8C.5050206@vanderbilt.edu>

Francisco J. Zagmutt wrote:
> If you want to obtain a data frame you can use the functions head and 
> tail like:
> 
> dat=data.frame(id=rep(1:5,3),num=rnorm(15), num2=rnorm(15))#Creates data 
> frame with id
> last=do.call("rbind",by(dat,dat$id,tail,1))#Selects the last observation 
> for each id
> first=do.call("rbind",by(dat,dat$id,head,1))#Selects the first 
> observation for each id
> newdat=rbind(first,last)#Joins data
> newdat=newdat[order(newdat$id),]#sorts data by id
> 
> Notice that rownames will give you the original row location of the 
> observations selected
> 
> I hope this helps
> 
> Francisco
> 
. . .

You might also look at section 4.3 of
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/RS/sintro.pdf
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From lyhin at netvigator.com  Wed May 25 13:09:00 2005
From: lyhin at netvigator.com (Dr L. Y Hin)
Date: Wed, 25 May 2005 19:09:00 +0800
Subject: [R] Can simulation involving random number generation be segmented?
Message-ID: <000001c56125$06a97420$0202a8c0@yourgk68c57jh8>

Dear all,
Apologies for this pedantic question that only arise when there is hardware
limitation.
Setting: R 2.1.0 for windows xp sp2.
Scenario:
To generate 1000 samples using rnorm for a simulation activity.
Background:
The simulation activity requires so much memory resources that generating
200 samples
clogs up the PF usage as indicated in the Windows Task Manager.
Therefore, short of implementing the simulation on a computer with more
resources,
the alternative is to generate the 1000 samples in 5 separate runs,
each generating 200 samples, closing the R window and re-opening between
runs.
Question to be addressed:
To maintain consistency and ensure reproducibility of the simulation
results, the 1000 samples
generated in one single run should be indentical to the 5x200 samples
generated on 5 separate
runs.
While such consistency can be ensured using set.seed()  in the case of one
single run, in the case
where 5 separate runs are performed, can we do the following to ensure
identical samples being
generated?
1. In the first run, specify the seed by, say, set.seed(1)

2. At the end of the first run, store the .Random.seed by the following
manner:
saved.seed.1<-.Random.seed

3. At the beginning of the second run, assign the saved.seed.1 to
.Random.seed as follows:
.Random.seed<-saved.seed.1

4. At the end of the first run, store the new .Random.seed by the following
manner:
saved.seed.2<-.Random.seed

5. At the beginning of the second run, assign the saved.seed.2 to
.Random.seed as follows:
.Random.seed<-saved.seed.2

This is repeated until 5 runs are completed.

Will the paths of random number generation be identical in these two
approaches? If not, is there
a way to ensure this?

Apologies again for this long-winded inquiry.

Thank you.
Best
Lin



From jgu at codan.dk  Wed May 25 14:30:35 2005
From: jgu at codan.dk (Jim Gustafsson)
Date: Wed, 25 May 2005 14:30:35 +0200
Subject: [R] Linear system
Message-ID: <OFEBD26836.8D959867-ONC125700C.00445498-C125700C.0044BBA4@codan.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/1a09c23a/attachment.pl

From rvalliant at survey.umd.edu  Wed May 25 14:39:31 2005
From: rvalliant at survey.umd.edu (Richard Valliant)
Date: Wed, 25 May 2005 08:39:31 -0400
Subject: [R] setting elements of matrix
Message-ID: <s29439ee.020@SURVEYGWIA.UMD.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/7861e27b/attachment.pl

From sundar.dorai-raj at pdf.com  Wed May 25 14:46:13 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 25 May 2005 05:46:13 -0700
Subject: [R] setting elements of matrix
In-Reply-To: <s29439ee.020@SURVEYGWIA.UMD.EDU>
References: <s29439ee.020@SURVEYGWIA.UMD.EDU>
Message-ID: <42947395.5070408@pdf.com>



Richard Valliant wrote:
> This is, no doubt, an easy problem, but I need help. I want to set
> values in a matrix based on entries in a vector. Here is an example:
>  
> D is an r x m matrix initilized to all zeroes. r=6, m=5 in the
> example.
>  
> Ro <- c(1, 3, 4, 4, 6)
> Co <- c(1, 2, 3, 4, 5)
>  
> I want to set D[Ro[j], Co[j] ] to 1. So, D becomes
>  
> 1 0 0 0 0
> 0 0 0 0 0
> 0 1 0 0 0
> 0 0 1 1 0
> 0 0 0 0 0
> 0 0 0 0 1
>  
> A loop works but is slow, which matters since this is being done many
> times in a simulation.
>  


How about:

D <- matrix(0, 6, 5)
D[cbind(Ro, Co)] <- 1

--sundar



From HDoran at air.org  Wed May 25 14:49:14 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 25 May 2005 08:49:14 -0400
Subject: [R] Linear system
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409174347@dc1ex2.air.org>

Yes, solving systems of linear equations is a strength of R and is very
easy. Doug Bates has a nice article in a prior version of R News showing
optimal methods for doing this in R. Here is an example using your data:

X <- matrix(c(353,45,29,45,29,3,29,3,4), ncol=3)
y <- c(79,5,8)

solve(crossprod(X))%*%crossprod(X,y)

See Doug's article at the following link:

http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jim Gustafsson
Sent: Wednesday, May 25, 2005 8:31 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Linear system


Dear R-help

I have a problem solving a linear system like

353a+45b+29c=79
45a+29b+3c=5
29a+3b+4c=8

Is there any way of doing this in R?

Best Regards
Jim


------------------------------------------------------------------------
------
This e-mail and any attachment may be confidential and may also be
privileged.
If you are not the intended recipient, please notify us immediately and
then delete this e-mail and any attachment without retaining copies or
disclosing the contents thereof to any other person.
Thank you.
------------------------------------------------------------------------
------
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed May 25 14:52:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 14:52:17 +0200
Subject: [R] Linear system
In-Reply-To: <OFEBD26836.8D959867-ONC125700C.00445498-C125700C.0044BBA4@codan.dk>
References: <OFEBD26836.8D959867-ONC125700C.00445498-C125700C.0044BBA4@codan.dk>
Message-ID: <42947501.4000707@statistik.uni-dortmund.de>

Jim Gustafsson wrote:

> Dear R-help
> 
> I have a problem solving a linear system like
> 
> 353a+45b+29c=79
> 45a+29b+3c=5
> 29a+3b+4c=8
 >
> Is there any way of doing this in R?


You already said it yourself ("solve"):


A <- rbind(c(353, 45, 29), c(45, 29, 3), c(29, 3, 4))
solve(A, c(79, 5, 8))
# [1]  0.1784625 -0.1924954  0.8505186

Uwe Ligges



> Best Regards
> Jim
> 
> 
> ------------------------------------------------------------------------------
> This e-mail and any attachment may be confidential and may also be privileged.
> If you are not the intended recipient, please notify us immediately and then
> delete this e-mail and any attachment without retaining copies or disclosing
> the contents thereof to any other person.
> Thank you.
> ------------------------------------------------------------------------------
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Wed May 25 14:53:18 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 25 May 2005 05:53:18 -0700
Subject: [R] Linear system
In-Reply-To: <OFEBD26836.8D959867-ONC125700C.00445498-C125700C.0044BBA4@codan.dk>
References: <OFEBD26836.8D959867-ONC125700C.00445498-C125700C.0044BBA4@codan.dk>
Message-ID: <4294753E.1070807@pdf.com>



Jim Gustafsson wrote:
> Dear R-help
> 
> I have a problem solving a linear system like
> 
> 353a+45b+29c=79
> 45a+29b+3c=5
> 29a+3b+4c=8
> 
> Is there any way of doing this in R?
> 
> Best Regards
> Jim
> 


Use ?solve:

X <- matrix(c(353, 45, 29,
                45, 29,  3,
                29,  3,  4), 3, 3, byrow = TRUE)
y <- c(79, 5, 8)
b <- solve(X, y)

Have you read the posting guide? In particular, have you tried simple 
queries using help.search or the R archives? This question has been 
answered many times.

HTH,

--sundar



From jfox at mcmaster.ca  Wed May 25 14:54:20 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 25 May 2005 08:54:20 -0400
Subject: [R] Problem with systemfit 0.7-3 and transformed variables
In-Reply-To: <1117014209.429448c1bb78f@www2.helsinki.fi>
Message-ID: <20050525125420.RLSD25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Mikko,

You might try the tsls function in the sem package (which also does 2SLS).

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mikko Pakkanen
> Sent: Wednesday, May 25, 2005 4:43 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Problem with systemfit 0.7-3 and transformed variables
> 
> The 'systemfit' function in systemfit 0.7-3 CRAN package 
> seems to have a problem with formulas that contain 
> transformed (eg. log) variables. If I have my data in a data 
> frame, apparently systemfit doesn't "pass" the information of 
> where the variables should be taken to the transforming function.
> 
> I'm not entirely sure if this is a bug or just a limitation, 
> I was just surprised when I attempted to estimate a model, 
> which I'd previously estimated with OLS using 'lm', with 2SLS 
> using 'systemfit' and it didn't accept those transformations 
> like 'lm' does.
> 
> Here's an example: this is, of course, OK:
> 
> > data(kmenta)
> > demand <- q ~ p + d
> > instr <- ~ d + f
> > fit1 <- systemfit("2SLS", eqns=list(demand), inst=instr, 
> data=kmenta)
> 
> But, now if I'd like to estimate a model with logarithm of p 
> as a regressor, an error occurs: 
> 
> > demand2 <- q ~ log(p) + d
> > fit2 <- systemfit("2SLS", eqns=list(demand2), inst=instr, 
> data=kmenta)
> Error in log(p) : Object "p" not found
> 
> However, estimating the same formula with OLS using the 
> regular 'lm' is OK:
> > fit2.ols <- lm(demand2, kmenta)
> 
> Transforming an instrument causes the same error too:
> 
> > instr2 <- ~ log(d) + f
> > fit3 <- systemfit("2SLS", eqns=list(demand), inst=instr2, 
> data=kmenta)
> Error in log(d) : Object "d" not found
> 
> One could certainly just create those transformed variables 
> to avoid the problem, but it would be much more convenient, 
> if it wasn't necessary, especially if several regressors are involved.
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    1.0            
> year     2005           
> month    04             
> day      18             
> language R 
> 
> Regards,
> 
> -Mikko Pakkanen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed May 25 14:56:22 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 14:56:22 +0200
Subject: [R] Can simulation involving random number generation be
	segmented?
In-Reply-To: <000001c56125$06a97420$0202a8c0@yourgk68c57jh8>
References: <000001c56125$06a97420$0202a8c0@yourgk68c57jh8>
Message-ID: <429475F6.8040807@statistik.uni-dortmund.de>

Dr L. Y Hin wrote:

> Dear all,
> Apologies for this pedantic question that only arise when there is hardware
> limitation.
> Setting: R 2.1.0 for windows xp sp2.
> Scenario:
> To generate 1000 samples using rnorm for a simulation activity.
> Background:
> The simulation activity requires so much memory resources that generating
> 200 samples
> clogs up the PF usage as indicated in the Windows Task Manager.
> Therefore, short of implementing the simulation on a computer with more
> resources,
> the alternative is to generate the 1000 samples in 5 separate runs,
> each generating 200 samples, closing the R window and re-opening between
> runs.
> Question to be addressed:
> To maintain consistency and ensure reproducibility of the simulation
> results, the 1000 samples
> generated in one single run should be indentical to the 5x200 samples
> generated on 5 separate
> runs.
> While such consistency can be ensured using set.seed()  in the case of one
> single run, in the case
> where 5 separate runs are performed, can we do the following to ensure
> identical samples being
> generated?
> 1. In the first run, specify the seed by, say, set.seed(1)
> 
> 2. At the end of the first run, store the .Random.seed by the following
> manner:
> saved.seed.1<-.Random.seed
> 
> 3. At the beginning of the second run, assign the saved.seed.1 to
> .Random.seed as follows:
> .Random.seed<-saved.seed.1
> 
> 4. At the end of the first run, store the new .Random.seed by the following
> manner:
> saved.seed.2<-.Random.seed
> 
> 5. At the beginning of the second run, assign the saved.seed.2 to
> .Random.seed as follows:
> .Random.seed<-saved.seed.2
> 
> This is repeated until 5 runs are completed.
> 
> Will the paths of random number generation be identical in these two
> approaches? 

Yes.

Uwe Ligges

If not, is there
> a way to ensure this?
> 
> Apologies again for this long-winded inquiry.
> 
> Thank you.
> Best
> Lin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ahenningsen at email.uni-kiel.de  Wed May 25 15:02:10 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 25 May 2005 15:02:10 +0200
Subject: [R] Linear system
In-Reply-To: <OFEBD26836.8D959867-ONC125700C.00445498-C125700C.0044BBA4@codan.dk>
References: <OFEBD26836.8D959867-ONC125700C.00445498-C125700C.0044BBA4@codan.dk>
Message-ID: <200505251502.10720.ahenningsen@email.uni-kiel.de>

On Wednesday 25 May 2005 14:30, Jim Gustafsson wrote:
> Dear R-help
>
> I have a problem solving a linear system like
>
> 353a+45b+29c=79
> 45a+29b+3c=5
> 29a+3b+4c=8
>
> Is there any way of doing this in R?


You can write this equation system in matrix form:
M * x = y

with 
x = ( a, b, c )
M = ( 353, 45, 29,
       45, 29,  3,
       29,  3,  4 )
y = ( 79, 5, 8 )

you can solve the system by 
x = M^(-1) * y

In R you can do this by
R> M = matrix( c( 353, 45, 29, 45, 29,  3, 29,  3,  4 ), 
   ncol = 3, byrow = TRUE )
R> y <- c( 79, 5, 8 )
R> x <- solve( M ) %*% y 
or
R> x <- solve( M, y )

Please read a basic book about linear algebra.

Arne


> Best Regards
> Jim
>
>
> ---------------------------------------------------------------------------
>--- This e-mail and any attachment may be confidential and may also be
> privileged. If you are not the intended recipient, please notify us
> immediately and then delete this e-mail and any attachment without
> retaining copies or disclosing the contents thereof to any other person.
> Thank you.
> ---------------------------------------------------------------------------
>--- [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From ligges at statistik.uni-dortmund.de  Wed May 25 15:15:21 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 15:15:21 +0200
Subject: [R] plot 3D
In-Reply-To: <20050525120500.25868.qmail@web26604.mail.ukl.yahoo.com>
References: <20050525120500.25868.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <42947A69.8020309@statistik.uni-dortmund.de>

Navarre Sabine wrote:

> Is it possible to do a graphic in 3D?
>  
> This is my source: but this one is on 2D and at moment variables put on other variables, so it is difiicult to differentiate them visibly.

It is always the question whether 3D helps, but if you think so, you 
might want to take a look at one of the following packages (functions):

lattice (cloud)
rgl (rgl.speheres)
scatterplot3d (scatterplot3d)
djmrgl (plot3d) - Windows only



> plot(corresp(data,nf=2),xlim=c(-1,1),ylim=c(-1,1));

Your code does not help us to help.
What is corresp() (I guess you mean the one from package MASS)? What is 
data? Why is there a ";"?
Because of my first two questions, this code is NOT reproducible - the 
posting guide asks you to specify reproducible examples.

Uwe Ligges



> Thanks
> 
> Sabine
> 
> 
> 		
> ---------------------------------
> 
> ils, photos et vid??os !
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed May 25 15:28:39 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 May 2005 09:28:39 -0400
Subject: [R] time-ordered object list
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E893@usctmx1106.merck.com>

As Uwe said, you can't really do that in R.  S-PLUS can do that because it
stores each object as a separate file on disk, thus you can get time stamp
from the file.  In R everything is in the global environment that can be
saved to a single workspace (.RData by default).

There have been some discussions quite a while ago on how time stamping
objects in R can be done.  You may want to search the archive.  The mbvutils
package has tools for organizing objects, but I don't know if it provides
time stamps.

Andy

> From: Robert Kinley
> 
> It's often useful to view objects in time order.
> 
> In Splus I can do this with 
> 
> > objects.summary(order = "dataset.date")
> 
> which delivers this sort of thing ...
> 
>                 data.class storage.mode    extent object.size 
> dataset.date 
> 
>   reference      data.frame         list    25 x 4        
> 1700 2004.09.13 
> 15:43
>           x      data.frame         list    15 x 4        
> 1175 2004.09.13 
> 15:43
>       lower         numeric       double       100         
> 841 2004.10.05 
> 11:10
>       upper         numeric       double       100         
> 841 2004.10.05 
> 11:10
>     barnard        function     function        12       
> 20013 2005.04.08 
> 13:09
>        sim7        function     function         5        
> 3657 2005.04.14 
> 15:36
> .Last.fixed       character    character         1          
> 52 2005.04.14 
> 15:38
>      runsim        function     function         6        
> 3952 2005.04.14 
> 15:38
>   last.dump            list         list         8        
> 1186 2005.05.12 
> 11:57
> 
> 
> How can one obtain something similar with  R ?       [ R 
> 2.1.0 / windows 
> 2000 ]
> 
>                 cheers 
>                                 Bob Kinley
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From BENJAMIN.G.MCMURTRY at saic.com  Wed May 25 15:39:13 2005
From: BENJAMIN.G.MCMURTRY at saic.com (McMurtry, Benjamin G. )
Date: Wed, 25 May 2005 09:39:13 -0400
Subject: [R] Table Help
In-Reply-To: <sdavis2@mail.nih.gov>
Message-ID: <E2F63ACD0DE93C4A99AF7BF75837161C018AB304@rec-its-exs01.mail.saic.com>

I'm slightly confused with aggregate, will I need to loop it until it stops
returning sub lists?  Or would I somehow need to convert my username column
into a unique list and then do FUN=sum??

Please excuse me, I was just introduced to R yesterday to calculate
statistics of over 250 gigs of text we have lol.

Thanks for the reply!

Ben  

-----Original Message-----
From: Sean Davis
To: McMurtry, Benjamin G. 
Cc: 'r-help at stat.math.ethz.ch'
Sent: 5/24/2005 4:56 PM
Subject: Re: [R] Table Help

see ?aggregate....

Sean

On May 24, 2005, at 4:39 PM, McMurtry, Benjamin G. wrote:
>
>
> Is there  easy way using a feature like bind to sum all of collumn two

> where
> column 1 is the same?



From uofiowa at gmail.com  Wed May 25 16:08:30 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 25 May 2005 10:08:30 -0400
Subject: [R] precision problem
Message-ID: <3f87cc6d05052507083041d095@mail.gmail.com>

I have prices that I am finding difficult to compare with ==, > and >,
due to precision. For example: the numbers should match, with '==',
but they differ in the magnitude of 1e-14 due to bunch of calculations
that I run on them. Programming with java, I am used to implementing a
function that compares the difference between the numbers to a pre
determined precision factor. This could be very slow when  I have two
matrices of numbers that I could otherwise compare with a simple '==',
'>'  or '<' in R.
What is teh best solution for this problem?
Can I control the precision of ==, > and < without having to
reimplement the operations in a slow way?



From joseclaudio.faria at terra.com.br  Wed May 25 16:10:27 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Wed, 25 May 2005 11:10:27 -0300
Subject: [R] Contingency tables from data.frames
In-Reply-To: <971536df050524152110a35e60@mail.gmail.com>
References: <4293A4F2.7080301@terra.com.br>
	<971536df050524152110a35e60@mail.gmail.com>
Message-ID: <42948753.3030109@terra.com.br>

Gabor Grothendieck wrote:
> On 5/24/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> 
>>Dear list,
>>
>>I'm trying to do a set of generic functions do make contingency tables from
>>data.frames. It is just running "nice" (I'm learning R), but I think it can be
>>better.
>>
>>I would like to filter the data.frame, i.e, eliminate all not numeric variables.
>>And I don't know how to make it: please, help me.
>>
>>Below one of the my functions ('er' is a mention to EasieR, because I'm trying
>>to do a package for myself and the my students):
>>
>>#2. Tables from data.frames
>>#2.1---er.table.df.br (User define breaks and right)------------
>>er.table.df.br <- function(df,
>>                           breaks = c('Sturges', 'Scott', 'FD'),
>>                           right = FALSE) {
>>
>>  if (is.data.frame(df) != 'TRUE')
>>    stop('need "data.frame" data')
>>
>>  dim_df <- dim(df)
>>
>>  tmpList <- list()
>>
>>  for (i in 1:dim_df[2]) {
>>
>>    x <- as.matrix(df[ ,i])
>>    x <- na.omit(x)
>>
>>    k <- switch(breaks[1],
>>                'Sturges' = nclass.Sturges(x),
>>                'Scott'   = nclass.scott(x),
>>                'FD'      = nclass.FD(x),
>>                stop("'breaks' must be 'Sturges', 'Scott' or 'FD'"))
>>
>>    tmp      <- range(x)
>>    classIni <- tmp[1] - tmp[2]/100
>>    classEnd <- tmp[2] + tmp[2]/100
>>    R        <- classEnd-classIni
>>    h        <- R/k
>>
>>    # Absolut frequency
>>    f <- table(cut(x, br = seq(classIni, classEnd, h), right = right))
>>
>>    # Relative frequency
>>    fr <- f/length(x)
>>
>>    # Relative frequency, %
>>    frP <- 100*(f/length(x))
>>
>>    # Cumulative frequency
>>    fac <- cumsum(f)
>>
>>    # Cumulative frequency, %
>>    facP <- 100*(cumsum(f/length(x)))
>>
>>    fi   <- round(f, 2)
>>    fr   <- round(as.numeric(fr), 2)
>>    frP  <- round(as.numeric(frP), 2)
>>    fac  <- round(as.numeric(fac), 2)
>>    facP <- round(as.numeric(facP),2)
>>
>>    # Table
>>    res <- data.frame(fi, fr, frP, fac, facP)
>>    names(res) <- c('Class limits', 'fi', 'fr', 'fr(%)', 'fac', 'fac(%)')
>>    tmpList <- c(tmpList, list(res))
>>  }
>>  names(tmpList) <- names(df)
>>  return(tmpList)
>>}
>>
>>To try the function:
>>
>>#a) runing nice
>>y1=rnorm(100, 10, 1)
>>y2=rnorm(100, 58, 4)
>>y3=rnorm(100, 500, 10)
>>mydf=data.frame(y1, y2, y3)
>>#tbdf=er.table.df.br (mydf, breaks = 'Sturges', right=F)
>>#tbdf=er.table.df.br (mydf, breaks = 'Scott', right=F)
>>tbdf=er.table.df.br (mydf, breaks = 'FD', right=F)
>>print(tbdf)
>>
>>
>>#b) One of the problems
>>y1=rnorm(100, 10, 1)
>>y2=rnorm(100, 58, 4)
>>y3=rnorm(100, 500, 10)
>>y4=rep(letters[1:10], 10)
>>mydf=data.frame(y1, y2, y3, y4)
>>tbdf=er.table.df.br (mydf, breaks = 'Scott', right=F)
>>print(tbdf)
>>
> 
> 
> Try this:
> 
> sapply(my.data.frame, is.numeric)
> 
> Also you might want to look up:
> 
> ?match.arg
> ?stopifnot
> ?ncol
> ?sapply
> ?lapply
> 

Thanks Gabor, you suggestion solve my basic problem.
I'm working is same basic (but I think useful) functions
for begginiers.

Below you can see the set of functions:

#######################
#   EasyeR - Package  #
#######################

# Common function---------------------------------------------------------------
er.make.table <- function(x,
                           classIni,
                           classEnd,
                           h,
                           right) {
   # Absolut frequency
   f <- table(cut(x, br = seq(classIni, classEnd, h), right = right))

   # Relative frequency
   fr <- f/length(x)

   # Relative frequency, %
   frP <- 100*(f/length(x))

   # Cumulative frequency
   fac <- cumsum(f)

   # Cumulative frequency, %
   facP <<- 100*(cumsum(f/length(x)))

   fi   <- round(f, 2)
   fr   <- round(as.numeric(fr), 2)
   frP  <- round(as.numeric(frP), 2)
   fac  <- round(as.numeric(fac), 2)
   facP <- round(as.numeric(facP),2)

   # Table
   res <- data.frame(fi, fr, frP, fac, facP)
   names(res) <- c('Class limits', 'fi', 'fr', 'fr(%)', 'fac', 'fac(%)')
   return(res)
}


#1. Tables from vectors
#1.1---er.table.br (User define breaks and right)-------------------------------
er.table.br <- function(x,
                         breaks = c('Sturges', 'Scott', 'FD'),
                         right = FALSE) {

   if (is.factor(x) || mode(x) != 'numeric') stop('need numeric data')

   x <- na.omit(x)

   k <- switch(breaks[1],
               'Sturges' = nclass.Sturges(x),
               'Scott'   = nclass.scott(x),
               'FD'      = nclass.FD(x),
               stop("'breaks' must be 'Sturges', 'Scott' or 'FD'"))

   tmp      <- range(x)
   classIni <- tmp[1] - abs(tmp[2])/100
   classEnd <- tmp[2] + abs(tmp[2])/100
   R        <- classEnd-classIni
   h        <- R/k

   tbl <- er.make.table(x, classIni, classEnd, h, right)
   return(tbl)
}


#1.2---er.table.kr (User define the class number (k) and right)-----------------
er.table.kr <- function(x,
                         k,
                         right = FALSE) {

   if (is.factor(x) || mode(x) != 'numeric') stop('need numeric data')
   if ((k == '') || (k == ' ')) stop('k not defined')

   x <- na.omit(x)

   tmp      <- range(x)
   classIni <- tmp[1] - abs(tmp[2])/100
   classEnd <- tmp[2] + abs(tmp[2])/100
   R        <- classEnd-classIni
   h        <- R/k

   tbl <- er.make.table(x, classIni, classEnd, h, right)
   return(tbl)
}

#1.3---er.table.ier (User define the classIni, classEnd and right)--------------
er.table.ier <- function(x,
                          classIni,
                          classEnd,
                          right = FALSE) {

   if (is.factor(x) || mode(x) != 'numeric') stop('need numeric data')
   if ((classIni == '') || (classIni == ' ')) stop('classIni not defined')
   if ((classEnd == '') || (classEnd == ' ')) stop('classEnd not defined')

   x <- na.omit(x)

   tmp <- range(x)
   R   <- classEnd-classIni
   k   <- sqrt(abs(R))
   if (k < 5)  k <- 5
   h   <- R/k

   tbl <- er.make.table(x, classIni, classEnd, h, right)
   return(tbl)
}


#1.4---er.table.all (User define classIni, ClassEnd, h and right)---------------
er.table.iehr <- function(x,
                          classIni,
                          classEnd,
                          h,
                          right=FALSE) {

   if (is.factor(x) || mode(x) != 'numeric') stop('need numeric data')
   if ((classIni == '') || (classIni == ' ')) stop('classIni not defined')
   if ((classEnd == '') || (classEnd == ' ')) stop('classEnd not defined')
   if ((h == '')        || (h == ' '))        stop('h not defined')

   x <- na.omit(x)

   tbl <- er.make.table(x, classIni, classEnd, h, right)
   return(tbl)
}

#2. Tables from data.frames
#2.1---er.table.df.br (User define breaks and right)----------------------------
er.table.df.br <- function(df,
                            breaks = c('Sturges', 'Scott', 'FD'),
                            right = FALSE) {

   tmpList <- list()

   if (is.data.frame(df) != 'TRUE') stop('need "data.frame" data')

   logCol  <- sapply(df, is.numeric)
   dim_df  <- dim(df)

   for (i in 1:dim_df[2]) {
     if (logCol[i]!=FALSE) {
       x <- as.matrix(df[ ,i])
       x <- na.omit(x)

       k <- switch(breaks[1],
                   'Sturges' = nclass.Sturges(x),
                   'Scott'   = nclass.scott(x),
                   'FD'      = nclass.FD(x),
                   stop("'breaks' must be 'Sturges', 'Scott' or 'FD'"))

       tmp      <- range(x)
       classIni <- tmp[1] - abs(tmp[2])/100
       classEnd <- tmp[2] + abs(tmp[2])/100
       R        <- classEnd-classIni
       h        <- R/k

       tbl <- er.make.table(x, classIni, classEnd, h, right)
       tmpList <- c(tmpList, list(tbl))
     }
   }
   valCol <- logCol[logCol!=FALSE]
   names(tmpList) <- names(valCol)
   return(tmpList)
}


#2.2---er.table.df.kr (User define the class number (k) and right)--------------
er.table.df.kr <- function(df,
                            k,
                            right = FALSE) {

   if ((k == '') || (k == ' ')) stop('k not defined')
   if (is.data.frame(df) != 'TRUE') stop('need "data.frame" data')

   tmpList <- list()

   logCol  <- sapply(df, is.numeric)
   dim_df  <- dim(df)

   for (i in 1:dim_df[2]) {
     if (logCol[i]!=FALSE) {
       x <- as.matrix(df[ ,i])
       x <- na.omit(x)

       tmp      <- range(x)
       classIni <- tmp[1] - abs(tmp[2])/100
       classEnd <- tmp[2] + abs(tmp[2])/100
       R        <- classEnd-classIni
       h        <- R/k

       tbl <- er.make.table(x, classIni, classEnd, h, right)
       tmpList <- c(tmpList, list(tbl))
     }
   }
   valCol <- logCol[logCol!=FALSE]
   names(tmpList) <- names(valCol)
   return(tmpList)
}

Best regards,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From ligges at statistik.uni-dortmund.de  Wed May 25 16:18:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 16:18:56 +0200
Subject: [R] precision problem
In-Reply-To: <3f87cc6d05052507083041d095@mail.gmail.com>
References: <3f87cc6d05052507083041d095@mail.gmail.com>
Message-ID: <42948950.7060805@statistik.uni-dortmund.de>

Omar Lakkis wrote:

> I have prices that I am finding difficult to compare with ==, > and >,
> due to precision. For example: the numbers should match, with '==',
> but they differ in the magnitude of 1e-14 due to bunch of calculations
> that I run on them. Programming with java, I am used to implementing a
> function that compares the difference between the numbers to a pre
> determined precision factor. This could be very slow when  I have two
> matrices of numbers that I could otherwise compare with a simple '==',
> '>'  or '<' in R.
> What is teh best solution for this problem?
> Can I control the precision of ==, > and < without having to
> reimplement the operations in a slow way?

The R FAQ "Why doesn't R think these numbers are equal?" points you to
?all.equal

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Roger.Bivand at nhh.no  Wed May 25 16:21:34 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 25 May 2005 16:21:34 +0200 (CEST)
Subject: [R] precision problem
In-Reply-To: <3f87cc6d05052507083041d095@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0505251621170.7757-100000@reclus.nhh.no>

On Wed, 25 May 2005, Omar Lakkis wrote:

?all.equal, I think

> I have prices that I am finding difficult to compare with ==, > and >,
> due to precision. For example: the numbers should match, with '==',
> but they differ in the magnitude of 1e-14 due to bunch of calculations
> that I run on them. Programming with java, I am used to implementing a
> function that compares the difference between the numbers to a pre
> determined precision factor. This could be very slow when  I have two
> matrices of numbers that I could otherwise compare with a simple '==',
> '>'  or '<' in R.
> What is teh best solution for this problem?
> Can I control the precision of ==, > and < without having to
> reimplement the operations in a slow way?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From br44114 at gmail.com  Wed May 25 16:21:43 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 25 May 2005 10:21:43 -0400
Subject: [R] precision problem
Message-ID: <8d5a3635050525072139d974f0@mail.gmail.com>

This is a FAQ, 7.31.


-----Original Message-----
From: Omar Lakkis [mailto:uofiowa at gmail.com]
Sent: Wednesday, May 25, 2005 10:09 AM
To: r-help at stat.math.ethz.ch
Subject: [R] precision problem


I have prices that I am finding difficult to compare with ==, > and >,
due to precision. For example: the numbers should match, with '==',
but they differ in the magnitude of 1e-14 due to bunch of calculations
that I run on them. Programming with java, I am used to implementing a
function that compares the difference between the numbers to a pre
determined precision factor. This could be very slow when  I have two
matrices of numbers that I could otherwise compare with a simple '==',
'>'  or '<' in R.
What is teh best solution for this problem?
Can I control the precision of ==, > and < without having to
reimplement the operations in a slow way?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bela_b at gmx.net  Wed May 25 16:22:47 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Wed, 25 May 2005 16:22:47 +0200 (MEST)
Subject: [R] Weird function call problem
Message-ID: <10311.1117030967@www49.gmx.net>

Hi,

I'm encountering a very odd problem with calls to anova.mlm() from within a
function.

Consider the following code (data.n is a matrix of numeric values):

mlmfit <- lm(data.n ~ 1)
mlmfit0 <- lm(data.n ~ 0)
print(mlmfit)
anova(mlmfit,mlmfit0,test="Spherical")

If I run it just like this from the console, it works just fine. If,
however, I call it from within a function, e.g. using

fct <- function(data.k) {
 # same code with data.n replaced by data.k
}
fct(data.n)

It gives me
> fct(data.n)

Call:
lm(formula = data.k ~ 1)

Coefficients:
...

Error in anova.mlmlist(object = mlmfit, mlmfit0, test = "Spherical") :
        Object "mlmfit" not found

What causes mlmfit to "disappear" between two lines? I haven't got the
slightest clue where to look for an answer...

Thank you very much for your help.

Bela Bauer

-- 
Weitersagen: GMX DSL-Flatrates mit Tempo-Garantie!
Ab 4,99 Euro/Monat: http://www.gmx.net/de/go/dsl



From jfox at mcmaster.ca  Wed May 25 16:27:57 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 25 May 2005 10:27:57 -0400
Subject: [R] precision problem
In-Reply-To: <3f87cc6d05052507083041d095@mail.gmail.com>
Message-ID: <20050525142757.LMNZ27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Omar,

Perhaps I'm missing something, but why not just subtract one matrix from the
other and test the difference in relation to the precision that you require
for the comparison? E.g., to test near equality, something like, abs(A - B)
< 1e-13.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omar Lakkis
> Sent: Wednesday, May 25, 2005 9:09 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] precision problem
> 
> I have prices that I am finding difficult to compare with ==, 
> > and >, due to precision. For example: the numbers should 
> match, with '==', but they differ in the magnitude of 1e-14 
> due to bunch of calculations that I run on them. Programming 
> with java, I am used to implementing a function that compares 
> the difference between the numbers to a pre determined 
> precision factor. This could be very slow when  I have two 
> matrices of numbers that I could otherwise compare with a 
> simple '==', '>'  or '<' in R.
> What is teh best solution for this problem?
> Can I control the precision of ==, > and < without having to 
> reimplement the operations in a slow way?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From friedm69 at msu.edu  Wed May 25 16:38:25 2005
From: friedm69 at msu.edu (Steven K Friedman)
Date: Wed, 25 May 2005 10:38:25 -0400
Subject: [R] Ternary Plots with continuous data
Message-ID: <E1Dax1d-0008KB-GE@sys21.mail.msu.edu>


Hello 

I have a data base consisting of soil parameters, and tree species 
densities.   The vcd (visualizing categorical data) package includes the 
ternaryplot function which plots the gravitation center of 3 prameters. 

I'd like to find a similar function for use with continuous data rather than 
categorical. 

Does anyone know of a function suitable for this objective? 

Thanks 

Steve Friedman



From uofiowa at gmail.com  Wed May 25 16:47:53 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 25 May 2005 10:47:53 -0400
Subject: [R] precision problem
In-Reply-To: <20050525142757.LMNZ27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>
References: <3f87cc6d05052507083041d095@mail.gmail.com>
	<20050525142757.LMNZ27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <3f87cc6d050525074788093a7@mail.gmail.com>

Thank you all. 
all.equal is very helpful since I am also interested in finding the
mismatched prices.

On 5/25/05, John Fox <jfox at mcmaster.ca> wrote:
> Dear Omar,
> 
> Perhaps I'm missing something, but why not just subtract one matrix from the
> other and test the difference in relation to the precision that you require
> for the comparison? E.g., to test near equality, something like, abs(A - B)
> < 1e-13.
> 
> I hope this helps,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omar Lakkis
> > Sent: Wednesday, May 25, 2005 9:09 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] precision problem
> >
> > I have prices that I am finding difficult to compare with ==,
> > > and >, due to precision. For example: the numbers should
> > match, with '==', but they differ in the magnitude of 1e-14
> > due to bunch of calculations that I run on them. Programming
> > with java, I am used to implementing a function that compares
> > the difference between the numbers to a pre determined
> > precision factor. This could be very slow when  I have two
> > matrices of numbers that I could otherwise compare with a
> > simple '==', '>'  or '<' in R.
> > What is teh best solution for this problem?
> > Can I control the precision of ==, > and < without having to
> > reimplement the operations in a slow way?
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
>



From dray at biomserv.univ-lyon1.fr  Wed May 25 17:09:38 2005
From: dray at biomserv.univ-lyon1.fr (=?UTF-8?B?U3TDqXBoYW5lIERyYXk=?=)
Date: Wed, 25 May 2005 17:09:38 +0200
Subject: [R] Ternary Plots with continuous data
In-Reply-To: <E1Dax1d-0008KB-GE@sys21.mail.msu.edu>
References: <E1Dax1d-0008KB-GE@sys21.mail.msu.edu>
Message-ID: <42949532.6090400@biomserv.univ-lyon1.fr>

I suppose that your data consists of percentage per soil categories. 
Otherwise, I do not really understand how you want use ternary plot.

see triangle.plot in ade4 package.

Steven K Friedman wrote:

>
> Hello
> I have a data base consisting of soil parameters, and tree species 
> densities.   The vcd (visualizing categorical data) package includes 
> the ternaryplot function which plots the gravitation center of 3 
> prameters.
> I'd like to find a similar function for use with continuous data 
> rather than categorical.
> Does anyone know of a function suitable for this objective?
> Thanks
> Steve Friedman
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 
StÅ√Å©phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://www.steph280.freesurf.fr/



From dmb at mrc-dunn.cam.ac.uk  Wed May 25 17:33:36 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 25 May 2005 16:33:36 +0100 (BST)
Subject: [R] weighted.mean and tapply (again)
Message-ID: <Pine.LNX.4.21.0505251623040.6478-100000@mail.mrc-dunn.cam.ac.uk>


I read answers to questions including the words "tapply" and
"weighted.mean", but I didn't understand either the problem (data) or the
solution provided.

Here is my question ...

> dat[1:10,]
  GROUP  VALUE FREQUENCY
1     2      2        78
2     2      3        40
3     2      4        16
4     2      5         3
5     2      6         1
6     2      8         1
7     3      3        19
8     3      4        10
9     3      5        19
1     3      6         4


For each GROUP, I would like to calculate the weighted.mean of VALUE using
the FREQUENCY as the weight, so for the snippet of data shown that would
be...

group.2 <- weighted.mean(c(2,3,4,5,6,8),c(78,40,16,3,1,1))
group.3 <- weighted.mean(c(3,4,5,6),    c(19,10,19,4))

> cbind(rbind(2,3),rbind(group.2,group.3))
        [,1]     [,2]
group.2    2 2.654676
group.3    3 4.153846

I would like to use tapply to automatically do this across the whole
dataset (dat) - which includes lots of other distinct grouping factors,
however, like I said, I couldn't understand (and therefore apply to my
data) any of the other solutions I found, so any help here would be
greatly appreciated!

All the best,
Dan.



From murdoch at stats.uwo.ca  Wed May 25 17:34:41 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 25 May 2005 11:34:41 -0400
Subject: [R] precision problem
In-Reply-To: <3f87cc6d05052507083041d095@mail.gmail.com>
References: <3f87cc6d05052507083041d095@mail.gmail.com>
Message-ID: <42949B11.1060505@stats.uwo.ca>

Omar Lakkis wrote:

>I have prices that I am finding difficult to compare with ==, > and >,
>due to precision. For example: the numbers should match, with '==',
>but they differ in the magnitude of 1e-14 due to bunch of calculations
>that I run on them. Programming with java, I am used to implementing a
>function that compares the difference between the numbers to a pre
>determined precision factor. This could be very slow when  I have two
>matrices of numbers that I could otherwise compare with a simple '==',
>'>'  or '<' in R.
>What is teh best solution for this problem?
>Can I control the precision of ==, > and < without having to
>reimplement the operations in a slow way?
>

The somewhat misleadingly named "all.equal()" function does what you 
want for ==.  For the inequalities, you may want to add some constant to 
one side, e.g.

x > y-.Machine$double.eps ^ 0.5

instead of x > y.

Duncan Murdoch



From bela_b at gmx.net  Wed May 25 17:38:57 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Wed, 25 May 2005 17:38:57 +0200
Subject: [R] Weird function call problem
In-Reply-To: <10311.1117030967@www49.gmx.net>
References: <10311.1117030967@www49.gmx.net>
Message-ID: <20050525173857.29312f47@bela>

Sorry, forgot to mention:

I'm using R 2.1.0 on a x86 Debian Sarge system. I can reproduce the behaviour
on a similar system.
Unfortunately, I don't have any other OS with R installed available to me.

Thanks again,

Bela

-- 
Bela Bauer - bela_b at gmx.net
PGP 0x97529F5C
http://www.aitrob.de



From Eric.Dube at fbn.ca  Wed May 25 17:42:36 2005
From: Eric.Dube at fbn.ca (=?ISO-8859-1?Q?=22Dub=E9=2C_Eric=22?=)
Date: Wed, 25 May 2005 11:42:36 -0400
Subject: [R]: Aremos TSD files
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0BF1155E@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/f1ecbb63/attachment.pl

From sdavis2 at mail.nih.gov  Wed May 25 17:43:25 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 25 May 2005 11:43:25 -0400
Subject: [R] Table Help
In-Reply-To: <E2F63ACD0DE93C4A99AF7BF75837161C018AB303@rec-its-exs01.mail.saic.com>
References: <E2F63ACD0DE93C4A99AF7BF75837161C018AB303@rec-its-exs01.mail.saic.com>
Message-ID: <b170ea4aa79457aaebdc192e20d75bbe@mail.nih.gov>




On May 25, 2005, at 9:35 AM, McMurtry, Benjamin G. wrote:

> I'm slightly confused with aggregate, will I need to loop it until it 
> stops
> returning sub lists?  Or would I somehow need to convert my username 
> column
> into a unique list and then do FUN=sum??

Ben,

Generally, it is OK (and encouraged) to post back to the list, just so 
we all learn from each others' posts.

Here is an example:

 > a <- data.frame(group=rep(letters[1:3],10),values=rnorm(30))
 > a
    group       values
1      a -0.146140983
2      b  0.190071267
3      c  0.207708591
4      a -1.126531350
5      b -0.013761243
6      c -0.212028409
7      a  1.012215244
8      b -0.487072132
9      c  0.667999520
10     a  0.276094321
11     b  0.835611948
12     c -1.049264490
13     a  0.399050191
14     b -1.177174650
15     c -1.471944114
16     a -0.410480517
17     b  0.006008521
18     c  1.608219688
19     a -0.120729414
20     b  1.074361542
21     c  1.782826882
22     a  1.336191577
23     b  0.721519413
24     c -0.594837423
25     a  1.120628507
26     b -2.011601905
27     c -0.076464599
28     a -1.512376840
29     b  1.182703732
30     c -1.620904211
 > aggregate(a[,2],by=list(a[,1]),sum)
   Group.1          x
1       a  0.8279207
2       b  0.3206665
3       c -0.7586886
 >

Hope that helps.

Sean



From nxt7 at psu.edu  Wed May 25 17:50:23 2005
From: nxt7 at psu.edu (NATALIA F TCHETCHERINA)
Date: Wed, 25 May 2005 11:50:23 -0400 (EDT)
Subject: [R] mixed model
Message-ID: <200505251550.j4PFoN515531@webmail13.cac.psu.edu>

 Hello all,
 I have problem with setting up random effects.
 I have a model:
 y=x1+x2+x1*x2+z1+z1*x2
 where x1, x2, x1*x2 are fixed effects
 and z1, z1*x2 are random effects (crossed effects)
 I use library(nlme) 'lme' function.
 My question is: how I should set up random effects?
 I did 
 lme(y~x1+x2+x1:x2, data=DATA, random=~z1+z1:x2, na.action='na.omit')
 but it did not work.

 Sincerely, Natalia.



From murdoch at stats.uwo.ca  Wed May 25 17:50:59 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 25 May 2005 11:50:59 -0400
Subject: [R] Weird function call problem
In-Reply-To: <10311.1117030967@www49.gmx.net>
References: <10311.1117030967@www49.gmx.net>
Message-ID: <42949EE3.4080702@stats.uwo.ca>

Bela Bauer wrote:

>Hi,
>
>I'm encountering a very odd problem with calls to anova.mlm() from within a
>function.
>
>Consider the following code (data.n is a matrix of numeric values):
>
>mlmfit <- lm(data.n ~ 1)
>mlmfit0 <- lm(data.n ~ 0)
>print(mlmfit)
>anova(mlmfit,mlmfit0,test="Spherical")
>
>If I run it just like this from the console, it works just fine. If,
>however, I call it from within a function, e.g. using
>
>fct <- function(data.k) {
> # same code with data.n replaced by data.k
>}
>fct(data.n)
>
>It gives me
>  
>
>>fct(data.n)
>>    
>>
>
>Call:
>lm(formula = data.k ~ 1)
>
>Coefficients:
>...
>
>Error in anova.mlmlist(object = mlmfit, mlmfit0, test = "Spherical") :
>        Object "mlmfit" not found
>
>What causes mlmfit to "disappear" between two lines? I haven't got the
>slightest clue where to look for an answer...
>
>Thank you very much for your help.
>  
>

This looks like a bug in the anova.mlm or anova.mlmlist functions.  One 
of them is looking in the wrong place for mlmfit.

Duncan Murdoch



From james.holtman at convergys.com  Wed May 25 17:56:37 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Wed, 25 May 2005 11:56:37 -0400
Subject: [R] weighted.mean and tapply (again)
In-Reply-To: <Pine.LNX.4.21.0505251623040.6478-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <OFC8B374BF.CAF349A6-ON8525700C.005788A9-8525700C.005795F5@nd.convergys.com>





> x.1 <- read.table('clipboard',header=T)
> x.1
   GROUP VALUE FREQUENCY
1      2     2        78
2      2     3        40
3      2     4        16
4      2     5         3
5      2     6         1
6      2     8         1
7      3     3        19
8      3     4        10
9      3     5        19
10     3     6         4
> by(x.1, x.1$GROUP, function(x) weighted.mean(x$VALUE, x$FREQUENCY))
x.1$GROUP: 2
[1] 2.654676
---------------------------------------------------------------------------
x.1$GROUP: 3
[1] 4.153846
>

Jim
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Dan Bolser                                                                                                           
                      <dmb at mrc-dunn.cam.ac.        To:       R mailing list <r-help at stat.math.ethz.ch>                                     
                      uk>                          cc:                                                                                     
                      Sent by:                     Subject:  [R] weighted.mean and tapply (again)                                          
                      r-help-bounces at stat.m                                                                                                
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      05/25/2005 11:33                                                                                                     
                                                                                                                                           





I read answers to questions including the words "tapply" and
"weighted.mean", but I didn't understand either the problem (data) or the
solution provided.

Here is my question ...

> dat[1:10,]
  GROUP  VALUE FREQUENCY
1     2      2        78
2     2      3        40
3     2      4        16
4     2      5         3
5     2      6         1
6     2      8         1
7     3      3        19
8     3      4        10
9     3      5        19
1     3      6         4


For each GROUP, I would like to calculate the weighted.mean of VALUE using
the FREQUENCY as the weight, so for the snippet of data shown that would
be...

group.2 <- weighted.mean(c(2,3,4,5,6,8),c(78,40,16,3,1,1))
group.3 <- weighted.mean(c(3,4,5,6),    c(19,10,19,4))

> cbind(rbind(2,3),rbind(group.2,group.3))
        [,1]     [,2]
group.2    2 2.654676
group.3    3 4.153846

I would like to use tapply to automatically do this across the whole
dataset (dat) - which includes lots of other distinct grouping factors,
however, like I said, I couldn't understand (and therefore apply to my
data) any of the other solutions I found, so any help here would be
greatly appreciated!

All the best,
Dan.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From HDoran at air.org  Wed May 25 18:06:32 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 25 May 2005 12:06:32 -0400
Subject: [R] mixed model
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74091743CF@dc1ex2.air.org>

I may not follow entirely here, but your random effects structure isn't
correct for lme. Also, nlme cannot handle (at least well) models with
crossed random effects. A better option would be to use the lmer
function. 

Setting up the structure for the random effects in nlme would look
something like:

lme(y~ fixed, data, random~= z1 + z2 |ID)

Where ID is a variable that contains the grouping structure of your
data.

In lmer, which is more appropriate for models with crossed random
effects, you lmer call might be something along the lines of:

lmer(y ~ fixed + (z1|ID) + (z2|ID), data)

See the most recent version of R news for more info on this topic.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of NATALIA F
TCHETCHERINA
Sent: Wednesday, May 25, 2005 11:50 AM
To: r-help at stat.math.ethz.ch
Subject: [R] mixed model

 Hello all,
 I have problem with setting up random effects.
 I have a model:
 y=x1+x2+x1*x2+z1+z1*x2
 where x1, x2, x1*x2 are fixed effects
 and z1, z1*x2 are random effects (crossed effects)  I use library(nlme)
'lme' function.
 My question is: how I should set up random effects?
 I did
 lme(y~x1+x2+x1:x2, data=DATA, random=~z1+z1:x2, na.action='na.omit')
but it did not work.

 Sincerely, Natalia.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Wed May 25 18:06:27 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 25 May 2005 11:06:27 -0500
Subject: [R] mixed model
In-Reply-To: <200505251550.j4PFoN515531@webmail13.cac.psu.edu>
References: <200505251550.j4PFoN515531@webmail13.cac.psu.edu>
Message-ID: <4294A283.5060407@stat.wisc.edu>

NATALIA F TCHETCHERINA wrote:
>  Hello all,
>  I have problem with setting up random effects.
>  I have a model:
>  y=x1+x2+x1*x2+z1+z1*x2
>  where x1, x2, x1*x2 are fixed effects
>  and z1, z1*x2 are random effects (crossed effects)
>  I use library(nlme) 'lme' function.
>  My question is: how I should set up random effects?
>  I did 
>  lme(y~x1+x2+x1:x2, data=DATA, random=~z1+z1:x2, na.action='na.omit')
>  but it did not work.
> 
>  Sincerely, Natalia.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

The answer will depend on the types of x1, x2 and z1 (i.e. whether each
of them is numeric or a factor).  Because you use x1:x2 I will assume
that x1, x2 and z1 are all factors.  In that case the formula term x1*x2
is equivalent to x1 + x2 + x1:x2 and you could write the call to lme as

lme(y ~ x1*x2, data = DATA, random = ~1|z1/x1)

For lmer from the lme4 package it would be

lmer(y ~ x1*x2 + (1|z1) + (1|z1:x1), data = DATA)



From LI at nsabp.pitt.edu  Wed May 25 18:15:20 2005
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Wed, 25 May 2005 12:15:20 -0400
Subject: [R] The error while using R2WinBUGS
Message-ID: <3D0B2434377E984E9C85CAA316F8B183019FC662@nsabpmail>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/9ce47585/attachment.pl

From mikko.pakkanen at helsinki.fi  Wed May 25 18:25:12 2005
From: mikko.pakkanen at helsinki.fi (Mikko Pakkanen)
Date: Wed, 25 May 2005 19:25:12 +0300
Subject: [R] Problem with systemfit 0.7-3 and transformed variables
Message-ID: <1117038312.4294a6e805fff@www3.helsinki.fi>

> We did not notice this shortcoming of systemfit() so far. Unfortunately,
> I 
> don't have the time in the next few days to look into the code and figure
> out 
> how to enable transformed variables. I suggest that you either create 
> transformed variables by hand or you modify the systemfit code to enable
> this 
> and send us the patch. I prefer the second :-) (that's the philosophy of
> open-source software like R: useRs become developeRs).

Luckily, I had some time to check the code. Debugger revealed that the
problems are caused by the model.frame function which is used to compile the
'$data' data frame. I don't need that data frame so much, so I just
substituted model.frame with model.matrix which apparently doesn't cause
this error with transformed variables. However, I tuned it a bit further, so
that it should still return an identical '$data' data frame, despite the
modification.
I've only tested this with my example and it appears to be OK. Still, I
think this should be considered a "quick & dirty" fix -there are probably
better ways to do it. But, I hope it gives the idea. Here's my attempt:

mikko at briscoe R $ diff -u systemfit.R systemfit-patched.R
--- systemfit.R 2004-11-26 11:17:36.000000000 +0200
+++ systemfit-patched.R 2005-05-25 18:55:55.568944699 +0300
@@ -624,7 +624,11 @@
     Terms <- terms( eqns[[i]], data = data)
     m$formula <- Terms
     m <- eval(m, parent.frame())
-    datai <- model.frame(Terms, m)
+    resp <- model.extract(m, "response")
+       ## using model.matrix instead of model.frame, need to get the output
variable separately
+    datai <- data.frame(cbind(resp, (model.matrix(Terms, m))[,-1]))
+       ## I guess there's a better way to extract the name of the output
variable?
+       names(datai)[1] <- as.character(terms(eqns[[i]]))[2]
     if(method=="2SLS" | method=="3SLS") {
       #datai <- cbind( datai, model.frame( instl[[i]] ))
       # the following lines have to be substituted for the previous
@@ -634,7 +638,8 @@
       Terms <- terms(instl[[i]], data = data)
       m$formula <- Terms
       m <- eval(m, parent.frame())
-      datai <- cbind( datai, model.frame(Terms, m))
+      ## used previously model.frame
+      datai <- cbind( datai, as.data.frame((model.matrix(Terms, m))[,-1]))
     }

     if(i==1) {

Regards,

-Mikko.



From OlsenN at pac.dfo-mpo.gc.ca  Wed May 25 18:28:50 2005
From: OlsenN at pac.dfo-mpo.gc.ca (OlsenN@pac.dfo-mpo.gc.ca)
Date: Wed, 25 May 2005 09:28:50 -0700
Subject: [R] weighted.mean and tapply (again)
Message-ID: <7CBBD627E4E688499349A5D11D07831602ECB966@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>

In this simple case you can just coerce directly to a vector like so:

> foo <- by(dat, dat$GROUP, function(x) {weighted.mean(x$VALUE,
x$FREQUENCY)})
> bar <- as.vector(foo)

but look at the examples for ?by; in particular the use of 'sapply'.

Norm

-----Original Message-----
From: Dan Bolser [mailto:dmb at mrc-dunn.cam.ac.uk] 
Sent: Wednesday, May 25, 2005 9:02 AM
To: OlsenN at pac.dfo-mpo.gc.ca
Subject: RE: [R] weighted.mean and tapply (again)

On Wed, 25 May 2005 OlsenN at pac.dfo-mpo.gc.ca wrote:

>I think "by" will do what you want:
>
>> by(dat, dat$GROUP, function(x) {weighted.mean(x$VALUE, x$FREQUENCY)})
>
>Norm

Thanks Norm and Jim. My question is now...

How do you turn a <quote> list of class '"by"' </quote> into a vector of
results (one value per GROUP)?

Cheers guys!

Dan.


>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
>Sent: Wednesday, May 25, 2005 8:34 AM
>To: R mailing list
>Subject: [R] weighted.mean and tapply (again)
>
>
>I read answers to questions including the words "tapply" and 
>"weighted.mean", but I didn't understand either the problem (data) or 
>the solution provided.
>
>Here is my question ...
>
>> dat[1:10,]
>  GROUP  VALUE FREQUENCY
>1     2      2        78
>2     2      3        40
>3     2      4        16
>4     2      5         3
>5     2      6         1
>6     2      8         1
>7     3      3        19
>8     3      4        10
>9     3      5        19
>1     3      6         4
>
>
>For each GROUP, I would like to calculate the weighted.mean of VALUE 
>using the FREQUENCY as the weight, so for the snippet of data shown 
>that would be...
>
>group.2 <- weighted.mean(c(2,3,4,5,6,8),c(78,40,16,3,1,1))
>group.3 <- weighted.mean(c(3,4,5,6),    c(19,10,19,4))
>
>> cbind(rbind(2,3),rbind(group.2,group.3))
>        [,1]     [,2]
>group.2    2 2.654676
>group.3    3 4.153846
>
>I would like to use tapply to automatically do this across the whole 
>dataset
>(dat) - which includes lots of other distinct grouping factors, 
>however, like I said, I couldn't understand (and therefore apply to my
>data) any of the other solutions I found, so any help here would be 
>greatly appreciated!
>
>All the best,
>Dan.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>



From simon.urbanek at r-project.org  Wed May 25 18:41:34 2005
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Wed, 25 May 2005 12:41:34 -0400
Subject: [R-SIG-Mac] Re: [R] R unable to run on Mac OS 10.4 Tiger
In-Reply-To: <42942700.3070405@gmail.com>
References: <429350DE.7060604@gmail.com>
	<FD9A2379-7B76-48C4-82A6-C45681B1396E@mac.com>
	<42942700.3070405@gmail.com>
Message-ID: <CDF8309C-9EE8-4CFF-B61D-B77D593A67F2@r-project.org>

On May 25, 2005, at 3:19 AM, Guillaume Chapron wrote:

> I suppose i will have to reinstall quicktime 7 and things will be  
> OK. But I don't understand why R does need quicktime ??

Neither R nor R.app does. Chances are that you pretty much broke your  
Tiger, because all R.app does is to link Cocoa which links AppKit. On  
my Tiger machine it doesn't even touch QuickTime, so it must be your  
plugin or whatever you did to poor Tiger ;). Why don't you just  
remove /Library/Quicktime? It's not a part of Tiger. Tiger's  
QuickTime files are in /System/Library.

If you want to understand better who's loading what, set  
DYLD_PRINT_LIBRARIES when running R or R.app.

Cheers,
Simon



From reid_huntsinger at merck.com  Wed May 25 18:46:13 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 25 May 2005 12:46:13 -0400
Subject: [R] Can simulation involving random number generation be
	segm ented?
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A944A@uswpmx00.merck.com>

I don't see why you need to shut R down between runs. Can you just replace
each time you generate a new 200 samples? You might also find "gc()" helpful
if you generate a lot of intermediate objects in a single run. Delete
objects you no longer need (unless you're going to replace them and can
afford to keep them around), then an explicit request gc() might help keep
you under the wire.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
Sent: Wednesday, May 25, 2005 8:56 AM
To: Dr L. Y Hin
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Can simulation involving random number generation be
segmented?


Dr L. Y Hin wrote:

> Dear all,
> Apologies for this pedantic question that only arise when there is
hardware
> limitation.
> Setting: R 2.1.0 for windows xp sp2.
> Scenario:
> To generate 1000 samples using rnorm for a simulation activity.
> Background:
> The simulation activity requires so much memory resources that generating
> 200 samples
> clogs up the PF usage as indicated in the Windows Task Manager.
> Therefore, short of implementing the simulation on a computer with more
> resources,
> the alternative is to generate the 1000 samples in 5 separate runs,
> each generating 200 samples, closing the R window and re-opening between
> runs.
> Question to be addressed:
> To maintain consistency and ensure reproducibility of the simulation
> results, the 1000 samples
> generated in one single run should be indentical to the 5x200 samples
> generated on 5 separate
> runs.
> While such consistency can be ensured using set.seed()  in the case of one
> single run, in the case
> where 5 separate runs are performed, can we do the following to ensure
> identical samples being
> generated?
> 1. In the first run, specify the seed by, say, set.seed(1)
> 
> 2. At the end of the first run, store the .Random.seed by the following
> manner:
> saved.seed.1<-.Random.seed
> 
> 3. At the beginning of the second run, assign the saved.seed.1 to
> .Random.seed as follows:
> .Random.seed<-saved.seed.1
> 
> 4. At the end of the first run, store the new .Random.seed by the
following
> manner:
> saved.seed.2<-.Random.seed
> 
> 5. At the beginning of the second run, assign the saved.seed.2 to
> .Random.seed as follows:
> .Random.seed<-saved.seed.2
> 
> This is repeated until 5 runs are completed.
> 
> Will the paths of random number generation be identical in these two
> approaches? 

Yes.

Uwe Ligges

If not, is there
> a way to ensure this?
> 
> Apologies again for this long-winded inquiry.
> 
> Thank you.
> Best
> Lin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Wed May 25 18:50:19 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 25 May 2005 09:50:19 -0700
Subject: [R] weighted.mean and tapply (again)
In-Reply-To: <7CBBD627E4E688499349A5D11D07831602ECB966@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>
Message-ID: <200505251650.j4PGoJFo020825@meitner.gene.com>

> 
> In this simple case you can just coerce directly to a vector like so:
> 
> > foo <- by(dat, dat$GROUP, function(x) {weighted.mean(x$VALUE,
> x$FREQUENCY)})
> > bar <- as.vector(foo)
> 
> but look at the examples for ?by; in particular the use of 'sapply'.
> 
> Norm

As a general practice, I believe unlist() is preferable. ?unlist explains
why.

Cheers,
Bert Gunter



From ligges at statistik.uni-dortmund.de  Wed May 25 19:00:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 May 2005 19:00:43 +0200
Subject: [R] The error while using R2WinBUGS
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B183019FC662@nsabpmail>
References: <3D0B2434377E984E9C85CAA316F8B183019FC662@nsabpmail>
Message-ID: <4294AF3B.2020009@statistik.uni-dortmund.de>

I don't know if I understand you correctly, but I think you missed to 
read ?bugs carefully enough:

    bugs(....., DIC = FALSE)

should disable calculation of DIC statistics. If your example still 
fails, please specify a reproducible example in a private message.

Asking the package maintainer / authors would have been a better idea 
for such a specialized question.


Uwe Ligges

BTW: You might want to try out the new CRAN package BRugs, which 
provides an interface to the (included) OpenBUGS (the next version of 
WinBUGS). Currenty, it only works on Windows, though.







Li, Jia wrote:

> Dear all,
>  
> I tried to run WinBUGS model in R by using package R2WinBUGS, but I
> failed because of the massage : cannot calculate DIC for model in log
> file of WinBUGS14. In fact, the model works in WinBUGS and I can get
> summaries of the model, such as density, stats..., except DIC , because
> DIC may not be appropriate in this kind of model due to some
> reasons(according to the manual of WinBUGS14), and WinBUGS cannot
> calculate it, but this is allowed in WinBUGS. If I just ran it in
> WinBUGS, that would be fine, because my goal is to get the summay not
> DIC, but I need to run it in R, and R must have DIC. (the example as
> follow). I am wondering what I can do to fix it in R?
>  
> for exampe: the school example:
> the output in R:
> 
>>print(schools.sim)
> 
> Inference for Bugs model at "c:/schools/schools.bug"
> 3 chains, each with 1000 iterations (first 500 discarded)
> n.sims = 1500 iterations saved
> mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff
> theta[1] 11.1 9.1 -3.0 5.0 10.0 16.0 31.8 1.1 39
> .....
> theta[8] 8.3 8.4 -6.6 2.8 8.1 12.7 26.2 1.0 64
> mu.theta 7.6 5.9 -3.0 3.7 8.0 11.0 19.5 1.1 35
> .....
> deviance 60.8 2.5 57.0 59.1 60.2 62.1 66.6 1.0 170
> pD = 3 and DIC = 63.8 (using the rule, pD = var(deviance)/2)
>  
> Thanks a lot,
>  
> Jia
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jerk_alert at hotmail.com  Wed May 25 19:27:01 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Wed, 25 May 2005 17:27:01 +0000
Subject: [R] Rounding fractional numbers to nearest fraction
Message-ID: <BAY101-F14830761204561E09130EEE80E0@phx.gbl>

Hi all,

I've got a matrix of fractional data that is all positive and greater than 
zero that I would like to "loosely" classify, for lack of a better word. It 
looks something like this :

1.07   1.11   1.27   1.59   0.97   0.76
2.23   0.98   0.71   0.88   1.19   1.02


What I'm looking for is a way to round these numbers to the nearest 0.25, 
i.e. the above matrix would be transformed to :

1.00   1.00   1.25   1.50   1.00   0.75
2.25   1.00   0.75   1.00   1.25   1.00


Anyone have a clever way to do this??

Thanks in advance,
Ken



From br44114 at gmail.com  Wed May 25 19:34:30 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 25 May 2005 13:34:30 -0400
Subject: [R] Rounding fractional numbers to nearest fraction
Message-ID: <8d5a363505052510342618ef88@mail.gmail.com>

Multiply by 4, round and divide by 4.
a <- c(1.15,5.82)
round(a*4,digits=0)/4



-----Original Message-----
From: Ken Termiso [mailto:jerk_alert at hotmail.com]
Sent: Wednesday, May 25, 2005 1:27 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Rounding fractional numbers to nearest fraction


Hi all,

I've got a matrix of fractional data that is all positive and greater than 
zero that I would like to "loosely" classify, for lack of a better word. It 
looks something like this :

1.07   1.11   1.27   1.59   0.97   0.76
2.23   0.98   0.71   0.88   1.19   1.02


What I'm looking for is a way to round these numbers to the nearest 0.25, 
i.e. the above matrix would be transformed to :

1.00   1.00   1.25   1.50   1.00   0.75
2.25   1.00   0.75   1.00   1.25   1.00


Anyone have a clever way to do this??

Thanks in advance,
Ken

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jerk_alert at hotmail.com  Wed May 25 20:46:03 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Wed, 25 May 2005 18:46:03 +0000
Subject: [R] Rounding fractional numbers to nearest fraction
Message-ID: <BAY101-F27AAB87FEF018168246AB6E80E0@phx.gbl>

Thx for the help...

mdiv <- function(dec)
{
	dec <- dec * 4
	dec <- round(dec, 0)
	dec <- dec / 4
	dec
}

where m is a matrix --

m1 <- apply(m, 1, mdiv)

...but it looks like as.integer(x.1*4)/4 or (round(x.1*4,0))/4 might be 
easier...

thx again,
ken

>From: james.holtman at convergys.com
>To: "Ken Termiso" <jerk_alert at hotmail.com>
>Subject: Re: [R] Rounding fractional numbers to nearest fraction
>Date: Wed, 25 May 2005 14:30:44 -0400
>
>
>
>
>
> > x.1
>  [1] 1.07 1.11 1.27 1.59 0.97 0.76 2.23 0.98 0.71 0.88 1.19 1.02
> > as.integer(x.1*4)/4
>  [1] 1.00 1.00 1.25 1.50 0.75 0.75 2.00 0.75 0.50 0.75 1.00 1.00
> >
>
>Jim
>__________________________________________________________
>James Holtman        "What is the problem you are trying to solve?"
>Executive Technical Consultant  --  Convergys Labs
>james.holtman at convergys.com
>+1 (513) 723-2929
>
>
>
>                       "Ken Termiso"
>                       <jerk_alert at hotmail.c        To:       
>r-help at stat.math.ethz.ch
>                       om>                          cc:
>                       Sent by:                     Subject:  [R] Rounding 
>fractional numbers to nearest fraction
>                       r-help-bounces at stat.m
>                       ath.ethz.ch
>
>
>                       05/25/2005 13:27
>
>
>
>
>
>Hi all,
>
>I've got a matrix of fractional data that is all positive and greater than
>zero that I would like to "loosely" classify, for lack of a better word. It
>
>looks something like this :
>
>1.07   1.11   1.27   1.59   0.97   0.76
>2.23   0.98   0.71   0.88   1.19   1.02
>
>
>What I'm looking for is a way to round these numbers to the nearest 0.25,
>i.e. the above matrix would be transformed to :
>
>1.00   1.00   1.25   1.50   1.00   0.75
>2.25   1.00   0.75   1.00   1.25   1.00
>
>
>Anyone have a clever way to do this??
>
>Thanks in advance,
>Ken
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>



From andy_liaw at merck.com  Wed May 25 20:54:34 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 May 2005 14:54:34 -0400
Subject: [R] Rounding fractional numbers to nearest fraction
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E89B@usctmx1106.merck.com>

> From: Ken Termiso
> 
> Thx for the help...
> 
> mdiv <- function(dec)
> {
> 	dec <- dec * 4
> 	dec <- round(dec, 0)
> 	dec <- dec / 4
> 	dec
> }
> 
> where m is a matrix --
> 
> m1 <- apply(m, 1, mdiv)

You don't need the apply():

> m <- matrix(runif(6), 2, 3)
> m
          [,1]      [,2]      [,3]
[1,] 0.6885473 0.4598892 0.2738169
[2,] 0.2393461 0.8863284 0.7891100
> round(m * 4) / 4
     [,1] [,2] [,3]
[1,] 0.75  0.5 0.25
[2,] 0.25  1.0 0.75
 
> ...but it looks like as.integer(x.1*4)/4 or 
> (round(x.1*4,0))/4 might be 
> easier...

Be careful: as.integer() trucate the fractional part (so as.integer(1.999)
is 1 intead of 2), which may not be what you want.

Andy

 
> thx again,
> ken
> 
> >From: james.holtman at convergys.com
> >To: "Ken Termiso" <jerk_alert at hotmail.com>
> >Subject: Re: [R] Rounding fractional numbers to nearest fraction
> >Date: Wed, 25 May 2005 14:30:44 -0400
> >
> >
> >
> >
> >
> > > x.1
> >  [1] 1.07 1.11 1.27 1.59 0.97 0.76 2.23 0.98 0.71 0.88 1.19 1.02
> > > as.integer(x.1*4)/4
> >  [1] 1.00 1.00 1.25 1.50 0.75 0.75 2.00 0.75 0.50 0.75 1.00 1.00
> > >
> >
> >Jim
> >__________________________________________________________
> >James Holtman        "What is the problem you are trying to solve?"
> >Executive Technical Consultant  --  Convergys Labs
> >james.holtman at convergys.com
> >+1 (513) 723-2929
> >
> >
> >
> >                       "Ken Termiso"
> >                       <jerk_alert at hotmail.c        To:       
> >r-help at stat.math.ethz.ch
> >                       om>                          cc:
> >                       Sent by:                     Subject: 
>  [R] Rounding 
> >fractional numbers to nearest fraction
> >                       r-help-bounces at stat.m
> >                       ath.ethz.ch
> >
> >
> >                       05/25/2005 13:27
> >
> >
> >
> >
> >
> >Hi all,
> >
> >I've got a matrix of fractional data that is all positive 
> and greater than
> >zero that I would like to "loosely" classify, for lack of a 
> better word. It
> >
> >looks something like this :
> >
> >1.07   1.11   1.27   1.59   0.97   0.76
> >2.23   0.98   0.71   0.88   1.19   1.02
> >
> >
> >What I'm looking for is a way to round these numbers to the 
> nearest 0.25,
> >i.e. the above matrix would be transformed to :
> >
> >1.00   1.00   1.25   1.50   1.00   0.75
> >2.25   1.00   0.75   1.00   1.25   1.00
> >
> >
> >Anyone have a clever way to do this??
> >
> >Thanks in advance,
> >Ken
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From swanson at yellowstoneresearch.org  Wed May 25 21:16:41 2005
From: swanson at yellowstoneresearch.org (Alan Swanson)
Date: Wed, 25 May 2005 13:16:41 -0600
Subject: [R] mexican hat wavelet?
Message-ID: <20050525191638.3407A5FA36@mail01.bridgeband.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/398e752e/attachment.pl

From antoniou at central.ntua.gr  Wed May 25 21:43:17 2005
From: antoniou at central.ntua.gr (Constantinos Antoniou)
Date: Wed, 25 May 2005 22:43:17 +0300
Subject: brian.pdf (reference suggested in old message [R] quasipoisson,
	glm.nb and AIC values)
Message-ID: <5A8F152A-0EB9-4549-9BFF-3397E19CED0E@central.ntua.gr>

Dear all,

Prof. Ripley had suggested the following as  reference to the topic  
of "quasipoisson, glm.nb and AIC values" in Mar 13 2003:

http://www.stats.gla.ac.uk/~goeran/euroworkshop/webpages/2002/slides/ 
brian.pdf

Unfortunately, the link does not seem to be working any more. If  
anyone knows of an alternate location/way to access this document, it  
would be much appreciated,

Thank you,

Constantinos Antoniou



-- 
Constantinos Antoniou, Ph.D.
Massachusetts Institute of Technology
Intelligent Transportation Systems Program
77 Massachusetts Ave., Rm. 1-249, Cambridge, MA 02139



From Pierre.Lapointe at nbf.ca  Wed May 25 21:48:56 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Wed, 25 May 2005 15:48:56 -0400
Subject: [R] Plot: Distance between tick and tick label on y-axis
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF357EB@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/6f3d501a/attachment.pl

From Michael.Grant at Colorado.EDU  Wed May 25 22:16:52 2005
From: Michael.Grant at Colorado.EDU (Michael Grant)
Date: Wed, 25 May 2005 14:16:52 -0600
Subject: [R] cor vs cor.test
Message-ID: <31788F8693FDA44BA587496BAE87D146B8084C@exchsrv01.colorado.edu>

Using Windows System, R 2.1.0

d is a data frame, 48 rows, 10 columns
cor(d) works properly providing all pairwise Pearson correlation
coefficients among columns
cor.test(d) gives error message "Error in cor.test.default(d) : argument
"y" is missing, with no default"

Why?

Thanks,
MCG



From blindglobe at gmail.com  Wed May 25 22:22:53 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 25 May 2005 22:22:53 +0200
Subject: [R] Can simulation involving random number generation be
	segmented?
In-Reply-To: <000001c56125$06a97420$0202a8c0@yourgk68c57jh8>
References: <000001c56125$06a97420$0202a8c0@yourgk68c57jh8>
Message-ID: <1abe3fa905052513222a8f7716@mail.gmail.com>

You might consider the rlecuyer package, which provides parallel
streams and avoids the potential worst case scenario from arbitrary
seed setting.

On 5/25/05, Dr L. Y Hin <lyhin at netvigator.com> wrote:
> Dear all,
> Apologies for this pedantic question that only arise when there is hardware
> limitation.
> Setting: R 2.1.0 for windows xp sp2.
> Scenario:
> To generate 1000 samples using rnorm for a simulation activity.
> Background:
> The simulation activity requires so much memory resources that generating
> 200 samples
> clogs up the PF usage as indicated in the Windows Task Manager.
> Therefore, short of implementing the simulation on a computer with more
> resources,
> the alternative is to generate the 1000 samples in 5 separate runs,
> each generating 200 samples, closing the R window and re-opening between
> runs.
> Question to be addressed:
> To maintain consistency and ensure reproducibility of the simulation
> results, the 1000 samples
> generated in one single run should be indentical to the 5x200 samples
> generated on 5 separate
> runs.
> While such consistency can be ensured using set.seed()  in the case of one
> single run, in the case
> where 5 separate runs are performed, can we do the following to ensure
> identical samples being
> generated?
> 1. In the first run, specify the seed by, say, set.seed(1)
> 
> 2. At the end of the first run, store the .Random.seed by the following
> manner:
> saved.seed.1<-.Random.seed
> 
> 3. At the beginning of the second run, assign the saved.seed.1 to
> .Random.seed as follows:
> .Random.seed<-saved.seed.1
> 
> 4. At the end of the first run, store the new .Random.seed by the following
> manner:
> saved.seed.2<-.Random.seed
> 
> 5. At the beginning of the second run, assign the saved.seed.2 to
> .Random.seed as follows:
> .Random.seed<-saved.seed.2
> 
> This is repeated until 5 runs are completed.
> 
> Will the paths of random number generation be identical in these two
> approaches? If not, is there
> a way to ensure this?
> 
> Apologies again for this long-winded inquiry.
> 
> Thank you.
> Best
> Lin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From roger.bos at gmail.com  Wed May 25 22:23:04 2005
From: roger.bos at gmail.com (roger bos)
Date: Wed, 25 May 2005 16:23:04 -0400
Subject: [R] website reference for building R packages
In-Reply-To: <42943B70.6050107@statistik.uni-dortmund.de>
References: <BAY10-F506FC10C31172C8BA726ADD60D0@phx.gbl>
	<42929EA3.6020708@columbia.edu>
	<42943B70.6050107@statistik.uni-dortmund.de>
Message-ID: <1db7268005052513234b0dd3f0@mail.gmail.com>

I used this tutorial and completed steps 1-6 without errors, but on
step 7 I tried to run R CMD check on my package called 'ram' directory
one level above the 'ram' folder and I got the following error?

I:\R_HOME>R CMD check ram
* checking for working latex ...Error: environment variable TMPDIR not set (or s
et to unusable value) and no default available.
 at c:\R\rw2010\share\perl/R/Utils.pm line 72

Can anyone help me with this?

Thanks,

Roger



On 5/25/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Suresh Krishna wrote:
> 
> >
> > it is the first link if you type "making packages" into the google
> > search box here:
> >
> > http://maths.newcastle.edu.au/~rking/R/
> 
> 
> Yes, please look into the archives as the posting guide asks you to do!
> 
> 
> > -s.
> >
> >
> > Laura Holt wrote:
> >
> >> Hi R People:
> >>
> >> A few weeks ago, someone put a link to a website for "how to" for
> >> building R packages.  It was very nice.
> 
> 
> Writing R Extensions is an *complete* and *up to date* documentation for
> this task. Package management is being steadily improved.
> Really, I recommend to use "Writing R Extensions"!
> 
> If you are working on Windows, you might want to look into the R
> Administration and Installation manual as well which descibes how to
> collect and set up the required tools...
> 
> Uwe Ligges
> 
> 
> >> But of course, I have misplaced the link.  Does anyone still have
> >> that, please?
> >>
> >> It was someone from the University of Chicago, I believe.
> >>
> >> Thanks in advance.
> >>
> >> Sincerely,
> >> Laura Holt
> >> mailto: lauraholt_983 at hotmail.com
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From erithid at bellsouth.net  Wed May 25 22:31:18 2005
From: erithid at bellsouth.net (BJ)
Date: Wed, 25 May 2005 16:31:18 -0400
Subject: [R] Any ideas on how to add a dotted line to a box plot to indicate
 a specific value?
Message-ID: <4294E096.3050000@bellsouth.net>

Basically, i want to create a boxplot from population data, with a solid 
line for the median, and then a dotted line for a specific persons 
value. Is there a way to artificially introduce another line or mark of 
some kind in a box plot? Thanks again for all of yoru help. Hopefully 
soon I wont have to keep bugging you all. ~Erithid :-p



From pguqnpu at coilplus.com  Wed May 25 23:59:05 2005
From: pguqnpu at coilplus.com (Anne Dalsiel)
Date: Wed, 25 May 2005 14:59:05 -0700
Subject: [R] H0t Pick With Results!
Message-ID: <40693165597544.4502183.7@talent-ip349.coilplus.com>

Dalsiel

 + Insider Profit Reporter: FRBV Is On Fire! +

Company - Fire Mountain Beverage
Ticker - FRBV
Current Price - .01
52 Week High - .35
3-5 Day target - .50
6months-target - 1.25

Detailed Information About The Company:

FRBV Chairman Anthony K. Miller commented: ``Our goal is
to provide investors with relevant information about our
Company. We are excited about our future and we want our
investors to have the same zeal for FRBV that we posses.
We ask investors to subscribe to our newsletters through
the website; we will forward all future newsletters and
press releases to them at the appropriate time.''

Fire Mountain Beverage Company (FRBV) develops, markets,
sells, and distributes branded purified and flavored water
beverages. The Company products are orientated to the
health conscious consumer looking for alternatives to tap
water and carbonated beverages containing sugar, caffeine,
sodium and carbohydrates. Fire Mountain's customer base
includes single and multi-store retail operations,
governmental agencies, distributors, convenience stores,
schools and other outlets. These products take advantage
of current market trends in the beverage industry that
enhance the quality of life. FRBV has relationships that
immediately provide the Company with distribution in
thousands of retail locations including major food chains.

- - - - - - - - - - - - - - - - - - - - - - - - - -
FRBV Will Skyrocket it is at an all time low at the moment
and will have a very impressive comeback all week and next
week with major promotions taking place including releases
in over 35 Newletters to millions of Investors. Also a fax
campaign will be launched on friday for great results next
week inside sources tell us. Be sure to get in immediately,
don't miss out!
- - - - - - - - - - - - - - - - - - - - - - - - - -

Safe Harbor Statement under the Private Securities
Litigation Reform Act: Statements in this news release
may contain forward-looking information within the meaning
of Section 27a of the U.S. Securities Act of 1993 and
Section 21E of the Securities and Exchange Act of 1934,
and is subject to the safe harbor created by those sections.
All statements, other than statements of historical fact,
are forward-looking statements that involve various risks
and uncertainties, which may, individually or mutually,
impact the matters described herein. There can be no
assurance that such statements will prove to be accurate,
and the actual results and future events could differ
materially from those anticipated in such statements.
The company assumes no obligation to update the
information contained in this release. Readers should
not place undue reliance on any forward-looking statements
contained herein. revising

loincloth



From ssk2031 at columbia.edu  Wed May 25 23:17:58 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Wed, 25 May 2005 17:17:58 -0400
Subject: [R] cor vs cor.test
In-Reply-To: <31788F8693FDA44BA587496BAE87D146B8084C@exchsrv01.colorado.edu>
References: <31788F8693FDA44BA587496BAE87D146B8084C@exchsrv01.colorado.edu>
Message-ID: <4294EB86.9070403@columbia.edu>


From:

?cor.test

Arguments:

     x, y: numeric vectors of data values.  'x' and 'y' must have the
           same length.

-s.

Michael Grant wrote:
> Using Windows System, R 2.1.0
> 
> d is a data frame, 48 rows, 10 columns
> cor(d) works properly providing all pairwise Pearson correlation
> coefficients among columns
> cor.test(d) gives error message "Error in cor.test.default(d) : argument
> "y" is missing, with no default"
> 
> Why?
> 
> Thanks,
> MCG
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pmbrando at ipam.org.br  Thu May 26 03:01:32 2005
From: pmbrando at ipam.org.br (Paulo Brando)
Date: Wed, 25 May 2005 18:01:32 -0700
Subject: [R] aggregate and stack
Message-ID: <022e01c5618e$763f2ff0$f10aa8c0@paulobrando>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/5b91c181/attachment.pl

From uofiowa at gmail.com  Wed May 25 17:07:37 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 25 May 2005 11:07:37 -0400
Subject: [R] precision problem
In-Reply-To: <3f87cc6d050525074788093a7@mail.gmail.com>
References: <3f87cc6d05052507083041d095@mail.gmail.com>
	<20050525142757.LMNZ27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>
	<3f87cc6d050525074788093a7@mail.gmail.com>
Message-ID: <3f87cc6d05052508075ce7a23b@mail.gmail.com>

all.equal is helpful when I am comparing equality of two matrices.
However, when I am comparing two individual number with > or < is my
best bet doing if( abs(x - y) < tolerence) or is there a function like
all.equal that has the same default tolerence?

On 5/25/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> Thank you all.
> all.equal is very helpful since I am also interested in finding the
> mismatched prices.
> 
> On 5/25/05, John Fox <jfox at mcmaster.ca> wrote:
> > Dear Omar,
> >
> > Perhaps I'm missing something, but why not just subtract one matrix from the
> > other and test the difference in relation to the precision that you require
> > for the comparison? E.g., to test near equality, something like, abs(A - B)
> > < 1e-13.
> >
> > I hope this helps,
> >  John
> >
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omar Lakkis
> > > Sent: Wednesday, May 25, 2005 9:09 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] precision problem
> > >
> > > I have prices that I am finding difficult to compare with ==,
> > > > and >, due to precision. For example: the numbers should
> > > match, with '==', but they differ in the magnitude of 1e-14
> > > due to bunch of calculations that I run on them. Programming
> > > with java, I am used to implementing a function that compares
> > > the difference between the numbers to a pre determined
> > > precision factor. This could be very slow when  I have two
> > > matrices of numbers that I could otherwise compare with a
> > > simple '==', '>'  or '<' in R.
> > > What is teh best solution for this problem?
> > > Can I control the precision of ==, > and < without having to
> > > reimplement the operations in a slow way?
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> >
> >
>



From andy_liaw at merck.com  Thu May 26 00:26:53 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 May 2005 18:26:53 -0400
Subject: [R] cor vs cor.test
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E8A3@usctmx1106.merck.com>

A while ago I posted a function that computes p-values for a correlation
matrix, by adapting code in cor.test().  Search the archive for it.  Also
see rcorr() in the Hmisc package.  That might do what you want.

Andy


> From: Suresh Krishna
> 
> From:
> 
> ?cor.test
> 
> Arguments:
> 
>      x, y: numeric vectors of data values.  'x' and 'y' must have the
>            same length.
> 
> -s.
> 
> Michael Grant wrote:
> > Using Windows System, R 2.1.0
> > 
> > d is a data frame, 48 rows, 10 columns
> > cor(d) works properly providing all pairwise Pearson correlation
> > coefficients among columns
> > cor.test(d) gives error message "Error in 
> cor.test.default(d) : argument
> > "y" is missing, with no default"
> > 
> > Why?
> > 
> > Thanks,
> > MCG
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From nxt7 at psu.edu  Thu May 26 00:33:05 2005
From: nxt7 at psu.edu (NATALIA F TCHETCHERINA)
Date: Wed, 25 May 2005 18:33:05 -0400 (EDT)
Subject: [R] global normalization
Message-ID: <200505252233.j4PMX5Z04111@webmail13.cac.psu.edu>

Hello all,
I have question about global normalization.
I have data from big experement(two-color cDNA arrays) where used three
different layouts but the same set of genes. 
I used limma package for within array normalization.
My question is: how I can do between array normalization (global) if arrays have
different layout(different location and different number technical replicates
(number of spots of a gene on an array))?  

Sincerely, Natalia.

From gunter.berton at gene.com  Thu May 26 00:40:13 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 25 May 2005 15:40:13 -0700
Subject: [R] global normalization
In-Reply-To: <200505252233.j4PMX5Z04111@webmail13.cac.psu.edu>
Message-ID: <200505252240.j4PMeD0Y028347@faraday.gene.com>

Please address this question to the appropriate BioConductor or other
microarray list (or to the limma package author, as the Posting Guide asks).
It is not appropriate for R-Help.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> NATALIA F TCHETCHERINA
> Sent: Wednesday, May 25, 2005 3:33 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] global normalization
> 
> Hello all,
> I have question about global normalization.
> I have data from big experement(two-color cDNA arrays) where 
> used three
> different layouts but the same set of genes. 
> I used limma package for within array normalization.
> My question is: how I can do between array normalization 
> (global) if arrays have
> different layout(different location and different number 
> technical replicates
> (number of spots of a gene on an array))?  
> 
> Sincerely, Natalia.
>



From Alexander.Herr at csiro.au  Thu May 26 00:45:06 2005
From: Alexander.Herr at csiro.au (Alexander.Herr@csiro.au)
Date: Thu, 26 May 2005 08:45:06 +1000
Subject: [R] Basic matematical functions with NAs
Message-ID: <062AE320EF971E40ACD0F6C93391D7694FF724@exqld1-tsv.nexus.csiro.au>


Conceptually your assumption is wrong: If you have the traps out and don't catch fruit in the trap than you have 0 captures. NA would come for example from a trap not functioning properly at a certain day.

To get R to recognize NA you need to define your variables as numeric or factors. Quickest I find to produce the data in a spreadsheet/database and have NAs defined as -99999. Than import into R via CSV format and replace -99999 with NA. Do read the R documentation on http://www.r-project.org/. Introduction to R is most helpful...

Herry

-------------------------------------------
Alexander Herr - Herry
Northern Futures
Davies Laboratory, CSIRO
PMB, Aitkenvale, QLD 4814
Phone (07) 4753 8510
Fax???? (07) 4753 8650
Home: http://herry.ausbats.org.au
Webadmin ABS: http://ausbats.org.au
Sustainable Ecosystems: http://www.cse.csiro.au/
--------------------------------------------

XXXXXXXXXXXXXXXXXXXXXXXXOriginal messageXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Message: 19
Date: Tue, 24 May 2005 10:22:53 -0700
From: "Paulo Brando" <pmbrando at ipam.org.br>
Subject: [R] Basic matematical functions with NAs
To: <r-help at stat.math.ethz.ch>
Message-ID: <003001c56085$38ea1930$f10aa8c0 at paulobrando>
Content-Type: text/plain

Dear All,

I've tried to sum columns -- different species of flowers, fruits plus twigs -- with NAs to get litterfall/trap, and then after use litterfall to calculate production (litterfall (grams)/ hectare/ day. But R 'sees' litterfall/trap as a string. 

My question: How to use basic mathematical functions to deal with NAs in data management. 

Example (as you can note I have many missing values -- no fruit  fell in the trap.


      area ponto date pseco psaco pliquido florg1 flor1 florg2 flor2 florg3 flor3 frutog1 fruto1 frutog2 fruto2 frutog3 fruto3 frutog4 fruto4 frutog5 fruto5 frutog6 fruto6 frutog7 fruto7 frutog8 fruto8 twigs  
      A A 1 38233 17.7 1.6 7.1 0.266 1                       
      A AA 1 38233 12.5 8.7 3.8                         
      A AB 1 38233 13.9 1.7 3.2       0.421 3                 
      A B 1 38233 12.1 1.6 1.5       0.248 2 0.435 7 0.16 1             
      A BORDA 1 38233                            
      A C 1 38233 15.6 1.7 4.9       0.374 2 0.298 3 0.231 1             
      A F 1 38233 14 1.5 3.5 0.366 45     0.153 1 0.15 1               



Paulo Brando
Inst. de Pesquisa Ambiental da Amaz??nia (IPAM)
Rua Rui Barbosa,136.
68.005.080 Santar??m, PA, Brazil.
Fone/Fax ++ 55 93 522 5538
www.ipam.org.br
	[[alternative HTML version deleted]]



-
??



From ahenningsen at agric-econ.uni-kiel.de  Thu May 26 00:49:31 2005
From: ahenningsen at agric-econ.uni-kiel.de (Arne Henningsen)
Date: Thu, 26 May 2005 00:49:31 +0200
Subject: [R] Problem with systemfit 0.7-3 and transformed variables
In-Reply-To: <1117038312.4294a6e805fff@www3.helsinki.fi>
References: <1117038312.4294a6e805fff@www3.helsinki.fi>
Message-ID: <200505260049.32138.ahenningsen@agric-econ.uni-kiel.de>

On Wednesday 25 May 2005 18:25, Mikko Pakkanen wrote:
> > We did not notice this shortcoming of systemfit() so far. Unfortunately,
> > I
> > don't have the time in the next few days to look into the code and figure
> > out
> > how to enable transformed variables. I suggest that you either create
> > transformed variables by hand or you modify the systemfit code to enable
> > this
> > and send us the patch. I prefer the second :-) (that's the philosophy of
> > open-source software like R: useRs become developeRs).
>
> Luckily, I had some time to check the code. Debugger revealed that the
> problems are caused by the model.frame function which is used to compile
> the '$data' data frame. I don't need that data frame so much, so I just
> substituted model.frame with model.matrix which apparently doesn't cause
> this error with transformed variables. However, I tuned it a bit further,
> so that it should still return an identical '$data' data frame, despite the
> modification.
> I've only tested this with my example and it appears to be OK. Still, I
> think this should be considered a "quick & dirty" fix -there are probably
> better ways to do it. But, I hope it gives the idea. Here's my attempt:
>
> mikko at briscoe R $ diff -u systemfit.R systemfit-patched.R
> --- systemfit.R 2004-11-26 11:17:36.000000000 +0200
> +++ systemfit-patched.R 2005-05-25 18:55:55.568944699 +0300
> @@ -624,7 +624,11 @@
>      Terms <- terms( eqns[[i]], data = data)
>      m$formula <- Terms
>      m <- eval(m, parent.frame())
> -    datai <- model.frame(Terms, m)
> +    resp <- model.extract(m, "response")
> +       ## using model.matrix instead of model.frame, need to get the
> output variable separately
> +    datai <- data.frame(cbind(resp, (model.matrix(Terms, m))[,-1]))
> +       ## I guess there's a better way to extract the name of the output
> variable?
> +       names(datai)[1] <- as.character(terms(eqns[[i]]))[2]
>      if(method=="2SLS" | method=="3SLS") {
>        #datai <- cbind( datai, model.frame( instl[[i]] ))
>        # the following lines have to be substituted for the previous
> @@ -634,7 +638,8 @@
>        Terms <- terms(instl[[i]], data = data)
>        m$formula <- Terms
>        m <- eval(m, parent.frame())
> -      datai <- cbind( datai, model.frame(Terms, m))
> +      ## used previously model.frame
> +      datai <- cbind( datai, as.data.frame((model.matrix(Terms, m))[,-1]))
>      }
>
>      if(i==1) {

Thank you very much for the patch. I have uploaded a patched version of 
systemfit (version 0.7-4) to CRAN. It is also available on my website now:
   http://www.uni-kiel.de/agrarpol/ahenningsen/index-e.html 

Best wishes,
Arne

> Regards,
>
> -Mikko.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From nashjc at uottawa.ca  Thu May 26 01:30:35 2005
From: nashjc at uottawa.ca (Prof J C Nash)
Date: Wed, 25 May 2005 23:30:35 +0000
Subject: [R] [Fwd: Re: [Fwd: failure delivery]]
Message-ID: <42950A9B.80000@uottawa.ca>

I appear to have hit one of the "drop" issues raised in some discussions
a couple of years ago by Frank Harrell. They don't seem to have been
fixed, and I'm under some pressure to get a quick solution for a
forecasting task I'm doing.

I have been modelling some retail sales data, and the days just after
Thanksgiving (US version!) are important. So I created some dummy
variables by a factor called "events" and (really ugly!!) have TG, TG+1,
TG+2, etc. Now I also have DEC1, and the calendar and data are such
that in the period I'm forecasting I have TG+3 but this is
NOT in the estimation data. There are also weekday factors (wdf) and some
cross factors (Saturday + some special days is highly significant).

The model is   Sales ~ daynumber + wdf*events + wdf*specialevents

where daynumber is the day sequence in the year and specialevents is a
set of factors to tell when the business has promotional activities.
The entire model has about 330 coefficients (it seriously needs some
economizing), but only about 140 of these are estimated.

I'm using lm() to do the estimation. I plan to change the model and possibly
the method once I've seen if forecasting works. The current model "works"
moderately well for in-sample fits, though I suspect there is too
much variability generally.

I want to advance 1 week at a time, reestimate, and iterate. This is
a test case where we know the "future". I can get this to work for a few
weeks starting at 20041101, but then get an error msg

		"new factor levels in 'events' ...".

I have tried putting drop.factor.levels = TRUE in predict(), but this
didn't seem to register. Also tried suggestion from web to use

          ifac <- sapply(estndta,is.factor)
          fcstdta[ifac] <- lapply(fcstdta[ifac],factor)

Still get same error.

I've tried a couple of dozen variants on this with no joy.

Finally have tried using the full data set in lm() but set weights for
the estimation period to 1, and those for the forecast period to 0. This
"computes", but the results include NAs at a point where there seems no
reason for them.

I'm starting to suspect that there's some sort of bug somewhere in the R
internals.


  Any advice welcome.



-- 
John C. Nash, School of Management, University of Ottawa,
Vanier Hall 451, 136 Jean-Jacques Lussier Private,
P.O. Box 450, Stn A, Ottawa, Ontario, K1N 6N5 Canada
email: nashjc on mail server uottawa.ca, voice mail: 613 562 5800 X 4796
fax 613 562 5164,  Web URL = http://macnash.admin.uottawa.ca
"Practical Forecasting for Managers" web site is at
http://www.arnoldpublishers.com/support/nash/



From p.murrell at auckland.ac.nz  Thu May 26 01:54:38 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 26 May 2005 11:54:38 +1200
Subject: [R] Any ideas on how to add a dotted line to a box plot to
	indicate a specific value?
References: <4294E096.3050000@bellsouth.net>
Message-ID: <4295103E.80503@stat.auckland.ac.nz>

Hi


BJ wrote:
> Basically, i want to create a boxplot from population data, with a solid 
> line for the median, and then a dotted line for a specific persons 
> value. Is there a way to artificially introduce another line or mark of 
> some kind in a box plot? Thanks again for all of yoru help. Hopefully 
> soon I wont have to keep bugging you all. ~Erithid :-p


Do you mean something like this ... ?

with(ToothGrowth,
      {
        boxplot(len ~ supp)
        # boxplots horizontally centred at 1 and 2
        # 0.6, 1.4 come from default boxwex of 0.8
        #   (1 +/- boxwex/2)
        # 18 I made up
        lines(c(0.6, 1.4), c(18, 18),
              lty="dashed", col="red")
      })

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.murrell at auckland.ac.nz  Thu May 26 01:58:11 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 26 May 2005 11:58:11 +1200
Subject: [R] Plot: Distance between tick and tick label on y-axis
References: <834204C0D7C6D611A3BB000255FC6E9D0DF357EB@lbmsg002.fbn-nbf.local>
Message-ID: <42951113.3050406@stat.auckland.ac.nz>

Hi

You might have more luck with par(mgp), for example, ...

par(las=1)
plot(runif(50), type="l",xaxt="n",yaxt="n",ylab="", bty="l")
axis(2, mgp=c(3, .5, 0))
axis(1, mgp=c(3, .3, 0))

Paul


Lapointe, Pierre wrote:
> Hello,
> 
> I want to reduce the distance between my ticks and their labels.   I managed
> to do it for the x-axis, but the y-axis puzzles me.  Here's an example where
> there is no space between the x-asix ticks and labels.
> 
> par(las=1)
> plot(runif(50), type="l",xaxt="n",yaxt="n",ylab="", bty="l")
> axis(2)
> axis(1,padj=-1.5)
> 
> #However, 
> axis(2,padj=-1.5) #does not work
> 
> I understand from ?axis that padj will take its direction from the par(las).
> In this case, padj will move labels up and down for both x-axis and y-axis.
> I want my y-axis labels to be horizontal.
> 
> I can I reduce the distance between y-axis ticks and labels? 
> 
> 
> Regards,
> 
> Pierre Lapointe
> Assistant Market Strategist
> 
> 
> 
> *********************************************************************************** 
> AVIS DE NON-RESPONSABILITE:\ Ce document transmis par courri...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From step_wolfe at yahoo.com  Thu May 26 02:23:14 2005
From: step_wolfe at yahoo.com (AC)
Date: Wed, 25 May 2005 17:23:14 -0700 (PDT)
Subject: [R] PAN: Need Help for Multiple Imputation Package
Message-ID: <20050526002314.1372.qmail@web53709.mail.yahoo.com>

Hello all.  I am trying to run PAN, multilevel
multiple imputation program, in R to impute missing
data in a longitudinal dataset.  I could successfully
run the multiple imputation when I only imputed one
variable.  However, when I tried to impute a
time-varying covariate as well as a response variable,
I received an error message, ìError: subscript out of
bounds.î  Can anyone tell if my commands contain any
mistakes?

First I imported SAS dataset ësimí which includes a
response variable ëMIY1í, a time-varying covariate
ëTCOV1í, TIME, GROUP (0 or 1), and ID.  200
participants were included and measurement occurred
six times.  Approximately 25% of participants dropped
out at end.      

> sim <- read.xport('c:\\xptds.dat')
> 
> int <- rep(1,1200)
> y <- cbind(sim$MIY1,sim$TCOV1)
> subj <- sim$ID
> pred <- cbind(int, sim$TIME, sim$GROUP)  
> 
> xcol <- 1:3
> zcol <- 1

> prior <- list(a=2,Binv=4,c=2,Dinv=4)

> result <-
pan(y,subj,pred,xcol,zcol,prior,seed=13579,iter=1000)
Error: subscript out of bounds


By the way, I also received the same error message
when I tried to include intercept and time in Zcol, a
matrix for random effect specification.  I used
command ì zcol <- 1:2î.  Does anybody know if this
error is due to sample size/proportion of missing data
or due to command mistake?

I truly appreciate any feedbacks.
Best regards,
Eishi



From jfox at mcmaster.ca  Thu May 26 03:09:52 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 25 May 2005 21:09:52 -0400
Subject: [R] precision problem
In-Reply-To: <3f87cc6d05052508075ce7a23b@mail.gmail.com>
Message-ID: <20050526010952.YZDH27245.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Omar,

It wasn't clear to me from your original question that you wanted to test
that *all* the corresponding entries were equal, as opposed to each
individual entry.

In any event, I don't think that you'll find a similar function for testing
inequality, so you can do as you suggest, but of course without abs().

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omar Lakkis
> Sent: Wednesday, May 25, 2005 10:08 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] precision problem
> 
> all.equal is helpful when I am comparing equality of two matrices.
> However, when I am comparing two individual number with > or 
> < is my best bet doing if( abs(x - y) < tolerence) or is 
> there a function like all.equal that has the same default tolerence?
> 
> On 5/25/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> > Thank you all.
> > all.equal is very helpful since I am also interested in finding the 
> > mismatched prices.
> > 
> > On 5/25/05, John Fox <jfox at mcmaster.ca> wrote:
> > > Dear Omar,
> > >
> > > Perhaps I'm missing something, but why not just subtract 
> one matrix 
> > > from the other and test the difference in relation to the 
> precision 
> > > that you require for the comparison? E.g., to test near equality, 
> > > something like, abs(A - B) < 1e-13.
> > >
> > > I hope this helps,
> > >  John
> > >
> > > --------------------------------
> > > John Fox
> > > Department of Sociology
> > > McMaster University
> > > Hamilton, Ontario
> > > Canada L8S 4M4
> > > 905-525-9140x23604
> > > http://socserv.mcmaster.ca/jfox
> > > --------------------------------
> > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Omar Lakkis
> > > > Sent: Wednesday, May 25, 2005 9:09 AM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: [R] precision problem
> > > >
> > > > I have prices that I am finding difficult to compare with ==,
> > > > > and >, due to precision. For example: the numbers should
> > > > match, with '==', but they differ in the magnitude of 
> 1e-14 due to 
> > > > bunch of calculations that I run on them. Programming 
> with java, I 
> > > > am used to implementing a function that compares the difference 
> > > > between the numbers to a pre determined precision factor. This 
> > > > could be very slow when  I have two matrices of numbers that I 
> > > > could otherwise compare with a simple '==', '>'  or '<' in R.
> > > > What is teh best solution for this problem?
> > > > Can I control the precision of ==, > and < without having to 
> > > > reimplement the operations in a slow way?
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Pierre.Lapointe at nbf.ca  Thu May 26 03:29:29 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Wed, 25 May 2005 21:29:29 -0400
Subject: [R] Useful tip: Use Google to find R scripts
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF357F0@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050525/74f168ea/attachment.pl

From weigand.stephen at charter.net  Thu May 26 04:44:56 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Wed, 25 May 2005 21:44:56 -0500
Subject: [R] aggregate and stack
In-Reply-To: <022e01c5618e$763f2ff0$f10aa8c0@paulobrando>
References: <022e01c5618e$763f2ff0$f10aa8c0@paulobrando>
Message-ID: <27a9f290c3ad0fbde7b87caaa31fd020@charter.net>

Dear Paulo,

On May 25, 2005, at 8:01 PM, Paulo Brando wrote:

> Dear All,
>
> I have tried to calculate tree mean growth but I think the structure I  
> used below (growthresumo) is not the most elegant, even though it  
> worked. The only problem I had in this first part was that  I cannot  
> use 'summary', just 'mean' (sorry but 'R' is pretty new for me).
>

In case you didn't notice, help(aggregate) indicates that 'FUN'
should be a scalar function, so summary won't work for that reason.

>> growthresumo <-  
>> aggregate(growth[,c(16,19,23,27,31,35,39,43,47,52,56,60,64,68,72,76,81 
>> ,85,89,93,97,101,105,109,113,117,121,125,129,133,137,
> 141,145,149,153,157,161,165,169,173,177,181,185,189,194,197,201,205,209 
> ,213,217,221,225,229,233,237,241)],
> by=(growth[,c(3,8)]),MEAN,na.rm=TRUE)
>

It's hard to know where 'growth' came from. Is it your own data.frame,
or from a package? It's better to provide a reproducible or toy example
(as you'll often read here).

> #after growth is calculated, I want to stack the results in just one  
> colunm.
>
>> growthvertical <- c(growthresumo[,3],...,growthresumo[,50]) # this is  
>> very time consuming though
>

This comes to my mind:

as.vector(as.matrix(growthresumo[,3:50]))

but look up the help on stack() because it's a very powerful tool that
is aptly named (and might do everything you want).

>> Parcel <- c("C9","S8"..."C9","S8") # 50 items
>

rep() could help with the above.

>> date < c("DATE1"...."DATE50")
>

paste() will help with this.

>> growthpermonth <- data.frame(Parcel, Date, growthvertical)
>
> Thank you very much!

Good luck with R!

Stephen



From munguiar at ecologia.edu.mx  Thu May 26 06:36:06 2005
From: munguiar at ecologia.edu.mx (Roberto Edmundo Munguia Steyer)
Date: Thu, 26 May 2005 04:36:06 GMT
Subject: [R] estimating p values bootstrap regression  
Message-ID: <20050526040811.8F936E6CCA@dns.ecologia.edu.mx>

Dear R-users:

I have a basic question.

I am trying to estimate the gradients of sexual selection on the males of a 
waterbug species.I performed a bootstrap analysis on a multiple regression 
using library boot and obtained the estimated coefficents and their respective 
standard errors. 

eri<-read.table("D:/efi/er2.txt",header=T)
grad1<-eri
grad1$fit<-fitted(gradi)
grad1$res<-resid(gradi)
grad.fit<-function(data){
mod<-lm(data$huevrel~data$slongitud*data$sabdomen*data$saparcap, 
weights=data$presencia, data=eri)
c(coef(mod),
summary(mod)$coef[,2]^2)}

gmodfun<-function(d,i){
d$huevrel<-d$fit+d$res[i]
grad.fit(d)}
grandi<-boot(grad1,gmodfun, R=999)
grandi


I was wondering if there is a way to estimate p-values in order to know wich 
independent variables are significant. Would be correct to add the bias to the 
regression coefficients and divide with de SE to obtain a t value and use pt 
function eg (1 - pt(abs(1.51),211))*2 in order to get the p values? 

Thanks a lot for your help.

Roberto
I use R 2.1, Windows XP.
 

Departamento de Biologia Evolutiva
Instituto de Ecolog??a, A.C.
Km 2.5 Carretera antigua a Coatepec
Ap. Postal 63 (excepto mensajer??a)
Xalapa, Veracruz 91000
MEXICO
Tel. (52)(228)8421800 ext.3009


 


---------------------------------------------
Instituto de Ecolog??a, A. C.
http://www.ecologia.edu.mx



From Stefano.Guazzetti at ausl.re.it  Thu May 26 08:35:57 2005
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Thu, 26 May 2005 08:35:57 +0200
Subject: R: [R] weighted.mean and tapply (again)
Message-ID: <B8A1EED732379B44A7E59D22E82E4442FB2D8D@IMHOTEP.ausl.org>

what about using "mapply"?

 splitted.value<-with(x.1, split(VALUE, GROUP))
 splitted.freq<-with(x.1, split(FREQUENCY, GROUP))
 mapply(weighted.mean, splitted.value, w=splitted.freq)


Stefano


   >-----Messaggio originale-----
   >Da: r-help-bounces at stat.math.ethz.ch
   >[mailto:r-help-bounces at stat.math.ethz.ch]Per conto di
   >james.holtman at convergys.com
   >Inviato: mercoled?? 25 maggio 2005 17.57
   >A: Dan Bolser
   >Cc: R mailing list; r-help-bounces at stat.math.ethz.ch
   >Oggetto: Re: [R] weighted.mean and tapply (again)
   >
   >
   >
   >
   >
   >
   >> x.1 <- read.table('clipboard',header=T)
   >> x.1
   >   GROUP VALUE FREQUENCY
   >1      2     2        78
   >2      2     3        40
   >3      2     4        16
   >4      2     5         3
   >5      2     6         1
   >6      2     8         1
   >7      3     3        19
   >8      3     4        10
   >9      3     5        19
   >10     3     6         4
   >> by(x.1, x.1$GROUP, function(x) weighted.mean(x$VALUE, 
   >x$FREQUENCY))
   >x.1$GROUP: 2
   >[1] 2.654676
   >------------------------------------------------------------
   >---------------
   >x.1$GROUP: 3
   >[1] 4.153846
   >>
   >
   >Jim
   >__________________________________________________________
   >James Holtman        "What is the problem you are trying to solve?"
   >Executive Technical Consultant  --  Office of Technology, Convergys
   >james.holtman at convergys.com
   >+1 (513) 723-2929
   >
   >
   >                                                            
   >                                                            
   >                   
   >                      Dan Bolser                            
   >                                                            
   >                   
   >                      <dmb at mrc-dunn.cam.ac.        To:      
   > R mailing list <r-help at stat.math.ethz.ch>                  
   >                   
   >                      uk>                          cc:      
   >                                                            
   >                   
   >                      Sent by:                     Subject: 
   > [R] weighted.mean and tapply (again)                       
   >                   
   >                      r-help-bounces at stat.m                 
   >                                                            
   >                   
   >                      ath.ethz.ch                           
   >                                                            
   >                   
   >                                                            
   >                                                            
   >                   
   >                                                            
   >                                                            
   >                   
   >                      05/25/2005 11:33                      
   >                                                            
   >                   
   >                                                            
   >                                                            
   >                   
   >
   >
   >
   >
   >
   >I read answers to questions including the words "tapply" and
   >"weighted.mean", but I didn't understand either the problem 
   >(data) or the
   >solution provided.
   >
   >Here is my question ...
   >
   >> dat[1:10,]
   >  GROUP  VALUE FREQUENCY
   >1     2      2        78
   >2     2      3        40
   >3     2      4        16
   >4     2      5         3
   >5     2      6         1
   >6     2      8         1
   >7     3      3        19
   >8     3      4        10
   >9     3      5        19
   >1     3      6         4
   >
   >
   >For each GROUP, I would like to calculate the weighted.mean 
   >of VALUE using
   >the FREQUENCY as the weight, so for the snippet of data 
   >shown that would
   >be...
   >
   >group.2 <- weighted.mean(c(2,3,4,5,6,8),c(78,40,16,3,1,1))
   >group.3 <- weighted.mean(c(3,4,5,6),    c(19,10,19,4))
   >
   >> cbind(rbind(2,3),rbind(group.2,group.3))
   >        [,1]     [,2]
   >group.2    2 2.654676
   >group.3    3 4.153846
   >
   >I would like to use tapply to automatically do this across the whole
   >dataset (dat) - which includes lots of other distinct 
   >grouping factors,
   >however, like I said, I couldn't understand (and therefore 
   >apply to my
   >data) any of the other solutions I found, so any help here would be
   >greatly appreciated!
   >
   >All the best,
   >Dan.
   >
   >______________________________________________
   >R-help at stat.math.ethz.ch mailing list
   >https://stat.ethz.ch/mailman/listinfo/r-help
   >PLEASE do read the posting guide!
   >http://www.R-project.org/posting-guide.html
   >
   >______________________________________________
   >R-help at stat.math.ethz.ch mailing list
   >https://stat.ethz.ch/mailman/listinfo/r-help
   >PLEASE do read the posting guide! 
   >http://www.R-project.org/posting-guide.html
   >



From vincent at 7d4.com  Thu May 26 09:20:38 2005
From: vincent at 7d4.com (vincent)
Date: Thu, 26 May 2005 09:20:38 +0200
Subject: [R] Any ideas on how to add a dotted line to a box plot to
	indicate a specific value?
In-Reply-To: <4294E096.3050000@bellsouth.net>
References: <4294E096.3050000@bellsouth.net>
Message-ID: <429578C6.70803@7d4.com>

lines(...) could perhaps be useful ?
hih



From deleeuw at stat.ucla.edu  Wed May 25 00:43:15 2005
From: deleeuw at stat.ucla.edu (Jan de Leeuw)
Date: Tue, 24 May 2005 15:43:15 -0700
Subject: [R] [R-pkgs] R Packages and code published in JSS in 2005
Message-ID: <7B373C8F-3647-4BCD-9884-58D785222759@stat.ucla.edu>


Get articles from http://www.jstatsoft.org

Firth
Bradley-Terry Models in R
Volume 12, Issue 01

Sturtz, Ligges, and Gelman
R2WinBUGS: A Package for Running WinBUGS from R
Volume 12, Issue 03

Mineo and Ruggieri
A Software Tool for the Exponential Power Distribution: The normalp  
Package
Volume 12, Issue 04

Ritz and Streibig
Bioassay Analysis Using R
Volume 12, Issue 05

Baddeley and Turner
spatstat: An R Package for Analyzing Spatial Point Patterns
Volume 12, Issue 06

Johnstone and Silverman
EbayesThresh: R Programs for Empirical Bayes Thresholding
Volume 12, Issue 08

Nason
pinktoe: Semi-automatic Traversal of Trees
Volume 14, Issue 01

Imai and Van Dyk
MNP: R Package for Fitting the Multinomial Probit Model
Volume 14, Issue 03

Buttrey
Calling the lp_solve Linear Program Software from R, S-PLUS and Excel
Volume 14, Issue 04

Zeileis and Grothendieck
zoo: S3 Infrastructure for Regular and Irregular Time Series
Volume 14, Issue 06

Terpstra and McKean
Rank-Based Analyses of Linear Models using R
Volume 14, Issue 07

===
Jan de Leeuw; Distinguished Professor and Chair, UCLA Department of  
Statistics;
Editor: Journal of Multivariate Analysis, Journal of Statistical  
Software
US mail: 8130 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095-1554
phone (310)-825-9550;  fax (310)-206-5658;  email: deleeuw at stat.ucla.edu
.mac: jdeleeuw ++++++  aim: deleeuwjan ++++++ skype: j_deleeuw
homepages: http://gifi.stat.ucla.edu ++++++ http://www.cuddyvalley.org
   
------------------------------------------------------------------------ 
-------------------------
           No matter where you go, there you are. --- Buckaroo Banzai
                    http://gifi.stat.ucla.edu/sounds/nomatter.au

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From pls at mevik.net  Sun May 22 23:15:59 2005
From: pls at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Sun, 22 May 2005 23:15:59 +0200
Subject: [R] [R-pkgs] pls version 1.0-3
Message-ID: <m0br73x8q8.fsf@bar.nemo-project.org>

Version 1.0-3 of the pls package is now available on CRAN.

The pls package implements partial least squares regression (PLSR) and
principal component regression (PCR).  Features of the package include

- Several plsr algorithms: orthogonal scores, kernel pls and simpls
- Flexible cross-validation
- A formula interface, with traditional methods like predict, coef,
  plot and summary
- Functions for extraction of scores and loadings, and calculation of
  (R)MSEP and R2
- A simple multiplicative scatter correction (msc) implementation
- Functions for plotting predictions, validation statistics,
  coefficients, scores, loadings and biplots.

(The pls package is meant to supersede the pls.pcr package.)

-- 
Ron Wehrens and Bj??rn-Helge Mevik

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From lbajuk at insightful.com  Wed May 25 02:04:31 2005
From: lbajuk at insightful.com (Lou Bajuk)
Date: Tue, 24 May 2005 17:04:31 -0700
Subject: [R] [Job ad] Insightful looking for statistical consultants to meet
	increasing consulting demand
Message-ID: <4A9EF66708CA0B4395CB5D88D13C5CA201731862@se2kexch01.insightful.com>

Insightful, the maker of S-PLUS, is looking for people with strong
statistical backgrounds and good experience in using S-PLUS or R for data
analysis, statistical modelling and statistical programming for permanent
and/or contract positions to meet increasing demands for S-PLUS consulting. 

For a full description and contact information, please see
http://www.insightful.com/company/jobdescription.asp?JobID=62

Best regards,
Lou
____________________________________________
Lou Bajuk-Yorgan 
Director of Professional Services and Sales Support
Insightful Corp.
1700 Westlake Ave N, Suite 500
Seattle WA 98109
PH: 800-569-0123 x328 / 206-802-2328
FAX: 206-283-8691



From hb at maths.lth.se  Thu May 26 10:58:37 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 26 May 2005 10:58:37 +0200
Subject: [R] website reference for building R packages
In-Reply-To: <1db7268005052513234b0dd3f0@mail.gmail.com>
References: <BAY10-F506FC10C31172C8BA726ADD60D0@phx.gbl>	<42929EA3.6020708@columbia.edu>	<42943B70.6050107@statistik.uni-dortmund.de>
	<1db7268005052513234b0dd3f0@mail.gmail.com>
Message-ID: <42958FBD.1020200@maths.lth.se>

To get a Command prompt in Windows that is ready for R and its tools, 
see "RCMDprompt.bat" at http://www.maths.lth.se/help/R/. It will fix 
your problem.

Cheers

Henrik

roger bos wrote:
> I used this tutorial and completed steps 1-6 without errors, but on
> step 7 I tried to run R CMD check on my package called 'ram' directory
> one level above the 'ram' folder and I got the following error?
> 
> I:\R_HOME>R CMD check ram
> * checking for working latex ...Error: environment variable TMPDIR not set (or s
> et to unusable value) and no default available.
>  at c:\R\rw2010\share\perl/R/Utils.pm line 72
> 
> Can anyone help me with this?
> 
> Thanks,
> 
> Roger
> 
> 
> 
> On 5/25/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Suresh Krishna wrote:
>>
>>
>>>it is the first link if you type "making packages" into the google
>>>search box here:
>>>
>>>http://maths.newcastle.edu.au/~rking/R/
>>
>>
>>Yes, please look into the archives as the posting guide asks you to do!
>>
>>
>>
>>>-s.
>>>
>>>
>>>Laura Holt wrote:
>>>
>>>
>>>>Hi R People:
>>>>
>>>>A few weeks ago, someone put a link to a website for "how to" for
>>>>building R packages.  It was very nice.
>>
>>
>>Writing R Extensions is an *complete* and *up to date* documentation for
>>this task. Package management is being steadily improved.
>>Really, I recommend to use "Writing R Extensions"!
>>
>>If you are working on Windows, you might want to look into the R
>>Administration and Installation manual as well which descibes how to
>>collect and set up the required tools...
>>
>>Uwe Ligges
>>
>>
>>
>>>>But of course, I have misplaced the link.  Does anyone still have
>>>>that, please?
>>>>
>>>>It was someone from the University of Chicago, I believe.
>>>>
>>>>Thanks in advance.
>>>>
>>>>Sincerely,
>>>>Laura Holt
>>>>mailto: lauraholt_983 at hotmail.com
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From constant.depiereux at aqte.be  Thu May 26 11:12:02 2005
From: constant.depiereux at aqte.be (=?ISO-8859-1?Q?Constant_Depi=E8reux?=)
Date: Thu, 26 May 2005 11:12:02 +0200
Subject: [R] Help needed - Use of RSrvr in macro mode
Message-ID: <429592E2.3020309@aqte.be>

Dear All,

I am experiencing a problem for which I need some help.

I have created a small file containing a simple function

File name = smalltest.r

Content :

smalltest=function(extvar)
{

itworks=paste('Ca marche',date())

setwd("c:/windows/temp")

write.table(itworks,file="resultat.csv", append=FALSE)

}

This function works when operated from R environment

To get it performed from Excel, I have write a small function as follows :

Excel file : see attachment (one sheet named 'test', one cell A1 
containing : smalltest(NA))

Macro :

Sub smalltest()

    Range("A3").Select
    Range(Selection, Selection.End(xlToRight)).Select
    Range(Selection, Selection.End(xlDown)).Select
    Selection.ClearContents
    Call RInterface.StartRServer
    Call RInterface.RunRFile("h:\R-Sources\smalltest.r")
    Windows("smalltest.xls").Activate
    CommandString = Range("test!a1")
    MsgBox (CommandString)
    Call RInterface.RRun("CommandString")
    Workbooks.Open Filename:="c:\windows\temp\resultat.csv"
    Range(Selection, Selection.End(xlToRight)).Select
    Range(Selection, Selection.End(xlDown)).Select
    Selection.Copy
    Windows("smalltest.xls").Activate
    Range("A3").Select
    ActiveSheet.Paste


End Sub

Function works (i.e. does complete without error message) when 
c:\windows\temp\resultat.csv exists (created using above smalltest 
function in R environment) but hangs when operated from excel (file not 
found).

My machine is a 4 Gb Memory - PIV 3.2Ghz  with disk mirroring running 
latest update of XP Pro SP2.

Both R and RServer are latest editions available.

Does anybody have an idea of what the problem might be?

Many thanks for your advices.

Best regards.


Constant Depi??reux


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: smalltest.r
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050526/f7f4e89c/smalltest.pl

From paul.bliese at us.army.mil  Thu May 26 11:31:08 2005
From: paul.bliese at us.army.mil (Bliese, Paul D LTC USAMH)
Date: Thu, 26 May 2005 11:31:08 +0200
Subject: [R] read.spss in R 2.1.0 & make basic dataframe
Message-ID: <FADCFAA8BA80C748890C1D3893C198D95856A3@amedmlmhah01.eur.amed.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050526/50875e38/attachment.pl

From Sebastian.Leuzinger at unibas.ch  Thu May 26 12:13:19 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Thu, 26 May 2005 12:13:19 +0200
Subject: [R] reading .irb files into R
Message-ID: <200505261213.19778.Sebastian.Leuzinger@unibas.ch>


Hi,
I am wondering if anybody has experience with importing .irb files 
(Temperature matrices from infratec Thermal cameras) into R.
A hint as to what package could be used would be appreciated.

OS Linux 9.3
R: 2.1.0

Sebastian



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu May 26 13:33:58 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 26 May 2005 07:33:58 -0400
Subject: [R] Useful tip: Use Google to find R scripts
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F407C@us-arlington-0668.mail.saic.com>

Great tip. Thanks.

One problem: a lot of hits are in REBOL language (whatever it is) that also
uses R extension.

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lapointe, Pierre
Sent: Wednesday, May 25, 2005 9:29 PM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] Useful tip: Use Google to find R scripts

Hello,

Ever wondered how people use a particular function in their programs?  Use
Google to find actual scripts:

filetype:R boxplot

will return real R scripts using the boxplot function.

Regards,

Pierre Lapointe
Assistant Market Strategist



****************************************************************************
*******
AVIS DE NON-RESPONSABILITE:\ Ce document transmis par courri...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Thu May 26 14:06:11 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 May 2005 08:06:11 -0400
Subject: [R] Useful tip: Use Google to find R scripts
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F407C@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F407C@us-arlington-0668.mail.saic.com>
Message-ID: <971536df05052605061559327a@mail.gmail.com>

Try:

filetype:R boxplot -rebol

On 5/26/05, Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI at saic.com> wrote:
> Great tip. Thanks.
> 
> One problem: a lot of hits are in REBOL language (whatever it is) that also
> uses R extension.
> 
> Jarek
> ====================================================\=======
> 
>  Jarek Tuszynski, PhD.                           o / \
>  Science Applications International Corporation  <\__,|
>  (703) 676-4192                                   ">   \
>  Jaroslaw.W.Tuszynski at saic.com                     `    \
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lapointe, Pierre
> Sent: Wednesday, May 25, 2005 9:29 PM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] Useful tip: Use Google to find R scripts
> 
> Hello,
> 
> Ever wondered how people use a particular function in their programs?  Use
> Google to find actual scripts:
> 
> filetype:R boxplot
> 
> will return real R scripts using the boxplot function.
> 
> Regards,
> 
> Pierre Lapointe
> Assistant Market Strategist
> 
> 
> 
> ****************************************************************************
> *******
> AVIS DE NON-RESPONSABILITE:\ Ce document transmis par courri...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rolf at math.unb.ca  Thu May 26 14:24:38 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 26 May 2005 09:24:38 -0300 (ADT)
Subject: [R] Single factor from interaction.
Message-ID: <200505261224.j4QCOcbD011768@erdos.math.unb.ca>


I have a vague recollection of seeing reference, fairly recently, to
a function that forms a single factor which ``codes'' the interaction
between two (or more?) factors.  Do I recollect correctly?  It would
be easy enough to roll one's own, but if there is an existing
function it probably does a much cleverer job than I would do.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From ccleland at optonline.net  Thu May 26 14:41:26 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 26 May 2005 08:41:26 -0400
Subject: [R] Single factor from interaction.
In-Reply-To: <200505261224.j4QCOcbD011768@erdos.math.unb.ca>
References: <200505261224.j4QCOcbD011768@erdos.math.unb.ca>
Message-ID: <4295C3F6.6060405@optonline.net>

Are you looking for interaction() ?

a <- gl(2, 4, 8)
b <- gl(2, 2, 8, label = c("ctrl", "treat"))
s <- gl(2, 1, 8, label = c("M", "F"))

interaction(a, b)
[1] 1.ctrl  1.ctrl  1.treat 1.treat 2.ctrl  2.ctrl  2.treat 2.treat
Levels: 1.ctrl 2.ctrl 1.treat 2.treat

interaction(a, b, s, sep = ":")
[1] 1:ctrl:M  1:ctrl:F  1:treat:M 1:treat:F 2:ctrl:M  2:ctrl:F 
2:treat:M 2:treat:F

Levels: 1:ctrl:M 2:ctrl:M 1:treat:M 2:treat:M 1:ctrl:F 2:ctrl:F 
1:treat:F 2:treat:F

Rolf Turner wrote:
> I have a vague recollection of seeing reference, fairly recently, to
> a function that forms a single factor which ``codes'' the interaction
> between two (or more?) factors.  Do I recollect correctly?  It would
> be easy enough to roll one's own, but if there is an existing
> function it probably does a much cleverer job than I would do.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From john.maindonald at anu.edu.au  Thu May 26 14:43:34 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 26 May 2005 13:43:34 +0100
Subject: [R] colors and palettes and things... 
In-Reply-To: <200505241004.j4OA3llh022828@hypatia.math.ethz.ch>
References: <200505241004.j4OA3llh022828@hypatia.math.ethz.ch>
Message-ID: <fd7c280dbee5c682232476464df74051@anu.edu.au>

The DAAG package has the following:

   show.colors(type=c("singles", "shades", "grayshades"), 
order.cols=TRUE)

I am sure there are better ways to do the ordering than my ad hoc 
approach,
though.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

On 24 May 2005, at 11:04 AM, r-help-request at stat.math.ethz.ch wrote:

> From: "Jeff D. Hamann" <jeff.hamann at forestinformatics.com>
> Date: 23 May 2005 4:35:27 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] colors and palettes and things...
> Reply-To: jeff.hamann at forestinformatics.com
>
>
> After trying to find if there was a color picker in the FAQs and the 
> help,
> I thought I would send a post here. I was overwhelmed with all the
> wonderful color choices R has predefined (discovered after typing in
> colors()) but can't figure out what they all (by name) look like. Is 
> there
> a color picker or some other method to display all those colors next to
> the name?
>
> I think I can put together palettes, but another question I have then
> regards the building of palettes (a list of variable length I can 
> select
> or create myself other than the ones defined by Palette) so I can pass
> these colors into functions instead of having to predefine a bunch of
> colors myself or use the predefined colors like terrain.colors(n)?
>
> Are there groups of colors in the colors() that I can group together to
> make some nice palettes for drawing barplots, etc?
>
> Thanks,
> Jeff.
>
>
> -- 
> Jeff D. Hamann
> Forest Informatics, Inc.
> PO Box 1421
> Corvallis, Oregon 97339-1421
> phone 541-754-1428
> fax 541-752-0288
> jeff.hamann at forestinformatics.com
> www.forestinformatics.com



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu May 26 15:17:18 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 26 May 2005 09:17:18 -0400
Subject: [R] Reading text files and readLine
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F407D@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050526/1c4aa1ce/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu May 26 15:36:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 May 2005 15:36:47 +0200
Subject: [R] website reference for building R packages
In-Reply-To: <1db7268005052513234b0dd3f0@mail.gmail.com>
References: <BAY10-F506FC10C31172C8BA726ADD60D0@phx.gbl>	
	<42929EA3.6020708@columbia.edu>	
	<42943B70.6050107@statistik.uni-dortmund.de>
	<1db7268005052513234b0dd3f0@mail.gmail.com>
Message-ID: <4295D0EF.7050400@statistik.uni-dortmund.de>

roger bos wrote:
> I used this tutorial and completed steps 1-6 without errors, but on
> step 7 I tried to run R CMD check on my package called 'ram' directory
> one level above the 'ram' folder and I got the following error?
> 
> I:\R_HOME>R CMD check ram
> * checking for working latex ...Error: environment variable TMPDIR not set (or s
> et to unusable value) and no default available.
>  at c:\R\rw2010\share\perl/R/Utils.pm line 72
> 
> Can anyone help me with this?


Yes, and this is exactly the reason why I do recommend the R manuals: 
they are much more complete for this topic!
It is described in Sections 3.1.9 "Checking the build" and 5.1 
"Installing packages" of R-2.1.0's "R Installation and Administration" 
manual.

Uwe Ligges



> Thanks,
> 
> Roger
> 
> 
> 
> On 5/25/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Suresh Krishna wrote:
>>
>>
>>>it is the first link if you type "making packages" into the google
>>>search box here:
>>>
>>>http://maths.newcastle.edu.au/~rking/R/
>>
>>
>>Yes, please look into the archives as the posting guide asks you to do!
>>
>>
>>
>>>-s.
>>>
>>>
>>>Laura Holt wrote:
>>>
>>>
>>>>Hi R People:
>>>>
>>>>A few weeks ago, someone put a link to a website for "how to" for
>>>>building R packages.  It was very nice.
>>
>>
>>Writing R Extensions is an *complete* and *up to date* documentation for
>>this task. Package management is being steadily improved.
>>Really, I recommend to use "Writing R Extensions"!
>>
>>If you are working on Windows, you might want to look into the R
>>Administration and Installation manual as well which descibes how to
>>collect and set up the required tools...
>>
>>Uwe Ligges
>>
>>
>>
>>>>But of course, I have misplaced the link.  Does anyone still have
>>>>that, please?
>>>>
>>>>It was someone from the University of Chicago, I believe.
>>>>
>>>>Thanks in advance.
>>>>
>>>>Sincerely,
>>>>Laura Holt
>>>>mailto: lauraholt_983 at hotmail.com
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>>



From francoisromain at free.fr  Thu May 26 15:34:09 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 26 May 2005 15:34:09 +0200
Subject: [R] Reading text files and readLine
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F407D@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F407D@us-arlington-0668.mail.saic.com>
Message-ID: <4295D051.8030701@free.fr>

Hello,

You can append an end-of-line character in your file doing :
 > sink("test2.xml",append=TRUE)
 > cat("\n")
 > sink()

Romain

 Le 26.05.2005 15:17, Tuszynski, Jaroslaw W. a ??crit :

>Hi,
>
>I am trying to write a function to read in a whole text file as a single
>string ( so I can calculate its "sha1" hash function using package
>"digest"). I need a single string containing the whole file, and so far I
>was using paste(readLines(fileName), collapse = ""). Unfortunately this
>function gives me warnings :
>
>	incomplete final line found by readLines on 'test2.xml' 
>
>due to lack of end-of-line character on the end of the file. Is there a way
>to suppress this warning? Or another function capable of reading whole text
>file into a string?
>
>Jarek
>=====================================\====                 
> Jarek Tuszynski, PhD.                               o / \ 
> Science Applications International Corporation  <\__,|  
> (703) 676-4192                        ">  \
> Jaroslaw.W.Tuszynski at saic.com                   `    \
>
>  
>

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ggrothendieck at gmail.com  Thu May 26 16:02:25 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 May 2005 10:02:25 -0400
Subject: [R] Reading text files and readLine
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F407D@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F407D@us-arlington-0668.mail.saic.com>
Message-ID: <971536df050526070238121bf8@mail.gmail.com>

If its just a matter of suppressing the warning see:

?suppressWarnings

On 5/26/05, Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI at saic.com> wrote:
> Hi,
> 
> I am trying to write a function to read in a whole text file as a single
> string ( so I can calculate its "sha1" hash function using package
> "digest"). I need a single string containing the whole file, and so far I
> was using paste(readLines(fileName), collapse = ""). Unfortunately this
> function gives me warnings :
> 
>        incomplete final line found by readLines on 'test2.xml'
> 
> due to lack of end-of-line character on the end of the file. Is there a way
> to suppress this warning? Or another function capable of reading whole text
> file into a string?
> 
> Jarek
> =====================================\====
>  Jarek Tuszynski, PhD.                               o / \
>  Science Applications International Corporation  <\__,|
>  (703) 676-4192                        ">  \
>  Jaroslaw.W.Tuszynski at saic.com                   `    \
> 
> 
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Thu May 26 16:08:09 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 26 May 2005 07:08:09 -0700 (PDT)
Subject: [R] read.spss in R 2.1.0 & make basic dataframe
In-Reply-To: <FADCFAA8BA80C748890C1D3893C198D95856A3@amedmlmhah01.eur.amed.ds.army.mil>
References: <FADCFAA8BA80C748890C1D3893C198D95856A3@amedmlmhah01.eur.amed.ds.army.mil>
Message-ID: <Pine.A41.4.61b.0505260656330.340892@homer07.u.washington.edu>

On Thu, 26 May 2005, Bliese, Paul D LTC USAMH wrote:
> On a related note, do other users routinely use read.spss with the
> defaults of  "to.data.frame=F" or "use.value.labels=T"?  My experience
> is that I am always using the non-default values in which case it would
> be helpful to change the defaults to "to.data.frame=T" and
> "use.value.labels=F".  It would also probably make sense to change the
> default for "trim.factor.names=T".  Interested in others' perspective.
>

Actually, most of this is me rather than Saikat.

I use use.value.labels=TRUE most of the time.  The main point of 
to.data.frame=TRUE is that it is quite a lot faster for large files, 
especially if you are going to use only a few of the variables. I think 
Brian Ripley spoke up in favour of it for this reason last time the issue 
was raised.

The reason I made trim.factor.names=FALSE the default was backwards 
compatibility, but it probably makes sense to switch it at some point.

Incidentally, PSPP (the original source of the code) now has a version 
that reads long variable names from post-version 12 SPSS files. This 
confirms that the "unrecognised record type 7, subtype 13" message really 
is due to long variable names and so is harmless.  It also means that 
anyone who wants long variable names badly enough could work out a patch.


 	-thomas



From owzar001 at mc.duke.edu  Thu May 26 16:09:51 2005
From: owzar001 at mc.duke.edu (Kouros Owzar)
Date: Thu, 26 May 2005 10:09:51 -0400
Subject: [R] Kouros Owzar is ooo.
Message-ID: <OF04463F3F.7DD03B1C-ON8525700D.004DCEB8-8525700D.004DCEB8@notes.duke.edu>


I will be out of the office starting  05/25/2005 and will not return until
06/01/2005.

I will be ooo 05/26-06/01



From neo27 at t-online.de  Thu May 26 16:10:41 2005
From: neo27 at t-online.de (Mark Hempelmann)
Date: Thu, 26 May 2005 16:10:41 +0200
Subject: [R] Survey and Stratification
Message-ID: <4295D8E1.509@t-online.de>

Dear WizaRds,

	Working through sampling theory, I tried to comprehend the concept of 
stratification and apply it with Survey to a small example. My question 
is more of theoretic nature, so I apologize if this does not fully fit 
this board's intention, but I have come to a complete stop in my efforts 
and need an expert to help me along. Please help:

age<-matrix(c(rep(1,5), rep(2,3), 1:8, rep(3,5), rep(4,3), rep(5,5), 
rep(3,3), rep(15,5), rep(12,3), 23,25,27,21,22, 33,27,29), ncol=6, byrow=F)
colnames(age)<-c("stratum", "id", "weight", "nh", "Nh", "y")
age<-as.data.frame(age)

## create survey design object
age.des1<-svydesign(ids=~id, strata=~stratum, weight=~Nh, data=age)
svymean(~y, age.des1)
## gives mean 25.568, SE 0.9257

age.des2<-svydesign(ids=~id, strata=~stratum, weight=~I(nh/Nh), data=age)
svymean(~y, age.des2)
## gives mean 25.483, SE 0.9227

age.des3<-svydesign(ids=~id, strata=~stratum, weight=~weight, data=age)
svymean(~y, age.des3)
## gives mean 26.296, SE 0.9862

age.des4<-svydesign(ids=~id, strata=~stratum, data=age)
svymean(~y, age.des4)
## gives mean 25.875, SE 0.9437

age.des3 is the only estimator I am able to compute per hand correctly. 
It is stratified random sampling with inverse probablility weighting 
with weight= nh/Nh ## sample size/ stratum size.

Basically, I thought the option weight=~Nh as well as weight=~I(nh/Nh) 
would result in the same number, but it does not. I am reading 
Thompson(02), Cochran(77) and of course Lumley on his Survey package, 
but I can't find my mistake.

I thought the Hansen-Hurwitz estimator per stratum offers the right numbers:
p1=5/15, p2=3/12, so y1.total=1/5*(3*118), y2.total=1/3*(4*89) and the 
stratified estimator with this design should be: 
1/27(y1.total+y2.total), obviously wrong. How on earth do I get the 
numbers Survey is calculating?

I am very sorry to bother you with this problem, however, I didn't find 
anybody who was willing to help me.

Thank you so much
Mark



From rchandler at forwild.umass.edu  Thu May 26 16:28:01 2005
From: rchandler at forwild.umass.edu (Richard Chandler)
Date: Thu, 26 May 2005 10:28:01 -0400
Subject: [R] two-part models
Message-ID: <1117117681.4295dcf1a3d75@mail-www2.oit.umass.edu>

Hello,

Is there a library that can be used to implement two-part models for
data with 'extra' zeros?  I am already aware of some of the
zero-inflated poisson functions out there and of the negative binomial
glm option. Thanks.

Richard

-- 
Richard Chandler
Department of Natural Resources Conservation
UMass Amherst
(413)545-1237



From deleeuw at stat.ucla.edu  Thu May 26 16:29:38 2005
From: deleeuw at stat.ucla.edu (Jan de Leeuw)
Date: Thu, 26 May 2005 07:29:38 -0700
Subject: [R] Users of the CRAN homals package
Message-ID: <5A97F0C3-6251-446A-B325-4F6E7FBB2A47@stat.ucla.edu>

The homals package will eventually be replaced by the gifi
package, which is in a preliminary version at

http://gifi.stat.ucla.edu/240/code/gifi-0.0.1.tar.gz

It has many more things than the homals package on
CRAN, but it is not ready for CRAN yet (will take quite a while).
Besides homals (multiple correspondence analysis with rank and  
additivity
constraints, can do regression, canonical analysis, principal component
analysis with optimal scaling of the variables, and so on), it has

-- interactive coding
-- fuzzy coding using B splines
-- maximizing arbitrary convex aspects of correlation matrices
-- approximating Benz??cri distances
-- projection plots from homals
-- simple correspondence analysis with many plot options
-- lineals program
-- coding dataframes as indicator matrices
-- making profile frequencies from data frames

More will be added. I will also recode most of the MCA stuff using
as the basic data structure a list of possibly fuzzy  indicators, coded
as sparse matrices.

Install into R by saying

R CMD INSTALL gifi-0.0.1.tar.gz

from the command line.

===
Jan de Leeuw; Distinguished Professor and Chair, UCLA Department of  
Statistics;
Editor: Journal of Multivariate Analysis, Journal of Statistical  
Software
US mail: 8130 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095-1554
phone (310)-825-9550;  fax (310)-206-5658;  email: deleeuw at stat.ucla.edu
.mac: jdeleeuw ++++++  aim: deleeuwjan ++++++ skype: j_deleeuw
homepages: http://gifi.stat.ucla.edu ++++++ http://www.cuddyvalley.org
   
------------------------------------------------------------------------ 
-------------------------
           No matter where you go, there you are. --- Buckaroo Banzai
                    http://gifi.stat.ucla.edu/sounds/nomatter.au



From ligges at statistik.uni-dortmund.de  Thu May 26 16:37:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 May 2005 16:37:44 +0200
Subject: [R] read.spss in R 2.1.0 & make basic dataframe
In-Reply-To: <FADCFAA8BA80C748890C1D3893C198D95856A3@amedmlmhah01.eur.amed.ds.army.mil>
References: <FADCFAA8BA80C748890C1D3893C198D95856A3@amedmlmhah01.eur.amed.ds.army.mil>
Message-ID: <4295DF38.6040209@statistik.uni-dortmund.de>

The main problem you are experiencing is that edit() (more precisely the 
method edit.data.frame()) is a bit restricted - I think contributions 
are welcome.
Note that coding must be done very careful here (and is not trivial at 
all) in order to deal with different kinds of attributes, in particular 
names and factor stuff.

Uwe Ligges





Bliese, Paul D LTC USAMH wrote:

> Recent changes to read.spss() in the foreign package return a dataframe
> containing additional attributes.  For example,
> 
>  
> 
> 
>>TEMP<-read.spss(choose.files(), to.data.frame=T,use.value.labels=F)
> 
> 
>  
> 
> 
>>str(TEMP)
> 
> 
> `data.frame':   780 obs. of  8 variables:
> 
>  $ EXPOS01: atomic  1 1 2 1 2 3 2 4 2 1 ...
> 
>   ..- attr(*, "value.labels")= Named num  5 4 3 2 1
> 
>   .. ..- attr(*, "names")= chr  "Yes, experienced it with Extreme
> Impact" "Yes, experienced it with Moderate Impact" "Yes, experienced it
> with A Little Impact" "Yes, experienced it with No Impact" ...
> 
>  $ EXPOS02: atomic  1 1 1 1 1 1 1 1 1 1 ...
> 
>   ..- attr(*, "value.labels")= Named num  5 4 3 2 1
> 
>   .. ..- attr(*, "names")= chr  "Yes, experienced it with Extreme
> Impact" "Yes, experienced it with Moderate Impact" "Yes, experienced it
> with A Little Impact" "Yes, experienced it with No Impact" ...
> 
>  
> 
>  
> 
> Unfortunately, these changes may be ahead of their time (certainly ahead
> of several functions).  For instance edit balks at the changes:
> 
>  
> 
> 
>>edit(TEMP)
> 
> 
> Error in edit.data.frame(TEMP) : can only handle vector and factor
> elements
> 
>  
> 
> It used to be that the command "as.data.frame" or "data.frame" would
> return a fairly basic data.frame and "fix" the problem.  However, this
> does not work (obviously because TEMP is already a data.frame).  For
> example,
> 
>  
> 
> 
>>TEMP<-as.data.frame(TEMP)
> 
> 
>>edit(TEMP)
> 
> 
> Error in edit.data.frame(TEMP) : can only handle vector and factor
> elements
> 
>  
> 
> It is possible to use "as.matrix", and then "data.frame" the result of
> "as.matrix", but this gets a bit cumbersome.
> 
>  
> 
> The question is:  Is there a simple command to strip additional
> attribute characteristics from a data.frame and get a simple, easy to
> use, uncomplicated data.frame?
> 
>  
> 
> On a related note, do other users routinely use read.spss with the
> defaults of  "to.data.frame=F" or "use.value.labels=T"?  My experience
> is that I am always using the non-default values in which case it would
> be helpful to change the defaults to "to.data.frame=T" and
> "use.value.labels=F".  It would also probably make sense to change the
> default for "trim.factor.names=T".  Interested in others' perspective.
> 
>  
> 
> Appreciate all the great work Saikat DebRoy has done...just trying to
> improve an already useful function.
> 
>  
> 
> Paul
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rxg218 at psu.edu  Thu May 26 16:36:40 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Thu, 26 May 2005 10:36:40 -0400
Subject: [R] a more elegant approach to getting the majority level
Message-ID: <1117118200.8707.15.camel@blue.chem.psu.edu>

Hi, I have a factor and I would like to find the most frequent level.

I think my current approach is a bit long winded and I was wondering if
there was a more elegant way to do it:

x <- factor(sample(1:0, 5,replace=TRUE))

levels(x)[ which( as.logical((table(x) == max(table(x)))) == TRUE ) ]

(The length of x will always be an odd number, so I wont get a tie in
max())

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Alcohol, an alternative to your self
- 'Alcohol' by the Bare Naked Ladies



From ligges at statistik.uni-dortmund.de  Thu May 26 16:45:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 May 2005 16:45:20 +0200
Subject: [R] Help needed - Use of RSrvr in macro mode
In-Reply-To: <429592E2.3020309@aqte.be>
References: <429592E2.3020309@aqte.be>
Message-ID: <4295E100.7030706@statistik.uni-dortmund.de>

This is a VERY special question. Please redirect your question to the 
appropriate mailing list. See
http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l
for more information.

Uwe Ligges


Constant Depi??reux wrote:

> Dear All,
> 
> I am experiencing a problem for which I need some help.
> 
> I have created a small file containing a simple function
> 
> File name = smalltest.r
> 
> Content :
> 
> smalltest=function(extvar)
> {
> 
> itworks=paste('Ca marche',date())
> 
> setwd("c:/windows/temp")
> 
> write.table(itworks,file="resultat.csv", append=FALSE)
> 
> }
> 
> This function works when operated from R environment
> 
> To get it performed from Excel, I have write a small function as follows :
> 
> Excel file : see attachment (one sheet named 'test', one cell A1 
> containing : smalltest(NA))
> 
> Macro :
> 
> Sub smalltest()
> 
>    Range("A3").Select
>    Range(Selection, Selection.End(xlToRight)).Select
>    Range(Selection, Selection.End(xlDown)).Select
>    Selection.ClearContents
>    Call RInterface.StartRServer
>    Call RInterface.RunRFile("h:\R-Sources\smalltest.r")
>    Windows("smalltest.xls").Activate
>    CommandString = Range("test!a1")
>    MsgBox (CommandString)
>    Call RInterface.RRun("CommandString")
>    Workbooks.Open Filename:="c:\windows\temp\resultat.csv"
>    Range(Selection, Selection.End(xlToRight)).Select
>    Range(Selection, Selection.End(xlDown)).Select
>    Selection.Copy
>    Windows("smalltest.xls").Activate
>    Range("A3").Select
>    ActiveSheet.Paste
> 
> 
> End Sub
> 
> Function works (i.e. does complete without error message) when 
> c:\windows\temp\resultat.csv exists (created using above smalltest 
> function in R environment) but hangs when operated from excel (file not 
> found).
> 
> My machine is a 4 Gb Memory - PIV 3.2Ghz  with disk mirroring running 
> latest update of XP Pro SP2.
> 
> Both R and RServer are latest editions available.
> 
> Does anybody have an idea of what the problem might be?
> 
> Many thanks for your advices.
> 
> Best regards.
> 
> 
> Constant Depi??reux
> 
> 
> 
> ------------------------------------------------------------------------
> 
> smalltest=function(extvar)
> {
> 
> itworks=paste('Ca marche',date())
> 
> setwd("c:/windows/temp")
> 
> write.table(itworks,file="resultat.csv", append=FALSE)
> 
> }
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu May 26 16:47:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 May 2005 16:47:17 +0200
Subject: [R] PAN: Need Help for Multiple Imputation Package
In-Reply-To: <20050526002314.1372.qmail@web53709.mail.yahoo.com>
References: <20050526002314.1372.qmail@web53709.mail.yahoo.com>
Message-ID: <4295E175.1010205@statistik.uni-dortmund.de>

Please read the posting guide.

It tells you to provide (a) small reproducible examples and (b)
'If the question relates to a package that is downloaded from CRAN try 
contacting the package maintainers first. You can also use 
find("functionname") and packageDescription("packagename") to find this 
information.'

Uwe Ligges



AC wrote:

> Hello all.  I am trying to run PAN, multilevel
> multiple imputation program, in R to impute missing
> data in a longitudinal dataset.  I could successfully
> run the multiple imputation when I only imputed one
> variable.  However, when I tried to impute a
> time-varying covariate as well as a response variable,
> I received an error message, ?Error: subscript out of
> bounds.?  Can anyone tell if my commands contain any
> mistakes?
> 
> First I imported SAS dataset ?sim? which includes a
> response variable ?MIY1?, a time-varying covariate
> ?TCOV1?, TIME, GROUP (0 or 1), and ID.  200
> participants were included and measurement occurred
> six times.  Approximately 25% of participants dropped
> out at end.      
> 
> 
>>sim <- read.xport('c:\\xptds.dat')
>>
>>int <- rep(1,1200)
>>y <- cbind(sim$MIY1,sim$TCOV1)
>>subj <- sim$ID
>>pred <- cbind(int, sim$TIME, sim$GROUP)  
>>
>>xcol <- 1:3
>>zcol <- 1
> 
> 
>>prior <- list(a=2,Binv=4,c=2,Dinv=4)
> 
> 
>>result <-
> 
> pan(y,subj,pred,xcol,zcol,prior,seed=13579,iter=1000)
> Error: subscript out of bounds
> 
> 
> By the way, I also received the same error message
> when I tried to include intercept and time in Zcol, a
> matrix for random effect specification.  I used
> command ? zcol <- 1:2?.  Does anybody know if this
> error is due to sample size/proportion of missing data
> or due to command mistake?
> 
> I truly appreciate any feedbacks.
> Best regards,
> Eishi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu May 26 16:49:21 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 26 May 2005 10:49:21 -0400
Subject: [R] a more elegant approach to getting the majority level
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E8AF@usctmx1106.merck.com>

Don't know if this is more elegant:

names(which.max(table(x)))

Andy

> From: Rajarshi Guha
> 
> Hi, I have a factor and I would like to find the most frequent level.
> 
> I think my current approach is a bit long winded and I was 
> wondering if
> there was a more elegant way to do it:
> 
> x <- factor(sample(1:0, 5,replace=TRUE))
> 
> levels(x)[ which( as.logical((table(x) == max(table(x)))) == TRUE ) ]
> 
> (The length of x will always be an odd number, so I wont get a tie in
> max())
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Alcohol, an alternative to your self
> - 'Alcohol' by the Bare Naked Ladies
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ligges at statistik.uni-dortmund.de  Thu May 26 16:56:31 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 May 2005 16:56:31 +0200
Subject: [R] a more elegant approach to getting the majority level
In-Reply-To: <1117118200.8707.15.camel@blue.chem.psu.edu>
References: <1117118200.8707.15.camel@blue.chem.psu.edu>
Message-ID: <4295E39F.2020709@statistik.uni-dortmund.de>

Rajarshi Guha wrote:

> Hi, I have a factor and I would like to find the most frequent level.
> 
> I think my current approach is a bit long winded and I was wondering if
> there was a more elegant way to do it:
> 
> x <- factor(sample(1:0, 5,replace=TRUE))
> 
> levels(x)[ which( as.logical((table(x) == max(table(x)))) == TRUE ) ]

(== TRUE) can ALWAYS be omitted, see also:
  library(fortunes)
  fortune("TRUE")

x == max(x) should be replaced by which.max(x)

as.logical() is superfluous


Hence we get:
   names(which.max(table(x)))

Uwe Ligges



> (The length of x will always be an odd number, so I wont get a tie in
> max())
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Alcohol, an alternative to your self
> - 'Alcohol' by the Bare Naked Ladies
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu May 26 16:59:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 May 2005 16:59:25 +0200
Subject: [R] [Fwd: Re: [Fwd: failure delivery]]
In-Reply-To: <42950A9B.80000@uottawa.ca>
References: <42950A9B.80000@uottawa.ca>
Message-ID: <4295E44D.9070809@statistik.uni-dortmund.de>

Can you please specify a small reproducible example?

Uwe Ligges




Prof J C Nash wrote:

> I appear to have hit one of the "drop" issues raised in some discussions
> a couple of years ago by Frank Harrell. They don't seem to have been
> fixed, and I'm under some pressure to get a quick solution for a
> forecasting task I'm doing.
> 
> I have been modelling some retail sales data, and the days just after
> Thanksgiving (US version!) are important. So I created some dummy
> variables by a factor called "events" and (really ugly!!) have TG, TG+1,
> TG+2, etc. Now I also have DEC1, and the calendar and data are such
> that in the period I'm forecasting I have TG+3 but this is
> NOT in the estimation data. There are also weekday factors (wdf) and some
> cross factors (Saturday + some special days is highly significant).
> 
> The model is   Sales ~ daynumber + wdf*events + wdf*specialevents
> 
> where daynumber is the day sequence in the year and specialevents is a
> set of factors to tell when the business has promotional activities.
> The entire model has about 330 coefficients (it seriously needs some
> economizing), but only about 140 of these are estimated.
> 
> I'm using lm() to do the estimation. I plan to change the model and 
> possibly
> the method once I've seen if forecasting works. The current model "works"
> moderately well for in-sample fits, though I suspect there is too
> much variability generally.
> 
> I want to advance 1 week at a time, reestimate, and iterate. This is
> a test case where we know the "future". I can get this to work for a few
> weeks starting at 20041101, but then get an error msg
> 
>         "new factor levels in 'events' ...".
> 
> I have tried putting drop.factor.levels = TRUE in predict(), but this
> didn't seem to register. Also tried suggestion from web to use
> 
>          ifac <- sapply(estndta,is.factor)
>          fcstdta[ifac] <- lapply(fcstdta[ifac],factor)
> 
> Still get same error.
> 
> I've tried a couple of dozen variants on this with no joy.
> 
> Finally have tried using the full data set in lm() but set weights for
> the estimation period to 1, and those for the forecast period to 0. This
> "computes", but the results include NAs at a point where there seems no
> reason for them.
> 
> I'm starting to suspect that there's some sort of bug somewhere in the R
> internals.
> 
> 
>  Any advice welcome.
> 
> 
>



From dimitris.rizopoulos at med.kuleuven.be  Thu May 26 17:00:23 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 26 May 2005 17:00:23 +0200
Subject: [R] a more elegant approach to getting the majority level
References: <1117118200.8707.15.camel@blue.chem.psu.edu>
Message-ID: <005001c56203$a4c56130$0540210a@www.domain>

you could try this:

x <- factor(sample(letters[1:3], 20, TRUE))
#######
tab <- table(x)
names(tab)[tab == max(tab)]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rajarshi Guha" <rxg218 at psu.edu>
To: "R" <r-help at stat.math.ethz.ch>
Sent: Thursday, May 26, 2005 4:36 PM
Subject: [R] a more elegant approach to getting the majority level


> Hi, I have a factor and I would like to find the most frequent 
> level.
>
> I think my current approach is a bit long winded and I was wondering 
> if
> there was a more elegant way to do it:
>
> x <- factor(sample(1:0, 5,replace=TRUE))
>
> levels(x)[ which( as.logical((table(x) == max(table(x)))) == 
> TRUE ) ]
>
> (The length of x will always be an odd number, so I wont get a tie 
> in
> max())
>
> Thanks,
>
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Alcohol, an alternative to your self
> - 'Alcohol' by the Bare Naked Ladies
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From lecoutre at stat.ucl.ac.be  Thu May 26 17:00:27 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 26 May 2005 17:00:27 +0200
Subject: [R] a more elegant approach to getting the majority level
In-Reply-To: <1117118200.8707.15.camel@blue.chem.psu.edu>
Message-ID: <000c01c56203$a749c220$6e8b6882@didacdom.stat.ucl.ac.be>

You could also use:

> names(rev(sort(table(x))))[1]

There is nonetheless a difference if there are several levels which
provides this maximum.
This method will only return one, yours would return all those levels
(which may not be desirable for some others processing).

HTH,

Eric

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte   


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rajarshi Guha
> Sent: jeudi 26 mai 2005 16:37
> To: R
> Subject: [R] a more elegant approach to getting the majority level
> 
> 
> Hi, I have a factor and I would like to find the most frequent level.
> 
> I think my current approach is a bit long winded and I was 
> wondering if there was a more elegant way to do it:
> 
> x <- factor(sample(1:0, 5,replace=TRUE))
> 
> levels(x)[ which( as.logical((table(x) == max(table(x)))) == TRUE ) ]
> 
> (The length of x will always be an odd number, so I wont get a tie in
> max())
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Alcohol, an alternative to your self
> - 'Alcohol' by the Bare Naked Ladies
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From stefaan.lhermitte at biw.kuleuven.be  Thu May 26 17:06:22 2005
From: stefaan.lhermitte at biw.kuleuven.be (Stefaan Lhermitte)
Date: Thu, 26 May 2005 17:06:22 +0200
Subject: [R] Simplify formula for heterogeneity
Message-ID: <4295E5EE.5000106@biw.kuleuven.be>

Dear R-ians,

I'm looking for a computational simplified formula to calculate a
measure for heterogeneity (let's say H ):

H = sqrt [ (Si (Sj (Xi - Xj)?? ) ) /n ]

where:
sqrt = square root
Si = summation over i  (= 0 to n)
Sj = summation over j (= 0 to n)
Xi = element of X with index i
Xj = element of X with index j

I can simplify the formula to:

H = sqrt [ ( 2 * n * Si (Xi) - 2 Si (Sj ( Xi * Xj)) ) / n]

Unfortunately this formula stays difficult in iterative programming,
because I have to keep every element of X to calculate H.

I know a computional simplified formula exists for the standard
deviation (sd) that is much easier in iterative programming.
Therefore I wondered I anybody knew about analog simplifications to
simplify H:

sd = sqrt [ ( Si (Xi - mean(X) )?? ) /n  ]  -> simplified computation ->
sqrt [ (n * Si( X?? ) - ( Si( X ) )?? )/ n?? ]

This simplied formula is much easier in iterative programming, since I
don't have to keep every element of X.
E.g.: I have a vector X[1:10]  and I already have caculated Si( X[1:10]??
) (I will call this A) and Si( X ) (I will call this B).
When X gets extendend by 1 element (eg. X[11]) it easy fairly simple to
calculate sd(X[1:11]) without having to reuse the elements of X[1:10].
I just have to calculate:

sd = sqrt [ (n * (A + X[11]??) - (A + X[11]??)?? ) / n?? ]

This is failry easy in an iterative process, since before we continue
with the next step we set:
A = (A + X[11]??)
B = (B + X[11])

Can anybody help me to do something comparable for H? Any other help to
calculate H easily in an iterative process is also welcome!

Thanx in advance!

Kind regards,
Stef



From cgarnier at ttz-Bremerhaven.de  Thu May 26 17:09:08 2005
From: cgarnier at ttz-Bremerhaven.de (BIBIS, Garnier, Christophe)
Date: Thu, 26 May 2005 17:09:08 +0200
Subject: [R] export the graphical result of bwplot()
Message-ID: <BF52B6AA9196D71195C80030052FDA9D465851@TTZBN>

Dear all,

Maybe somebody can help me to understand my problem:

Inside a R script, I try to export the graphic results of 'bwplot' in some
jpeg files.

The data source ('main') is a mix of numeric and factor values 
the "analysis_bwplot()" contains the loops (i and j) and calls the
"analysis_var_var_bwplot()" method.
"analysis_var_var_bwplot()" uses bwplot() to plot into a file (name n)

If i run the script, i receive some error messages (see below) and the
result files are always empty.
But if i manually run the commands of the method
"analysis_var_var_bwplot()", it exports the jpeg file as expected.

Curiously, i have a mix of numeric and factor data and it always recognize
some "numeric" data (in the "analysis_var_var_bwplot()" method)


Has somebody an idea?


Thanks,
Christophe



---ERROR MESSAGES--------------------------------------------

> analysis_bwplot()

 warn = 1
[1] "bwplot_var1_var2.jpg"
[1] "mode: numeric numeric"
Warning in bwplot(var1 ~ var2) : x should be numeric
...

 warn = 2
[1] "bwplot_var21_var22.jpg"
[1] "mode: numeric numeric"
Error in bwplot(var1 ~ var2) : (converted from warning) x should be numeric
Execution halted

---CODE--------------------------------------------
library(RMySQL)
library(XML)
library(lattice)

...

analysis_var_var_bwplot <- function(var1 , var1name , var2 , var2name )
{
    n <- paste("bwplot_" , var1name , "_" , var2name , ".jpg" , sep="")
    print(n)

    print ( paste("mode:" , mode(var1) , mode(var2) ) )
    
    jpeg(file=n)
    bwplot(var1 ~ var2 )
    dev.off()      

}

analysis_bwplot <- function()
  {
    length <- length(main)
    names <- names(main)

    ow <- options("warn")
    
    for (i in 1:length)  
      {

        options(warn = i )
        cat ("\n warn =" , i , "\n" )
        
        for (j in i+1:length)
          {
            if ( j > length ) break

            x <- main[[i]]
            y <- main[[j]]
            
            variableiname <- names[i]
            variablejname <- names[j]

            analysis_var_var_bwplot (x , variableiname  ,y , variablejname )

          }

      }

    warnings()
    options(ow) #reset
    
  }





*************************************************************** 
Dr. Christophe Garnier 	E-Mail: 	cgarnier at ttz-bremerhaven.de 
TTZ Bremerhaven 	Tel: 	+49 (0) 471-4832 183 
 			Fax: 	+49 (0) 471-4832 129 
 			Mobile:  +49 (0) 163-494 48 77 
BIBIS (Bremerhavener Institut f??r biologische Informationssysteme)
Fischkai 1 27572 Bremerhaven 
<http://www.ttz-bremerhaven.de>



From matthew_wiener at merck.com  Thu May 26 17:15:13 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 26 May 2005 11:15:13 -0400
Subject: [R] a more elegant approach to getting the majority level
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04994609@uswsmx03.merck.com>

The which.max solution is fine as long as the maximum is always unique.
Otherwise, which.max will give you the first maximum.

So using the "x == max(x) " version will have an advantage if there can be
ties.

Regards,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
Sent: Thursday, May 26, 2005 10:57 AM
To: rxg218 at psu.edu
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] a more elegant approach to getting the majority level


Rajarshi Guha wrote:

> Hi, I have a factor and I would like to find the most frequent level.
> 
> I think my current approach is a bit long winded and I was wondering if
> there was a more elegant way to do it:
> 
> x <- factor(sample(1:0, 5,replace=TRUE))
> 
> levels(x)[ which( as.logical((table(x) == max(table(x)))) == TRUE ) ]

(== TRUE) can ALWAYS be omitted, see also:
  library(fortunes)
  fortune("TRUE")

x == max(x) should be replaced by which.max(x)

as.logical() is superfluous


Hence we get:
   names(which.max(table(x)))

Uwe Ligges



> (The length of x will always be an odd number, so I wont get a tie in
> max())
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Alcohol, an alternative to your self
> - 'Alcohol' by the Bare Naked Ladies
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bret at tamu.edu  Thu May 26 17:15:53 2005
From: bret at tamu.edu (Bret Collier)
Date: Thu, 26 May 2005 10:15:53 -0500
Subject: [R] Plotting Ranges as Vertical Lines
Message-ID: <s295a203.085@wfscgate.tamu.edu>

R Users,
I have not been able to find anything close to what I want searching
R-help and I am hoping someone could point me in the right direction.

The data consists of differences in length of individual salamanders
collected at time of initial capture and last recapture (excerpt below).
 What I am trying to do is plot a vertical line for each individual (ID)
representing the change in size between initial capture (CAPTURESVL) and
last recapture (RECAPSVL) (the range of sizes would be on the y-axis),
hence vertical line length would be equal to GROWTH (which can be + or
-).  I would like to make this plot over the time between recaptures
(MONTHELAPSED) which would be on the x-axis.

       ID CAPMONTH CAPTURESVL CAPTUREMASSGR RECAPMONTH RECAPSVL
RECAPMASSGR GROWTH MASSGR MONTHELAPSED
1    SJC8        3         21           0.1         11       22        
0.2      1    0.1            8
2    SJC9        3         33           0.7         13       35        
0.9      2    0.2           10
3   SJC10        3         19           0.5          9       20        
0.2      1   -0.3            6
4   SJC11        4         36           1.1         13       41        
1.5      5    0.4            9
5   SJC12        4         19           0.1         15       28        
0.4      9    0.3           11
6   SJC17        6         24           0.3         11       26        
0.4      2    0.1            5
7   SJC18        6         43           1.1         24       43        
1.8      0    0.7           18
8   SJC23        7         16           0.1         15       20        
0.1      4    0.0            8
9   SJC24        7         22           0.5         22       40        
1.9     18    1.4           15
10  SJC25        8         23           0.4         15       27        
0.6      4    0.2            7
11  SJC32        9         37           1.1         16       38        
1.5      1    0.4            7
12  SJC34        9         31           0.8         12       33        
1.1      2    0.3            3
13  SJC35        9         19           0.2         14       23        
0.3      4    0.1            5
14  SJC53       11         41           1.2         12       40        
1.4     -1    0.2            1
15  SJC55       11         41           1.3         12       40        
1.7     -1    0.4            1
16  SJC60       11         39           1.5         22       41        
1.8      2    0.3           11
17  SJC65       12         41           1.5         23       44        
1.8      3    0.3           11

I am stuck on how to 1) create the vertical segments and 2) link them
to x-axis values for MONTHELAPSED?  I would have provided some example
code, except I have not been able to figure out how to approach this
yet.  Since I am probably not looking in the right place, could someone
point me in the right direction or towards some example figure code/help
files that I must have missed in my searching?

Thanks,
Bret
TX A&M

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R



From erithid at bellsouth.net  Thu May 26 17:31:35 2005
From: erithid at bellsouth.net (BJ)
Date: Thu, 26 May 2005 11:31:35 -0400
Subject: [R] Any ideas on how to add a dotted line to a box plot
	to	indicate a specific value?- Thank you all for your help
In-Reply-To: <429578C6.70803@7d4.com>
References: <4294E096.3050000@bellsouth.net> <429578C6.70803@7d4.com>
Message-ID: <4295EBD7.9060802@bellsouth.net>

Lots of good ideas, and great advice! Thank you as always ~Erithid

vincent wrote:

> lines(...) could perhaps be useful ?
> hih
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From George_Heine at blm.gov  Thu May 26 17:31:55 2005
From: George_Heine at blm.gov (George_Heine@blm.gov)
Date: Thu, 26 May 2005 09:31:55 -0600
Subject: [R] warnings from hist(): parameter XXX couldn't be set in
 high-level plot functionN
Message-ID: <OF87730193.39DC56BA-ON8725700D.00522DB3-8725700D.00555247@blm.gov>





Hello -

This is not a real problem, just an annoyance.  Sometimes, but not always,
I get a set of strange warnings from hist().  Example follows.

############################
#Produce a histogram of start dates for a set of field measurements.

# I didn't reproduce all the dates, because not sure it's relevant, but
here's a sample.
# Note the invalid date in the fifth element.
> StartDate[sample(1:length(StartDate),20)]
 [1] "2004-08-26" "1997-09-08" "2004-08-19" "1997-09-08" "0999-07-20"
 [6] "2001-11-28" "2000-11-02" "1997-09-08" "2004-08-19" "2004-10-28"
[11] "1997-09-08" "1997-09-09" "1997-09-08" "1998-09-08" "1997-09-09"
[16] "1997-09-08" "2004-10-28" "1997-09-08" "2004-08-19" "1997-09-08"
> years<-as.Date(c(#
+    "1995-01-01","1996-01-01","1996-01-01",
+    "1997-01-01","1998-01-01","1999-01-01","2000-01-01","2001-01-01",
+    "2002-01-01","2003-01-01","2004-01-01","2005-01-01"))

> years
 [1] "1995-01-01" "1996-01-01" "1996-01-01" "1997-01-01" "1998-01-01"
 [6] "1999-01-01" "2000-01-01" "2001-01-01" "2002-01-01" "2003-01-01"
[11] "2004-01-01" "2005-01-01"

>  TimeSpan<-as.integer(difftime(StopDate,StartDate))
Warning message:
NAs introduced by coercion
#  Note:  NAs correspond to 3 invalid dates, apparently typos

> h<-hist(StartDate[TimeSpan>0 & StartDate>years[1]], breaks=years, freq=T,
+       main="",xlab="",ylab="number of sites", col="grey")
Warning messages:
1: parameter "main" couldn't be set in high-level plot() function
2: parameter "ylab" couldn't be set in high-level plot() function
3: parameter "main" couldn't be set in high-level plot() function
4: parameter "ylab" couldn't be set in high-level plot() function
#########################################3

The resulting histogram was generated, and the plot labels main, xlab,
ylab, were as expected.  Just curious about what the warning messages mean.
Why are there two warnings each about 'main' and 'ylab', and none about
'xlab'?   Another puzzling thing is that this message does not always
occur,
Searched the FAQ and the mail archives and couldn't find anything helpful.

 By the way, is there a way to use seq() to generate something like the
"years" sequence above?

I'm using R 2.0.1 (precompiled version for windows.)

Thanks for your help !
<>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>
George Heine, PhD
Mathematical Analyst
National IRM Center
U.S. Bureau of Land Management
voice   (303) 236-0099
fax       (303) 236-1974
cell      (303) 905-5382
pager   gheine at my2way.com
<>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>



From rlee at fpcc.net  Thu May 26 17:59:51 2005
From: rlee at fpcc.net (rlee@fpcc.net)
Date: Thu, 26 May 2005 09:59:51 -0600 (MDT)
Subject: [R] polar smoothing code
Message-ID: <33859.136.177.22.105.1117123191.squirrel@webmail.fpcc.net>

I have some polar smoothing code that allows x-y smoothing of middle,
outer, and inner regions.  I'm thinking of posting it as a CRAN
package.  Does this functionality already exist in R or a CRAN package?



From MBock at arcadis-us.com  Thu May 26 18:01:17 2005
From: MBock at arcadis-us.com (Bock, Michael)
Date: Thu, 26 May 2005 10:01:17 -0600
Subject: [R] Confidence intervals for prediction based on the logistic
	equation
Message-ID: <0016F5677B1F1D4281EEBC034993595102F99473@CORPEXBE1.arcadis-us.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050526/a2b0190d/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Thu May 26 18:05:17 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 26 May 2005 17:05:17 +0100 (BST)
Subject: [R] Simplify formula for heterogeneity
In-Reply-To: <4295E5EE.5000106@biw.kuleuven.be>
Message-ID: <XFMail.050526170517.Ted.Harding@nessie.mcc.ac.uk>

On 26-May-05 Stefaan Lhermitte wrote:
> Dear R-ians,
> 
> I'm looking for a computational simplified formula to calculate a
> measure for heterogeneity (let's say H ):
> 
> H = sqrt [ (Si (Sj (Xi - Xj)?? ) ) /n ]
> 
> where:
> sqrt = square root
> Si = summation over i  (= 0 to n)
> Sj = summation over j (= 0 to n)
> Xi = element of X with index i
> Xj = element of X with index j

If I have understood your formula correctly (and you are
applying it to a vector X of length n) then it seems that
your H reduces to

  sqrt[(Si(n*(Xi - Xbar)^2) + Sj(n*(Xj - Xbar)^2))/n]

  = sqrt[2*(n-1)var(X)] = sd(X)*sqrt(2*(n-1))

(where Xbar is the mean of the values in X).

So I don't see what the special point of H is anyway.
But at least this simplifies it1

Best wishes,
Ted.

> I can simplify the formula to:
> 
> H = sqrt [ ( 2 * n * Si (Xi) - 2 Si (Sj ( Xi * Xj)) ) / n]
> 
> Unfortunately this formula stays difficult in iterative programming,
> because I have to keep every element of X to calculate H.
> 
> I know a computional simplified formula exists for the standard
> deviation (sd) that is much easier in iterative programming.
> Therefore I wondered I anybody knew about analog simplifications to
> simplify H:
> 
> sd = sqrt [ ( Si (Xi - mean(X) )?? ) /n  ]  -> simplified computation ->
> sqrt [ (n * Si( X?? ) - ( Si( X ) )?? )/ n?? ]
> 
> This simplied formula is much easier in iterative programming, since I
> don't have to keep every element of X.
> E.g.: I have a vector X[1:10]  and I already have caculated Si(
> X[1:10]??
> ) (I will call this A) and Si( X ) (I will call this B).
> When X gets extendend by 1 element (eg. X[11]) it easy fairly simple to
> calculate sd(X[1:11]) without having to reuse the elements of X[1:10].
> I just have to calculate:
> 
> sd = sqrt [ (n * (A + X[11]??) - (A + X[11]??)?? ) / n?? ]
> 
> This is failry easy in an iterative process, since before we continue
> with the next step we set:
> A = (A + X[11]??)
> B = (B + X[11])
> 
> Can anybody help me to do something comparable for H? Any other help to
> calculate H easily in an iterative process is also welcome!
> 
> Thanx in advance!
> 
> Kind regards,
> Stef
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-May-05                                       Time: 17:05:13
------------------------------ XFMail ------------------------------



From ggrothendieck at gmail.com  Thu May 26 18:27:50 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 May 2005 12:27:50 -0400
Subject: [R] warnings from hist(): parameter XXX couldn't be set in
	high-level plot functionN
In-Reply-To: <OF87730193.39DC56BA-ON8725700D.00522DB3-8725700D.00555247@blm.gov>
References: <OF87730193.39DC56BA-ON8725700D.00522DB3-8725700D.00555247@blm.gov>
Message-ID: <971536df05052609271a015a66@mail.gmail.com>

On 5/26/05, George_Heine at blm.gov <George_Heine at blm.gov> wrote:
> > years
>  [1] "1995-01-01" "1996-01-01" "1996-01-01" "1997-01-01" "1998-01-01"
>  [6] "1999-01-01" "2000-01-01" "2001-01-01" "2002-01-01" "2003-01-01"
> [11] "2004-01-01" "2005-01-01"
> 
> 
>  By the way, is there a way to use seq() to generate something like the
> "years" sequence above?
> 

seq( as.Date("1995-01-01"),  length = 11, by = "year" )



From ligges at statistik.uni-dortmund.de  Thu May 26 18:29:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 May 2005 18:29:01 +0200
Subject: [R] export the graphical result of bwplot()
In-Reply-To: <BF52B6AA9196D71195C80030052FDA9D465851@TTZBN>
References: <BF52B6AA9196D71195C80030052FDA9D465851@TTZBN>
Message-ID: <4295F94D.70108@statistik.uni-dortmund.de>

BIBIS, Garnier, Christophe wrote:

> Dear all,
> 
> Maybe somebody can help me to understand my problem:
> 
> Inside a R script, I try to export the graphic results of 'bwplot' in some
> jpeg files.
> 
> The data source ('main') is a mix of numeric and factor values 
> the "analysis_bwplot()" contains the loops (i and j) and calls the
> "analysis_var_var_bwplot()" method.
> "analysis_var_var_bwplot()" uses bwplot() to plot into a file (name n)
> 
> If i run the script, i receive some error messages (see below) and the
> result files are always empty.
> But if i manually run the commands of the method
> "analysis_var_var_bwplot()", it exports the jpeg file as expected.
> 
> Curiously, i have a mix of numeric and factor data and it always recognize
> some "numeric" data (in the "analysis_var_var_bwplot()" method)
> 
> 
> Has somebody an idea?


At least two points:
  i+1:length == i + (1:length)
but I guess you want
  (i+1):length


You have to print() lattice graphics, see the FAQs.

Also, I guess you have factors rather than numeric values:
  mode(factor("a")) # is numeric!


Uwe Ligges



> 
> Thanks,
> Christophe
> 
> 
> 
> ---ERROR MESSAGES--------------------------------------------
> 
> 
>>analysis_bwplot()
> 
> 
>  warn = 1
> [1] "bwplot_var1_var2.jpg"
> [1] "mode: numeric numeric"
> Warning in bwplot(var1 ~ var2) : x should be numeric
> ...
> 
>  warn = 2
> [1] "bwplot_var21_var22.jpg"
> [1] "mode: numeric numeric"
> Error in bwplot(var1 ~ var2) : (converted from warning) x should be numeric
> Execution halted
> 
> ---CODE--------------------------------------------
> library(RMySQL)
> library(XML)
> library(lattice)
> 
> ...
> 
> analysis_var_var_bwplot <- function(var1 , var1name , var2 , var2name )
> {
>     n <- paste("bwplot_" , var1name , "_" , var2name , ".jpg" , sep="")
>     print(n)
> 
>     print ( paste("mode:" , mode(var1) , mode(var2) ) )
>     
>     jpeg(file=n)
>     bwplot(var1 ~ var2 )
>     dev.off()      
> 
> }
> 
> analysis_bwplot <- function()
>   {
>     length <- length(main)
>     names <- names(main)
> 
>     ow <- options("warn")
>     
>     for (i in 1:length)  
>       {
> 
>         options(warn = i )
>         cat ("\n warn =" , i , "\n" )
>         
>         for (j in i+1:length)
>           {
>             if ( j > length ) break
> 
>             x <- main[[i]]
>             y <- main[[j]]
>             
>             variableiname <- names[i]
>             variablejname <- names[j]
> 
>             analysis_var_var_bwplot (x , variableiname  ,y , variablejname )
> 
>           }
> 
>       }
> 
>     warnings()
>     options(ow) #reset
>     
>   }
> 
> 
> 
> 
> 
> *************************************************************** 
> Dr. Christophe Garnier 	E-Mail: 	cgarnier at ttz-bremerhaven.de 
> TTZ Bremerhaven 	Tel: 	+49 (0) 471-4832 183 
>  			Fax: 	+49 (0) 471-4832 129 
>  			Mobile:  +49 (0) 163-494 48 77 
> BIBIS (Bremerhavener Institut f??r biologische Informationssysteme)
> Fischkai 1 27572 Bremerhaven 
> <http://www.ttz-bremerhaven.de>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From devens8765 at yahoo.com  Thu May 26 18:30:56 2005
From: devens8765 at yahoo.com (Dave Evens)
Date: Thu, 26 May 2005 09:30:56 -0700 (PDT)
Subject: [R] Q: changing the class of an object
Message-ID: <20050526163056.88417.qmail@web61312.mail.yahoo.com>


Dear All,

I have a list of dataframes, each cell in every
dataframe (after I have cleaned up the dataframes) is
either real or NA  but have class character (I think).
I would like to know how to change the class of every
cell without using a for-loop. I currently have this

dataframes <- sapply(1: no.of.subs, function(k)
apply(dataframes[[k]], 2, function(x) {         
if(class(x)=="character") x <- as(x, "numeric"); x }))

but this neither changes the cells to numeric nor
keeps the dataframes in a list.

It creates one dataframe with number of rows=(no of
rows in a dataframe*no of colums) and number of
columns = no.of.subs

Can someone please help? Thanks in advance for any
help.

Dave



From ligges at statistik.uni-dortmund.de  Thu May 26 18:32:05 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 May 2005 18:32:05 +0200
Subject: [R] Plotting Ranges as Vertical Lines
In-Reply-To: <s295a203.085@wfscgate.tamu.edu>
References: <s295a203.085@wfscgate.tamu.edu>
Message-ID: <4295FA05.5010603@statistik.uni-dortmund.de>

Some example code using segments():

  x <- 1:10
  low <- rnorm(10)
  high <- rnorm(10)
  plot(x, low, ylim = range(c(low, high)), type="n")
  segments(x, low, x, high)

Uwe Ligges


Bret Collier wrote:

> R Users,
> I have not been able to find anything close to what I want searching
> R-help and I am hoping someone could point me in the right direction.
> 
> The data consists of differences in length of individual salamanders
> collected at time of initial capture and last recapture (excerpt below).
>  What I am trying to do is plot a vertical line for each individual (ID)
> representing the change in size between initial capture (CAPTURESVL) and
> last recapture (RECAPSVL) (the range of sizes would be on the y-axis),
> hence vertical line length would be equal to GROWTH (which can be + or
> -).  I would like to make this plot over the time between recaptures
> (MONTHELAPSED) which would be on the x-axis.
> 
>        ID CAPMONTH CAPTURESVL CAPTUREMASSGR RECAPMONTH RECAPSVL
> RECAPMASSGR GROWTH MASSGR MONTHELAPSED
> 1    SJC8        3         21           0.1         11       22        
> 0.2      1    0.1            8
> 2    SJC9        3         33           0.7         13       35        
> 0.9      2    0.2           10
> 3   SJC10        3         19           0.5          9       20        
> 0.2      1   -0.3            6
> 4   SJC11        4         36           1.1         13       41        
> 1.5      5    0.4            9
> 5   SJC12        4         19           0.1         15       28        
> 0.4      9    0.3           11
> 6   SJC17        6         24           0.3         11       26        
> 0.4      2    0.1            5
> 7   SJC18        6         43           1.1         24       43        
> 1.8      0    0.7           18
> 8   SJC23        7         16           0.1         15       20        
> 0.1      4    0.0            8
> 9   SJC24        7         22           0.5         22       40        
> 1.9     18    1.4           15
> 10  SJC25        8         23           0.4         15       27        
> 0.6      4    0.2            7
> 11  SJC32        9         37           1.1         16       38        
> 1.5      1    0.4            7
> 12  SJC34        9         31           0.8         12       33        
> 1.1      2    0.3            3
> 13  SJC35        9         19           0.2         14       23        
> 0.3      4    0.1            5
> 14  SJC53       11         41           1.2         12       40        
> 1.4     -1    0.2            1
> 15  SJC55       11         41           1.3         12       40        
> 1.7     -1    0.4            1
> 16  SJC60       11         39           1.5         22       41        
> 1.8      2    0.3           11
> 17  SJC65       12         41           1.5         23       44        
> 1.8      3    0.3           11
> 
> I am stuck on how to 1) create the vertical segments and 2) link them
> to x-axis values for MONTHELAPSED?  I would have provided some example
> code, except I have not been able to figure out how to approach this
> yet.  Since I am probably not looking in the right place, could someone
> point me in the right direction or towards some example figure code/help
> files that I must have missed in my searching?
> 
> Thanks,
> Bret
> TX A&M
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Thu May 26 18:48:49 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 26 May 2005 09:48:49 -0700
Subject: [R] Q: changing the class of an object
In-Reply-To: <20050526163056.88417.qmail@web61312.mail.yahoo.com>
Message-ID: <200505261648.j4QGmnvp009416@compton.gene.com>

Datadrames don't have "cells" -- class, type, etc. is determined on a column
by column basis (which is the same as a component by component basis, since
datframes are also lists) Please read "An Introduction to R" before
proceeding and posting so that you understand R's basic data structures and
conventions.

BTW -- what do you have against for loops? One should vectorize calculations
whenever possible,of course, but for loops are often useful and essentially
instantaneous. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dave Evens
> Sent: Thursday, May 26, 2005 9:31 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Q: changing the class of an object
> 
> 
> Dear All,
> 
> I have a list of dataframes, each cell in every
> dataframe (after I have cleaned up the dataframes) is
> either real or NA  but have class character (I think).
> I would like to know how to change the class of every
> cell without using a for-loop. I currently have this
> 
> dataframes <- sapply(1: no.of.subs, function(k)
> apply(dataframes[[k]], 2, function(x) {         
> if(class(x)=="character") x <- as(x, "numeric"); x }))
> 
> but this neither changes the cells to numeric nor
> keeps the dataframes in a list.
> 
> It creates one dataframe with number of rows=(no of
> rows in a dataframe*no of colums) and number of
> columns = no.of.subs
> 
> Can someone please help? Thanks in advance for any
> help.
> 
> Dave
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Thu May 26 18:50:28 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 May 2005 18:50:28 +0200
Subject: [R] warnings from hist(): parameter XXX couldn't be set in
	high-level plot functionN
In-Reply-To: <OF87730193.39DC56BA-ON8725700D.00522DB3-8725700D.00555247@blm.gov>
References: <OF87730193.39DC56BA-ON8725700D.00522DB3-8725700D.00555247@blm.gov>
Message-ID: <4295FE54.9010300@statistik.uni-dortmund.de>

George_Heine at blm.gov wrote:

> 
> 
> 
> Hello -
> 
> This is not a real problem, just an annoyance.  Sometimes, but not always,
> I get a set of strange warnings from hist().  Example follows.
> 
> ############################
> #Produce a histogram of start dates for a set of field measurements.
> 
> # I didn't reproduce all the dates, because not sure it's relevant, but
> here's a sample.
> # Note the invalid date in the fifth element.
> 
>>StartDate[sample(1:length(StartDate),20)]
> 
>  [1] "2004-08-26" "1997-09-08" "2004-08-19" "1997-09-08" "0999-07-20"
>  [6] "2001-11-28" "2000-11-02" "1997-09-08" "2004-08-19" "2004-10-28"
> [11] "1997-09-08" "1997-09-09" "1997-09-08" "1998-09-08" "1997-09-09"
> [16] "1997-09-08" "2004-10-28" "1997-09-08" "2004-08-19" "1997-09-08"
> 
>>years<-as.Date(c(#
> 
> +    "1995-01-01","1996-01-01","1996-01-01",
> +    "1997-01-01","1998-01-01","1999-01-01","2000-01-01","2001-01-01",
> +    "2002-01-01","2003-01-01","2004-01-01","2005-01-01"))
> 
> 
>>years
> 
>  [1] "1995-01-01" "1996-01-01" "1996-01-01" "1997-01-01" "1998-01-01"
>  [6] "1999-01-01" "2000-01-01" "2001-01-01" "2002-01-01" "2003-01-01"
> [11] "2004-01-01" "2005-01-01"
> 
> 
>> TimeSpan<-as.integer(difftime(StopDate,StartDate))
> 
> Warning message:
> NAs introduced by coercion
> #  Note:  NAs correspond to 3 invalid dates, apparently typos
> 
> 
>>h<-hist(StartDate[TimeSpan>0 & StartDate>years[1]], breaks=years, freq=T,
> 
> +       main="",xlab="",ylab="number of sites", col="grey")
> Warning messages:
> 1: parameter "main" couldn't be set in high-level plot() function
> 2: parameter "ylab" couldn't be set in high-level plot() function
> 3: parameter "main" couldn't be set in high-level plot() function
> 4: parameter "ylab" couldn't be set in high-level plot() function
> #########################################3
> 
> The resulting histogram was generated, and the plot labels main, xlab,
> ylab, were as expected.  Just curious about what the warning messages mean.
> Why are there two warnings each about 'main' and 'ylab', and none about
> 'xlab'?   Another puzzling thing is that this message does not always
> occur,
> Searched the FAQ and the mail archives and couldn't find anything helpful.


Because xlab is a formal argument of the Date method of hist(), while 
ylab and main are not and passed to underlying functions via the "..." 
argument. Functions like axis() not knowing about arguments like main 
and ylab complain by sending a warning. Hence you can ignore these warnings.



>  By the way, is there a way to use seq() to generate something like the
> "years" sequence above?

[Gabor G. has already answered this one.]


Uwe Ligges


> I'm using R 2.0.1 (precompiled version for windows.)
> 
> Thanks for your help !
> <>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>
> George Heine, PhD
> Mathematical Analyst
> National IRM Center
> U.S. Bureau of Land Management
> voice   (303) 236-0099
> fax       (303) 236-1974
> cell      (303) 905-5382
> pager   gheine at my2way.com
> <>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu May 26 18:55:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 May 2005 18:55:18 +0200
Subject: [R] Q: changing the class of an object
In-Reply-To: <20050526163056.88417.qmail@web61312.mail.yahoo.com>
References: <20050526163056.88417.qmail@web61312.mail.yahoo.com>
Message-ID: <4295FF76.3090208@statistik.uni-dortmund.de>

Dave Evens wrote:

> Dear All,
> 
> I have a list of dataframes, each cell in every

what is a cell?

> dataframe (after I have cleaned up the dataframes) is
> either real or NA  but have class character (I think).
> I would like to know how to change the class of every
> cell without using a for-loop. I currently have this
> 
> dataframes <- sapply(1: no.of.subs, function(k)
> apply(dataframes[[k]], 2, function(x) {         
> if(class(x)=="character") x <- as(x, "numeric"); x }))
> 
> but this neither changes the cells to numeric nor
> keeps the dataframes in a list.
> 
> It creates one dataframe with number of rows=(no of
> rows in a dataframe*no of colums) and number of
> columns = no.of.subs
> 
> Can someone please help? Thanks in advance for any
> help.


A simple way is

   data.frame(sapply(X, as.numeric))

but I guess the real problem is some steps before. You should take a 
look why you got character vectors in your data.frame rather than 
numeric ones.

Uwe Ligges



> Dave
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Thu May 26 19:14:15 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 26 May 2005 19:14:15 +0200
Subject: [R] rotate pie chart
In-Reply-To: <42941A43.60407@ims.uni-stuttgart.de>
References: <42933587.509@pik-potsdam.de>
	<9d7c0a31ab9e685806ce1497347b433a@mail.nih.gov>
	<42939F88.50502@stat.auckland.ac.nz>
	<42941A43.60407@ims.uni-stuttgart.de>
Message-ID: <17046.999.824147.502835@stat.math.ethz.ch>


>>>>> "Kati" == Katrin Schweitzer <Katrin.Schweitzer at ims.uni-stuttgart.de>
>>>>>     on Wed, 25 May 2005 08:25:07 +0200 writes:

    >> I think you have to get your hands dirty on this one, but it's not too
    >> hard.  Here's a function pie90() which is a tiny modification of pie().
    >> Does that do the trick?
    >> 

    Kati> Yes, it works perfectly fine, at least for what I
    Kati> wanted... :) Thanks a lot, to Lars for asking, and
    Kati> to Paul for getting your hands dirty!

    Kati> Kati

    Kati> PS: I know one shouldn't use pie charts at all... :)

yes, indeed, really!

    Kati> but if I do so, is there a reason why they work
    Kati> counter-clockwise in R? Is that convention?  Sorry
    Kati> if its a silly question, my intuition (which might
    Kati> very likely be horrible) just expected them to start
    Kati> at 12 o'clock and fill the pie clockwisely.


you see how very tired I must be at the moment; otherwise I
wouldn't spend time for this (a function that shouldn't be used),
but anyway I did....

I've worked on Paul's pie90() and on Kati's extra-wish
to produce a version of standard pie() that allows Kati and Lars
(and whoever) to produce  pie()s  starting at an arbitrary angle and
go clockwise instead of the usual ``mathematically positive''
direction.

Look for this (+ documentation) in a few days "in R-devel".
Martin



From chz939 at mail.usask.ca  Thu May 26 19:23:39 2005
From: chz939 at mail.usask.ca (Chunhua Zhang)
Date: Thu, 26 May 2005 11:23:39 -0600
Subject: [R] How to interpret the differences between visually checked range
 and simulated range
Message-ID: <4296061B.2060300@mail.usask.ca>

Hi lists,

I am working with semivariogram now.
I found that the ranges I got from either likfit or variofit are smaller 
than that from visual check.
Say the value for visual check is around 50 m, simulated ranges are 
around 35 m.
So how can I interpret the difference? Or is it possible that I did 
something wrong?
One problem with my data is that my sampling interval is a little big.

Many thanks!

Chunhua



From h.brunschwig at utoronto.ca  Thu May 26 20:20:39 2005
From: h.brunschwig at utoronto.ca (h.brunschwig@utoronto.ca)
Date: Thu, 26 May 2005 14:20:39 -0400
Subject: [R] longitudinal survey data
Message-ID: <1117131639.429613772e707@webmail.utoronto.ca>


Dear R-Users!

Is there a possibility in R to do analyze longitudinal survey data (repeated
measures in a survey)? I know that for longitudinal data I can use lme() to
incorporate the correlation structure within individual and I know that there is
the package survey for analyzing survey data. How can I combine both? I am
trying to calculate design-based estimates. However, if I use svyglm() from the
survey package I would ignore the correlation structure of the repeated measures.

Thanks!

Dassy



From LI at nsabp.pitt.edu  Thu May 26 21:43:36 2005
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Thu, 26 May 2005 15:43:36 -0400
Subject: [R] To find how R defined the function of "coxph"?
Message-ID: <3D0B2434377E984E9C85CAA316F8B183357CA1@nsabpmail>

Dear all,

I am wondering if there is a way to find how R defined(or wrote) the function of 
"coxph"? I don not mean the one that we get by checking help(coxph), but the one 
like coxph<-function(....){...}

Thanks,

Jia



From sundar.dorai-raj at pdf.com  Thu May 26 21:56:17 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 26 May 2005 12:56:17 -0700
Subject: [R] To find how R defined the function of "coxph"?
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B183357CA1@nsabpmail>
References: <3D0B2434377E984E9C85CAA316F8B183357CA1@nsabpmail>
Message-ID: <429629E1.5090508@pdf.com>



Li, Jia wrote:
> Dear all,
> 
> I am wondering if there is a way to find how R defined(or wrote) the function of 
> "coxph"? I don not mean the one that we get by checking help(coxph), but the one 
> like coxph<-function(....){...}
> 
> Thanks,
> 
> Jia
> 

At the commandline, type the following:

library(survival)
coxph

BTW, I'm assuming you are referring to the coxph in the survival 
package. Please be more specific next time. Better yet, read the posting 
guide.

--sundar



From tlumley at u.washington.edu  Thu May 26 22:04:32 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 26 May 2005 13:04:32 -0700 (PDT)
Subject: [R] Survey and Stratification
In-Reply-To: <4295D8E1.509@t-online.de>
References: <4295D8E1.509@t-online.de>
Message-ID: <Pine.A41.4.61b.0505261237160.303294@homer12.u.washington.edu>

On Thu, 26 May 2005, Mark Hempelmann wrote:

> Dear WizaRds,
>
> 	Working through sampling theory, I tried to comprehend the concept of 
> stratification and apply it with Survey to a small example. My question is 
> more of theoretic nature, so I apologize if this does not fully fit this 
> board's intention, but I have come to a complete stop in my efforts and need 
> an expert to help me along. Please help:
>
> age<-matrix(c(rep(1,5), rep(2,3), 1:8, rep(3,5), rep(4,3), rep(5,5), 
> rep(3,3), rep(15,5), rep(12,3), 23,25,27,21,22, 33,27,29), ncol=6, byrow=F)
> colnames(age)<-c("stratum", "id", "weight", "nh", "Nh", "y")
> age<-as.data.frame(age)

Ok.  Assuming that Nh are the population sizes in each stratum, you have 
5/15 sampled in stratum 1 and 3/12 in stratum 2.

This can be specified in a number of ways
You can use
   sampling weights of 15/5 and 12/3
   sampling probabilities of 5/15 and 3/12
without or without specifiying the finite population correction. The 
finite population correction can be specified as 15 and 12 or 5/15 and 
3/12, and if the finite population correction is specified the weights are 
then optional.

So
   d1<-svydesign(ids=~id, strata=~stratum, weight=~I(Nh/nh), data=age)
   d2<-svydesign(ids=~id, strata=~stratum, prob=~I(nh/Nh), data=age)
give the with-replacement design (agreeing with your age.des3) and
   d3<-svydesign(ids=~id, strata=~stratum, weight=~I(Nh/nh), fpc=~Nh,data=age)
   d4<-svydesign(ids=~id, strata=~stratum, prob=~I(nh/Nh), fpc=~Nh,data=age)
   d5<-svydesign(ids=~id, strata=~stratum, weight=~I(Nh/nh), fpc=~I(nh/Nh),data=age)
   d6<-svydesign(ids=~id, strata=~stratum, prob=~I(nh/Nh), fpc=~I(nh/Nh),data=age)
   d7<-svydesign(ids=~id, strata=~stratum, fpc=~Nh,data=age)
   d8<-svydesign(ids=~id, strata=~stratum, fpc=~I(nh/Nh),data=age)
all give the without-replacement design. We get
> svymean(~y,d1)
     mean     SE
y 26.296 0.9862
> svymean(~y,d2)
     mean     SE
y 26.296 0.9862
> svymean(~y,d3)
     mean     SE
y 26.296 0.8364
> svymean(~y,d4)
     mean     SE
y 26.296 0.8364
> svymean(~y,d5)
     mean     SE
y 26.296 0.8364
> svymean(~y,d6)
     mean     SE
y 26.296 0.8364
> svymean(~y,d7)
     mean     SE
y 26.296 0.8364
> svymean(~y,d8)
     mean     SE
y 26.296 0.8364

Now, looking at your examples
> ## create survey design object
> age.des1<-svydesign(ids=~id, strata=~stratum, weight=~Nh, data=age)
> svymean(~y, age.des1)
> ## gives mean 25.568, SE 0.9257

This is wrong: the sampling weight is Nh/nh, not Nh

> age.des2<-svydesign(ids=~id, strata=~stratum, weight=~I(nh/Nh), data=age)
> svymean(~y, age.des2)
> ## gives mean 25.483, SE 0.9227

This is wrong: the sampling weight is Nh/nh. You need prob=~I(nh/Nh) to 
specify sampling fractions.

> age.des3<-svydesign(ids=~id, strata=~stratum, weight=~weight, data=age)
> svymean(~y, age.des3)
> ## gives mean 26.296, SE 0.9862

This is correct and agrees with d1 and d2

> age.des4<-svydesign(ids=~id, strata=~stratum, data=age)
> svymean(~y, age.des4)
> ## gives mean 25.875, SE 0.9437

This is a stratified, unweighted mean, ie mean(age$y).


> age.des3 is the only estimator I am able to compute per hand correctly. It is 
> stratified random sampling with inverse probablility weighting with weight= 
> nh/Nh ## sample size/ stratum size.
>
> Basically, I thought the option weight=~Nh as well as weight=~I(nh/Nh) would 
> result in the same number, but it does not.

No, it does not.  A weight of 3 is not the same as a weight of 1/3.  With 
the finite population correction it is safe to assume that numbers less 
than 1 are sampling fractions and numbers greater than 1 are population 
sizes, but this isn't safe when it comes to weights.  It is possible that 
someone could want to use sampling weights less than 1.

>
> I thought the Hansen-Hurwitz estimator per stratum offers the right numbers:
> p1=5/15, p2=3/12, so y1.total=1/5*(3*118), y2.total=1/3*(4*89) and the 
> stratified estimator with this design should be: 1/27(y1.total+y2.total), 
> obviously wrong.

Since this gives a mean of 7.01 for numbers around 25 it can't be right. 
You have divided by sample size twice. You should have
   y1.total<-3*118
   y2.total<-4*89
You then will get  (y1.total+y2.total)/27 to be 26.29630, in agreement 
with svymean().


 	-thomas



From tlumley at u.washington.edu  Thu May 26 22:11:52 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 26 May 2005 13:11:52 -0700 (PDT)
Subject: [R] longitudinal survey data
In-Reply-To: <1117131639.429613772e707@webmail.utoronto.ca>
References: <1117131639.429613772e707@webmail.utoronto.ca>
Message-ID: <Pine.A41.4.61b.0505261305140.303294@homer12.u.washington.edu>

On Thu, 26 May 2005 h.brunschwig at utoronto.ca wrote:

>
> Dear R-Users!
>
> Is there a possibility in R to do analyze longitudinal survey data (repeated
> measures in a survey)? I know that for longitudinal data I can use lme() to
> incorporate the correlation structure within individual and I know that there is
> the package survey for analyzing survey data. How can I combine both? I am
> trying to calculate design-based estimates. However, if I use svyglm() from the
> survey package I would ignore the correlation structure of the repeated measures.
>

You *can* fit regression models to these data with svyglm(). Remember that 
from a design-based point of view there is no such thing as a correlation 
structure of repeated measures -- only the sampling is random, not the 
population data.


If you *want* to fit mixed models (eg because you are interested in 
estimating variance components, or perhaps to gain efficiency) then it's 
quite a bit trickier. You can't just use the sampling weights in lme(). 
You can correct for the biased sampling if you put the variables that 
affect the weights in as predictors in the model.  Cluster sampling could 
perhaps then be modelled as another level of random effect.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From elvis at xlsolutions-corp.com  Thu May 26 22:33:19 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Thu, 26 May 2005 13:33:19 -0700
Subject: [R] Consulting Positions Open at XLSolutions Corp
Message-ID: <20050526203319.29334.qmail@gem-wbe01.mesa1.secureserver.net>

XLSolutions Corp. is currently looking for junior/senior -
contract/permanent -
consultants with math or stats background and capable of programming in
R or S-plus
and C or Java. Skills in data analysis are important for our clients
partnering with us in search of the most talented and accomplished
project
leaders. PhD required for senior positions.

Telecommuters are encouraged to apply! Please email your resume to 
hr at xlsolutions-corp.com

Best, Elvis



From brett at hbrc.govt.nz  Fri May 27 00:00:34 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Fri, 27 May 2005 10:00:34 +1200
Subject: [R] Chi Square Test on two groups of variables
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA9868B349@MSX2>


Dear R help
I have been trying to conduct a chi square test on two groups of variables
to test whether there is any relationship between the two sets of variables

chisq.test(oxygen, train)

        Pearson's Chi-squared test

data:  oxygen 
X-squared = 26.6576, df = 128, p-value = 1

> chisq.test(oxygen)

        Pearson's Chi-squared test

data:  oxygen 
X-squared = 26.6576, df = 128, p-value = 1

It looks as if R is only reading the first set of variables pertaining to
oxygen. Is there a way for it to test for two groups of variables?

brett stansfield

Brett Stansfield 
Environmental Scientist - Water Quality 
Hawke's Bay Regional Council 
102 Vautier Street 
Private Bag 6006 
Napier 
Phone (06) 835-9200 extn 9334 
Fax (06) 835-3601



From kzhao at usc.edu  Fri May 27 01:16:43 2005
From: kzhao at usc.edu (Keyan Zhao)
Date: Thu, 26 May 2005 16:16:43 -0700
Subject: [R] specifying values in correlation matrix in nlme
Message-ID: <1117149404.22493.37.camel@cycad.usc.edu>

Could anyone help with a linear mixed model fitting problem ?

The model is : 

Y= Xp + Zu + e 
where X, Z are known design matrix, p is fixed effect factor, u is
random effect,  u~ (0, G) , e~(0,R)

The main problem is , I want to fix the covariance matrix G to be a
constant times a known covariance matrix A,   G = c*A (c is positive
constant, A is a predefined matrix with values manually set by me.

I know the correlation option in lme function can specify some kind of
correlation. but only with the Construct function defined, not whatever
ever form I want.

Any good ideas of how to do this in R ?

Thanks a lot in advance,

Keyan Zhao
Computational Biology and Bioinformatics program 
Univ of Southern California
Email: kzhao at usc.edu



From nusbj at hotmail.com  Fri May 27 02:29:58 2005
From: nusbj at hotmail.com (Zhen Pang)
Date: Fri, 27 May 2005 00:29:58 +0000
Subject: [R] define matrix by outer?
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B183357CA1@nsabpmail>
Message-ID: <BAY22-F374DAD0D06587CCA030B11AF000@phx.gbl>

Dear all,

I'd like to define a k by k matrix where the element is defined by

mn<-function(m,n) sum(choose(m,(m-0:m))*choose((k-m),(n-m+0:m)))

the mn function works fine for scalar m and n, however, it fails to define 
the matrix by outer.

a<-outer(1:k,1:k,mn)

gives error message:

Error in outer(1:k, 1:k, mn) : dim<- : dims [product 64] do not match the 
length of object [1]
In addition: Warning messages:
1: Numerical expression has 64 elements: only the first used in: 0:m
2: Numerical expression has 64 elements: only the first used in: 0:m

I can define this matrix by loop or some apply function, however, the outer 
function should be the most straitforward way. How to deal with it? Thanks.

Regards,

Zhen



From ggrothendieck at gmail.com  Fri May 27 03:15:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 May 2005 21:15:09 -0400
Subject: [R] define matrix by outer?
In-Reply-To: <BAY22-F374DAD0D06587CCA030B11AF000@phx.gbl>
References: <3D0B2434377E984E9C85CAA316F8B183357CA1@nsabpmail>
	<BAY22-F374DAD0D06587CCA030B11AF000@phx.gbl>
Message-ID: <971536df050526181592e84ed@mail.gmail.com>

On 5/26/05, Zhen Pang <nusbj at hotmail.com> wrote:
> Dear all,
> 
> I'd like to define a k by k matrix where the element is defined by
> 
> mn<-function(m,n) sum(choose(m,(m-0:m))*choose((k-m),(n-m+0:m)))
> 
> the mn function works fine for scalar m and n, however, it fails to define
> the matrix by outer.
> 
> a<-outer(1:k,1:k,mn)
> 
> gives error message:
> 
> Error in outer(1:k, 1:k, mn) : dim<- : dims [product 64] do not match the
> length of object [1]
> In addition: Warning messages:
> 1: Numerical expression has 64 elements: only the first used in: 0:m
> 2: Numerical expression has 64 elements: only the first used in: 0:m
> 
> I can define this matrix by loop or some apply function, however, the outer
> function should be the most straitforward way. How to deal with it? Thanks.
> 


Check out ?outer where it mentions that the function must be
able to handle vector arguments.  Yours does not but you can
transform it to one which does using mapply:

outer(1:k, 1:k, function(x,y) mapply(mn, x, y))



From gohidg at gmail.com  Fri May 27 03:22:39 2005
From: gohidg at gmail.com (Guohui Ding)
Date: Fri, 27 May 2005 09:22:39 +0800
Subject: [R] Chi Square Test on two groups of variables
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA9868B349@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA9868B349@MSX2>
Message-ID: <f04a1d1d050526182254e49255@mail.gmail.com>

If 'oxygen' is a matrix with one row or column, 'oxygen' is treated as
a one-dimensional contingency table.  In this case, the hypothesis
tested is whether the population probabilities equal those in 'p', or
are all equal  if 'p' is not given.

you can convert the 'oxygen' and 'train' variables to vector or take a
two-dimensional contingency table by matrix(c(oxygen, train), nr=2).

you will get more information when you type ?chisq.test.

2005/5/27, Brett Stansfield <brett at hbrc.govt.nz>:
> 
> Dear R help
> I have been trying to conduct a chi square test on two groups of variables
> to test whether there is any relationship between the two sets of variables
> 
> chisq.test(oxygen, train)
> 
>        Pearson's Chi-squared test
> 
> data:  oxygen
> X-squared = 26.6576, df = 128, p-value = 1
> 
> > chisq.test(oxygen)
> 
>        Pearson's Chi-squared test
> 
> data:  oxygen
> X-squared = 26.6576, df = 128, p-value = 1
> 
> It looks as if R is only reading the first set of variables pertaining to
> oxygen. Is there a way for it to test for two groups of variables?
> 
> brett stansfield
> 
> Brett Stansfield
> Environmental Scientist - Water Quality
> Hawke's Bay Regional Council
> 102 Vautier Street
> Private Bag 6006
> Napier
> Phone (06) 835-9200 extn 9334
> Fax (06) 835-3601
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
ADDRESS: Bioinformatics Center, Shanghai Institutes for Biological
Sciences, Chinese Academy of Sciences
320 Yueyang Road, Shanghai 200031, P.R.China
TELEPHONE: 86-21-54920086



From lauraholt_983 at hotmail.com  Fri May 27 06:18:50 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Thu, 26 May 2005 23:18:50 -0500
Subject: [R] Power set
Message-ID: <BAY10-F8C0AD26CC102059B45367D6000@phx.gbl>

Hi again!

I have a data.frame with the columns y, x1, x2, x3.

I would like to fit linear models with one variable at a time,
then 2 variables at a time, and then 3.

Makes me think of a power set.

Anyhow, is there a function to produce the right hand side of the formulas, 
please?

thanks,
Laura Holt
mailto: lauraholt_983 at hotmail.com
R Version 2.1.0 Windows



From mail at bymouth.com  Fri May 27 06:22:33 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Fri, 27 May 2005 14:22:33 +1000
Subject: [R] logistic regression
Message-ID: <007d01c56273$b86a4320$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050527/7bb57f3c/attachment.pl

From Simon.Blomberg at anu.edu.au  Fri May 27 06:37:08 2005
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Fri, 27 May 2005 14:37:08 +1000
Subject: [R] logistic regression
In-Reply-To: <007d01c56273$b86a4320$9701a8c0@Tablet>
References: <007d01c56273$b86a4320$9701a8c0@Tablet>
Message-ID: <a06110401bebc53b2d947@[150.203.51.113]>

predict.glm by default produces predictions on the scale of the 
linear predictors. If in a logistic regression, you want the 
predictions to be on the response scale [0,1],  use

x <- predict(logistic.model, medians, type="response")

for example. See ?predict.glm for details.

Cheers,

Simon.



>Hi
>
>I am working on corpora of automatically recognized utterances, looking
>for features that predict error in the hypothesis the recognizer is
>proposing. 
>
>I am using the glm functions to do logistic regression.  I do this type
>of thing:
>
>*       logistic.model = glm(formula = similarity ~., family = binomial,
>data = data)
>
>and end up with a model:
>
>>  summary(logistic.model)
>
>Call:
>glm(formula = similarity ~ ., family = binomial, data = data)
>
>Deviance Residuals:
>     Min       1Q   Median       3Q      Max 
>-3.1599   0.2334   0.3307   0.4486   1.2471 
>
>Coefficients:
>                         Estimate Std. Error z value Pr(>|z|)   
>(Intercept)           11.1923783  4.6536898   2.405  0.01617 * 
>length                -0.3529775  0.2416538  -1.461  0.14410   
>meanPitch             -0.0203590  0.0064752  -3.144  0.00167 **
>minimumPitch           0.0257213  0.0053092   4.845 1.27e-06 ***
>maximumPitch          -0.0003454  0.0030008  -0.115  0.90838   
>meanF1                 0.0137880  0.0047035   2.931  0.00337 **
>meanF2                 0.0040238  0.0041684   0.965  0.33439   
>meanF3                -0.0075497  0.0026751  -2.822  0.00477 **
>meanF4                -0.0005362  0.0007443  -0.720  0.47123   
>meanF5                -0.0001560  0.0003936  -0.396  0.69187   
>ratioF2ToF1            0.2668678  2.8926149   0.092  0.92649   
>ratioF3ToF1            1.7339087  1.7655757   0.982  0.32607   
>jitter                -5.2571384 10.8043359  -0.487  0.62656   
>shimmer               -2.3040826  3.0581950  -0.753  0.45120   
>percentUnvoicedFrames  0.1959342  1.3041689   0.150  0.88058   
>numberOfVoiceBreaks   -0.1022074  0.0823266  -1.241  0.21443   
>percentOfVoiceBreaks  -0.0590097  1.2580202  -0.047  0.96259   
>meanIntensity         -0.0765124  0.0612008  -1.250  0.21123   
>minimumIntensity       0.1037980  0.0331899   3.127  0.00176 **
>maximumIntensity      -0.0389995  0.0430368  -0.906  0.36484   
>ratioIntensity        -2.0329346  1.2420286  -1.637  0.10168   
>noSyllsIntensity       0.1157678  0.0947699   1.222  0.22187   
>startSpeech            0.0155578  0.1343117   0.116  0.90778   
>speakingRate          -0.2583315  0.1648337  -1.567  0.11706   
>---
>Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
>(Dispersion parameter for binomial family taken to be 1)
>
>     Null deviance: 2462.3  on 4310  degrees of freedom
>Residual deviance: 2209.5  on 4287  degrees of freedom
>AIC: 2257.5
>
>Number of Fisher Scoring iterations: 6
>
>
>I have seen models where almost all the features are showing one in a
>thousand significance but I accept that I could improve my model by
>normalizing some of the features (some are left skewed and I understand
>that I will get a better fir by taking their logs, for example).
>
>What really worries me is that the logistic function produces
>predictions that appear to fall well outside 0 to 1.
>
>If I make a dataset of the medians of the above features and use my
>logistic.model on it, it produces a
>figure of:
>
>  > x = predict(logistic.model, medians)
>>  x
>[1] 2.82959
>>
>
>which is well outside the range of 0 to 1.
>
>The actual distribution of all the predictions is:
>
>>  summary(pred)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>  -1.516   2.121   2.720   2.731   3.341   6.387
>>
>
>I can get the model to give some sort of prediction by doing this:
>
>>  pred = predict(logistic.model, data)
>>  pred[pred <= 1.5] = 0
>>  pred[pred > 1.5] = 1
>>  t = table(pred, data[,24])
>>  t
>    
>pred 0    1  
>    0  102  253
>    1  255 3701
>>
>>  classAgreement(t)
>$diag
>[1] 0.8821619
>
>$kappa
>[1] 0.2222949
>
>$rand
>[1] 0.7920472
>
>$crand
>[1] 0.1913888
>
>>
>
>but as you can see I am using a break point well outside the range 0 to
>1 and the kappa is rather low (I think).
>
>I am a bit of a novice in this, and the results worry me. 
>
>Can anyone comment if the results look strange, or if they know I am
>doing something wrong?
>
>Stephen
>
>
>--
>No virus found in this outgoing message.
>Checked by AVG Anti-Virus.
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Visiting Fellow
School of Botany & Zoology
The Australian National University
Canberra ACT 0200
Australia

T: +61 2 6125 8057  email: Simon.Blomberg at anu.edu.au
F: +61 2 6125 5573

CRICOS Provider # 00120C



From siwulayid at gmail.com  Fri May 27 09:56:08 2005
From: siwulayid at gmail.com (Luwis Tapiwa Diya)
Date: Fri, 27 May 2005 09:56:08 +0200
Subject: [R] Accounting for clustered data in Rpart or Mvpart
Message-ID: <ddf1e2bc0505270056171c9709@mail.gmail.com>

I am working on data that is in clusters (eg events from persons in
the family).So I have a clustered data set up and would like to build
a tree(regression /classification tree) taking into account this
clustered nature of the data.

Do anybody know how to do this or maybe the code to take into account
the clustered data in rpart



From slist at oomvanlieshout.net  Fri May 27 10:08:50 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 27 May 2005 10:08:50 +0200
Subject: [R] Soil texture triangle in R?
Message-ID: <4296D592.5040108@oomvanlieshout.net>

Dear R users,

has anybody made an attempt to create the soil texture triangle graph in 
R? For an example see here:

http://www.teachingkate.org/images/soiltria.gif

I would like to get the lines in black and texture labels in gray to 
allow for plotting my texture results on top.

Any examples or suggestions are very welcome!

Thanks in advance,

Sander.

-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From john.marsland at wmgfunds.com  Fri May 27 10:31:08 2005
From: john.marsland at wmgfunds.com (John Marsland)
Date: Fri, 27 May 2005 09:31:08 +0100
Subject: [R] mixed-integer optimisation
Message-ID: <44A6F6B2-4D0B-42CA-8DBF-F8E08C52CE2A@wmgfunds.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050527/b39d3acf/attachment.pl

From joseclaudio.faria at terra.com.br  Fri May 27 11:39:32 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Fri, 27 May 2005 06:39:32 -0300
Subject: [R] Contingency tables from data.frames
In-Reply-To: <971536df050524152110a35e60@mail.gmail.com>
References: <4293A4F2.7080301@terra.com.br>
	<971536df050524152110a35e60@mail.gmail.com>
Message-ID: <4296EAD4.20804@terra.com.br>

The final version with the help of Gabor Grotendieck (thanks Gabor, very much!)

#######################
#   EasieR - Package  #
#######################

# Common function
er.make.table <- function(x,
                           start,
                           end,
                           h,
                           right) {
   # Absolut frequency
   f <- table(cut(x, br=seq(start, end, h), right=right))

   # Relative frequency
   fr <- f/length(x)

   # Relative frequency, %
   frP <- 100*(f/length(x))

   # Cumulative frequency
   fac <- cumsum(f)

   # Cumulative frequency, %
   facP <<- 100*(cumsum(f/length(x)))

   fi   <- round(f, 2)
   fr   <- round(as.numeric(fr), 2)
   frP  <- round(as.numeric(frP), 2)
   fac  <- round(as.numeric(fac), 2)
   facP <- round(as.numeric(facP),2)

   # Make final table
   res <- data.frame(fi, fr, frP, fac, facP)
   names(res) <- c('Class limits', 'fi', 'fr', 'fr(%)', 'fac', 'fac(%)')
   return(res)

}

#With Gabor Grotendieck suggestions (thanks Gabor, very much!)
er.table <- function(x, ...) UseMethod("er.table")

er.table.default <- function(x,
                              k,
                              start,
                              end,
                              h,
                              breaks=c('Sturges', 'Scott', 'FD'),
                              right=FALSE) {

   #User define nothing or not 'x' isn't numeric -> stop
   stopifnot(is.numeric(x))

   #User define only 'x'
   #(x, {k, start, end, h}, [breaks, right])
   if (missing(k) && missing(start) && missing(end) && missing(h) ){

     x <- na.omit(x)

     brk <- match.arg(breaks)
     switch(brk,
            Sturges = k <- nclass.Sturges(x),
            Scott   = k <- nclass.scott(x),
            FD      = k <- nclass.FD(x))

     tmp   <- range(x)
     start <- tmp[1] - abs(tmp[2])/100
     end   <- tmp[2] + abs(tmp[2])/100
     R     <- end-start
     h     <- R/k

   }

   #User define 'x' and 'k'
   #(x, k, {start, end, h}, [breaks, right])
   else if (missing(start) && missing(end) && missing(h)) {

     stopifnot(length(k) >= 1)

     x <- na.omit(x)

     tmp   <- range(x)
     start <- tmp[1] - abs(tmp[2])/100
     end   <- tmp[2] + abs(tmp[2])/100
     R     <- end-start
     h     <- R/abs(k)

   }

   #User define 'x', 'start' and 'end'
   #(x, {k,} start, end, {h,} [breaks, right])
   else if (missing(k) && missing(h)) {

     stopifnot(length(start) >= 1, length(end) >=1)

     x <- na.omit(x)

     tmp <- range(x)
     R   <- end-start
     k   <- sqrt(abs(R))
     if (k < 5)  k <- 5 #min value of k
     h   <- R/k

   }

   #User define 'x', 'start', 'end' and 'h'
   #(x, {k,} start, end, h, [breaks, right])
   else if (missing(k)) {

     stopifnot(length(start) >= 1, length(end) >= 1, length(h) >= 1)
     x <- na.omit(x)

   }

   else stop('Error, please, see the function sintax!')

   tbl <- er.make.table(x, start, end, h, right)
   return(tbl)

}

er.table.data.frame <- function(df,
                                 k,
                                 breaks=c('Sturges', 'Scott', 'FD'),
                                 right=FALSE) {

   stopifnot(is.data.frame(df))

   tmpList <- list()
   logCol  <- sapply(df, is.numeric)

   for (i in 1:ncol(df)) {

     if (logCol[i]) {

       x <- as.matrix(df[ ,i])
       x <- na.omit(x)

       #User define only x and/or 'breaks'
       #(x, {k,}[breaks, right])
       if (missing(k)) {

         brk <- match.arg(breaks)
         switch(brk,
                Sturges = k <- nclass.Sturges(x),
                Scott   = k <- nclass.scott(x),
                FD      = k <- nclass.FD(x))

         tmp   <- range(x)
         start <- tmp[1] - abs(tmp[2])/100
         end   <- tmp[2] + abs(tmp[2])/100
         R     <- end-start
         h     <- R/k

       }

       #User define 'x' and 'k'
       #(x, k,[breaks, right])
       else {

         tmp   <- range(x)
         start <- tmp[1] - abs(tmp[2])/100
         end   <- tmp[2] + abs(tmp[2])/100
         R     <- end-start
         h     <- R/abs(k)

       }

       tbl     <- er.make.table(x, start, end, h, right)
       tmpList <- c(tmpList, list(tbl))

     }

   }

   valCol <- logCol[logCol]
   names(tmpList) <- names(valCol)
   return(tmpList)

}

Best,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From stefaan.lhermitte at biw.kuleuven.be  Fri May 27 12:04:23 2005
From: stefaan.lhermitte at biw.kuleuven.be (Stefaan Lhermitte)
Date: Fri, 27 May 2005 12:04:23 +0200
Subject: [R] Simplify formula for heterogeneity
In-Reply-To: <XFMail.050526170517.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050526170517.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <4296F0A7.8020301@biw.kuleuven.be>

Thank you very much Ted! I have been looking at your simplification for 
more then an hour, but I don't see how you did it.
Could you perhaps, if it is not to much work, explain me how you reduced 
H? It would help me to understand what I am realy doing.

Looking at the result, it seems indeed that H does add more information 
than sd already did. Intuitively I thought the square of the sum of all 
possible differences would not be related to the standard deviation. 
Looking at your result it seems it is related by a factor sqrt(2*(n-1)) 
so there is no special point in calculating  H and I know I cannot trust 
my intuition anymore.

Thanks again!

Kind regards,
Stef



From D.T.Jacho-Chavez at lse.ac.uk  Fri May 27 12:18:26 2005
From: D.T.Jacho-Chavez at lse.ac.uk (Jacho-Chavez,DT  (pgr))
Date: Fri, 27 May 2005 11:18:26 +0100
Subject: [R] Testing Nonlinear Restrictions
Message-ID: <44B42F3306B4BC4A9DBC8BDEE92947700291B7D2@exs1.backup>

Dear all,

I'm interested in testing 2 nonlinear restrictions on coefficients of a nls object. Is there a package for doing this? Something in the lines of `test(nls object, res=c("res 1","res 2"),...)'
I only found the function delta.method in the alr3 library that calculates the se of a singleton nonlinear restriction of a nls object using the delta method.

Thanks in advanced for your help and suggestions.


David



From zjaoy at yahoo.fr  Fri May 27 12:52:22 2005
From: zjaoy at yahoo.fr (zjao yana)
Date: Fri, 27 May 2005 12:52:22 +0200 (CEST)
Subject: [R] cancor
Message-ID: <20050527105222.71738.qmail@web26406.mail.ukl.yahoo.com>

Hello,
I'm a beginner to use R. 
When use cancor to analysis two data set X,Y, for
example:
 can<-cancor(X, Y)
 then 5 components are returned, that is cor, xcoef,
ycoef, xcentre, ycentre; 

The explained variance can be calculated by X %*%
can$xcoef or Y %*% can$ycoef, but how to alculate the
canonical maps of field X and Y? 
Thanks!

yana



From ajayshah at mayin.org  Fri May 27 13:11:31 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Fri, 27 May 2005 16:41:31 +0530
Subject: [R] R commandline editor question
Message-ID: <20050527111131.GL17261@lubyanka.local>

I am using R 2.1 on Apple OS X.

When I get the ">" prompt, I find it works well with emacs commandline
editing. Keys like M-f C-k etc. work fine.

The one thing that I really yearn for, which is missing, is bracket
matching When I am doing something which ends in )))) it is really
useful to have emacs or vi-style bracket matching, so as to be able
to visually keep track of whether I have the correct matching
brackets, whether ( or { or [.

I'm sure this is possible. I will be most grateful if someone will
show the way :-) Thanks,

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From r.hankin at noc.soton.ac.uk  Fri May 27 13:36:28 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 27 May 2005 12:36:28 +0100
Subject: [R] R commandline editor question
In-Reply-To: <20050527111131.GL17261@lubyanka.local>
References: <20050527111131.GL17261@lubyanka.local>
Message-ID: <e737f5395d5c4d3b583e859380c15d08@soc.soton.ac.uk>

Hi  Ajay

well ESS has such a facility.

However, I think Mathematica has a super scheme: unbalanced brackets 
show up
in red, making them obvious.

This is particularly good for spotting wrongly interleaved brackets, as 
in

([  blah di blah  )]

<note bracket closure is out of order>

in which case both opening braces are highlighted in red: and the 
system won't
accept a newline until the closures are all correctly matched.

Would anyone else find such a thing useful?

Could the ESS team make something like this happen?





On May 27, 2005, at 12:11 pm, Ajay Narottam Shah wrote:

> I am using R 2.1 on Apple OS X.
>
> When I get the ">" prompt, I find it works well with emacs commandline
> editing. Keys like M-f C-k etc. work fine.
>
> The one thing that I really yearn for, which is missing, is bracket
> matching When I am doing something which ends in )))) it is really
> useful to have emacs or vi-style bracket matching, so as to be able
> to visually keep track of whether I have the correct matching
> brackets, whether ( or { or [.
>
> I'm sure this is possible. I will be most grateful if someone will
> show the way :-) Thanks,
>
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From f.harrell at vanderbilt.edu  Fri May 27 13:36:42 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 27 May 2005 07:36:42 -0400
Subject: [R] Power set
In-Reply-To: <BAY10-F8C0AD26CC102059B45367D6000@phx.gbl>
References: <BAY10-F8C0AD26CC102059B45367D6000@phx.gbl>
Message-ID: <4297064A.1040103@vanderbilt.edu>

Laura Holt wrote:
> Hi again!
> 
> I have a data.frame with the columns y, x1, x2, x3.
> 
> I would like to fit linear models with one variable at a time,
> then 2 variables at a time, and then 3.
> 
> Makes me think of a power set.

Makes me think of irreproducible results if you use the output to select 
a single model  :-)

Frank Harrell

> 
> Anyhow, is there a function to produce the right hand side of the 
> formulas, please?
> 
> thanks,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> R Version 2.1.0 Windows

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From t.muhlhofer at lse.ac.uk  Fri May 27 13:38:00 2005
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Fri, 27 May 2005 12:38:00 +0100
Subject: [R] Function environments lm() weights
Message-ID: <42970698.80409@lse.ac.uk>

I am writing a function of weighted regression, as a procedure for 
heteroskedasticity.

The function runs an auxiliary regression whose fitted values I assign 
to fit, and then I go:

     w <- 1/(exp(fit/2))

     ## Rerun the old regression ##
     if(gls) {
       wtd.model <- glm(model, weights=w)
     }

     if(!gls) {
       wtd.model <- lm(model, weights=w, x=TRUE)
     }

In this version, R complains that it can't find w. How can I tell it to 
look for w in the function's environment, rather than in environment 1 
or whatever?

An easy workaround, of course, is to superassign w and remove it 
afterwards, but that's a little messy, in case the user already has a 
variable called w in his environment.

Thanks,
	Tobias Muhlhofer



From ajayshah at mayin.org  Fri May 27 13:47:19 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Fri, 27 May 2005 17:17:19 +0530
Subject: [R] R commandline editor question
In-Reply-To: <e737f5395d5c4d3b583e859380c15d08@soc.soton.ac.uk>
References: <20050527111131.GL17261@lubyanka.local>
	<e737f5395d5c4d3b583e859380c15d08@soc.soton.ac.uk>
Message-ID: <20050527114719.GN17261@lubyanka.local>

> well ESS has such a facility.
> 
> However, I think Mathematica has a super scheme: unbalanced brackets 
> show up
> in red, making them obvious.
> 
> This is particularly good for spotting wrongly interleaved brackets, as 
> in
> 
> ([  blah di blah  )]
> 
> <note bracket closure is out of order>
> 
> in which case both opening braces are highlighted in red: and the 
> system won't
> accept a newline until the closures are all correctly matched.
> 
> Would anyone else find such a thing useful?
> 
> Could the ESS team make something like this happen?

ess is great, but I was asking about the R commandline. I tend to
write a lot of stuff on the fly at the R commandline.

Yes, colours are a great way to deal with this, and this feature
should ideally be in ESS.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From Luisr at frs.fo  Fri May 27 14:22:11 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Fri, 27 May 2005 13:22:11 +0100
Subject: [R] Round a line
Message-ID: <s2971f09.052@ffdata.setur.fo>

R-help,

I have lloked in the archives found no answer to how to round the line
joint.

I have usedthe arguments lnd, ljoin in par but I get no differences in
the plotting.

x=1:10
par(ljoin="round",lend="round")
plot(x,sin(x),type="l",lwd=2)


Any suggestions?

I run on a Windows XP machine.

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R  

Thank you in advance



From plamy at daimi.au.dk  Fri May 27 14:23:50 2005
From: plamy at daimi.au.dk (Philippe Lamy)
Date: Fri, 27 May 2005 14:23:50 +0200
Subject: [R] Postscript
Message-ID: <1117196630.429711564f012@webmail.daimi.au.dk>


Hi,

I would like to create a multi-page postscript file. How can I do that in R ? Is
it possible ?

Thanks for help.

Philippe



From spencer.graves at pdf.com  Fri May 27 14:25:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 27 May 2005 21:25:47 +0900
Subject: [R] Power set
In-Reply-To: <4297064A.1040103@vanderbilt.edu>
References: <BAY10-F8C0AD26CC102059B45367D6000@phx.gbl>
	<4297064A.1040103@vanderbilt.edu>
Message-ID: <429711CB.8020301@pdf.com>

Hi, Laura:

	  Have you considered "regsubsets" in library(leaps)?  Also, have you 
done an R site search "www.r-project.org" -> search -> "R site search" 
for something like "all subsets regression"?

	  Consistent with Frank Harrell's comment, comments, I suggest you 
randomly permute your response variable a hundred or a thousand times 
and count how many times the procedure finds a non-null model.  This 
might help you calibrate the procedure.  The archives include an email 
from Frank Harrell citing two relevant articles.

	  Hope this helps.
	  spencer graves	

Frank E Harrell Jr wrote:

> Laura Holt wrote:
> 
>> Hi again!
>>
>> I have a data.frame with the columns y, x1, x2, x3.
>>
>> I would like to fit linear models with one variable at a time,
>> then 2 variables at a time, and then 3.
>>
>> Makes me think of a power set.
> 
> 
> Makes me think of irreproducible results if you use the output to select 
> a single model  :-)
> 
> Frank Harrell
> 
>>
>> Anyhow, is there a function to produce the right hand side of the 
>> formulas, please?
>>
>> thanks,
>> Laura Holt
>> mailto: lauraholt_983 at hotmail.com
>> R Version 2.1.0 Windows
> 
>



From jmacdon at med.umich.edu  Fri May 27 14:42:44 2005
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Fri, 27 May 2005 08:42:44 -0400
Subject: [R] Power set
In-Reply-To: <429711CB.8020301@pdf.com>
References: <BAY10-F8C0AD26CC102059B45367D6000@phx.gbl>	<4297064A.1040103@vanderbilt.edu>
	<429711CB.8020301@pdf.com>
Message-ID: <429715C4.3080606@med.umich.edu>

Spencer Graves wrote:

> Hi, Laura:
>
>       Have you considered "regsubsets" in library(leaps)?  Also, have 
> you done an R site search "www.r-project.org" -> search -> "R site 
> search" for something like "all subsets regression"?

Or much simpler if you are running R-2.1.0, use RSiteSearch("all subsets 
regression")

Best,

Jim

-- 
James W. MacDonald
Affymetrix and cDNA Microarray Facility
University of Michigan
Comprehensive Cancer Center
1500 E. Medical Center Drive
Ann Arbor MI 48109



From ligges at statistik.uni-dortmund.de  Fri May 27 14:58:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 May 2005 14:58:45 +0200
Subject: [R] Postscript
In-Reply-To: <1117196630.429711564f012@webmail.daimi.au.dk>
References: <1117196630.429711564f012@webmail.daimi.au.dk>
Message-ID: <42971985.40207@statistik.uni-dortmund.de>

Philippe Lamy wrote:
> Hi,
> 
> I would like to create a multi-page postscript file. How can I do that in R ? Is
> it possible ?


Yes, simply draw more than one plot. See ?postscript and its argument 
"onefile", which already defaults to TRUE.

Uwe Ligges


> Thanks for help.
> 
> Philippe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri May 27 15:00:20 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 27 May 2005 22:00:20 +0900
Subject: [R] Testing Nonlinear Restrictions
In-Reply-To: <44B42F3306B4BC4A9DBC8BDEE92947700291B7D2@exs1.backup>
References: <44B42F3306B4BC4A9DBC8BDEE92947700291B7D2@exs1.backup>
Message-ID: <429719E4.9090408@pdf.com>

	  What kind of nonlinear restriction?  Can you solve for one or more of 
the parameters in terms of the other(s) [either directly or implicitly]? 
  If yes, then let

	  fit1 <- nls(... full model ... )
	  fit2 <- nls(... restricted model ...)

	  anova(fit1, fit2)

	  If my memory is correct, Doug Bates, in his PhD dissertation ~25 
years ago, decomposed the nonlinearity in nonlinear least squares into 
"intrinsic curvature" and "parameter effects curvature".  The Wald test 
is distorted by both sources types of nonlinearity, but the standard 
likelihood ratio anova is affected only by "intrinsic curvature", and 
not "parameter effects" (provided the algorithm actually converges 
appropriately).  Moreover, by reanalyzing a fair number of published 
data sets, Doug demostrated that in a nearly all practical application, 
the parameter effects curvature was much larger than the intrinsic 
curvature, and the latter was close to negligible in nearly all cases, 
while the parameter effects curvature was often of sufficient magnitude 
to substantively distort the answers.  For more information, see Bates & 
Watts (1988) Nonlinear Regression Analysis and Its Applications (Wiley).

	  hope this helps.   	
	  spencer graves	

Jacho-Chavez,DT (pgr) wrote:

> Dear all,
> 
> I'm interested in testing 2 nonlinear restrictions on coefficients of a nls object. Is there a package for doing this? Something in the lines of `test(nls object, res=c("res 1","res 2"),...)'
> I only found the function delta.method in the alr3 library that calculates the se of a singleton nonlinear restriction of a nls object using the delta method.
> 
> Thanks in advanced for your help and suggestions.
> 
> 
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri May 27 15:03:31 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 May 2005 15:03:31 +0200
Subject: [R] Round a line
In-Reply-To: <s2971f09.052@ffdata.setur.fo>
References: <s2971f09.052@ffdata.setur.fo>
Message-ID: <42971AA3.2070306@statistik.uni-dortmund.de>

Luis Ridao Cruz wrote:

> R-help,
> 
> I have lloked in the archives found no answer to how to round the line
> joint.
> 
> I have usedthe arguments lnd, ljoin in par but I get no differences in
> the plotting.
> 
> x=1:10
> par(ljoin="round",lend="round")
> plot(x,sin(x),type="l",lwd=2)
> 
> 
> Any suggestions?


Well, round is the default! You have to zoom in or make even thicker 
lines. Hence you might want to try out the folowing to see differences:

   x <- 1:10
   par(mfrow = c(1, 2))
   plot(x, sin(x), type="l", lwd=10)
   par(ljoin="mitre", lend="butt")
   plot(x, sin(x), type="l", lwd=10)

Uwe Ligges



> I run on a Windows XP machine.
> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    1.0            
> year     2005           
> month    04             
> day      18             
> language R  
> 
> Thank you in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Allan at STATS.uct.ac.za  Fri May 27 15:24:05 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Fri, 27 May 2005 15:24:05 +0200
Subject: [R] r: LEXICOGRAPHIC ORDERING
Message-ID: <42971F75.47B4677C@STATS.uct.ac.za>

HI all

i have a seemingly simple question.

given a sequence of numbers say, 1,2,3,4,5.

i would like to get all of the possible two number arrangments
(combinations), all 3 number arrangents ... 5 number arrangements
(combinations).

i.e. 

in the 2 number case:

12,13,14,15,23,24,25,34,35,45



any ideas?

From luan_sheng at yahoo.com.cn  Fri May 27 15:26:11 2005
From: luan_sheng at yahoo.com.cn (luan_sheng)
Date: Fri, 27 May 2005 21:26:11 +0800
Subject: [R] how to get this kind of binomial distribution simulation number?
Message-ID: <200505271326.j4RDQMB9029441@hypatia.math.ethz.ch>



hai, I want to perform a simulation like thisÅ£Å∫

Suppose that I have one population  ,it's size is 500, is composed of x,y
and z.  The probability of x, y and z is respectively is 0.3, 0.5, 0.2. I
wan to simulate a new same size population based ratio of x, y and z, how
can I get and assess  the number of x, y and z. ?

luan

Key Laboratory for Sustainable Utilization of Marine Fisheries Resources,
Ministry of Agriculture ,Yellow Sea Fisheries Research Institute, Chinese
Academy of Fishery Sciences, Qingdao 266071,China

__________________________________________________

Å—Å≈ÅªÅ¢Å√Å‚Å∑Å—GÅ”Å ÅœÅ‰Å£Å≠Å÷Å–ÅπÅ˙ÅµÅ⁄Å“ÅªÅæÅ¯ÅŒÅﬁÅ¿Å¨ÅªÅ¯Å”Å ÅºÅ˛Å…ÅßÅ»Å≈Å≥Å¨Å¥ÅÛÅ”Å ÅœÅ‰



From slist at oomvanlieshout.net  Fri May 27 15:32:54 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 27 May 2005 15:32:54 +0200
Subject: [R] Soil texture triangle in R?
In-Reply-To: <4296D592.5040108@oomvanlieshout.net>
References: <4296D592.5040108@oomvanlieshout.net>
Message-ID: <42972186.50404@oomvanlieshout.net>

Hi Jim,

This looks impressive! It gives me the 'background' graph. However, I'm 
not sure how I can use this function to plot my soil texture values! Can 
you explain?

I would like to be able to plot my soil texture samples in the same 
graph as the one your function plots.

Maybe I should try to figure out how to replicate your code inside a 
ternaryplot{vcd} call.

Cheers,

Sander.

Jim Lemon wrote:
 > Sander Oom wrote:
 >> Dear R users,
 >>
 >> has anybody made an attempt to create the soil texture triangle graph
 >> in R? For an example see here:
 >>
 >> http://www.teachingkate.org/images/soiltria.gif
 >>
 >> I would like to get the lines in black and texture labels in gray to
 >> allow for plotting my texture results on top.
 >>
 >> Any examples or suggestions are very welcome!
 >>
 > It's not too hard to write a plot function to do this, but I'm not sure
 > whether this will be what you want. Anyway, try it out.
 >
 > Jim
 >
 > ------------------------------------------------------------------------
 >
 > soil.triangle<-function() {
 >  oldpar<-par(no.readonly=TRUE)
 >  plot(0:1,type="n",axes=FALSE,xlim=c(0,1.1),ylim=c(0,1),
 >   main="Soil Triangle",xlab="",ylab="")
 >  # first draw the triangle
 >  x1<-c(0,0,0.5)
 >  sin60<-sin(pi/3)
 >  x2<-c(1,0.5,1)
 >  y1<-c(0,0,sin60)
 >  y2<-c(0,sin60,0)
 >  segments(x1,y1,x2,y2)
 >  # now the bottom internal ticks
 >  x1<-seq(0.1,0.9,by=0.1)
 >  x2<-x1
 >  y1<-rep(0,9)
 >  y2<-rep(0.02,9)
 >  segments(x1,y1,x2,y2)
 >  text(x1,y1-0.03,as.character(rev(seq(10,90,by=10))))
 >  # now the left internal ticks
 >  y1<-x1*sin60
 >  x1<-x1*0.5
 >  x2<-x1+0.02*sin60
 >  y2<-y1-0.02*0.5
 >  segments(x1,y1,x2,y2)
 >  text(x1-0.03,y1+0.015,as.character(seq(10,90,by=10)))
 >  x1<-rev(x1+0.5-0.02*sin60)
 >  x2<-x1+0.02*sin60
 >  segments(x1,y2,x2,y1)
 >  text(x2+0.03,y1+0.015,as.character(rev(seq(10,90,by=10))))
 >  text(0.5,0.9,"100% clay")
 >  par(xpd=TRUE)
 >  text(-0.1,0,"100% sand")
 >  text(1.1,0,"100% loam")
 >  text(0.07,0.43,"percent clay")
 >  text(0.93,0.43,"percent silt")
 >  text(0.5,-0.1,"percent sand")
 >  # boundary of clay with extensions
 >  x1<-c(0.275,0.35,0.6)
 >  x2<-c(0.4,0.79,0.7)
 >  y1<-c(0.55*sin60,0.41*sin60,0.41*sin60)
 >  y2<-c(0.285*sin60,0.41*sin60,0.6*sin60)
 >  segments(x1,y1,x2,y2)
 >  text(0.5,0.57,"Clay")
 >  # lower bound of clay loam & silty divider
 >  x1<-c(0.4,0.68)
 >  x2<-c(0.86,0.6)
 >  y1<-c(0.285*sin60,0.285*sin60)
 >  y2<-c(0.285*sin60,0.41*sin60)
 >  segments(x1,y1,x2,y2)
 >  text(0.7,0.49*sin60,"Silty")
 >  text(0.7,0.44*sin60,"clay")
 >  text(0.73,0.37*sin60,"Silty clay")
 >  text(0.73,0.33*sin60,"loam")
 >  text(0.5,0.35*sin60,"Clay loam")
 >  x1<-c(0.185,0.1,0.37)
 >  x2<-c(0.36,0.37,0.4)
 >  y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
 >  y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
 >  segments(x1,y1,x2,y2)
 >  text(0.27,0.43*sin60,"Sandy")
 >  text(0.27,0.39*sin60,"clay")
 >  text(0.27,0.3*sin60,"Sandy clay")
 >  text(0.27,0.26*sin60,"loam")
 >  # sand corner
 >  x1<-c(0.05,0.075)
 >  x2<-c(0.12,0.3)
 >  y1<-c(0.1*sin60,0.15*sin60)
 >  y2<-c(0,0)
 >  segments(x1,y1,x2,y2)
 >  text(0.25,0.13*sin60,"Sandy loam")
 >  text(0.13,0.075*sin60,"Loamy")
 >  text(0.15,0.035*sin60,"sand")
 >  text(0.055,0.021,"Sand")
 >  x1<-c(0.37,0.42,0.5,0.8,0.86)
 >  x2<-c(0.42,0.54,0.65,0.86,0.94)
 >  y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
 >  y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
 >  segments(x1,y1,x2,y2)
 >  text(0.49,0.18*sin60,"Loam")
 >  text(0.72,0.15*sin60,"Silt loam")
 >  text(0.9,0.06*sin60,"Silt")
 >  par(oldpar)
 > }



From slist at oomvanlieshout.net  Fri May 27 15:42:43 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 27 May 2005 15:42:43 +0200
Subject: [R] Soil texture triangle in R?
In-Reply-To: <4296D592.5040108@oomvanlieshout.net>
References: <4296D592.5040108@oomvanlieshout.net>
Message-ID: <429723D3.6040505@oomvanlieshout.net>

Hi Jari,

I assume this has been superseded by the ternaryplot{vcd} function!?

Thanks,

Sander.

Jari Oksanen wrote:
 > Sander,
 >
 > Just a quick note before I go to the field.
 >
 > I attach a tri.R file for drawing ternary plots. The base function was
 > posted to R News someday. One thing that I added was option to plot
 > nothing (type="n") plus (invisible) return of plotting coordinates. This
 > means that you can take the coordinates for drawing segments (in
 > original values), feed them through this functions, and you get
 > translated coordinates to use ordinary segments or lines commands to
 > overlay your lines into an existing ternary plot.
 >
 > We used this in an Applied Vegetation Science paper (Hellstr??m as the
 > first author) to overlay arrows onto ternary plots.
 >
 > cheers, jari oksanen
 >
 >
 > ------------------------------------------------------------------------
 >
 > "tri" <-
 >     function(a, f, m, symb = 2, grid = F, ...)
 > {
 >     ta <- paste(substitute(a))
 >     tf <- paste(substitute(f))
 >     tm <- paste(substitute(m))
 >
 >     tot <- 100/(a + f +m)
 >     b <- f * tot
 >     y <- b * .878
 >     x <- m * tot + b/2
 >     par(pty = "s")
 >     oldcol <- par("col")
 >     plot(x, y, axes = F, xlab = "", ylab = "", xlim = c(-10, 110), ylim
 >          = c(-10, 110), type = "n", ...)
 >     points(x,y,pch=symb)
 >     par(col = oldcol)
 >     trigrid(grid)
 >     text(-5, -5, ta)
 >     text(105, -5, tm)
 >     text(50, 93, tf)
 >     par(pty = "m")
 >     invisible(cbind(x,y))
 > }
 >



From ligges at statistik.uni-dortmund.de  Fri May 27 15:46:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 May 2005 15:46:01 +0200
Subject: [R] r: LEXICOGRAPHIC ORDERING
In-Reply-To: <42971F75.47B4677C@STATS.uct.ac.za>
References: <42971F75.47B4677C@STATS.uct.ac.za>
Message-ID: <42972499.9040704@statistik.uni-dortmund.de>

Clark Allan wrote:
> HI all
> 
> i have a seemingly simple question.
> 
> given a sequence of numbers say, 1,2,3,4,5.
> 
> i would like to get all of the possible two number arrangments
> (combinations), all 3 number arrangents ... 5 number arrangements
> (combinations).
> 
> i.e. 
> 
> in the 2 number case:
> 
> 12,13,14,15,23,24,25,34,35,45

   library(gtools)
   combinations(5, 2) %*% c(10, 1)

Uwe Ligges


> 
> 
> any ideas?
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri May 27 15:47:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 May 2005 15:47:42 +0200
Subject: [R] how to get this kind of binomial distribution simulation
	number?
In-Reply-To: <200505271326.j4RDQMB9029441@hypatia.math.ethz.ch>
References: <200505271326.j4RDQMB9029441@hypatia.math.ethz.ch>
Message-ID: <429724FE.1050902@statistik.uni-dortmund.de>

luan_sheng wrote:

> 
> hai, I want to perform a simulation like thisÅ£Å∫
> 
> Suppose that I have one population  ,it's size is 500, is composed of x,y
> and z.  The probability of x, y and z is respectively is 0.3, 0.5, 0.2. I
> wan to simulate a new same size population based ratio of x, y and z, how
> can I get and assess  the number of x, y and z. ?


PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

I guess you are looking for ?table, but your question is much too
unspecific....

Uwe Ligges



> luan
> 
> Key Laboratory for Sustainable Utilization of Marine Fisheries Resources,
> Ministry of Agriculture ,Yellow Sea Fisheries Research Institute, Chinese
> Academy of Fishery Sciences, Qingdao 266071,China
> 
> __________________________________________________
> 
> Å—Å≈ÅªÅ¢Å√Å‚Å∑Å—GÅ”Å ÅœÅ‰Å£Å≠Å÷Å–ÅπÅ˙ÅµÅ⁄Å“ÅªÅæÅ¯ÅŒÅﬁÅ¿Å¨ÅªÅ¯Å”Å ÅºÅ˛Å…ÅßÅ»Å≈Å≥Å¨Å¥ÅÛÅ”Å ÅœÅ‰
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.be  Fri May 27 15:47:06 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 27 May 2005 15:47:06 +0200
Subject: [R] r: LEXICOGRAPHIC ORDERING
References: <42971F75.47B4677C@STATS.uct.ac.za>
Message-ID: <008701c562c2$91e48fe0$0540210a@www.domain>

here is a possible solution:

subsets <- function (n, r, s = 1:n) {
    # from V&Rs (2000) S Programming, Springer, p.49
    if (mode(n) != "numeric" || length(n) != 1 || n < 1 || (n%%1) != 
0) stop("bad value of n")
    if (mode(r) != "numeric" || length(r) != 1 || r < 1 || (r%%1) != 
0) stop("bad value of r")
    if (!is.atomic(s) || length(s) < n) stop("s is either non-atomic 
or too short")
    fun <- function(n, r, s)
        if (r <= 0) vector(mode(s), 0)
        else if (r >= n) s[1:n]
        else rbind(cbind(s[1], Recall(n - 1, r - 1, s[-1])), 
Recall(n - 1, r, s[-1]))
    fun(n, r, s)
}
##############
x <- c(1, 3, 5, 7, 9)

ind <- subsets(length(x), 2)
apply(matrix(x[ind], nc = ncol(ind)), 1, function(x) 
as.numeric(paste(x, collapse = "")))

ind <- subsets(length(x), 3)
apply(matrix(x[ind], nc = ncol(ind)), 1, function(x) 
as.numeric(paste(x, collapse = "")))



I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Clark Allan" <Allan at stats.uct.ac.za>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, May 27, 2005 3:24 PM
Subject: [R] r: LEXICOGRAPHIC ORDERING


> HI all
>
> i have a seemingly simple question.
>
> given a sequence of numbers say, 1,2,3,4,5.
>
> i would like to get all of the possible two number arrangments
> (combinations), all 3 number arrangents ... 5 number arrangements
> (combinations).
>
> i.e.
>
> in the 2 number case:
>
> 12,13,14,15,23,24,25,34,35,45
>
>
>
> any ideas?


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From koen.pelleriaux at gmail.com  Fri May 27 15:48:38 2005
From: koen.pelleriaux at gmail.com (Koen Pelleriaux)
Date: Fri, 27 May 2005 15:48:38 +0200
Subject: [R] longitudinal survey data
In-Reply-To: <Pine.A41.4.61b.0505261305140.303294@homer12.u.washington.edu>
References: <1117131639.429613772e707@webmail.utoronto.ca>
	<Pine.A41.4.61b.0505261305140.303294@homer12.u.washington.edu>
Message-ID: <5a165e4f050527064829819768@mail.gmail.com>

On 5/26/05, Thomas Lumley <tlumley at u.washington.edu> wrote:

> If you *want* to fit mixed models (eg because you are interested in
> estimating variance components, or perhaps to gain efficiency) then it's
> quite a bit trickier. You can't just use the sampling weights in lme().
> You can correct for the biased sampling if you put the variables that
> affect the weights in as predictors in the model.  Cluster sampling could
> perhaps then be modelled as another level of random effect.


I've been struggeling with case weights (in the case of unequal
selection probabilities) in mixed effects models. Those are not
possible in lme(). Isn't it, however, possible to use case weights in
glmmPQL from MASS?

Koen Pelleriaux
Sociologist
University of Antwerp



From reid_huntsinger at merck.com  Fri May 27 16:00:23 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 27 May 2005 10:00:23 -0400
Subject: [R] how to get this kind of binomial distribution
	simulation number?
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A945F@uswpmx00.merck.com>

rmultinom(n=1,size=500,prob=c(0.3,0.5,0.2))

to get n samples each of size 500 just use the "n=" argument.

Reid Huntsinger


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of luan_sheng
Sent: Friday, May 27, 2005 9:26 AM
To: r-help at stat.math.ethz.ch; R-help at stat.math.ethz.ch
Subject: [R] how to get this kind of binomial distribution simulation
number?




hai, I want to perform a simulation like thisÅ£Å∫

Suppose that I have one population  ,it's size is 500, is composed of x,y
and z.  The probability of x, y and z is respectively is 0.3, 0.5, 0.2. I
wan to simulate a new same size population based ratio of x, y and z, how
can I get and assess  the number of x, y and z. ?

luan

Key Laboratory for Sustainable Utilization of Marine Fisheries Resources,
Ministry of Agriculture ,Yellow Sea Fisheries Research Institute, Chinese
Academy of Fishery Sciences, Qingdao 266071,China

__________________________________________________

Å—Å≈ÅªÅ¢Å√Å‚Å∑Å—GÅ”Å ÅœÅ‰Å£Å≠Å÷Å–ÅπÅ˙ÅµÅ⁄Å“ÅªÅæÅ¯ÅŒÅﬁÅ¿Å¨ÅªÅ¯Å”Å ÅºÅ˛Å…ÅßÅ»Å≈Å≥Å¨Å¥ÅÛÅ”Å ÅœÅ‰

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gsxej2 at cam.ac.uk  Fri May 27 16:05:56 2005
From: gsxej2 at cam.ac.uk (Gregory Jefferis)
Date: Fri, 27 May 2005 15:05:56 +0100
Subject: [R] 3D density estimation with library sm - no estimate returned
Message-ID: <BEBCE7D4.5817%gsxej2@cam.ac.uk>

Dear List,

I have been trying to use library sm to do density estimation on a 3D
dataset.  I am using the current MacOS X binary of sm from CRAN.  If I do
this on a 2D dataset, sm.density returns a list including the component
"estimate" which contains the density estimate over a uniform grid.  When
doing this with 3D data, although I get a nice plot (even when I don't ask
for one), the returned list only contains the original data (see below).  It
would appear that the internal function sm.density.3d is returning NULL.
Have I misunderstood what should be returned?  Is this a platform specific
problem?  Can someone suggest a fix or an alternative library for my
application?  Very many thanks for your help,

Greg Jefferis.

> R.version
         _         
platform powerpc-apple-darwin7.9.0
arch     powerpc   
os       darwin7.9.0
system   powerpc, darwin7.9.0
status   Patched   
major    2         
minor    1.0       
year     2005      
month    05        
day      12        
language R

> library(sm)
> str(sm.density(matrix(rnorm(300),ncol=3),display="none"))
List of 2
 $ data:List of 3
  ..$ x    : num [1:100, 1:3]  0.9470 -1.5112  1.0589 -0.0884 -0.1900 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:3] "" "" ""
  ..$ nbins: num 0
  ..$ freq : num [1:100] 1 1 1 1 1 1 1 1 1 1 ...
 $ call: language sm.density(x = matrix(rnorm(300), ncol = 3), display =
"none")
> str(sm.density(matrix(rnorm(200),ncol=2),display="none"))
List of 10
 $ eval.points: num [1:50, 1:2] -2.67 -2.57 -2.47 -2.38 -2.28 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : NULL
  .. ..$ : chr [1:2] "xnew" "ynew"
 $ estimate   : num [1:50, 1:50] 3.76e-05 5.10e-05 7.57e-05 1.22e-04
2.05e-04 ...
 $ h          : Named num [1:2] 0.414 0.512
  ..- attr(*, "names")= chr [1:2] "" ""
 $ h.weights  : num [1:100] 1 1 1 1 1 1 1 1 1 1 ...
 $ weights    : num [1:100] 1 1 1 1 1 1 1 1 1 1 ...
 $ se         : num [1:50, 1:50] 0.0306 0.0306 0.0306 0.0306 0.0306 ...
 $ upper      : num [1:50, 1:50] 0.00454 0.00468 0.00489 0.00523 0.00571 ...
 $ lower      : num [1:50, 1:50] 0 0 0 0 0 0 0 0 0 0 ...
 $ data       :List of 3
  ..$ x    : num [1:100, 1:2] -0.694 -0.192  0.149 -0.718 -0.357 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:2] "" ""
  ..$ nbins: num 0
  ..$ freq : num [1:100] 1 1 1 1 1 1 1 1 1 1 ...
 $ call       : language sm.density(x = matrix(rnorm(200), ncol = 2),
display = "none")
> 



-- 
Gregory Jefferis, PhD                               and:
Research Fellow
Department of Zoology                               St John's College
Downing Street                                      Cambridge
Cambridge, CB2 3EJ                                  CB2 1TP

Tel: +44 (0)1223 336683                             +44 (0)1223 339899
Fax: +44 (0)1223 336676                             +44 (0)1223 337720

gsxej2 at cam.ac.uk



From h.brunschwig at utoronto.ca  Fri May 27 16:06:07 2005
From: h.brunschwig at utoronto.ca (h.brunschwig@utoronto.ca)
Date: Fri, 27 May 2005 10:06:07 -0400
Subject: [R] longitudinal survey data
In-Reply-To: <Pine.A41.4.61b.0505261305140.303294@homer12.u.washington.edu>
References: <1117131639.429613772e707@webmail.utoronto.ca>
	<Pine.A41.4.61b.0505261305140.303294@homer12.u.washington.edu>
Message-ID: <1117202767.4297294fd6e37@webmail.utoronto.ca>


Thank you for your reply.

Does that mean that in order to take in account the repeated measures I denote
these as another cluster in R?

Dassy


Quoting Thomas Lumley <tlumley at u.washington.edu>:

> On Thu, 26 May 2005 h.brunschwig at utoronto.ca wrote:
> 
> >
> > Dear R-Users!
> >
> > Is there a possibility in R to do analyze longitudinal survey data
> (repeated
> > measures in a survey)? I know that for longitudinal data I can use lme()
> to
> > incorporate the correlation structure within individual and I know that
> there is
> > the package survey for analyzing survey data. How can I combine both? I
> am
> > trying to calculate design-based estimates. However, if I use svyglm() from
> the
> > survey package I would ignore the correlation structure of the repeated
> measures.
> >
> 
> You *can* fit regression models to these data with svyglm(). Remember that 
> from a design-based point of view there is no such thing as a correlation 
> structure of repeated measures -- only the sampling is random, not the 
> population data.
> 
> 
> If you *want* to fit mixed models (eg because you are interested in 
> estimating variance components, or perhaps to gain efficiency) then it's 
> quite a bit trickier. You can't just use the sampling weights in lme(). 
> You can correct for the biased sampling if you put the variables that 
> affect the weights in as predictors in the model.  Cluster sampling could 
> perhaps then be modelled as another level of random effect.
> 
> 
>  	-thomas
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
>



From esg at felix.unife.it  Fri May 27 16:09:41 2005
From: esg at felix.unife.it (Josef Eschgfaeller)
Date: Fri, 27 May 2005 16:09:41 +0200 (CEST)
Subject: [R] Chars as numbers
Message-ID: <Pine.LNX.4.63.0505271608510.20103@dns.unife.it>


Is there a proper function for transforming
a character to a number instead of using

   i=match('c',letters)
   # 3

Thanks.
Josef Eschgf??ller

-- 
Josef Eschgf??ller
Dipartimento Matematico
Universita' di Ferrara
http://felix.unife.it

From JAROSLAW.W.TUSZYNSKI at saic.com  Fri May 27 16:14:57 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 27 May 2005 10:14:57 -0400
Subject: [R] xmlAttrs and problems with reading node attributes of XML file
	(b ug?)
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F407F@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050527/7d3abf89/attachment.pl

From erithid at bellsouth.net  Fri May 27 16:17:30 2005
From: erithid at bellsouth.net (BJ)
Date: Fri, 27 May 2005 10:17:30 -0400
Subject: [R] plotting box plots on same x
Message-ID: <42972BFA.6030602@bellsouth.net>

I am trying to construct a graph of 6 box plots of blood pressures. I 
want them to be on a single set of axis and I want the SBP to be ontop 
of the DBP. I have an array bp with the data in it and I tried

a[1,]<-c(145,60,147,62,140,57)
a[2,]<-c(160,75,160,74,160,70)
a[3,]<-c(140,55,140,65,142,55)
 boxplot(data.frame(a), main = "Blood Pressures", at=c(1,1,2,2,3,3), 
names=c("sit","","lie","","stand",""))

which is close to what I want, but it gives me a bunch of empty space at 
the end. is there a better way to do this to avoid this?

As always, Thank You. ~Erithid



From efg at stowers-institute.org  Fri May 27 16:13:16 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Fri, 27 May 2005 09:13:16 -0500
Subject: [R] Round a line
References: <s2971f09.052@ffdata.setur.fo>
Message-ID: <d779ps$69n$1@sea.gmane.org>

"Luis Ridao Cruz" <Luisr at frs.fo> wrote in message
news:s2971f09.052 at ffdata.setur.fo...
> R-help,
>
> I have lloked in the archives found no answer to how to round the line
> joint.
>
> I have usedthe arguments lnd, ljoin in par but I get no differences in
> the plotting.
>
> x=1:10
> par(ljoin="round",lend="round")
> plot(x,sin(x),type="l",lwd=2)

On my Windows 2000 machine using R 2.1.0, par()$ljoin and par()$lend are
already "round" by default.  The par()$lmitre parameter is 10,

Paul Murrell's article "Fonts, lines, and transparency ..." in R News 4/2
(Sept 2004) gives some clues under "The end of the line":
http://cran.stat.auckland.ac.nz/doc/Rnews/Rnews_2004-2.pdf



"All lines are drawn using a particular style for line ends and joins,
though the difference only becomes obvious when lines become thick."



Is it possible that with par()$lmitre at 10, and a lwd=2, you won't see any
difference?



efg



From erithid at bellsouth.net  Fri May 27 16:27:36 2005
From: erithid at bellsouth.net (BJ)
Date: Fri, 27 May 2005 10:27:36 -0400
Subject: [R] box plots without whiskers
Message-ID: <42972E58.9010600@bellsouth.net>

I searched the archives, but couldnt find any way to do this with 
boxplot. Is there a way? Thanks again ~BJ



From t.muhlhofer at lse.ac.uk  Fri May 27 16:28:15 2005
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Fri, 27 May 2005 15:28:15 +0100
Subject: [R] Chars as numbers
In-Reply-To: <Pine.LNX.4.63.0505271608510.20103@dns.unife.it>
References: <Pine.LNX.4.63.0505271608510.20103@dns.unife.it>
Message-ID: <42972E7F.7040708@lse.ac.uk>

Josef,

Not sure if this is exactly what you mean, but there is a generic function

as()

to which you can then specify "numeric" as an argument and which then 
coerces stuff into numeric format.

Tobias


Josef Eschgfaeller wrote:
> 
> Is there a proper function for transforming
> a character to a number instead of using
> 
>   i=match('c',letters)
>   # 3
> 
> Thanks.
> Josef Eschgf??ller
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
**************************************************************************
When Thomas Edison invented the light bulb he tried over 2000
experiments before he got it to work. A young reporter asked
him how it felt to have failed so many times. He said
"I never failed once. I invented the light bulb.
It just happened to be a 2000-step process."



From vincent at 7d4.com  Fri May 27 16:39:38 2005
From: vincent at 7d4.com (vincent)
Date: Fri, 27 May 2005 16:39:38 +0200
Subject: [R] Chars as numbers
In-Reply-To: <Pine.LNX.4.63.0505271608510.20103@dns.unife.it>
References: <Pine.LNX.4.63.0505271608510.20103@dns.unife.it>
Message-ID: <4297312A.6080902@7d4.com>

as.numeric()
hih



From ligges at statistik.uni-dortmund.de  Fri May 27 16:42:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 May 2005 16:42:50 +0200
Subject: [R] Chars as numbers
In-Reply-To: <Pine.LNX.4.63.0505271608510.20103@dns.unife.it>
References: <Pine.LNX.4.63.0505271608510.20103@dns.unife.it>
Message-ID: <429731EA.8070601@statistik.uni-dortmund.de>

Josef Eschgfaeller wrote:
> 
> Is there a proper function for transforming
> a character to a number instead of using
> 
>   i=match('c',letters)
>   # 3

I'd suggest to use the above if you really mean it. Note that 
"transforming a character to a number" is not well defined, because you 
have to think about encodings and characters at first. See the article 
by Brian Ripley in the most recent issue of R News, for example.

Uwe Ligges


> Thanks.
> Josef Eschgf??ller
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri May 27 16:45:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 May 2005 16:45:27 +0200
Subject: [R] box plots without whiskers
In-Reply-To: <42972E58.9010600@bellsouth.net>
References: <42972E58.9010600@bellsouth.net>
Message-ID: <42973287.8010005@statistik.uni-dortmund.de>

BJ wrote:
> I searched the archives, but couldnt find any way to do this with 
> boxplot. Is there a way? Thanks again ~BJ

See ?boxplot and its argument par which points you to ?bxp:

Now, you can do stuff like
   boxplot(1:10, pars=list(staplelty=0, whisklty=0))

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri May 27 16:52:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 May 2005 16:52:42 +0200
Subject: [R] plotting box plots on same x
In-Reply-To: <42972BFA.6030602@bellsouth.net>
References: <42972BFA.6030602@bellsouth.net>
Message-ID: <4297343A.1050601@statistik.uni-dortmund.de>

BJ wrote:
> I am trying to construct a graph of 6 box plots of blood pressures. I 
> want them to be on a single set of axis and I want the SBP to be ontop 
> of the DBP. I have an array bp with the data in it and I tried


Folks, please invest 1 minute of time to rethink whether other people 
will understand your question!

What is "SBP", "DBP", and where are the "6" boxplots from?

> a[1,]<-c(145,60,147,62,140,57)

"a" must already be defined here, or we cannot replace a column!

> a[2,]<-c(160,75,160,74,160,70)
> a[3,]<-c(140,55,140,65,142,55)
> boxplot(data.frame(a), main = "Blood Pressures", at=c(1,1,2,2,3,3), 
> names=c("sit","","lie","","stand",""))
> 
> which is close to what I want, but it gives me a bunch of empty space at 
> the end. is there a better way to do this to avoid this?

Well, the first point is that you should write questions that you would 
understand yourself. In a next step you might find someone who is able 
to answer it ....

> As always, Thank You. ~Erithid

"As always"? Does not sound very enthusiastic ...


Uwe Ligges



> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From culaterz2003 at yahoo.com  Fri May 27 16:55:50 2005
From: culaterz2003 at yahoo.com (manav ram)
Date: Fri, 27 May 2005 07:55:50 -0700 (PDT)
Subject: [R] Using R for classifying new samples
Message-ID: <20050527145550.81610.qmail@web52008.mail.yahoo.com>

Hello,
I do not have any statistical background, So I shall
apologise if I am asking trivial question or help.

I am trying to work with R.
The problem I have on hand is:
I have 2 sets of data(means & SD for each sample in
the group of both sets). The sample size is
massive(2000+ in each grp).
I have a new set of experimental data and I like to
classify this to either of the grps based on
statistical evidence.
I thought using Linear discriminant analysis of R(MASS
package) I can solve this problem.
But unfortunately I am not quite sure its the right
way and further with given my background I am not even
sure how to prepare the data to test.

To explain my problem further, I have prepared a small
set of data to be tested....so that it might be easy
to suggest the right way to go ahead.The data is in
the
form""Object>Query>mean_grp1>SD_grp1>mean_grp2>SD_grp2""

A>3.890>3.315>1.105>1.395>0.345
B>0.915>1.193>0.334>0.638>0.034
C>2.059>2.155>0.614>1.042>0.159
D>1.372>0.901>0.314>0.384>0.174

What I would like to test is if the "query" belongs to
grp 1 or 2(for all my samples,2000+)and if it can read
out to me teh query belongs to grp_1 or grp_2.
Is there some good examples that I can refer to which
can teach me step wise right from preparing the data
to testing???or would there be someone who can help me
with this problem?????

Thanks a lot for your time...look forward to hear some
help and suggestions

Manav



From erithid at bellsouth.net  Fri May 27 16:54:37 2005
From: erithid at bellsouth.net (BJ)
Date: Fri, 27 May 2005 10:54:37 -0400
Subject: [R] box plots without whiskers
In-Reply-To: <42973287.8010005@statistik.uni-dortmund.de>
References: <42972E58.9010600@bellsouth.net>
	<42973287.8010005@statistik.uni-dortmund.de>
Message-ID: <429734AD.2070609@bellsouth.net>

Hmm ok, that seems to work with that example, but when I add an "at=" 
option i get graphs that are only verticle lines. Sorry to be difficult. ~BJ

Uwe Ligges wrote:

> BJ wrote:
>
>> I searched the archives, but couldnt find any way to do this with 
>> boxplot. Is there a way? Thanks again ~BJ
>
>
> See ?boxplot and its argument par which points you to ?bxp:
>
> Now, you can do stuff like
>   boxplot(1:10, pars=list(staplelty=0, whisklty=0))
>
> Uwe Ligges
>
>
>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
>



From ligges at statistik.uni-dortmund.de  Fri May 27 17:04:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 May 2005 17:04:39 +0200
Subject: [R] box plots without whiskers
In-Reply-To: <429734AD.2070609@bellsouth.net>
References: <42972E58.9010600@bellsouth.net>
	<42973287.8010005@statistik.uni-dortmund.de>
	<429734AD.2070609@bellsouth.net>
Message-ID: <42973707.3030409@statistik.uni-dortmund.de>

BJ wrote:
> Hmm ok, that seems to work with that example, but when I add an "at=" 
> option i get graphs that are only verticle lines. Sorry to be difficult. 


He? So you need to pe much more precisely in this message as well!

boxplot(data.frame(1:10, 2:11), at = 1:2,
   pars=list(staplelty=0, whisklty=0), at=1:2)

works perfectly for me! People are not that happy to invest their time 
for helping you on bad specified problems...

*REALLY*:
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

Uwe Ligges



> ~BJ
> 
> Uwe Ligges wrote:
> 
>> BJ wrote:
>>
>>> I searched the archives, but couldnt find any way to do this with 
>>> boxplot. Is there a way? Thanks again ~BJ
>>
>>
>>
>> See ?boxplot and its argument par which points you to ?bxp:
>>
>> Now, you can do stuff like
>>   boxplot(1:10, pars=list(staplelty=0, whisklty=0))
>>
>> Uwe Ligges
>>
>>
>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>>



From erithid at bellsouth.net  Fri May 27 17:01:39 2005
From: erithid at bellsouth.net (BJ)
Date: Fri, 27 May 2005 11:01:39 -0400
Subject: [R] plotting box plots on same x
In-Reply-To: <4297343A.1050601@statistik.uni-dortmund.de>
References: <42972BFA.6030602@bellsouth.net>
	<4297343A.1050601@statistik.uni-dortmund.de>
Message-ID: <42973653.1000100@bellsouth.net>

Does it matter what they are? they are just names. The six box plots are 
from the array I created with columns. I forgot to add teh dim 
statement, my mistake. But I thought it was obvious that i was using a 
matrix from the data frame call and the assignments I provided. My 
question was simply about setting the x axis so that it stopped after 
x=3, even though I had 6 plots.  For teh record, before I get jumped on 
for the statistics, I am just using a 3 x 6 matrix to test the code 
before applying it to actual data. Also, I was not being scarcastic. I 
have recieved a lot of help from this mailing list. The R documentation 
is hard to hunt down and not complete. Without the help of actual people 
I would be dead in the water. I am sorry for any hostility that I have 
incurred. ~Erithid

Uwe Ligges wrote:

> BJ wrote:
>
>> I am trying to construct a graph of 6 box plots of blood pressures. I 
>> want them to be on a single set of axis and I want the SBP to be 
>> ontop of the DBP. I have an array bp with the data in it and I tried
>
>
>
> Folks, please invest 1 minute of time to rethink whether other people 
> will understand your question!
>
> What is "SBP", "DBP", and where are the "6" boxplots from?
>
>> a[1,]<-c(145,60,147,62,140,57)
>
>
> "a" must already be defined here, or we cannot replace a column!
>
>> a[2,]<-c(160,75,160,74,160,70)
>> a[3,]<-c(140,55,140,65,142,55)
>> boxplot(data.frame(a), main = "Blood Pressures", at=c(1,1,2,2,3,3), 
>> names=c("sit","","lie","","stand",""))
>>
>> which is close to what I want, but it gives me a bunch of empty space 
>> at the end. is there a better way to do this to avoid this?
>
>
> Well, the first point is that you should write questions that you 
> would understand yourself. In a next step you might find someone who 
> is able to answer it ....
>
>> As always, Thank You. ~Erithid
>
>
> "As always"? Does not sound very enthusiastic ...
>
>
> Uwe Ligges
>
>
>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
>



From esg at felix.unife.it  Fri May 27 17:05:13 2005
From: esg at felix.unife.it (Josef Eschgfaeller)
Date: Fri, 27 May 2005 17:05:13 +0200 (CEST)
Subject: [R] Chars as numbers
Message-ID: <Pine.LNX.4.63.0505271703550.24183@dns.unife.it>


> as.numeric

Some weeks ago one discussed on the list about how
to transform a hexadecimal representation to
decimal digits. Specifically one has to transform
'a' to 10 etc. In C one does this with 'a'-87,
but I did not find a function for this in R.
Using match on letterdigits as in the following
is of course a little slow when repeated often.
---------------------------------------
Hex = function (rep16)
{rep16=tolower(rep16)
u=strsplit(rep16,'',perl=T)[[1]]
letterdigits=c(0:9,letters)
v=sapply(u,function (x)
   match(x,letterdigits)-1)
v=as.numeric(v)
Horner(v,16)}

Horner = function (v,alfa=2)
{b=v[1]; m=length(v)
if (m>1) for (i in 2:m)
b=b*alfa+v[i]; b}

# Example:
for (x in c('0','a0','10e','f0ae'))
print(Hex(x))
# 0
# 160
# 270
# 61614
---------------------------------------
Josef Eschgf??ller

-- 
Josef Eschgf??ller
Dipartimento Matematico
Universita' di Ferrara
http://felix.unife.it

From MSchwartz at mn.rr.com  Fri May 27 17:10:58 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 27 May 2005 10:10:58 -0500
Subject: [R] plotting box plots on same x
In-Reply-To: <42972BFA.6030602@bellsouth.net>
References: <42972BFA.6030602@bellsouth.net>
Message-ID: <1117206658.3527.45.camel@horizons.localdomain>

On Fri, 2005-05-27 at 10:17 -0400, BJ wrote:
> I am trying to construct a graph of 6 box plots of blood pressures. I 
> want them to be on a single set of axis and I want the SBP to be ontop 
> of the DBP. I have an array bp with the data in it and I tried
> 
> a[1,]<-c(145,60,147,62,140,57)
> a[2,]<-c(160,75,160,74,160,70)
> a[3,]<-c(140,55,140,65,142,55)
>  boxplot(data.frame(a), main = "Blood Pressures", at=c(1,1,2,2,3,3), 
> names=c("sit","","lie","","stand",""))
> 
> which is close to what I want, but it gives me a bunch of empty space at 
> the end. is there a better way to do this to avoid this?
> 
> As always, Thank You. ~Erithid

To answer both of your posts, use the following:

# Review how your data is structured above. Your code for creating "a" 
# is not replicable. For those lacking clinical insight, explaining
# your acronyms would also be helpful...  :-)

SBP <- matrix(c(145, 160, 140, 
                147, 160, 140,
                140, 160, 142), ncol = 3)
 
DBP <- matrix(c(60, 75, 55, 
                62, 74, 65,
                57, 70, 55), ncol = 3)

colnames(SBP) <- colnames(DBP) <- c("sit","lie","stand")

# The key here is to only plot three at a time, lest boxplot() 
# default to a 'xlim' of 0.5 to 6.5 (1:# of groups +/- 0.5)
# Then use 'add = TRUE' to plot the second group of 3
# Note also that I set the 'ylim' to the range of the combined
# values in the first plot.

boxplot(data.frame(SBP), main = "Blood Pressures", 
        ylim = range(c(SBP, DBP)),
        whisklty = 0, staplelty = 0)

boxplot(data.frame(DBP), add = TRUE, whisklty = 0, staplelty = 0)

Note the final two arguments, which result in the whiskers being drawn
with an "invisible" line.

See ?boxplot and ?bxp for more information.

HTH,

Marc Schwartz



From br44114 at gmail.com  Fri May 27 17:18:32 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 27 May 2005 11:18:32 -0400
Subject: [R] Using R for classifying new samples
Message-ID: <8d5a363505052708183ec8d9aa@mail.gmail.com>

Read this book, "Multivariate Statistical Analysis: A Conceptual
Introduction" by Sam Kash Kachigan. I think it's *great*, and perfect
for someone without any statistical background.


-----Original Message-----
From: manav ram [mailto:culaterz2003 at yahoo.com]
Sent: Friday, May 27, 2005 10:56 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Using R for classifying new samples


Hello,
I do not have any statistical background, So I shall
apologise if I am asking trivial question or help.

I am trying to work with R.
The problem I have on hand is:
I have 2 sets of data(means & SD for each sample in
the group of both sets). The sample size is
massive(2000+ in each grp).
I have a new set of experimental data and I like to
classify this to either of the grps based on
statistical evidence.
I thought using Linear discriminant analysis of R(MASS
package) I can solve this problem.
But unfortunately I am not quite sure its the right
way and further with given my background I am not even
sure how to prepare the data to test.

To explain my problem further, I have prepared a small
set of data to be tested....so that it might be easy
to suggest the right way to go ahead.The data is in
the
form""Object>Query>mean_grp1>SD_grp1>mean_grp2>SD_grp2""

A>3.890>3.315>1.105>1.395>0.345
B>0.915>1.193>0.334>0.638>0.034
C>2.059>2.155>0.614>1.042>0.159
D>1.372>0.901>0.314>0.384>0.174

What I would like to test is if the "query" belongs to
grp 1 or 2(for all my samples,2000+)and if it can read
out to me teh query belongs to grp_1 or grp_2.
Is there some good examples that I can refer to which
can teach me step wise right from preparing the data
to testing???or would there be someone who can help me
with this problem?????

Thanks a lot for your time...look forward to hear some
help and suggestions

Manav

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From erithid at bellsouth.net  Fri May 27 17:30:03 2005
From: erithid at bellsouth.net (BJ)
Date: Fri, 27 May 2005 11:30:03 -0400
Subject: [R] box plots without whiskers]
Message-ID: <42973CFB.2020909@bellsouth.net>


-------------- next part --------------
An embedded message was scrubbed...
From: BJ <erithid at bellsouth.net>
Subject: Re: [R] box plots without whiskers
Date: Fri, 27 May 2005 11:29:38 -0400
Size: 3099
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050527/d9a20fe2/Rboxplotswithoutwhiskers.mht

From lockwood at rand.org  Fri May 27 17:29:56 2005
From: lockwood at rand.org (J.R. Lockwood)
Date: Fri, 27 May 2005 11:29:56 -0400 (EDT)
Subject: [R] plotting box plots on same x
In-Reply-To: <42973653.1000100@bellsouth.net>
References: <42972BFA.6030602@bellsouth.net><4297343A.1050601@statistik.uni-dortmund.de>
	<42973653.1000100@bellsouth.net>
Message-ID: <Pine.LNX.4.58.0505271125540.17851@penguin.rand.org>

BJ-

For the record as well I could not make heads or tails of your question.

The dedicated folks who day in, day out field poorly specified questions have
no obligation to do so, and have every right to get frustrated when it seems
that their time is being taken advantage of.

On Fri, 27 May 2005, BJ wrote:

> Date: Fri, 27 May 2005 11:01:39 -0400
> From: BJ <erithid at bellsouth.net>
> To: Uwe Ligges <ligges at statistik.uni-dortmund.de>
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] plotting box plots on same x
> 
> Does it matter what they are? they are just names. The six box plots are 
> from the array I created with columns. I forgot to add teh dim 
> statement, my mistake. But I thought it was obvious that i was using a 
> matrix from the data frame call and the assignments I provided. My 
> question was simply about setting the x axis so that it stopped after 
> x=3, even though I had 6 plots.  For teh record, before I get jumped on 
> for the statistics, I am just using a 3 x 6 matrix to test the code 
> before applying it to actual data. Also, I was not being scarcastic. I 
> have recieved a lot of help from this mailing list. The R documentation 
> is hard to hunt down and not complete. Without the help of actual people 
> I would be dead in the water. I am sorry for any hostility that I have 
> incurred. ~Erithid
> 
> Uwe Ligges wrote:
> 
> > BJ wrote:
> >
> >> I am trying to construct a graph of 6 box plots of blood pressures. I 
> >> want them to be on a single set of axis and I want the SBP to be 
> >> ontop of the DBP. I have an array bp with the data in it and I tried
> >
> >
> >
> > Folks, please invest 1 minute of time to rethink whether other people 
> > will understand your question!
> >
> > What is "SBP", "DBP", and where are the "6" boxplots from?
> >
> >> a[1,]<-c(145,60,147,62,140,57)
> >
> >
> > "a" must already be defined here, or we cannot replace a column!
> >
> >> a[2,]<-c(160,75,160,74,160,70)
> >> a[3,]<-c(140,55,140,65,142,55)
> >> boxplot(data.frame(a), main = "Blood Pressures", at=c(1,1,2,2,3,3), 
> >> names=c("sit","","lie","","stand",""))
> >>
> >> which is close to what I want, but it gives me a bunch of empty space 
> >> at the end. is there a better way to do this to avoid this?
> >
> >
> > Well, the first point is that you should write questions that you 
> > would understand yourself. In a next step you might find someone who 
> > is able to answer it ....
> >
> >> As always, Thank You. ~Erithid
> >
> >
> > "As always"? Does not sound very enthusiastic ...
> >
> >
> > Uwe Ligges
> >
> >
> >
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/statistics/bios/

--------------------

This email message is for the sole use of the intended recip...{{dropped}}



From erithid at bellsouth.net  Fri May 27 17:39:15 2005
From: erithid at bellsouth.net (BJ)
Date: Fri, 27 May 2005 11:39:15 -0400
Subject: [R] I never made any assumption athat anyone had any obligation to
 do anything
Message-ID: <42973F23.7020805@bellsouth.net>

I just asked a question. If I was too vague, then i am sorry. I dont 
expect anyone to help me, but I thought that it was ok to put the 
question out there in case someone wanted to help me. I didnt expect 
abject hostility for it. Human decency was the only thing I did expect. 
If my question was a pain,badly formatted, or too juvinile, then i have 
no problem being ignored.I asked a question that the archives did not 
satisfactorily answer. I appreciate the help of the poeple on this list 
as they are an invaluable resource, especially since the R documentation 
is sketchy at times. I am sorry for wasting everyones time. I just dont 
like being yelled at first thing in the morning for asking for help on a 
help list. See you all around  ~Erithid



From tlumley at u.washington.edu  Fri May 27 18:31:35 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 27 May 2005 09:31:35 -0700 (PDT)
Subject: [R] longitudinal survey data
In-Reply-To: <1117202767.4297294fd6e37@webmail.utoronto.ca>
References: <1117131639.429613772e707@webmail.utoronto.ca>
	<Pine.A41.4.61b.0505261305140.303294@homer12.u.washington.edu>
	<1117202767.4297294fd6e37@webmail.utoronto.ca>
Message-ID: <Pine.A41.4.61b.0505270930200.112250@homer10.u.washington.edu>

On Fri, 27 May 2005 h.brunschwig at utoronto.ca wrote:

>
> Thank you for your reply.
>
> Does that mean that in order to take in account the repeated measures I denote
> these as another cluster in R?
>

Yes, but unless you have multistage finite population corrections to put 
in the design object only the first stage of clustering affects the 
results, so you may not need to bother.

 	-thomas



From slist at oomvanlieshout.net  Fri May 27 18:33:29 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 27 May 2005 18:33:29 +0200
Subject: [R] Soil texture triangle in R?
In-Reply-To: <42972186.50404@oomvanlieshout.net>
References: <4296D592.5040108@oomvanlieshout.net>
	<42972186.50404@oomvanlieshout.net>
Message-ID: <42974BD9.6080609@oomvanlieshout.net>

Right,

Got the data points plotted on top of the soil texture background, 
thanks to Jim and ternaryplot{vcd}! See code below.

Now there is some fine tuning to do, as it should really look like this 
graph:
http://soil.scijournals.org/content/vol65/issue4/images/large/1038f2.jpeg

Things to do:
- rotate axis labels;
- correct small errors in class divisions;
- correct the partial covering of the bottom tick labels;
- rotate ticks in order to simplify viewing the graph.

Any help still appreciated!

Cheers,

Sander.


soil.triangle <- function() {
   oldpar <- par(no.readonly=TRUE)
   ## now the bottom internal ticks
   x1<-seq(0.1,0.9,by=0.1)
   x2<-x1
   y1<-rep(0,9)
   y2<-rep(-0.02,9)
   segments(x1,y1,x2,y2)
   text(x1,y1-0.03,as.character(rev(seq(10,90,by=10)))) #, cex=0.8)
   ## now the left internal ticks
   y1<-x1*sin60
   x1<-x1*0.5
   x2<-x1+0.02*sin60
   y2<-y1-0.02*0.5
   segments(x1,y1,x2,y2)
   text(x1-0.03,y1+0.015,as.character(seq(10,90,by=10)))
   ## now the right internal ticks
   x1<-rev(x1+0.5-0.02*sin60)
   x2<-x1+0.02*sin60
   segments(x1,y2,x2,y1)
   text(x2+0.03,y1+0.015,as.character(rev(seq(10,90,by=10))))
   ## the labels at the corners
   par(xpd=TRUE)
#   text(0.5,0.9,"100% clay")
#   text(-0.1,0,"100% sand")
#   text(1.1,0,"100% loam")
   text(0.09,0.43,"% Clay")
   text(0.90,0.43,"% Silt")
   text(0.5,-0.1,"% Sand")
   # boundary of clay with extensions
   x1<-c(0.275,0.35,0.6)
   x2<-c(0.4,0.79,0.7)
   y1<-c(0.55*sin60,0.41*sin60,0.41*sin60)
   y2<-c(0.285*sin60,0.41*sin60,0.6*sin60)
   segments(x1,y1,x2,y2, col="grey")
   text(0.5,0.57,"Clay", col="grey")
   # lower bound of clay loam & silty divider
   x1<-c(0.4,0.68)
   x2<-c(0.86,0.6)
   y1<-c(0.285*sin60,0.285*sin60)
   y2<-c(0.285*sin60,0.41*sin60)
   segments(x1,y1,x2,y2, col="grey")
   text(0.7,0.49*sin60,"Silty", col="grey")
   text(0.7,0.44*sin60,"clay", col="grey")
   text(0.73,0.37*sin60,"Silty clay", col="grey")
   text(0.73,0.33*sin60,"loam", col="grey")
   text(0.5,0.35*sin60,"Clay loam", col="grey")
   x1<-c(0.185,0.1,0.37)
   x2<-c(0.36,0.37,0.4)
   y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
   y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
   segments(x1,y1,x2,y2, col="grey")
   text(0.27,0.43*sin60,"Sandy", col="grey")
   text(0.27,0.39*sin60,"clay", col="grey")
   text(0.27,0.3*sin60,"Sandy clay", col="grey")
   text(0.27,0.26*sin60,"loam", col="grey")
   # sand corner
   x1<-c(0.05,0.075)
   x2<-c(0.12,0.3)
   y1<-c(0.1*sin60,0.15*sin60)
   y2<-c(0,0)
   segments(x1,y1,x2,y2, col="grey")
   text(0.25,0.13*sin60,"Sandy loam", col="grey")
   text(0.13,0.075*sin60,"Loamy", col="grey")
   text(0.15,0.035*sin60,"sand", col="grey")
   text(0.055,0.021,"Sand", col="grey")
   x1<-c(0.37,0.42,0.5,0.8,0.86)
   x2<-c(0.42,0.54,0.65,0.86,0.94)
   y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
   y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
   segments(x1,y1,x2,y2, col="grey")
   text(0.49,0.18*sin60,"Loam", col="grey")
   text(0.72,0.15*sin60,"Silt loam", col="grey")
   text(0.9,0.06*sin60,"Silt", col="grey")
   par(oldpar)
}

tmp <- array(dim=c(10,3))
tmp[,1] <- abs(rnorm(10)*20)
tmp[,2] <- abs(rnorm(10)*10)
tmp[,3] <- 100-tmp[,1]-tmp[,2]
tmp

library(vcd)
## Mark groups
ternaryplot(tmp,
   grid=FALSE,
   dimnames.position = "none",
   pch=1, col="black",
   scale=1, main=NULL,
   prop.size=FALSE,
   )
soil.triangle()


Sander Oom wrote:
> Hi Jim,
> 
> This looks impressive! It gives me the 'background' graph. However, I'm 
> not sure how I can use this function to plot my soil texture values! Can 
> you explain?
> 
> I would like to be able to plot my soil texture samples in the same 
> graph as the one your function plots.
> 
> Maybe I should try to figure out how to replicate your code inside a 
> ternaryplot{vcd} call.
> 
> Cheers,
> 
> Sander.
> 
> Jim Lemon wrote:
>  > Sander Oom wrote:
>  >> Dear R users,
>  >>
>  >> has anybody made an attempt to create the soil texture triangle graph
>  >> in R? For an example see here:
>  >>
>  >> http://www.teachingkate.org/images/soiltria.gif
>  >>
>  >> I would like to get the lines in black and texture labels in gray to
>  >> allow for plotting my texture results on top.
>  >>
>  >> Any examples or suggestions are very welcome!
>  >>
>  > It's not too hard to write a plot function to do this, but I'm not sure
>  > whether this will be what you want. Anyway, try it out.
>  >
>  > Jim
>  >
>  > ------------------------------------------------------------------------
>  >
>  > soil.triangle<-function() {
>  >  oldpar<-par(no.readonly=TRUE)
>  >  plot(0:1,type="n",axes=FALSE,xlim=c(0,1.1),ylim=c(0,1),
>  >   main="Soil Triangle",xlab="",ylab="")
>  >  # first draw the triangle
>  >  x1<-c(0,0,0.5)
>  >  sin60<-sin(pi/3)
>  >  x2<-c(1,0.5,1)
>  >  y1<-c(0,0,sin60)
>  >  y2<-c(0,sin60,0)
>  >  segments(x1,y1,x2,y2)
>  >  # now the bottom internal ticks
>  >  x1<-seq(0.1,0.9,by=0.1)
>  >  x2<-x1
>  >  y1<-rep(0,9)
>  >  y2<-rep(0.02,9)
>  >  segments(x1,y1,x2,y2)
>  >  text(x1,y1-0.03,as.character(rev(seq(10,90,by=10))))
>  >  # now the left internal ticks
>  >  y1<-x1*sin60
>  >  x1<-x1*0.5
>  >  x2<-x1+0.02*sin60
>  >  y2<-y1-0.02*0.5
>  >  segments(x1,y1,x2,y2)
>  >  text(x1-0.03,y1+0.015,as.character(seq(10,90,by=10)))
>  >  x1<-rev(x1+0.5-0.02*sin60)
>  >  x2<-x1+0.02*sin60
>  >  segments(x1,y2,x2,y1)
>  >  text(x2+0.03,y1+0.015,as.character(rev(seq(10,90,by=10))))
>  >  text(0.5,0.9,"100% clay")
>  >  par(xpd=TRUE)
>  >  text(-0.1,0,"100% sand")
>  >  text(1.1,0,"100% loam")
>  >  text(0.07,0.43,"percent clay")
>  >  text(0.93,0.43,"percent silt")
>  >  text(0.5,-0.1,"percent sand")
>  >  # boundary of clay with extensions
>  >  x1<-c(0.275,0.35,0.6)
>  >  x2<-c(0.4,0.79,0.7)
>  >  y1<-c(0.55*sin60,0.41*sin60,0.41*sin60)
>  >  y2<-c(0.285*sin60,0.41*sin60,0.6*sin60)
>  >  segments(x1,y1,x2,y2)
>  >  text(0.5,0.57,"Clay")
>  >  # lower bound of clay loam & silty divider
>  >  x1<-c(0.4,0.68)
>  >  x2<-c(0.86,0.6)
>  >  y1<-c(0.285*sin60,0.285*sin60)
>  >  y2<-c(0.285*sin60,0.41*sin60)
>  >  segments(x1,y1,x2,y2)
>  >  text(0.7,0.49*sin60,"Silty")
>  >  text(0.7,0.44*sin60,"clay")
>  >  text(0.73,0.37*sin60,"Silty clay")
>  >  text(0.73,0.33*sin60,"loam")
>  >  text(0.5,0.35*sin60,"Clay loam")
>  >  x1<-c(0.185,0.1,0.37)
>  >  x2<-c(0.36,0.37,0.4)
>  >  y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
>  >  y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
>  >  segments(x1,y1,x2,y2)
>  >  text(0.27,0.43*sin60,"Sandy")
>  >  text(0.27,0.39*sin60,"clay")
>  >  text(0.27,0.3*sin60,"Sandy clay")
>  >  text(0.27,0.26*sin60,"loam")
>  >  # sand corner
>  >  x1<-c(0.05,0.075)
>  >  x2<-c(0.12,0.3)
>  >  y1<-c(0.1*sin60,0.15*sin60)
>  >  y2<-c(0,0)
>  >  segments(x1,y1,x2,y2)
>  >  text(0.25,0.13*sin60,"Sandy loam")
>  >  text(0.13,0.075*sin60,"Loamy")
>  >  text(0.15,0.035*sin60,"sand")
>  >  text(0.055,0.021,"Sand")
>  >  x1<-c(0.37,0.42,0.5,0.8,0.86)
>  >  x2<-c(0.42,0.54,0.65,0.86,0.94)
>  >  y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
>  >  y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
>  >  segments(x1,y1,x2,y2)
>  >  text(0.49,0.18*sin60,"Loam")
>  >  text(0.72,0.15*sin60,"Silt loam")
>  >  text(0.9,0.06*sin60,"Silt")
>  >  par(oldpar)
>  > }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From pohl at marge.statistik.uni-koeln.de  Fri May 27 19:57:22 2005
From: pohl at marge.statistik.uni-koeln.de (Stefan Pohl)
Date: Fri, 27 May 2005 19:57:22 +0200
Subject: [R] nlminb to optmin
Message-ID: <036201c562e5$8b88d340$1b9e5f86@simurechner>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050527/bed8bd9a/attachment.pl

From rmh at temple.edu  Fri May 27 20:18:24 2005
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 27 May 2005 14:18:24 -0400
Subject: [ESS] Re: [R] R commandline editor question
Message-ID: <8f04ef78.3233353.81a0300@po-d.temple.edu>

If you are running R in the *R* buffer inside emacs, then you automatically
have highlighting of mismatched parentheses and brackets.

Rich



From blindglobe at gmail.com  Fri May 27 20:21:00 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Fri, 27 May 2005 20:21:00 +0200
Subject: [R] R commandline editor question
In-Reply-To: <e737f5395d5c4d3b583e859380c15d08@soc.soton.ac.uk>
References: <20050527111131.GL17261@lubyanka.local>
	<e737f5395d5c4d3b583e859380c15d08@soc.soton.ac.uk>
Message-ID: <1abe3fa90505271121492bfc96@mail.gmail.com>

Of course it should be in ESS.  In fact, it is for me, anyway
(different colors depending on whether they match or not).

Check out the paren-hilit or paren-match (or something like that)
customize options.

I.e. 

M-x customize-groups <ret> paren <spc>

(which ought to complete on names of groups/options starting with
paren)  or blink-paren.

(I'm not on an Emacs-enabled computer right now).

On 5/27/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi  Ajay
> 
> well ESS has such a facility.
> 
> However, I think Mathematica has a super scheme: unbalanced brackets
> show up
> in red, making them obvious.
> 
> This is particularly good for spotting wrongly interleaved brackets, as
> in
> 
> ([  blah di blah  )]
> 
> <note bracket closure is out of order>
> 
> in which case both opening braces are highlighted in red: and the
> system won't
> accept a newline until the closures are all correctly matched.
> 
> Would anyone else find such a thing useful?
> 
> Could the ESS team make something like this happen?
> 
> 
> 
> 
> 
> On May 27, 2005, at 12:11 pm, Ajay Narottam Shah wrote:
> 
> > I am using R 2.1 on Apple OS X.
> >
> > When I get the ">" prompt, I find it works well with emacs commandline
> > editing. Keys like M-f C-k etc. work fine.
> >
> > The one thing that I really yearn for, which is missing, is bracket
> > matching When I am doing something which ends in )))) it is really
> > useful to have emacs or vi-style bracket matching, so as to be able
> > to visually keep track of whether I have the correct matching
> > brackets, whether ( or { or [.
> >
> > I'm sure this is possible. I will be most grateful if someone will
> > show the way :-) Thanks,
> >
> > --
> > Ajay Shah                                                   Consultant
> > ajayshah at mayin.org                      Department of Economic Affairs
> > http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From sundar.dorai-raj at pdf.com  Fri May 27 20:39:19 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 27 May 2005 11:39:19 -0700
Subject: [R] nlminb to optmin
In-Reply-To: <036201c562e5$8b88d340$1b9e5f86@simurechner>
References: <036201c562e5$8b88d340$1b9e5f86@simurechner>
Message-ID: <42976957.8090700@pdf.com>



Stefan Pohl wrote:
> Hi!
> 
> I want to convert S-Plus 6.2 code to R 2.1.0. Instead of the function nlminb I use the function optmin
> 
> optmin(start,fn,gr,method="L-BFGS-B", lower, upper, hess,...)
> 
> But then I get the Error in optmin ...: L-BFGS-B needs finite values of fn
> 
> Then I used optmin(start,fn,gr,method="BFGS", hess, ...)
> 
> But then I get the Error in optmin ...: initial value in vmmin is not finite
> 
> I know the final parameter estimates from S-Plus which I use as starting values in R.
> The upper and lower bounds are close around the final estimates.
> So there is not much to maximize.
> 
> What can I do?
> 
> Thank you for help,
> 
> Peter
> 


What is "optmin"? Do you mean "optim"?

Either way, you can always try your function at the initial values 
outside of optim. If it returns Inf, you have a problem with your 
objective function. Have you considered that your objective function in 
S-PLUS didn't port the way you expected to R? I.e. if you call your 
objective function in S-PLUS and then in R, using the same inputs, do 
you get identical outputs?

--sundar



From bates at stat.wisc.edu  Fri May 27 20:57:38 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 27 May 2005 13:57:38 -0500
Subject: [R] nlminb to optmin
In-Reply-To: <036201c562e5$8b88d340$1b9e5f86@simurechner>
References: <036201c562e5$8b88d340$1b9e5f86@simurechner>
Message-ID: <42976DA2.6070406@stat.wisc.edu>

Stefan Pohl wrote:
> Hi!
> 
> I want to convert S-Plus 6.2 code to R 2.1.0. Instead of the function nlminb I use the function optmin
> 
> optmin(start,fn,gr,method="L-BFGS-B", lower, upper, hess,...)
> 
> But then I get the Error in optmin ...: L-BFGS-B needs finite values of fn
> 
> Then I used optmin(start,fn,gr,method="BFGS", hess, ...)
> 
> But then I get the Error in optmin ...: initial value in vmmin is not finite
> 
> I know the final parameter estimates from S-Plus which I use as starting values in R.
> The upper and lower bounds are close around the final estimates.
> So there is not much to maximize.
> 
> What can I do?
> 
> Thank you for help,

I have a test version of a package available as

http://www.stat.wisc.edu/~bates/port_0.1-1.tar.gz

that provides nlminb for R.  If you can install packages from source
code then you may want to try that.  If you are running under Windows
and only install binary packages then we will need to ask for a
volunteer to create a Windows binary from the source package.

I do not plan to upload this package to CRAN.  Instead I plan to
incorporate nlminb into r-devel in time to have it become part of R-2.2.0



From h.brunschwig at utoronto.ca  Fri May 27 21:26:53 2005
From: h.brunschwig at utoronto.ca (h.brunschwig@utoronto.ca)
Date: Fri, 27 May 2005 15:26:53 -0400
Subject: [R] longitudinal survey data
In-Reply-To: <Pine.A41.4.61b.0505270930200.112250@homer10.u.washington.edu>
References: <1117131639.429613772e707@webmail.utoronto.ca>
	<Pine.A41.4.61b.0505261305140.303294@homer12.u.washington.edu>
	<1117202767.4297294fd6e37@webmail.utoronto.ca>
	<Pine.A41.4.61b.0505270930200.112250@homer10.u.washington.edu>
Message-ID: <1117222013.4297747d4e2de@webmail.utoronto.ca>

Sorry, still confused. If I dont have fpc's ready in my dataset (calculate
myself?) that means that R will use the weight of an individual for each of his
repeated observations. But is that then still correct? The "cluster" individual
is ignored and each observation of an individual has the same weight.

Thanks a lot.

Dassy

Quoting Thomas Lumley <tlumley at u.washington.edu>:

> On Fri, 27 May 2005 h.brunschwig at utoronto.ca wrote:
> 
> >
> > Thank you for your reply.
> >
> > Does that mean that in order to take in account the repeated measures I
> denote
> > these as another cluster in R?
> >
> 
> Yes, but unless you have multistage finite population corrections to put 
> in the design object only the first stage of clustering affects the 
> results, so you may not need to bother.
> 
>  	-thomas
> 
>



From slist at oomvanlieshout.net  Fri May 27 22:27:37 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 27 May 2005 22:27:37 +0200
Subject: [R] Soil texture triangle in R?
In-Reply-To: <42974BD9.6080609@oomvanlieshout.net>
References: <4296D592.5040108@oomvanlieshout.net>	<42972186.50404@oomvanlieshout.net>
	<42974BD9.6080609@oomvanlieshout.net>
Message-ID: <429782B9.5060102@oomvanlieshout.net>

Cleaned up the class divisions and created a full function.

Still to do:
- rotate axis labels;
- correct the partial covering of the bottom tick labels;
- rotate ticks in order to simplify viewing the graph.
See: 
http://soil.scijournals.org/content/vol65/issue4/images/large/1038f2.jpeg

Wonder whether triangle.plot{ade4} will give more flexibility!?

Anyway, hopefully the result so far is useful for other people.

Cheers,

Sander.

plot.soiltexture <- function(x,pch,col) {
   ## triangle plots:
   ##   triangle.plot {ade4}
   ##   triplot{klaR}
   ##   ternaryplot {vcd}
   require(vcd)
   require(Zelig)
   ternaryplot(x,
     grid=FALSE,
     dimnames.position = "none",
     pch=pch, col=col,
     scale=1, main=NULL,
     prop.size=FALSE
   )
   oldpar <- par(no.readonly=TRUE)

   ticlength <- 0.01
   ## now the bottom internal ticks
   x1<-seq(0.1,0.9,by=0.1)
   x2<-x1
   y1<-rep(0,9)
   y2<-rep(ticlength,9)
   segments(x1,y1,x2,y2)
   text(x1,y1-0.03,as.character(rev(seq(10,90,by=10)))) #, cex=0.8)
   ## now the left internal ticks
   y1<-x1*sin60
   x1<-x1*0.5
   x2<-x1+ticlength*sin60
   y2<-y1-ticlength*0.5
   segments(x1,y1,x2,y2)
   text(x1-0.03,y1+0.015,as.character(seq(10,90,by=10)))
   ## now the right internal ticks
   x1<-rev(x1+0.5-ticlength*sin60)
   x2<-x1+ticlength*sin60
   segments(x1,y2,x2,y1)
   text(x2+0.03,y1+0.015,as.character(rev(seq(10,90,by=10))))

   ## the labels at the corners
   par(xpd=TRUE)
#   text(0.5,0.9,"100% clay")
#   text(-0.1,0,"100% sand")
#   text(1.1,0,"100% loam")

   ## the axis labels
   text(0.09,0.43,"% Clay")
   text(0.90,0.43,"% Silt")
   text(0.5,-0.1,"% Sand")

   # boundary of clay with extensions
   x1<-c(0.275,0.355,0.6)
   x2<-c(0.415,0.8,0.7)
   y1<-c(0.55*sin60,0.4*sin60,0.4*sin60)
   y2<-c(0.285*sin60,0.4*sin60,0.6*sin60)
   segments(x1,y1,x2,y2, col="grey")
   text(0.5,0.57,"Clay", col="grey")
   # lower bound of clay loam & silty divider
   x1<-c(0.415,0.66)
   x2<-c(0.856,0.6)
   y1<-c(0.285*sin60,0.285*sin60)
   y2<-c(0.285*sin60,0.40*sin60)
   segments(x1,y1,x2,y2, col="grey")
   text(0.7,0.49*sin60,"Silty", col="grey")
   text(0.7,0.44*sin60,"clay", col="grey")
   text(0.72,0.36*sin60,"Silty clay", col="grey")
   text(0.73,0.32*sin60,"loam", col="grey")
   text(0.5,0.35*sin60,"Clay loam", col="grey")
   x1<-c(0.185,0.1,0.37)
   x2<-c(0.37,0.37,0.415)
   y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
   y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
   segments(x1,y1,x2,y2, col="grey")
   text(0.28,0.43*sin60,"Sandy", col="grey")
   text(0.27,0.39*sin60,"clay", col="grey")
   text(0.27,0.3*sin60,"Sandy clay", col="grey")
   text(0.27,0.26*sin60,"loam", col="grey")
   # sand corner
   x1<-c(0.05,0.075)
   x2<-c(0.15,0.3)
   y1<-c(0.1*sin60,0.15*sin60)
   y2<-c(0,0)
   segments(x1,y1,x2,y2, col="grey")
   text(0.25,0.13*sin60,"Sandy loam", col="grey")
   text(0.14,0.07*sin60,"Loamy", col="grey")
   text(0.18,0.03*sin60,"sand", col="grey")
   text(0.06,0.021,"Sand", col="grey")
   x1<-c(0.37,0.435,0.5,0.8,0.86)
   x2<-c(0.435,0.537,0.64,0.86,0.94)
   y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
   y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
   segments(x1,y1,x2,y2, col="grey")
   text(0.49,0.18*sin60,"Loam", col="grey")
   text(0.72,0.15*sin60,"Silt loam", col="grey")
   text(0.9,0.06*sin60,"Silt", col="grey")

   ternarypoints(x, pch = pch, col = col)

   par(oldpar)
}

tmp <- array(dim=c(10,3))
tmp[,2] <- abs(rnorm(10)*20)
tmp[,3] <- abs(rnorm(10)*10)
tmp[,1] <- 100-tmp[,2]-tmp[,3]
col <- rep("black",10)
pch <- rep(1, 10)
plot.soiltexture(tmp,pch,col="black")


Sander Oom wrote:
> Right,
> 
> Got the data points plotted on top of the soil texture background, 
> thanks to Jim and ternaryplot{vcd}! See code below.
> 
> Now there is some fine tuning to do, as it should really look like this 
> graph:
> http://soil.scijournals.org/content/vol65/issue4/images/large/1038f2.jpeg
> 
> Things to do:
> - rotate axis labels;
> - correct small errors in class divisions;
> - correct the partial covering of the bottom tick labels;
> - rotate ticks in order to simplify viewing the graph.
> 
> Any help still appreciated!
> 
> Cheers,
> 
> Sander.
> 
> 
> soil.triangle <- function() {
>   oldpar <- par(no.readonly=TRUE)
>   ## now the bottom internal ticks
>   x1<-seq(0.1,0.9,by=0.1)
>   x2<-x1
>   y1<-rep(0,9)
>   y2<-rep(-0.02,9)
>   segments(x1,y1,x2,y2)
>   text(x1,y1-0.03,as.character(rev(seq(10,90,by=10)))) #, cex=0.8)
>   ## now the left internal ticks
>   y1<-x1*sin60
>   x1<-x1*0.5
>   x2<-x1+0.02*sin60
>   y2<-y1-0.02*0.5
>   segments(x1,y1,x2,y2)
>   text(x1-0.03,y1+0.015,as.character(seq(10,90,by=10)))
>   ## now the right internal ticks
>   x1<-rev(x1+0.5-0.02*sin60)
>   x2<-x1+0.02*sin60
>   segments(x1,y2,x2,y1)
>   text(x2+0.03,y1+0.015,as.character(rev(seq(10,90,by=10))))
>   ## the labels at the corners
>   par(xpd=TRUE)
> #   text(0.5,0.9,"100% clay")
> #   text(-0.1,0,"100% sand")
> #   text(1.1,0,"100% loam")
>   text(0.09,0.43,"% Clay")
>   text(0.90,0.43,"% Silt")
>   text(0.5,-0.1,"% Sand")
>   # boundary of clay with extensions
>   x1<-c(0.275,0.35,0.6)
>   x2<-c(0.4,0.79,0.7)
>   y1<-c(0.55*sin60,0.41*sin60,0.41*sin60)
>   y2<-c(0.285*sin60,0.41*sin60,0.6*sin60)
>   segments(x1,y1,x2,y2, col="grey")
>   text(0.5,0.57,"Clay", col="grey")
>   # lower bound of clay loam & silty divider
>   x1<-c(0.4,0.68)
>   x2<-c(0.86,0.6)
>   y1<-c(0.285*sin60,0.285*sin60)
>   y2<-c(0.285*sin60,0.41*sin60)
>   segments(x1,y1,x2,y2, col="grey")
>   text(0.7,0.49*sin60,"Silty", col="grey")
>   text(0.7,0.44*sin60,"clay", col="grey")
>   text(0.73,0.37*sin60,"Silty clay", col="grey")
>   text(0.73,0.33*sin60,"loam", col="grey")
>   text(0.5,0.35*sin60,"Clay loam", col="grey")
>   x1<-c(0.185,0.1,0.37)
>   x2<-c(0.36,0.37,0.4)
>   y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
>   y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
>   segments(x1,y1,x2,y2, col="grey")
>   text(0.27,0.43*sin60,"Sandy", col="grey")
>   text(0.27,0.39*sin60,"clay", col="grey")
>   text(0.27,0.3*sin60,"Sandy clay", col="grey")
>   text(0.27,0.26*sin60,"loam", col="grey")
>   # sand corner
>   x1<-c(0.05,0.075)
>   x2<-c(0.12,0.3)
>   y1<-c(0.1*sin60,0.15*sin60)
>   y2<-c(0,0)
>   segments(x1,y1,x2,y2, col="grey")
>   text(0.25,0.13*sin60,"Sandy loam", col="grey")
>   text(0.13,0.075*sin60,"Loamy", col="grey")
>   text(0.15,0.035*sin60,"sand", col="grey")
>   text(0.055,0.021,"Sand", col="grey")
>   x1<-c(0.37,0.42,0.5,0.8,0.86)
>   x2<-c(0.42,0.54,0.65,0.86,0.94)
>   y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
>   y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
>   segments(x1,y1,x2,y2, col="grey")
>   text(0.49,0.18*sin60,"Loam", col="grey")
>   text(0.72,0.15*sin60,"Silt loam", col="grey")
>   text(0.9,0.06*sin60,"Silt", col="grey")
>   par(oldpar)
> }
> 
> tmp <- array(dim=c(10,3))
> tmp[,1] <- abs(rnorm(10)*20)
> tmp[,2] <- abs(rnorm(10)*10)
> tmp[,3] <- 100-tmp[,1]-tmp[,2]
> tmp
> 
> library(vcd)
> ## Mark groups
> ternaryplot(tmp,
>   grid=FALSE,
>   dimnames.position = "none",
>   pch=1, col="black",
>   scale=1, main=NULL,
>   prop.size=FALSE,
>   )
> soil.triangle()
> 
> 
> Sander Oom wrote:
>> Hi Jim,
>>
>> This looks impressive! It gives me the 'background' graph. However, 
>> I'm not sure how I can use this function to plot my soil texture 
>> values! Can you explain?
>>
>> I would like to be able to plot my soil texture samples in the same 
>> graph as the one your function plots.
>>
>> Maybe I should try to figure out how to replicate your code inside a 
>> ternaryplot{vcd} call.
>>
>> Cheers,
>>
>> Sander.
>>
>> Jim Lemon wrote:
>>  > Sander Oom wrote:
>>  >> Dear R users,
>>  >>
>>  >> has anybody made an attempt to create the soil texture triangle graph
>>  >> in R? For an example see here:
>>  >>
>>  >> http://www.teachingkate.org/images/soiltria.gif
>>  >>
>>  >> I would like to get the lines in black and texture labels in gray to
>>  >> allow for plotting my texture results on top.
>>  >>
>>  >> Any examples or suggestions are very welcome!
>>  >>
>>  > It's not too hard to write a plot function to do this, but I'm not 
>> sure
>>  > whether this will be what you want. Anyway, try it out.
>>  >
>>  > Jim
>>  >
>>  > 
>> ------------------------------------------------------------------------
>>  >
>>  > soil.triangle<-function() {
>>  >  oldpar<-par(no.readonly=TRUE)
>>  >  plot(0:1,type="n",axes=FALSE,xlim=c(0,1.1),ylim=c(0,1),
>>  >   main="Soil Triangle",xlab="",ylab="")
>>  >  # first draw the triangle
>>  >  x1<-c(0,0,0.5)
>>  >  sin60<-sin(pi/3)
>>  >  x2<-c(1,0.5,1)
>>  >  y1<-c(0,0,sin60)
>>  >  y2<-c(0,sin60,0)
>>  >  segments(x1,y1,x2,y2)
>>  >  # now the bottom internal ticks
>>  >  x1<-seq(0.1,0.9,by=0.1)
>>  >  x2<-x1
>>  >  y1<-rep(0,9)
>>  >  y2<-rep(0.02,9)
>>  >  segments(x1,y1,x2,y2)
>>  >  text(x1,y1-0.03,as.character(rev(seq(10,90,by=10))))
>>  >  # now the left internal ticks
>>  >  y1<-x1*sin60
>>  >  x1<-x1*0.5
>>  >  x2<-x1+0.02*sin60
>>  >  y2<-y1-0.02*0.5
>>  >  segments(x1,y1,x2,y2)
>>  >  text(x1-0.03,y1+0.015,as.character(seq(10,90,by=10)))
>>  >  x1<-rev(x1+0.5-0.02*sin60)
>>  >  x2<-x1+0.02*sin60
>>  >  segments(x1,y2,x2,y1)
>>  >  text(x2+0.03,y1+0.015,as.character(rev(seq(10,90,by=10))))
>>  >  text(0.5,0.9,"100% clay")
>>  >  par(xpd=TRUE)
>>  >  text(-0.1,0,"100% sand")
>>  >  text(1.1,0,"100% loam")
>>  >  text(0.07,0.43,"percent clay")
>>  >  text(0.93,0.43,"percent silt")
>>  >  text(0.5,-0.1,"percent sand")
>>  >  # boundary of clay with extensions
>>  >  x1<-c(0.275,0.35,0.6)
>>  >  x2<-c(0.4,0.79,0.7)
>>  >  y1<-c(0.55*sin60,0.41*sin60,0.41*sin60)
>>  >  y2<-c(0.285*sin60,0.41*sin60,0.6*sin60)
>>  >  segments(x1,y1,x2,y2)
>>  >  text(0.5,0.57,"Clay")
>>  >  # lower bound of clay loam & silty divider
>>  >  x1<-c(0.4,0.68)
>>  >  x2<-c(0.86,0.6)
>>  >  y1<-c(0.285*sin60,0.285*sin60)
>>  >  y2<-c(0.285*sin60,0.41*sin60)
>>  >  segments(x1,y1,x2,y2)
>>  >  text(0.7,0.49*sin60,"Silty")
>>  >  text(0.7,0.44*sin60,"clay")
>>  >  text(0.73,0.37*sin60,"Silty clay")
>>  >  text(0.73,0.33*sin60,"loam")
>>  >  text(0.5,0.35*sin60,"Clay loam")
>>  >  x1<-c(0.185,0.1,0.37)
>>  >  x2<-c(0.36,0.37,0.4)
>>  >  y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
>>  >  y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
>>  >  segments(x1,y1,x2,y2)
>>  >  text(0.27,0.43*sin60,"Sandy")
>>  >  text(0.27,0.39*sin60,"clay")
>>  >  text(0.27,0.3*sin60,"Sandy clay")
>>  >  text(0.27,0.26*sin60,"loam")
>>  >  # sand corner
>>  >  x1<-c(0.05,0.075)
>>  >  x2<-c(0.12,0.3)
>>  >  y1<-c(0.1*sin60,0.15*sin60)
>>  >  y2<-c(0,0)
>>  >  segments(x1,y1,x2,y2)
>>  >  text(0.25,0.13*sin60,"Sandy loam")
>>  >  text(0.13,0.075*sin60,"Loamy")
>>  >  text(0.15,0.035*sin60,"sand")
>>  >  text(0.055,0.021,"Sand")
>>  >  x1<-c(0.37,0.42,0.5,0.8,0.86)
>>  >  x2<-c(0.42,0.54,0.65,0.86,0.94)
>>  >  y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
>>  >  y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
>>  >  segments(x1,y1,x2,y2)
>>  >  text(0.49,0.18*sin60,"Loam")
>>  >  text(0.72,0.15*sin60,"Silt loam")
>>  >  text(0.9,0.06*sin60,"Silt")
>>  >  par(oldpar)
>>  > }
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From bates at stat.wisc.edu  Fri May 27 22:49:51 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 27 May 2005 15:49:51 -0500
Subject: [R] Windows binary version of port_0.1-1 available
Message-ID: <429787EF.9000108@stat.wisc.edu>

In an earlier message today I mentioned that a source package with a
version of nlminb for R was available as

http://www.stat.wisc.edu/~bates/port_0.1-1.tar.gz

Thanks to Kjetil Halvorsen there is now a Windows binary version
available as

http://www.stat.wisc.edu/~bates/port_0.1-1.zip



From spencer.graves at pdf.com  Fri May 27 23:34:33 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 28 May 2005 06:34:33 +0900
Subject: [R] I never made any assumption athat anyone had any obligation
	to do anything
In-Reply-To: <42973F23.7020805@bellsouth.net>
References: <42973F23.7020805@bellsouth.net>
Message-ID: <42979269.8050307@pdf.com>

Hello, Erithid:

	  I'm very sorry you feel you got yelled at first thing in the morning. 
  Please try not to take it personally, though I know that may be 
difficult.  After one superficially insulting reply, I just laughed and 
told my manager, "Look what I learned for exposing myself to ridicule!"

	  My philosophy on this was expressed last December and subsequently 
added to the "fortunes" pacakage, from which I will quote for you now:

 > library(fortunes)
 > fortune("Graves")

Our great-great grandchilren as yet unborn may read some of the stupid 
questions
and/or answers that I and perhaps others give from time to time. I'd 
rather get
flamed for saying something stupid in public on this list than to 
continue to
provide substandard service to the people with whom I work because I 
perpetrated
the same mistake in an environment in which no one questioned so 
effectively my
errors.
    -- Spencer Graves (in a discussion on whether answers on R-help 
should be more
       polite)
       R-help (December 2004)

	  I've heard from some of the best on this listserve that it happens to 
everyone.  That doesn't make it right.  If you can learn to accept this 
as a different subculture operating by different rules of diplomacy, you 
might get more out of this list, do better work with whatever you are 
attempting to do, and enjoy life more.  (My wife's middle name is Rose. 
  When she gives me a hard time about something, I just try to listen 
and console myself with the thought that, "If I want to sleep in a bed 
of roses, I must get used to the thorns.")

	  Best Wishes,
	  spencer graves

BJ wrote:

> I just asked a question. If I was too vague, then i am sorry. I dont 
> expect anyone to help me, but I thought that it was ok to put the 
> question out there in case someone wanted to help me. I didnt expect 
> abject hostility for it. Human decency was the only thing I did expect. 
> If my question was a pain,badly formatted, or too juvinile, then i have 
> no problem being ignored.I asked a question that the archives did not 
> satisfactorily answer. I appreciate the help of the poeple on this list 
> as they are an invaluable resource, especially since the R documentation 
> is sketchy at times. I am sorry for wasting everyones time. I just dont 
> like being yelled at first thing in the morning for asking for help on a 
> help list. See you all around  ~Erithid
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From zhliur at yahoo.com  Fri May 27 23:42:51 2005
From: zhliur at yahoo.com (yyan liu)
Date: Fri, 27 May 2005 14:42:51 -0700 (PDT)
Subject: [R] images and maps in R
Message-ID: <20050527214252.62689.qmail@web53110.mail.yahoo.com>

Hi:
  I have a question arising from my project. 
  A sample of the data is below. The first row stands
for the names of state in USA. The second row stand
for some numeric value in that state. Some of them are
NA. I can use the commands "data(stateMapEnv)" and
"map('state', fill = F)" in library "maps" to make a
plot of USA states. What I want to do is: 1. put the
corresponding state name on the Map 2. give different
state different colors which is related to their
value. For example, "red" for values ranging from
0-30, "green" for values from 80-90, etc. 3. if
possible, put the value of each state within the state
on the map. here, take the numeric value as some text.

Thank you very much!

AB	AK	AL	AR	AZ  CT   CA
91	80	NA	NA	17  33   20



From rjc0408 at yahoo.com  Sat May 28 00:19:14 2005
From: rjc0408 at yahoo.com (R JC)
Date: Fri, 27 May 2005 15:19:14 -0700 (PDT)
Subject: [R] installing spatstat in OSX 
Message-ID: <20050527221914.1001.qmail@web31409.mail.mud.yahoo.com>



Friends, 

I am trying to install the current version of spatstat
on Mac OS 10.3.9, but the compilation fails with the
following messages at the end: 


ld: warning -L: directory name
(/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2)
does not exist
ld: can't locate file for: -lg2c
make: *** [spatstat.so] Error 1
ERROR: compilation failed for package 'spatstat'

The current version of sm (which spatstat needs) also
fails to compile when I try that separately, with the
same error message. I remember that binaries of these
packages were formerly available for OSX, but not
anymore. 

I would appreciate your help with installing these.

Thanks,

-Robert



From macq at llnl.gov  Sat May 28 00:37:56 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 27 May 2005 15:37:56 -0700
Subject: [R] images and maps in R
In-Reply-To: <20050527214252.62689.qmail@web53110.mail.yahoo.com>
References: <20050527214252.62689.qmail@web53110.mail.yahoo.com>
Message-ID: <p06210219bebd4f9c5146@[128.115.153.6]>

This is is not difficult from the online help for the map() function. 
Here is an example.

map('state', region = c('new york', 'new jersey', 'penn'),fill=TRUE,col=1:4)

There is also an example there in how to add text to the map.

Another way uses the maptools package.

require(maptools)
?plot.Map

and then follow the examples given in the help page for plot.Map().

At 2:42 PM -0700 5/27/05, yyan liu wrote:
>Hi:
>   I have a question arising from my project.
>   A sample of the data is below. The first row stands
>for the names of state in USA. The second row stand
>for some numeric value in that state. Some of them are
>NA. I can use the commands "data(stateMapEnv)" and
>"map('state', fill = F)" in library "maps" to make a
>plot of USA states. What I want to do is: 1. put the
>corresponding state name on the Map 2. give different
>state different colors which is related to their
>value. For example, "red" for values ranging from
>0-30, "green" for values from 80-90, etc. 3. if
>possible, put the value of each state within the state
>on the map. here, take the numeric value as some text.
>
>Thank you very much!
>
>AB	AK	AL	AR	AZ  CT   CA
>91	80	NA	NA	17  33   20
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From tlumley at u.washington.edu  Sat May 28 01:35:25 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 27 May 2005 16:35:25 -0700 (PDT)
Subject: [R] longitudinal survey data
In-Reply-To: <1117222013.4297747d4e2de@webmail.utoronto.ca>
References: <1117131639.429613772e707@webmail.utoronto.ca>
	<Pine.A41.4.61b.0505261305140.303294@homer12.u.washington.edu>
	<1117202767.4297294fd6e37@webmail.utoronto.ca>
	<Pine.A41.4.61b.0505270930200.112250@homer10.u.washington.edu>
	<1117222013.4297747d4e2de@webmail.utoronto.ca>
Message-ID: <Pine.A41.4.61b.0505271622160.369692@homer10.u.washington.edu>

On Fri, 27 May 2005 h.brunschwig at utoronto.ca wrote:

> Sorry, still confused. If I dont have fpc's ready in my dataset (calculate
> myself?) that means that R will use the weight of an individual for each of his
> repeated observations. But is that then still correct? The "cluster" individual
> is ignored and each observation of an individual has the same weight.
>

Well, it depends to some extent on what inferences you are making, but 
yes, you probably do want each observation to have the same weight.

Suppose you have 4 measurements on each person, and you are working with a 
simple random sample of 1000 people from a population of 1,000,000. If you 
had done these 4 measurements on the whole population you would have 
4,000,000 measurements, so the 4000 measurements you have are 1/1000 of 
the population.  This is the same weighting as if you had a single 
measurement person person, giving 1000 measurements in the sample and 
1,000,000 in the population.

If different individuals have different numbers of measurements then 
things get a bit trickier. It depends then on why there are different 
numbers of measurements.If they are the result of non-response you might 
want to rescale the weights at later time points to give the right 
population totals.  If they are part of the sampling design then the 
design will specify what to do with them.


 	-thomas



From jmacdon at med.umich.edu  Sat May 28 02:09:26 2005
From: jmacdon at med.umich.edu (James MacDonald)
Date: Fri, 27 May 2005 20:09:26 -0400
Subject: [R] I never made any assumption athat anyone had any
	obligation to do anything
Message-ID: <s2977e92.013@med-gwia-01a.med.umich.edu>

To add to Spencer's reply, I have two things.

First, if you look back at your earlier list entries, Uwe asked you
politely at least twice to read the posting guide. The reason for this
is that the posting guide gives guidance to allow you to post questions
that are clear enough to answer, and points you to other sources of
information that you might not already know about. It was only after you
appeared to ignore his advice that he 'yelled' at you.

Second, you ultimately learned several things. As far as I can tell, Uwe
did answer your questions, but more importantly, you learned how to
interact on a list where you get free advice on how to use free software
(the operative term here being *free*). Isn't that better than being
ignored and not learning anything?

Best,

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623
>>> Spencer Graves <spencer.graves at pdf.com> 05/27/05 5:34 PM >>>
Hello, Erithid:

	  I'm very sorry you feel you got yelled at first thing in the
morning. 
  Please try not to take it personally, though I know that may be 
difficult.  After one superficially insulting reply, I just laughed and 
told my manager, "Look what I learned for exposing myself to ridicule!"

	  My philosophy on this was expressed last December and
subsequently 
added to the "fortunes" pacakage, from which I will quote for you now:

 > library(fortunes)
 > fortune("Graves")

Our great-great grandchilren as yet unborn may read some of the stupid 
questions
and/or answers that I and perhaps others give from time to time. I'd 
rather get
flamed for saying something stupid in public on this list than to 
continue to
provide substandard service to the people with whom I work because I 
perpetrated
the same mistake in an environment in which no one questioned so 
effectively my
errors.
    -- Spencer Graves (in a discussion on whether answers on R-help 
should be more
       polite)
       R-help (December 2004)

	  I've heard from some of the best on this listserve that it
happens to 
everyone.  That doesn't make it right.  If you can learn to accept this 
as a different subculture operating by different rules of diplomacy, you

might get more out of this list, do better work with whatever you are 
attempting to do, and enjoy life more.  (My wife's middle name is Rose. 
  When she gives me a hard time about something, I just try to listen 
and console myself with the thought that, "If I want to sleep in a bed 
of roses, I must get used to the thorns.")

	  Best Wishes,
	  spencer graves

BJ wrote:

> I just asked a question. If I was too vague, then i am sorry. I dont 
> expect anyone to help me, but I thought that it was ok to put the 
> question out there in case someone wanted to help me. I didnt expect 
> abject hostility for it. Human decency was the only thing I did
expect. 
> If my question was a pain,badly formatted, or too juvinile, then i
have 
> no problem being ignored.I asked a question that the archives did not 
> satisfactorily answer. I appreciate the help of the poeple on this
list 
> as they are an invaluable resource, especially since the R
documentation 
> is sketchy at times. I am sorry for wasting everyones time. I just
dont 
> like being yelled at first thing in the morning for asking for help on
a 
> help list. See you all around  ~Erithid
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.



From ggrothendieck at gmail.com  Sat May 28 02:56:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 27 May 2005 20:56:00 -0400
Subject: [R] I never made any assumption athat anyone had any obligation
	to do anything
In-Reply-To: <s2977e92.013@med-gwia-01a.med.umich.edu>
References: <s2977e92.013@med-gwia-01a.med.umich.edu>
Message-ID: <971536df05052717561c214d02@mail.gmail.com>

Perhaps the list would work better if posters who do not follow
the posting guide are nicely told that they will more likely get
helpful replies if they read and follow the posting guide and,
in particular, provide a reproducible example.

If the poster does not follow that advice then the poster can just 
be ignored.   

This thread pointed out that even though the original poster
did not follow the guide and ignored direct suggestions that he 
still got his answer.  The problem is that he felt insulted in the 
process.  If the above procedure were followed he might not have 
received an answer to his problem but there would have been less 
animosity all around and maybe that's more important. 

Perhaps the posting guide could even suggest such an approach.


On 5/27/05, James MacDonald <jmacdon at med.umich.edu> wrote:
> To add to Spencer's reply, I have two things.
> 
> First, if you look back at your earlier list entries, Uwe asked you
> politely at least twice to read the posting guide. The reason for this
> is that the posting guide gives guidance to allow you to post questions
> that are clear enough to answer, and points you to other sources of
> information that you might not already know about. It was only after you
> appeared to ignore his advice that he 'yelled' at you.
> 
> Second, you ultimately learned several things. As far as I can tell, Uwe
> did answer your questions, but more importantly, you learned how to
> interact on a list where you get free advice on how to use free software
> (the operative term here being *free*). Isn't that better than being
> ignored and not learning anything?
> 
> Best,
> 
> Jim
> 
> 
> 
> James W. MacDonald
> Affymetrix and cDNA Microarray Core
> University of Michigan Cancer Center
> 1500 E. Medical Center Drive
> 7410 CCGC
> Ann Arbor MI 48109
> 734-647-5623
> >>> Spencer Graves <spencer.graves at pdf.com> 05/27/05 5:34 PM >>>
> Hello, Erithid:
> 
>          I'm very sorry you feel you got yelled at first thing in the
> morning.
>  Please try not to take it personally, though I know that may be
> difficult.  After one superficially insulting reply, I just laughed and
> told my manager, "Look what I learned for exposing myself to ridicule!"
> 
>          My philosophy on this was expressed last December and
> subsequently
> added to the "fortunes" pacakage, from which I will quote for you now:
> 
>  > library(fortunes)
>  > fortune("Graves")
> 
> Our great-great grandchilren as yet unborn may read some of the stupid
> questions
> and/or answers that I and perhaps others give from time to time. I'd
> rather get
> flamed for saying something stupid in public on this list than to
> continue to
> provide substandard service to the people with whom I work because I
> perpetrated
> the same mistake in an environment in which no one questioned so
> effectively my
> errors.
>    -- Spencer Graves (in a discussion on whether answers on R-help
> should be more
>       polite)
>       R-help (December 2004)
> 
>          I've heard from some of the best on this listserve that it
> happens to
> everyone.  That doesn't make it right.  If you can learn to accept this
> as a different subculture operating by different rules of diplomacy, you
> 
> might get more out of this list, do better work with whatever you are
> attempting to do, and enjoy life more.  (My wife's middle name is Rose.
>  When she gives me a hard time about something, I just try to listen
> and console myself with the thought that, "If I want to sleep in a bed
> of roses, I must get used to the thorns.")
> 
>          Best Wishes,
>          spencer graves
> 
> BJ wrote:
> 
> > I just asked a question. If I was too vague, then i am sorry. I dont
> > expect anyone to help me, but I thought that it was ok to put the
> > question out there in case someone wanted to help me. I didnt expect
> > abject hostility for it. Human decency was the only thing I did
> expect.
> > If my question was a pain,badly formatted, or too juvinile, then i
> have
> > no problem being ignored.I asked a question that the archives did not
> > satisfactorily answer. I appreciate the help of the poeple on this
> list
> > as they are an invaluable resource, especially since the R
> documentation
> > is sketchy at times. I am sorry for wasting everyones time. I just
> dont
> > like being yelled at first thing in the morning for asking for help on
> a
> > help list. See you all around  ~Erithid
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> **********************************************************
> Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From henric.nilsson at statisticon.se  Sat May 28 12:02:44 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Sat, 28 May 2005 12:02:44 +0200
Subject: [R] No ~ in JGR
In-Reply-To: <42946EAC.2020402@evp.slu.se>
References: <42946EAC.2020402@evp.slu.se>
Message-ID: <429841C4.3090001@statisticon.se>

CG Pettersson said the following on 2005-05-25 14:25:
> R2.1.0
> JGR 1.2
> W2k
> 
> Hello all!
> I??ve just installed JGR on my both R-equipped computers and am very 
> pleased with the look and functionality.
> 
> Except in one, very important, way.
> 
> I can??t figure out how to get the ~ sign from the keyboard to the 
> console. Copying it from old code works fine. Using the traditional GUI 
> works as usual.
> 
> I have a Swedish keyboard layout, where ~ shares key with ?? and ^, all 
> reguiring a space after the hit to produce the sign on the screen.
> 
> The other signs from the key works, but AltGr + ~ followed by space just 
> gives a blank. What do I do wrong?

Probably nothing, or we're both doing something wrong.

I'm experiencing the same thing, and the asked the JGR developers about 
it some time ago. See the thread that starts with

http://mailman.rz.uni-augsburg.de/pipermail/stats-rosuda-devel/2005q1/000030.html


Henric



From slist at oomvanlieshout.net  Sat May 28 13:58:27 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Sat, 28 May 2005 13:58:27 +0200
Subject: [R] Soil texture triangle in R?
In-Reply-To: <429782B9.5060102@oomvanlieshout.net>
References: <4296D592.5040108@oomvanlieshout.net>	<42972186.50404@oomvanlieshout.net>	<42974BD9.6080609@oomvanlieshout.net>
	<429782B9.5060102@oomvanlieshout.net>
Message-ID: <42985CE3.7080709@oomvanlieshout.net>

Apologies, removed a crucial line by accident! Here the working version 
of the function, with added control over the colours used.

Enjoy!

plot.soiltexture <- function(x,pch,col,colgrid,coltext) {
   ## triangle plots:
   ##   triangle.plot {ade4}
   ##   triplot{klaR}
   ##   ternaryplot {vcd}
   require(vcd)

   ternaryplot(x,
     grid=FALSE,
     dimnames.position = "none",
     pch=pch, col=col,
     scale=1, main=NULL,
     prop.size=FALSE
   )
   oldpar <- par(no.readonly=TRUE)

   ticlength <- 0.01
   sin60<-sin(pi/3)
   ## now the bottom internal ticks
   x1<-seq(0.1,0.9,by=0.1)
   x2<-x1
   y1<-rep(0,9)
   y2<-rep(ticlength,9)
   segments(x1,y1,x2,y2)
   text(x1,y1-0.03,as.character(rev(seq(10,90,by=10)))) #, cex=0.8)
   ## now the left internal ticks
   y1<-x1*sin60
   x1<-x1*0.5
   x2<-x1+ticlength*sin60
   y2<-y1-ticlength*0.5
   segments(x1,y1,x2,y2)
   text(x1-0.03,y1+0.015,as.character(seq(10,90,by=10)))
   ## now the right internal ticks
   x1<-rev(x1+0.5-ticlength*sin60)
   x2<-x1+ticlength*sin60
   segments(x1,y2,x2,y1)
   text(x2+0.03,y1+0.015,as.character(rev(seq(10,90,by=10))))

   ## the labels at the corners
   par(xpd=TRUE)
#   text(0.5,0.9,"100% clay")
#   text(-0.1,0,"100% sand")
#   text(1.1,0,"100% loam")

   ## the axis labels
   text(0.09,0.43,"% Clay")
   text(0.90,0.43,"% Silt")
   text(0.5,-0.1,"% Sand")

   # boundary of clay with extensions
   x1<-c(0.275,0.355,0.6)
   x2<-c(0.415,0.8,0.7)
   y1<-c(0.55*sin60,0.4*sin60,0.4*sin60)
   y2<-c(0.285*sin60,0.4*sin60,0.6*sin60)
   segments(x1,y1,x2,y2, col=colgrid)
   text(0.5,0.57,"Clay", col=coltext)
   # lower bound of clay loam & silty divider
   x1<-c(0.415,0.66)
   x2<-c(0.856,0.6)
   y1<-c(0.285*sin60,0.285*sin60)
   y2<-c(0.285*sin60,0.40*sin60)
   segments(x1,y1,x2,y2, col=colgrid)
   text(0.7,0.49*sin60,"Silty", col=coltext)
   text(0.7,0.44*sin60,"clay", col=coltext)
   text(0.72,0.36*sin60,"Silty clay", col=coltext)
   text(0.73,0.32*sin60,"loam", col=coltext)
   text(0.5,0.35*sin60,"Clay loam", col=coltext)
   x1<-c(0.185,0.1,0.37)
   x2<-c(0.37,0.37,0.415)
   y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
   y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
   segments(x1,y1,x2,y2, col=colgrid)
   text(0.28,0.43*sin60,"Sandy", col=coltext)
   text(0.27,0.39*sin60,"clay", col=coltext)
   text(0.27,0.3*sin60,"Sandy clay", col=coltext)
   text(0.27,0.26*sin60,"loam", col=coltext)
   # sand corner
   x1<-c(0.05,0.075)
   x2<-c(0.15,0.3)
   y1<-c(0.1*sin60,0.15*sin60)
   y2<-c(0,0)
   segments(x1,y1,x2,y2, col=colgrid)
   text(0.25,0.13*sin60,"Sandy loam", col=coltext)
   text(0.14,0.07*sin60,"Loamy", col=coltext)
   text(0.18,0.03*sin60,"sand", col=coltext)
   text(0.06,0.021,"Sand", col=coltext)
   x1<-c(0.37,0.435,0.5,0.8,0.86)
   x2<-c(0.435,0.537,0.64,0.86,0.94)
   y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
   y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
   segments(x1,y1,x2,y2, col=colgrid)
   text(0.49,0.18*sin60,"Loam", col=coltext)
   text(0.72,0.15*sin60,"Silt loam", col=coltext)
   text(0.9,0.06*sin60,"Silt", col=coltext)
   par(oldpar)
}

tmp <- array(dim=c(10,3))
tmp[,2] <- abs(rnorm(10)*20)
tmp[,3] <- abs(rnorm(10)*10)
tmp[,1] <- 100-tmp[,2]-tmp[,3]
col <- rep("black",10)
pch <- rep(1, 10)
plot.soiltexture(tmp,pch,col=NULL,colgrid="black", coltext="black")

plot.soiltexture(tmp,pch,col="black",colgrid="grey", coltext="white")


Sander Oom wrote:
> Cleaned up the class divisions and created a full function.
> 
> Still to do:
> - rotate axis labels;
> - correct the partial covering of the bottom tick labels;
> - rotate ticks in order to simplify viewing the graph.
> See: 
> http://soil.scijournals.org/content/vol65/issue4/images/large/1038f2.jpeg
> 
> Wonder whether triangle.plot{ade4} will give more flexibility!?
> 
> Anyway, hopefully the result so far is useful for other people.
> 
> Cheers,
> 
> Sander.
> 
> plot.soiltexture <- function(x,pch,col) {
>   ## triangle plots:
>   ##   triangle.plot {ade4}
>   ##   triplot{klaR}
>   ##   ternaryplot {vcd}
>   require(vcd)
>   require(Zelig)
>   ternaryplot(x,
>     grid=FALSE,
>     dimnames.position = "none",
>     pch=pch, col=col,
>     scale=1, main=NULL,
>     prop.size=FALSE
>   )
>   oldpar <- par(no.readonly=TRUE)
> 
>   ticlength <- 0.01
>   ## now the bottom internal ticks
>   x1<-seq(0.1,0.9,by=0.1)
>   x2<-x1
>   y1<-rep(0,9)
>   y2<-rep(ticlength,9)
>   segments(x1,y1,x2,y2)
>   text(x1,y1-0.03,as.character(rev(seq(10,90,by=10)))) #, cex=0.8)
>   ## now the left internal ticks
>   y1<-x1*sin60
>   x1<-x1*0.5
>   x2<-x1+ticlength*sin60
>   y2<-y1-ticlength*0.5
>   segments(x1,y1,x2,y2)
>   text(x1-0.03,y1+0.015,as.character(seq(10,90,by=10)))
>   ## now the right internal ticks
>   x1<-rev(x1+0.5-ticlength*sin60)
>   x2<-x1+ticlength*sin60
>   segments(x1,y2,x2,y1)
>   text(x2+0.03,y1+0.015,as.character(rev(seq(10,90,by=10))))
> 
>   ## the labels at the corners
>   par(xpd=TRUE)
> #   text(0.5,0.9,"100% clay")
> #   text(-0.1,0,"100% sand")
> #   text(1.1,0,"100% loam")
> 
>   ## the axis labels
>   text(0.09,0.43,"% Clay")
>   text(0.90,0.43,"% Silt")
>   text(0.5,-0.1,"% Sand")
> 
>   # boundary of clay with extensions
>   x1<-c(0.275,0.355,0.6)
>   x2<-c(0.415,0.8,0.7)
>   y1<-c(0.55*sin60,0.4*sin60,0.4*sin60)
>   y2<-c(0.285*sin60,0.4*sin60,0.6*sin60)
>   segments(x1,y1,x2,y2, col="grey")
>   text(0.5,0.57,"Clay", col="grey")
>   # lower bound of clay loam & silty divider
>   x1<-c(0.415,0.66)
>   x2<-c(0.856,0.6)
>   y1<-c(0.285*sin60,0.285*sin60)
>   y2<-c(0.285*sin60,0.40*sin60)
>   segments(x1,y1,x2,y2, col="grey")
>   text(0.7,0.49*sin60,"Silty", col="grey")
>   text(0.7,0.44*sin60,"clay", col="grey")
>   text(0.72,0.36*sin60,"Silty clay", col="grey")
>   text(0.73,0.32*sin60,"loam", col="grey")
>   text(0.5,0.35*sin60,"Clay loam", col="grey")
>   x1<-c(0.185,0.1,0.37)
>   x2<-c(0.37,0.37,0.415)
>   y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
>   y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
>   segments(x1,y1,x2,y2, col="grey")
>   text(0.28,0.43*sin60,"Sandy", col="grey")
>   text(0.27,0.39*sin60,"clay", col="grey")
>   text(0.27,0.3*sin60,"Sandy clay", col="grey")
>   text(0.27,0.26*sin60,"loam", col="grey")
>   # sand corner
>   x1<-c(0.05,0.075)
>   x2<-c(0.15,0.3)
>   y1<-c(0.1*sin60,0.15*sin60)
>   y2<-c(0,0)
>   segments(x1,y1,x2,y2, col="grey")
>   text(0.25,0.13*sin60,"Sandy loam", col="grey")
>   text(0.14,0.07*sin60,"Loamy", col="grey")
>   text(0.18,0.03*sin60,"sand", col="grey")
>   text(0.06,0.021,"Sand", col="grey")
>   x1<-c(0.37,0.435,0.5,0.8,0.86)
>   x2<-c(0.435,0.537,0.64,0.86,0.94)
>   y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
>   y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
>   segments(x1,y1,x2,y2, col="grey")
>   text(0.49,0.18*sin60,"Loam", col="grey")
>   text(0.72,0.15*sin60,"Silt loam", col="grey")
>   text(0.9,0.06*sin60,"Silt", col="grey")
> 
>   ternarypoints(x, pch = pch, col = col)
> 
>   par(oldpar)
> }
> 
> tmp <- array(dim=c(10,3))
> tmp[,2] <- abs(rnorm(10)*20)
> tmp[,3] <- abs(rnorm(10)*10)
> tmp[,1] <- 100-tmp[,2]-tmp[,3]
> col <- rep("black",10)
> pch <- rep(1, 10)
> plot.soiltexture(tmp,pch,col="black")
> 
> 
> Sander Oom wrote:
>> Right,
>>
>> Got the data points plotted on top of the soil texture background, 
>> thanks to Jim and ternaryplot{vcd}! See code below.
>>
>> Now there is some fine tuning to do, as it should really look like 
>> this graph:
>> http://soil.scijournals.org/content/vol65/issue4/images/large/1038f2.jpeg
>>
>> Things to do:
>> - rotate axis labels;
>> - correct small errors in class divisions;
>> - correct the partial covering of the bottom tick labels;
>> - rotate ticks in order to simplify viewing the graph.
>>
>> Any help still appreciated!
>>
>> Cheers,
>>
>> Sander.
>>
>>
>> soil.triangle <- function() {
>>   oldpar <- par(no.readonly=TRUE)
>>   ## now the bottom internal ticks
>>   x1<-seq(0.1,0.9,by=0.1)
>>   x2<-x1
>>   y1<-rep(0,9)
>>   y2<-rep(-0.02,9)
>>   segments(x1,y1,x2,y2)
>>   text(x1,y1-0.03,as.character(rev(seq(10,90,by=10)))) #, cex=0.8)
>>   ## now the left internal ticks
>>   y1<-x1*sin60
>>   x1<-x1*0.5
>>   x2<-x1+0.02*sin60
>>   y2<-y1-0.02*0.5
>>   segments(x1,y1,x2,y2)
>>   text(x1-0.03,y1+0.015,as.character(seq(10,90,by=10)))
>>   ## now the right internal ticks
>>   x1<-rev(x1+0.5-0.02*sin60)
>>   x2<-x1+0.02*sin60
>>   segments(x1,y2,x2,y1)
>>   text(x2+0.03,y1+0.015,as.character(rev(seq(10,90,by=10))))
>>   ## the labels at the corners
>>   par(xpd=TRUE)
>> #   text(0.5,0.9,"100% clay")
>> #   text(-0.1,0,"100% sand")
>> #   text(1.1,0,"100% loam")
>>   text(0.09,0.43,"% Clay")
>>   text(0.90,0.43,"% Silt")
>>   text(0.5,-0.1,"% Sand")
>>   # boundary of clay with extensions
>>   x1<-c(0.275,0.35,0.6)
>>   x2<-c(0.4,0.79,0.7)
>>   y1<-c(0.55*sin60,0.41*sin60,0.41*sin60)
>>   y2<-c(0.285*sin60,0.41*sin60,0.6*sin60)
>>   segments(x1,y1,x2,y2, col="grey")
>>   text(0.5,0.57,"Clay", col="grey")
>>   # lower bound of clay loam & silty divider
>>   x1<-c(0.4,0.68)
>>   x2<-c(0.86,0.6)
>>   y1<-c(0.285*sin60,0.285*sin60)
>>   y2<-c(0.285*sin60,0.41*sin60)
>>   segments(x1,y1,x2,y2, col="grey")
>>   text(0.7,0.49*sin60,"Silty", col="grey")
>>   text(0.7,0.44*sin60,"clay", col="grey")
>>   text(0.73,0.37*sin60,"Silty clay", col="grey")
>>   text(0.73,0.33*sin60,"loam", col="grey")
>>   text(0.5,0.35*sin60,"Clay loam", col="grey")
>>   x1<-c(0.185,0.1,0.37)
>>   x2<-c(0.36,0.37,0.4)
>>   y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
>>   y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
>>   segments(x1,y1,x2,y2, col="grey")
>>   text(0.27,0.43*sin60,"Sandy", col="grey")
>>   text(0.27,0.39*sin60,"clay", col="grey")
>>   text(0.27,0.3*sin60,"Sandy clay", col="grey")
>>   text(0.27,0.26*sin60,"loam", col="grey")
>>   # sand corner
>>   x1<-c(0.05,0.075)
>>   x2<-c(0.12,0.3)
>>   y1<-c(0.1*sin60,0.15*sin60)
>>   y2<-c(0,0)
>>   segments(x1,y1,x2,y2, col="grey")
>>   text(0.25,0.13*sin60,"Sandy loam", col="grey")
>>   text(0.13,0.075*sin60,"Loamy", col="grey")
>>   text(0.15,0.035*sin60,"sand", col="grey")
>>   text(0.055,0.021,"Sand", col="grey")
>>   x1<-c(0.37,0.42,0.5,0.8,0.86)
>>   x2<-c(0.42,0.54,0.65,0.86,0.94)
>>   y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
>>   y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
>>   segments(x1,y1,x2,y2, col="grey")
>>   text(0.49,0.18*sin60,"Loam", col="grey")
>>   text(0.72,0.15*sin60,"Silt loam", col="grey")
>>   text(0.9,0.06*sin60,"Silt", col="grey")
>>   par(oldpar)
>> }
>>
>> tmp <- array(dim=c(10,3))
>> tmp[,1] <- abs(rnorm(10)*20)
>> tmp[,2] <- abs(rnorm(10)*10)
>> tmp[,3] <- 100-tmp[,1]-tmp[,2]
>> tmp
>>
>> library(vcd)
>> ## Mark groups
>> ternaryplot(tmp,
>>   grid=FALSE,
>>   dimnames.position = "none",
>>   pch=1, col="black",
>>   scale=1, main=NULL,
>>   prop.size=FALSE,
>>   )
>> soil.triangle()
>>
>>
>> Sander Oom wrote:
>>> Hi Jim,
>>>
>>> This looks impressive! It gives me the 'background' graph. However, 
>>> I'm not sure how I can use this function to plot my soil texture 
>>> values! Can you explain?
>>>
>>> I would like to be able to plot my soil texture samples in the same 
>>> graph as the one your function plots.
>>>
>>> Maybe I should try to figure out how to replicate your code inside a 
>>> ternaryplot{vcd} call.
>>>
>>> Cheers,
>>>
>>> Sander.
>>>
>>> Jim Lemon wrote:
>>>  > Sander Oom wrote:
>>>  >> Dear R users,
>>>  >>
>>>  >> has anybody made an attempt to create the soil texture triangle 
>>> graph
>>>  >> in R? For an example see here:
>>>  >>
>>>  >> http://www.teachingkate.org/images/soiltria.gif
>>>  >>
>>>  >> I would like to get the lines in black and texture labels in gray to
>>>  >> allow for plotting my texture results on top.
>>>  >>
>>>  >> Any examples or suggestions are very welcome!
>>>  >>
>>>  > It's not too hard to write a plot function to do this, but I'm not 
>>> sure
>>>  > whether this will be what you want. Anyway, try it out.
>>>  >
>>>  > Jim
>>>  >
>>>  > 
>>> ------------------------------------------------------------------------
>>>  >
>>>  > soil.triangle<-function() {
>>>  >  oldpar<-par(no.readonly=TRUE)
>>>  >  plot(0:1,type="n",axes=FALSE,xlim=c(0,1.1),ylim=c(0,1),
>>>  >   main="Soil Triangle",xlab="",ylab="")
>>>  >  # first draw the triangle
>>>  >  x1<-c(0,0,0.5)
>>>  >  sin60<-sin(pi/3)
>>>  >  x2<-c(1,0.5,1)
>>>  >  y1<-c(0,0,sin60)
>>>  >  y2<-c(0,sin60,0)
>>>  >  segments(x1,y1,x2,y2)
>>>  >  # now the bottom internal ticks
>>>  >  x1<-seq(0.1,0.9,by=0.1)
>>>  >  x2<-x1
>>>  >  y1<-rep(0,9)
>>>  >  y2<-rep(0.02,9)
>>>  >  segments(x1,y1,x2,y2)
>>>  >  text(x1,y1-0.03,as.character(rev(seq(10,90,by=10))))
>>>  >  # now the left internal ticks
>>>  >  y1<-x1*sin60
>>>  >  x1<-x1*0.5
>>>  >  x2<-x1+0.02*sin60
>>>  >  y2<-y1-0.02*0.5
>>>  >  segments(x1,y1,x2,y2)
>>>  >  text(x1-0.03,y1+0.015,as.character(seq(10,90,by=10)))
>>>  >  x1<-rev(x1+0.5-0.02*sin60)
>>>  >  x2<-x1+0.02*sin60
>>>  >  segments(x1,y2,x2,y1)
>>>  >  text(x2+0.03,y1+0.015,as.character(rev(seq(10,90,by=10))))
>>>  >  text(0.5,0.9,"100% clay")
>>>  >  par(xpd=TRUE)
>>>  >  text(-0.1,0,"100% sand")
>>>  >  text(1.1,0,"100% loam")
>>>  >  text(0.07,0.43,"percent clay")
>>>  >  text(0.93,0.43,"percent silt")
>>>  >  text(0.5,-0.1,"percent sand")
>>>  >  # boundary of clay with extensions
>>>  >  x1<-c(0.275,0.35,0.6)
>>>  >  x2<-c(0.4,0.79,0.7)
>>>  >  y1<-c(0.55*sin60,0.41*sin60,0.41*sin60)
>>>  >  y2<-c(0.285*sin60,0.41*sin60,0.6*sin60)
>>>  >  segments(x1,y1,x2,y2)
>>>  >  text(0.5,0.57,"Clay")
>>>  >  # lower bound of clay loam & silty divider
>>>  >  x1<-c(0.4,0.68)
>>>  >  x2<-c(0.86,0.6)
>>>  >  y1<-c(0.285*sin60,0.285*sin60)
>>>  >  y2<-c(0.285*sin60,0.41*sin60)
>>>  >  segments(x1,y1,x2,y2)
>>>  >  text(0.7,0.49*sin60,"Silty")
>>>  >  text(0.7,0.44*sin60,"clay")
>>>  >  text(0.73,0.37*sin60,"Silty clay")
>>>  >  text(0.73,0.33*sin60,"loam")
>>>  >  text(0.5,0.35*sin60,"Clay loam")
>>>  >  x1<-c(0.185,0.1,0.37)
>>>  >  x2<-c(0.36,0.37,0.4)
>>>  >  y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
>>>  >  y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
>>>  >  segments(x1,y1,x2,y2)
>>>  >  text(0.27,0.43*sin60,"Sandy")
>>>  >  text(0.27,0.39*sin60,"clay")
>>>  >  text(0.27,0.3*sin60,"Sandy clay")
>>>  >  text(0.27,0.26*sin60,"loam")
>>>  >  # sand corner
>>>  >  x1<-c(0.05,0.075)
>>>  >  x2<-c(0.12,0.3)
>>>  >  y1<-c(0.1*sin60,0.15*sin60)
>>>  >  y2<-c(0,0)
>>>  >  segments(x1,y1,x2,y2)
>>>  >  text(0.25,0.13*sin60,"Sandy loam")
>>>  >  text(0.13,0.075*sin60,"Loamy")
>>>  >  text(0.15,0.035*sin60,"sand")
>>>  >  text(0.055,0.021,"Sand")
>>>  >  x1<-c(0.37,0.42,0.5,0.8,0.86)
>>>  >  x2<-c(0.42,0.54,0.65,0.86,0.94)
>>>  >  y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
>>>  >  y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
>>>  >  segments(x1,y1,x2,y2)
>>>  >  text(0.49,0.18*sin60,"Loam")
>>>  >  text(0.72,0.15*sin60,"Silt loam")
>>>  >  text(0.9,0.06*sin60,"Silt")
>>>  >  par(oldpar)
>>>  > }
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>>
> 
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From slist at oomvanlieshout.net  Sat May 28 14:16:13 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Sat, 28 May 2005 14:16:13 +0200
Subject: [R] Soil texture triangle in R?
In-Reply-To: <42985CE3.7080709@oomvanlieshout.net>
References: <4296D592.5040108@oomvanlieshout.net>	<42972186.50404@oomvanlieshout.net>	<42974BD9.6080609@oomvanlieshout.net>	<429782B9.5060102@oomvanlieshout.net>
	<42985CE3.7080709@oomvanlieshout.net>
Message-ID: <4298610D.7000901@oomvanlieshout.net>

Hi Jim,

Your email was classified as spam, so I missed it previously. 
Unfortunately you did not send a cc to the R-help list. I send a cc to 
the mailing list now, so all code gets archived.

Thanks for the improvements on your function! It seems that drawing the 
ternary graph and the points with the generic plot functions works well. 
Makes the function less dependent on other packages!

Will merge the two functions into one and post it back to the mailing 
list! Then the graph might be ready for the graph gallery!

Thanks,

Sander.


Jim Lemon wrote:
 > Sander Oom wrote:
 >> Hi Jim,
 >>
 >> This looks impressive! It gives me the 'background' graph. However,
 >> I'm not sure how I can use this function to plot my soil texture
 >> values! Can you explain?
 >>
 >> I would like to be able to plot my soil texture samples in the same
 >> graph as the one your function plots.
 >>
 > Yes, that's just the background figure for which you attached the link.
 > I was unsure of how you were going to use this, that is, do you just
 > place a symbol for each soil sample? Aha! Just read your latest email
 > and I think that's what you want.
 >
 > Let's say you have one or more soil samples with the proportions of
 > clay, sand and silt.
 >
 > sample1<-c(0.1,0.4,0.5)
 > sample2<-c(0.2,0.5,0.3)
 > soil.samples<-rbind(sample1,sample2)
 > colnames(soil.samples)<-c("clay","sand","silt")
 >
 > This more complicated function allows you to overlay colored points
 > representing soil samples on either an empty triangle, a gridded
 > triangle or a triangle with soil type names. It also has an optional
 > legend.
 >
 > par(ask=TRUE)
 > soil.triangle(soil.samples)
 > soil.triangle(soil.samples,show.grid=TRUE)
 > soil.triangle(soil.samples,soil.names=TRUE,legend=TRUE)
 > par(ask=FALSE)
 >
 > Jim
 >
 > ------------------------------------------------------------------------
 >
 > soil.triangle<-function(soilprop,pch=NULL,col=NULL,soil.names=FALSE,
 >  show.grid=FALSE,show.legend=FALSE) {
 >  if(missing(soilprop))
 >   stop("Usage: 
soil.triangle(soilprop,pch=NULL,col=NULL,soil.names=FALSE,show.grid=FALSE)")
 >  if(!is.matrix(soilprop))
 >   stop("soilprop must be a matrix with at least three columns and one 
row.")
 >  if(any(soilprop > 1) || any(soilprop < 0))
 >   stop("All soil proportions must be between zero and one.")
 >  if(any(abs(rowSums(soilprop)-1) > 0.01))
 >   warning("At least one set of soil proportions does not equal one.")
 >  oldpar<-par(no.readonly=TRUE)
 >  plot(0:1,type="n",axes=FALSE,xlim=c(0,1.1),ylim=c(0,1),
 >   main="Soil Triangle",xlab="",ylab="")
 >  # first draw the triangle
 >  x1<-c(0,0,0.5)
 >  sin60<-sin(pi/3)
 >  x2<-c(1,0.5,1)
 >  y1<-c(0,0,sin60)
 >  y2<-c(0,sin60,0)
 >  segments(x1,y1,x2,y2)
 >  # now the bottom internal ticks
 >  bx1<-seq(0.1,0.9,by=0.1)
 >  bx2<-bx1-0.01
 >  by1<-rep(0,9)
 >  by2<-rep(0.02*sin60,9)
 >  segments(bx1,by1,bx2,by2)
 >  text(bx1,by1-0.03,as.character(rev(seq(10,90,by=10))))
 >  # now the left internal ticks
 >  ly1<-bx1*sin60
 >  lx1<-bx1*0.5
 >  lx2<-lx1+0.02
 >  ly2<-ly1
 >  segments(lx1,ly1,lx2,ly2)
 >  text(lx1-0.03,ly1,as.character(seq(10,90,by=10)))
 >  # right internal ticks
 >  rx1<-rev(lx1+0.5-0.01)
 >  rx2<-rx1+0.01
 >  ry1<-ly1-0.02*sin60
 >  ry2<-ly2
 >  segments(rx1,ry1,rx2,ry2)
 >  if(show.grid) {
 >   segments(bx2,by2,lx1,ly1,lty=3)
 >   segments(lx2,ly2,rx2,ry2,lty=3)
 >   segments(rev(rx1),rev(ry1),bx1,by1,lty=3)
 >  }
 >  text(rx2+0.03,ry1+0.025,as.character(rev(seq(10,90,by=10))))
 >  text(0.5,0.9,"100% clay")
 >  par(xpd=TRUE)
 >  text(-0.1,0,"100% sand")
 >  text(1.1,0,"100% loam")
 >  text(0.07,0.43,"percent clay")
 >  text(0.93,0.43,"percent silt")
 >  text(0.5,-0.1,"percent sand")
 >  if(soil.names) {
 >   # boundary of clay with extensions
 >   x1<-c(0.275,0.35,0.6)
 >   x2<-c(0.4,0.79,0.7)
 >   y1<-c(0.55*sin60,0.41*sin60,0.41*sin60)
 >   y2<-c(0.285*sin60,0.41*sin60,0.6*sin60)
 >   segments(x1,y1,x2,y2)
 >   # lower bound of clay loam & silty divider
 >   x1<-c(0.4,0.68)
 >   x2<-c(0.86,0.6)
 >   y1<-c(0.285*sin60,0.285*sin60)
 >   y2<-c(0.285*sin60,0.41*sin60)
 >   segments(x1,y1,x2,y2)
 >   x1<-c(0.185,0.1,0.37)
 >   x2<-c(0.36,0.37,0.4)
 >   y1<-c(0.37*sin60,0.2*sin60,0.2*sin60)
 >   y2<-c(0.37*sin60,0.2*sin60,0.285*sin60)
 >   segments(x1,y1,x2,y2)
 >   # sand corner
 >   x1<-c(0.05,0.075)
 >   x2<-c(0.12,0.3)
 >   y1<-c(0.1*sin60,0.15*sin60)
 >   y2<-c(0,0)
 >   segments(x1,y1,x2,y2)
 >   x1<-c(0.37,0.42,0.5,0.8,0.86)
 >   x2<-c(0.42,0.54,0.65,0.86,0.94)
 >   y1<-c(0.2*sin60,0.08*sin60,0,0,0.12*sin60)
 >   y2<-c(0.08*sin60,0.08*sin60,0.285*sin60,0.12*sin60,0.12*sin60)
 >   segments(x1,y1,x2,y2)
 >   text(0.5,0.57,"Clay")
 >   text(0.7,0.49*sin60,"Silty")
 >   text(0.7,0.44*sin60,"clay")
 >   text(0.73,0.37*sin60,"Silty clay")
 >   text(0.73,0.33*sin60,"loam")
 >   text(0.5,0.35*sin60,"Clay loam")
 >   text(0.27,0.43*sin60,"Sandy")
 >   text(0.27,0.39*sin60,"clay")
 >   text(0.27,0.3*sin60,"Sandy clay")
 >   text(0.27,0.26*sin60,"loam")
 >   text(0.25,0.13*sin60,"Sandy loam")
 >   text(0.13,0.075*sin60,"Loamy")
 >   text(0.15,0.035*sin60,"sand")
 >   text(0.055,0.021,"Sand")
 >   text(0.49,0.18*sin60,"Loam")
 >   text(0.72,0.15*sin60,"Silt loam")
 >   text(0.9,0.06*sin60,"Silt")
 >  }
 >  if(is.null(pch)) pch<-1:nrow(soilprop)
 >  if(is.null(col)) col<-2:(nrow(soilprop)+1)
 >  points(1-soilprop[,2]+(soilprop[,2]-(1-soilprop[,3]))*0.5,
 >   soilprop[,1]*sin60,pch=pch,col=col)
 >  if(show.legend) {
 >   samplenames<-rownames(soilprop)
 > 
legend(0,0.8+0.05*length(samplenames),legend=samplenames,pch=pch,col=col)
 >  }
 >  par(oldpar)
 > }



From martin.klaffenboeck at gmx.at  Sat May 28 15:23:12 2005
From: martin.klaffenboeck at gmx.at (Martin Klaffenboeck)
Date: Sat, 28 May 2005 13:23:12 +0000
Subject: [R] read.spss trouble
Message-ID: <1117286593.18296.7.camel@localhost>

Hello!

I'm not sure if this is an german list, so I will post in english.

I'm using R on my gentoo linux and now I got an spss .sav file.  I found
that r has a function like read.spss("file.sav") but when I try this I
get the Error: couldn't find function "read.spss".  Can anyone tell me
what's going wrong?

Thanks for your help,
Martin



From Roger.Bivand at nhh.no  Sat May 28 15:27:44 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 28 May 2005 15:27:44 +0200 (CEST)
Subject: [R] images and maps in R
In-Reply-To: <p06210219bebd4f9c5146@[128.115.153.6]>
Message-ID: <Pine.LNX.4.44.0505281516170.5023-100000@reclus.nhh.no>

On Fri, 27 May 2005, Don MacQueen wrote:

> This is is not difficult from the online help for the map() function. 
> Here is an example.
> 
> map('state', region = c('new york', 'new jersey', 'penn'),fill=TRUE,col=1:4)
> 
> There is also an example there in how to add text to the map.
> 
> Another way uses the maptools package.
> 
> require(maptools)
> ?plot.Map
> 
> and then follow the examples given in the help page for plot.Map().

Both for map() in the maps package and plot.Map() and plot.polylist() in 
the maptools package, there are three things to sort out. Firstly, are the 
map polygons the same, and in the same order as the data you want to map? 
Next, which palette of colours do you want to use (the RColorBrewer 
palettes are well-chosen)? And finally, which breakpoints will you use 
for the class intervals to divide the data into colours?

If the breakpoints are brks:

cols <- brewer.pal(length(brks)-1, "your choice of palette)
col_choices <- findInterval(data, brks, all.inside=TRUE)

and your data are the same length and in the same order as the polygons, 
thematic maps are not too difficult.

> 
> At 2:42 PM -0700 5/27/05, yyan liu wrote:
> >Hi:
> >   I have a question arising from my project.
> >   A sample of the data is below. The first row stands
> >for the names of state in USA. The second row stand
> >for some numeric value in that state. Some of them are
> >NA. I can use the commands "data(stateMapEnv)" and
> >"map('state', fill = F)" in library "maps" to make a
> >plot of USA states. What I want to do is: 1. put the
> >corresponding state name on the Map 2. give different
> >state different colors which is related to their
> >value. For example, "red" for values ranging from
> >0-30, "green" for values from 80-90, etc. 3. if
> >possible, put the value of each state within the state
> >on the map. here, take the numeric value as some text.
> >
> >Thank you very much!
> >
> >AB	AK	AL	AR	AZ  CT   CA
> >91	80	NA	NA	17  33   20
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Kevin.Wang at maths.anu.edu.au  Sat May 28 15:31:25 2005
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Sat, 28 May 2005 23:31:25 +1000
Subject: [R] read.spss trouble
In-Reply-To: <1117286593.18296.7.camel@localhost>
References: <1117286593.18296.7.camel@localhost>
Message-ID: <429872AD.3090505@maths.anu.edu.au>

Hi Martin,

Martin Klaffenboeck wrote:
> Hello!
> 
> I'm not sure if this is an german list, so I will post in english.

It's an English list.

> I'm using R on my gentoo linux and now I got an spss .sav file.  I found
> that r has a function like read.spss("file.sav") but when I try this I
> get the Error: couldn't find function "read.spss".  Can anyone tell me
> what's going wrong?

It's in the package "foreign".  Try:
   library(foreign)

HTH,

Kevin

-- 
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From MSchwartz at mn.rr.com  Sat May 28 15:35:52 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 28 May 2005 08:35:52 -0500
Subject: [R] read.spss trouble
In-Reply-To: <1117286593.18296.7.camel@localhost>
References: <1117286593.18296.7.camel@localhost>
Message-ID: <1117287352.22595.8.camel@horizons.localdomain>

On Sat, 2005-05-28 at 13:23 +0000, Martin Klaffenboeck wrote:
> Hello!
> 
> I'm not sure if this is an german list, so I will post in english.

The official language is English.

> I'm using R on my gentoo linux and now I got an spss .sav file.  I found
> that r has a function like read.spss("file.sav") but when I try this I
> get the Error: couldn't find function "read.spss".  Can anyone tell me
> what's going wrong?

read.spss() is part of the "foreign" package, which is installed as part
of the base R installation, but it is not loaded by default.

Thus, you need to use:

 library(foreign)
 read.spss(...)

See ?library and page 74 in "An Introduction to R".

HTH,

Marc Schwartz



From jsorkin at grecc.umaryland.edu  Sat May 28 16:52:20 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sat, 28 May 2005 10:52:20 -0400
Subject: [R] read.spss trouble
Message-ID: <s2984d83.039@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050528/5c2dabdb/attachment.pl

From Eric.Le.Bigot at spectro.jussieu.fr  Sat May 28 18:12:40 2005
From: Eric.Le.Bigot at spectro.jussieu.fr (Eric-Olivier Le Bigot)
Date: Sat, 28 May 2005 18:12:40 +0200 (CEST)
Subject: [R] Errors in Variables
In-Reply-To: <s29436f7.056@rauzen.rau.ac.za>
References: <s29436f7.056@rauzen.rau.ac.za>
Message-ID: <Pine.OSX.4.61.0505281805350.17155@kroll.local>

I'm interested in this "2D line fitting" too!  I've been looking, without 
success, in the list of R packages.

It might be possible to implement quite easily some of the formalism that you 
can find in Numerical Recipes (Fortran 77, 2nd ed.), paragraph 15.3.  As a 
matter of fact, I did this in R but only for a model of the form y ~ x (with 
a given covariance matrix between x and y).  I can send you the R code 
(preliminary version: I wrote it yesterday), if you want.

Another interesting reference might be Am. J. Phys. 60, p. 66 (1992).  But, 
again, you would have to implement things by yourself.

All the best,

EOL

--
Dr. Eric-Olivier LE BIGOT (EOL)                CNRS Associate Researcher
~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~o~o~~~
Kastler Brossel Laboratory (LKB)                   http://www.lkb.ens.fr
Universit? P. & M. Curie and Ecole Normale Sup?rieure, Case 74
4 place Jussieu              75252 Paris CEDEX 05                 France
~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~o~o~~~
office  : 01 44 27 73 67                             fax: 01 44 27 38 45
ECR room: 01 44 27 47 12                      x-ray room: 01 44 27 63 00
home: 01 73 74 61 87      For int'l calls: 33 + number without leading 0


On Wed, 25 May 2005, Jacob van Wyk wrote:

> I hope somebody can help.
> A student of mine is doing a study on Measurement Error models
> (errors-in-variables, total least squares, etc.). I have an old
> reference to a "multi archive"  that contains
> leiv3: Programs for best line fitting with errors in both coordinates.
> (The date is October 1989, by B.D. Ripley et al.)
> I have done a search for something similar in R withour success. Has
> this been implemented in a R-package, possibly under some sort of
> assumptions about variances. I would lke my student to apply some
> regression techniques to data that fit this profile.
> Any help is much appreciated.
> (If I have not done my search more carefully - my apologies.)
> Thanks
> Jacob
>
>
> Jacob L van Wyk
> Department of Mathematics and Statistics
> University of Johannesburg APK
> P O Box 524
> Auckland Park 2006
> South Africa
> Tel: +27-11-489-3080
> Fax: +27-11-489-2832
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

From ales.ziberna at guest.arnes.si  Sat May 28 19:07:56 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Sat, 28 May 2005 19:07:56 +0200
Subject: [R] Forcing ticks in plot for hclust object outside the limits
Message-ID: <004e01c563a7$d0bb6f00$598debd4@ales>

Hello!

I have the following problem.

I would like to plot the hclust object "hcd" (bellow, at the end of the 
mail) with ticks at seq(0.05,0.25,by=0.05). I tried using the code
plot(hcd)
and
plot(hcd,axes=FALSE)
axis(2,seq(0.05,0.25,by=0.05))

In both cases, the resoult is the same, ticks at 0.05 and 0.25 are not 
printed. I tried changing the ylim argumet in plot, however I got a warning 
"parameter "ylim" couldn't be set in high-level plot() function".

I would like to force those two ticks. Can this be done with axis or plot, 
or should I use "lines"?

Thanks in advance for any suggestions!
Ale? ?iberna


"hcd" <-
structure(list(merge = structure(as.integer(c(-4, -5, -1, -7,
-6, -10, -3, -12, -8, -2, 8, 1, -9, -11, 2, 3, -13, 4, 6, 5,
7, 9, 10, 11)), .Dim = as.integer(c(12, 2))), height = c(0.0906288626438465,
0.10278145998519, 0.114885217561497, 0.127812745765521, 0.132377986666522,
0.168594637784091, 0.177187802444346, 0.209803430657638, 0.210361529934791,
0.218946973173863, 0.234000873708654, 0.235702383243089), order = 
as.integer(c(4,
9, 12, 6, 13, 2, 8, 3, 10, 7, 1, 5, 11)), labels = NULL, method = "single",
    call = quote(hclust(d = d, method = "single")), dist.method = NULL), 
.Names = c("merge",
"height", "order", "labels", "method", "call", "dist.method"), class = 
"hclust")



From amsa36060 at yahoo.com  Sat May 28 19:48:09 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Sat, 28 May 2005 10:48:09 -0700 (PDT)
Subject: [R] Lag selection
Message-ID: <20050528174809.10745.qmail@web60416.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050528/2a838514/attachment.pl

From ligges at statistik.uni-dortmund.de  Sat May 28 19:58:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 28 May 2005 19:58:18 +0200
Subject: [R] Forcing ticks in plot for hclust object outside the limits
In-Reply-To: <004e01c563a7$d0bb6f00$598debd4@ales>
References: <004e01c563a7$d0bb6f00$598debd4@ales>
Message-ID: <4298B13A.50907@statistik.uni-dortmund.de>

Ale? ?iberna wrote:
> Hello!
> 
> I have the following problem.
> 
> I would like to plot the hclust object "hcd" (bellow, at the end of the 
> mail) with ticks at seq(0.05,0.25,by=0.05). I tried using the code
> plot(hcd)
> and
> plot(hcd,axes=FALSE)
> axis(2,seq(0.05,0.25,by=0.05))
> 
> In both cases, the resoult is the same, ticks at 0.05 and 0.25 are not 
> printed. I tried changing the ylim argumet in plot, however I got a 
> warning "parameter "ylim" couldn't be set in high-level plot() function".
> 
> I would like to force those two ticks. Can this be done with axis or 
> plot, or should I use "lines"?


Not easy with the plot method for a hclust object, because 0.05 and 0.25 
are getting clipped, because they are out od the "usr" coordinated of 
the plot.

Of course you could hack plot.hclust (in Namespace stats) by modifying 
.../R/library/src/stats/R/hclust.R as follows (in order to provide a 
proper ylim argument):

diff hclust.R-orig hclust.R

91c91,92
<               sub = NULL, xlab = NULL, ylab = "Height", ...)
---
 >               sub = NULL, xlab = NULL, ylab = "Height",
 >               ylim = NULL, ...)
116c117,120
<     .Internal(dend.window(n, merge, height, hang, labels, ...))
---
 >     height2 <- height
 >     if(!is.null(ylim))
 >         height2 <- c(range(ylim), rep(ylim[1], length(height) - 2))
 >     .Internal(dend.window(n, merge, height2, hang, labels, ...))


Note that this quick hack is a *dirty* solution.

Uwe Ligges




> Thanks in advance for any suggestions!
> Ale? ?iberna
> 
> 
> "hcd" <-
> structure(list(merge = structure(as.integer(c(-4, -5, -1, -7,
> -6, -10, -3, -12, -8, -2, 8, 1, -9, -11, 2, 3, -13, 4, 6, 5,
> 7, 9, 10, 11)), .Dim = as.integer(c(12, 2))), height = 
> c(0.0906288626438465,
> 0.10278145998519, 0.114885217561497, 0.127812745765521, 0.132377986666522,
> 0.168594637784091, 0.177187802444346, 0.209803430657638, 0.210361529934791,
> 0.218946973173863, 0.234000873708654, 0.235702383243089), order = 
> as.integer(c(4,
> 9, 12, 6, 13, 2, 8, 3, 10, 7, 1, 5, 11)), labels = NULL, method = "single",
>    call = quote(hclust(d = d, method = "single")), dist.method = NULL), 
> .Names = c("merge",
> "height", "order", "labels", "method", "call", "dist.method"), class = 
> "hclust")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andreas.zankl at gmail.com  Sat May 28 10:56:03 2005
From: andreas.zankl at gmail.com (Andreas Zankl)
Date: Sat, 28 May 2005 10:56:03 +0200
Subject: [R] how to make legends on histograms
Message-ID: <a06210200bebddf9b3756@[10.0.1.4]>

I have a histogram with histograms for several datasets superimposed. 
How can I add a legend that indicates which dataset uses which 
linetype?

Thanks
Andreas

-- 

------------------------------
Dr. med. Andreas Zankl
Division de Pediatrie Moleculaire
Clinique Infantile 02/50
CHUV
Avenue Pierre Decker 2
CH-1011 Lausanne
Suisse
Tel.: +41-21-3143778
Fax: +41-21-3143546
Email: andreas.zankl at hospvd.ch



From slist at oomvanlieshout.net  Sat May 28 21:51:46 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Sat, 28 May 2005 21:51:46 +0200
Subject: [R] Soil texture triangle in R?
In-Reply-To: <4298610D.7000901@oomvanlieshout.net>
References: <4296D592.5040108@oomvanlieshout.net>	<42972186.50404@oomvanlieshout.net>	<42974BD9.6080609@oomvanlieshout.net>	<429782B9.5060102@oomvanlieshout.net>	<42985CE3.7080709@oomvanlieshout.net>
	<4298610D.7000901@oomvanlieshout.net>
Message-ID: <4298CBD2.4090009@oomvanlieshout.net>

Dear R users,

Please find attached a new plot function, plot.soiltexture, to plot soil 
texture data on a triangular plot with an optional backdrop of the USDA 
soil texture classification, written by Jim Lemon and me.

I tried to write the function and documentation confirm the R 
conventions. However, this is a new experience for me, so any comments 
and suggestions are welcome!

I tried to find a suitable package for the plot function, but none are 
obvious.

I have approached Todd Skaggs to ask permission to include sample data 
from the paper:
Skaggs, T.H., L.M. Arya, P.J. Shouse, and B.P. Mohanty. 2001. Estimating 
particle-size distribution from limited soil texture data. Soil Sci. 
Soc. Am. J., 65:1038-1044.
http://soil.scijournals.org/cgi/content/full/65/4/1038

Things still to do:
- rotate axis labels;
- rotate axis tick labels
- provide option to plot ticks inside or outside plot area.

Thus making it look like:
http://soils.usda.gov/technical/manual/images/fig3-16_large.jpg
or
http://soil.scijournals.org/content/vol65/issue4/images/large/1038f2.jpeg

Enjoy,

Sander.
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: plot.soiltexture.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050528/5e09fa86/plot.soiltexture.pl

From kjetil at acelerate.com  Sat May 28 21:54:08 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sat, 28 May 2005 15:54:08 -0400
Subject: [R] how to make legends on histograms
In-Reply-To: <a06210200bebddf9b3756@[10.0.1.4]>
References: <a06210200bebddf9b3756@[10.0.1.4]>
Message-ID: <4298CC60.30409@acelerate.com>

Andreas Zankl wrote:

> I have a histogram with histograms for several datasets superimposed. 
> How can I add a legend that indicates which dataset uses which linetype?
>
> Thanks
> Andreas
>
?legend
?locator

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From spencer.graves at pdf.com  Sat May 28 23:47:19 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 29 May 2005 06:47:19 +0900
Subject: [R] Errors in Variables
In-Reply-To: <Pine.OSX.4.61.0505281805350.17155@kroll.local>
References: <s29436f7.056@rauzen.rau.ac.za>
	<Pine.OSX.4.61.0505281805350.17155@kroll.local>
Message-ID: <4298E6E7.9020105@pdf.com>

	  I'm sorry, I have not followed this thread, but I wonder if you have 
considered library(sem), "structural equations modeling"?  "Errors in 
variables" problems are the canonical special case.

	  Also, have you done a search of "www.r-project.org" -> search -> "R 
site search" for terms like "errors in variables regression"?  This just 
led me to "ODRpack", which is NOT a CRAN package but is apparently 
available after a Google search.  If it were my problem, I'd first try 
to figure out "sem";  if that seemed too difficult, I might then look at 
"ODRpack".

	  Also, have you read the posting guide! 
http://www.R-project.org/posting-guide.html?  This suggests, among other 
things, that you provide a toy example that a potential respondant could 
easily copy from your email, test a few modifications, and prase a reply 
in a minute or so.  This also helps clarify your question so any 
respondants are more likely to suggest something that is actually useful 
to you.  Moreover, many people have reported that they were able to 
answer their own question in the course of preparing a question for this 
list using the posting guide.

	  hope this helps.  spencer graves

Eric-Olivier Le Bigot wrote:

> I'm interested in this "2D line fitting" too!  I've been looking, 
> without success, in the list of R packages.
> 
> It might be possible to implement quite easily some of the formalism 
> that you can find in Numerical Recipes (Fortran 77, 2nd ed.), paragraph 
> 15.3.  As a matter of fact, I did this in R but only for a model of the 
> form y ~ x (with a given covariance matrix between x and y).  I can send 
> you the R code (preliminary version: I wrote it yesterday), if you want.
> 
> Another interesting reference might be Am. J. Phys. 60, p. 66 (1992).  
> But, again, you would have to implement things by yourself.
> 
> All the best,
> 
> EOL
> 
> -- 
> Dr. Eric-Olivier LE BIGOT (EOL)                CNRS Associate Researcher
> ~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~o~o~~~
> Kastler Brossel Laboratory (LKB)                   http://www.lkb.ens.fr
> Universit?? P. & M. Curie and Ecole Normale Sup??rieure, Case 74
> 4 place Jussieu              75252 Paris CEDEX 05                 France
> ~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~o~o~~~
> office  : 01 44 27 73 67                             fax: 01 44 27 38 45
> ECR room: 01 44 27 47 12                      x-ray room: 01 44 27 63 00
> home: 01 73 74 61 87      For int'l calls: 33 + number without leading 0
> 
> 
> On Wed, 25 May 2005, Jacob van Wyk wrote:
> 
>> I hope somebody can help.
>> A student of mine is doing a study on Measurement Error models
>> (errors-in-variables, total least squares, etc.). I have an old
>> reference to a "multi archive"  that contains
>> leiv3: Programs for best line fitting with errors in both coordinates.
>> (The date is October 1989, by B.D. Ripley et al.)
>> I have done a search for something similar in R withour success. Has
>> this been implemented in a R-package, possibly under some sort of
>> assumptions about variances. I would lke my student to apply some
>> regression techniques to data that fit this profile.
>> Any help is much appreciated.
>> (If I have not done my search more carefully - my apologies.)
>> Thanks
>> Jacob
>>
>>
>> Jacob L van Wyk
>> Department of Mathematics and Statistics
>> University of Johannesburg APK
>> P O Box 524
>> Auckland Park 2006
>> South Africa
>> Tel: +27-11-489-3080
>> Fax: +27-11-489-2832
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
>>
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Sat May 28 23:55:25 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 29 May 2005 06:55:25 +0900
Subject: [R] Lag selection
In-Reply-To: <20050528174809.10745.qmail@web60416.mail.yahoo.com>
References: <20050528174809.10745.qmail@web60416.mail.yahoo.com>
Message-ID: <4298E8CD.5000102@pdf.com>

	  What kind of problem?  For one variable or more?

	  What have you tried?  For only a single time series, the standard 
approach that I learned from Box and Jenkins, Time Series Analysis, 
Forecasting and Control, starts by preparing both acf and pacf, both of 
which are functions in R.  If the acf shows only 1, 2 or 3 significant 
spikes, it suggests a moving average only model of the indicated order. 
  If the pacf shows only 1, 2, or 3 significant spikes, that suggests an 
autogregression only model.  If the acf shows very high autoregressions 
of all orders with very slow decay, it suggests at least one difference. 
  If both acf and pacf show reasonable, possibly oscillating decay with 
no clear cut-off, then I'd first try an ARMA(1,1), then look at the 
residuals and expand the model as needed.

	  Finally, have you read the posting guide! 
http://www.R-project.org/posting-guide.html?  This suggests, among other
things, that you provide a toy example that a potential respondant could
easily copy from your email, test a few modifications, and prase a reply
in a minute or so.  This also helps clarify your question so any
respondants are more likely to suggest something that is actually useful
to you.  Moreover, many people have reported that they were able to
answer their own question in the course of preparing a question for this
list using the posting guide.

	  hope this helps.
	  spencer graves

Amir Safari wrote:

>  
>  
>  Dear All ,
> Is it possible to find and select the best lags for time series  in R? ( Lag Selection Problem )
> Could you please introduce a package or function for this ?
> Thanks a lot
>  
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Sun May 29 00:46:41 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sat, 28 May 2005 18:46:41 -0400
Subject: [R] Incompatibility with VGAM
Message-ID: <4298F4D1.4080303@acelerate.com>

I just discovered that when the VGAM package (not on CRAN) is loaded,
glm() doesn't work. This is because VGAM defines a family function() 
which gets found
by glm() in place of the family function from stats.
Then VGAM:::family returns an object which doesn't have a $family 
component, (it has a component
$vfamily).

I thought namespaces should protect us from this happening?

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From jfox at mcmaster.ca  Sun May 29 15:03:29 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 29 May 2005 09:03:29 -0400
Subject: [R] Errors in Variables
In-Reply-To: <4298E6E7.9020105@pdf.com>
Message-ID: <20050529130328.XHED26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Spencer,

The reason that I didn't respond to the original posting (I'm the author of
the sem package), that that without additional information (such as the
error variance of x), a model with error in both x and y will be
underidentified (unless there are multiple indicators of x, which didn't
seem to be the case here). I figured that what Jacob had in mind was
something like minimizing the least (orthogonal) distance of the points to
the regression line (implying by the way that x and y are on the same scale
or somehow standardized), which isn't doable with sem as far as I'm aware.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Saturday, May 28, 2005 4:47 PM
> To: Eric-Olivier Le Bigot
> Cc: r-help at stat.math.ethz.ch; Jacob van Wyk
> Subject: Re: [R] Errors in Variables
> 
> 	  I'm sorry, I have not followed this thread, but I 
> wonder if you have considered library(sem), "structural 
> equations modeling"?  "Errors in variables" problems are the 
> canonical special case.
> 
> 	  Also, have you done a search of "www.r-project.org" 
> -> search -> "R site search" for terms like "errors in 
> variables regression"?  This just led me to "ODRpack", which 
> is NOT a CRAN package but is apparently available after a 
> Google search.  If it were my problem, I'd first try to 
> figure out "sem";  if that seemed too difficult, I might then 
> look at "ODRpack".
> 
> 	  Also, have you read the posting guide! 
> http://www.R-project.org/posting-guide.html?  This suggests, 
> among other things, that you provide a toy example that a 
> potential respondant could easily copy from your email, test 
> a few modifications, and prase a reply in a minute or so.  
> This also helps clarify your question so any respondants are 
> more likely to suggest something that is actually useful to 
> you.  Moreover, many people have reported that they were able 
> to answer their own question in the course of preparing a 
> question for this list using the posting guide.
> 
> 	  hope this helps.  spencer graves
> 
> Eric-Olivier Le Bigot wrote:
> 
> > I'm interested in this "2D line fitting" too!  I've been looking, 
> > without success, in the list of R packages.
> > 
> > It might be possible to implement quite easily some of the 
> formalism 
> > that you can find in Numerical Recipes (Fortran 77, 2nd ed.), 
> > paragraph 15.3.  As a matter of fact, I did this in R but 
> only for a 
> > model of the form y ~ x (with a given covariance matrix 
> between x and 
> > y).  I can send you the R code (preliminary version: I 
> wrote it yesterday), if you want.
> > 
> > Another interesting reference might be Am. J. Phys. 60, p. 
> 66 (1992).  
> > But, again, you would have to implement things by yourself.
> > 
> > All the best,
> > 
> > EOL
> > 
> > -- 
> > Dr. Eric-Olivier LE BIGOT (EOL)                CNRS 
> Associate Researcher
> > 
> ~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> ~~~~o~o~~~
> > Kastler Brossel Laboratory (LKB)                   
> http://www.lkb.ens.fr
> > Universit?? P. & M. Curie and Ecole Normale Sup??rieure, Case 74
> > 4 place Jussieu              75252 Paris CEDEX 05           
>       France
> > 
> ~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> ~~~~o~o~~~
> > office  : 01 44 27 73 67                             fax: 
> 01 44 27 38 45
> > ECR room: 01 44 27 47 12                      x-ray room: 
> 01 44 27 63 00
> > home: 01 73 74 61 87      For int'l calls: 33 + number 
> without leading 0
> > 
> > 
> > On Wed, 25 May 2005, Jacob van Wyk wrote:
> > 
> >> I hope somebody can help.
> >> A student of mine is doing a study on Measurement Error models 
> >> (errors-in-variables, total least squares, etc.). I have an old 
> >> reference to a "multi archive"  that contains
> >> leiv3: Programs for best line fitting with errors in both 
> coordinates.
> >> (The date is October 1989, by B.D. Ripley et al.) I have done a 
> >> search for something similar in R withour success. Has this been 
> >> implemented in a R-package, possibly under some sort of 
> assumptions 
> >> about variances. I would lke my student to apply some regression 
> >> techniques to data that fit this profile.
> >> Any help is much appreciated.
> >> (If I have not done my search more carefully - my 
> apologies.) Thanks 
> >> Jacob
> >>
> >>
> >> Jacob L van Wyk
> >> Department of Mathematics and Statistics University of 
> Johannesburg 
> >> APK P O Box 524 Auckland Park 2006 South Africa
> >> Tel: +27-11-489-3080
> >> Fax: +27-11-489-2832
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list 
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >>
> > 
> >>
> > 
> > 
> ----------------------------------------------------------------------
> > --
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun May 29 15:21:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 29 May 2005 15:21:42 +0200
Subject: [R] Incompatibility with VGAM
In-Reply-To: <4298F4D1.4080303@acelerate.com>
References: <4298F4D1.4080303@acelerate.com>
Message-ID: <4299C1E6.2020009@statistik.uni-dortmund.de>

Kjetil Brinchmann Halvorsen wrote:
> I just discovered that when the VGAM package (not on CRAN) is loaded,
> glm() doesn't work. This is because VGAM defines a family function() 
> which gets found
> by glm() in place of the family function from stats.
> Then VGAM:::family returns an object which doesn't have a $family 
> component, (it has a component
> $vfamily).
> 
> I thought namespaces should protect us from this happening?


Yes and no:

Yes: The namespace would protect you if glm would call internally the 
family function such as poisson().

No: Probably you have asked something like glm(....., family=poisson()). 
Now the first instance of poisson() in the search path is used. You are 
passing a call to poisson(), and this is not bound to any namespace. Of 
course you can specify tha namespace such as
glm(....., family=stats::poisson()).

Uwe Ligges



> Kjetil
>



From ggrothendieck at gmail.com  Sun May 29 15:23:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 29 May 2005 09:23:55 -0400
Subject: [R] Incompatibility with VGAM
In-Reply-To: <4298F4D1.4080303@acelerate.com>
References: <4298F4D1.4080303@acelerate.com>
Message-ID: <971536df050529062373f22de3@mail.gmail.com>

As a workaround you could try detaching and re-attaching vgam 
before and after running glm.

detach("package:vgam")
glm(...whatever...)
library(vgam)

On 5/28/05, Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> wrote:
> I just discovered that when the VGAM package (not on CRAN) is loaded,
> glm() doesn't work. This is because VGAM defines a family function()
> which gets found
> by glm() in place of the family function from stats.
> Then VGAM:::family returns an object which doesn't have a $family
> component, (it has a component
> $vfamily).
> 
> I thought namespaces should protect us from this happening?
> 
> Kjetil
> 
> --
> 
> Kjetil Halvorsen.
> 
> Peace is the most effective weapon of mass construction.
>               --  Mahdi Elmandjra
> 
> 
> 
> 
> 
> --
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From hellik at web.de  Sun May 29 18:07:42 2005
From: hellik at web.de (Helmut Kudrnovsky)
Date: Sun, 29 May 2005 18:07:42 +0200
Subject: [R] "text"-function: adding text in an x,y-plot
Message-ID: <6.2.1.2.0.20050529180249.0367c3b0@pop3.web.de>

Hello R-friends,

i have a question to the "text"-function.

a little test-dataset for better understanding:

-the dataset was imported with read.table(....,header=TRUE)
s1-s10 are the samplenames

     var1 var2 var3
s1     1    1    2
s2     2    3    1
s3     2    2    3
s4     5    4    3
s5     4    2    3
s6     6    3    2
s7     8    5    4
s8     7    2    1
s9     9    3    2
s10    8    6    4

-then i??ve performed a pincipal component analysis with prcomp

-for displaying the result in a x,y-graph i transformed out.pca$x into the 
dataframe points
  points <- out.pca$x
 > points
            PC1         PC2        PC3
s1  -4.7055777 -0.14781544 -0.3683602
s2  -3.1854599  0.19661476  1.5212455
s3  -3.2687980  0.78193513 -0.6352458
s4   0.2278948  1.00061498  0.2164108
s5  -1.4382847  0.02500633 -0.9114340
s6   0.6216283 -0.68606440  0.2071083
s7   3.4951878  1.17343675 -0.3266629
s8   1.0153619 -2.37274378  0.1978058
s9   3.3673983 -1.82145761 -0.2071739
s10  3.8706492  1.85047328  0.3063065

- with plot(points$PC1, points$PC2, type="n") i start a graph without 
displaying anything at the x,y-coordinates

- i want now to display the samplenames at the right x,y-coordinate;  i 
generated a dataframe called lab with the samplenames

 > lab
     V1
1   s1
2   s2
3   s3
4   s4
5   s5
6   s6
7   s7
8   s8
9   s9
10 s10

- i??ve studied the "text"-helppage and so i tried the following:

text(pca$PC1, pca$PC2, labels=lab)

- in the plot there is now c(1,3,4,5,6,7,8,9,10,2) displayed instead of s1-s10

is out there any idea what i'am doing wrong? is there maybe another way of 
displaying the samplenames at the right x,y-coordinate?

greetings from the sunny tirol with thanks in advance
helli

platform i386-pc-mingw32
arch     i386
os       mingw32 - win xp
system   i386, mingw32
status
major    2
minor    1.0
year     2005
month    04
day      18
language R



From ligges at statistik.uni-dortmund.de  Sun May 29 18:15:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 29 May 2005 18:15:47 +0200
Subject: [R] "text"-function: adding text in an x,y-plot
In-Reply-To: <6.2.1.2.0.20050529180249.0367c3b0@pop3.web.de>
References: <6.2.1.2.0.20050529180249.0367c3b0@pop3.web.de>
Message-ID: <4299EAB3.3030400@statistik.uni-dortmund.de>

Helmut Kudrnovsky wrote:

> Hello R-friends,
> 
> i have a question to the "text"-function.
> 
> a little test-dataset for better understanding:
> 
> -the dataset was imported with read.table(....,header=TRUE)
> s1-s10 are the samplenames
> 
>     var1 var2 var3
> s1     1    1    2
> s2     2    3    1
> s3     2    2    3
> s4     5    4    3
> s5     4    2    3
> s6     6    3    2
> s7     8    5    4
> s8     7    2    1
> s9     9    3    2
> s10    8    6    4
> 
> -then i??ve performed a pincipal component analysis with prcomp
> 
> -for displaying the result in a x,y-graph i transformed out.pca$x into 
> the dataframe points
>  points <- out.pca$x
>  > points
>            PC1         PC2        PC3
> s1  -4.7055777 -0.14781544 -0.3683602
> s2  -3.1854599  0.19661476  1.5212455
> s3  -3.2687980  0.78193513 -0.6352458
> s4   0.2278948  1.00061498  0.2164108
> s5  -1.4382847  0.02500633 -0.9114340
> s6   0.6216283 -0.68606440  0.2071083
> s7   3.4951878  1.17343675 -0.3266629
> s8   1.0153619 -2.37274378  0.1978058
> s9   3.3673983 -1.82145761 -0.2071739
> s10  3.8706492  1.85047328  0.3063065
> 
> - with plot(points$PC1, points$PC2, type="n") i start a graph without 
> displaying anything at the x,y-coordinates

I think points should be a matrix rather than a list or data.frame, 
hence the code above won't work. Instead:


  plot(points[,"PC1"], points[,"PC2"], type="n")


> - i want now to display the samplenames at the right x,y-coordinate;  i 
> generated a dataframe called lab with the samplenames
> 
>  > lab
>     V1
> 1   s1
> 2   s2
> 3   s3
> 4   s4
> 5   s5
> 6   s6
> 7   s7
> 8   s8
> 9   s9
> 10 s10
> 
> - i??ve studied the "text"-helppage and so i tried the following:
> text(pca$PC1, pca$PC2, labels=lab)

Hmm, what is pca ???


  lab <- rownames(points)
  text(points[,"PC1"], points[,"PC2"], labels=lab)

works for me.


> - in the plot there is now c(1,3,4,5,6,7,8,9,10,2) displayed instead of 
> s1-s10

Quite probably you lab was a *factor* ...


> is out there any idea what i'am doing wrong? is there maybe another way 
> of displaying the samplenames at the right x,y-coordinate?

We cannot say what you did wrong when constructing your plot, but we 
know that your descibtion above is wrong, because it does not even work 
to generate the empty plot the way ...

Please read the posting guide and rethink at least one time whether the 
code you are giving us is reproducible. Your code was not.

Uwe Ligges



> greetings from the sunny tirol with thanks in advance
> helli
> 
> platform i386-pc-mingw32
> arch     i386
> os       mingw32 - win xp
> system   i386, mingw32
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Sun May 29 18:15:57 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 29 May 2005 12:15:57 -0400
Subject: [R] "text"-function: adding text in an x,y-plot
In-Reply-To: <6.2.1.2.0.20050529180249.0367c3b0@pop3.web.de>
Message-ID: <20050529161556.VFUH16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Helmut,

The problem is that you're implicitly coercing the entire data frame lab to
character, producing

> as.character(lab)
[1] "c(1, 3, 4, 5, 6, 7, 8, 9, 10, 2)"

The numbers themselves come from the numeric coding of the factor V1 in this
data frame; as.numeric(lab$V1) shows you what's going on.

Instead you could specify labels=lab$V1 in the text() command. But why put
the row names in a data frame in the first place; why not simply use
labels=rownames(out.pca)?

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Helmut 
> Kudrnovsky
> Sent: Sunday, May 29, 2005 11:08 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] "text"-function: adding text in an x,y-plot
> 
> Hello R-friends,
> 
> i have a question to the "text"-function.
> 
> a little test-dataset for better understanding:
> 
> -the dataset was imported with read.table(....,header=TRUE) 
> s1-s10 are the samplenames
> 
>      var1 var2 var3
> s1     1    1    2
> s2     2    3    1
> s3     2    2    3
> s4     5    4    3
> s5     4    2    3
> s6     6    3    2
> s7     8    5    4
> s8     7    2    1
> s9     9    3    2
> s10    8    6    4
> 
> -then i??ve performed a pincipal component analysis with prcomp
> 
> -for displaying the result in a x,y-graph i transformed 
> out.pca$x into the dataframe points
>   points <- out.pca$x
>  > points
>             PC1         PC2        PC3
> s1  -4.7055777 -0.14781544 -0.3683602
> s2  -3.1854599  0.19661476  1.5212455
> s3  -3.2687980  0.78193513 -0.6352458
> s4   0.2278948  1.00061498  0.2164108
> s5  -1.4382847  0.02500633 -0.9114340
> s6   0.6216283 -0.68606440  0.2071083
> s7   3.4951878  1.17343675 -0.3266629
> s8   1.0153619 -2.37274378  0.1978058
> s9   3.3673983 -1.82145761 -0.2071739
> s10  3.8706492  1.85047328  0.3063065
> 
> - with plot(points$PC1, points$PC2, type="n") i start a graph 
> without displaying anything at the x,y-coordinates
> 
> - i want now to display the samplenames at the right 
> x,y-coordinate;  i generated a dataframe called lab with the 
> samplenames
> 
>  > lab
>      V1
> 1   s1
> 2   s2
> 3   s3
> 4   s4
> 5   s5
> 6   s6
> 7   s7
> 8   s8
> 9   s9
> 10 s10
> 
> - i??ve studied the "text"-helppage and so i tried the following:
> 
> text(pca$PC1, pca$PC2, labels=lab)
> 
> - in the plot there is now c(1,3,4,5,6,7,8,9,10,2) displayed 
> instead of s1-s10
> 
> is out there any idea what i'am doing wrong? is there maybe 
> another way of displaying the samplenames at the right x,y-coordinate?
> 
> greetings from the sunny tirol with thanks in advance helli
> 
> platform i386-pc-mingw32
> arch     i386
> os       mingw32 - win xp
> system   i386, mingw32
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From rwcitek at alum.calberkeley.org  Sun May 29 19:13:48 2005
From: rwcitek at alum.calberkeley.org (Robert Citek)
Date: Sun, 29 May 2005 12:13:48 -0500
Subject: [R] R GUI for Linux?
Message-ID: <AF296FB1-8C37-496D-83AA-643093716811@alum.calberkeley.org>


Hello all,

I noticed that both Windows and OS X version of R have a GUI  
(Rconsole).  Is there a GUI for Linux?  I'm running Debian on which  
the CLI for R works just fine.

Regards,
- Robert



From slist at oomvanlieshout.net  Sun May 29 20:24:27 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Sun, 29 May 2005 20:24:27 +0200
Subject: [R] R GUI for Linux?
In-Reply-To: <AF296FB1-8C37-496D-83AA-643093716811@alum.calberkeley.org>
References: <AF296FB1-8C37-496D-83AA-643093716811@alum.calberkeley.org>
Message-ID: <429A08DB.9020901@oomvanlieshout.net>

HI Robert,

Of course Linux already has a console! Just type R in the Terminal 
console and R will start (assuming all is installed correctly). Graphics 
will be launched in separate windows.

If you want more then the Terminal console, try:

http://www.sciviews.org/_rgui/

Good luck,

Sander.

Robert Citek wrote:
> 
> Hello all,
> 
> I noticed that both Windows and OS X version of R have a GUI  
> (Rconsole).  Is there a GUI for Linux?  I'm running Debian on which  the 
> CLI for R works just fine.
> 
> Regards,
> - Robert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From ales.ziberna at guest.arnes.si  Sun May 29 20:39:35 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Sun, 29 May 2005 20:39:35 +0200
Subject: [R] Forcing ticks in plot for hclust object outside the limits
References: <004e01c563a7$d0bb6f00$598debd4@ales>
	<4298B13A.50907@statistik.uni-dortmund.de>
Message-ID: <001701c5647d$c3b58530$598debd4@ales>

Thank you very much for your reply and your *dirty* sloution, which does the 
trick for me!

Ale? ?iberna


----- Original Message ----- 
From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
To: "Ale? ?iberna" <ales.ziberna at guest.arnes.si>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Saturday, May 28, 2005 7:58 PM
Subject: Re: [R] Forcing ticks in plot for hclust object outside the limits


> Ale? ?iberna wrote:
>> Hello!
>>
>> I have the following problem.
>>
>> I would like to plot the hclust object "hcd" (bellow, at the end of the 
>> mail) with ticks at seq(0.05,0.25,by=0.05). I tried using the code
>> plot(hcd)
>> and
>> plot(hcd,axes=FALSE)
>> axis(2,seq(0.05,0.25,by=0.05))
>>
>> In both cases, the resoult is the same, ticks at 0.05 and 0.25 are not 
>> printed. I tried changing the ylim argumet in plot, however I got a 
>> warning "parameter "ylim" couldn't be set in high-level plot() function".
>>
>> I would like to force those two ticks. Can this be done with axis or 
>> plot, or should I use "lines"?
>
>
> Not easy with the plot method for a hclust object, because 0.05 and 0.25 
> are getting clipped, because they are out od the "usr" coordinated of the 
> plot.
>
> Of course you could hack plot.hclust (in Namespace stats) by modifying 
> .../R/library/src/stats/R/hclust.R as follows (in order to provide a 
> proper ylim argument):
>
> diff hclust.R-orig hclust.R
>
> 91c91,92
> <               sub = NULL, xlab = NULL, ylab = "Height", ...)
> ---
> >               sub = NULL, xlab = NULL, ylab = "Height",
> >               ylim = NULL, ...)
> 116c117,120
> <     .Internal(dend.window(n, merge, height, hang, labels, ...))
> ---
> >     height2 <- height
> >     if(!is.null(ylim))
> >         height2 <- c(range(ylim), rep(ylim[1], length(height) - 2))
> >     .Internal(dend.window(n, merge, height2, hang, labels, ...))
>
>
> Note that this quick hack is a *dirty* solution.
>
> Uwe Ligges
>
>
>
>
>> Thanks in advance for any suggestions!
>> Ale? ?iberna
>>
>>
>> "hcd" <-
>> structure(list(merge = structure(as.integer(c(-4, -5, -1, -7,
>> -6, -10, -3, -12, -8, -2, 8, 1, -9, -11, 2, 3, -13, 4, 6, 5,
>> 7, 9, 10, 11)), .Dim = as.integer(c(12, 2))), height = 
>> c(0.0906288626438465,
>> 0.10278145998519, 0.114885217561497, 0.127812745765521, 
>> 0.132377986666522,
>> 0.168594637784091, 0.177187802444346, 0.209803430657638, 
>> 0.210361529934791,
>> 0.218946973173863, 0.234000873708654, 0.235702383243089), order = 
>> as.integer(c(4,
>> 9, 12, 6, 13, 2, 8, 3, 10, 7, 1, 5, 11)), labels = NULL, method = 
>> "single",
>>    call = quote(hclust(d = d, method = "single")), dist.method = NULL), 
>> .Names = c("merge",
>> "height", "order", "labels", "method", "call", "dist.method"), class = 
>> "hclust")
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
>



From edd at debian.org  Sun May 29 20:41:39 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 29 May 2005 13:41:39 -0500
Subject: [R] R GUI for Linux?
In-Reply-To: <AF296FB1-8C37-496D-83AA-643093716811@alum.calberkeley.org>
References: <AF296FB1-8C37-496D-83AA-643093716811@alum.calberkeley.org>
Message-ID: <17050.3299.658067.353057@basebud.nulle.part>


On 29 May 2005 at 12:13, Robert Citek wrote:
| 
| Hello all,
| 
| I noticed that both Windows and OS X version of R have a GUI  
| (Rconsole).  Is there a GUI for Linux?  I'm running Debian on which  
| the CLI for R works just fine.


$ apt-get install r-gnome     ## works up until R 2.0.1
$ R --gui=gnome &

That said, as of R 2.1.0, the Gnome UI is no longer shipped with the R
sources. Hence, I no longer provide it in the r-gnome package.  However, the
code is now provided in an add-one package 'gnomeGUI' on CRAN which you could
install directly.

Hope this helps, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From ggrothendieck at gmail.com  Sun May 29 20:56:23 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 29 May 2005 14:56:23 -0400
Subject: [R] R GUI for Linux?
In-Reply-To: <429A08DB.9020901@oomvanlieshout.net>
References: <AF296FB1-8C37-496D-83AA-643093716811@alum.calberkeley.org>
	<429A08DB.9020901@oomvanlieshout.net>
Message-ID: <971536df05052911566900a55@mail.gmail.com>

Check out the JGR link on the web page mentioned by Sander below.

On 5/29/05, Sander Oom <slist at oomvanlieshout.net> wrote:
> HI Robert,
> 
> Of course Linux already has a console! Just type R in the Terminal
> console and R will start (assuming all is installed correctly). Graphics
> will be launched in separate windows.
> 
> If you want more then the Terminal console, try:
> 
> http://www.sciviews.org/_rgui/
> 
> Good luck,
> 
> Sander.
> 
> Robert Citek wrote:
> >
> > Hello all,
> >
> > I noticed that both Windows and OS X version of R have a GUI
> > (Rconsole).  Is there a GUI for Linux?  I'm running Debian on which  the
> > CLI for R works just fine.
> >
> > Regards,
> > - Robert
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> 
> --
> --------------------------------------------
> Dr Sander P. Oom
> Animal, Plant and Environmental Sciences,
> University of the Witwatersrand
> Private Bag 3, Wits 2050, South Africa
> Tel (work)      +27 (0)11 717 64 04
> Tel (home)      +27 (0)18 297 44 51
> Fax             +27 (0)18 299 24 64
> Email   sander at oomvanlieshout.net
> Web     www.oomvanlieshout.net/sander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jyzz88 at gmail.com  Sun May 29 21:15:02 2005
From: jyzz88 at gmail.com (Luke)
Date: Sun, 29 May 2005 15:15:02 -0400
Subject: [R] get the bug context
Message-ID: <27583b4005052912155651fa5e@mail.gmail.com>

Hi R-users,

How to get a bug contex? 

My R code ran there for several hours, but a bug crashed it, printing
such message like "Error: subscript out of bounds". I want to use
browser() to catch the bug, but I don't know which loop caused the bug
(there are many loops in the code). I even don't know which line of
code caused the bug. Is there any utility or something in R which can
let you know which line of code causes a bug?

Thanks,

-Luke



From ligges at statistik.uni-dortmund.de  Sun May 29 22:04:08 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 29 May 2005 22:04:08 +0200
Subject: [R] get the bug context
In-Reply-To: <27583b4005052912155651fa5e@mail.gmail.com>
References: <27583b4005052912155651fa5e@mail.gmail.com>
Message-ID: <429A2038.90304@statistik.uni-dortmund.de>

Luke wrote:
> Hi R-users,
> 
> How to get a bug contex? 
> 
> My R code ran there for several hours, but a bug crashed it, printing
> such message like "Error: subscript out of bounds". I want to use
> browser() to catch the bug, but I don't know which loop caused the bug
> (there are many loops in the code). I even don't know which line of
> code caused the bug. Is there any utility or something in R which can
> let you know which line of code causes a bug?

No, but ...

Have you tried traceback() to see which function is the culprit?

You can also set options(error = recover) to look at the objects at the 
time the error occured. Maybe you see which object is already defined 
and can derive the point of the error that way.

Uwe Ligges



> Thanks,
> 
> -Luke
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Sun May 29 23:12:53 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 30 May 2005 06:12:53 +0900
Subject: [R] Errors in Variables
In-Reply-To: <20050529130328.XHED26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <20050529130328.XHED26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <429A3055.7070305@pdf.com>

Hi, John:

	  Thanks for the clarification.  I know that the "errors in X problem" 
requires additional information, most commonly one of the variances or 
the correlation.  The question I saw (below) indicated he had tried 
"model of the form y ~ x (with a given covariance matrix ...)", which 
made me think of "sem".

	  If he wants "the least (orthogonal) distance", could he could get it 
indirectly from "sem" by calling "sem" repeatedly giving, say, a 
variance for "x", then averaging the variances of "x" and "y" and trying 
that in "sem"?

	  Also, what do you know about "ODRpack"?  It looks like that might 
solve "the least (orthogonal) distance".

	  Thanks again for your note, John.
	  Best Wishes,
	  Spencer Graves	

John Fox wrote:

> Dear Spencer,
> 
> The reason that I didn't respond to the original posting (I'm the author of
> the sem package), that that without additional information (such as the
> error variance of x), a model with error in both x and y will be
> underidentified (unless there are multiple indicators of x, which didn't
> seem to be the case here). I figured that what Jacob had in mind was
> something like minimizing the least (orthogonal) distance of the points to
> the regression line (implying by the way that x and y are on the same scale
> or somehow standardized), which isn't doable with sem as far as I'm aware.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>>Sent: Saturday, May 28, 2005 4:47 PM
>>To: Eric-Olivier Le Bigot
>>Cc: r-help at stat.math.ethz.ch; Jacob van Wyk
>>Subject: Re: [R] Errors in Variables
>>
>>	  I'm sorry, I have not followed this thread, but I 
>>wonder if you have considered library(sem), "structural 
>>equations modeling"?  "Errors in variables" problems are the 
>>canonical special case.
>>
>>	  Also, have you done a search of "www.r-project.org" 
>>-> search -> "R site search" for terms like "errors in 
>>variables regression"?  This just led me to "ODRpack", which 
>>is NOT a CRAN package but is apparently available after a 
>>Google search.  If it were my problem, I'd first try to 
>>figure out "sem";  if that seemed too difficult, I might then 
>>look at "ODRpack".
>>
>>	  Also, have you read the posting guide! 
>>http://www.R-project.org/posting-guide.html?  This suggests, 
>>among other things, that you provide a toy example that a 
>>potential respondant could easily copy from your email, test 
>>a few modifications, and prase a reply in a minute or so.  
>>This also helps clarify your question so any respondants are 
>>more likely to suggest something that is actually useful to 
>>you.  Moreover, many people have reported that they were able 
>>to answer their own question in the course of preparing a 
>>question for this list using the posting guide.
>>
>>	  hope this helps.  spencer graves
>>
>>Eric-Olivier Le Bigot wrote:
>>
>>
>>>I'm interested in this "2D line fitting" too!  I've been looking, 
>>>without success, in the list of R packages.
>>>
>>>It might be possible to implement quite easily some of the 
>>
>>formalism 
>>
>>>that you can find in Numerical Recipes (Fortran 77, 2nd ed.), 
>>>paragraph 15.3.  As a matter of fact, I did this in R but 
>>
>>only for a 
>>
>>>model of the form y ~ x (with a given covariance matrix 
>>
>>between x and 
>>
>>>y).  I can send you the R code (preliminary version: I 
>>
>>wrote it yesterday), if you want.
>>
>>>Another interesting reference might be Am. J. Phys. 60, p. 
>>
>>66 (1992).  
>>
>>>But, again, you would have to implement things by yourself.
>>>
>>>All the best,
>>>
>>>EOL
>>>
>>>-- 
>>>Dr. Eric-Olivier LE BIGOT (EOL)                CNRS 
>>
>>Associate Researcher
>>
>>~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>~~~~o~o~~~
>>
>>>Kastler Brossel Laboratory (LKB)                   
>>
>>http://www.lkb.ens.fr
>>
>>>Universit?? P. & M. Curie and Ecole Normale Sup??rieure, Case 74
>>>4 place Jussieu              75252 Paris CEDEX 05           
>>
>>      France
>>
>>~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>~~~~o~o~~~
>>
>>>office  : 01 44 27 73 67                             fax: 
>>
>>01 44 27 38 45
>>
>>>ECR room: 01 44 27 47 12                      x-ray room: 
>>
>>01 44 27 63 00
>>
>>>home: 01 73 74 61 87      For int'l calls: 33 + number 
>>
>>without leading 0
>>
>>>
>>>On Wed, 25 May 2005, Jacob van Wyk wrote:
>>>
>>>
>>>>I hope somebody can help.
>>>>A student of mine is doing a study on Measurement Error models 
>>>>(errors-in-variables, total least squares, etc.). I have an old 
>>>>reference to a "multi archive"  that contains
>>>>leiv3: Programs for best line fitting with errors in both 
>>
>>coordinates.
>>
>>>>(The date is October 1989, by B.D. Ripley et al.) I have done a 
>>>>search for something similar in R withour success. Has this been 
>>>>implemented in a R-package, possibly under some sort of 
>>
>>assumptions 
>>
>>>>about variances. I would lke my student to apply some regression 
>>>>techniques to data that fit this profile.
>>>>Any help is much appreciated.
>>>>(If I have not done my search more carefully - my 
>>
>>apologies.) Thanks 
>>
>>>>Jacob
>>>>
>>>>
>>>>Jacob L van Wyk
>>>>Department of Mathematics and Statistics University of 
>>
>>Johannesburg 
>>
>>>>APK P O Box 524 Auckland Park 2006 South Africa
>>>>Tel: +27-11-489-3080
>>>>Fax: +27-11-489-2832
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list 
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>----------------------------------------------------------------------
>>
>>>--
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
> 
>



From jfox at mcmaster.ca  Sun May 29 23:56:10 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 29 May 2005 17:56:10 -0400
Subject: [R] Errors in Variables
In-Reply-To: <429A3055.7070305@pdf.com>
Message-ID: <20050529215610.JINS25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Spencer,

> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Sent: Sunday, May 29, 2005 4:13 PM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch; 'Jacob van Wyk'; 'Eric-Olivier Le Bigot'
> Subject: Re: [R] Errors in Variables
> 
> Hi, John:
> 
> 	  Thanks for the clarification.  I know that the 
> "errors in X problem" 
> requires additional information, most commonly one of the 
> variances or the correlation.  The question I saw (below) 
> indicated he had tried "model of the form y ~ x (with a given 
> covariance matrix ...)", which made me think of "sem".
> 
> 	  If he wants "the least (orthogonal) distance", could 
> he could get it indirectly from "sem" by calling "sem" 
> repeatedly giving, say, a variance for "x", then averaging 
> the variances of "x" and "y" and trying that in "sem"?
> 

I'm not sure how that would work, but seems similar to averaging the
regressions of y on x and x on y.

> 	  Also, what do you know about "ODRpack"?  It looks 
> like that might solve "the least (orthogonal) distance".
> 

I'm not familiar with ODRpack, but it seems to me that one could fairly
simply minimize the sum of squared least distances using, e.g., optim.

Regards,
 John

> 	  Thanks again for your note, John.
> 	  Best Wishes,
> 	  Spencer Graves	
> 
> John Fox wrote:
> 
> > Dear Spencer,
> > 
> > The reason that I didn't respond to the original posting (I'm the 
> > author of the sem package), that that without additional 
> information 
> > (such as the error variance of x), a model with error in 
> both x and y 
> > will be underidentified (unless there are multiple indicators of x, 
> > which didn't seem to be the case here). I figured that what 
> Jacob had 
> > in mind was something like minimizing the least 
> (orthogonal) distance 
> > of the points to the regression line (implying by the way 
> that x and y 
> > are on the same scale or somehow standardized), which isn't 
> doable with sem as far as I'm aware.
> > 
> > Regards,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > 
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Spencer Graves
> >>Sent: Saturday, May 28, 2005 4:47 PM
> >>To: Eric-Olivier Le Bigot
> >>Cc: r-help at stat.math.ethz.ch; Jacob van Wyk
> >>Subject: Re: [R] Errors in Variables
> >>
> >>	  I'm sorry, I have not followed this thread, but I 
> wonder if you 
> >>have considered library(sem), "structural equations modeling"?  
> >>"Errors in variables" problems are the canonical special case.
> >>
> >>	  Also, have you done a search of "www.r-project.org" 
> >>-> search -> "R site search" for terms like "errors in
> >>variables regression"?  This just led me to "ODRpack", 
> which is NOT a 
> >>CRAN package but is apparently available after a Google 
> search.  If it 
> >>were my problem, I'd first try to figure out "sem";  if that seemed 
> >>too difficult, I might then look at "ODRpack".
> >>
> >>	  Also, have you read the posting guide! 
> >>http://www.R-project.org/posting-guide.html?  This suggests, among 
> >>other things, that you provide a toy example that a potential 
> >>respondant could easily copy from your email, test a few 
> >>modifications, and prase a reply in a minute or so.
> >>This also helps clarify your question so any respondants are more 
> >>likely to suggest something that is actually useful to you. 
>  Moreover, 
> >>many people have reported that they were able to answer their own 
> >>question in the course of preparing a question for this 
> list using the 
> >>posting guide.
> >>
> >>	  hope this helps.  spencer graves
> >>
> >>Eric-Olivier Le Bigot wrote:
> >>
> >>
> >>>I'm interested in this "2D line fitting" too!  I've been looking, 
> >>>without success, in the list of R packages.
> >>>
> >>>It might be possible to implement quite easily some of the
> >>
> >>formalism
> >>
> >>>that you can find in Numerical Recipes (Fortran 77, 2nd ed.), 
> >>>paragraph 15.3.  As a matter of fact, I did this in R but
> >>
> >>only for a
> >>
> >>>model of the form y ~ x (with a given covariance matrix
> >>
> >>between x and
> >>
> >>>y).  I can send you the R code (preliminary version: I
> >>
> >>wrote it yesterday), if you want.
> >>
> >>>Another interesting reference might be Am. J. Phys. 60, p. 
> >>
> >>66 (1992).  
> >>
> >>>But, again, you would have to implement things by yourself.
> >>>
> >>>All the best,
> >>>
> >>>EOL
> >>>
> >>>-- 
> >>>Dr. Eric-Olivier LE BIGOT (EOL)                CNRS 
> >>
> >>Associate Researcher
> >>
> >>~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> >>~~~~o~o~~~
> >>
> >>>Kastler Brossel Laboratory (LKB)                   
> >>
> >>http://www.lkb.ens.fr
> >>
> >>>Universit?? P. & M. Curie and Ecole Normale Sup??rieure, Case 74
> >>>4 place Jussieu              75252 Paris CEDEX 05           
> >>
> >>      France
> >>
> >>~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> >>~~~~o~o~~~
> >>
> >>>office  : 01 44 27 73 67                             fax: 
> >>
> >>01 44 27 38 45
> >>
> >>>ECR room: 01 44 27 47 12                      x-ray room: 
> >>
> >>01 44 27 63 00
> >>
> >>>home: 01 73 74 61 87      For int'l calls: 33 + number 
> >>
> >>without leading 0
> >>
> >>>
> >>>On Wed, 25 May 2005, Jacob van Wyk wrote:
> >>>
> >>>
> >>>>I hope somebody can help.
> >>>>A student of mine is doing a study on Measurement Error models 
> >>>>(errors-in-variables, total least squares, etc.). I have an old 
> >>>>reference to a "multi archive"  that contains
> >>>>leiv3: Programs for best line fitting with errors in both
> >>
> >>coordinates.
> >>
> >>>>(The date is October 1989, by B.D. Ripley et al.) I have done a 
> >>>>search for something similar in R withour success. Has this been 
> >>>>implemented in a R-package, possibly under some sort of
> >>
> >>assumptions
> >>
> >>>>about variances. I would lke my student to apply some regression 
> >>>>techniques to data that fit this profile.
> >>>>Any help is much appreciated.
> >>>>(If I have not done my search more carefully - my
> >>
> >>apologies.) Thanks
> >>
> >>>>Jacob
> >>>>
> >>>>
> >>>>Jacob L van Wyk
> >>>>Department of Mathematics and Statistics University of
> >>
> >>Johannesburg
> >>
> >>>>APK P O Box 524 Auckland Park 2006 South Africa
> >>>>Tel: +27-11-489-3080
> >>>>Fax: +27-11-489-2832
> >>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list 
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide! 
> >>>>http://www.R-project.org/posting-guide.html
> >>>>
> >>>
> >>>
> >>------------------------------------------------------------
> ----------
> >>
> >>>--
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list 
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! 
> >>>http://www.R-project.org/posting-guide.html
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> > 
> >



From brett at hbrc.govt.nz  Mon May 30 00:01:06 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Mon, 30 May 2005 10:01:06 +1200
Subject: [R] joining files after canonical correlation
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA9868B355@MSX2>

Dear R,
I recently did a canonical correlation analysis on two subsets of data
(location and weather). So I now have canonical scores for location and
weather. but I'd now like to do a scatterplot matrix using the pairs
statement. 

Is there a way to somehow join location.U and weather.V to become a new data
set from which I could undertake a scatterplot matrix of the canonical
variates?

brett


Brett Stansfield 
Environmental Scientist - Water Quality 
Hawke's Bay Regional Council 
102 Vautier Street 
Private Bag 6006 
Napier 
Phone (06) 835-9200 extn 9334 
Fax (06) 835-3601



From t.yee at auckland.ac.nz  Mon May 30 00:26:13 2005
From: t.yee at auckland.ac.nz (Thomas Yee)
Date: Mon, 30 May 2005 10:26:13 +1200
Subject: [R] Incompatibility with VGAM
Message-ID: <429A4185.9060009@auckland.ac.nz>

Hello,
 

It seems that if glm used a namespace then the conflict would be avoided?
 

FYI, I released VGAM 0.6-3 last Friday which has binomial(), poisson()
etc. removed so that it should no longer conflict with glm().  The
families that work under VGAM are called binomialff(), poissonff() etc.
Also, the Windows zip file for VGAM 0.6-3 was built under R 2.1.0.
 

cheers
 

Thomas



From rob.dunne at gmail.com  Mon May 30 00:35:38 2005
From: rob.dunne at gmail.com (Rob Dunne)
Date: Mon, 30 May 2005 08:35:38 +1000
Subject: [R] spatially constrained clustering
Message-ID: <d7dfv3$1d1$1@sea.gmane.org>

Hi List,
         does anyone know of an implementation of spatially constrained 
clustering in R?

This is where there is a vector of measurements for points on a plane 
and only neighbors can be clustered together. I have tried implementint 
in myself -- but if someone has alkready done it !

I have searched on the obvios terms "spatially constrained clustering" 
without any luck.


Bye
Rob



From yongfei.wang at yale.edu  Mon May 30 00:57:06 2005
From: yongfei.wang at yale.edu (Yongfei Wang)
Date: Sun, 29 May 2005 18:57:06 -0400
Subject: [R] Nomogram
In-Reply-To: <d7dfv3$1d1$1@sea.gmane.org>
References: <d7dfv3$1d1$1@sea.gmane.org>
Message-ID: <429A48C2.2070309@yale.edu>

Hi, List:

I heard some where about "nomogram" function in R, which package include 
this function?

Thanks
Yongfei

>



From spencer.graves at pdf.com  Mon May 30 01:15:12 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 30 May 2005 08:15:12 +0900
Subject: [R] Nomogram
In-Reply-To: <429A48C2.2070309@yale.edu>
References: <d7dfv3$1d1$1@sea.gmane.org> <429A48C2.2070309@yale.edu>
Message-ID: <429A4D00.5030807@pdf.com>

	  From www.r-project.org -> search -> "R site search", I got 40 hits 
for "nomogram".  The first one was for a function "nomogram" in the 
"Design" library.  I haven't used it, but other people have, and some of 
the other 39 hits might answer other questions you might have in trying 
to use it.

	  hope this helps.
	  spencer graves
p.s.  Have you read the posting guide! 
"http://www.R-project.org/posting-guide.html"?  It has helped others (a) 
find answers to their own questions, and (b) improve the chances that 
they get useful replies when they do ask questions.

Yongfei Wang wrote:

> Hi, List:
> 
> I heard some where about "nomogram" function in R, which package include 
> this function?
> 
> Thanks
> Yongfei
> 
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From talih at xena.hunter.cuny.edu  Mon May 30 02:17:13 2005
From: talih at xena.hunter.cuny.edu (Makram Talih)
Date: Sun, 29 May 2005 20:17:13 -0400 (EDT)
Subject: [R] trouble with cumsum?
Message-ID: <Pine.GSO.4.58.0505292007250.27862@xena.hunter.cuny.edu>

Dear R users,

I am using R version 2.0.1 (2004/11/15) on an i386-pc-mingw32 platform. I
encounter the following problem while using cumsum:

> a <- rep(0.01, 100)
> b <- cumsum(a)
> sum(a) == 1
[1] TRUE
> b[100] == 1
[1] FALSE

Am I missing something? Should cumsum have such an outcome?

Thanks in advance for any clarifications any of you can offer.

Regards,

Makram Talih

--
Makram Talih, Ph.D.
Assistant Professor
Department of Mathematics and Statistics
Hunter College of the City University of New York
695 Park Avenue, Room 905 HE
New York, NY 10021

Website: http://stat.hunter.cuny.edu/talih
E-mail: talih at math.hunter.cuny.edu
Tel: 212-772-5308
Fax: 212-772-4858



From rob.dunne at gmail.com  Mon May 30 02:43:46 2005
From: rob.dunne at gmail.com (Rob Dunne)
Date: Mon, 30 May 2005 10:43:46 +1000
Subject: [R] Nomogram
In-Reply-To: <429A4D00.5030807@pdf.com>
References: <d7dfv3$1d1$1@sea.gmane.org> <429A48C2.2070309@yale.edu>
	<429A4D00.5030807@pdf.com>
Message-ID: <d7dnfb$g29$1@sea.gmane.org>

Spencer Graves wrote:
>       From www.r-project.org -> search -> "R site search", I got 40 hits 
> for "nomogram".  The first one was for a function "nomogram" in the 
> "Design" library.  I haven't used it, but other people have, and some of 
> the other 39 hits might answer other questions you might have in trying 
> to use it.
> 
>       hope this helps.
>       spencer graves
> p.s.  Have you read the posting guide! 
> "http://www.R-project.org/posting-guide.html"?  It has helped others (a) 
> find answers to their own questions, and (b) improve the chances that 
> they get useful replies when they do ask questions.
>

Hi list,
            yes I read the posting guide and looked on "R site search" 
and searched on "spatially constrained clustering"

I didnt think to search on "nomogram" which appears to be a totally 
different thing. See

http://en.wikipedia.org/wiki/Nomogram


Bye
Rob



From edd at debian.org  Mon May 30 02:46:23 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 29 May 2005 19:46:23 -0500
Subject: [R] trouble with cumsum?
In-Reply-To: <Pine.GSO.4.58.0505292007250.27862@xena.hunter.cuny.edu>
References: <Pine.GSO.4.58.0505292007250.27862@xena.hunter.cuny.edu>
Message-ID: <17050.25183.775285.601147@basebud.nulle.part>


On 29 May 2005 at 20:17, Makram Talih wrote:
| Dear R users,
| 
| I am using R version 2.0.1 (2004/11/15) on an i386-pc-mingw32 platform. I
| encounter the following problem while using cumsum:
| 
| > a <- rep(0.01, 100)
| > b <- cumsum(a)
| > sum(a) == 1
| [1] TRUE
| > b[100] == 1
| [1] FALSE
| 
| Am I missing something? 

Yes, FAQ section 7.31 entitled "Why doesn't R think these numbers are equal?".

| Should cumsum have such an outcome?

> a <- rep(0.01, 100)
> b <- cumsum(a)
> sum(a) == 1
[1] FALSE
> all.equal(sum(a), 1)
[1] TRUE
> all.equal(b[100], 1)
[1] TRUE
> 

On my Linux system with R 2.1.0, the first equality fails whereas it "worked"
for you. Morale: don't forget about floating-point math ...

Hth, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From luan_sheng at yahoo.com.cn  Mon May 30 03:26:14 2005
From: luan_sheng at yahoo.com.cn (luan_sheng)
Date: Mon, 30 May 2005 09:26:14 +0800
Subject: [R] trouble with cumsum?
In-Reply-To: <Pine.GSO.4.58.0505292007250.27862@xena.hunter.cuny.edu>
Message-ID: <200505300126.j4U1QUxl021121@hypatia.math.ethz.ch>

 R help document showed us that you should not use "==" for comparing
objects, and you will use "all.equal".

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Makram Talih
Sent: Monday, May 30, 2005 8:17 AM
To: r-help at stat.math.ethz.ch
Subject: [R] trouble with cumsum?

Dear R users,

I am using R version 2.0.1 (2004/11/15) on an i386-pc-mingw32 platform. I
encounter the following problem while using cumsum:

> a <- rep(0.01, 100)
> b <- cumsum(a)
> sum(a) == 1
[1] TRUE
> b[100] == 1
[1] FALSE

Am I missing something? Should cumsum have such an outcome?

Thanks in advance for any clarifications any of you can offer.

Regards,

Makram Talih

--
Makram Talih, Ph.D.
Assistant Professor
Department of Mathematics and Statistics Hunter College of the City
University of New York
695 Park Avenue, Room 905 HE
New York, NY 10021

Website: http://stat.hunter.cuny.edu/talih
E-mail: talih at math.hunter.cuny.edu
Tel: 212-772-5308
Fax: 212-772-4858

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

__________________________________________________

Å—Å≈ÅªÅ¢Å√Å‚Å∑Å—GÅ”Å ÅœÅ‰Å£Å≠Å÷Å–ÅπÅ˙ÅµÅ⁄Å“ÅªÅæÅ¯ÅŒÅﬁÅ¿Å¨ÅªÅ¯Å”Å ÅºÅ˛Å…ÅßÅ»Å≈Å≥Å¨Å¥ÅÛÅ”Å ÅœÅ‰



From Tom.Mulholland at dpi.wa.gov.au  Mon May 30 04:31:12 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Mon, 30 May 2005 10:31:12 +0800
Subject: [R] Nomogram
Message-ID: <4702645135092E4497088F71D9C8F51A128B80@afhex01.dpi.wa.gov.au>

I don't think Spencer replied to your message. It wasn't addressed to you. He was replying to a specific post on nomograms. However you might try being less specific as a search on "spatial cluster" gave back 

[R] mclust - clustering by spatial patterns http://finzi.psych.upenn.edu/R/Rhelp02a/archive/13474.html

[R] Spatial Clustering http://finzi.psych.upenn.edu/R/Rhelp02a/archive/19480.html amongst others.

I also did a help.search("spatial cluster") which came up with references to the package DCluster


Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Rob Dunne
> Sent: Monday, 30 May 2005 8:44 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Nomogram
> 
> 
> Spencer Graves wrote:
> >       From www.r-project.org -> search -> "R site search", 
> I got 40 hits 
> > for "nomogram".  The first one was for a function "nomogram" in the 
> > "Design" library.  I haven't used it, but other people 
> have, and some of 
> > the other 39 hits might answer other questions you might 
> have in trying 
> > to use it.
> > 
> >       hope this helps.
> >       spencer graves
> > p.s.  Have you read the posting guide! 
> > "http://www.R-project.org/posting-guide.html"?  It has 
> helped others (a) 
> > find answers to their own questions, and (b) improve the 
> chances that 
> > they get useful replies when they do ask questions.
> >
> 
> Hi list,
>             yes I read the posting guide and looked on "R 
> site search" 
> and searched on "spatially constrained clustering"
> 
> I didnt think to search on "nomogram" which appears to be a totally 
> different thing. See
> 
> http://en.wikipedia.org/wiki/Nomogram
> 
> 
> Bye
> Rob
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mcclatchie.sam at saugov.sa.gov.au  Mon May 30 04:48:11 2005
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Mon, 30 May 2005 12:18:11 +0930
Subject: [R] sapply following using by with a list of factors
Message-ID: <BEA6A7E18959A04385DC14D24619F89F123C74@sagemsg0008.sagemsmrd01.sa.gov.au>

Background:
OS: Linux Mandrake 10.1
release: R 2.0.0
editor: GNU Emacs 21.3.2
front-end: ESS 5.2.3
---------------------------------
Colleagues

I am having some trouble extracting results from the function by, used to
average variables in a data.frame first by one factor (depth) and then by a
second factor (station). The real data.frame is quite large 
> dim(data.2001)
[1] 32049  11

Here is a snippet of code:

## bin density data for each station into 1 m depth bins, containing means
    data.2001.test$integer.Depth <- as.factor(round(data.2001.test$Depth,
digits=0))
    attach(data.2001.test)
    binned.data.2001 <- by(data.2001.test[,5:11], list(depth=integer.Depth,
station=Station), mean)

and here is a snippet of the data.frame

> dim(data.2001.test)
[1] 150  11
> dump("data.2001.test", file=stdout())
data.2001.test <-
structure(list(Cruise = structure(as.integer(c(1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)), .Label =
"Ngerin01", class = "factor"), 
    Station = structure(as.integer(c(2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 
    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 
    4, 4, 4, 4, 4, 4, 4, 5, 5)), .Label = c("a1", "a2", "a3", 
    "a4", "a5", "a6", "a7", "a8", "a9", "b1", "b2", "b3", "b4", 
    "b5", "b6", "b7", "c1", "c2", "c3", "c4", "c5", "c6", "c7", 
    "d1", "d2", "d3", "d4", "d5", "d6", "e1", "e2", "e3", "e4", 
    "e5", "e6", "e7", "f1", "f2", "f3", "f4", "f5", "f6", "f7", 
    "f8", "f9", "g1", "g10", "g11", "g2", "g3", "g4", "g5", "g6", 
    "g7", "g8", "g9", "gsvc1", "gsvc2", "gsvc3", "gsvc4", "gsvc5", 
    "gsvc6", "gsvc7", "gsvc8", "gsvc9", "gsvd1", "gsvd2", "gsvd3", 
    "gsvd4", "gsvd5", "gsvd6", "gsvd7", "gsvd8", "h1", "h11", 
    "h2", "h3", "h4", "h5", "h6", "h7", "h8", "h9", "i1", "i10", 
    "i2", "i3", "i4", "i5", "i6", "i7", "i8", "i9", "j1", "j10", 
    "j2", "j3", "j4", "j5", "j6", "j7", "j8", "j9", "k1", "k10", 
    "k2", "k3", "k4", "k5", "k6", "k7", "k8", "k9", "l1", "l10", 
    "l11", "l12", "l2", "l3", "l4", "l5", "l6", "l7", "l8", "l9", 
    "m1", "m10", "m11", "m2", "m3", "m4", "m5", "m6", "m7", "m8", 
    "m9", "mx", "n1", "n10", "n11", "n2", "n3", "n4", "n5", "n6", 
    "n7", "n8", "n9", "nx", "o1", "o10", "o11", "o13", "o2", 
    "o3", "o4", "o5", "o6", "o7", "o8", "o9", "ox", "p10", "p11", 
    "p2", "p3", "p4", "p5", "p6", "p7", "p8", "p9", "px", "q1", 
    "q10", "q11", "q12", "q13", "q2", "q3", "q4", "q5", "q6", 
    "q7", "q8", "q9", "qx", "r1", "r10", "r11", "r12", "r13", 
    "r14", "r15", "r2", "r3", "r4", "r5", "r6", "r7", "r8", "r9", 
    "rx", "s1", "s10", "s11", "s12", "s13", "s14", "s15", "s16", 
    "s2", "s3", "s4", "s5", "s6", "s7", "s8", "s9", "sgc1", "sgc2", 
    "sgc3", "sgc4", "sgc5", "sgc6", "sgc7", "sx", "t1", "t10", 
    "t11", "t12", "t13", "t14", "t15", "t16", "t2", "t3", "t4", 
    "t5", "t6", "t7", "t8", "t9", "tx", "u1", "u10", "u11", "u12", 
    "u13", "u14", "u15", "u2", "u3", "u4", "u5", "u6", "u7", 
    "u8", "u9", "ux", "v1", "v10", "v11", "v2", "v3", "v4", "v5", 
    "v6", "v7", "v8", "v9", "vx", "w2", "w3", "w4", "w5", "w6", 
    "w7", "w8", "w9", "wx", "x2", "x3", "x4", "x5", "x6", "x7", 
    "x8"), class = "factor"), Lon = c(138.421, 138.421, 138.421, 
    138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
    138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
    138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
    138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
    138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
    138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
    138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
    138.421, 138.421, 138.352, 138.352, 138.352, 138.352, 138.352, 
    138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
    138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
    138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
    138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
    138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
    138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
    138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.277, 
    138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 
    138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 
    138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 
    138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 
    138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 
    138.277, 138.277, 138.277, 138.277, 138.277, 138.201, 138.201
    ), Lat = c(-35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
    -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
    -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
    -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
    -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
    -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
    -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
    -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.827, 
    -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
    -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
    -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
    -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
    -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
    -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
    -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
    -35.827, -35.827, -35.827, -35.883, -35.883, -35.883, -35.883, 
    -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, 
    -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, 
    -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, 
    -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, 
    -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, 
    -35.883, -35.883, -35.95, -35.95), Depth = c(5.092, 5.289, 
    5.584, 5.78, 6.075, 6.37, 6.665, 7.156, 7.451, 7.845, 8.238, 
    8.533, 8.729, 8.926, 9.123, 9.319, 9.614, 10.106, 10.499, 
    10.991, 11.58, 12.17, 12.76, 13.35, 13.94, 14.53, 15.119, 
    15.709, 16.201, 16.594, 16.987, 17.282, 17.577, 18.069, 18.364, 
    18.659, 18.954, 19.15, 19.347, 19.543, 19.838, 20.232, 20.723, 
    21.215, 21.706, 22.394, 22.788, 23.082, 23.377, 23.574, 23.869, 
    24.262, 24.557, 24.95, 5.19, 5.584, 6.075, 6.567, 7.058, 
    7.55, 7.845, 8.14, 8.336, 8.729, 9.024, 9.516, 10.007, 10.597, 
    11.089, 11.777, 12.367, 12.858, 13.252, 13.645, 14.038, 14.333, 
    14.726, 15.021, 15.414, 16.004, 16.397, 16.987, 17.381, 17.872, 
    18.167, 18.462, 18.954, 19.445, 19.838, 20.428, 20.92, 21.51, 
    22.099, 22.591, 23.181, 23.574, 23.869, 23.869, 23.869, 23.967, 
    23.967, 24.066, 24.066, 24.262, 24.262, 24.36, 24.36, 5.387, 
    5.78, 5.977, 6.173, 6.468, 6.763, 7.156, 7.55, 8.041, 8.434, 
    9.024, 9.418, 9.811, 10.007, 10.302, 10.597, 10.991, 11.482, 
    11.875, 12.367, 12.858, 13.252, 13.645, 13.94, 14.333, 14.726, 
    15.218, 15.808, 16.397, 16.987, 17.479, 18.069, 18.659, 19.445, 
    19.937, 20.723, 21.215, 21.804, 22.198, 22.591, 22.984, 5.485, 
    5.977), Temperature.oC = c(19.743, 19.7421, 19.7396, 19.7354, 
    19.7162, 19.6801, 19.6181, 19.5434, 19.403, 19.2851, 19.2514, 
    19.2278, 19.2303, 19.2379, 19.248, 19.2227, 19.205, 19.2, 
    19.1941, 19.1907, 19.1856, 19.1772, 19.1662, 19.1561, 19.1493, 
    19.1434, 19.1383, 19.1341, 19.1282, 19.1172, 19.1096, 19.1113, 
    19.1189, 19.1451, 19.2152, 19.2641, 19.259, 19.2405, 19.2278, 
    19.2244, 19.2017, 19.1772, 19.1527, 19.0868, 18.6986, 18.2519, 
    17.9853, 17.871, 17.8237, 17.8366, 17.7143, 17.4717, 17.36, 
    17.3071, 19.6151, 19.6135, 19.6126, 19.6134, 19.6126, 19.6118, 
    19.6118, 19.6109, 19.6109, 19.6109, 19.6109, 19.6109, 19.6101, 
    19.6084, 19.6092, 19.61, 19.6092, 19.6075, 19.605, 19.5974, 
    19.5916, 19.5832, 19.5765, 19.5706, 19.5597, 19.5328, 19.5042, 
    19.4084, 19.2998, 19.1665, 18.9958, 18.8331, 18.6955, 18.6207, 
    18.5594, 18.4802, 18.4008, 18.3214, 18.2265, 18.1631, 18.1426, 
    18.1126, 18.1049, 18.062, 18.098, 18.11, 18.0286, 18.1117, 
    18.0989, 18.0989, 18.0114, 18.1152, 17.9513, 19.3769, 19.371, 
    19.3718, 19.3685, 19.344, 19.3137, 19.2269, 19.1036, 19.0013, 
    18.9488, 18.9082, 18.8573, 18.8268, 18.8157, 18.8124, 18.7996, 
    18.7674, 18.7198, 18.679, 18.6535, 18.6416, 18.6305, 18.611, 
    18.5922, 18.5514, 18.5139, 18.4934, 18.4841, 18.4968, 18.49, 
    18.4644, 18.4534, 18.4465, 18.4414, 18.4389, 18.4295, 18.4346, 
    18.4337, 18.4269, 18.4064, 18.4022, 19.3895, 19.3912), Salinity =
c(35.166, 
    35.1667, 35.2001, 35.2246, 35.2577, 35.2959, 35.3443, 35.3771, 
    35.4354, 35.5, 35.4849, 35.5368, 35.532, 35.5466, 35.5127, 
    35.5236, 35.5294, 35.5402, 35.5438, 35.5451, 35.5453, 35.5417, 
    35.5442, 35.5473, 35.5476, 35.5484, 35.5526, 35.5453, 35.5409, 
    35.5409, 35.5565, 35.567, 35.5962, 35.7026, 35.7273, 35.6732, 
    35.6655, 35.6442, 35.6696, 35.6644, 35.66, 35.6623, 35.6751, 
    35.3696, 35.2176, 35.4344, 35.5435, 35.6172, 35.6572, 35.5419, 
    35.3845, 35.5031, 35.6039, 35.5771, 35.5487, 35.5421, 35.5584, 
    35.5746, 35.5923, 35.6046, 35.5229, 35.592, 35.5998, 35.626, 
    35.6298, 35.6309, 35.6341, 35.6432, 35.6463, 35.6466, 35.6458, 
    35.647, 35.649, 35.6514, 35.6496, 35.654, 35.6491, 35.6579, 
    35.6381, 35.645, 35.5544, 35.5725, 35.5076, 35.4342, 35.4509, 
    35.4715, 35.5495, 35.5413, 35.582, 35.5519, 35.5219, 35.4919, 
    35.5158, 35.5611, 35.553, 35.5694, 35.5706, 35.5304, 35.5371, 
    35.5565, 35.5527, 35.5686, 35.5947, 35.5688, 35.5104, 35.5478, 
    35.5164, 35.4598, 35.5322, 35.5115, 35.5196, 35.5457, 35.598, 
    35.651, 35.6238, 35.5903, 35.5912, 35.5766, 35.5761, 35.5781, 
    35.5903, 35.5944, 35.5731, 35.5405, 35.5652, 35.5627, 35.5872, 
    35.5879, 35.5772, 35.5738, 35.5589, 35.5497, 35.5779, 35.5766, 
    35.6047, 35.5866, 35.5559, 35.5738, 35.5778, 35.5808, 35.585, 
    35.5708, 35.5894, 35.5848, 35.5732, 35.5587, 35.5776, 35.5798, 
    35.5151, 35.5029), Fluoresence.Volts = c(0.6947, 0.6789, 
    0.6923, 0.6935, 0.6996, 0.7045, 0.6825, 0.6911, 0.6886, 0.685, 
    0.6801, 0.6874, 0.7009, 0.6996, 0.6935, 0.6899, 0.7045, 0.7106, 
    0.7082, 0.7021, 0.7009, 0.7375, 0.7302, 0.7253, 0.7436, 0.7692, 
    0.7741, 0.7814, 0.7619, 0.7851, 0.7961, 0.7998, 0.7851, 0.7863, 
    0.7998, 0.823, 0.8388, 0.8462, 0.8718, 0.8864, 0.895, 0.873, 
    0.8632, 0.9035, 0.9744, 1.0049, 1.0537, 1.1026, 1.1258, 1.1258, 
    1.1013, 1.0952, 1.072, 1.0488, 0.7265, 0.7241, 0.7143, 0.7326, 
    0.7497, 0.7546, 0.7582, 0.7546, 0.7741, 0.7766, 0.7741, 0.7595, 
    0.7546, 0.7827, 0.7839, 0.7729, 0.7705, 0.7985, 0.8071, 0.8254, 
    0.8144, 0.8144, 0.8181, 0.8217, 0.8083, 0.8339, 0.8376, 0.8437, 
    0.8791, 0.8913, 0.9096, 0.9133, 0.9267, 0.928, 0.9158, 0.9609, 
    0.956, 0.9328, 0.9292, 0.9365, 0.9304, 0.9365, 0.9121, 0.917, 
    0.9158, 0.9109, 0.9206, 0.9048, 0.9096, 0.917, 0.9109, 0.9206, 
    0.9243, 0.7717, 0.7839, 0.7656, 0.7643, 0.801, 0.8217, 0.8352, 
    0.8523, 0.8852, 0.8864, 0.8889, 0.8706, 0.8877, 0.8913, 0.8864, 
    0.8901, 0.8864, 0.8962, 0.8938, 0.8901, 0.8803, 0.8999, 0.8974, 
    0.895, 0.8828, 0.8645, 0.884, 0.8889, 0.873, 0.8742, 0.8828, 
    0.8828, 0.8926, 0.8706, 0.8816, 0.8742, 0.8742, 0.8669, 0.8742, 
    0.8791, 0.8755, 0.7411, 0.7387), Density.kg.m3 = c(24.9796, 
    24.9812, 25.0087, 25.0293, 25.0609, 25.1008, 25.1553, 25.202, 
    25.2843, 25.3659, 25.3648, 25.4118, 25.4084, 25.4184, 25.3908, 
    25.4065, 25.4168, 25.4285, 25.4345, 25.4386, 25.4426, 25.4446, 
    25.452, 25.4595, 25.4641, 25.4689, 25.476, 25.4741, 25.4744, 
    25.479, 25.4946, 25.5035, 25.5251, 25.6019, 25.604, 25.5513, 
    25.548, 25.5374, 25.561, 25.5587, 25.5625, 25.5723, 25.5906, 
    25.3762, 25.3611, 25.6424, 25.794, 25.88, 25.9236, 25.8329, 
    25.7436, 25.8955, 26.0012, 25.9952, 25.3056, 25.3027, 25.3176, 
    25.3319, 25.3478, 25.3596, 25.2985, 25.3528, 25.3596, 25.3813, 
    25.3855, 25.3885, 25.3934, 25.4034, 25.4077, 25.4107, 25.4129, 
    25.4164, 25.4204, 25.4259, 25.4278, 25.4346, 25.4344, 25.4439, 
    25.4334, 25.4483, 25.3883, 25.4296, 25.4099, 25.3904, 25.4483, 
    25.5069, 25.6037, 25.6185, 25.6669, 25.6664, 25.6656, 25.6651, 
    25.7098, 25.7624, 25.7639, 25.7857, 25.7898, 25.7697, 25.7659, 
    25.7782, 25.7955, 25.7875, 25.8107, 25.7917, 25.7687, 25.772, 
    25.7886, 25.3006, 25.3591, 25.344, 25.3519, 25.3795, 25.4286, 
    25.4932, 25.506, 25.5088, 25.5246, 25.5264, 25.5408, 25.5518, 
    25.5648, 25.5701, 25.5584, 25.5434, 25.5765, 25.5867, 25.6141, 
    25.6198, 25.6161, 25.6202, 25.6149, 25.6199, 25.6526, 25.659, 
    25.6854, 25.671, 25.6518, 25.6741, 25.6826, 25.6892, 25.6972, 
    25.6891, 25.7092, 25.7065, 25.7005, 25.6928, 25.7142, 25.7187, 
    25.34, 25.3324), Brunt.Vaisala.Freq.cycl.h = c(15.222, 6.59496, 
    17.1348, 17.8131, 18.7135, 21.4038, 22.0582, 22.8786, 28.2346, 
    14.8181, 5.59839, 11.962, -5.12458, 7.09748, -16.0581, 1.9386, 
    10.7242, 7.05743, 5.22594, 3.27465, 1.23835, 0.574964, 5.06922, 
    4.53486, 3.30098, 3.8572, 1.97145, -4.39978, -1.18065, 5.75535, 
    10.227, 9.24193, 14.7518, 16.7049, 2.46144, -23.7603, -6.95984, 
    -12.1827, 10.4526, 0.0706585, 4.84537, 7.81757, 1.99304, 
    -32.6947, -1.00587, 35.229, 34.2314, 30.4428, 22.2066, -17.7357, 
    -32.0899, 34.5392, 33.4394, -1.37745, -6.26124, -0.815708, 
    8.91883, 9.02382, 8.76263, -8.16761, -11.1727, 18.9067, 15.0293, 
    9.97694, 4.92582, 2.9718, 4.79608, 5.26987, 2.84495, -0.264973, 
    0.20777, 3.36723, 4.50213, 4.51221, 2.79895, 6.4508, -0.770261, 
    7.62994, 2.5534, -2.43617, -17.015, 5.25272, -12.8585, -1.89404, 
    24.5016, 24.5592, 21.4044, 11.1924, 7.70434, -4.09892, -4.29682, 
    0.10429, 15.62, 15.2757, 1.12576, 12.3537, 6.28031, 6.28031, 
    6.28031, 9.70625, 9.70625, 16.6939, 16.6939, -16.9003, -16.9003, 
    -16.5469, -16.5469, -1.24881, 2.12382, -8.88365, 11.8552, 
    18.6224, 22.4818, 17.7073, 7.05875, 5.60178, 7.24009, 3.48756, 
    9.56937, 10.7906, 13.3156, 2.31384, -11.7071, -5.06014, 11.9814, 
    9.25461, 10.2528, 1.04765, -3.92472, 0.97033, -6.84208, 7.41494, 
    13.7583, 6.60244, 5.18133, -9.85804, -4.08612, 10.1503, 5.32709, 
    4.55223, -0.413365, -5.7495, 3.18065, -5.70216, -7.24117, 
    -7.31205, 12.0855, 4.33021, -4.98771, -2.93681), integer.Depth = c(5, 
    5, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 
    11, 12, 12, 13, 13, 14, 15, 15, 16, 16, 17, 17, 17, 18, 18, 
    18, 19, 19, 19, 19, 20, 20, 20, 21, 21, 22, 22, 23, 23, 23, 
    24, 24, 24, 25, 25, 5, 6, 6, 7, 7, 8, 8, 8, 8, 9, 9, 10, 
    10, 11, 11, 12, 12, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 
    17, 17, 18, 18, 18, 19, 19, 20, 20, 21, 22, 22, 23, 23, 24, 
    24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 5, 6, 6, 6, 6, 
    7, 7, 8, 8, 8, 9, 9, 10, 10, 10, 11, 11, 11, 12, 12, 13, 
    13, 14, 14, 14, 15, 15, 16, 16, 17, 17, 18, 19, 19, 20, 21, 
    21, 22, 22, 23, 23, 5, 6)), .Names = c("Cruise", "Station", 
"Lon", "Lat", "Depth", "Temperature.oC", "Salinity", "Fluoresence.Volts", 
"Density.kg.m3", "Brunt.Vaisala.Freq.cycl.h", "integer.Depth"
), row.names = c("19", "20", "21", "22", "23", "24", "25", "26", 
"27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", 
"38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", 
"49", "50", "51", "52", "53", "54", "55", "56", "57", "58", "59", 
"60", "61", "62", "63", "64", "65", "66", "67", "68", "69", "70", 
"71", "72", "88", "89", "90", "91", "92", "93", "94", "95", "96", 
"97", "98", "99", "100", "101", "102", "103", "104", "105", "106", 
"107", "108", "109", "110", "111", "112", "113", "114", "115", 
"116", "117", "118", "119", "120", "121", "122", "123", "124", 
"125", "126", "127", "128", "129", "130", "131", "132", "133", 
"134", "135", "136", "137", "138", "139", "140", "154", "155", 
"156", "157", "158", "159", "160", "161", "162", "163", "164", 
"165", "166", "167", "168", "169", "170", "171", "172", "173", 
"174", "175", "176", "177", "178", "179", "180", "181", "182", 
"183", "184", "185", "186", "187", "188", "189", "190", "191", 
"192", "193", "194", "209", "210"), class = "data.frame")

When I run this code on the full dataset, calculations continue long enough
to suggest I am generating a huge matrix, so perhaps I'm doing something
silly? Eventually (well, after maybe 5 minutes) I get a by class object of
109 rows (depth category) by 288 columns (station category), so it does seem
to be working.

I know that you use sapply to get the by class back to a data.frame. I want
to extract a matrix of mean densities (one of the original variables) at
each of the 109 depths and 288 stations.
I have not quite got this right...can you help?
 
Thanks in advance

Sam
----
Sam McClatchie,
Biological oceanography 
South Australian Aquatic Sciences Centre
PO Box 120, Henley Beach 5022
Adelaide, South Australia
email <mcclatchie.sam at saugov.sa.gov.au>
Telephone: (61-8) 8207 5448
FAX: (61-8) 8207 5481
Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
  
                   /\
      ...>><xX(??> 
                //// \\\\
                   <??)Xx><<
              /////  \\\\\\
                        ><(((??> 
  >><(((??>   ...>><xX(??>O<??)Xx><<



From ggrothendieck at gmail.com  Mon May 30 05:42:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 29 May 2005 23:42:12 -0400
Subject: [R] sapply following using by with a list of factors
In-Reply-To: <BEA6A7E18959A04385DC14D24619F89F123C74@sagemsg0008.sagemsmrd01.sa.gov.au>
References: <BEA6A7E18959A04385DC14D24619F89F123C74@sagemsg0008.sagemsmrd01.sa.gov.au>
Message-ID: <971536df0505292042582988ec@mail.gmail.com>

On 5/29/05, McClatchie, Sam (PIRSA-SARDI)
<mcclatchie.sam at saugov.sa.gov.au> wrote:
> Background:
> OS: Linux Mandrake 10.1
> release: R 2.0.0
> editor: GNU Emacs 21.3.2
> front-end: ESS 5.2.3
> ---------------------------------
> Colleagues
> 
> I am having some trouble extracting results from the function by, used to
> average variables in a data.frame first by one factor (depth) and then by a
> second factor (station). The real data.frame is quite large
> > dim(data.2001)
> [1] 32049  11
> 
> Here is a snippet of code:
> 
> ## bin density data for each station into 1 m depth bins, containing means
>    data.2001.test$integer.Depth <- as.factor(round(data.2001.test$Depth,
> digits=0))
>    attach(data.2001.test)
>    binned.data.2001 <- by(data.2001.test[,5:11], list(depth=integer.Depth,
> station=Station), mean)
> 
> and here is a snippet of the data.frame
> 
> > dim(data.2001.test)
> [1] 150  11
> > dump("data.2001.test", file=stdout())
> data.2001.test <-
> structure(list(Cruise = structure(as.integer(c(1, 1, 1, 1, 1,


Try the following.  To keep this short lets just take a subset
of rows called dd.  Also, we drop the Station levels 
that are not being used since this test only uses 2 levels
and there are 288 Station levels in total.  The function that we apply using
by returns a vector consisting of the integer.Depth, Station
and the column means of columns 5 to 10.  (Asking for just the
mean of those, as in your example, would take all the numbers 
in all the columns passed to mean and give back a grand mean
 rather than a mean per column.)   Finally we rbind it all back together.

> # data.2001.test is your data frame including the integer.Depth column
> dd <- data.2001.test[50:60,]
> dd$Station <- dd$Station[drop = TRUE]
> dd.bin <- by(dd, list(dd$integer.Depth, dd$Station), function(x)
+ c(integer.Depth = x$integer.Depth[1], Station = x$Station[1], 
+ colMeans(x[,5:10])))
> do.call("rbind", dd.bin)
     integer.Depth Station    Depth Temperature.oC Salinity Fluoresence.Volts
[1,]            20       1 23.90167       17.67420 35.47650          1.107433
[2,]            21       1 24.75350       17.33355 35.59050          1.060400
[3,]             1       2  5.19000       19.61510 35.54870          0.726500
[4,]             2       2  5.82950       19.61305 35.55025          0.719200
[5,]             3       2  6.81250       19.61300 35.58345          0.741150
[6,]             4       2  7.55000       19.61180 35.60460          0.754600
     Density.kg.m3 Brunt.Vaisala.Freq.cycl.h
[1,]      25.82400                 -5.095467
[2,]      25.99820                 16.030975
[3,]      25.30560                 -6.261240
[4,]      25.31015                  4.051561
[5,]      25.33985                  8.893225
[6,]      25.35960                 -8.167610



From ggrothendieck at gmail.com  Mon May 30 05:58:51 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 29 May 2005 23:58:51 -0400
Subject: [R] sapply following using by with a list of factors
In-Reply-To: <971536df0505292042582988ec@mail.gmail.com>
References: <BEA6A7E18959A04385DC14D24619F89F123C74@sagemsg0008.sagemsmrd01.sa.gov.au>
	<971536df0505292042582988ec@mail.gmail.com>
Message-ID: <971536df0505292058500151db@mail.gmail.com>

On 5/29/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 5/29/05, McClatchie, Sam (PIRSA-SARDI)
> <mcclatchie.sam at saugov.sa.gov.au> wrote:
> > Background:
> > OS: Linux Mandrake 10.1
> > release: R 2.0.0
> > editor: GNU Emacs 21.3.2
> > front-end: ESS 5.2.3
> > ---------------------------------
> > Colleagues
> >
> > I am having some trouble extracting results from the function by, used to
> > average variables in a data.frame first by one factor (depth) and then by a
> > second factor (station). The real data.frame is quite large
> > > dim(data.2001)
> > [1] 32049  11
> >
> > Here is a snippet of code:
> >
> > ## bin density data for each station into 1 m depth bins, containing means
> >    data.2001.test$integer.Depth <- as.factor(round(data.2001.test$Depth,
> > digits=0))
> >    attach(data.2001.test)
> >    binned.data.2001 <- by(data.2001.test[,5:11], list(depth=integer.Depth,
> > station=Station), mean)
> >
> > and here is a snippet of the data.frame
> >
> > > dim(data.2001.test)
> > [1] 150  11
> > > dump("data.2001.test", file=stdout())
> > data.2001.test <-
> > structure(list(Cruise = structure(as.integer(c(1, 1, 1, 1, 1,
> 
> 
> Try the following.  To keep this short lets just take a subset
> of rows called dd.  Also, we drop the Station levels
> that are not being used since this test only uses 2 levels
> and there are 288 Station levels in total.  The function that we apply using
> by returns a vector consisting of the integer.Depth, Station
> and the column means of columns 5 to 10.  (Asking for just the
> mean of those, as in your example, would take all the numbers
> in all the columns passed to mean and give back a grand mean
>  rather than a mean per column.)   Finally we rbind it all back together.
> 
> > # data.2001.test is your data frame including the integer.Depth column
> > dd <- data.2001.test[50:60,]
> > dd$Station <- dd$Station[drop = TRUE]
> > dd.bin <- by(dd, list(dd$integer.Depth, dd$Station), function(x)
> + c(integer.Depth = x$integer.Depth[1], Station = x$Station[1],
> + colMeans(x[,5:10])))
> > do.call("rbind", dd.bin)
>     integer.Depth Station    Depth Temperature.oC Salinity Fluoresence.Volts
> [1,]            20       1 23.90167       17.67420 35.47650          1.107433
> [2,]            21       1 24.75350       17.33355 35.59050          1.060400
> [3,]             1       2  5.19000       19.61510 35.54870          0.726500
> [4,]             2       2  5.82950       19.61305 35.55025          0.719200
> [5,]             3       2  6.81250       19.61300 35.58345          0.741150
> [6,]             4       2  7.55000       19.61180 35.60460          0.754600
>     Density.kg.m3 Brunt.Vaisala.Freq.cycl.h
> [1,]      25.82400                 -5.095467
> [2,]      25.99820                 16.030975
> [3,]      25.30560                 -6.261240
> [4,]      25.31015                  4.051561
> [5,]      25.33985                  8.893225
> [6,]      25.35960                 -8.167610
> 

Here is a correction for the fact that the first two columns are 
factors.  This time, instead of creating a vector in the function we create a
one row data frame.

> # previous lines as above
> dd.bin <- by(dd, list(dd$integer.Depth, dd$Station), function(x)
+   cbind(data.frame(integer.Depth = x$integer.Depth[1], 
+   Station = x$Station[1]), t(colMeans(x[,5:10]))))
> do.call("rbind", dd.bin)
   integer.Depth Station    Depth Temperature.oC Salinity Fluoresence.Volts
1             24      a2 23.90167       17.67420 35.47650          1.107433
11            25      a2 24.75350       17.33355 35.59050          1.060400
12             5      a3  5.19000       19.61510 35.54870          0.726500
13             6      a3  5.82950       19.61305 35.55025          0.719200
14             7      a3  6.81250       19.61300 35.58345          0.741150
15             8      a3  7.55000       19.61180 35.60460          0.754600
   Density.kg.m3 Brunt.Vaisala.Freq.cycl.h
1       25.82400                 -5.095467
11      25.99820                 16.030975
12      25.30560                 -6.261240
13      25.31015                  4.051561
14      25.33985                  8.893225
15      25.35960                 -8.167610



From manojsw at gmail.com  Mon May 30 07:00:27 2005
From: manojsw at gmail.com (ManojW)
Date: Mon, 30 May 2005 14:00:27 +0900
Subject: [R] Vector Manipulation
Message-ID: <003101c564d4$99e408b0$6900a8c0@HCJP.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050530/5d0aa4a4/attachment.pl

From manojsw at gmail.com  Mon May 30 07:12:54 2005
From: manojsw at gmail.com (ManojW)
Date: Mon, 30 May 2005 14:12:54 +0900
Subject: [R] Re: Vector Manipulation
Message-ID: <023701c564d6$560d5360$6900a8c0@HCJP.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050530/c4c5a6d1/attachment.pl

From manojsw at gmail.com  Mon May 30 07:19:07 2005
From: manojsw at gmail.com (ManojW)
Date: Mon, 30 May 2005 14:19:07 +0900
Subject: [R] Re: Vector Manipulation
Message-ID: <024201c564d7$1eca0730$6900a8c0@HCJP.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050530/ff12a737/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon May 30 08:24:08 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 30 May 2005 08:24:08 +0200
Subject: [R] Incompatibility with VGAM
In-Reply-To: <429A4185.9060009@auckland.ac.nz>
References: <429A4185.9060009@auckland.ac.nz>
Message-ID: <429AB188.7040701@statistik.uni-dortmund.de>

Thomas Yee wrote:
> Hello,
> 
> 
> It seems that if glm used a namespace then the conflict would be avoided?

No. I already descibed why *not*, and I descibed how to work with stats' 
family functions even if VGAM has been loaded. Please reread my former 
message and tell me what was unclear. You might also want to read the R 
News article on Namespaces by Luke Tierney.
Moreover, glm is in stats which *has* got a namespace.

Uwe Ligges


> FYI, I released VGAM 0.6-3 last Friday which has binomial(), poisson()
> etc. removed so that it should no longer conflict with glm().  The
> families that work under VGAM are called binomialff(), poissonff() etc.
> Also, the Windows zip file for VGAM 0.6-3 was built under R 2.1.0.
> 
> cheers
> 
> 
> Thomas
> 
> 
> 
>



From ligges at statistik.uni-dortmund.de  Mon May 30 08:57:52 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 30 May 2005 08:57:52 +0200
Subject: [R] joining files after canonical correlation
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA9868B355@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA9868B355@MSX2>
Message-ID: <429AB970.5060805@statistik.uni-dortmund.de>

Brett Stansfield wrote:
> Dear R,
> I recently did a canonical correlation analysis on two subsets of data
> (location and weather). So I now have canonical scores for location and
> weather. but I'd now like to do a scatterplot matrix using the pairs
> statement. 
> 
> Is there a way to somehow join location.U and weather.V to become a new data
> set from which I could undertake a scatterplot matrix of the canonical
> variates?

Not sure about your question, but I guess you are simply looking for ?rbind.

Uwe Ligges

> brett
> 
> 
> Brett Stansfield 
> Environmental Scientist - Water Quality 
> Hawke's Bay Regional Council 
> 102 Vautier Street 
> Private Bag 6006 
> Napier 
> Phone (06) 835-9200 extn 9334 
> Fax (06) 835-3601
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From minhuangr at gmail.com  Mon May 30 09:20:56 2005
From: minhuangr at gmail.com (huang min)
Date: Mon, 30 May 2005 15:20:56 +0800
Subject: [R] how to invert the matrix with quite small eigenvalues
Message-ID: <bfc6766805053000202d6fa5ed@mail.gmail.com>

Dear all,

I encounter some covariance matrix with quite small eigenvalues
(around 1e-18), which are smaller than the machine precision. The
dimension of my matrix is 17. Here I just fake some small matrix for
illustration.

 a<-diag(c(rep(3,4),1e-18)) # a matrix with small eigenvalues
 b<-matrix(1:25,ncol=5) # define b to get an orthogonal matrix
 b<-b+t(b)
 bb<-eigen(b,symmetric=T)
 aah<-bb$vectors%*%diag(1/sqrt(diag(a)))
 aa<-aah%*%t(aah) # aa should have the same eigenvalues as a and
should be #invertable,however,
 solve(aa) # can not be solved
 solve(aa,tol=1e-19) # can be inverted, however, it is not symmetric
and furthermore,
 solve(aa,tol=1e-19)%*%aa # deviate much from the identity matrix

I have already define aa to make sure it is symmetric. So the inverse
should be symmetric.

Does the problem come from the rounding error since the eigenvalue is
smaller than the machine precision? In fact, the eigenvalue of aa is
negative now, but at least, it is still invertable. How can I get the
inverse? Thanks.



From Reinhold.Hafner at risklab.de  Mon May 30 10:47:08 2005
From: Reinhold.Hafner at risklab.de (Hafner, Reinhold (Risklab))
Date: Mon, 30 May 2005 10:47:08 +0200
Subject: [R] Menu Language
Message-ID: <34309ADC21099A4A92CE4F8A8123391A025434A3@afwpm006.intradit.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050530/748502c4/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Mon May 30 10:51:12 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 30 May 2005 09:51:12 +0100 (BST)
Subject: [R] how to invert the matrix with quite small eigenvalues
In-Reply-To: <bfc6766805053000202d6fa5ed@mail.gmail.com>
Message-ID: <XFMail.050530094424.Ted.Harding@nessie.mcc.ac.uk>

On 30-May-05 huang min wrote:
> Dear all,
> 
> I encounter some covariance matrix with quite small eigenvalues
> (around 1e-18), which are smaller than the machine precision. The
> dimension of my matrix is 17. Here I just fake some small matrix for
> illustration.
> 
>  a<-diag(c(rep(3,4),1e-18)) # a matrix with small eigenvalues
>  b<-matrix(1:25,ncol=5) # define b to get an orthogonal matrix
>  b<-b+t(b)

NB: b is not *orthogonal*! Each row of b is equal to the preceding
row plus (row2 - row1) (and similar for the columns), and b has
rank 2 (though eigen(b) taken literally says that it has 5 non-zero
eugenvalues; however, 3 of these are snaller that 10^(-14)).

Perhaps you meant "define b to get a symmetric matrix".

>  bb<-eigen(b,symmetric=T)
>  aah<-bb$vectors%*%diag(1/sqrt(diag(a)))
>  aa<-aah%*%t(aah) # aa should have the same eigenvalues as a and
> should be #invertable,however,
>  solve(aa) # can not be solved

Well, I did get a (non-symmetric) result for solve(aa) ...

>  solve(aa,tol=1e-19) # can be inverted, however, it is not symmetric
> and furthermore,

and an idenitical (to solve(aa)) result for this.

>  solve(aa,tol=1e-19)%*%aa # deviate much from the identity matrix

But here I agree with you!

> I have already define aa to make sure it is symmetric. So the inverse
> should be symmetric.
> 
> Does the problem come from the rounding error since the eigenvalue is
> smaller than the machine precision? In fact, the eigenvalue of aa is
> negative now, but at least, it is still invertable. How can I get the
> inverse? Thanks.

It does indeed, like the eigenvalue result for b above, come from
the rounding error.

You should clarify in your mind why you want to ensure that you get
correct results for matrices like these.

You are (and in your example deliberately so) treading on the very
edge of what is capable of being computed, and results are very likely
to lead to unexpected gross anomalies (such as being unable to
invert a mathematically invertible matrix, or getting a non-symmetric
inverse to a symmetric matrix [depending on the algorithm], or
getting non-zero values for eigenvalues which should be zero, or
the gross difference from the identity matrix which you expected).

It is like using a mechanical ditch-digger to peel an apple.

Exactly what will happen in any particular case can only be
determined by a very fine-grained analysis of the operation
of the numerical algorithm, which is beyond your reach in the
normal usage of R.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 30-May-05                                       Time: 09:43:56
------------------------------ XFMail ------------------------------



From lzhtom at hotmail.com  Mon May 30 11:15:32 2005
From: lzhtom at hotmail.com (zhihua li)
Date: Mon, 30 May 2005 09:15:32 +0000
Subject: [R] how to "singlify" entries
Message-ID: <BAY12-F3C0A0AEA46B66B1F32F64C7030@phx.gbl>

hi netters

I have a rather simple question.  I have a data frame with two variables X 
and Y, both of which are factors. X has 100 levels while Y has 10 levels 
only. The data frame has 100 rows in all, so for X the values are unique, 
and Y has many replicate values.  Now I wanna reduce the data frame into 10 
rows only, according to the 10 levels of Y.  I don't care which value of X 
is in the same row with Y in the final data frame, as long as it is in 
agreement with the original data frame.

I think this task can be carried out with some function like aggregate. but 
I failed in figuring it out. Could anybody give me a hint?

Thanks a lot!



From ligges at statistik.uni-dortmund.de  Mon May 30 11:31:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 30 May 2005 11:31:46 +0200
Subject: [R] Menu Language
In-Reply-To: <34309ADC21099A4A92CE4F8A8123391A025434A3@afwpm006.intradit.net>
References: <34309ADC21099A4A92CE4F8A8123391A025434A3@afwpm006.intradit.net>
Message-ID: <429ADD82.2000805@statistik.uni-dortmund.de>

Hafner, Reinhold (Risklab) wrote:
> Hello,
> In my windows xp system the regional settings are set to German. However, I
> still would like to use the English version of R with English menu items and
> texts. RWinEdt tells me this is necessary in order to use the mdi framework.
> How can this be achieved with the 2.1 version. In 2.0.1 this wasn't a
> problem.

Yes, because R-2.0.1 has not had any translations.
You can set an envrionment variable  LANGUAGE=en  for exmaple...
See the new documentation and the most recent R Newsletter which has 
some nice articles on languages and internationalization by Brian Ripley.

Uwe Ligges


> Reinhold
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From minhuangr at gmail.com  Mon May 30 11:39:52 2005
From: minhuangr at gmail.com (huang min)
Date: Mon, 30 May 2005 17:39:52 +0800
Subject: [R] how to invert the matrix with quite small eigenvalues
In-Reply-To: <XFMail.050530094424.Ted.Harding@nessie.mcc.ac.uk>
References: <bfc6766805053000202d6fa5ed@mail.gmail.com>
	<XFMail.050530094424.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <bfc676680505300239ce69f4d@mail.gmail.com>

Maybe I should state more clear that I define b  to get the orthogonal
matrix bb$vectors.

We also can define diag(b)<-diag(b)+100, which will make the
eigenvalues of b much bigger to make sure the orthogonal matrix is
reliable.

My intention is to invert the covariance matrix to perform some
algorithm which is common in the estimating equations like GEE.

I meet difficulty to invert the covariance matrix. Two possibilities here:

1. The rounding error in defining the covariance matrix make the
eigenvalue to small.

2. The solve function in R can not cope with the matrix with so small
an eigenvalue.

For the first possibility, I think it can not be improved unless we
can  define more precise number than the double precision. So I ask
for the possiblity of coping with the second.

I can not find the default way to invert the matrix with solve().

For the symmetric matrix, I wonder if there are some algorithm which
can naturally make the inverse matrix symmetric and make sure it is
the inverse in the sense that the product is an identity matrix. I
know there are many decompositions which can be used to find the
inverse of a matrix. QR, SVD, Chol, and maybe some iterative method. I
wonder anyone can suggest me which algorithm might be good.

Another strange point is that my friend use the LU decomposition in
Fortran to solve the inverse matrix of aa for me. He used double
precision of course, otherwise no inverse matrix in Fortran too. He
show that the product of the two matrix is almost identity(The biggest
off-digonal element is about 1e-8). I copy his inverse matrix(with 31
digits!) to R and read in aa I sent to him(16 digits). The product is
also not an identity matrix. It is fairly strange! Any suggestion?

Regards,

Huang


On 5/30/05, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
> On 30-May-05 huang min wrote:
> > Dear all,
> >
> > I encounter some covariance matrix with quite small eigenvalues
> > (around 1e-18), which are smaller than the machine precision. The
> > dimension of my matrix is 17. Here I just fake some small matrix for
> > illustration.
> >
> >  a<-diag(c(rep(3,4),1e-18)) # a matrix with small eigenvalues
> >  b<-matrix(1:25,ncol=5) # define b to get an orthogonal matrix
> >  b<-b+t(b)
> 
> NB: b is not *orthogonal*! Each row of b is equal to the preceding
> row plus (row2 - row1) (and similar for the columns), and b has
> rank 2 (though eigen(b) taken literally says that it has 5 non-zero
> eugenvalues; however, 3 of these are snaller that 10^(-14)).
> 
> Perhaps you meant "define b to get a symmetric matrix".
> 
> >  bb<-eigen(b,symmetric=T)
> >  aah<-bb$vectors%*%diag(1/sqrt(diag(a)))
> >  aa<-aah%*%t(aah) # aa should have the same eigenvalues as a and
> > should be #invertable,however,
> >  solve(aa) # can not be solved
> 
> Well, I did get a (non-symmetric) result for solve(aa) ...
> 
> >  solve(aa,tol=1e-19) # can be inverted, however, it is not symmetric
> > and furthermore,
> 
> and an idenitical (to solve(aa)) result for this.
> 
> >  solve(aa,tol=1e-19)%*%aa # deviate much from the identity matrix
> 
> But here I agree with you!
> 
> > I have already define aa to make sure it is symmetric. So the inverse
> > should be symmetric.
> >
> > Does the problem come from the rounding error since the eigenvalue is
> > smaller than the machine precision? In fact, the eigenvalue of aa is
> > negative now, but at least, it is still invertable. How can I get the
> > inverse? Thanks.
> 
> It does indeed, like the eigenvalue result for b above, come from
> the rounding error.
> 
> You should clarify in your mind why you want to ensure that you get
> correct results for matrices like these.
> 
> You are (and in your example deliberately so) treading on the very
> edge of what is capable of being computed, and results are very likely
> to lead to unexpected gross anomalies (such as being unable to
> invert a mathematically invertible matrix, or getting a non-symmetric
> inverse to a symmetric matrix [depending on the algorithm], or
> getting non-zero values for eigenvalues which should be zero, or
> the gross difference from the identity matrix which you expected).
> 
> It is like using a mechanical ditch-digger to peel an apple.
> 
> Exactly what will happen in any particular case can only be
> determined by a very fine-grained analysis of the operation
> of the numerical algorithm, which is beyond your reach in the
> normal usage of R.
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 30-May-05                                       Time: 09:43:56
> ------------------------------ XFMail ------------------------------
>



From petr.pikal at precheza.cz  Mon May 30 11:48:23 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 30 May 2005 11:48:23 +0200
Subject: [R] how to "singlify" entries
In-Reply-To: <BAY12-F3C0A0AEA46B66B1F32F64C7030@phx.gbl>
Message-ID: <429AFD87.6177.E42788@localhost>

Hallo

On 30 May 2005 at 9:15, zhihua li wrote:

> hi netters
> 
> I have a rather simple question.  I have a data frame with two

Well, I do not understand you simple question fully. You have 
something like that

dat<-data.frame(X=1:100, Y=sample(1:10,10))
dat$X<-factor(dat$X)
dat$Y<-factor(dat$Y)
dat$Y[5]<-10

> variables X and Y, both of which are factors. X has 100 levels while Y
> has 10 levels only. The data frame has 100 rows in all, so for X the
> values are unique, and Y has many replicate values.  Now I wanna
> reduce the data frame into 10 rows only, according to the 10 levels of
> Y.  I don't care which value of X is in the same row with Y in the
> final data frame, as long as it is in agreement with the original data
> frame.

Do you want to choose only some rows from your data frame to 
get unique Y and any corresponding X?

dat[!duplicated(dat$Y),]

Or do you want something different?

HTH
Petr

> 
> I think this task can be carried out with some function like
> aggregate. but I failed in figuring it out. Could anybody give me a
> hint?
> 
> Thanks a lot!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From maechler at stat.math.ethz.ch  Mon May 30 11:57:41 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 30 May 2005 11:57:41 +0200
Subject: [R] Chars as numbers
In-Reply-To: <Pine.LNX.4.63.0505271703550.24183@dns.unife.it>
References: <Pine.LNX.4.63.0505271703550.24183@dns.unife.it>
Message-ID: <17050.58261.884914.45913@stat.math.ethz.ch>

Package 'sfsmisc'
has a function  AsciiToInt() and a few useful related "R-code"
only functions such as chars8bit();  see the help pages once
you've installed and attached the package.

But do note that these things do depend on the encoding, as Uwe
Ligges has already told you.
Things work fine for pure  ASCII {independently of encoding}
but already differ for Umlauts / Accents
between iso-latin-1 or Unicode.

Martin Maechler, ETH Zurich



From murdoch at stats.uwo.ca  Mon May 30 12:31:11 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 30 May 2005 06:31:11 -0400
Subject: [R] joining files after canonical correlation
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA9868B355@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA9868B355@MSX2>
Message-ID: <429AEB6F.5010800@stats.uwo.ca>

Brett Stansfield wrote:
> Dear R,
> I recently did a canonical correlation analysis on two subsets of data
> (location and weather). So I now have canonical scores for location and
> weather. but I'd now like to do a scatterplot matrix using the pairs
> statement. 
> 
> Is there a way to somehow join location.U and weather.V to become a new data
> set from which I could undertake a scatterplot matrix of the canonical
> variates?

cbind() can create a matrix by binding together the columns of two (or 
more) input matrices.

Duncan Murdoch



From Ted.Harding at nessie.mcc.ac.uk  Mon May 30 12:41:33 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 30 May 2005 11:41:33 +0100 (BST)
Subject: [R] how to invert the matrix with quite small eigenvalues
In-Reply-To: <bfc676680505300239ce69f4d@mail.gmail.com>
Message-ID: <XFMail.050530114133.Ted.Harding@nessie.mcc.ac.uk>

On 30-May-05 huang min wrote:
> Maybe I should state more clear that I define b to get the
> orthogonal matrix bb$vectors.

OK. Certainly bbv<-bb$vectors is close to orthogonal: bbv%*%bbv
differs from the unit matrix only in that the off-diagonal
terms are O(10^(-16)).

> We also can define diag(b)<-diag(b)+100, which will make the
> eigenvalues of b much bigger to make sure the orthogonal matrix is
> reliable.
> 
> My intention is to invert the covariance matrix to perform some
> algorithm which is common in the estimating equations like GEE.

Which comes back to my general question: why do you need to ensure
that you get correct results for matrices like these? This matrix
is very nearly singular, and in many contexts you would interpret
it as exactly singular (e.g. if I got such a matrix as the
empirical covariance matrix from a sample, or by computation from
the structure of a model such as a design matrix, I would not
normally want to preserve this very small margin of non-singularity:
I would replace it with the lower-dimensional singular version,
e.g. by decomposing it with eigen() or svd() and reconstructing
the singular version after setting very small eigenvalues to 0).

But then of course there would be no inverse, which might be
required by the further computational expressions you want to
evaluate. However, in that case (depending on your application),
you may be able to proceed satisfactorily by using ginv() (see
package MASS) instead of solve(). But if you take that route,
then you have to bear in mind that a generalised inverse is
not a true inverse (if only because the latter does not exist):
the only property required for G=ginv(M) is that

  M%*%G%*%M = M

If such a matrix G will work for the rest of your calculations,
then you are OK, in particular if what you need is a possible
solution to an under-determined system of linear equations.
If not, then you should seriously consider whether your
computational strategy is suitable for the problem you have
in hand.

> [...]
> Another strange point is that my friend use the LU decomposition
> in Fortran to solve the inverse matrix of aa for me. He used
> double precision of course, otherwise no inverse matrix in
> Fortran too. He show that the product of the two matrix is
> almost identity (The biggest off-digonal element is about 1e-8).
> I copy his inverse matrix(with 31 digits!) to R and read in aa
> I sent to him(16 digits). The product is also not an identity
> matrix. It is fairly strange! Any suggestion?

These phenomena are yet another instance of the fact that at the
margins of computability the results will be anomalouus. Your
friend is simply using a narrower margin, but it is still not
exact! Not strange at all.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 30-May-05                                       Time: 11:41:28
------------------------------ XFMail ------------------------------



From f.harrell at vanderbilt.edu  Mon May 30 13:34:33 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 30 May 2005 07:34:33 -0400
Subject: [R] sapply following using by with a list of factors
In-Reply-To: <BEA6A7E18959A04385DC14D24619F89F123C74@sagemsg0008.sagemsmrd01.sa.gov.au>
References: <BEA6A7E18959A04385DC14D24619F89F123C74@sagemsg0008.sagemsmrd01.sa.gov.au>
Message-ID: <429AFA49.9040708@vanderbilt.edu>

McClatchie, Sam (PIRSA-SARDI) wrote:
> Background:
> OS: Linux Mandrake 10.1
> release: R 2.0.0
> editor: GNU Emacs 21.3.2
> front-end: ESS 5.2.3
> ---------------------------------
> Colleagues
> 
> I am having some trouble extracting results from the function by, used to
> average variables in a data.frame first by one factor (depth) and then by a
> second factor (station). The real data.frame is quite large 
> 
>>dim(data.2001)
> 
> [1] 32049  11
> 
> Here is a snippet of code:
> 
> ## bin density data for each station into 1 m depth bins, containing means
>     data.2001.test$integer.Depth <- as.factor(round(data.2001.test$Depth,
> digits=0))
>     attach(data.2001.test)
>     binned.data.2001 <- by(data.2001.test[,5:11], list(depth=integer.Depth,
> station=Station), mean)
> 
> and here is a snippet of the data.frame
> 
> 
>>dim(data.2001.test)
> 
> [1] 150  11
> 
>>dump("data.2001.test", file=stdout())
> 
> data.2001.test <-
> structure(list(Cruise = structure(as.integer(c(1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)), .Label =
> "Ngerin01", class = "factor"), 
>     Station = structure(as.integer(c(2, 2, 2, 2, 2, 2, 2, 2, 
>     2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
>     2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
>     2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
>     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
>     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
>     3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 
>     4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 
>     4, 4, 4, 4, 4, 4, 4, 5, 5)), .Label = c("a1", "a2", "a3", 
>     "a4", "a5", "a6", "a7", "a8", "a9", "b1", "b2", "b3", "b4", 
>     "b5", "b6", "b7", "c1", "c2", "c3", "c4", "c5", "c6", "c7", 
>     "d1", "d2", "d3", "d4", "d5", "d6", "e1", "e2", "e3", "e4", 
>     "e5", "e6", "e7", "f1", "f2", "f3", "f4", "f5", "f6", "f7", 
>     "f8", "f9", "g1", "g10", "g11", "g2", "g3", "g4", "g5", "g6", 
>     "g7", "g8", "g9", "gsvc1", "gsvc2", "gsvc3", "gsvc4", "gsvc5", 
>     "gsvc6", "gsvc7", "gsvc8", "gsvc9", "gsvd1", "gsvd2", "gsvd3", 
>     "gsvd4", "gsvd5", "gsvd6", "gsvd7", "gsvd8", "h1", "h11", 
>     "h2", "h3", "h4", "h5", "h6", "h7", "h8", "h9", "i1", "i10", 
>     "i2", "i3", "i4", "i5", "i6", "i7", "i8", "i9", "j1", "j10", 
>     "j2", "j3", "j4", "j5", "j6", "j7", "j8", "j9", "k1", "k10", 
>     "k2", "k3", "k4", "k5", "k6", "k7", "k8", "k9", "l1", "l10", 
>     "l11", "l12", "l2", "l3", "l4", "l5", "l6", "l7", "l8", "l9", 
>     "m1", "m10", "m11", "m2", "m3", "m4", "m5", "m6", "m7", "m8", 
>     "m9", "mx", "n1", "n10", "n11", "n2", "n3", "n4", "n5", "n6", 
>     "n7", "n8", "n9", "nx", "o1", "o10", "o11", "o13", "o2", 
>     "o3", "o4", "o5", "o6", "o7", "o8", "o9", "ox", "p10", "p11", 
>     "p2", "p3", "p4", "p5", "p6", "p7", "p8", "p9", "px", "q1", 
>     "q10", "q11", "q12", "q13", "q2", "q3", "q4", "q5", "q6", 
>     "q7", "q8", "q9", "qx", "r1", "r10", "r11", "r12", "r13", 
>     "r14", "r15", "r2", "r3", "r4", "r5", "r6", "r7", "r8", "r9", 
>     "rx", "s1", "s10", "s11", "s12", "s13", "s14", "s15", "s16", 
>     "s2", "s3", "s4", "s5", "s6", "s7", "s8", "s9", "sgc1", "sgc2", 
>     "sgc3", "sgc4", "sgc5", "sgc6", "sgc7", "sx", "t1", "t10", 
>     "t11", "t12", "t13", "t14", "t15", "t16", "t2", "t3", "t4", 
>     "t5", "t6", "t7", "t8", "t9", "tx", "u1", "u10", "u11", "u12", 
>     "u13", "u14", "u15", "u2", "u3", "u4", "u5", "u6", "u7", 
>     "u8", "u9", "ux", "v1", "v10", "v11", "v2", "v3", "v4", "v5", 
>     "v6", "v7", "v8", "v9", "vx", "w2", "w3", "w4", "w5", "w6", 
>     "w7", "w8", "w9", "wx", "x2", "x3", "x4", "x5", "x6", "x7", 
>     "x8"), class = "factor"), Lon = c(138.421, 138.421, 138.421, 
>     138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
>     138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
>     138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
>     138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
>     138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
>     138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
>     138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 138.421, 
>     138.421, 138.421, 138.352, 138.352, 138.352, 138.352, 138.352, 
>     138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
>     138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
>     138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
>     138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
>     138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
>     138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 
>     138.352, 138.352, 138.352, 138.352, 138.352, 138.352, 138.277, 
>     138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 
>     138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 
>     138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 
>     138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 
>     138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 138.277, 
>     138.277, 138.277, 138.277, 138.277, 138.277, 138.201, 138.201
>     ), Lat = c(-35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
>     -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
>     -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
>     -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
>     -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
>     -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
>     -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, 
>     -35.766, -35.766, -35.766, -35.766, -35.766, -35.766, -35.827, 
>     -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
>     -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
>     -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
>     -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
>     -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
>     -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
>     -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, -35.827, 
>     -35.827, -35.827, -35.827, -35.883, -35.883, -35.883, -35.883, 
>     -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, 
>     -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, 
>     -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, 
>     -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, 
>     -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, -35.883, 
>     -35.883, -35.883, -35.95, -35.95), Depth = c(5.092, 5.289, 
>     5.584, 5.78, 6.075, 6.37, 6.665, 7.156, 7.451, 7.845, 8.238, 
>     8.533, 8.729, 8.926, 9.123, 9.319, 9.614, 10.106, 10.499, 
>     10.991, 11.58, 12.17, 12.76, 13.35, 13.94, 14.53, 15.119, 
>     15.709, 16.201, 16.594, 16.987, 17.282, 17.577, 18.069, 18.364, 
>     18.659, 18.954, 19.15, 19.347, 19.543, 19.838, 20.232, 20.723, 
>     21.215, 21.706, 22.394, 22.788, 23.082, 23.377, 23.574, 23.869, 
>     24.262, 24.557, 24.95, 5.19, 5.584, 6.075, 6.567, 7.058, 
>     7.55, 7.845, 8.14, 8.336, 8.729, 9.024, 9.516, 10.007, 10.597, 
>     11.089, 11.777, 12.367, 12.858, 13.252, 13.645, 14.038, 14.333, 
>     14.726, 15.021, 15.414, 16.004, 16.397, 16.987, 17.381, 17.872, 
>     18.167, 18.462, 18.954, 19.445, 19.838, 20.428, 20.92, 21.51, 
>     22.099, 22.591, 23.181, 23.574, 23.869, 23.869, 23.869, 23.967, 
>     23.967, 24.066, 24.066, 24.262, 24.262, 24.36, 24.36, 5.387, 
>     5.78, 5.977, 6.173, 6.468, 6.763, 7.156, 7.55, 8.041, 8.434, 
>     9.024, 9.418, 9.811, 10.007, 10.302, 10.597, 10.991, 11.482, 
>     11.875, 12.367, 12.858, 13.252, 13.645, 13.94, 14.333, 14.726, 
>     15.218, 15.808, 16.397, 16.987, 17.479, 18.069, 18.659, 19.445, 
>     19.937, 20.723, 21.215, 21.804, 22.198, 22.591, 22.984, 5.485, 
>     5.977), Temperature.oC = c(19.743, 19.7421, 19.7396, 19.7354, 
>     19.7162, 19.6801, 19.6181, 19.5434, 19.403, 19.2851, 19.2514, 
>     19.2278, 19.2303, 19.2379, 19.248, 19.2227, 19.205, 19.2, 
>     19.1941, 19.1907, 19.1856, 19.1772, 19.1662, 19.1561, 19.1493, 
>     19.1434, 19.1383, 19.1341, 19.1282, 19.1172, 19.1096, 19.1113, 
>     19.1189, 19.1451, 19.2152, 19.2641, 19.259, 19.2405, 19.2278, 
>     19.2244, 19.2017, 19.1772, 19.1527, 19.0868, 18.6986, 18.2519, 
>     17.9853, 17.871, 17.8237, 17.8366, 17.7143, 17.4717, 17.36, 
>     17.3071, 19.6151, 19.6135, 19.6126, 19.6134, 19.6126, 19.6118, 
>     19.6118, 19.6109, 19.6109, 19.6109, 19.6109, 19.6109, 19.6101, 
>     19.6084, 19.6092, 19.61, 19.6092, 19.6075, 19.605, 19.5974, 
>     19.5916, 19.5832, 19.5765, 19.5706, 19.5597, 19.5328, 19.5042, 
>     19.4084, 19.2998, 19.1665, 18.9958, 18.8331, 18.6955, 18.6207, 
>     18.5594, 18.4802, 18.4008, 18.3214, 18.2265, 18.1631, 18.1426, 
>     18.1126, 18.1049, 18.062, 18.098, 18.11, 18.0286, 18.1117, 
>     18.0989, 18.0989, 18.0114, 18.1152, 17.9513, 19.3769, 19.371, 
>     19.3718, 19.3685, 19.344, 19.3137, 19.2269, 19.1036, 19.0013, 
>     18.9488, 18.9082, 18.8573, 18.8268, 18.8157, 18.8124, 18.7996, 
>     18.7674, 18.7198, 18.679, 18.6535, 18.6416, 18.6305, 18.611, 
>     18.5922, 18.5514, 18.5139, 18.4934, 18.4841, 18.4968, 18.49, 
>     18.4644, 18.4534, 18.4465, 18.4414, 18.4389, 18.4295, 18.4346, 
>     18.4337, 18.4269, 18.4064, 18.4022, 19.3895, 19.3912), Salinity =
> c(35.166, 
>     35.1667, 35.2001, 35.2246, 35.2577, 35.2959, 35.3443, 35.3771, 
>     35.4354, 35.5, 35.4849, 35.5368, 35.532, 35.5466, 35.5127, 
>     35.5236, 35.5294, 35.5402, 35.5438, 35.5451, 35.5453, 35.5417, 
>     35.5442, 35.5473, 35.5476, 35.5484, 35.5526, 35.5453, 35.5409, 
>     35.5409, 35.5565, 35.567, 35.5962, 35.7026, 35.7273, 35.6732, 
>     35.6655, 35.6442, 35.6696, 35.6644, 35.66, 35.6623, 35.6751, 
>     35.3696, 35.2176, 35.4344, 35.5435, 35.6172, 35.6572, 35.5419, 
>     35.3845, 35.5031, 35.6039, 35.5771, 35.5487, 35.5421, 35.5584, 
>     35.5746, 35.5923, 35.6046, 35.5229, 35.592, 35.5998, 35.626, 
>     35.6298, 35.6309, 35.6341, 35.6432, 35.6463, 35.6466, 35.6458, 
>     35.647, 35.649, 35.6514, 35.6496, 35.654, 35.6491, 35.6579, 
>     35.6381, 35.645, 35.5544, 35.5725, 35.5076, 35.4342, 35.4509, 
>     35.4715, 35.5495, 35.5413, 35.582, 35.5519, 35.5219, 35.4919, 
>     35.5158, 35.5611, 35.553, 35.5694, 35.5706, 35.5304, 35.5371, 
>     35.5565, 35.5527, 35.5686, 35.5947, 35.5688, 35.5104, 35.5478, 
>     35.5164, 35.4598, 35.5322, 35.5115, 35.5196, 35.5457, 35.598, 
>     35.651, 35.6238, 35.5903, 35.5912, 35.5766, 35.5761, 35.5781, 
>     35.5903, 35.5944, 35.5731, 35.5405, 35.5652, 35.5627, 35.5872, 
>     35.5879, 35.5772, 35.5738, 35.5589, 35.5497, 35.5779, 35.5766, 
>     35.6047, 35.5866, 35.5559, 35.5738, 35.5778, 35.5808, 35.585, 
>     35.5708, 35.5894, 35.5848, 35.5732, 35.5587, 35.5776, 35.5798, 
>     35.5151, 35.5029), Fluoresence.Volts = c(0.6947, 0.6789, 
>     0.6923, 0.6935, 0.6996, 0.7045, 0.6825, 0.6911, 0.6886, 0.685, 
>     0.6801, 0.6874, 0.7009, 0.6996, 0.6935, 0.6899, 0.7045, 0.7106, 
>     0.7082, 0.7021, 0.7009, 0.7375, 0.7302, 0.7253, 0.7436, 0.7692, 
>     0.7741, 0.7814, 0.7619, 0.7851, 0.7961, 0.7998, 0.7851, 0.7863, 
>     0.7998, 0.823, 0.8388, 0.8462, 0.8718, 0.8864, 0.895, 0.873, 
>     0.8632, 0.9035, 0.9744, 1.0049, 1.0537, 1.1026, 1.1258, 1.1258, 
>     1.1013, 1.0952, 1.072, 1.0488, 0.7265, 0.7241, 0.7143, 0.7326, 
>     0.7497, 0.7546, 0.7582, 0.7546, 0.7741, 0.7766, 0.7741, 0.7595, 
>     0.7546, 0.7827, 0.7839, 0.7729, 0.7705, 0.7985, 0.8071, 0.8254, 
>     0.8144, 0.8144, 0.8181, 0.8217, 0.8083, 0.8339, 0.8376, 0.8437, 
>     0.8791, 0.8913, 0.9096, 0.9133, 0.9267, 0.928, 0.9158, 0.9609, 
>     0.956, 0.9328, 0.9292, 0.9365, 0.9304, 0.9365, 0.9121, 0.917, 
>     0.9158, 0.9109, 0.9206, 0.9048, 0.9096, 0.917, 0.9109, 0.9206, 
>     0.9243, 0.7717, 0.7839, 0.7656, 0.7643, 0.801, 0.8217, 0.8352, 
>     0.8523, 0.8852, 0.8864, 0.8889, 0.8706, 0.8877, 0.8913, 0.8864, 
>     0.8901, 0.8864, 0.8962, 0.8938, 0.8901, 0.8803, 0.8999, 0.8974, 
>     0.895, 0.8828, 0.8645, 0.884, 0.8889, 0.873, 0.8742, 0.8828, 
>     0.8828, 0.8926, 0.8706, 0.8816, 0.8742, 0.8742, 0.8669, 0.8742, 
>     0.8791, 0.8755, 0.7411, 0.7387), Density.kg.m3 = c(24.9796, 
>     24.9812, 25.0087, 25.0293, 25.0609, 25.1008, 25.1553, 25.202, 
>     25.2843, 25.3659, 25.3648, 25.4118, 25.4084, 25.4184, 25.3908, 
>     25.4065, 25.4168, 25.4285, 25.4345, 25.4386, 25.4426, 25.4446, 
>     25.452, 25.4595, 25.4641, 25.4689, 25.476, 25.4741, 25.4744, 
>     25.479, 25.4946, 25.5035, 25.5251, 25.6019, 25.604, 25.5513, 
>     25.548, 25.5374, 25.561, 25.5587, 25.5625, 25.5723, 25.5906, 
>     25.3762, 25.3611, 25.6424, 25.794, 25.88, 25.9236, 25.8329, 
>     25.7436, 25.8955, 26.0012, 25.9952, 25.3056, 25.3027, 25.3176, 
>     25.3319, 25.3478, 25.3596, 25.2985, 25.3528, 25.3596, 25.3813, 
>     25.3855, 25.3885, 25.3934, 25.4034, 25.4077, 25.4107, 25.4129, 
>     25.4164, 25.4204, 25.4259, 25.4278, 25.4346, 25.4344, 25.4439, 
>     25.4334, 25.4483, 25.3883, 25.4296, 25.4099, 25.3904, 25.4483, 
>     25.5069, 25.6037, 25.6185, 25.6669, 25.6664, 25.6656, 25.6651, 
>     25.7098, 25.7624, 25.7639, 25.7857, 25.7898, 25.7697, 25.7659, 
>     25.7782, 25.7955, 25.7875, 25.8107, 25.7917, 25.7687, 25.772, 
>     25.7886, 25.3006, 25.3591, 25.344, 25.3519, 25.3795, 25.4286, 
>     25.4932, 25.506, 25.5088, 25.5246, 25.5264, 25.5408, 25.5518, 
>     25.5648, 25.5701, 25.5584, 25.5434, 25.5765, 25.5867, 25.6141, 
>     25.6198, 25.6161, 25.6202, 25.6149, 25.6199, 25.6526, 25.659, 
>     25.6854, 25.671, 25.6518, 25.6741, 25.6826, 25.6892, 25.6972, 
>     25.6891, 25.7092, 25.7065, 25.7005, 25.6928, 25.7142, 25.7187, 
>     25.34, 25.3324), Brunt.Vaisala.Freq.cycl.h = c(15.222, 6.59496, 
>     17.1348, 17.8131, 18.7135, 21.4038, 22.0582, 22.8786, 28.2346, 
>     14.8181, 5.59839, 11.962, -5.12458, 7.09748, -16.0581, 1.9386, 
>     10.7242, 7.05743, 5.22594, 3.27465, 1.23835, 0.574964, 5.06922, 
>     4.53486, 3.30098, 3.8572, 1.97145, -4.39978, -1.18065, 5.75535, 
>     10.227, 9.24193, 14.7518, 16.7049, 2.46144, -23.7603, -6.95984, 
>     -12.1827, 10.4526, 0.0706585, 4.84537, 7.81757, 1.99304, 
>     -32.6947, -1.00587, 35.229, 34.2314, 30.4428, 22.2066, -17.7357, 
>     -32.0899, 34.5392, 33.4394, -1.37745, -6.26124, -0.815708, 
>     8.91883, 9.02382, 8.76263, -8.16761, -11.1727, 18.9067, 15.0293, 
>     9.97694, 4.92582, 2.9718, 4.79608, 5.26987, 2.84495, -0.264973, 
>     0.20777, 3.36723, 4.50213, 4.51221, 2.79895, 6.4508, -0.770261, 
>     7.62994, 2.5534, -2.43617, -17.015, 5.25272, -12.8585, -1.89404, 
>     24.5016, 24.5592, 21.4044, 11.1924, 7.70434, -4.09892, -4.29682, 
>     0.10429, 15.62, 15.2757, 1.12576, 12.3537, 6.28031, 6.28031, 
>     6.28031, 9.70625, 9.70625, 16.6939, 16.6939, -16.9003, -16.9003, 
>     -16.5469, -16.5469, -1.24881, 2.12382, -8.88365, 11.8552, 
>     18.6224, 22.4818, 17.7073, 7.05875, 5.60178, 7.24009, 3.48756, 
>     9.56937, 10.7906, 13.3156, 2.31384, -11.7071, -5.06014, 11.9814, 
>     9.25461, 10.2528, 1.04765, -3.92472, 0.97033, -6.84208, 7.41494, 
>     13.7583, 6.60244, 5.18133, -9.85804, -4.08612, 10.1503, 5.32709, 
>     4.55223, -0.413365, -5.7495, 3.18065, -5.70216, -7.24117, 
>     -7.31205, 12.0855, 4.33021, -4.98771, -2.93681), integer.Depth = c(5, 
>     5, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 
>     11, 12, 12, 13, 13, 14, 15, 15, 16, 16, 17, 17, 17, 18, 18, 
>     18, 19, 19, 19, 19, 20, 20, 20, 21, 21, 22, 22, 23, 23, 23, 
>     24, 24, 24, 25, 25, 5, 6, 6, 7, 7, 8, 8, 8, 8, 9, 9, 10, 
>     10, 11, 11, 12, 12, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 
>     17, 17, 18, 18, 18, 19, 19, 20, 20, 21, 22, 22, 23, 23, 24, 
>     24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 5, 6, 6, 6, 6, 
>     7, 7, 8, 8, 8, 9, 9, 10, 10, 10, 11, 11, 11, 12, 12, 13, 
>     13, 14, 14, 14, 15, 15, 16, 16, 17, 17, 18, 19, 19, 20, 21, 
>     21, 22, 22, 23, 23, 5, 6)), .Names = c("Cruise", "Station", 
> "Lon", "Lat", "Depth", "Temperature.oC", "Salinity", "Fluoresence.Volts", 
> "Density.kg.m3", "Brunt.Vaisala.Freq.cycl.h", "integer.Depth"
> ), row.names = c("19", "20", "21", "22", "23", "24", "25", "26", 
> "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", 
> "38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", 
> "49", "50", "51", "52", "53", "54", "55", "56", "57", "58", "59", 
> "60", "61", "62", "63", "64", "65", "66", "67", "68", "69", "70", 
> "71", "72", "88", "89", "90", "91", "92", "93", "94", "95", "96", 
> "97", "98", "99", "100", "101", "102", "103", "104", "105", "106", 
> "107", "108", "109", "110", "111", "112", "113", "114", "115", 
> "116", "117", "118", "119", "120", "121", "122", "123", "124", 
> "125", "126", "127", "128", "129", "130", "131", "132", "133", 
> "134", "135", "136", "137", "138", "139", "140", "154", "155", 
> "156", "157", "158", "159", "160", "161", "162", "163", "164", 
> "165", "166", "167", "168", "169", "170", "171", "172", "173", 
> "174", "175", "176", "177", "178", "179", "180", "181", "182", 
> "183", "184", "185", "186", "187", "188", "189", "190", "191", 
> "192", "193", "194", "209", "210"), class = "data.frame")
> 
> When I run this code on the full dataset, calculations continue long enough
> to suggest I am generating a huge matrix, so perhaps I'm doing something
> silly? Eventually (well, after maybe 5 minutes) I get a by class object of
> 109 rows (depth category) by 288 columns (station category), so it does seem
> to be working.
> 
> I know that you use sapply to get the by class back to a data.frame. I want
> to extract a matrix of mean densities (one of the original variables) at
> each of the 109 depths and 288 stations.
> I have not quite got this right...can you help?
>  
> Thanks in advance
> 
> Sam
> ----
> Sam McClatchie,
> Biological oceanography 
> South Australian Aquatic Sciences Centre

You might try the summarize function in the Hmisc package.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ggrothendieck at gmail.com  Mon May 30 14:42:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 May 2005 08:42:13 -0400
Subject: [R] Re: Vector Manipulation
In-Reply-To: <024201c564d7$1eca0730$6900a8c0@HCJP.COM>
References: <024201c564d7$1eca0730$6900a8c0@HCJP.COM>
Message-ID: <971536df05053005422b086fe0@mail.gmail.com>

x[cumsum(x!=0)!=0]

or 

x[!!cumsum(!!x)]

will also do it.

On 5/30/05, ManojW <manojsw at gmail.com> wrote:
> OK...x[min(which(x!=0)):length(x)] does the trick!
> 
> I guess the coffee is slowly but surely working! .
> 
> Manoj
>  ----- Original Message -----
>  From: ManojW
>  To: R-help
>  Sent: Monday, May 30, 2005 2:12 PM
>  Subject: Re: Vector Manipulation
> 
> 
>  I should clarify that I tried x[cumsum(x)!=0] but the problem is that I might have negative numbers in the vector that can potentially make cumsum(x) equal to zero somewhere down the line in the vector.
> 
>  Manoj
>    ----- Original Message -----
>    From: ManojW
>    To: R-help
>    Sent: Monday, May 30, 2005 2:00 PM
>    Subject: Vector Manipulation
> 
> 
>    Dear All,
>        For any given vector, I want to extract a sub-vector such that the new vector skips all zeros, if any , at the start of vector. Is it possible to achieve this w/o looping?
> 
>        E.G :     >    x    = c(0,0,1,2,3,4,5,0,0,8,9)
>                     >    y    = somefunc(x);
>                    > y
>                    [1]    1 2 3 4 5 0 0 8 9
> 
>        In the example above, I want to skip the two leading zeroes till I hit the non-zero (1 in the above case). I also want to retain all zero after the first non-zero digit (again 1 in this case).
> 
>        Hope I am not confusing you guys. Thanks in advance for your help.
> 
>    Regards
> 
>    Manoj
> 
> 
> 
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From charles-r-nospam at plessy.org  Mon May 30 14:56:57 2005
From: charles-r-nospam at plessy.org (Charles Plessy)
Date: Mon, 30 May 2005 21:56:57 +0900
Subject: [R] how to "singlify" entries
In-Reply-To: <BAY12-F3C0A0AEA46B66B1F32F64C7030@phx.gbl>
References: <BAY12-F3C0A0AEA46B66B1F32F64C7030@phx.gbl>
Message-ID: <20050530125657.GB32697@kunpuu.plessy.org>

On Mon, May 30, 2005 at 09:15:32AM +0000, zhihua li wrote :
> hi netters
> 
> I have a rather simple question.  I have a data frame with two variables X 
> and Y, both of which are factors. X has 100 levels while Y has 10 levels 
> only. The data frame has 100 rows in all, so for X the values are unique, 
> and Y has many replicate values.  Now I wanna reduce the data frame into 10 
> rows only, according to the 10 levels of Y.  I don't care which value of X 
> is in the same row with Y in the final data frame, as long as it is in 
> agreement with the original data frame.

Dear list,

I am a new subscriber, using R to analyse genomics data. I have a
similar question, maybe even identical, but I am not sure...

>From a data frame with two factors and one value, I would like to obtain a data
frame with one factor and one value per level in the removed factor.

For instance:

F1      F2      V
-----------------
A       X       3
A       Y       6
B       X       5
C       X       9
C       Y       3

Would become:

F1      VX      VY
------------------
A       3       6
B       5       0
C       9       3

I am sure I have seen a tool to do this some time ago, but I do not remember
its name.

Can somebody help me ?

Best regards,

-- 
Charles Plessy, Ph.D. - Genome Science Laboratory
The Institute for Physical and Chemical Research (RIKEN)
2-1 Hirosawa, Wako, Saitama 351-0198, Japan
plessy at riken.jp --  Fax: 048-462-4686  --  Tel: 048-467-9515



From ernestinan at hotdak.net  Sun May 29 18:29:21 2005
From: ernestinan at hotdak.net (michale baillet)
Date: Mon, 30 May 2005 02:29:21 +1000
Subject: [R] You can appreciate the reduced prices on rxdrugs and check our
	weekly specials.
Message-ID: <9C46876F.481D1D8@hotdak.net>

Check our cybershop and you will uncover how others cut their expenses on
rxmeds significantly. 
 
Check our cybershop for quality rxdrugs on male tissue dysfunction, pain,
affliction, relaxants for muscles, man's care, over-wt. and other
discomforts. Our company provides customers quick and reliable distribution
services.

You can appreciate the reduced prices on rxdrugs and check our weekly
specials.

http://xyne.4.sinkinpleasure.com/s6/

Have your case profile checked by licensed experts in a timely manner.


-----original message-----
From: Teodoro at ypmt.com [mailto:Sal at vgqr.com]
Sent: Thursday, March 6, 2005 4:02PM
To: Reginald; Carmen at vgvt.com; ; Roscoe; Dannie
Subject: The company provides complimentary case profile check. Our
customers choose us for this simple convenience.

I was not aware how much eshopping could reduce my expenses. Placing the
or-der at your cybershop is a right decision for me. I just got the tableets
the other day. The rxdrugs are as effective as the ones bought from the
local stores. Thank you for your great items. -- Cindy W. in OH


slowly se, Persian,     The Biblic books  evel that lift to pass and     
cont and
u are diseas'd,  inue beyond.  You are also asking me questions and I hear
you,  inquiringly,
or rheumatic, or a pros began
and prophets, and deep idyls of the Nazarene,     The 1 on the 46  Iliad,
Odyssey, plots, do other side,



From ggrothendieck at gmail.com  Mon May 30 15:09:27 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 May 2005 09:09:27 -0400
Subject: [R] how to "singlify" entries
In-Reply-To: <20050530125657.GB32697@kunpuu.plessy.org>
References: <BAY12-F3C0A0AEA46B66B1F32F64C7030@phx.gbl>
	<20050530125657.GB32697@kunpuu.plessy.org>
Message-ID: <971536df05053006097c7de8ec@mail.gmail.com>

On 5/30/05, Charles Plessy <charles-r-nospam at plessy.org> wrote:
> On Mon, May 30, 2005 at 09:15:32AM +0000, zhihua li wrote :
> > hi netters
> >
> > I have a rather simple question.  I have a data frame with two variables X
> > and Y, both of which are factors. X has 100 levels while Y has 10 levels
> > only. The data frame has 100 rows in all, so for X the values are unique,
> > and Y has many replicate values.  Now I wanna reduce the data frame into 10
> > rows only, according to the 10 levels of Y.  I don't care which value of X
> > is in the same row with Y in the final data frame, as long as it is in
> > agreement with the original data frame.
> 
> Dear list,
> 
> I am a new subscriber, using R to analyse genomics data. I have a
> similar question, maybe even identical, but I am not sure...
> 
> >From a data frame with two factors and one value, I would like to obtain a data
> frame with one factor and one value per level in the removed factor.
> 
> For instance:
> 
> F1      F2      V
> -----------------
> A       X       3
> A       Y       6
> B       X       5
> C       X       9
> C       Y       3
> 
> Would become:
> 
> F1      VX      VY
> ------------------
> A       3       6
> B       5       0
> C       9       3
> 
> I am sure I have seen a tool to do this some time ago, but I do not remember
> its name.
> 
Try using reshape, e.g. if dd is your data frame:

reshape(dd, dir = "wide", idvar = "F1", timevar = "F2", 
    varying = list(c("VX","VY")))



From luc_tardieu at yahoo.fr  Mon May 30 15:19:28 2005
From: luc_tardieu at yahoo.fr (luc tardieu)
Date: Mon, 30 May 2005 15:19:28 +0200 (CEST)
Subject: [R] values of bars in barplot
Message-ID: <20050530131928.27384.qmail@web25809.mail.ukl.yahoo.com>

Hi, 

I couldn't find how to have the values written on the
top of each bar in a barplot. When using hist(), it is
possible to use labels=T, but this option does not
seem to exist for barplot().

Is there a trick I could use to do that ?

Thanks to all

Luc



From petr.pikal at precheza.cz  Mon May 30 15:27:53 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 30 May 2005 15:27:53 +0200
Subject: [R] how to "singlify" entries
In-Reply-To: <20050530125657.GB32697@kunpuu.plessy.org>
References: <BAY12-F3C0A0AEA46B66B1F32F64C7030@phx.gbl>
Message-ID: <429B30F9.11001.1AD1D64@localhost>



On 30 May 2005 at 21:56, Charles Plessy wrote:

> On Mon, May 30, 2005 at 09:15:32AM +0000, zhihua li wrote :
> > hi netters
> > 
> > I have a rather simple question.  I have a data frame with two
> > variables X and Y, both of which are factors. X has 100 levels while
> > Y has 10 levels only. The data frame has 100 rows in all, so for X
> > the values are unique, and Y has many replicate values.  Now I wanna
> > reduce the data frame into 10 rows only, according to the 10 levels
> > of Y.  I don't care which value of X is in the same row with Y in
> > the final data frame, as long as it is in agreement with the
> > original data frame.
> 
> Dear list,
> 
> I am a new subscriber, using R to analyse genomics data. I have a
> similar question, maybe even identical, but I am not sure...

Hallo

Me neither.

?reshape

> dat<-read.table("clipboard", header=T)
> dat
  F1 F2 V
1  A  X 3
2  A  Y 6
3  B  X 5
4  C  X 9
5  C  Y 3
> reshape(dat, idvar="F1", timevar="F2",direction="wide")
  F1 V.X V.Y
1  A   3   6
3  B   5  NA
4  C   9   3


Homework: change NA to zero.

HTH
Petr

> 
> >From a data frame with two factors and one value, I would like to
> >obtain a data
> frame with one factor and one value per level in the removed factor.
> 
> For instance:
> 
> F1      F2      V
> -----------------
> A       X       3
> A       Y       6
> B       X       5
> C       X       9
> C       Y       3
> 
> Would become:
> 
> F1      VX      VY
> ------------------
> A       3       6
> B       5       0
> C       9       3
> 
> I am sure I have seen a tool to do this some time ago, but I do not
> remember its name.
> 
> Can somebody help me ?
> 
> Best regards,
> 
> -- 
> Charles Plessy, Ph.D. - Genome Science Laboratory
> The Institute for Physical and Chemical Research (RIKEN)
> 2-1 Hirosawa, Wako, Saitama 351-0198, Japan
> plessy at riken.jp --  Fax: 048-462-4686  --  Tel: 048-467-9515
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ggrothendieck at gmail.com  Mon May 30 15:31:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 May 2005 09:31:42 -0400
Subject: [R] values of bars in barplot
In-Reply-To: <20050530131928.27384.qmail@web25809.mail.ukl.yahoo.com>
References: <20050530131928.27384.qmail@web25809.mail.ukl.yahoo.com>
Message-ID: <971536df050530063168c842fc@mail.gmail.com>

On 5/30/05, luc tardieu <luc_tardieu at yahoo.fr> wrote:
> Hi,
> 
> I couldn't find how to have the values written on the
> top of each bar in a barplot. When using hist(), it is
> possible to use labels=T, but this option does not
> seem to exist for barplot().
> 
> Is there a trick I could use to do that ?
> 
> Thanks to all
> 

Here is an example you can modify:

x <- c(44,56,34,35,44,51,55)
nams <- LETTERS[1:7]
bp <- barplot(x,horiz=T,col="light blue",xlim=c(0,60))
text(30,-1.25,xpd=NA,"Networks",cex=1.5) # x title
text(-6,4,xpd=NA,"Research Areas",cex=1.5,srt=90)  # y title
text(x,bp,x,pos=4) # place numbers to right of bars
text(0,bp,nams,cex=1.2,pos=4)  # label bars right on the bars themselves



From j.van_den_hoff at fz-rossendorf.de  Mon May 30 15:32:47 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Mon, 30 May 2005 15:32:47 +0200
Subject: [R] values of bars in barplot
In-Reply-To: <20050530131928.27384.qmail@web25809.mail.ukl.yahoo.com>
References: <20050530131928.27384.qmail@web25809.mail.ukl.yahoo.com>
Message-ID: <429B15FF.7060203@fz-rossendorf.de>

luc tardieu wrote:
> Hi, 
> 
> I couldn't find how to have the values written on the
> top of each bar in a barplot. When using hist(), it is
> possible to use labels=T, but this option does not
> seem to exist for barplot().
> 
> Is there a trick I could use to do that ?
> 
> Thanks to all
> 
> Luc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

something like:


x <- 1:3
#ensure some free space above the bar and remember midpoints:
bmp <- barplot(x, ylim = c(0, 1.1*max(x)))

text(bmp, x + .03*max(x), x)


should do.



From petr.pikal at precheza.cz  Mon May 30 15:37:00 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 30 May 2005 15:37:00 +0200
Subject: [R] values of bars in barplot
In-Reply-To: <20050530131928.27384.qmail@web25809.mail.ukl.yahoo.com>
Message-ID: <429B331C.527.1B577E1@localhost>



On 30 May 2005 at 15:19, luc tardieu wrote:

> Hi, 
> 
> I couldn't find how to have the values written on the
> top of each bar in a barplot. When using hist(), it is
> possible to use labels=T, but this option does not
> seem to exist for barplot().

Hallo

>From help page

Value:

     A numeric vector (or matrix, when 'beside = TRUE'), say 'mp',
     giving the coordinates of _all_ the bar midpoints drawn, useful
     for adding to the graph.


e.g.

> dat<-trunc(runif(5)*10)
> dat
[1] 5 7 1 9 5
> bpl<-barplot(dat, ylim=c(0,10))
> text(bpl,dat+.2, dat)

HTH
Petr


> 
> Is there a trick I could use to do that ?
> 
> Thanks to all
> 
> Luc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From lutz.thieme at amd.com  Mon May 30 15:51:50 2005
From: lutz.thieme at amd.com (lutz.thieme@amd.com)
Date: Mon, 30 May 2005 15:51:50 +0200
Subject: [R] rbind wastes memory
Message-ID: <7C60FEAE2EE12D4BA1B232662554CDE12FDEC8@SF30EXMB1.amd.com>

Hello everybody,

if I try to (r)bind a number of large dataframes I run out of memory because R
wastes memory and seems to "forget" to release memory. 

For example I have 10 files. Each file contains a large dataframe "ds" (3500 cols 
by 800 rows) which needs ~20 MB RAM if it is loaded as the only object.
Now I try to bind all data frames to a large one and need more than 1165MB (!)
RAM (To simplify the R code, I use the same file ten times):

________ start example 1 __________
load(myFile)					
ds.tmp	<- ds					
for (Cycle in 1:10) {
	ds.tmp	<- rbind(ds.tmp, ds)
}
________ end example 1 __________



Stepping into details I found the following (comment shows RAM usage after this line 
was executed):
load(myFile)			# 40MB (19MB for R itself)
ds.tmp	<- ds			# 40MB; => only a pointer seems to be copied
x<-rbind(ds.tmp, ds)		# 198MB
x<-rbind(ds.tmp, ds)		# 233MB; the same instruction a second time leads to  
				# 35MB more RAM usage - why?


Now I played around, but I couldn't find a solution. For example I bound each dataframe 
step by step and removed the variables and cleared memory, but I still need 1140MB(!) 
RAM:

________ start example 2 __________
tmpFile<- paste(myFile,'.tmp',sep="")
load(myFile)
ds.tmp	 <- ds
save(ds.tmp, file=tmpFile, compress=T)

for (Cycle in 1:10) {
	ds	<- NULL
	ds.tmp <- NULL
	rm(ds, ds.tmp)
	gc()
	load(tmpFile)
	load(myFile)
	ds.tmp	<- rbind(ds.tmp, ds)
	save(ds.tmp,file=tmpFile, compress=T)
	cat(Cycle,': ',object.size(ds),object.size(ds.tmp),'\n')
}
________ end example 1 __________


platform i386-pc-solaris2.8
arch     i386              
os       solaris2.8        
system   i386, solaris2.8  
status                     
major    1                 
minor    9.1               
year     2004              
month    06                
day      21                
language R       




How can I avoid to run in that memory problem? Any ideas are very appreciated. 
Thank you in advance & kind regards,



Lutz Thieme
AMD Saxony/ Product Engineering	AMD Saxony Limited Liability Company & Co. KG
phone:	+ 49-351-277-4269		M/S E22-PE, Wilschdorfer Landstr. 101
fax:	+ 49-351-277-9-4269		D-01109 Dresden, Germany



From bates at stat.wisc.edu  Mon May 30 16:03:04 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 30 May 2005 09:03:04 -0500
Subject: [R] rbind wastes memory
In-Reply-To: <7C60FEAE2EE12D4BA1B232662554CDE12FDEC8@SF30EXMB1.amd.com>
References: <7C60FEAE2EE12D4BA1B232662554CDE12FDEC8@SF30EXMB1.amd.com>
Message-ID: <429B1D18.1000600@stat.wisc.edu>

lutz.thieme at amd.com wrote:
> Hello everybody,
> 
> if I try to (r)bind a number of large dataframes I run out of memory because R
> wastes memory and seems to "forget" to release memory. 
> 
> For example I have 10 files. Each file contains a large dataframe "ds" (3500 cols 
> by 800 rows) which needs ~20 MB RAM if it is loaded as the only object.
> Now I try to bind all data frames to a large one and need more than 1165MB (!)
> RAM (To simplify the R code, I use the same file ten times):
> 
> ________ start example 1 __________
> load(myFile)					
> ds.tmp	<- ds					
> for (Cycle in 1:10) {
> 	ds.tmp	<- rbind(ds.tmp, ds)
> }
> ________ end example 1 __________
> 
> 
> 
> Stepping into details I found the following (comment shows RAM usage after this line 
> was executed):
> load(myFile)			# 40MB (19MB for R itself)
> ds.tmp	<- ds			# 40MB; => only a pointer seems to be copied
> x<-rbind(ds.tmp, ds)		# 198MB
> x<-rbind(ds.tmp, ds)		# 233MB; the same instruction a second time leads to  
> 				# 35MB more RAM usage - why?
> 
> 
> Now I played around, but I couldn't find a solution. For example I bound each dataframe 
> step by step and removed the variables and cleared memory, but I still need 1140MB(!) 
> RAM:
> 
> ________ start example 2 __________
> tmpFile<- paste(myFile,'.tmp',sep="")
> load(myFile)
> ds.tmp	 <- ds
> save(ds.tmp, file=tmpFile, compress=T)
> 
> for (Cycle in 1:10) {
> 	ds	<- NULL
> 	ds.tmp <- NULL
> 	rm(ds, ds.tmp)
> 	gc()
> 	load(tmpFile)
> 	load(myFile)
> 	ds.tmp	<- rbind(ds.tmp, ds)
> 	save(ds.tmp,file=tmpFile, compress=T)
> 	cat(Cycle,': ',object.size(ds),object.size(ds.tmp),'\n')
> }
> ________ end example 1 __________
> 
> 
> platform i386-pc-solaris2.8
> arch     i386              
> os       solaris2.8        
> system   i386, solaris2.8  
> status                     
> major    1                 
> minor    9.1               
> year     2004              
> month    06                
> day      21                
> language R       
> 
> 
> 
> 
> How can I avoid to run in that memory problem? Any ideas are very appreciated. 
> Thank you in advance & kind regards,

If you are going to look at the memory usage you should use gc(), and
perhaps repeated calls to gc(), before checking the memory footprint.
This will force a garbage collection.

Also, you will probably save memory by treating your data frames as
lists and concatenating them, then converting the result to a data frame.



From murdoch at stats.uwo.ca  Mon May 30 16:08:01 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 30 May 2005 10:08:01 -0400
Subject: [R] rbind wastes memory
In-Reply-To: <7C60FEAE2EE12D4BA1B232662554CDE12FDEC8@SF30EXMB1.amd.com>
References: <7C60FEAE2EE12D4BA1B232662554CDE12FDEC8@SF30EXMB1.amd.com>
Message-ID: <429B1E41.5010802@stats.uwo.ca>

lutz.thieme at amd.com wrote:
> Hello everybody,
> 
> if I try to (r)bind a number of large dataframes I run out of memory because R
> wastes memory and seems to "forget" to release memory. 
> 
> For example I have 10 files. Each file contains a large dataframe "ds" (3500 cols 
> by 800 rows) which needs ~20 MB RAM if it is loaded as the only object.
> Now I try to bind all data frames to a large one and need more than 1165MB (!)
> RAM (To simplify the R code, I use the same file ten times):
> 
> ________ start example 1 __________
> load(myFile)					
> ds.tmp	<- ds					
> for (Cycle in 1:10) {
> 	ds.tmp	<- rbind(ds.tmp, ds)
> }
> ________ end example 1 __________
> 
> 
> 
> Stepping into details I found the following (comment shows RAM usage after this line 
> was executed):
> load(myFile)			# 40MB (19MB for R itself)
> ds.tmp	<- ds			# 40MB; => only a pointer seems to be copied
> x<-rbind(ds.tmp, ds)		# 198MB
> x<-rbind(ds.tmp, ds)		# 233MB; the same instruction a second time leads to  
> 				# 35MB more RAM usage - why?

I'm guessing your problem is fragmented memory.  You are creating big 
objects, then making them bigger.  This means R needs to go looking for 
large allocations for the replacements, but they won't fit in the spots 
left by the things you've deleted, so those are being left empty.

A solution to this is to use two passes:  first figure out how much 
space you need, then allocate it and fill it.  E.g.

for (Cycle in 1:10) {
     rows[Cycle] <- .... some calculation based on the data ...
}

ds.tmp <- data.frame(x=double(sum(rows)), y=double(sum(rows)), ...

for (Cycle in 1:10) {
     ds.tmp[ appropriate rows, ] <- new data
}


Duncan Murdoch



From Andreas.Friedrich at dit.de  Mon May 30 16:19:10 2005
From: Andreas.Friedrich at dit.de (Friedrich, Andreas (dit))
Date: Mon, 30 May 2005 16:19:10 +0200
Subject: [R] Egarch
Message-ID: <D86FDEC0CB88E54391764DDF2169CC9E03360D80@afwpm005.intradit.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050530/9703ffbf/attachment.pl

From martin.klaffenboeck at gmx.at  Mon May 30 16:25:54 2005
From: martin.klaffenboeck at gmx.at (Martin Klaffenboeck)
Date: Mon, 30 May 2005 16:25:54 +0200
Subject: [R] Reference Card?
Message-ID: <1117463154.17915.3.camel@localhost>

Hello!

For LaTeX I found a reference Card at
http://www.cs.ualberta.ca/~c603/LaTeX_docs/Symbol_Source/latex_symbols.pdf

Is there something available for R?

thanks,
Martin



From Kevin.Wang at maths.anu.edu.au  Mon May 30 16:38:02 2005
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Tue, 31 May 2005 00:38:02 +1000
Subject: [R] Reference Card?
In-Reply-To: <1117463154.17915.3.camel@localhost>
References: <1117463154.17915.3.camel@localhost>
Message-ID: <429B254A.3090304@maths.anu.edu.au>

Hi,

Have you tried looking under Documentation -> Contributed, under CRAN?

Kev

Martin Klaffenboeck wrote:
> Hello!
> 
> For LaTeX I found a reference Card at
> http://www.cs.ualberta.ca/~c603/LaTeX_docs/Symbol_Source/latex_symbols.pdf
> 
> Is there something available for R?
> 
> thanks,
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From Matthias.Templ at statistik.gv.at  Mon May 30 16:40:30 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 30 May 2005 16:40:30 +0200
Subject: [R] Reference Card?
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAAA5@xchg1.statistik.local>

> Hello!
> 
> For LaTeX I found a reference Card at 
> http://www.cs.ualberta.ca/~c603/LaTeX_docs/Symbol_Source/latex
> _symbols.pdf
> 
> Is there something available for R?

Hello Martin,

See the reference cards on
http://cran.r-project.org/other-docs.html

Best,
Matthias

> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From h.andersson at nioo.knaw.nl  Mon May 30 16:43:20 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Mon, 30 May 2005 16:43:20 +0200
Subject: [R] Formatting numbers with a limited amount of digits consistently
Message-ID: <d7f8op$fa3$1@sea.gmane.org>

I have tried to get signif, round and format to display numbers like 
these consistently in a table, using e.g. signif(x,digits=3)

17.01
18.15

I want

17.0
18.2

Not

17
18.2


Why is the last digit stripped off in the case when it is zero!

Is this a "feature" of R or did I miss something?



---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From charles-r-nospam at plessy.org  Mon May 30 16:54:59 2005
From: charles-r-nospam at plessy.org (Charles Plessy)
Date: Mon, 30 May 2005 23:54:59 +0900
Subject: [R] how to "singlify" entries
In-Reply-To: <971536df05053006097c7de8ec@mail.gmail.com>
References: <BAY12-F3C0A0AEA46B66B1F32F64C7030@phx.gbl>
	<20050530125657.GB32697@kunpuu.plessy.org>
	<971536df05053006097c7de8ec@mail.gmail.com>
Message-ID: <20050530145459.GB999@kunpuu.plessy.org>

On Mon, May 30, 2005 at 09:09:27AM -0400, Gabor Grothendieck wrote :

> Try using reshape, e.g. if dd is your data frame:
> 
> reshape(dd, dir = "wide", idvar = "F1", timevar = "F2", 
>     varying = list(c("VX","VY")))


Thank you very much, and to Petr Pikal too. Reshape is exactly what I had forgotten.

Now the bad news is that I have simplified my example ; I am in a
slightly more complex situation :

I have three factors, and one value

> count_per_tc[1:10,]
   rna   lib           tc x
1  CAB 114BA T01F00380F47 1
2  CAE 114BB T01F00381273 1
3  CAJ 114BA T01F0048F6D1 1
4  CAB 114BC T01F0048F6D1 1
5  CAB 114BA T01F00498689 2
6  CAC 114BA T01F00498689 1
7  CAE 114BA T01F00498689 2
8  CAG 114BA T01F00498689 2
9  CAH 114BA T01F00498689 1
10 CAI 114BA T01F00498689 2

I would like a data frame where I have the value of x for each combination of
"rna" and "lib", for each "tc"

> reshape(count_per_tc[1:10,], direction="wide", timevar="tc", idvar=c("rna","lib"))
   rna   lib x.T01F00380F47 x.T01F00381273 x.T01F0048F6D1 x.T01F00498689
1  CAB 114BA              1             NA             NA              2
2  CAE 114BB             NA              1             NA             NA
3  CAJ 114BA             NA             NA              1             NA
4  CAB 114BC             NA             NA              1             NA
6  CAC 114BA             NA             NA             NA              1
7  CAE 114BA             NA             NA             NA              2
8  CAG 114BA             NA             NA             NA              2
9  CAH 114BA             NA             NA             NA              1
10 CAI 114BA             NA             NA             NA              2

oops, the other way round :

> t(reshape(count_per_tc[1:10,], direction="wide", timevar="tc", idvar=c("rna","lib")))
               1       2       3       4       6       7       8       9       10     
rna            "CAB"   "CAE"   "CAJ"   "CAB"   "CAC"   "CAE"   "CAG"   "CAH"   "CAI"  
lib            "114BA" "114BB" "114BA" "114BC" "114BA" "114BA" "114BA" "114BA" "114BA"
x.T01F00380F47 " 1"    NA      NA      NA      NA      NA      NA      NA      NA     
x.T01F00381273 NA      " 1"    NA      NA      NA      NA      NA      NA      NA     
x.T01F0048F6D1 NA      NA      " 1"    " 1"    NA      NA      NA      NA      NA     
x.T01F00498689 " 2"    NA      NA      NA      " 1"    " 2"    " 2"    " 1"    " 2"   

The ultimate goal is (after proper renaming of the columns) to do things like

plot(CAA-114BA[CAA-114BA >0 & CAA-114BB > 0], CAA-114BB[CAA-114BA >0 & CAA-114BB > 0])

(this combination will appear if I reshape the whole data frame, which has 200,000 rows.)

and then proper statistical tests (which I still have to learn / remember from
12 years ago).

once again, thank you, and please warn me if I am doing something stupid with
this transposition of the reshaped table.

Best regards,

-- 
Charles



From rpeng at jhsph.edu  Mon May 30 17:00:12 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 30 May 2005 11:00:12 -0400
Subject: [R] rbind wastes memory
In-Reply-To: <7C60FEAE2EE12D4BA1B232662554CDE12FDEC8@SF30EXMB1.amd.com>
References: <7C60FEAE2EE12D4BA1B232662554CDE12FDEC8@SF30EXMB1.amd.com>
Message-ID: <429B2A7C.20304@jhsph.edu>

Rather than 'rbind' in a loop, try putting your dataframes in a list and 
then doing something like 'do.call("rbind", list.of.data.frames")'.

-roger

lutz.thieme at amd.com wrote:
> Hello everybody,
> 
> if I try to (r)bind a number of large dataframes I run out of memory because R
> wastes memory and seems to "forget" to release memory. 
> 
> For example I have 10 files. Each file contains a large dataframe "ds" (3500 cols 
> by 800 rows) which needs ~20 MB RAM if it is loaded as the only object.
> Now I try to bind all data frames to a large one and need more than 1165MB (!)
> RAM (To simplify the R code, I use the same file ten times):
> 
> ________ start example 1 __________
> load(myFile)					
> ds.tmp	<- ds					
> for (Cycle in 1:10) {
> 	ds.tmp	<- rbind(ds.tmp, ds)
> }
> ________ end example 1 __________
> 
> 
> 
> Stepping into details I found the following (comment shows RAM usage after this line 
> was executed):
> load(myFile)			# 40MB (19MB for R itself)
> ds.tmp	<- ds			# 40MB; => only a pointer seems to be copied
> x<-rbind(ds.tmp, ds)		# 198MB
> x<-rbind(ds.tmp, ds)		# 233MB; the same instruction a second time leads to  
> 				# 35MB more RAM usage - why?
> 
> 
> Now I played around, but I couldn't find a solution. For example I bound each dataframe 
> step by step and removed the variables and cleared memory, but I still need 1140MB(!) 
> RAM:
> 
> ________ start example 2 __________
> tmpFile<- paste(myFile,'.tmp',sep="")
> load(myFile)
> ds.tmp	 <- ds
> save(ds.tmp, file=tmpFile, compress=T)
> 
> for (Cycle in 1:10) {
> 	ds	<- NULL
> 	ds.tmp <- NULL
> 	rm(ds, ds.tmp)
> 	gc()
> 	load(tmpFile)
> 	load(myFile)
> 	ds.tmp	<- rbind(ds.tmp, ds)
> 	save(ds.tmp,file=tmpFile, compress=T)
> 	cat(Cycle,': ',object.size(ds),object.size(ds.tmp),'\n')
> }
> ________ end example 1 __________
> 
> 
> platform i386-pc-solaris2.8
> arch     i386              
> os       solaris2.8        
> system   i386, solaris2.8  
> status                     
> major    1                 
> minor    9.1               
> year     2004              
> month    06                
> day      21                
> language R       
> 
> 
> 
> 
> How can I avoid to run in that memory problem? Any ideas are very appreciated. 
> Thank you in advance & kind regards,
> 
> 
> 
> Lutz Thieme
> AMD Saxony/ Product Engineering	AMD Saxony Limited Liability Company & Co. KG
> phone:	+ 49-351-277-4269		M/S E22-PE, Wilschdorfer Landstr. 101
> fax:	+ 49-351-277-9-4269		D-01109 Dresden, Germany
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From zangshizhu at yahoo.com.cn  Mon May 30 17:11:02 2005
From: zangshizhu at yahoo.com.cn (shizhu zang)
Date: Mon, 30 May 2005 23:11:02 +0800 (CST)
Subject: [R] a question about read.marrayRaw 
Message-ID: <20050530151102.9502.qmail@web15801.mail.cnb.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050530/1dc3bedb/attachment.pl

From tlumley at u.washington.edu  Mon May 30 17:45:29 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 30 May 2005 08:45:29 -0700 (PDT)
Subject: [R] Incompatibility with VGAM
In-Reply-To: <429A4185.9060009@auckland.ac.nz>
References: <429A4185.9060009@auckland.ac.nz>
Message-ID: <Pine.A41.4.61b.0505300842090.75024@homer12.u.washington.edu>

On Mon, 30 May 2005, Thomas Yee wrote:

> Hello,
>
>
> It seems that if glm used a namespace then the conflict would be avoided?
>

No.  glm does use a namespace, so this can't be true.  Remember that R 
passes arguments by value, and consider
   glm(y~x, family=poisson())

The namespace ensures that functions called from glm are looked up in the 
stats namespace, but family() not called from glm(). It is called by the 
user and the result is passed as an argument to glm().

The exception is when no family argument is supplied. In that case the 
default argument is created by a call to family() from inside glm(), which 
should always find stats::family

 	-thomas



From amsa36060 at yahoo.com  Mon May 30 17:47:11 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Mon, 30 May 2005 08:47:11 -0700 (PDT)
Subject: [R] How to access to sum of dissimilarities in CLARA
Message-ID: <20050530154711.65676.qmail@web60418.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050530/463d1c68/attachment.pl

From charles.edwin.white at us.army.mil  Mon May 30 17:51:50 2005
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Mon, 30 May 2005 11:51:50 -0400
Subject: [R] R GUI for Linux?
Message-ID: <8BAEC5E546879B4FAA536200A292C6140DD112@AMEDMLNARMC135.amed.ds.army.mil>

I feel your pain. <grin> I am a new Linux user who has spent most of the weekend trying to get a functional R setup. When I installed Fedora Core 3 (FC3) on my home computer, I thought using R in a terminal would be a snap. I installed R using the rpm packages and tried to use it with the FC3 default terminal (GNOME Terminal 2.7.3). Before long, I found out the terminal was rudely discarding output beyond a set number of lines. I could increase the number of lines kept by the terminal but that didn't strike me as an acceptable solution. Cutting to my stress relieving intermediate solution, I am currently using xemacs with ESS as my R programming environment under FC3. Eventually, I will want to run JGR as my programming environment and Rcmdr as both a teaching tool and means to distribute code to some of my clients. On my way to xemacs, I also tried to install emacs and gnomeGUI. I will briefly document my experience with trying to install each of these packages below:

XEMACS with ESS: XEMACS is within the 'walled garden' of packages tuned specifically to run under FC3 and XEMACS provides a tuned installation for ESS. Since I had already compiled R from source with shared libraries enabled (the rmp does not enable shared libraries), I don't know if XEMACS will work with the rpm version of R. Note also that I installed this package using yum; 'Add or Remove Applications' lists xemacs but wouldn't allow me to install.

JGR: I have installed jdk1.5.0_03 and MOST of the the output from make looks like JGR is compiling correctly. JavaGD and rJava are not finding jini.h. I don't see an explicit statement of how to start JGR but I assume that is done by typing JGR (or maybe jgr) in a terminal window. Nothing happens. Two potential problems are: (a) I never should have downloaded JavaGD and rJava from CRAN (they won't uninstall, deleting the directories doesn't stop the problem, and I can't use yum to remove R to start over because yum doesn't recognise that R is installed.) or (b) I need to uninstall some of the stray versions of java littering my hard drive. I haven't removed the symbolic link between jre1.5.0_02 and firefox.

Rcmdr: There are all sorts of things in FC3 that seem to be tcl/tk related but Rcmdr doesn't seem to work with them. Since some are part of the base FC3 installation, I'm nervious about replacing them or installing competing software. Potentially conflicting software in FC3 are listed below:

tcl.i386                                 8.4.7-2                installed
tclx.i386                                8.3.5-4                installed
db4-tcl.i386                             4.2.52-6               base
postgresql-tcl.i386                      7.4.8-1.FC3.1          updates-released
ruby-tcltk.i386                          1.8.2-1.FC3.1          updates-released
tcl-devel.i386                           8.4.7-2                base
tcl-html.i386                            8.4.7-2                base
tclx-devel.i386                          8.3.5-4                base
tclx-doc.i386                            8.3.5-4                base

EMACS with ESS: A version of EMACS is tuned to FC3 but ESS has to be obtained elsewhere. Installing ESS requires editing the hidden .emacs file. I know how to see hidden files but this one does not appear to be where the ESS directions say to look.

gnomeGUI: Error message says that I don't have Gnome installed. It would be nice and GNU to have a working copy of gnome GUI.

I hope you have found my message to be entertaining. If anybody can stop me before I do something else stupid, I'd appreciate it. Thanks.

Chuck

Charles E. White, Senior Biostatistician, MS
Walter Reed Army Institute of Research
503 Robert Grant Ave., Room 1w102
Silver Spring, MD 20910-1557
Personal/Professional Site: http://users.starpower.net/cwhite571/professional/



From Matthias.Templ at statistik.gv.at  Mon May 30 18:14:08 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 30 May 2005 18:14:08 +0200
Subject: [R] How to access to sum of dissimilarities in CLARA
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAAA6@xchg1.statistik.local>

Hello,

#Example:

data(xclara)
p <- clara(xclara,3)
names(p)
p$diss

Best,
Matthias

> Dear All ,
> Since dissimilarity is one of quality measures in clustering 
> , I'm trying to access to the sum of dissimilarity as a whole 
> measure. But after running my data using CLARA I obtain : 
> 1128 dissimilarities, summarized :
>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
> 0.033155 0.934630 2.257000 2.941600 4.876600 8.943700 
> But I can not find the sum of dissimilarity.How can i access to it?
>  
> Thanks a lot
> Safari
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Mon May 30 18:24:43 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 30 May 2005 09:24:43 -0700 (PDT)
Subject: [R] how to invert the matrix with quite small eigenvalues
In-Reply-To: <bfc676680505300239ce69f4d@mail.gmail.com>
References: <bfc6766805053000202d6fa5ed@mail.gmail.com>
	<XFMail.050530094424.Ted.Harding@nessie.mcc.ac.uk>
	<bfc676680505300239ce69f4d@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0505300853270.75024@homer12.u.washington.edu>

On Mon, 30 May 2005, huang min wrote:>
> My intention is to invert the covariance matrix to perform some
> algorithm which is common in the estimating equations like GEE.

In that case there is no benefit in being able to invert very extreme 
covariance matrices. The asymptotic approximations to the distribution of 
regression coefficients will break down really badly with such extreme 
working covariance matrices.

I think in a case like this you should either
1) report an error and stop
2) shrink the covariance matrix towards a diagonal one, eg increase the 
diagonal entries until the condition number becomes reasonable.
3) Use a one-step estimator from the independence working model (which is 
asymptotically equivalent to the full solution and better behaved).

Remember that in GEE the matrix V^{-1} is just a set of weights, chosen to 
get good efficiency.   Your matrix solve(a) is not a good set of weights.

I think in an earlier thread on this topic Brian Ripley recommended using 
the singular value decomposition if you really have to compute something 
like D^TV^{-1}D.  In your example this still isn't good enough.

>
> Another strange point is that my friend use the LU decomposition in
> Fortran to solve the inverse matrix of aa for me. He used double
> precision of course, otherwise no inverse matrix in Fortran too. He
> show that the product of the two matrix is almost identity(The biggest
> off-digonal element is about 1e-8). I copy his inverse matrix(with 31
> digits!) to R and read in aa I sent to him(16 digits). The product is
> also not an identity matrix. It is fairly strange! Any suggestion?
>

It isn't that strange.  The system is computationally singular, meaning 
that you should expect to get different answers with apparently similar 
computations on different systems.

Also remember that what you care about for GEE is the result of 
solve(a,y-mu), rather than solve(a,a). Getting one of these right is no 
guarantee of getting the other one right.

If you really had to work with this matrix in double precision you would 
need to track very carefully the error bounds on all your computations, 
which would be very difficult.  Fortunately this is almost never necessary 
in statistics, and I don't think it's necessary in your case.

A good habit to get into when you aren't tracking error bounds carefully 
is to think of the last couple of digits of the result of any calculation 
as random.


 	-thomas



From 0034058 at fudan.edu.cn  Mon May 30 18:29:38 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 31 May 2005 00:29:38 +0800
Subject: [R] problem in R2.2-dev
Message-ID: <0IHB00BBJ9C1FD@mail.fudan.edu.cn>

when i install the R2.2-dev(customed install,the with the chinese translation msg ,etc.),i found the tanslated GUI can not display correctly.totally unlike Chinese characters.



From murdoch at stats.uwo.ca  Mon May 30 19:20:50 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 30 May 2005 13:20:50 -0400
Subject: [R] Formatting numbers with a limited amount of digits
	consistently
In-Reply-To: <d7f8op$fa3$1@sea.gmane.org>
References: <d7f8op$fa3$1@sea.gmane.org>
Message-ID: <429B4B72.9000109@stats.uwo.ca>

Henrik Andersson wrote:
> I have tried to get signif, round and format to display numbers like 
> these consistently in a table, using e.g. signif(x,digits=3)
> 
> 17.01
> 18.15
> 
> I want
> 
> 17.0
> 18.2
> 
> Not
> 
> 17
> 18.2
> 
> 
> Why is the last digit stripped off in the case when it is zero!

signif() changes the value; you don't want that, you want to affect how 
a number is displayed.  Use format() or formatC() instead, for example

 > x <- c(17.01, 18.15)
 > format(x, digits=3)
[1] "17.0" "18.1"
 > noquote(format(x, digits=3))
[1] 17.0 18.1

> Is this a "feature" of R or did I miss something?

I'd say both.

Duncan Murdoch



From murdoch at stats.uwo.ca  Mon May 30 19:23:10 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 30 May 2005 13:23:10 -0400
Subject: [R] problem in R2.2-dev
In-Reply-To: <0IHB00BBJ9C1FD@mail.fudan.edu.cn>
References: <0IHB00BBJ9C1FD@mail.fudan.edu.cn>
Message-ID: <429B4BFE.3020107@stats.uwo.ca>

ronggui wrote:
> when i install the R2.2-dev(customed install,the with the chinese translation msg ,etc.),i found the tanslated GUI can not display correctly.totally unlike Chinese characters.

I don't think you've given nearly enough information for anyone to act
on this.  Please describe what OS you're on, what customizations you did
during installation, and give an example of something that should work
but doesn't.

Hopefully someone who uses a Chinese locale will be able to help you.

Duncan Murdoch



From ggrothendieck at gmail.com  Mon May 30 19:57:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 May 2005 13:57:28 -0400
Subject: [R] Formatting numbers with a limited amount of digits
	consistently
In-Reply-To: <429B4B72.9000109@stats.uwo.ca>
References: <d7f8op$fa3$1@sea.gmane.org> <429B4B72.9000109@stats.uwo.ca>
Message-ID: <971536df05053010571ad00617@mail.gmail.com>

On 5/30/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> Henrik Andersson wrote:
> > I have tried to get signif, round and format to display numbers like
> > these consistently in a table, using e.g. signif(x,digits=3)
> >
> > 17.01
> > 18.15
> >
> > I want
> >
> > 17.0
> > 18.2
> >
> > Not
> >
> > 17
> > 18.2
> >
> >
> > Why is the last digit stripped off in the case when it is zero!
> 
> signif() changes the value; you don't want that, you want to affect how
> a number is displayed.  Use format() or formatC() instead, for example
> 
>  > x <- c(17.01, 18.15)
>  > format(x, digits=3)
> [1] "17.0" "18.1"
>  > noquote(format(x, digits=3))
> [1] 17.0 18.1
> 

That works in the above context but I don't think it works generally:

R> f <- head(faithful)
R> f
  eruptions waiting
1     3.600      79
2     1.800      54
3     3.333      74
4     2.283      62
5     4.533      85
6     2.883      55

R> format(f, digits = 3)
  eruptions waiting
1      3.60      79
2      1.80      54
3      3.33      74
4      2.28      62
5      4.53      85
6      2.88      55

R> # this works in this case
R> noquote(prettyNum(round(f,1), nsmall = 1))
     eruptions waiting
[1,] 3.6       79.0   
[2,] 1.8       54.0   
[3,] 3.3       74.0   
[4,] 2.3       62.0   
[5,] 4.5       85.0   
[6,] 2.9       55.0   

and even that does not work in the desired way (which presumably
is not to use exponent format) if you have some
large enough numbers like 1e6 which it will display using
the e notation rather than using ordinary notation.

R> f[1,1] <- 1e6 + 0.11
R> noquote(prettyNum(round(f,1), nsmall = 1))
     eruptions waiting
[1,] 1.0e+06   79.0   
[2,] 1.8e+00   54.0   
[3,] 3.3e+00   74.0   
[4,] 2.3e+00   62.0   
[5,] 4.5e+00   85.0   
[6,] 2.9e+00   55.0   

I have struggled with this myself and have generally been able
to come up with something for specific instances but I have generally 
found it a pain to do a simple thing like format a table exactly as I want 
without undue effort.  Maybe someone else has figured this out.



From slist at oomvanlieshout.net  Mon May 30 20:29:33 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Mon, 30 May 2005 20:29:33 +0200
Subject: [R] R GUI for Linux?
In-Reply-To: <8BAEC5E546879B4FAA536200A292C6140DD112@AMEDMLNARMC135.amed.ds.army.mil>
References: <8BAEC5E546879B4FAA536200A292C6140DD112@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <429B5B8D.9070402@oomvanlieshout.net>

Hi Charles,

Warm felt sympathies for your struggles. I consider myself a happy GUI 
user and have also struggled with the 'command line' history and lack of 
out-of-the-box functionality associated with Linux. However, Linux does 
have many, many advantages over other OS's, so I will stick to it. That 
said, I am reluctantly suggesting it to others, as there are lots of 
other things one can do on a Saturday afternoon. ;-)

After many long afternoons, I have come to the conclusion that accepting 
what comes as standard is the best approach to using Linux if you do 
have better things to do. I tried ESS, but I found it impenetrable at my 
first try, so gave up.

I run SuSE linux and R without any problem. The RPM was downloaded from 
CRAN and installed without any errors. I run update.packages to 
download, install and refresh all contributed and other packages.

I also looked for an R GUI, but must admit I stopped way before you did. 
I ended up using my favorite editor (Kate, comes standard with KDE) and 
cut and paste code into an R console (available as standard in the Kate 
window). Gives a nice clean window for writing code and running R. Of 
course Kate has code highlighting facilities (as standard). I soon 
realized that the console has a limited buffer for commands, such that 
long code sequences are abruptly ended part way through. Thus I have a 
second console open from which I source whole R files. The setup gives a 
relatively comfortable code debugging environment!

I was amazed when I read the instruction for JGR on Linux. I thought the 
whole point of Java was to create platform independent software. I have 
given up on any instructions that tell me to run 'make'. Linux 
distributions are just to idiosyncratic for it to be worth the effort. 
I'll just wait until Novell packs JGR with the new version of SuSE 
Linux. Even if I have to pay something for the added bonus!

Will keep following the GUI developments with interest,

Sander.


White, Charles E WRAIR-Wash DC wrote:
> I feel your pain. <grin> I am a new Linux user who has spent most of the weekend trying to get a functional R setup. When I installed Fedora Core 3 (FC3) on my home computer, I thought using R in a terminal would be a snap. I installed R using the rpm packages and tried to use it with the FC3 default terminal (GNOME Terminal 2.7.3). Before long, I found out the terminal was rudely discarding output beyond a set number of lines. I could increase the number of lines kept by the terminal but that didn't strike me as an acceptable solution. Cutting to my stress relieving intermediate solution, I am currently using xemacs with ESS as my R programming environment under FC3. Eventually, I will want to run JGR as my programming environment and Rcmdr as both a teaching tool and means to distribute code to some of my clients. On my way to xemacs, I also tried to install emacs and gnomeGUI. I will briefly document my experience with trying to install each of these packages below:
> 
> XEMACS with ESS: XEMACS is within the 'walled garden' of packages tuned specifically to run under FC3 and XEMACS provides a tuned installation for ESS. Since I had already compiled R from source with shared libraries enabled (the rmp does not enable shared libraries), I don't know if XEMACS will work with the rpm version of R. Note also that I installed this package using yum; 'Add or Remove Applications' lists xemacs but wouldn't allow me to install.
> 
> JGR: I have installed jdk1.5.0_03 and MOST of the the output from make looks like JGR is compiling correctly. JavaGD and rJava are not finding jini.h. I don't see an explicit statement of how to start JGR but I assume that is done by typing JGR (or maybe jgr) in a terminal window. Nothing happens. Two potential problems are: (a) I never should have downloaded JavaGD and rJava from CRAN (they won't uninstall, deleting the directories doesn't stop the problem, and I can't use yum to remove R to start over because yum doesn't recognise that R is installed.) or (b) I need to uninstall some of the stray versions of java littering my hard drive. I haven't removed the symbolic link between jre1.5.0_02 and firefox.
> 
> Rcmdr: There are all sorts of things in FC3 that seem to be tcl/tk related but Rcmdr doesn't seem to work with them. Since some are part of the base FC3 installation, I'm nervious about replacing them or installing competing software. Potentially conflicting software in FC3 are listed below:
> 
> tcl.i386                                 8.4.7-2                installed
> tclx.i386                                8.3.5-4                installed
> db4-tcl.i386                             4.2.52-6               base
> postgresql-tcl.i386                      7.4.8-1.FC3.1          updates-released
> ruby-tcltk.i386                          1.8.2-1.FC3.1          updates-released
> tcl-devel.i386                           8.4.7-2                base
> tcl-html.i386                            8.4.7-2                base
> tclx-devel.i386                          8.3.5-4                base
> tclx-doc.i386                            8.3.5-4                base
> 
> EMACS with ESS: A version of EMACS is tuned to FC3 but ESS has to be obtained elsewhere. Installing ESS requires editing the hidden .emacs file. I know how to see hidden files but this one does not appear to be where the ESS directions say to look.
> 
> gnomeGUI: Error message says that I don't have Gnome installed. It would be nice and GNU to have a working copy of gnome GUI.
> 
> I hope you have found my message to be entertaining. If anybody can stop me before I do something else stupid, I'd appreciate it. Thanks.
> 
> Chuck
> 
> Charles E. White, Senior Biostatistician, MS
> Walter Reed Army Institute of Research
> 503 Robert Grant Ave., Room 1w102
> Silver Spring, MD 20910-1557
> Personal/Professional Site: http://users.starpower.net/cwhite571/professional/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From murdoch at stats.uwo.ca  Mon May 30 21:12:37 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 30 May 2005 15:12:37 -0400
Subject: [R] Formatting numbers with a limited amount of digits
	consistently
In-Reply-To: <971536df05053010571ad00617@mail.gmail.com>
References: <d7f8op$fa3$1@sea.gmane.org> <429B4B72.9000109@stats.uwo.ca>
	<971536df05053010571ad00617@mail.gmail.com>
Message-ID: <429B65A5.3000503@stats.uwo.ca>

Gabor Grothendieck wrote:
> On 5/30/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> 
>>Henrik Andersson wrote:
>>
>>>I have tried to get signif, round and format to display numbers like
>>>these consistently in a table, using e.g. signif(x,digits=3)
>>>
>>>17.01
>>>18.15
>>>
>>>I want
>>>
>>>17.0
>>>18.2
>>>
>>>Not
>>>
>>>17
>>>18.2
>>>
>>>
>>>Why is the last digit stripped off in the case when it is zero!
>>
>>signif() changes the value; you don't want that, you want to affect how
>>a number is displayed.  Use format() or formatC() instead, for example
>>
>> > x <- c(17.01, 18.15)
>> > format(x, digits=3)
>>[1] "17.0" "18.1"
>> > noquote(format(x, digits=3))
>>[1] 17.0 18.1
>>
> 
> 
> That works in the above context but I don't think it works generally:
> 
> R> f <- head(faithful)
> R> f
>   eruptions waiting
> 1     3.600      79
> 2     1.800      54
> 3     3.333      74
> 4     2.283      62
> 5     4.533      85
> 6     2.883      55
> 
> R> format(f, digits = 3)
>   eruptions waiting
> 1      3.60      79
> 2      1.80      54
> 3      3.33      74
> 4      2.28      62
> 5      4.53      85
> 6      2.88      55
> 
> R> # this works in this case
> R> noquote(prettyNum(round(f,1), nsmall = 1))
>      eruptions waiting
> [1,] 3.6       79.0   
> [2,] 1.8       54.0   
> [3,] 3.3       74.0   
> [4,] 2.3       62.0   
> [5,] 4.5       85.0   
> [6,] 2.9       55.0   
> 
> and even that does not work in the desired way (which presumably
> is not to use exponent format) if you have some
> large enough numbers like 1e6 which it will display using
> the e notation rather than using ordinary notation.

formatC with format="f" seems to work for me, though it assumes you're 
specifying decimal places rather than significant digits.  It also wants 
a vector of numbers as input, not a dataframe.  So the following gives 
pretty flexible control over what a table will look like:

 > data.frame(eruptions = formatC(f$eruptions, digits=2, format='f'),
+            waiting = formatC(f$waiting, digits=1, format='f'))
    eruptions waiting
1 1000000.11    79.0
2       1.80    54.0
3       3.33    74.0
4       2.28    62.0
5       4.53    85.0
6       2.88    55.0

> 
> I have struggled with this myself and have generally been able
> to come up with something for specific instances but I have generally 
> found it a pain to do a simple thing like format a table exactly as I want 
> without undue effort.  Maybe someone else has figured this out.

I think that formatting tables properly requires some thought, and R is 
no good at thinking.  You can easily recognize a badly formatted table, 
but it's very hard to write down rules that work in general 
circumstances.  It's also a matter of taste, so if I managed to write a 
function that matched my taste, you would find you wanted to make changes.

It's sort of like expecting plot(x, y) to always come up with the best 
possible plot of y versus x.  It's just not a reasonable expectation. 
It's better to provide tools (like abline() for plots or formatC() for 
tables) that allow you to tailor a plot or table to your particular needs.

Duncan Murdoch



From tarmo.remmel at utoronto.ca  Mon May 30 21:50:10 2005
From: tarmo.remmel at utoronto.ca (Tarmo Remmel)
Date: Mon, 30 May 2005 15:50:10 -0400
Subject: [R] Unique arrangements of a vector
Message-ID: <BEEKLAMKBJMOPPLNMKNLIEBECOAA.tarmo.remmel@utoronto.ca>

Dear List,

Running on a PC (Windows 2000) with 256 MB RAM, Version R1.9.1

I have a relatively simple problem, which I can solve for relatively small
datasets, but run into difficulties with larger ones.  I believe that my
approach is a hack rather than something elegant and I was hoping that
somebody on this list might help me improve my code.  Basically, given a
vector of values (e.g., 0,0,1,1), I want to generate all of the unique
arrangements of these values, of which there are 4!/(2!2!) = 6.

0 0 1 1
0 1 0 1
0 1 1 0
1 0 0 1
1 0 1 0
1 1 0 0

Using unique() in conjunction with expand.grid(), and later filtering
impossible results, I can obtain the answer.  However, this is slow, and
does not work for large initial vectors and is difficult to filter when
using values beyond 0,1.  Is there some mathematically elegant method for
doing this?  I'd hope to have initial vectors significantly longer than the
demonstrated 4 values (e.g., thousands).

Any help is appreciated and I will gladly SUM afterwards.

Thank you,

Tarmo

__________________________________________
Tarmo Remmel  Ph.D.
GUESS Lab, Department of Geography
University of Toronto at Mississauga
Mississauga, Ontario, L5L 1C6
Tel: 905-828-3868
Fax: 905-828-5273
Skype: tarmoremmel
http://eratos.erin.utoronto.ca/remmelt



From intuitionist at gmail.com  Mon May 30 21:57:29 2005
From: intuitionist at gmail.com (Aamir M)
Date: Mon, 30 May 2005 15:57:29 -0400
Subject: [R] "FANNY" function in R package "cluster"
Message-ID: <b93b870405053012573f5d8d35@mail.gmail.com>

Dear All,

I am attempting to use the FANNY fuzzy clustering function in R
(Kaufman & Rousseeuw, 1990), found in the "cluster" package. I have
run into a variety of difficulties; the two most crucial difficulties
are enumerated below.

1. Where is the 'm' parameter in FANNY? 
In _Finding Groups in Data: An Introduction to Cluster Analysis_
(1990) by Kaufman & Rousseeuw, the authors discuss the FANNY
algorithm. On pages 189-190, they attempt to demonstrate an
equivalence between Fuzzy c-Means (FCM) and FANNY. In doing so they,
appear to be assuming that the value of the 'm' parameter in FCM (a
measure of the fuzziness) is fixed at m=2. Although this is how FCM
was originally formulated, it eventually became apparent that m should
be a user-specified parameter and not a fixed value of m=2.  My
question, then, is twofold. Firstly, am I correct in saying that
Kaufman & Rousseeuw have assumed m=2 in their formulation of Fuzzy
c-Means and FANNY? Secondly, is it possible to modify the FANNY
algorithm to allow user-specification of the m (fuzziness) parameter?

2. What do I do with training data?
Is there any way to use FANNY for assigning clustering membership
values to new, test data? In Fuzzy c-Means, new data is compared to
the cluster centers in order to assign clustering membership values to
the test data. However, in FANNY these centers do not exist. Is there,
then, any way to compute the FANNY clustering membership values of a
test data point without affecting the clustering membership values of
the training data? Perhaps there are enough conditions to use the
objective function as a way of computing the membership values of the
test data?

Aamir M
University of Toronto



From jfox at mcmaster.ca  Mon May 30 22:20:04 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 30 May 2005 16:20:04 -0400
Subject: [R] R GUI for Linux?
In-Reply-To: <8BAEC5E546879B4FAA536200A292C6140DD112@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <20050530202003.FEON27508.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Charles,


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of White, 
> Charles E WRAIR-Wash DC
> Sent: Monday, May 30, 2005 10:52 AM
> To: r-help at stat.math.ethz.ch
> Cc: sander at oomvanlieshout.net
> Subject: [R] R GUI for Linux?
> 

. . .

> 
> Rcmdr: There are all sorts of things in FC3 that seem to be 
> tcl/tk related but Rcmdr doesn't seem to work with them. 
> Since some are part of the base FC3 installation, I'm 
> nervious about replacing them or installing competing 
> software. Potentially conflicting software in FC3 are listed below:
> 
> tcl.i386                                 8.4.7-2              
>   installed
> tclx.i386                                8.3.5-4              
>   installed
> db4-tcl.i386                             4.2.52-6               base
> postgresql-tcl.i386                      7.4.8-1.FC3.1        
>   updates-released
> ruby-tcltk.i386                          1.8.2-1.FC3.1        
>   updates-released
> tcl-devel.i386                           8.4.7-2                base
> tcl-html.i386                            8.4.7-2                base
> tclx-devel.i386                          8.3.5-4                base
> tclx-doc.i386                            8.3.5-4                base
> 

. . .

I haven't tried the Rcmdr package with FC3, but I ran it under an older
version of Red Hat Linux some time ago, and the current version (Rcmdr
1.0-2) under Quantian.

Can you be more specific about the problems that you encountered? Are these
general to the tcltk package or specific to the Rcmdr?

I'm sorry that you're experiencing problems.

John



From jjorgensen at fastmail.fm  Mon May 30 22:53:47 2005
From: jjorgensen at fastmail.fm (jjorgensen@fastmail.fm)
Date: Mon, 30 May 2005 15:53:47 -0500
Subject: [R] persp, add lines/highlights
Message-ID: <1117486427.1669.235259524@webmail.messagingengine.com>

Hello R-sters,

I'm trying to add several lines to a response surface that I've plotted
using persp().  I've tried lines() using the "trans3d" function but I've
been unsuccessful in getting it to work (R v2.0.1).  Essentially, I'm
trying to highlight one or more of the surface wireframe lines in a
bolder (or different) color.  Any tips from those of you who have some
experience with this would be greatly appreciated.  [Would it be easier
using wireframe() in library(lattice) instead?]

And, any suggestions on how to add text outside of the persp() plot next
to the highlighted line would be much appreciated.

Thanks,

Jeff Jorgensen



From p.connolly at hortresearch.co.nz  Tue May 31 01:33:48 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 31 May 2005 11:33:48 +1200
Subject: [R] R GUI for Linux?
In-Reply-To: <429B5B8D.9070402@oomvanlieshout.net>
References: <8BAEC5E546879B4FAA536200A292C6140DD112@AMEDMLNARMC135.amed.ds.a
	rmy.mil> <429B5B8D.9070402@oomvanlieshout.net>
Message-ID: <20050530233348.GD17105@hortresearch.co.nz>

On Mon, 30-May-2005 at 08:29PM +0200, Sander Oom wrote:


|> ...... I have 
|> given up on any instructions that tell me to run 'make'. Linux 
|> distributions are just to[o] idiosyncratic for it to be worth the effort. 

That might be true of Linux distributions in general, but installing R
is *very* easy.  There are a couple of lines you can paste from the
README file to get the configure and make thing working.  Depending on
your machine, it might take some time to do all its stuff which is
extensive and thorough.

Installing ESS is only slightly different.  Simply extract the tar.gz
file, find the README file and adjust your .emacs file to let it know
where you've installed ESS.

Granted, it does take a bit to get used to the way Emacs does things,
but you need only about 1% of its capability and then it is extremely
easy to use.  Many questions asked on this list just don't arise.

best

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From amandal at isye.gatech.edu  Tue May 31 01:38:58 2005
From: amandal at isye.gatech.edu (Abhyuday Mandal)
Date: Mon, 30 May 2005 19:38:58 -0400 (EDT)
Subject: [R] Piecewise Linear Regression
Message-ID: <Pine.GSO.4.10.10505301928280.26258-100000@castle.isye.gatech.edu>

Hi,

I need to fit a piecewise linear regression.

x = c(6.25,6.25,12.50,12.50,18.75,25.00,25.00,25.00,31.25,31.25,37.50,37.50,50.00,50.00,62.50,62.50,75.00,75.00,75.00,100.00,100.00)
y = c(0.328,0.395,0.321,0.239,0.282,0.230,0.273,0.347,0.211,0.210,0.259,0.186,0.301,0.270,0.252,0.247,0.277,0.229,0.225,0.168,0.202)

there are two change points. so the fitted curve should look like



\ 
 \  /\
  \/  \
       \
        \

How do I do this in R ?

Thank you,
Abhyuday



From sdavis2 at mail.nih.gov  Tue May 31 01:51:13 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 30 May 2005 19:51:13 -0400
Subject: [R] Piecewise Linear Regression
References: <Pine.GSO.4.10.10505301928280.26258-100000@castle.isye.gatech.edu>
Message-ID: <001901c56572$76285900$5179f345@WATSON>

Abhyuday,

There are a number of answers in the archives:

http://www.google.com/u/newcastlemaths?q=piecewise+linear+regression&sa=Google+Search

Do any of those meet your needs?

Sean

----- Original Message ----- 
From: "Abhyuday Mandal" <amandal at isye.gatech.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, May 30, 2005 7:38 PM
Subject: [R] Piecewise Linear Regression


> Hi,
>
> I need to fit a piecewise linear regression.
>
> x = 
> c(6.25,6.25,12.50,12.50,18.75,25.00,25.00,25.00,31.25,31.25,37.50,37.50,50.00,50.00,62.50,62.50,75.00,75.00,75.00,100.00,100.00)
> y = 
> c(0.328,0.395,0.321,0.239,0.282,0.230,0.273,0.347,0.211,0.210,0.259,0.186,0.301,0.270,0.252,0.247,0.277,0.229,0.225,0.168,0.202)
>
> there are two change points. so the fitted curve should look like
>
>
>
> \
> \  /\
>  \/  \
>       \
>        \
>
> How do I do this in R ?
>
> Thank you,
> Abhyuday
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From chess.player at oninet.pt  Tue May 31 02:11:51 2005
From: chess.player at oninet.pt (jose silva)
Date: Tue, 31 May 2005 01:11:51 +0100
Subject: [R] labels on map
Message-ID: <4444dccc0b554860b26ffd234630135f@oninet.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/bcbd3a7e/attachment.pl

From p.murrell at auckland.ac.nz  Tue May 31 02:14:34 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 31 May 2005 12:14:34 +1200
Subject: [R] persp, add lines/highlights
References: <1117486427.1669.235259524@webmail.messagingengine.com>
Message-ID: <429BAC6A.1010306@stat.auckland.ac.nz>

Hi


jjorgensen at fastmail.fm wrote:
> Hello R-sters,
> 
> I'm trying to add several lines to a response surface that I've plotted
> using persp().  I've tried lines() using the "trans3d" function but I've
> been unsuccessful in getting it to work (R v2.0.1).  Essentially, I'm
> trying to highlight one or more of the surface wireframe lines in a
> bolder (or different) color.  Any tips from those of you who have some
> experience with this would be greatly appreciated.  [Would it be easier
> using wireframe() in library(lattice) instead?]


Here's an example that just overlays two persp() plots ...

x <- seq(-10, 10, length= 30)
y <- x
f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
z <- outer(x, y, f)
z[is.na(z)] <- 1
op <- par(bg = "white")
persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue",
       zlim=range(z))

# overlay plot with just "highlighted" surface area
par(new=TRUE)
z2 <- matrix(NA, ncol=30, nrow=30)
xi <- 15:18
yi <- 13:14
z2[xi, yi] <- z[xi, yi]
persp(x, y, z2, theta = 30, phi = 30, expand = 0.5,
       zlim=range(z), border="red", col="pink", box=FALSE, axes=FALSE)


> And, any suggestions on how to add text outside of the persp() plot next
> to the highlighted line would be much appreciated.


Do you mean like a legend?

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From roger at ysidro.econ.uiuc.edu  Tue May 31 02:23:19 2005
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Mon, 30 May 2005 19:23:19 -0500
Subject: [R] Piecewise Linear Regression
In-Reply-To: <Pine.GSO.4.10.10505301928280.26258-100000@castle.isye.gatech.edu>
References: <Pine.GSO.4.10.10505301928280.26258-100000@castle.isye.gatech.edu>
Message-ID: <0C6A6F03-FE36-4D61-96E7-9CCB3835E31C@ysidro.econ.uiuc.edu>

It is conventional to fit piecewise linear models by assuming  
Gaussian error and
using least squares methods, but one can argue that median regression  
provides
a more robust approach to this problem.  You might consider the  
following fit:

  x = c 
(6.25,6.25,12.50,12.50,18.75,25.00,25.00,25.00,31.25,31.25,37.50,37.50,5 
0.00,50.00,62.50,62.50,75.00,75.00,75.00,100.00,100.00)
  y = c 
(0.328,0.395,0.321,0.239,0.282,0.230,0.273,0.347,0.211,0.210,0.259,0.186 
,0.301,0.270,0.252,0.247,0.277,0.229,0.225,0.168,0.202)
library(quantreg)
plot(x,y)
fit <- rqss(y ~ qss(x))
plot(fit)

it gives 5 segments not 3, but this can be controlled by the choice  
of lambda in the qss
function, for example, try:

fit <- rqss(y ~ qss(x,lambda=3)
plot(fit,col="red")

which gives a fit like you suggest might be reasonable with only  
three segments.



url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On May 30, 2005, at 6:38 PM, Abhyuday Mandal wrote:

> Hi,
>
> I need to fit a piecewise linear regression.
>
> x = c 
> (6.25,6.25,12.50,12.50,18.75,25.00,25.00,25.00,31.25,31.25,37.50,37.50 
> ,50.00,50.00,62.50,62.50,75.00,75.00,75.00,100.00,100.00)
> y = c 
> (0.328,0.395,0.321,0.239,0.282,0.230,0.273,0.347,0.211,0.210,0.259,0.1 
> 86,0.301,0.270,0.252,0.247,0.277,0.229,0.225,0.168,0.202)
>
> there are two change points. so the fitted curve should look like
>
>
>
> \
>  \  /\
>   \/  \
>        \
>         \
>
> How do I do this in R ?
>
> Thank you,
> Abhyuday
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From murdoch at stats.uwo.ca  Tue May 31 02:29:41 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 30 May 2005 20:29:41 -0400
Subject: [R] labels on map
In-Reply-To: <4444dccc0b554860b26ffd234630135f@oninet.pt>
References: <4444dccc0b554860b26ffd234630135f@oninet.pt>
Message-ID: <429BAFF5.9050602@stats.uwo.ca>

jose silva wrote:
> dear all:
> 
> Im trying to obtain maps on R, under mapdata library, but I cannot define the labels.
> Here is an example:
> 
> library(mapdata)
> map("worldHires", c("portugal","spain"),ylim=c(34,46),xlim=c(-14,3.5))
> axis(1,at=seq(-12,3,3))
> axis(2)
> 
> when I try the parameter xlab or ylab in axis, i get: 
> parameter "ylab" couldn't be set in high-level plot() function 
> 
> any suggestion?  thanks in advance for your always useful advices

Despite their names, x and y axis labels aren't really part of the axis, 
they're titles in the margins of the plot.  Use the title() function to 
set them.

You could probably also set them in map(), but I don't have it installed 
to check.  mtext() is another general purpose way to put text in the 
margins.

Duncan Murdoch



From Tom.Mulholland at dpi.wa.gov.au  Tue May 31 02:42:09 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 31 May 2005 08:42:09 +0800
Subject: [R] labels on map
Message-ID: <4702645135092E4497088F71D9C8F51A128B82@afhex01.dpi.wa.gov.au>

What makes you think that there is a ylab parameter?

> args(map)
function (database = "world", regions = ".", exact = FALSE, boundary = TRUE, 
    interior = TRUE, projection = "", parameters = NULL, orientation = NULL, 
    fill = FALSE, col = 1, plot = TRUE, add = FALSE, namesonly = FALSE, 
    xlim = NULL, ylim = NULL, wrap = FALSE, resolution = if (plot) 1 else 0, 
    type = "l", bg = par("bg"), mar = c(0, 0, par("mar")[3], 
        0.1), border = 0.01, ...) 
NULL
>

Just because there are ellipses does not mean that ylab is an appropriate parameter. If you type map you'll see the function. Go looking for where the ellipses are used and you find

polygon(coord, col = col, ...)
lines(coord, col = col, type = type, ...)

So how why would these functions take a ylab parameter, they are for use within a plot. You have to remember that each function will only do what the author has programmed it to do.

So if you need to put a label somewhere you could always use mtext(side = 2,line = 1,"A label on side 2")

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of jose silva
> Sent: Tuesday, 31 May 2005 8:12 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] labels on map
> 
> 
> dear all:
> 
> Im trying to obtain maps on R, under mapdata library, but I 
> cannot define the labels.
> Here is an example:
> 
> library(mapdata)
> map("worldHires", c("portugal","spain"),ylim=c(34,46),xlim=c(-14,3.5))
> axis(1,at=seq(-12,3,3))
> axis(2)
> 
> when I try the parameter xlab or ylab in axis, i get: 
> parameter "ylab" couldn't be set in high-level plot() function??
> 
> any suggestion??? thanks in advance for your always useful advices
> 
> j. silva
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From chess.player at oninet.pt  Tue May 31 02:47:54 2005
From: chess.player at oninet.pt (jose silva)
Date: Tue, 31 May 2005 01:47:54 +0100
Subject: [R] labels on map
Message-ID: <3e0d170b54df47d19d06b1abdc9fb74b@oninet.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/8e44dd57/attachment.pl

From charles.edwin.white at us.army.mil  Tue May 31 05:00:39 2005
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Mon, 30 May 2005 23:00:39 -0400
Subject: [R] R GUI for Linux?
Message-ID: <8BAEC5E546879B4FAA536200A292C6140DD114@AMEDMLNARMC135.amed.ds.army.mil>

John:

Thank you for your interest. After more investigation I see that my problem is with linking tcltk to R. tcl 8.4.7-2 and tk 8.4.7-2 are tuned to fc3 (Fedora Core 3) and part of the standard installation. However, required file locations are different than what is expected by R. I set the locations for tclConfig.sh and tkConfig.sh using ./configure but R tells me something to the effect that Tcl and Tk major or minor versions disagree. Your help or the help of others on the R-List would be appreciated. FYI, I am also including the locations of important files and directories.

tclConfig.sh: /usr/lib
tkConfig.sh:  /usr/lib
tcl library:  /usr/lib/tcl8.4 (symbolically linked from /usr/share/tcl8.4)
tk library:   /usr/lib/tk8.4 (separate copy in /usr/share/tk8.4)
tcl.h:        I can't find it with full drive search
tk.h:         I can't find it with full drive search

Thanks for your time.

Chuck
-----Original Message-----
From: John Fox [mailto:jfox at mcmaster.ca]
Sent: Mon 5/30/2005 4:20 PM
To: White, Charles E WRAIR-Wash DC
Cc: sander at oomvanlieshout.net; r-help at stat.math.ethz.ch
Subject: RE: [R] R GUI for Linux?
 
Dear Charles,


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of White, 
> Charles E WRAIR-Wash DC
> Sent: Monday, May 30, 2005 10:52 AM
> To: r-help at stat.math.ethz.ch
> Cc: sander at oomvanlieshout.net
> Subject: [R] R GUI for Linux?
> 

. . .

I haven't tried the Rcmdr package with FC3, but I ran it under an older
version of Red Hat Linux some time ago, and the current version (Rcmdr
1.0-2) under Quantian.

Can you be more specific about the problems that you encountered? Are these
general to the tcltk package or specific to the Rcmdr?

I'm sorry that you're experiencing problems.

John



From ggrothendieck at gmail.com  Tue May 31 05:19:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 May 2005 23:19:09 -0400
Subject: [R] Problem going back to a viewport with gridBase
Message-ID: <971536df05053020191ebe8f44@mail.gmail.com>

I am setting up base plots -- one in viewport A and and one in B.  This part
works fine.  But if I go back to A after having done B and add
horizontal lines it seems
to not use the correct coordinates.  How do I tell it to resume using A's
coordinates?  I am already using par(fig = gridFIG()) but it seems that that's
not enough to reestablish them.  What happens is that when I go back to 
A it draws the horizontal lines as if its relative to B's coordinates
rather than
restablishing A's coordinates.  As a result the horizontal lines are
drawn near the
bottom of the graph instead of at the correct heights.  Try running the code
below to see what I mean.

I have also tried to use baseViewports with this but did not have any
success.

How do I modify this example so that the horizontal red lines come out
at the appropriate levels?    Note that this is just an example and in 
the future I will want to have multiple viewports each with a base plot and
add arbitrary additional line or point plots to them so the solution needs
to be sufficiently general that I can so generalize it.

Thanks.


library(gridBase)

opar <- par(no.readonly = TRUE)
grid.newpage()

# two columns, one row
unit. <- unit(c(1,1), c("null","null"))
pushViewport(viewport(layout = grid.layout(1, 2, widths = unit.)))

# draw green graph in first column (viewport A)
pushViewport(viewport(layout.pos.col = 1, name = "A"))
par(fig = gridFIG()); par(new = TRUE)
plot(1:10, col = "green", pch = 20)
upViewport(1)

# draw purple graph in second column (viewport B)
pushViewport(viewport(layout.pos.col = 2, name = "B"))
par(fig = gridFIG()); par(new = TRUE)
plot(1:100, col = "purple", pch = 18)
upViewport()

# go back to A and add horizontal grid lines
seekViewport("A")
par(fig = gridFIG())
abline(h=1:10, col = "red")  #### THESE DO NOT GET DRAWN AS EXPECTED
popViewport()

# go back to B and add vertical grid lines
seekViewport("B")
par(fig = gridFIG())
abline(v=1:10, col = "red")
popViewport()
par(opar)



From ggrothendieck at gmail.com  Tue May 31 05:53:20 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 May 2005 23:53:20 -0400
Subject: [R] Formatting numbers with a limited amount of digits
	consistently
In-Reply-To: <429B65A5.3000503@stats.uwo.ca>
References: <d7f8op$fa3$1@sea.gmane.org> <429B4B72.9000109@stats.uwo.ca>
	<971536df05053010571ad00617@mail.gmail.com>
	<429B65A5.3000503@stats.uwo.ca>
Message-ID: <971536df05053020533592e59c@mail.gmail.com>

On 5/30/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> Gabor Grothendieck wrote:
> > On 5/30/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> >
> >>Henrik Andersson wrote:
> >>
> >>>I have tried to get signif, round and format to display numbers like
> >>>these consistently in a table, using e.g. signif(x,digits=3)
> >>>
> >>>17.01
> >>>18.15
> >>>
> >>>I want
> >>>
> >>>17.0
> >>>18.2
> >>>
> >>>Not
> >>>
> >>>17
> >>>18.2
> >>>
> >>>
> >>>Why is the last digit stripped off in the case when it is zero!
> >>
> >>signif() changes the value; you don't want that, you want to affect how
> >>a number is displayed.  Use format() or formatC() instead, for example
> >>
> >> > x <- c(17.01, 18.15)
> >> > format(x, digits=3)
> >>[1] "17.0" "18.1"
> >> > noquote(format(x, digits=3))
> >>[1] 17.0 18.1
> >>
> >
> >
> > That works in the above context but I don't think it works generally:
> >
> > R> f <- head(faithful)
> > R> f
> >   eruptions waiting
> > 1     3.600      79
> > 2     1.800      54
> > 3     3.333      74
> > 4     2.283      62
> > 5     4.533      85
> > 6     2.883      55
> >
> > R> format(f, digits = 3)
> >   eruptions waiting
> > 1      3.60      79
> > 2      1.80      54
> > 3      3.33      74
> > 4      2.28      62
> > 5      4.53      85
> > 6      2.88      55
> >
> > R> # this works in this case
> > R> noquote(prettyNum(round(f,1), nsmall = 1))
> >      eruptions waiting
> > [1,] 3.6       79.0
> > [2,] 1.8       54.0
> > [3,] 3.3       74.0
> > [4,] 2.3       62.0
> > [5,] 4.5       85.0
> > [6,] 2.9       55.0
> >
> > and even that does not work in the desired way (which presumably
> > is not to use exponent format) if you have some
> > large enough numbers like 1e6 which it will display using
> > the e notation rather than using ordinary notation.
> 
> formatC with format="f" seems to work for me, though it assumes you're
> specifying decimal places rather than significant digits.  It also wants
> a vector of numbers as input, not a dataframe.  So the following gives
> pretty flexible control over what a table will look like:
> 
>  > data.frame(eruptions = formatC(f$eruptions, digits=2, format='f'),
> +            waiting = formatC(f$waiting, digits=1, format='f'))
>    eruptions waiting
> 1 1000000.11    79.0
> 2       1.80    54.0
> 3       3.33    74.0
> 4       2.28    62.0
> 5       4.53    85.0
> 6       2.88    55.0
> 
> >
> > I have struggled with this myself and have generally been able
> > to come up with something for specific instances but I have generally
> > found it a pain to do a simple thing like format a table exactly as I want
> > without undue effort.  Maybe someone else has figured this out.
> 
> I think that formatting tables properly requires some thought, and R is
> no good at thinking.  You can easily recognize a badly formatted table,
> but it's very hard to write down rules that work in general
> circumstances.  It's also a matter of taste, so if I managed to write a
> function that matched my taste, you would find you wanted to make changes.
> 
> It's sort of like expecting plot(x, y) to always come up with the best
> possible plot of y versus x.  It's just not a reasonable expectation.
> It's better to provide tools (like abline() for plots or formatC() for
> tables) that allow you to tailor a plot or table to your particular needs.
> 

Thanks.  That seems to be the idiom I was missing.  One thing that would
be nice would be if formatC could handle data frames.



From ajayshah at mayin.org  Mon May 30 19:19:51 2005
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 30 May 2005 22:49:51 +0530 (IST)
Subject: [R] Trying to write a linear regression using MLE and optim()
Message-ID: <20050530171951.B88A313C9B4@lubyanka.local>


I wrote this:

# Setup problem
x <- runif(100)
y <- 2 + 3*x + rnorm(100)
X <- cbind(1, x)

# True OLS --
lm(y ~ x)

# OLS likelihood function --
ols.lf <- function(theta, K, y, X) {
  beta <- theta[1:K]
  sigma <- exp(theta[K+1])
  e <- (y - X%*%beta)/sigma
  logl <- sum(log(dnorm(e)))
  return(logl)
}

optim(c(2,3,0), ols.lf, gr=NULL,
      method="BFGS", lower=-Inf, upper=Inf,
      control=list(trace=2, fnscale=-1),
      # Now for the "..." stuff
      K, y, X)


I get:

Error in fn(par, ...) : argument "X" is missing, with no default
In addition: Warning message:
numerical expression has 100 elements: only the first used in: 1:K 
Execution halted

If someone can show me the way, it'll be most appreciated. :-)



From shigesong at gmail.com  Tue May 31 07:49:15 2005
From: shigesong at gmail.com (Shige Song)
Date: Mon, 30 May 2005 22:49:15 -0700
Subject: [R] Installing RGL on SuSE 9.3
Message-ID: <5abc11d805053022495b90c554@mail.gmail.com>

I am running R 2.10 on SuSE 9.3. When I tried to install the package
"rgl", I got error message:

...
In file included from pixmap.cpp:13:
pngpixmap.h: In static member function `static void
   PNGPixmapFormat::Load::info_callback(png_struct*, png_info*)':
pngpixmap.h:149: error: invalid conversion from `long unsigned int*' to `
   png_uint_32*'
pngpixmap.h:149: error: invalid conversion from `long unsigned int*' to `
   png_uint_32*'
make: *** [pixmap.o] Error 1
ERROR: compilation failed for package 'rgl'
** Removing '/usr/local/lib/R/library/rgl'
** Restoring previous '/usr/local/lib/R/library/rgl'
...

Any ideas why?

Shige



From ggrothendieck at gmail.com  Tue May 31 08:06:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 May 2005 02:06:32 -0400
Subject: [R] Trying to write a linear regression using MLE and optim()
In-Reply-To: <20050530171951.B88A313C9B4@lubyanka.local>
References: <20050530171951.B88A313C9B4@lubyanka.local>
Message-ID: <971536df05053023061a990425@mail.gmail.com>

On 5/30/05, Ajay Shah <ajayshah at mayin.org> wrote:
> 
> I wrote this:
> 
> # Setup problem
> x <- runif(100)
> y <- 2 + 3*x + rnorm(100)
> X <- cbind(1, x)
> 
> # True OLS --
> lm(y ~ x)
> 
> # OLS likelihood function --
> ols.lf <- function(theta, K, y, X) {
>  beta <- theta[1:K]
>  sigma <- exp(theta[K+1])
>  e <- (y - X%*%beta)/sigma
>  logl <- sum(log(dnorm(e)))
>  return(logl)
> }
> 
> optim(c(2,3,0), ols.lf, gr=NULL,
>      method="BFGS", lower=-Inf, upper=Inf,
>      control=list(trace=2, fnscale=-1),
>      # Now for the "..." stuff
>      K, y, X)

The last line should be:

K=K, y=y, X=X)

and also you have to set K.


> 
> 
> I get:
> 
> Error in fn(par, ...) : argument "X" is missing, with no default
> In addition: Warning message:
> numerical expression has 100 elements: only the first used in: 1:K
> Execution halted
> 
> If someone can show me the way, it'll be most appreciated. :-)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Tue May 31 09:05:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 May 2005 09:05:25 +0200
Subject: [R] Unique arrangements of a vector
In-Reply-To: <BEEKLAMKBJMOPPLNMKNLIEBECOAA.tarmo.remmel@utoronto.ca>
References: <BEEKLAMKBJMOPPLNMKNLIEBECOAA.tarmo.remmel@utoronto.ca>
Message-ID: <429C0CB5.2050201@statistik.uni-dortmund.de>

Tarmo Remmel wrote:

> Dear List,
> 
> Running on a PC (Windows 2000) with 256 MB RAM, Version R1.9.1

This one is quite outdated...

> I have a relatively simple problem, which I can solve for relatively small
> datasets, but run into difficulties with larger ones.  I believe that my
> approach is a hack rather than something elegant and I was hoping that
> somebody on this list might help me improve my code.  Basically, given a
> vector of values (e.g., 0,0,1,1), I want to generate all of the unique
> arrangements of these values, of which there are 4!/(2!2!) = 6.
> 
> 0 0 1 1
> 0 1 0 1
> 0 1 1 0
> 1 0 0 1
> 1 0 1 0
> 1 1 0 0
> 
> Using unique() in conjunction with expand.grid(), and later filtering
> impossible results, I can obtain the answer.  However, this is slow, and
> does not work for large initial vectors and is difficult to filter when
> using values beyond 0,1.  Is there some mathematically elegant method for
> doing this?  I'd hope to have initial vectors significantly longer than the
> demonstrated 4 values (e.g., thousands).

Nice for length 4, but you will get problems far sooner than for length 
1000... please calculate the size before!
For thing as short as 4, you might want to try out permutations() in 
package "gtools" (formerly in bundle gregmisc, since yesterday a single 
package).

Uwe Ligges


> Any help is appreciated and I will gladly SUM afterwards.
> 
> Thank you,
> 
> Tarmo
> 
> __________________________________________
> Tarmo Remmel  Ph.D.
> GUESS Lab, Department of Geography
> University of Toronto at Mississauga
> Mississauga, Ontario, L5L 1C6
> Tel: 905-828-3868
> Fax: 905-828-5273
> Skype: tarmoremmel
> http://eratos.erin.utoronto.ca/remmelt
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gavin.simpson at ucl.ac.uk  Tue May 31 09:29:49 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 31 May 2005 08:29:49 +0100
Subject: [R] R GUI for Linux?
In-Reply-To: <8BAEC5E546879B4FAA536200A292C6140DD114@AMEDMLNARMC135.amed.ds.army.mil>
References: <8BAEC5E546879B4FAA536200A292C6140DD114@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <429C126D.1080209@ucl.ac.uk>

White, Charles E WRAIR-Wash DC wrote:
> John:
> 
> Thank you for your interest. After more investigation I see that my
> problem is with linking tcltk to R. tcl 8.4.7-2 and tk 8.4.7-2 are
> tuned to fc3 (Fedora Core 3) and part of the standard installation.
> However, required file locations are different than what is expected
> by R. I set the locations for tclConfig.sh and tkConfig.sh using
> ./configure but R tells me something to the effect that Tcl and Tk
> major or minor versions disagree. Your help or the help of others on
> the R-List would be appreciated. FYI, I am also including the
> locations of important files and directories.
> 
> tclConfig.sh: /usr/lib tkConfig.sh:  /usr/lib tcl library:
> /usr/lib/tcl8.4 (symbolically linked from /usr/share/tcl8.4) tk
> library:   /usr/lib/tk8.4 (separate copy in /usr/share/tk8.4) tcl.h:
> I can't find it with full drive search tk.h:         I can't find it
> with full drive search

You need to install the devel packages for tcl and tk to get those 
header files.

on FC3, the preferred updater is yum so in a terminal:

su -c "yum install tcl-devel tk-devel"

Should install the *.h files for you.

HTH

Gav



From navarre_sabine at yahoo.fr  Tue May 31 10:22:26 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Tue, 31 May 2005 10:22:26 +0200 (CEST)
Subject: [R] Barplot2 Title
Message-ID: <20050531082226.58868.qmail@web26602.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/8aeeb113/attachment.pl

From maechler at stat.math.ethz.ch  Tue May 31 10:40:32 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 31 May 2005 10:40:32 +0200
Subject: [R] "FANNY" function in R package "cluster"
In-Reply-To: <b93b870405053012573f5d8d35@mail.gmail.com>
References: <b93b870405053012573f5d8d35@mail.gmail.com>
Message-ID: <17052.8960.155858.341883@stat.math.ethz.ch>

>>>>> "Aamir" == Aamir M <intuitionist at gmail.com>
>>>>>     on Mon, 30 May 2005 15:57:29 -0400 writes:

    Aamir> Dear All, I am attempting to use the FANNY fuzzy
    Aamir> clustering function in R (Kaufman & Rousseeuw, 1990),
    Aamir> found in the "cluster" package. I have run into a
    Aamir> variety of difficulties; the two most crucial
    Aamir> difficulties are enumerated below.

    Aamir> 1. Where is the 'm' parameter in FANNY?  In _Finding
    Aamir> Groups in Data: An Introduction to Cluster Analysis_
    Aamir> (1990) by Kaufman & Rousseeuw, the authors discuss
    Aamir> the FANNY algorithm. On pages 189-190, they attempt
    Aamir> to demonstrate an equivalence between Fuzzy c-Means
    Aamir> (FCM) and FANNY. 

The section is called "*Related* Methods and References"
and they (as most Statisticians) use the word "k-Means"...

The first point in that section is to explain the
NON-equivalence:  fanny() works with arbitrary dissimilarities d[i,j]
whereas all versions of k-means have to assume a euclidean
measurement space.  K&R show that *when* you have a data matrix X,
and then use distances  d[i,j] :=  || x_{.,i} - x_{.,j} ||^2
(i.e. SQUARED Euclidean distances), then fanny() does the same
as fuzzy k-means.  But they even say there, that they'd rather
use the non-squared distance {as fanny() does by default}, i.e.,
exponent 1, not 2,  for good robustness reasons.  If this is
your 'm' below, then fanny() already does what you might want by default.
OTOH, you can always pass squared distances to fanny() ..

    Aamir> In doing so they, appear to be
    Aamir> assuming that the value of the 'm' parameter in FCM
    Aamir> (a measure of the fuzziness) is fixed at m=2.

there is no 'm' in the book there, but they talk about the
exponent "^ 2" used in some places {but "^ 1" in other places},
notably in   5.2 "Why did we choose FANNY?" 

There is no "fuzziness" parameter defined there, so can you be
more specific?

    Aamir>  Although this is how FCM was originally
    Aamir> formulated, it eventually became apparent that m
    Aamir> should be a user-specified parameter and not a fixed
    Aamir> value of m=2.  My question, then, is
    Aamir> twofold. Firstly, am I correct in saying that Kaufman
    Aamir> & Rousseeuw have assumed m=2 in their formulation of
    Aamir> Fuzzy c-Means and FANNY? Secondly, is it possible to
    Aamir> modify the FANNY algorithm to allow
    Aamir> user-specification of the m (fuzziness) parameter?

Maybe, if you tell me what you are taking about.
See the section 5.2 mentioned above
Is it the exponent  2 in  u_{jv}^2 ?
That one is currently fixed at 2, and yes, that could be made a
parameter though K & R warn against going all the way to "1"
where their algorithm can happend to converge very slowly.


    Aamir> 2. What do I do with training data?  Is there any way
    Aamir> to use FANNY for assigning clustering membership
    Aamir> values to new, test data? In Fuzzy c-Means, new data
    Aamir> is compared to the cluster centers in order to assign
    Aamir> clustering membership values to the test
    Aamir> data. However, in FANNY these centers do not
    Aamir> exist.

correct.  Note again that in general such centers don't even
make sense, since the sample space may contain unordered
categorical variables mixed with continuous ones {and you would
be advised to use  daisy(), not dist() to compute
dissimilarities for such data}.

    Aamir> Is there, then, any way to compute the FANNY
    Aamir> clustering membership values of a test data point
    Aamir> without affecting the clustering membership values of
    Aamir> the training data? Perhaps there are enough
    Aamir> conditions to use the objective function as a way of
    Aamir> computing the membership values of the test data?

    Aamir> Aamir M University of Toronto

That's an interesting proposal, at least the way I choose to
understand you :-)

Yes, why not look at the objective function C {eq.(1), p.182}

One could think of optimizing it with respect to new data only,
by keeping all "old data" memberships.
For that to work, one would need the n dissimilarites 
    d[i', j]   where  i'  : `index for' new data 
	       j = 1,..,n : indices for training data.
Is this feasible in your situation?

Alternatively, when we *did* assume ``all continuous'' data 
*and* the use of simple Euclidean distances,
we could easily compute the cluster centers, determine (by
minimization!) memberships for new observations.

In any case that needs some assumptions (and code!) currently
not part of fanny().



From paul.bliese at us.army.mil  Tue May 31 10:42:40 2005
From: paul.bliese at us.army.mil (Bliese, Paul D LTC USAMH)
Date: Tue, 31 May 2005 10:42:40 +0200
Subject: [R] apply the function "factor" to multiple columns
Message-ID: <FADCFAA8BA80C748890C1D3893C198D9585981@amedmlmhah01.eur.amed.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/0bc334be/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Tue May 31 10:57:15 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 31 May 2005 10:57:15 +0200
Subject: [R] apply the function "factor" to multiple columns
References: <FADCFAA8BA80C748890C1D3893C198D9585981@amedmlmhah01.eur.amed.ds.army.mil>
Message-ID: <011c01c565be$bdbaba50$0540210a@www.domain>

you could try this way:

dat <- data.frame(V1 = factor(1:3), V2 = factor(1:3), V3 = 
factor(1:3))
dat[1:3] <- lapply(dat[1:3], factor, labels = c("None", "Low Impact", 
"MedHigh Imp"))
dat


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Bliese, Paul D LTC USAMH" <paul.bliese at us.army.mil>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, May 31, 2005 10:42 AM
Subject: [R] apply the function "factor" to multiple columns


>I have a case where I would like to change multiple columns 
>containing
> numbers to factors.  I can change each column one at a time as in:
>
>
>
> TEMP.FACT$EXPOS01<-factor(TEMP.FACT$EXPOS01,levels=c(1,2,3),labels=c("No
> ne","Low Impact","MedHigh Imp"))
>
> TEMP.FACT$EXPOS02<-factor(TEMP.FACT$EXPOS02,levels=c(1,2,3),labels=c("No
> ne","Low Impact","MedHigh Imp"))
>
> TEMP.FACT$EXPOS03<-factor(TEMP.FACT$EXPOS03,levels=c(1,2,3),labels=c("No
> ne","Low Impact","MedHigh Imp"))
>
>
>
>> summary(TEMP.FACT[,1:3])
>
>        EXPOS01           EXPOS02           EXPOS03
>
> None       :219   None       :432   None       :377
>
> Low Impact :428   Low Impact :248   Low Impact :297
>
> MedHigh Imp:108   MedHigh Imp: 77   MedHigh Imp: 83
>
> NA's       : 25   NA's       : 23   NA's       : 23
>
>
>
> It would be much easier, however to use apply as in:
>
>
>
> TEMP.FACT 
> [,1:3]<-apply(TEMP.FACT[,1:3],2,factor,labels=c("None","Low
> Impact","MedHigh Imp"))
>
>
>
> This appears to work (no error messages); however, this does not
> actually change the variables to factors.  That is they are still
> treated as numbers:
>
>
>
>> summary(TEMP.FACT[,1:3])
>
>    EXPOS01          EXPOS02          EXPOS03
>
> Min.   : 1.000   Min.   : 1.000   Min.   : 1.000
>
> 1st Qu.: 1.000   1st Qu.: 1.000   1st Qu.: 1.000
>
> Median : 2.000   Median : 1.000   Median : 2.000
>
> Mean   : 1.853   Mean   : 1.531   Mean   : 1.612
>
> 3rd Qu.: 2.000   3rd Qu.: 2.000   3rd Qu.: 2.000
>
> Max.   : 3.000   Max.   : 3.000   Max.   : 3.000
>
> NA's   :25.000   NA's   :23.000   NA's   :23.000
>
>
>
> Any ideas on how I could efficiently change a lot of columns to 
> factors?
>
>
>
> Thanks,
>
>
>
> PB
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ajayshah at mayin.org  Tue May 31 11:17:09 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Tue, 31 May 2005 14:47:09 +0530
Subject: [R] Solved: linear regression example using MLE using optim()
Message-ID: <20050531091709.GD7131@lubyanka.local>

Thanks to Gabor for setting me right. My code is as follows. I found
it useful for learning optim(), and you might find it similarly
useful. I will be most grateful if you can guide me on how to do this
better. Should one be using optim() or stats4::mle?

set.seed(101)                           # For replicability

# Setup problem
X <- cbind(1, runif(100))
theta.true <- c(2,3,1)
y <- X %*% theta.true[1:2] + rnorm(100)

# OLS --
d <- summary(lm(y ~ X[,2]))
theta.ols <- c(d$coefficients[,1], d$sigma)

# Switch to log sigma as the free parameter
theta.true[3] <- log(theta.true[3])
theta.ols[3]  <- log(theta.ols[3])

# OLS likelihood function --
ols.lf <- function(theta, K, y, X) {
  beta <- theta[1:K]
  sigma <- exp(theta[K+1])
  e <- (y - X%*%beta)/sigma
  logl <- sum(log(dnorm(e)/sigma))
  return(logl)
}

# Experiment with the LF --
cat("Evaluating LogL at stupid theta : ", ols.lf(c(1,2,1), 2, y, X), "\n")
cat("Evaluating LogL at true params  : ", ols.lf(theta.true, 2, y, X), "\n")
cat("Evaluating LogL at OLS estimates: ", ols.lf(theta.ols, 2, y, X), "\n")

optim(c(1,2,3),                          # Starting values
      ols.lf,                            # Likelihood function
      control=list(trace=1, fnscale=-1), # See ?optim for all controls
      K=2, y=y, X=X                      # "..." stuff into ols.lf()
     )
# He will use numerical derivatives by default.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From r.hankin at noc.soton.ac.uk  Tue May 31 11:20:01 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 31 May 2005 10:20:01 +0100
Subject: [R] persp, add lines/highlights
In-Reply-To: <429BAC6A.1010306@stat.auckland.ac.nz>
References: <1117486427.1669.235259524@webmail.messagingengine.com>
	<429BAC6A.1010306@stat.auckland.ac.nz>
Message-ID: <ecebd24df865ae9a51fcb562fa82160d@soc.soton.ac.uk>

Hello everyone

I always cut-n-paste Paul's suggestions and end up learning something!

In this case, though, I have a query.

I was wondering how the second persp() call dealt with hidden line
removal,  because pieces of mesh with NA values are see-through.

If you replace xi in the code below with 10:28
and yi with 19:20, then all the pink grid
appears in front of the blue grid; I would expect  the blue grid to 
hide some of the pink
grid.  Is there any way to enforce
hidden line removal here?


rksh


On May 31, 2005, at 01:14 am, Paul Murrell wrote:

> Hi
>
>
> jjorgensen at fastmail.fm wrote:
>> Hello R-sters,
>> I'm trying to add several lines to a response surface that I've 
>> plotted
>> using persp().  I've tried lines() using the "trans3d" function but 
>> I've
>> been unsuccessful in getting it to work (R v2.0.1).  Essentially, I'm
>> trying to highlight one or more of the surface wireframe lines in a
>> bolder (or different) color.  Any tips from those of you who have some
>> experience with this would be greatly appreciated.  [Would it be 
>> easier
>> using wireframe() in library(lattice) instead?]
>
>
> Here's an example that just overlays two persp() plots ...
>
> x <- seq(-10, 10, length= 30)
> y <- x
> f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
> z <- outer(x, y, f)
> z[is.na(z)] <- 1
> op <- par(bg = "white")
> persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue",
>       zlim=range(z))
>
> # overlay plot with just "highlighted" surface area
> par(new=TRUE)
> z2 <- matrix(NA, ncol=30, nrow=30)
> xi <- 15:18
> yi <- 13:14
> z2[xi, yi] <- z[xi, yi]
> persp(x, y, z2, theta = 30, phi = 30, expand = 0.5,
>       zlim=range(z), border="red", col="pink", box=FALSE, axes=FALSE)
>
>
>> And, any suggestions on how to add text outside of the persp() plot 
>> next
>> to the highlighted line would be much appreciated.
>
>
> Do you mean like a legend?
>
> Paul
> -- 
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From amsa36060 at yahoo.com  Tue May 31 11:20:24 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Tue, 31 May 2005 02:20:24 -0700 (PDT)
Subject: [R] How to access to sum of dissimilarities in CLARA
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BAAA6@xchg1.statistik.local>
Message-ID: <20050531092024.51138.qmail@web60418.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/07959124/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Tue May 31 11:21:32 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 31 May 2005 11:21:32 +0200
Subject: [R] Piecewise Linear Regression
In-Reply-To: <Pine.GSO.4.10.10505301928280.26258-100000@castle.isye.gatech.edu>
References: <Pine.GSO.4.10.10505301928280.26258-100000@castle.isye.gatech.edu>
Message-ID: <20050531112132.1325d717.Achim.Zeileis@wu-wien.ac.at>

On Mon, 30 May 2005 19:38:58 -0400 (EDT) Abhyuday Mandal wrote:

> Hi,
> 
> I need to fit a piecewise linear regression.
> 
> x =
> c(6.25,6.25,12.50,12.50,18.75,25.00,25.00,25.00,31.25,31.25,37.50,37.
> 50,50.00,50.00,62.50,62.50,75.00,75.00,75.00,100.00,100.00) y =
> c(0.328,0.395,0.321,0.239,0.282,0.230,0.273,0.347,0.211,0.210,0.259,0
> .186,0.301,0.270,0.252,0.247,0.277,0.229,0.225,0.168,0.202)
> 
> there are two change points. so the fitted curve should look like
> 
> 
> 
> \ 
>  \  /\
>   \/  \
>        \
>         \
> 
> How do I do this in R ?

There are various ways, as the previous replies already indicated. One
approach would be to consider this in a structural change framework, as
implemented in strucchange or segmented.

With strucchange you can do

## collect data
dat <- data.frame(y, x)
## add small noise
set.seed(123)
dat$x <- dat$x + rnorm(21, sd = 0.001)
## fit breakpoint model
bp <- breakpoints(y ~ x, data = dat)
## visualize
plot(x, y)
lines(x, fitted(bp))

The problem with breakpoints() is that it also fits models on very small
subsets, which can lead to singular regressor matrices if there are
bindings among the regressors. Hence, I did an ugly hack and added a
small error to avoid singularities.

segmented can also fit segmented regressions, but always fits broken
line segments (i.e. continuous curves) whereas strucchange fits fully
segmented models. The latter can also be easily fitted via
  lm(y ~ fac/x)
if fac is a factor coding the different segments. See also ?breakfactor
in strucchange. So you can use any segmentation algorithm and then plug
the result easily into lm().
Z



> Thank you,
> Abhyuday
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ernesto at ipimar.pt  Tue May 31 11:43:02 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 31 May 2005 10:43:02 +0100
Subject: [R] R GUI for Linux?
In-Reply-To: <429B5B8D.9070402@oomvanlieshout.net>
References: <8BAEC5E546879B4FAA536200A292C6140DD112@AMEDMLNARMC135.amed.ds.army.mil>
	<429B5B8D.9070402@oomvanlieshout.net>
Message-ID: <429C31A6.6080406@ipimar.pt>

Sander Oom wrote:
> Hi Charles,
> 
> Warm felt sympathies for your struggles. I consider myself a happy GUI 
> user and have also struggled with the 'command line' history and lack of 
> out-of-the-box functionality associated with Linux. However, Linux does 
> have many, many advantages over other OS's, so I will stick to it. That 
> said, I am reluctantly suggesting it to others, as there are lots of 
> other things one can do on a Saturday afternoon. ;-)
> 
> After many long afternoons, I have come to the conclusion that accepting 
> what comes as standard is the best approach to using Linux if you do 
> have better things to do. I tried ESS, but I found it impenetrable at my 
> first try, so gave up.
> 
> I run SuSE linux and R without any problem. The RPM was downloaded from 
> CRAN and installed without any errors. I run update.packages to 
> download, install and refresh all contributed and other packages.
> 
> I also looked for an R GUI, but must admit I stopped way before you did. 
> I ended up using my favorite editor (Kate, comes standard with KDE) and 
> cut and paste code into an R console (available as standard in the Kate 
> window). Gives a nice clean window for writing code and running R. Of 
> course Kate has code highlighting facilities (as standard). I soon 
> realized that the console has a limited buffer for commands, such that 
> long code sequences are abruptly ended part way through. Thus I have a 
> second console open from which I source whole R files. The setup gives a 
> relatively comfortable code debugging environment!
> 
> I was amazed when I read the instruction for JGR on Linux. I thought the 
> whole point of Java was to create platform independent software. I have 
> given up on any instructions that tell me to run 'make'. Linux 
> distributions are just to idiosyncratic for it to be worth the effort. 
> I'll just wait until Novell packs JGR with the new version of SuSE 
> Linux. Even if I have to pay something for the added bonus!
> 
> Will keep following the GUI developments with interest,
> 
> Sander.
> 
> 
> White, Charles E WRAIR-Wash DC wrote:
> 
>> I feel your pain. <grin> I am a new Linux user who has spent most of 
>> the weekend trying to get a functional R setup. When I installed 
>> Fedora Core 3 (FC3) on my home computer, I thought using R in a 
>> terminal would be a snap. I installed R using the rpm packages and 
>> tried to use it with the FC3 default terminal (GNOME Terminal 2.7.3). 
>> Before long, I found out the terminal was rudely discarding output 
>> beyond a set number of lines. I could increase the number of lines 
>> kept by the terminal but that didn't strike me as an acceptable 
>> solution. Cutting to my stress relieving intermediate solution, I am 
>> currently using xemacs with ESS as my R programming environment under 
>> FC3. Eventually, I will want to run JGR as my programming environment 
>> and Rcmdr as both a teaching tool and means to distribute code to some 
>> of my clients. On my way to xemacs, I also tried to install emacs and 
>> gnomeGUI. I will briefly document my experience with trying to install 
>> each of these packages below:
>>
>> XEMACS with ESS: XEMACS is within the 'walled garden' of packages 
>> tuned specifically to run under FC3 and XEMACS provides a tuned 
>> installation for ESS. Since I had already compiled R from source with 
>> shared libraries enabled (the rmp does not enable shared libraries), I 
>> don't know if XEMACS will work with the rpm version of R. Note also 
>> that I installed this package using yum; 'Add or Remove Applications' 
>> lists xemacs but wouldn't allow me to install.
>>
>> JGR: I have installed jdk1.5.0_03 and MOST of the the output from make 
>> looks like JGR is compiling correctly. JavaGD and rJava are not 
>> finding jini.h. I don't see an explicit statement of how to start JGR 
>> but I assume that is done by typing JGR (or maybe jgr) in a terminal 
>> window. Nothing happens. Two potential problems are: (a) I never 
>> should have downloaded JavaGD and rJava from CRAN (they won't 
>> uninstall, deleting the directories doesn't stop the problem, and I 
>> can't use yum to remove R to start over because yum doesn't recognise 
>> that R is installed.) or (b) I need to uninstall some of the stray 
>> versions of java littering my hard drive. I haven't removed the 
>> symbolic link between jre1.5.0_02 and firefox.
>>
>> Rcmdr: There are all sorts of things in FC3 that seem to be tcl/tk 
>> related but Rcmdr doesn't seem to work with them. Since some are part 
>> of the base FC3 installation, I'm nervious about replacing them or 
>> installing competing software. Potentially conflicting software in FC3 
>> are listed below:
>>
>> tcl.i386                                 8.4.7-2                installed
>> tclx.i386                                8.3.5-4                installed
>> db4-tcl.i386                             4.2.52-6               base
>> postgresql-tcl.i386                      7.4.8-1.FC3.1          
>> updates-released
>> ruby-tcltk.i386                          1.8.2-1.FC3.1          
>> updates-released
>> tcl-devel.i386                           8.4.7-2                base
>> tcl-html.i386                            8.4.7-2                base
>> tclx-devel.i386                          8.3.5-4                base
>> tclx-doc.i386                            8.3.5-4                base
>>
>> EMACS with ESS: A version of EMACS is tuned to FC3 but ESS has to be 
>> obtained elsewhere. Installing ESS requires editing the hidden .emacs 
>> file. I know how to see hidden files but this one does not appear to 
>> be where the ESS directions say to look.
>>
>> gnomeGUI: Error message says that I don't have Gnome installed. It 
>> would be nice and GNU to have a working copy of gnome GUI.
>>
>> I hope you have found my message to be entertaining. If anybody can 
>> stop me before I do something else stupid, I'd appreciate it. Thanks.
>>
>> Chuck
>>
>> Charles E. White, Senior Biostatistician, MS
>> Walter Reed Army Institute of Research
>> 503 Robert Grant Ave., Room 1w102
>> Silver Spring, MD 20910-1557
>> Personal/Professional Site: 
>> http://users.starpower.net/cwhite571/professional/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> 

Hi Charles,

I'm pretty much inline with Chuck. I use Nedit for the last 2/3 years to 
edit R files and source them into R and I'm pretty much convinced that 
it is the best way to use R.

ESS looks very good but why should I load more than 30MB on the memory 
to work on a text file ? and why do I need to lear all the tricks and 
features of emacs just to edit a text file ?

JGR looks very good but it does not really give anything new, once 
you're used with the command line.

Regards

EJ



From amsa36060 at yahoo.com  Tue May 31 12:04:04 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Tue, 31 May 2005 03:04:04 -0700 (PDT)
Subject: [R] How to access to Data within a cluster
Message-ID: <20050531100404.28553.qmail@web60422.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/eaffaa3a/attachment.pl

From mail at bymouth.com  Tue May 31 12:02:24 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Tue, 31 May 2005 20:02:24 +1000
Subject: [R] LPC
Message-ID: <000001c565c8$1f8dd2e0$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/e91cf953/attachment.pl

From jarioksa at sun3.oulu.fi  Tue May 31 12:19:44 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Tue, 31 May 2005 13:19:44 +0300
Subject: [R] R GUI for Linux?
In-Reply-To: <429C31A6.6080406@ipimar.pt>
References: <8BAEC5E546879B4FAA536200A292C6140DD112@AMEDMLNARMC135.amed.ds.army.mil>
	<429B5B8D.9070402@oomvanlieshout.net>  <429C31A6.6080406@ipimar.pt>
Message-ID: <1117534784.1466.17.camel@biol102145.oulu.fi>

On Tue, 2005-05-31 at 10:43 +0100, Ernesto Jardim wrote:

> ESS looks very good but why should I load more than 30MB on the memory 
> to work on a text file ? and why do I need to lear all the tricks and 
> features of emacs just to edit a text file ?
> 
With emacs tricks you perhaps refer to four-finger key combinations that
even gave name to emacs: it's supposed to be an acronym for Esc-Meta-
Alt-Ctrl-Shift (say the enemies). However, you don't need those tricks.
The only ones you need are Alt-x R to start R in ESS, and Esc-p to go
back in command history -- these at least are the only ones I use,
although I know people who really like acrobatic key combinations (I
don't). All the rest is in the menus -- even in ordinary GNU Emacs (and
also in its MacOS X and Windows ports). Installing ESS may be more
trouble with GNU Emacs, but if you use Debian based distributions (like
Ubuntu), you can directly install a deb package for ess (I once found an
rpm for ESS as well, but that's not official nor standard). I avoid
XEmacs, but people say things are even easier there.

I'd suggest you try Emacs + ESS + R. It is really good and likable.
Emacs is much more user friendly than it used to be some decade ago.

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From Arne.Muller at sanofi-aventis.com  Tue May 31 12:34:00 2005
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Tue, 31 May 2005 12:34:00 +0200
Subject: [R] lm/lme cross-validation
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF38C@CRBSMXSUSR04>

Hello,

is there a special package/method to cross-validate linear fixed effects and mixed effects models (from lme)? I've tried cv.glm on an lme (hoping that it may deal with any kind of linear model ...), but it raises an error:

Error in eval(expr, envir, enclos) : couldn't find function "lme.formula"

so I guess it's not dealing with an lme.

I've realized that removing randomly some lines from the data frame used for lme strongly changes the the estimates and reduces the correlation between fitted and actual values. Therefore I'd like to get a more "realistic" view of the prediction performance.

Any ideas are welcome,

	+thanks,

	Arne



From blindglobe at gmail.com  Tue May 31 13:20:38 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 31 May 2005 13:20:38 +0200
Subject: [R] R GUI for Linux?
In-Reply-To: <1117534784.1466.17.camel@biol102145.oulu.fi>
References: <8BAEC5E546879B4FAA536200A292C6140DD112@AMEDMLNARMC135.amed.ds.army.mil>
	<429B5B8D.9070402@oomvanlieshout.net> <429C31A6.6080406@ipimar.pt>
	<1117534784.1466.17.camel@biol102145.oulu.fi>
Message-ID: <1abe3fa9050531042024ea0703@mail.gmail.com>

If you really want something closer to an IDE, add ECB to that mix
(Emacs Code Browser).  ESS does some minimal connections to it, for
code browsing between functions, data.frame creations, and S4 class
constructions.



On 5/31/05, Jari Oksanen <jarioksa at sun3.oulu.fi> wrote:
> On Tue, 2005-05-31 at 10:43 +0100, Ernesto Jardim wrote:
> 
> > ESS looks very good but why should I load more than 30MB on the memory
> > to work on a text file ? and why do I need to lear all the tricks and
> > features of emacs just to edit a text file ?
> >
> With emacs tricks you perhaps refer to four-finger key combinations that
> even gave name to emacs: it's supposed to be an acronym for Esc-Meta-
> Alt-Ctrl-Shift (say the enemies). However, you don't need those tricks.
> The only ones you need are Alt-x R to start R in ESS, and Esc-p to go
> back in command history -- these at least are the only ones I use,
> although I know people who really like acrobatic key combinations (I
> don't). All the rest is in the menus -- even in ordinary GNU Emacs (and
> also in its MacOS X and Windows ports). Installing ESS may be more
> trouble with GNU Emacs, but if you use Debian based distributions (like
> Ubuntu), you can directly install a deb package for ess (I once found an
> rpm for ESS as well, but that's not official nor standard). I avoid
> XEmacs, but people say things are even easier there.
> 
> I'd suggest you try Emacs + ESS + R. It is really good and likable.
> Emacs is much more user friendly than it used to be some decade ago.
> 
> cheers, jari oksanen
> --
> Jari Oksanen <jarioksa at sun3.oulu.fi>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From ales.ziberna at guest.arnes.si  Tue May 31 13:22:22 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Tue, 31 May 2005 13:22:22 +0200
Subject: [R] Null space (or kernel) and image of a matrix
Message-ID: <008b01c565d3$04f902a0$598debd4@ales>

Hello!

Does anyone now if there exist a function that would compute a "null space" 
(or "kernel" - "Ker") of a matrix and maybe also one that would compute an 
"image" ("Im") of a matrix.

I tried R-site search and google, However I found notnihg useful!

Thanks for any sugestions! I am also not sure what an "image" of a matrix 
is, so suggestion in this directions  would also be  apreciated.

Ale? ?iberna



From sdavis2 at mail.nih.gov  Tue May 31 13:42:21 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 31 May 2005 07:42:21 -0400
Subject: [R] How to access to Data within a cluster
In-Reply-To: <20050531100404.28553.qmail@web60422.mail.yahoo.com>
References: <20050531100404.28553.qmail@web60422.mail.yahoo.com>
Message-ID: <ed089c5120a8b8bf96f92d8016d9226c@mail.nih.gov>


On May 31, 2005, at 6:04 AM, Amir Safari wrote:

>
>
> Dear All ,
> For more process on data resulting clustering, one needs to access 
> data within any cluster. How it is possible in CLARA?
> $clustering :gives us clustering vector as mixed, so it isn't as 
> separate
>

Amir,

I assume that you want to know what observations belong to what 
cluster?  Then I think the "clustering" element of the returned object 
holds that information.  If you want the information in a different 
form, you will have to do a bit more work.  As an example, to find all 
the observations in the first cluster, you could do simply:

first.clust <- which(my.clara.object$clustering==1)

Does that give you the type of information you want?

Sean



From fooms at euroscreen.be  Tue May 31 13:51:27 2005
From: fooms at euroscreen.be (=?iso-8859-1?Q?Fr=E9d=E9ric_Ooms?=)
Date: Tue, 31 May 2005 13:51:27 +0200
Subject: [R] Pairs plot 
Message-ID: <5198ADA420721246BC35BFA666E24F16D742EC@euromail.euroscreen.be>

Hello,
I would like to draw a pair plot with a lot of descriptors (see the attached file) and due to the number of descriptors contained in the file I am not able to view the plot I only end up with really small square in the graphic device. In am working with the windows version of R.
Thanks for helping me to get a better view;
Fred 
 <<DB_molprop.txt>> 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: DB_molprop.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/44e1df30/DB_molprop.txt

From Matthias.Templ at statistik.gv.at  Tue May 31 13:47:24 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Tue, 31 May 2005 13:47:24 +0200
Subject: [R] How to access to sum of dissimilarities in CLARA
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAAA9@xchg1.statistik.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/222a2c54/attachment.pl

From j.van_den_hoff at fz-rossendorf.de  Tue May 31 13:49:29 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Tue, 31 May 2005 13:49:29 +0200
Subject: [R] X11() and pseudo color
In-Reply-To: <20050531082226.58868.qmail@web26602.mail.ukl.yahoo.com>
References: <20050531082226.58868.qmail@web26602.mail.ukl.yahoo.com>
Message-ID: <429C4F49.3050501@fz-rossendorf.de>

we are running R under Solaris with SunRay Terminals, which are set to 8 
bit color to comply with some other software. In this configuration, 
X11() opens with colortype=true, i.e., it is not recognized that 
actually the display is only 8 bit. This leads to error messages 
(advising to use 'pseudo.cube').

question 1: is X11() supposed to recognize  the actual color 
capabilities? i.e. is this a bug?

question 2: is there a possibility to query the color capabilities from 
within R in order to being able to open the X11() displays always (for 
true color as well as for 8 bit) with the correct colortype setting from 
within a function?

regards,
joerg



From paul.bliese at us.army.mil  Tue May 31 14:04:05 2005
From: paul.bliese at us.army.mil (Bliese, Paul D LTC USAMH)
Date: Tue, 31 May 2005 14:04:05 +0200
Subject: [R] lars / lasso with glm
Message-ID: <FADCFAA8BA80C748890C1D3893C198D95C7185@amedmlmhah01.eur.amed.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/18bd4e7c/attachment.pl

From jean-luc_allard at uqac.ca  Tue May 31 14:10:47 2005
From: jean-luc_allard at uqac.ca (Jean-Luc Allard)
Date: 31 May 2005 05:10:47 PDT
Subject: [R] Please help me update my address book on Ringo
Message-ID: <20050531121047.2681BDD95F@ringotouch3.ringo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/5b56af39/attachment.pl

From d.scott at auckland.ac.nz  Tue May 31 14:26:40 2005
From: d.scott at auckland.ac.nz (David Scott)
Date: Wed, 1 Jun 2005 00:26:40 +1200 (NZST)
Subject: [R] POSIX problem
Message-ID: <Pine.LNX.4.61.0506010017100.18901@stat12.stat.auckland.ac.nz>


I am having trouble with creating a POSIXct object. If I create a variable 
of class Date first out of the date part of my data, I am ok, but if I 
just paste the date and time parts together and try and create the POSIXct 
object, I have problems.

Here is a toy example created from the actual data which caused the 
problem. I am using R 2.0.1 on Windows XP.

> # Data frame with dates and times, as character
> PeopleData.df
    StartDate StartTime
1 29/10/2001     15:26
2  7/12/2001     10:32
3 16/11/2001     13:58
4 28/11/2001     14:00
5  2/11/2001     15:22
6 26/11/2001     11:15
> str(PeopleData.df)
`data.frame':   6 obs. of  2 variables:
  $ StartDate: chr  "29/10/2001" "7/12/2001" "16/11/2001" "28/11/2001" ...
  $ StartTime: chr  "15:26" "10:32" "13:58" "14:00" ...
> dput(PeopleData.df)
structure(list(StartDate = c("29/10/2001", "7/12/2001", "16/11/2001",
"28/11/2001", "2/11/2001", "26/11/2001"), StartTime = c("15:26",
"10:32", "13:58", "14:00", "15:22", "11:15")), .Names = c("StartDate",
"StartTime"), row.names = c("1", "2", "3", "4", "5", "6"), class = 
"data.frame")
> BeginDate <- as.Date(PeopleData.df$StartDate,format="%d/%m/%Y")
> BeginDate
[1] "2001-10-29" "2001-12-07" "2001-11-16" "2001-11-28" "2001-11-02"
[6] "2001-11-26"
> # Create POSIXct date-time object without difficulty
> BeginTime <- as.POSIXct(format(paste(BeginDate,PeopleData.df$StartTime),
+                                 format="%Y/%m/%d %H:%M"))
> BeginTime
[1] "2001-10-29 15:26:00 New Zealand Standard Time"
[2] "2001-12-07 10:32:00 New Zealand Standard Time"
[3] "2001-11-16 13:58:00 New Zealand Standard Time"
[4] "2001-11-28 14:00:00 New Zealand Standard Time"
[5] "2001-11-02 15:22:00 New Zealand Standard Time"
[6] "2001-11-26 11:15:00 New Zealand Standard Time"
> # But not directly from the dates and times
> BeginTime <- 
as.POSIXct(format(paste(PeopleData.df$StartDate,PeopleData.df$StartTime),
+                                 format="%d/%m/%Y %H:%M"))
> BeginTime
[1] "0029-10-20 New Zealand Standard Time"
[2] "0007-12-20 New Zealand Standard Time"
[3] "0016-11-20 New Zealand Standard Time"
[4] "0028-11-20 New Zealand Standard Time"
[5] "0002-11-20 New Zealand Standard Time"
[6] "0026-11-20 New Zealand Standard Time"
> # Format looks correct to me
> paste(PeopleData.df$StartDate,PeopleData.df$StartTime)
[1] "29/10/2001 15:26" "7/12/2001 10:32"  "16/11/2001 13:58" "28/11/2001 
14:00"
[5] "2/11/2001 15:22"  "26/11/2001 11:15"

What I think might be causing the problem is the lack of a leading zero 
for some of the days (as in 7/12/2001). This doesn't phase as.Date though.

David Scott



_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From ligges at statistik.uni-dortmund.de  Tue May 31 14:32:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 May 2005 14:32:09 +0200
Subject: [R] Pairs plot
In-Reply-To: <5198ADA420721246BC35BFA666E24F16D742EC@euromail.euroscreen.be>
References: <5198ADA420721246BC35BFA666E24F16D742EC@euromail.euroscreen.be>
Message-ID: <429C5949.9060903@statistik.uni-dortmund.de>

I guess you (well, in fact do not) want something like

   pairs(X, gap=0, pch=".")

Uwe Ligges




Fr??d??ric Ooms wrote:

> Hello,
> I would like to draw a pair plot with a lot of descriptors (see the attached file) and due to the number of descriptors contained in the file I am not able to view the plot I only end up with really small square in the graphic device. In am working with the windows version of R.
> Thanks for helping me to get a better view;
> Fred 
>  <<DB_molprop.txt>> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From HDoran at air.org  Tue May 31 14:40:04 2005
From: HDoran at air.org (Doran, Harold)
Date: Tue, 31 May 2005 08:40:04 -0400
Subject: [R] Plot Font Size in Sweave
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409174892@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/60373977/attachment.pl

From kotze at mappi.helsinki.fi  Tue May 31 14:41:36 2005
From: kotze at mappi.helsinki.fi (Johan Kotze)
Date: Tue, 31 May 2005 15:41:36 +0300
Subject: [R] Theta and clumping (or aggregation) parameter k
Message-ID: <1117543296.429c5b807453e@www3.helsinki.fi>

Hi all, in R the glm.nb models return a theta value, the aggregation
parameter for negative binomial distributions. Crawly in his S-Plus book
talks about the clumping (aggregation) parameter k. Are these two things
(theta and k) different, and if so, what would be the relationship between
the two?

Thank you in advance, 
Johan
--
Johan Kotze
Department of Biological and Environmental Sciences
PO Box 65 (Biocenter 3, Viikinkaari 1)
FI-00014, University of Helsinki
Helsinki, FINLAND
Tel: +358 (0)9 191 57707  Fax: +358 (0)9 191 57694
E-mail: johan.kotze at helsinki.fi

Editor-in-Chief, Journal of Negative Results - EEB (www.jnr-eeb.org)
www.helsinki.fi/ml/ekol/spatial_ecology.html
www.helsinki.fi/science/globenet/



From charles.edwin.white at us.army.mil  Tue May 31 14:43:52 2005
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Tue, 31 May 2005 08:43:52 -0400
Subject: [R] R GUI for Linux?
Message-ID: <8BAEC5E546879B4FAA536200A292C6145728A0@AMEDMLNARMC135.amed.ds.army.mil>

Thanks for the tip. I thought that 'Base' files were installed already
but I know better now. Unfortunately, I'm still not up and running yet.
Since I don't have access to the subject machine at the moment, I'll be
even more vague than usual.

I installed the development versions of tcltk and specified the
locations for the *.sh files, the *.h files, and both of the individual
directories in /usr/lib. Based on my reading of one of the *.sh files,
it looks like I can specify multiple directories for a configuration
variable by putting them in quotes and leaving a space in between like
'path1 path2'. My reward was that ./configure didn't complain about
searching for any tcltk files and I got the clear message that R
couldn't compile/link to tcltk. Hmph. <grin>

Next steps when I get home tonight:
(a) Replace library locations in /usr/lib with those in /usr/share
(b) Include all four library locations
(c) Wait until Fedora Core 4 comes out in a couple of weeks, wipe my
drive, and start all over again.

Any other suggestions? Thanks again.

Chuck
-----Original Message-----
From: Gavin Simpson [mailto:gavin.simpson at ucl.ac.uk] 
Sent: Tuesday, May 31, 2005 3:30 AM
To: White, Charles E WRAIR-Wash DC
Cc: John Fox; sander at oomvanlieshout.net; r-help at stat.math.ethz.ch
Subject: Re: [R] R GUI for Linux?

White, Charles E WRAIR-Wash DC wrote:
> John:
> 
> Thank you for your interest. After more investigation I see that my
> problem is with linking tcltk to R. tcl 8.4.7-2 and tk 8.4.7-2 are
> tuned to fc3 (Fedora Core 3) and part of the standard installation.
> However, required file locations are different than what is expected
> by R. I set the locations for tclConfig.sh and tkConfig.sh using
> ./configure but R tells me something to the effect that Tcl and Tk
> major or minor versions disagree. Your help or the help of others on
> the R-List would be appreciated. FYI, I am also including the
> locations of important files and directories.
> 
> tclConfig.sh: /usr/lib tkConfig.sh:  /usr/lib tcl library:
> /usr/lib/tcl8.4 (symbolically linked from /usr/share/tcl8.4) tk
> library:   /usr/lib/tk8.4 (separate copy in /usr/share/tk8.4) tcl.h:
> I can't find it with full drive search tk.h:         I can't find it
> with full drive search

You need to install the devel packages for tcl and tk to get those 
header files.

on FC3, the preferred updater is yum so in a terminal:

su -c "yum install tcl-devel tk-devel"

Should install the *.h files for you.

HTH

Gav



From MSchwartz at mn.rr.com  Tue May 31 14:49:34 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 31 May 2005 07:49:34 -0500
Subject: [R] Barplot2 Title
In-Reply-To: <20050531082226.58868.qmail@web26602.mail.ukl.yahoo.com>
References: <20050531082226.58868.qmail@web26602.mail.ukl.yahoo.com>
Message-ID: <1117543774.22595.75.camel@horizons.localdomain>

On Tue, 2005-05-31 at 10:22 +0200, Navarre Sabine wrote:
> Hello,
> I would like to know if it's possible to modify the name of groups of
> bar because on my barplot2, I have  5 groups of bars and one of them
> is called "User Contributes" and when I save the plot "User
> contributes" is to big so I don't have it on my plot! Is it pssible to
> put the name vertically!
>  
> Thanks!
> 
> Sabine

Sabine,

You can rotate the bar labels, by using something like:

par(mar = c(10, 4, 4, 2))
barplot2(1:5, names.arg = paste("Long Bar Label", 1:5), las = 2)

Note that I first increase the margin area for the x axis by setting par
("mar"). This provides sufficient room for the vertical labels. Setting
par("las") results in rotated labels on both axes.

See ?par for more information. 

Also see R FAQ 7.27 on creating rotated axis labels for an example of
creating labels that are rotated 45 degrees.


Another option is to have the long labels wrap to multiple lines:

bar.names <- paste("Long Bar Label", 1:5)

> bar.names
[1] "Long Bar Label 1" "Long Bar Label 2" "Long Bar Label 3"
[4] "Long Bar Label 4" "Long Bar Label 5"

# Here I use strwrap() to split the longer names into groups of
# around 10 characters each and then paste() them together using
# a newline character
short.names <- sapply(bar.names, 
                       function(x) paste(strwrap(x, 10), 
                                         collapse = "\n"), 
                       USE.NAMES = FALSE) 

> short.names
[1] "Long Bar\nLabel 1" "Long Bar\nLabel 2" "Long Bar\nLabel 3"
[4] "Long Bar\nLabel 4" "Long Bar\nLabel 5"

# Now do the barplot with the wrapped labels
barplot2(1:5, names.arg = short.names)


See ?strwrap for more information.

HTH,

Marc Schwartz



From ben.bob at gmail.com  Tue May 31 14:53:40 2005
From: ben.bob at gmail.com (Bo Peng)
Date: Tue, 31 May 2005 07:53:40 -0500
Subject: [R] R GUI for Linux?
In-Reply-To: <1abe3fa9050531042024ea0703@mail.gmail.com>
References: <8BAEC5E546879B4FAA536200A292C6140DD112@AMEDMLNARMC135.amed.ds.army.mil>
	<429B5B8D.9070402@oomvanlieshout.net> <429C31A6.6080406@ipimar.pt>
	<1117534784.1466.17.camel@biol102145.oulu.fi>
	<1abe3fa9050531042024ea0703@mail.gmail.com>
Message-ID: <6ea7b543050531055335fddf08@mail.gmail.com>

> > > ESS looks very good but why should I load more than 30MB on the memory
> > > to work on a text file ? and why do I need to lear all the tricks and
> > > features of emacs just to edit a text file ?

One of the advantages (and disadvantages) of linux applications like
emacs is that you can, if you know how to, customize it to your taste
so you do not have to 'learn all the tricks and features' of it. You
can use cua package if you are used to C-x,C-c,C-v and other windows
stuff. You can change keybindings of ESS if you do not like them.

The reason why I like ESS is that I can step through or run a piece of
R code easily. For my ESS installation, I have several ESS buttons on
the toolbar to start R, eval line and step, eval region and eval
function. I have also linked these functions to shortcuts F9,F10,F11
by defining
  (define-key ess-mode-map  (kbd "<f9>")  (quote ess-eval-line-and-step))
  (define-key ess-mode-map  (kbd "<f10>")  (quote ess-eval-region))
  (define-key ess-mode-map  (kbd "<f11>") (quote ess-eval-buffer))
in .emacs.

I do not see any reason why an embedded R session would occupy more
RAM than a standalone R session, other than the time/RAM emacs uses to
color the output. This can be a problem if the output is extremely
long so I usually clean the R-cmd buffer once a while.

Bo



From gavin.simpson at ucl.ac.uk  Tue May 31 14:57:28 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 31 May 2005 13:57:28 +0100
Subject: [R] R GUI for Linux?
In-Reply-To: <8BAEC5E546879B4FAA536200A292C6145728A0@AMEDMLNARMC135.amed.ds.army.mil>
References: <8BAEC5E546879B4FAA536200A292C6145728A0@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <429C5F38.9040807@ucl.ac.uk>

White, Charles E WRAIR-Wash DC wrote:
> Thanks for the tip. I thought that 'Base' files were installed already
> but I know better now. Unfortunately, I'm still not up and running yet.
> Since I don't have access to the subject machine at the moment, I'll be
> even more vague than usual.

Base refers to the repository in which the packages reside. Unless you 
did a Full/Complete installation of FC3 from the CD's/DVD then you will 
*not* have all of Base installed.

> I installed the development versions of tcltk and specified the
> locations for the *.sh files, the *.h files, and both of the individual
> directories in /usr/lib. Based on my reading of one of the *.sh files,
> it looks like I can specify multiple directories for a configuration
> variable by putting them in quotes and leaving a space in between like
> 'path1 path2'. My reward was that ./configure didn't complain about
> searching for any tcltk files and I got the clear message that R
> couldn't compile/link to tcltk. Hmph. <grin>
> 
> Next steps when I get home tonight:
> (a) Replace library locations in /usr/lib with those in /usr/share
> (b) Include all four library locations
> (c) Wait until Fedora Core 4 comes out in a couple of weeks, wipe my
> drive, and start all over again.
> 
> Any other suggestions? Thanks again.
> 
> Chuck

How are you specifying the locations of the *.sh and *.h files?

I've done very little to my FC boxes and as long as tcl tk, tcl-devel 
and tk-devel packages are installed, ./configure picks up the locations 
just fine. You shouldn't have to do anything. Perhaps undo you what did 
after installing the tcl/tk and devel packages and try it again as this 
most definitely works for me (almost) out of the box.

If that doesn't work, wait till you get back to your 'subject' machine 
and post back exactly what messages you are receiving that indicate R 
couldn't compile/link to tcl/tk

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From DEM01_WatchDog_Demon at de.ey.com  Tue May 31 14:59:33 2005
From: DEM01_WatchDog_Demon at de.ey.com (DEM01_WatchDog_Demon@de.ey.com)
Date: Tue, 31 May 2005 14:59:33 +0200
Subject: [R] [DEM01 WD/MV: Virus!]
Message-ID: <OF521759E8.E0D74DA0-ONC1257012.00475EF5@de.ey.com>





GROUP securiQ.Watchdog
Server: DEM01
-----------------------------------------------------------------------

This mail item contained attachments which were  virus-infected.
 The attachment(s) has/have been removed and were NOT sent on.

Please update your ANTVIRUS program and virus-check your computer and the
attachment(s)  before re-sending it.

In case of problems reply to IT SERVICES at EY-DE.
-----------------------------------------------------------------------

Mail-Info

From:              r-help at lists.r-project.org @ EY-DE
To:                benedetta.bellocchio at de.ey.com
Rec.: CN=Benedetta Bellocchio/OU=TAX/O=EY-DE/C=DE
Date:              05/31/2005 02:59:23 PM
Subject:           hello

-----------------------------------------------------------------------
file contains virus:                    document05.zip



From JAROSLAW.W.TUSZYNSKI at saic.com  Tue May 31 15:12:37 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Tue, 31 May 2005 09:12:37 -0400
Subject: [R] FYI: Problems while loading package "class (VR)"
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4082@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/f4cb070b/attachment.pl

From laet_99 at yahoo.fr  Tue May 31 15:43:43 2005
From: laet_99 at yahoo.fr (Laetitia Mestdagh)
Date: Tue, 31 May 2005 15:43:43 +0200 (CEST)
Subject: [R] GLM question
Message-ID: <20050531134343.70712.qmail@web26801.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/fc21d5c6/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue May 31 15:49:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 May 2005 15:49:06 +0200
Subject: [R] FYI: Problems while loading package "class (VR)"
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F4082@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F4082@us-arlington-0668.mail.saic.com>
Message-ID: <429C6B52.90708@statistik.uni-dortmund.de>

Tuszynski, Jaroslaw W. wrote:

> Hi,
> 
> Today, I performed automatic updates of packages and somehow lost "class"
> package. I loaded a library that depended on it and got:
> 	Error: package 'class' could not be loaded
> 	In addition: Warning message: there is no package called 'class' in:
> library(pkg, character.only = TRUE, logical = TRUE, lib.loc = lib.loc)
> I tried to load it by instal-package: class (VR) and got error: "dependency
> 'class (VR)' is not available". I did succesfully loaded it by
> instal-package: VR.

What happens after you reinstalling it?

Version of R?
Version of VR?
Operating Sytstem?

PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html

Uwe Ligges


> Jarek
> =====================================\====                 
>  Jarek Tuszynski, PhD.                               o / \ 
>  Science Applications International Corporation  <\__,|  
>  (703) 676-4192                        ">  \
>  Jaroslaw.W.Tuszynski at saic.com                   `    \
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Tue May 31 15:49:21 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 31 May 2005 08:49:21 -0500
Subject: [R] Solved: linear regression example using MLE using optim()
In-Reply-To: <20050531091709.GD7131@lubyanka.local>
References: <20050531091709.GD7131@lubyanka.local>
Message-ID: <429C6B61.4040105@stat.wisc.edu>

Ajay Narottam Shah wrote:
> Thanks to Gabor for setting me right. My code is as follows. I found
> it useful for learning optim(), and you might find it similarly
> useful. I will be most grateful if you can guide me on how to do this
> better. Should one be using optim() or stats4::mle?
> 
> set.seed(101)                           # For replicability
> 
> # Setup problem
> X <- cbind(1, runif(100))
> theta.true <- c(2,3,1)
> y <- X %*% theta.true[1:2] + rnorm(100)
> 
> # OLS --
> d <- summary(lm(y ~ X[,2]))
> theta.ols <- c(d$coefficients[,1], d$sigma)
> 
> # Switch to log sigma as the free parameter
> theta.true[3] <- log(theta.true[3])
> theta.ols[3]  <- log(theta.ols[3])
> 
> # OLS likelihood function --
> ols.lf <- function(theta, K, y, X) {
>   beta <- theta[1:K]
>   sigma <- exp(theta[K+1])
>   e <- (y - X%*%beta)/sigma
>   logl <- sum(log(dnorm(e)/sigma))
>   return(logl)
> }
> 
> # Experiment with the LF --
> cat("Evaluating LogL at stupid theta : ", ols.lf(c(1,2,1), 2, y, X), "\n")
> cat("Evaluating LogL at true params  : ", ols.lf(theta.true, 2, y, X), "\n")
> cat("Evaluating LogL at OLS estimates: ", ols.lf(theta.ols, 2, y, X), "\n")
> 
> optim(c(1,2,3),                          # Starting values
>       ols.lf,                            # Likelihood function
>       control=list(trace=1, fnscale=-1), # See ?optim for all controls
>       K=2, y=y, X=X                      # "..." stuff into ols.lf()
>      )
> # He will use numerical derivatives by default.
> 

It looks as if you are dividing e by sigma twice in your ols.lf
function.  Did you intend that?  Also when you find yourself writing
log(d<dist>(...)) it is usually better to write d<dist>(..., log =
TRUE).  Finally, you can avoid defining and passing K if you make
log(sigma) the first element of the parameter vector theta.  Combining
all these would give you a likelihood function of the form

ols.lf <- function(theta, y, X) {
   sum(dnorm((y - X %*% theta[-1])/exp(theta[1]), log = TRUE)))
}

or, perhaps,

ols.lf <- function(theta, y, X) {
   sum(dnorm(y, mean = X %*% theta[-1], sd = exp(theta[1]), log = TRUE))
}

The use of log = TRUE in the dnorm function is more than cosmetic.
Frequently it is easier to evaluate the log-density than to evaluate the
density.  Also the log-density can be evaluated accurately over a wider
range than can the density followed by the logarithm.



From Markus.Gesmann at lloyds.com  Tue May 31 15:54:40 2005
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Tue, 31 May 2005 14:54:40 +0100
Subject: [R] GLM question
Message-ID: <321C3EEBDB00C24185705B8BF733DADD0503F87C@LNVCNTEXCH01.corp.lloydsnet>

It appears that you try to do some insurance reserving analysis. 
Have a look at http://finzi.psych.upenn.edu/R/Rhelp02a/archive/15315.html, this might be helpful.

Regards

Markus


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laetitia Mestdagh
Sent: 31 May 2005 14:44
To: r-help at stat.math.ethz.ch
Subject: [R] GLM question



I am unfamiliar with R and I'm trying to do few statistical things like GLM and GAM with it. I hope my following questions will be clear enough:

 

My datas ( y(i,j ))are run off triangles for example :

 

 

J=1

J=2

J=3

I=1

1

2

3

I=2

 

4

5

 

I=3

6

 

 

 

 

My model is :

 

E[y(i,j)] =m(i,j)

Var[y(i,j)] =constant *m(i,j)

 

Log(m(i,j)) = eta (i,j)

 

eta (i,j) = c + alpha(i) + beta(j)

 

The y(i,j) are the response and they have no specified distribution.

 

Here is what I did and I'm not getting the right results:

 

> y1<-c(1,0,0,0,0)

> y2<-c(1,0,0,1,0)

> y3<-c(1,0,0,0,1)

> y4<-c(1,1,0,0,0)

> y5<-c(1,1,0,1,0)

> y6<-c(1,0,1,0,0)

> C<-matrix(nrow = 6, ncol = 5, byrow= TRUE)

>  C[1,]<-y1

>   C[2,]<-y2

>   C[3,]<-y3

>   C[4,]<-y4

>   C[5,]<-y5

>   C[6,]<-x6

 

> m<-c(1,2,3,4,5,6)

> Cdata<-data.frame(C[,1],C[,2],C[,3],C[,4],C[,5])

>fmp<-glm(m~C,family = quasipoisson(link = log),data=Cdata)

> fitted.values(fmp)

   1    2    3    4    5    6 

1.25 1.75 3.00 3.75 5.25 6.00

 

So my question are :     -     Why are the fitted wrong (except for 3 and 6)?

-         Is the quasipoisson the right family for my model?

 

I am a little bit lost and not an expert of R, so I thank in advance for any kind of advice

 

Laetitia


		
---------------------------------

ils, photos et vid??os !

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html





************LNSCNTMCS01***************************************************
The information in this E-Mail and in any attachments is CON...{{dropped}}



From JAROSLAW.W.TUSZYNSKI at saic.com  Tue May 31 16:02:43 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Tue, 31 May 2005 10:02:43 -0400
Subject: [R] FYI: Problems while loading package "class (VR)"
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4083@us-arlington-0668.mail.saic.com>

> What happens after you reinstalling it?
After installing VR bundle everything works perfectly. All problems solved. 
> Version of R?      
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R       
> Version of VR?     
packageDescription("class")$Version = "7.2-15"
> Operating Sytstem? 
Win xp
> PLEASE do read the posting guide!
Sorry about incomplete posting. 

My email was to inform that there are some problems with instaling package
"class (VR)" by itself. If I go to Menu
Packages/Install-package(s)/class(VR) I am still getting:
> utils:::menuInstallPkgs()
  dependency 'class (VR)' is not available

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Tuesday, May 31, 2005 9:49 AM
To: Tuszynski, Jaroslaw W.
Cc: (r-help at stat.math.ethz.ch.); 'ripley at stats.ox.ac.uk'
Subject: Re: [R] FYI: Problems while loading package "class (VR)"

Tuszynski, Jaroslaw W. wrote:

> Hi,
> 
> Today, I performed automatic updates of packages and somehow lost "class"
> package. I loaded a library that depended on it and got:
> 	Error: package 'class' could not be loaded
> 	In addition: Warning message: there is no package called 'class' in:
> library(pkg, character.only = TRUE, logical = TRUE, lib.loc = lib.loc) 
> I tried to load it by instal-package: class (VR) and got error: 
> "dependency 'class (VR)' is not available". I did succesfully loaded 
> it by
> instal-package: VR.

What happens after you reinstalling it?

Version of R?
Version of VR?
Operating Sytstem?

PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html

Uwe Ligges


> Jarek
> =====================================\====                 
>  Jarek Tuszynski, PhD.                               o / \ 
>  Science Applications International Corporation  <\__,|  
>  (703) 676-4192                        ">  \
>  Jaroslaw.W.Tuszynski at saic.com                   `    \
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Tue May 31 16:01:52 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 31 May 2005 10:01:52 -0400
Subject: [R] Null space (or kernel) and image of a matrix
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9469@uswpmx00.merck.com>

The null space or kernel of a matrix A would be the subspace of vectors v
such that Av = 0. That is, it's the solution space of a homogeneous linear
system of equations whose coefficients are the rows of A. R implements many
ways to solve such systems. The image is the set of vectors of the form Av
as v ranges over all vectors, which is the same as the span of the columns
of A, aka the column space of A. You can reduce this to a basis or transform
it to an orthogonal basis. Have a look at a linear algebra book to get an
idea for the possibilities. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ale?? ??iberna
Sent: Tuesday, May 31, 2005 7:22 AM
To: R-help
Subject: [R] Null space (or kernel) and image of a matrix


Hello!

Does anyone now if there exist a function that would compute a "null space" 
(or "kernel" - "Ker") of a matrix and maybe also one that would compute an 
"image" ("Im") of a matrix.

I tried R-site search and google, However I found notnihg useful!

Thanks for any sugestions! I am also not sure what an "image" of a matrix 
is, so suggestion in this directions  would also be  apreciated.

Ale?? ??iberna

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Jussi.Makinen at valtiokonttori.fi  Tue May 31 16:15:31 2005
From: Jussi.Makinen at valtiokonttori.fi (=?iso-8859-1?Q?M=E4kinen_Jussi?=)
Date: Tue, 31 May 2005 17:15:31 +0300
Subject: [R] A suggestion to improve ifelse behaviour with vector yes/no
	arguments
Message-ID: <0BDE2460F08BF0429F933A40431A61E801E8244E@vk2kmail01.valtiokonttori.local>

Dear All,

I luckily found the following feature (or problem) when tried to apply ifelse-function to an ordered data.

> test <- c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE)
> ifelse(test, 0, 1:4)
[1] 0 0 0 4 1 2 3
>

It roots into the ifelse-syntax:

ans[!test & !nas] <- rep(no, length.out = length(ans))[!test & !nas]

Would it be possible to disable this feature in the next R-version? For instance change the code to be:

ans[!test & !nas] <- rep(no, length.out = length(ans[!test & !nas]))

which seems to solve the problem.

Best regards,

Jussi M??kinen

Jussi M??kinen
Analyst
State Treasury, Finland
www.statetreasury.fi
mailto:jussi.makinen at valtiokonttori.fi



From dimitris.rizopoulos at med.kuleuven.be  Tue May 31 16:22:54 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 31 May 2005 16:22:54 +0200
Subject: [R] A suggestion to improve ifelse behaviour with vector
	yes/noarguments
References: <0BDE2460F08BF0429F933A40431A61E801E8244E@vk2kmail01.valtiokonttori.local>
Message-ID: <002601c565ec$3c21c320$0540210a@www.domain>

there is nothing problematic there! According to the help-page of 
ifelse(), in the Details section you get:

"If yes or no are too short, their elements are recycled."
                                                ^^^^^^^^
and this is what you get in your example.


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm




----- Original Message ----- 
From: "M??kinen Jussi" <Jussi.Makinen at valtiokonttori.fi>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, May 31, 2005 4:15 PM
Subject: [R] A suggestion to improve ifelse behaviour with vector 
yes/noarguments


> Dear All,
>
> I luckily found the following feature (or problem) when tried to 
> apply ifelse-function to an ordered data.
>
>> test <- c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE)
>> ifelse(test, 0, 1:4)
> [1] 0 0 0 4 1 2 3
>>
>
> It roots into the ifelse-syntax:
>
> ans[!test & !nas] <- rep(no, length.out = length(ans))[!test & !nas]
>
> Would it be possible to disable this feature in the next R-version? 
> For instance change the code to be:
>
> ans[!test & !nas] <- rep(no, length.out = length(ans[!test & !nas]))
>
> which seems to solve the problem.
>
> Best regards,
>
> Jussi M??kinen
>
> Jussi M??kinen
> Analyst
> State Treasury, Finland
> www.statetreasury.fi
> mailto:jussi.makinen at valtiokonttori.fi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From davidr at rhotrading.com  Tue May 31 16:28:40 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Tue, 31 May 2005 09:28:40 -0500
Subject: [R] Null space (or kernel) and image of a matrix
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FB4B2@rhosvr02.rhotrading.com>

help.search("null space")
gives 
Null(MASS)                          Null Spaces of Matrices

PLEASE PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

David Reiner
Rho Trading Securities, LLC

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Huntsinger, Reid
Sent: Tuesday, May 31, 2005 9:02 AM
To: 'Ale?? ??iberna'; R-help
Subject: RE: [R] Null space (or kernel) and image of a matrix

The null space or kernel of a matrix A would be the subspace of vectors v
such that Av = 0. That is, it's the solution space of a homogeneous linear
system of equations whose coefficients are the rows of A. R implements many
ways to solve such systems. The image is the set of vectors of the form Av
as v ranges over all vectors, which is the same as the span of the columns
of A, aka the column space of A. You can reduce this to a basis or transform
it to an orthogonal basis. Have a look at a linear algebra book to get an
idea for the possibilities. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ale?? ??iberna
Sent: Tuesday, May 31, 2005 7:22 AM
To: R-help
Subject: [R] Null space (or kernel) and image of a matrix


Hello!

Does anyone now if there exist a function that would compute a "null space" 
(or "kernel" - "Ker") of a matrix and maybe also one that would compute an 
"image" ("Im") of a matrix.

I tried R-site search and google, However I found notnihg useful!

Thanks for any sugestions! I am also not sure what an "image" of a matrix 
is, so suggestion in this directions  would also be  apreciated.

Ale?? ??iberna

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MSchwartz at mn.rr.com  Tue May 31 16:30:05 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 31 May 2005 09:30:05 -0500
Subject: [R] Formatting numbers with a limited amount of
	digits	consistently
In-Reply-To: <971536df05053020533592e59c@mail.gmail.com>
References: <d7f8op$fa3$1@sea.gmane.org> <429B4B72.9000109@stats.uwo.ca>
	<971536df05053010571ad00617@mail.gmail.com>
	<429B65A5.3000503@stats.uwo.ca>
	<971536df05053020533592e59c@mail.gmail.com>
Message-ID: <1117549806.22595.144.camel@horizons.localdomain>

On Mon, 2005-05-30 at 23:53 -0400, Gabor Grothendieck wrote:
> On 5/30/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > Gabor Grothendieck wrote:
> > > On 5/30/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > >
> > >>Henrik Andersson wrote:
> > >>
> > >>>I have tried to get signif, round and format to display numbers like
> > >>>these consistently in a table, using e.g. signif(x,digits=3)
> > >>>
> > >>>17.01
> > >>>18.15
> > >>>
> > >>>I want
> > >>>
> > >>>17.0
> > >>>18.2
> > >>>
> > >>>Not
> > >>>
> > >>>17
> > >>>18.2
> > >>>
> > >>>
> > >>>Why is the last digit stripped off in the case when it is zero!
> > >>
> > >>signif() changes the value; you don't want that, you want to affect how
> > >>a number is displayed.  Use format() or formatC() instead, for example
> > >>
> > >> > x <- c(17.01, 18.15)
> > >> > format(x, digits=3)
> > >>[1] "17.0" "18.1"
> > >> > noquote(format(x, digits=3))
> > >>[1] 17.0 18.1
> > >>
> > >
> > >
> > > That works in the above context but I don't think it works generally:
> > >
> > > R> f <- head(faithful)
> > > R> f
> > >   eruptions waiting
> > > 1     3.600      79
> > > 2     1.800      54
> > > 3     3.333      74
> > > 4     2.283      62
> > > 5     4.533      85
> > > 6     2.883      55
> > >
> > > R> format(f, digits = 3)
> > >   eruptions waiting
> > > 1      3.60      79
> > > 2      1.80      54
> > > 3      3.33      74
> > > 4      2.28      62
> > > 5      4.53      85
> > > 6      2.88      55
> > >
> > > R> # this works in this case
> > > R> noquote(prettyNum(round(f,1), nsmall = 1))
> > >      eruptions waiting
> > > [1,] 3.6       79.0
> > > [2,] 1.8       54.0
> > > [3,] 3.3       74.0
> > > [4,] 2.3       62.0
> > > [5,] 4.5       85.0
> > > [6,] 2.9       55.0
> > >
> > > and even that does not work in the desired way (which presumably
> > > is not to use exponent format) if you have some
> > > large enough numbers like 1e6 which it will display using
> > > the e notation rather than using ordinary notation.
> > 
> > formatC with format="f" seems to work for me, though it assumes you're
> > specifying decimal places rather than significant digits.  It also wants
> > a vector of numbers as input, not a dataframe.  So the following gives
> > pretty flexible control over what a table will look like:
> > 
> >  > data.frame(eruptions = formatC(f$eruptions, digits=2, format='f'),
> > +            waiting = formatC(f$waiting, digits=1, format='f'))
> >    eruptions waiting
> > 1 1000000.11    79.0
> > 2       1.80    54.0
> > 3       3.33    74.0
> > 4       2.28    62.0
> > 5       4.53    85.0
> > 6       2.88    55.0
> > 
> > >
> > > I have struggled with this myself and have generally been able
> > > to come up with something for specific instances but I have generally
> > > found it a pain to do a simple thing like format a table exactly as I want
> > > without undue effort.  Maybe someone else has figured this out.
> > 
> > I think that formatting tables properly requires some thought, and R is
> > no good at thinking.  You can easily recognize a badly formatted table,
> > but it's very hard to write down rules that work in general
> > circumstances.  It's also a matter of taste, so if I managed to write a
> > function that matched my taste, you would find you wanted to make changes.
> > 
> > It's sort of like expecting plot(x, y) to always come up with the best
> > possible plot of y versus x.  It's just not a reasonable expectation.
> > It's better to provide tools (like abline() for plots or formatC() for
> > tables) that allow you to tailor a plot or table to your particular needs.
> > 
> 
> Thanks.  That seems to be the idiom I was missing.  One thing that would
> be nice would be if formatC could handle data frames.


Guys, perhaps I am missing something here, but there seems to be some
confusion as to how the numbers are stored internally, versus how the
output is displayed and the meaning of "significant digits", which is
what I believe Henrik's original query was about.

By default, R's printed output uses the settings from options("digits")
and options("scipen") to define output based upon the number of
significant digits, which is of course not the same as the number of
decimal places. Hence the variance in the output that Henrik gets and
why the trailing zero is dropped.

The use of signif() does not help here because it is still based upon
the number of significant digits, where the trailing zero still gets
dropped.

The use of the above are "inexact" when it comes to creating formatted
output for a table with a consistent number of decimal places to align
columns of numbers.

format() is still problematic here because it too uses the number of
significant digits, defaulting to options("digits").

Using formatC() or sprintf() in conjunction with cat() is usually the
best way to gain control over how numeric output is formatted,
especially in a nicely aligned table. This is what I use in CrossTable
(), where I want decimal aligned columns for numbers in the tabular
output, along with fixed width columns for textual output (ie. labels,
etc.).

Briefly, along the lines of Gabor's example on the output using the
faithful dataset above, one could use something like:

> f <- head(faithful)

> noquote(apply(f, 2, function(x) formatC(x, format = "f", digits = 1)))
  eruptions waiting
1 3.6       79.0
2 1.8       54.0
3 3.3       74.0
4 2.3       62.0
5 4.5       85.0
6 2.9       55.0

which only affects how the data is printed, not the data itself. It can
work fine for a 2D object that has all numeric columns. 

Note however that the numeric columns are left-aligned, not right-
aligned, as in the default print method, since the output of the above
function is a character matrix, rather than a data.frame with numeric
columns. Hence, note:

> f
  eruptions waiting
1     3.600      79
2     1.800      54
3     3.333      74
4     2.283      62
5     4.533      85
6     2.883      55


Thus, for greater control, one should use sprintf() and cat():


out.lines <- sprintf("%15s %15s\n", colnames(f)[1], colnames(f)[2])

for (i in 1:nrow(f))
{
  out.lines <- c(out.lines, 
                 sprintf("%14.1f  %14.1f\n", f[i, 1], f[i, 2]))
}


> cat(out.lines)
      eruptions         waiting
            3.6            79.0
            1.8            54.0
            3.3            74.0
            2.3            62.0
            4.5            85.0
            2.9            55.0



In the above case, one can specify the column widths for the column
labels and the row values. Of course, the above could be extended to
become a generic function for data frames with multiple data types, with
arguments enabling the specification of column widths, number of decimal
places, etc. One might even want more than one specification for the
number of decimal places depending upon the nature of the columns on the
object to be printed, so vectors could be used for these arguments.

I'll leave that for further exercise.

Final note to Henrik: Note that the IEEE 754 rounding standard as
implemented in R results in:

> round(18.15, 1)
[1] 18.1
> formatC(18.15, format = "f", digits = 1)
[1] "18.1"
> sprintf("%5.1f", 18.15)
[1] " 18.1"

This is because the rounding method implemented is the "go to the even
digit" approach. Thus, you don't get 18.2. 

See ?round for more information.

HTH,

Marc Schwartz



From tlumley at u.washington.edu  Tue May 31 16:37:52 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 31 May 2005 07:37:52 -0700 (PDT)
Subject: [R] lars / lasso with glm
In-Reply-To: <FADCFAA8BA80C748890C1D3893C198D95C7185@amedmlmhah01.eur.amed.ds.army.mil>
References: <FADCFAA8BA80C748890C1D3893C198D95C7185@amedmlmhah01.eur.amed.ds.army.mil>
Message-ID: <Pine.A41.4.61b.0505310736210.3422@homer12.u.washington.edu>

On Tue, 31 May 2005, Bliese, Paul D LTC USAMH wrote:

> We have been using Least Angle Regression (lars) to help identify
> predictors in models where the outcome is continuous.  To do so we have
> been relying on the lars package.  Theoretically, it should be possible
> to use the lars procedure within a general linear model (glm) framework
> - we are particular interested in a logistic regression model.  Does
> anyone have examples of using lars with logistic regression?
>

I think the problem is that the nice efficient algorithm for lars doesn't 
work with generalized linear models because the weights depend on the 
fitted means.

 	-thomas



From fooms at euroscreen.be  Tue May 31 16:46:45 2005
From: fooms at euroscreen.be (=?iso-8859-1?Q?Fr=E9d=E9ric_Ooms?=)
Date: Tue, 31 May 2005 16:46:45 +0200
Subject: [R] Scaling data
Message-ID: <5198ADA420721246BC35BFA666E24F16D742EF@euromail.euroscreen.be>

Hello,
How could I scale and center the data contained in the attached file since I would like to do this before doing a PCA analysis on them as well as drawing boxplot.
 
 <<DB_molprop.txt>> 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: DB_molprop.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/acc11f6c/DB_molprop.txt

From andy_liaw at merck.com  Tue May 31 16:52:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 31 May 2005 10:52:00 -0400
Subject: [R] lars / lasso with glm
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E8E1@usctmx1106.merck.com>

> From: Thomas Lumley
> 
> On Tue, 31 May 2005, Bliese, Paul D LTC USAMH wrote:
> 
> > We have been using Least Angle Regression (lars) to help identify
> > predictors in models where the outcome is continuous.  To 
> do so we have
> > been relying on the lars package.  Theoretically, it should 
> be possible
> > to use the lars procedure within a general linear model 
> (glm) framework
> > - we are particular interested in a logistic regression model.  Does
> > anyone have examples of using lars with logistic regression?
> >
> 
> I think the problem is that the nice efficient algorithm for 
> lars doesn't 
> work with generalized linear models because the weights depend on the 
> fitted means.
> 
>  	-thomas


Jerry Friedman's PathSeeker is similar to LAR, and can do (2-class)
classification, using a different loss function.  See his web site for tech
report and software.

Andy



From sdavis2 at mail.nih.gov  Tue May 31 16:53:28 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 31 May 2005 10:53:28 -0400
Subject: [R] Scaling data
In-Reply-To: <5198ADA420721246BC35BFA666E24F16D742EF@euromail.euroscreen.be>
References: <5198ADA420721246BC35BFA666E24F16D742EF@euromail.euroscreen.be>
Message-ID: <20ef07104d98227c46654bde18d7e7a5@mail.nih.gov>

I didn't look at your data, but you might try:

help(scale)

Sean

On May 31, 2005, at 10:46 AM, Fr??d??ric Ooms wrote:

> Hello,
> How could I scale and center the data contained in the attached file 
> since I would like to do this before doing a PCA analysis on them as 
> well as drawing boxplot.
>
>  <<DB_molprop.txt>>
>
> <DB_molprop.txt>______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Jerome.Audouy.ext at siemensvdo.com  Tue May 31 17:00:45 2005
From: Jerome.Audouy.ext at siemensvdo.com (Audouy Jerome (ext_sub))
Date: Tue, 31 May 2005 17:00:45 +0200
Subject: [R] jpeg function problem with rterm.exe
Message-ID: <C4D8022D8C6AAA489A7D44067394D53001BD9C84@tlsm383a.ww011.siemens.net>

Hello,

I'm starting using R and runned a little script. The generated graphic could be exported correctly with RGui.exe ("file -> save as -> jpeg -> etc.")

But if I try to run the same script with Rterm.exe and followed by:
jpeg(file="test.jpeg", quality=90)
The created jpeg file is completely white without any color, do you know why ?

Sum up of the script:
    library(DBI)
    library(RODBC)
    library(graphics)
    # my script process
    ...
    ...
    # save picture
    jpeg(file="test.jpeg", quality=90)


Thanks to help me.

Regards,
J??r??me.



From ligges at statistik.uni-dortmund.de  Tue May 31 17:04:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 May 2005 17:04:15 +0200
Subject: [R] Scaling data
In-Reply-To: <5198ADA420721246BC35BFA666E24F16D742EF@euromail.euroscreen.be>
References: <5198ADA420721246BC35BFA666E24F16D742EF@euromail.euroscreen.be>
Message-ID: <429C7CEF.1070906@statistik.uni-dortmund.de>

Fr??d??ric Ooms wrote:

> Hello,
> How could I scale and center the data contained in the attached file since I would like to do this before doing a PCA analysis on them as well as drawing boxplot.
>  
>  <<DB_molprop.txt>> 
> 

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html

Among MANY other tipps, it suggests to use help.search("keyword")

Please do so!

Please also read help pages, already the question related to pairs() 
could have been avoided by reading the help pages ....

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Tue May 31 17:10:05 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 May 2005 17:10:05 +0200
Subject: [R] jpeg function problem with rterm.exe
In-Reply-To: <C4D8022D8C6AAA489A7D44067394D53001BD9C84@tlsm383a.ww011.siemens.net>
References: <C4D8022D8C6AAA489A7D44067394D53001BD9C84@tlsm383a.ww011.siemens.net>
Message-ID: <429C7E4D.60104@statistik.uni-dortmund.de>

Audouy Jerome (ext_sub) wrote:

> Hello,
> 
> I'm starting using R and runned a little script. The generated graphic could be exported correctly with RGui.exe ("file -> save as -> jpeg -> etc.")
> 
> But if I try to run the same script with Rterm.exe and followed by:
> jpeg(file="test.jpeg", quality=90)
> The created jpeg file is completely white without any color, do you know why ?
> 
> Sum up of the script:
>     library(DBI)
>     library(RODBC)
>     library(graphics)
>     # my script process
>     ...
>     ...
>     # save picture
>     jpeg(file="test.jpeg", quality=90)


Please read An Introduction to R.
Please read ?Devices
Please read ?jpeg and see its examples!

jpeg() is not intended to save a plot, but to open a jpeg device you can 
plot into and close it afterwards:

   jpeg(file="test.jpeg", quality=90)
   plot(1:10)
   dev.off()

Please try it interaktively at first.

Uwe Ligges




> 
> Thanks to help me.
> 
> Regards,
> J??r??me.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Tue May 31 17:11:55 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 31 May 2005 11:11:55 -0400
Subject: [R] Formatting numbers with a limited amount
	of	digits	consistently
In-Reply-To: <1117549806.22595.144.camel@horizons.localdomain>
References: <d7f8op$fa3$1@sea.gmane.org> <429B4B72.9000109@stats.uwo.ca>	
	<971536df05053010571ad00617@mail.gmail.com>
	<429B65A5.3000503@stats.uwo.ca>	
	<971536df05053020533592e59c@mail.gmail.com>
	<1117549806.22595.144.camel@horizons.localdomain>
Message-ID: <429C7EBB.6060705@stats.uwo.ca>

Marc Schwartz wrote:

> Final note to Henrik: Note that the IEEE 754 rounding standard as
> implemented in R results in:
> 
> 
>>round(18.15, 1)
> 
> [1] 18.1
> 
>>formatC(18.15, format = "f", digits = 1)
> 
> [1] "18.1"
> 
>>sprintf("%5.1f", 18.15)
> 
> [1] " 18.1"
> 
> This is because the rounding method implemented is the "go to the even
> digit" approach. Thus, you don't get 18.2. 
> 
> See ?round for more information.

I don't think "go to the even digit" is being applied here:  ".1" is not 
  an even digit.

I suspect what's going on in this example is that 18.15 is not being 
represented exactly; it's stored internally as something slightly less 
than that value, so it rounds down.

You'd see the "go to the even digit" rule applied when rounding 17.5 or 
18.5, which can be represented exactly, being fractions with a power of 
2 in the denominator:

 > round(18.5, 0)
[1] 18
 > round(17.5, 0)
[1] 18

(This is very gratifying.  Usually when I try to predict the exact 
behaviour of round() or signif() I end up having to rewrite my 
prediction afterwards.  But this time I got it right. Honest!)

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Tue May 31 17:13:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 May 2005 17:13:56 +0200
Subject: [R] FYI: Problems while loading package "class (VR)"
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F4083@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F4083@us-arlington-0668.mail.saic.com>
Message-ID: <429C7F34.8070404@statistik.uni-dortmund.de>

Tuszynski, Jaroslaw W. wrote:

>>What happens after you reinstalling it?
> 
> After installing VR bundle everything works perfectly. All problems solved. 
> 
>>Version of R?      
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    1.0            
> year     2005           
> month    04             
> day      18             
> language R       
> 
>>Version of VR?     
> 
> packageDescription("class")$Version = "7.2-15"
> 
>>Operating Sytstem? 
> 
> Win xp
> 
>>PLEASE do read the posting guide!
> 
> Sorry about incomplete posting. 
> 
> My email was to inform that there are some problems with instaling package
> "class (VR)" by itself. If I go to Menu
> Packages/Install-package(s)/class(VR) I am still getting:
> 
>>utils:::menuInstallPkgs()
> 
>   dependency 'class (VR)' is not available


I see, so you have used the menu in Windows ...
This is a bug. I will followup on R-bugs or R-devel later this day.

BTW: install.packages("class") works for me as expected.

Uwe Ligges






> Jarek
> ====================================================\=======
> 
>  Jarek Tuszynski, PhD.                           o / \ 
>  Science Applications International Corporation  <\__,|  
>  (703) 676-4192                                   ">   \
>  Jaroslaw.W.Tuszynski at saic.com                     `    \
> 
> 
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: Tuesday, May 31, 2005 9:49 AM
> To: Tuszynski, Jaroslaw W.
> Cc: (r-help at stat.math.ethz.ch.); 'ripley at stats.ox.ac.uk'
> Subject: Re: [R] FYI: Problems while loading package "class (VR)"
> 
> Tuszynski, Jaroslaw W. wrote:
> 
> 
>>Hi,
>>
>>Today, I performed automatic updates of packages and somehow lost "class"
>>package. I loaded a library that depended on it and got:
>>	Error: package 'class' could not be loaded
>>	In addition: Warning message: there is no package called 'class' in:
>>library(pkg, character.only = TRUE, logical = TRUE, lib.loc = lib.loc) 
>>I tried to load it by instal-package: class (VR) and got error: 
>>"dependency 'class (VR)' is not available". I did succesfully loaded 
>>it by
>>instal-package: VR.
> 
> 
> What happens after you reinstalling it?
> 
> Version of R?
> Version of VR?
> Operating Sytstem?
> 
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> Uwe Ligges
> 
> 
> 
>>Jarek
>>=====================================\====                 
>> Jarek Tuszynski, PhD.                               o / \ 
>> Science Applications International Corporation  <\__,|  
>> (703) 676-4192                        ">  \
>> Jaroslaw.W.Tuszynski at saic.com                   `    \
>>
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html



From liuwensui at gmail.com  Tue May 31 17:15:50 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 31 May 2005 11:15:50 -0400
Subject: [R] lars / lasso with glm
In-Reply-To: <FADCFAA8BA80C748890C1D3893C198D95C7185@amedmlmhah01.eur.amed.ds.army.mil>
References: <FADCFAA8BA80C748890C1D3893C198D95C7185@amedmlmhah01.eur.amed.ds.army.mil>
Message-ID: <1115a2b0050531081520e6eba0@mail.gmail.com>

please check the library lasso2.

On 5/31/05, Bliese, Paul D LTC USAMH <paul.bliese at us.army.mil> wrote:
> We have been using Least Angle Regression (lars) to help identify
> predictors in models where the outcome is continuous.  To do so we have
> been relying on the lars package.  Theoretically, it should be possible
> to use the lars procedure within a general linear model (glm) framework
> - we are particular interested in a logistic regression model.  Does
> anyone have examples of using lars with logistic regression?
> 
> 
> 
> Thanks,
> 
> 
> 
> PB
> 
> 
> 
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From murdoch at stats.uwo.ca  Tue May 31 17:16:48 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 31 May 2005 11:16:48 -0400
Subject: [R] A suggestion to improve ifelse behaviour with vector yes/no
	arguments
In-Reply-To: <0BDE2460F08BF0429F933A40431A61E801E8244E@vk2kmail01.valtiokonttori.local>
References: <0BDE2460F08BF0429F933A40431A61E801E8244E@vk2kmail01.valtiokonttori.local>
Message-ID: <429C7FE0.6010302@stats.uwo.ca>

M??kinen Jussi wrote:
> Dear All,
> 
> I luckily found the following feature (or problem) when tried to apply ifelse-function to an ordered data.
> 
> 
>>test <- c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE)
>>ifelse(test, 0, 1:4)
> 
> [1] 0 0 0 4 1 2 3
> 
> 
> It roots into the ifelse-syntax:
> 
> ans[!test & !nas] <- rep(no, length.out = length(ans))[!test & !nas]
> 
> Would it be possible to disable this feature in the next R-version? For instance change the code to be:
> 
> ans[!test & !nas] <- rep(no, length.out = length(ans[!test & !nas]))
> 
> which seems to solve the problem.

As Dimitris said, this is just recycling.  I think getting rid of 
recycling on vectors with length greater than 1 would have been a good 
decision in S about 15 years ago, but it's too late now.

Duncan Murdoch



From mmccall at mail.nih.gov  Tue May 31 17:22:28 2005
From: mmccall at mail.nih.gov (Matt McCall)
Date: Tue, 31 May 2005 11:22:28 -0400
Subject: [R] pdf error msg
Message-ID: <7095ebaa8d4659b5a06405351cb22339@mail.nih.gov>

I get the following error message when trying to start a pdf file:

 > pdf("hi.pdf")
Error in PDF(file, old$family, old$encoding, old$bg, old$fg, width, 
height,  :
	unable to start device pdf
In addition: Warning message:
cannot open `pdf' file argument `hi.pdf'
*** malloc[477]: Deallocation of a pointer not malloced: 0x143cf5c0; 
This could be a double free(), or free() called with the middle of an 
allocated block; Try setting environment variable MallocHelp to see 
tools to help debug

          _
platform powerpc-apple-darwin6.8
arch     powerpc
os       darwin6.8
system   powerpc, darwin6.8
status
major    2
minor    0.0
year     2004
month    10
day      04
language R

 > sessionInfo()
R version 2.0.0, 2004-10-04, powerpc-apple-darwin6.8

attached base packages:
[1] "tools"     "methods"   "stats"     "graphics"  "grDevices" "utils" 
     "datasets"  "base"

other attached packages:
     convert     seanlib      gplots       gdata      gtools geneplotter 
    annotate     Biobase      marray       limma   firstlook
     "1.1.9"       "1.0"     "2.0.0"     "2.0.0"     "2.0.0"     "1.4.3" 
     "1.5.1"    "1.4.22"    "1.5.24"     "1.8.1"       "1.4"

Matt McCall



From amberfer at ull.es  Tue May 31 17:29:31 2005
From: amberfer at ull.es (amberfer@ull.es)
Date: Tue, 31 May 2005 16:29:31 +0100
Subject: [R] post hoc kruskal wallis
Message-ID: <1117553371.429c82dbe7b21@correoweb.ccti.ull.es>

Hola, 
Tengo un problema con el post hoc kruskal wallis. No encuentro en el SSPS 
ning??n test estad??stico para el post hoc. En el libro de "Zar" proponen el 
Turkey-type nonparametric multicomparison. Mi problema es que tengo muchos 
datos y hacerlo a mano es complicado. El R es un programa que conozco menos, no 
se si habr?? alguna prueba que pueda servir. Los test del paquete npmc no se si 
sirven para el post hoc ni tampoco entoy seguro que el test NDWD del paquete 
coin ser??a adecuado.
Un saludo y gracias por su ayuda
Alfredo Berm??dez



From buser at stat.math.ethz.ch  Tue May 31 17:32:30 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 31 May 2005 17:32:30 +0200
Subject: [R] POSIX problem
In-Reply-To: <Pine.LNX.4.61.0506010017100.18901@stat12.stat.auckland.ac.nz>
References: <Pine.LNX.4.61.0506010017100.18901@stat12.stat.auckland.ac.nz>
Message-ID: <17052.33678.501430.123302@stat.math.ethz.ch>

Dear David

I tried to reproduce your example. It is not exact the same. When I create
a dataframe, character are transformed to factors except I use I() to
protect "them".

> PeopleData.df <- data.frame(StartDate = c("29/10/2001", "7/12/2001",
                                "16/11/2001", "28/11/2001", "2/11/2001",
                                "26/11/2001"),
                              StartTime = c("15:26", "10:32", "13:58", "14:00",
                                "15:22", "11:15"))
> str(PeopleData.df)
data.frame':	6 obs. of  2 variables:
 $ StartDate: Factor w/ 6 levels "16/11/2001","2/..",..: 5 6 1 4 2 3
 $ StartTime: Factor w/ 6 levels "10:32","11:15",..: 6 1 3 4 5 2

So there is a small difference to your dataframe. Then the following
produces what you want

> as.POSIXct(strptime(paste(PeopleData.df$StartDate,PeopleData.df$StartTime),
                      format="%d/%m/%Y %H:%M"))
[1] "2001-10-29 15:26:00 CET" "2001-12-07 10:32:00 CET"
[3] "2001-11-16 13:58:00 CET" "2001-11-28 14:00:00 CET"
[5] "2001-11-02 15:22:00 CET" "2001-11-26 11:15:00 CET"

Regards,

Christoph

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



David Scott writes:
 > 
 > I am having trouble with creating a POSIXct object. If I create a variable 
 > of class Date first out of the date part of my data, I am ok, but if I 
 > just paste the date and time parts together and try and create the POSIXct 
 > object, I have problems.
 > 
 > Here is a toy example created from the actual data which caused the 
 > problem. I am using R 2.0.1 on Windows XP.
 > 
 > > # Data frame with dates and times, as character
 > > PeopleData.df
 >     StartDate StartTime
 > 1 29/10/2001     15:26
 > 2  7/12/2001     10:32
 > 3 16/11/2001     13:58
 > 4 28/11/2001     14:00
 > 5  2/11/2001     15:22
 > 6 26/11/2001     11:15
 > > str(PeopleData.df)
 > `data.frame':   6 obs. of  2 variables:
 >   $ StartDate: chr  "29/10/2001" "7/12/2001" "16/11/2001" "28/11/2001" ...
 >   $ StartTime: chr  "15:26" "10:32" "13:58" "14:00" ...
 > > dput(PeopleData.df)
 > structure(list(StartDate = c("29/10/2001", "7/12/2001", "16/11/2001",
 > "28/11/2001", "2/11/2001", "26/11/2001"), StartTime = c("15:26",
 > "10:32", "13:58", "14:00", "15:22", "11:15")), .Names = c("StartDate",
 > "StartTime"), row.names = c("1", "2", "3", "4", "5", "6"), class = 
 > "data.frame")
 > > BeginDate <- as.Date(PeopleData.df$StartDate,format="%d/%m/%Y")
 > > BeginDate
 > [1] "2001-10-29" "2001-12-07" "2001-11-16" "2001-11-28" "2001-11-02"
 > [6] "2001-11-26"
 > > # Create POSIXct date-time object without difficulty
 > > BeginTime <- as.POSIXct(format(paste(BeginDate,PeopleData.df$StartTime),
 > +                                 format="%Y/%m/%d %H:%M"))
 > > BeginTime
 > [1] "2001-10-29 15:26:00 New Zealand Standard Time"
 > [2] "2001-12-07 10:32:00 New Zealand Standard Time"
 > [3] "2001-11-16 13:58:00 New Zealand Standard Time"
 > [4] "2001-11-28 14:00:00 New Zealand Standard Time"
 > [5] "2001-11-02 15:22:00 New Zealand Standard Time"
 > [6] "2001-11-26 11:15:00 New Zealand Standard Time"
 > > # But not directly from the dates and times
 > > BeginTime <- 
 > as.POSIXct(format(paste(PeopleData.df$StartDate,PeopleData.df$StartTime),
 > +                                 format="%d/%m/%Y %H:%M"))
 > > BeginTime
 > [1] "0029-10-20 New Zealand Standard Time"
 > [2] "0007-12-20 New Zealand Standard Time"
 > [3] "0016-11-20 New Zealand Standard Time"
 > [4] "0028-11-20 New Zealand Standard Time"
 > [5] "0002-11-20 New Zealand Standard Time"
 > [6] "0026-11-20 New Zealand Standard Time"
 > > # Format looks correct to me
 > > paste(PeopleData.df$StartDate,PeopleData.df$StartTime)
 > [1] "29/10/2001 15:26" "7/12/2001 10:32"  "16/11/2001 13:58" "28/11/2001 
 > 14:00"
 > [5] "2/11/2001 15:22"  "26/11/2001 11:15"
 > 
 > What I think might be causing the problem is the lack of a leading zero 
 > for some of the days (as in 7/12/2001). This doesn't phase as.Date though.
 > 
 > David Scott
 > 
 > 
 > 
 > _________________________________________________________________
 > David Scott	Department of Statistics, Tamaki Campus
 >  		The University of Auckland, PB 92019
 >  		Auckland	NEW ZEALAND
 > Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
 > Email:	d.scott at auckland.ac.nz
 > 
 > 
 > Graduate Officer, Department of Statistics
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jrclmilks at joimail.com  Tue May 31 17:40:09 2005
From: jrclmilks at joimail.com (Jim Milks)
Date: Tue, 31 May 2005 11:40:09 -0400
Subject: [R] plane3d
Message-ID: <30d43f27fe4feb9d67d683089bce4b0b@joimail.com>

I am attempting to fit a logistic regression plane to a 3-D scatterplot 
(which was generated using scatterplot3d).  I've noticed that the help 
pages of scatterplot3d include a function titled "plane3d."  However, 
when I attempt to use the function, I get the following message:

Error: couldn't find function "plane3d"

I've searched the archives and found no references to such a function.  
Is (or was) plane3d an actual function or is there just a typo in the 
scatterplot3d help page?  If it is a function, how would I tap into it?

Thanks.

Jim Milks
Graduate Student
Environmental Sciences Ph.D. Program
Wright State University
3640 Colonel Glenn Hwy
Dayton, OH 45435



From tlumley at u.washington.edu  Tue May 31 17:45:56 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 31 May 2005 08:45:56 -0700 (PDT)
Subject: [R] A suggestion to improve ifelse behaviour with vector yes/no
	arguments
In-Reply-To: <429C7FE0.6010302@stats.uwo.ca>
References: <0BDE2460F08BF0429F933A40431A61E801E8244E@vk2kmail01.valtiokonttori.local>
	<429C7FE0.6010302@stats.uwo.ca>
Message-ID: <Pine.A41.4.61b.0505310838360.190182@homer05.u.washington.edu>

On Tue, 31 May 2005, Duncan Murdoch wrote:

> M?kinen Jussi wrote:
>> Dear All,
>> 
>> I luckily found the following feature (or problem) when tried to apply 
>> ifelse-function to an ordered data.
>> 
>> 
>>> test <- c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE)
>>> ifelse(test, 0, 1:4)
>> 
>> [1] 0 0 0 4 1 2 3
>> 
<snippage>
>
> As Dimitris said, this is just recycling.  I think getting rid of recycling 
> on vectors with length greater than 1 would have been a good decision in S 
> about 15 years ago, but it's too late now.

It wouldn't help the original poster, though.  I agree that 0,0,0,4,1,2,3 
is a slightly weird result, but I can't think of any reasonable model for 
the behaviour of ifelse() that would give any other result except an error 
message. [or  0,NA,NA,4,NA,NA,NA, I suppose].

 	-thomas

From tlumley at u.washington.edu  Tue May 31 17:53:00 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 31 May 2005 08:53:00 -0700 (PDT)
Subject: [R] pdf error msg
In-Reply-To: <7095ebaa8d4659b5a06405351cb22339@mail.nih.gov>
References: <7095ebaa8d4659b5a06405351cb22339@mail.nih.gov>
Message-ID: <Pine.A41.4.61b.0505310848430.190182@homer05.u.washington.edu>

On Tue, 31 May 2005, Matt McCall wrote:

> I get the following error message when trying to start a pdf file:
>
>> pdf("hi.pdf")
> Error in PDF(file, old$family, old$encoding, old$bg, old$fg, width, height, 
> :
> 	unable to start device pdf
> In addition: Warning message:
> cannot open `pdf' file argument `hi.pdf'

In my experience, when R can't open pdf output file it is usually either 
because the file is locked by eg Acrobat or because you don't have write 
permission to the current directory.  The Mac's Preview program doesn't 
lock pdf files it is displaying, so the first explanation may not apply.


> *** malloc[477]: Deallocation of a pointer not malloced: 0x143cf5c0; This 
> could be a double free(), or free() called with the middle of an allocated 
> block; Try setting environment variable MallocHelp to see tools to help debug

That, on the other hand, does definitely sound like a bug.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From kevinvol2002 at yahoo.com  Tue May 31 17:55:28 2005
From: kevinvol2002 at yahoo.com (Hai Lin)
Date: Tue, 31 May 2005 08:55:28 -0700 (PDT)
Subject: [R] read.delim2 regarding "#" 
Message-ID: <20050531155528.58907.qmail@web32405.mail.mud.yahoo.com>

Hello R experts:

When I tried to read my data into R, it does not take
# sign 

A subset of Exp.txt is:
Experiment name  assay id	Varname	
(A1)DBA TPA 6h/DBA Acetone rep1(A1) #3	4090 	A90C1
(A2)DBA TPA 6h/DBA Acetone rep2(A2) #3	4091 	A91C1	
The command is:
Exp <- read.delim2("Exp.txt",check.names=F,as.is=T)

It is excuted but gave me all the NAs. Can you all
drop me a hint?

Thanks in advance.

Kevin



From tlumley at u.washington.edu  Tue May 31 18:03:37 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 31 May 2005 09:03:37 -0700 (PDT)
Subject: [R] pdf error msg
In-Reply-To: <7095ebaa8d4659b5a06405351cb22339@mail.nih.gov>
References: <7095ebaa8d4659b5a06405351cb22339@mail.nih.gov>
Message-ID: <Pine.A41.4.61b.0505310900420.190182@homer05.u.washington.edu>

On Tue, 31 May 2005, Matt McCall wrote:

> *** malloc[477]: Deallocation of a pointer not malloced: 0x143cf5c0; This 
> could be a double free(), or free() called with the middle of an allocated 
> block; Try setting environment variable MallocHelp to see tools to help debug
>

And this is a double free(). It looks as though the function for creating 
a PDF device used not to free the passed-in structure in case of error, 
but it now does.

Thanks for reporting this.

 	-thomas



From ligges at statistik.uni-dortmund.de  Tue May 31 18:05:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 May 2005 18:05:49 +0200
Subject: [R] plane3d
In-Reply-To: <30d43f27fe4feb9d67d683089bce4b0b@joimail.com>
References: <30d43f27fe4feb9d67d683089bce4b0b@joimail.com>
Message-ID: <429C8B5D.8050500@statistik.uni-dortmund.de>

Jim Milks wrote:
> I am attempting to fit a logistic regression plane to a 3-D scatterplot 
> (which was generated using scatterplot3d).  I've noticed that the help 
> pages of scatterplot3d include a function titled "plane3d."  However, 
> when I attempt to use the function, I get the following message:
> 
> Error: couldn't find function "plane3d"
> 
> I've searched the archives and found no references to such a function.  
> Is (or was) plane3d an actual function or is there just a typo in the 
> scatterplot3d help page?  If it is a function, how would I tap into it?


Please read ?scatterplot3d carefully and completely. The function is 
*retunred* in form of a closure from a call to scatterplot3d() and there 
is an example how to use it in the examples section of the help file:

   library(scatterplot3d)

   ## example 5
   data(trees)
   s3d <- scatterplot3d(trees, type="h", highlight.3d=TRUE,
       angle=55, scale.y=0.7, pch=16, main="scatterplot3d - 5")
   # Now adding some points to the "scatterplot3d"
   s3d$points3d(seq(10,20,2), seq(85,60,-5), seq(60,10,-10),
       col="blue", type="h", pch=16)
   # Now adding a regression plane to the "scatterplot3d"
   attach(trees)
   my.lm <- lm(Volume ~ Girth + Height)
   s3d$plane3d(my.lm, lty.box = "solid")

Uwe Ligges




> Thanks.
> 
> Jim Milks
> Graduate Student
> Environmental Sciences Ph.D. Program
> Wright State University
> 3640 Colonel Glenn Hwy
> Dayton, OH 45435
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From 0034058 at fudan.edu.cn  Tue May 31 18:04:22 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Wed, 01 Jun 2005 00:04:22 +0800
Subject: [R] is there material about Longitudinal Data Analysis with R?
Message-ID: <0IHD00BOZ2TW34@mail.fudan.edu.cn>

i am studying Longitudinal Data Analysis and want to carry it with R.anyone knows  any materials about Longitudinal Data Analysis with R in the internet which i can download?

thank you.



From Achim.Zeileis at wu-wien.ac.at  Tue May 31 17:49:26 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 31 May 2005 17:49:26 +0200
Subject: [R] plane3d
In-Reply-To: <30d43f27fe4feb9d67d683089bce4b0b@joimail.com>
References: <30d43f27fe4feb9d67d683089bce4b0b@joimail.com>
Message-ID: <20050531174926.252d4c57.Achim.Zeileis@wu-wien.ac.at>

On Tue, 31 May 2005 11:40:09 -0400 Jim Milks wrote:

> I am attempting to fit a logistic regression plane to a 3-D
> scatterplot (which was generated using scatterplot3d).  I've noticed
> that the help pages of scatterplot3d include a function titled
> "plane3d."  However, when I attempt to use the function, I get the
> following message:
> 
> Error: couldn't find function "plane3d"
> 
> I've searched the archives and found no references to such a function.
>  
> Is (or was) plane3d an actual function or is there just a typo in the 
> scatterplot3d help page?  If it is a function, how would I tap into
> it?

Look harder at the man page ?scatterplot3d. plane3d is a function that
is part of the return value of scatterplot3d as it depends on the
particular scatter plot generated. 

Look again at this part of the examples to see how it works:

library(scatterplot3d)
attach(trees)
my.lm <- lm(Volume ~ Girth + Height)
s3d <- scatterplot3d(trees, type = "h", highlight.3d = TRUE, 
  angle = 55, scale.y = 0.7, pch = 16, main = "scatterplot3d - 5")
s3d$plane3d(my.lm, lty.box = "solid")

Z
 
> Thanks.
> 
> Jim Milks
> Graduate Student
> Environmental Sciences Ph.D. Program
> Wright State University
> 3640 Colonel Glenn Hwy
> Dayton, OH 45435
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Tue May 31 18:13:17 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 31 May 2005 11:13:17 -0500
Subject: [R] read.delim2 regarding "#"
In-Reply-To: <20050531155528.58907.qmail@web32405.mail.mud.yahoo.com>
References: <20050531155528.58907.qmail@web32405.mail.mud.yahoo.com>
Message-ID: <429C8D1D.2080200@pdf.com>



Hai Lin wrote:
> Hello R experts:
> 
> When I tried to read my data into R, it does not take
> # sign 
> 
> A subset of Exp.txt is:
> Experiment name  assay id	Varname	
> (A1)DBA TPA 6h/DBA Acetone rep1(A1) #3	4090 	A90C1
> (A2)DBA TPA 6h/DBA Acetone rep2(A2) #3	4091 	A91C1	
> The command is:
> Exp <- read.delim2("Exp.txt",check.names=F,as.is=T)
> 
> It is excuted but gave me all the NAs. Can you all
> drop me a hint?
> 
> Thanks in advance.
> 
> Kevin


Please (re)read ?read.delim2. It is just a wrapper for read.table. The 
latter has an argument for comment.char. Maybe try:

Exp <- read.delim2("Exp.txt", check.names = FALSE,
                     as.is = TRUE, comment.char = "")

--sundar



From khobson at fd9ns01.okladot.state.ok.us  Tue May 31 18:32:52 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Tue, 31 May 2005 11:32:52 -0500
Subject: [R] Add Columns and Order for Rbind?
Message-ID: <OF81B6A0B5.238B5180-ON86257012.0058EF5A-86257012.005AC726@fd9ns01.okladot.state.ok.us>





I am using rbind to add one list with one row to another master list.  The
problem is that not all columns exist in both lists.

What methods would you recommend to reorder one list based on the column
names of another?   If both sets have the same order and columns, I can
then use rbind.  I can live with columns being added to the master list set
it makes the task easier using something like union for column name
matching and order.

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From jsorkin at grecc.umaryland.edu  Tue May 31 18:39:40 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 31 May 2005 12:39:40 -0400
Subject: [R] Errors in Variables
Message-ID: <s29c5b22.095@grecc.umaryland.edu>

I have a routine that corrects regression coefficients for the bias towards zero that occurs when there is error in the measurement of the independent variable. The code only works for a single independent variable, i.e. y~x. At this time the program does not calculate the SE of the coefficient. The program uses properly weighted perpendicular least squares regression. I would be happy to share the code if asked to do so by anyone who has participated in this thread. 
John 

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu

>>> "John Fox" <jfox at mcmaster.ca> 5/29/2005 5:56:10 PM >>>

Dear Spencer,

> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Sent: Sunday, May 29, 2005 4:13 PM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch; 'Jacob van Wyk'; 'Eric-Olivier Le Bigot'
> Subject: Re: [R] Errors in Variables
> 
> Hi, John:
> 
> 	  Thanks for the clarification.  I know that the 
> "errors in X problem" 
> requires additional information, most commonly one of the 
> variances or the correlation.  The question I saw (below) 
> indicated he had tried "model of the form y ~ x (with a given 
> covariance matrix ...)", which made me think of "sem".
> 
> 	  If he wants "the least (orthogonal) distance", could 
> he could get it indirectly from "sem" by calling "sem" 
> repeatedly giving, say, a variance for "x", then averaging 
> the variances of "x" and "y" and trying that in "sem"?
> 

I'm not sure how that would work, but seems similar to averaging the
regressions of y on x and x on y.

> 	  Also, what do you know about "ODRpack"?  It looks 
> like that might solve "the least (orthogonal) distance".
> 

I'm not familiar with ODRpack, but it seems to me that one could fairly
simply minimize the sum of squared least distances using, e.g., optim.

Regards,
 John

> 	  Thanks again for your note, John.
> 	  Best Wishes,
> 	  Spencer Graves	
> 
> John Fox wrote:
> 
> > Dear Spencer,
> > 
> > The reason that I didn't respond to the original posting (I'm the 
> > author of the sem package), that that without additional 
> information 
> > (such as the error variance of x), a model with error in 
> both x and y 
> > will be underidentified (unless there are multiple indicators of x, 
> > which didn't seem to be the case here). I figured that what 
> Jacob had 
> > in mind was something like minimizing the least 
> (orthogonal) distance 
> > of the points to the regression line (implying by the way 
> that x and y 
> > are on the same scale or somehow standardized), which isn't 
> doable with sem as far as I'm aware.
> > 
> > Regards,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox 
> > --------------------------------
> > 
> > 
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Spencer Graves
> >>Sent: Saturday, May 28, 2005 4:47 PM
> >>To: Eric-Olivier Le Bigot
> >>Cc: r-help at stat.math.ethz.ch; Jacob van Wyk
> >>Subject: Re: [R] Errors in Variables
> >>
> >>	  I'm sorry, I have not followed this thread, but I 
> wonder if you 
> >>have considered library(sem), "structural equations modeling"?  
> >>"Errors in variables" problems are the canonical special case.
> >>
> >>	  Also, have you done a search of "www.r-project.org" 
> >>-> search -> "R site search" for terms like "errors in
> >>variables regression"?  This just led me to "ODRpack", 
> which is NOT a 
> >>CRAN package but is apparently available after a Google 
> search.  If it 
> >>were my problem, I'd first try to figure out "sem";  if that seemed 
> >>too difficult, I might then look at "ODRpack".
> >>
> >>	  Also, have you read the posting guide! 
> >>http://www.R-project.org/posting-guide.html?  This suggests, among 
> >>other things, that you provide a toy example that a potential 
> >>respondant could easily copy from your email, test a few 
> >>modifications, and prase a reply in a minute or so.
> >>This also helps clarify your question so any respondants are more 
> >>likely to suggest something that is actually useful to you. 
>  Moreover, 
> >>many people have reported that they were able to answer their own 
> >>question in the course of preparing a question for this 
> list using the 
> >>posting guide.
> >>
> >>	  hope this helps.  spencer graves
> >>
> >>Eric-Olivier Le Bigot wrote:
> >>
> >>
> >>>I'm interested in this "2D line fitting" too!  I've been looking, 
> >>>without success, in the list of R packages.
> >>>
> >>>It might be possible to implement quite easily some of the
> >>
> >>formalism
> >>
> >>>that you can find in Numerical Recipes (Fortran 77, 2nd ed.), 
> >>>paragraph 15.3.  As a matter of fact, I did this in R but
> >>
> >>only for a
> >>
> >>>model of the form y ~ x (with a given covariance matrix
> >>
> >>between x and
> >>
> >>>y).  I can send you the R code (preliminary version: I
> >>
> >>wrote it yesterday), if you want.
> >>
> >>>Another interesting reference might be Am. J. Phys. 60, p. 
> >>
> >>66 (1992).  
> >>
> >>>But, again, you would have to implement things by yourself.
> >>>
> >>>All the best,
> >>>
> >>>EOL
> >>>
> >>>-- 
> >>>Dr. Eric-Olivier LE BIGOT (EOL)                CNRS 
> >>
> >>Associate Researcher
> >>
> >>~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> >>~~~~o~o~~~
> >>
> >>>Kastler Brossel Laboratory (LKB)                   
> >>
> >>http://www.lkb.ens.fr 
> >>
> >>>Universit?? P. & M. Curie and Ecole Normale Sup??rieure, Case 74
> >>>4 place Jussieu              75252 Paris CEDEX 05           
> >>
> >>      France
> >>
> >>~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> >>~~~~o~o~~~
> >>
> >>>office  : 01 44 27 73 67                             fax: 
> >>
> >>01 44 27 38 45
> >>
> >>>ECR room: 01 44 27 47 12                      x-ray room: 
> >>
> >>01 44 27 63 00
> >>
> >>>home: 01 73 74 61 87      For int'l calls: 33 + number 
> >>
> >>without leading 0
> >>
> >>>
> >>>On Wed, 25 May 2005, Jacob van Wyk wrote:
> >>>
> >>>
> >>>>I hope somebody can help.
> >>>>A student of mine is doing a study on Measurement Error models 
> >>>>(errors-in-variables, total least squares, etc.). I have an old 
> >>>>reference to a "multi archive"  that contains
> >>>>leiv3: Programs for best line fitting with errors in both
> >>
> >>coordinates.
> >>
> >>>>(The date is October 1989, by B.D. Ripley et al.) I have done a 
> >>>>search for something similar in R withour success. Has this been 
> >>>>implemented in a R-package, possibly under some sort of
> >>
> >>assumptions
> >>
> >>>>about variances. I would lke my student to apply some regression 
> >>>>techniques to data that fit this profile.
> >>>>Any help is much appreciated.
> >>>>(If I have not done my search more carefully - my
> >>
> >>apologies.) Thanks
> >>
> >>>>Jacob
> >>>>
> >>>>
> >>>>Jacob L van Wyk
> >>>>Department of Mathematics and Statistics University of
> >>
> >>Johannesburg
> >>
> >>>>APK P O Box 524 Auckland Park 2006 South Africa
> >>>>Tel: +27-11-489-3080
> >>>>Fax: +27-11-489-2832
> >>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list 
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help 
> >>>>PLEASE do read the posting guide! 
> >>>>http://www.R-project.org/posting-guide.html 
> >>>>
> >>>
> >>>
> >>------------------------------------------------------------
> ----------
> >>
> >>>--
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list 
> >>>https://stat.ethz.ch/mailman/listinfo/r-help 
> >>>PLEASE do read the posting guide! 
> >>>http://www.R-project.org/posting-guide.html 
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help 
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html 
> > 
> >

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From JAROSLAW.W.TUSZYNSKI at saic.com  Tue May 31 19:00:01 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Tue, 31 May 2005 13:00:01 -0400
Subject: [R] xmlAttrs and problems with reading node attributes of XML
	file (b ug?)
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4084@us-arlington-0668.mail.saic.com>

Duncan,

Thanks, that that make sense, I have never encountered namespaces so I did
not recognized them. So in my example xmlns="a" xmlns:xsi="b" are namespace
variables, and xsi:schemaLocation="c" is a "Schema-Related Markup"
attribute. This last one seemed to be handled strangely, since it is listed
as attribute but its name got truncated from "xsi:schemaLocation" to
"schemaLocation".

Jarek



-----Original Message-----
From: Duncan Temple Lang [mailto:duncan at wald.ucdavis.edu] 
Sent: Tuesday, May 31, 2005 11:38 AM
To: Tuszynski, Jaroslaw W.
Cc: (r-help at stat.math.ethz.ch.); 'duncan at research.bell-labs.com'
Subject: Re: [R] xmlAttrs and problems with reading node attributes of XML
file (b ug?)


Hi Jarek.

Is the problem you are referring to simply that the xmlns="a" and
xmlns:xsi="b" are not appearing when your handler prints the attributes?  If
it is this the case, the explanation is quite simple - they are namespace
definitions, not attributes.
They are in the node object

    x$namespaceDefinitions


names(unclass(x)) will help to see what is in the node.

HTH,
  D.


Tuszynski, Jaroslaw W. wrote:
> Hi,
> 
> Consider the following code:
> 
> 	  require(XML)
> 	  xmlFile = paste( "<?xml version=\"1.0\"
> encoding=\"ISO-8859-1\"?>\n", 
> 	            "<mzXML xmlns=\"a\" xmlns:xsi=\"b\"
> xsi:schemaLocation=\"c\">\n",
> 	            "<parentFile a=\"a\" b=\"b\" />\n",
> 	            "</mzXML>\n")
> 	  cat(xmlFile)
> 	  
> 	  a = function(x,...){
> 	    cat("Attributes of ", xmlName(x), ": "); 
> 	    print(xmlAttrs(x));
> 	    cat("\n"); 
> 	    NULL
> 	  }
> 	  
> 	  xmlTreeParse(file=xmlFile, asText=TRUE,
> handlers=list("startElement"=a) )
> 
> And its output:
> 
> 	 <?xml version="1.0" encoding="ISO-8859-1"?>
> 	 <mzXML xmlns="a" xmlns:xsi="b" xsi:schemaLocation="c">
> 	 <parentFile a="a" b="b" />
> 	 </mzXML>
> 
> 	Attributes of  parentFile :   a   b 
> 	                                     "a" "b" 
> 
> 	Attributes of  mzXML : schemaLocation 
> 	                                  "c" 
> It seems to me that XML parser was able to correctly list all the 
> attributes of the "parentFile" node but it failed on "mzXML" node.
> Am I missusing the functions somehow or is it a bug?
> Attribute names in "mzXML" node are part of mzXML file format and can 
> not be changed (removing all ':' would fix the problem) and I would 
> like to store than so whan I write mzXML file I have the same header.
> 
> Jarek
> ====================================================\=======
> 
>  Jarek Tuszynski, PhD.                           o / \ 
>  Science Applications International Corporation  <\__,|  
>  (703) 676-4192                                   ">   \
>  Jaroslaw.W.Tuszynski at saic.com                     `    \
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Tue May 31 19:04:08 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 May 2005 13:04:08 -0400
Subject: [R] Add Columns and Order for Rbind?
In-Reply-To: <OF81B6A0B5.238B5180-ON86257012.0058EF5A-86257012.005AC726@fd9ns01.okladot.state.ok.us>
References: <OF81B6A0B5.238B5180-ON86257012.0058EF5A-86257012.005AC726@fd9ns01.okladot.state.ok.us>
Message-ID: <971536df05053110043bd520ba@mail.gmail.com>

On 5/31/05, khobson at fd9ns01.okladot.state.ok.us
<khobson at fd9ns01.okladot.state.ok.us> wrote:
> 
> 
> 
> 
> I am using rbind to add one list with one row to another master list.  The
> problem is that not all columns exist in both lists.
> 
> What methods would you recommend to reorder one list based on the column
> names of another?   If both sets have the same order and columns, I can
> then use rbind.  I can live with columns being added to the master list set
> it makes the task easier using something like union for column name
> matching and order.
> 

Suppose we have this test data:

# test data
irish <- head(iris)
one.row <- data.frame(Sepal.Length = 5, Sepal.Area = 10)

# Then we rbind an NA row to irish and fill it with one.row

irish <- rbind(irish, NA)
irish[nrow(irish),names(one.row)] <- one.row


# If you don't want columns added to the master list do this instead:

irish <- head(iris)  # recreate test data
irish  <- rbind(irish, NA)
both <- intersect(names(irish), names(one.row))
irish[nrow(irish), both] <- one.row[,both]

Note that appending rows one at a time can be slow.



From ggrothendieck at gmail.com  Tue May 31 19:25:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 May 2005 13:25:01 -0400
Subject: [R] Formatting numbers with a limited amount of digits
	consistently
In-Reply-To: <1117549806.22595.144.camel@horizons.localdomain>
References: <d7f8op$fa3$1@sea.gmane.org> <429B4B72.9000109@stats.uwo.ca>
	<971536df05053010571ad00617@mail.gmail.com>
	<429B65A5.3000503@stats.uwo.ca>
	<971536df05053020533592e59c@mail.gmail.com>
	<1117549806.22595.144.camel@horizons.localdomain>
Message-ID: <971536df0505311025530eecf8@mail.gmail.com>

On 5/31/05, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
> On Mon, 2005-05-30 at 23:53 -0400, Gabor Grothendieck wrote:
> > On 5/30/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > > Gabor Grothendieck wrote:
> > > > On 5/30/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > > >
> > > >>Henrik Andersson wrote:
> > > >>
> > > >>>I have tried to get signif, round and format to display numbers like
> > > >>>these consistently in a table, using e.g. signif(x,digits=3)
> > > >>>
> > > >>>17.01
> > > >>>18.15
> > > >>>
> > > >>>I want
> > > >>>
> > > >>>17.0
> > > >>>18.2
> > > >>>
> > > >>>Not
> > > >>>
> > > >>>17
> > > >>>18.2
> > > >>>
> > > >>>
> > > >>>Why is the last digit stripped off in the case when it is zero!
> > > >>
> > > >>signif() changes the value; you don't want that, you want to affect how
> > > >>a number is displayed.  Use format() or formatC() instead, for example
> > > >>
> > > >> > x <- c(17.01, 18.15)
> > > >> > format(x, digits=3)
> > > >>[1] "17.0" "18.1"
> > > >> > noquote(format(x, digits=3))
> > > >>[1] 17.0 18.1
> > > >>
> > > >
> > > >
> > > > That works in the above context but I don't think it works generally:
> > > >
> > > > R> f <- head(faithful)
> > > > R> f
> > > >   eruptions waiting
> > > > 1     3.600      79
> > > > 2     1.800      54
> > > > 3     3.333      74
> > > > 4     2.283      62
> > > > 5     4.533      85
> > > > 6     2.883      55
> > > >
> > > > R> format(f, digits = 3)
> > > >   eruptions waiting
> > > > 1      3.60      79
> > > > 2      1.80      54
> > > > 3      3.33      74
> > > > 4      2.28      62
> > > > 5      4.53      85
> > > > 6      2.88      55
> > > >
> > > > R> # this works in this case
> > > > R> noquote(prettyNum(round(f,1), nsmall = 1))
> > > >      eruptions waiting
> > > > [1,] 3.6       79.0
> > > > [2,] 1.8       54.0
> > > > [3,] 3.3       74.0
> > > > [4,] 2.3       62.0
> > > > [5,] 4.5       85.0
> > > > [6,] 2.9       55.0
> > > >
> > > > and even that does not work in the desired way (which presumably
> > > > is not to use exponent format) if you have some
> > > > large enough numbers like 1e6 which it will display using
> > > > the e notation rather than using ordinary notation.
> > >
> > > formatC with format="f" seems to work for me, though it assumes you're
> > > specifying decimal places rather than significant digits.  It also wants
> > > a vector of numbers as input, not a dataframe.  So the following gives
> > > pretty flexible control over what a table will look like:
> > >
> > >  > data.frame(eruptions = formatC(f$eruptions, digits=2, format='f'),
> > > +            waiting = formatC(f$waiting, digits=1, format='f'))
> > >    eruptions waiting
> > > 1 1000000.11    79.0
> > > 2       1.80    54.0
> > > 3       3.33    74.0
> > > 4       2.28    62.0
> > > 5       4.53    85.0
> > > 6       2.88    55.0
> > >
> > > >
> > > > I have struggled with this myself and have generally been able
> > > > to come up with something for specific instances but I have generally
> > > > found it a pain to do a simple thing like format a table exactly as I want
> > > > without undue effort.  Maybe someone else has figured this out.
> > >
> > > I think that formatting tables properly requires some thought, and R is
> > > no good at thinking.  You can easily recognize a badly formatted table,
> > > but it's very hard to write down rules that work in general
> > > circumstances.  It's also a matter of taste, so if I managed to write a
> > > function that matched my taste, you would find you wanted to make changes.
> > >
> > > It's sort of like expecting plot(x, y) to always come up with the best
> > > possible plot of y versus x.  It's just not a reasonable expectation.
> > > It's better to provide tools (like abline() for plots or formatC() for
> > > tables) that allow you to tailor a plot or table to your particular needs.
> > >
> >
> > Thanks.  That seems to be the idiom I was missing.  One thing that would
> > be nice would be if formatC could handle data frames.
> 
> 
> Guys, perhaps I am missing something here, but there seems to be some
> confusion as to how the numbers are stored internally, versus how the
> output is displayed and the meaning of "significant digits", which is
> what I believe Henrik's original query was about.
> 
> By default, R's printed output uses the settings from options("digits")
> and options("scipen") to define output based upon the number of
> significant digits, which is of course not the same as the number of
> decimal places. Hence the variance in the output that Henrik gets and
> why the trailing zero is dropped.
> 
> The use of signif() does not help here because it is still based upon
> the number of significant digits, where the trailing zero still gets
> dropped.
> 
> The use of the above are "inexact" when it comes to creating formatted
> output for a table with a consistent number of decimal places to align
> columns of numbers.
> 
> format() is still problematic here because it too uses the number of
> significant digits, defaulting to options("digits").

Good point.  It would be nice if format had an argument that allowed
one to specify the number of digits after the decimal place.  I think
this would reduce frustrations in quickly formatting data frames.



From chess.player at oninet.pt  Tue May 31 19:34:59 2005
From: chess.player at oninet.pt (jose silva)
Date: Tue, 31 May 2005 18:34:59 +0100
Subject: [R] help
Message-ID: <b7a5c128bb4c4db6afaab75c54d89580@oninet.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/2364f2b9/attachment.pl

From pohl at marge.statistik.uni-koeln.de  Tue May 31 19:40:26 2005
From: pohl at marge.statistik.uni-koeln.de (Stefan Pohl)
Date: Tue, 31 May 2005 19:40:26 +0200
Subject: [R] Shared Frailty in survival package (left truncation,
	time-dep. covariates)
Message-ID: <005301c56607$d77217b0$1b9e5f86@simurechner>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/a2e0ba1f/attachment.pl

From tobias.verbeke at telenet.be  Tue May 31 19:56:06 2005
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Tue, 31 May 2005 19:56:06 +0200
Subject: [R] is there material about Longitudinal Data Analysis with R?
In-Reply-To: <0IHD00BOZ2TW34@mail.fudan.edu.cn>
References: <0IHD00BOZ2TW34@mail.fudan.edu.cn>
Message-ID: <20050531195606.1fca71fc.tobias.verbeke@telenet.be>

On Wed, 01 Jun 2005 00:04:22 +0800
ronggui <0034058 at fudan.edu.cn> wrote:

> i am studying Longitudinal Data Analysis and want to carry it with R.anyone knows  any materials about Longitudinal Data Analysis with R in the internet which i can download?

You may have a look at

http://www.ats.ucla.edu/stat/examples/alda.htm

or

http://research.bus.wisc.edu/jfrees/Book/PDataBook.htm

HTH,
Tobias


> thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Tue May 31 20:07:59 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 31 May 2005 11:07:59 -0700 (PDT)
Subject: [R] Shared Frailty in survival package (left truncation, time-dep.
	covariates)
In-Reply-To: <005301c56607$d77217b0$1b9e5f86@simurechner>
References: <005301c56607$d77217b0$1b9e5f86@simurechner>
Message-ID: <Pine.A41.4.61b.0505311103500.190182@homer05.u.washington.edu>

On Tue, 31 May 2005, Stefan Pohl wrote:

> Dear list,
>
> I want o fit a shared gamma frailty model with the frailty specification 
> in the survival package.
>
> I have partly left-truncated data and time-dependent covariates. Is it 
> possible to combine these two things in the frailty function. Or are the 
> results wrong if I use data in the start-stop-formulation which account 
> for delayed entry?

It should work: as usual in the Cox model you get left-truncation and 
time-dependent covariates for free, since the partial likelihood only 
considers one risk set at a time.  You do need to make sure you specify 
the correct id variable, of course, and left truncation may reduce the 
amount of information available for estimating the frailty.


 	-thomas



From fezzi at stat.unibo.it  Tue May 31 20:38:35 2005
From: fezzi at stat.unibo.it (Carlo Fezzi)
Date: Tue, 31 May 2005 20:38:35 +0200 (CEST)
Subject: [R] prediction using gls with correlated residuals
Message-ID: <2582472.1117564715921.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Dear all,
I am a beginner user of R and I tried to fit a gls model with
explanatory variables and an AR(1) correlation component using the
function "gls" with:

correlation = corAR1 (form = ~ 1)

It should mean that the residual follows an AR(1) process, isn't it?

The problem is that, if I use the funcion "predict" I noticed that the
predicted values are the same as if I estimate the same model with the
function "lm" considering the residuals indipendent (this is true both
in and out of sample).

Does it means that "predict" doesn't use the information about the
residual correlation to improve predictions?

How can I overcome this problem?

Thank you so much for your help,

Carlo Fezzi



From murdoch at stats.uwo.ca  Tue May 31 21:10:58 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 31 May 2005 15:10:58 -0400
Subject: [R] FYI: Problems while loading package "class (VR)"
In-Reply-To: <429C7F34.8070404@statistik.uni-dortmund.de>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F4083@us-arlington-0668.mail.saic.com>
	<429C7F34.8070404@statistik.uni-dortmund.de>
Message-ID: <429CB6C2.5060604@stats.uwo.ca>

Uwe Ligges wrote:
>Tuszynski, Jaroslaw W. wrote:
>>My email was to inform that there are some problems with instaling package
>>"class (VR)" by itself. If I go to Menu
>>Packages/Install-package(s)/class(VR) I am still getting:
>>
>>
>>>utils:::menuInstallPkgs()
>>
>>  dependency 'class (VR)' is not available
> 
> 
> 
> I see, so you have used the menu in Windows ...
> This is a bug. I will followup on R-bugs or R-devel later this day.
> 
> BTW: install.packages("class") works for me as expected.

No need for a followup; I fixed the bug.

Duncan Murdoch



From murdoch at stats.uwo.ca  Tue May 31 21:14:10 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 31 May 2005 15:14:10 -0400
Subject: [R] A suggestion to improve ifelse behaviour with vector yes/no
	arguments
In-Reply-To: <Pine.A41.4.61b.0505310838360.190182@homer05.u.washington.edu>
References: <0BDE2460F08BF0429F933A40431A61E801E8244E@vk2kmail01.valtiokonttori.local>	<429C7FE0.6010302@stats.uwo.ca>
	<Pine.A41.4.61b.0505310838360.190182@homer05.u.washington.edu>
Message-ID: <429CB782.3050302@stats.uwo.ca>

Thomas Lumley wrote:
> On Tue, 31 May 2005, Duncan Murdoch wrote:
> 
> 
>>M??kinen Jussi wrote:
>>
>>>Dear All,
>>>
>>>I luckily found the following feature (or problem) when tried to apply 
>>>ifelse-function to an ordered data.
>>>
>>>
>>>
>>>>test <- c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE)
>>>>ifelse(test, 0, 1:4)
>>>
>>>[1] 0 0 0 4 1 2 3
>>>
> 
> <snippage>
> 
>>As Dimitris said, this is just recycling.  I think getting rid of recycling 
>>on vectors with length greater than 1 would have been a good decision in S 
>>about 15 years ago, but it's too late now.
> 
> 
> It wouldn't help the original poster, though.  I agree that 0,0,0,4,1,2,3 
> is a slightly weird result, but I can't think of any reasonable model for 
> the behaviour of ifelse() that would give any other result except an error 
> message. [or  0,NA,NA,4,NA,NA,NA, I suppose].

I would vote for the error message.  I can't think of a single example 
where a vector of length 7 is needed, and a vector of length 4 is 
recycled to give it, that *doesn't* give a slightly weird result.

Maybe this is something that should have been changed in R 2.0.0; we 
squandered that change from 1.x.x to 2.x.x.

Duncan Murdoch



From Scott.Waichler at pnl.gov  Tue May 31 21:35:25 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Tue, 31 May 2005 12:35:25 -0700
Subject: [R] RE: Soil texture triangle in R?
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A05F542@pnlmse35.pnl.gov>


Sander,

>has anybody made an attempt to create the soil texture triangle >graph
in R? 

See ternaryplot() in package vcd.  Its options are quite limited, but it
may help.  I think you'll need to write your own function to do a
ternary plot like the one you point to, perhaps using functions in the
grid package as your building blocks.

Scott Waichler, Senior Research Scientist
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From jatwood at montana.edu  Tue May 31 21:47:36 2005
From: jatwood at montana.edu (jatwood)
Date: Tue, 31 May 2005 13:47:36 -0600
Subject: [R] (no subject)
Message-ID: <5.2.0.9.0.20050531134444.01c8e098@gemini.msu.montana.edu>

Are there any R packages available that let me directly impose a set of 
linear parameter restrictions when estimating the simple linear and/or GLS 
regression model and directly recover the restricted coefficients, 
predicted values and residuals?  Can I do this via the use of contrasts 
under the model.matrix command?

Thanks
jatwood



From intuitionist at gmail.com  Tue May 31 22:07:11 2005
From: intuitionist at gmail.com (Aamir M)
Date: Tue, 31 May 2005 16:07:11 -0400
Subject: [R] "FANNY" function in R package "cluster"
Message-ID: <b93b8704050531130745173ea1@mail.gmail.com>

Martin> there is no 'm' in the book there, but they talk about the
Martin> exponent "^ 2" used in some places {but "^ 1" in other places},
Martin> notably in   5.2 "Why did we choose FANNY?" 

Martin> There is no "fuzziness" parameter defined there, so can you be
Martin> more specific?

Martin> Is it the exponent  2 in  u_{jv}^2 ?
Martin> That one is currently fixed at 2, and yes, that could be made a
Martin> parameter though K & R warn against going all the way to "1"
Martin> where their algorithm can happend to converge very slowly.

Yes, that is what I am referring to. If you refer to equation (1) in
section 4.1 of K&R (1990), where the FANNY objective function is
defined, you can see that the membership values are all raised to the
power two. In fact, the choice of raising them to the power 2 is
arbitrary. Rather, the value of this exponent should be a user
specified parameter. This  is called the "m parameter" or the
"fuzziness parameter" in Fuzzy k-Means.

Now that you mentioned it, I see that K&R did in fact comment on this
in section 5.2. K&R say that setting m=1 will cause slower
convergence; in Fuzzy k-Means, setting m=1 will cause a hard
clustering (minumum fuzziness), and setting m=infinity will cause
maximum fuzziness (i.e. all cluster membership values will be equal to
1/k). They go on to say that "exponents equal to 2 seem to be a
reasonable choice, as is confirmed by actual clustering analyses." I
do not know about FANNY, but in Fuzzy k-Means, studies have shown that
values of the exponents between 1 and 2 can lead to better results
than the rather arbitrary choice of m=2.

    Aamir> Is there, then, any way to compute the FANNY
    Aamir> clustering membership values of a test data point
    Aamir> without affecting the clustering membership values of
    Aamir> the training data? Perhaps there are enough
    Aamir> conditions to use the objective function as a way of
    Aamir> computing the membership values of the test data?

Martin> That's an interesting proposal, at least the way I choose to
understand you :-)

Martin> Yes, why not look at the objective function C {eq.(1), p.182}

Martin> One could think of optimizing it with respect to new data only,
Martin> by keeping all "old data" memberships.
Martin> For that to work, one would need the n dissimilarites 
Martin>     d[i', j]   where  i'  : `index for' new data 
Martin> 	       j = 1,..,n : indices for training data.
Martin> Is this feasible in your situation?

Yes, this would be feasible, I think. If I understand it correctly,
this would just involve recomputing the DAISY dissimilarity matrix on
the combined set of both training data and test data. It seems that
the resulting optimization problem would also be uniquely solvable.

Martin> Alternatively, when we *did* assume ``all continuous'' data 
Martin> *and* the use of simple Euclidean distances,
Martin> we could easily compute the cluster centers, determine (by
Martin> minimization!) memberships for new observations.

The problem of "predicting" fuzzy cluster memberships for new data
appears to be much simpler in Euclidean space; one could just compare
the new data to the cluster centers computed in Fuzzy k-Means.
Unfortunately, the data I'm working with is not all continuous.

Martin> In any case that needs some assumptions (and code!) currently
Martin> not part of fanny().

I'll have to work on this. Thought I'm guessing fanny() is written in
FORTRAN, which I cannot (yet) program in.

- Aamir



From lauraholt_983 at hotmail.com  Tue May 31 22:35:30 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 31 May 2005 15:35:30 -0500
Subject: [R] question regarding new class
Message-ID: <BAY10-F2182A2D1A11FFCF3CB63E9D6040@phx.gbl>

Hi R people:

I have created a new class for a project that I am working on.  It works 
fine.

Just one question, please:  When I access the slots, is there any way that I 
could use
x$Name instead on x at Name, please?  Or is it that way by design, please?

Thanks.

Laura Holt



From p.murrell at auckland.ac.nz  Tue May 31 22:41:44 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 01 Jun 2005 08:41:44 +1200
Subject: [R] persp, add lines/highlights
References: <1117486427.1669.235259524@webmail.messagingengine.com>
	<429BAC6A.1010306@stat.auckland.ac.nz>
	<ecebd24df865ae9a51fcb562fa82160d@soc.soton.ac.uk>
Message-ID: <429CCC08.6000403@stat.auckland.ac.nz>

Hi


Robin Hankin wrote:
> Hello everyone
> 
> I always cut-n-paste Paul's suggestions and end up learning something!
> 
> In this case, though, I have a query.
> 
> I was wondering how the second persp() call dealt with hidden line
> removal,  because pieces of mesh with NA values are see-through.
> 
> If you replace xi in the code below with 10:28
> and yi with 19:20, then all the pink grid
> appears in front of the blue grid; I would expect  the blue grid to hide 
> some of the pink
> grid.  Is there any way to enforce
> hidden line removal here?


persp() just draws the mesh from "back" to "front";  a sort of 
brute-force hidden-line removal.  R graphics has a simple "painters" 
model with later output overlaying previous output;  so you can only 
draw extra stuff on the "front" of a persp() plot.  If you want more 
sophisticated hidden-line removal you'll probably have to go to the rgl 
package (which does the real thing).

Paul


> On May 31, 2005, at 01:14 am, Paul Murrell wrote:
> 
>> Hi
>>
>>
>> jjorgensen at fastmail.fm wrote:
>>
>>> Hello R-sters,
>>> I'm trying to add several lines to a response surface that I've plotted
>>> using persp().  I've tried lines() using the "trans3d" function but I've
>>> been unsuccessful in getting it to work (R v2.0.1).  Essentially, I'm
>>> trying to highlight one or more of the surface wireframe lines in a
>>> bolder (or different) color.  Any tips from those of you who have some
>>> experience with this would be greatly appreciated.  [Would it be easier
>>> using wireframe() in library(lattice) instead?]
>>
>>
>>
>> Here's an example that just overlays two persp() plots ...
>>
>> x <- seq(-10, 10, length= 30)
>> y <- x
>> f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
>> z <- outer(x, y, f)
>> z[is.na(z)] <- 1
>> op <- par(bg = "white")
>> persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue",
>>       zlim=range(z))
>>
>> # overlay plot with just "highlighted" surface area
>> par(new=TRUE)
>> z2 <- matrix(NA, ncol=30, nrow=30)
>> xi <- 15:18
>> yi <- 13:14
>> z2[xi, yi] <- z[xi, yi]
>> persp(x, y, z2, theta = 30, phi = 30, expand = 0.5,
>>       zlim=range(z), border="red", col="pink", box=FALSE, axes=FALSE)
>>
>>
>>> And, any suggestions on how to add text outside of the persp() plot next
>>> to the highlighted line would be much appreciated.
>>
>>
>>
>> Do you mean like a legend?
>>
>> Paul
>> -- 
>> Dr Paul Murrell
>> Department of Statistics
>> The University of Auckland
>> Private Bag 92019
>> Auckland
>> New Zealand
>> 64 9 3737599 x85392
>> paul at stat.auckland.ac.nz
>> http://www.stat.auckland.ac.nz/~paul/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
> -- 
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From sms13+ at pitt.edu  Tue May 31 22:47:53 2005
From: sms13+ at pitt.edu (sms13+@pitt.edu)
Date: Tue, 31 May 2005 16:47:53 -0400
Subject: [R] simple predict question
Message-ID: <2954407796.1117558073@Lab26.DOMAIN.IE.PITT.EDU>

Excuse the simple question...
I'm not sure what I'm doing wrong with predict, but let me use this example:
Suppose I do:
dat<-matrix(c(0,0,10,20),2,byrow=T)
lm1<-lm(dat[,2]~dat[,1])

Suppose I want to generate the linearly-interpolated y-values between the 
point (0,0) and (0,20) at every unit interval.
I thought I just do:
predict(lm1, data.frame(seq(0,10,1))) to get 0,2,4,6...,18,20, but instead 
I just get:
1    2
0  20

Any suggestions?

Thanks,
Steven



From Manuel.A.Morales at williams.edu  Tue May 31 22:47:06 2005
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Tue, 31 May 2005 16:47:06 -0400
Subject: [R] Increasing Console "Paste Buffer"
Message-ID: <1117572426.11503.12.camel@localhost.localdomain>

Hello list.

I'm using R from the gnome-terminal in Fedora. My preference is to write
programs in VIM, and then source the file from R, or copy and paste the
lines into the console. I'm wondering if there is a way to increase the
"paste buffer" as an alternative to "sourcing" large analyses. As was
mentioned in a recent thread on Linux GUI's, I find that if I paste in a
large amount of text, the lines end up getting cut off at some point. I
wonder if this is an R restriction, because it seems like I am able to
paste substantially more text in other console-based programs. Is there
any way to increase the amount of text that I can paste into an R
session?

Thanks!

Manuel



From ggrothendieck at gmail.com  Tue May 31 23:00:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 May 2005 17:00:15 -0400
Subject: [R] simple predict question
In-Reply-To: <2954407796.1117558073@Lab26.DOMAIN.IE.PITT.EDU>
References: <2954407796.1117558073@Lab26.DOMAIN.IE.PITT.EDU>
Message-ID: <971536df050531140029ff1b41@mail.gmail.com>

On 5/31/05, sms13+ at pitt.edu <sms13+ at pitt.edu> wrote:
> Excuse the simple question...
> I'm not sure what I'm doing wrong with predict, but let me use this example:
> Suppose I do:
> dat<-matrix(c(0,0,10,20),2,byrow=T)
> lm1<-lm(dat[,2]~dat[,1])
> 
> Suppose I want to generate the linearly-interpolated y-values between the
> point (0,0) and (0,20) at every unit interval.
> I thought I just do:
> predict(lm1, data.frame(seq(0,10,1))) to get 0,2,4,6...,18,20, but instead
> I just get:
> 1    2
> 0  20
> 

I think the names are confusing it.  Try:

x <- dat[,1]; y <- dat[,2]
lm1 <- lm(y ~ x)
predict(lm1, data.frame(x = 1:10))



From PAlspach at hortresearch.co.nz  Tue May 31 23:16:38 2005
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Wed, 01 Jun 2005 09:16:38 +1200
Subject: [R] simple predict question
Message-ID: <s29d7d4a.082@hra2.marc.hort.cri.nz>


Alternatively, one can convert dat to a data frame first (?lm states that data is "an optional data frame containing the variables in the model."):

dat<-matrix(c(0,0,10,20),2,byrow=T)
dat <- as.data.frame(dat)
lm1<-lm(V2~V1, dat)
predict(lm1, data.frame(V1=seq(0,10,1)))
 1  2  3  4  5  6  7  8  9 10 11 
 0  2  4  6  8 10 12 14 16 18 20 

Cheers .........

Peter Alspach


>>> Gabor Grothendieck <ggrothendieck at gmail.com> 01/06/05 09:00:15 >>>
On 5/31/05, sms13+ at pitt.edu <sms13+ at pitt.edu> wrote:
> Excuse the simple question...
> I'm not sure what I'm doing wrong with predict, but let me use this example:
> Suppose I do:
> dat<-matrix(c(0,0,10,20),2,byrow=T)
> lm1<-lm(dat[,2]~dat[,1])
> 
> Suppose I want to generate the linearly-interpolated y-values between the
> point (0,0) and (0,20) at every unit interval.
> I thought I just do:
> predict(lm1, data.frame(seq(0,10,1))) to get 0,2,4,6...,18,20, but instead
> I just get:
> 1    2
> 0  20
> 

I think the names are confusing it.  Try:

x <- dat[,1]; y <- dat[,2]
lm1 <- lm(y ~ x)
predict(lm1, data.frame(x = 1:10))

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From graumann at caltech.edu  Tue May 31 23:31:34 2005
From: graumann at caltech.edu (Johannes Graumann)
Date: Tue, 31 May 2005 14:31:34 -0700
Subject: [R] subset-analogue removing fed in indexes?
Message-ID: <1117575094.3174.67.camel@localhost>

Hello,

Here's my issue:

I want to plot the following vectors:
> x <- c(0.0, 2.0, 15.0, 100.0, 105.0, 105.1, 110.0, 120.0, 120.1,
130.0)
> data <- c(8.75, 8.75, 16.25, 38.75, 61.25, 8.75, NA, 8.75, NA, NA)

and avoid the line discontinuations caused by 'NA'.
> plot_data <- na.omit(data)
will clean up 'data' for me, but now I need to get a 'plot_x' which
omits the values indexed with what's spit out by 'na.exclude(data)'.

Can anybody let me know a smooth way of how to delete entries with
certain indexes from a vector?

Thanks, Joih



From ggrothendieck at gmail.com  Tue May 31 23:38:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 May 2005 17:38:01 -0400
Subject: [R] subset-analogue removing fed in indexes?
In-Reply-To: <1117575094.3174.67.camel@localhost>
References: <1117575094.3174.67.camel@localhost>
Message-ID: <971536df05053114381997562e@mail.gmail.com>

On 5/31/05, Johannes Graumann <graumann at caltech.edu> wrote:
> Hello,
> 
> Here's my issue:
> 
> I want to plot the following vectors:
> > x <- c(0.0, 2.0, 15.0, 100.0, 105.0, 105.1, 110.0, 120.0, 120.1,
> 130.0)
> > data <- c(8.75, 8.75, 16.25, 38.75, 61.25, 8.75, NA, 8.75, NA, NA)
> 
> and avoid the line discontinuations caused by 'NA'.
> > plot_data <- na.omit(data)
> will clean up 'data' for me, but now I need to get a 'plot_x' which
> omits the values indexed with what's spit out by 'na.exclude(data)'.
> 
> Can anybody let me know a smooth way of how to delete entries with
> certain indexes from a vector?

plot(approx(x,data))



From schulerm at bc.edu  Tue May 31 23:39:08 2005
From: schulerm at bc.edu (Mike Schuler)
Date: Tue, 31 May 2005 17:39:08 -0400
Subject: [R] Loading matrices and other things
Message-ID: <429CD97C.2030502@bc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/7d1100f2/attachment.pl

