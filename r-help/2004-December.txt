From dhinds at sonic.net  Wed Dec  1 00:01:32 2004
From: dhinds at sonic.net (David Hinds)
Date: Tue, 30 Nov 2004 15:01:32 -0800
Subject: [R] New trellis settings
In-Reply-To: <200411301633.50366.deepayan@stat.wisc.edu>
References: <cogtiq$ovm$1@sea.gmane.org>
	<200411292323.36010.deepayan@stat.wisc.edu>
	<20041130193313.GA3743@sonic.net>
	<200411301633.50366.deepayan@stat.wisc.edu>
Message-ID: <20041130230132.GA7051@sonic.net>

On Tue, Nov 30, 2004 at 04:33:50PM -0600, Deepayan Sarkar wrote:
> >
> > I've fiddled with these and can pretty much get what I want.  But
> > without really understanding what I'm doing.  It isn't clear to me,
> > for instance, why the "key" settings affect my plot when no key is
> > drawn.
> 
> It shouldn't (unless you have messed with lattice.options()). Can you 
> provide an example? The following doesn't do anything unusual for me:
> 
> xyplot(1~1, 
>        par.settings = 
>        list(layout.heights = list(key.top = 100, key.bottom = 100)))
> 
> (I'm running r-devel though, so there's a small chance something might 
> be different.)

I'm only talking about the *.padding options, like:

  xyplot(1:4~1:4,
         par.settings=list(layout.heights=list(key.axis.padding=10)))

-- Dave



From tuechler at gmx.at  Wed Dec  1 00:06:35 2004
From: tuechler at gmx.at (Heinz Tuechler)
Date: Wed, 01 Dec 2004 00:06:35 +0100
Subject: [R] Attn Heinz Tuechler: Re: problem with sp ecial characters (
	=?iso-8859-1?Q?=E4?=  =?iso-8859-1?Q?,=F6,=FC)?=
In-Reply-To: <l64pq01252g98aki53brfihdfipa96drg5@4ax.com>
References: <3.0.6.32.20041127233123.00794d20@pop.gmx.net>
	<3.0.6.32.20041127233123.00794d20@pop.gmx.net>
Message-ID: <3.0.6.32.20041201000635.0079bae0@pop.gmx.net>


At 10:27 30.11.2004 -0500, Duncan Murdoch wrote:
>[I tried to send this message privately, but the return address
>bounced.]
>
>I think this has been fixed in R-patched, but I doubt if the fix has
>been tested in Win98.  Could you please download a copy from
><http://cran.r-project.org/bin/windows/base/rpatched.html> and confirm
>that it has been fixed?
>
>Duncan Murdoch
>
Now I installed the version 2.0.1 Patched (2004-11-27) and it works well.
The bug, which is, as I have learned, not a bug of R but of Win98, does not
appeare any more. Many thanks to those who helped me to cope with one bug
of windows.

Heinz T??chler

>On Sat, 27 Nov 2004 23:31:23 +0100, Heinz Tuechler <tuechler at gmx.at>
>wrote :
>
>>Dear Developers!
>>
>>Using special characters I found a strange behaviour in R 2.0.1 and equally
>>in 
>>R : Copyright 2004, The R Foundation for Statistical Computing
>>Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
>>
>>Operating System: Windows 98SE
>>
>>example:
>>factor1<-as.factor(c("weiblich","m??nnlich","??sterreichisch","fr??hreif","Gru??
>>"))
>>factor1
>>> factor1
>>[1] weiblich           m\344nnlich        \366sterreichisch  fr\374hreif
>>   
>>[5] Gru\337           
>>Levels: fr??hreif Gru?? m??nnlich ??sterreichisch weiblich
>>
>>with best wishes
>>
>>Heinz T??chler
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From p.dalgaard at biostat.ku.dk  Wed Dec  1 00:05:03 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Dec 2004 00:05:03 +0100
Subject: [R] Combined variable names
In-Reply-To: <41ACF742.1020408@lse.ac.uk>
References: <41ACF742.1020408@lse.ac.uk>
Message-ID: <x2oehf0wgw.fsf@biostat.ku.dk>

Tobias Muhlhofer <t.muhlhofer at lse.ac.uk> writes:

> I am trying to define a large number of variables through a loop construct.
> 
> I have my loop variable i being cycled through 1:100 and I would like
> the variables produced by this to be called
> 
> vi (i.e. v1 v2 v3 etc)
> 
> so, for example I'm going:
> 
> for(i in 1:100) {
> 
> <blank> <- a[i:N] # or whatever else you want to put on the right side
> }
> 
> where N is previously defined.
> 
> What goes in for <blank>?

...
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
...

Did you? Not to put too fine a point on it, but this particular
question spurred a lengthy thread over the weekend about how polite
one should be (or not) to people who obviously haven't read the FAQ...

This is Question 7.21, to be precise.

Short answer: use assign(paste(...)) if you must, but you're usually
better off constructing a list, for instance like this:

> a <- 1:5 ; l <- lapply(1:5,function(i) a[i:5])
> l
[[1]]
[1] 1 2 3 4 5

[[2]]
[1] 2 3 4 5

[[3]]
[1] 3 4 5

[[4]]
[1] 4 5

[[5]]
[1] 5



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From deepayan at stat.wisc.edu  Wed Dec  1 00:30:14 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 30 Nov 2004 17:30:14 -0600
Subject: [R] New trellis settings
In-Reply-To: <20041130230132.GA7051@sonic.net>
References: <cogtiq$ovm$1@sea.gmane.org>
	<200411301633.50366.deepayan@stat.wisc.edu>
	<20041130230132.GA7051@sonic.net>
Message-ID: <200411301730.14964.deepayan@stat.wisc.edu>

On Tuesday 30 November 2004 17:01, David Hinds wrote:
> On Tue, Nov 30, 2004 at 04:33:50PM -0600, Deepayan Sarkar wrote:
> > > I've fiddled with these and can pretty much get what I want.  But
> > > without really understanding what I'm doing.  It isn't clear to
> > > me, for instance, why the "key" settings affect my plot when no
> > > key is drawn.
> >
> > It shouldn't (unless you have messed with lattice.options()). Can
> > you provide an example? The following doesn't do anything unusual
> > for me:
> >
> > xyplot(1~1,
> >        par.settings =
> >        list(layout.heights = list(key.top = 100, key.bottom =
> > 100)))
> >
> > (I'm running r-devel though, so there's a small chance something
> > might be different.)
>
> I'm only talking about the *.padding options, like:
>
>   xyplot(1:4~1:4,
>          par.settings=list(layout.heights=list(key.axis.padding=10)))

Ah. Well, the layout always contains a row for the key (with height 0), 
even if the key is not actually there (the same holds for all the other 
components). Not having it would mean a lot of headache for me (as the 
programmer) since I would have to keep track of a lot of information to 
know which row in the layout corresponds to what. 

So, for instance, the space above the plot area (assuming no key, no 
main) would be [the space at the top] + [space between the 0-height 
main and the 0-height key] + [space between the key and the top axes].

Deepayan



From andy_liaw at merck.com  Wed Dec  1 01:11:27 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 30 Nov 2004 19:11:27 -0500
Subject: [R] predict.gam problem
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3A5@usrymx25.merck.com>

I thought the documentation you quoted is rather clear.  The GAM is
something like:

g(Y) = sum f_j(x_j)

so predict.gam() returns sum f_j(x_j) by default, and if type="response",
g^-1(sum f_j(x_j)).  This is similar to predict.glm, I believe.

HTH,
Andy

> From: Jon Egil Strand
> 
> 
> -----------------
> Mac OS X 10.3
> R 2.0.1
> gam 0.9.2
> -----------------
> 
> Greetings
> 
> I am facing difficulties in prediction on using Trevor 
> Hasties GAM package. 
> 
> How can one interpret the response from predict.gam?
> 
> Documentation on the type-parameter:
>   The default (link) produces predictions on the scale of the 
> additive 
>   predictors, ... 
> 
>   If "response" is selected, the predictions are on the scale of the
>   response, and are monotone trans-formations of the additive 
> predictors,
>   using the inverse link function.
> 
> Does "on the scale of the response" ammount to residals? 
> 
> All I want to do is get the predicted response from running 
> my GAM model
> on new data. The results given are way off, but I am not able 
> to dechipher 
> the documentation, nor use any scale information.
> 
> 
> Any help will be highly appreciated.
> 
> 
> All the best
> 
> Jon Egil Strand
> 
> 
> 
> 
> -- 
> 
> Jon Egil Strand
> jes at luretanker.no
> Phone: +47 45030081
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ok at cs.otago.ac.nz  Wed Dec  1 02:37:27 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 1 Dec 2004 14:37:27 +1300 (NZDT)
Subject: [R] Creating a factor from a combination of vectors
Message-ID: <200412010137.iB11bRLI357468@atlas.otago.ac.nz>

Yves Brostaux <brostaux.y at fsagx.ac.be> wrote:
	I want to produce a factor from a subset of the combination of two 
	vectors. I have the vectors a et b in a data-frame :
	
	 > df <- expand.grid(a=c(0, 5, 10, 25, 50), b=c(0, 25, 50, 100, 200))
	...
	and want to create a factor which levels correspond to particular 
	combinations of a and b (let's say Low for a=0 & b=0, Medium for a=10 & 
	b=50, High for a=50 & b=200, others levels set to NA), reading them from 
	a data-frame which describes the desired subset and corresponding levels.
	
	Here's my own solution (inputs are data-frames df and cas, output is the 

Why not do it the obvious way?

	ifelse(a == 0 & b == 0, "Low",
	  ifelse(a == 10 & b == 50, "Medium",
	    ifelse(a == 50 & b == 200, "High", 
	      "Other")))

gives you the mapping from vectors a and b to strings you want.
To get at the vectors locally, you need

	with(df, ...)

To convert the vector of strings you get to an ordered factor,
with "Other" mapped to NA, just do

	ordered(..., levels = c("Low","Medium","High"))

because any string not listed in levels= will be mapped to NA.
Put these pieces together, and you get

    output <- ordered(with(df,
		ifelse(a == 0 & b == 0, "Low",
		  ifelse(a == 10 & b == 50, "Medium",
		    ifelse(a == 50 & b == 200, "High",
		      "Other")))),
		levels = c("Low","Medium","High"))



From ksm32 at student.canterbury.ac.nz  Wed Dec  1 04:23:36 2004
From: ksm32 at student.canterbury.ac.nz (Karla Meurk)
Date: Wed, 01 Dec 2004 16:23:36 +1300
Subject: [R] time data
Message-ID: <41AD3938.7040403@student.canterbury.ac.nz>

I have a data set with date, time and rainfall (sample below)

my questions are (a) is there a command like as.date for time? and (b) 
Can R smooth data 6 hourly with such unevenly spaced data?

Thanks

Carla

09/29/02	 19:33	0
09/29/02	 19:34	0
09/29/02	 19:35	0
09/29/02	 19:36	0.1
09/29/02	 19:37	0
09/29/02	 19:38	0
09/29/02	 19:39	0
09/29/02	 19:40	0.1
09/29/02	 19:41	0
09/29/02	 19:42	0
09/30/02	 00:00	0
09/30/02	 01:50	0.1
09/30/02	 02:20	0.1
09/30/02	 02:40	0.2
09/30/02	 02:50	0.4
09/30/02	 05:00	0.7



From ok at cs.otago.ac.nz  Wed Dec  1 04:25:36 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 1 Dec 2004 16:25:36 +1300 (NZDT)
Subject: [R] Combined variable names
Message-ID: <200412010325.iB13Pa6X361377@atlas.otago.ac.nz>

Tobias Muhlhofer <t.muhlhofer at lse.ac.uk> wrote:
	I am trying to define a large number of variables through a loop
	construct.
	
He wants to do
	for (i in 1:100) {
	    assign(paste("v", i, sep=""), ....something or other...)
	}

This is, of course, a FAQ.  It's such a FAQ that I must have seen it
once a day for the last several days.

What I want to know is *WHY* people are doing this?
What, precisely, does it buy you to have variables called v1...v100
rather than variables called v[[1]]...v[[100]]?
Apart from persistent inconvenience, that is?

What, really, is wrong with

    v <- lapply(1:100, function (i) ....something or other...)



From m.rahgozar at uswr.ac.ir  Wed Dec  1 05:54:13 2004
From: m.rahgozar at uswr.ac.ir (rahgozar)
Date: Wed, 01 Dec 2004 08:24:13 +0330
Subject: [R] Request for Gamma Frailty Simulation
Message-ID: <200412010458.iB14wLSi004851@hypatia.math.ethz.ch>


   Dear Sir/Madam,

  I need to use " Gamma Frailty Model " in a simulation study ,so I 
should replicate the program many times and get estimates of 
" THETA and its Variance " or "Variance-Covariance" matrix Or 
Variace of random effect in Gamma frailty model in Survival analysis
at each time of replication.
 But when I use " $frail " and " $Var " and "$THETA " to retrieve 
the estimate of THETA and its Variance for each replicate , I get NULL
response and when I use " $coef " I retrieve the coeficients only but 
not THETA and its Variance.
 Since I need to get THETA and its Variance at each time of replication 
so I need to know the name of these variables as used in the source 
program . 
  
 I would be very grateful if you could kindly guide and help me to use
appropriate statment to get the name of required varibles used in the 
source program or if you could introduce me to anybody that could help 
me .    

  Thanks a lot in advance .
 
 With The Best Regards ,

  Mehdi Rahgozar



From adslvdll at tpg.com.au  Wed Dec  1 06:53:38 2004
From: adslvdll at tpg.com.au (stephenc)
Date: Wed, 1 Dec 2004 16:53:38 +1100
Subject: [R] tuning SVM's
Message-ID: <000001c4d76a$1f3a82d0$0d01a8c0@tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041201/b89788df/attachment.pl

From chancs at gis.a-star.edu.sg  Wed Dec  1 06:59:09 2004
From: chancs at gis.a-star.edu.sg (CHAN Chee Seng)
Date: Wed, 1 Dec 2004 13:59:09 +0800
Subject: [R] core dump during make check when building 64-bit R on
	Solaris8/9
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E0@BIONIC.biopolis.one-north.com>

Hi,

After reading some of the posting in this list, I came across this
posting from:

From: Peter Dalgaard <p.dalgaard_at_biostat.ku.dk>
Date: Fri 29 Oct 2004 - 08:02:40 EST
Replying to Re: [R] Errors during make check

He described a problem similar to mine, that build 64-bit R (I am
building R version 2.0.1) with sunperf library gives a core dump during
make check.  So I configured my build without the sunperf (BLAS and
lapack) library.  My configure statement looks like this:

./configure --prefix=/usr/local/R-2.0.1
--with-tcl-config=/usr/sfw/lib/tclCo
nfig.sh --with-tk-config=/usr/sfw/lib/tkConfig.sh --without-blas

The "make check" ran without a problem after this.

I hope that the sun compiler people can look into this and see if there
is a problem with the sunperf library.  Maybe Peter Dalgaard ha further
insights into this?  Thanks.

Regards,
Chee Seng


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of CHAN Chee Seng
Sent: Monday, November 29, 2004 6:13 PM
To: r-help at stat.math.ethz.ch
Subject: [R] core dump during make check when building 64-bit R on
Solaris8/9

Hi,

I am building a 64-bit R 2.0.1 on Solaris 9.  The compiler is Sun Studio
8.  Make was successful but I have a core dump during a make check.  By
the way, this problem also happens on my Solaris 8 machine though I did
not get a core dump.  I do not have 64-bit versions of the readline,
tcl/tk, libncurses, etc libraries so these caused configure not to use
them with ELFCLASS32 errors.

I use the following flags in config.site:
CC="cc -xarch=v9"
CFLAGS="-xO5 -xlibmil -dalign"
F77="f95 -xarch=v9"
FFLAGS="-xO5 -xlibmil -dalign"
CXX="CC -xarch=v9"


I pasted the make check error messages below:
$ make check
collecting examples for package 'base' ...
 >>> Building/Updating help pages for package 'base'
     Formats: text html latex example 
running code in 'base-Ex.R' ...*** Error code 1
make: Fatal error: Command failed for target `base-Ex.Rout'
Current working directory /export/home/cheeseng/R-2.0.1/tests/Examples
*** Error code 1
make: Fatal error: Command failed for target `test-Examples-Base'
Current working directory /export/home/cheeseng/R-2.0.1/tests/Examples
*** Error code 1
make: Fatal error: Command failed for target `test-Examples'
Current working directory /export/home/cheeseng/R-2.0.1/tests
*** Error code 1
make: Fatal error: Command failed for target `test-all-basics'
Current working directory /export/home/cheeseng/R-2.0.1/tests
*** Error code 1
make: Fatal error: Command failed for target `check'


The last few lines in base-Ex.Rout.fail is:
> ### * kappa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: kappa
> ### Title: Estimate the Condition Number
> ### Aliases: kappa kappa.default kappa.lm kappa.qr kappa.tri
> ### Keywords: math
> 
> ### ** Examples
> 
> kappa(x1 <- cbind(1,1:10))# 15.71
[1] 15.70590


Doing a dbx on the core dump shows that the following:
program terminated by signal SEGV (no mapping at the fault address)
0xffffffff790fdf54: ___pl_dgesdd_64_+0x1654:    std      %f4, [%o1]

I hope some one have a more successful build and can show how you did
it.

Thanks,
Chee Seng
UNIX Administrator
Genome Institute of Singapore
60 Biopolis Street, Genome
#02-01 Singapore 138672
DID 64788065
------------------------------- 
This email is confidential and may be privileged.  If you are not the
intended recipient, please delete it and notify us immediately. Please
do not copy or use it for any purpose, or disclose its contents to any
other person. Thank you.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jemwa at sun.ac.za  Wed Dec  1 07:05:38 2004
From: jemwa at sun.ac.za (Gorden Jemwa)
Date: Wed, 01 Dec 2004 08:05:38 +0200
Subject: [R]  Kernel Fisher Discriminant in R?
Message-ID: <41AD5F32.6030703@sun.ac.za>

You could us the kernlab package as a building block. Implementation of 
Kernel Fisher Discriminant Analysis (or Generalized Fisher Discriminant 
Analysis) formulation are fairly straightforward and should be difficult.

Rgds,

Gorden

> Date: Mon, 29 Nov 2004 16:57:28 -0500
> From: "Huh, Seungho" <huh at rti.org>
> Subject: [R] Kernel Fisher Discriminant in R?
> To: <r-help at stat.math.ethz.ch>
> Message-ID:
> 	<54535CBB16ABDA469D72C7614960429B097405 at rtpwexc04.RCC_NT.RTI.ORG>
> Content-Type: text/plain
> 
> Dear members,
> 
>  
> 
> I am wondering if there are any functions to perform the Kernel Fisher
> Discriminant method, especially for multi-class problems, in R. I would
> appreciate any kind of information on this.
> 
>  
> 
> Thanks for your time.
> 
>  
> 
> Seungho Huh, Ph.D.



From jemwa at sun.ac.za  Wed Dec  1 07:07:09 2004
From: jemwa at sun.ac.za (Gorden Jemwa)
Date: Wed, 01 Dec 2004 08:07:09 +0200
Subject: [Fwd: Re: [R]  Kernel Fisher Discriminant in R?]
Message-ID: <41AD5F8D.1080609@sun.ac.za>


Ooops...I meant "formulations shouldn't be difficult"


-------- Original Message --------
Subject: Re: [R]  Kernel Fisher Discriminant in R?
Date: Wed, 01 Dec 2004 08:05:38 +0200
From: Gorden Jemwa <jemwa at sun.ac.za>
To: huh at rti.org
CC: r-help at stat.math.ethz.ch

You could us the kernlab package as a building block. Implementation of
Kernel Fisher Discriminant Analysis (or Generalized Fisher Discriminant
Analysis) formulation are fairly straightforward and should be difficult.

Rgds,

Gorden

> Date: Mon, 29 Nov 2004 16:57:28 -0500
> From: "Huh, Seungho" <huh at rti.org>
> Subject: [R] Kernel Fisher Discriminant in R?
> To: <r-help at stat.math.ethz.ch>
> Message-ID:
> 	<54535CBB16ABDA469D72C7614960429B097405 at rtpwexc04.RCC_NT.RTI.ORG>
> Content-Type: text/plain
> 
> Dear members,
> 
>  
> 
> I am wondering if there are any functions to perform the Kernel Fisher
> Discriminant method, especially for multi-class problems, in R. I would
> appreciate any kind of information on this.
> 
>  
> 
> Thanks for your time.
> 
>  
> 
> Seungho Huh, Ph.D.




-- 
I'd rather be certain of a good result than hopeful of a great one - 
Warren Buffett



From ligges at statistik.uni-dortmund.de  Wed Dec  1 08:37:20 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 01 Dec 2004 08:37:20 +0100
Subject: [R] Relative subscripting
In-Reply-To: <XFMail.041130213402.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041130213402.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41AD74B0.2050504@statistik.uni-dortmund.de>

(Ted Harding) wrote:
> On 30-Nov-04 Tobias Muhlhofer wrote:
> 
>>[...]
>>I have monthly a dataset with, among other things, "marketcap",
>>and "return".
>>
>>I want to multiply return by the marketcap of the previous month.
>>How do I do this?
>>
>>In STATA (which I have used frequently in the past) I would simply
>>use an expression of the form
>>
>>return[_n]*marketcap[_n-1]
>>
>>I believe they call this relative subscripting. Is there something
>>like this in R?
> 
> 
> Not as such, as far as I know. But there's an easy way to achieve
> the same effect:
> 
> Let
> 
>   N<-length(return)
>   new.var <- return[2:N]*marketcap[1:(N-1)]

Ted, I aggree to all of your points, but we can simplify by negative 
indices (and hence circumvent your note 1):
  return[-1] * marketcap[-N]

Uwe Ligges



> Notes:
> 1. Note the parantheses in "1:(N-1)".
>      > 1:(6-1)
>      [1] 1 2 3 4 5
>      > 1:6-1
>      [1] 0 1 2 3 4 5
>    (i.e. ":" is evaluated before "-")
> 
> 2. You could also define n <- 2:(N-1) in which case you could
>    definitely write
> 
>      return[n]*marketcap[n-1]
> 
>    which looks very similar to what you wrote, but I don't
>    know whether it implies the same underlying mechanism.
> 
> 
> 3. I'm not sure you should use "return" as the name of a variable,
>    since it's a predefined function in R, which you can put in a
>    function definition to tell it what to return:
>      > sq<-function(x){return(x^2)}
>      > sq(4)
>      [1] 16
> 
>    In my experiments, it didn't seem to change this behaviour
>    to assign something to "return", e.g. return<-2, even inside
>    the function definition; but I'd recommend avoiding the practice!
> 
>    You could avoid it here by using "Return" instead of "return"
>    for the name of your variable.
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 30-Nov-04                                       Time: 21:34:02
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Dec  1 08:42:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 1 Dec 2004 07:42:27 +0000 (GMT)
Subject: [R] core dump during make check when building 64-bit R on
	Solaris8/9
In-Reply-To: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E0@BIONIC.biopolis.one-north.com>
References: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E0@BIONIC.biopolis.one-north.com>
Message-ID: <Pine.LNX.4.61.0412010739240.19824@gannet.stats>

We know of at least two builds which successfully used libsunperf, one 
(mine) with Studio ONE 7 and one with 9.  So this may well be a problem 
with your particular compiler suite.

On Wed, 1 Dec 2004, CHAN Chee Seng wrote:

> Hi,
>
> After reading some of the posting in this list, I came across this
> posting from:
>
> From: Peter Dalgaard <p.dalgaard_at_biostat.ku.dk>
> Date: Fri 29 Oct 2004 - 08:02:40 EST
> Replying to Re: [R] Errors during make check
>
> He described a problem similar to mine, that build 64-bit R (I am
> building R version 2.0.1) with sunperf library gives a core dump during
> make check.  So I configured my build without the sunperf (BLAS and
> lapack) library.  My configure statement looks like this:
>
> ./configure --prefix=/usr/local/R-2.0.1
> --with-tcl-config=/usr/sfw/lib/tclCo
> nfig.sh --with-tk-config=/usr/sfw/lib/tkConfig.sh --without-blas
>
> The "make check" ran without a problem after this.
>
> I hope that the sun compiler people can look into this and see if there
> is a problem with the sunperf library.  Maybe Peter Dalgaard ha further
> insights into this?  Thanks.
>
> Regards,
> Chee Seng
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of CHAN Chee Seng
> Sent: Monday, November 29, 2004 6:13 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] core dump during make check when building 64-bit R on
> Solaris8/9
>
> Hi,
>
> I am building a 64-bit R 2.0.1 on Solaris 9.  The compiler is Sun Studio
> 8.  Make was successful but I have a core dump during a make check.  By
> the way, this problem also happens on my Solaris 8 machine though I did
> not get a core dump.  I do not have 64-bit versions of the readline,
> tcl/tk, libncurses, etc libraries so these caused configure not to use
> them with ELFCLASS32 errors.
>
> I use the following flags in config.site:
> CC="cc -xarch=v9"
> CFLAGS="-xO5 -xlibmil -dalign"
> F77="f95 -xarch=v9"
> FFLAGS="-xO5 -xlibmil -dalign"
> CXX="CC -xarch=v9"
>
>
> I pasted the make check error messages below:
> $ make check
> collecting examples for package 'base' ...
> >>> Building/Updating help pages for package 'base'
>     Formats: text html latex example
> running code in 'base-Ex.R' ...*** Error code 1
> make: Fatal error: Command failed for target `base-Ex.Rout'
> Current working directory /export/home/cheeseng/R-2.0.1/tests/Examples
> *** Error code 1
> make: Fatal error: Command failed for target `test-Examples-Base'
> Current working directory /export/home/cheeseng/R-2.0.1/tests/Examples
> *** Error code 1
> make: Fatal error: Command failed for target `test-Examples'
> Current working directory /export/home/cheeseng/R-2.0.1/tests
> *** Error code 1
> make: Fatal error: Command failed for target `test-all-basics'
> Current working directory /export/home/cheeseng/R-2.0.1/tests
> *** Error code 1
> make: Fatal error: Command failed for target `check'
>
>
> The last few lines in base-Ex.Rout.fail is:
>> ### * kappa
>>
>> flush(stderr()); flush(stdout())
>>
>> ### Name: kappa
>> ### Title: Estimate the Condition Number
>> ### Aliases: kappa kappa.default kappa.lm kappa.qr kappa.tri
>> ### Keywords: math
>>
>> ### ** Examples
>>
>> kappa(x1 <- cbind(1,1:10))# 15.71
> [1] 15.70590
>
>
> Doing a dbx on the core dump shows that the following:
> program terminated by signal SEGV (no mapping at the fault address)
> 0xffffffff790fdf54: ___pl_dgesdd_64_+0x1654:    std      %f4, [%o1]
>
> I hope some one have a more successful build and can show how you did
> it.
>
> Thanks,
> Chee Seng
> UNIX Administrator
> Genome Institute of Singapore
> 60 Biopolis Street, Genome
> #02-01 Singapore 138672
> DID 64788065
> -------------------------------
> This email is confidential and may be privileged.  If you are not the
> intended recipient, please delete it and notify us immediately. Please
> do not copy or use it for any purpose, or disclose its contents to any
> other person. Thank you.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From brostaux.y at fsagx.ac.be  Wed Dec  1 09:11:57 2004
From: brostaux.y at fsagx.ac.be (Yves Brostaux)
Date: Wed, 01 Dec 2004 09:11:57 +0100
Subject: [R] Creating a factor from a combination of vectors
In-Reply-To: <200412010137.iB11bRLI357468@atlas.otago.ac.nz>
References: <200412010137.iB11bRLI357468@atlas.otago.ac.nz>
Message-ID: <41AD7CCD.3050004@fsagx.ac.be>

Thank you Richard, but I dismissed the 'ifelse' solution because it 
needs explicit manual definition of the factor levels and corresponding 
vectors' combinations and does not define it automaticaly  from the 
'cas' data-frame (from which values, number of levels and rownames can 
vary).

Eric Lecoutre's code does exactly what I want, many thanks to both of you.

Richard A. O'Keefe a ??crit :

>Yves Brostaux <brostaux.y at fsagx.ac.be> wrote:
>	I want to produce a factor from a subset of the combination of two 
>	vectors. I have the vectors a et b in a data-frame :
>	
>	 > df <- expand.grid(a=c(0, 5, 10, 25, 50), b=c(0, 25, 50, 100, 200))
>	...
>	and want to create a factor which levels correspond to particular 
>	combinations of a and b (let's say Low for a=0 & b=0, Medium for a=10 & 
>	b=50, High for a=50 & b=200, others levels set to NA), reading them from 
>	a data-frame which describes the desired subset and corresponding levels.
>	
>	Here's my own solution (inputs are data-frames df and cas, output is the 
>
>Why not do it the obvious way?
>
>	ifelse(a == 0 & b == 0, "Low",
>	  ifelse(a == 10 & b == 50, "Medium",
>	    ifelse(a == 50 & b == 200, "High", 
>	      "Other")))
>
>gives you the mapping from vectors a and b to strings you want.
>To get at the vectors locally, you need
>
>	with(df, ...)
>
>To convert the vector of strings you get to an ordered factor,
>with "Other" mapped to NA, just do
>
>	ordered(..., levels = c("Low","Medium","High"))
>
>because any string not listed in levels= will be mapped to NA.
>Put these pieces together, and you get
>
>    output <- ordered(with(df,
>		ifelse(a == 0 & b == 0, "Low",
>		  ifelse(a == 10 & b == 50, "Medium",
>		    ifelse(a == 50 & b == 200, "High",
>		      "Other")))),
>		levels = c("Low","Medium","High"))
>
>
>
>  
>

-- 
Ir. Yves BROSTAUX
Unit?? de Statistique et Informatique
Facult?? universitaire des Sciences agronomiques de Gembloux (FUSAGx)
8, avenue de la Facult??
B-5030 Gembloux
Belgique
T??l: +32 81 62 24 69
Email: brostaux.y at fsagx.ac.be



From Ted.Harding at nessie.mcc.ac.uk  Wed Dec  1 09:00:22 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 01 Dec 2004 08:00:22 -0000 (GMT)
Subject: [R] Relative subscripting
In-Reply-To: <41AD74B0.2050504@statistik.uni-dortmund.de>
Message-ID: <XFMail.041201080022.Ted.Harding@nessie.mcc.ac.uk>

On 01-Dec-04 Uwe Ligges wrote:
> (Ted Harding) wrote:
>> [...]
>> Let
>> 
>>   N<-length(return)
>>   new.var <- return[2:N]*marketcap[1:(N-1)]
> 
> Ted, I aggree to all of your points, but we can simplify by negative 
> indices (and hence circumvent your note 1):
>   return[-1] * marketcap[-N]
> 
> Uwe Ligges

Neat footwork, Uwe!
(Why didn;t I think of that? No, don't answer ... )
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 01-Dec-04                                       Time: 08:00:22
------------------------------ XFMail ------------------------------



From hack at ffos.hr  Wed Dec  1 09:34:44 2004
From: hack at ffos.hr (Branimir K. Hackenberger)
Date: Wed, 1 Dec 2004 09:34:44 +0100
Subject: [R] spearman correction
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA/KN/oP+2mk+FvuiZg1iX78KAAAAQAAAAjlGUYmKRh0CkPNVQtEO12QEAAAAA@ffos.hr>

Dear All !

Who knows which correction (by tied numbers) is used by Spearman-test in
function cor.test (e.g. cor.test(a,b,method="spearman"))?

a	b
12	135
13	153
15	144
17	180
18	162
19	198
20	234
21	225
25	270
26	261

rs(corrected)=rs(uncorrected)= 0.951515152

a	b
12	135
13	153
15	144
17	180
18	180
19	180
20	180
21	225
21	270
21	261

rs(corrected)= 0.943208012
rs(uncorrected)= 0.945454545


Thanks!

With best regards

Branimir K. Hackenberger



From ripley at stats.ox.ac.uk  Wed Dec  1 09:43:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 1 Dec 2004 08:43:22 +0000 (GMT)
Subject: [R] Relative subscripting
In-Reply-To: <XFMail.041201080022.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041201080022.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0412010837330.20599@gannet.stats>

I've not seen anyone mention that this is really a time-series problem of 
lagging variables, and something like

tmp <- ts.intersect(as.ts(return), lag(marketcap, -1))
tmp[,1]*tmp[,2]

is a lot more intuitive, easier to generalize and keeps the timebase 
information around.

On Wed, 1 Dec 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 01-Dec-04 Uwe Ligges wrote:
>> (Ted Harding) wrote:
>>> [...]
>>> Let
>>>
>>>   N<-length(return)
>>>   new.var <- return[2:N]*marketcap[1:(N-1)]
>>
>> Ted, I aggree to all of your points, but we can simplify by negative
>> indices (and hence circumvent your note 1):
>>   return[-1] * marketcap[-N]
>>
>> Uwe Ligges
>
> Neat footwork, Uwe!
> (Why didn;t I think of that? No, don't answer ... )
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 01-Dec-04                                       Time: 08:00:22
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Dec  1 09:50:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 1 Dec 2004 08:50:43 +0000 (GMT)
Subject: [R] spearman correction
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA/KN/oP+2mk+FvuiZg1iX78KAAAAQAAAAjlGUYmKRh0CkPNVQtEO12QEAAAAA@ffos.hr>
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA/KN/oP+2mk+FvuiZg1iX78KAAAAQAAAAjlGUYmKRh0CkPNVQtEO12QEAAAAA@ffos.hr>
Message-ID: <Pine.LNX.4.61.0412010845360.20599@gannet.stats>

On Wed, 1 Dec 2004, Branimir K. Hackenberger wrote:

> Who knows which correction (by tied numbers) is used by Spearman-test in
> function cor.test (e.g. cor.test(a,b,method="spearman"))?

Anyone who reads the sources!  The critical line in cor.test.default is

             r <- cor(rank(x), rank(y))

so it uses rank(), and that averages ranks of tied observations.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Wed Dec  1 10:50:08 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 01 Dec 2004 10:50:08 +0100
Subject: [R] time data
In-Reply-To: <41AD3938.7040403@student.canterbury.ac.nz>
Message-ID: <41ADA1E0.15749.B39553@localhost>



From ripley at stats.ox.ac.uk  Wed Dec  1 10:49:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 1 Dec 2004 09:49:55 +0000 (GMT)
Subject: [R] Unable to understand strptime() behaviour
In-Reply-To: <41ACF5F5.9010403@optushome.com.au>
References: <41ACF5F5.9010403@optushome.com.au>
Message-ID: <Pine.LNX.4.61.0412010946040.21230@gannet.stats>

The result of strptime is a 9-element list.  From the help page

Value:

      'strptime' turns character representations into an object of class
      '"POSIXlt"'.

See Also:

      DateTimeClasses for details of the date-time classes; 'locales' to
      query or set a locale.

Try ?DateTimeClasses.


On Wed, 1 Dec 2004, Tim Churches wrote:

> R V2.0.1 on Windows XP.
>
> I have read the help pages on strptime() over and over, but can't
> understand why strptime() is producing the following results.
>
>  > v <- format("2002-11-31", format="%Y-%m-%d")
>  > v
> [1] "2002-11-31"
>  > factor(v, levels=v)
> [1] 2002-11-31
> Levels: 2002-11-31
>  > x <- strptime("2002-11-31", format="%Y-%m-%d")
>  > x
> [1] "2002-12-01"
>  > factor(x, levels=x)
> [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> Levels: 2002-12-01 NA NA NA NA NA NA NA NA

I can't understand why you are trying to turn a classed object into a 
factor.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bxc at steno.dk  Wed Dec  1 10:52:20 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 1 Dec 2004 10:52:20 +0100
Subject: [R] Unable to understand strptime() behaviour
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FDD66@exdkba022.novo.dk>

Try to say:

class(x)
unclass(x)

and it will dawn on you what goes on.

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tim Churches
> Sent: Tuesday, November 30, 2004 11:37 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Unable to understand strptime() behaviour
> 
> 
> R V2.0.1 on Windows XP.
> 
> I have read the help pages on strptime() over and over, but 
> can't understand why strptime() is producing the following results.
> 
>    > v <- format("2002-11-31", format="%Y-%m-%d")
>    > v
> [1] "2002-11-31"
>    > factor(v, levels=v)
> [1] 2002-11-31
> Levels: 2002-11-31
>    > x <- strptime("2002-11-31", format="%Y-%m-%d")
>    > x
> [1] "2002-12-01"
>    > factor(x, levels=x)
> [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> Levels: 2002-12-01 NA NA NA NA NA NA NA NA
> 
> Tim C
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From tuechler at gmx.at  Wed Dec  1 10:53:43 2004
From: tuechler at gmx.at (Heinz Tuechler)
Date: Wed, 01 Dec 2004 10:53:43 +0100
Subject: [R] How to know if a bug was recognised
In-Reply-To: <c65pq0h6rmcrrjtegoluesvvniroat4f64@4ax.com>
References: <3.0.6.32.20041130132443.007a2a30@pop.gmx.net>
	<3.0.6.32.20041130132443.007a2a30@pop.gmx.net>
Message-ID: <3.0.6.32.20041201105343.007ad6b0@pop.gmx.net>

At 10:43 30.11.2004 -0500, you wrote:
...
>If you send a private email, please use a return address that works.
>I got messages that tuechler at gmx.at has been disabled when I tried to
>respond there.
>
>Duncan Murdoch
>
...
As I tried to tell you, there seems to be a problem on both sides of our
addresses.

I erroneously sent my response to your posting to you
(murdoch at stats.uwo.ca) and not to the list. The answer was what you see below.

X-Flags: 0000
Delivered-To: GMX delivery to tuechler at gmx.at
Date: 30 Nov 2004 15:55:37 -0000
From: MAILER-DAEMON at mail.gmx.net
To: tuechler at gmx.at
Subject: failure notice
X-GMX-Antivirus: -1 (not scanned, may not use virus scanner)
X-GMX-Antispam: 0 (Mail was not recognized as spam)

Hi. This is the qmail-send program at mail.gmx.net.
I'm afraid I wasn't able to deliver your message to the following addresses.
This is a permanent error; I've given up. Sorry it didn't work out.

<murdoch at stats.uwo.ca>:
Connected_to_129.100.45.201_but_sender_was_rejected./Remote_host_said:_550_5
.7.1_<tuechler at gmx.at>..._Access_denied/

--- Below this line is a copy of the message.

Return-Path: <tuechler at gmx.at>
Received: (qmail 16050 invoked by uid 65534); 30 Nov 2004 15:55:31 -0000
Received: from N015P019.adsl.highway.telekom.at (HELO ipc) (213.33.1.211)
  by mail.gmx.net (mp020) with SMTP; 30 Nov 2004 16:55:31 +0100
X-Authenticated: #933343
Message-Id: <3.0.6.32.20041130165453.007a3100 at pop.gmx.net>
X-Sender: 933343 at pop.gmx.net
X-Mailer: QUALCOMM Windows Eudora Light Version 3.0.6 (32)
Date: Tue, 30 Nov 2004 16:54:53 +0100
To: Duncan Murdoch <murdoch at stats.uwo.ca>
From: Heinz Tuechler <tuechler at gmx.at>
Subject: Re: [R] Attn Heinz Tuechler: Re:  problem with sp ecial characters (
 =?iso-8859-1?Q?=E4?=  =?iso-8859-1?Q?,=F6,=FC)?=
In-Reply-To: <l64pq01252g98aki53brfihdfipa96drg5 at 4ax.com>
References: <3.0.6.32.20041127233123.00794d20 at pop.gmx.net>
 <3.0.6.32.20041127233123.00794d20 at pop.gmx.net>
Mime-Version: 1.0
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Thank you for the information. I did already download the 27-11-2004
version and I have the intention to try it within a day and report on the
result, if this is of interest.

My address (tuechler at gmx.at) should work but I will check that too.

...



From andrewr at uidaho.edu  Wed Dec  1 10:53:06 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 1 Dec 2004 20:53:06 +1100
Subject: [R] Unable to understand strptime() behaviour
In-Reply-To: <41ACF5F5.9010403@optushome.com.au>
References: <41ACF5F5.9010403@optushome.com.au>
Message-ID: <20041201095306.GN653@uidaho.edu>

Tim,

which aspect of the results?  November has 30 days :)  What is your goal?

(I'm guessing) try:

x <- as.POSIXct(strptime("2002-11-31", format="%Y-%m-%d"))

see e.g.

https://stat.ethz.ch/pipermail/r-help/2003-May/032823.html

I hope that this helps,

Andrew.

On Wed, Dec 01, 2004 at 09:36:37AM +1100, Tim Churches wrote:
> R V2.0.1 on Windows XP.
> 
> I have read the help pages on strptime() over and over, but can't
> understand why strptime() is producing the following results.
> 
>   > v <- format("2002-11-31", format="%Y-%m-%d")
>   > v
> [1] "2002-11-31"
>   > factor(v, levels=v)
> [1] 2002-11-31
> Levels: 2002-11-31
>   > x <- strptime("2002-11-31", format="%Y-%m-%d")
>   > x
> [1] "2002-12-01"
>   > factor(x, levels=x)
> [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> Levels: 2002-12-01 NA NA NA NA NA NA NA NA
> 
> Tim C
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From petr.pikal at precheza.cz  Wed Dec  1 10:59:17 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 01 Dec 2004 10:59:17 +0100
Subject: [R] Unable to understand strptime() behaviour
In-Reply-To: <41ACF5F5.9010403@optushome.com.au>
Message-ID: <41ADA405.14735.BBF5DB@localhost>

Hi Tim

On 1 Dec 2004 at 9:36, Tim Churches wrote:

> R V2.0.1 on Windows XP.
> 
> I have read the help pages on strptime() over and over, but can't
> understand why strptime() is producing the following results.
> 
>    > v <- format("2002-11-31", format="%Y-%m-%d")
>    > v
> [1] "2002-11-31"
>    > factor(v, levels=v)
> [1] 2002-11-31
> Levels: 2002-11-31
>    > x <- strptime("2002-11-31", format="%Y-%m-%d")
>    > x
> [1] "2002-12-01"
>    > factor(x, levels=x)
> [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> Levels: 2002-12-01 NA NA NA NA NA NA NA NA


This is due to difference in POSIXlt and POSIXct
see

> length(factor(x, levels=x))
[1] 9
> y<-as.POSIXct(x)
> factor(y, levels=y)
[1] 2002-12-01
Levels: 2002-12-01

I do now the result but I do not now the intimate details about 
classes.

Cheers
Petr


> 
> Tim C
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From michael.watson at bbsrc.ac.uk  Wed Dec  1 11:46:51 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 1 Dec 2004 10:46:51 -0000
Subject: [R] barplot() using beside=TRUE and the density argument
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89924@iahce2knas1.iah.bbsrc.reserved>

Hi

I am using barplot() to draw some barplots, with a matrix as the data so
that multiple bars are drawn for each data point.  I want to use the
argument "beside=TRUE" to juxtapose the bars instead of stacking them.

If I execute:

barplot(data,names.arg=names,density=c(20,10),beside=FALSE)

I get the expected behaviour i.e. the bottom part of the column is
shaded 20 lines per inch, the top part 10 lines per inch.  However, if I
try:

barplot(data,names.arg=names,density=c(20,10),beside=TRUE)

I don't get what *I* would expect (which admittedly might be the wrong
thing!).  What happens is that the left bar for each data point is
shaded 20 lines per inch, and the right bar is not shaded at all.

Any help would be very much appreciated.

Mick



From wolfram at fischer-zim.ch  Wed Dec  1 12:00:20 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Wed, 1 Dec 2004 12:00:20 +0100
Subject: [R] split() and paste() a vector to get a multi line string
Message-ID: <20041201110020.GA17944@s1x.local>

How can I get a multi line string from a vector of string tokens
in an easy manner (e.g. for the use as xlab of a plot)?

I have e.g.:
>	tokens <- letters[1:5]
[1] "a" "b" "c" "d" "e"

I search:
[1] "a, b, c\nd, e"

I tried:
>   nlines <- 2
>   ntokens.line <- ceiling(length(tokens) / nlines)
>	token.list <- split(tokens, rep( 1:ntokens.line, each=ntokens.line, len=length(tokens)))
$"1"
[1] "a" "b" "c"
$"2"
[1] "d" "e"

I could work with a data.frame, e.g.:
>	paste(collapse='\n', apply(token.df, MARGIN=1, FUN=paste, collapse=', '))

but I got an error when converting token.list to a data frame:
>	token.df <- data.frame( token.list )
Error in data.frame("1" = c("a", "b", "c"), "2" = c("d", "e"), check.names = FALSE) : 
        arguments imply differing number of rows: 3, 2

What can I do (other than using a loop now)?

Thanks - Wolfram



From p.dalgaard at biostat.ku.dk  Wed Dec  1 11:58:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Dec 2004 11:58:51 +0100
Subject: [R] core dump during make check when building 64-bit R on
	Solaris8/9
In-Reply-To: <Pine.LNX.4.61.0412010739240.19824@gannet.stats>
References: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E0@BIONIC.biopolis.one-north.com>
	<Pine.LNX.4.61.0412010739240.19824@gannet.stats>
Message-ID: <x2k6s2mgic.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> We know of at least two builds which successfully used libsunperf, one
> (mine) with Studio ONE 7 and one with 9.  So this may well be a
> problem with your particular compiler suite.

Or maybe libsunperf version issues. The one that caused me trouble was
this one

/opt/FD8/SUNWspro/lib/v9b/libsunperf.so.4

(on the DTU machines) 

I'm pretty sure it's the same issue as Mr. Chan is seeing:


> example(kappa)

kappa> kappa(x1 <- cbind(1, 1:10))
[1] 15.70590

kappa> kappa(x1, exact = TRUE)
Reading lapack.so
Reading libRlapack.so
signal SEGV (no mapping at the fault address) in ___pl_dgesdd_64_ at 0xffffffff793e3374
0xffffffff793e3374: ___pl_dgesdd_64_+0x1654:    std      %f4, [%o1]


Does this ring any bells?:

euler(kubspd) $ cc -V
cc: Sun C 5.5 Patch 112760-09 2004/03/31
...

and Makeconf has this:

CC = cc -xarch=v9b
CFLAGS = -xO5 -xlibmil -dalign
CPICFLAGS = -KPIC
CPPFLAGS = -I/usr/local/include -I/opt/sfw/include

The top of the stack looks like this

=>[1] ___pl_dgesdd_64_(0xffffffff7fff86a8, 0xffffffff7fff86d8, 0xffffffff7fff86d0, 0x10099a078, 0xffffffff7fff86c8, 0xffffffff7fff86b0), at 0xffffffff793e3374
  [2] ___pl_dgesdd_(0x2, 0x1, 0xffffffff7fff88cc, 0xa, 0x100d5b640, 0xffffffff7fff86b0), at 0xffffffff78d42494
  [3] modLa_svd(0x1012f4278, 0x1012f4210, 0x101388e00, 0x101388dd8, 0x100d555b8, 0x100d5b618), at 0xffffffff76f05bf0
  [4] La_svd(0x10040ec00, 0xffffffff76f056e8, 0x1016e03d0, 0x101388dd8, 0x100d555b8, 0x100d5b618), at 0x100108f2c
  [5] do_dotcall(0x10131b8a0, 0x0, 0xaf4, 0x1c, 0x1000aa06c, 0x1004153a8), at 0x1000aab80

which looks like a pretty straightforward SVD operation. However, it
would be a good idea if someone who knows his way around dbx (I don't)
could trace this and see if crud got injected into the matrix at some
earlier stage. Or maybe use a simplified example:

> svd(matrix(1:4,2))
signal SEGV (no mapping at the fault address) in ___pl_dgesdd_64_ at
0xfff....

That one looks potentially simple enough that you might cook up a little
standalone program showing the problem and ship it to Sun.


> On Wed, 1 Dec 2004, CHAN Chee Seng wrote:
> 
> > Hi,
> >
> > After reading some of the posting in this list, I came across this
> > posting from:
> >
> > From: Peter Dalgaard <p.dalgaard_at_biostat.ku.dk>
> > Date: Fri 29 Oct 2004 - 08:02:40 EST
> > Replying to Re: [R] Errors during make check
> >
> > He described a problem similar to mine, that build 64-bit R (I am
> > building R version 2.0.1) with sunperf library gives a core dump during
> > make check.  So I configured my build without the sunperf (BLAS and
> > lapack) library.  My configure statement looks like this:
> >
> > ./configure --prefix=/usr/local/R-2.0.1
> > --with-tcl-config=/usr/sfw/lib/tclCo
> > nfig.sh --with-tk-config=/usr/sfw/lib/tkConfig.sh --without-blas
> >
> > The "make check" ran without a problem after this.
> >
> > I hope that the sun compiler people can look into this and see if there
> > is a problem with the sunperf library.  Maybe Peter Dalgaard ha further
> > insights into this?  Thanks.
> >
> > Regards,
> > Chee Seng
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of CHAN Chee Seng
> > Sent: Monday, November 29, 2004 6:13 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] core dump during make check when building 64-bit R on
> > Solaris8/9
> >
> > Hi,
> >
> > I am building a 64-bit R 2.0.1 on Solaris 9.  The compiler is Sun Studio
> > 8.  Make was successful but I have a core dump during a make check.  By
> > the way, this problem also happens on my Solaris 8 machine though I did
> > not get a core dump.  I do not have 64-bit versions of the readline,
> > tcl/tk, libncurses, etc libraries so these caused configure not to use
> > them with ELFCLASS32 errors.
> >
> > I use the following flags in config.site:
> > CC="cc -xarch=v9"
> > CFLAGS="-xO5 -xlibmil -dalign"
> > F77="f95 -xarch=v9"
> > FFLAGS="-xO5 -xlibmil -dalign"
> > CXX="CC -xarch=v9"
> >
> >
> > I pasted the make check error messages below:
> > $ make check
> > collecting examples for package 'base' ...
> > >>> Building/Updating help pages for package 'base'
> >     Formats: text html latex example
> > running code in 'base-Ex.R' ...*** Error code 1
> > make: Fatal error: Command failed for target `base-Ex.Rout'
> > Current working directory /export/home/cheeseng/R-2.0.1/tests/Examples
> > *** Error code 1
> > make: Fatal error: Command failed for target `test-Examples-Base'
> > Current working directory /export/home/cheeseng/R-2.0.1/tests/Examples
> > *** Error code 1
> > make: Fatal error: Command failed for target `test-Examples'
> > Current working directory /export/home/cheeseng/R-2.0.1/tests
> > *** Error code 1
> > make: Fatal error: Command failed for target `test-all-basics'
> > Current working directory /export/home/cheeseng/R-2.0.1/tests
> > *** Error code 1
> > make: Fatal error: Command failed for target `check'
> >
> >
> > The last few lines in base-Ex.Rout.fail is:
> >> ### * kappa
> >>
> >> flush(stderr()); flush(stdout())
> >>
> >> ### Name: kappa
> >> ### Title: Estimate the Condition Number
> >> ### Aliases: kappa kappa.default kappa.lm kappa.qr kappa.tri
> >> ### Keywords: math
> >>
> >> ### ** Examples
> >>
> >> kappa(x1 <- cbind(1,1:10))# 15.71
> > [1] 15.70590
> >
> >
> > Doing a dbx on the core dump shows that the following:
> > program terminated by signal SEGV (no mapping at the fault address)
> > 0xffffffff790fdf54: ___pl_dgesdd_64_+0x1654:    std      %f4, [%o1]
> >
> > I hope some one have a more successful build and can show how you did
> > it.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Dec  1 12:08:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 1 Dec 2004 11:08:15 +0000 (GMT)
Subject: [R] core dump during make check when building 64-bit R on
	Solaris8/9
In-Reply-To: <x2k6s2mgic.fsf@biostat.ku.dk>
References: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E0@BIONIC.biopolis.one-north.com>
	<Pine.LNX.4.61.0412010739240.19824@gannet.stats>
	<x2k6s2mgic.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.61.0412011102480.22064@gannet.stats>

On Wed, 1 Dec 2004, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> We know of at least two builds which successfully used libsunperf, one
>> (mine) with Studio ONE 7 and one with 9.  So this may well be a
>> problem with your particular compiler suite.
>
> Or maybe libsunperf version issues. The one that caused me trouble was

libsunperf *is* part of the compiler suite, as your path below shows.

> this one
>
> /opt/FD8/SUNWspro/lib/v9b/libsunperf.so.4
>
> (on the DTU machines)

I am using

/opt/SunONE7/SUNWspro/lib/v9/libsunperf.so.4

The precise architecture selected comes into play, too (v9 vs v9b).

My point was that we do know of working examples.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lecoutre at stat.ucl.ac.be  Wed Dec  1 12:09:12 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Wed, 01 Dec 2004 12:09:12 +0100
Subject: [R] split() and paste() a vector to get a multi line string
In-Reply-To: <20041201110020.GA17944@s1x.local>
References: <20041201110020.GA17944@s1x.local>
Message-ID: <6.0.1.1.2.20041201120655.01ff1ec0@stat4ux.stat.ucl.ac.be>

Hi:

here is something that works on your token.list object:

 > paste(lapply(token.list,paste,collapse=", "),collapse="\n")
[1] "a, b, c\nd, e"

You can't use data.frames, as your vectors dont have the same number of 
elements.
But you still have all the information, in a list. So you can use lapply to 
compute on elements of this list.
If we decompose, the lapply does:

 > lapply(token.list,paste,collapse=", ")
$"1"
[1] "a, b, c"

$"2"
[1] "d, e"

On which we call a paste.

HTH,

Eric



At 12:00 1/12/2004, Wolfram Fischer wrote:
>How can I get a multi line string from a vector of string tokens
>in an easy manner (e.g. for the use as xlab of a plot)?
>
>I have e.g.:
> >       tokens <- letters[1:5]
>[1] "a" "b" "c" "d" "e"
>
>I search:
>[1] "a, b, c\nd, e"
>
>I tried:
> >   nlines <- 2
> >   ntokens.line <- ceiling(length(tokens) / nlines)
> >       token.list <- split(tokens, rep( 1:ntokens.line, 
> each=ntokens.line, len=length(tokens)))
>$"1"
>[1] "a" "b" "c"
>$"2"
>[1] "d" "e"
>
>I could work with a data.frame, e.g.:
> >       paste(collapse='\n', apply(token.df, MARGIN=1, FUN=paste, 
> collapse=', '))
>
>but I got an error when converting token.list to a data frame:
> >       token.df <- data.frame( token.list )
>Error in data.frame("1" = c("a", "b", "c"), "2" = c("d", "e"), check.names 
>= FALSE) :
>         arguments imply differing number of rows: 3, 2
>
>What can I do (other than using a loop now)?
>
>Thanks - Wolfram
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From p.dalgaard at biostat.ku.dk  Wed Dec  1 12:34:57 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Dec 2004 12:34:57 +0100
Subject: [R] spearman correction
In-Reply-To: <Pine.LNX.4.61.0412010845360.20599@gannet.stats>
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA/KN/oP+2mk+FvuiZg1iX78KAAAAQAAAAjlGUYmKRh0CkPNVQtEO12QEAAAAA@ffos.hr>
	<Pine.LNX.4.61.0412010845360.20599@gannet.stats>
Message-ID: <x2fz2qmeu6.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Wed, 1 Dec 2004, Branimir K. Hackenberger wrote:
> 
> > Who knows which correction (by tied numbers) is used by Spearman-test in
> > function cor.test (e.g. cor.test(a,b,method="spearman"))?
> 
> Anyone who reads the sources!  The critical line in cor.test.default is
> 
>              r <- cor(rank(x), rank(y))
> 
> so it uses rank(), and that averages ranks of tied observations.

...and correcting the p value for that is on the wish list for some of
us. 

As far as I remember, that is quite easy to do in the asymptotic
approximation (but hard to do exactly): Start by working out the
covariance matrix of the y ranks given the tied sets of values; a
matrix with compound symmetry, obviously, and with rows/columns
summing to zero since the sum of the ranks is known, so you really
just need the variance of ry_1. Then note that the statistic is
equivalent to sum(rx*ry) which, conditionally on the values of x (and
y), has the distribution of a linear combination of the ry, the
variance of which is...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From michael.watson at bbsrc.ac.uk  Wed Dec  1 12:50:45 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 1 Dec 2004 11:50:45 -0000
Subject: [R] Data Frame Manipulations
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B8AE@iahce2knas1.iah.bbsrc.reserved>

Hi

I have used merge() to merge two data frames, very much like performing
a SQL join.  Now I want to do a few different SQL-style things and I
wondered if there were functions to do it... 

Is there a "group by" style function?  For example if I merge() two data
frames and end up with multiple rows for each "id", and want to take the
average of the values of a particular column?  I know I can probably put
something together using merge() and by() and then munging the results
together myself, but is there something in R to perform this
automatically?

The second thing I'd like to do is like a cross-tab query; that is when
after a merge() I end up with multiple rows for a particular "id", and
want to cross-tab the data so that the multiple values become columns
and I end up with one row for each "id" again e.g.

ID	Val
1	5
1	10
2	15
2	20

Becomes

ID	Val1	Val2
1	5	10
2	15	20

Thanks in advance!

Mick



From snvk4u at yahoo.co.in  Wed Dec  1 13:04:37 2004
From: snvk4u at yahoo.co.in (neela v)
Date: Wed, 1 Dec 2004 04:04:37 -0800 (PST)
Subject: [R] request for help on statistical applications in pharma
Message-ID: <20041201120437.53125.qmail@web8408.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041201/088db9b4/attachment.pl

From petr.pikal at precheza.cz  Wed Dec  1 13:06:49 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 01 Dec 2004 13:06:49 +0100
Subject: [R] Data Frame Manipulations
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B8AE@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41ADC1E9.22015.1313220@localhost>

Hi Michael

On 1 Dec 2004 at 11:50, michael watson (IAH-C) wrote:

> Hi
> 
> I have used merge() to merge two data frames, very much like
> performing a SQL join.  Now I want to do a few different SQL-style
> things and I wondered if there were functions to do it... 
> 
> Is there a "group by" style function?  For example if I merge() two
> data frames and end up with multiple rows for each "id", and want to
> take the average of the values of a particular column?  I know I can
> probably put something together using merge() and by() and then
> munging the results together myself, but is there something in R to
> perform this automatically?

aggregate, by, *apply but AFAIK only after merge

> 
> The second thing I'd like to do is like a cross-tab query; that is
> when after a merge() I end up with multiple rows for a particular
> "id", and want to cross-tab the data so that the multiple values
> become columns and I end up with one row for each "id" again e.g.

reshape in base or reShape in Hmisc is probably what you need

Cheers
Petr


> 
> ID	Val
> 1	5
> 1	10
> 2	15
> 2	20
> 
> Becomes
> 
> ID	Val1	Val2
> 1	5	10
> 2	15	20
> 
> Thanks in advance!
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Wed Dec  1 13:14:57 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 01 Dec 2004 13:14:57 +0100
Subject: [R] barplot() using beside=TRUE and the density argument
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89924@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41ADC3D1.10073.138A50C@localhost>



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Dec  1 13:13:58 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 1 Dec 2004 13:13:58 +0100
Subject: [R] Data Frame Manipulations
References: <8975119BCD0AC5419D61A9CF1A923E950121B8AE@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <003201c4d79f$3c270aa0$0540210a@www.domain>

Hi Michael,

regarding your second question you could use the `reshape()' function, 
i.e.,

dat <- data.frame(ID=rep(1:2, each=2), Val=seq(5,20,5))
######
dat$time <- unlist(lapply(split(dat$ID, dat$ID), function(x) 
1:length(x)), use.names=FALSE)
reshape(dat, direction="wide", idvar="ID", v.names="Val")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, December 01, 2004 12:50 PM
Subject: [R] Data Frame Manipulations


> Hi
>
> I have used merge() to merge two data frames, very much like 
> performing
> a SQL join.  Now I want to do a few different SQL-style things and I
> wondered if there were functions to do it...
>
> Is there a "group by" style function?  For example if I merge() two 
> data
> frames and end up with multiple rows for each "id", and want to take 
> the
> average of the values of a particular column?  I know I can probably 
> put
> something together using merge() and by() and then munging the 
> results
> together myself, but is there something in R to perform this
> automatically?
>
> The second thing I'd like to do is like a cross-tab query; that is 
> when
> after a merge() I end up with multiple rows for a particular "id", 
> and
> want to cross-tab the data so that the multiple values become 
> columns
> and I end up with one row for each "id" again e.g.
>
> ID Val
> 1 5
> 1 10
> 2 15
> 2 20
>
> Becomes
>
> ID Val1 Val2
> 1 5 10
> 2 15 20
>
> Thanks in advance!
>
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From 0034058 at fudan.edu.cn  Wed Dec  1 12:43:31 2004
From: 0034058 at fudan.edu.cn (ronggui)
Date: Wed, 01 Dec 2004 19:43:31 +0800
Subject: [R] how to add a legend in a barchart
Message-ID: <0I8100G17K8FVJ@mail.fudan.edu.cn>

i use 
#pic 1
library(lattice)
barchart(V1~V2,data,panel=function(x,y,...){
panel.barchart(x,y,...)
panel.abline(v=6.76,col="red")},
)
to draw a picture.and i want to add a legend to telling others the line vertical is x=6.76.so how should i do? legend does not works.
thank you .
i have use help.search("legend") but no result i want.



From Robert at sanctumfi.com  Wed Dec  1 13:38:40 2004
From: Robert at sanctumfi.com (Robert Sams)
Date: Wed, 1 Dec 2004 12:38:40 -0000
Subject: [R] can't install r package on debian due to linker problem
Message-ID: <E585EABA11227445B918BFB74C1A4D36015971@sanctum01.sanctumfi.com>

hi,

my attempt to install the package Hmisc v3.0-1 fails with the message:

/usr/bin/ld: cannot find -lfrtbegin
collect2: ld returned 1 exit status
make: *** [Hmisc.so] Error 1
ERROR: compilation failed for package 'Hmisc'

i'm at a loss here. any hints will be very much appreciated.

i'm running:

debian stable
R version 2.0.1
gcc 2.95.4-14
g77 2.95.4-14
binutils 2.12.90.0.1-4


robert




Robert Sams

SANCTUM FI LLP
Charles House
18B Charles Street
Mayfair
London W1J 5DU
Tel: +44 (0) 207 667 6360
Dir: +44 (0) 207 667 6363
fax: +44 (0) 207 667 6460
email: robert at sanctumfi.com

Authorised and Regulated by the FSA. 

Sending encrypted mail:

See http://pgp.mit.edu (search string 'sanctumfi') for updates.

-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1.0.6 (GNU/Linux)
Comment: For info see http://www.gnupg.org

mQGiBD/xA1kRBAC2RUt8WyHDjXDoO1eu2Mli83cuEV37FicaBs/Wj5ry1QIz1drj
ubP25QQvu0lYOOnP7iS48bcOVP77uQYqLbsvzQ4fChFLCg9O3D4ourolZEK69ooJ
74r87PPV/LEnURL4T8E7QCDrRmylJ1iCffgJ9JWkAN4qUG+6fFuOyiqxDwCgiMkM
oLaYHjl3unc7Anx59xAlm+MD/R1EdXM9uewCj2kS3SdN+akklg6QVy+gTl3+HGzc
MC6ZcLsk1uIklkvCfoYDN7oeC3uVGik7QYkE3n02vfAMGjW7AqsQEoblzi3QscoX
Kzitd04NcWeDYIXRThCCydIJ64DdpF293ewJf2fRykmBdAbV0oaqL+zgdrRqFxYJ
m2d9A/9m3DoHwXBXPyqueX8naY5hmmeG+mihkdI4H4MBmaPJZW9DnZxor6P2Nm/X
muDSf7aZ1t9J0t75oEY/SyjoCYhWlMJS+wnOUq3u5XRNRyo9oI1qQaF5zw8ZmIzW
x7nfQMlXpXpETGwbNxZuUoucbN1cDrZBymHxlI5AiBCZ/fya3rQzUm9iZXJ0IFNh
bXMgKFNhbmN0dW0gRkkgTExQKSA8cm9iZXJ0QHNhbmN0dW1maS5jb20+iF0EExEC
AB0FAj/xA1kFCQHhM4AFCwcKAwQDFQMCAxYCAQIXgAAKCRD3WGga9bCIIPV6AJ9t
cDQkN8jW8CxzbU5K2O3dxaMlKwCeNOYQsAgW7S1qAlM345QpDXxbjgC5Ag0EP/ED
bRAIANoxq2NjQFdrUMSlQEaitR3pFTmOC7n2rICBAbU/hxlVs1PFSyh6Tr00vzFM
py6n4uBCOzrz3b5u5YbRaQzs8ipkqnSzoDD6GKfMEEWYQvZ76kkShWt5zDkLQ2X4
V4X+xJ0iuFT+9cK7VuJ102pjsAwltDUGPKsSwWqs55tzBN8BwAxqNMxRtNYbOTAB
Dpnm1BsiZ1TLqHIr4a1t2ZEuqhKV0HEP9VugP9XQz9u1f5QZrriNW/foxBwLuXS7
g+945IGXZq/qxHzgQjJQhC3jIaHUrchrQxy6snoQxgAnuO2/g9SLI8BBsvpVZ+Ac
mpkhPtT4pGujwsK/oRFDloAb7BsAAwYIAMlQrB2GPn1ZNFIf9zN+euTv2jWx5Hv4
ZEhqeBTqq00KCT3NSrnOHBTX4x6F4L+ofRzl2L5zIi685wWTpLgqQI7hzKvAxerJ
xe1qpz5GfGX976uaqxEfzwQZqcZB2iihhjeOUTxalSWdkX73yNtRmLLikTr0U3E0
v0dB1laMqldYub4X+GeH7tAeQGqYfS6Y+BdNDWfIcwADM0ggLIbNsw+IsjdQNOpq
5R4p1E2o5kfvafIFLpMOZACKKdTBkfiAqOZq8ezDpNHwLrRG4RvQ1K80pGGqaikI
XFbJIthvimA5w4MjvenuIn367zj+bz5eFE7YeQ0KG7NAdg2DkxD4W9CITAQYEQIA
DAUCP/EDbQUJAeEzgAAKCRD3WGga9bCIIL8RAJ4o9zXtkqK5RMKXxJTmZejtDjTC
kwCdFevBc9z4ermWaKb9BsDU7OYdgM8=
=6Y3T
-----END PGP PUBLIC KEY BLOCK-----



From david.meyer at wu-wien.ac.at  Wed Dec  1 13:49:22 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Wed, 1 Dec 2004 13:49:22 +0100
Subject: [R] tuning SVM's
Message-ID: <20041201134922.13cafe79.david.meyer@wu-wien.ac.at>

Stephen:

Your calls to best.svm() do not tune anything unless you specify the
parameter ranges (see the examples on the help page). Your calls are
just using the defaults which are very unlikely to yield models with
good performance.

[I think some day, I will have to remove the defaults in svm()...]

Another point: why aren't you using classification machines (which is
done automatically by providing a factor as dependent variable)?

There is classAgreement() in e1071, too, you might want to look at.

Cheers,
David






Hi  
 
I am doing this  sort of thing:
 
POLY:
 
> > obj = best.tune(svm, similarity ~., data = training, kernel =
"polynomial")
> summary(obj)
 
Call:
 best.tune(svm, similarity ~ ., data = training, kernel = "polynomial") 
 
Parameters:
   SVM-Type:  eps-regression 
 SVM-Kernel:  polynomial 
       cost:  1 
     degree:  3 
      gamma:  0.04545455 
     coef.0:  0 
    epsilon:  0.1 
 
 
Number of Support Vectors:  754
 
> svm.model <- svm(similarity ~., data = training, kernel  =
"polynomial", cost = 1, degree = 3, gamma = 0.04545455, coef.0 = 0,
epsilon = 0.1)
> pred=predict(svm.model, testing)
> pred[pred > .5] = 1
> pred[pred <= .5] = 0  
> table(testing$similarity, pred)
   pred
    0  1 
  0 30  8
  1 70 63
> obj = best.tune(svm, similarity ~., data = training, kernel =
"linear")
> summary(obj)
 
LINEAR:
 
Call:
 best.tune(svm, similarity ~ ., data = training, kernel = "linear") 
 
Parameters:
   SVM-Type:  eps-regression 
 SVM-Kernel:  linear 
       cost:  1 
      gamma:  0.04545455 
    epsilon:  0.1 
 
 
Number of Support Vectors:  697
 
> svm.model <- svm(similarity ~., data = training, kernel  = "linear",
cost = 1, gamma = 0.04545455, epsilon = 0.1)
> pred=predict(svm.model, testing)
> pred[pred > .5] = 1
> pred[pred <= .5] = 0  
> table(testing$similarity, pred)
   pred
    0   1  
  0   6  32
  1   4 129
 
 
RADIAL:
 
> obj = best.tune(svm, similarity ~., data = training, kernel =
"radial")
> summary(obj)
 
Call:
 best.tune(svm, similarity ~ ., data = training, kernel = "linear") 
 
Parameters:
   SVM-Type:  eps-regression 
 SVM-Kernel:  linear 
       cost:  1 
      gamma:  0.04545455 
    epsilon:  0.1 
 
 
Number of Support Vectors:  697
 
> svm.model <- svm(similarity ~., data = training, kernel  = "radial",
cost = 1, gamma = 0.04545455, epsilon = 0.1)
> pred=predict(svm.model, testing)
> pred[pred > .5] = 1
> pred[pred <= .5] = 0  
> table(testing$similarity, pred)
   pred
    0  1 
  0 27 11
  1 64 69
 
 
SIGMOID:
 
> obj = best.tune(svm, similarity ~., data = training, kernel =
"sigmoid")
> summary(obj)
 
Call:
 best.tune(svm, similarity ~ ., data = training, kernel = "sigmoid") 
 
Parameters:
   SVM-Type:  eps-regression 
 SVM-Kernel:  sigmoid 
       cost:  1 
      gamma:  0.04545455 
     coef.0:  0 
    epsilon:  0.1 
 
 
Number of Support Vectors:  986
 
> svm.model <- svm(similarity ~., data = training, kernel  = "sigmoid",
cost = 1, gamma = 0.04545455, coef.0 = 0, epsilon = 0.1)
> pred=predict(svm.model, testing)
> pred[pred > .5] = 1
> pred[pred <= .5] = 0  
> table(testing$similarity, pred)
   pred
    0   1  
  0   8  30
  1  26 107
>
 
and then taking out the kappa statistic to see if I am getting anything
significant.
 
I get kappas of 15 - 17% - I don't think that is very good.  I know
kappa is really for comparing the outcomes of two taggers but it seems a
good way to measure if your results might be by chance.
 
Two questions:
 
Any comments on Kappa and what it might be telling me?
 
What can I do to tune my kernels further?
 
Stephen
-- 
Dr. David Meyer
Department of Information Systems

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From ripley at stats.ox.ac.uk  Wed Dec  1 13:55:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 1 Dec 2004 12:55:22 +0000 (GMT)
Subject: [R] can't install r package on debian due to linker problem
In-Reply-To: <E585EABA11227445B918BFB74C1A4D36015971@sanctum01.sanctumfi.com>
References: <E585EABA11227445B918BFB74C1A4D36015971@sanctum01.sanctumfi.com>
Message-ID: <Pine.LNX.4.61.0412011250030.23310@gannet.stats>

On Wed, 1 Dec 2004, Robert Sams wrote:

> hi,
>
> my attempt to install the package Hmisc v3.0-1 fails with the message:
>
> /usr/bin/ld: cannot find -lfrtbegin
> collect2: ld returned 1 exit status
> make: *** [Hmisc.so] Error 1
> ERROR: compilation failed for package 'Hmisc'
>
> i'm at a loss here. any hints will be very much appreciated.
>
> i'm running:
>
> debian stable
> R version 2.0.1
> gcc 2.95.4-14
> g77 2.95.4-14
> binutils 2.12.90.0.1-4

Edit R_HOME/etc/Makeconf and remove -lfrtbegin from FLIBS.  It is not 
needed, and 2.1.0 will work that out.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jarioksa at sun3.oulu.fi  Wed Dec  1 13:58:33 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 01 Dec 2004 14:58:33 +0200
Subject: [R] can't install r package on debian due to linker problem
In-Reply-To: <E585EABA11227445B918BFB74C1A4D36015971@sanctum01.sanctumfi.com>
References: <E585EABA11227445B918BFB74C1A4D36015971@sanctum01.sanctumfi.com>
Message-ID: <1101905913.14863.4.camel@biol102145.oulu.fi>

On Wed, 2004-12-01 at 14:38, Robert Sams wrote:
> hi,
> 
> my attempt to install the package Hmisc v3.0-1 fails with the message:
> 
> /usr/bin/ld: cannot find -lfrtbegin
> collect2: ld returned 1 exit status
> make: *** [Hmisc.so] Error 1
> ERROR: compilation failed for package 'Hmisc'
> 
It is funny to see this error message in Debian which is a GNU/Linux
system. Typically you see the very same error message in MacOS X which
is a GNU/BSD system. There this is caused by missing Fortran compiler.
Indeed, at least in Red Hat Linux, libfrbegin.a is owned by Fortran
(g77). However, you claim below that you have installed Fortran (g77). I
suggest you look for if you some Fortran related packages are missing,
or you can try to 'locate' libfrtbegin.a in your system and see if it is
in the linker search path.
 
> i'm at a loss here. any hints will be very much appreciated.
> 
> i'm running:
> 
> debian stable
> R version 2.0.1
> gcc 2.95.4-14
> g77 2.95.4-14
> binutils 2.12.90.0.1-4
> 
cheers, jari oksanen

-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From stu44414 at mail.uni-kiel.de  Wed Dec  1 14:14:52 2004
From: stu44414 at mail.uni-kiel.de (Andreas Franke)
Date: Wed, 1 Dec 2004 14:14:52 +0100
Subject: [R] plotting data in non-orthogonal coords.
In-Reply-To: <41AC71F6.15899.12DDE03@localhost>
References: <41AC4AEB.15710.955B5A@localhost>
	<41AC71F6.15899.12DDE03@localhost>
Message-ID: <200412011414.52970.stu44414@mail.uni-kiel.de>

Am Dienstag, 30. November 2004 13:13 schrieb Petr Pikal:
> On 30 Nov 2004 at 11:06, Andreas Franke wrote:
>
> <snip>
>
> > Hi !
> > Thanks for replying to my post and sorry for not being specific
> > enough. Maybe I am the one who didnt get the point , but as far as I
> > understand R plots filled.contour(x,y,z,...) in the following way:
> >
> > x,y define a grid in cartesian coords, i.e. the angle between x and y
> > is 90??. So if I have data on such a grid I am fine. My data is on an
> > equally spaced grid in a coordsystem where x and y are at an angle of
> > 60??. If you transform into cartesiancoords this isnt an equally spaced
> > grid anymore.  You could interpret it as an equally spaced grid on
> > which there is only data given on every second grid point.
> >
> > It would be nice if one could just plot data given as F(x,y) where you
> > supply x and y for every data point seperatly so that you dont need
> > any specific grid.
>
> Maybe interp() in akima package can help you, but as I said it
> strongly depends on what you really want to achieve.
>
> Cheers
> Petr
>
> > I hope that someone can help.
> > Thanks in advance. Andreas
>
> Petr Pikal
> petr.pikal at precheza.cz


Hi !

Thanks alot !!! The akima package was exactly what I needed. Looks like it 
works out now.

Best wishes and a fine christmas time ...

Andreas



From Robert at sanctumfi.com  Wed Dec  1 14:34:14 2004
From: Robert at sanctumfi.com (Robert Sams)
Date: Wed, 1 Dec 2004 13:34:14 -0000
Subject: [R] can't install r package on debian due to linker problem
Message-ID: <E585EABA11227445B918BFB74C1A4D36015973@sanctum01.sanctumfi.com>

thank you both. 
i edited the Makeconf file per brian ripley's instruction and all works well.

cheers,
robert


-----Original Message-----
From: Jari Oksanen [mailto:jarioksa at sun3.oulu.fi]
Sent: Wednesday, December 01, 2004 12:59 PM
To: Robert Sams
Cc: R-News
Subject: Re: [R] can't install r package on debian due to linker problem


On Wed, 2004-12-01 at 14:38, Robert Sams wrote:
> hi,
> 
> my attempt to install the package Hmisc v3.0-1 fails with the message:
> 
> /usr/bin/ld: cannot find -lfrtbegin
> collect2: ld returned 1 exit status
> make: *** [Hmisc.so] Error 1
> ERROR: compilation failed for package 'Hmisc'
> 
It is funny to see this error message in Debian which is a GNU/Linux
system. Typically you see the very same error message in MacOS X which
is a GNU/BSD system. There this is caused by missing Fortran compiler.
Indeed, at least in Red Hat Linux, libfrbegin.a is owned by Fortran
(g77). However, you claim below that you have installed Fortran (g77). I
suggest you look for if you some Fortran related packages are missing,
or you can try to 'locate' libfrtbegin.a in your system and see if it is
in the linker search path.
 
> i'm at a loss here. any hints will be very much appreciated.
> 
> i'm running:
> 
> debian stable
> R version 2.0.1
> gcc 2.95.4-14
> g77 2.95.4-14
> binutils 2.12.90.0.1-4
> 
cheers, jari oksanen

-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From vito_ricci at yahoo.com  Wed Dec  1 14:29:58 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 1 Dec 2004 14:29:58 +0100 (CET)
Subject: [R] Re: request for help on statistical applications in pharma
Message-ID: <20041201132958.98228.qmail@web41208.mail.yahoo.com>

Hi Krishna,

I don't have direct exeperience in using statistics in
pharma industry, but I know that some notes (in
Italian) of prof. Soliani are used in courses taken
for Italian Pharma Industries. You can find that at
this address:

http://www.dsa.unipr.it/soliani/soliani.html

I try to translate you the main issues treated:

statistical distributions, probability, frequencies,
descriptive statistics, parametric and non-parametric
hypothesis testing for 1, 2 or more samples, analysis
of variance, linear models, correlation, contingency
tables.

I hope I give you a little help.

Best
Vito

 

you wrote:

Hi all there
 
Can some one who have experience and knowledge in the
pharma industry give me broad details on the subject
"Statistical applications for pharma industry". I am
new to the subject i.e. what kind of statistical tools
used being unique /or atleast specific for pharma
industry. 
 
It will be of great help as i am actively looking into
the subject. though i am quite familiar with
statistical analysis still now my focus was on time
series analysis.
 
thanks for the attention
 
krishna

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From andy_liaw at merck.com  Wed Dec  1 14:37:30 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 1 Dec 2004 08:37:30 -0500
Subject: [R] request for help on statistical applications in pharma
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3AB@usrymx25.merck.com>

This is off-topic for the list!

Anyway, you may find the following useful:
http://www.elmo.ch/doc/statistics-in-pharma/

Andy

> From: neela v
> 
> Hi all there
>  
> Can some one who have experience and knowledge in the pharma 
> industry give me broad details on the subject "Statistical 
> applications for pharma industry". I am new to the subject 
> i.e. what kind of statistical tools used being unique /or 
> atleast specific for pharma industry. 
>  
> It will be of great help as i am actively looking into the 
> subject. though i am quite familiar with statistical analysis 
> still now my focus was on time series analysis.
>  
> thanks for the attention
>  
> krishna
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From MSchwartz at MedAnalytics.com  Wed Dec  1 14:43:47 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 01 Dec 2004 07:43:47 -0600
Subject: [R] barplot() using beside=TRUE and the density argument
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89924@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89924@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <1101908627.856.11.camel@horizons.localdomain>

On Wed, 2004-12-01 at 10:46 +0000, michael watson (IAH-C) wrote:
> Hi
> 
> I am using barplot() to draw some barplots, with a matrix as the data so
> that multiple bars are drawn for each data point.  I want to use the
> argument "beside=TRUE" to juxtapose the bars instead of stacking them.
> 
> If I execute:
> 
> barplot(data,names.arg=names,density=c(20,10),beside=FALSE)
> 
> I get the expected behaviour i.e. the bottom part of the column is
> shaded 20 lines per inch, the top part 10 lines per inch.  However, if I
> try:
> 
> barplot(data,names.arg=names,density=c(20,10),beside=TRUE)
> 
> I don't get what *I* would expect (which admittedly might be the wrong
> thing!).  What happens is that the left bar for each data point is
> shaded 20 lines per inch, and the right bar is not shaded at all.
> 
> Any help would be very much appreciated.
> 
> Mick

Without a reproducible example, it is hard to know exactly what you are
seeing. 

Running the following example:

barplot(matrix(1:12, ncol = 6), density = c(20, 10), beside = TRUE)

I will admit that the right hand bars in the plot have a shading that is
very light and may be difficult to see on your system, but they are
there.

You might want to try the following:

barplot(matrix(1:12, ncol = 6), density = c(20, 10), 
        col = c("red", "blue"), beside = TRUE)

to see if the addition of color makes a difference, or if not, try to
increase the second density value.

If this does not help, please provide a reproducible example.

HTH,

Marc Schwartz



From ggrothendieck at myway.com  Wed Dec  1 15:11:30 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 1 Dec 2004 14:11:30 +0000 (UTC)
Subject: [R] Relative subscripting
References: <XFMail.041201080022.Ted.Harding@nessie.mcc.ac.uk>
	<Pine.LNX.4.61.0412010837330.20599@gannet.stats>
Message-ID: <loom.20041201T150920-53@post.gmane.org>


And, in general, using the right classes or data structures 
makes modelling your problem in R much easier.  It is 
surprising that there was an entire discussion on this
before Brian brought up the right approach.

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

: 
: I've not seen anyone mention that this is really a time-series problem of 
: lagging variables, and something like
: 
: tmp <- ts.intersect(as.ts(return), lag(marketcap, -1))
: tmp[,1]*tmp[,2]
: 
: is a lot more intuitive, easier to generalize and keeps the timebase 
: information around.
: 
: On Wed, 1 Dec 2004 Ted.Harding <at> nessie.mcc.ac.uk wrote:
: 
: > On 01-Dec-04 Uwe Ligges wrote:
: >> (Ted Harding) wrote:
: >>> [...]
: >>> Let
: >>>
: >>>   N<-length(return)
: >>>   new.var <- return[2:N]*marketcap[1:(N-1)]
: >>
: >> Ted, I aggree to all of your points, but we can simplify by negative
: >> indices (and hence circumvent your note 1):
: >>   return[-1] * marketcap[-N]
: >>
: >> Uwe Ligges
: >
: > Neat footwork, Uwe!
: > (Why didn;t I think of that? No, don't answer ... )
: > Ted.



From dnogues at ipe.csic.es  Wed Dec  1 16:09:21 2004
From: dnogues at ipe.csic.es (=?ISO-8859-1?Q?David_Nogu=E9s?=)
Date: Wed, 01 Dec 2004 16:09:21 +0100
Subject: [R] step.gam
Message-ID: <41ADDEA1.5070505@ipe.csic.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041201/93519780/attachment.pl

From c.j.houldcroft at swansea.ac.uk  Wed Dec  1 16:16:55 2004
From: c.j.houldcroft at swansea.ac.uk (Caroline Houldcroft)
Date: 01 Dec 2004 15:16:55 +0000
Subject: [R] Manipulating contour plots
Message-ID: <1101914215.9117.97.camel@ggwalker.swan.ac.uk>

Dear R users,

I would like to create a filled.contour plot with colour bar and overlay
a map.

I need to create several of these representing variables with different
spatial extents for comparison. When I try to fix the plot area size and
axes limits as follows:

par(fig=c(0,1.0,0.3125,1.0)

filled.contour(x=seq((320/2)-180.5,(480/2)-180.5,len=nrow(oct94)),
y=seq((190/2)-90.5,(240/2)-90.5,len=ncol(oct94)),
z=as.matrix(oct94),
color=heat.colors,
levels=seq(0,30,2),
plot.axes={axis(1,seq(-60,60,by=20));
axis(2,seq(5,30,by=5))})


my definitions were ignored producing a stretched image of the area with
limits defined by the contour plot.  


I have also had difficulties overlaying a map on the same axes. The
following example, taken from the r-help archives, gave the error
"couldn't find function "world""

library(maps)
filled.contour(x=seq((320/2)-180.5,(480/2)-180.5,len=nrow(oct94)),
y=seq((190/2)-90.5,(240/2)-90.5,len=ncol(oct94)),
z=as.matrix(oct94),
color=heat.colors,
levels=seq(0,30,2),
plot.axes={world(ylim=c(5,30),xlim=c(-30,50),add=T);
axis(1);axis(2)})

I would be very grateful for any help with these problems.

Caroline Houldcroft



From jarioksa at sun3.oulu.fi  Wed Dec  1 16:20:24 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 01 Dec 2004 17:20:24 +0200
Subject: [R] step.gam
In-Reply-To: <41ADDEA1.5070505@ipe.csic.es>
References: <41ADDEA1.5070505@ipe.csic.es>
Message-ID: <1101914424.14863.11.camel@biol102145.oulu.fi>

On Wed, 2004-12-01 at 17:09, David Nogu??s wrote:
> Dear R-users:
> 
> Im trying (using gam package) to develop a stepwise analysis. My gam 
> object contains five pedictor variables (a,b,c,d,e,f). I define the 
> step.gam:
> 
> step.gam(gamobject, scope=list("a"= ~s(a,4), "b"= ~s(b,4), "c"= ~s(c,4), 
> "d"= ~s(d,4), "e"= ~s(e,4), "f"= ~s(f,4)))
> 
Your scope doesn't look much like Trevor Hastie's help page. Have you
tried formulating your scope like Hastie tells you to do? That is, for
"a" you should list all possible cases for stepping instead of only one.
That is, something like ..."a" = ~ 1 + a + s(a, 2) + s(a, 4).....

Why do you want to use this kind of stepping, when the standard package
mgcv has a much better way of model building using generalized cross
validation?

Dave Roberts discusses R/S-plus (or mgcv/gam package level) gam fitting
in ecological context at
http://labdsv.nr.usu.edu/splus_R/lab5/lab5.html. You may find some
useful hints here, as Dave is partial to the traditional S-plus gam as
well.

cheers, jari oksanen
 
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/



From deepayan at stat.wisc.edu  Wed Dec  1 16:26:27 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 1 Dec 2004 09:26:27 -0600
Subject: [R] how to add a legend in a barchart
In-Reply-To: <0I8100G17K8FVJ@mail.fudan.edu.cn>
References: <0I8100G17K8FVJ@mail.fudan.edu.cn>
Message-ID: <200412010926.27772.deepayan@stat.wisc.edu>

On Wednesday 01 December 2004 05:43, ronggui wrote:
> i use
> #pic 1
> library(lattice)
> barchart(V1~V2,data,panel=function(x,y,...){
> panel.barchart(x,y,...)
> panel.abline(v=6.76,col="red")},
> )
> to draw a picture.and i want to add a legend to telling others the
> line vertical is x=6.76.so how should i do? legend does not works.
> thank you .
> i have use help.search("legend") but no result i want.

But have you read help(barchart)? Look for 'legend' and 'key'.

What you want is not exactly standard, but you may be satisfied with 

barchart(..., 
         key = list(text = list("6.76"), lines = list(col = "red")))

If not, you can create arbitrary legends using grid.

Deepayan



From j.logsdon at quantex-research.com  Wed Dec  1 16:42:28 2004
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Wed, 1 Dec 2004 15:42:28 +0000 (GMT)
Subject: [R] 2.0.1 compilation problem on Fedora Core 2
Message-ID: <Pine.LNX.4.10.10412011456210.21617-100000@mercury.quantex>

I have a compilation problem on FC2, 2xXeon box.  

The following dialogue output from the end of the compilation illustrates:

[very large snipping sound ...]
* DONE (cluster)
begin installing recommended package foreign
make[2]: *** [foreign.ts] Error 1
make[2]: Leaving directory
`/usr/src/redhat/SOURCES/R-2.0.1/src/library/Recommended'
make[1]: *** [recommended-packages] Error 2
make[1]: Leaving directory
`/usr/src/redhat/SOURCES/R-2.0.1/src/library/Recommended'
make: *** [stamp-recommended] Error 2

There are no error messages in the configuration step - it only says I
can't prepare DVI or PDF versions of the manual.  Fair enough - info will
do quite nicely.

It seems there is a problem trying to install package 'foreign' and
perhaps this leads to the other two errors.

The compiler suite is 3.3.3-7.

I have the full config.out and error.out files stored for inspection if
required.  Nothing appears to be installed.

Any clues?

TIA

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com



From rshah3 at cs.mcgill.ca  Wed Dec  1 16:52:36 2004
From: rshah3 at cs.mcgill.ca (Rohan Shah)
Date: Wed, 1 Dec 2004 10:52:36 -0500 (EST)
Subject: [R] prediction 
Message-ID: <1888.132.216.217.54.1101916356.squirrel@mail.cs.mcgill.ca>

Hi!

I have a dataset of the lifespans (birth/death date) of about 100,000
people. I also have the birth dates of about 1,000,000 people who are
still alive. I also have other information for each of these people
including faculty,state,year graduated etc. I would like to do some
statistical analysis but since the data is factored i'm having trouble. I
looked into using the SVM package to train and then predict lifespans but
it assumes nonfactored data.

Any ideas?



Rohan



From edd at debian.org  Wed Dec  1 17:02:45 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 1 Dec 2004 10:02:45 -0600
Subject: [R] can't install r package on debian due to linker problem
In-Reply-To: <E585EABA11227445B918BFB74C1A4D36015973@sanctum01.sanctumfi.com>
References: <E585EABA11227445B918BFB74C1A4D36015973@sanctum01.sanctumfi.com>
Message-ID: <20041201160245.GA7174@sonny.eddelbuettel.com>

On Wed, Dec 01, 2004 at 01:34:14PM -0000, Robert Sams wrote:
> thank you both. 
> i edited the Makeconf file per brian ripley's instruction and all works well.

Good to know. Generally speaking, you could also make the leap from Debian
stable to testing which does have R 2.0.1 as well as several dozen CRAN
packages, including Hmisc and Design, pre-built. Why not use them?

On a related note, Quantian 0.6.9.2 was just uploaded yesterday, and it
contains _all_ CRAN and BioConductor packages. I will try to post a full
release announcement sometime this eve.

Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From ripley at stats.ox.ac.uk  Wed Dec  1 17:03:21 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 1 Dec 2004 16:03:21 +0000 (GMT)
Subject: [R] 2.0.1 compilation problem on Fedora Core 2
In-Reply-To: <Pine.LNX.4.10.10412011456210.21617-100000@mercury.quantex>
References: <Pine.LNX.4.10.10412011456210.21617-100000@mercury.quantex>
Message-ID: <Pine.LNX.4.61.0412011600480.25323@gannet.stats>

On Wed, 1 Dec 2004, John Logsdon wrote:

> I have a compilation problem on FC2, 2xXeon box.
>
> The following dialogue output from the end of the compilation illustrates:
>
> [very large snipping sound ...]
> * DONE (cluster)
> begin installing recommended package foreign
> make[2]: *** [foreign.ts] Error 1
> make[2]: Leaving directory
> `/usr/src/redhat/SOURCES/R-2.0.1/src/library/Recommended'

Take a look at the file foreign.out in that directory.  (R-devel does this 
better, by cat-ing the file at that point.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Wed Dec  1 17:07:29 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 01 Dec 2004 10:07:29 -0600
Subject: [R] Combined variable names
In-Reply-To: <200412010325.iB13Pa6X361377@atlas.otago.ac.nz>
References: <200412010325.iB13Pa6X361377@atlas.otago.ac.nz>
Message-ID: <41ADEC41.2090604@stat.wisc.edu>

Richard A. O'Keefe wrote:
> Tobias Muhlhofer <t.muhlhofer at lse.ac.uk> wrote:
> 	I am trying to define a large number of variables through a loop
> 	construct.
> 	
> He wants to do
> 	for (i in 1:100) {
> 	    assign(paste("v", i, sep=""), ....something or other...)
> 	}
> 
> This is, of course, a FAQ.  It's such a FAQ that I must have seen it
> once a day for the last several days.
> 
> What I want to know is *WHY* people are doing this?
> What, precisely, does it buy you to have variables called v1...v100
> rather than variables called v[[1]]...v[[100]]?
> Apart from persistent inconvenience, that is?

I believe it is an SPSSism.  It has been many, many years since I last 
used SPSS but I vaguely recall a facility for specifying ranges of 
columns in the form x1-x99.

Duncan Temple Lang's sig file used to have a quote something like 
"Language shapes the way we think" and that applies here.



From vito_ricci at yahoo.com  Wed Dec  1 17:09:31 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 1 Dec 2004 17:09:31 +0100 (CET)
Subject: [R] RE: prediction
Message-ID: <20041201160931.31761.qmail@web41211.mail.yahoo.com>

Try to use neural network for prediction: see package
nnet.

Regards
Vito

you wrote:

Hi!

I have a dataset of the lifespans (birth/death date)
of about 100,000
people. I also have the birth dates of about 1,000,000
people who are
still alive. I also have other information for each of
these people
including faculty,state,year graduated etc. I would
like to do some
statistical analysis but since the data is factored
i'm having trouble. I
looked into using the SVM package to train and then
predict lifespans but
it assumes nonfactored data.

Any ideas?

Rohan


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From das at cshl.edu  Wed Dec  1 17:15:09 2004
From: das at cshl.edu (Rajdeep Das)
Date: Wed, 1 Dec 2004 11:15:09 -0500
Subject: [R] SVM distance
References: <Pine.LNX.4.10.10412011456210.21617-100000@mercury.quantex>
	<Pine.LNX.4.61.0412011600480.25323@gannet.stats>
Message-ID: <02e601c4d7c0$ed6c31c0$6807308f@artney>

Hi,
I am using SVM from e1071 package. I am using RBF kernel. I would like to 
know how I can get "d", the perpendicular distance from a datapoint to the 
hyperplane, that SVM calculates in higher dimensional space to classify it. 
Although, this is not something that people usually use, but for my case I 
like to do something with that distance. Does anybody have any idea how to 
get  it?
Thanks.
Raj



From e.gandouin at univ.u-3mrs.fr  Wed Dec  1 17:36:09 2004
From: e.gandouin at univ.u-3mrs.fr (Emmanuel GANDOUIN)
Date: Wed, 1 Dec 2004 17:36:09 +0100
Subject: [R] depth constrained cluster
Message-ID: <003601c4d7c3$dce890c0$c97b11ac@Aragorn>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041201/9bda61a1/attachment.pl

From r.g.brown at cefas.co.uk  Wed Dec  1 17:46:07 2004
From: r.g.brown at cefas.co.uk (Robert Brown FM CEFAS)
Date: Wed, 1 Dec 2004 16:46:07 -0000
Subject: [R] Protocol for answering basic questions
Message-ID: <3589BC4D64C84341AE0C258244F977A2B60B73@expressa.corp.cefas.co.uk>

I have been following the discussions on 'Reasons not to answer very basic questions in a straightforward way' with interest as someone who is also new to R and has had similar experiences.  As such it with sadness that I note that most seem to agree with the present approach to the responses to basic questions.  I must thank those respondants to my own questions who have been helpful, but there are some whose replies are in my opinion not only unhelpful but actually rude.  Indeed I've now started using Splus instead of R so as to have access to a 'proper' support service.  Indeed, the main thing I've learned from R is a new respect for the values of commercial software and a scepticism regarding free software. In the end my experience of r help is that you get what you pay for. Many of the so called socratic responses (in this list and the wider academic community) can be seen as simply way to avoid additional work of a complete reply. 

Experienced R users don't seem to understand how difficult the program can be to new users. Responding that the questioner should read the 'Introduction to R' or a similar document is like  answering a question for directions to one's house with 'Buy a map'.  Most likely both such questioners have already tried that and are asking because that approach failed.  R is a language and like all languages it is simple to those that understand it and complex to those who do not. Every schoolboy in Spain speaks Spanish, but I know from experience that for most English people it is very difficult to learn Spanish and take years of study.  If I'm asked a question from a novice of a language (be it Spanish or R) do I reply 'consult the dictionary'. I would hope not!  I can tell repondants that whilst many of my basic questions may seem simple it is not for lack of studying the very sources they refer to.  If only learning was so simple.  I suspect that the same is true of most questioners.

I speak as someone with a PhD and many years as a researcher in my speciality as well as someone close to completing a masters in statistics with distinction. As such I am not a total novice and would suggest that if I'm having problems so are many; and it is not a result of lack of study as so many responses seem to suggest.  Indeed it is revealing that several responses suggest that they want to discourage questions so they don't overwhelm r-help.  Understandable but not a recipe to encourage the use of R by other than experts. The R community needs to decide of they really only want expert statisticians users and make this clear if it is the case.  Alternatively if they are to encourage novices the present approach is not the way to do it.

I can appreciate that many of the respondants are busy, but if that is the case it would be better if they didn't reply at all. I was taught many years ago that if you can't say anything nice/useful then to say nothing at all.  Something similar could well be applied to this list.  I must say that some respondants are very helpful; and I thank them.  Leave these simple questions to such people.  Indeed it seems surprising that some exteremely experienced R users choose to reply to these basic messages at all; and it seem it is mostly these people who are rude.  I would have thought it might be better for them to concentrate on complex problems more suited to their skills and interests and leave the simple questions to more sympathetic souls.

Perhaps there is a case for two r help lists catering to basic and advanced questions? Certainly if the R community is serious about appealling to users outside advanced statisticians there is a need for a change of approach in r help and elsewhere.  Russ Ackoff identified much of the failure of management science as due to those who were 'mathematically sophisticated but conceptually naive' and much the same could be said for many in the R community.

Finally, let me once again thank those who have responded helpful to my queries in the past and ask them to continue in that vein; their assistance and effort is greatly appreciated.



  


***********************************************************************************
This email and any attachments are intended for the named recipient(s) only.  Its unauthorised use, distribution, disclosure, storage or copying is not permitted.  If you have received it in error, please destroy all copies and notify the sender.  In messages of a non-business nature, the views and opinions expressed are the author's own and do not necessarily reflect those of the organisation from which it is sent.  All emails may be subject to monitoring.



From pkhomski at wiwi.uni-bielefeld.de  Wed Dec  1 17:49:40 2004
From: pkhomski at wiwi.uni-bielefeld.de (Pavel Khomski)
Date: Wed, 01 Dec 2004 17:49:40 +0100
Subject: (Re: [R] lme in R-2.0.0: Problem with lmeControl)   and parameter
	specification
Message-ID: <41ADF624.7030903@wiwi.uni-bielefeld.de>

Hello!

Thanks a lot to Douglas Bates for your advice.

concerning the lme(...) function i wanted to put four other questions.

1.    in the specification of initial values in the pdMat-constructor i 
probably define a standard deviation (sigma_b) and not a variance 
(sigma_b^2). For instance
       in the Rail  example in
       Pinheiro/Bates on p.81 if i specify a random parameter as 
random=pdIdent(value=lambda<-diag(1000,1),form=~1),   (in S-plus),   
then the call to the lme(...)
       with just 0 iterations produces:
 
 > 
fm1Rail.lme<-lme(travel~1,data=Rail,random=pdIdent(value=lambda<-diag(1000,1),form=~1),control=list(msMaxIter=0,msVerbose=TRUE,niterEM=0))
Iteration:  0 ,  1  function calls, F=  66.37359
Parameters:
[1] -3.453878
Warning messages:
  ITERATION LIMIT REACHED WITHOUT OTHER CONVERGENCE in: ms( ~  - 
logLik(lmeSt,
    lmePars), start = list(lmePars = c(coef(lmeSt))),  ....

if i now print out an estimated std.dev.  for sigma_b   i get:

 > (fm1Rail.lme$sigma)*exp(unlist(fm1Rail.lme$modelStruct))
 reStruct.Rail
      107.6767

 > lambda
   [,1]
[1,] 1000

so that the estimated variance would be 107.6767 ^2 = 11594.27 what is 
much grater then 1000.  But hier we know that with  iterations the
value of variance will reduce  ( and at the convergence the StdDev is 
24.80547  )

so i think that lambda=1000 is specified equal to sigma_b as initial value.



2.    What is the meaning for 0-Iteration?



3.    are the parameters    fixed=beta   and   random=sigma    being 
calculated (just on time) only after all iterations have run, or they 
also be updated
       at every iteration with new value of teta ?   if the latter, how 
can i get them for each run ?



4.    Is it in principle possible to hold a variance components 
parameter, say   sigma_b,   as in Rail-example,   fixed (on specified 
value) through all the
        iteration steps (without changing it) and  only optimize  for   
teta=log(sigma_b/sigma_epsilon)  with fixed known value of sigma_b ?  
        how can it be done  ?

       
Thank you for replay



From foadi at ysbl.york.ac.uk  Wed Dec  1 17:56:16 2004
From: foadi at ysbl.york.ac.uk (James Foadi)
Date: Wed, 1 Dec 2004 16:56:16 +0000
Subject: [R] Protocol for answering basic questions
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B73@expressa.corp.cefas.co.uk>
References: <3589BC4D64C84341AE0C258244F977A2B60B73@expressa.corp.cefas.co.uk>
Message-ID: <200412011656.16358.foadi@ysbl.york.ac.uk>

On Wednesday 01 Dec 2004 4:46 pm, Robert Brown FM CEFAS wrote:

> Understandable but not a recipe to encourage the use of R by other than
> experts. The R community needs to decide of they really only want expert
> statisticians users and make this clear if it is the case.  Alternatively
> if they are to encourage novices the present approach is not the way to do
> it.

I perfectly agree with Robert Brown. Althogh I have been captivated by "R",
and will keep using it, I would appreciate if "R" gurus could make this clear.

Thanks

James
-- 
Dr James Foadi
Structural Biology Laboratory
Department of Chemistry
University of York
YORK YO10 5YW
UK



From simon at stats.gla.ac.uk  Wed Dec  1 18:07:43 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Wed, 1 Dec 2004 17:07:43 +0000 (GMT)
Subject: [R] step.gam
In-Reply-To: <1101914424.14863.11.camel@biol102145.oulu.fi>
References: <41ADDEA1.5070505@ipe.csic.es>
	<1101914424.14863.11.camel@biol102145.oulu.fi>
Message-ID: <Pine.LNX.4.58.0412011654200.12496@moon.stats.gla.ac.uk>

> Dave Roberts discusses R/S-plus (or mgcv/gam package level) gam fitting
> in ecological context at
> http://labdsv.nr.usu.edu/splus_R/lab5/lab5.html. You may find some
> useful hints here, as Dave is partial to the traditional S-plus gam as
> well.

This looks good to me, too. One or two things have changed since it was 
written, though - in particular mgcv does have summary and anova methods, 
and mgcv::gam can handle smooth interactions using s() or te() terms 
(gam::gam can handle smooth interactions using lo() terms, which behave 
more like mgcv s() terms rather than te() terms). 

Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From rkoenker at uiuc.edu  Wed Dec  1 18:22:28 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 1 Dec 2004 11:22:28 -0600
Subject: [R] Protocol for answering basic questions
In-Reply-To: <200412011656.16358.foadi@ysbl.york.ac.uk>
References: <3589BC4D64C84341AE0C258244F977A2B60B73@expressa.corp.cefas.co.uk>
	<200412011656.16358.foadi@ysbl.york.ac.uk>
Message-ID: <93719796-43BD-11D9-A0AF-000A95A7E3AA@uiuc.edu>

Maybe it would be helpful to think of R-help as something more than
the Oracle of Delphi.  Questions, ideally, should  be framed in such a
way that they might lead to improvements in R:  extensions of the code
or, more frequently  clarifications or extensions of the documentation.
Indeed the R-help archive itself serves this function and could 
profitably
be searched prior  to firing off a question to R-help.  As traffic on 
R-help
increases there is a delicate balance that must be maintained in order
to keep knowledgeable users interested in the list.

url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Dec 1, 2004, at 10:56 AM, James Foadi wrote:

> On Wednesday 01 Dec 2004 4:46 pm, Robert Brown FM CEFAS wrote:
>
>> Understandable but not a recipe to encourage the use of R by other 
>> than
>> experts. The R community needs to decide of they really only want 
>> expert
>> statisticians users and make this clear if it is the case.  
>> Alternatively
>> if they are to encourage novices the present approach is not the way 
>> to do
>> it.
>
> I perfectly agree with Robert Brown. Althogh I have been captivated by 
> "R",
> and will keep using it, I would appreciate if "R" gurus could make 
> this clear.
>
> Thanks
>
> James
> -- 
> Dr James Foadi
> Structural Biology Laboratory
> Department of Chemistry
> University of York
> YORK YO10 5YW
> UK
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From FowlerM at mar.dfo-mpo.gc.ca  Wed Dec  1 18:42:28 2004
From: FowlerM at mar.dfo-mpo.gc.ca (Fowler, Mark)
Date: Wed, 01 Dec 2004 13:42:28 -0400
Subject: [R] Protocol for answering basic questions
Message-ID: <1A4AC4BAB9C50A42854582B69B08C0340694D40B@MSGMARBIO05>

Although I agree that sometimes a response to a question seems rude, and
some degree of arrogance asserts itself from time to time (actually appears
to cycle), I don't see what in the nature of the commercial S environment
rectifies this problem. I've been using S since the late 80's, R for less
than a year. The only substantive difference in obtaining assistance for
[statistics-related] problems, that I've perceived, would simply be the
volume of questions. R has a couple of orders of magnitude on S in this
respect, restricting the observation to the period since the last change in
ownership of S (previously S had much more question traffic). But other than
the volume, I really don't see a difference. The only circumstances for
which I've noted an advantage of S over R in terms of responding to
questions are concerned with programming the interface and server
applications, where the problems may often require the knowledge of S
development staff to resolve (or not), and they are pretty good about
intercepting questions that might better have been directed to them. 

If one considers the number of times we see a question we feel was frivolous
for lack of effort, versus the number of times we see a response we feel was
unwarranted for lack of consideration, is there much difference, or are we
maybe as close to the line as we could reasonably expect to be.

>	Mark Fowler
>	Marine Fish Division
>	Bedford Inst of Oceanography
>	Dept Fisheries & Oceans
>	Dartmouth NS Canada
>	fowlerm at mar.dfo-mpo.gc.ca
>


-----Original Message-----
From: Robert Brown FM CEFAS [mailto:r.g.brown at cefas.co.uk] 
Sent: December 1, 2004 12:46 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Protocol for answering basic questions


I have been following the discussions on 'Reasons not to answer very basic
questions in a straightforward way' with interest as someone who is also new
to R and has had similar experiences.  As such it with sadness that I note
that most seem to agree with the present approach to the responses to basic
questions.  I must thank those respondants to my own questions who have been
helpful, but there are some whose replies are in my opinion not only
unhelpful but actually rude.  Indeed I've now started using Splus instead of
R so as to have access to a 'proper' support service.  Indeed, the main
thing I've learned from R is a new respect for the values of commercial
software and a scepticism regarding free software. In the end my experience
of r help is that you get what you pay for. Many of the so called socratic
responses (in this list and the wider academic community) can be seen as
simply way to avoid additional work of a complete reply. 

Experienced R users don't seem to understand how difficult the program can
be to new users. Responding that the questioner should read the
'Introduction to R' or a similar document is like  answering a question for
directions to one's house with 'Buy a map'.  Most likely both such
questioners have already tried that and are asking because that approach
failed.  R is a language and like all languages it is simple to those that
understand it and complex to those who do not. Every schoolboy in Spain
speaks Spanish, but I know from experience that for most English people it
is very difficult to learn Spanish and take years of study.  If I'm asked a
question from a novice of a language (be it Spanish or R) do I reply
'consult the dictionary'. I would hope not!  I can tell repondants that
whilst many of my basic questions may seem simple it is not for lack of
studying the very sources they refer to.  If only learning was so simple.  I
suspect that the same is true of most question!  ers.

I speak as someone with a PhD and many years as a researcher in my
speciality as well as someone close to completing a masters in statistics
with distinction. As such I am not a total novice and would suggest that if
I'm having problems so are many; and it is not a result of lack of study as
so many responses seem to suggest.  Indeed it is revealing that several
responses suggest that they want to discourage questions so they don't
overwhelm r-help.  Understandable but not a recipe to encourage the use of R
by other than experts. The R community needs to decide of they really only
want expert statisticians users and make this clear if it is the case.
Alternatively if they are to encourage novices the present approach is not
the way to do it.

I can appreciate that many of the respondants are busy, but if that is the
case it would be better if they didn't reply at all. I was taught many years
ago that if you can't say anything nice/useful then to say nothing at all.
Something similar could well be applied to this list.  I must say that some
respondants are very helpful; and I thank them.  Leave these simple
questions to such people.  Indeed it seems surprising that some exteremely
experienced R users choose to reply to these basic messages at all; and it
seem it is mostly these people who are rude.  I would have thought it might
be better for them to concentrate on complex problems more suited to their
skills and interests and leave the simple questions to more sympathetic
souls.

Perhaps there is a case for two r help lists catering to basic and advanced
questions? Certainly if the R community is serious about appealling to users
outside advanced statisticians there is a need for a change of approach in r
help and elsewhere.  Russ Ackoff identified much of the failure of
management science as due to those who were 'mathematically sophisticated
but conceptually naive' and much the same could be said for many in the R
community.

Finally, let me once again thank those who have responded helpful to my
queries in the past and ask them to continue in that vein; their assistance
and effort is greatly appreciated.



  


****************************************************************************
*******
This email and any attachments are intended for the named re...{{dropped}}



From chencheva at gmail.com  Wed Dec  1 18:46:38 2004
From: chencheva at gmail.com (Hu Chen)
Date: Thu, 2 Dec 2004 01:46:38 +0800
Subject: [R] how to use print() without column names?
Message-ID: <6f3fc9ee041201094654c1243a@mail.gmail.com>

Hi all,
any guy who knows how to print a data frame without column names printed?
I didn't find a parameter in print() function to turn off this.
Neither in options().
Thanks in advance.



From DupliseaD at dfo-mpo.gc.ca  Wed Dec  1 18:46:30 2004
From: DupliseaD at dfo-mpo.gc.ca (DupliseaD@dfo-mpo.gc.ca)
Date: Wed, 1 Dec 2004 12:46:30 -0500 
Subject: [R] Protocol for answering basic questions
Message-ID: <AAAECF1CF69A704EB103E5284B741CF902CEAB0D@lauimls05.qc.dfo.ca>

I have been a member for only a few days but I find the tone of some
responses are inappropriate for a list dubbing itself a "help list". I also
completely understand that traffic needs to be kept at a modest level to
keep advanced users interested; therefore, I suggest that a second help list
be created to deal with "advanced R help". Belong to both lists if you wish
and filter your email for cursory glances or a detailed reading. Users must
judge themselves the level of their queries and perhaps a note saying
something like "requests to the advanced list are generally made by users
who already have a very good working knowledge of R" or some very rough
benchmark for judging your level like 2 years.

I do not know how much work this would involve or resources available for
this - it is a blind proposal. I think it might deal with many of the
problems both beginner and advanced users have with the present list.


Daniel

________________________
Daniel E. Duplisea
Fisheries and Oceans/P??ches et Oc??ans Canada
Institut Maurice-Lamontagne
850 route de la mer
Mont-Joli, QC
Canada    G5H 3Z4

tel: (418) 775-0881
fax: (418) 775-0740
duplisead at dfo-mpo.gc.ca



-----Original Message-----
From: roger koenker [mailto:rkoenker at uiuc.edu] 
Sent: 1 d??cembre 2004 12:26
To: James Foadi
Cc: Robert Brown FM CEFAS; R mailing list
Subject: Re: [R] Protocol for answering basic questions


Maybe it would be helpful to think of R-help as something more than the
Oracle of Delphi.  Questions, ideally, should  be framed in such a way that
they might lead to improvements in R:  extensions of the code or, more
frequently  clarifications or extensions of the documentation. Indeed the
R-help archive itself serves this function and could 
profitably
be searched prior  to firing off a question to R-help.  As traffic on 
R-help
increases there is a delicate balance that must be maintained in order to
keep knowledgeable users interested in the list.

url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Dec 1, 2004, at 10:56 AM, James Foadi wrote:

> On Wednesday 01 Dec 2004 4:46 pm, Robert Brown FM CEFAS wrote:
>
>> Understandable but not a recipe to encourage the use of R by other
>> than
>> experts. The R community needs to decide of they really only want 
>> expert
>> statisticians users and make this clear if it is the case.  
>> Alternatively
>> if they are to encourage novices the present approach is not the way 
>> to do
>> it.
>
> I perfectly agree with Robert Brown. Althogh I have been captivated by
> "R",
> and will keep using it, I would appreciate if "R" gurus could make 
> this clear.
>
> Thanks
>
> James
> --
> Dr James Foadi
> Structural Biology Laboratory
> Department of Chemistry
> University of York
> YORK YO10 5YW
> UK
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Wed Dec  1 18:48:29 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 1 Dec 2004 09:48:29 -0800 (PST)
Subject: [R] Protocol for answering basic questions
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B73@expressa.corp.cefas.co.uk>
References: <3589BC4D64C84341AE0C258244F977A2B60B73@expressa.corp.cefas.co.uk>
Message-ID: <Pine.A41.4.61b.0412010908010.243432@homer08.u.washington.edu>

On Wed, 1 Dec 2004, Robert Brown FM CEFAS wrote:

> I have been following the discussions on 'Reasons not to answer very 
> basic questions in a straightforward way' with interest as someone who 
> is also new to R and has had similar experiences.  As such it with 
> sadness that I note that most seem to agree with the present approach to 
> the responses to basic questions.

I think the term "basic questions" in this thread is unfortunate. Many of 
the questions at issue are actually quite complex and advanced, but are 
asked very frequently.

If someone says they have read, to pick a frequent example over the past 
few days, FAQ 7.21, and doesn't understand it or can't work out how to 
apply the advice, they are likely to get a friendly and detailed answer. 
I wrote that FAQ answer and I know that it doesn't contain everything you 
might possibly need to know.  On the other hand, if someone doesn't appear 
to have read the FAQ, I will respond "See the FAQ" or (if I remember the 
number "See FAQ 7.21").  Other people may respond more forcefully; they 
are usually people who spend more time than I do on answering questions.

If someone says they can't understand a particular section of "An 
Introduction to R", again, they will get a much more friendly response 
than if they don't appear to have even looked at it.  I learned S-PLUS 
from an earlier version of that document (as a MSc student with no 
statistical qualifications at all), and while it is helpful, it could 
certainly be expanded.

Incidentally, the suggestion elsewhere in this thread that the R community 
should decide whether it wants new users seems to reflect a complete 
misunderstanding of the situation.  The "R community" has no 
decision-making procedure and a hugely diverse range of views on almost 
every topic (except perhaps the relative usefulness of SPSS and R). It 
can't decide anything and probably couldn't agree if it tried to.

My personal view is that a separate mailing list for low-level questions 
would probably not be useful (this issue has been raised before), but I am 
not stopping anyone from setting one up, and I'm sure that if one were 
started the CRAN maintainers would be willing to post information about 
subscribing, link to archives, etc.  The list doesn't have to be hosted by 
Martin Maechler and ETH Zurich just because r-devel and r-help are.


 	-thomas



From tlumley at u.washington.edu  Wed Dec  1 18:50:27 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 1 Dec 2004 09:50:27 -0800 (PST)
Subject: [R] how to use print() without column names?
In-Reply-To: <6f3fc9ee041201094654c1243a@mail.gmail.com>
References: <6f3fc9ee041201094654c1243a@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0412010949310.243432@homer08.u.washington.edu>

On Thu, 2 Dec 2004, Hu Chen wrote:

> Hi all,
> any guy who knows how to print a data frame without column names printed?
> I didn't find a parameter in print() function to turn off this.
> Neither in options().

One option is to use write.table() instead of print().

 	-thomas



From rolf at math.unb.ca  Wed Dec  1 18:52:31 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 1 Dec 2004 13:52:31 -0400 (AST)
Subject: [R] Re: Protocol for answering basic questions
Message-ID: <200412011752.iB1HqV4H010777@erdos.math.unb.ca>


I think that enough bandwidth has been expended on this topic.  Many
people have attempted, patiently, to explain why the protocol to
which the r-help list currently adhers is necessary and close to
optimal.  The thin-skinned whiners who wish to be told ``Yes, dear,
that was an ***excellent*** question'' are not going to listen to or
understand these explanations.  If they wish to take their business
elsewhere, let them.  They won't be missed.  (It will do them less
than no good, in the long run, to do so --- but that's their lookout.)

I don't know if there is a causative relationship, but this whiny
attitude is strongly reminiscent of the philosophy which seems to
prevail in most school systems in the English speaking world whereby
teachers dare not criticize students' work for fear of damaging their
self esteem.

				cheers,

					Rolf Turner
					rolf at math.unb.ca

P. S.  And here I was, all these years, thinking that
       ``self esteem'' was Italian for ``sauna''.

					R. T.



From j.logsdon at quantex-research.com  Wed Dec  1 18:59:01 2004
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Wed, 1 Dec 2004 17:59:01 +0000 (GMT)
Subject: [R] 2.0.1 compilation problem on Fedora Core 2
In-Reply-To: <Pine.LNX.4.61.0412011600480.25323@gannet.stats>
Message-ID: <Pine.LNX.4.10.10412011753370.21617-100000@mercury.quantex>

A useful clue, Brian.  Maybe this is the reason as foreign.ts.out
contains:

* Installing *source* package 'foreign' ...
/usr/src/redhat/SOURCES/R-2.0.1/bin/INSTALL: ./configure: /bin/sh: bad
interpreter: Permission denied
ERROR: configuration failed for package 'foreign'

Does this try and execute from /tmp by any chance?  I had a similar
problem recently because /tmp is mounted (rw,noexec,nosuid) for security
reasons.  I had to alter /etc/fstab temporarily and reboot as /tmp is used
by many things.

The others installed so far have 0 size:

-rw-r--r--  1 root root      0 Dec  1 15:19 VR.ts
-rw-r--r--  1 root root      0 Dec  1 15:19 boot.ts
-rw-r--r--  1 root root    196 Dec  1 15:20 foreign.ts.out
-rw-r--r--  1 root root      0 Dec  1 15:20 cluster.ts

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


On Wed, 1 Dec 2004, Prof Brian Ripley wrote:

> On Wed, 1 Dec 2004, John Logsdon wrote:
> 
> > I have a compilation problem on FC2, 2xXeon box.
> >
> > The following dialogue output from the end of the compilation illustrates:
> >
> > [very large snipping sound ...]
> > * DONE (cluster)
> > begin installing recommended package foreign
> > make[2]: *** [foreign.ts] Error 1
> > make[2]: Leaving directory
> > `/usr/src/redhat/SOURCES/R-2.0.1/src/library/Recommended'
> 
> Take a look at the file foreign.out in that directory.  (R-devel does this 
> better, by cat-ing the file at that point.)
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From tiago17 at socrates.Berkeley.EDU  Wed Dec  1 19:01:22 2004
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Wed, 1 Dec 2004 18:01:22 +0000
Subject: [R] Combined variable names (two concrete suggestions)
Message-ID: <p06100500bdd3a2812285@[83.132.28.212]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041201/ce9b96b9/attachment.pl

From michael.watson at bbsrc.ac.uk  Wed Dec  1 19:14:11 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 1 Dec 2004 18:14:11 -0000
Subject: [R] Re: Protocol for answering basic questions
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B8B4@iahce2knas1.iah.bbsrc.reserved>

Here here!

Getting flamed for asking dumb questions on a public mailing list is all part of growing up and being a man/woman.  We've all been there, and quite frankly, even the most basic questions on R-Help get a decent answer from someone, and all for free, so who cares if you get a bit of rudeness or not!

Next we'll be seeing lawsuits being brought against people on R-Help for not being polite enough.  You couldn't make it up!

M


-----Original Message-----
From:	r-help-bounces at stat.math.ethz.ch on behalf of Rolf Turner
Sent:	Wed 12/1/2004 5:52 PM
To:	r-help at stat.math.ethz.ch
Cc:	
Subject:	[R] Re: Protocol for answering basic questions

I think that enough bandwidth has been expended on this topic.  Many
people have attempted, patiently, to explain why the protocol to
which the r-help list currently adhers is necessary and close to
optimal.  The thin-skinned whiners who wish to be told ``Yes, dear,
that was an ***excellent*** question'' are not going to listen to or
understand these explanations.  If they wish to take their business
elsewhere, let them.  They won't be missed.  (It will do them less
than no good, in the long run, to do so --- but that's their lookout.)

I don't know if there is a causative relationship, but this whiny
attitude is strongly reminiscent of the philosophy which seems to
prevail in most school systems in the English speaking world whereby
teachers dare not criticize students' work for fear of damaging their
self esteem.

				cheers,

					Rolf Turner
					rolf at math.unb.ca

P. S.  And here I was, all these years, thinking that
       ``self esteem'' was Italian for ``sauna''.

					R. T.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jari.oksanen at oulu.fi  Wed Dec  1 19:14:26 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Wed, 01 Dec 2004 20:14:26 +0200
Subject: [R] Protocol for answering basic questions
In-Reply-To: <AAAECF1CF69A704EB103E5284B741CF902CEAB0D@lauimls05.qc.dfo.ca>
References: <AAAECF1CF69A704EB103E5284B741CF902CEAB0D@lauimls05.qc.dfo.ca>
Message-ID: <D5B5C260-43C4-11D9-9C45-000A95C76CA8@oulu.fi>


On 1 Dec 2004, at 19:46, DupliseaD at dfo-mpo.gc.ca wrote:

> I have been a member for only a few days but I find the tone of some
> responses are inappropriate for a list dubbing itself a "help list". I 
> also
> completely understand that traffic needs to be kept at a modest level 
> to
> keep advanced users interested; therefore, I suggest that a second 
> help list
> be created to deal with "advanced R help". Belong to both lists if you 
> wish
> and filter your email for cursory glances or a detailed reading. Users 
> must
> judge themselves the level of their queries and perhaps a note saying
> something like "requests to the advanced list are generally made by 
> users
> who already have a very good working knowledge of R" or some very rough
> benchmark for judging your level like 2 years.
>
> I do not know how much work this would involve or resources available 
> for
> this - it is a blind proposal. I think it might deal with many of the
> problems both beginner and advanced users have with the present list.
>
You may have not been long enough on this list to see that some of the 
old-time gurus have reached a demigod like status. Demigods have all 
rights to be `rude' (that's almost a definition of a demi-deity). That 
said, I do know your sentiments: I'd be afraid to post a question to 
this list. I also remember that I was shocked that the first message I 
sent here got answers from people like V&R (both) and many others, and 
these were friendly and useful answers (although I could have found the 
answer to my question with careful reading of documents -- it was about 
specifying offset in glm).

This is a subscribed mailing list. As such, this is a restrictive list 
with more stringent rules than open newsgroups. Well, newsgroups can be 
really harsh places, too. I don't think that it would be wise to 
establish a parallel novice mailing list. That would add only one extra 
irritation: cross-posting to several lists. However, I do think that 
novice questions could be be better served in a newsgroup (Usenet) than 
in a closed mailing list. There have been several suggestions of 
transforming this mailing list into a newsgroup, but these suggestions 
have been rejected, and rightly. However, if you want to have novice 
group with slacker netiquette, you could try to establish a parallel 
and alternative newsgroup with different emphasis than this mailing 
list. I am sure that many of the greatest gurus wouldn't follow you 
into this newsgroup, but they would keep to this mailing list. If you 
want to  have answers to 'basic', 'silly' or 'simple' questions, you 
don't need them either.

Suggesting a Usenet newsgroup a generation thing. I think some of the 
younger users would prefer a Wiki or a Forum (these are words I've 
seen, but I wouldn't visit places like this, talking about my 
g-g-generation).

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From murdoch at stats.uwo.ca  Wed Dec  1 19:32:13 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 01 Dec 2004 13:32:13 -0500
Subject: [R] Protocol for answering basic questions
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B73@expressa.corp.cefas.co.uk>
References: <3589BC4D64C84341AE0C258244F977A2B60B73@expressa.corp.cefas.co.uk>
Message-ID: <b82sq0dn6ab01euise579dpf1488evd6qe@4ax.com>

On Wed, 1 Dec 2004 16:46:07 -0000, "Robert Brown FM CEFAS"
<r.g.brown at cefas.co.uk> wrote :

> In the end my experience of r help is that you get what you pay for. 

I think this statement is very true, but not necessarily in the way
you meant it.  R and R-help are "free" in the open source sense, and
"free" in that you don't need to pay money to someone to use them, but
they aren't "free" in the sense of requiring no effort to use.  If you
don't devote effort to understanding R and to framing questions that
get good responses in R-help, then you won't get nearly as much value
out of them as if you did.

My own experience is that effort is required with commercial software
too, but I've never really taken advantage of support contracts, so
maybe they really do make things effortlessly easy.

>Many of the so called socratic responses (in this list and the wider academic 
>community) can be seen as simply way to avoid additional work of a complete reply. 

That's definitely part of it.  If a short reference to an FAQ answers
a question, or an RTFM message encourages someone to RTFM, then that's
a good thing.  I get "complete replies" when I call the support desk
of my ISP, and they are almost always completely useless.  I'd rather
get a short response from someone knowledgeable than a long one from
someone reading a script.

>Experienced R users don't seem to understand how difficult the program can be to new users.

I don't think it's valid to generalize like that.  Some do, some
don't.

> Responding that the questioner should read the 'Introduction to R' or a similar 
>document is like  answering a question for directions to one's house with 'Buy a map'.  

I think it's more like answering a question about which bus to take
downtown with directions on how to get a free bus route map.  

>Most likely both such questioners have already tried that and are asking because that approach failed.  

I doubt if that's true.  I think most such questioners are just used
to being customers, being served by someone.  R has no customers.  It
has a community of users and developers who help each other.  One part
of the help is to tell beginners where the resources are that they
should learn from.

Duncan Murdoch



From spencer.graves at pdf.com  Wed Dec  1 19:33:20 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 01 Dec 2004 10:33:20 -0800
Subject: [R] Re: Protocol for answering basic questions
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B8B4@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B8B4@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41AE0E70.1020502@pdf.com>

      Our great-great grandchilren as yet unborn may read some of the 
stupid questions and / or answers that I and perhaps others give from 
time to time.  I'd rather get flamed for saying something stupid in 
public on this list than to continue to provide substandard service to 
the people with whom I work because I perpetrated the same mistake in an 
environment in which no one questioned so effectively my errors. 

      Best Wishes,
      spencer

michael watson (IAH-C) wrote:

>Here here!
>
>Getting flamed for asking dumb questions on a public mailing list is all part of growing up and being a man/woman.  We've all been there, and quite frankly, even the most basic questions on R-Help get a decent answer from someone, and all for free, so who cares if you get a bit of rudeness or not!
>
>Next we'll be seeing lawsuits being brought against people on R-Help for not being polite enough.  You couldn't make it up!
>
>M
>
>
>-----Original Message-----
>From:	r-help-bounces at stat.math.ethz.ch on behalf of Rolf Turner
>Sent:	Wed 12/1/2004 5:52 PM
>To:	r-help at stat.math.ethz.ch
>Cc:	
>Subject:	[R] Re: Protocol for answering basic questions
>
>I think that enough bandwidth has been expended on this topic.  Many
>people have attempted, patiently, to explain why the protocol to
>which the r-help list currently adhers is necessary and close to
>optimal.  The thin-skinned whiners who wish to be told ``Yes, dear,
>that was an ***excellent*** question'' are not going to listen to or
>understand these explanations.  If they wish to take their business
>elsewhere, let them.  They won't be missed.  (It will do them less
>than no good, in the long run, to do so --- but that's their lookout.)
>
>I don't know if there is a causative relationship, but this whiny
>attitude is strongly reminiscent of the philosophy which seems to
>prevail in most school systems in the English speaking world whereby
>teachers dare not criticize students' work for fear of damaging their
>self esteem.
>
>				cheers,
>
>					Rolf Turner
>					rolf at math.unb.ca
>
>P. S.  And here I was, all these years, thinking that
>       ``self esteem'' was Italian for ``sauna''.
>
>					R. T.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From cyracules at yahoo.co.uk  Wed Dec  1 19:47:51 2004
From: cyracules at yahoo.co.uk (John)
Date: Wed, 1 Dec 2004 18:47:51 +0000 (GMT)
Subject: [R] [BASIC] Solution of creating a sequence of object names
In-Reply-To: <1101914257.3389.75.camel@ndmpc126.ihs.ox.ac.uk>
Message-ID: <20041201184751.35761.qmail@web26307.mail.ukl.yahoo.com>

Thanks a lot, Adai. 
I am sure that your tip is very useful for those who
are not familiar with 'list'.

What is good for using this kind of help list is that
you can learn 'additional' info and tips from 'kind'
users, which you don't usually expect by reading basic
documentations. They are by no means explaining
everything in the best understandable way(in fact,
this is why we have dozens of textbooks in every
subject).

I want to add one more small tip for creating object
names with numbering. I wanted to generate file names
in a following format: 'my01', 'my02', ... , 'my10'.
I used the function, 'formatC', with two arguments
'width' and 'flag'.

Here is the code,

> formatC(99, width=3, flag="0")
[1] "099"
>
> for ( i in 1:10 ) {
+ assign(paste("my", formatC(i, width=2, flag="0"),
sep=""), NULL)
+ }
>
> ls(pat="^my")
 [1] "my01" "my02" "my03" "my04" "my05" "my06" "my07"
"my08"
 [9] "my09" "my10"
> 

But, I do not know if this is covered in basic
documentations and if this is a standard way to do the
job. I'd be interested in learning more about this.

Regards,

John


 --- Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
wrote: 
> Yes, usability is an important aspect too. For
> programming efficiency
> sake I would suggest a list. Simplified example of
> the 3rd solution (by
> James Holtman) :
> 
> mylist <-list(NULL)
> mylist <-list(NULL)
> for(i in 1:3) mylist[[ i ]] <- i
> mylist
> [[1]]
> [1] 1
> 
> [[2]]
> [1] 2
> 
> [[3]]
> [1] 3
> 
> 
> Alternative, you can name the indexes
> 
> mylist <-list(NULL)
> for(i in 1:3) mylist[[  letters[i]  ]] <- i
> 
> mylist
> [[1]]
> NULL
> 
> $a
> [1] 1
> 
> $b
> [1] 2
> 
> $c
> [1] 3
> 
> 
> And these can be called by 
> 
> mylist$b
> [1] 2
> 
> mylist[["c"]]
> [1] 3
> 
> Keep in mind that you can put and assign different
> classes into a list
> which makes them very handy.
> 
> a      <- list(NULL)
> a[[1]] <- "Created on 1st December"
> a[[2]] <- c(1,2,3)
> a[[3]] <- matrix( 1:4, nc=2 )
> 
> 
> 
> On Mon, 2004-11-29 at 22:53, John wrote:
> > It was enough for me to use the 'assign' function
> > alone. But I'll remember the 'get' function for
> future
> > reference. Thanks a lot for the note.
> > 
> > John
> >  
> > 
> >  --- bogdan romocea <br44114 at yahoo.com> wrote: 
> > > You may be missing something. After you create
> all
> > > those objects,
> > > you'll want to use them. Use get():
> > > for (i in 1:10) ...
> get(paste("object",i,sep=""))
> > > ... 
> > > It took me about a week to find out how to do
> this.
> > > I waited for a
> > > few days, but before I got to ask this
> basic/rtfm
> > > question, someone
> > > else - fortunately :-) - did.
> > > 
> > > HTH,
> > > b.
> > > 
> > > 
> > > -----Original Message-----
> > > From: John [mailto:cyracules at yahoo.co.uk]
> > > Sent: Monday, November 29, 2004 4:03 PM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] [BASIC] Solution of creating a
> sequence
> > > of object names
> > > 
> > > 
> > > Dear R-users,
> > > 
> > > I state that this is for beginners, so you may
> > > ignore
> > > this in order not to be irritated.
> > > 
> > > By the way, "patience" is another important
> thing,
> > > together with "kindness", we should keep in mind
> > > when
> > > we teach students and our own children as Jim
> Lemon
> > > pointed out well in the context of the Socratic
> > > method. You may know that being kind does not
> mean 
> > > giving "spoonfed" answers to questioners.
> > > 
> > > ---------------------
> > > 
> > > I was asked for the solution of my problem, and
> a
> > > couple of answers were given to me in private
> > > emails.
> > > I am not sure if it was a mere accident. I post
> them
> > > now, without their permission, for those who are
> > > interested in learning them. So if you're happy
> to
> > > know the solution, thanks should go to the
> person
> > > concerned. I thank all the three people named
> below.
> > > 
> > > (1) my solution after reading the R-FAQ 7.21 as
> Uwe
> > > Ligges pointed out
> > > 
> > > > for ( i in 1:10 ) {
> > > + assign(paste("my.file.", i, sep=""), NULL)
> > > + }
> > > >
> > > 
> > > (2) Adai Ramasamy's solution
> > > 
> > > > for(obj in paste("my.ftn", 1:10, sep=""))
> > > assign(obj, NULL)
> > > > 
> > > ### or 
> > > > 
> > > > for(i in 1:10) assign(paste("my.ftn", i,
> sep=""),
> > > NULL)
> > > >
> > > 
> > > (3) James Holtman's solution
> > > 
> > > # For example, if you want to generate 10 groups
> 
> > > # of 5 random numbers and store them 
> > > # under then names "GRPn" where n is 1 -> 10, 
> > > # the following can be used:
> > > #
> > > > Result <- list()      # create the list
> > > > for (i in 1:10) Result[[paste("GRP", i,
> sep='')]]
> > > <-
> > > runif(5)   # store each result
> > > > Result    # print out the data
> > > $GRP1
> > > [1] 0.2655087 0.3721239 0.5728534 0.9082078
> > > 0.2016819
> > > 
> > > $GRP2
> > > [1] 0.89838968 0.94467527 0.66079779 0.62911404
> > > 0.06178627
> > > 
> > > $GRP3
> > > [1] 0.2059746 0.1765568 0.6870228 0.3841037
> > > 0.7698414
> > > 
> > > $GRP4
> > > [1] 0.4976992 0.7176185 0.9919061 0.3800352
> > > 0.7774452
> > > 
> > > $GRP5
> > > [1] 0.9347052 0.2121425 0.6516738 0.1255551
> > > 0.2672207
> > > 
> > > $GRP6
> > > [1] 0.38611409 0.01339033 0.38238796 0.86969085
> > > 0.34034900
> > > 
> > > $GRP7
> > > [1] 0.4820801 0.5995658 0.4935413 0.1862176
> > > 0.8273733
> > > 
> > > $GRP8
> > > [1] 0.6684667 0.7942399 0.1079436 0.7237109
> > > 0.4112744
> > > 
> > > $GRP9
> > > [1] 0.8209463 0.6470602 0.7829328 0.5530363
> > > 0.5297196
> > > 
> > > $GRP10
> > > [1] 0.78935623 0.02333120 0.47723007 0.73231374
> > > 0.69273156
> > > 
> > > >
> > > 
> > > Regards,
> > > 
> > > John
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > 
> > > 
> > > 
> > > 	
> > > 		
> > > __________________________________ 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Wed Dec  1 20:15:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 1 Dec 2004 19:15:16 +0000 (GMT)
Subject: [R] 2.0.1 compilation problem on Fedora Core 2
In-Reply-To: <Pine.LNX.4.10.10412011753370.21617-100000@mercury.quantex>
References: <Pine.LNX.4.10.10412011753370.21617-100000@mercury.quantex>
Message-ID: <Pine.LNX.4.61.0412011911390.27491@gannet.stats>

On Wed, 1 Dec 2004, John Logsdon wrote:

> A useful clue, Brian.  Maybe this is the reason as foreign.ts.out
> contains:
>
> * Installing *source* package 'foreign' ...
> /usr/src/redhat/SOURCES/R-2.0.1/bin/INSTALL: ./configure: /bin/sh: bad
> interpreter: Permission denied
> ERROR: configuration failed for package 'foreign'
>
> Does this try and execute from /tmp by any chance?  I had a similar

Yes, it does, a configure script.  I believe you can alter that by setting 
TMPDIR to point somewhere else.  That's a pretty esoteric situation: why 
is /tmp so special to you?

> problem recently because /tmp is mounted (rw,noexec,nosuid) for security
> reasons.  I had to alter /etc/fstab temporarily and reboot as /tmp is used
> by many things.
>
> The others installed so far have 0 size:
>
> -rw-r--r--  1 root root      0 Dec  1 15:19 VR.ts
> -rw-r--r--  1 root root      0 Dec  1 15:19 boot.ts
> -rw-r--r--  1 root root    196 Dec  1 15:20 foreign.ts.out
> -rw-r--r--  1 root root      0 Dec  1 15:20 cluster.ts
>
> John
>
> John Logsdon                               "Try to make things as simple
> Quantex Research Ltd, Manchester UK         as possible but not simpler"
> j.logsdon at quantex-research.com              a.einstein at relativity.org
> +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
>
>
> On Wed, 1 Dec 2004, Prof Brian Ripley wrote:
>
>> On Wed, 1 Dec 2004, John Logsdon wrote:
>>
>>> I have a compilation problem on FC2, 2xXeon box.
>>>
>>> The following dialogue output from the end of the compilation illustrates:
>>>
>>> [very large snipping sound ...]
>>> * DONE (cluster)
>>> begin installing recommended package foreign
>>> make[2]: *** [foreign.ts] Error 1
>>> make[2]: Leaving directory
>>> `/usr/src/redhat/SOURCES/R-2.0.1/src/library/Recommended'
>>
>> Take a look at the file foreign.out in that directory.  (R-devel does this
>> better, by cat-ing the file at that point.)
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From biotopia at biotopia.info  Wed Dec  1 20:47:40 2004
From: biotopia at biotopia.info (Alexander Keller)
Date: Wed, 1 Dec 2004 20:47:40 +0100
Subject: [R] chisq.test probabilities method unclear
Message-ID: <DBE4C719-43D1-11D9-811C-000D93679632@biotopia.info>

Hi list,

i've got a question about the chisq.test function.
in the use of the "given probabilities" method  (p= ...), normally 
there should be typed in probabilities in the range of 0 to 1 with the 
absolute sum of 1.0 (r-help)
But it is possible to use probabilities > than 1. or the sum <1.! 
without any warning message

Ok, now the question, what does r calcutate in these cases,
this doesn't make sense in my (poor statistical) view.

i thought it might calculate relations, but it differs in the results
(example: of typing p=c(1,1,1,1), p=c(6,6,6,6), 
p=c(0.25,0.25,0.25,0.25))

can someone tell my about this chisq method, and perhaps show me an 
explaining example?

thank you,
Alex Keller



From RBaskin at ahrq.gov  Wed Dec  1 20:48:20 2004
From: RBaskin at ahrq.gov (Baskin, Robert)
Date: Wed, 1 Dec 2004 14:48:20 -0500 
Subject: [R] Protocol for answering basic questions
Message-ID: <6BCD3F430455B1418750004BCD27925902926430@exchange2.ahrq.gov>

> number.of.years.using.R * runif(1)
[1] 1.064863
> 
I waited this many hours before responding:)


First, let me say thank you very much to the R team for ...the software
...the help-list ...other intangibles.  I am a relatively new R user and I
am struggling in my own way to learn R.  I follow the list regularly for
several reasons.  No matter what kind of answers I see, I think the benefits
of R far outweigh any irritation with any responder - this environment is an
incredible gift.  Bytheway - responses to my questions on this list have
been completely helpful and it has been clear that the responders had taken
time to respond thoughtfully and fully.


Second, I know that the list(s) maintainers have a difficult job and really
deserve a vote of thanks, and that the developers are really spending a huge
amount of time already, but is there some possibility that an "environment"
could be created so that a person such as myself could ask a discussion
question without generating unnecessary email traffic for the people who
should be spending their time working on developing the software?  Splitting
off a beginner list has been suggested before but I don't think this is the
answer - of course I don't have a good answer.  Also, not having 'expert
review' of answers is totally unacceptable.  But there is getting to be a
problem with bandwidth here (which I just incremented by 1).


Third, it appears to me - and I do say here that I am not necessarily
qualified to judge - that there might be some inconsistency in the way
responses are handled.  I know this is not well phrased but there has been
somewhat of an example the last few days that I am thinking about.  There
have been several questions asking how to construct variables named v1-v10
(for example).  The very useful (I don't think rude but what do I know)
replies have typically been along the lines of 'this is FAQ whatever *but
this isn't a good way to do things in R*'.  The first time I saw this reply
it was extremely helpful to ME because of the *don't do it this way in R*
part - no matter how rude it might have been perceived by the person who
wrote the question.  The *don't do it this way in R* part made me think
about some code I was working on.  Sandwiched in the middle of these
excruciatingly similar questions was a question about 'how do I write a for
loop to take a crosstab over v1-v10'.  There was a perfectly good response
with a for loop but no response about options to avoid a for loop - maybe it
wasn't appropriate in the situation but what do I know - so I wrote some
fake data and tried xtabs(cbind(v1,v2) ~ predictor) and it worked:)  Perhaps
I should have sent a question to the list about this - a year ago I would
not have done so because of thin-skinnedness - but I didn't do it now
because I am getting seriously concerned about bandwidth.

**I would like to respectfully say that it is important to ME and to other
beginners that the gurus provide the 'don't do it this way in R' pointers
(please beat the SAS out of me:)


After a lot of rambling, my main concerns are:
* learning good R programming techniques
* access to the R environment (including r-help)
* bandwidth on this list
I regret that I cannot be as articulate as the responses from the R team
members that I have seen on this list.

Thanks for R
bob:)



-----Original Message-----
From: Robert Brown FM CEFAS [mailto:r.g.brown at cefas.co.uk] 
Sent: Wednesday, December 01, 2004 11:46 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Protocol for answering basic questions


I have been following the discussions on 'Reasons not to answer very basic
questions in a straightforward way' with interest as someone who is also new
to R and has had similar experiences.  As such it with sadness that I note
that most seem to agree with the present approach to the responses to basic
questions.  I must thank those respondants to my own questions who have been
helpful, but there are some whose replies are in my opinion not only
unhelpful but actually rude.  Indeed I've now started using Splus instead of
R so as to have access to a 'proper' support service.  Indeed, the main
thing I've learned from R is a new respect for the values of commercial
software and a scepticism regarding free software. In the end my experience
of r help is that you get what you pay for. Many of the so called socratic
responses (in this list and the wider academic community) can be seen as
simply way to avoid additional work of a complete reply. 

Experienced R users don't seem to understand how difficult the program can
be to new users. Responding that the questioner should read the
'Introduction to R' or a similar document is like  answering a question for
directions to one's house with 'Buy a map'.  Most likely both such
questioners have already tried that and are asking because that approach
failed.  R is a language and like all languages it is simple to those that
understand it and complex to those who do not. Every schoolboy in Spain
speaks Spanish, but I know from experience that for most English people it
is very difficult to learn Spanish and take years of study.  If I'm asked a
question from a novice of a language (be it Spanish or R) do I reply
'consult the dictionary'. I would hope not!  I can tell repondants that
whilst many of my basic questions may seem simple it is not for lack of
studying the very sources they refer to.  If only learning was so simple.  I
suspect that the same is true of most question!  ers.

I speak as someone with a PhD and many years as a researcher in my
speciality as well as someone close to completing a masters in statistics
with distinction. As such I am not a total novice and would suggest that if
I'm having problems so are many; and it is not a result of lack of study as
so many responses seem to suggest.  Indeed it is revealing that several
responses suggest that they want to discourage questions so they don't
overwhelm r-help.  Understandable but not a recipe to encourage the use of R
by other than experts. The R community needs to decide of they really only
want expert statisticians users and make this clear if it is the case.
Alternatively if they are to encourage novices the present approach is not
the way to do it.

I can appreciate that many of the respondants are busy, but if that is the
case it would be better if they didn't reply at all. I was taught many years
ago that if you can't say anything nice/useful then to say nothing at all.
Something similar could well be applied to this list.  I must say that some
respondants are very helpful; and I thank them.  Leave these simple
questions to such people.  Indeed it seems surprising that some exteremely
experienced R users choose to reply to these basic messages at all; and it
seem it is mostly these people who are rude.  I would have thought it might
be better for them to concentrate on complex problems more suited to their
skills and interests and leave the simple questions to more sympathetic
souls.

Perhaps there is a case for two r help lists catering to basic and advanced
questions? Certainly if the R community is serious about appealling to users
outside advanced statisticians there is a need for a change of approach in r
help and elsewhere.  Russ Ackoff identified much of the failure of
management science as due to those who were 'mathematically sophisticated
but conceptually naive' and much the same could be said for many in the R
community.

Finally, let me once again thank those who have responded helpful to my
queries in the past and ask them to continue in that vein; their assistance
and effort is greatly appreciated.



  


****************************************************************************
*******
This email and any attachments are intended for the named re...{{dropped}}



From spencer.graves at pdf.com  Wed Dec  1 21:00:13 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 01 Dec 2004 12:00:13 -0800
Subject: [R] Protocol for answering basic questions
In-Reply-To: <6BCD3F430455B1418750004BCD27925902926430@exchange2.ahrq.gov>
References: <6BCD3F430455B1418750004BCD27925902926430@exchange2.ahrq.gov>
Message-ID: <41AE22CD.8020007@pdf.com>



Baskin, Robert wrote:

>>number.of.years.using.R * runif(1)
>>    
>>
>[1] 1.064863
>  
>
><snip>
>
>After a lot of rambling, my main concerns are:
>* learning good R programming techniques
>
         Apart from the free documentation at "www.r-project.org" and 
this list, the following books have been very helpful for me: 

      Venables & Ripley (2002) Modern Applied Statistics with S 
(Springer) and (2000) S Programming (Springer). 

      Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus 
(Springer). 

      I'm sure there are many other excellent books, but I am not 
sufficiently familiar with them to comment. 

      hope this helps.  spencer graves      

><snip>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From hb at maths.lth.se  Wed Dec  1 21:02:46 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 1 Dec 2004 21:02:46 +0100
Subject: [R] Protocol for answering basic questions
In-Reply-To: <D5B5C260-43C4-11D9-9C45-000A95C76CA8@oulu.fi>
Message-ID: <003601c4d7e0$bbb9f930$5d0040d5@hblaptop>

PROBLEM:
To many "beginners questions" are send to the r-help list cluttering up the
mailbox making it hard to see "real" questions.

OBJECTIVE:
To get fewer "beginners questions".

CURRENT SOLUTION:
Being "rude". I don't like the word rude here, but that's the word used in
this thread.

Will we get less beginners questions by being rude? It will certainly
prevent people from doing the same "mistake" again, but it does obviously
not prevent (some? most? any?) *first-time* submitters. 

The first "PLEASE do read the posting guide!" you will see is when you sign
up (https://stat.ethz.ch/mailman/listinfo/r-help), but that, I bet, is
missed by many eagerly scrolling down to fill in their email adress (I doubt
there is a solution to this). The second one is when they start received
messages from the list, which might be in a reply to your own very first
question asking you to "PLEASE do read the posting guide!".

TECHNICAL SOLUTION?
1) Is it possible in MailMan to force beginners to read the FAQ before
their, say, first ten messages get through to the r-help list? 

2) In addition, it could be useful to anyone who replies to see for how long
the person has been on the list, or how many questions he or she has asked
and how many questions he or she has answered. Having this information, in
addition to helping you to answer a question, I assume the one who receives
an answer could see what "experience" the answerer has. Of course, counters
are just proxies, but it would indeed work almost automatically.

I skimmed through the MailMan pages/docs
(http://www.gnu.org/software/mailman/), but could not find anything on this.
[ Maybe I should send a "beginners question" to their list(s!) ;) ] 

3) I have been thinking to do a first proof-of-concept of the following many
time. It should be straightforward to have a popup message (tcltk or at the
prompt) when starting R giving you a random tip or FAQ. Experiences users
will know how to turn this off (hmm, maybe this will generate "How can I
turn of the tip/FAQ message?" questions). I'll see if I ever get the time to
do this. I believe this would be useful not only for basic R, but for many
large package.

Best wishes

Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jari Oksanen
> Sent: Wednesday, December 01, 2004 7:14 PM
> To: DupliseaD at dfo-mpo.gc.ca
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Protocol for answering basic questions
> 
> 
> 
> On 1 Dec 2004, at 19:46, DupliseaD at dfo-mpo.gc.ca wrote:
> 
> > I have been a member for only a few days but I find the 
> tone of some 
> > responses are inappropriate for a list dubbing itself a 
> "help list". I 
> > also completely understand that traffic needs to be kept at 
> a modest 
> > level to
> > keep advanced users interested; therefore, I suggest that a second 
> > help list
> > be created to deal with "advanced R help". Belong to both 
> lists if you 
> > wish
> > and filter your email for cursory glances or a detailed 
> reading. Users 
> > must
> > judge themselves the level of their queries and perhaps a 
> note saying
> > something like "requests to the advanced list are generally made by 
> > users
> > who already have a very good working knowledge of R" or 
> some very rough
> > benchmark for judging your level like 2 years.
> >
> > I do not know how much work this would involve or resources 
> available
> > for
> > this - it is a blind proposal. I think it might deal with 
> many of the
> > problems both beginner and advanced users have with the 
> present list.
> >
> You may have not been long enough on this list to see that 
> some of the 
> old-time gurus have reached a demigod like status. Demigods have all 
> rights to be `rude' (that's almost a definition of a 
> demi-deity). That 
> said, I do know your sentiments: I'd be afraid to post a question to 
> this list. I also remember that I was shocked that the first 
> message I 
> sent here got answers from people like V&R (both) and many 
> others, and 
> these were friendly and useful answers (although I could have 
> found the 
> answer to my question with careful reading of documents -- it 
> was about 
> specifying offset in glm).
> 
> This is a subscribed mailing list. As such, this is a 
> restrictive list 
> with more stringent rules than open newsgroups. Well, 
> newsgroups can be 
> really harsh places, too. I don't think that it would be wise to 
> establish a parallel novice mailing list. That would add only 
> one extra 
> irritation: cross-posting to several lists. However, I do think that 
> novice questions could be be better served in a newsgroup 
> (Usenet) than 
> in a closed mailing list. There have been several suggestions of 
> transforming this mailing list into a newsgroup, but these 
> suggestions 
> have been rejected, and rightly. However, if you want to have novice 
> group with slacker netiquette, you could try to establish a parallel 
> and alternative newsgroup with different emphasis than this mailing 
> list. I am sure that many of the greatest gurus wouldn't follow you 
> into this newsgroup, but they would keep to this mailing list. If you 
> want to  have answers to 'basic', 'silly' or 'simple' questions, you 
> don't need them either.
> 
> Suggesting a Usenet newsgroup a generation thing. I think some of the 
> younger users would prefer a Wiki or a Forum (these are words I've 
> seen, but I wouldn't visit places like this, talking about my 
> g-g-generation).
> 
> cheers, jari oksanen
> --
> Jari Oksanen, Oulu, Finland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ray at mcs.vuw.ac.nz  Wed Dec  1 21:06:42 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Thu, 2 Dec 2004 09:06:42 +1300 (NZDT)
Subject: [R] Manipulating contour plots
Message-ID: <200412012006.iB1K6gtb002772@tahi.mcs.vuw.ac.nz>

From: Caroline Houldcroft <c.j.houldcroft at swansea.ac.uk>
Date: 01 Dec 2004 15:16:55 +0000

> I would like to create a filled.contour plot with colour bar and overlay
> a map.
> 
> I need to create several of these representing variables with different
> spatial extents for comparison. When I try to fix the plot area size and
> axes limits as follows:
> 
> par(fig=c(0,1.0,0.3125,1.0)
> 
> filled.contour(x=seq((320/2)-180.5,(480/2)-180.5,len=nrow(oct94)),
> y=seq((190/2)-90.5,(240/2)-90.5,len=ncol(oct94)),
> z=as.matrix(oct94),
> color=heat.colors,
> levels=seq(0,30,2),
> plot.axes={axis(1,seq(-60,60,by=20));
> axis(2,seq(5,30,by=5))})
> 
> 
> my definitions were ignored producing a stretched image of the area with
> limits defined by the contour plot.  
> 
I cannot answer this part of the question because your example is not
reproducible (Error in nrow(oct94) : Object "oct94" not found) [and the
first line contains a syntax error.]
> 
> I have also had difficulties overlaying a map on the same axes. The
> following example, taken from the r-help archives, gave the error
> "couldn't find function "world""
> 
> library(maps)
> filled.contour(x=seq((320/2)-180.5,(480/2)-180.5,len=nrow(oct94)),
> y=seq((190/2)-90.5,(240/2)-90.5,len=ncol(oct94)),
> z=as.matrix(oct94),
> color=heat.colors,
> levels=seq(0,30,2),
> plot.axes={world(ylim=c(5,30),xlim=c(-30,50),add=T);
> axis(1);axis(2)})
> 
The world() function is not part of the maps package, it is in the
fields package.  If you want to use the maps package, try:
map("world", ylim=c(5,30), xlim=c(-30, 50), add=T)

Hope this helps,
Ray Brownrigg



From rolf at math.unb.ca  Wed Dec  1 21:07:26 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 1 Dec 2004 16:07:26 -0400 (AST)
Subject: [R] chisq.test probabilities method unclear
Message-ID: <200412012007.iB1K7QPw018070@erdos.math.unb.ca>

Alexander Keller wrote:

> i've got a question about the chisq.test function.  in the use of the
> "given probabilities" method  (p= ...), normally there should be
> typed in probabilities in the range of 0 to 1 with the absolute sum
> of 1.0 (r-help) But it is possible to use probabilities > than 1. or
> the sum <1.!  without any warning message
	.
	.
	.
> Ok, now the question, what does r calcutate in these cases,
> this doesn't make sense in my (poor statistical) view.
>
> can someone tell my about this chisq method, and perhaps show me an 
> explaining example?

Take a look at the code; it just sets E <- n*p, and then calculates
sum((x-E)^2/E).  This sum is calculable even if p is not a vector of
probabilities.

And you are quite right; it ***doesn't*** make any sense in
those circumstances.

This might be termed a bug; of course only a silly user would supply
a p argument that wasn't a vector of probabilities .... but there are
a lot of silly users out there/here! :-)

(A p could arise from some other calculations where things could
go wrong in an unforseen way ....)

At the very least the code fails to practice ``safe statistical
computing''.

IMHO there ought be a check in the code to make sure that the vector
p makes sense --- perhaps renormalizing it to sum to 1 (if all
entries are positive) along the lines of sample().

This would be very easy to write --- I'd volunteer, except that
I'm sure that the R core team would disdain my assistance.

			cheers,

				Rolf Turner
				rolf at math.unb.ca



From Robert at sanctumfi.com  Wed Dec  1 21:22:20 2004
From: Robert at sanctumfi.com (Robert Sams)
Date: Wed, 1 Dec 2004 20:22:20 -0000
Subject: [R] Protocol for answering basic questions
Message-ID: <E585EABA11227445B918BFB74C1A4D36015976@sanctum01.sanctumfi.com>

> Experienced R users don't seem to understand how difficult the program can be to new users. Responding that the questioner > should read the 'Introduction to R' or a similar document is like answering a question for directions to one's house with 
> 'Buy a map'.

from the posting guide:

"Good manners: Remember that customs differ. Some people are very direct. Others surround everything they say with hedges and apologies. Be tolerant. Rudeness is never warranted, but sometimes `read the manual' is the appropriate response. Don't waste time discussing such matters on the list."

indeed. when in rome...

robert



From dyang at NRCan.gc.ca  Wed Dec  1 21:32:29 2004
From: dyang at NRCan.gc.ca (Yang, Richard)
Date: Wed, 1 Dec 2004 15:32:29 -0500 
Subject: [R] gnls(0 error: invalid variable type
Message-ID: <F0E0B899CB43D5118D220002A55113CF04FE56BC@s2-edm-r1.nofc.cfs.nrcan.gc.ca>

Dear R-helpers;

While using gnls() to fit a function 	

         > Gbht0t.gnls <- gnls(h2 ~ Rht(b0, b1, b2, h1,t1, t2), data=gbht10,
+     params=list(b0 + b1 + b2 ~ Sisp -1), start=c(strssb0,strssb1,strssb2))

I encountered an error: 
    "Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  :  invalid variable type " 

Rht is a defined function to be fitted  and initial values are stored in
three vectors strssb0-b2. The error occurred at the gnls() source line:

	dataMod <- do.call("model.frame", mfArgs)

All variables in the data frame are listed below. By examining those 5
variables used in the function, I failed to spot any offending one: t1, t2,
h1, and h2 are all numeric and Sisp is a factor of 15 levels. 
> str(gbht10)
`data.frame':   2400 obs. of  15 variables:
 $ sidx44: num  17.8 17.8 17.8 17.8 17.8 ...
 $ tree  : int  1 1 1 1 1 1 1 1 2 2 ...
 $ blk   : int  1 1 1 1 1 1 1 1 1 1 ...
 $ sp    : int  200 200 200 200 200 200 200 200 200 200 ...
 $ tn    : int  8 8 8 8 8 8 8 8 19 19 ...
 $ re    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ si    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ tdbh  : num  11.4 11.4 11.4 11.4 11.4 ...
 $ t1    : int  7 9 14 19 24 29 34 39 7 9 ...
 $ h1    : num  0.5 0.8 2.1 4 6.3 8.4 10.1 11.8 0.5 0.9 ...
 $ t2    : int  9 14 19 24 29 34 39 44 9 14 ...
 $ h2    : num  0.8 2.1 4 6.3 8.4 10.1 11.8 12.6 0.9 2.1 ...
 $ Site  : chr  "L" "L" "L" "L" ...
 $ Sp    : chr  "1" "1" "1" "1" ...
 $ Sisp  : chr  "L1" "L1" "L1" "L1" ...

What puzzles me most is the same function was successfully run a few days
back. The original data frame was modified  by adding an additional column.
Here are bits of info from the previous run (Gbht02.gnls):

 > Gbht02.gnls$call
gnls(model = h2 ~ Rht(b0, b1, b2, h1, t1, t2), data = gbht10, 
    params = list(b0 + b1 + b2 ~ Sisp - 1), start = c(strssb0, 
        strssb1, strssb2))
> Gbht02.gnls$coef[1:5]
b0.SispH1 b0.SispH2 b0.SispH3 b0.SispH4 b0.SispH5 
0.9354108 0.8420247 0.8681101 0.8998161 0.7892491 

Any insight and ideas of the error message? I use R2.0.0 on XP. 

TIA,

Richard Yang



From damian.cirelli at maine.edu  Wed Dec  1 21:50:13 2004
From: damian.cirelli at maine.edu (=?ISO-8859-1?Q?Dami=E1n_Cirelli?=)
Date: Wed, 01 Dec 2004 15:50:13 -0500
Subject: [R] unbalanced design
Message-ID: <41AE2E85.5040105@maine.edu>

Hi all,
I'm new to R and have the following problem:
I have a 2 factor design (a has 2 levels, b has 3 levels). I have an 
object kidney.aov which is an aov(y ~ a*b), and when I ask for 
model.tables(kidney.avo, se=T) I get the following message along with 
the table of effects:

Design is unbalanced - use se.contrast() for se's

but the design is NOT unbalanced... each fator level combination has the 
same n

I' d appreciate any help.
Thanks.



From br44114 at yahoo.com  Wed Dec  1 22:18:22 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 1 Dec 2004 13:18:22 -0800 (PST)
Subject: [R] Protocol for answering basic questions
Message-ID: <20041201211822.60475.qmail@web50310.mail.yahoo.com>

I'm also an R beginner. I have asked stupid questions, and received
RTFM replies. I believe such replies are _GREAT_, as long as they
include a brief reference to what to read, and where. (In some cases
searches don't work unless you happen to use the 'right' keywords,
and in other cases it may be relatively easy to miss a paragraph in a
manual - or even FAQ.)

I believe that rudeness (perceived or real) doesn't matter. It is
only solving the problem that matters. In this respect, it seems to
me that most (if not all) users who ask a question on R-help figure
out what to do.  

In regards to politeness, I think that the solution - and the problem
- lies almost completely in the other camp: those who ask (and not
those who reply). I would recommend all R beginners to not feel
easily offended, and to not be afraid to ask stupid questions. So
what if you risk being perceived a lazy idiot? (As I occasionally am,
and certainly will be again.) Do go ahead and ask, if you must. Do
you need to solve your problem or not?

Many many many thanks to all those who bother to answer questions on
R-help. (I still find it hard to believe that experts such as Brian
Ripley and Peter Dalgaard, to quote just two names, take the trouble
to answer so many questions, including basic ones.) And, of course,
thank heavens and the R Core Team that R exists.
b.


-----Original Message-----
From: Robert Brown FM CEFAS [mailto:r.g.brown at cefas.co.uk]
Sent: Wednesday, December 01, 2004 11:46 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Protocol for answering basic questions


I have been following the discussions on 'Reasons not to answer very
basic questions in a straightforward way' with interest as someone
who is also new to R and has had similar experiences.  As such it
with sadness that I note that most seem to agree with the present
approach to the responses to basic questions.  I must thank those
respondants to my own questions who have been helpful, but there are
some whose replies are in my opinion not only unhelpful but actually
rude.



From PAlspach at hortresearch.co.nz  Wed Dec  1 22:35:36 2004
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Thu, 02 Dec 2004 10:35:36 +1300
Subject: [R] unbalanced design
Message-ID: <s1aef01c.098@hra2.marc.hort.cri.nz>


Dami??n

I asked a similar question a few months ago (3 August 2004):

> temp.aov <- aov(S~rep+trt1*trt2*trt3, data=dummy.data)
> model.tables(temp.aov, type='mean', se=T)
>
> Returns the means, but states "Design is unbalanced - use se.contrasts
> for se's" which is a little surprising since the design is balanced. 

To which Prof Ripley replied: If you used the default treatment contrasts, it is not.  Try Helmert
contrasts with aov().

If I recall correctly, following Prof Ripley's suggestion led aov() to accept the design was balanced, but model.tables() still did not (but that could have been my error).  However, se.contrast() worked.

Cheers ........

Peter Alspach



>>> Dami??n Cirelli <damian.cirelli at maine.edu> 02/12/04 09:50:13 >>>
Hi all,
I'm new to R and have the following problem:
I have a 2 factor design (a has 2 levels, b has 3 levels). I have an
object kidney.aov which is an aov(y ~ a*b), and when I ask for
model.tables(kidney.avo, se=T) I get the following message along with
the table of effects:

Design is unbalanced - use se.contrast() for se's

but the design is NOT unbalanced... each fator level combination has the
same n

I' d appreciate any help.
Thanks.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From kjetil at acelerate.com  Wed Dec  1 14:49:17 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 01 Dec 2004 09:49:17 -0400
Subject: [R] Combined variable names
In-Reply-To: <200412010325.iB13Pa6X361377@atlas.otago.ac.nz>
References: <200412010325.iB13Pa6X361377@atlas.otago.ac.nz>
Message-ID: <41ADCBDD.1070004@acelerate.com>

Richard A. O'Keefe wrote:

>Tobias Muhlhofer <t.muhlhofer at lse.ac.uk> wrote:
>	I am trying to define a large number of variables through a loop
>	construct.
>	
>He wants to do
>	for (i in 1:100) {
>	    assign(paste("v", i, sep=""), ....something or other...)
>	}
>
>This is, of course, a FAQ.  It's such a FAQ that I must have seen it
>once a day for the last several days.
>
>What I want to know is *WHY* people are doing this?
>  
>
Bad habits from less expressive languages?

Kjetil

>What, precisely, does it buy you to have variables called v1...v100
>rather than variables called v[[1]]...v[[100]]?
>Apart from persistent inconvenience, that is?
>
>What, really, is wrong with
>
>    v <- lapply(1:100, function (i) ....something or other...)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From wfaulk at icoria.com  Wed Dec  1 22:45:08 2004
From: wfaulk at icoria.com (William Faulk)
Date: Wed, 01 Dec 2004 16:45:08 -0500
Subject: [R] R on Irix 6.5 fails when trying to load external data?
Message-ID: <41AE3B64.3070300@icoria.com>

I've previously installed R on Solaris and everything went fine there.

I'm now trying to install on Irix 6.5 (about which, unfortunately, I 
have less than thorough knowledge) and it is failing.  R itself seems to 
be working okay.  Initially, Irix's dumb make program didn't understand 
some Makefile syntax in the tests, so I just ignored them and installed it.

After the installation, I tried to run some demos, but when I typed 
demo(graphics), for example, it said "Error in demo(graphics) : No demo 
found for topic 'graphics'".  But when I copied-and-pasted the commands 
from doing the same thing under Solaris, everything worked fine.  Until 
I got to the commands that tried to deal with the iris dataset, to which 
it responded:

> Error in pairs(iris[1:4], main = "Edgar Anderson's Iris Data", font.main = 4,  : 
>         Object "iris" not found

So I started over and built gmake for Irix, hoping that the Irix make 
had done something wrong somewhere, but other than not having to tweak 
the check makefiles, nothing seems to be fixed.  Now that I can get the 
tests to run, the first test fails also due to not being able to load 
external datasets:

> > sw <- swiss[1:5, 1:4]  # select a manageable subset
> Error: Object "swiss" not found
> Execution halted

I don't know what's failing to load.  The files iris.R and swiss.R would 
seem to be in the proper locations, but, then again, neither Solaris nor 
Irix system call traces show R trying to open them at any point.

I'd appreciate it if someone could give me some pointers.  It's 
important to note that I know virtually nothing about R -- I'm just a 
sysadmin asked to install R for the real users -- so treat me like an idiot.

-Bitt Faulk



From damian.cirelli at maine.edu  Wed Dec  1 23:02:53 2004
From: damian.cirelli at maine.edu (=?ISO-8859-1?Q?Dami=E1n_Cirelli?=)
Date: Wed, 01 Dec 2004 17:02:53 -0500
Subject: [R] unbalanced design
In-Reply-To: <s1aef01c.097@hra2.marc.hort.cri.nz>
References: <s1aef01c.097@hra2.marc.hort.cri.nz>
Message-ID: <41AE3F8D.9020101@maine.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041201/12a83df4/attachment.pl

From gunter.berton at gene.com  Wed Dec  1 23:13:09 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 1 Dec 2004 14:13:09 -0800
Subject: [R] unbalanced design
In-Reply-To: <41AE3F8D.9020101@maine.edu>
Message-ID: <200412012213.iB1MD95X002355@meitner.gene.com>

This is a standard "gotcha" in linear models. Gets me, too. Try ?C and
?contr.sum to help you understand. Also Venables's and Ripley's MASS 4 ,
section 6.2 gives a short not too technical summary; any linear models text
will provide a more complete (and more technical) discussion.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dami??n Cirelli
> Sent: Wednesday, December 01, 2004 2:03 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] unbalanced design
> 
> 
> Thanks Peter,
> 
> I still wonder why it thinks it's unbalanced...
> 
>



From quesada at gmail.com  Wed Dec  1 23:16:10 2004
From: quesada at gmail.com (Jose Quesada)
Date: Wed, 1 Dec 2004 22:16:10 +0000
Subject: [R] rank in descending order?
Message-ID: <21c05c7d04120114167170982e@mail.gmail.com>

Hi,

Is there any simple solution to get ranks in descending order?
Example, 
a <- c(10, 98, 98, 98, 99, 100)
r <- rank(a, ties.method="average")

produces
1 3 3 3 5 6

I would want this instead:
6 5 3 3 3 1

Note that reversing r doesn't work but in small examples.

Thanks,
-Jose

-- 
 
jquesada at andrew.cmu.edu		Research associate
http://lsa.colorado.edu/~quesadaj	Dept. of Social and Decision Sciences
http://www.andrew.cmu.edu/~jquesada	Carnegie Mellon University
Porter Hall	Phone: 412 268 6011
office PH208-J	Fax:   412 268 6938
5000 Forbes ave.
15213, Pittsburgh, PA



From abu3ammar at gmail.com  Wed Dec  1 23:21:08 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Wed, 1 Dec 2004 17:21:08 -0500
Subject: [R] SJava
Message-ID: <b1d3150404120114213ad9df62@mail.gmail.com>

Is theer an SJava mailing list?

I have the following SJava related question:

I am calling a Java methos in this mannor:

a <- .Java("className", "methodName")
print("done")

As defined, the mother returns an Object  in some case this Object
points to an array of 100,000 Strings. The method prints a statement
when its done. I see ahuge delay between teh last statement in teh
java method and the "done" statement above. I am looking for a way to
improve the speed of SJava converting the java result into R.



From damian.cirelli at maine.edu  Wed Dec  1 23:26:26 2004
From: damian.cirelli at maine.edu (=?ISO-8859-1?Q?Dami=E1n_Cirelli?=)
Date: Wed, 01 Dec 2004 17:26:26 -0500
Subject: [R] rank in descending order?
In-Reply-To: <21c05c7d04120114167170982e@mail.gmail.com>
References: <21c05c7d04120114167170982e@mail.gmail.com>
Message-ID: <41AE4512.6030706@maine.edu>

Try this:

r <- sort(rank(a, ties.method="average"), decreasing=T)


Jose Quesada wrote:

>Hi,
>
>Is there any simple solution to get ranks in descending order?
>Example, 
>a <- c(10, 98, 98, 98, 99, 100)
>r <- rank(a, ties.method="average")
>
>produces
>1 3 3 3 5 6
>
>I would want this instead:
>6 5 3 3 3 1
>
>Note that reversing r doesn't work but in small examples.
>
>Thanks,
>-Jose
>
>  
>



From mzp3769 at yahoo.com  Wed Dec  1 23:39:36 2004
From: mzp3769 at yahoo.com (m p)
Date: Wed, 1 Dec 2004 14:39:36 -0800 (PST)
Subject: [R] points plotting
Message-ID: <20041201223936.7997.qmail@web51005.mail.yahoo.com>

Hello,
I use 
points(a,b,pch=20,col="black") or pch=19 but in both
cases bullets are too large. Is the a way to decrease
the size of the bullet - but be larger than a period.
Thanks,
Mark



From p.dalgaard at biostat.ku.dk  Wed Dec  1 23:42:41 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Dec 2004 23:42:41 +0100
Subject: [R] Combined variable names
In-Reply-To: <41ADCBDD.1070004@acelerate.com>
References: <200412010325.iB13Pa6X361377@atlas.otago.ac.nz>
	<41ADCBDD.1070004@acelerate.com>
Message-ID: <x2pt1t8wta.fsf@biostat.ku.dk>

Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> writes:

> >This is, of course, a FAQ.  It's such a FAQ that I must have seen it
> >once a day for the last several days.
> >
> >What I want to know is *WHY* people are doing this?
> >
> Bad habits from less expressive languages?

Actually, from less _regular_ languages. E.g. SAS will in some
contexts allow v1-v4 for v1 v2 v3 v4, which is expressive, but of
course it means something completely different in other contexts.

R generally tries to be a regular programming language where things
mean the same wherever they are used, so that expressions can be
picked apart and understood piece by piece. E.g. R doesn't allow you
to specify the tail end of an array using syntax like a[i:end], where
"end" would be equivalent to length(a). 

There are irregularities, e.g. the fact that you do help(foo), not
help("foo"), but they tend to get a pain in the long run (How do you
get help on a name contained in a variable? A GUI designer will need a
way to do just that...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Wed Dec  1 23:39:53 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 01 Dec 2004 22:39:53 -0000 (GMT)
Subject: [R] chisq.test probabilities method unclear
In-Reply-To: <DBE4C719-43D1-11D9-811C-000D93679632@biotopia.info>
Message-ID: <XFMail.041201214211.Ted.Harding@nessie.mcc.ac.uk>

On 01-Dec-04 Alexander Keller wrote:
> Hi list,
> 
> i've got a question about the chisq.test function.
> in the use of the "given probabilities" method  (p= ...), normally 
> there should be typed in probabilities in the range of 0 to 1 with the 
> absolute sum of 1.0 (r-help)
> But it is possible to use probabilities > than 1. or the sum <1.! 
> without any warning message
> 
> Ok, now the question, what does r calcutate in these cases,
> this doesn't make sense in my (poor statistical) view.
> 
> i thought it might calculate relations, but it differs in the results
> (example: of typing p=c(1,1,1,1), p=c(6,6,6,6), 
> p=c(0.25,0.25,0.25,0.25))
> 
> can someone tell my about this chisq method, and perhaps show me an 
> explaining example?

Experiment shows:

  x<-c(6,6,6,6,6,6)
  P<-c(6,6,6,6,6,6)
  chisq.test(x,p=P)

          Chi-squared test for given probabilities

  data:  x 
  X-squared = 1225, df = 5, p-value = < 2.2e-16

  sum(((x-36*x)^2)/(36*6))
  [1] 1225

that chisq.test(x,p=P) appears to calculate

  sum( ((x - n*P)^2)/(n*P) )

whether the P sum to 1 or not (where n = sum(x)).

So you'd better make sure that they do!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 01-Dec-04                                       Time: 21:42:11
------------------------------ XFMail ------------------------------



From MSchwartz at MedAnalytics.com  Wed Dec  1 23:52:06 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 01 Dec 2004 16:52:06 -0600
Subject: [R] points plotting
In-Reply-To: <20041201223936.7997.qmail@web51005.mail.yahoo.com>
References: <20041201223936.7997.qmail@web51005.mail.yahoo.com>
Message-ID: <1101941526.30279.16.camel@horizons.localdomain>

On Wed, 2004-12-01 at 14:39 -0800, m p wrote:
> Hello,
> I use 
> points(a,b,pch=20,col="black") or pch=19 but in both
> cases bullets are too large. Is the a way to decrease
> the size of the bullet - but be larger than a period.
> Thanks,
> Mark


Try using the 'cex' argument, which is described in the help for
points() and has a default value of 1.

In this case, use something <1 such as:

points(a, b, pch=20, col="black", cex = 0.5)

See ?points for more information.

HTH,

Marc Schwartz



From olafm at tako.de  Wed Dec  1 23:54:16 2004
From: olafm at tako.de (Olaf Mersmann)
Date: Wed, 01 Dec 2004 23:54:16 +0100
Subject: [R] points plotting
In-Reply-To: <20041201223936.7997.qmail@web51005.mail.yahoo.com>
References: <20041201223936.7997.qmail@web51005.mail.yahoo.com>
Message-ID: <41AE4B98.7050603@tako.de>

Hi m p,

m p wrote:
> Hello,
> I use 
> points(a,b,pch=20,col="black") or pch=19 but in both
> cases bullets are too large. Is the a way to decrease
> the size of the bullet - but be larger than a period.
Try ?plot.default and read the entry for 'cex'

HTH
Olaf



From p.dalgaard at biostat.ku.dk  Wed Dec  1 23:58:16 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Dec 2004 23:58:16 +0100
Subject: [R] R on Irix 6.5 fails when trying to load external data?
In-Reply-To: <41AE3B64.3070300@icoria.com>
References: <41AE3B64.3070300@icoria.com>
Message-ID: <x2llch8w3b.fsf@biostat.ku.dk>

William Faulk <wfaulk at icoria.com> writes:

> I've previously installed R on Solaris and everything went fine there.
> 
> I'm now trying to install on Irix 6.5 (about which, unfortunately, I
> have less than thorough knowledge) and it is failing.  R itself seems
> to be working okay.  Initially, Irix's dumb make program didn't
> understand some Makefile syntax in the tests, so I just ignored them
> and installed it.
> 
> After the installation, I tried to run some demos, but when I typed
> demo(graphics), for example, it said "Error in demo(graphics) : No
> demo found for topic 'graphics'".  But when I copied-and-pasted the
> commands from doing the same thing under Solaris, everything worked
> fine.  Until I got to the commands that tried to deal with the iris
> dataset, to which it responded:
> 
> > Error in pairs(iris[1:4], main = "Edgar Anderson's Iris Data",
> > font.main = 4,  :         Object "iris" not found
> 
> So I started over and built gmake for Irix, hoping that the Irix make
> had done something wrong somewhere, but other than not having to tweak
> the check makefiles, nothing seems to be fixed.  Now that I can get
> the tests to run, the first test fails also due to not being able to
> load external datasets:
> 
> > > sw <- swiss[1:5, 1:4]  # select a manageable subset
> > Error: Object "swiss" not found
> > Execution halted
> 
> I don't know what's failing to load.  The files iris.R and swiss.R
> would seem to be in the proper locations, but, then again, neither
> Solaris nor Irix system call traces show R trying to open them at any
> point.
> 
> I'd appreciate it if someone could give me some pointers.  It's
> important to note that I know virtually nothing about R -- I'm just a
> sysadmin asked to install R for the real users -- so treat me like an
> idiot.

Oooh, that's tempting ;-) Unfortunately, I can't spot that you've done
anything really idiotic. You did clean up and start from scratch after
building gmake, right? The best idea I can come up with is that you
might have a dud "install" command sitting somewhere and getting in
the way. I believe this has tripped IRIX users before, but I could be
wrong. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From kjetil at acelerate.com  Wed Dec  1 23:34:15 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 01 Dec 2004 18:34:15 -0400
Subject: [R] how to use print() without column names?
In-Reply-To: <6f3fc9ee041201094654c1243a@mail.gmail.com>
References: <6f3fc9ee041201094654c1243a@mail.gmail.com>
Message-ID: <41AE46E7.4090401@acelerate.com>

Hu Chen wrote:

>Hi all,
>any guy who knows how to print a data frame without column names printed?
>I didn't find a parameter in print() function to turn off this.
>Neither in options().
>Thanks in advance.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>
If your data.frame is purely numerical, seems like you can do:
library(MASS)
names(your.df) <- NULL
write.matrix(your.df)

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From bates at cs.wisc.edu  Thu Dec  2 00:41:20 2004
From: bates at cs.wisc.edu (Douglas Bates)
Date: Wed, 01 Dec 2004 17:41:20 -0600
Subject: [R] Combined variable names (two concrete suggestions)
In-Reply-To: <p06100500bdd3a2812285@[83.132.28.212]>
References: <p06100500bdd3a2812285@[83.132.28.212]>
Message-ID: <41AE56A0.50305@cs.wisc.edu>

Tiago R Magalhaes wrote:

> For loops are conceptually very easy to understand. Lists are not 
> easy to understand (why list[[1]] instead of list[1]? it's not 
> completely intuitive) .

I try to explain it as comparable to the difference between a subset 
that consists of one element (the "[" function always returns a vector 
of the same mode as its argument) and the element itself.

Whether that explanation makes sense depends on how comfortable the 
listener is with mathematical abstractions.  As they say, "There are 10 
types of people in this world: those who understand binary numbers and 
those who don't."



From bitwrit at ozemail.com.au  Fri Dec  3 10:48:15 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 3 Dec 2004 20:48:15 +1100
Subject: [R] A possible way to reduce basic questions
Message-ID: <20041201234357.SZIR17222.smta06.mail.ozemail.net@there>

I have been thinking about how to reduce the number of basic questions that 
elicit the ...ahem... robust debate that has occurred about how to answer 
them.

One theme of such questions is "I want to do this, is there a function to do 
it?" These usually receive the:

?something

reply if the respondent is in a good mood. I hypothesize that if those 
seeking a function had a searchable list of all the functions available, they 
would be more likely to discover the answer for themselves. I sincerely hope 
that this would help both the questioners and the answerers and the list 
maintainers.

To this end, I have written a small C program that compiles and runs on my 
Linux system. Given the R library directory as an argument, it builds a list 
of all the INDEX files, then writes an HTML page with a table for each 
package present. As all HTML browsers with which I am familiar can search a 
page for arbitrary strings, this will allow the questioner to search for the 
presence of the term they seek in the INDEX file of any package.

I think this program or something similar could produce a regularly updated 
list of functions available on CRAN, and that list would be easy enough to 
search to encourage anyone who was looking for a function to try it.

It's a bit rough at the moment, as INDEX files vary in format, but I am 
willing to refine the program if those responsible for the CRAN sites would 
be willing to use it.

Jim

From cjgb at wanadoo.es  Thu Dec  2 00:55:53 2004
From: cjgb at wanadoo.es (Carlos Javier Gil Bellosta)
Date: Thu, 02 Dec 2004 00:55:53 +0100
Subject: [R] Protocol for answering basic questions
In-Reply-To: <D5B5C260-43C4-11D9-9C45-000A95C76CA8@oulu.fi>
References: <AAAECF1CF69A704EB103E5284B741CF902CEAB0D@lauimls05.qc.dfo.ca>
	<D5B5C260-43C4-11D9-9C45-000A95C76CA8@oulu.fi>
Message-ID: <41AE5A09.3050408@wanadoo.es>

Here we are facing two problems:

	1) Traffic at the r-help list should be kept at a reasonable level lest 
it becomes completely unmanageable and un-r-helpful.

	2) People will continue to misread, overlook or disregard the manuals 
and come up with questions that many others will consider silly.

R comes with superb documentation and, certainly, the "An Introduction 
to R" is a must for a beginner. But I am sure that my first (and many 
other's) hours with R would have been much more productive spending them 
in front of a computer with an expert in it than reading the document 
and toying at the keyboard. How long does it take until a novice in R 
starts finding more answers than questions while browsing the huge 
documentation? How often would he think "this is a silly question, but I 
cannot find the answer for it; should I had someone to ask it, a simple 
word would put me back on track"?

In fact, most basic questions that appear on r-help are, essentially, 
answered with a simple word (usually preceded by a question mark and 
perhaps accompanied by an invitation to re-read the manuals).

Couldn't the R-community come up with a more economical and less stark 
way to help these newcomers? Couldn't it devise a procedure to provide a 
somewhat more interactive way --even more than a mail-list-- to provide 
them with this one-word answers without having to replicate it in 
thousands of mailboxes throughout the world?

Over the years, while learning C, Java or Python, I have found very 
useful a few IRC channels on those languages where one could get (and 
provide!!) peer-to-peer support. Should a rather informal, open and 
publicited one exist for R, I believe it could channel many of these 
basic questions and, probably, many others some novices do not dare ask.

Perhaps, it does not sound as a very "serious" procedure but... there 
goes my "granito de arena" on the topic.

Carlos J. Gil Bellosta



From baron at psych.upenn.edu  Thu Dec  2 00:58:44 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 1 Dec 2004 18:58:44 -0500
Subject: [R] A possible way to reduce basic questions
In-Reply-To: <20041201234357.SZIR17222.smta06.mail.ozemail.net@there>
References: <20041201234357.SZIR17222.smta06.mail.ozemail.net@there>
Message-ID: <20041201235844.GA15126@psych>

On 12/03/04 20:48, Jim Lemon wrote:
>I think this program or something similar could produce a regularly updated
>list of functions available on CRAN, and that list would be easy enough to
>search to encourage anyone who was looking for a function to try it.

My search site (below) also allows searching functions.  The
problem I often find is that the terms used in the query are not
in the help files for the functions, or even in the r-help
discussion of them.  I think there is no solution to this
problem, unless someone wants to write a program that translates
(e.g.) from economics-English to psychology-English and both to
statistics-English.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From t.muhlhofer at lse.ac.uk  Thu Dec  2 01:31:03 2004
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Thu, 02 Dec 2004 00:31:03 +0000
Subject: [R] dropping rows
Message-ID: <41AE6247.8030004@lse.ac.uk>

Hi!

Sorry for asking a trivial questions, but I can't seem to figure this out.

I have a dataframe called master containing 30-odd variables.

In this dataframe, I have observations across these 30 variables from 
1930 to 2003 (I've made a "year" variable). How can I drop all rows for 
which the year is less than 1960? I'm assuming something with ifelse() 
but I can't quite figure it out.

I would appreciate a suggestion of some syntax.

Thanks!
	Toby



From p.dalgaard at biostat.ku.dk  Thu Dec  2 01:38:23 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Dec 2004 01:38:23 +0100
Subject: [R] A possible way to reduce basic questions
In-Reply-To: <20041201234357.SZIR17222.smta06.mail.ozemail.net@there>
References: <20041201234357.SZIR17222.smta06.mail.ozemail.net@there>
Message-ID: <x2hdn58rgg.fsf@biostat.ku.dk>

Jim Lemon <bitwrit at ozemail.com.au> writes:

> One theme of such questions is "I want to do this, is there a function to do 
> it?" These usually receive the:
> 
> ?something
> 
> reply if the respondent is in a good mood.

Notice however, that any respondent worth his salt usually checks
whether the relevant information is actually on that help page, reads
the post again and gives a few thoughts to whether ?something should
be sufficient to get the poster on track. Usually takes at least a
couple of minutes. (And, given email delays, the responder often finds
half an hour later that 6 other people did likewise.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Thu Dec  2 01:42:38 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 1 Dec 2004 19:42:38 -0500
Subject: [R] dropping rows
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3B1@usrymx25.merck.com>

See ?subset, especially the example there.

(I'm surprised this function isn't in R-intro...)

Andy

> From: Tobias Muhlhofer
> 
> Hi!
> 
> Sorry for asking a trivial questions, but I can't seem to 
> figure this out.
> 
> I have a dataframe called master containing 30-odd variables.
> 
> In this dataframe, I have observations across these 30 variables from 
> 1930 to 2003 (I've made a "year" variable). How can I drop 
> all rows for 
> which the year is less than 1960? I'm assuming something with 
> ifelse() 
> but I can't quite figure it out.
> 
> I would appreciate a suggestion of some syntax.
> 
> Thanks!
> 	Toby
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From wfaulk at icoria.com  Thu Dec  2 01:43:29 2004
From: wfaulk at icoria.com (William Faulk)
Date: Wed, 01 Dec 2004 19:43:29 -0500
Subject: [R] R on Irix 6.5 fails when trying to load external data?
In-Reply-To: <x2llch8w3b.fsf@biostat.ku.dk>
References: <41AE3B64.3070300@icoria.com> <x2llch8w3b.fsf@biostat.ku.dk>
Message-ID: <41AE6531.9060906@icoria.com>

Peter Dalgaard wrote:
> 
> You did clean up and start from scratch after building gmake, right?

Yup.

> The best idea I can come up with is that you might have a dud
> "install" command sitting somewhere and getting in the way. I believe
> this has tripped IRIX users before, but I could be wrong.

You seem to have hit the nail on the head, sir.  Spot on.  Apparently, 
Irix's /usr/bin/install is ... poor.  (This is a *family* mailing list, 
right?)  Along with its make.  Why do they bother including them if they 
don't work right?  On the other side of the fence, why do configure'd 
programs look for it at all when they've got install-sh sitting right 
there that does everything they want?  Ah, well; as long as it works....

Now I have another error in regard to being unable to resolve a symbol
in a library, but I think that's back in the realm of things I can 
figure out.

Thanks very much,

-Bitt



From bates at stat.wisc.edu  Thu Dec  2 01:42:57 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 01 Dec 2004 18:42:57 -0600
Subject: [R] dropping rows
In-Reply-To: <41AE6247.8030004@lse.ac.uk>
References: <41AE6247.8030004@lse.ac.uk>
Message-ID: <41AE6511.4040908@stat.wisc.edu>

Tobias Muhlhofer wrote:
> Hi!
> 
> Sorry for asking a trivial questions, but I can't seem to figure this out.
> 
> I have a dataframe called master containing 30-odd variables.
> 
> In this dataframe, I have observations across these 30 variables from 
> 1930 to 2003 (I've made a "year" variable). How can I drop all rows for 
> which the year is less than 1960? I'm assuming something with ifelse() 
> but I can't quite figure it out.
> 
> I would appreciate a suggestion of some syntax.

In R this is called subsetting and the simplest way to do this is with 
the subset function.

older <- subset(master, year < 1960)

See ?subset for more variations on this theme.



From p.dalgaard at biostat.ku.dk  Thu Dec  2 01:42:33 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Dec 2004 01:42:33 +0100
Subject: [R] dropping rows
In-Reply-To: <41AE6247.8030004@lse.ac.uk>
References: <41AE6247.8030004@lse.ac.uk>
Message-ID: <x2d5xt8r9i.fsf@biostat.ku.dk>

Tobias Muhlhofer <t.muhlhofer at lse.ac.uk> writes:

> Hi!
> 
> Sorry for asking a trivial questions, but I can't seem to figure this out.
> 
> I have a dataframe called master containing 30-odd variables.
> 
> In this dataframe, I have observations across these 30 variables from
> 1930 to 2003 (I've made a "year" variable). How can I drop all rows
> for which the year is less than 1960? I'm assuming something with
> ifelse() but I can't quite figure it out.
> 
> I would appreciate a suggestion of some syntax.


myframe[myframe$year>=1960,]

or

subset(myframe, year >= 1960)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From t.muhlhofer at lse.ac.uk  Thu Dec  2 01:57:47 2004
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Thu, 02 Dec 2004 00:57:47 +0000
Subject: [R] dropping rows
In-Reply-To: <41AE64DB.1040006@noaa.gov>
References: <41AE6247.8030004@lse.ac.uk> <41AE64DB.1040006@noaa.gov>
Message-ID: <41AE688B.4070702@lse.ac.uk>

Thanks.

The problem is that there is extremely little on dataframes or matrices 
in "An Intro to R", which I did read and I frankly don't know where else 
to go.

Once I know a function like subset() exists, I can then read the help 
files on it and that's fine, but I would never dream this function up 
myself...

As for indexing, I DID read "An Introduction to R" and I did NOT catch 
the part where it says you can use any variable in the dataframe to 
index it, nor would I have thought of it by myself. From that 
documentation, I only learned about using row-labels to index things...

But I am definitely thankful for the quick help given to me by people on 
this list, and so I guess being RTFM'ed is a small price to pay for 
figuring out how to solve the problem I need to solve.

Toby


Jeff Laake wrote:
> Here's an example:
> 
> earlydata=data[data$year<1960,]
> 
> Lookup help and read manuals on manipulating dataframes.
> 
> 
> Tobias Muhlhofer wrote:
> 
>> Hi!
>>
>> Sorry for asking a trivial questions, but I can't seem to figure this 
>> out.
>>
>> I have a dataframe called master containing 30-odd variables.
>>
>> In this dataframe, I have observations across these 30 variables from 
>> 1930 to 2003 (I've made a "year" variable). How can I drop all rows 
>> for which the year is less than 1960? I'm assuming something with 
>> ifelse() but I can't quite figure it out.
>>
>> I would appreciate a suggestion of some syntax.
>>
>> Thanks!
>>     Toby
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> 
> 
> 

-- 
**************************************************************************
When Thomas Edison invented the light bulb he tried over 2000
experiments before he got it to work. A young reporter asked
him how it felt to have failed so many times. He said
"I never failed once. I invented the light bulb.
It just happened to be a 2000-step process."



From bates at stat.wisc.edu  Thu Dec  2 02:03:21 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 01 Dec 2004 19:03:21 -0600
Subject: [R] dropping rows
In-Reply-To: <41AE688B.4070702@lse.ac.uk>
References: <41AE6247.8030004@lse.ac.uk> <41AE64DB.1040006@noaa.gov>
	<41AE688B.4070702@lse.ac.uk>
Message-ID: <41AE69D9.4060403@stat.wisc.edu>

Tobias Muhlhofer wrote:
> Thanks.
> 
> The problem is that there is extremely little on dataframes or matrices 
> in "An Intro to R", which I did read and I frankly don't know where else 
> to go.
> 
> Once I know a function like subset() exists, I can then read the help 
> files on it and that's fine, but I would never dream this function up 
> myself...
> 
> As for indexing, I DID read "An Introduction to R" and I did NOT catch 
> the part where it says you can use any variable in the dataframe to 
> index it, nor would I have thought of it by myself. From that 
> documentation, I only learned about using row-labels to index things...
> 
> But I am definitely thankful for the quick help given to me by people on 
> this list, and so I guess being RTFM'ed is a small price to pay for 
> figuring out how to solve the problem I need to solve.

Being RTFM'ed is even more valuable when the answer given is wrong (as 
mine was) but the manual page is right.  I showed you how to select the 
rows where year < 1960 rather than how to drop those rows.  :-)



From ggrothendieck at myway.com  Thu Dec  2 02:23:36 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  1 Dec 2004 20:23:36 -0500 (EST)
Subject: [R] A possible way to reduce basic questions
Message-ID: <20041202012336.D7E4A3986@mprdmxin.myway.com>


Jim Lemon <bitwrit <at> ozemail.com.au> writes:

> I have been thinking about how to reduce the number of basic questions that 
> elicit the ...ahem... robust debate that has occurred about how to answer 


The traffic on r-help could be reduced by creating a second list where
more elementary questions are asked.  

There may be other ways to partition the universe of questions as well.



From tlumley at u.washington.edu  Thu Dec  2 02:35:03 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 1 Dec 2004 17:35:03 -0800 (PST)
Subject: [R] dropping rows
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E3B1@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E3B1@usrymx25.merck.com>
Message-ID: <Pine.A41.4.61b.0412011730560.96002@homer10.u.washington.edu>

On Wed, 1 Dec 2004, Liaw, Andy wrote:

> See ?subset, especially the example there.
>
> (I'm surprised this function isn't in R-intro...)

Much of R-intro was written as an introduction to S before R existed and 
later translated, so there are quite a few things that aren't in it but 
would be in a guide written from scratch.

 	-thomas

>
> Andy
>
>> From: Tobias Muhlhofer
>>
>> Hi!
>>
>> Sorry for asking a trivial questions, but I can't seem to
>> figure this out.
>>
>> I have a dataframe called master containing 30-odd variables.
>>
>> In this dataframe, I have observations across these 30 variables from
>> 1930 to 2003 (I've made a "year" variable). How can I drop
>> all rows for
>> which the year is less than 1960? I'm assuming something with
>> ifelse()
>> but I can't quite figure it out.
>>
>> I would appreciate a suggestion of some syntax.
>>
>> Thanks!
>> 	Toby
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From PAlspach at hortresearch.co.nz  Thu Dec  2 03:17:28 2004
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Thu, 02 Dec 2004 15:17:28 +1300
Subject: [R] dropping rows
Message-ID: <s1af3235.095@hra2.marc.hort.cri.nz>


Tobias

I remember finding Patrick Burns' "S Poetry" (see http://www.burns-stat.com/ ) worth reading - and it covers this sort of thing nicely.

Peter Alspach


>>> Tobias Muhlhofer <t.muhlhofer at lse.ac.uk> 02/12/04 13:57:47 >>>
Thanks.

The problem is that there is extremely little on dataframes or matrices 
in "An Intro to R", which I did read and I frankly don't know where else 
to go.

Once I know a function like subset() exists, I can then read the help 
files on it and that's fine, but I would never dream this function up 
myself...

As for indexing, I DID read "An Introduction to R" and I did NOT catch 
the part where it says you can use any variable in the dataframe to 
index it, nor would I have thought of it by myself. From that 
documentation, I only learned about using row-labels to index things...

But I am definitely thankful for the quick help given to me by people on 
this list, and so I guess being RTFM'ed is a small price to pay for 
figuring out how to solve the problem I need to solve.

Toby


Jeff Laake wrote:
> Here's an example:
> 
> earlydata=data[data$year<1960,]
> 
> Lookup help and read manuals on manipulating dataframes.
> 
> 
> Tobias Muhlhofer wrote:
> 
>> Hi!
>>
>> Sorry for asking a trivial questions, but I can't seem to figure this 
>> out.
>>
>> I have a dataframe called master containing 30-odd variables.
>>
>> In this dataframe, I have observations across these 30 variables from 
>> 1930 to 2003 (I've made a "year" variable). How can I drop all rows 
>> for which the year is less than 1960? I'm assuming something with 
>> ifelse() but I can't quite figure it out.
>>
>> I would appreciate a suggestion of some syntax.
>>
>> Thanks!
>>     Toby
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help 
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html 
> 
> 
> 
> 
> 

-- 
**************************************************************************
When Thomas Edison invented the light bulb he tried over 2000
experiments before he got it to work. A young reporter asked
him how it felt to have failed so many times. He said
"I never failed once. I invented the light bulb.
It just happened to be a 2000-step process."

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From murdoch at stats.uwo.ca  Thu Dec  2 03:52:34 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 01 Dec 2004 21:52:34 -0500
Subject: [R] Protocol for answering basic questions
In-Reply-To: <41AE5A09.3050408@wanadoo.es>
References: <AAAECF1CF69A704EB103E5284B741CF902CEAB0D@lauimls05.qc.dfo.ca>
	<D5B5C260-43C4-11D9-9C45-000A95C76CA8@oulu.fi>
	<41AE5A09.3050408@wanadoo.es>
Message-ID: <mh0tq0l1v0mbqj250iruappi18dauv2frc@4ax.com>

On Thu, 02 Dec 2004 00:55:53 +0100, Carlos Javier Gil Bellosta
<cjgb at wanadoo.es> wrote:


>Over the years, while learning C, Java or Python, I have found very 
>useful a few IRC channels on those languages where one could get (and 
>provide!!) peer-to-peer support. Should a rather informal, open and 
>publicited one exist for R, I believe it could channel many of these 
>basic questions and, probably, many others some novices do not dare ask.

As far as I know, there is currently no IRC channel on R.  Why not set
one up?  (I've got no idea what is involved in that.)  You'll need to
attract participants to it.  Various ways are:

  - announce on this mailing list when you have created it.
  - put a note about it in your signature
  - use it, and be helpful to other users on it
  - get a mention of it on an appropriate r-project.org page.

I've never used IRC and wouldn't be likely to be a participant (in
general I dislike live online chat), but to each his own.  

Duncan Murdoch



From ok at cs.otago.ac.nz  Thu Dec  2 04:50:19 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 2 Dec 2004 16:50:19 +1300 (NZDT)
Subject: [R] Protocol for answering basic questions
Message-ID: <200412020350.iB23oJw5375269@atlas.otago.ac.nz>

DupliseaD at dfo-mpo.gc.ca wrote:
	I have been a member for only a few days but I find the tone of some
	responses are inappropriate for a list dubbing itself a "help list".

We have experts giving hours of their time every day to be helpful
FOR *NOTHING* (no money, no honours, just the reward of being helpful)
and people want to drive them away because they are human and
occasionally get fed up at being asked the same question over and
over again?

I'm on mailing lists for several languages and packages.  There's only
one that even comes close to R in volume, and a lot of that is insider
chat with more flaming in a day than I've seen in this mailing list in
a week.  This is more help, more effective, more educational, and more
timely than I've _ever_ had for _any_ software, free or commercial.

	I also completely understand that traffic needs to be kept at a
	modest level to keep advanced users interested; therefore, I
	suggest that a second help list be created to deal with "advanced R
	help".

With the utmost possible respect, I've seen this kind of thing tried
before, and the only time it works is if someone moderates the 'advanced'
list.  If someone wants to volunteer for the thankless job of moderation,
well and good, but without moderation, what happens is that the clueless
send their messages to both lists, and the clueful beginners lose the
benefit of the experts who've been driven away from the non-advanced list.

One of my earliest messages in this list got slapped down in what I
_still_ think was an unkind and unwarranted way, but I wasn't so thin-
skinned as to run off crying to Mother.

By and large, the people who are most informative in this list are some
of the very same people who worked on things like the FAQ and the
Introduction and some of the tutorials and some of the books.   It's
not beginner status alone that makes someone say "I can't be bothered
reading anything you have written to help me and made readily available
I WANNA HELP NOW!"

As for arbitrary thresholds like 2 years, I have been using R since
1996 or 1997, and I would still find it necessary to be on the 'nonexpert'
mailing list.  I beg the keepers of the flame: DON'T split the list.



From edd at debian.org  Thu Dec  2 05:40:56 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 1 Dec 2004 22:40:56 -0600
Subject: [R] [R-pkgs] Quantian 0.6.9.2 with over 500 CRAN / BioC packages
Message-ID: <20041202044055.GA13834@sonny.eddelbuettel.com>

I posted the following a little earlier on the quantian-announce list -- but
it may be of interest here as well as this version contains all (but three,
see below) packages from CRAN, and all of BioConductor.

Quantian strives to provide the easiest way to set up a very complete
scientific computing environment with a wide variety of tools -- including
R, ESS, Ggobi, almost all of CRAN, all of BioConductor, as well as openMosix
support, and many more applications -- in minutes from a bootable dvd.

Feedback is always welcome, in particular on how to communicate more
directly with specific communities which may benefit from Quantian.  

Dirk



(Please see note [1] below regarding recipients for this posting. Thanks!)


Executive Summary: 

    Quantian 0.6.9.2 extends the Quantian series based on Knoppix 3.6 and the
    corresponding clusterKnoppix release. Over 475 new packages from CRAN and
    BioConductor provide unparalleled support for statistical computing, data
    analysis and graphical methods via the R environment and language.
    Several new packages have been added, and numerous other packages have
    been updated as well.


Announcing Quantian release 0.6.9.2
===================================


I   What is it?

    Quantian is a remastering of Knoppix, the self-configuring and directly
    bootable cdrom/dvd that turns any pc or laptop into a full-featured Linux
    workstation, and clusterKnoppix, which adds support for openMosix-based 
    cluster computing. However, Quantian differs from (cluster)Knoppix by
    adding a very large set of programs of interest to applied or theoretical
    workers in quantitative or data-driven fields. 

    See http://dirk.eddelbuettel.com/quantian.html for more details.

    
II  What is new?

    o Second release based on Knoppix 3.6 adding 
        - over 500 R packages from CRAN and BioConductor (see next point)
        - several new packages such as labplot, python-matplotlib,
          python-gdal, subversion, tla, dia, ...
        - several packages with newer upstream versions such as R 2.0.1, gretl 1.3.0,
        - added most Recommended: packages for already installed packages
 
    o The main new feature is the unique support for almost all packages from
        - CRAN (http://cran.r-project.org), and 
        - BioConductor (http://www.bioconductor.org)
      About 60 packages are installed directly as Debian packages, and 478
      more are installed directly into /usr/local/lib/R/site-packages using
      the mechanisms provided by R, CRAN and BioConductor (and as such, these
      packages will not appear in the Debian package list).  
      The only missing CRAN packages are mimR (windows-only), ROracle
      (requires Oracle) and seao.gui which didn't build in batch mode.
      
    o Adding the Recommended: packages provides additional documentation, 
      as well as header files, which complements the installed packages.
     
    o Total size is 2.0 gb for the compressed iso image corresponding to
      almost 6 gb of uncompressed software. 

    o See http://dirk.eddelbuettel.com/quantian/changelog.html for details.

    o See http://dirk.eddelbuettel.com/quantian/howto.html for several
      short HOWTOs on booting Quantian from hd on either Windows or Linux,
      booting via a bootcd (such as clusterKnoppix), or botting from a 
      USB memory device.  Contributions, corrections, and feedback on these
      HOWTOs is always appreciated.


III Where do I get it?

    o Downloads are available from the main hosts already listed above, i.e.
      on the West Coast at

            http://www.analytics.washington.edu/downloads/quantian/

      and at the East Coast at

            http://research.warnes.net/downloads/quantian/CURRENT/
	    ftp://research.warnes.net/users/edd/quantian/CURRENT/

      The most recent release is also available at

            http://quantian.alioth.debian.org/

    o Bittorrents are available via

            http://www.analytics.washington.edu:6969/

    o The main European mirror has already caught up:

            http://sunsite.rediris.es/mirror/quantian

      and the second Sunsite mirror should hopefully catch up soon:

            http://sunsite.informatik.rwth-aachen.de/ftp/pub/Linux/quantian

    o CD/DVD vendors will probably update their offerings soon as well. 
      BudgetLinuxCDs.com is typically the fastest:

            http://www.budgetlinuxcds.com/index.php?page=Choose&letter=Q&sort=upd

      and CheapBytes should follow shortly:

            http://www.cheapbytes.com

      Pointers to other vendors are always appreciated.


IV  Mailing lists

    o Two mailing lists exist for Quantian

        quantian-announce	  for announcements, intended to be low volume
        quantian-general	  for general discussions about Quantian

      available via

        http://alioth.debian.org/mail/?group_id=1425

      for subscription info etc., and start using the quantian-general lists
      for general questions, comments, suggestions or discussions about 
      Quantian.

      Quantian-general is subscribed to quantian-announce, so you only need
      to subscribe to one (but can of course subscribe to both).

      Reply-To: for this message is quantian-general at lists.alioth.debian.org
      so that discussions can be continued on the list.


V  Known Bugs

    o Sound does not appear to get configured on my laptops.


VI   Other items

    o Feedback / poll on package additions or removal

      As always, I welcome comments and suggestions about programs to be
      added or removed. Existing Debian packages get pushed to the front of
      the line.

      Please send feedback, questions, comments, ... to the 
      
	quantian-general at lists.alioth.debian.org

      list to maximise the number of eyes glancing at any one question.

    o Feedback would also be appreciated on ways to better communicate with
      difference scientific communities that could be interested in Quantian.


VII Notes

    [1] This email is sent via the quantian-announce mailing list. I have
        subscribed those whose email addresses are in my quantian mail folder 
        due to prior emails. The quantian-announce mailing list only sends
        moderator-approved posts -- so there should be no spam whatsoever. I 
        also added LWN and DWN who had run previous announcements, and as 
        suggested, the openmosix-general, clusterknoppix and debian-knoppix 
        lists. Anybody who considers this unwanted is kindly asked to send me
        a private mail to get unsubscribed immediately.



Best regards,  Dirk



-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From bitwrit at ozemail.com.au  Fri Dec  3 16:14:31 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Sat, 4 Dec 2004 02:14:31 +1100
Subject: [R] A possible way to reduce basic questions
Message-ID: <20041202051213.CBHI12869.smta00.mail.ozemail.net@there>

As there seems to be some interest in this concept, I'll include the C code 
(The attachment seems to have been deleted from my last message).

I compiled this as follows:

gcc -o indxlst indxlst.c

It runs like this:

indxlst -t <path_to_R_library_directory>

producing a file named

INDEX_list.html

that should be viewable with any HTML browser.

Jim

/* indxlst.c
A program to search R INDEX files and build an HTML page of their contents
*/

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <ctype.h>
#include <sys/stat.h>

#define MAX_LINE 128

typedef struct {
 char *top_dir;
 FILE *instream;
 FILE *htmlstream;
} IL_INFO;

void InitializeILInfo(IL_INFO *il_info) {
 il_info->top_dir = NULL;
}

/* Usage
Displays a simple usage message.
*/

void Usage(void) {
 char *helplines[] =
  {"\nUsage: indxlst -t <R library directory> -h\n",
   NULL
  };
 int line = 0;

 while(helplines) fprintf(stdout,"%s",helplines[line++]);
}

/* ProcessArgs
Tries to process all valid arguments supplied to the program
on the command line.
	Return value	0 = all arguments valid AND
				R library path specified
			1 = at least one bad argument OR
				R library path not specified
*/

int ProcessArgs(int nargs,char **arg,IL_INFO *il_info) {
 int optchar;
 int badargs = 0;

 while((optchar = getopt(nargs,arg,"ht:")) != -1) {
  switch(optchar) {
   case 'h' :
    Usage();
    break;
   case 't' :
    il_info->top_dir = optarg;
    break;
   case '?' :
    badargs++;
    fprintf(stderr,"Invalid argument - %c\n",optopt);
  }
 }
 return(badargs || !il_info->top_dir);
}

void StartHTMLfile(FILE *current_stream) {
 fprintf(current_stream,"<html><head><title>R function 
listing</title></head>\n");
 fprintf(current_stream,"<body><center><h1>R function 
listing</h1></center>\n");
}

void EndHTMLfile(FILE *current_stream) {
 fprintf(current_stream,"</body></html>\n");
}

void GetINDEXpaths(IL_INFO *il_info) {
 char find_cmd[MAX_LINE];
 strcpy(find_cmd,"find ");
 strcat(find_cmd,il_info->top_dir);
 strcat(find_cmd," -name INDEX > INDEX_list.txt\n");
 system(find_cmd);
}

int ProcessINDEXFiles(IL_INFO *il_info) {
 FILE *IFstream;
 char current_file[MAX_LINE];
 char *package_name;
 char *desc_ptr;
 char INDEX_line[MAX_LINE];
 int retval = 1;
 int filename_index;

 il_info->htmlstream = fopen("INDEX_list.html","w");
 if(il_info->htmlstream) {
  StartHTMLfile(il_info->htmlstream);
  IFstream = fopen("INDEX_list.txt","r");
  if(IFstream) {
   while(!feof(IFstream)) {
    if(fgets(current_file,MAX_LINE,IFstream)) {
     filename_index = 0;
     while(current_file[filename_index] > 31) filename_index++;
     current_file[filename_index] = 0;
     il_info->instream = fopen(current_file,"r");
     if(il_info->instream) {
      /* find the last slash in the path */
      package_name = strrchr(current_file,'/');
      /* null terminate at the slash */
      *package_name = 0;
      /* go back to the next slash */
      package_name = strrchr(current_file,'/');
      /* move to the package name */
      package_name++;
      fprintf(il_info->htmlstream,"<h2>%s</h2><table>\n",package_name);
      while(!feof(il_info->instream)) {
       fgets(INDEX_line,MAX_LINE,il_info->instream);
       desc_ptr = INDEX_line;
       while(*desc_ptr > 32) desc_ptr++;
       /* if there was more than one word on the line */
       if(desc_ptr) {
        *desc_ptr = 0;
        *desc_ptr++;
        fprintf(il_info->htmlstream,"<tr><td>%s<td>%s</tr>\n",
         INDEX_line,desc_ptr);
       }
       else fprintf(il_info->htmlstream,"<tr><td>%s</tr>\n",INDEX_line);
      }
      fprintf(il_info->htmlstream,"</table>\n");
      fclose(il_info->instream);
     }
     else printf("Can't open INDEX file %s",current_file);
    }
    retval = 0;
   }
   fclose(IFstream);
  }
  else printf("Can't open INDEX_list.txt\n");
  EndHTMLfile(il_info->htmlstream);
  fclose(il_info->htmlstream);
 }
 else printf("Can't open INDEX_list.html\n");
}

int main(int argc,char *argv[]) {
 IL_INFO il_info;
 int retval = 1;

 InitializeILInfo(&il_info);
 if(!ProcessArgs(argc,argv,&il_info)) {
  GetINDEXpaths(&il_info);
  ProcessINDEXFiles(&il_info);
  /* get rid of the list of INDEX files */
  unlink("INDEX_list.txt");
 }
 else Usage();
 return(retval);
}



From Tom.Mulholland at dpi.wa.gov.au  Thu Dec  2 06:18:27 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 2 Dec 2004 13:18:27 +0800
Subject: [R] Protocol for answering basic questions
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA3D@afhex01.dpi.wa.gov.au>

I would support the notion that there is no defined point, after which you do not need to ask basic questions. I would not like the list to be split.

There is no need to change anything fundamental. I do not believe that it is rude to expect people to put effort into ensuring that they are not needlessly using other people's time, because of their lack of skill. If the response's are at time's a little bit curt, it is an exceptionally small price to pay for the aid given by the list. 

Imho, people who follow the posting guide do not receive inappropriate replies. 

Tom Mulholland 

-----Original Message-----
From: Richard A. O'Keefe [mailto:ok at cs.otago.ac.nz]
Sent: Thursday, 2 December 2004 11:50 AM
To: r-help at stat.math.ethz.ch
Subject: RE: [R] Protocol for answering basic questions

... As for arbitrary thresholds like 2 years, I have been using R since
1996 or 1997, and I would still find it necessary to be on the 'nonexpert'
mailing list.  I beg the keepers of the flame: DON'T split the list.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jarioksa at sun3.oulu.fi  Thu Dec  2 07:30:41 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 02 Dec 2004 08:30:41 +0200
Subject: [R] depth constrained cluster
In-Reply-To: <003601c4d7c3$dce890c0$c97b11ac@Aragorn>
References: <003601c4d7c3$dce890c0$c97b11ac@Aragorn>
Message-ID: <1101969041.19515.5.camel@biol102145.oulu.fi>

On Wed, 2004-12-01 at 18:36, Emmanuel GANDOUIN wrote:
> Please could you help me to find a package to apply a depth-constrained cluster analysis on palaeoecological data (in order to zone subfossil diagram)?
> 
I assume that  you made an exhaustive search in CRAN, and the lack of
answers indicates that there is no such a function in R. You may check
Pierre Legendre's "Progiciel R" instead (yes, this is a different R and
has a priority to the name, our R being a later homonym). The "progiciel
R" is available at Thttp://www.fas.umontreal.ca/biol/casgrain/fr/labo/.
his package seems to have both onedimensional or chronologically
constrained clustering and 2dim or spatially constrained clustering.

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From m.rahgozar at uswr.ac.ir  Thu Dec  2 07:35:28 2004
From: m.rahgozar at uswr.ac.ir (rahgozar)
Date: Thu, 02 Dec 2004 10:05:28 +0330
Subject: [R] Request for Gamma frailty
Message-ID: <200412020639.iB26dO1J002513@hypatia.math.ethz.ch>


  Dear Sir/Madam ,

  I need to use " Gamma Frailty Model " in a simulation study ,so I 
should replicate the program many times and get estimates of 
" THETA and its Variance " or "Variance-Covariance" matrix Or 
Variace of random effect in Gamma frailty model in Survival analysis
at each time of replication.
 But when I use " $frail " and " $Var " and "$THETA " to retrieve 
the estimate of THETA and its Variance for each replicate , I get NULL
response and when I use " $coef " I retrieve the coeficients only but 
not THETA and its Variance.
 Since I need to get THETA and its Variance at each time of replication 
so I need to know the name of these variables as used in the source 
program . 
  
 I would be very grateful if you could kindly guide and help me to use
appropriate statment to get the name of required varibles used in the 
source program or if you could introduce me to anybody that could help 
me .    

  Thanks a lot in advance .
 
 With The Best Regards ,

  Mehdi Rahgozar



From bhx2 at mevik.net  Thu Dec  2 09:08:39 2004
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Thu, 02 Dec 2004 09:08:39 +0100
Subject: [R] Combined variable names
In-Reply-To: <x2pt1t8wta.fsf@biostat.ku.dk> (Peter Dalgaard's message of "01
	Dec 2004 23:42:41 +0100")
References: <200412010325.iB13Pa6X361377@atlas.otago.ac.nz>
	<41ADCBDD.1070004@acelerate.com> <x2pt1t8wta.fsf@biostat.ku.dk>
Message-ID: <m0r7m92kc8.fsf@bar.nemo-project.org>

Peter Dalgaard writes:

> There are irregularities, e.g. the fact that you do help(foo), not
> help("foo"), but they tend to get a pain in the long run (How do you
> get help on a name contained in a variable?

v <- "lm"; help(v)
works for me :-)

(But I totally agree that the regularity of R (or S) is part of what
makes it so much better than for instance Matlab.  At least for me.)

-- 
Bj??rn-Helge Mevik



From ripley at stats.ox.ac.uk  Thu Dec  2 09:13:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 2 Dec 2004 08:13:37 +0000 (GMT)
Subject: [R] rank in descending order?
In-Reply-To: <21c05c7d04120114167170982e@mail.gmail.com>
References: <21c05c7d04120114167170982e@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0412020807530.2869@gannet.stats>

On Wed, 1 Dec 2004, Jose Quesada wrote:

> Is there any simple solution to get ranks in descending order?
> Example,
> a <- c(10, 98, 98, 98, 99, 100)
> r <- rank(a, ties.method="average")
>
> produces
> 1 3 3 3 5 6
>
> I would want this instead:
> 6 5 3 3 3 1

What does that correspond to (why are 98 and 99 ranked the same)?  I 
believe you want 6 4 4 4 2 1, which is length(a) + 1 - r, as well as 
rank(-a).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From HankeA at mar.dfo-mpo.gc.ca  Wed Dec  1 20:07:16 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Wed, 01 Dec 2004 15:07:16 -0400
Subject: [R] Hexidecimal conversion
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124A75@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041201/91d1c2d8/attachment.pl

From anne.piotet at urbanet.ch  Thu Dec  2 09:34:01 2004
From: anne.piotet at urbanet.ch (anne.piotet@urbanet.ch)
Date: Thu, 2 Dec 2004 09:34:01 +0100
Subject: [R] problem with using transace
Message-ID: <200412020834.iB28Y1i1031754@smtp.hispeed.ch>

Frank, thank you very much for your answer, as it fixed up nicely my problem. I am really sorry to have asked a question which you already answered a long time ago, but I am a bit overwhelmed with the R, contributed packages and help list documentation....Anyway  thanks again


Anne


PS isn't it fun to use packages developped for biostats in metal industry applications? 

>anne.piotet at urbanet.ch wrote:
>>>I am trying to use the Hmisc function transace to transform predictors
>>>
>>>test<-cbind(flowstress,pressres,alloy)
>>>xtrans<-transace(x,binary=pressres',monotonic='flowstress', categorical='alloy')
>>>
>>>
>>>and I am getting the following message??
>>>Error in ace(x[, -i], x[, i], monotone = im, categorical = ic) : 
>>>	unused argument(s) (monotone ...)
>>>
>>>Any idea?
>> 
>> 
>> thanks anne
>> 
>>>thank for your help
>>>Anne
>
>The corrected version (below) will fix that problem but note that there 
>is a bug in ace causing it not to allow a monotonicity constraint when a 
>variable is on the left hand side.  This is inconsistent with the ace 
>documentation.  There are other problems in ace in which it checks 
>column numbers against the number of rows in the x matrix instead of the 
>number of columns.  The internal version of ace defined inside areg.boot 
>fixes the latter problem.  Note that I reported these problems a long 
>time ago.
>
>Frank
>
>transace <- function(x, monotonic=NULL, categorical=NULL, binary=NULL, 
>pl=TRUE) {
>   if(.R.) require('acepack')  # provides ace, avas
>
>nam <- dimnames(x)[[2]]
>omit <- is.na(x %*% rep(1,ncol(x)))
>omitted <- (1:nrow(x))[omit]
>if(length(omitted)) x <- x[!omit,]
>p <- ncol(x)
>xt <- x  # binary variables retain original coding
>if(!length(nam)) stop("x must have column names")
>rsq <- rep(NA, p)
>names(rsq) <- nam
>
>
>for(i in (1:p)[!(nam %in% binary)])	{
>   lab <- nam[-i]
>   w <- 1:(p-1)
>   im <- w[lab %in% monotonic]
>   ic <- w[lab %in% categorical]
>   if(nam[i] %in% monotonic) im <- c(0, im)
>   if(nam[i] %in% categorical) ic <- c(0, ic)
>   m <- 10*(length(im)>0)+(length(ic)>0)
>   if(m==11) a <- ace(x[,-i], x[,i], mon=im, cat=ic)
>   else if (m==10) a <- ace(x[,-i], x[,i], mon=im)
>   else if(m==1) a <- ace(x[,-i], x[,i], cat=ic)
>   else a <- ace(x[,-i], x[,i])
>   xt[,i] <- a$ty
>   rsq[i] <- a$rsq
>   if(pl)plot(x[,i], xt[,i], xlab=nam[i], ylab=paste("Transformed",nam[i]))
>}	
>
>cat("R-squared achieved in predicting each variable:\n\n")
>print(rsq)
>
>attr(xt, "rsq") <- rsq
>attr(xt, "omitted") <- omitted
>invisible(xt)
>}
>
>>>
>>>
>> 
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>
>
>-- 
>Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt University
>
>



From marwan.khawaja at aub.edu.lb  Wed Dec  1 21:41:14 2004
From: marwan.khawaja at aub.edu.lb (Marwan Khawaja)
Date: Wed, 1 Dec 2004 22:41:14 +0200
Subject: [R] Protocol for answering basic questions
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B73@expressa.corp.cefas.co.uk>
Message-ID: <E1CZlP3-0007Sw-00@spool.aub.edu.lb>

Well, if you do not like the way some people answer queries, why not just delete
the reply without reading the response.
Since we're not paying anyone for answering questions, we should be grateful to
those who put their time in replying to our basic questions.
And why join this community? -- if you think most are 'conceptually na??ve'!

Marwan
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert 
> Brown FM CEFAS
> Sent: Wednesday, December 01, 2004 6:46 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Protocol for answering basic questions
> 
> I have been following the discussions on 'Reasons not to 
> answer very basic questions in a straightforward way' with 
> interest as someone who is also new to R and has had similar 
> experiences.  As such it with sadness that I note that most 
> seem to agree with the present approach to the responses to 
> basic questions.  I must thank those respondants to my own 
> questions who have been helpful, but there are some whose 
> replies are in my opinion not only unhelpful but actually 
> rude.  Indeed I've now started using Splus instead of R so as 
> to have access to a 'proper' support service.  Indeed, the 
> main thing I've learned from R is a new respect for the 
> values of commercial software and a scepticism regarding free 
> software. In the end my experience of r help is that you get 
> what you pay for. Many of the so called socratic responses 
> (in this list and the wider academic community) can be seen 
> as simply way to avoid additional work of a complete reply. 
> 
> Experienced R users don't seem to understand how difficult 
> the program can be to new users. Responding that the 
> questioner should read the 'Introduction to R' or a similar 
> document is like  answering a question for directions to 
> one's house with 'Buy a map'.  Most likely both such 
> questioners have already tried that and are asking because 
> that approach failed.  R is a language and like all languages 
> it is simple to those that understand it and complex to those 
> who do not. Every schoolboy in Spain speaks Spanish, but I 
> know from experience that for most English people it is very 
> difficult to learn Spanish and take years of study.  If I'm 
> asked a question from a novice of a language (be it Spanish 
> or R) do I reply 'consult the dictionary'. I would hope not!  
> I can tell repondants that whilst many of my basic questions 
> may seem simple it is not for lack of studying the very 
> sources they refer to.  If only learning was so simple.  I 
> suspect that the same is true of most question!
>  ers.
> 
> I speak as someone with a PhD and many years as a researcher 
> in my speciality as well as someone close to completing a 
> masters in statistics with distinction. As such I am not a 
> total novice and would suggest that if I'm having problems so 
> are many; and it is not a result of lack of study as so many 
> responses seem to suggest.  Indeed it is revealing that 
> several responses suggest that they want to discourage 
> questions so they don't overwhelm r-help.  Understandable but 
> not a recipe to encourage the use of R by other than experts. 
> The R community needs to decide of they really only want 
> expert statisticians users and make this clear if it is the 
> case.  Alternatively if they are to encourage novices the 
> present approach is not the way to do it.
> 
> I can appreciate that many of the respondants are busy, but 
> if that is the case it would be better if they didn't reply 
> at all. I was taught many years ago that if you can't say 
> anything nice/useful then to say nothing at all.  Something 
> similar could well be applied to this list.  I must say that 
> some respondants are very helpful; and I thank them.  Leave 
> these simple questions to such people.  Indeed it seems 
> surprising that some exteremely experienced R users choose to 
> reply to these basic messages at all; and it seem it is 
> mostly these people who are rude.  I would have thought it 
> might be better for them to concentrate on complex problems 
> more suited to their skills and interests and leave the 
> simple questions to more sympathetic souls.
> 
> Perhaps there is a case for two r help lists catering to 
> basic and advanced questions? Certainly if the R community is 
> serious about appealling to users outside advanced 
> statisticians there is a need for a change of approach in r 
> help and elsewhere.  Russ Ackoff identified much of the 
> failure of management science as due to those who were 
> 'mathematically sophisticated but conceptually naive' and 
> much the same could be said for many in the R community.
> 
> Finally, let me once again thank those who have responded 
> helpful to my queries in the past and ask them to continue in 
> that vein; their assistance and effort is greatly appreciated.
> 
> 
> 
>   
> 
> 
> **************************************************************
> *********************
> This email and any attachments are intended for the named 
> recipient(s) only.  Its unauthorised use, distribution, 
> disclosure, storage or copying is not permitted.  If you have 
> received it in error, please destroy all copies and notify 
> the sender.  In messages of a non-business nature, the views 
> and opinions expressed are the author's own and do not 
> necessarily reflect those of the organisation from which it 
> is sent.  All emails may be subject to monitoring.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From j.logsdon at quantex-research.com  Thu Dec  2 09:40:08 2004
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Thu, 2 Dec 2004 08:40:08 +0000 (GMT)
Subject: [R] 2.0.1 compilation problem on Fedora Core 2
In-Reply-To: <Pine.LNX.4.61.0412011911390.27491@gannet.stats>
Message-ID: <Pine.LNX.4.10.10412020822130.21617-100000@mercury.quantex>

Thanks to Brian this was solved immediately without rebooting.  

The reason for /tmp being (rw,noexec,nosuid) was to avoid executing and
suiding.  This is probably more important in a networked environment which
the machine is not really in at the moment.  I may well change it as it
has already caused two problems!  On the other hand other programs have
installed without a problem. Maybe there is a case for a tmp directory
being set up in the source tree, then there will be no problem.

JOhn

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


On Wed, 1 Dec 2004, Prof Brian Ripley wrote:

> On Wed, 1 Dec 2004, John Logsdon wrote:
> 
> > A useful clue, Brian.  Maybe this is the reason as foreign.ts.out
> > contains:
> >
> > * Installing *source* package 'foreign' ...
> > /usr/src/redhat/SOURCES/R-2.0.1/bin/INSTALL: ./configure: /bin/sh: bad
> > interpreter: Permission denied
> > ERROR: configuration failed for package 'foreign'
> >
> > Does this try and execute from /tmp by any chance?  I had a similar
> 
> Yes, it does, a configure script.  I believe you can alter that by setting 
> TMPDIR to point somewhere else.  That's a pretty esoteric situation: why 
> is /tmp so special to you?
> 
> > problem recently because /tmp is mounted (rw,noexec,nosuid) for security
> > reasons.  I had to alter /etc/fstab temporarily and reboot as /tmp is used
> > by many things.
> >
> > The others installed so far have 0 size:
> >
> > -rw-r--r--  1 root root      0 Dec  1 15:19 VR.ts
> > -rw-r--r--  1 root root      0 Dec  1 15:19 boot.ts
> > -rw-r--r--  1 root root    196 Dec  1 15:20 foreign.ts.out
> > -rw-r--r--  1 root root      0 Dec  1 15:20 cluster.ts
> >
> > John
> >
> > John Logsdon                               "Try to make things as simple
> > Quantex Research Ltd, Manchester UK         as possible but not simpler"
> > j.logsdon at quantex-research.com              a.einstein at relativity.org
> > +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
> >
> >
> > On Wed, 1 Dec 2004, Prof Brian Ripley wrote:
> >
> >> On Wed, 1 Dec 2004, John Logsdon wrote:
> >>
> >>> I have a compilation problem on FC2, 2xXeon box.
> >>>
> >>> The following dialogue output from the end of the compilation illustrates:
> >>>
> >>> [very large snipping sound ...]
> >>> * DONE (cluster)
> >>> begin installing recommended package foreign
> >>> make[2]: *** [foreign.ts] Error 1
> >>> make[2]: Leaving directory
> >>> `/usr/src/redhat/SOURCES/R-2.0.1/src/library/Recommended'
> >>
> >> Take a look at the file foreign.out in that directory.  (R-devel does this
> >> better, by cat-ing the file at that point.)
> >>
> >> --
> >> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >> University of Oxford,             Tel:  +44 1865 272861 (self)
> >> 1 South Parks Road,                     +44 1865 272866 (PA)
> >> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From p.gamble at zeria.com  Thu Dec  2 09:48:47 2004
From: p.gamble at zeria.com (Patrick Gamble)
Date: Thu, 2 Dec 2004 08:48:47 -0000
Subject: [R] Protocol for answering basic questions
References: <E1CZlP3-0007Sw-00@spool.aub.edu.lb>
Message-ID: <001001c4d84b$c1705d90$0300a8c0@home>

Possessing neither a PhD, nor being near a Masters in Statistics 
(with distinction no less), I was surprised that it is possible to
reach these august heights without mastering at least one statistical
programming language sufficiently well not to need to resort
regularly to help services. 

Is this a sign of the times? I'm told that candidates are not as well 
prepared as they used to be.



From p.dalgaard at biostat.ku.dk  Thu Dec  2 09:59:52 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Dec 2004 09:59:52 +0100
Subject: [R] Combined variable names
In-Reply-To: <m0r7m92kc8.fsf@bar.nemo-project.org>
References: <200412010325.iB13Pa6X361377@atlas.otago.ac.nz>
	<41ADCBDD.1070004@acelerate.com> <x2pt1t8wta.fsf@biostat.ku.dk>
	<m0r7m92kc8.fsf@bar.nemo-project.org>
Message-ID: <x2y8ghdqif.fsf@biostat.ku.dk>

bhx2 at mevik.net (Bj??rn-Helge Mevik) writes:

> Peter Dalgaard writes:
> 
> > There are irregularities, e.g. the fact that you do help(foo), not
> > help("foo"), but they tend to get a pain in the long run (How do you
> > get help on a name contained in a variable?
> 
> v <- "lm"; help(v)
> works for me :-)

Eh? When did that sneak in? (1.8.0, I suppose) It's actually not as
working as documented...

   topic: usually, the name on  which documentation is sought. The name
          may be quoted or unquoted (but note that if 'topic' is the
          name of a variable containing a character string
          documentation is provided for the name, not for the character
          string).


c <- "lm"; help(c)

works too, for some values of "works"... (I.e. not if you were looking
for help on c()). Also

> help(.Library)
No documentation for '/usr/lib/R/library' in specified packages and
libraries:
you could try 'help.search("/usr/lib/R/library")'

whereas ?.Library does work. 

And my workaround inside tkStartGUI is now exactly the Wrong Thing to
do. Oh well...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Dec  2 10:08:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 2 Dec 2004 09:08:20 +0000 (GMT)
Subject: [R] 2.0.1 compilation problem on Fedora Core 2
In-Reply-To: <Pine.LNX.4.10.10412020822130.21617-100000@mercury.quantex>
References: <Pine.LNX.4.10.10412020822130.21617-100000@mercury.quantex>
Message-ID: <Pine.LNX.4.61.0412020905050.3790@gannet.stats>

On Thu, 2 Dec 2004, John Logsdon wrote:

> Thanks to Brian this was solved immediately without rebooting.
>
> The reason for /tmp being (rw,noexec,nosuid) was to avoid executing and
> suiding.  This is probably more important in a networked environment which
> the machine is not really in at the moment.  I may well change it as it
> has already caused two problems!  On the other hand other programs have
> installed without a problem. Maybe there is a case for a tmp directory
> being set up in the source tree, then there will be no problem.

As I said, why is /tmp so special to you?  If you can exec and suid from 
the source tree, what useful extra protection do you have?

We definitely do not want to unpack in the source tree, which on my 
machine is on a readonly file system (unpacked on another machine).
Perhaps in the build tree, but R CMD INSTALL is also used later.

A note in the R-admin manual seems sufficient here, as this it the only 
time this has come up in literally thousands of installations.

>
> JOhn
>
> John Logsdon                               "Try to make things as simple
> Quantex Research Ltd, Manchester UK         as possible but not simpler"
> j.logsdon at quantex-research.com              a.einstein at relativity.org
> +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
>
>
> On Wed, 1 Dec 2004, Prof Brian Ripley wrote:
>
>> On Wed, 1 Dec 2004, John Logsdon wrote:
>>
>>> A useful clue, Brian.  Maybe this is the reason as foreign.ts.out
>>> contains:
>>>
>>> * Installing *source* package 'foreign' ...
>>> /usr/src/redhat/SOURCES/R-2.0.1/bin/INSTALL: ./configure: /bin/sh: bad
>>> interpreter: Permission denied
>>> ERROR: configuration failed for package 'foreign'
>>>
>>> Does this try and execute from /tmp by any chance?  I had a similar
>>
>> Yes, it does, a configure script.  I believe you can alter that by setting
>> TMPDIR to point somewhere else.  That's a pretty esoteric situation: why
>> is /tmp so special to you?
>>
>>> problem recently because /tmp is mounted (rw,noexec,nosuid) for security
>>> reasons.  I had to alter /etc/fstab temporarily and reboot as /tmp is used
>>> by many things.
>>>
>>> The others installed so far have 0 size:
>>>
>>> -rw-r--r--  1 root root      0 Dec  1 15:19 VR.ts
>>> -rw-r--r--  1 root root      0 Dec  1 15:19 boot.ts
>>> -rw-r--r--  1 root root    196 Dec  1 15:20 foreign.ts.out
>>> -rw-r--r--  1 root root      0 Dec  1 15:20 cluster.ts
>>>
>>> John
>>>
>>> John Logsdon                               "Try to make things as simple
>>> Quantex Research Ltd, Manchester UK         as possible but not simpler"
>>> j.logsdon at quantex-research.com              a.einstein at relativity.org
>>> +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
>>>
>>>
>>> On Wed, 1 Dec 2004, Prof Brian Ripley wrote:
>>>
>>>> On Wed, 1 Dec 2004, John Logsdon wrote:
>>>>
>>>>> I have a compilation problem on FC2, 2xXeon box.
>>>>>
>>>>> The following dialogue output from the end of the compilation illustrates:
>>>>>
>>>>> [very large snipping sound ...]
>>>>> * DONE (cluster)
>>>>> begin installing recommended package foreign
>>>>> make[2]: *** [foreign.ts] Error 1
>>>>> make[2]: Leaving directory
>>>>> `/usr/src/redhat/SOURCES/R-2.0.1/src/library/Recommended'
>>>>
>>>> Take a look at the file foreign.out in that directory.  (R-devel does this
>>>> better, by cat-ing the file at that point.)
>>>>
>>>> --
>>>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>>> University of Oxford,             Tel:  +44 1865 272861 (self)
>>>> 1 South Parks Road,                     +44 1865 272866 (PA)
>>>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rconroy at rcsi.ie  Thu Dec  2 10:10:05 2004
From: rconroy at rcsi.ie (=?ISO-8859-1?Q?Ron=E1n_Conroy?=)
Date: Thu, 02 Dec 2004 09:10:05 +0000
Subject: [R] Final comment from a newbie
Message-ID: <41AEDBED.2000300@rcsi.ie>

As a person new to R, I joined the list. I am a long-term member of the 
Stata list, which has a tradition of being very helpful to newbies. 
Imagine my surprise when my first day's mail included a rough rebuke to 
someone for asking a simple question (including mentioning that the 
poster had been reprimanded about this sort of thing before).

In the week I've been on it, there have been questions I could not 
understand, struggling as I am with the basics of R syntax, which were 
answered, and questions I could understand, which were met with 
irritation and rebuke.

Made me decide not to post my hopelessly amateur question on getting R 
to remember my proxy settings between sessions...

It's reasonable to say that for every naive question there were at least 
two curt replies. Amazing how people can find the time to be rude, given 
how busy they must be.

Well, I'm signing off now. I will learn R, but from the book. And I will 
look forward to the time when the list divides into two lists: R-help 
and R-helpful.

R is a great project. It's a pity there's no forum that will help the 
struggling newcomer.

-- 

Ronan M Conroy (rconroy at rcsi.ie) 
Senior Lecturer in Biostatistics 
Royal College of Surgeons 
Dublin 2, Ireland 
+353 1 402 2431 (fax 2764) 
-------------------- 
Just say no to drug reps 
http://www.nofreelunch.org/



From r.g.brown at cefas.co.uk  Thu Dec  2 10:14:14 2004
From: r.g.brown at cefas.co.uk (Robert Brown FM CEFAS)
Date: Thu, 2 Dec 2004 09:14:14 -0000
Subject: [R] Protocol for answering basic questions
Message-ID: <3589BC4D64C84341AE0C258244F977A2B60B77@expressa.corp.cefas.co.uk>

A very interesting document which certainly makes me understand the attituides on the r help list better. Perhaps it should be part of the protocol for submitting questions.

Certainly I hadn't realised before that politeness and technical knowledge were so widely accepted as mutually exclusive. Neither had I realised that rudeness was considered such a powerful motivational force for learning.  It is very sad to realise how much of a chore politeness is viewed by some people.  I was also very naive to think that some people answered questions as a way to return something to the community rather than purely for selfish reasons.  Long live misanthropy.  At least now I will be able to approach r help and other mailing lists with a more realistic attitude.

It is interesting to note that there is a widespread notion that many questions result from insufficient research by the questioner.  As someone with a serious interest in R who is new to the environment I can't stress enough that I have never seen a question that I thought was trivial and didn't deserve a reply.  Indeed in my position I often find the answers to these questions the most useful. Often I find myself spending hours searching for a solution to a problem that I know is simple, but which I just don't have the experience to find (R documentation can be cryptic, assumes a lot of knowledge of programming and is weak in addressing programming/data manipulation issues). When I do find the answer I invariably cry 'of course' and researching such questions independently is not a learning experience, just a pain.   I know an experienced user could answer such questions in seconds and surely r help is an appropriate forum to address these questions.   

I realise that many users are busy and don't want to be swamped with 'uninteresting' emails.  However, if there are so many of these 'basic' questions that r help is in danger of being swamped then there is clearly a need which should be addressed.  If there are only a few then what is the problem.  Let those prepared to answer do so and the others can just ignore them, but accept their utility.   

-----Original Message-----
From: Greer, Braden (NIH/NCI) [mailto:greerb at mail.nih.gov]
Sent: 01 December 2004 17:05
To: Robert Brown FM CEFAS
Subject: RE: [R] Protocol for answering basic questions



Thanks for your challenge, Robert.  It's a breath of fresh air.

What you've astutely observed is plaguing the online forum community at
large, unfortunately.  And I believe it comes down to the original sin (of
which I too suffer) of human pride.  Folks have written extensive documents
(http://www.catb.org/~esr/faqs/smart-questions.html) detailing how to dance
around the egos in the online forums (while I appreciate their efforts to
write this--it's unfortunate that it had to be written).  The dogma of help
forums is counterproductive and a bit childish.  Perhaps your words will
make some think for a minute.

Braden Greer



-----Original Message-----
From: Robert Brown FM CEFAS [mailto:r.g.brown at cefas.co.uk] 
Sent: Wednesday, December 01, 2004 11:46 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Protocol for answering basic questions


I have been following the discussions on 'Reasons not to answer very basic
questions in a straightforward way' with interest as someone who is also new
to R and has had similar experiences.  As such it with sadness that I note
that most seem to agree with the present approach to the responses to basic
questions.  I must thank those respondants to my own questions who have been
helpful, but there are some whose replies are in my opinion not only
unhelpful but actually rude.  Indeed I've now started using Splus instead of
R so as to have access to a 'proper' support service.  Indeed, the main
thing I've learned from R is a new respect for the values of commercial
software and a scepticism regarding free software. In the end my experience
of r help is that you get what you pay for. Many of the so called socratic
responses (in this list and the wider academic community) can be seen as
simply way to avoid additional work of a complete reply. 

Experienced R users don't seem to understand how difficult the program can
be to new users. Responding that the questioner should read the
'Introduction to R' or a similar document is like  answering a question for
directions to one's house with 'Buy a map'.  Most likely both such
questioners have already tried that and are asking because that approach
failed.  R is a language and like all languages it is simple to those that
understand it and complex to those who do not. Every schoolboy in Spain
speaks Spanish, but I know from experience that for most English people it
is very difficult to learn Spanish and take years of study.  If I'm asked a
question from a novice of a language (be it Spanish or R) do I reply
'consult the dictionary'. I would hope not!  I can tell repondants that
whilst many of my basic questions may seem simple it is not for lack of
studying the very sources they refer to.  If only learning was so simple.  I
suspect that the same is true of most question!  ers.

I speak as someone with a PhD and many years as a researcher in my
speciality as well as someone close to completing a masters in statistics
with distinction. As such I am not a total novice and would suggest that if
I'm having problems so are many; and it is not a result of lack of study as
so many responses seem to suggest.  Indeed it is revealing that several
responses suggest that they want to discourage questions so they don't
overwhelm r-help.  Understandable but not a recipe to encourage the use of R
by other than experts. The R community needs to decide of they really only
want expert statisticians users and make this clear if it is the case.
Alternatively if they are to encourage novices the present approach is not
the way to do it.

I can appreciate that many of the respondants are busy, but if that is the
case it would be better if they didn't reply at all. I was taught many years
ago that if you can't say anything nice/useful then to say nothing at all.
Something similar could well be applied to this list.  I must say that some
respondants are very helpful; and I thank them.  Leave these simple
questions to such people.  Indeed it seems surprising that some exteremely
experienced R users choose to reply to these basic messages at all; and it
seem it is mostly these people who are rude.  I would have thought it might
be better for them to concentrate on complex problems more suited to their
skills and interests and leave the simple questions to more sympathetic
souls.

Perhaps there is a case for two r help lists catering to basic and advanced
questions? Certainly if the R community is serious about appealling to users
outside advanced statisticians there is a need for a change of approach in r
help and elsewhere.  Russ Ackoff identified much of the failure of
management science as due to those who were 'mathematically sophisticated
but conceptually naive' and much the same could be said for many in the R
community.

Finally, let me once again thank those who have responded helpful to my
queries in the past and ask them to continue in that vein; their assistance
and effort is greatly appreciated.



  


****************************************************************************
*******
This email and any attachments are intended for the named re...{{dropped}}



From michael.watson at bbsrc.ac.uk  Thu Dec  2 10:17:29 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 2 Dec 2004 09:17:29 -0000
Subject: [R] Final comment from a newbie
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E8993A@iahce2knas1.iah.bbsrc.reserved>

> Made me decide not to post my hopelessly amateur question on getting 
> R to remember my proxy settings between sessions...

On Linux/unix or Windows or something else?

On linux/unix, set the environment variable http_proxy *and* UNSET the environment variable no_proxy, if it exists.
On Windows, start R with the option --internet2 will pick up the proxy information from Internet explorer.

Mick

P.s. the best way to learn R is not from a book IMHO; sit down, just you, your PC and R, and sweat it out :-) Trust me, it's worth it!

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ron??n Conroy
Sent: 02 December 2004 09:10
To: R Help list
Subject: [R] Final comment from a newbie


As a person new to R, I joined the list. I am a long-term member of the 
Stata list, which has a tradition of being very helpful to newbies. 
Imagine my surprise when my first day's mail included a rough rebuke to 
someone for asking a simple question (including mentioning that the 
poster had been reprimanded about this sort of thing before).

In the week I've been on it, there have been questions I could not 
understand, struggling as I am with the basics of R syntax, which were 
answered, and questions I could understand, which were met with 
irritation and rebuke.

Made me decide not to post my hopelessly amateur question on getting R 
to remember my proxy settings between sessions...

It's reasonable to say that for every naive question there were at least 
two curt replies. Amazing how people can find the time to be rude, given 
how busy they must be.

Well, I'm signing off now. I will learn R, but from the book. And I will 
look forward to the time when the list divides into two lists: R-help 
and R-helpful.

R is a great project. It's a pity there's no forum that will help the 
struggling newcomer.

-- 

Ronan M Conroy (rconroy at rcsi.ie) 
Senior Lecturer in Biostatistics 
Royal College of Surgeons 
Dublin 2, Ireland 
+353 1 402 2431 (fax 2764)
-------------------- 
Just say no to drug reps 
http://www.nofreelunch.org/

______________________________________________
R-help at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From j.logsdon at quantex-research.com  Thu Dec  2 10:19:26 2004
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Thu, 2 Dec 2004 09:19:26 +0000 (GMT)
Subject: [R] Protocol for answering basic questions
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B73@expressa.corp.cefas.co.uk>
Message-ID: <Pine.LNX.4.10.10412020843370.21617-100000@mercury.quantex>

A brief note from someone who has rejoined the list after a year or so.  I
notice that it is more busy that it was - about 50 messages a day.  This
is not surprising - R has essentially taken over from many other packages
for statistical computing these days and has a massive user base.  It is
one of the shining examples of the power of the open source development
community and a great tribute to the core team.

But the excessive amount of mail is a bother - much of it is quite trivial
to an expert but essential to a beginner. 

There are three ways of tackling this as far as I see:

First would be to make the list a Reply to Sender so that most of us don't
see the replies. This would keep the traffic down and if any topic was of
interest to another member, s/he could ask the originator whether it had
been solved or the solution could also be posted as a summary.  One
advantage of Reply to Sender is that it is only the Sender sees the
multiple messages sent saying the same thing from good souls around the
world who haven't seen the N-1 other messages...

Second is to split it and here, there appear to be 3 types of questions on
this list:

a) Installation or questions that are primarily to do with computing -
such as the problem I had that was immediately answered by the ever
helpful Brian Ripley;

b) R-coding and programming questions, such as the one about subsetting
columns in a matrix.  In principle these are RTFM questions but TFM is not
always explicit and may not be to hand;

c) Advances statistical questions, such as the one on lmeControl.

Of course, there is always a crossover, particularly between b) and c) and
this could be done instead of actually splitting the list by subject line
comments like INSTALL, RTFM and ADVANCED much as is done in Allstat.

The third way is more formal and to have a ticket system so that the
request is 'taken' by the first person who wants to help.  This is much
more difficult in an OS environment and would need a dynamic web site
writing ...

I am sure that this has all been thought about and discussed in my year's
absence but sometimes people are too close and busy to have time to see.

I would suggest a combination of the first two approaches would reduce the
traffic to a manageable amount so that members can see the wood for the
trees. Otherwise people will drop in and out as they see the need and
treat it as a service facility without then contributing to the community.

R is really a victim of it's own success.

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com



From michael.watson at bbsrc.ac.uk  Thu Dec  2 10:30:11 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 2 Dec 2004 09:30:11 -0000
Subject: [R] Gap between axis and bars in barplot()
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E8993C@iahce2knas1.iah.bbsrc.reserved>

Hi

Windows XP, R 2.0.1.

I am drawing a very large barplot using jpeg() - setting the width to
10,000 as there are over 5000 bars.

This all works fine and I get exactly what I want - except there is a
huge bit of white space between the Y-axis and the first bar - so much
in fact that I have to scroll two screens from the Y-axis before I see
the first bar.  After that the bars are evenly spaced and close
together.

Any ideas what is going on here?

Thanks in advance

Mick



From siegfried.gonzi at stud.uni-graz.at  Thu Dec  2 10:18:10 2004
From: siegfried.gonzi at stud.uni-graz.at (Siegfried Gonzi)
Date: Thu, 02 Dec 2004 10:18:10 +0100
Subject: [R] A somewhat off the line question to a log normal distribution
References: <Pine.LNX.4.10.10412020843370.21617-100000@mercury.quantex>
Message-ID: <41AEDDD2.1040700@stud.uni-graz.at>

Hello:

Oh yes I know it isn't so much related to R, but I gather there are a 
lot of statisticians reading the mailing list.

My boss repeatedly tried to explain me the following.

==
Lets assume you have got daily measurements of a variable in natural 
sciences. It turned out that the aformentioned daily measurements follow 
a log-normal distribution when considered over the course of a year. 
Okay. He also tried to explain me that the monthly means (based on the 
daily measurements) must follow a log-normal distribution too then over 
the course of a year.
==

I somehow get his explanation.

But I have measurements which are log-normal distributed when evaluated 
on a daily basis over the course of a year  but they are close to a 
Gaussian distribution when considered under the light of monthly means 
over the course of a year.

Is such a latter case feasible. And if not why.

Regards,
Siegfried Gonzi

>
>
>



From Michael.Griffiths at lgc.co.uk  Thu Dec  2 10:42:17 2004
From: Michael.Griffiths at lgc.co.uk (Michael Griffiths)
Date: Thu, 02 Dec 2004 09:42:17 +0000
Subject: [R] Final comment from a newbie_and another
Message-ID: <s1aee393.000@tedmail2.lgc.co.uk>

I wholeheartedly agree. Maybe in time the list will divide into two whereby 'apparantly' simple questions will be answered without a curt rebuke!
I too am now signing off the list and will bury my head in a manual ot two. As a last comment; everyone has to start the learning process somewhere and lets face it, with a deadline looming and what appears to be an insurmaountable question wouldn't anyone ask for help?

Signing off now.......maybe in a hour just to see some responses to these postings  

Michael Griffiths, Ph.D.
Chemometrician
Training, Quality and Statistics Group
LGC Limited
Queens Road
Teddington
Middlesex, TW11 0LY, UK
Tel: +44 (0)20 8943 7352
Fax: +44 (0)20 8943 2767
e-mail: michael.griffiths at lgc.co.uk

>>> Ron??n Conroy <rconroy at rcsi.ie> 02/12/2004 09:10:05 >>>
As a person new to R, I joined the list. I am a long-term member of the 
Stata list, which has a tradition of being very helpful to newbies. 
Imagine my surprise when my first day's mail included a rough rebuke to 
someone for asking a simple question (including mentioning that the 
poster had been reprimanded about this sort of thing before).

In the week I've been on it, there have been questions I could not 
understand, struggling as I am with the basics of R syntax, which were 
answered, and questions I could understand, which were met with 
irritation and rebuke.

Made me decide not to post my hopelessly amateur question on getting R 
to remember my proxy settings between sessions...

It's reasonable to say that for every naive question there were at least 
two curt replies. Amazing how people can find the time to be rude, given 
how busy they must be.

Well, I'm signing off now. I will learn R, but from the book. And I will 
look forward to the time when the list divides into two lists: R-help 
and R-helpful.

R is a great project. It's a pity there's no forum that will help the 
struggling newcomer.

-- 

Ronan M Conroy (rconroy at rcsi.ie) 
Senior Lecturer in Biostatistics 
Royal College of Surgeons 
Dublin 2, Ireland 
+353 1 402 2431 (fax 2764) 
-------------------- 
Just say no to drug reps 
http://www.nofreelunch.org/ 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

*******************************************************************
This email and any attachments are confidential. Any use, co...{{dropped}}



From vito_ricci at yahoo.com  Thu Dec  2 11:07:35 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 2 Dec 2004 11:07:35 +0100 (CET)
Subject: [R] Re: A somewhat off the line question to a log normal
	distribution
Message-ID: <20041202100735.9087.qmail@web41202.mail.yahoo.com>

Dear Siegfried,

I believe your boss is wrong saying that:
>He also tried to explain me that the monthly means
>(based on the daily measurements) must follow a
>log-normal distribution too then over the course of a
year.

every statistician know that increasing the sample
size the sample distribution of the mean is proxy to a
gaussian distribution (Central Limit Theorem)
independently from the original distribution of data
(in your case log-normal).

See this:

The Central Limit Theorem is a statement about the
characteristics of the sampling distribution of means
of random samples from a given population. That is, it
describes the characteristics of the distribution of
values we would obtain if we were able to draw an
infinite number of random samples of a given size from
a given population and we calculated the mean of each
sample.

The Central Limit Theorem consists of three
statements:

[1] The mean of the sampling distribution of means is
equal to the mean of the population from which the
samples were drawn.

[2] The variance of the sampling distribution of means
is equal to the variance of the population from which
the samples were drawn divided by the size of the
samples.

--> [3] If the original population is distributed
normally (i.e. it is bell shaped), the sampling
distribution of means will also be normal. If the
original population is not normally distributed, the
sampling distribution of means will increasingly
approximate a normal distribution as sample size
increases. (i.e. when increasingly large samples are
drawn) <--

So results you got are just in this way!

I think your boss doesn't know well statistics!

Regards
Vito

You wrote:

Hello:

Oh yes I know it isn't so much related to R, but I
gather there are a 
lot of statisticians reading the mailing list.

My boss repeatedly tried to explain me the following.

==
Lets assume you have got daily measurements of a
variable in natural 
sciences. It turned out that the aformentioned daily
measurements follow 
a log-normal distribution when considered over the
course of a year. 
Okay. He also tried to explain me that the monthly
means (based on the 
daily measurements) must follow a log-normal
distribution too then over 
the course of a year.
==

I somehow get his explanation.

But I have measurements which are log-normal
distributed when evaluated 
on a daily basis over the course of a year  but they
are close to a 
Gaussian distribution when considered under the light
of monthly means 
over the course of a year.

Is such a latter case feasible. And if not why.

Regards,
Siegfried Gonzi


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From kahra at mpsgr.it  Thu Dec  2 11:14:30 2004
From: kahra at mpsgr.it (Kahra Hannu)
Date: Thu, 2 Dec 2004 11:14:30 +0100
Subject: [R] A somewhat off the line question to a log normal distribution
Message-ID: <C9FC71F7E9356F40AFE2ACC2099DE14722519A@MAILSERVER-B.mpsgr.it>

Sigfried,

I am not a statistician, but I have learned that according to the Central Limit Theorem (CLT) sums of random variables, regardless of their form, will tend to be normally distributed. CLT does not require the variables in the sum to come from the same underlying distribution.

Ciao,

Hannu Kahra 
Progetti Speciali 
Monte Paschi Asset Management SGR S.p.A. 
Via San Vittore, 37
IT-20123 Milano, Italia 

Tel.: +39 02 43828 754 
Mobile: +39 333 876 1558 
Fax: +39 02 43828 247 
E-mail: kahra at mpsgr.it 
Web: www.mpsam.it 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Siegfried Gonzi
Sent: Thursday, December 02, 2004 10:18 AM
To: r-help at stat.math.ethz.ch
Subject: [R] A somewhat off the line question to a log normal
distribution


Hello:

Oh yes I know it isn't so much related to R, but I gather there are a 
lot of statisticians reading the mailing list.

My boss repeatedly tried to explain me the following.

==
Lets assume you have got daily measurements of a variable in natural 
sciences. It turned out that the aformentioned daily measurements follow 
a log-normal distribution when considered over the course of a year. 
Okay. He also tried to explain me that the monthly means (based on the 
daily measurements) must follow a log-normal distribution too then over 
the course of a year.
==

I somehow get his explanation.

But I have measurements which are log-normal distributed when evaluated 
on a daily basis over the course of a year  but they are close to a 
Gaussian distribution when considered under the light of monthly means 
over the course of a year.

Is such a latter case feasible. And if not why.

Regards,
Siegfried Gonzi

>
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From michael.watson at bbsrc.ac.uk  Thu Dec  2 11:46:33 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 2 Dec 2004 10:46:33 -0000
Subject: [R] Drawing a rectangle around a barplot()
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89943@iahce2knas1.iah.bbsrc.reserved>

Hi

I want to draw a rectangle behind a barplot such that it highlights
those particular bars from the rest of the plot.  I have figured out how
to draw a barplot(), and then how to draw a rectangle of the correct
shape and size (using rect()), but when I use rect() it draws over the
top of the bars, and then when I re-draw the bars, it draws with a white
background, thus eliminating my rectangle!  I can't use box() as I don't
want to draw a box round my entire plot, I just want to draw a box as
the background to certain subsets of the bars.

Can anyone help?  (those of you familiar with microsoft word/excel
drawing tools, I want something similar to the command "send to back"
such that one drawing object is placed behind the others)

Thanks in advance

Mick



From giovanna.jonalasinio at uniroma1.it  Thu Dec  2 11:53:45 2004
From: giovanna.jonalasinio at uniroma1.it (giovanna jona lasinio)
Date: Thu, 2 Dec 2004 11:53:45 +0100
Subject: [R] Computation of space-time empirical crosscovariances
Message-ID: <001201c4d85d$32156e80$65036497@sta.uniroma1.it>

Dear all,
Is there any "simple" way to estimate an empirical crosscovariance
function for space-time data? More precisely if I have n space locations
in T times on which I observe one variable, I cannot find an already
made  R function that can compute the empirical crosscovariance.

Any suggestion?

Thanks 
Giovanna Jona Lasinio



From stuart.leask at nottingham.ac.uk  Thu Dec  2 11:57:31 2004
From: stuart.leask at nottingham.ac.uk (Stuart Leask)
Date: Thu, 2 Dec 2004 10:57:31 -0000
Subject: [R] Protocol for answering basic questions
References: <E1CZlP3-0007Sw-00@spool.aub.edu.lb>
Message-ID: <003e01c4d85d$b8cb35e0$f2e1f380@OPENZAURUS>

I suspect that a lot of the rationalizing of behaviour that has gone on in
this thread (eg. "terse replies prevent the list being overwhelmed") is just
that and no more. In fact, busy people are taking time to reply as
succinctly as they can. In practice the questioner gets a remarkably rapid,
well-informed response. They may not like its contents for any number of
reasons! Most things in life a worth what you pay for them - the currency
here seems to be whether folks consider that a questioner has made an
effort; if so, respondants seem happy to match it.

I think it is astonishing that the R list maintains such an enthusiastic
body of world-class contributors. I look at many other help lists
(commercial and open-source) and see great oceans of unanswered queries.
Hands up anyone who has time to politely answer swathes of questions already
covered elsewhere. That folks complain that they have not had a reply in 48
hours is a testament to the tremendous effort that is put in.

What I would be in favour of is another list: R-complaints! Here folks with
bruised egos and/or higher expectations of their fellow man can pursue
threads like this one into the next millenium....

Stuart



----- Original Message ----- 
From: "Marwan Khawaja" <marwan.khawaja at aub.edu.lb>
To: "'Robert Brown FM CEFAS'" <r.g.brown at cefas.co.uk>;
<r-help at stat.math.ethz.ch>
Sent: Wednesday, December 01, 2004 8:41 PM
Subject: RE: [R] Protocol for answering basic questions


Well, if you do not like the way some people answer queries, why not just
delete
the reply without reading the response.
Since we're not paying anyone for answering questions, we should be grateful
to
those who put their time in replying to our basic questions.
And why join this community? -- if you think most are 'conceptually na??ve'!

Marwan


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert
> Brown FM CEFAS
> Sent: Wednesday, December 01, 2004 6:46 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Protocol for answering basic questions
>
> I have been following the discussions on 'Reasons not to
> answer very basic questions in a straightforward way' with
> interest as someone who is also new to R and has had similar
> experiences.  As such it with sadness that I note that most
> seem to agree with the present approach to the responses to
> basic questions.  I must thank those respondants to my own
> questions who have been helpful, but there are some whose
> replies are in my opinion not only unhelpful but actually
> rude.  Indeed I've now started using Splus instead of R so as
> to have access to a 'proper' support service.  Indeed, the
> main thing I've learned from R is a new respect for the
> values of commercial software and a scepticism regarding free
> software. In the end my experience of r help is that you get
> what you pay for. Many of the so called socratic responses
> (in this list and the wider academic community) can be seen
> as simply way to avoid additional work of a complete reply.
>
> Experienced R users don't seem to understand how difficult
> the program can be to new users. Responding that the
> questioner should read the 'Introduction to R' or a similar
> document is like  answering a question for directions to
> one's house with 'Buy a map'.  Most likely both such
> questioners have already tried that and are asking because
> that approach failed.  R is a language and like all languages
> it is simple to those that understand it and complex to those
> who do not. Every schoolboy in Spain speaks Spanish, but I
> know from experience that for most English people it is very
> difficult to learn Spanish and take years of study.  If I'm
> asked a question from a novice of a language (be it Spanish
> or R) do I reply 'consult the dictionary'. I would hope not!
> I can tell repondants that whilst many of my basic questions
> may seem simple it is not for lack of studying the very
> sources they refer to.  If only learning was so simple.  I
> suspect that the same is true of most question!
>  ers.
>
> I speak as someone with a PhD and many years as a researcher
> in my speciality as well as someone close to completing a
> masters in statistics with distinction. As such I am not a
> total novice and would suggest that if I'm having problems so
> are many; and it is not a result of lack of study as so many
> responses seem to suggest.  Indeed it is revealing that
> several responses suggest that they want to discourage
> questions so they don't overwhelm r-help.  Understandable but
> not a recipe to encourage the use of R by other than experts.
> The R community needs to decide of they really only want
> expert statisticians users and make this clear if it is the
> case.  Alternatively if they are to encourage novices the
> present approach is not the way to do it.
>
> I can appreciate that many of the respondants are busy, but
> if that is the case it would be better if they didn't reply
> at all. I was taught many years ago that if you can't say
> anything nice/useful then to say nothing at all.  Something
> similar could well be applied to this list.  I must say that
> some respondants are very helpful; and I thank them.  Leave
> these simple questions to such people.  Indeed it seems
> surprising that some exteremely experienced R users choose to
> reply to these basic messages at all; and it seem it is
> mostly these people who are rude.  I would have thought it
> might be better for them to concentrate on complex problems
> more suited to their skills and interests and leave the
> simple questions to more sympathetic souls.
>
> Perhaps there is a case for two r help lists catering to
> basic and advanced questions? Certainly if the R community is
> serious about appealling to users outside advanced
> statisticians there is a need for a change of approach in r
> help and elsewhere.  Russ Ackoff identified much of the
> failure of management science as due to those who were
> 'mathematically sophisticated but conceptually naive' and
> much the same could be said for many in the R community.
>
> Finally, let me once again thank those who have responded
> helpful to my queries in the past and ask them to continue in
> that vein; their assistance and effort is greatly appreciated.
>
>
>
>
>
>
> **************************************************************
> *********************
> This email and any attachments are intended for the named
> recipient(s) only.  Its unauthorised use, distribution,
> disclosure, storage or copying is not permitted.  If you have
> received it in error, please destroy all copies and notify
> the sender.  In messages of a non-business nature, the views
> and opinions expressed are the author's own and do not
> necessarily reflect those of the organisation from which it
> is sent.  All emails may be subject to monitoring.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


This message has been scanned but we cannot guarantee that it and any
attachments are free from viruses or other damaging content: you are
advised to perform your own checks.  Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.



From Christoph.Scherber at uni-jena.de  Thu Dec  2 12:00:57 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Thu, 02 Dec 2004 12:00:57 +0100
Subject: [R] treatment contrasts and summary.lm
Message-ID: <41AEF5E9.4090404@uni-jena.de>

Dear list members,

I have a 2-factor ANOVA where the summary.lm output looks like this 
(using treatment contrasts):

                    Value Std. Error t value Pr(>|t|)
      (Intercept)  0.0389  0.0220     1.7695  0.0817
as.factor(Block)1  0.0156  0.0066     2.3597  0.0215
as.factor(Block)2 -0.0018  0.0037    -0.4857  0.6289
as.factor(Block)3 -0.0007  0.0026    -0.2812  0.7795
   as.factor(AZ)1 -0.0066  0.0076    -0.8670  0.3893
   as.factor(AZ)2  0.0064  0.0047     1.3530  0.1810
   as.factor(AZ)3 -0.0015  0.0031    -0.4863  0.6284
   as.factor(AZ)4  0.0054  0.0025     2.1499  0.0355
   as.factor(AZ)5  0.0062  0.0037     1.6653  0.1009

Block has 4 levels and AZ has 6 levels. My question now is: What exactly 
do the values for AZ show? I know it??s somehow differences between 
intercepts, but it would be great if someone could tell me more on how 
exactly to interprete the output.

Thanks very much in advance!
Christoph



From B.Rowlingson at lancaster.ac.uk  Thu Dec  2 12:07:03 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 02 Dec 2004 11:07:03 +0000
Subject: [R] Drawing a rectangle around a barplot()
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89943@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89943@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41AEF757.20907@lancaster.ac.uk>

michael watson (IAH-C) wrote:
> Hi
> 
>  I can't use box() as I don't
> want to draw a box round my entire plot, I just want to draw a box as
> the background to certain subsets of the bars.

   One idea. Edit the barplot.default function so it doesnt call plot.new.

  do:

  myBarplot = barplot.default

  then edit the myBarplot function. Add an extra parameter to the 
argument list 'add', and make it False by default:

             cex.names = par("cex.axis"), inside = TRUE, plot = TRUE,
             axis.lty = 0, add=F, ...)

  then find plot.new and wrap it in a condition:

     if(!add)plot.new()

  Now try:

  tN <- table(Ni <- rpois(100, lambda=5))
  myBarplot(tN)
  rect(2,1,6.5,8.5)

  - this puts the rectangle on top of the bars, which you dont want, so 
you call myBarplot with add=T so that plot.new isnt called and the 
barplot appears over the rectangle:

  myBarplot(tN,add=T)

works for me, if I understand your problem correctly!

Baz



From arturodezan at gmail.com  Thu Dec  2 12:14:15 2004
From: arturodezan at gmail.com (Arturo T. De Zan)
Date: Thu, 2 Dec 2004 12:14:15 +0100
Subject: [R] Arrow and text on a 3d plot
Message-ID: <2c3d7c68041202031470c3d928@mail.gmail.com>

I am currently working on surface plots (with R 2.0.1) and I want to
improve the aspect of them. Having a typical 'cow-boy hat' surface
[source: Spector (1994), p. 206]:

pts <- seq(from = -8, to = 8, length = 50)
cow <- function(x, y)
{
z <- sqrt(x^2+y^2)
sin(z)/z
}
out <- outer(pts, pts, cow)
sur <- persp(x=pts, y=pts, z=out, theta=45, phi=30)

What I exactly wanted is:

a) how to add an arrow pointing the maximum point in the surface, for
example: a short arrow starting on the maximum point, with slope 45??
and finishing near the top-edge of the plot.

b) how to add a typical text at the end of the arrow, as "this is the
maximum point", as described in the above reference.

N.B.: the author used the S function "perspp", but R does not have it.

Could anyone help me? 

Thank you very much in advance.

Arturo De Zan
PhD candidate
UPC-Universitat Politecnica de Catalunya
Barcelona, Spain.



From vito_ricci at yahoo.com  Thu Dec  2 12:44:09 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 2 Dec 2004 12:44:09 +0100 (CET)
Subject: [R] R: Computation of space-time empirical crosscovariances
Message-ID: <20041202114409.64597.qmail@web41201.mail.yahoo.com>

Hi,

see: http://agec221.agecon.uiuc.edu/csiss/Rgeo/

for R-spacial packages!

Regards
Vito

you wrote:

Dear all,
Is there any "simple" way to estimate an empirical
crosscovariance
function for space-time data? More precisely if I have
n space locations
in T times on which I observe one variable, I cannot
find an already
made  R function that can compute the empirical
crosscovariance.

Any suggestion?

Thanks 
Giovanna Jona Lasinio

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From r.hankin at soc.soton.ac.uk  Thu Dec  2 12:53:40 2004
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 2 Dec 2004 11:53:40 +0000
Subject: [R] Arrow and text on a 3d plot
In-Reply-To: <2c3d7c68041202031470c3d928@mail.gmail.com>
References: <2c3d7c68041202031470c3d928@mail.gmail.com>
Message-ID: <CF4A2659-4458-11D9-91A6-000A95D86AA8@soc.soton.ac.uk>

Hi

the best way to do it is to make a function like the trans3d() function 
in help(persp)
and use it in your own function.  trans3d()
translates from (x,y,z) coords to plottable positions.  The following  
works for me:



  xyz <- matrix(rnorm(60),20,3)
xyz1 <- xyz+rnorm(60)/10

p3dpairs(xyz,xyz1,col="red",theta=40,r=1e9)

[ignore the warnings]

with

"p3dpairs" <-
function(x,x1, xlim=NULL,ylim=NULL,zlim=NULL,col=par("col"), 
pch=par("pch"), cex=par("cex"), ...){
   if(is.matrix(x)){
     z <- x[,3]
     y <- x[,2]
     x <- x[,1]
   }

     if(is.matrix(x1)){
     z1 <- x1[,3]
     y1 <- x1[,2]
     x1 <- x1[,1]
   }

   if(missing(zlim)) {
     z.grid <- matrix(range(z),2,2)
   } else {
     z.grid <- matrix(zlim,2,2)
   }

   if(missing(xlim)){ xlim <- range(x) }
   if(missing(ylim)){ ylim <- range(y) }

   persp(xlim, ylim, z.grid, col = NA, border=NA, ...) -> res

   trans3d <- function(x,y,z, pmat) {
     tr <- cbind(x,y,z,1) %*% pmat
     list(x = tr[,1]/tr[,4], y= tr[,2]/tr[,4])
   }

   out <- trans3d(x,y,z,pm=res)
   out1 <- trans3d(x1,y1,z1,pm=res)
   points(out, col=col, pch=pch, cex=cex, ...)

   for(i in 1:length(out$x)){
     lines(c(out$x[i],out1$x[i]),c(out$y[i],out1$y[i]), col="gray", ...)
   }
   return(invisible(out))
}


On Dec 2, 2004, at 11:14 am, Arturo T. De Zan wrote:

> I am currently working on surface plots (with R 2.0.1) and I want to
> improve the aspect of them. Having a typical 'cow-boy hat' surface
> [source: Spector (1994), p. 206]:
>
> pts <- seq(from = -8, to = 8, length = 50)
> cow <- function(x, y)
> {
> z <- sqrt(x^2+y^2)
> sin(z)/z
> }
> out <- outer(pts, pts, cow)
> sur <- persp(x=pts, y=pts, z=out, theta=45, phi=30)
>
> What I exactly wanted is:
>
> a) how to add an arrow pointing the maximum point in the surface, for
> example: a short arrow starting on the maximum point, with slope 45??
> and finishing near the top-edge of the plot.
>
> b) how to add a typical text at the end of the arrow, as "this is the
> maximum point", as described in the above reference.
>
> N.B.: the author used the S function "perspp", but R does not have it.
>
> Could anyone help me?
>
> Thank you very much in advance.
>
> Arturo De Zan
> PhD candidate
> UPC-Universitat Politecnica de Catalunya
> Barcelona, Spain.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec  2 12:30:01 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 02 Dec 2004 11:30:01 -0000 (GMT)
Subject: [R] A somewhat off the line question to a log normal distrib
In-Reply-To: <41AEDDD2.1040700@stud.uni-graz.at>
Message-ID: <XFMail.041202113001.Ted.Harding@nessie.mcc.ac.uk>

On 02-Dec-04 Siegfried Gonzi wrote:
> Hello:
> 
> Oh yes I know it isn't so much related to R, but I gather
> there are a lot of statisticians reading the mailing list.
> 
> My boss repeatedly tried to explain me the following.
> 
> ==
> Lets assume you have got daily measurements of a variable
> in natural sciences. It turned out that the aformentioned
> daily measurements follow a log-normal distribution when
> considered over the course of a year. 
> Okay. He also tried to explain me that the monthly means
> (based on the daily measurements) must follow a log-normal
> distribution too then over the course of a year.
> ==
> 
> I somehow get his explanation.

Hmm, perhaps you should think again! If X and Y have log-normal
distributions (mathematically exactly), then (X+Y)/2 does not
(mathematically) have a log-normal distribution -- still less
the arithmetic mean of some 30 such variables. So one wonders
what the basis of his "explanation" was.

However, the conclusion would hold for the *geometric* mean
of the variables. X has a log-normal distribution if log(X)
has a normal distribution. So let X, Y, ... be log-normal.
The geometric mean is exp((log(X)+log(Y)+...)/n), and since
log(X), log(Y), ... are normal, so is (log(X)+log(Y)+...)/n,
and so the geometric mean is log-normal.

> But I have measurements which are log-normal distributed
> when evaluated on a daily basis over the course of a year
> but they are close to a Gaussian distribution when considered
> under the light of monthly means over the course of a year.
> 
> Is such a latter case feasible. And if not why.

This is, broadly, to be expected. If X1, X2, ... are independent
and with similar means and variances, then regardless of their
precise distributions the distribution of (X1+X2+...+Xn)/n
approaches the normal distribution as n->infinity ("Central
Limit Theorem").

How rapidly this happens depends on how much the distributions
of X1,... differ from a normal distribution. One feature which
can cause the approach to "normal" to be slow is skewness: the
more skew the distribution of each X1, ... , the slower the
convergence. The log-normal distribution is positively skewed,
sometimes grossly so -- experiment on the lines of:

  X<-exp(0+1.0*rnorm(10000)); hist(X,n=100)
  X<-exp(0+0.8*rnorm(10000)); hist(X,n=100)
  X<-exp(0+0.6*rnorm(10000)); hist(X,n=100)
  X<-exp(0+0.4*rnorm(10000)); hist(X,n=100)
  X<-exp(0+0.3*rnorm(10000)); hist(X,n=100)
  X<-exp(0+0.2*rnorm(10000)); hist(X,n=100)
  X<-exp(0+0.1*rnorm(10000)); hist(X,n=100)
  X<-exp(1+1.0*rnorm(10000)); hist(X,n=100)
  X<-exp(1+0.8*rnorm(10000)); hist(X,n=100)
  X<-exp(1+0.7*rnorm(10000)); hist(X,n=100)
  X<-exp(1+0.6*rnorm(10000)); hist(X,n=100)
  X<-exp(1+0.4*rnorm(10000)); hist(X,n=100)
  X<-exp(1+0.2*rnorm(10000)); hist(X,n=100)
  X<-exp(1+0.1*rnorm(10000)); hist(X,n=100)
  X<-exp(1+0.05*rnorm(10000)); hist(X,n=100)

(hoping this brings the query "on-topic") to get an impression
of the variety. A few of these look approximately normal as
they stand; the majority do not.

As for exploring the "central limit" tendency, you can try
things like

  N<-500;X<-exp(0+1.0*rnorm(N*1000)); Y<-matrix(X,nrow=N);
     M<-colMeans(Y);hist(M,n=20)
  hist(X,n=100)

[The first line draws a histogram of 1000 means, each of
 N=500 log-normal variates. The second shows a histogram
 of the original N*1000 variates, allowing you to compare
 the two and perceive the extent to which the approach
 to a normal distribution had been achieved. In this case,
 the means still have a perceptibly skew distribution,
 and of course the original data were very heavily skewed.
 You can evaluate the results for less skew log-normals
 in a similar way, building on the information from
 the first series of experiments.

 This may have been a consideration underlying your boss's
 argument: If the original data are heavily skew, then
 the distribution of the monthly means may well still be
 quite skew and better described by a log-normal than by
 a normal. However, your observation that the monthly means
 seem to be close to a normal distribution perhaps indicates
 this was not the case, so probably the original data, though
 log-normal, were not so skew that the N=30 or so gave results
 which were still perceptibly non-normal. (As stated above,
 as N -> infinity, you will eventually get a normal).]

So you can use R usefully to eveluate general statisical
issues of this kind!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 02-Dec-04                                       Time: 11:30:01
------------------------------ XFMail ------------------------------



From jarioksa at sun3.oulu.fi  Thu Dec  2 13:07:08 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 02 Dec 2004 14:07:08 +0200
Subject: [R] Protocol for answering basic questions
In-Reply-To: <Pine.LNX.4.10.10412020843370.21617-100000@mercury.quantex>
References: <Pine.LNX.4.10.10412020843370.21617-100000@mercury.quantex>
Message-ID: <1101989228.19515.48.camel@biol102145.oulu.fi>

On Thu, 2004-12-02 at 11:19, John Logsdon wrote:

> There are three ways of tackling this as far as I see:
> 
> First would be to make the list a Reply to Sender so that most of us don't
> see the replies. This would keep the traffic down and if any topic was of
> interest to another member, s/he could ask the originator whether it had
> been solved or the solution could also be posted as a summary.  One
> advantage of Reply to Sender is that it is only the Sender sees the
> multiple messages sent saying the same thing from good souls around the
> world who haven't seen the N-1 other messages...
> 
This seems to depend on the mail reader. This already is the default
behaviour with me (Evolution mail reader). I have to select
"Reply-to-All" to send the message to r-help as well -- and then it goes
to Cc list as well. It seems that some other mail software behaves
differently. It seems that R-help mail has two candidates to Reply:

From: this field is the original poster
Sender: r-help-bounces at stat.math.ethz.ch

Obviously my mail reader picks only "From", but John's picks both "From"
and "Sender".

Some other mail lists add to headers a new field "Reply-To" which equals
to "From" (original poster). It seems that this would be sufficient to
make many mail readers to use this as a default address.

Another issue is whether it is nice to divert a public discussion to a
private conversation. In several cases the solutions to the problem
remain private as well. After all, the purpose of the mailing list is a
public discussion instead of a public call to a private discussion.

cheers, jari oksanen
-- 
Jari Oksanen -- Oulu, Finland.
"But, Mousie, thou art no thy lane, In proving foresight may be vain;
The best-laid schemes o' mice an 'men, Gang aft agley,
An'lea'e us nought but grief an' pain, For promis'd joy!" (Robert Burns)



From r.hankin at soc.soton.ac.uk  Thu Dec  2 13:22:29 2004
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 2 Dec 2004 12:22:29 +0000
Subject: [R] A somewhat off the line question to a log normal distrib
In-Reply-To: <XFMail.041202113001.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041202113001.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <D5B07920-445C-11D9-853E-000A95D86AA8@soc.soton.ac.uk>

[stuff about the CLT deleted]

>
> So you can use R usefully to eveluate general statisical
> issues of this kind!
>

absolutely!  R is excellent for this sort of thing.  I use it for 
teaching stats all the time.
I'd say that without a tool like R you cannot learn statistics.


Consider an exponential distribution, which is very skewed.

f <- function(n){mean(rexp(n))}

then f(10) gives the mean of 10 independent exponentially distributed 
random
variables.  Then

hist(replicate(10000,f(10)))

gives us a histogram of 10000 observations of a variable that is itself 
the mean of 10 exponential variables.  It still looks a bit skew to me. 
  Try 100 exponential variables:

hist(replicate(10000,f(100)))

Still a tiny bit skew.


hist(replicate(1000,f(1000)))
which is indistinguishable from a Gaussian.


So as n -> infinity, the CLT kicks in.  But here 100 is a bit less than 
infinity and 1000 ~= infinity.

It's one thing to know a theoretical result, it's quite another to 
verify it numerically.

Kia Ora






> Best wishes,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 02-Dec-04                                       Time: 11:30:01
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From pcampbell at econ.bbk.ac.uk  Thu Dec  2 13:42:03 2004
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Thu, 2 Dec 2004 12:42:03 -0000
Subject: [R] Re: A somewhat off the line question to a log normal
	distribution
In-Reply-To: <20041202100735.9087.qmail@web41202.mail.yahoo.com>
Message-ID: <NGECIFANPOJAGABBAEAPKEGHDLAA.pcampbell@econ.bbk.ac.uk>

It has always been my understanding that an arbitrary lognormal distribution
has a sufficient quantity of finite moments to ensure a CLT holds, it is not
uniquely defined by these moments.  Thus although sums of IID lognormal
variates will converge to a normal distribution we cannot know, a priori,
what the parameters of this distribution will be.

If a distribution is strictly positive and has sufficient moments then the
sums of logs of the variables will converge to a normal distribution, thus
in estimating parameters that are closely related to the mean there would
appear to be little to loose by logging the data.

Phineas Campbell


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Vito Ricci
Sent: Thursday, December 02, 2004 10:08 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Re: A somewhat off the line question to a log normal
distribution


Dear Siegfried,

I believe your boss is wrong saying that:
>He also tried to explain me that the monthly means
>(based on the daily measurements) must follow a
>log-normal distribution too then over the course of a
year.

every statistician know that increasing the sample
size the sample distribution of the mean is proxy to a
gaussian distribution (Central Limit Theorem)
independently from the original distribution of data
(in your case log-normal).

See this:

The Central Limit Theorem is a statement about the
characteristics of the sampling distribution of means
of random samples from a given population. That is, it
describes the characteristics of the distribution of
values we would obtain if we were able to draw an
infinite number of random samples of a given size from
a given population and we calculated the mean of each
sample.

The Central Limit Theorem consists of three
statements:

[1] The mean of the sampling distribution of means is
equal to the mean of the population from which the
samples were drawn.

[2] The variance of the sampling distribution of means
is equal to the variance of the population from which
the samples were drawn divided by the size of the
samples.

--> [3] If the original population is distributed
normally (i.e. it is bell shaped), the sampling
distribution of means will also be normal. If the
original population is not normally distributed, the
sampling distribution of means will increasingly
approximate a normal distribution as sample size
increases. (i.e. when increasingly large samples are
drawn) <--

So results you got are just in this way!

I think your boss doesn't know well statistics!

Regards
Vito

You wrote:

Hello:

Oh yes I know it isn't so much related to R, but I
gather there are a
lot of statisticians reading the mailing list.

My boss repeatedly tried to explain me the following.

==
Lets assume you have got daily measurements of a
variable in natural
sciences. It turned out that the aformentioned daily
measurements follow
a log-normal distribution when considered over the
course of a year.
Okay. He also tried to explain me that the monthly
means (based on the
daily measurements) must follow a log-normal
distribution too then over
the course of a year.
==

I somehow get his explanation.

But I have measurements which are log-normal
distributed when evaluated
on a daily basis over the course of a year  but they
are close to a
Gaussian distribution when considered under the light
of monthly means
over the course of a year.

Is such a latter case feasible. And if not why.

Regards,
Siegfried Gonzi


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze
the scientific learning process."
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese
http://www.modugno.it/archivio/palese/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From david.whiting at ncl.ac.uk  Thu Dec  2 13:48:29 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 02 Dec 2004 12:48:29 +0000
Subject: [R] A somewhat off the line question to a log normal distrib
In-Reply-To: <D5B07920-445C-11D9-853E-000A95D86AA8@soc.soton.ac.uk>
References: <XFMail.041202113001.Ted.Harding@nessie.mcc.ac.uk>
	<D5B07920-445C-11D9-853E-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <m24qj4dfxe.fsf@192.168.57.36>

Robin Hankin <r.hankin at soc.soton.ac.uk> writes:

> [stuff about the CLT deleted]
> 
> >
> > So you can use R usefully to eveluate general statisical
> > issues of this kind!
> >
> 
> absolutely!  R is excellent for this sort of thing.  I use it for
> teaching stats all the time.
> I'd say that without a tool like R you cannot learn statistics.

I believe Fisher and a few others managed to get by without it.

-- 
David Whiting
University of Newcastle upon Tyne, UK



From s-plus at wiwi.uni-bielefeld.de  Thu Dec  2 14:07:23 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Thu, 02 Dec 2004 14:07:23 +0100
Subject: [R] Hexidecimal conversion
References: <E37EEC6DE3A0C5439B7E7B07406C24AE124A75@msgmarsta01.bio.dfo.ca>
Message-ID: <41AF138B.9050503@wiwi.uni-bielefeld.de>

Some years ago I wrote a function to convert a number
from number system "base.in" to number system "base.out".
Here is the code:
<<*>>=
chcode <- function(b, base.in=2, base.out=10, digits="0123456789ABCDEF"){
   # change of number systems, pw 10/02
   # e.g.: from 2 2 2 2 ...  ->  16 16 16 ...
   digits<-substring(digits,1:nchar(digits),1:nchar(digits))
   if(length(base.in)==1) base.in <- rep(base.in, max(nchar(b)-1))
   if(is.numeric(b)) b <- as.character(as.integer(b))
   b.num <- lapply(strsplit(b,""), function(x) match(x,digits)-1  )
   result <- lapply(b.num, function(x){
                cumprod(rev(c(base.in,1))[ 1:length(x) ] ) %*% rev(x)
             } )
   number<-unlist(result)
   cat("decimal representation:",number,"\n")

   if(length(base.out)==1){
      base.out<-rep(base.out,1+ceiling(log( max(number), base=base.out ) ) )
   }
   n.base <- length(base.out); result <- NULL
   for(i in n.base:1){
     result <- rbind(number %% base.out[i], result)
     number <- floor(number/base.out[i])
   }
   result[]<-digits[result+1]
   apply(result, 2, paste, collapse="")
}


@
... a short check:
<<*>>=
chcode("9F",16,10)

@
output-start
decimal representation: 159
[1] "0159"
output-end
@
... conversion backwards:
<<*>>=
chcode("159",10,16)

@
output-start
decimal representation: 159
[1] "09F"
output-end

Is chcode the function you are looking for?

Peter Wolf


Hanke, Alex wrote:

>Help
>I can produce the hexidecimal equivalent of a decimal number but I am having
>a hard time reversing the operation. I'm good for hex representations to 159
>and am close to extending to 2559. The archives are not clear on the
>existence of a function for this task. Is there one?
>Here is what I have got so far:
>#Good for hex values to "9F"
>as.decmode<-function(as.character(x)){
>              hexDigit<-c(0:9,LETTERS[1:6])
>              z<-matrix(c(strsplit(x, NULL),recursive=T),
>              length(x),2,byrow=T)
>              z.1<-as.numeric(z[,1])
>              z.2<- match(z[,2],hexDigit)-1
>              dec<-16*z.1+z.2
>              return(dec)
>              }
>Alex Hanke
>Department of Fisheries and Oceans
>St. Andrews Biological Station
>531 Brandy Cove Road
>St. Andrews, NB
>Canada
>E5B 2L9
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From MSchwartz at MedAnalytics.com  Thu Dec  2 14:09:39 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 02 Dec 2004 07:09:39 -0600
Subject: [R] Drawing a rectangle around a barplot()
In-Reply-To: <41AEF757.20907@lancaster.ac.uk>
References: <8975119BCD0AC5419D61A9CF1A923E95E89943@iahce2knas1.iah.bbsrc.reserved>
	<41AEF757.20907@lancaster.ac.uk>
Message-ID: <1101992979.5637.62.camel@horizons.localdomain>

On Thu, 2004-12-02 at 11:07 +0000, Barry Rowlingson wrote:
> michael watson (IAH-C) wrote:
> > Hi
> > 
> >  I can't use box() as I don't
> > want to draw a box round my entire plot, I just want to draw a box as
> > the background to certain subsets of the bars.
> 
>    One idea. Edit the barplot.default function so it doesnt call plot.new.
> 
>   do:
> 
>   myBarplot = barplot.default
> 
>   then edit the myBarplot function. Add an extra parameter to the 
> argument list 'add', and make it False by default:
> 
>              cex.names = par("cex.axis"), inside = TRUE, plot = TRUE,
>              axis.lty = 0, add=F, ...)
> 
>   then find plot.new and wrap it in a condition:
> 
>      if(!add)plot.new()
> 
>   Now try:
> 
>   tN <- table(Ni <- rpois(100, lambda=5))
>   myBarplot(tN)
>   rect(2,1,6.5,8.5)
> 
>   - this puts the rectangle on top of the bars, which you dont want, so 
> you call myBarplot with add=T so that plot.new isnt called and the 
> barplot appears over the rectangle:
> 
>   myBarplot(tN,add=T)
> 
> works for me, if I understand your problem correctly!
> 
> Baz


In follow up to Baz' post, the 'add' argument is already in the 
barplot2() function, which is in the gregmisc bundle (gplots package) on
CRAN.

I have also been looking at adding a 'panel.first' and 'panel.last'
argument to barplot2, to enable functionality similar to that of
plot.default(). This would provide the ability to add additional plot
components before and/or after the bars are drawn. I'll get to that as
soon as time permits.

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Thu Dec  2 14:10:54 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Dec 2004 14:10:54 +0100
Subject: [R] A somewhat off the line question to a log normal distrib
In-Reply-To: <m24qj4dfxe.fsf@192.168.57.36>
References: <XFMail.041202113001.Ted.Harding@nessie.mcc.ac.uk>
	<D5B07920-445C-11D9-853E-000A95D86AA8@soc.soton.ac.uk>
	<m24qj4dfxe.fsf@192.168.57.36>
Message-ID: <x21xe8etgh.fsf@biostat.ku.dk>

David Whiting <david.whiting at ncl.ac.uk> writes:

> > I'd say that without a tool like R you cannot learn statistics.
> 
> I believe Fisher and a few others managed to get by without it.

But think how far they could have got with R! :-)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From michael.watson at bbsrc.ac.uk  Thu Dec  2 14:14:55 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 2 Dec 2004 13:14:55 -0000
Subject: [R] Drawing a rectangle around a barplot()
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E8994D@iahce2knas1.iah.bbsrc.reserved>

Thanks!

Barry's suggestion did the trick.  

Mick
-----Original Message-----
From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com] 
Sent: 02 December 2004 13:10
To: Barry Rowlingson
Cc: michael watson (IAH-C); R-Help
Subject: Re: [R] Drawing a rectangle around a barplot()


On Thu, 2004-12-02 at 11:07 +0000, Barry Rowlingson wrote:
> michael watson (IAH-C) wrote:
> > Hi
> > 
> >  I can't use box() as I don't
> > want to draw a box round my entire plot, I just want to draw a box 
> > as the background to certain subsets of the bars.
> 
>    One idea. Edit the barplot.default function so it doesnt call 
> plot.new.
> 
>   do:
> 
>   myBarplot = barplot.default
> 
>   then edit the myBarplot function. Add an extra parameter to the
> argument list 'add', and make it False by default:
> 
>              cex.names = par("cex.axis"), inside = TRUE, plot = TRUE,
>              axis.lty = 0, add=F, ...)
> 
>   then find plot.new and wrap it in a condition:
> 
>      if(!add)plot.new()
> 
>   Now try:
> 
>   tN <- table(Ni <- rpois(100, lambda=5))
>   myBarplot(tN)
>   rect(2,1,6.5,8.5)
> 
>   - this puts the rectangle on top of the bars, which you dont want, 
> so
> you call myBarplot with add=T so that plot.new isnt called and the 
> barplot appears over the rectangle:
> 
>   myBarplot(tN,add=T)
> 
> works for me, if I understand your problem correctly!
> 
> Baz


In follow up to Baz' post, the 'add' argument is already in the 
barplot2() function, which is in the gregmisc bundle (gplots package) on
CRAN.

I have also been looking at adding a 'panel.first' and 'panel.last'
argument to barplot2, to enable functionality similar to that of
plot.default(). This would provide the ability to add additional plot
components before and/or after the bars are drawn. I'll get to that as
soon as time permits.

HTH,

Marc Schwartz



From kjetil at acelerate.com  Thu Dec  2 12:34:44 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 02 Dec 2004 07:34:44 -0400
Subject: [R] Protocol for answering basic questions
In-Reply-To: <003e01c4d85d$b8cb35e0$f2e1f380@OPENZAURUS>
References: <E1CZlP3-0007Sw-00@spool.aub.edu.lb>
	<003e01c4d85d$b8cb35e0$f2e1f380@OPENZAURUS>
Message-ID: <41AEFDD4.30804@acelerate.com>

Stuart Leask wrote:

>reasons! Most things in life a worth what you pay for them - 
>
? I did'nt pay for my wife.

Kjetil


>the currency
>here seems to be whether folks consider that a questioner has made an
>effort; if so, respondants seem happy to match it.
>
>I think it is astonishing that the R list maintains such an enthusiastic
>body of world-class contributors. I look at many other help lists
>(commercial and open-source) and see great oceans of unanswered queries.
>Hands up anyone who has time to politely answer swathes of questions already
>covered elsewhere. That folks complain that they have not had a reply in 48
>hours is a testament to the tremendous effort that is put in.
>
>What I would be in favour of is another list: R-complaints! Here folks with
>bruised egos and/or higher expectations of their fellow man can pursue
>threads like this one into the next millenium....
>
>Stuart
>
>
>
>----- Original Message ----- 
>From: "Marwan Khawaja" <marwan.khawaja at aub.edu.lb>
>To: "'Robert Brown FM CEFAS'" <r.g.brown at cefas.co.uk>;
><r-help at stat.math.ethz.ch>
>Sent: Wednesday, December 01, 2004 8:41 PM
>Subject: RE: [R] Protocol for answering basic questions
>
>
>Well, if you do not like the way some people answer queries, why not just
>delete
>the reply without reading the response.
>Since we're not paying anyone for answering questions, we should be grateful
>to
>those who put their time in replying to our basic questions.
>And why join this community? -- if you think most are 'conceptually na??ve'!
>
>Marwan
>
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert
>>Brown FM CEFAS
>>Sent: Wednesday, December 01, 2004 6:46 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Protocol for answering basic questions
>>
>>I have been following the discussions on 'Reasons not to
>>answer very basic questions in a straightforward way' with
>>interest as someone who is also new to R and has had similar
>>experiences.  As such it with sadness that I note that most
>>seem to agree with the present approach to the responses to
>>basic questions.  I must thank those respondants to my own
>>questions who have been helpful, but there are some whose
>>replies are in my opinion not only unhelpful but actually
>>rude.  Indeed I've now started using Splus instead of R so as
>>to have access to a 'proper' support service.  Indeed, the
>>main thing I've learned from R is a new respect for the
>>values of commercial software and a scepticism regarding free
>>software. In the end my experience of r help is that you get
>>what you pay for. Many of the so called socratic responses
>>(in this list and the wider academic community) can be seen
>>as simply way to avoid additional work of a complete reply.
>>
>>Experienced R users don't seem to understand how difficult
>>the program can be to new users. Responding that the
>>questioner should read the 'Introduction to R' or a similar
>>document is like  answering a question for directions to
>>one's house with 'Buy a map'.  Most likely both such
>>questioners have already tried that and are asking because
>>that approach failed.  R is a language and like all languages
>>it is simple to those that understand it and complex to those
>>who do not. Every schoolboy in Spain speaks Spanish, but I
>>know from experience that for most English people it is very
>>difficult to learn Spanish and take years of study.  If I'm
>>asked a question from a novice of a language (be it Spanish
>>or R) do I reply 'consult the dictionary'. I would hope not!
>>I can tell repondants that whilst many of my basic questions
>>may seem simple it is not for lack of studying the very
>>sources they refer to.  If only learning was so simple.  I
>>suspect that the same is true of most question!
>> ers.
>>
>>I speak as someone with a PhD and many years as a researcher
>>in my speciality as well as someone close to completing a
>>masters in statistics with distinction. As such I am not a
>>total novice and would suggest that if I'm having problems so
>>are many; and it is not a result of lack of study as so many
>>responses seem to suggest.  Indeed it is revealing that
>>several responses suggest that they want to discourage
>>questions so they don't overwhelm r-help.  Understandable but
>>not a recipe to encourage the use of R by other than experts.
>>The R community needs to decide of they really only want
>>expert statisticians users and make this clear if it is the
>>case.  Alternatively if they are to encourage novices the
>>present approach is not the way to do it.
>>
>>I can appreciate that many of the respondants are busy, but
>>if that is the case it would be better if they didn't reply
>>at all. I was taught many years ago that if you can't say
>>anything nice/useful then to say nothing at all.  Something
>>similar could well be applied to this list.  I must say that
>>some respondants are very helpful; and I thank them.  Leave
>>these simple questions to such people.  Indeed it seems
>>surprising that some exteremely experienced R users choose to
>>reply to these basic messages at all; and it seem it is
>>mostly these people who are rude.  I would have thought it
>>might be better for them to concentrate on complex problems
>>more suited to their skills and interests and leave the
>>simple questions to more sympathetic souls.
>>
>>Perhaps there is a case for two r help lists catering to
>>basic and advanced questions? Certainly if the R community is
>>serious about appealling to users outside advanced
>>statisticians there is a need for a change of approach in r
>>help and elsewhere.  Russ Ackoff identified much of the
>>failure of management science as due to those who were
>>'mathematically sophisticated but conceptually naive' and
>>much the same could be said for many in the R community.
>>
>>Finally, let me once again thank those who have responded
>>helpful to my queries in the past and ask them to continue in
>>that vein; their assistance and effort is greatly appreciated.
>>
>>
>>
>>
>>
>>
>>**************************************************************
>>*********************
>>This email and any attachments are intended for the named
>>recipient(s) only.  Its unauthorised use, distribution,
>>disclosure, storage or copying is not permitted.  If you have
>>received it in error, please destroy all copies and notify
>>the sender.  In messages of a non-business nature, the views
>>and opinions expressed are the author's own and do not
>>necessarily reflect those of the organisation from which it
>>is sent.  All emails may be subject to monitoring.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>
>This message has been scanned but we cannot guarantee that it and any
>attachments are free from viruses or other damaging content: you are
>advised to perform your own checks.  Email communications with the
>University of Nottingham may be monitored as permitted by UK legislation.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From r.hankin at soc.soton.ac.uk  Thu Dec  2 14:21:36 2004
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 2 Dec 2004 13:21:36 +0000
Subject: [R] A somewhat off the line question to a log normal distrib
In-Reply-To: <m24qj4dfxe.fsf@192.168.57.36>
References: <XFMail.041202113001.Ted.Harding@nessie.mcc.ac.uk>
	<D5B07920-445C-11D9-853E-000A95D86AA8@soc.soton.ac.uk>
	<m24qj4dfxe.fsf@192.168.57.36>
Message-ID: <17ECB8FA-4465-11D9-853E-000A95D86AA8@soc.soton.ac.uk>




>>
>> I'd say that without a tool like R you cannot learn statistics.
>
> I believe Fisher and a few others managed to get by without it.
>

Fair point.  Maybe it should have said  " without a tool like R *I* 
cannot learn statistics"
Or even "for me, 'understanding concept X' == 'coding X up in R and 
playing with it' "

(maybe Fisher had some R-like tool hardwired into his neural apparatus).

The other point I was trying to make was that R is a natural language 
for
the expression of statistical ideas, and I usually interpret my 
inability to express
a statistical idea in R idiom as a symptom of not understanding the 
stats.
Once I understand the concepts, coding them up is easy, usually.

The CLT is  a case in point, and I've been playing with the Extreme 
Value Distribution recently
in the same way.


best wishes

rksh



From andy_liaw at merck.com  Thu Dec  2 14:33:30 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 2 Dec 2004 08:33:30 -0500
Subject: [R] A somewhat off the line question to a log normal
 distrib
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3B4@usrymx25.merck.com>

> From: Robin Hankin
> 
> [stuff about the CLT deleted]
> 
> >
> > So you can use R usefully to eveluate general statisical
> > issues of this kind!
> >
> 
> absolutely!  R is excellent for this sort of thing.  I use it for 
> teaching stats all the time.
> I'd say that without a tool like R you cannot learn statistics.
> 
> 
> Consider an exponential distribution, which is very skewed.
> 
> f <- function(n){mean(rexp(n))}
> 
> then f(10) gives the mean of 10 independent exponentially distributed 
> random
> variables.  Then
> 
> hist(replicate(10000,f(10)))
> 
> gives us a histogram of 10000 observations of a variable that 
> is itself 
> the mean of 10 exponential variables.  It still looks a bit 
> skew to me. 
>   Try 100 exponential variables:
> 
> hist(replicate(10000,f(100)))
> 
> Still a tiny bit skew.
> 
> 
> hist(replicate(1000,f(1000)))
> which is indistinguishable from a Gaussian.
> 
> 
> So as n -> infinity, the CLT kicks in.  But here 100 is a bit 
> less than 
> infinity and 1000 ~= infinity.
> 
> It's one thing to know a theoretical result, it's quite another to 
> verify it numerically.

... but one needs to be careful when going the other direction:  It's
dangerous to take what is seen in numerical experiments as the `truth',
without verifying it more rigorously.

Andy
 
> Kia Ora
> 
> > Best wishes,
> > Ted.
> >
> >
> > --------------------------------------------------------------------
> > E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> > Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> > Date: 02-Dec-04                                       Time: 11:30:01
> > ------------------------------ XFMail ------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From karl at huftis.org  Thu Dec  2 14:43:40 2004
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Thu, 2 Dec 2004 14:43:40 +0100
Subject: [R] A somewhat off the line question to a log normal distrib
In-Reply-To: <D5B07920-445C-11D9-853E-000A95D86AA8@soc.soton.ac.uk>
References: <XFMail.041202113001.Ted.Harding@nessie.mcc.ac.uk>
	<D5B07920-445C-11D9-853E-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <200412021443.40280.karl@huftis.org>

2004-12-02 Robin Hankin wrote:

> hist(replicate(10000,f(10)))
>
> gives us a histogram of 10000 observations of a variable that is
> itself the mean of 10 exponential variables.

You might want to use 'truehist' from the 'MASS' package instead. It 
draws a real histogram (where the area of the bars sum to 1), and the 
number of bars increases with the number of observations.

Example:

library(MASS)
truehist(rnorm(100), col="wheat")
truehist(rnorm(1000), col="wheat")
truehist(rnorm(10000), col="wheat")

x = rnorm(100000)
truehist(x, col="wheat")
plot(density(x))   # Density estimate

(Regarding col="wheat": The default colour used by 'truehist' is 
really eye-blinding ... :) )

-- 
Regards,
Karl Ove Hufthammer



From MSchwartz at MedAnalytics.com  Thu Dec  2 14:55:56 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 02 Dec 2004 07:55:56 -0600
Subject: [R] Gap between axis and bars in barplot()
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E8993C@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E8993C@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <1101995756.5637.93.camel@horizons.localdomain>

On Thu, 2004-12-02 at 09:30 +0000, michael watson (IAH-C) wrote:
> Hi
> 
> Windows XP, R 2.0.1.
> 
> I am drawing a very large barplot using jpeg() - setting the width to
> 10,000 as there are over 5000 bars.
> 
> This all works fine and I get exactly what I want - except there is a
> huge bit of white space between the Y-axis and the first bar - so much
> in fact that I have to scroll two screens from the Y-axis before I see
> the first bar.  After that the bars are evenly spaced and close
> together.
> 
> Any ideas what is going on here?
> 
> Thanks in advance
> 
> Mick


To use an example to demonstrate the effects, the default barplot is:

barplot(1:5000)

Note that in this case, par("usr") which is c(x1, x2, y1, y2) is:

> par("usr")
[1] -239.792 6239.992  -50.000 5000.000

As you note, a fair amount of space on either side of the bars.


Now, specify the x axis style by using 'xaxs = "i"':

barplot(1:5000, xaxs = "i")

Note that par("usr") is now:

> par("usr")
[1]    0.2 6000.0  -50.0 5000.0

There is now essentially no space on either side of the bars, as the use
of "i" sets the axis range to exactly the required values for the data,
which in this case is comprised of the bar widths (set to 1 by default)
and the space between the bars (set to 0.2 by default).

In the initial example, the default axis style is "r", which extends the
axis range by 4%. So if you take (5,000 bars * 1) and add (5,000 * 0.2)
for the space, that gives you 6,000 * 1.04, which is 6,240. The
additional 240, is then also subtracted from the minimum x axis value to
provide for symmetric spacing on either side of the bars.

If you want "some space", you can further adjust it by using the 'xlim'
argument:

barplot(1:5000, xaxs = "i", xlim = c(-100, 6100))

This increases the x axis range by 100 (instead of 240) on either side
of the bars and you can further adjust to your requirements.

See ?par for more information on adjustments to the axes, etc.

HTH,

Marc Schwartz



From HankeA at mar.dfo-mpo.gc.ca  Thu Dec  2 15:28:32 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Thu, 02 Dec 2004 10:28:32 -0400
Subject: [R] Hexidecimal conversion
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124A78@msgmarsta01.bio.dfo.ca>

Thanks to Patrick Burns, Peter Wolf and Duncan Murdoch who all provided me
with workable solutions to the hexidecimal conversion problem. They all work
and basically differ in the number of bells and whistles. 
Alex

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
Sent: December 2, 2004 9:42 AM
To: Hanke, Alex
Subject: Re: [R] Hexidecimal conversion


On Wed, 01 Dec 2004 15:07:16 -0400, "Hanke, Alex"
<HankeA at mar.dfo-mpo.gc.ca> wrote :

>
>Help
>I can produce the hexidecimal equivalent of a decimal number but I am
having
>a hard time reversing the operation. I'm good for hex representations to
159
>and am close to extending to 2559. The archives are not clear on the
>existence of a function for this task. Is there one?

I don't think so.

>Here is what I have got so far:
>#Good for hex values to "9F"
>as.decmode<-function(as.character(x)){
>              hexDigit<-c(0:9,LETTERS[1:6])
>              z<-matrix(c(strsplit(x, NULL),recursive=T),
>              length(x),2,byrow=T)
>              z.1<-as.numeric(z[,1])
>              z.2<- match(z[,2],hexDigit)-1
>              dec<-16*z.1+z.2
>              return(dec)
>              }

I think what you're missing is a loop over the characters.  You can
probably vectorize this to make it more efficient, but here's a
sketch:

hex2numeric <- function(x) {
    hexDigits <- c(0:9, LETTERS[1:6])
    chars <- strsplit(toupper(x), split=NULL)
    result <- rep(0, length(chars))
    for (i in seq(along=chars)) {
	for (j in seq(along=chars[[i]])) 
	    result[i] <- 16*result[i] + match(chars[[i]][j],
hexDigits) - 1
    }
    result
}

Duncan Murdoch



From r.eschen at cabi.org  Thu Dec  2 15:29:12 2004
From: r.eschen at cabi.org (Rene Eschen)
Date: Thu, 02 Dec 2004 06:29:12 -0800
Subject: [R] Dominant factors in aov?
Message-ID: <AC769F794461D8118F0800A0C9EA256502DA85@delemont_srvr.cabi_europe.local>

Hi all,

I'm using R 2.0.1. for Windows to analyze the influence of following factors
on response Y:

A (four levels)
B (three levels)
C (two levels)
D (29 levels) with
E (four replicates)

The dataset looks like this:
A	B	C	D	E	Y
0	1	1	1	1	491.9
0	1	1	1	2	618.7
0	1	1	1	3	448.2
0	1	1	1	4	632.9
250	1	1	1	1	92.4
250	1	1	1	2	117
250	1	1	1	3	35.5
250	1	1	1	4	102.7
500	1	1	1	1	47
500	1	1	1	2	57.4
500	1	1	1	3	6.5
500	1	1	1	4	50.9
1000	1	1	1	1	0.7
1000	1	1	1	2	6.2
1000	1	1	1	3	0.5
1000	1	1	1	4	1.1
0	2	2	2	1	6
0	2	2	2	2	4.2
0	2	2	2	3	20.3
0	2	2	2	4	3.5
250	2	2	2	1	8.4
250	2	2	2	2	2.8

etc.

If I ask the following: summary(aov(Y~A+B+C+D+E))

R gives me this answer:

   		 Df  Sum Sq Mean Sq  F value Pr(>F)    
A  		  3 135.602  45.201 310.2166 <2e-16 ***
B  		  2   0.553   0.276   1.8976 0.1512    
C  		  1   0.281   0.281   1.9264 0.1659    
D  		 25  92.848   3.714  25.4890 <2e-16 ***
E  		  3   0.231   0.077   0.5279 0.6634    
Residuals   411  59.885   0.146   

Can someone explain me why factor C has only 25 Df (in stead of 28, what I
expected), and why this number changes when I leave out factors B or C (but
not A)? Why do factors B and C (but again: not A) not show up in the
calculation if they appear later in the formula than D?

When I ask summary.lm(aov(Y~A+B+C+D+E)), R tells me that three levels of D
were not defined because of "singularities" (what does this word mean?).
After checking and playing around with the dataset, I find no logical reason
for which levels are not defined. Even if I construct a "perfect" dataset
(balanced, no missing values) I never get the correct number of Df. 

My other datasets are analyzed as expected using the similar function calls
and similar datasets. Am I doing something wrong here?

Many thanks,

Ren?? Eschen.

___
drs. Ren?? Eschen
CABI Bioscience Switzerland Centre
1 Rue des Grillons
CH-2800 Del??mont
Switzerland
+41 32 421 48 87 (Direct)
+41 32 421 48 70 (Secretary)
+41 32 421 48 71 (Fax)

http://www.unifr.ch/biol/ecology/muellerschaerer/group/eschen/eschen.html



From baron at psych.upenn.edu  Thu Dec  2 16:17:41 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 2 Dec 2004 10:17:41 -0500
Subject: [R] Dominant factors in aov?
In-Reply-To: <AC769F794461D8118F0800A0C9EA256502DA85@delemont_srvr.cabi_europe.local>
References: <AC769F794461D8118F0800A0C9EA256502DA85@delemont_srvr.cabi_europe.local>
Message-ID: <20041202151741.GA28504@psych>

I'm not a statistician, so take what I say with a grain of salt.

On 12/02/04 06:29, Rene Eschen wrote:
>Can someone explain me why factor C has only 25 Df (in stead of 28, what I
>expected), and why this number changes when I leave out factors B or C (but
>not A)? Why do factors B and C (but again: not A) not show up in the
>calculation if they appear later in the formula than D?
>
>When I ask summary.lm(aov(Y~A+B+C+D+E)), R tells me that three levels of D
>were not defined because of "singularities" (what does this word mean?).
>After checking and playing around with the dataset, I find no logical reason
>for which levels are not defined. Even if I construct a "perfect" dataset
>(balanced, no missing values) I never get the correct number of Df.

I would guess that the factors are somewhat predictable from each
other.  That is, there is some redundancy.  Try predicting each
factor from all the others, without the dependent variable.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From MSchwartz at MedAnalytics.com  Thu Dec  2 16:36:25 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 02 Dec 2004 09:36:25 -0600
Subject: [R] Protocol for answering basic questions
In-Reply-To: <mh0tq0l1v0mbqj250iruappi18dauv2frc@4ax.com>
References: <AAAECF1CF69A704EB103E5284B741CF902CEAB0D@lauimls05.qc.dfo.ca>
	<D5B5C260-43C4-11D9-9C45-000A95C76CA8@oulu.fi>
	<41AE5A09.3050408@wanadoo.es>
	<mh0tq0l1v0mbqj250iruappi18dauv2frc@4ax.com>
Message-ID: <1102001785.5637.157.camel@horizons.localdomain>

On Wed, 2004-12-01 at 21:52 -0500, Duncan Murdoch wrote:
> On Thu, 02 Dec 2004 00:55:53 +0100, Carlos Javier Gil Bellosta
> <cjgb at wanadoo.es> wrote:
> 
> 
> >Over the years, while learning C, Java or Python, I have found very 
> >useful a few IRC channels on those languages where one could get (and 
> >provide!!) peer-to-peer support. Should a rather informal, open and 
> >publicited one exist for R, I believe it could channel many of these 
> >basic questions and, probably, many others some novices do not dare ask.
> 
> As far as I know, there is currently no IRC channel on R.  Why not set
> one up?  (I've got no idea what is involved in that.)  You'll need to
> attract participants to it.  Various ways are:
> 
>   - announce on this mailing list when you have created it.
>   - put a note about it in your signature
>   - use it, and be helpful to other users on it
>   - get a mention of it on an appropriate r-project.org page.
> 
> I've never used IRC and wouldn't be likely to be a participant (in
> general I dislike live online chat), but to each his own.  
> 
> Duncan Murdoch

It is relatively easy to set up an IRC channel. One needs to select an
IRC network and register and configure a channel on that network and
then make that information available to a target audience. The key is
that someone or multiple folks would need to be designated as channel
"ops", who in effect become the moderators for the channel and can act
accordingly when required.

There are various IRC clients for multiple operating systems that would
allow for relatively ubiquitous access.

In my mind, a significant part of the problem with IRC however, is that
knowledge is lost. There is no searchable log or archive of the
conversations and the exchanges that take place. Thus, there is no way
for others to take advantage of the knowledge sharing and this only
serves to perpetuate the repetition of queries and responses in that
environment.

Even worse, as the traffic level increases, many of the exchanges tend
to move from the public channel to DCC chats, because the main channel
traffic becomes impossible to follow. Thus, even those who may log in to
the channel as potential new users, can eventually find that there may
be a minimal amount of traffic in the main channel, as many of the
conversations are now taking place in private one-on-one or group chats.

While the above is clearly a generalization, I have experienced it
personally over the years in a variety of IRC channels. It can become
quite frustrating.

Part of the problem with attempting to separate out "basic" questions to
a new list is how does one define "basic"? Where is the line, or more
likely the grey buffer zone, between what may be clearly basic versus
something that ultimately becomes more involved.

How does one separate out what may seem to be a basic plot query at the
outset, when that query is actually entangled with the more involved
aspects of the plot/predict methods applicable to a linear regression
model or how data needs to be structured for a particular plot function?

These present difficult challenges and choices, many of which have been
discussed here more than once, at least over the past three plus years
that I have been in this community.

That does not mean that a single, highly motivated person or group of
folks cannot pursue an alternative venue for sharing knowledge apart
from these lists. That is quite common in most communities.

Indeed, this is the basis for the add-on packages in CRAN, Contributed
Documentation, the various GUI's that are available and the Bioconductor
group as examples. In each case, the basic functionality of R has been
substantially and meaningfully extended by individuals and/or groups who
defined a need and took it upon themselves to fulfill it. Those
additions have been made available back to the community at large, which
is consistent with the overall philosophy of open source initiatives and
the selfless desire to make contributions available to aid others.

As another example, how may different independent online support
resources are there for the various Linux distributions between online
forums, FAQ's, wiki's, e-mail lists, personal web sites and Usenet
groups? Some are "official" resources, while others are created
independently by users for the benefit of other users.

And...if one wonders whether 'basic' questions get asked in other
communities, one only need review a message that was posted to the main
Fedora list yesterday, which had a subject line of:

"Does Fedora include Linux?"

The key question in the body of the message was:

"So i was wondering does the Fedora Download include a OS (operating
system) with the download? Such as like Linux?"

The author of this message BTW is a 'webmaster' by his sig. You can
imagine what the range of responses to that question was.


There are several web sites already created by useRs that are available
which are highly complementary to these lists and to the main R web
site. There is nothing to prevent folks from continuing to advance those
types of activities and making those resources known to the community at
large. 

If those independent resources serve to advance the sharing of knowledge
with respect to R, terrific! That is what this is all about, users
helping other users. 

Ultimately, it is likely that each such resource will attract some
particular subset of self-selected participants, perhaps based upon
skill level, the nature of the participants and/or specific areas of
interest. In this way, users can locate and participate in one or more
support resources that best meets their particular requirements.

To make that a reality however, much like the recent discussion on
documentation, will take one or more persons who are motivated to make
it happen. 

Hopefully, some will step up and do just that. 

It is unreasonable and incredibly unfair to expect that R Core can or
will do it all. Given the selfless sacrifices that they have made to
bring R to its current state, it is incredible that they continue to
contribute to these lists as much as they do. One might want to keep
that in mind when on the receiving end of the occasional terse RTFM
reply.

Best regards to all,

Marc Schwartz



From bates at stat.wisc.edu  Thu Dec  2 16:47:31 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 02 Dec 2004 09:47:31 -0600
Subject: [R] Quotes from BHH2e
Message-ID: <41AF3913.4090002@stat.wisc.edu>

Yesterday I had the opportunity to attend a seminar by George Box where 
he discussed some of the ideas that will be incorporated in the second 
edition of Box, Hunter, and Hunter "Statistics for Experimenters" due 
out in a few months.

At the end of the presentation he distributed a list of quotes from the 
book and I felt that many of these would be appealing to members of this 
mailing list.

I refer those who want R-related content in messages to this group to 
the quote "Seek computer programs that allow you to do the thinking."

My thanks to Professor Box for giving me permission to forward these.

QUAQUAVERSAL QUOTES

The following list of quotations may be used for a number of purposes.

You may wish to be reminded of some of the ideas in this book.

Your boss, who may not have time to read the whole book, can employ
them to understand the philosophy of what you are doing.

If you use the book to teach a course some quotes can be used as
topics for short essays.

  Among the factors to be considered there will usually be a vital few
  and a trivial many.  (J.M. Juran)

  A process should be routinely operated in an evolutionary mode so as
  to produce not only product but information on how to improve the
  product.

  Sometimes the only thing you can do with a poorly designed experiment
  is to try to find out what it died of.  (R.A. Fisher)

  The experimenter who believes that only one factor at a time should
  be varied is amply provided for by using a factorial experiment.

  If there were a probability of only p = 0.04 of finding a crock of
  gold behind the next tree, wouldn't you go and look?

  The democratization of Scientific method.

  Designing an experiment is like gambling with the devil: only a
  random strategy can defeat all his betting systems.  (R.A. Fisher)

  Seek computer programs that allow you to do the thinking.

  When the ratio of the largest to smallest observation is large you
  should question whether the data are being analyzed in the right
  metric (transformation) .

  Original data should be presented in a way that will preserve the
  evidence in the original data.  (W. A. Shewhart)

  You can see a lot by just looking.  (Yogi Berra)

  A computer should make both calculations and graphs.  Both sorts of
  output should be studied; each will contribute to understanding.
  (F.J. Anscombe)

  Murphy works hard to ensure that anything that can go wrong will go
  wrong. With an adequate system of process monitoring, therefore, more
  and more of the things that can go wrong will be corrected and more
  and more of Murphy's tricks can be permanently stymied.

  A useful type of time series model is a recipe for transforming
  serial data into white noise.

  When you see the credits roll at the end of a successful movie you
  realize there are many more things that must be attended to in
  addition to choosing a good script.  Similarly in running a
  successful experiment there are many more things that must be
  attended to in addition to choosing a good experimental design.

  Iterative inductive-deductive problem solving is geared to the
  structure of the human brain and is part of every day experience.

  What does what to what? How, with a minimum of effort, can you
  discover which factors do what to which responses?

  Only in exceptional circumstances, do you need to try to answer all
  questions with one experiment.

  Actions called for as a result of an experiment are of two kinds:
   1) "Cashing in" on new knowledge
   2) Using the new knowledge to look for further possibilities of
   improvement

  The business of life, is to endeavor to find out what you don't know
  from what you do; that's what I called "guessing what was at the
  other side of the hill".  (Duke of Wellington)*

  The best time to plan an experiment is after you've done
  it. (R.A. Fisher)

  Every model is an approximation.

  It is the data that are real (they actually happened!)

  The model is a hypothetical conjecture that might or might not
  summarize and/or explain important features of the data.

  All models are wrong; some models are useful.

  Don't fall in love with a model.

  It is a capital mistake to theorize before one has data.  Sherlock
  Homes in "Scandal in Bohemia" (Conan Doyle)

  It is not unusual for a well-designed experiment to analyze itself.

  Correlation may have nothing to do with causation: beware the lurking
  variables(s)!

  The idea of a process in a perfect state of control contravenes the
  second law of thermodynamics: thus a state of control is an
  unrealizable and must be regarded as a purely theoretical concept.

  The design of experiments was invented by R.A. Fisher to make it
  possible to conduct valid experiments in an environment (agricultural
  trials) that was never in a state of control.

  To find out what happens when you change something it is necessary to
  change it.

  It's better to solve the right problem approximately than the wrong
  problem exactly.  (J.W. Tukey)

  Experiment and you'll see!

  Perfection is not possible it's always an approximation.

  Most often an experiment does not allow us to make a final decision
  but to see what's worth trying.

  "Block what you can and randomize what you can't" can approximately
  justify an analysis "as if" standard assumptions were true.

  The largest member of any group is large - but is it exceptionally
  large?

  Where there are three or four machines, one will be substantially
  better or worse than the others.  (Ellis Ott)

  That conclusions reached in one environment (say from lab
  experiments) will apply in a different environment (say the full
  scale process) is based not on statistical reasoning but on what
  Deming called "a leap of faith".  Statistical methods can reduce but
  not eliminate the necessary leap.

  Discovering the unexpected is more important than confirming the
  known.

  One must learn by doing the thing; for though you think you know it,
  you have no certainty until you try.  (Sophocles)

  We should not be afraid of discovering something.

  When running an experiment the safest assumption is that unless
  extraordinary precautions are taken it will be run incorrectly.

  Knowledge is power (Francis Bacon)

  Show me the data!

  With sequential assembly, designs can be build up so that the
  complexity of the design matches that of the problem.

  At any given stage, the current model helps us appreciate not only
  what is known but what else it may be important to find out.



From Christoph.Scherber at uni-jena.de  Thu Dec  2 16:54:20 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Thu, 02 Dec 2004 16:54:20 +0100
Subject: [R] Dominant factors in aov?
In-Reply-To: <AC769F794461D8118F0800A0C9EA256502DA85@delemont_srvr.cabi_europe.local>
References: <AC769F794461D8118F0800A0C9EA256502DA85@delemont_srvr.cabi_europe.local>
Message-ID: <41AF3AAC.7030504@uni-jena.de>

Dear Rene,

First of all, note that A,B,C,D, and E need to be declared as factors in 
the beginning, using factor() (but I think you did this already). Also, 
make sure that the data are read into R in the correct way (i.e. "." 
separating decimal places).

The reason for the "singularities" is that B, C and D are not 
independent (in fact, they??re identical in their factor levels, and 
hence in their effect on Y).

For this reason, only the effects of A, B and E can be estimated:

           Df Sum Sq Mean Sq F value    Pr(>F)   
A            3 302286  100762  7.9887  0.002396 **
B            1 422869  422869 33.5263 4.683e-05 ***
E            3  22281    7427  0.5888  0.632334   
Residuals   14 176583   12613                     

A has 4 levels so there should be 3 d.f. (that??s correct in the table)
B has 2 levels so there is only 1 d.f. (that??s also correct)
E has 4 levels so there should be 3 d.f. (also O.K.)

In total, there are [(n=22)-(3)-(1)-(3)] -1 = 14 residual d.f., so 
that??s OK, too.

Hope this helps,
Christoph



levels(A)
[1] "0"    "250"  "500"  "1000"
 > levels(B)
[1] "1" "2"
 > levels(E)
[1] "1" "2" "3" "4"





Rene Eschen wrote:

>Hi all,
>
>I'm using R 2.0.1. for Windows to analyze the influence of following factors
>on response Y:
>
>A (four levels)
>B (three levels)
>C (two levels)
>D (29 levels) with
>E (four replicates)
>
>The dataset looks like this:
>A	B	C	D	E	Y
>0	1	1	1	1	491.9
>0	1	1	1	2	618.7
>0	1	1	1	3	448.2
>0	1	1	1	4	632.9
>250	1	1	1	1	92.4
>250	1	1	1	2	117
>250	1	1	1	3	35.5
>250	1	1	1	4	102.7
>500	1	1	1	1	47
>500	1	1	1	2	57.4
>500	1	1	1	3	6.5
>500	1	1	1	4	50.9
>1000	1	1	1	1	0.7
>1000	1	1	1	2	6.2
>1000	1	1	1	3	0.5
>1000	1	1	1	4	1.1
>0	2	2	2	1	6
>0	2	2	2	2	4.2
>0	2	2	2	3	20.3
>0	2	2	2	4	3.5
>250	2	2	2	1	8.4
>250	2	2	2	2	2.8
>
>etc.
>
>If I ask the following: summary(aov(Y~A+B+C+D+E))
>
>R gives me this answer:
>
>   		 Df  Sum Sq Mean Sq  F value Pr(>F)    
>A  		  3 135.602  45.201 310.2166 <2e-16 ***
>B  		  2   0.553   0.276   1.8976 0.1512    
>C  		  1   0.281   0.281   1.9264 0.1659    
>D  		 25  92.848   3.714  25.4890 <2e-16 ***
>E  		  3   0.231   0.077   0.5279 0.6634    
>Residuals   411  59.885   0.146   
>
>Can someone explain me why factor C has only 25 Df (in stead of 28, what I
>expected), and why this number changes when I leave out factors B or C (but
>not A)? Why do factors B and C (but again: not A) not show up in the
>calculation if they appear later in the formula than D?
>
>When I ask summary.lm(aov(Y~A+B+C+D+E)), R tells me that three levels of D
>were not defined because of "singularities" (what does this word mean?).
>After checking and playing around with the dataset, I find no logical reason
>for which levels are not defined. Even if I construct a "perfect" dataset
>(balanced, no missing values) I never get the correct number of Df. 
>
>My other datasets are analyzed as expected using the similar function calls
>and similar datasets. Am I doing something wrong here?
>
>Many thanks,
>
>Ren?? Eschen.
>
>___
>drs. Ren?? Eschen
>CABI Bioscience Switzerland Centre
>1 Rue des Grillons
>CH-2800 Del??mont
>Switzerland
>+41 32 421 48 87 (Direct)
>+41 32 421 48 70 (Secretary)
>+41 32 421 48 71 (Fax)
>
>http://www.unifr.ch/biol/ecology/muellerschaerer/group/eschen/eschen.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec  2 16:53:03 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 02 Dec 2004 15:53:03 -0000 (GMT)
Subject: [R] A somewhat off the line question to a log normal distrib
In-Reply-To: <m24qj4dfxe.fsf@192.168.57.36>
Message-ID: <XFMail.041202155303.Ted.Harding@nessie.mcc.ac.uk>

On 02-Dec-04 David Whiting wrote:
> Robin Hankin <r.hankin at soc.soton.ac.uk> writes:
> 
>> [stuff about the CLT deleted]
>> 
>> >
>> > So you can use R usefully to eveluate general statisical
>> > issues of this kind!
>> >
>> 
>> absolutely!  R is excellent for this sort of thing.  I use
>> it for teaching stats all the time.
>> I'd say that without a tool like R you cannot learn statistics.
> 
> I believe Fisher and a few others managed to get by without it.

But the rest of us can depend quite heavily on groping through
instances until we see the light (I can remember doing crude
simulations using a slide rule and Kendall & Babington Smith's
"Tables of Random Normal Deviates" ... . You yougsters are
spoiled these days, with resources like R.)

Even the great, however, resorted to laborious simulations.
Student's pioneering paper "On the probable error of a mean"
(Biometrika 1908) gives the analytical form of the t
distribution (though not quite in its modern formulation:
he used z = mean/SD). In the paper he obtains it by laboriously
evaluating analytical moments of the numerator, of the
denominator, and their correlation (showing this to be zero
and hence "inferring" independence); he can then analytically
integrate their "joint distribution" to obtain the equation
of the z-distribution.

But, in a later section, he writes:

  "Before I had succeeded in solving my problem analytically,
   I had endeavoured to do so empirically. The material used
   was a correlation table containing the height and left
   middle finger measurements of 3000 criminals, from a paper
   by W. R. Macdonnell (Biometrika, I, p. 219). The measurements
   were written out on 3000 pieces of cardboard, which were then
   very thoroughly shuffled and drawn at random. As each card
   was drawn its numbers were written down in a book, which thus
   contains the measurements of 3000 criminals in a random order.
   Finally, each consecutive set of 4 was taken as a sample--750
   in all--and the mean, standard deviation, and correlation of
   each sample determined. The difference between the mean of
   each sample and the mean of the population was then divided
   by the standard deviation of the sample, giving us the z
   of Section III. This provides us with two sets of 750 standard
   deviations and two sets of 750 z's on which to test the
   theoretical results arrived at."

While in this paper he compares these results with his theoretical
formula, as a test, I seem to recall (which someone may be able to
confirm or refute) that originally (which is consistent with his
statement "Before I had succeeded in solving my problem analytically,
I had endeavoured to do so empirically") that he had used such
a sampling simulation to obtain the first 4 empirical moments
of the distribution of his z, and used these to identify the
distribution as a "Pearson Type VII" which is, in effect, the
t-distribution. If true, this would be an instance of one of
the great having been led to the truth by experimental exploration
of the kind being discussed.

As a further historical snippet: Fisher, as a Cambridge student
whose tutor was F.J.M. Stratton, noticed an apparent discrepancy
between Student's results and what he had worked out for himself
and drew this to the attention of Stratton (who knew Student);
On Stratton's suggestion Fisher contacted Student in 1912, out
of which correspondence came a correct proof. Student himself
wrote at one point to Stratton, somewhat complaining of "two
foolscap pages covered with mathematics of the deepest dye"
which had been sent him by "this chap Fisher". (Student's
letter is in fact about a later communication from Fisher
"so nice and mathematical that it might appeal to some people.")

(See "R.A. Fisher: the Life of a Scientist" by Joan Fisher Box).

Sorry to be drifting off-topic again, but I couldn't resist
"this chap Fisher".

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 02-Dec-04                                       Time: 15:53:03
------------------------------ XFMail ------------------------------



From tlumley at u.washington.edu  Thu Dec  2 17:06:58 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 2 Dec 2004 08:06:58 -0800 (PST)
Subject: [R] A somewhat off the line question to a log normal distrib
In-Reply-To: <XFMail.041202113001.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041202113001.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.61b.0412020804260.243438@homer07.u.washington.edu>

On Thu, 2 Dec 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

>
> Hmm, perhaps you should think again! If X and Y have log-normal
> distributions (mathematically exactly), then (X+Y)/2 does not
> (mathematically) have a log-normal distribution -- still less
> the arithmetic mean of some 30 such variables. So one wonders
> what the basis of his "explanation" was.
>

The original poster's boss did not (as far as we know) claim that the 
measurements were either independent or identically distributed.  While 
the problem would be simpler if they were, there is no guarantee that the 
answer would be remotely relevant.

 	-thomas



From damian.betebenner at bc.edu  Thu Dec  2 17:12:52 2004
From: damian.betebenner at bc.edu (Damian Betebenner)
Date: Thu, 02 Dec 2004 11:12:52 -0500
Subject: [R] How about a mascot for R?
Message-ID: <web-2697598@be1.bc.edu>

R users,

How come R doesn't have a mascot? Linux has one and so does LaTeX, so shouldn't R? I personally think that associating a "friendly face" with R would be a good thing for R (one letter names can be quite intimidating).

I apologize if this is addressed in the FAQ. I searched the FAQ as well as the mailing list archives and checked   

?mascot   

but to no avail. ;-)

Damian


Damian Betebenner
Educational Research, Measurement & Evaluation
Lynch School of Education
Boston College
Chestnut Hill, MA 02467

(617) 552 4491



From gunter.berton at gene.com  Thu Dec  2 17:13:47 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 2 Dec 2004 08:13:47 -0800
Subject: [R] treatment contrasts and summary.lm
In-Reply-To: <41AEF5E9.4090404@uni-jena.de>
Message-ID: <200412021613.iB2GDlK2000501@ohm.gene.com>


?C  ?contr.treatment  may help. Also 6.2 of Venables's and Ripley's MASS for
a more thorough discussion.

The coefficients are the differences between the mean at the stated levels
of the factor and the first level.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Christoph Scherber
> Sent: Thursday, December 02, 2004 3:01 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] treatment contrasts and summary.lm
> 
> Dear list members,
> 
> I have a 2-factor ANOVA where the summary.lm output looks like this 
> (using treatment contrasts):
> 
>                     Value Std. Error t value Pr(>|t|)
>       (Intercept)  0.0389  0.0220     1.7695  0.0817
> as.factor(Block)1  0.0156  0.0066     2.3597  0.0215
> as.factor(Block)2 -0.0018  0.0037    -0.4857  0.6289
> as.factor(Block)3 -0.0007  0.0026    -0.2812  0.7795
>    as.factor(AZ)1 -0.0066  0.0076    -0.8670  0.3893
>    as.factor(AZ)2  0.0064  0.0047     1.3530  0.1810
>    as.factor(AZ)3 -0.0015  0.0031    -0.4863  0.6284
>    as.factor(AZ)4  0.0054  0.0025     2.1499  0.0355
>    as.factor(AZ)5  0.0062  0.0037     1.6653  0.1009
> 
> Block has 4 levels and AZ has 6 levels. My question now is: 
> What exactly 
> do the values for AZ show? I know it??s somehow differences between 
> intercepts, but it would be great if someone could tell me 
> more on how 
> exactly to interprete the output.
> 
> Thanks very much in advance!
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From abu3ammar at gmail.com  Thu Dec  2 17:37:57 2004
From: abu3ammar at gmail.com (Slim Shady)
Date: Thu, 2 Dec 2004 11:37:57 -0500
Subject: [R] SJava converter
Message-ID: <b1d3150404120208373d2283a1@mail.gmail.com>

public static Object myMethod () {
   ...
      Object obj = null;
      if () { obj = new String [size] ; }
      else { obj = new double [size]; }
   ...
  System.out.println("in java");
  return obj;
}

This Java function returns String[] or Double[]. 

This is how I call the function from my R code.  
data <- .Java(myClass, "myMethod")
print("out of java")

When the function returns Double[] the conversion between Java and R
happens quickly. I am measuring the convertion time by how long it
takes between printing "in java" and "out of java". When teh returned
is String[] the conversion takes too long and in some cases where size
is large (150,000 for Strings of 10 chars each) the JVM runs out of
memory. Can I solve this by implementing my own converter? is there an
obvious reason why the default conversion for String[] is too slow and
runs out of memory?
These are my current converters:
> getJavaConverterDescriptions()
$fromJava
 [1] "From Java java.util.Vector to R list/vector"          
 [2] "org.omegahat.Environment.Utils.OrderedTable from Java"
 [3] "From Java 1-D array to R list/vector"                 
 [4] "Converts any Java InterfaceReference"                 
 [5] "class == java.lang.reflect.Method"                    
 [6] "class == java.lang.reflect.Constructor"               
 [7] "instanceof java.util.Properties"                      
 [8] "Integer[]"                                            
 [9] "Double[]"                                             
[10] "Boolean[]"                                            
[11] "[[<primitive> to a list of built-in vectors"          

$toJava
[1] "list object to Java"                  
[2] "org/omegahat/R/Java/RFunctionListener"



From MSchwartz at MedAnalytics.com  Thu Dec  2 17:38:20 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 02 Dec 2004 10:38:20 -0600
Subject: [R] How about a mascot for R?
In-Reply-To: <web-2697598@be1.bc.edu>
References: <web-2697598@be1.bc.edu>
Message-ID: <1102005500.5637.174.camel@horizons.localdomain>

On Thu, 2004-12-02 at 11:12 -0500, Damian Betebenner wrote:
> R users,
> 
> How come R doesn't have a mascot? Linux has one and so does LaTeX, so
> shouldn't R? I personally think that associating a "friendly face"
> with R would be a good thing for R (one letter names can be quite
> intimidating).
> 
> I apologize if this is addressed in the FAQ. I searched the FAQ as
> well as the mailing list archives and checked   
> 
> ?mascot   
> 
> but to no avail. ;-)


Well...curiously, O'Reilly has selected the Ram for the cover of an as
yet unpublished work on R for Bioinformatics:

http://www.amazon.com/exec/obidos/tg/detail/-/059600544X

What is interesting, is that the Ram (in a different pose) is already
used for the well known Perl Cookbook:

http://www.amazon.com/exec/obidos/ASIN/0596003137


Does that mean that O'Reilly is running out of iconic animals?


Perhaps we need another graphics contest amongst the artistically
challenged useR's for a mascot? 

< Sorry Paul :-) >

Marc



From ernesto at ipimar.pt  Thu Dec  2 17:24:19 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 02 Dec 2004 16:24:19 +0000
Subject: [R] How about a mascot for R?
In-Reply-To: <web-2697598@be1.bc.edu>
References: <web-2697598@be1.bc.edu>
Message-ID: <1102004659.10726.24.camel@mordor.ipimar.pt>

What about a Ray ?

Take a look at:

http://www.fishbase.org/Photos/ThumbnailsSummary.cfm?ID=15487

a shark family called "Rhinobatus rhinobatus" check

http://www.fishbase.org/Photos/ThumbnailsSummary.cfm?ID=5016


;-)

regards

EJ


On Thu, 2004-12-02 at 16:12, Damian Betebenner wrote:
> R users,
> 
> How come R doesn't have a mascot? Linux has one and so does LaTeX, so shouldn't R? I personally think that associating a "friendly face" with R would be a good thing for R (one letter names can be quite intimidating).
> 
> I apologize if this is addressed in the FAQ. I searched the FAQ as well as the mailing list archives and checked   
> 
> ?mascot   
> 
> but to no avail. ;-)
> 
> Damian
> 
> 
> Damian Betebenner
> Educational Research, Measurement & Evaluation
> Lynch School of Education
> Boston College
> Chestnut Hill, MA 02467
> 
> (617) 552 4491
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Dec  2 17:40:52 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Dec 2004 17:40:52 +0100
Subject: [R] How about a mascot for R?
In-Reply-To: <web-2697598@be1.bc.edu>
References: <web-2697598@be1.bc.edu>
Message-ID: <x2k6s0d563.fsf@biostat.ku.dk>

Damian Betebenner <damian.betebenner at bc.edu> writes:

> R users,
> 
> How come R doesn't have a mascot? Linux has one and so does LaTeX, so shouldn't R? I personally think that associating a "friendly face" with R would be a good thing for R (one letter names can be quite intimidating).
> 
> I apologize if this is addressed in the FAQ. I searched the FAQ as well as the mailing list archives and checked   
> 
> ?mascot   
> 
> but to no avail. ;-)

A furry and cuddly inchworm? 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Christoph.Scherber at uni-jena.de  Thu Dec  2 17:51:23 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Thu, 02 Dec 2004 17:51:23 +0100
Subject: [R] Dominant factors in aov?
In-Reply-To: <AC769F794461D8118F0800A0C9EA256502DA86@delemont_srvr.cabi_europe.local>
References: <AC769F794461D8118F0800A0C9EA256502DA86@delemont_srvr.cabi_europe.local>
Message-ID: <41AF480B.2070207@uni-jena.de>

Dear Rene,

At least from the part of the data.frame attached to your mail, I 
assumed that C,D and E changed in identical ways (but maybe I got this 
wrong).

With your following combination of factors:

A (four levels)
B (three levels)
C (two levels)
D (29 levels) with
E (four replicates)

And assuming independence of the treatment levels, you should get

3 d.f. for A
2 d.f. for B
28 d.f. for D
3 d.f. for E
? residual d.f. (how big is total number of Y values?)

The problem arises if parts of treatments B,D and E are applied to the same subjects, e.g.

B	D	E	Y	
1	1	1	400
2	2	2	300
2	2	3	420
2	2	4	350
(etc)

then you immediately run into problems because treatments B and D (in this case) change in an identical way, i.e. the variances calculated for each level of B and D are the same; this is what causes the ??singularities??. Errors need to be independent, otherwise you will have order dependence in your analyses.

i.e. the output of your aov model will change depending on the sequence in which the terms A,B,C,D,E are entered.

Did I get this right? It would probably help to see the full dataset

Best wishes
Christoph







Rene Eschen wrote:

>Dear Christoph, 
>
>  
>
>>The reason for the "singularities" is that B, C and D are not 
>>independent (in fact, they??re identical in their factor levels, and 
>>hence in their effect on Y).
>>    
>>
>
>I do not understand this. You gave the correct levels for A, B and E, but I
>do not see how they are identical. They have different levels and different
>codings, or is it because A has the same number of levels as E, and E shares
>some of the coding with B?
>
>Ren?? Eschen.
>
>---
>
>For this reason, only the effects of A, B and E can be estimated:
>
>           Df Sum Sq Mean Sq F value    Pr(>F)   
>A            3 302286  100762  7.9887  0.002396 **
>B            1 422869  422869 33.5263 4.683e-05 ***
>E            3  22281    7427  0.5888  0.632334   
>Residuals   14 176583   12613                     
>
>A has 4 levels so there should be 3 d.f. (that??s correct in the table)
>B has 2 levels so there is only 1 d.f. (that??s also correct)
>E has 4 levels so there should be 3 d.f. (also O.K.)
>
>In total, there are [(n=22)-(3)-(1)-(3)] -1 = 14 residual d.f., so 
>that??s OK, too.
>
>Hope this helps,
>Christoph
>
>
>
>levels(A)
>[1] "0"    "250"  "500"  "1000"
> > levels(B)
>[1] "1" "2"
> > levels(E)
>[1] "1" "2" "3" "4"
>
>
>
>
>
>Rene Eschen wrote:
>
>  
>
>>Hi all,
>>
>>I'm using R 2.0.1. for Windows to analyze the influence of following
>>    
>>
>factors
>  
>
>>on response Y:
>>
>>A (four levels)
>>B (three levels)
>>C (two levels)
>>D (29 levels) with
>>E (four replicates)
>>
>>The dataset looks like this:
>>A	B	C	D	E	Y
>>0	1	1	1	1	491.9
>>0	1	1	1	2	618.7
>>0	1	1	1	3	448.2
>>0	1	1	1	4	632.9
>>250	1	1	1	1	92.4
>>250	1	1	1	2	117
>>250	1	1	1	3	35.5
>>250	1	1	1	4	102.7
>>500	1	1	1	1	47
>>500	1	1	1	2	57.4
>>500	1	1	1	3	6.5
>>500	1	1	1	4	50.9
>>1000	1	1	1	1	0.7
>>1000	1	1	1	2	6.2
>>1000	1	1	1	3	0.5
>>1000	1	1	1	4	1.1
>>0	2	2	2	1	6
>>0	2	2	2	2	4.2
>>0	2	2	2	3	20.3
>>0	2	2	2	4	3.5
>>250	2	2	2	1	8.4
>>250	2	2	2	2	2.8
>>
>>etc.
>>
>>If I ask the following: summary(aov(Y~A+B+C+D+E))
>>
>>R gives me this answer:
>>
>>  		 Df  Sum Sq Mean Sq  F value Pr(>F)    
>>A  		  3 135.602  45.201 310.2166 <2e-16 ***
>>B  		  2   0.553   0.276   1.8976 0.1512    
>>C  		  1   0.281   0.281   1.9264 0.1659    
>>D  		 25  92.848   3.714  25.4890 <2e-16 ***
>>E  		  3   0.231   0.077   0.5279 0.6634    
>>Residuals   411  59.885   0.146   
>>
>>Can someone explain me why factor C has only 25 Df (in stead of 28, what I
>>expected), and why this number changes when I leave out factors B or C (but
>>not A)? Why do factors B and C (but again: not A) not show up in the
>>calculation if they appear later in the formula than D?
>>
>>When I ask summary.lm(aov(Y~A+B+C+D+E)), R tells me that three levels of D
>>were not defined because of "singularities" (what does this word mean?).
>>After checking and playing around with the dataset, I find no logical
>>    
>>
>reason
>  
>
>>for which levels are not defined. Even if I construct a "perfect" dataset
>>(balanced, no missing values) I never get the correct number of Df. 
>>
>>My other datasets are analyzed as expected using the similar function calls
>>and similar datasets. Am I doing something wrong here?
>>
>>Many thanks,
>>
>>Ren?? Eschen.
>>
>>___
>>drs. Ren?? Eschen
>>CABI Bioscience Switzerland Centre
>>1 Rue des Grillons
>>CH-2800 Del??mont
>>Switzerland
>>+41 32 421 48 87 (Direct)
>>+41 32 421 48 70 (Secretary)
>>+41 32 421 48 71 (Fax)
>>
>>http://www.unifr.ch/biol/ecology/muellerschaerer/group/eschen/eschen.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>    
>>
>http://www.R-project.org/posting-guide.html
>  
>
>> 
>>
>>    
>>
>
>  
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec  2 17:52:46 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 02 Dec 2004 16:52:46 -0000 (GMT)
Subject: [R] A somewhat off the line question to a log normal distrib
In-Reply-To: <Pine.A41.4.61b.0412020804260.243438@homer07.u.washington.edu>
Message-ID: <XFMail.041202165246.Ted.Harding@nessie.mcc.ac.uk>

On 02-Dec-04 Thomas Lumley wrote:
> On Thu, 2 Dec 2004 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> Hmm, perhaps you should think again! If X and Y have log-normal
>> distributions (mathematically exactly), then (X+Y)/2 does not
>> (mathematically) have a log-normal distribution -- still less
>> the arithmetic mean of some 30 such variables. So one wonders
>> what the basis of his "explanation" was.
> 
> The original poster's boss did not (as far as we know) claim
> that the measurements were either independent or identically
> distributed.  While the problem would be simpler if they were,
> there is no guarantee that the answer would be remotely relevant.

Not quite sure of your point here, Thomas. I certainly wasn't
writing on the basis that the boss had claimed that they were
either independent or identically disitributed, and the paragraph
you quote was in reposnse to:

  "The aformentioned daily measurements follow a log-normal
   distribution when considered over the course of a year. 
   Okay. He also tried to explain me that the monthly means
  (based on the daily measurements) must follow a log-normal
  distribution too then over the course of a year."

which I interpreted as arguing that "if daily data log-normal,
then monthly means must consequently be log-normal", i.e. that
the mean of log-normals is log-normal; and I was simply pointing
out that this is a false implication (which would be the case
even if the data are neither independent nor identically distributed,
except in the extreme case where they are all copies of the one
log-normal variable).

Granted I later used i.i.d log-normals as examples; but then
pointed out that the mean of log-normals could remain sufficiently
skew that a log-normal could still be a useful distribution to
adopt.

Of course the boss may in reality have argued that the distribution
of monthly means was, as a matter of fact, skew, and therefore
log-normal would be approriate; but Siegfried Gonzi stated that,
according to his observations, these were more like Gaussian.
So I came to the conclusion that my first interprtation was
appropriate. Maybe Siegfried could clarify?

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 02-Dec-04                                       Time: 16:52:46
------------------------------ XFMail ------------------------------



From B.Rowlingson at lancaster.ac.uk  Thu Dec  2 18:01:01 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 02 Dec 2004 17:01:01 +0000
Subject: [R] How about a mascot for R?
In-Reply-To: <1102005500.5637.174.camel@horizons.localdomain>
References: <web-2697598@be1.bc.edu>
	<1102005500.5637.174.camel@horizons.localdomain>
Message-ID: <41AF4A4D.8080108@lancaster.ac.uk>

Marc Schwartz wrote:

> Well...curiously, O'Reilly has selected the Ram for the cover of an as
> yet unpublished work on R for Bioinformatics:

  And how come Amazon are selling non-new copies of unpublished books? 
Is someone selling a review copy?

  Mascot eh? Hmmm. Let's change the name of R to "Rooster", standing for 
the "R Object Oriented STatistics EnviRonment", and then the mascot 
would be a cock.

Or not.

Baz



From ernesto at ipimar.pt  Thu Dec  2 17:46:43 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 02 Dec 2004 16:46:43 +0000
Subject: [R] How about a mascot for R?
In-Reply-To: <x2k6s0d563.fsf@biostat.ku.dk>
References: <web-2697598@be1.bc.edu>  <x2k6s0d563.fsf@biostat.ku.dk>
Message-ID: <1102006002.10726.32.camel@mordor.ipimar.pt>

On Thu, 2004-12-02 at 16:40, Peter Dalgaard wrote:
> Damian Betebenner <damian.betebenner at bc.edu> writes:
> 
> > R users,
> > 
> > How come R doesn't have a mascot? Linux has one and so does LaTeX, so shouldn't R? I personally think that associating a "friendly face" with R would be a good thing for R (one letter names can be quite intimidating).
> > 
> > I apologize if this is addressed in the FAQ. I searched the FAQ as well as the mailing list archives and checked   
> > 
> > ?mascot   
> > 
> > but to no avail. ;-)
> 
> A furry and cuddly inchworm? 

Like this :-)

http://www.missouriplants.com/Bugs/Inchworm.jpg

EJ



From damian.betebenner at bc.edu  Thu Dec  2 18:07:27 2004
From: damian.betebenner at bc.edu (Damian Betebenner)
Date: Thu, 02 Dec 2004 12:07:27 -0500
Subject: [R] How about a mascot for R?
Message-ID: <web-2698523@be1.bc.edu>

Excellent replies,

So a couple of questions about preferences for the mascot:

1. Does the mascot need to have a name that starts with R? Is that usually the way it works?

So far the possibilities put forward are: Ray, Ram, Inch Worm, Rhinoceros

2. Should the depiction of the mascot be more "literal" like the O'Reilly Ram, or cartoonish like the Linux and LaTeX mascots? Or could there be multiple representations?

Damian

Damian Betebenner
Educational Research, Measurement & Evaluation
Lynch School of Education
Boston College
Chestnut Hill, MA 02467

(617) 552 4491



From tjrc at sanger.ac.uk  Thu Dec  2 18:17:21 2004
From: tjrc at sanger.ac.uk (Tim Cutts)
Date: Thu, 2 Dec 2004 17:17:21 +0000
Subject: [R] A possible way to reduce basic questions
In-Reply-To: <20041202012336.D7E4A3986@mprdmxin.myway.com>
References: <20041202012336.D7E4A3986@mprdmxin.myway.com>
Message-ID: <06C2E003-4486-11D9-AC51-000A95B2B140@sanger.ac.uk>


On 2 Dec 2004, at 1:23 am, Gabor Grothendieck wrote:

>
> Jim Lemon <bitwrit <at> ozemail.com.au> writes:
>
>> I have been thinking about how to reduce the number of basic 
>> questions that
>> elicit the ...ahem... robust debate that has occurred about how to 
>> answer
>
>
> The traffic on r-help could be reduced by creating a second list where
> more elementary questions are asked.

But how many people here would read it, and help the novices (like me) 
out?  There is always the danger that novice lists just become 
write-only lists.

Tim

-- 
Dr Tim Cutts
Informatics Systems Group, Wellcome Trust Sanger Institute
GPG: 1024D/E3134233 FE3D 6C73 BBD6 726A A3F5  860B 3CDD 3F56 E313 4233



From hb at maths.lth.se  Thu Dec  2 18:18:39 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 2 Dec 2004 18:18:39 +0100
Subject: [R] How about a mascot for R?
In-Reply-To: <web-2698523@be1.bc.edu>
Message-ID: <000901c4d892$f941cb40$e502eb82@hblaptop>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Damian 
> Betebenner
> Sent: Thursday, December 02, 2004 6:07 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How about a mascot for R?
> 
> 
> Excellent replies,
> 
> So a couple of questions about preferences for the mascot:
> 
> 1. Does the mascot need to have a name that starts with R? Is 
> that usually the way it works?
> 
> So far the possibilities put forward are: Ray, Ram, Inch 
> Worm, Rhinoceros

R.oo (http://www.maths.lth.se/help/R/R.oo/), ooops Roo, which is Australian
slang for Kangaroo. http://images.google.com/images?q=roo

Cheers

Henrik Bengtsson
 
> 2. Should the depiction of the mascot be more "literal" like 
> the O'Reilly Ram, or cartoonish like the Linux and LaTeX 
> mascots? Or could there be multiple representations?
> 
> Damian
> 
> Damian Betebenner
> Educational Research, Measurement & Evaluation
> Lynch School of Education
> Boston College
> Chestnut Hill, MA 02467
> 
> (617) 552 4491
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From daniele.medri at libero.it  Thu Dec  2 18:29:31 2004
From: daniele.medri at libero.it (Daniele Medri)
Date: Thu, 2 Dec 2004 18:29:31 +0100
Subject: [R] Text Mining with R
Message-ID: <200412021829.31979.daniele.medri@libero.it>

Dears,

anyone has experiences with text mining and R?
I'll be very greatfull for tutorial or examples.

Thanks
-- 
Daniele Medri - http://www.medri.org



From jones at reed.edu  Thu Dec  2 18:36:01 2004
From: jones at reed.edu (Albyn Jones)
Date: Thu, 2 Dec 2004 09:36:01 -0800
Subject: [R] How about a mascot for R?
In-Reply-To: <000901c4d892$f941cb40$e502eb82@hblaptop>
References: <web-2698523@be1.bc.edu> <000901c4d892$f941cb40$e502eb82@hblaptop>
Message-ID: <20041202173601.GU20533@sellwood.reed.edu>

On Thu, Dec 02, 2004 at 06:18:39PM +0100, Henrik Bengtsson wrote:
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Damian 
> > Betebenner
> > Sent: Thursday, December 02, 2004 6:07 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] How about a mascot for R?
> > 
> > 
> > Excellent replies,
> > 
> > So a couple of questions about preferences for the mascot:
> > 
> > 1. Does the mascot need to have a name that starts with R? Is 
> > that usually the way it works?
> > 
> > So far the possibilities put forward are: Ray, Ram, Inch 
> > Worm, Rhinoceros
> 
> R.oo (http://www.maths.lth.se/help/R/R.oo/), ooops Roo, which is Australian
> slang for Kangaroo. http://images.google.com/images?q=roo
> 
> Cheers
> 
> Henrik Bengtsson
>  

how about a Shark?  that would be a bit more subtle:-)

albyn



From MSchwartz at MedAnalytics.com  Thu Dec  2 18:36:57 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 02 Dec 2004 11:36:57 -0600
Subject: [R] How about a mascot for R?
In-Reply-To: <41AF4A4D.8080108@lancaster.ac.uk>
References: <web-2697598@be1.bc.edu>
	<1102005500.5637.174.camel@horizons.localdomain>
	<41AF4A4D.8080108@lancaster.ac.uk>
Message-ID: <1102009017.5637.183.camel@horizons.localdomain>

On Thu, 2004-12-02 at 17:01 +0000, Barry Rowlingson wrote:
> Marc Schwartz wrote:
> 
> > Well...curiously, O'Reilly has selected the Ram for the cover of an as
> > yet unpublished work on R for Bioinformatics:
> 
>   And how come Amazon are selling non-new copies of unpublished books? 
> Is someone selling a review copy?

Good questions Baz. I do not know the answer and according to the single
review post there, it would appear to be behind schedule and possibly at
risk of not being completed.

A Google search yields some information from other sites suggesting that
it was initially targeted for release this past February.

Perhaps someone here or on the BioC list might have more details.

>   Mascot eh? Hmmm. Let's change the name of R to "Rooster", standing for 
> the "R Object Oriented STatistics EnviRonment", and then the mascot 
> would be a cock.

> Or not.

May I just say...Wow...  ;-)

Marc



From MSchwartz at MedAnalytics.com  Thu Dec  2 18:44:26 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 02 Dec 2004 11:44:26 -0600
Subject: [R] How about a mascot for R?
In-Reply-To: <web-2698523@be1.bc.edu>
References: <web-2698523@be1.bc.edu>
Message-ID: <1102009466.5637.187.camel@horizons.localdomain>

On Thu, 2004-12-02 at 12:07 -0500, Damian Betebenner wrote:
> Excellent replies,
> 
> So a couple of questions about preferences for the mascot:
> 
> 1. Does the mascot need to have a name that starts with R? Is that
> usually the way it works?
> 
> So far the possibilities put forward are: Ray, Ram, Inch Worm,
> Rhinoceros
> 
> 2. Should the depiction of the mascot be more "literal" like the
> O'Reilly Ram, or cartoonish like the Linux and LaTeX mascots? Or could
> there be multiple representations?
> 
> Damian

OK...I have one more to add:

The Tuatara, which is native to New Zealand and would pay homage to R's
(R and R's) roots.

More information is here:

http://www.mtbruce.doc.govt.nz/tuatara.htm

:-)

Marc



From ernesto at ipimar.pt  Thu Dec  2 18:33:21 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 02 Dec 2004 17:33:21 +0000
Subject: [R] How about a mascot for R?
In-Reply-To: <41AF4A4D.8080108@lancaster.ac.uk>
References: <web-2697598@be1.bc.edu>
	<1102005500.5637.174.camel@horizons.localdomain>
	<41AF4A4D.8080108@lancaster.ac.uk>
Message-ID: <1102008800.10726.34.camel@mordor.ipimar.pt>

On Thu, 2004-12-02 at 17:01, Barry Rowlingson wrote:
> Marc Schwartz wrote:
> 
> > Well...curiously, O'Reilly has selected the Ram for the cover of an as
> > yet unpublished work on R for Bioinformatics:
> 
>   And how come Amazon are selling non-new copies of unpublished books? 
> Is someone selling a review copy?
> 
>   Mascot eh? Hmmm. Let's change the name of R to "Rooster", standing for 
> the "R Object Oriented STatistics EnviRonment", and then the mascot 
> would be a cock.
> 
> Or not.
> 
> Baz
> 


What about "Rolling Stones" ? we could use Mick Jagger's face ? They
even have a music call "The little red rooster".

;-)

EJ



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec  2 18:46:54 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 02 Dec 2004 17:46:54 -0000 (GMT)
Subject: [R] How about a mascot for R?
In-Reply-To: <000901c4d892$f941cb40$e502eb82@hblaptop>
Message-ID: <XFMail.041202174654.Ted.Harding@nessie.mcc.ac.uk>

On 02-Dec-04 Henrik Bengtsson wrote:
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Damian 
>> Betebenner
>> Sent: Thursday, December 02, 2004 6:07 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] How about a mascot for R?
>> 
>> 
>> Excellent replies,
>> 
>> So a couple of questions about preferences for the mascot:
>> 
>> 1. Does the mascot need to have a name that starts with R? Is 
>> that usually the way it works?
>> 
>> So far the possibilities put forward are: Ray, Ram, Inch 
>> Worm, Rhinoceros
> 
> R.oo (http://www.maths.lth.se/help/R/R.oo/), ooops Roo, which is
> Australian slang for Kangaroo. http://images.google.com/images?q=roo
> 
> Cheers
> 
> Henrik Bengtsson

(And of course ".oo" suggests the OO aspect of R as well).

But what appeals to me about this suggestion is that it made
me recall "cartoon" drawings I saw many years ago, illustrating
"leptokurtic" and "platykurtic".

The "platykurtic" was a profile drawing of a platypus,
illustrating the flat-topped profile of such a distribution.

The "leptokurtic" showed two kanagaroos in profile, upright,
face-to-face, with tails outstretched on the ground behind them.
The envelope of this drawing illustrated the high peak and the
long tails. (And of course they are good "leppers").

Can anyone remember where this appeared?

Perhaps these would be really appropriate mascots!

(And if we call the kangaroo "R.oo", what shall we call the
platypus? Perhaps our cobbers in Oz can give us a hint.)

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 02-Dec-04                                       Time: 17:46:54
------------------------------ XFMail ------------------------------



From jfox at al.noaa.gov  Thu Dec  2 19:22:57 2004
From: jfox at al.noaa.gov (Jenny Fox)
Date: Thu, 2 Dec 2004 11:22:57 -0700
Subject: [R] image() or plotting functions or something else broken in 2.0.1
	on OS X?
Message-ID: <3107545C-448F-11D9-975A-00039366A72C@al.noaa.gov>

Hello.

I just upgraded to 2.01 on Mac OS 10.3.6.  I used to use the image() 
function in 1.9.x.  Now, running the example code from the image() help 
page gives me the following error:

 > x <- y <- seq(-4*pi, 4*pi, len=27)
 > r <- sqrt(outer(x^2, y^2, "+"))
 > image(z = z <- cos(r^2)*exp(-r/6), col=gray((0:32)/32))
2004-12-02 10:35:27.815 R[2649] *** Assertion failure in -[RDeviceView 
lockFocus], AppKit.subproj/NSView.m:2746
2004-12-02 10:35:27.817 R[2649] *** NSTimer discarding exception 
'NSInternalInconsistencyException' (reason 'lockFocus sent to a view 
whose window is deferred and does not yet have a corresponding platform 
window') that raised during firing of timer with target 3b9a60 and 
selector 'runRELP:'

And then R quits unexpectedly.

I ran the demo(graphics) and it ran fine.  But then I tried to run 
coplot() independently of the demo and I got the same error:

 > ## Tonga Trench Earthquakes
 > coplot(lat ~ long | depth, data = quakes)
2004-12-02 11:17:45.150 R[8541] *** Assertion failure in -[RDeviceView 
lockFocus], AppKit.subproj/NSView.m:2746
2004-12-02 11:17:45.152 R[8541] *** NSTimer discarding exception 
'NSInternalInconsistencyException' (reason 'lockFocus sent to a view 
whose window is deferred and does not yet have a corresponding platform 
window') that raised during firing of timer with target 3b9a60 and 
selector 'runRELP:'

I'm not sure what I'm doing wrong - a quartz window appears when I run 
the command, so it should in fact have a "corresponding platform 
window".

Any advice would be appreciated.

--jenny

Jennifer Fox
Graduate Researcher
NOAA Aeronomy Laboratory
Boulder, CO

jfox at al.noaa.gov



From pburns at pburns.seanet.com  Thu Dec  2 19:22:54 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 02 Dec 2004 18:22:54 +0000
Subject: [R] How about a mascot for R?
In-Reply-To: <x2k6s0d563.fsf@biostat.ku.dk>
References: <web-2697598@be1.bc.edu> <x2k6s0d563.fsf@biostat.ku.dk>
Message-ID: <41AF5D7E.6030501@pburns.seanet.com>

Or a furry and cuddly raccoon?

Peter Dalgaard wrote:

>Damian Betebenner <damian.betebenner at bc.edu> writes:
>
>  
>
>>R users,
>>
>>How come R doesn't have a mascot? Linux has one and so does LaTeX, so shouldn't R? I personally think that associating a "friendly face" with R would be a good thing for R (one letter names can be quite intimidating).
>>
>>I apologize if this is addressed in the FAQ. I searched the FAQ as well as the mailing list archives and checked   
>>
>>?mascot   
>>
>>but to no avail. ;-)
>>    
>>
>
>A furry and cuddly inchworm? 
>
>  
>



From abu3ammar at gmail.com  Thu Dec  2 19:25:01 2004
From: abu3ammar at gmail.com (Slim Shady)
Date: Thu, 2 Dec 2004 13:25:01 -0500
Subject: [R] SJava
Message-ID: <b1d3150404120210255d753fa9@mail.gmail.com>

is the SJava package still being maintained and who maintains it? Who
should I send SJava issues to? The email on the omegahat site does not
deliver.



From abu3ammar at gmail.com  Thu Dec  2 20:12:19 2004
From: abu3ammar at gmail.com (Slim Shady)
Date: Thu, 2 Dec 2004 14:12:19 -0500
Subject: [R] SJava convert problem
Message-ID: <b1d3150404120211126cc880b8@mail.gmail.com>

Consider this Java class:
class myClass {
    final public static String[] testString () {
        return new String[]{"my", "name", "is"} ;
    }
    
    final public static int[] testInt () {
        return new int[]{5, 10, 15};
    }    
    
    final public static char[] testChar () {
        return new char[]{'x', 'y', 'z'};
    }

}

When invoked in R:
> s <- .Java(className, "testString")
> class(s)
[1] "list"
> length(s)
[1] 3
> s
[[1]]
[1] "my"

[[2]]
[1] "name"

[[3]]
[1] "is"


> c <- .Java(className, "testChar")
> class(c)
[1] "character"
> length(c)
[1] 1
> c
[1] "X"

> i <- .Java(className, "testInt")
> class(i)
[1] "integer"
> length(i)
[1] 3
> i
[1]  5 10 15

Note that the conversion of testInt is just what I expected (and what
I get in S-PLUS). The conversion of testChar is flat wrong because
data is lost. The conversion of testString is problematic since I have
to convert the list back using as.character (in S-PLUS this function
returns character instaid of list and saves me the convertion from
list to character)
Is the convertion of char[] a bug? is there is a way to make the
conversion of String[] returns a character class instaid of a list
just like in the S-PLUS Java Connect?



From bitwrit at ozemail.com.au  Sat Dec  4 06:21:05 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Sat, 4 Dec 2004 16:21:05 +1100
Subject: [R] How about a mascot for R?
Message-ID: <20041202191645.WGAG4289.smta01.mail.ozemail.net@there>

Ahh, to heck with all them animals. R is a system of interacting components, 
so I suggest Mr Gearhead as a suitable mascot.

http://www.bitwrit.com.au/gearhead.png

Jim



From Charles.Annis at StatisticalEngineering.com  Thu Dec  2 20:24:45 2004
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 2 Dec 2004 14:24:45 -0500
Subject: [R] How about a mascot for R?
In-Reply-To: <20041202191645.WGAG4289.smta01.mail.ozemail.net@there>
Message-ID: <200412021924.iB2JOneW014070@hypatia.math.ethz.ch>

I vote for Mr. Gearhead!

But then, I'm a (shudder) engineer.  ~:-)

Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  503-217-5849
http://www.StatisticalEngineering.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jim Lemon
Sent: Saturday, December 04, 2004 12:21 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] How about a mascot for R?

Ahh, to heck with all them animals. R is a system of interacting components,

so I suggest Mr Gearhead as a suitable mascot.

http://www.bitwrit.com.au/gearhead.png

Jim

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jari.oksanen at oulu.fi  Thu Dec  2 20:32:43 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Thu, 02 Dec 2004 21:32:43 +0200
Subject: [R] How about a mascot for R?
In-Reply-To: <XFMail.041202174654.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041202174654.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <F0145B23-4498-11D9-9382-000A95C76CA8@oulu.fi>


On 2 Dec 2004, at 19:46, (Ted Harding) wrote:

> On 02-Dec-04 Henrik Bengtsson wrote:
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Damian
>>> Betebenner
>>> Sent: Thursday, December 02, 2004 6:07 PM
>>> To: r-help at stat.math.ethz.ch
>>> Subject: [R] How about a mascot for R?
>>>
>>>
>>> Excellent replies,
>>>
>>> So a couple of questions about preferences for the mascot:
>>>
>>> 1. Does the mascot need to have a name that starts with R? Is
>>> that usually the way it works?
>>>
>>> So far the possibilities put forward are: Ray, Ram, Inch
>>> Worm, Rhinoceros
>>
>> R.oo (http://www.maths.lth.se/help/R/R.oo/), ooops Roo, which is
>> Australian slang for Kangaroo. http://images.google.com/images?q=roo
>>
>> Cheers
>>
>> Henrik Bengtsson
>
> (And of course ".oo" suggests the OO aspect of R as well).
>
> But what appeals to me about this suggestion is that it made
> me recall "cartoon" drawings I saw many years ago, illustrating
> "leptokurtic" and "platykurtic".
>
> The "platykurtic" was a profile drawing of a platypus,
> illustrating the flat-topped profile of such a distribution.
>
> The "leptokurtic" showed two kanagaroos in profile, upright,
> face-to-face, with tails outstretched on the ground behind them.
> The envelope of this drawing illustrated the high peak and the
> long tails. (And of course they are good "leppers").
>
> Can anyone remember where this appeared?
> LEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
I can check that tomorrow when I'm at my office. You can have a look at 
the image at

http://cc.oulu.fi/~jarioksa/mascot.html

I think this is a copyright picture, and it cannot be used freely as a 
mascot (and will disappear soon from this address).

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From pierre.bady at univ-lyon1.fr  Thu Dec  2 20:41:46 2004
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Thu, 02 Dec 2004 20:41:46 +0100
Subject: [R] How about a mascot for R?
In-Reply-To: <41AF5D7E.6030501@pburns.seanet.com>
References: <x2k6s0d563.fsf@biostat.ku.dk> <web-2697598@be1.bc.edu>
	<x2k6s0d563.fsf@biostat.ku.dk>
Message-ID: <5.1.0.14.2.20041202204034.034701d8@pop.univ-lyon1.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041202/a7f06059/attachment.pl

From pierre.bady at univ-lyon1.fr  Thu Dec  2 20:57:06 2004
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Thu, 02 Dec 2004 20:57:06 +0100
Subject: [R] How about a mascot for R?
In-Reply-To: <5.1.0.14.2.20041202204034.034701d8@pop.univ-lyon1.fr>
References: <41AF5D7E.6030501@pburns.seanet.com> <x2k6s0d563.fsf@biostat.ku.dk>
	<web-2697598@be1.bc.edu> <x2k6s0d563.fsf@biostat.ku.dk>
Message-ID: <5.1.0.14.2.20041202205502.00ba2740@pop.univ-lyon1.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041202/910e406a/attachment.pl

From hodgess at gator.uhd.edu  Thu Dec  2 21:08:12 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Thu, 2 Dec 2004 14:08:12 -0600
Subject: [R] How about a mascot for R?
Message-ID: <200412022008.iB2K8C108678@gator.dt.uh.edu>

How about an R-madillo?

Erin from Texas



From tchur at optushome.com.au  Thu Dec  2 21:16:35 2004
From: tchur at optushome.com.au (Tim Churches)
Date: Fri, 03 Dec 2004 07:16:35 +1100
Subject: [R] How about a mascot for R?
In-Reply-To: <web-2697598@be1.bc.edu>
References: <web-2697598@be1.bc.edu>
Message-ID: <41AF7823.1080101@optushome.com.au>

Damian Betebenner wrote:
> R users,
> 
> How come R doesn't have a mascot? 

Perhaps someone with artistic flair could create a mascot based on this 
image? It would help to give newcomers to R-help the right idea:

http://www.accesscom.com/~alvaro/alien/thepics/ripley1__.jpg

Tim C


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 263 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20041203/bd28451b/signature.bin

From damian.cirelli at maine.edu  Thu Dec  2 21:46:02 2004
From: damian.cirelli at maine.edu (=?ISO-8859-1?Q?Dami=E1n_Cirelli?=)
Date: Thu, 02 Dec 2004 15:46:02 -0500
Subject: [R] How about a mascot for R?
In-Reply-To: <web-2697598@be1.bc.edu>
References: <web-2697598@be1.bc.edu>
Message-ID: <41AF7F0A.7090603@maine.edu>

I think "Ross & Robert" should be consulted...
Tux was born because Linus happend to say in a forum that he was rather 
fond of penguins ;-)
May be these guys have a favourite animal... and it would be cool if 
each had a different favourite animal and thus one mythic "beast" could 
be created from their junction... with emphasis on coolness.
Just a thought... carry on.



From p.connolly at hortresearch.co.nz  Thu Dec  2 21:50:22 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 3 Dec 2004 09:50:22 +1300
Subject: [R] How about a mascot for R?
In-Reply-To: <200412022008.iB2K8C108678@gator.dt.uh.edu>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>
Message-ID: <20041202205022.GJ18464@hortresearch.co.nz>

On Thu, 02-Dec-2004 at 02:08PM -0600, Erin Hodgess wrote:

|> How about an R-madillo?

I'm for R-gnu which already has a song (as pointed out by Murray
Jorgensen some time back).  I'm sure an arrangement could be worked
out with the Emacs people.


The Flanders and Swann song, the exact title of which I don't recall,
was a response to someone voicing the opinion that the said animal was
an Helk.  Rather witty in my view and has a spirit in keeping with
this list.


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From stram at usc.edu  Thu Dec  2 21:56:52 2004
From: stram at usc.edu (Dan Stram)
Date: Thu, 02 Dec 2004 12:56:52 -0800
Subject: [R] R and Fortran in Windows
Message-ID: <000201c4d8b1$73334f50$3c207d80@NIEHS.usc.edu>

I just joined the list and appologize if this has been answered before
but I am trying to interface between R and the Compaq Visual Fortran
compiler version 6.6 for Windows. 

I found the following instructions on the web -- and an example.  When I
follow these directions exactly. R 2.0.0 crashes. Has anyone had any
experience with this?

Below are the instructions that I located:

Thanks


Dan Stram

Professor
Department of Preventive Medicine
Division of Genetic Epidemiology and Biostatistics
University of Southern California
1540 Alcazar Street, Suite 220
Los Angeles, CA 90033
tel 323-442-1817
fax 323-442-2349
email stram at usc.edu
web http://www-rcf.usc.edu/~stram





These notes were written by J.R.M. Hosking.

Using Fortran routines from R with CVF

1.  Take a working Fortran subroutine, and put its name in an
ATTRIBUTES DLLEXPORT directive to ensure that the routine name
is exported from the DLL that will be built.  Example:

      SUBROUTINE MYSUB(X,N,XMEAN)
CDEC$ ATTRIBUTES DLLEXPORT :: MYSUB
     IMPLICIT DOUBLE PRECISION (A-H,O-Z)
     DOUBLE PRECISION X(N)
     XMEAN=0D0
     DO 10 J=1,N
     XMEAN=XMEAN+X(J)
  10 CONTINUE
     XMEAN=XMEAN/N
     RETURN
     END

2. Compile and link the routine using the CVF options /assume:underscore
and  /dll.  E.g., supposing that the routine is in a file rtest.f,
at the command prompt type

   df rtest.f /assume:underscore /dll

This will create a file rtest.dll file that should be kept, and
.obj, .lib and .exp files that are not needed for R and can be deleted.
(/assume:underscore is needed because R expects that a routine name
exported from a DLL will have an underscore appended to it, but CVF
does not do this by default.)

3.  Now from an R command prompt, use dyn.load to load the DLL and
.Fortan("MYSUB",...) to call the function.  Example:

  > dyn.load("E:\\rtest.dll")
 > is.loaded(symbol.For("MYSUB"))
 [1] TRUE
 > x<-1:6
 > .Fortran("MYSUB",as.double(x),as.integer(length(x)),xmean=double(1))
 [[1]]
 [1] 1 2 3 4 5 6

  [[2]]
 [1] 6

  $xmean
 [1] 3.5

  >



From p.dalgaard at biostat.ku.dk  Thu Dec  2 21:56:16 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Dec 2004 21:56:16 +0100
Subject: [R] How about a mascot for R?
In-Reply-To: <41AF7823.1080101@optushome.com.au>
References: <web-2697598@be1.bc.edu> <41AF7823.1080101@optushome.com.au>
Message-ID: <x21xe8ctcf.fsf@biostat.ku.dk>

Tim Churches <tchur at optushome.com.au> writes:

> Damian Betebenner wrote:
> > R users,
> > How come R doesn't have a mascot?
> 
> Perhaps someone with artistic flair could create a mascot based on
> this image? It would help to give newcomers to R-help the right idea:
> 
> http://www.accesscom.com/~alvaro/alien/thepics/ripley1__.jpg

Or maybe this one:

http://www.accesscom.com/~alvaro/alien/thepics/bg10s.jpg

or (apologies to Pat Burns):

http://www.accesscom.com/~alvaro/alien/thepics/alien102_.jpg

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From krcabrer at epm.net.co  Thu Dec  2 23:10:12 2004
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Thu, 02 Dec 2004 17:10:12 -0500
Subject: [R] How about a mascot for R?
In-Reply-To: <200412022008.iB2K8C108678@gator.dt.uh.edu>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>
Message-ID: <41AF92C4.2030001@epm.net.co>

I vote for R-madillo!


Erin Hodgess wrote:

>How about an R-madillo?
>
>Erin from Texas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>

-- 
Kenneth Roy Cabrera Torres
Celular +57 (315) 504 9339



From gcutler at amgen.com  Thu Dec  2 23:19:31 2004
From: gcutler at amgen.com (Gene Cutler)
Date: Thu, 2 Dec 2004 14:19:31 -0800
Subject: [R] R language file for BBEdit
In-Reply-To: <200412021110.iB2B53xG006790@hypatia.math.ethz.ch>
References: <200412021110.iB2B53xG006790@hypatia.math.ethz.ch>
Message-ID: <3D5440BF-44B0-11D9-A4D6-000A95C91324@amgen.com>

For those of you who are using the latest version of the BBEdit text 
editor, I have put together a language definition file for R so that 
you can get nice syntax coloring.  It is available at 
http://smalltime.com/gene/R.plist

Copy the file into "~/Library/Application Support/BBEdit/Language 
Modules/" and restart BBEdit.  Make sure to switch the mapping of ".r" 
files from Rez to R in the language preferences.

I just scanned through the help files on my R install with a perl 
script and pulled out all the keywords and function names I could find.

If anyone uses this and makes any improvements, please let me know.



From arrayprofile at yahoo.com  Thu Dec  2 23:42:06 2004
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 2 Dec 2004 14:42:06 -0800 (PST)
Subject: [R] contrast reference level
Message-ID: <20041202224207.99619.qmail@web40822.mail.yahoo.com>

suppose I have a factor with 4 levels:
'a','b','c','d'. I would like to do analysis of
variance using aov() with the factor as independent
variable. How can I specify the level "b" as the
reference level just like the level "a" would be the
reference level if using contr.treatment as the
contrast?

Thanks



From p.dalgaard at biostat.ku.dk  Thu Dec  2 23:45:45 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Dec 2004 23:45:45 +0100
Subject: [R] contrast reference level
In-Reply-To: <20041202224207.99619.qmail@web40822.mail.yahoo.com>
References: <20041202224207.99619.qmail@web40822.mail.yahoo.com>
Message-ID: <x2vfbkb9pi.fsf@biostat.ku.dk>

array chip <arrayprofile at yahoo.com> writes:

> suppose I have a factor with 4 levels:
> 'a','b','c','d'. I would like to do analysis of
> variance using aov() with the factor as independent
> variable. How can I specify the level "b" as the
> reference level just like the level "a" would be the
> reference level if using contr.treatment as the
> contrast?

relevel() should do just that.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ok at cs.otago.ac.nz  Fri Dec  3 00:05:45 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 3 Dec 2004 12:05:45 +1300 (NZDT)
Subject: [R] dropping rows
Message-ID: <200412022305.iB2N5j1f363112@atlas.otago.ac.nz>

Douglas Bates <bates at stat.wisc.edu> wrote:
	In R this is called subsetting and the simplest way to do this
	is with the subset function.
	
	older <- subset(master, year < 1960)
	
I'm not sure that it's the "simplest".
Since rows for year < 1960 were to be dropped,
I'd say the _simplest_ way to do it is one which exploits
a primitive feature of R:

    master[master$year >= 1960,]

For me, the fact that the 'subset' argument of subset() is evaluated
in the scope of the data frame makes subset() quite a complicated way
to do things.  It's certainly something I'd hesitate to use inside a
function which might be given a data frame without knowing _exactly_
which column names were going to be in scope for the 2nd argument.
The fact that the 'subset' argument is *not* evaluated in the scope
of the 1st argument in other cases also makes subset() a somewhat
confusing function, compared with simple logical indexing.

Strengths of subset() include
 - you can select which columns you want, either instead of choosing
   a subset or at the same time (but you can do this with indexing too)
 - the drop= argument of indexing defaults to FALSE instead of TRUE
   (but this is not a problem for indexing data frames, where
    master[master$year == 1960,] will give you a data frame even if
    there is exactly one row with year 1960)

I would suggest that people who aren't yet thoroughly familiar with
what a simple "[" can do should add subset() to the list of things to
learn about _after_ they've done learning about "[".  On second thoughts,
maybe looking at the implementation of subset.default and subset.data.frame
would be helpful in learning about "[".



From yzhang4 at pobox.une.edu.au  Fri Dec  3 00:07:01 2004
From: yzhang4 at pobox.une.edu.au (Yuandan Zhang)
Date: Fri, 3 Dec 2004 10:07:01 +1100
Subject: [R] A possible way to reduce basic questions
In-Reply-To: <06C2E003-4486-11D9-AC51-000A95B2B140@sanger.ac.uk>
References: <20041202012336.D7E4A3986@mprdmxin.myway.com>
	<06C2E003-4486-11D9-AC51-000A95B2B140@sanger.ac.uk>
Message-ID: <20041203100701.7cff9904.yzhang4@pobox.une.edu.au>

I would like to add a few points on the increasing size of r-help. 

1. this is a possitive sign that more and more people migrate to R. For starting users of R or a R package, the r-help is unique way to seek answers. Gradually, once starting, I trust, users are finding ways to search answers. 

2. DO NOT disencourage users to ask bassic questions, this may scare starters away from R. I believe, most of us experienced such transsion, from basic questions to good familarity with R or some R packages. We should encourage experienced users contribute to Q-A.

3. We may start forming interesting groups to deal with some specifical questions. It is true that not everyone using all packages, or not many people mastering on all packages, but a few users may know almost every aspect of some packages. If a task force can analyse the previous questions, cluster them and identify a number of key areas, this may be a useful start to group people. Directing questions to target groups may produce quick and fruitful answers.

Yuandan
On Thu, 2 Dec 2004 17:17:21 +0000
Tim Cutts <tjrc at sanger.ac.uk> wrote:

> 
> On 2 Dec 2004, at 1:23 am, Gabor Grothendieck wrote:
> 
> >
> > Jim Lemon <bitwrit <at> ozemail.com.au> writes:
> >
> >> I have been thinking about how to reduce the number of basic 
> >> questions that
> >> elicit the ...ahem... robust debate that has occurred about how to 
> >> answer
> >
> >
> > The traffic on r-help could be reduced by creating a second list where
> > more elementary questions are asked.
> 
> But how many people here would read it, and help the novices (like me) 
> out?  There is always the danger that novice lists just become 
> write-only lists.
> 
> Tim
> 
> -- 
> Dr Tim Cutts
> Informatics Systems Group, Wellcome Trust Sanger Institute
> GPG: 1024D/E3134233 FE3D 6C73 BBD6 726A A3F5  860B 3CDD 3F56 E313 4233
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Animal Genetics and Breeding Unit
The University of New England
Armidale, NSW, Australia, 2351



From liaoby at umich.edu  Fri Dec  3 00:23:28 2004
From: liaoby at umich.edu (Ben-Yang Liao)
Date: Thu, 2 Dec 2004 18:23:28 -0500
Subject: [R] combine two strings
Message-ID: <000a01c4d8c5$f140cfd0$6501a8c0@IBMX40>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041202/5ab62bcc/attachment.pl

From tmulholland at bigpond.com  Fri Dec  3 00:50:54 2004
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Fri, 03 Dec 2004 07:50:54 +0800
Subject: [R] How about a mascot for R?
In-Reply-To: <XFMail.041202174654.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041202174654.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41AFAA5E.2020408@bigpond.com>

G'day mate

We're not too clever down here. kangeROO and platyPUS

PUS doesn't sound quite right! D'ya think she'll gofor PUSSY mate?

After all "after a hard days work, a man could really go for" a roo and 
some pussy.

The TV advert that I stole the by-line from ends with "a real cold beer"



Ted Harding wrote:
> On 02-Dec-04 Henrik Bengtsson wrote:
> 
>>>-----Original Message-----
>>>From: r-help-bounces at stat.math.ethz.ch 
>>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Damian 
>>>Betebenner
>>>Sent: Thursday, December 02, 2004 6:07 PM
>>>To: r-help at stat.math.ethz.ch
>>>Subject: [R] How about a mascot for R?
>>>
>>>
>>>Excellent replies,
>>>
>>>So a couple of questions about preferences for the mascot:
>>>
>>>1. Does the mascot need to have a name that starts with R? Is 
>>>that usually the way it works?
>>>
>>>So far the possibilities put forward are: Ray, Ram, Inch 
>>>Worm, Rhinoceros
>>
>>R.oo (http://www.maths.lth.se/help/R/R.oo/), ooops Roo, which is
>>Australian slang for Kangaroo. http://images.google.com/images?q=roo
>>
>>Cheers
>>
>>Henrik Bengtsson
> 
> 
> (And of course ".oo" suggests the OO aspect of R as well).
> 
> But what appeals to me about this suggestion is that it made
> me recall "cartoon" drawings I saw many years ago, illustrating
> "leptokurtic" and "platykurtic".
> 
> The "platykurtic" was a profile drawing of a platypus,
> illustrating the flat-topped profile of such a distribution.
> 
> The "leptokurtic" showed two kanagaroos in profile, upright,
> face-to-face, with tails outstretched on the ground behind them.
> The envelope of this drawing illustrated the high peak and the
> long tails. (And of course they are good "leppers").
> 
> Can anyone remember where this appeared?
> 
> Perhaps these would be really appropriate mascots!
> 
> (And if we call the kangaroo "R.oo", what shall we call the
> platypus? Perhaps our cobbers in Oz can give us a hint.)
> 
> Cheers,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 02-Dec-04                                       Time: 17:46:54
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tmulholland at bigpond.com  Fri Dec  3 00:56:51 2004
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Fri, 03 Dec 2004 07:56:51 +0800
Subject: [R] How about a mascot for R?
In-Reply-To: <41AFAA5E.2020408@bigpond.com>
References: <XFMail.041202174654.Ted.Harding@nessie.mcc.ac.uk>
	<41AFAA5E.2020408@bigpond.com>
Message-ID: <41AFABC3.2000102@bigpond.com>

Apologies for that I meant only to send it to Ted

Tom Mulholland wrote:
> G'day mate
> 
...
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Fri Dec  3 00:57:59 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 2 Dec 2004 18:57:59 -0500
Subject: [R] combine two strings
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3BE@usrymx25.merck.com>

Use string3 <- paste(string1, string2, sep="").

Andy

> From: Ben-Yang Liao
> Sent: Thursday, December 02, 2004 6:23 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] combine two strings
> 
> 
> Hello,
> 
> I would like to combine two strings while using R.
> For instance,
> string1 <- "abcde"
> string2 <- "WXYZ"
> I'd like to combine string1 and string2 into Sting3;
> and string3 should be "abcdeWXZY".
> Would you please tell me how to do it?
> Thank you very much
> 
> Ben-Yang
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ok at cs.otago.ac.nz  Fri Dec  3 01:09:27 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 3 Dec 2004 13:09:27 +1300 (NZDT)
Subject: [R] Hexidecimal conversion
Message-ID: <200412030009.iB309Rtf388927@atlas.otago.ac.nz>

"Hanke, Alex" <HankeA at mar.dfo-mpo.gc.ca> wrote:
	I can produce the hex(a)decimal equivalent of a decimal number
	but I am having a hard time reversing the operation.

There are bound to be better ways, but perhaps the most obvious method
is
    (1) Break the string into single characters.
    (2) Match them up against the hexadecimal digits.
    (3) Multiply each digit by the appropriate power of 16.
    (4) Add 'em up.

hexdigits <- c("0","1","2","3","4","5","6","7",
               "8","9","A","B","C","D","E","F")

fromhex <- function (s) {
    s <- match(strsplit(s, "")[[1]], hexdigits) - 1
    sum(s * 16 ^ ((length(s)-1) : 0))
}



From andrewr at uidaho.edu  Fri Dec  3 01:14:55 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Fri, 3 Dec 2004 11:14:55 +1100
Subject: [R] How about a mascot for R?
In-Reply-To: <41AFAA5E.2020408@bigpond.com>
References: <XFMail.041202174654.Ted.Harding@nessie.mcc.ac.uk>
	<41AFAA5E.2020408@bigpond.com>
Message-ID: <20041203001455.GK827@uidaho.edu>

My handy copy of the Oxford English Dictionary attributes the
association that you quote, including the reference to lepping, to
Yule and Kendall 1937, p. 165.  The entry is under "platykurtic".

Andrew

> Ted Harding wrote:
> >
> >But what appeals to me about this suggestion is that it made
> >me recall "cartoon" drawings I saw many years ago, illustrating
> >"leptokurtic" and "platykurtic".
> >
> >The "platykurtic" was a profile drawing of a platypus,
> >illustrating the flat-topped profile of such a distribution.
> >
> >The "leptokurtic" showed two kanagaroos in profile, upright,
> >face-to-face, with tails outstretched on the ground behind them.
> >The envelope of this drawing illustrated the high peak and the
> >long tails. (And of course they are good "leppers").
> >
> >Can anyone remember where this appeared?
> >
> >Perhaps these would be really appropriate mascots!
> >
> >(And if we call the kangaroo "R.oo", what shall we call the
> >platypus? Perhaps our cobbers in Oz can give us a hint.)
> >
> >Cheers,
> >Ted.
> >
> >
> >--------------------------------------------------------------------
> >E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> >Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> >Date: 02-Dec-04                                       Time: 17:46:54
> >------------------------------ XFMail ------------------------------
> >

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From r_pinedam at hotmail.com  Fri Dec  3 01:19:11 2004
From: r_pinedam at hotmail.com (=?iso-8859-1?B?UmVu6SBQaW5lZGE=?=)
Date: Fri, 03 Dec 2004 00:19:11 +0000
Subject: [R] (sin asunto)
Message-ID: <BAY10-F904489385AC8D92AB376098B10@phx.gbl>



From N.L.Pace at m.cc.utah.edu  Fri Dec  3 01:24:30 2004
From: N.L.Pace at m.cc.utah.edu (Nathan Leon Pace, MD, MStat)
Date: Thu, 02 Dec 2004 17:24:30 -0700
Subject: [R] isotonic regression
Message-ID: <B2A5AE4A-44C1-11D9-B9F2-000D93B1F724@utah.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041202/17d506c9/attachment.pl

From yzhang4 at pobox.une.edu.au  Fri Dec  3 01:28:38 2004
From: yzhang4 at pobox.une.edu.au (Yuandan Zhang)
Date: Fri, 3 Dec 2004 11:28:38 +1100
Subject: [R] combine two strings
In-Reply-To: <000a01c4d8c5$f140cfd0$6501a8c0@IBMX40>
References: <000a01c4d8c5$f140cfd0$6501a8c0@IBMX40>
Message-ID: <20041203112838.03983f6e.yzhang4@pobox.une.edu.au>

 x<-paste(string1, string2, sep="")

On Thu, 2 Dec 2004 18:23:28 -0500
"Ben-Yang Liao" <liaoby at umich.edu> wrote:

> Hello,
> 
> I would like to combine two strings while using R.
> For instance,
> string1 <- "abcde"
> string2 <- "WXYZ"
> I'd like to combine string1 and string2 into Sting3;
> and string3 should be "abcdeWXZY".
> Would you please tell me how to do it?
> Thank you very much
> 
> Ben-Yang
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Fri Dec  3 01:53:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 2 Dec 2004 19:53:05 -0500
Subject: [R] isotonic regression
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3C0@usrymx25.merck.com>

Not sure what model you have in mind.  If a model of the form E(y) = f(x1,
x2) = f1(x1) + f2(x2) (i.e., additive) is sufficient, you can use the
packages `mgcv' or `gbm', both of which provide options to constrain f1 or
f2 to be monotone.

HTH,
Andy

> From: Nathan Leon Pace, MD, MStat
> 
> Hi,
> 
> Has anyone written code for isotonic regression on ordered 
> rectangular 
> grids?
> 
> Nathan
> 
> Nathan Leon Pace, MD, MStat
> University of Utah
> Salt Lake City, UT 84132
> Office: 801.581.6393
> Fax: 801.581.4367
> Cell: 801.558.3987
> Pager: 801.291.9019
> Home: 801.467.2925
> 
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From kee at wehi.EDU.AU  Fri Dec  3 02:07:24 2004
From: kee at wehi.EDU.AU (Thuan-Jin Kee)
Date: Fri, 3 Dec 2004 12:07:24 +1100 (EST)
Subject: [R] Getting R to emit an image file as a pipe or Base64 stream: Mac
 OSX 10.3 - R 2.0.1
Message-ID: <1934.192.168.20.199.1102036044.squirrel@192.168.20.199>

Hi All,

Anybody know how to make R emit base64 encoded text in some way that perl
can grab it, instead of planting a file on your harddrive when calling
JPEG or PNG?
I've managed to get these scripts to work and put a file on the harddisk

#!/usr/bin/perl -Wall
# by jin kee. a simple script to demonstrate
# the needed steps to get R to emit a jpeg.

use strict;

my($callR, $callRold);

# need to start X if is isn't already started.
`open /Applications/Utilities/X11.app`;


#need to get let the R program know where to look
#for the display immediately before calling
#the R executible.
$callR =<<MARKER;
DISPLAY=:0.0; export DISPLAY;
/usr/bin/R --vanilla <plotscript.R;
MARKER

system($callR);

# end script

#!/usr/bin/R
peg("~/Desktop/test.jpg");
plot(rnorm(100));
dev.off();
q(save = "no");


My sysadmin says that the apache user can't write to the disk due to
security policy, so he wants to know if I can emit the jpeg as a base64
stream and embedd it into the dynamically generated tag using a DATA tag
to inline the image.

http://www.elf.org/essay/inline-image.html

http://www.faqs.org/rfcs/rfc2397.html

i've tried searching the R-project.org site and help.search() and no luck.

Yours
Jin



From ok at cs.otago.ac.nz  Fri Dec  3 02:18:47 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 3 Dec 2004 14:18:47 +1300 (NZDT)
Subject: [R] How about a mascot for R?
Message-ID: <200412030118.iB31IlAK376299@atlas.otago.ac.nz>

Given R's origin in Auckland, NZ, perhaps something a little less
North American than a raccoon might be appropriate.
If a mascot didn't have to be animate, a pohutakawa flower
(metrosideros excelsa) would be an excellent choice:

 - The Pohutakawa is called 'The New Zealand Christmas Tree',
   the flowers are red, the leaves are green, it flowers around Christmas,
   and R has been a real "Christmas present" to the world

 - The Pohutakawa tree is not a particularly beautiful tree, but it's
   pretty tough, thriving on coastal cliffs.  They can grow fairly big,
   can be bigger than an oak tree.

 - The flower is a bottlebrush; think of it as a "vectorised" flower and
   its aptness for R is obvious.

 - The Metrosideros genus is wider spread (it's also in the Kermadec
   Islands, Australia, and Hawaii) and the Pohutakawa proper is now
   found in many parts of Australia.

 - An image of a Pohutakawa flower would just be so gorgeous.



From ok at cs.otago.ac.nz  Fri Dec  3 02:53:04 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 3 Dec 2004 14:53:04 +1300 (NZDT)
Subject: [R] Combined variable names
Message-ID: <200412030153.iB31r4K7390753@atlas.otago.ac.nz>

I wrote about the perennial "assign to V1 ... Vn" problem:
	> >What I want to know is *WHY* people are doing this?

I failed to make myself clear.

What I meant was "what happens NEXT?  Once someone has got past the
stage of generating V1 ... V100 (or whatever the magic number is),
what do they intend to DO with them?

I think this is the important question, and I think that this is what
the FAQ needs to give help with, and once I understand what people are
trying to accomplish *overall* I will be happy to offer some text for
the FAQ.

I've used stats packages myself that let you abbreviate a range of
variables.  The thing is that this was supported by *analysis* methods
(and output methods), not just *input* methods.  Now it seems to me
that pretty much everything I would want to do with a bunch of
separate variables like this in such a package would mean in R that I
desperately wanted these things to be columns in a data frame.

Perhaps the next time someone asks this question we can ask them what
they intend to do with the variables once they have them.  On past
history, we shan't have to wait very long.



From d.scott at auckland.ac.nz  Fri Dec  3 03:04:52 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Fri, 3 Dec 2004 15:04:52 +1300 (NZDT)
Subject: [R] How about a mascot for R?
In-Reply-To: <20041202205022.GJ18464@hortresearch.co.nz>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>
	<20041202205022.GJ18464@hortresearch.co.nz>
Message-ID: <Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>

On Fri, 3 Dec 2004, Patrick Connolly wrote:

> On Thu, 02-Dec-2004 at 02:08PM -0600, Erin Hodgess wrote:
>
> |> How about an R-madillo?
>
> I'm for R-gnu which already has a song (as pointed out by Murray
> Jorgensen some time back).  I'm sure an arrangement could be worked
> out with the Emacs people.
>
>
> The Flanders and Swann song, the exact title of which I don't recall,
> was a response to someone voicing the opinion that the said animal was
> an Helk.  Rather witty in my view and has a spirit in keeping with
> this list.
>

Knowledge of and a liking for Flanders and Swan does rather date one, but 
I must confess my partiality for their humour. For the words of the Gnu 
Song:

http://www.members.optusnet.com.au/pennywyatt/Interests/FlandersSwann/DropOfaHat/At%20the%20Drop%20of%20a%20Hat04.html

As to an animal mascot, I think a New Zealand mascot is a must, and 
suggestions of Australian ones would not be warmly received by New 
Zealanders. (To clarify, despite the address, I am Australian.)

My suggestion is the Kea: inquisitive and intelligent. See:

http://www.doc.govt.nz/Conservation/001~Plants-and-Animals/001~Native-Animals/Kea.asp

David Scott
_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From andrewr at uidaho.edu  Fri Dec  3 03:18:18 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Fri, 3 Dec 2004 13:18:18 +1100
Subject: [R] How about a mascot for R?
Message-ID: <20041203021818.GW827@uidaho.edu>

David, you raise an interesting point.  However, my wife reminds me
that another reason to consider the use of the kangaroo is the popular
story that the origin of the word, far from being the name of the
animal, is the local dialect for "I don't know".

The OED notes this origin in passing, along with a disclaimer that
there is no evidence to support it.

Andrew

On Fri, Dec 03, 2004 at 03:04:52PM +1300, David Scott wrote:
> As to an animal mascot, I think a New Zealand mascot is a must, and 
> suggestions of Australian ones would not be warmly received by New 
> Zealanders. (To clarify, despite the address, I am Australian.)

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From gblevins at mn.rr.com  Fri Dec  3 04:19:06 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Thu, 2 Dec 2004 21:19:06 -0600
Subject: [R] Difficulty implementing "scales" in a lattice plot
Message-ID: <075201c4d8e6$db7879c0$aaca5e18@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041202/7ed798b7/attachment.pl

From chancs at gis.a-star.edu.sg  Fri Dec  3 04:38:59 2004
From: chancs at gis.a-star.edu.sg (CHAN Chee Seng)
Date: Fri, 3 Dec 2004 11:38:59 +0800
Subject: [R] core dump during make check when building 64-bit R on
	Solaris8/9
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E4@BIONIC.biopolis.one-north.com>

Hi All,

Thanks for all the replies.  As mentioned in first mail I am using Sun
Studio 8, which is giving me the problem.  I am glad that there are
people who got this to work.  However SS8 is all I have right now and
since there is a problem I would like to get this fixed somehow.

And Peter,

Yes, that's looks like the problem that I am facing (SEGV and the stack
trace).
My cc -V looks like this:

cc: Sun C 5.5 2003/03/12

I am using /opt/SUNWspro/lib/v9/libsunperf.so.4.  And I am not aware of
the difference between v9, v9a and v9b.

Can you write/send me the standalone program to show the problem so I
can send to some Sun people that I know?

BTW, how did you run R under dbx?  (Sorry but I am not familiar with
using dbx.)  I get this error:

Fatal error: R home directory is not defined

Thanks.

Regards,
Chee Seng

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Wednesday, December 01, 2004 7:08 PM
To: Peter Dalgaard
Cc: CHAN Chee Seng; r-help at stat.math.ethz.ch
Subject: Re: [R] core dump during make check when building 64-bit R on
Solaris8/9

On Wed, 1 Dec 2004, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> We know of at least two builds which successfully used libsunperf,
one
>> (mine) with Studio ONE 7 and one with 9.  So this may well be a
>> problem with your particular compiler suite.
>
> Or maybe libsunperf version issues. The one that caused me trouble was

libsunperf *is* part of the compiler suite, as your path below shows.

> this one
>
> /opt/FD8/SUNWspro/lib/v9b/libsunperf.so.4
>
> (on the DTU machines)

I am using

/opt/SunONE7/SUNWspro/lib/v9/libsunperf.so.4

The precise architecture selected comes into play, too (v9 vs v9b).

My point was that we do know of working examples.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Fri Dec  3 05:03:05 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 2 Dec 2004 22:03:05 -0600
Subject: [R] Difficulty implementing "scales" in a lattice plot
In-Reply-To: <075201c4d8e6$db7879c0$aaca5e18@glblpyirxqz5lp>
References: <075201c4d8e6$db7879c0$aaca5e18@glblpyirxqz5lp>
Message-ID: <200412022203.05544.deepayan@stat.wisc.edu>

On Thursday 02 December 2004 21:19, Greg Blevins wrote:
> Hello all,
>
> I am rather new to lattice and have a simple question regarding
> formatting text labels on the axes. I have looked through my own
> archive on lattice notes, searched and examined 30 or so hits on Dr.
> Baron's search site, looked through my MASS book, my Data Analysis
> and Graphics Using R book, R news articles, and I have in my hand the
> lattice package instructions and have read and re-read the "scales"
> section. Here is the problem I am having.
>
> Scanf is a factor with 20 levels and the mean values for these levels
> are being plotted with CIs.
>
> I am trying to reduce the text size for the labels for this variable.
> The first function below does not attempt to set the text size for
> the y axes, and all works well.
>
>   xx2 <- with(xx,summarize(q27a, llist(Scanf,bumo), smean.cl.boot,
> conf.int=.90)) Dotplot(Scanf ~ Cbind(q27a,Lower, Upper)| bumo,
> xlim=c(5,10),cex=.6, par.strip.text=list(cex=.8), xlab="",main="",
> data=xx2)
>
> Now, when I add to the above function, scales=(cex=.5), as I show in
> the function below, the graph reduces the size of the y labels, but
> now, rather than the text labels printing, the numbers 5, 10, 15, 20
> appear on y--in other words the numeric values are used. xx2 <-
> with(xx,summarize(q27a, llist(Scanf,bumo), smean.cl.boot,
> conf.int=.90)) Dotplot(Scanf ~ Cbind(q27a,Lower, Upper)| bumo,
> xlim=c(5,10), cex=.6, scales = (cex=.5)),
> par.strip.text=list(cex=.8), xlab="",main="", data=xx2) So, seeking
> advice on how to reduce text size while retaining the text labels for
> the Scanf variable.

This doesn't really make sense, since scales has to be either a list or 
a character string. Your example is not reproducible either, so I'm 
guessing, but perhaps you want 

  scales = list(y = list(cex = 0.5))

?


Deepayan



From ripley at stats.ox.ac.uk  Fri Dec  3 06:32:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 3 Dec 2004 05:32:03 +0000 (GMT)
Subject: [R] core dump during make check when building 64-bit R on
	Solaris8/9
In-Reply-To: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E4@BIONIC.biopolis.one-north.com>
References: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E4@BIONIC.biopolis.one-north.com>
Message-ID: <Pine.LNX.4.61.0412030529340.25981@gannet.stats>

On Fri, 3 Dec 2004, CHAN Chee Seng wrote:

> Hi All,
>
> Thanks for all the replies.  As mentioned in first mail I am using Sun
> Studio 8, which is giving me the problem.  I am glad that there are
> people who got this to work.  However SS8 is all I have right now and
> since there is a problem I would like to get this fixed somehow.

You already have a fix: do not use libsunperf if it is buggy.

> And Peter,
>
> Yes, that's looks like the problem that I am facing (SEGV and the stack
> trace).
> My cc -V looks like this:
>
> cc: Sun C 5.5 2003/03/12
>
> I am using /opt/SUNWspro/lib/v9/libsunperf.so.4.  And I am not aware of
> the difference between v9, v9a and v9b.

They are variations on the Sparc architecture.

> Can you write/send me the standalone program to show the problem so I
> can send to some Sun people that I know?
>
> BTW, how did you run R under dbx?  (Sorry but I am not familiar with
> using dbx.)  I get this error:
>
> Fatal error: R home directory is not defined
>
> Thanks.
>
> Regards,
> Chee Seng
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Wednesday, December 01, 2004 7:08 PM
> To: Peter Dalgaard
> Cc: CHAN Chee Seng; r-help at stat.math.ethz.ch
> Subject: Re: [R] core dump during make check when building 64-bit R on
> Solaris8/9
>
> On Wed, 1 Dec 2004, Peter Dalgaard wrote:
>
>> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>>
>>> We know of at least two builds which successfully used libsunperf,
> one
>>> (mine) with Studio ONE 7 and one with 9.  So this may well be a
>>> problem with your particular compiler suite.
>>
>> Or maybe libsunperf version issues. The one that caused me trouble was
>
> libsunperf *is* part of the compiler suite, as your path below shows.
>
>> this one
>>
>> /opt/FD8/SUNWspro/lib/v9b/libsunperf.so.4
>>
>> (on the DTU machines)
>
> I am using
>
> /opt/SunONE7/SUNWspro/lib/v9/libsunperf.so.4
>
> The precise architecture selected comes into play, too (v9 vs v9b).
>
> My point was that we do know of working examples.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From adrian at maths.uwa.edu.au  Fri Dec  3 06:45:19 2004
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Fri, 3 Dec 2004 13:45:19 +0800
Subject: [R] factor matrix
Message-ID: <16815.64879.855864.173146@maths.uwa.edu.au>

Sorry if this is a FAQ.

Is there a good reason why a factor has to be
a one-dimensional vector and cannot be a matrix? 

I want to construct matrices of categorical values. 

Vain attempts like
   matrix(factor(c(T,F,F,T), 2,2) 
yield a matrix of character strings representing the factor levels, 
not the levels themselves, while 
   factor(matrix(c(T,F,F,T), 2,2))  
converts the matrix to a logical vector of length 4
then converts the vector to a factor.

Tia
---
Adrian Baddeley



From Tom.Mulholland at dpi.wa.gov.au  Fri Dec  3 07:03:50 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 3 Dec 2004 14:03:50 +0800
Subject: [R] factor matrix
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C91D@afhex01.dpi.wa.gov.au>

I'm not sure, but is this what you want
matrix(as.numeric(factor(c(T,F,F,T))), 2,2)

Tom Mulholland


> -----Original Message-----
> From: Adrian Baddeley [mailto:adrian at maths.uwa.edu.au]
> Sent: Friday, 3 December 2004 1:45 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] factor matrix
> 
> 
> Sorry if this is a FAQ.
> 
> Is there a good reason why a factor has to be
> a one-dimensional vector and cannot be a matrix? 
> 
> I want to construct matrices of categorical values. 
> 
> Vain attempts like
>    matrix(factor(c(T,F,F,T), 2,2) 
> yield a matrix of character strings representing the factor levels, 
> not the levels themselves, while 
>    factor(matrix(c(T,F,F,T), 2,2))  
> converts the matrix to a logical vector of length 4
> then converts the vector to a factor.
> 
> Tia
> ---
> Adrian Baddeley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From yzhang4 at pobox.une.edu.au  Fri Dec  3 07:15:23 2004
From: yzhang4 at pobox.une.edu.au (Yuandan Zhang)
Date: Fri, 3 Dec 2004 17:15:23 +1100
Subject: [R] Getting R to emit an image file as a pipe or Base64 stream:
	Mac OSX 10.3 - R 2.0.1
In-Reply-To: <1934.192.168.20.199.1102036044.squirrel@192.168.20.199>
References: <1934.192.168.20.199.1102036044.squirrel@192.168.20.199>
Message-ID: <20041203171523.4422485a.yzhang4@pobox.une.edu.au>

If you want to call R from perl, why don't you do a simple system call like:

$callR="/usr/loca/bin/R CMD BATCH plotscript.R";
system ($callR);

It is not necessary to start X display if anything can be done in background




On Fri, 3 Dec 2004 12:07:24 +1100 (EST)
"Thuan-Jin Kee" <kee at wehi.EDU.AU> wrote:

> Hi All,
> 
> Anybody know how to make R emit base64 encoded text in some way that perl
> can grab it, instead of planting a file on your harddrive when calling
> JPEG or PNG?
> I've managed to get these scripts to work and put a file on the harddisk
> 
> #!/usr/bin/perl -Wall
> # by jin kee. a simple script to demonstrate
> # the needed steps to get R to emit a jpeg.
> 
> use strict;
> 
> my($callR, $callRold);
> 
> # need to start X if is isn't already started.
> `open /Applications/Utilities/X11.app`;
> 
> 
> #need to get let the R program know where to look
> #for the display immediately before calling
> #the R executible.
> $callR =<<MARKER;
> DISPLAY=:0.0; export DISPLAY;
> /usr/bin/R --vanilla <plotscript.R;
> MARKER
> 
> system($callR);
> 
> # end script
> 
> #!/usr/bin/R
> peg("~/Desktop/test.jpg");
> plot(rnorm(100));
> dev.off();
> q(save = "no");
> 
> 
> My sysadmin says that the apache user can't write to the disk due to
> security policy, so he wants to know if I can emit the jpeg as a base64
> stream and embedd it into the dynamically generated tag using a DATA tag
> to inline the image.
> 
> http://www.elf.org/essay/inline-image.html
> 
> http://www.faqs.org/rfcs/rfc2397.html
> 
> i've tried searching the R-project.org site and help.search() and no luck.
> 
> Yours
> Jin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 

--
Yuandan Zhang, PhD

Animal Genetics and Breeding Unit
The University of New England
Armidale, NSW, Australia, 2351

E-mail:   yzhang4 at metz.une.edu.au
Phone:    (61) 02 6773 3786
Fax:      (61) 02 6773 3266
http://agbu.une.edu.au
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  AGBU is a joint venture of NSW Primary Industries 
  and The University of New England to undertake 
  genetic R&D for Australia's Livestock Industries           
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From jarioksa at sun3.oulu.fi  Fri Dec  3 07:37:02 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 03 Dec 2004 08:37:02 +0200
Subject: [R] How about a mascot for R?
In-Reply-To: <x21xe8ctcf.fsf@biostat.ku.dk>
References: <web-2697598@be1.bc.edu> <41AF7823.1080101@optushome.com.au>
	<x21xe8ctcf.fsf@biostat.ku.dk>
Message-ID: <1102055822.26053.22.camel@biol102145.oulu.fi>

On Thu, 2004-12-02 at 22:56, Peter Dalgaard wrote:
> Tim Churches <tchur at optushome.com.au> writes:
> 
> > Damian Betebenner wrote:
> > > R users,
> > > How come R doesn't have a mascot?
> > 
> > Perhaps someone with artistic flair could create a mascot based on
> > this image? It would help to give newcomers to R-help the right idea:
> > 
> > http://www.accesscom.com/~alvaro/alien/thepics/ripley1__.jpg
> 
> Or maybe this one:
> 
> http://www.accesscom.com/~alvaro/alien/thepics/bg10s.jpg
> 
> or (apologies to Pat Burns):
> 
> http://www.accesscom.com/~alvaro/alien/thepics/alien102_.jpg

It seems that tastes for movies vary. I've never liked movies about
ecologically non-sustainable and energetically impossible life forms.
The current sub-theme brings to my mind something completely different:
http://www.hundland.com/posters/t/TheTalentedMr.Ripley.jpg.

cheers, jari o.
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From spencer.graves at pdf.com  Fri Dec  3 07:43:11 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 02 Dec 2004 22:43:11 -0800
Subject: [R] SJava convert problem
In-Reply-To: <b1d3150404120211126cc880b8@mail.gmail.com>
References: <b1d3150404120211126cc880b8@mail.gmail.com>
Message-ID: <41B00AFF.3060302@pdf.com>

      I have not previous knowledge of SJava, but I just went to 
www.r-project.org -> CRAN -> (selected a local mirror) -> "Software:  
Packages", and did not find SJava on the list.  Conclusion:  It's not a 
current R package.  I then went back to www.r-project -> search -> "R 
site search" -> SJava, and got 401 matches.  If you've already looked 
through them, then I apologize for wasting your time. 

      hope this helps. 
      spencer graves

Slim Shady wrote:

>Consider this Java class:
>class myClass {
>    final public static String[] testString () {
>        return new String[]{"my", "name", "is"} ;
>    }
>    
>    final public static int[] testInt () {
>        return new int[]{5, 10, 15};
>    }    
>    
>    final public static char[] testChar () {
>        return new char[]{'x', 'y', 'z'};
>    }
>
>}
>
>When invoked in R:
>  
>
>>s <- .Java(className, "testString")
>>class(s)
>>    
>>
>[1] "list"
>  
>
>>length(s)
>>    
>>
>[1] 3
>  
>
>>s
>>    
>>
>[[1]]
>[1] "my"
>
>[[2]]
>[1] "name"
>
>[[3]]
>[1] "is"
>
>
>  
>
>>c <- .Java(className, "testChar")
>>class(c)
>>    
>>
>[1] "character"
>  
>
>>length(c)
>>    
>>
>[1] 1
>  
>
>>c
>>    
>>
>[1] "X"
>
>  
>
>>i <- .Java(className, "testInt")
>>class(i)
>>    
>>
>[1] "integer"
>  
>
>>length(i)
>>    
>>
>[1] 3
>  
>
>>i
>>    
>>
>[1]  5 10 15
>
>Note that the conversion of testInt is just what I expected (and what
>I get in S-PLUS). The conversion of testChar is flat wrong because
>data is lost. The conversion of testString is problematic since I have
>to convert the list back using as.character (in S-PLUS this function
>returns character instaid of list and saves me the convertion from
>list to character)
>Is the convertion of char[] a bug? is there is a way to make the
>conversion of String[] returns a character class instaid of a list
>just like in the S-PLUS Java Connect?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From adrian at maths.uwa.edu.au  Fri Dec  3 07:43:54 2004
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Fri, 3 Dec 2004 14:43:54 +0800
Subject: [R] factor matrix
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3C91D@afhex01.dpi.wa.gov.au>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C91D@afhex01.dpi.wa.gov.au>
Message-ID: <16816.2858.849554.545444@maths.uwa.edu.au>


> matrix(as.numeric(factor(c(T,F,F,T))), 2,2)

No, this produces a matrix with numeric values,
not categorical values.



From Tom.Mulholland at dpi.wa.gov.au  Fri Dec  3 08:13:51 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 3 Dec 2004 15:13:51 +0800
Subject: [R] factor matrix
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C91E@afhex01.dpi.wa.gov.au>

This is my playing about

The last one is a matrix of factors of dimension 2 by 2. It;s just that it does not look that way.

> tt <- matrix(as.numeric(factor(c(T,F,F,T))), 2,2)
> str(tt)
 num [1:2, 1:2] 2 1 1 2
> tt <- matrix((factor(c(T,F,F,T))), 2,2)
> str(tt)
 chr [1:2, 1:2] "TRUE" "FALSE" "FALSE" "TRUE"
> tt <- c(T,F,F,T)
> str(tt)
 logi [1:4]  TRUE FALSE FALSE  TRUE
> dim(tt) <- c(2,2)
> str(tt)
 logi [1:2, 1:2]  TRUE FALSE FALSE  TRUE
> tt <- factor(c(T,F,F,T))
> dim(tt) <- c(2,2)
> str(tt)
 factor [1:2, 1:2] TRUE FALSE FALSE TRUE
 - attr(*, "levels")= chr [1:2] "FALSE" "TRUE"
 - attr(*, "class")= chr "factor"
> tt
[1] TRUE  FALSE FALSE TRUE 
Levels: FALSE TRUE
> 

> -----Original Message-----
> From: Adrian Baddeley [mailto:adrian at maths.uwa.edu.au]
> Sent: Friday, 3 December 2004 2:44 PM
> To: Mulholland, Tom
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] factor matrix
> 
> 
> 
> > matrix(as.numeric(factor(c(T,F,F,T))), 2,2)
> 
> No, this produces a matrix with numeric values,
> not categorical values.
> 
> 
>



From petr.pikal at precheza.cz  Fri Dec  3 08:33:09 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 03 Dec 2004 08:33:09 +0100
Subject: [R] factor matrix
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3C91E@afhex01.dpi.wa.gov.au>
Message-ID: <41B024C5.18740.256B58@localhost>



From cjmaccault at yahoo.com  Fri Dec  3 08:42:06 2004
From: cjmaccault at yahoo.com (ebru apaydfffffdn)
Date: Thu, 2 Dec 2004 23:42:06 -0800 (PST)
Subject: [R] applying data generating function
Message-ID: <20041203074206.61902.qmail@web50008.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041202/c33cefd9/attachment.pl

From bob.ohara at helsinki.fi  Fri Dec  3 08:51:52 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Fri, 03 Dec 2004 09:51:52 +0200
Subject: [R] How about a mascot for R?
In-Reply-To: <Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>	<20041202205022.GJ18464@hortresearch.co.nz>
	<Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>
Message-ID: <41B01B18.8060402@helsinki.fi>

David Scott wrote:

> On Fri, 3 Dec 2004, Patrick Connolly wrote:
>
>> On Thu, 02-Dec-2004 at 02:08PM -0600, Erin Hodgess wrote:
>>
>> |> How about an R-madillo?
>>
>> I'm for R-gnu which already has a song (as pointed out by Murray
>> Jorgensen some time back).  I'm sure an arrangement could be worked
>> out with the Emacs people.
>>
>>
>> The Flanders and Swann song, the exact title of which I don't recall,
>> was a response to someone voicing the opinion that the said animal was
>> an Helk.  Rather witty in my view and has a spirit in keeping with
>> this list.
>>
>
> Knowledge of and a liking for Flanders and Swan does rather date one, 
> but I must confess my partiality for their humour. For the words of 
> the Gnu Song:
>
> http://www.members.optusnet.com.au/pennywyatt/Interests/FlandersSwann/DropOfaHat/At%20the%20Drop%20of%20a%20Hat04.html 
>
>
Indeed, they also had a song about an R-madillo:
<http://www.nyanko.pwp.blueyonder.co.uk/fas/bestiary_armadillo.html>
And no, this doesn't date you - they had split up before I was even 
born.  I think it just shows that you have the good taste to be partial 
to humour of timeless quality.

Whilst I'm wasting bandwidth, I found the original platypus and kangaroo 
(Kangaroo?  Another F & S composition!) reference:

<http://members.aol.com/jeff570/k.html> (see the bottom)

The paper should be available on JSTOR, for those of you with access to it.

Bob

-- 
Bob O'Hara
Department of Mathematics and Statistics
P.O. Box 68 (Gustaf H??llstr??min katu 2b)
FIN-00014 University of Helsinki
Finland

Telephone: +358-9-191 51479
Mobile: +358 50 599 0540
Fax:  +358-9-191 51400
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: www.jnr-eeb.org



From r.g.brown at cefas.co.uk  Fri Dec  3 09:07:51 2004
From: r.g.brown at cefas.co.uk (Robert Brown FM CEFAS)
Date: Fri, 3 Dec 2004 08:07:51 -0000
Subject: [R] Protocol for answering basic questions
Message-ID: <3589BC4D64C84341AE0C258244F977A2B60B78@expressa.corp.cefas.co.uk>

How will I delete the reply without reading first unless I delete all replies? I've made it quite clear that some replies are useful, but some are unhelpful. I and other would like to see an improvement in r help; to just say take it or leave it, as many infer, is conceptually naive.

I joined the community because I want technical assistance and I don't question the technical skills of many of the respondants, but the wider appreciation of needs of users i.e conceptual naivety.  It's clear from this string that many beginners are leaving the forum and this should be a cause for concern if we are seriously concerned with propogating knowledge.  Unfortuantely this string does seem to indicate this is not a major concern and so be it.  



-----Original Message-----
From: Marwan Khawaja [mailto:marwan.khawaja at aub.edu.lb]
Sent: 01 December 2004 20:41
To: Robert Brown FM CEFAS; r-help at stat.math.ethz.ch
Subject: RE: [R] Protocol for answering basic questions


Well, if you do not like the way some people answer queries, why not just delete
the reply without reading the response.
Since we're not paying anyone for answering questions, we should be grateful to
those who put their time in replying to our basic questions.
And why join this community? -- if you think most are 'conceptually na??ve'!

Marwan
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert 
> Brown FM CEFAS
> Sent: Wednesday, December 01, 2004 6:46 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Protocol for answering basic questions
> 
> I have been following the discussions on 'Reasons not to 
> answer very basic questions in a straightforward way' with 
> interest as someone who is also new to R and has had similar 
> experiences.  As such it with sadness that I note that most 
> seem to agree with the present approach to the responses to 
> basic questions.  I must thank those respondants to my own 
> questions who have been helpful, but there are some whose 
> replies are in my opinion not only unhelpful but actually 
> rude.  Indeed I've now started using Splus instead of R so as 
> to have access to a 'proper' support service.  Indeed, the 
> main thing I've learned from R is a new respect for the 
> values of commercial software and a scepticism regarding free 
> software. In the end my experience of r help is that you get 
> what you pay for. Many of the so called socratic responses 
> (in this list and the wider academic community) can be seen 
> as simply way to avoid additional work of a complete reply. 
> 
> Experienced R users don't seem to understand how difficult 
> the program can be to new users. Responding that the 
> questioner should read the 'Introduction to R' or a similar 
> document is like  answering a question for directions to 
> one's house with 'Buy a map'.  Most likely both such 
> questioners have already tried that and are asking because 
> that approach failed.  R is a language and like all languages 
> it is simple to those that understand it and complex to those 
> who do not. Every schoolboy in Spain speaks Spanish, but I 
> know from experience that for most English people it is very 
> difficult to learn Spanish and take years of study.  If I'm 
> asked a question from a novice of a language (be it Spanish 
> or R) do I reply 'consult the dictionary'. I would hope not!  
> I can tell repondants that whilst many of my basic questions 
> may seem simple it is not for lack of studying the very 
> sources they refer to.  If only learning was so simple.  I 
> suspect that the same is true of most question!
>  ers.
> 
> I speak as someone with a PhD and many years as a researcher 
> in my speciality as well as someone close to completing a 
> masters in statistics with distinction. As such I am not a 
> total novice and would suggest that if I'm having problems so 
> are many; and it is not a result of lack of study as so many 
> responses seem to suggest.  Indeed it is revealing that 
> several responses suggest that they want to discourage 
> questions so they don't overwhelm r-help.  Understandable but 
> not a recipe to encourage the use of R by other than experts. 
> The R community needs to decide of they really only want 
> expert statisticians users and make this clear if it is the 
> case.  Alternatively if they are to encourage novices the 
> present approach is not the way to do it.
> 
> I can appreciate that many of the respondants are busy, but 
> if that is the case it would be better if they didn't reply 
> at all. I was taught many years ago that if you can't say 
> anything nice/useful then to say nothing at all.  Something 
> similar could well be applied to this list.  I must say that 
> some respondants are very helpful; and I thank them.  Leave 
> these simple questions to such people.  Indeed it seems 
> surprising that some exteremely experienced R users choose to 
> reply to these basic messages at all; and it seem it is 
> mostly these people who are rude.  I would have thought it 
> might be better for them to concentrate on complex problems 
> more suited to their skills and interests and leave the 
> simple questions to more sympathetic souls.
> 
> Perhaps there is a case for two r help lists catering to 
> basic and advanced questions? Certainly if the R community is 
> serious about appealling to users outside advanced 
> statisticians there is a need for a change of approach in r 
> help and elsewhere.  Russ Ackoff identified much of the 
> failure of management science as due to those who were 
> 'mathematically sophisticated but conceptually naive' and 
> much the same could be said for many in the R community.
> 
> Finally, let me once again thank those who have responded 
> helpful to my queries in the past and ask them to continue in 
> that vein; their assistance and effort is greatly appreciated.
> 
> 
> 
>   
> 
> 
> **************************************************************
> *********************
> This email and any attachments are intended for the named 
> recipient(s) only.  Its unauthorised use, distribution, 
> disclosure, storage or copying is not permitted.  If you have 
> received it in error, please destroy all copies and notify 
> the sender.  In messages of a non-business nature, the views 
> and opinions expressed are the author's own and do not 
> necessarily reflect those of the organisation from which it 
> is sent.  All emails may be subject to monitoring.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Fri Dec  3 09:31:35 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Dec 2004 09:31:35 +0100
Subject: [R] core dump during make check when building 64-bit R on
	Solaris8/9
In-Reply-To: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E4@BIONIC.biopolis.one-north.com>
References: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E4@BIONIC.biopolis.one-north.com>
Message-ID: <x2d5xrvl3s.fsf@biostat.ku.dk>

"CHAN Chee Seng" <chancs at gis.a-star.edu.sg> writes:

> Can you write/send me the standalone program to show the problem so I
> can send to some Sun people that I know?

Not at this point, I was hoping you had the time.
 
> BTW, how did you run R under dbx?  (Sorry but I am not familiar with
> using dbx.)  I get this error:

R -d dbx

then "run", etc...


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Fri Dec  3 09:34:56 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 3 Dec 2004 09:34:56 +0100
Subject: [R] Combined variable names
In-Reply-To: <200412030153.iB31r4K7390753@atlas.otago.ac.nz>
References: <200412030153.iB31r4K7390753@atlas.otago.ac.nz>
Message-ID: <16816.9520.195392.140785@gargle.gargle.HOWL>

>>>>> "Richard" == Richard A O'Keefe <ok at cs.otago.ac.nz>
>>>>>     on Fri, 3 Dec 2004 14:53:04 +1300 (NZDT) writes:

    Richard> I wrote about the perennial "assign to V1 ... Vn" problem:

    >> >What I want to know is *WHY* people are doing this?

    Richard> I failed to make myself clear.

    Richard> What I meant was "what happens NEXT?  Once someone
    Richard> has got past the stage of generating V1 ... V100
    Richard> (or whatever the magic number is), what do they
    Richard> intend to DO with them?

    Richard> I think this is the important question, and I think
    Richard> that this is what the FAQ needs to give help with,
    Richard> and once I understand what people are trying to
    Richard> accomplish *overall* I will be happy to offer some
    Richard> text for the FAQ.

that would be very useful, thank you in advance.

    Richard> I've used stats packages myself that let you
    Richard> abbreviate a range of variables.  The thing is that
    Richard> this was supported by *analysis* methods (and
    Richard> output methods), not just *input* methods.  Now it
    Richard> seems to me that pretty much everything I would
    Richard> want to do with a bunch of separate variables like
    Richard> this in such a package would mean in R that I
    Richard> desperately wanted these things to be columns in a
    Richard> data frame.

or--more generally--named list components. -- important, e.g.,
when the v_j's have differing lengths.

    Richard> Perhaps the next time someone asks this question we
    Richard> can ask them what they intend to do with the
    Richard> variables once they have them.  On past history, we
    Richard> shan't have to wait very long.

good idea (and it may well have been "implemented" occasionaly
in the past).

Martin Maechler, ETH Zurich



From siegfried.gonzi at stud.uni-graz.at  Fri Dec  3 10:19:45 2004
From: siegfried.gonzi at stud.uni-graz.at (Siegfried Gonzi)
Date: Fri, 03 Dec 2004 10:19:45 +0100
Subject: [R] A somewhat off the line question to a log normal distrib
References: <XFMail.041202165246.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41B02FB1.1020906@stud.uni-graz.at>

>
>
>
>
(Ted Harding) wrote:

>
>Not quite sure of your point here, Thomas. I certainly wasn't
>writing on the basis that the boss had claimed that they were
>either independent or identically disitributed, and the paragraph
>you quote was in reposnse to:
>
>  "The aformentioned daily measurements follow a log-normal
>   distribution when considered over the course of a year. 
>   Okay. He also tried to explain me that the monthly means
>  (based on the daily measurements) must follow a log-normal
>  distribution too then over the course of a year."
>
>which I interpreted as arguing that "if daily data log-normal,
>then monthly means must consequently be log-normal", i.e. that
>the mean of log-normals is log-normal; and I was simply pointing
>out that this is a false implication (which would be the case
>even if the data are neither independent nor identically distributed,
>except in the extreme case where they are all copies of the one
>log-normal variable).
>
>Granted I later used i.i.d log-normals as examples; but then
>pointed out that the mean of log-normals could remain sufficiently
>skew that a log-normal could still be a useful distribution to
>adopt.
>

Hello:

Let me cut short it. The variables in questions are "aerosol optical 
depth measurements" (go to the NASA 'AERONET' site if you want to learn 
more about it). It is likely that not everybody knows what it is meant 
by it; but one can think on "temperature measurements" for a good proxy, 
though not directly related to my variables.

My data base was not based on a single observing station; I have used 50 
stations for my evaluation. The stations were located in Europe. 
Although, the data base was rather scattered because some stations 
didn't observe every day and every month, even.

But thanks again for the useful tips (especially the link to the CLT). 
It is rather this: my paper had been rejected.  But we know: we will 
struggle as long as the paper will eventually get accepted ( I have a 
colleague and friend with a good name at NASA who daily motivates me not 
to give up).

There were other reasons too but one complaint from a reviewer actually 
was that there exists a paper that "aerosol optical depths" are rather 
skewed to the left.

My argument actually was that my averaging removed quite a lot of 
outliers. Okay, honestly speaking: at that time I didn't know about the CLT.

I recalculated the matter, based on a log normal distribution, and it 
turned out that after transforming the variables to a log-normal 
distribution the median and mean become similar and comparable to my 
"heavy averaged former means". Surely, there is one difference to my 
former averaging: the 3. quantile and the maximum value is larger due to 
the log-normal distribution.


Regards,
Siegfried Gonzi



From s_shokohi at yahoo.com  Fri Dec  3 10:54:04 2004
From: s_shokohi at yahoo.com (sepideh shokohi)
Date: Fri, 3 Dec 2004 01:54:04 -0800 (PST)
Subject: [R] help : the datail of Fortran subroutine that use in R
In-Reply-To: <000201c4d8b1$73334f50$3c207d80@NIEHS.usc.edu>
Message-ID: <20041203095404.33217.qmail@web41812.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041203/57024c12/attachment.pl

From johan at inforsense.com  Fri Dec  3 11:01:56 2004
From: johan at inforsense.com (Johan Hedlund)
Date: Fri, 3 Dec 2004 10:01:56 -0000
Subject: [R] arima and optim error
Message-ID: <E1CaAGJ-0000zJ-00@lime.webfusion.co.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041203/2f4e9592/attachment.pl

From c.sprenger at gmail.com  Fri Dec  3 11:09:19 2004
From: c.sprenger at gmail.com (Charlie Sprenger)
Date: Fri, 3 Dec 2004 02:09:19 -0800
Subject: [R] multinomial probit
Message-ID: <1abbab52041203020965005fe8@mail.gmail.com>

Hello All, 

 I'm trying to run a multinomial probit on a dataset with 28 data
points and five levels (0,1,2,3,4) in the latent choice involving
response variable.

I downloaded the latest mnp package to run the regression. It starts
the calculation and then crashes the rpogram. I wish I could give the
error message but it literally shuts down R without a warning.

I'm using the R that's been ported to OS X. Is it possible that there
are RAM restrictions and that when something computationally heavy
goes through the program is cut off?

As well, is there actually a "probit" method for polr from nnet package?

All help appreciated as masters thesis topics are due soon.

Charlie Sprenger
UCL - student



From annap at mate.polimi.it  Fri Dec  3 11:19:20 2004
From: annap at mate.polimi.it (Anna Maria Paganoni)
Date: Fri, 03 Dec 2004 11:19:20 +0100
Subject: [R] information
Message-ID: <6.1.0.6.1.20041203111659.02684788@pop2.mate.polimi.it>

I would like to know if there exist any package on the profyle analysis, or 
on repeated measures, (AUC)...
Thank for answering me,


Anna Maria Paganoni		
Dipartimento di Matematica
Politecnico di Milano
piazza Leonardo da Vinci, 32
20133 Milano, Italy
tel +39-0223994574 fax +39-0223994568
e-mail annap at mate.polimi.it



From machud at intellektik.informatik.tu-darmstadt.de  Fri Dec  3 10:47:55 2004
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Fri, 3 Dec 2004 10:47:55 +0100 (CET)
Subject: [R] organising the display in Trellis plots
In-Reply-To: <200411301116.iAUB53F8024271@hypatia.math.ethz.ch>
References: <200411301116.iAUB53F8024271@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0412031036380.15100@kika.intellektik.informatik.tu-darmstadt.de>

Hallo,

I would like to organise at my pleasure the layout of a trellis plot.

Currently I have a 3x3 matrix display and 7 plots. Is it possible to
choose which specific panels will stay empty?

I tried index.perm to arrange the order. Then there is perm.cond which I
could not understand if it can serve for my purpose since I cant really
find which kind of data it accepts.

Thank you for the help.


Marco

P.S.  I attach some code.

plot <- Dotplot(algo~Cbind(y,lower,upper) | group,data=OUT,
                  pch=3, method="bars",pch.bar=2,lwd=3,
                  cex=0.8,
                  par.strip.text=list(cex=1),
                  panel=function(x,y,...) {
                    print(attr(x,'other'))
                    panel.abline(v=attr(x,'other'),lty=2,col="grey70")
                    panel.Dotplot(x,y,...)
                  },
                  scales = list(cex=1,rot=c(0,45),
                    x=list(alternating=c(1,1,1,1),limits=c(1,nlevels(OUT$algo)),relation="same"),
                    y=list(at=c(1:nlevels(OUT$algo)),labels=levels(OUT$algo),alternating=c(1,1,1,1))
                    ),
                main=list(label=""),
                  xlab=list(cex=1,label="Average rank"),
                  ylab="",
                  aspect="fill",as.table=FALSE,
                  layout=c(3,3)
                  )
    print(plot)
    update(plot,perm.cond=c(3,2,1,4,5,6))  #this does not work
    update(plot,index.cond=c(3,2,1,4,5,6))  #this works it does not what I wish



From gwenael.jacob at wsl.ch  Fri Dec  3 11:39:57 2004
From: gwenael.jacob at wsl.ch (Gwenael Jacob)
Date: Fri, 3 Dec 2004 11:39:57 +0100
Subject: [R] vector to matrix transformation
Message-ID: <a06100500bdd5f1274069@[10.17.2.158]>

Dear,

Some analysis (linear regression) can only be 
done from a vectorized dataset whereas others 
require a matrix (Mantel tests). I use the two 
analyses and thus need to format my data in 
matrix and vector. I spent some time trying to 
solve the problem and I just gave up. Did anyone 
knows how to transform a matrix into a vector and 
back-transform a vector into a matrix?

Thanks by advance,
Gwena??l Jacob
-- 

-------------------------------------
Gwena??l JACOB
Division Biodiversity
Swiss Federal Research Institute WSL
Zuercherstrasse 111/Postfach
CH-8903 Birmensdorf

SWITZERLAND

Phone : ++41 1 7392 504
Fax    : ++41 1 7392 215



From Roger.Bivand at nhh.no  Fri Dec  3 11:45:01 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 3 Dec 2004 11:45:01 +0100 (CET)
Subject: [R] help : the datail of Fortran subroutine that use in R
In-Reply-To: <20041203095404.33217.qmail@web41812.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0412031139390.23717-100000@reclus.nhh.no>

On Fri, 3 Dec 2004, sepideh shokohi wrote:

> 
> How can I see the datail of Fortran subroutine that use in R.
> for example I want to see detail of "emn" routine that wrote in Fortran.
> 
> 
> .Fortran("emn", s$d, old, start, tobs, s$p, 
>             s$psi, s$n, s$x, s$npatt, s$r, s$mdpst, s$nmdp, tmp, 
>            tmp, numeric(s$p), mle, tau, m, mu0, lambdainv)[[3]]

Download the source package norm_1.0-9.tar.gz from your nearest CRAN
mirror and unpack it in a suitable temporary directory. Look in the
norm/src directory for the Fortran file containing the code for the emn
subroutine. If you are using Windows, you have probably installed a
pre-compiled version of the package; to read the Fortran/C/C++ code, you
need the source package.

> 
> it was in em.norm in norm package.
> 
> With regards 
> Sepideh Shokohi
> 
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From vito_ricci at yahoo.com  Fri Dec  3 11:50:15 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 3 Dec 2004 11:50:15 +0100 (CET)
Subject: [R] R: vector to matrix transformation
Message-ID: <20041203105015.85398.qmail@web41202.mail.yahoo.com>

Hi,

did you see:

as.data.frame()
as.matrix()
as.vector()
matrix()

> x
  a b c
1 1 2 3
2 1 2 3
3 2 3 4
4 3 4 5
> is.data.frame(x)
[1] TRUE
> as.matrix(x)
  a b c
1 1 2 3
2 1 2 3
3 2 3 4
4 3 4 5
> y<-as.matrix(x)
> is.matrix(y)
[1] TRUE
> as.vector(y)
 [1] 1 1 2 3 2 2 3 4 3 3 4 5
> z<-as.vector(y)

> m<-matrix(z,ncol=3)
> m
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    1    2    3
[3,]    2    3    4
[4,]    3    4    5

I hope I give you a little help.

Best
Vito

you wrote:

Dear,

Some analysis (linear regression) can only be 
done from a vectorized dataset whereas others 
require a matrix (Mantel tests). I use the two 
analyses and thus need to format my data in 
matrix and vector. I spent some time trying to 
solve the problem and I just gave up. Did anyone 
knows how to transform a matrix into a vector and 
back-transform a vector into a matrix?

Thanks by advance,
Gwena??l Jacob

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From e.pebesma at geog.uu.nl  Fri Dec  3 11:49:48 2004
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Fri, 03 Dec 2004 11:49:48 +0100
Subject: [R] Re: Computation of space-time empirical crosscovariances
Message-ID: <41B044CC.50105@geog.uu.nl>

Giovanna,

space-time cross covariance has never seemed to me something
that can be done "simple", but package gstat at least offers you:
1. to include time as a third dimension, and model 3D anisotropy,
2. to calculate cross variograms between different moments in
time, and proceed with cokriging.
--
Edzer



From ligges at statistik.uni-dortmund.de  Fri Dec  3 11:58:10 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 03 Dec 2004 11:58:10 +0100
Subject: [R] help : the datail of Fortran subroutine that use in R
In-Reply-To: <20041203095404.33217.qmail@web41812.mail.yahoo.com>
References: <20041203095404.33217.qmail@web41812.mail.yahoo.com>
Message-ID: <41B046C2.1090100@statistik.uni-dortmund.de>

sepideh shokohi wrote:

> How can I see the datail of Fortran subroutine that use in R.
> for example I want to see detail of "emn" routine that wrote in Fortran.
> 
> 
> .Fortran("emn", s$d, old, start, tobs, s$p, 
>             s$psi, s$n, s$x, s$npatt, s$r, s$mdpst, s$nmdp, tmp, 
>            tmp, numeric(s$p), mle, tau, m, mu0, lambdainv)[[3]]
> 
> it was in em.norm in norm package.

Download the source package "norm" and look into its sub-directory "src".

Uwe Ligges


> With regards 
> Sepideh Shokohi
> 
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From giovanna.jonalasinio at uniroma1.it  Fri Dec  3 12:13:00 2004
From: giovanna.jonalasinio at uniroma1.it (giovanna jona lasinio)
Date: Fri, 3 Dec 2004 12:13:00 +0100
Subject: R: [R] Re: Computation of space-time empirical crosscovariances
In-Reply-To: <41B044CC.50105@geog.uu.nl>
Message-ID: <001d01c4d929$0d1da670$65036497@sta.uniroma1.it>

Thanks, I'm looking at gstat and trying to understand in which way it is
appropriate to add time as the third dimension... Indeed the "simple"
attribute is not for space-time problems :-)

-----Messaggio originale-----
Da: Edzer J. Pebesma [mailto:e.pebesma at geog.uu.nl] 
Inviato: venerd??, 3. dicembre 2004 11:50
A: r-help at stat.math.ethz.ch; giovanna.jonalasinio at uniroma1.it
Oggetto: [R] Re: Computation of space-time empirical crosscovariances


Giovanna,

space-time cross covariance has never seemed to me something that can be
done "simple", but package gstat at least offers you: 1. to include time
as a third dimension, and model 3D anisotropy, 2. to calculate cross
variograms between different moments in time, and proceed with
cokriging.
--
Edzer



From andy_liaw at merck.com  Fri Dec  3 12:15:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 3 Dec 2004 06:15:35 -0500
Subject: [R] Getting R to emit an image file as a pipe or Base64
	strea m: Mac OSX 10.3 - R 2.0.1
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3C3@usrymx25.merck.com>

> From: Yuandan Zhang
> 
> If you want to call R from perl, why don't you do a simple 
> system call like:
> 
> $callR="/usr/loca/bin/R CMD BATCH plotscript.R";
> system ($callR);
> 
> It is not necessary to start X display if anything can be 
> done in background

But the problem is jpeg()/png() are not available unless an X display is
available to the R process (one of the FAQs).  

Andy 
 
> On Fri, 3 Dec 2004 12:07:24 +1100 (EST)
> "Thuan-Jin Kee" <kee at wehi.EDU.AU> wrote:
> 
> > Hi All,
> > 
> > Anybody know how to make R emit base64 encoded text in some 
> way that perl
> > can grab it, instead of planting a file on your harddrive 
> when calling
> > JPEG or PNG?
> > I've managed to get these scripts to work and put a file on 
> the harddisk
> > 
> > #!/usr/bin/perl -Wall
> > # by jin kee. a simple script to demonstrate
> > # the needed steps to get R to emit a jpeg.
> > 
> > use strict;
> > 
> > my($callR, $callRold);
> > 
> > # need to start X if is isn't already started.
> > `open /Applications/Utilities/X11.app`;
> > 
> > 
> > #need to get let the R program know where to look
> > #for the display immediately before calling
> > #the R executible.
> > $callR =<<MARKER;
> > DISPLAY=:0.0; export DISPLAY;
> > /usr/bin/R --vanilla <plotscript.R;
> > MARKER
> > 
> > system($callR);
> > 
> > # end script
> > 
> > #!/usr/bin/R
> > peg("~/Desktop/test.jpg");
> > plot(rnorm(100));
> > dev.off();
> > q(save = "no");
> > 
> > 
> > My sysadmin says that the apache user can't write to the disk due to
> > security policy, so he wants to know if I can emit the jpeg 
> as a base64
> > stream and embedd it into the dynamically generated tag 
> using a DATA tag
> > to inline the image.
> > 
> > http://www.elf.org/essay/inline-image.html
> > 
> > http://www.faqs.org/rfcs/rfc2397.html
> > 
> > i've tried searching the R-project.org site and 
> help.search() and no luck.
> > 
> > Yours
> > Jin
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> -- 
> 
> --
> Yuandan 
> Zhang, PhD
> 
> Animal Genetics and Breeding Unit
> The University of New England
> Armidale, NSW, Australia, 2351
> 
> E-mail:   yzhang4 at metz.une.edu.au
> Phone:    (61) 02 6773 3786
> Fax:      (61) 02 6773 3266
> http://agbu.une.edu.au
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>   AGBU is a joint venture of NSW Primary Industries 
>   and The University of New England to undertake 
>   genetic R&D for Australia's Livestock Industries           
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Fri Dec  3 12:19:31 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 3 Dec 2004 06:19:31 -0500
Subject: [R] help : the datail of Fortran subroutine that use in R
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3C4@usrymx25.merck.com>

Download the package source from CRAN (it would be something like
norm_x.y-z.tar.gz, where x.y-z is the version).  Unpack it somewhere and
look norm/src.

Andy

> From: sepideh shokohi
> 
> 
> How can I see the datail of Fortran subroutine that use in R.
> for example I want to see detail of "emn" routine that wrote 
> in Fortran.
> 
> 
> .Fortran("emn", s$d, old, start, tobs, s$p, 
>             s$psi, s$n, s$x, s$npatt, s$r, s$mdpst, s$nmdp, tmp, 
>            tmp, numeric(s$p), mle, tau, m, mu0, lambdainv)[[3]]
> 
> it was in em.norm in norm package.
> 
> With regards 
> Sepideh Shokohi
> 
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sdavis2 at mail.nih.gov  Fri Dec  3 12:25:12 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 3 Dec 2004 06:25:12 -0500
Subject: [R] vector to matrix transformation
In-Reply-To: <a06100500bdd5f1274069@[10.17.2.158]>
References: <a06100500bdd5f1274069@[10.17.2.158]>
Message-ID: <FF30EA84-451D-11D9-9DF2-000D933565E8@mail.nih.gov>


On Dec 3, 2004, at 5:39 AM, Gwenael Jacob wrote:

> Dear,
>
> Some analysis (linear regression) can only be done from a vectorized 
> dataset whereas others require a matrix (Mantel tests). I use the two 
> analyses and thus need to format my data in matrix and vector. I spent 
> some time trying to solve the problem and I just gave up. Did anyone 
> knows how to transform a matrix into a vector and back-transform a 
> vector into a matrix?

See ?as.vector and ?matrix.  Do those answer your question?  If not, 
perhaps you could give a bit more detail of what you have tried.

Sean



From ozric at web.de  Fri Dec  3 12:46:27 2004
From: ozric at web.de (Christian Schulz)
Date: Fri, 03 Dec 2004 12:46:27 +0100
Subject: [R] Text Mining with R
In-Reply-To: <200412021829.31979.daniele.medri@libero.it>
References: <200412021829.31979.daniele.medri@libero.it>
Message-ID: <41B05213.20608@web.de>

Hi,

i didn't know anything and imho R is not the perfect basis
for this , which i'm recognize when want
doing a lot of text data manipulation.

But with the open-sources software weka there exist some possibility's
and subprojects related to text-mining.

http://www.cs.waikato.ac.nz/ml/weka/

regards, christian



Daniele Medri wrote:

>Dears,
>
>anyone has experiences with text mining and R?
>I'll be very greatfull for tutorial or examples.
>
>Thanks
>  
>



From jeaneid at chass.utoronto.ca  Fri Dec  3 13:10:38 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 3 Dec 2004 07:10:38 -0500
Subject: [R] multinomial probit
In-Reply-To: <1abbab52041203020965005fe8@mail.gmail.com>
Message-ID: <Pine.SGI.4.40.0412030709350.57432359-100000@origin.chass.utoronto.ca>

I think the polr is in MASS and indeed has a probit method.

Jean,

On Fri, 3 Dec 2004, Charlie Sprenger wrote:

> Hello All,
>
>  I'm trying to run a multinomial probit on a dataset with 28 data
> points and five levels (0,1,2,3,4) in the latent choice involving
> response variable.
>
> I downloaded the latest mnp package to run the regression. It starts
> the calculation and then crashes the rpogram. I wish I could give the
> error message but it literally shuts down R without a warning.
>
> I'm using the R that's been ported to OS X. Is it possible that there
> are RAM restrictions and that when something computationally heavy
> goes through the program is cut off?
>
> As well, is there actually a "probit" method for polr from nnet package?
>
> All help appreciated as masters thesis topics are due soon.
>
> Charlie Sprenger
> UCL - student
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Fri Dec  3 13:16:36 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 03 Dec 2004 13:16:36 +0100
Subject: [R] vector to matrix transformation
In-Reply-To: <a06100500bdd5f1274069@[10.17.2.158]>
Message-ID: <41B06734.27217.1286C27@localhost>



On 3 Dec 2004 at 11:39, Gwenael Jacob wrote:

> Dear,
> 
> Some analysis (linear regression) can only be 
> done from a vectorized dataset whereas others 
> require a matrix (Mantel tests). I use the two 
> analyses and thus need to format my data in 
> matrix and vector. I spent some time trying to 
> solve the problem and I just gave up. Did anyone 

Hi Gwenael

You probably gave up too early

did you read  ?as.matrix, ?as.vector

> x<-matrix(1:6,3,2)
> x
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
> as.vector(x)
[1] 1 2 3 4 5 6

> y<-as.vector(x)

> z<-matrix(y,3,2)
> z
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6

Cheers
Petr






> knows how to transform a matrix into a vector and 
> back-transform a vector into a matrix?
> 
> Thanks by advance,
> Gwena??l Jacob
> -- 
> 
> -------------------------------------
> Gwena??l JACOB
> Division Biodiversity
> Swiss Federal Research Institute WSL
> Zuercherstrasse 111/Postfach
> CH-8903 Birmensdorf
> 
> SWITZERLAND
> 
> Phone : ++41 1 7392 504
> Fax    : ++41 1 7392 215
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From phgrosjean at sciviews.org  Fri Dec  3 14:25:42 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 3 Dec 2004 14:25:42 +0100
Subject: [R] How about a mascot for R?
In-Reply-To: <1102006002.10726.32.camel@mordor.ipimar.pt>
Message-ID: <200412031328.iB3DSPoY1376410@hedwig1.umh.ac.be>

I'd vote for the inchworm because what is more representative to stat? The
Gauss curve, of course! What looks like a Gauss curve? The inchworm:

http://www.floridanature.org/photos/Geometridae,_Tallahassee,_20011230.jpg

http://www.daniellesplace.com/Images2/inchwormcoloractivity.gif

Best,

Philippe Grosjean



From andy_liaw at merck.com  Fri Dec  3 14:28:36 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 3 Dec 2004 08:28:36 -0500
Subject: [R] Protocol for answering basic questions
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3C6@usrymx25.merck.com>

Apologies to those who are tired of these rather off-topic discussions.
I'll try to be brief.

> From: Robert Brown FM CEFAS
> 
> How will I delete the reply without reading first unless I 
> delete all replies? I've made it quite clear that some 
> replies are useful, but some are unhelpful. I and other would 
> like to see an improvement in r help; to just say take it or 
> leave it, as many infer, is conceptually naive.

I believe what was meant is that if you find some posts
rude/offensive/not-to-your-liking, just delete _all_ posts by those people
w/o reading.  For USENET newsgroup, some (all?) news readers allow you to
create a `kill' list that automatically kill messages posted by people that
annoy you.  You can do that with most email clients, I believe.  Of course,
for those who find that unacceptable, they are free to unsubscribe.

> I joined the community because I want technical assistance 
> and I don't question the technical skills of many of the 
> respondants, but the wider appreciation of needs of users i.e 
> conceptual naivety.  It's clear from this string that many 
> beginners are leaving the forum and this should be a cause 
> for concern if we are seriously concerned with propogating 
> knowledge.  Unfortuantely this string does seem to indicate 
> this is not a major concern and so be it.  

I will toss in my perspective, instead of speaking for others.  To me, it's
not how basic the questions are, but how they are being asked.  It's been
pointed out by several people:  If you showed some effort in trying to solve
the problem yourself (by describing what you have tried and how that
failed), you will almost always get useful replies without being chastised.
Those who received less than enthusiastic responses are generally those that
do not show any apparent efforts in trying to solve the problem themselves.
As been said ad nauseam before, R is a purely volunteer-based project, and
people on this list help others out of their good will.  It's rude and
discourteous to abuse that.  It's OK if you need some spoonfeeding (I need
that quite often myself), but at least show how you have tried to use the
spoon yourself, instead of just showing us your open mouth.

For those who think commercial support is somehow `better', please have a
look at a recent thread on the SUSE AMD64 mailing list on SUSE's
installation support (Peter would know what I mean).  BTW, that's also a
prime example of how `gentle' R-help is compared to most other lists.  I
suspect those whose egos are buised by responses to their questions probably
haven't had much experience with mailing lists. 

Also, I think it should be made clear that the R user community is (much?)
larger than those who subscribe to R-help/R-devel/R-*/BioC lists.  I know
many who use R as their primary tool, yet do not subscribe to R-help.  These
people managed to get by just fine, either with help pages/manuals/books, or
more experienced colleagues.

Cheers,
Andy 
 
> -----Original Message-----
> From: Marwan Khawaja [mailto:marwan.khawaja at aub.edu.lb]
> Sent: 01 December 2004 20:41
> To: Robert Brown FM CEFAS; r-help at stat.math.ethz.ch
> Subject: RE: [R] Protocol for answering basic questions
> 
> 
> Well, if you do not like the way some people answer queries, 
> why not just delete
> the reply without reading the response.
> Since we're not paying anyone for answering questions, we 
> should be grateful to
> those who put their time in replying to our basic questions.
> And why join this community? -- if you think most are 
> 'conceptually na??ve'!
> 
> Marwan
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert 
> > Brown FM CEFAS
> > Sent: Wednesday, December 01, 2004 6:46 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Protocol for answering basic questions
> > 
> > I have been following the discussions on 'Reasons not to 
> > answer very basic questions in a straightforward way' with 
> > interest as someone who is also new to R and has had similar 
> > experiences.  As such it with sadness that I note that most 
> > seem to agree with the present approach to the responses to 
> > basic questions.  I must thank those respondants to my own 
> > questions who have been helpful, but there are some whose 
> > replies are in my opinion not only unhelpful but actually 
> > rude.  Indeed I've now started using Splus instead of R so as 
> > to have access to a 'proper' support service.  Indeed, the 
> > main thing I've learned from R is a new respect for the 
> > values of commercial software and a scepticism regarding free 
> > software. In the end my experience of r help is that you get 
> > what you pay for. Many of the so called socratic responses 
> > (in this list and the wider academic community) can be seen 
> > as simply way to avoid additional work of a complete reply. 
> > 
> > Experienced R users don't seem to understand how difficult 
> > the program can be to new users. Responding that the 
> > questioner should read the 'Introduction to R' or a similar 
> > document is like  answering a question for directions to 
> > one's house with 'Buy a map'.  Most likely both such 
> > questioners have already tried that and are asking because 
> > that approach failed.  R is a language and like all languages 
> > it is simple to those that understand it and complex to those 
> > who do not. Every schoolboy in Spain speaks Spanish, but I 
> > know from experience that for most English people it is very 
> > difficult to learn Spanish and take years of study.  If I'm 
> > asked a question from a novice of a language (be it Spanish 
> > or R) do I reply 'consult the dictionary'. I would hope not!  
> > I can tell repondants that whilst many of my basic questions 
> > may seem simple it is not for lack of studying the very 
> > sources they refer to.  If only learning was so simple.  I 
> > suspect that the same is true of most question!
> >  ers.
> > 
> > I speak as someone with a PhD and many years as a researcher 
> > in my speciality as well as someone close to completing a 
> > masters in statistics with distinction. As such I am not a 
> > total novice and would suggest that if I'm having problems so 
> > are many; and it is not a result of lack of study as so many 
> > responses seem to suggest.  Indeed it is revealing that 
> > several responses suggest that they want to discourage 
> > questions so they don't overwhelm r-help.  Understandable but 
> > not a recipe to encourage the use of R by other than experts. 
> > The R community needs to decide of they really only want 
> > expert statisticians users and make this clear if it is the 
> > case.  Alternatively if they are to encourage novices the 
> > present approach is not the way to do it.
> > 
> > I can appreciate that many of the respondants are busy, but 
> > if that is the case it would be better if they didn't reply 
> > at all. I was taught many years ago that if you can't say 
> > anything nice/useful then to say nothing at all.  Something 
> > similar could well be applied to this list.  I must say that 
> > some respondants are very helpful; and I thank them.  Leave 
> > these simple questions to such people.  Indeed it seems 
> > surprising that some exteremely experienced R users choose to 
> > reply to these basic messages at all; and it seem it is 
> > mostly these people who are rude.  I would have thought it 
> > might be better for them to concentrate on complex problems 
> > more suited to their skills and interests and leave the 
> > simple questions to more sympathetic souls.
> > 
> > Perhaps there is a case for two r help lists catering to 
> > basic and advanced questions? Certainly if the R community is 
> > serious about appealling to users outside advanced 
> > statisticians there is a need for a change of approach in r 
> > help and elsewhere.  Russ Ackoff identified much of the 
> > failure of management science as due to those who were 
> > 'mathematically sophisticated but conceptually naive' and 
> > much the same could be said for many in the R community.
> > 
> > Finally, let me once again thank those who have responded 
> > helpful to my queries in the past and ask them to continue in 
> > that vein; their assistance and effort is greatly appreciated.
> > 
> > 
> > 
> >   
> > 
> > 
> > **************************************************************
> > *********************
> > This email and any attachments are intended for the named 
> > recipient(s) only.  Its unauthorised use, distribution, 
> > disclosure, storage or copying is not permitted.  If you have 
> > received it in error, please destroy all copies and notify 
> > the sender.  In messages of a non-business nature, the views 
> > and opinions expressed are the author's own and do not 
> > necessarily reflect those of the organisation from which it 
> > is sent.  All emails may be subject to monitoring.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From julien.ruiz at airfrance.fr  Fri Dec  3 14:33:10 2004
From: julien.ruiz at airfrance.fr (julien.ruiz@airfrance.fr)
Date: Fri, 3 Dec 2004 14:33:10 +0100
Subject: =?ISO-8859-1?Q?R=E9f=2E_=3A_Re=3A_[R]_Choice_modelling_=28was=3A=28no?=
	=?ISO-8859-1?Q?_subject=29=29?=
Message-ID: <OF8D991D49.40A452C0-ONC1256F5F.00474D15@airfrance.fr>





>>I could incorporate indicators of choice availability as explanotary
>>variables, but it does not seem a very good way to do it.
>>Instead, for a logit model, I have coded a likelihood computation of the
>>underlying model with varying choice set and I use optim function to get
>>the "maximum".
>>
>>
>Could you post the code?

I can but will be ashamed of my coding...

Something like the following :

calcLikelihood<-function(beta,x,y, S) {
#beta  : parameter to be estimated
#S: 0/1 matrix of choices available for each individual
#x matrix of choice caracteristic (only compute intercept)
#beta[k] : intercept of choice k

#y : choice made for each indiv
      logLikObs <- 0
      for (t in 1:length(y)) {
            #utility of choice at t
            u_ch_t= beta %*%  x[y[t],]
            # overall utility of possible choices at t
            u_tot_t = sum( exp( beta %*% x )) + 1
            logLikObs=logLikObs + (u_ch_t - log (u_tot_t) )
      }

      return(logLikObs)
}


#Dummy exemple :
calcProbfromUtil <- function (utilVector) {
      v<-exp(utilVector)
      return(v/sum(v))
}

drawFromProb <- function (probVector) {
      x<-(1:length(probVector))
      u<-runif(1)
      if (u > max(probVector)) {
            return(min(x[probVector==max(probVector)]))
      } else {
            return(min(x[probVector==min(probVector[probVector>u])]))
      }
}

nb_class<-4 #nb of choices

maxBeta <- 5
minBeta <-  -maxBeta
betaFree <-round(runif(nb_class-1, min=minBeta, max=maxBeta),2)
beta <- c(0,betaFree)  #parameter to be estimated

x <- diag(nb_class)

T <- (nb_class*5000)  #nb of individuals

S <- array(ifelse(runif(nb_class*T)>0.5,1,0),dim=c(T,nb_class)) #0/1 matrix of choices available for each individual

util <- t(apply(S,1,function(x) {x*beta})) #utility for each choices

prob <- t(apply(util,1,calcProbfromUtil)) #corresponding probability

y <- as.vector(apply(prob,1,drawFromProb))


#betaFree : param??tre libre de beta

calcLikelihoodFromBetaFree <- function(betaFree) {
      return(calcLikelihood(c(0,betaFree),x,0,y, S))
}


#AND FINALLY
resOptim <- optim(par=rep(0,nb_class-1),
      fn=calcLikelihoodFromBetaFree,
      method="L-BFGS-B",
      lower=rep(minBeta*1.5,nb_class-1),
      upper=rep(maxBeta*1.5,nb_class-1),
      control=list(trace=0,fnscale=-1))

beta_est<-c(0,resOptim$par)

#of course , in order to do it right beta_est should be "normalized"...



From abunn at whrc.org  Fri Dec  3 14:40:12 2004
From: abunn at whrc.org (Andy Bunn)
Date: Fri, 3 Dec 2004 08:40:12 -0500
Subject: [R] vector to matrix transformation
In-Reply-To: <a06100500bdd5f1274069@[10.17.2.158]>
Message-ID: <NEBBIPHDAMMOKDKPOFFIIECBCNAA.abunn@whrc.org>

In addition to Sean's reply look at ?dist and other ways of creating
distance / similarity matrices for applications like Mantels Test. Package
vegan might be particularly useful.

HTH, Andy

R > x <- rnorm(10)
R > y <- dist(x)
R > str(x)
 num [1:10] -0.431  0.564  0.901 -1.407 -0.991 ...
R > str(y)
Class 'dist'  atomic [1:45] 0.995 1.332 0.977 0.560 0.909 ...
  ..- attr(*, "Size")= int 10
  ..- attr(*, "Diag")= logi FALSE
  ..- attr(*, "Upper")= logi FALSE
  ..- attr(*, "method")= chr "euclidean"
  ..- attr(*, "call")= language dist(x = x)
R > class(y)
[1] "dist"
R > class(x)
[1] "numeric"
R >




> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Gwenael Jacob
> Sent: Friday, December 03, 2004 5:40 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] vector to matrix transformation
>
>
> Dear,
>
> Some analysis (linear regression) can only be
> done from a vectorized dataset whereas others
> require a matrix (Mantel tests). I use the two
> analyses and thus need to format my data in
> matrix and vector. I spent some time trying to
> solve the problem and I just gave up. Did anyone
> knows how to transform a matrix into a vector and
> back-transform a vector into a matrix?
>
> Thanks by advance,
> Gwena??l Jacob
> --
>
> -------------------------------------
> Gwena??l JACOB
> Division Biodiversity
> Swiss Federal Research Institute WSL
> Zuercherstrasse 111/Postfach
> CH-8903 Birmensdorf
>
> SWITZERLAND
>
> Phone : ++41 1 7392 504
> Fax    : ++41 1 7392 215
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From kjetil at acelerate.com  Fri Dec  3 01:05:25 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 02 Dec 2004 20:05:25 -0400
Subject: [R] combine two strings
In-Reply-To: <000a01c4d8c5$f140cfd0$6501a8c0@IBMX40>
References: <000a01c4d8c5$f140cfd0$6501a8c0@IBMX40>
Message-ID: <41AFADC5.1010206@acelerate.com>

Ben-Yang Liao wrote:

>Hello,
>
>I would like to combine two strings while using R.
>For instance,
>string1 <- "abcde"
>string2 <- "WXYZ"
>I'd like to combine string1 and string2 into Sting3;
>and string3 should be "abcdeWXZY".
>  
>
paste(string1, string2, sep="")

>Would you please tell me how to do it?
>Thank you very much
>
>Ben-Yang
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Fri Dec  3 15:20:42 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 03 Dec 2004 10:20:42 -0400
Subject: [R] multinomial probit
In-Reply-To: <1abbab52041203020965005fe8@mail.gmail.com>
References: <1abbab52041203020965005fe8@mail.gmail.com>
Message-ID: <41B0763A.4000804@acelerate.com>

Charlie Sprenger wrote:

>Hello All, 
>
> I'm trying to run a multinomial probit on a dataset with 28 data
>points and five levels (0,1,2,3,4) in the latent choice involving
>response variable.
>
>I downloaded the latest mnp package to run the regression. It starts
>  
>
                                                    MNP

>the calculation and then crashes the rpogram. I wish I could give the
>error message but it literally shuts down R without a warning.
>  
>
I am also struggling with this. could you give more details, also try 
very short runs (n.draws=1!),
try different set.seed(), ...

Kjetil

>I'm using the R that's been ported to OS X. Is it possible that there
>are RAM restrictions and that when something computationally heavy
>goes through the program is cut off?
>
>As well, is there actually a "probit" method for polr from nnet package?
>
>All help appreciated as masters thesis topics are due soon.
>
>Charlie Sprenger
>UCL - student
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From RVARADHAN at JHMI.EDU  Fri Dec  3 15:20:51 2004
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Fri, 03 Dec 2004 09:20:51 -0500
Subject: [R] Computing the minimal polynomial or, at least, its degree
Message-ID: <0I85002OUH6RSO@jhuml1.jhmi.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041203/6fd50abb/attachment.pl

From kjetil at acelerate.com  Fri Dec  3 15:42:39 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 03 Dec 2004 10:42:39 -0400
Subject: [R] applying data generating function
In-Reply-To: <20041203074206.61902.qmail@web50008.mail.yahoo.com>
References: <20041203074206.61902.qmail@web50008.mail.yahoo.com>
Message-ID: <41B07B5F.6060501@acelerate.com>

ebru apayd??fffffdn wrote:

>Can  you  write an R function to generate from N  samples from the given Gibbs  algorithm.
>Also  we  must  repeat the study for  different N  values  and  different L  values .And  plot  iterations vs F1 and iteration vs F2.
> 
> F1, F2  ~N2 (0, ( 1   L )  )
>                         1   L
>
>		
>  
>
Did you hit the send button accidentally? Any way, maybe have a look at 
MCMCpack and the
function MCMCmetrop1R

Kjetil


>---------------------------------
>
> Send a seasonal email greeting and help others. Do good.
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Fri Dec  3 15:49:20 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 03 Dec 2004 10:49:20 -0400
Subject: [R] information
In-Reply-To: <6.1.0.6.1.20041203111659.02684788@pop2.mate.polimi.it>
References: <6.1.0.6.1.20041203111659.02684788@pop2.mate.polimi.it>
Message-ID: <41B07CF0.4080002@acelerate.com>

Anna Maria Paganoni wrote:

> I would like to know if there exist any package on the profyle 
> analysis, or on repeated measures, (AUC)...

I don't know about "profyle" analyses, but have a look at the packages 
nlme (or lme4) which can do
repeated measures.

Kjetil

> Thank for answering me,
>
>
> Anna Maria Paganoni       
> Dipartimento di Matematica
> Politecnico di Milano
> piazza Leonardo da Vinci, 32
> 20133 Milano, Italy
> tel +39-0223994574 fax +39-0223994568
> e-mail annap at mate.polimi.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Fri Dec  3 16:03:01 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 03 Dec 2004 11:03:01 -0400
Subject: [R] How about a mascot for R?
In-Reply-To: <200412031328.iB3DSPoY1376410@hedwig1.umh.ac.be>
References: <200412031328.iB3DSPoY1376410@hedwig1.umh.ac.be>
Message-ID: <41B08025.2040104@acelerate.com>

Philippe Grosjean wrote:

>I'd vote for the inchworm because what is more representative to stat? The
>Gauss curve, of course! What looks like a Gauss curve? The inchworm:
>  
>
Erling B. Andersen tells in his paper in "Rasch Models: Foundations, 
Recent Developments, and Applications"
edited by  G. H. Fischer and I. W. Molenaar:
"Georg Rasch had a very obvious animosity towards the normal 
distribution. At certain occasions, when we had all
consumed a generous amount of alcohol, he would invite all persons 
present to a party on his front lawn to burn all books containing the
word 'normal distribution'."

Kjetil

>http://www.floridanature.org/photos/Geometridae,_Tallahassee,_20011230.jpg
>
>http://www.daniellesplace.com/Images2/inchwormcoloractivity.gif
>
>Best,
>
>Philippe Grosjean
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From RVARADHAN at JHMI.EDU  Fri Dec  3 16:24:39 2004
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Fri, 03 Dec 2004 10:24:39 -0500
Subject: [R] Computing the minimal polynomial or, at least, its degree
Message-ID: <0I850091LK54BP@jhuml1.jhmi.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041203/af68d6b4/attachment.pl

From cyracules at yahoo.co.uk  Fri Dec  3 16:33:07 2004
From: cyracules at yahoo.co.uk (John)
Date: Fri, 3 Dec 2004 15:33:07 +0000 (GMT)
Subject: [R] [BASIC] Solution of combining two strings
Message-ID: <20041203153307.34338.qmail@web26304.mail.ukl.yahoo.com>

You can try the following code:

R> string3 <- paste(string1, string2, sep="")
R> ?paste

Have a look at the following functions too which I
found useful when I wanted to use 'paste':

R> ?assign
R> ?sub
R> ?gsub

Please don't forget to read the posting guide.

http://www.R-project.org/posting-guide.html

John

ps. I hope that you haven't received any "vitriolic"
and "curt" responses to your basic question as some
questioners sometimes do.

 --- Ben-Yang Liao <liaoby at umich.edu> wrote:
> Hello,
> 
> I would like to combine two strings while using R.
> For instance,
> string1 <- "abcde"
> string2 <- "WXYZ"
> I'd like to combine string1 and string2 into Sting3;
> and string3 should be "abcdeWXZY".
> Would you please tell me how to do it?
> Thank you very much
> 
> Ben-Yang



From kimai at Princeton.Edu  Fri Dec  3 16:34:34 2004
From: kimai at Princeton.Edu (Kosuke Imai)
Date: Fri, 3 Dec 2004 10:34:34 -0500 (EST)
Subject: [R] multinomial probit
Message-ID: <Pine.LNX.4.44.0412031027110.8899-100000@wws-6qcbw21.Princeton.EDU>

Dear Charlie,
  The multinomial probit model estimates the covariance matrix of the
latent variables in addition to the coefficents. In your case with 5
alternatives, there are 9 parameters in the covariance matrix. This means
that even if you only have intercepts in your model (no covariate), you
will have a total of 13 parameters to estimate with only 28 data points.  
If you still want to fit the model, you need an informative prior. The
default prior is quite diffuse, and so you need to adjust p.var, p.df, and
p.scale arguments in mnp(). See the full documentation available at
http://www.princeton.edu/~kimai/research/MNP.html and the paper for more
details and examples:

Imai, Kosuke, and David A. van  Dyk. (2005). ``A Bayesian Analysis of the  
Multinomial Probit Model Using Marginal Data Augmentation.''  Journal of 
Econometrics, Vol. 124, No. 2 (February), pp. 311-334.

Hope this helps,
Kosuke

---------------------------------------------------------
Kosuke Imai               Office: Corwin Hall 041
Assistant Professor       Phone: 609-258-6601 
Department of Politics    eFax:  973-556-1929
Princeton University      Email: kimai at Princeton.Edu
Princeton, NJ 08544-1012  http://www.princeton.edu/~kimai
---------------------------------------------------------

> From: Charlie Sprenger <c.sprenger at gmail.com>
> Date: December 3, 2004 4:09:19 AM CST
> To: r-help at stat.math.ethz.ch
> Subject: [R] multinomial probit
> Reply-To: Charlie Sprenger <c.sprenger at gmail.com>
>
> Hello All,
>
>  I'm trying to run a multinomial probit on a dataset with 28 data
> points and five levels (0,1,2,3,4) in the latent choice involving
> response variable.
>
> I downloaded the latest mnp package to run the regression. It starts
> the calculation and then crashes the rpogram. I wish I could give the
> error message but it literally shuts down R without a warning.
>
> I'm using the R that's been ported to OS X. Is it possible that there
> are RAM restrictions and that when something computationally heavy
> goes through the program is cut off?
>
> As well, is there actually a "probit" method for polr from nnet 
> package?
>
> All help appreciated as masters thesis topics are due soon.
>
> Charlie Sprenger
> UCL - student
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

--
Andrew D. Martin, Ph.D.
Department of Political Science
Washington University
Campus Box 1063
One Brookings Drive
St. Louis, MO 63130
(314) 935-5863 (Office)
(314) 753-8377 (Cell)
(314) 935-5856 (Fax)

Office: Eliot Hall 326
Email: admartin at wustl.edu
WWW:   http://adm.wustl.edu



From esguerra at rci.rutgers.edu  Fri Dec  3 17:05:57 2004
From: esguerra at rci.rutgers.edu (Mauricio Esguerra)
Date: Fri, 3 Dec 2004 11:05:57 -0500 (EST)
Subject: [R] Label data points in scatterplot matrices
Message-ID: <Pine.GSO.4.58.0412031100190.10430@niflheim.rutgers.edu>


Hello,

I am new to R and would like to know how to label data points in the
matrices of scatterplots made by the pairs() command.
To be more specific, I want to assign a number to each data point, instead
of the small circumference that appears as a data point.

If anyone here knows if its possible to do this with R, I would greatly
appreciate your help.

Thank you,

Mauricio Esguerra
PhD candidate
Chemistry and Chemical Biology Department
Rutgers, the State University of New Jersey



From spencer.graves at pdf.com  Fri Dec  3 17:13:27 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 03 Dec 2004 08:13:27 -0800
Subject: [R] Computing the minimal polynomial or, at least, its degree
In-Reply-To: <0I85002OUH6RSO@jhuml1.jhmi.edu>
References: <0I85002OUH6RSO@jhuml1.jhmi.edu>
Message-ID: <41B090A7.5030802@pdf.com>

      Have you looked at library(polynom)?  Will that with 
unique(eigen(A)$values) allow you to compute what you want? 

      hope this helps. 
      spencer graves

Ravi Varadhan wrote:

>Hi,
>
> 
>
>I would like to know whether there exist algorithms to compute the
>coefficients or, at least, the degree of the minimal polynomial of a square
>matrix A (over the field of complex numbers)? I don't know whether this
>would require symbolic computation.  If not, has any of the algorithms been
>implemented in R?  
>
> 
>
>Thanks very much,
>
>Ravi.
>
> 
>
>P.S.  Just for the sake of completeness, a minimal polynomial is a monic
>polynomial (whose leading coefficient is unity) of least degree, which
>divides all the annihilating polynomial of A. In particular, the minimal
>polynomial divides the characteristic polynomial.  Knowing the degree of the
>minimal polynomial is useful in characterizing the convergence properties of
>a certain class of numerical schemes for iteratively solving linear (and
>nonlinear) system of equations.
>
> 
>
>--------------------------------------------------------------------------
>
>Ravi Varadhan, Ph.D.
>
>Assistant Professor,  The Center on Aging and Health
>
>Division of Geriatric Medicine and Gerontology
>
>Johns Hopkins Univerisity
>
>Ph: (410) 502-2619
>
>Fax: (410) 614-9625
>
>Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu
>
>--------------------------------------------------------------------------
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From deepayan at stat.wisc.edu  Fri Dec  3 17:48:47 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 3 Dec 2004 10:48:47 -0600
Subject: [R] organising the display in Trellis plots
In-Reply-To: <Pine.LNX.4.58.0412031036380.15100@kika.intellektik.informatik.tu-darmstadt.de>
References: <200411301116.iAUB53F8024271@hypatia.math.ethz.ch>
	<Pine.LNX.4.58.0412031036380.15100@kika.intellektik.informatik.tu-darmstadt.de>
Message-ID: <200412031048.47425.deepayan@stat.wisc.edu>

On Friday 03 December 2004 03:47, Marco Chiarandini wrote:
> Hallo,
>
> I would like to organise at my pleasure the layout of a trellis plot.
>
> Currently I have a 3x3 matrix display and 7 plots. Is it possible to
> choose which specific panels will stay empty?

Well, you have already specified 'layout', so the only issues are what 
order to plot the panels in, and where to plot them. I'm not sure which 
of these issues you are interested in (perhaps both).

To skip some panels, try 'skip', e.g., skip = c(F,F,F,T,F,T,F,F,F)

To change the order, you need to use index.cond (see below).

> I tried index.perm to arrange the order. Then there is perm.cond
> which I could not understand if it can serve for my purpose since I
> cant really find which kind of data it accepts.
>
> Thank you for the help.
>
>
> Marco
>
> P.S.  I attach some code.
>
> plot <- Dotplot(algo~Cbind(y,lower,upper) | group,data=OUT,
>                   pch=3, method="bars",pch.bar=2,lwd=3,
>                   cex=0.8,
>                   par.strip.text=list(cex=1),
>                   panel=function(x,y,...) {
>                     print(attr(x,'other'))
>                    
> panel.abline(v=attr(x,'other'),lty=2,col="grey70")
> panel.Dotplot(x,y,...)
>                   },
>                   scales = list(cex=1,rot=c(0,45),
>                    
> x=list(alternating=c(1,1,1,1),limits=c(1,nlevels(OUT$algo)),relation=
>"same"),
> y=list(at=c(1:nlevels(OUT$algo)),labels=levels(OUT$algo),alternating=
>c(1,1,1,1)) ),
>                 main=list(label=""),
>                   xlab=list(cex=1,label="Average rank"),
>                   ylab="",
>                   aspect="fill",as.table=FALSE,
>                   layout=c(3,3)
>                   )
>     print(plot)
>     update(plot,perm.cond=c(3,2,1,4,5,6))  #this does not work

>From ?xyplot, 

perm.cond: numeric vector, a permutation of '1:n', where 'n' is the
          number of conditioning variables. ...

In your case, n = 1, so this doesn't make sense.

>     update(plot,index.cond=c(3,2,1,4,5,6))  #this works it does not
> what I wish

index.cond needs to be a list of length n (one component for each 
conditioning variable), so you probably want

update(plot,index.cond=list(c(3,2,1,4,5,6)))

Deepayan



From p.dalgaard at biostat.ku.dk  Fri Dec  3 18:18:16 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Dec 2004 18:18:16 +0100
Subject: [R] How about a mascot for R?
In-Reply-To: <41B08025.2040104@acelerate.com>
References: <200412031328.iB3DSPoY1376410@hedwig1.umh.ac.be>
	<41B08025.2040104@acelerate.com>
Message-ID: <x2y8gf2td3.fsf@biostat.ku.dk>

Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> writes:

> Philippe Grosjean wrote:
> 
> >I'd vote for the inchworm because what is more representative to stat? The
> >Gauss curve, of course! What looks like a Gauss curve? The inchworm:
> >
> Erling B. Andersen tells in his paper in "Rasch Models: Foundations,
> Recent Developments, and Applications"
> edited by  G. H. Fischer and I. W. Molenaar:
> "Georg Rasch had a very obvious animosity towards the normal
> distribution. At certain occasions, when we had all
> consumed a generous amount of alcohol, he would invite all persons
> present to a party on his front lawn to burn all books containing the
> word 'normal distribution'."

However, Erling himself was (as you may know, he died in September,
only just over a month away from his retirement) no stranger to models
in which a Gaussian random effect was allowed in the Rasch model. And
Rasch's life-long hobby horse was "measurement methods", which sort of
fits rather nicely with other aspects of the inchworm theme.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From kjetil at acelerate.com  Fri Dec  3 17:18:02 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 03 Dec 2004 12:18:02 -0400
Subject: [R] R program for row-column designs!
In-Reply-To: <20041128004636.99294.qmail@web51804.mail.yahoo.com>
References: <20041128004636.99294.qmail@web51804.mail.yahoo.com>
Message-ID: <41B091BA.1050207@acelerate.com>

Xianggui QU wrote:

>Does anybody have an R program to generate row-column designs and to produce the incidence matrices? I would appreciate if you could share it with me! Thank you!
>Harvey.
>
>  
>
CRAN package crossdes has something for row-col design, but specifically 
for
crossover designs, it seems. Is that what you need?

Kjetil


>		
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From marcos at est.ufpr.br  Fri Dec  3 19:48:44 2004
From: marcos at est.ufpr.br (=?iso-8859-1?Q?Marcos_Aur=E9lio_Carrero?=)
Date: Fri, 3 Dec 2004 16:48:44 -0200 (BRST)
Subject: [R] setGeneric(rm)
Message-ID: <Pine.LNX.4.58L0.0412031633310.29071@est.ufpr.br>

Hi,
We are developing a package using S4 classes. The S4 classes
are wrappers to C++ classes. So S4 classes contain an integer
that is the memory address of one C++ object. If an user
calls the rm() function, the C++ object must be deleted. But our generic
rm() function apparently doesn't work. Here is the code:

# The class
setClass("component", representation(pointer="integer", "VIRTUAL"))

# The generic method
setGeneric("rm", function(..., list = object, pos = -1, envir = as.environment(pos), inherits = FALSE)  standardGeneric("rm"))

setMethod("rm", "component", function(list=object, pos, envir, inherits) {
	warning("object deleted")
	.Call("Rm");i
})

We implemented generic methods for print(), show() and summary() succesfully.

Best,
Marcos Carrero



From susufudan at hotmail.com  Fri Dec  3 19:42:25 2004
From: susufudan at hotmail.com (su su)
Date: Fri, 03 Dec 2004 13:42:25 -0500
Subject: [R] sampling
Message-ID: <BAY10-F6C7D9A48E21AF83EBC1B8B7B10@phx.gbl>

Hi:
I wonder if there is a way to resample paired value to do bootstrip?

Thanks!



From andy_liaw at merck.com  Fri Dec  3 19:47:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 3 Dec 2004 13:47:28 -0500
Subject: [R] sampling
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3CA@usrymx25.merck.com>

Yes, by bootstrapping the index.  E.g., if the pairs are in x and y, you can
do something like:

ind <- sample(length(x))
x.boot <- x[ind]
y.boot <- y[ind]

Andy

> From: su su
> 
> Hi:
> I wonder if there is a way to resample paired value to do bootstrip?
> 
> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sfalcon at fhcrc.org  Fri Dec  3 19:55:58 2004
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 3 Dec 2004 10:55:58 -0800
Subject: [R] Package dev: Depends, require, SaveImage best practices?
Message-ID: <20041203185558.GF21035@queenbee.fhcrc.org>

I'm trying to sort out some best practices for package development and
understand some behavior of R CMD check that has me confused.

Best practice question:  If a package foo appears in the Depends field in
the DESCRIPTION file of mypkg, should I refrain from using require(foo)
in the R source files of mypkg?

The Writing R Extensions manual says this about the Depends field:

    ... the list of packages which is specified will be attached before
    the current package, both when library is called and when saving an
    image or preparing for lazy-loading.

I conclude that I need not require() a package that I've listed in
Depends.  However, this isn't always working for me.  I have a package
that has Ruuid in Depends.  If I also have SaveImage: Yes, then R CMD
check completes without error.  However, if I omit SaveImage, then I get
this:

    $ R CMD check graph
    WARNING: ignoring environment value of R_HOME
    * checking for working latex ... OK
    * using log directory
    * '/home/sfalcon/proj-svn-unix/graph-proj/graph.Rcheck'
    * checking for file 'graph/DESCRIPTION' ... OK
    * checking if this is a source package ... OK

    * Installing *source* package 'graph' ...
    ** libs
    make: `graph.so' is up to date.
    ** R
    ** data
    ** inst
    ** preparing package for lazy loading
    Error in assign("nullgraphID", getuuid()) :
            couldn't find function "getuuid"
    Execution halted
    ERROR: lazy loading failed for package 'graph'
    ERROR
    Installation failed.

To conclude: what am I missing?  Would it be better to just
require(Ruuid)?  Always use SaveImage? 

Much tanks,

+ seth



From tobias.verbeke at telenet.be  Fri Dec  3 20:15:57 2004
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Fri, 3 Dec 2004 19:15:57 +0000
Subject: [R] Text Mining with R
In-Reply-To: <200412021829.31979.daniele.medri@libero.it>
References: <200412021829.31979.daniele.medri@libero.it>
Message-ID: <20041203191557.043b3141.tobias.verbeke@telenet.be>

On Thu, 2 Dec 2004 18:29:31 +0100
Daniele Medri <daniele.medri at libero.it> wrote:

> Dears,
> 
> anyone has experiences with text mining and R?
> I'll be very greatfull for tutorial or examples.
> 

You may have a look at:

http://wwwpeople.unil.ch/jean-pierre.mueller/

HTH,
Tobias


> -- 
> Daniele Medri - http://www.medri.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jfox at mcmaster.ca  Fri Dec  3 20:20:40 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 3 Dec 2004 14:20:40 -0500
Subject: [R] [R-pkgs] New package: polycor
Message-ID: <20041203192039.ZCJ2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear list members,

I've uploaded a new package, called polycor (version 0.5-0), to CRAN. The
package has functions for computing polychoric and polyserial correlations,
either by maximum-likelihood (in which case, standard errors are available)
or by faster approximations. There's also a function to compute
"heterogeneous" correlation matrices composed of product-moment, polychoric,
and polyserial correlations, as appropriate to each pair of variables.

Comments and suggestions would be appreciated.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From Roger.Bivand at nhh.no  Fri Dec  3 20:44:13 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 3 Dec 2004 20:44:13 +0100 (CET)
Subject: [R] setGeneric(rm)
In-Reply-To: <Pine.LNX.4.58L0.0412031633310.29071@est.ufpr.br>
Message-ID: <Pine.LNX.4.44.0412032034450.2664-100000@reclus.nhh.no>

On Fri, 3 Dec 2004, Marcos Aur??lio Carrero wrote:

> Hi,
> We are developing a package using S4 classes. The S4 classes
> are wrappers to C++ classes. So S4 classes contain an integer
> that is the memory address of one C++ object. If an user
> calls the rm() function, the C++ object must be deleted. But our generic
> rm() function apparently doesn't work. Here is the code:
> 
> # The class
> setClass("component", representation(pointer="integer", "VIRTUAL"))
> 
> # The generic method
> setGeneric("rm", function(..., list = object, pos = -1, envir = as.environment(pos), inherits = FALSE)  standardGeneric("rm"))
> 
> setMethod("rm", "component", function(list=object, pos, envir, inherits) {
> 	warning("object deleted")
> 	.Call("Rm");i
> })
> 
> We implemented generic methods for print(), show() and summary() succesfully.
> 

I think that you may find reading the code written by Timothy Keitt in the 
rgdal package, especially R/gdal.R, useful. He defines a class with 

representation(handle = 'externalptr')

which I think should be considered as an alternative to integer, because
integer is a vector and externalptr is not - it is an external pointer and
just that. In addition, his code shows how to use .setCollectorFun() as
the collector calling reg.finalizer() in the body of the class definition.
It may well be that you don't need an rm method for your object, I think
reading Tim's code may help you to find a cleaner structure, and 
contribute to solving this question.

Roger

> Best,
> Marcos Carrero
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From peterwyang at gmail.com  Fri Dec  3 20:56:11 2004
From: peterwyang at gmail.com (Peter Yang)
Date: Fri, 3 Dec 2004 14:56:11 -0500
Subject: [R] how can I get the coefficients of x^0, x^1, x^2, . ,
	x^6 from expansion of (1+x+x^2)^3
Message-ID: <41b0c4e0.23e38b32.5a98.00c0@smtp.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041203/13f99d98/attachment.pl

From roebuck at odin.mdacc.tmc.edu  Fri Dec  3 21:09:51 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Fri, 3 Dec 2004 14:09:51 -0600 (CST)
Subject: [R] setGeneric(rm)
In-Reply-To: <Pine.LNX.4.58L0.0412031633310.29071@est.ufpr.br>
References: <Pine.LNX.4.58L0.0412031633310.29071@est.ufpr.br>
Message-ID: <Pine.OSF.4.58.0412031334170.137475@odin.mdacc.tmc.edu>

On Fri, 3 Dec 2004, Marcos Carrero wrote:

> We are developing a package using S4 classes. The S4 classes are
> wrappers to C++ classes. So S4 classes contain an integer that is the
> memory address of one C++ object. If an user calls the rm() function,
> the C++ object must be deleted. But our generic rm() function apparently
> doesn't work. Here is the code:
>
> # The class
> setClass("component", representation(pointer="integer", "VIRTUAL"))
>
> # The generic method
> setGeneric("rm", function(..., list = object, pos = -1,
>                           envir = as.environment(pos),
>                           inherits = FALSE)  standardGeneric("rm"))
>
> setMethod("rm", "component", function(list=object, pos, envir, inherits) {
> 	warning("object deleted")
> 	.Call("Rm");i
> })
>
> We implemented generic methods for print(), show() and summary() succesfully.

The method signatures don't match. Does the warning message
actually display?

BTW, isn't it dangerous to assume a pointer and integer
occupy the same amount of storage?

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From quesada at gmail.com  Fri Dec  3 21:23:16 2004
From: quesada at gmail.com (Jose Quesada)
Date: Fri, 3 Dec 2004 20:23:16 +0000
Subject: [R] Text Mining with R
In-Reply-To: <20041203191557.043b3141.tobias.verbeke@telenet.be>
References: <200412021829.31979.daniele.medri@libero.it>
	<20041203191557.043b3141.tobias.verbeke@telenet.be>
Message-ID: <21c05c7d04120312233eb46bf4@mail.gmail.com>

Tobias,

I just created a zip file from the tar, and used the "install from
zip" option of the Rwin console.

ttda is shown in the list of installed packages. However, when I try
"load packages", or the equivaent "library(ttda)", I get:

Error in library(ttda) : 'ttda' is not a valid package -- installed < 2.0.0?
> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
Error in library(pkg, character.only = TRUE) : 
        'ttda' is not a valid package -- installed < 2.0.0?

Do you know why?

Thanks,
-Jose

On Fri, 3 Dec 2004 19:15:57 +0000, Tobias Verbeke
<tobias.verbeke at telenet.be> wrote:
> On Thu, 2 Dec 2004 18:29:31 +0100
> Daniele Medri <daniele.medri at libero.it> wrote:
> 
> > Dears,
> >
> > anyone has experiences with text mining and R?
> > I'll be very greatfull for tutorial or examples.
> >
> 
> You may have a look at:
> 
> http://wwwpeople.unil.ch/jean-pierre.mueller/
> 
> HTH,
> Tobias
> 
> 
> 
> 
> > --
> > Daniele Medri - http://www.medri.org
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
* I have finally put online an exhibition project with my newer paintings *
http://lsa.colorado.edu/~quesadaj/FTexhibitionProject/
Jose Quesada, PhD.
 
jquesada at andrew.cmu.edu		Research associate
http://lsa.colorado.edu/~quesadaj	Dept. of Social and Decision Sciences
http://www.andrew.cmu.edu/~jquesada	Carnegie Mellon University
Porter Hall	Phone: 412 268 6011
office PH208-J	Fax:   412 268 6938
5000 Forbes ave.
15213, Pittsburgh, PA



From james.holtman at convergys.com  Fri Dec  3 21:43:43 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Fri, 3 Dec 2004 15:43:43 -0500
Subject: [R] how can I get the coefficients of x^0, x^1, x^2, . , x^6 from
	expansion of (1+x+x^2)^3
Message-ID: <OFE346EB51.40A79942-ON85256F5F.0071BB57@nd.convergys.com>





Use the 'polynom' library:

> p <- as.polynomial(c(1,1,1))
> p
1 + x + x^2
> p^3
1 + 3*x + 6*x^2 + 7*x^3 + 6*x^4 + 3*x^5 + x^6
 > unclass(p^3)
[1] 1 3 6 7 6 3 1
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      "Peter Yang"                                                                                                         
                      <peterwyang at gmail.com        To:       <r-help at stat.math.ethz.ch>                                                    
                      >                            cc:                                                                                     
                      Sent by:                     Subject:  [R] how can I get the coefficients of x^0, x^1, x^2, . ,    x^6 from          
                      r-help-bounces at stat.m         expansion of (1+x+x^2)^3                                                               
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      12/03/2004 14:56                                                                                                     
                                                                                                                                           
                                                                                                                                           




Hi,



I would like to get the coefficients of x^0, x^1, x^2,  . , x^6 from
expansion of (1+x+x^2)^3.

The result should be 1, 3, 6, 7, 6, 3, 1;



How can I calculate in R?



You help will be greatly appreciated.



Peter






             [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tom_woody at swissinfo.org  Fri Dec  3 21:37:42 2004
From: tom_woody at swissinfo.org (=?ISO-8859-15?Q?Thomas_Sch=F6nhoff?=)
Date: Fri, 03 Dec 2004 21:37:42 +0100
Subject: [R] Protocol for answering basic questions
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E3C6@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E3C6@usrymx25.merck.com>
Message-ID: <41B0CE96.5060708@swissinfo.org>

Hello,


Liaw, Andy schrieb:
> Apologies to those who are tired of these rather off-topic discussions.
> I'll try to be brief.
> 
> 

> I will toss in my perspective, instead of speaking for others.  To me, it's
> not how basic the questions are, but how they are being asked.  It's been
> pointed out by several people:  If you showed some effort in trying to solve
> the problem yourself (by describing what you have tried and how that
> failed), you will almost always get useful replies without being chastised.
> Those who received less than enthusiastic responses are generally those that
> do not show any apparent efforts in trying to solve the problem themselves.
> As been said ad nauseam before, R is a purely volunteer-based project, and
> people on this list help others out of their good will.  It's rude and
> discourteous to abuse that.  It's OK if you need some spoonfeeding (I need
> that quite often myself), but at least show how you have tried to use the
> spoon yourself, instead of just showing us your open mouth.

Even if you did , but missed to get a look at R FAQ and other 
resources, you're pointed at the appropriate issue! I think that's 
quite okay with me!
Well its not only a question of either getting a RTFM or being just 
ruffled by someone. AFAIR I sometimes got both......well that okay, 
and teaches you some decency regarding what questions might be raised 
to bother generally helpful people on this list.
> BTW, that's also a
> prime example of how `gentle' R-help is compared to most other lists.  I
> suspect those whose egos are buised by responses to their questions probably
> haven't had much experience with mailing lists. 

There are many lists which don't give a dam about postings containing 
no meaningful subject or a proper description of your problem. It 
stunning to see that some people even don't care to make up their 
minds related to this issue (well, maybe only to the point when they 
start using the mail archive and give up on browsing too many no 
subjects-mails.
Well, basically I am  agree to your statement, Andy!
> 
> Also, I think it should be made clear that the R user community is (much?)
> larger than those who subscribe to R-help/R-devel/R-*/BioC lists.  I know
> many who use R as their primary tool, yet do not subscribe to R-help.  These
> people managed to get by just fine, either with help pages/manuals/books, or
> more experienced colleagues.

Hmm, maybe that is the hardcore way of doing things. I am sure there 
are lots of people who like this learning method, but not quite sure 
if this the majority ........
With regard to this I (still) think that using R demands some far 
reaching skills, i.e. not entirely restricted to profound knowledge in 
statistics, but also related to OO programming. If you never did any 
programming even help pages, manuals and books are of limited usefulness.
So, it takes time to get into and not all of your problems might me 
solved soon.......hmm, where is this damned......heh...my..spoon... ah 
.....

Well to all beginners, like me, it takes time and some efforts to get 
into, sure that this investment will sum up to something good. I can 
recommend the mail archive of R-help, since I am sure that nearly 
almost (all) of my stupid beginners questions are already answered 
(beware this may only correct for a= 0.05 CI), the rest is found in R 
FAQ..........



sincerely

Thomas



From roebuck at odin.mdacc.tmc.edu  Fri Dec  3 21:52:20 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Fri, 3 Dec 2004 14:52:20 -0600 (CST)
Subject: [R] Text Mining with R
In-Reply-To: <21c05c7d04120312233eb46bf4@mail.gmail.com>
References: <200412021829.31979.daniele.medri@libero.it><20041203191557.043b3141.tobias.verbeke@telenet.be>
	<21c05c7d04120312233eb46bf4@mail.gmail.com>
Message-ID: <Pine.OSF.4.58.0412031430230.137475@odin.mdacc.tmc.edu>

On Fri, 3 Dec 2004, Jose Quesada wrote:

> On Fri, 3 Dec 2004 19:15:57 +0000, Tobias Verbeke wrote:
>
> > You may have a look at:
> >
> > http://wwwpeople.unil.ch/jean-pierre.mueller/
>
> I just created a zip file from the tar, and used the "install from
> zip" option of the Rwin console.
>
> ttda is shown in the list of installed packages. However, when I try
> "load packages", or the equivaent "library(ttda)", I get:
>
> Error in library(ttda) : 'ttda' is not a valid package -- installed < 2.0.0?
> > local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> Error in library(pkg, character.only = TRUE) :
>         'ttda' is not a valid package -- installed < 2.0.0?
>
> Do you know why?

Converting from one archive format to another doesn't
change the fact that it's still a source archive. The Windows
"install from zip" is expecting a precompiled, binary archive.
Unless you used the R development tools, it ain't gonna fly.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From wfaulk at icoria.com  Fri Dec  3 22:01:34 2004
From: wfaulk at icoria.com (William Faulk)
Date: Fri, 03 Dec 2004 16:01:34 -0500
Subject: [R] Odd underflow(?) error
Message-ID: <41B0D42E.7000604@icoria.com>

I'm still trying to install R on my Irix machine.  Now I have a new 
problem that crops up during the checks.  I've found the root cause, and 
it's that R is returning zero for certain things for reasons I don't 
understand.

2.225073859e-308, entered directly into R, responds "2.225074e-308".
2.225073858e-308 responds "0".

Their negative values respond similarly, so it would appear that 
somewhere in there is the smallest absolute value that that installation 
of R will hold.

On another machine where the checks passed, both responses are correct, 
not just the first one.  The underflow there is significantly lower, 
with much less accuracy, as opposed to what seems to be good accuracy on 
what looks like the broken one.  The values there are:

2.4703282293e-324 gives 4.940656e-324
2.4703282292e-324 gives 0

My first thought was that GMP and/or MPFR weren't working properly, so I 
recompiled GMP, which passed all of its tests, and GCC, making sure to 
imclude support for them.  This made no difference.

Anyone have any ideas, including which one is broken?  Both?

-Bitt



From ligges at statistik.uni-dortmund.de  Fri Dec  3 22:42:13 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 03 Dec 2004 22:42:13 +0100
Subject: [R] Label data points in scatterplot matrices
In-Reply-To: <Pine.GSO.4.58.0412031100190.10430@niflheim.rutgers.edu>
References: <Pine.GSO.4.58.0412031100190.10430@niflheim.rutgers.edu>
Message-ID: <41B0DDB5.3050606@statistik.uni-dortmund.de>

Mauricio Esguerra wrote:

> Hello,
> 
> I am new to R and would like to know how to label data points in the
> matrices of scatterplots made by the pairs() command.
> To be more specific, I want to assign a number to each data point, instead
> of the small circumference that appears as a data point.
> 
> If anyone here knows if its possible to do this with R, I would greatly
> appreciate your help.


   A <- data.frame(a1=1:3, a2=1:3, a3=1:3)
   pairs(A, pch=letters[1:3])

Uwe Ligges



> Thank you,
> 
> Mauricio Esguerra
> PhD candidate
> Chemistry and Chemical Biology Department
> Rutgers, the State University of New Jersey
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From hjb at pdq.com  Fri Dec  3 21:00:47 2004
From: hjb at pdq.com (Heather J. Branton)
Date: Fri, 03 Dec 2004 15:00:47 -0500
Subject: [R] How to wrap or split labels on plot
Message-ID: <41B0C5EF.5060203@pdq.com>

Dear R gurus,

I want to wrap labels that are too long for a plot. I have looked at 
strsplit(), substr(), nchar(), and strwrap(). I think it's some 
combination but I'm having difficulty trying to figure out the right 
combo. I think I need to create some new matrix containing the labels 
already split, though I'm not sure if maybe there is a quick and dirty 
way to address this without my wandering around the block.

I am using R 1.9.1, Windows XP.  (Note:  we are currently in the midst 
of a big project and probably won't upgrade to R 2.0.1 for another 
couple of weeks -- *unless* that's what I need to do to address these 
issues.)

Here is my script with two label options at the bottom that are not working.

# Settings
win.graph(width=8, height=8, pointsize=10)

# Read in data
test <- matrix(data=c(2.52,9.5,3.07,2.5,1.99,8.95), nrow = 6, byrow=TRUE)

# Read in and attach labels (names) to data
rownames(test) <- c("Mount Pleasant","Jordan","Oil City","Pleasant 
Valley","Village of Lake Isabella","Rosebush")

# Set plot limits:
xmax <- nrow(test)
nvec <- ncol(test)
ymax <- ceiling(max(test))
yinc <- 1

# Generate Pareto order
test <- test[order(test[,1],decreasing=TRUE),]

# Set color palette
MyCols <- rep(c("lightcyan","cornsilk","lavender"), each = xmax)

# Adjust the margins
par(mar = c(7, 5, 6, 3))

# Bar graph
mp <- barplot(test, beside = TRUE,
    col = MyCols,
    axisnames = FALSE,
    names.arg = rep(names(test),nvec),
    las = 2,
    cex.names = 0.75,
    ylab = "IXYV",
    ylim = c(0,ymax),
    yaxt = "n")

# Set up the y axis tick marks and labels
ifelse (ymax<=10,decpt <- 2,decpt <- 0)
ticks <- seq(0, ymax, yinc)
axis(2, at = ticks, las = 1,
     labels = formatC(ticks, format = "f", digits = decpt))

# Draw a box around the whole thing
box()

# Draw the x axis labels
mtext(side = 1, at = rowMeans(mp)-.2, line = .5, las=2, text = 
strsplit(names(test)," "))
mtext(side = 1, at = rowMeans(mp), line = .5, las=2, text = 
strwrap(names(test),7))
mtext(side = 1, line = 5.5, text = "Division")

# Draw titles
title(main="Central", outer=F, font.main=4, line=4)
title(main="IXYV by Division", outer=F, font.main=2, line=2.5)


Thank you kindly for your help.

...heather

 
_______________________________________________________________________
Heather J. Branton                                  Public Data Queries
Data Specialist                                 310 Depot Street, Ste C
734.213.4964 x312                                  Ann Arbor, MI  48104

               U.S. Census Microdata At Your Fingertips
                          http://www.pdq.com



From t.yee at auckland.ac.nz  Fri Dec  3 23:46:41 2004
From: t.yee at auckland.ac.nz (Thomas Yee)
Date: Sat, 04 Dec 2004 11:46:41 +1300
Subject: [R] How about a mascot for R?
Message-ID: <41B0ECD1.2030709@stat.auckland.ac.nz>


On Fri, Dec 03, 2004 at 03:04:52PM +1300, David Scott wrote:

 >As to an animal mascot, I think a New Zealand mascot is a must, and
 >suggestions of Australian ones would not be warmly received by New
 >Zealanders. (To clarify, despite the address, I am Australian.)
 >
 >My suggestion is the Kea: inquisitive and intelligent. See:
 >
 >http://www.doc.govt.nz/Conservation/001~Plants-and-Animals/001~Native-Animals/Kea.asp


Hello,

I endorse David's comment that an Australian mascot chosen for R would
not be warmly received by New Zealanders!  But at the end of the day,
Ross and Robert ought to give whatever choice the definitive yes or no.

ps. Ross has Maori origins, so a native NZ animal is a better idea
than usual. One good thing about the Kea is that another native NZ bird,
called a
Weka, is the name of a well-known machine learning software package
(also free) which was developed about 100 miles south of Auckland,
at the CS department of Waikato University.
See http://www.cs.waikato.ac.nz/~ml
However, having R indirectly associated with a similar free software package
has its good and bad points.

cheers
Thomas


Thomas W. Yee,               Telephone: 64 - 9 - 3737599 extn 86857 or 85055
Department of Statistics,    Fax:       64 - 9 - 3737000 or 3677149
University of Auckland,      E-mail:    t.yee at auckland.ac.nz
Private Bag 92019,                      yee at stat.auckland.ac.nz
Auckland 1001, New Zealand.



From tlumley at u.washington.edu  Sat Dec  4 00:22:07 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 3 Dec 2004 15:22:07 -0800 (PST)
Subject: [R] Odd underflow(?) error
In-Reply-To: <41B0D42E.7000604@icoria.com>
References: <41B0D42E.7000604@icoria.com>
Message-ID: <Pine.A41.4.61b.0412031512140.201322@homer06.u.washington.edu>

On Fri, 3 Dec 2004, William Faulk wrote:

> I'm still trying to install R on my Irix machine.  Now I have a new problem 
> that crops up during the checks.  I've found the root cause, and it's that R 
> is returning zero for certain things for reasons I don't understand.
>
> 2.225073859e-308, entered directly into R, responds "2.225074e-308".
> 2.225073858e-308 responds "0".
>
> Their negative values respond similarly, so it would appear that somewhere in 
> there is the smallest absolute value that that installation of R will hold.

Yes.  .Machine$double.xmin tells you the smallest number representable to 
full precision, which is 2.225074e-308 (I think on all machines where R 
works)

> On another machine where the checks passed, both responses are correct, not 
> just the first one.  The underflow there is significantly lower, with much 
> less accuracy, as opposed to what seems to be good accuracy on what looks 
> like the broken one.  The values there are:
>
> 2.4703282293e-324 gives 4.940656e-324
> 2.4703282292e-324 gives 0

Machines can differ in what they do with numbers smaller than 
.Machine$double.xmin. They can report zero, or they can add leading zeros 
and so lose precision.  Suppose you had a 4-digit base 10 machine with 2 
digits of exponent.  The smallest number representable to full accuracy 
would be
     1.000e-99
but by allowing the leading digits to be zero you could represent
     0.001e-99
ie, 1e-102, to one digit accuracy (these are called "denormalized" 
numbers).

My Mac laptop denormalizes, and agrees with your other computer, giving 
the smallest representable number as 4.940656e-324. It is 
.Machine$double.xmin/2^52.   This number has very few bits left to 
represent values, so for example
> (a/2^52)*1.3==(a/2^52)
[1] TRUE
where a is .Machine$double.xmin


Both your machines should be correct. I don't think we deliberately 
require denormalized numbers to work anywhere.


 	-thomas



From john.maindonald at anu.edu.au  Sat Dec  4 02:01:53 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sat, 4 Dec 2004 12:01:53 +1100
Subject: [R] Protocol for answering basic questions
In-Reply-To: <200412031219.iB3CH9sx022306@hypatia.math.ethz.ch>
References: <200412031219.iB3CH9sx022306@hypatia.math.ethz.ch>
Message-ID: <15FE07BE-4590-11D9-80AF-000A95CDA0F2@anu.edu.au>

Given the huge range of demands and interests of the people who
are subscribed, the list may well do about as much as can
reasonably be expected to address the needs of those starting out
with R.  A fair level of discipline is necessary, else the list will
become unmanageable.  Already some of my colleagues do not
subscribe because of the sheer volume.  There is a fair level of
tolerance for basic questions that are clearly and succinctly posed.
Submissions of the type "I am a total beginner - how do I start"
really do require rather prompt redirection to the FAQ and to the
official and contributed documentation.

The posting guide is already rather full.  But maybe a link could
be added to a page of the ilk: "I am new to R, where do I start?"
John Fox has I think occasionally posted responses that could
be used as a basis for such a page.  It should probably be in two
parts:
a) I am new to R and have limited statistical knowledge.
b) I am new to R, but with (I think) some reasonable level of
statistical knowledge.
If this is thought a useful idea  and there is not already such a
document, I am happy to help with it.

The discussion has ranged over a large number of issues, which
I think need to be separated:
1) There is a "getting started in R issue".
2) There are issues that relate to gaining the statistical knowledge
that will allow effective use of R.
3) There are issues of good statistical practice -- how can the
standard of use of statistical methods in application areas be
improved?
4) There are quirky points that cannot easily (or at all) be gleaned
from the documentation, and where a hint from others will be a
huge help both the questioner and almost certainly to others on
the list.  ("I am glad that you asked that.")
5) There are issues that are slowly being addressed, as part of
the ongoing development of R -- in the improvement of
documentation and in mechanisms that may make R easier and
slicker for everyone, novices and experts, to use.

Items 4 and 5 are well handled by this list, or by r-devel.  It can
and should make some contribution to 3.  There is a nether-nether
land between 1-2 and 4-5 where it can be useful.  Apart from
these contributions at the margin, 1-3 are really statistical and
R training issues, that are not well handled by this list, and
probably not by any list.

The most useful response to issues 1-2 (and, often 3) is to
direct inquirers to suitable training resources.  What is "suitable"
will however depend on personal circumstances and
geographical location.  So what do we say?  That it is good that
that this question has been asked, and that good answers are
sure to emerge slowly over the course of time?

John Maindonald.

On 3 Dec 2004, at 11:19 PM, r-help-request at stat.math.ethz.ch wrote:

> From: "Robert Brown FM CEFAS" <r.g.brown at cefas.co.uk>
> Date: 3 December 2004 7:07:51 PM
> To: <r-help at stat.math.ethz.ch>
> Subject: RE: [R] Protocol for answering basic questions
>
> . . . .
>
> I joined the community because I want technical assistance and I don't 
> question the technical skills of many of the respondants, but the 
> wider appreciation of needs of users i.e conceptual naivety.  It's 
> clear from this string that many beginners are leaving the forum and 
> this should be a cause for concern if we are seriously concerned with 
> propogating knowledge.  Unfortuantely this string does seem to 
> indicate this is not a major concern and so be it.
>
John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From tmulholland at bigpond.com  Sat Dec  4 02:20:10 2004
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Sat, 04 Dec 2004 09:20:10 +0800
Subject: [R] Label data points in scatterplot matrices
In-Reply-To: <Pine.GSO.4.58.0412031100190.10430@niflheim.rutgers.edu>
References: <Pine.GSO.4.58.0412031100190.10430@niflheim.rutgers.edu>
Message-ID: <41B110CA.9060608@bigpond.com>

This does job, but it reveals that I don't really understand panels. 
What I would like to know is how do you get the same result but without 
the warnings.

This is essentially taken from the ?pairs help.

data(USJudgeRatings)
# There are 43 observations in this data.frame
z <- 1:43
panel.text <- function(x,y,z, ...)
      {
          text(x,y,z)
      }
  panel.hist <- function(x, ...)
      {
          usr <- par("usr"); on.exit(par(usr))
          par(usr = c(usr[1:2], 0, 1.5) )
          h <- hist(x, plot = FALSE)
          breaks <- h$breaks; nB <- length(breaks)
          y <- h$counts; y <- y/max(y)
          rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
      }

pairs(USJudgeRatings[1:5], panel=panel.text,z=z,,
            cex = 1.5, pch = 24, bg="light blue",
            diag.panel=panel.hist, cex.labels = 2, font.labels=2)


Mauricio Esguerra wrote:
> Hello,
> 
> I am new to R and would like to know how to label data points in the
> matrices of scatterplots made by the pairs() command.
> To be more specific, I want to assign a number to each data point, instead
> of the small circumference that appears as a data point.
> 
> If anyone here knows if its possible to do this with R, I would greatly
> appreciate your help.
> 
> Thank you,
> 
> Mauricio Esguerra
> PhD candidate
> Chemistry and Chemical Biology Department
> Rutgers, the State University of New Jersey
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Sat Dec  4 03:08:12 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 03 Dec 2004 18:08:12 -0800
Subject: [R] how can I get the coefficients of x^0, x^1, x^2, . ,	x^6
	from expansion of (1+x+x^2)^3
In-Reply-To: <41b0c4e0.23e38b32.5a98.00c0@smtp.gmail.com>
References: <41b0c4e0.23e38b32.5a98.00c0@smtp.gmail.com>
Message-ID: <41B11C0C.4030008@pdf.com>

      Have you considered library(polynom)?  If you don't already have 
it but have R, install.packages("polynom") should get it. 

      hope this helps.  spencer graves

Peter Yang wrote:

>Hi, 
>
> 
>
>I would like to get the coefficients of x^0, x^1, x^2,  . , x^6 from
>expansion of (1+x+x^2)^3.
>
>The result should be 1, 3, 6, 7, 6, 3, 1; 
>
> 
>
>How can I calculate in R? 
>
> 
>
>You help will be greatly appreciated.
>
> 
>
>Peter 
>
> 
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From ggrothendieck at myway.com  Sat Dec  4 03:24:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri,  3 Dec 2004 21:24:07 -0500 (EST)
Subject: [R] how can I get the coefficients of x^0, x^1, x^2, . ,
	x^6 from expansion of (1+x+x^2)^3
Message-ID: <20041204022407.6ACF53983@mprdmxin.myway.com>


From:   Peter Yang <peterwyang at gmail.com>

>I would like to get the coefficients of x^0, x^1, x^2, . , x^6 from
>expansion of (1+x+x^2)^3.


# modification of DD in example(D) to support 0th derivative
DD <- function(expr,name, order = 0) {
   if(order == 0) 
	expr
   else DD(D(expr, name), name, order - 1)
}

# take symbolic derivatives, evaluate at 0 and divide by factorial n
sapply(0:6, function(i) eval(DD(e,"x",i),list(x=0)))/factorial(0:6)



From ggrothendieck at myway.com  Sat Dec  4 03:27:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri,  3 Dec 2004 21:27:07 -0500 (EST)
Subject: [R] how can I get the coefficients of x^0, x^1, x^2, . ,
	x^6 from expansion of (1+x+x^2)^3
Message-ID: <20041204022707.81D5E3983@mprdmxin.myway.com>


 

From: Peter Yang <peterwyang at gmail.com>

>I would like to get the coefficients of x^0, x^1, x^2, . , x^6 from
>expansion of (1+x+x^2)^3.


# modification of DD in example(D) to support 0th derivative
DD <- function(expr,name, order = 0) {
if(order == 0) 
     expr
else DD(D(expr, name), name, order - 1)
}

# take symbolic derivatives, evaluate at 0 and divide by factorial n
sapply(0:6, function(i) eval(DD(e,"x",i),list(x=0)))/factorial(0:6)


By the way, e in the above is your expression, in this case:

 e <- expression((1+x+x^2)^3)



From MSchwartz at MedAnalytics.com  Sat Dec  4 03:21:22 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 03 Dec 2004 20:21:22 -0600
Subject: [R] How to wrap or split labels on plot
In-Reply-To: <41B0C5EF.5060203@pdq.com>
References: <41B0C5EF.5060203@pdq.com>
Message-ID: <1102126882.431.103.camel@horizons.localdomain>

On Fri, 2004-12-03 at 15:00 -0500, Heather J. Branton wrote:
> Dear R gurus,
> 
> I want to wrap labels that are too long for a plot. I have looked at 
> strsplit(), substr(), nchar(), and strwrap(). I think it's some 
> combination but I'm having difficulty trying to figure out the right 
> combo. I think I need to create some new matrix containing the labels 
> already split, though I'm not sure if maybe there is a quick and dirty 
> way to address this without my wandering around the block.
> 
> I am using R 1.9.1, Windows XP.  (Note:  we are currently in the midst 
> of a big project and probably won't upgrade to R 2.0.1 for another 
> couple of weeks -- *unless* that's what I need to do to address these 
> issues.)
> 
> Here is my script with two label options at the bottom that are not working.
> 
> # Settings
> win.graph(width=8, height=8, pointsize=10)
> 
> # Read in data
> test <- matrix(data=c(2.52,9.5,3.07,2.5,1.99,8.95), nrow = 6, byrow=TRUE)
> 
> # Read in and attach labels (names) to data
> rownames(test) <- c("Mount Pleasant","Jordan","Oil City","Pleasant 
> Valley","Village of Lake Isabella","Rosebush")
> 
> # Set plot limits:
> xmax <- nrow(test)
> nvec <- ncol(test)
> ymax <- ceiling(max(test))
> yinc <- 1
> 
> # Generate Pareto order
> test <- test[order(test[,1],decreasing=TRUE),]
> 
> # Set color palette
> MyCols <- rep(c("lightcyan","cornsilk","lavender"), each = xmax)
> 
> # Adjust the margins
> par(mar = c(7, 5, 6, 3))
> 
> # Bar graph
> mp <- barplot(test, beside = TRUE,
>     col = MyCols,
>     axisnames = FALSE,
>     names.arg = rep(names(test),nvec),
>     las = 2,
>     cex.names = 0.75,
>     ylab = "IXYV",
>     ylim = c(0,ymax),
>     yaxt = "n")
> 
> # Set up the y axis tick marks and labels
> ifelse (ymax<=10,decpt <- 2,decpt <- 0)
> ticks <- seq(0, ymax, yinc)
> axis(2, at = ticks, las = 1,
>      labels = formatC(ticks, format = "f", digits = decpt))
> 
> # Draw a box around the whole thing
> box()
> 
> # Draw the x axis labels
> mtext(side = 1, at = rowMeans(mp)-.2, line = .5, las=2, text = 
> strsplit(names(test)," "))
> mtext(side = 1, at = rowMeans(mp), line = .5, las=2, text = 
> strwrap(names(test),7))
> mtext(side = 1, line = 5.5, text = "Division")
> 
> # Draw titles
> title(main="Central", outer=F, font.main=4, line=4)
> title(main="IXYV by Division", outer=F, font.main=2, line=2.5)


Heather,

There is likely to be more than one approach, but the one that I
generally use is to explicitly put a newline character "\n" into the
plot labels where required. So, in this case, you could do something
like:

names(test) <- c("Mount\nPleasant","Jordan","Oil City",
                 "Pleasant\nValley",
                 "Village of\nLake Isabella",
                 "Rosebush")


Also, there are some confusing things in your code, which I suspect may
tie back to your test data versus the actual data you are using. If I am
missing something here, you might want to clarify that, since things
like your colors and other things don't entirely make sense.

Here is something of a simplified approach using the test data as you
have it:

# Test can be a vector
test <- c(2.52, 9.5, 3.07, 2.5, 1.99, 8.95)

names(test) <- c("Mount\nPleasant","Jordan","Oil City",
                 "Pleasant\nValley",
                 "Village of\nLake Isabella",
                 "Rosebush")

# Use sort here
test <- sort(test, decreasing = TRUE)

ymax <- ceiling(max(test))

par(mar = c(7, 5, 6, 3))

# Note that you can use the names here for names.arg
# As a result of the "\n", the titles will print on two lines
mp <- barplot(test, names.arg = names(test),  
              cex.names = 0.8, ylab = "IXYV", 
              yaxt = "n", ylim = c(0, ymax))

ticks <- seq(0, ymax, 1)
axis(2, at = ticks, las = 1,
     labels = formatC(ticks, format = "f",
                      digits = ifelse(ymax <= 10, 2, 0)))

box()

mtext(side = 1, line = 3.5, text = "Division")

# you can combine the two title() calls into one mtext() call
mtext(side = 3, text = c("Central", "IXYV by Division"),
      font = c(4, 2), line = c(4, 2.5))



If your actual data is a more complex matrix, adjust the above
accordingly.

HTH,

Marc Schwartz



From spencer.graves at pdf.com  Sat Dec  4 03:56:01 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 03 Dec 2004 18:56:01 -0800
Subject: [R] Computing the minimal polynomial or, at least, its degree
In-Reply-To: <0I85002OUH6RSO@jhuml1.jhmi.edu>
References: <0I85002OUH6RSO@jhuml1.jhmi.edu>
Message-ID: <41B12741.2030601@pdf.com>

	  How about the following:


library(polynom)
help(package="polynom")
A <- diag(c(1:2, 2))
eigVals <- eigen(A)$values
multEig <- table(eigVals)
k <- length(multEig)
ratPoly <- minPoly <- 1
for(i in 1:k){
   poly.i <- polynomial(c(-as.numeric(names(multEig)[i]), 1))
   minPoly <- (minPoly*poly.i)
   if(multEig[i]>1)
     ratPoly <- (ratPoly*poly.i^(multEig[i]-1))
}

 > minPoly
2 - 3*x + x^2
 > ratPoly
-2 + x
 >
	  hope this helps.  spencer graves

###############################
Spencer,

One could do this by a brute force approach. Suppose A is an nxn matrix, and
the distinct eigenvalues are: lambda_1, ..., lambda_k, with multiplicities
m_1, ..., m_k, such that they sum to n. Then the characteristic polynomial
is:
C(lambda) = \prod_i (lambda - lambda_i)^{m_i}
The minimal polynomial is given by:
M(lambda) = \prod_i (lambda - lambda_i)^{p_i},
where 1 \leq p_i \leq m_i.
So, one could run through all possible p_i, starting with the smallest
degree polynomial (within constraint), and stop when we find one that
exactly divides C(lambda).

Is there a cleverer way to do this?

Ravi.
#################################################
      Have you looked at library(polynom)?  Will that with
unique(eigen(A)$values) allow you to compute what you want?

      hope this helps.
      spencer graves

Ravi Varadhan wrote:

>Hi,
>
> 
>
>I would like to know whether there exist algorithms to compute the
>coefficients or, at least, the degree of the minimal polynomial of a square
>matrix A (over the field of complex numbers)? I don't know whether this
>would require symbolic computation.  If not, has any of the algorithms been
>implemented in R?  
>
> 
>
>Thanks very much,
>
>Ravi.
>
> 
>
>P.S.  Just for the sake of completeness, a minimal polynomial is a monic
>polynomial (whose leading coefficient is unity) of least degree, which
>divides all the annihilating polynomial of A. In particular, the minimal
>polynomial divides the characteristic polynomial.  Knowing the degree of the
>minimal polynomial is useful in characterizing the convergence properties of
>a certain class of numerical schemes for iteratively solving linear (and
>nonlinear) system of equations.
>
> 
>
>--------------------------------------------------------------------------
>
>Ravi Varadhan, Ph.D.
>
>Assistant Professor,  The Center on Aging and Health
>
>Division of Geriatric Medicine and Gerontology
>
>Johns Hopkins Univerisity
>
>Ph: (410) 502-2619
>
>Fax: (410) 614-9625
>
>Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu
>
>--------------------------------------------------------------------------
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From spencer.graves at pdf.com  Sat Dec  4 04:01:57 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 03 Dec 2004 19:01:57 -0800
Subject: [R] how can I get the coefficients of x^0, x^1, x^2, . ,	x^6
	from expansion of (1+x+x^2)^3
In-Reply-To: <20041204022707.81D5E3983@mprdmxin.myway.com>
References: <20041204022707.81D5E3983@mprdmxin.myway.com>
Message-ID: <41B128A5.7060508@pdf.com>

      Alternatively, how about the following: 

library(polynom)
coefficients(polynomial(c(1,1,1))^3)
[1] 1 3 6 7 6 3 1

      hope this helps.  spencer graves

Gabor Grothendieck wrote:

> 
>
>From: Peter Yang <peterwyang at gmail.com>
>
>  
>
>>I would like to get the coefficients of x^0, x^1, x^2, . , x^6 from
>>expansion of (1+x+x^2)^3.
>>    
>>
>
>
># modification of DD in example(D) to support 0th derivative
>DD <- function(expr,name, order = 0) {
>if(order == 0) 
>     expr
>else DD(D(expr, name), name, order - 1)
>}
>
># take symbolic derivatives, evaluate at 0 and divide by factorial n
>sapply(0:6, function(i) eval(DD(e,"x",i),list(x=0)))/factorial(0:6)
>
>
>By the way, e in the above is your expression, in this case:
>
> e <- expression((1+x+x^2)^3)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From ggrothendieck at myway.com  Sat Dec  4 04:12:51 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri,  3 Dec 2004 22:12:51 -0500 (EST)
Subject: [R] vector to matrix transformation
Message-ID: <20041204031251.8A0243975@mprdmxin.myway.com>



If v is a vector, as.matrix(v) and t(v) give row and column 
matrices and matrix(v,nrow=nr, ncol=nc) gives a matrix with 
nr rows and nc columns such that the vector fills the first 
column, then the second, etc.  You only have to provide nr
or nc in most cases.

If m is a matrix c(m) is a vector formed by stringing out
the columns one after another.  as.vector is similar.


Date:   Fri, 3 Dec 2004 11:39:57 +0100 
From:   Gwenael Jacob <gwenael.jacob at wsl.ch>
To:   <R-help at stat.math.ethz.ch> 
Subject:   [R] vector to matrix transformation 

 
Dear,

Some analysis (linear regression) can only be 
done from a vectorized dataset whereas others 
require a matrix (Mantel tests). I use the two 
analyses and thus need to format my data in 
matrix and vector. I spent some time trying to 
solve the problem and I just gave up. Did anyone 
knows how to transform a matrix into a vector and 
back-transform a vector into a matrix?

Thanks by advance,
Gwenal Jacob
-- 

-------------------------------------
Gwenal JACOB
Division Biodiversity
Swiss Federal Research Institute WSL
Zuercherstrasse 111/Postfach
CH-8903 Birmensdorf

SWITZERLAND

Phone : ++41 1 7392 504
Fax : ++41 1 7392 215



From R.P.Clement at westminster.ac.uk  Sat Dec  4 09:38:02 2004
From: R.P.Clement at westminster.ac.uk (Ross Clement)
Date: Sat, 4 Dec 2004 08:38:02 GMT
Subject: [R] Protocol for answering basic questions
Message-ID: <200412040838.iB48c2Cx025702@tiger.wmin.ac.uk>

I'm a recent subscriber to the list. I was very impressed by the 
quality of people subscribing to the list, including the authors
of all the books on R thateither I own or are present in my local
uni library. However, I was astonished by the volume of messages.
I have set up a folder for R messages, route the messages there
automatically, and browse it at times of low panic levels.

Personally, I think this list would be much better served by 
a standard bulletin board. The list could be broken down into 
a number of topics (e.g. Newbie questions, etc. etc.), the messages
would be stored under threads so that people could choose to read
or not read based on the topic. 'Sticky' threads could be left at the
top so that new subscriberts would see them, and people who only want
to follow a very few threads could tick the box for email alerts. Finally,
there could be a search box enabling people to search out past answers
(I know that this is possible now).

An example board is: 

http://www.linuxquestions.org/questions/index.php

Comments?

Cheers,

Ross-c



From ligges at statistik.uni-dortmund.de  Sat Dec  4 11:52:54 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 04 Dec 2004 11:52:54 +0100
Subject: [R] Label data points in scatterplot matrices
In-Reply-To: <41B110CA.9060608@bigpond.com>
References: <Pine.GSO.4.58.0412031100190.10430@niflheim.rutgers.edu>
	<41B110CA.9060608@bigpond.com>
Message-ID: <41B19706.4020007@statistik.uni-dortmund.de>

Tom Mulholland wrote:
> This does job, but it reveals that I don't really understand panels. 
> What I would like to know is how do you get the same result but without 
> the warnings.
> 
> This is essentially taken from the ?pairs help.
> 
> data(USJudgeRatings)
> # There are 43 observations in this data.frame
> z <- 1:43
> panel.text <- function(x,y,z, ...)
>      {
>          text(x,y,z)
>      }
>  panel.hist <- function(x, ...)
>      {
>          usr <- par("usr"); on.exit(par(usr))
>          par(usr = c(usr[1:2], 0, 1.5) )
>          h <- hist(x, plot = FALSE)
>          breaks <- h$breaks; nB <- length(breaks)
>          y <- h$counts; y <- y/max(y)
>          rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
>      }
> 
> pairs(USJudgeRatings[1:5], panel=panel.text,z=z,,
>            cex = 1.5, pch = 24, bg="light blue",
>            diag.panel=panel.hist, cex.labels = 2, font.labels=2)


Well, in principle you cannot easily (it's a warning, not an errror, BTW).
But you can fake a little bit by, e.g., renaming "z" to "pch":



data(USJudgeRatings)
# There are 43 observations in this data.frame
z <- 1:43
panel.text <- function(x,y,pch, ...)
      {
          text(x,y,pch)
      }
  panel.hist <- function(x, ...)
      {
          usr <- par("usr"); on.exit(par(usr))
          par(usr = c(usr[1:2], 0, 1.5) )
          h <- hist(x, plot = FALSE)
          breaks <- h$breaks; nB <- length(breaks)
          y <- h$counts; y <- y/max(y)
          rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
      }

pairs(USJudgeRatings[1:5], panel=panel.text,
            cex = 1.5, pch = z, bg="light blue",
            diag.panel=panel.hist, cex.labels = 2, font.labels=2)



Uwe Ligges



> 
> Mauricio Esguerra wrote:
> 
>> Hello,
>>
>> I am new to R and would like to know how to label data points in the
>> matrices of scatterplots made by the pairs() command.
>> To be more specific, I want to assign a number to each data point, 
>> instead
>> of the small circumference that appears as a data point.
>>
>> If anyone here knows if its possible to do this with R, I would greatly
>> appreciate your help.
>>
>> Thank you,
>>
>> Mauricio Esguerra
>> PhD candidate
>> Chemistry and Chemical Biology Department
>> Rutgers, the State University of New Jersey
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Dec  4 12:14:03 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 04 Dec 2004 12:14:03 +0100
Subject: [R] How to wrap or split labels on plot
In-Reply-To: <1102126882.431.103.camel@horizons.localdomain>
References: <41B0C5EF.5060203@pdq.com>
	<1102126882.431.103.camel@horizons.localdomain>
Message-ID: <41B19BFB.8080504@statistik.uni-dortmund.de>

Marc Schwartz wrote:

> On Fri, 2004-12-03 at 15:00 -0500, Heather J. Branton wrote:
> 
>>Dear R gurus,
>>
>>I want to wrap labels that are too long for a plot. I have looked at 
>>strsplit(), substr(), nchar(), and strwrap(). I think it's some 
>>combination but I'm having difficulty trying to figure out the right 
>>combo. I think I need to create some new matrix containing the labels 
>>already split, though I'm not sure if maybe there is a quick and dirty 
>>way to address this without my wandering around the block.
>>
>>I am using R 1.9.1, Windows XP.  (Note:  we are currently in the midst 
>>of a big project and probably won't upgrade to R 2.0.1 for another 
>>couple of weeks -- *unless* that's what I need to do to address these 
>>issues.)
>>
>>Here is my script with two label options at the bottom that are not working.
>>
>># Settings
>>win.graph(width=8, height=8, pointsize=10)
>>
>># Read in data
>>test <- matrix(data=c(2.52,9.5,3.07,2.5,1.99,8.95), nrow = 6, byrow=TRUE)
>>
>># Read in and attach labels (names) to data
>>rownames(test) <- c("Mount Pleasant","Jordan","Oil City","Pleasant 
>>Valley","Village of Lake Isabella","Rosebush")
>>
>># Set plot limits:
>>xmax <- nrow(test)
>>nvec <- ncol(test)
>>ymax <- ceiling(max(test))
>>yinc <- 1
>>
>># Generate Pareto order
>>test <- test[order(test[,1],decreasing=TRUE),]
>>
>># Set color palette
>>MyCols <- rep(c("lightcyan","cornsilk","lavender"), each = xmax)
>>
>># Adjust the margins
>>par(mar = c(7, 5, 6, 3))
>>
>># Bar graph
>>mp <- barplot(test, beside = TRUE,
>>    col = MyCols,
>>    axisnames = FALSE,
>>    names.arg = rep(names(test),nvec),
>>    las = 2,
>>    cex.names = 0.75,
>>    ylab = "IXYV",
>>    ylim = c(0,ymax),
>>    yaxt = "n")
>>
>># Set up the y axis tick marks and labels
>>ifelse (ymax<=10,decpt <- 2,decpt <- 0)
>>ticks <- seq(0, ymax, yinc)
>>axis(2, at = ticks, las = 1,
>>     labels = formatC(ticks, format = "f", digits = decpt))
>>
>># Draw a box around the whole thing
>>box()
>>
>># Draw the x axis labels
>>mtext(side = 1, at = rowMeans(mp)-.2, line = .5, las=2, text = 
>>strsplit(names(test)," "))
>>mtext(side = 1, at = rowMeans(mp), line = .5, las=2, text = 
>>strwrap(names(test),7))
>>mtext(side = 1, line = 5.5, text = "Division")
>>
>># Draw titles
>>title(main="Central", outer=F, font.main=4, line=4)
>>title(main="IXYV by Division", outer=F, font.main=2, line=2.5)
> 
> 
> 
> Heather,
> 
> There is likely to be more than one approach, but the one that I
> generally use is to explicitly put a newline character "\n" into the
> plot labels where required. So, in this case, you could do something
> like:
> 
> names(test) <- c("Mount\nPleasant","Jordan","Oil City",
>                  "Pleasant\nValley",
>                  "Village of\nLake Isabella",
>                  "Rosebush")
> 


... or automatically by combining strwrap() and paste():

   names(test) <-
     sapply(lapply(names(test), strwrap, 15),
       paste, collapse = "\n")

Uwe Ligges



> Also, there are some confusing things in your code, which I suspect may
> tie back to your test data versus the actual data you are using. If I am
> missing something here, you might want to clarify that, since things
> like your colors and other things don't entirely make sense.
> 
> Here is something of a simplified approach using the test data as you
> have it:
> 
> # Test can be a vector
> test <- c(2.52, 9.5, 3.07, 2.5, 1.99, 8.95)
> 
> names(test) <- c("Mount\nPleasant","Jordan","Oil City",
>                  "Pleasant\nValley",
>                  "Village of\nLake Isabella",
>                  "Rosebush")
> 
> # Use sort here
> test <- sort(test, decreasing = TRUE)
> 
> ymax <- ceiling(max(test))
> 
> par(mar = c(7, 5, 6, 3))
> 
> # Note that you can use the names here for names.arg
> # As a result of the "\n", the titles will print on two lines
> mp <- barplot(test, names.arg = names(test),  
>               cex.names = 0.8, ylab = "IXYV", 
>               yaxt = "n", ylim = c(0, ymax))
> 
> ticks <- seq(0, ymax, 1)
> axis(2, at = ticks, las = 1,
>      labels = formatC(ticks, format = "f",
>                       digits = ifelse(ymax <= 10, 2, 0)))
> 
> box()
> 
> mtext(side = 1, line = 3.5, text = "Division")
> 
> # you can combine the two title() calls into one mtext() call
> mtext(side = 3, text = c("Central", "IXYV by Division"),
>       font = c(4, 2), line = c(4, 2.5))
> 
> 
> 
> If your actual data is a more complex matrix, adjust the above
> accordingly.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Sat Dec  4 12:45:24 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 4 Dec 2004 12:45:24 +0100
Subject: [R] Odd underflow(?) error
In-Reply-To: <Pine.A41.4.61b.0412031512140.201322@homer06.u.washington.edu>
References: <41B0D42E.7000604@icoria.com>
	<Pine.A41.4.61b.0412031512140.201322@homer06.u.washington.edu>
Message-ID: <16817.41812.164656.71501@gargle.gargle.HOWL>

>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>     on Fri, 3 Dec 2004 15:22:07 -0800 (PST) writes:

    TL> On Fri, 3 Dec 2004, William Faulk wrote:
    >> I'm still trying to install R on my Irix machine.  Now I have a new problem 
    >> that crops up during the checks.  I've found the root cause, and it's that R 
    >> is returning zero for certain things for reasons I don't understand.
    >> 
    >> 2.225073859e-308, entered directly into R, responds "2.225074e-308".
    >> 2.225073858e-308 responds "0".
    >> 
    >> Their negative values respond similarly, so it would appear that somewhere in 
    >> there is the smallest absolute value that that installation of R will hold.

    TL> Yes.  .Machine$double.xmin tells you the smallest number representable to 
    TL> full precision, which is 2.225074e-308 (I think on all machines where R 
    TL> works)

    >> On another machine where the checks passed, both responses are correct, not 
    >> just the first one.  The underflow there is significantly lower, with much 
    >> less accuracy, as opposed to what seems to be good accuracy on what looks 
    >> like the broken one.  The values there are:
    >> 
    >> 2.4703282293e-324 gives 4.940656e-324
    >> 2.4703282292e-324 gives 0

    TL> Machines can differ in what they do with numbers smaller than 
    TL> .Machine$double.xmin. They can report zero, or they can add leading zeros 
    TL> and so lose precision.  Suppose you had a 4-digit base 10 machine with 2 
    TL> digits of exponent.  The smallest number representable to full accuracy 
    TL> would be
    TL> 1.000e-99
    TL> but by allowing the leading digits to be zero you could represent
    TL> 0.001e-99
    TL> ie, 1e-102, to one digit accuracy (these are called "denormalized" 
    TL> numbers).

    TL> My Mac laptop denormalizes, and agrees with your other computer, giving 
    TL> the smallest representable number as 4.940656e-324. It is 
    TL> .Machine$double.xmin/2^52.   This number has very few bits left to 
    TL> represent values, so for example
    >> (a/2^52)*1.3==(a/2^52)
    TL> [1] TRUE
    TL> where a is .Machine$double.xmin

(very nice explanation, thanks Thomas!)


    TL> Both your machines should be correct. I don't think we deliberately 
    TL> require denormalized numbers to work anywhere.

yes, indeed.
I can imagine that some of regression tests (aka "validation" !)
implicitly use some property -- but as Thomas said, that's not
deliberate (and a buglet in our tests).

William, could you move this topic from R-help to R-devel and
give more specifics about what is failing for your installation?

Martin



From maechler at stat.math.ethz.ch  Sat Dec  4 15:19:40 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 4 Dec 2004 15:19:40 +0100
Subject: [R] How about a mascot for R?
In-Reply-To: <Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>
	<20041202205022.GJ18464@hortresearch.co.nz>
	<Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>
Message-ID: <16817.51068.638596.414032@gargle.gargle.HOWL>

>>>>> "DScottNZ" == David Scott <d.scott at auckland.ac.nz>
>>>>>     on Fri, 3 Dec 2004 15:04:52 +1300 (NZDT) writes:

	  <........>

    DScottNZ> As to an animal mascot, I think a New Zealand
    DScottNZ> mascot is a must,

well, thinking that "must" is bit strong, I agree that 
I have had the same idea (NZ animal) before your post.
I first thought of the obvious Kiwi, but hoping for something
more beautiful had been googling around for "New Zealand animals",
then had been side tracted by the Kakapo which I found nice,
intriguing, but in his fight against extinction didn't seem to
fit to my notion of R..

    DScottNZ> and suggestions of Australian
    DScottNZ> ones would not be warmly received by New
    DScottNZ> Zealanders. (To clarify, despite the address, I am
    DScottNZ> Australian.)

    DScottNZ> My suggestion is the Kea: inquisitive and intelligent. See:

    DScottNZ> http://www.doc.govt.nz/Conservation/001~Plants-and-Animals/001~Native-Animals/Kea.asp

Yesterday, when I posed the question at our group's coffee
break, someone also immediately mentioned the Kea. 

Given all the "things" I've read in the mean time, I did like
the "R-madillo" from it's name, but then, from an aesthetic
point of view, I'd also vote for the Kea.  OTOH, before getting
into more, I'd also like to hear from R & R -- particularly
about the "must" part...

Another thought that I think hasn't been raise: The mascot
should typically also be representable as a monochrome "line
drawing" (such as the O'Reilly book covers), and also be
somewhat easily identifiable from a relatively small 
(eg. 32 x 32 ?) icons, since presumably it would eventually
replace the current R logo, at least in some places.

Also, is anyone willing to put up a web site collecting the
proposals and maybe also allowing to collect votes?
Though, I'm not at all sure we will reach that state.

Martin Maechler, ETH Zurich



From rolf at math.unb.ca  Sat Dec  4 15:49:48 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 4 Dec 2004 10:49:48 -0400 (AST)
Subject: [R] Excel *.xls files, RODBC
Message-ID: <200412041449.iB4EnmCg008102@erdos.math.unb.ca>


I gather from reading the back-issues of r-help that it should be
possible (modulo a number of caveats) to read an excel (yuck!) file
into R using RODBC.  I have obtained and installed ODBC and the RODBC
package, but cannot for the life of me figure out how to go about
it.  Can anyone give me a simple recipe?

I have an excel file on cdrom, say:

	/mnt/cdrom/melvin.xls

I have started R and loaded the RODBC package.  I want to create
a data frame ``melvin'' by reading in /mnt/cdrom/melvin.xls.
What (in monosyllables --- step by step) do I do next?

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From ccleland at optonline.net  Sat Dec  4 16:10:01 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 04 Dec 2004 10:10:01 -0500
Subject: [R] Excel *.xls files, RODBC
In-Reply-To: <200412041449.iB4EnmCg008102@erdos.math.unb.ca>
References: <200412041449.iB4EnmCg008102@erdos.math.unb.ca>
Message-ID: <41B1D349.9030100@optonline.net>

   The following works for me under WinXP Pro to create "myframe" as a 
data frame:

library(RODBC)
z <- odbcConnectExcel("c:/myfolder/mydata.xls")
myframe <- sqlFetch(z, "Sheet1")
close(z)

   Are you indicating the name of the worksheet you want within the 
*.xls file?  I suspect there could be additional issues on a non-Windows 
OS that I don't know about.

hope this helps,

Chuck Cleland

Rolf Turner wrote:
> I gather from reading the back-issues of r-help that it should be
> possible (modulo a number of caveats) to read an excel (yuck!) file
> into R using RODBC.  I have obtained and installed ODBC and the RODBC
> package, but cannot for the life of me figure out how to go about
> it.  Can anyone give me a simple recipe?
> 
> I have an excel file on cdrom, say:
> 
> 	/mnt/cdrom/melvin.xls
> 
> I have started R and loaded the RODBC package.  I want to create
> a data frame ``melvin'' by reading in /mnt/cdrom/melvin.xls.
> What (in monosyllables --- step by step) do I do next?

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From anne.piotet at urbanet.ch  Thu Dec  2 21:15:41 2004
From: anne.piotet at urbanet.ch (Anne)
Date: Thu, 2 Dec 2004 21:15:41 +0100
Subject: [R] A possible way to reduce basic questions
References: <20041202012336.D7E4A3986@mprdmxin.myway.com>
	<06C2E003-4486-11D9-AC51-000A95B2B140@sanger.ac.uk>
Message-ID: <000901c4da19$94e54ef0$6c00a8c0@mtd4>

What about starting a database? I know it is a lot of work but one of the
difficulties one encounter with R is taht i can be diffcult to know where to
look for answers...I do agree that a basic list will tend to be a write only
list! (and I take the opportunity here tp thank all of you for your patient
answers !)

Anne

----- Original Message ----- 
From: "Tim Cutts" <tjrc at sanger.ac.uk>
To: <ggrothendieck at myway.com>
Cc: <r-help at stat.math.ethz.ch>; <bitwrit at ozemail.com.au>
Sent: Thursday, December 02, 2004 6:17 PM
Subject: Re: [R] A possible way to reduce basic questions


>
> On 2 Dec 2004, at 1:23 am, Gabor Grothendieck wrote:
>
> >
> > Jim Lemon <bitwrit <at> ozemail.com.au> writes:
> >
> >> I have been thinking about how to reduce the number of basic
> >> questions that
> >> elicit the ...ahem... robust debate that has occurred about how to
> >> answer
> >
> >
> > The traffic on r-help could be reduced by creating a second list where
> > more elementary questions are asked.
>
> But how many people here would read it, and help the novices (like me)
> out?  There is always the danger that novice lists just become
> write-only lists.
>
> Tim
>
> -- 
> Dr Tim Cutts
> Informatics Systems Group, Wellcome Trust Sanger Institute
> GPG: 1024D/E3134233 FE3D 6C73 BBD6 726A A3F5  860B 3CDD 3F56 E313 4233
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From kee at wehi.EDU.AU  Sat Dec  4 17:48:53 2004
From: kee at wehi.EDU.AU (Thuan-Jin Kee)
Date: Sun, 5 Dec 2004 03:48:53 +1100 (EST)
Subject: [R] Getting R to emit an image file as a pipe or Base64 
	stream: Mac OSX 10.3 - R 2.0.1
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E3C3@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E3C3@usrymx25.merck.com>
Message-ID: <61401.211.28.209.197.1102178933.squirrel@211.28.209.197>

That's right,

Thanks for your replies.

The reason why I started X11.app first is because Mac OS 10.3.5 doesn't
launch X on startup, and instead uses its own Aqua/Quartz gui to render
most windows until you specifically ask for X.

The issue we're looking at is how to get R to emit the file as base64, or
to somehow hand me a pipe. Is there a way to redirect the file in jpeg()
to stdout?

still hacking.

Jin Kee

>> From: Yuandan Zhang
>>
>> If you want to call R from perl, why don't you do a simple
>> system call like:
>>
>> $callR="/usr/loca/bin/R CMD BATCH plotscript.R";
>> system ($callR);
>>
>> It is not necessary to start X display if anything can be
>> done in background
>
> But the problem is jpeg()/png() are not available unless an X display is
> available to the R process (one of the FAQs).
>
> Andy
>
>> On Fri, 3 Dec 2004 12:07:24 +1100 (EST)
>> "Thuan-Jin Kee" <kee at wehi.EDU.AU> wrote:
>>
>> > Hi All,
>> >
>> > Anybody know how to make R emit base64 encoded text in some
>> way that perl
>> > can grab it, instead of planting a file on your harddrive
>> when calling
>> > JPEG or PNG?
>> > I've managed to get these scripts to work and put a file on
>> the harddisk
>> >
>> > #!/usr/bin/perl -Wall
>> > # by jin kee. a simple script to demonstrate
>> > # the needed steps to get R to emit a jpeg.
>> >
>> > use strict;
>> >
>> > my($callR, $callRold);
>> >
>> > # need to start X if is isn't already started.
>> > `open /Applications/Utilities/X11.app`;
>> >
>> >
>> > #need to get let the R program know where to look
>> > #for the display immediately before calling
>> > #the R executible.
>> > $callR =<<MARKER;
>> > DISPLAY=:0.0; export DISPLAY;
>> > /usr/bin/R --vanilla <plotscript.R;
>> > MARKER
>> >
>> > system($callR);
>> >
>> > # end script
>> >
>> > #!/usr/bin/R
>> > peg("~/Desktop/test.jpg");
>> > plot(rnorm(100));
>> > dev.off();
>> > q(save = "no");
>> >
>> >
>> > My sysadmin says that the apache user can't write to the disk due to
>> > security policy, so he wants to know if I can emit the jpeg
>> as a base64
>> > stream and embedd it into the dynamically generated tag
>> using a DATA tag
>> > to inline the image.
>> >
>> > http://www.elf.org/essay/inline-image.html
>> >
>> > http://www.faqs.org/rfcs/rfc2397.html
>> >
>> > i've tried searching the R-project.org site and
>> help.search() and no luck.
>> >
>> > Yours
>> > Jin
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>> --
>>
>> --
>> Yuandan
>> Zhang, PhD
>>
>> Animal Genetics and Breeding Unit
>> The University of New England
>> Armidale, NSW, Australia, 2351
>>
>> E-mail:   yzhang4 at metz.une.edu.au
>> Phone:    (61) 02 6773 3786
>> Fax:      (61) 02 6773 3266
>> http://agbu.une.edu.au
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>   AGBU is a joint venture of NSW Primary Industries
>>   and The University of New England to undertake
>>   genetic R&D for Australia's Livestock Industries
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ozric at web.de  Sat Dec  4 18:07:29 2004
From: ozric at web.de (Christian Schulz)
Date: Sat, 04 Dec 2004 18:07:29 +0100
Subject: [R] How about a mascot for R?
In-Reply-To: <16817.51068.638596.414032@gargle.gargle.HOWL>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>	<20041202205022.GJ18464@hortresearch.co.nz>	<Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>
	<16817.51068.638596.414032@gargle.gargle.HOWL>
Message-ID: <41B1EED1.9050704@web.de>

hi,

some weka guys  use "kea" (..with the same intenion to the bird in nz!)
for a automatic keyphrase extraction tool (java).

http://www.nzdl.org/Kea/

regards, christian


Martin Maechler wrote:

>>>>>>"DScottNZ" == David Scott <d.scott at auckland.ac.nz>
>>>>>>    on Fri, 3 Dec 2004 15:04:52 +1300 (NZDT) writes:
>>>>>>            
>>>>>>
>
>	  <........>
>
>    DScottNZ> As to an animal mascot, I think a New Zealand
>    DScottNZ> mascot is a must,
>
>well, thinking that "must" is bit strong, I agree that 
>I have had the same idea (NZ animal) before your post.
>I first thought of the obvious Kiwi, but hoping for something
>more beautiful had been googling around for "New Zealand animals",
>then had been side tracted by the Kakapo which I found nice,
>intriguing, but in his fight against extinction didn't seem to
>fit to my notion of R..
>
>    DScottNZ> and suggestions of Australian
>    DScottNZ> ones would not be warmly received by New
>    DScottNZ> Zealanders. (To clarify, despite the address, I am
>    DScottNZ> Australian.)
>
>    DScottNZ> My suggestion is the Kea: inquisitive and intelligent. See:
>
>    DScottNZ> http://www.doc.govt.nz/Conservation/001~Plants-and-Animals/001~Native-Animals/Kea.asp
>
>Yesterday, when I posed the question at our group's coffee
>break, someone also immediately mentioned the Kea. 
>
>Given all the "things" I've read in the mean time, I did like
>the "R-madillo" from it's name, but then, from an aesthetic
>point of view, I'd also vote for the Kea.  OTOH, before getting
>into more, I'd also like to hear from R & R -- particularly
>about the "must" part...
>
>Another thought that I think hasn't been raise: The mascot
>should typically also be representable as a monochrome "line
>drawing" (such as the O'Reilly book covers), and also be
>somewhat easily identifiable from a relatively small 
>(eg. 32 x 32 ?) icons, since presumably it would eventually
>replace the current R logo, at least in some places.
>
>Also, is anyone willing to put up a web site collecting the
>proposals and maybe also allowing to collect votes?
>Though, I'm not at all sure we will reach that state.
>
>Martin Maechler, ETH Zurich
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From p.dalgaard at biostat.ku.dk  Sat Dec  4 18:19:06 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Dec 2004 18:19:06 +0100
Subject: [R] Getting R to emit an image file as a pipe or Base64 stream:
	Mac OSX 10.3 - R 2.0.1
In-Reply-To: <61401.211.28.209.197.1102178933.squirrel@211.28.209.197>
References: <3A822319EB35174CA3714066D590DCD50994E3C3@usrymx25.merck.com>
	<61401.211.28.209.197.1102178933.squirrel@211.28.209.197>
Message-ID: <x2mzwurng5.fsf@biostat.ku.dk>

"Thuan-Jin Kee" <kee at wehi.EDU.AU> writes:

> That's right,
> 
> Thanks for your replies.
> 
> The reason why I started X11.app first is because Mac OS 10.3.5 doesn't
> launch X on startup, and instead uses its own Aqua/Quartz gui to render
> most windows until you specifically ask for X.
> 
> The issue we're looking at is how to get R to emit the file as base64, or
> to somehow hand me a pipe. Is there a way to redirect the file in jpeg()
> to stdout?

This is really nasty, but on linuxen, you can do things like

bitmap(file="/proc/self/fd/1",type="png256");plot(0);dev.off()

which gives you several screenfulls of junk, the first characters of
which is "PNG"... 

A cleaner way would be if the file argument to bitmap() could be a
connection, but it can't (and it is nontrivial to change).
 
> still hacking.
> 
> Jin Kee

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dieter.menne at menne-biomed.de  Sat Dec  4 18:26:29 2004
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 4 Dec 2004 17:26:29 +0000 (UTC)
Subject: [R] Excel *.xls files, RODBC
References: <200412041449.iB4EnmCg008102@erdos.math.unb.ca>
	<41B1D349.9030100@optonline.net>
Message-ID: <loom.20041204T182304-357@post.gmane.org>

 
> library(RODBC)
> z <- odbcConnectExcel("c:/myfolder/mydata.xls")
> myframe <- sqlFetch(z, "Sheet1")
> close(z)

I found the reading of whole sheets somewhat unsafe, so I always create a named
range (here: data) including header and do the following. 
Never had problems with this.

channel = odbcConnectExcel("macronutrients.xls")
ac = sqlQuery(channel,"select * from data")
odbcClose(channel)


Dieter



From tplate at acm.org  Sat Dec  4 19:07:35 2004
From: tplate at acm.org (Tony Plate)
Date: Sat, 04 Dec 2004 11:07:35 -0700
Subject: [R] Re: Protocol for answering basic questions
In-Reply-To: <200412011752.iB1HqV4H010777@erdos.math.unb.ca>
References: <200412011752.iB1HqV4H010777@erdos.math.unb.ca>
Message-ID: <6.1.0.6.2.20041201131047.03f69be0@mailhost.blackmesacapital.com>

Perhaps something like the following paragraph should be added to the start 
of the "Posting Guide" (as a new paragraph right after the existing first 
paragraph):

Note that R-help is *not* intended for questions that are easily answered 
by consulting one of the FAQs or other introductory material (see "Do your 
homework before posting" below).    Such questions are actively discouraged 
and are likely to evoke a brusque response.  Questions about seemingly 
simple matters that are mentioned in the FAQs or other introductory 
material *are welcomed* on R-help *when the questioner obviously has done 
their homework and the question is accompanied by an explanation* like "FAQ 
7.2.1 seems to be relevant to this but I couldn't understand/apply the 
answer because ...".

Something like this would make it very clear up front what type of 
questions are not appropriate.  (I'm not trying at all to dictate the 
policy, but as far as I can tell, the above summaries the attitude of the 
majority of very knowledgeable helpers that respond to questions on R-help.)

Also, I think that John Maindonald's idea of a "I am new to R, where do I 
start?" page, with a link from the posting guide, is an excellent idea.

I'm aware that some feel that the posting guide is already too long, but my 
feeling is that if users don't read a very easily accessible posting guide 
AND post inappropriate questions AND become offended by brusque responses, 
then they are beyond where they can easily be helped.  The most important 
thing is to make it very clear what types of questions are and are not 
considered appropriate, so that beginning users know what they are getting 
into.

And the following might merit inclusion in the FAQ:

Why is R-help not for hand-holding beginner questions?
R-help is a high traffic list and the general sentiment is that too many 
very simple questions will overwhelm everyone and most importantly result 
in the knowledgeable helpers ceasing to participate.  The reason that there 
is no "R-help-me-quickly-I-dont-want-to-read-the-documentation" list is 
that no-one has felt that it would work well -- it is unlikely that many 
knowledgeable users of R would be willing to participate.  Without such 
users participating, it is likely that sometimes bad advice would be 
offered and stand uncorrected, because R is a complex language with many 
ways of doing things, some markedly inferior to others.  For these reasons, 
some feel it would be a very bad idea to create such a list.  (However, 
anyone who believes otherwise and wishes to start and maintain such a list 
or other similar service is free to do so.)  One reason for this overall 
state of affairs is that R is free software and consequently there is no 
revenue stream to support a hand-holding support service with paid 
employees.  So although the actual software is free, some investment in 
terms of time spent reading documentation is required in order to use 
it.  Furthermore, many of the frequent helpers on R-help have written 
introductory documents intended to help beginners with many aspects of 
learning and using R (e.g., "An Introduction to R", and the various 
FAQs).  Consequently they sometimes get fed up getting asked again and 
again the same question they have already written a document to 
explain.  Nonetheless, the general sentiment on R-help is very helpful -- a 
quote summarizes it well: "It's OK if you need some spoonfeeding (I need 
that quite often myself), but at least show how you have tried to use the 
spoon yourself, instead of just showing us your open mouth."  [Attribution 
to Andy Liaw, or remain anonymous?]


As some feel that sufficient time and bandwidth has already been spent on 
this issue, if anyone has any comments on this particular matter of an 
addition to the posting guide (or FAQ), feel free to choose to respond to 
me privately, and I will summarize as appropriate.

-- Tony Plate



From rolf at math.unb.ca  Sat Dec  4 19:32:24 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 4 Dec 2004 14:32:24 -0400 (AST)
Subject: [R] Excel *.xls files, RODBC
Message-ID: <200412041832.iB4IWOuG017201@erdos.math.unb.ca>

Chuck Cleland wrote:

> The following works for me under WinXP Pro to create "myframe" as a 
> data frame:
> 
> library(RODBC)
> z <- odbcConnectExcel("c:/myfolder/mydata.xls")
> myframe <- sqlFetch(z, "Sheet1")
> close(z)

I tried that and got the error message:

Error: couldn't find function "odbcConnectExcel"

> Are you indicating the name of the worksheet you want within the
> *.xls file?  I suspect there could be additional issues on a
> non-Windows OS that I don't know about.

On which Brian Ripley commented:

> Most notably the absence of an Excel ODBC driver.

I guess that's the problem.  In my initial message I forgot to
indicate that I am working on a Linux box.  Sorry; mea culpa.

It would appear then, that there is NO WAY to read Excel files into R
save by transporting them to a Windoze system, saving them as .csv
files and then transporting these back reading them into R.  A bit
unsatisfactory, but it ***is*** a workaround.

Thanks to all who contributed advice/comments.

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From dgrove at fhcrc.org  Sat Dec  4 19:33:12 2004
From: dgrove at fhcrc.org (Douglas Grove)
Date: Sat, 4 Dec 2004 10:33:12 -0800 (PST)
Subject: [R] How about a mascot for R?
In-Reply-To: <16817.51068.638596.414032@gargle.gargle.HOWL>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>
	<20041202205022.GJ18464@hortresearch.co.nz>
	<Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>
	<16817.51068.638596.414032@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.58.0412041027060.16219@bear.fhcrc.org>

When I think of New Zealand I think "Rabbit" :)

How 'bout something like the Monty Python rabbit from 
"the Holy Grail" ("nasty pointy teeth...", "look at the bones!")

Doug



From rxg218 at psu.edu  Sat Dec  4 19:38:17 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Sat, 04 Dec 2004 13:38:17 -0500
Subject: [R] Excel *.xls files, RODBC
In-Reply-To: <200412041832.iB4IWOuG017201@erdos.math.unb.ca>
References: <200412041832.iB4IWOuG017201@erdos.math.unb.ca>
Message-ID: <1102185497.3709.1.camel@localhost.localdomain>

On Sat, 2004-12-04 at 14:32 -0400, Rolf Turner wrote:
> Chuck Cleland wrote:
> 
> > The following works for me under WinXP Pro to create "myframe" as a 
> > data frame:
> > 
> > library(RODBC)
> > z <- odbcConnectExcel("c:/myfolder/mydata.xls")
> > myframe <- sqlFetch(z, "Sheet1")
> > close(z)
> 
> It would appear then, that there is NO WAY to read Excel files into R
> save by transporting them to a Windoze system, saving them as .csv
> files and then transporting these back reading them into R.  A bit
> unsatisfactory, but it ***is*** a workaround.

To stay on Linux one possibility would be to use perl:
http://www-106.ibm.com/developerworks/linux/library/l-pexcel/



From tobias.verbeke at telenet.be  Sat Dec  4 19:46:27 2004
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Sat, 4 Dec 2004 18:46:27 +0000
Subject: [R] Excel *.xls files, RODBC
In-Reply-To: <200412041832.iB4IWOuG017201@erdos.math.unb.ca>
References: <200412041832.iB4IWOuG017201@erdos.math.unb.ca>
Message-ID: <20041204184627.6837d9fe.tobias.verbeke@telenet.be>

On Sat, 4 Dec 2004 14:32:24 -0400 (AST)
Rolf Turner <rolf at math.unb.ca> wrote:

> Chuck Cleland wrote:
> 
> > The following works for me under WinXP Pro to create "myframe" as a 
> > data frame:
> > 
> > library(RODBC)
> > z <- odbcConnectExcel("c:/myfolder/mydata.xls")
> > myframe <- sqlFetch(z, "Sheet1")
> > close(z)
> 
> I tried that and got the error message:
> 
> Error: couldn't find function "odbcConnectExcel"
> 
> > Are you indicating the name of the worksheet you want within the
> > *.xls file?  I suspect there could be additional issues on a
> > non-Windows OS that I don't know about.
> 
> On which Brian Ripley commented:
> 
> > Most notably the absence of an Excel ODBC driver.
> 
> I guess that's the problem.  In my initial message I forgot to
> indicate that I am working on a Linux box.  Sorry; mea culpa.
> 
> It would appear then, that there is NO WAY to read Excel files into R
> save by transporting them to a Windoze system, saving them as .csv
> files and then transporting these back reading them into R.  A bit
> unsatisfactory, but it ***is*** a workaround.

library(gdata)
?read.xls

HTH,
Tobias



From rolf at math.unb.ca  Sat Dec  4 20:43:07 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 4 Dec 2004 15:43:07 -0400 (AST)
Subject: [R] Excel *.xls files, RODBC
Message-ID: <200412041943.iB4Jh7JF019408@erdos.math.unb.ca>


Success!  Tobias Verbeke's kind suggestion of read.xls from the
gdata package (from the gregmisc bundle) works like a charm.
It's perl based, so no problema on Linux.

The R community is wonderful!

					cheers,

						Rolf Turner



From info at rhkoning.com  Sat Dec  4 20:43:47 2004
From: info at rhkoning.com (Ruud H. Koning)
Date: Sat, 04 Dec 2004 20:43:47 +0100
Subject: [R] How about a mascot for R?
In-Reply-To: <Pine.LNX.4.58.0412041027060.16219@bear.fhcrc.org>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>
	<20041202205022.GJ18464@hortresearch.co.nz>
	<Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>
	<16817.51068.638596.414032@gargle.gargle.HOWL>
	<Pine.LNX.4.58.0412041027060.16219@bear.fhcrc.org>
Message-ID: <200412042043470149.014AF95F@localhost>

Hello, I have a dataset with three numerical variables, and two factor
variables, one of which is shown:

> deel1[,1:4]
      median       ucl       lcl coupon.period
1  10.894672        NA 14.255623      fixed0-1
5  12.536729 11.658164 13.341038      fixed1-5
9  10.616561  9.979676 11.039264      fixed5-7
13  8.457571  8.048390  8.723679     fixed7-10
17  7.537831  7.274149  7.895592    fixed10-15
21  4.392874  4.279858  4.586663   fixed15more

Think of the data as six regression coefficients, with an upper and lower
confidence limit. I would like to make a lattice plot, with the factor
(fixed0-1,fixed1-5, etc) on the vertical axis, and the median on the
horizontal axis. This is simple: 

xyplot(coupon.period ~ median,data=prepayment,
 panel=function(x,y,...){
  panel.xyplot(x,y)
 }
)

does the trick. Now I want to have a line extending from the dots
representing the median, that run from the median to upper/lower confidence
limit. How do pass the information of deel1$ucl and deel1$lcl to the panel
function, and how do I make segments within the panel function?
Thanks for any help, Ruud



From info at rhkoning.com  Sat Dec  4 20:56:04 2004
From: info at rhkoning.com (Ruud H. Koning)
Date: Sat, 04 Dec 2004 20:56:04 +0100
Subject: [R] lattice graph with segments (erroneously posted earlier)
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>
	<20041202205022.GJ18464@hortresearch.co.nz>
	<Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>
	<16817.51068.638596.414032@gargle.gargle.HOWL>
	<Pine.LNX.4.58.0412041027060.16219@bear.fhcrc.org>
	<200412042043470149.014AF95F@localhost>
Message-ID: <200412042056040910.01563B40@localhost>

Hello, I have a dataset with three numerical variables, and two factor
variables, one of which is shown:

> deel1[,1:4]
      median       ucl       lcl coupon.period
1  10.894672        NA 14.255623      fixed0-1
5  12.536729 11.658164 13.341038      fixed1-5
9  10.616561  9.979676 11.039264      fixed5-7
13  8.457571  8.048390  8.723679     fixed7-10
17  7.537831  7.274149  7.895592    fixed10-15
21  4.392874  4.279858  4.586663   fixed15more

Think of the data as six regression coefficients, with an upper and lower
confidence limit. I would like to make a lattice plot, with the factor
(fixed0-1,fixed1-5, etc) on the vertical axis, and the median on the
horizontal axis. This is simple: 

xyplot(coupon.period ~ median,data=prepayment,
 panel=function(x,y,...){
  panel.xyplot(x,y)
 }
)

does the trick. Now I want to have a line extending from the dots
representing the median, that run from the median to upper/lower confidence
limit. How do pass the information of deel1$ucl and deel1$lcl to the panel
function, and how do I make segments within the panel function?
Thanks for any help, Ruud



From andrewr at uidaho.edu  Sat Dec  4 21:11:38 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Sun, 5 Dec 2004 07:11:38 +1100
Subject: [R] How about a mascot for R?
In-Reply-To: <200412042043470149.014AF95F@localhost>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>
	<20041202205022.GJ18464@hortresearch.co.nz>
	<Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>
	<16817.51068.638596.414032@gargle.gargle.HOWL>
	<Pine.LNX.4.58.0412041027060.16219@bear.fhcrc.org>
	<200412042043470149.014AF95F@localhost>
Message-ID: <20041204201138.GA613@uidaho.edu>

Ruud,

try something like the following (not debugged, no coffee yet):


xyplot(coupon.period ~ median, data=prepayment,
 subscripts=T,	     
 panel=function(x,y,subscripts,...){
   panel.xyplot(x,y)
   panel.segments(deel1$lcl[subscripts], deel$ucl[subscripts])
 }
)


I hope that this helps,

Andrew

On Sat, Dec 04, 2004 at 08:43:47PM +0100, Ruud H. Koning wrote:
> Hello, I have a dataset with three numerical variables, and two factor
> variables, one of which is shown:
> 
> > deel1[,1:4]
>       median       ucl       lcl coupon.period
> 1  10.894672        NA 14.255623      fixed0-1
> 5  12.536729 11.658164 13.341038      fixed1-5
> 9  10.616561  9.979676 11.039264      fixed5-7
> 13  8.457571  8.048390  8.723679     fixed7-10
> 17  7.537831  7.274149  7.895592    fixed10-15
> 21  4.392874  4.279858  4.586663   fixed15more
> 
> Think of the data as six regression coefficients, with an upper and lower
> confidence limit. I would like to make a lattice plot, with the factor
> (fixed0-1,fixed1-5, etc) on the vertical axis, and the median on the
> horizontal axis. This is simple: 
> 
> xyplot(coupon.period ~ median,data=prepayment,
>  panel=function(x,y,...){
>   panel.xyplot(x,y)
>  }
> )
> 
> does the trick. Now I want to have a line extending from the dots
> representing the median, that run from the median to upper/lower confidence
> limit. How do pass the information of deel1$ucl and deel1$lcl to the panel
> function, and how do I make segments within the panel function?
> Thanks for any help, Ruud
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From tchur at optushome.com.au  Sat Dec  4 21:40:52 2004
From: tchur at optushome.com.au (Tim Churches)
Date: Sun, 05 Dec 2004 07:40:52 +1100
Subject: [R] Lattice graph with segments
In-Reply-To: <20041204201138.GA613@uidaho.edu>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>	<20041202205022.GJ18464@hortresearch.co.nz>	<Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>	<16817.51068.638596.414032@gargle.gargle.HOWL>	<Pine.LNX.4.58.0412041027060.16219@bear.fhcrc.org>	<200412042043470149.014AF95F@localhost>
	<20041204201138.GA613@uidaho.edu>
Message-ID: <41B220D4.6070200@optushome.com.au>

Andrew Robinson wrote:
> Ruud,
> 
> try something like the following (not debugged, no coffee yet):
> 
> 
> xyplot(coupon.period ~ median, data=prepayment,
>  subscripts=T,	     
>  panel=function(x,y,subscripts,...){
>    panel.xyplot(x,y)
>    panel.segments(deel1$lcl[subscripts], deel$ucl[subscripts])
>  }
> )
> 

Andrew Robinson wrote:
 > Ruud,
 >
 > try something like the following (not debugged, no coffee yet):
 >
 >
 > xyplot(coupon.period ~ median, data=prepayment,
 >  subscripts=T,	
 >  panel=function(x,y,subscripts,...){
 >    panel.xyplot(x,y)
 >    panel.segments(deel1$lcl[subscripts], deel$ucl[subscripts])
 >  }
 > )
 >

Not quite:

library(lattice)
prepayment <- data.frame(median=c(10.89,12.54,10.62,8.46,7.54,4.39),
                          ucl=c(NA,11.66,9.98,8.05,7.27,4.28),
                          lcl=c(14.26,13.34,11.04,8.72,7.90,4.59),
                          coupon.period=c('a','b','c','d','e','f'))

xyplot(coupon.period ~ median, data=prepayment,
  subscripts=T,	
  panel=function(x,y,subscripts,...){
    panel.xyplot(x,y)
    panel.segments(prepayment$lcl[subscripts], prepayment$ucl[subscripts])
  }
)

throws the error:

Error in max(length(x0), length(x1), length(y0), length(y1)) :
         Argument "x1" is missing, with no default

Tim C



From liuwensui at gmail.com  Sat Dec  4 21:43:36 2004
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 4 Dec 2004 15:43:36 -0500
Subject: [R] R plus SAS
Message-ID: <1115a2b004120412431d51278a@mail.gmail.com>

Is it possible use SAS/DDE to communicate with R?

Sorry for offending useR. ^_^

Thanks a lot.



From deepayan at stat.wisc.edu  Sat Dec  4 22:12:54 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 4 Dec 2004 15:12:54 -0600
Subject: [R] Lattice graph with segments
In-Reply-To: <41B220D4.6070200@optushome.com.au>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>
	<20041204201138.GA613@uidaho.edu>
	<41B220D4.6070200@optushome.com.au>
Message-ID: <200412041512.54838.deepayan@stat.wisc.edu>

On Saturday 04 December 2004 14:40, Tim Churches wrote:
> Andrew Robinson wrote:
> > Ruud,
> >
> > try something like the following (not debugged, no coffee yet):
> >
> >
> > xyplot(coupon.period ~ median, data=prepayment,
> >  subscripts=T,
> >  panel=function(x,y,subscripts,...){
> >    panel.xyplot(x,y)
> >    panel.segments(deel1$lcl[subscripts], deel$ucl[subscripts])
> >  }
> > )
>
> Not quite:
>
> library(lattice)
> prepayment <- data.frame(median=c(10.89,12.54,10.62,8.46,7.54,4.39),
>                           ucl=c(NA,11.66,9.98,8.05,7.27,4.28),
>                           lcl=c(14.26,13.34,11.04,8.72,7.90,4.59),
>                           coupon.period=c('a','b','c','d','e','f'))
>
> xyplot(coupon.period ~ median, data=prepayment,
>   subscripts=T,
>   panel=function(x,y,subscripts,...){
>     panel.xyplot(x,y)
>     panel.segments(prepayment$lcl[subscripts],
> prepayment$ucl[subscripts]) }
> )
>
> throws the error:
>
> Error in max(length(x0), length(x1), length(y0), length(y1)) :
>          Argument "x1" is missing, with no default


Right. Also, to make the resulting object self-contained (i.e. not 
dependent on having a particular data frame in the scope), I would do 
something similar to (either one of):


with(prepayment,
     xyplot(coupon.period ~ median, 
            lcl = lcl, ucl = ucl, 
            panel=function(x, y, subscripts, lcl, ucl, ...) {
                panel.xyplot(x, y, ...)
                panel.segments(lcl[subscripts], as.numeric(y),
                               ucl[subscripts], as.numeric(y), ...)
            }))



xyplot(coupon.period ~ median, data = prepayment,
       lcl = prepayment$lcl, ucl = prepayment$ucl, 
       panel=function(x, y, subscripts, lcl, ucl, ...) {
           panel.xyplot(x, y, ...)
           panel.segments(lcl[subscripts], as.numeric(y),
                          ucl[subscripts], as.numeric(y), ...)
       })


Deepayan



From r_pinedam at hotmail.com  Sat Dec  4 22:47:00 2004
From: r_pinedam at hotmail.com (=?iso-8859-1?B?UmVu6SBQaW5lZGE=?=)
Date: Sat, 04 Dec 2004 21:47:00 +0000
Subject: [R] (sin asunto)
Message-ID: <BAY10-F21A2CA37680E3A4159BFC98B20@phx.gbl>



From r_pinedam at hotmail.com  Sat Dec  4 23:01:28 2004
From: r_pinedam at hotmail.com (=?iso-8859-1?B?UmVu6SBQaW5lZGE=?=)
Date: Sat, 04 Dec 2004 22:01:28 +0000
Subject: [R] Kalman Filtering
Message-ID: <BAY10-F1788016A5AAE6514E2D2B398B20@phx.gbl>



From jfox at mcmaster.ca  Sun Dec  5 00:19:04 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 4 Dec 2004 18:19:04 -0500
Subject: [R] Testing for S4 objects (redux)
In-Reply-To: <61842888041130064036da550b@mail.gmail.com>
Message-ID: <20041204231903.QKUQ1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear John,

I've encountered the following problem:

> x <- rnorm(100)
> y <- sample(2, 100, replace=TRUE)
> res <- by(x, y, mean)
> res
INDICES: 1
[1] 0.1129494
------------------------------------------------------------ 
INDICES: 2
[1] -0.2066684
> 
> isS4object <- function(object)(length(attr(object, "class"))==1 &&
+  !is.null(getClass(class(object))))
> 
> isS4object(res)
Error in getClass(class(object)) : "by" is not a defined class
> 

For a simple question, this has gotten rather complicated. Any suggestions
would be appreciated.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: John Chambers [mailto:johnmchambers at gmail.com] 
> Sent: Tuesday, November 30, 2004 9:40 AM
> To: John Fox
> Cc: Martin Maechler; r-help at stat.math.ethz.ch
> Subject: Re: [R] Testing for S4 objects
> 
> Let me suggest a different test, because slotNames was 
> written to work differently when given a string or a class 
> definition.  With your definition,
> 
> R> x <- "classRepresentation"
> R> isS4object(x)
> [1] TRUE
> 
> which I assume is not what you wanted.  (Given a single string,
> slotNames() tries to look up the class definition of that name.)
> 
> How about the following?  The logic is that an S4 object must 
> have an actual class attribute of length 1 (that rules out 
> basic data types, where class(x) is a string but there is no 
> actual attribute, and also rules out some S3 objects).  Then 
> if that's true, try to look up the class definition.  If it 
> is non-null, seems like an S4 object.
> 
> R> isS4object <- function(object)(length(attr(object, "class"))==1 &&
> +     !is.null(getClass(class(object))))
> R> isS4object(x)
> [1] FALSE
> R> isS4object(getClass(class(x)))
> [1] TRUE
> 
> This definition seems to work, at least on the examples I 
> could think of right away.  Notice though, that some classes, 
> such as "ts", that have been around for a long while are 
> nevertheless legitimate S4 classes, so:
> 
> R> t1 = ts(1:12)
> R> isS4object(t1)
> [1] TRUE
> 
> (this applies to either version of isS4object).
> 
> There are a couple of details, more appropriate for the r-devel list. 
> Seems  a good candidate for a function to add to R.
> 
> 
> On Sat, 27 Nov 2004 17:48:30 -0500, John Fox <jfox at mcmaster.ca> wrote:
> > Dear Martin,
> > 
> > As it turns out, the test that I proposed (i.e., testing for NULL 
> > slotNames) sometimes fails. For example:
> > 
> > > library(car)
> > > data(Prestige)
> > > sum <- summary(lm(prestige ~ income + education, data=Prestige))
> > > slotNames(sum)
> > character(0)
> > 
> > The following, however, seems to work (at least as far as I've been 
> > able to
> > ascertain):
> > 
> > isS4object <- function(object) length(slotNames(object)) != 0
> > 
> > I hope that this is a more robust test.
> > 
> > 
> > 
> > John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > > -----Original Message-----
> > > From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
> > > Sent: Friday, November 26, 2004 3:18 AM
> > > To: John Fox
> > > Cc: r-help at stat.math.ethz.ch
> > > Subject: Re: [R] Testing for S4 objects
> > >
> > > >>>>> "JohnF" == John Fox <jfox at mcmaster.ca>
> > > >>>>>     on Thu, 25 Nov 2004 22:28:50 -0500 writes:
> > >
> > >     JohnF> Dear r-help list members, Is there a way to test
> > >     JohnF> whether an object is an S4 object? The best that I've
> > >     JohnF> been able to come up with is
> > >
> > >     JohnF>    isS4object <- function(object)
> > > !(is.null(slotNames(object)))
> > >
> > > you can drop one pair of "(..)" to give
> > >
> > >   isS4object <- function(object) !is.null(slotNames(object))
> > >
> > >
> > >     JohnF> which assumes that an S4 object has at least one
> > >     JohnF> slot. I think this is safe, but perhaps I'm missing
> > >     JohnF> something.
> > >
> > > The question is a very good one -- that I have posed to R-core a 
> > > while ago myself.
> > >
> > > Inside  utils:::str.default  {which doesn't show the many 
> commments 
> > > in the *source* of str.default()}, I have wanted a way that even 
> > > works when the 'methods' package is not attached and use the more 
> > > obscure
> > >
> > >     #NOT yet:if(has.class <- !is.null(cl <- class(object)))
> > >     if(has.class <- !is.null(cl <- attr(object, "class")))#
> > > S3 or S4 class
> > >       S4 <- !is.null(attr(cl, "package"))## <<<'kludge' FIXME!
> > >       ##or length(methods::getSlots(cl)) > 0
> > >
> > > For the time being, I'd keep your function, but I don't 
> think we'd 
> > > guarantee that it will remain the appropriate test in all 
> future.  
> > > But till then many things will have happened (if not all of them 
> > > ;-).
> > >
> > > Martin Maechler, ETH Zurich
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >



From Benjamin.Osborne at uvm.edu  Sun Dec  5 00:41:13 2004
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Sat,  4 Dec 2004 18:41:13 -0500
Subject: [R] AIC, AICc, and K
Message-ID: <1102203673.41b24b193a89e@webmail.uvm.edu>

How can I extract K (number of parameters) from an AIC calculation, both to
report K itself and to calculate AICc?  I'm aware of the conversion from AIC ->
AICc, where AICc = AIC + 2K(K+1)/(n-K-1), but not sure of how K is calculated
or how to extract that value from either an AIC or logLik calculation.

This is probably more of a basic statistics question than an R question, but I
thank you for your help.

-Ben Osborne

--
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440



From patfoley at csus.edu  Sun Dec  5 01:49:01 2004
From: patfoley at csus.edu (Patrick Foley)
Date: Sat, 04 Dec 2004 16:49:01 -0800
Subject: [R] What is the most useful way to detect nonlinearity in logistic
	regression?
Message-ID: <41B25AFD.7010505@csus.edu>

It is easy to spot response nonlinearity in normal linear models using 
plot(something.lm).
However plot(something.glm) produces artifactual peculiarities since the 
diagnostic residuals are constrained  by the fact that y can only take 
values 0 or 1.
What do R users find most useful in checking the linearity assumption of 
logistic regression (i.e. log-odds =a+bx)?

Patrick Foley
patfoley at csus.edu



From jari.oksanen at oulu.fi  Sun Dec  5 01:51:04 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Sun, 05 Dec 2004 02:51:04 +0200
Subject: [R] How about a mascot for R?
In-Reply-To: <16817.51068.638596.414032@gargle.gargle.HOWL>
References: <200412022008.iB2K8C108678@gator.dt.uh.edu>
	<20041202205022.GJ18464@hortresearch.co.nz>
	<Pine.LNX.4.61.0412031451430.3049@hydra.stat.auckland.ac.nz>
	<16817.51068.638596.414032@gargle.gargle.HOWL>
Message-ID: <BE0086F9-4657-11D9-97ED-000A95C76CA8@oulu.fi>


On 4 Dec 2004, at 16:19, Martin Maechler wrote:

>>>>>> "DScottNZ" == David Scott <d.scott at auckland.ac.nz>
>>>>>>     on Fri, 3 Dec 2004 15:04:52 +1300 (NZDT) writes:
>
> 	  <........>
>
>     DScottNZ> As to an animal mascot, I think a New Zealand
>     DScottNZ> mascot is a must,
>
> well, thinking that "must" is bit strong, I agree that
> I have had the same idea (NZ animal) before your post.
> I first thought of the obvious Kiwi, but hoping for something
> more beautiful had been googling around for "New Zealand animals",
> then had been side tracted by the Kakapo which I found nice,
> intriguing, but in his fight against extinction didn't seem to
> fit to my notion of R..
>
Firstly, Kiwi is a rip snorter for a bird. Secondly, there are other 
kind of kiwis than the kiwi bird. I'm living about as far a away from 
NZ as is it is possible (you're getting closer if you try to get away), 
but even I've heard of 'kiwi fruit', 'kiwi bear' (brushtail possum) and 
'kiwi' as people. So it could be something 'kiwi'. I do think that a 
kiwi bird would be mascotty like a creature: cuddly and round and 
easiesh to draw. One parallel story brought about here is the penguin 
as a Linux mascot. Actually, this is a not-so-pleasant story: Linus 
Torvalds told somewhere that a penguin (hardly gentoo but some other 
species) tried to bite off his finger in a zoo, which made him to like 
those animals (he's a Swedish speaking Finn which helps to explain this 
attitude). With this attitude, you could pick a gray, mouse-like 
nocturnal bird as a mascot. Naturally, this is none of my business, so 
you should not let this message influence your opinion (it wouldn't 
anyway).

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From spencer.graves at pdf.com  Sun Dec  5 01:59:45 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 04 Dec 2004 16:59:45 -0800
Subject: [R] Testing for S4 objects (redux)
In-Reply-To: <20041204231903.QKUQ1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <20041204231903.QKUQ1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <41B25D81.9080101@pdf.com>

      Which version of R? 

      I just replicated it in R 2.0.0patched.  I know that 2.0.1 is now 
available, but I haven't found time to upgrade yet. 

      By the way, I can replicate the error message with only one line: 

 > getClass("by")
Error in getClass("by") : "by" is not a defined class

      hope this helps.  spencer graves

John Fox wrote:

>Dear John,
>
>I've encountered the following problem:
>
>  
>
>>x <- rnorm(100)
>>y <- sample(2, 100, replace=TRUE)
>>res <- by(x, y, mean)
>>res
>>    
>>
>INDICES: 1
>[1] 0.1129494
>------------------------------------------------------------ 
>INDICES: 2
>[1] -0.2066684
>  
>
>>isS4object <- function(object)(length(attr(object, "class"))==1 &&
>>    
>>
>+  !is.null(getClass(class(object))))
>  
>
>>isS4object(res)
>>    
>>
>Error in getClass(class(object)) : "by" is not a defined class
>  
>
>
>For a simple question, this has gotten rather complicated. Any suggestions
>would be appreciated.
>
>Regards,
> John
>
>--------------------------------
>John Fox
>Department of Sociology
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>905-525-9140x23604
>http://socserv.mcmaster.ca/jfox 
>-------------------------------- 
>
>  
>
>>-----Original Message-----
>>From: John Chambers [mailto:johnmchambers at gmail.com] 
>>Sent: Tuesday, November 30, 2004 9:40 AM
>>To: John Fox
>>Cc: Martin Maechler; r-help at stat.math.ethz.ch
>>Subject: Re: [R] Testing for S4 objects
>>
>>Let me suggest a different test, because slotNames was 
>>written to work differently when given a string or a class 
>>definition.  With your definition,
>>
>>R> x <- "classRepresentation"
>>R> isS4object(x)
>>[1] TRUE
>>
>>which I assume is not what you wanted.  (Given a single string,
>>slotNames() tries to look up the class definition of that name.)
>>
>>How about the following?  The logic is that an S4 object must 
>>have an actual class attribute of length 1 (that rules out 
>>basic data types, where class(x) is a string but there is no 
>>actual attribute, and also rules out some S3 objects).  Then 
>>if that's true, try to look up the class definition.  If it 
>>is non-null, seems like an S4 object.
>>
>>R> isS4object <- function(object)(length(attr(object, "class"))==1 &&
>>+     !is.null(getClass(class(object))))
>>R> isS4object(x)
>>[1] FALSE
>>R> isS4object(getClass(class(x)))
>>[1] TRUE
>>
>>This definition seems to work, at least on the examples I 
>>could think of right away.  Notice though, that some classes, 
>>such as "ts", that have been around for a long while are 
>>nevertheless legitimate S4 classes, so:
>>
>>R> t1 = ts(1:12)
>>R> isS4object(t1)
>>[1] TRUE
>>
>>(this applies to either version of isS4object).
>>
>>There are a couple of details, more appropriate for the r-devel list. 
>>Seems  a good candidate for a function to add to R.
>>
>>
>>On Sat, 27 Nov 2004 17:48:30 -0500, John Fox <jfox at mcmaster.ca> wrote:
>>    
>>
>>>Dear Martin,
>>>
>>>As it turns out, the test that I proposed (i.e., testing for NULL 
>>>slotNames) sometimes fails. For example:
>>>
>>>      
>>>
>>>>library(car)
>>>>data(Prestige)
>>>>sum <- summary(lm(prestige ~ income + education, data=Prestige))
>>>>slotNames(sum)
>>>>        
>>>>
>>>character(0)
>>>
>>>The following, however, seems to work (at least as far as I've been 
>>>able to
>>>ascertain):
>>>
>>>isS4object <- function(object) length(slotNames(object)) != 0
>>>
>>>I hope that this is a more robust test.
>>>
>>>
>>>
>>>John
>>>
>>>--------------------------------
>>>John Fox
>>>Department of Sociology
>>>McMaster University
>>>Hamilton, Ontario
>>>Canada L8S 4M4
>>>905-525-9140x23604
>>>http://socserv.mcmaster.ca/jfox
>>>--------------------------------
>>>
>>>      
>>>
>>>>-----Original Message-----
>>>>From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
>>>>Sent: Friday, November 26, 2004 3:18 AM
>>>>To: John Fox
>>>>Cc: r-help at stat.math.ethz.ch
>>>>Subject: Re: [R] Testing for S4 objects
>>>>
>>>>        
>>>>
>>>>>>>>>"JohnF" == John Fox <jfox at mcmaster.ca>
>>>>>>>>>    on Thu, 25 Nov 2004 22:28:50 -0500 writes:
>>>>>>>>>                  
>>>>>>>>>
>>>>    JohnF> Dear r-help list members, Is there a way to test
>>>>    JohnF> whether an object is an S4 object? The best that I've
>>>>    JohnF> been able to come up with is
>>>>
>>>>    JohnF>    isS4object <- function(object)
>>>>!(is.null(slotNames(object)))
>>>>
>>>>you can drop one pair of "(..)" to give
>>>>
>>>>  isS4object <- function(object) !is.null(slotNames(object))
>>>>
>>>>
>>>>    JohnF> which assumes that an S4 object has at least one
>>>>    JohnF> slot. I think this is safe, but perhaps I'm missing
>>>>    JohnF> something.
>>>>
>>>>The question is a very good one -- that I have posed to R-core a 
>>>>while ago myself.
>>>>
>>>>Inside  utils:::str.default  {which doesn't show the many 
>>>>        
>>>>
>>commments 
>>    
>>
>>>>in the *source* of str.default()}, I have wanted a way that even 
>>>>works when the 'methods' package is not attached and use the more 
>>>>obscure
>>>>
>>>>    #NOT yet:if(has.class <- !is.null(cl <- class(object)))
>>>>    if(has.class <- !is.null(cl <- attr(object, "class")))#
>>>>S3 or S4 class
>>>>      S4 <- !is.null(attr(cl, "package"))## <<<'kludge' FIXME!
>>>>      ##or length(methods::getSlots(cl)) > 0
>>>>
>>>>For the time being, I'd keep your function, but I don't 
>>>>        
>>>>
>>think we'd 
>>    
>>
>>>>guarantee that it will remain the appropriate test in all 
>>>>        
>>>>
>>future.  
>>    
>>
>>>>But till then many things will have happened (if not all of them 
>>>>;-).
>>>>
>>>>Martin Maechler, ETH Zurich
>>>>
>>>>        
>>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>>      
>>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From spencer.graves at pdf.com  Sun Dec  5 02:09:26 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 04 Dec 2004 17:09:26 -0800
Subject: [R] AIC, AICc, and K
In-Reply-To: <1102203673.41b24b193a89e@webmail.uvm.edu>
References: <1102203673.41b24b193a89e@webmail.uvm.edu>
Message-ID: <41B25FC6.7030504@pdf.com>

      I don't know the "best" way, but the following looks like it will 
work: 

tstDF <- data.frame(x=1:3, y=c(1,1,2))
fit0 <- lm(y~1, tstDF)
fitDF <- lm(y~x, tstDF)
AIC(fitDF,fit0)
      df      AIC
fitDF  3 5.842516
fit0   2 8.001399

      The function AIC with only 1 argument returns only a single 
number.  However, given nested models, it returns a data.frame with 
colums df and AIC.  At least in this example (and I would think in all 
other contexts as well), "df" is the K you want. 
   
      hope this helps. 
      Spencer Graves

Benjamin M. Osborne wrote:

>How can I extract K (number of parameters) from an AIC calculation, both to
>report K itself and to calculate AICc?  I'm aware of the conversion from AIC ->
>AICc, where AICc = AIC + 2K(K+1)/(n-K-1), but not sure of how K is calculated
>or how to extract that value from either an AIC or logLik calculation.
>
>This is probably more of a basic statistics question than an R question, but I
>thank you for your help.
>
>-Ben Osborne
>
>--
>Botany Department
>University of Vermont
>109 Carrigan Drive
>Burlington, VT 05405
>
>benjamin.osborne at uvm.edu
>phone: 802-656-0297
>fax: 802-656-0440
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From n.l.pace at utah.edu  Sun Dec  5 02:37:43 2004
From: n.l.pace at utah.edu (Nathan Leon Pace, MD, MStat)
Date: Sat, 4 Dec 2004 18:37:43 -0700
Subject: [R] boot package
Message-ID: <42129374-465E-11D9-9661-000393B3E9D0@utah.edu>

Hi,

I using the boot package 1.2-20 on R 2.0.1.

My statistics function estimates 6 parameters.

In a small percentage of resampled data sets my statistics function 
doesn't produce an estimate for one parameter and the boot function 
stops with an error.

I can write an ifelse(exists('parameter.estimate'), parameter.estimate, 
NA) statement within the statistic function to substitute an NA for the 
missing estimate value.

However, the boot.ci function to generate CIs from the boot object 
won't accept NAs.

My problem is writing code to impute a numeric value for the missing 
estimate. ifelse won't generate a numeric value if the test is mode 
logical.

Any suggestions?

Nathan


Nathan Leon Pace, MD, MStat	Work:n.l.pace at utah.edu
Department of Anesthesiology	Home:nlpaces at comcast.net
University of Utah			Work:801.581.6393
Salt Lake City, Utah			    Home:801.467.2925
					Fax:801.581.4367										Cell:801.558.3987



From p.dalgaard at biostat.ku.dk  Sun Dec  5 03:50:22 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Dec 2004 03:50:22 +0100
Subject: [R] What is the most useful way to detect nonlinearity in
	logistic regression?
In-Reply-To: <41B25AFD.7010505@csus.edu>
References: <41B25AFD.7010505@csus.edu>
Message-ID: <x2oeh95uhd.fsf@biostat.ku.dk>

Patrick Foley <patfoley at csus.edu> writes:

> It is easy to spot response nonlinearity in normal linear models using
> plot(something.lm).
> However plot(something.glm) produces artifactual peculiarities since
> the diagnostic residuals are constrained  by the fact that y can only
> take values 0 or 1.
> What do R users find most useful in checking the linearity assumption
> of logistic regression (i.e. log-odds =a+bx)?

Well, there's basically

        - grouping
        - higher-order terms
        - smoothed residuals

A simple technique is to include a variable _both_ as a continuous
term and cut up into a factor (as in ~ age + cut(age,seq(30,70,10))).
The model that you are fitting is a bit weird but it gives you a clean
test for omitting the grouped term. A somewhat nicer variant of the
same theme is to do a linear spline (or a higher order one for that
matter) with selected knots.

Re. the smoothed residuals, you do need to be careful about the
smoother. Some of the "robust" ones will do precisely the wrong thing
in this context: You really are interested in the mean, not some
trimmed mean (which can easily amount to throwing away all your
cases...). Here's an idea:

x <- runif(500)
y <- rbinom(500,size=1,p=plogis(x))
xx <- predict(loess(resid(glm(y~x,binomial))~x),se=T)
matplot(x,cbind(xx$fit, 2*xx$se.fit, -2*xx$se.fit),pch=20)

Not sure my money isn't still on the splines, though.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From baron at psych.upenn.edu  Sun Dec  5 04:39:27 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 4 Dec 2004 22:39:27 -0500
Subject: [R] A possible way to reduce basic questions
In-Reply-To: <000901c4da19$94e54ef0$6c00a8c0@mtd4>
References: <20041202012336.D7E4A3986@mprdmxin.myway.com>
	<06C2E003-4486-11D9-AC51-000A95B2B140@sanger.ac.uk>
	<000901c4da19$94e54ef0$6c00a8c0@mtd4>
Message-ID: <20041205033927.GA22882@psych>

On 12/02/04 21:15, Anne wrote:
>What about starting a database?

Of what?  Like the one in the  last line of my .sig?

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From Whit.Armstrong at tudor.com  Sun Dec  5 05:25:09 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Sat, 4 Dec 2004 23:25:09 -0500 
Subject: [R] Excel *.xls files, RODBC
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF042B3BD0@tudor.com>

There is another way to read and write excel files using jakarta POI.
Hopefully, I'll have a package available in a week or so.  I have a working
example of writing a matrix from R to excel, but I haven't finished the read
excel portion of the code.  If anyone wants to give it a spin, contact me
off list and I'll send a copy.

http://jakarta.apache.org/poi/index.html

Regards,
Whit


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rolf Turner
Sent: Saturday, December 04, 2004 2:43 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Excel *.xls files, RODBC


Success!  Tobias Verbeke's kind suggestion of read.xls from the gdata
package (from the gregmisc bundle) works like a charm. It's perl based, so
no problema on Linux.

The R community is wonderful!

					cheers,

						Rolf Turner

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Sun Dec  5 05:37:50 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 4 Dec 2004 23:37:50 -0500
Subject: [R] Testing for S4 objects (redux)
In-Reply-To: <41B25D81.9080101@pdf.com>
Message-ID: <20041205043749.OUXH1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Spencer,


> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Sent: Saturday, December 04, 2004 8:00 PM
> To: John Fox
> Cc: 'John Chambers'; r-help at stat.math.ethz.ch; 'Martin Maechler'
> Subject: Re: [R] Testing for S4 objects (redux)
> 
>       Which version of R? 
> 

Sorry -- this was with R 2.0.1 under Windows.

John

>       I just replicated it in R 2.0.0patched.  I know that 
> 2.0.1 is now available, but I haven't found time to upgrade yet. 
> 
>       By the way, I can replicate the error message with only 
> one line: 
> 
>  > getClass("by")
> Error in getClass("by") : "by" is not a defined class
> 
>       hope this helps.  spencer graves
> 
> John Fox wrote:
> 
> >Dear John,
> >
> >I've encountered the following problem:
> >
> >  
> >
> >>x <- rnorm(100)
> >>y <- sample(2, 100, replace=TRUE)
> >>res <- by(x, y, mean)
> >>res
> >>    
> >>
> >INDICES: 1
> >[1] 0.1129494
> >------------------------------------------------------------
> >INDICES: 2
> >[1] -0.2066684
> >  
> >
> >>isS4object <- function(object)(length(attr(object, "class"))==1 &&
> >>    
> >>
> >+  !is.null(getClass(class(object))))
> >  
> >
> >>isS4object(res)
> >>    
> >>
> >Error in getClass(class(object)) : "by" is not a defined class
> >  
> >
> >
> >For a simple question, this has gotten rather complicated. Any 
> >suggestions would be appreciated.
> >
> >Regards,
> > John
> >
> >--------------------------------
> >John Fox
> >Department of Sociology
> >McMaster University
> >Hamilton, Ontario
> >Canada L8S 4M4
> >905-525-9140x23604
> >http://socserv.mcmaster.ca/jfox
> >--------------------------------
> >
> >  
> >
> >>-----Original Message-----
> >>From: John Chambers [mailto:johnmchambers at gmail.com]
> >>Sent: Tuesday, November 30, 2004 9:40 AM
> >>To: John Fox
> >>Cc: Martin Maechler; r-help at stat.math.ethz.ch
> >>Subject: Re: [R] Testing for S4 objects
> >>
> >>Let me suggest a different test, because slotNames was 
> written to work 
> >>differently when given a string or a class definition.  With your 
> >>definition,
> >>
> >>R> x <- "classRepresentation"
> >>R> isS4object(x)
> >>[1] TRUE
> >>
> >>which I assume is not what you wanted.  (Given a single string,
> >>slotNames() tries to look up the class definition of that name.)
> >>
> >>How about the following?  The logic is that an S4 object 
> must have an 
> >>actual class attribute of length 1 (that rules out basic 
> data types, 
> >>where class(x) is a string but there is no actual 
> attribute, and also 
> >>rules out some S3 objects).  Then if that's true, try to 
> look up the 
> >>class definition.  If it is non-null, seems like an S4 object.
> >>
> >>R> isS4object <- function(object)(length(attr(object, 
> "class"))==1 &&
> >>+     !is.null(getClass(class(object))))
> >>R> isS4object(x)
> >>[1] FALSE
> >>R> isS4object(getClass(class(x)))
> >>[1] TRUE
> >>
> >>This definition seems to work, at least on the examples I 
> could think 
> >>of right away.  Notice though, that some classes, such as 
> "ts", that 
> >>have been around for a long while are nevertheless legitimate S4 
> >>classes, so:
> >>
> >>R> t1 = ts(1:12)
> >>R> isS4object(t1)
> >>[1] TRUE
> >>
> >>(this applies to either version of isS4object).
> >>
> >>There are a couple of details, more appropriate for the 
> r-devel list. 
> >>Seems  a good candidate for a function to add to R.
> >>
> >>
> >>On Sat, 27 Nov 2004 17:48:30 -0500, John Fox 
> <jfox at mcmaster.ca> wrote:
> >>    
> >>
> >>>Dear Martin,
> >>>
> >>>As it turns out, the test that I proposed (i.e., testing for NULL
> >>>slotNames) sometimes fails. For example:
> >>>
> >>>      
> >>>
> >>>>library(car)
> >>>>data(Prestige)
> >>>>sum <- summary(lm(prestige ~ income + education, data=Prestige))
> >>>>slotNames(sum)
> >>>>        
> >>>>
> >>>character(0)
> >>>
> >>>The following, however, seems to work (at least as far as 
> I've been 
> >>>able to
> >>>ascertain):
> >>>
> >>>isS4object <- function(object) length(slotNames(object)) != 0
> >>>
> >>>I hope that this is a more robust test.
> >>>
> >>>
> >>>
> >>>John
> >>>
> >>>--------------------------------
> >>>John Fox
> >>>Department of Sociology
> >>>McMaster University
> >>>Hamilton, Ontario
> >>>Canada L8S 4M4
> >>>905-525-9140x23604
> >>>http://socserv.mcmaster.ca/jfox
> >>>--------------------------------
> >>>
> >>>      
> >>>
> >>>>-----Original Message-----
> >>>>From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
> >>>>Sent: Friday, November 26, 2004 3:18 AM
> >>>>To: John Fox
> >>>>Cc: r-help at stat.math.ethz.ch
> >>>>Subject: Re: [R] Testing for S4 objects
> >>>>
> >>>>        
> >>>>
> >>>>>>>>>"JohnF" == John Fox <jfox at mcmaster.ca>
> >>>>>>>>>    on Thu, 25 Nov 2004 22:28:50 -0500 writes:
> >>>>>>>>>                  
> >>>>>>>>>
> >>>>    JohnF> Dear r-help list members, Is there a way to test
> >>>>    JohnF> whether an object is an S4 object? The best that I've
> >>>>    JohnF> been able to come up with is
> >>>>
> >>>>    JohnF>    isS4object <- function(object)
> >>>>!(is.null(slotNames(object)))
> >>>>
> >>>>you can drop one pair of "(..)" to give
> >>>>
> >>>>  isS4object <- function(object) !is.null(slotNames(object))
> >>>>
> >>>>
> >>>>    JohnF> which assumes that an S4 object has at least one
> >>>>    JohnF> slot. I think this is safe, but perhaps I'm missing
> >>>>    JohnF> something.
> >>>>
> >>>>The question is a very good one -- that I have posed to R-core a 
> >>>>while ago myself.
> >>>>
> >>>>Inside  utils:::str.default  {which doesn't show the many
> >>>>        
> >>>>
> >>commments
> >>    
> >>
> >>>>in the *source* of str.default()}, I have wanted a way that even 
> >>>>works when the 'methods' package is not attached and use the more 
> >>>>obscure
> >>>>
> >>>>    #NOT yet:if(has.class <- !is.null(cl <- class(object)))
> >>>>    if(has.class <- !is.null(cl <- attr(object, "class")))#
> >>>>S3 or S4 class
> >>>>      S4 <- !is.null(attr(cl, "package"))## <<<'kludge' FIXME!
> >>>>      ##or length(methods::getSlots(cl)) > 0
> >>>>
> >>>>For the time being, I'd keep your function, but I don't
> >>>>        
> >>>>
> >>think we'd
> >>    
> >>
> >>>>guarantee that it will remain the appropriate test in all
> >>>>        
> >>>>
> >>future.  
> >>    
> >>
> >>>>But till then many things will have happened (if not all of them 
> >>>>;-).
> >>>>
> >>>>Martin Maechler, ETH Zurich
> >>>>
> >>>>        
> >>>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list 
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! 
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>>      
> >>>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> >  
> >
> 
> --
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
>



From jfox at mcmaster.ca  Sun Dec  5 05:41:07 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 4 Dec 2004 23:41:07 -0500
Subject: [R] What is the most useful way to detect nonlinearity in
	logisticregression?
In-Reply-To: <41B25AFD.7010505@csus.edu>
Message-ID: <20041205044105.UEZ1613.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Patrick,

Component+residual plots can be defined for generalized linear models
(including logistic regression) as for linear models, but they may require
smoothing for interpretation. See, e.g., the cr.plots() functions in the car
package, which works with glm objects.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Foley
> Sent: Saturday, December 04, 2004 7:49 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] What is the most useful way to detect 
> nonlinearity in logisticregression?
> 
> It is easy to spot response nonlinearity in normal linear 
> models using plot(something.lm).
> However plot(something.glm) produces artifactual 
> peculiarities since the diagnostic residuals are constrained  
> by the fact that y can only take values 0 or 1.
> What do R users find most useful in checking the linearity 
> assumption of logistic regression (i.e. log-odds =a+bx)?
> 
> Patrick Foley
> patfoley at csus.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dieter.menne at menne-biomed.de  Sun Dec  5 10:34:31 2004
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 5 Dec 2004 09:34:31 +0000 (UTC)
Subject: [R] Label data points in scatterplot matrices
References: <Pine.GSO.4.58.0412031100190.10430@niflheim.rutgers.edu>
	<41B110CA.9060608@bigpond.com>
	<41B19706.4020007@statistik.uni-dortmund.de>
Message-ID: <loom.20041205T103144-217@post.gmane.org>

Uwe Ligges <ligges <at> statistik.uni-dortmund.de> writes:

> Well, in principle you cannot easily (it's a warning, not an errror, BTW).
> But you can fake a little bit by, e.g., renaming "z" to "pch":
> 
>   panel.hist <- function(x, ...)
>       {
>           usr <- par("usr"); on.exit(par(usr))
>           par(usr = c(usr[1:2], 0, 1.5) )
>           h <- hist(x, plot = FALSE)
>           breaks <- h$breaks; nB <- length(breaks)
>           y <- h$counts; y <- y/max(y)
>           rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
>       }
> 
> pairs(USJudgeRatings[1:5], panel=panel.text,
>             cex = 1.5, pch = z, bg="light blue",
>             diag.panel=panel.hist, cex.labels = 2, font.labels=2)
 

You are mixing standard graphics of hist with lattice/trellis. Give
panel.histogram a try instead.

Dieter



From murdoch at stats.uwo.ca  Sun Dec  5 13:02:32 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 05 Dec 2004 07:02:32 -0500
Subject: [R] boot package
In-Reply-To: <42129374-465E-11D9-9661-000393B3E9D0@utah.edu>
References: <42129374-465E-11D9-9661-000393B3E9D0@utah.edu>
Message-ID: <ust5r0pvr01hmuib9iup0jkvm05tbq2fj8@4ax.com>

On Sat, 4 Dec 2004 18:37:43 -0700, "Nathan Leon Pace, MD, MStat"
<n.l.pace at utah.edu> wrote:

>Hi,
>
>I using the boot package 1.2-20 on R 2.0.1.
>
>My statistics function estimates 6 parameters.
>
>In a small percentage of resampled data sets my statistics function 
>doesn't produce an estimate for one parameter and the boot function 
>stops with an error.
>
>I can write an ifelse(exists('parameter.estimate'), parameter.estimate, 
>NA) statement within the statistic function to substitute an NA for the 
>missing estimate value.
>
>However, the boot.ci function to generate CIs from the boot object 
>won't accept NAs.
>
>My problem is writing code to impute a numeric value for the missing 
>estimate. ifelse won't generate a numeric value if the test is mode 
>logical.

The test in ifelse is always logical or is coerced to be logical, so
this isn't right.  For example,

x <- c(1,2,NA)
ifelse(is.na(x), 0, x)

I suspect the problem has to do with the fact that a bare "NA" is mode
logical, but without sample code, I can't see exactly where you're
going wrong.  Perhaps you got the arguments to ifelse in the wrong
order?

x <- NA
ifelse(is.na(x), 0, x)  # Gives numeric result
ifelse(is.na(x), x, 0)  # Gives logical result

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Sun Dec  5 13:27:01 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 05 Dec 2004 13:27:01 +0100
Subject: [R] R plus SAS
In-Reply-To: <1115a2b004120412431d51278a@mail.gmail.com>
References: <1115a2b004120412431d51278a@mail.gmail.com>
Message-ID: <41B2FE95.5050106@statistik.uni-dortmund.de>

Wensui Liu wrote:
> Is it possible use SAS/DDE to communicate with R?

I don't know of any DDE implementation for R.

Uwe Ligges


> Sorry for offending useR. ^_^
> 
> Thanks a lot.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Sun Dec  5 13:58:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 5 Dec 2004 12:58:19 +0000 (UTC)
Subject: [R] Excel *.xls files, RODBC
References: <200412041449.iB4EnmCg008102@erdos.math.unb.ca>
Message-ID: <loom.20041205T135314-987@post.gmane.org>

Rolf Turner <rolf <at> math.unb.ca> writes:

: 
: I gather from reading the back-issues of r-help that it should be
: possible (modulo a number of caveats) to read an excel (yuck!) file
: into R using RODBC.  I have obtained and installed ODBC and the RODBC
: package, but cannot for the life of me figure out how to go about
: it.  Can anyone give me a simple recipe?
: 
: I have an excel file on cdrom, say:
: 
: 	/mnt/cdrom/melvin.xls
: 
: I have started R and loaded the RODBC package.  I want to create
: a data frame ``melvin'' by reading in /mnt/cdrom/melvin.xls.
: What (in monosyllables --- step by step) do I do next?

The xlhtml program at

  http://freshmeat.net/projects/xlhtml/

can not only convert .xls to .html but also to .csv using something
like:

  xlhtml -te -xc:1-10 -csv



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec  5 13:59:08 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 05 Dec 2004 12:59:08 -0000 (GMT)
Subject: [R] What is the most useful way to detect nonlinearity in lo
In-Reply-To: <41B25AFD.7010505@csus.edu>
Message-ID: <XFMail.041205125908.Ted.Harding@nessie.mcc.ac.uk>

On 05-Dec-04 Patrick Foley wrote:
> It is easy to spot response nonlinearity in normal linear
> models using plot(something.lm).
> However plot(something.glm) produces artifactual peculiarities
> since the diagnostic residuals are constrained  by the fact
> that y can only take values 0 or 1.
> What do R users find most useful in checking the linearity
> assumption of logistic regression (i.e. log-odds =a+bx)?
> 
> Patrick Foley
> patfoley at csus.edu

The "most useful way to detect nonlinearity in logistic
regression" is:

a) have an awful lot of data
b) have the x (covariate) values judiciously placed.

Don't be optimistic about this prohlem. The amount of
information, especially about non-linearity, in the binary
responses is often a lot less than people intuitively expect.

This is an area where R can be especially useful for
self-education by exploring possibilities and simulation.

For example, define the function (for quadratic nonlinearity):

  testlin2<-function(a,b,N){
    x<-c(-1.0,-0.5,0.0,0.5,1.0)
    lp<-a*x+b*x^2; p<-exp(lp)/(1+exp(lp))
    n<-N*c(1,1,1,1,1)
    r<-c(rbinom(1,n[1],p[1]),rbinom(1,n[2],p[2]),
         rbinom(1,n[3],p[3]),rbinom(1,n[4],p[4]),
         rbinom(1,n[5],p[5])
        )
    resp<-cbind(r,n-r)
    X<-cbind(x,x^2);colnames(X)<-c("x","x2")
    summary(glm(formula = resp ~ X - 1,
            family = binomial),correlation=TRUE)
  }

This places N observations at each of (-1.0,0.5,0.0.5,1.0),
generates the N binary responses with probability p(x)
where log(p/(1-p)) = a*x + b*x^2, fits a logistic regression
forcing the "intercept" term to be 0 (so that you're not
diluting the info by estimating a parameter you know to be 0),
and returns the summary(glm(...)) from which the p-values
can be extracted:

The p-value for x^2 is testlin2(a,b,N)$coefficients[2,4]}

You can run this function as a one-off for various values of
a, b, N to get a feel for what happens. You can run a simulation
on the lines of

  pvals<-numeric(1000);
  for(i in (1:1000)){
    pvals[i]<-testlin2(1,0.1,500)$coefficients[2,4]
  }

so that you can test how often you get a "significant" result.

For example, adopting the ritual "sigificant == P<0.05,
power = 80%", you can see a histogram of the p-values
over the conventional "significance breaks" with

hist(pvals,breaks=c(0,0.01,0.03,0.1,0.5,0.9,0.95,0.99,1),freq=TRUE)

and you can see your probability of getting a "significant" result
as e.g. sum(pvals < 0.05)/1000

I found that, with testlin2(1,0.1,N), i.e. a = 1.0, b = 0.1
corresponding to log(p/(1-p)) = x + 0.1*x^2 (a possibly
interesting degree of nonlinearity), I had to go up to N=2000
before I was getting more than 80% of the p-values < 0.05.
That corresponds to 2000 observations at each value of x, or
10,000 observations in all.

Compare this with a similar test for non-linearity with
normally-distributed responses [exercise for the reader].

You can write functions similar to testlin2 for higher-order
nonlinearlities, e.g. testlin3 for a*x + b*x^3, testlin23 for
a*x + b*x^2 + c*x^3, etc., (the modifications required are
obvious) and see how you get on. As I say, don't be optimistic!

In particular, run testlin3 a few times and see the sort of
mess that can come out -- in particular gruesome correlations,
which is why "correlation=TRUE" is set in the call to
summary(glm(...),correlation=TRUE).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 05-Dec-04                                       Time: 12:59:08
------------------------------ XFMail ------------------------------



From f.harrell at vanderbilt.edu  Sun Dec  5 14:17:21 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 05 Dec 2004 08:17:21 -0500
Subject: [R] What is the most useful way to detect nonlinearity in lo
In-Reply-To: <XFMail.041205125908.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041205125908.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41B30A61.1040203@vanderbilt.edu>

(Ted Harding) wrote:
> On 05-Dec-04 Patrick Foley wrote:
> 
>>It is easy to spot response nonlinearity in normal linear
>>models using plot(something.lm).
>>However plot(something.glm) produces artifactual peculiarities
>>since the diagnostic residuals are constrained  by the fact
>>that y can only take values 0 or 1.
>>What do R users find most useful in checking the linearity
>>assumption of logistic regression (i.e. log-odds =a+bx)?
>>
>>Patrick Foley
>>patfoley at csus.edu
> 
> 
> The "most useful way to detect nonlinearity in logistic
> regression" is:
> 
> a) have an awful lot of data
> b) have the x (covariate) values judiciously placed.
> 
> Don't be optimistic about this prohlem. The amount of
> information, especially about non-linearity, in the binary
> responses is often a lot less than people intuitively expect.
> 
> This is an area where R can be especially useful for
> self-education by exploring possibilities and simulation.
> 
> For example, define the function (for quadratic nonlinearity):
> 
>   testlin2<-function(a,b,N){
>     x<-c(-1.0,-0.5,0.0,0.5,1.0)
>     lp<-a*x+b*x^2; p<-exp(lp)/(1+exp(lp))
>     n<-N*c(1,1,1,1,1)
>     r<-c(rbinom(1,n[1],p[1]),rbinom(1,n[2],p[2]),
>          rbinom(1,n[3],p[3]),rbinom(1,n[4],p[4]),
>          rbinom(1,n[5],p[5])
>         )
>     resp<-cbind(r,n-r)
>     X<-cbind(x,x^2);colnames(X)<-c("x","x2")
>     summary(glm(formula = resp ~ X - 1,
>             family = binomial),correlation=TRUE)
>   }
> 
> This places N observations at each of (-1.0,0.5,0.0.5,1.0),
> generates the N binary responses with probability p(x)
> where log(p/(1-p)) = a*x + b*x^2, fits a logistic regression
> forcing the "intercept" term to be 0 (so that you're not
> diluting the info by estimating a parameter you know to be 0),
> and returns the summary(glm(...)) from which the p-values
> can be extracted:
> 
> The p-value for x^2 is testlin2(a,b,N)$coefficients[2,4]}
> 
> You can run this function as a one-off for various values of
> a, b, N to get a feel for what happens. You can run a simulation
> on the lines of
> 
>   pvals<-numeric(1000);
>   for(i in (1:1000)){
>     pvals[i]<-testlin2(1,0.1,500)$coefficients[2,4]
>   }
> 
> so that you can test how often you get a "significant" result.
> 
> For example, adopting the ritual "sigificant == P<0.05,
> power = 80%", you can see a histogram of the p-values
> over the conventional "significance breaks" with
> 
> hist(pvals,breaks=c(0,0.01,0.03,0.1,0.5,0.9,0.95,0.99,1),freq=TRUE)
> 
> and you can see your probability of getting a "significant" result
> as e.g. sum(pvals < 0.05)/1000
> 
> I found that, with testlin2(1,0.1,N), i.e. a = 1.0, b = 0.1
> corresponding to log(p/(1-p)) = x + 0.1*x^2 (a possibly
> interesting degree of nonlinearity), I had to go up to N=2000
> before I was getting more than 80% of the p-values < 0.05.
> That corresponds to 2000 observations at each value of x, or
> 10,000 observations in all.
> 
> Compare this with a similar test for non-linearity with
> normally-distributed responses [exercise for the reader].
> 
> You can write functions similar to testlin2 for higher-order
> nonlinearlities, e.g. testlin3 for a*x + b*x^3, testlin23 for
> a*x + b*x^2 + c*x^3, etc., (the modifications required are
> obvious) and see how you get on. As I say, don't be optimistic!
> 
> In particular, run testlin3 a few times and see the sort of
> mess that can come out -- in particular gruesome correlations,
> which is why "correlation=TRUE" is set in the call to
> summary(glm(...),correlation=TRUE).
> 
> Best wishes,
> Ted.

library(Design)  # also requires Hmisc
f <- lrm(sick ~ sex + rcs(age,4) + rcs(blood.pressure,5))
# Restricted cubic spline in age with 4 knots, blood.pressure with 5
anova(f)  # automatic tests of linearity of all predictors
latex(f)  # see algebraic form of fit
summary(f)# get odds ratios for meaningful changes in predictors

But beware of using the tests of linearity.  If non-significant results 
cause you to reduce an effect to linear, confidence levels and type I 
errors are no longer preserved.  I use tests of linearity mainly to 
demonstrate that effects are more often nonlinear than linear, given 
sufficient sample size.  I.e., good analysts are needed.  I usually 
leave non-significant nonlinearities in the model.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec  5 14:54:11 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 05 Dec 2004 13:54:11 -0000 (GMT)
Subject: [R] What is the most useful way to detect nonlinearity in lo
In-Reply-To: <XFMail.041205125908.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.041205135411.Ted.Harding@nessie.mcc.ac.uk>

On 05-Dec-04 Ted Harding wrote:
> [...]
> For example, adopting the ritual "sigificant == P<0.05,
> power = 80%", you can see a histogram of the p-values
> over the conventional "significance breaks" with
> 
> hist(pvals,breaks=c(0,0.01,0.03,0.1,0.5,0.9,0.95,0.99,1),freq=TRUE)

Sorry for the typo! That should of course (if you were looking
closely) have been

hist(pvals,breaks=c(0,0.01,0.05,0.1,0.5,0.9,0.95,0.99,1),freq=TRUE)

Apologies to anyone who was misled by pasting in the bad version.

> and you can see your probability of getting a "significant" result
> as e.g. sum(pvals < 0.05)/1000
> [...]

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 05-Dec-04                                       Time: 13:54:11
------------------------------ XFMail ------------------------------



From ernesto at ipimar.pt  Sun Dec  5 15:42:35 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Sun, 05 Dec 2004 14:42:35 +0000
Subject: [R] boot package
In-Reply-To: <42129374-465E-11D9-9661-000393B3E9D0@utah.edu>
References: <42129374-465E-11D9-9661-000393B3E9D0@utah.edu>
Message-ID: <1102257755.9234.4.camel@moria.ipimar.pt>

On Sun, 2004-12-05 at 01:37, Nathan Leon Pace, MD, MStat wrote:
> Hi,
> 
> I using the boot package 1.2-20 on R 2.0.1.
> 
> My statistics function estimates 6 parameters.
> 
> In a small percentage of resampled data sets my statistics function 
> doesn't produce an estimate for one parameter and the boot function 
> stops with an error.
> 
> I can write an ifelse(exists('parameter.estimate'), parameter.estimate, 
> NA) statement within the statistic function to substitute an NA for the 
> missing estimate value.
> 
> However, the boot.ci function to generate CIs from the boot object 
> won't accept NAs.
> 
> My problem is writing code to impute a numeric value for the missing 
> estimate. ifelse won't generate a numeric value if the test is mode 
> logical.
> 
> Any suggestions?
> 
> Nathan
> 
> 

Hi Nathan,

I had this problem also. The boot package assumes that the bootstrap
statistic has all ways the same dimension. The way I found to over come
this was to create a vector with the number of dimensions my statistic
was suppose to have and allocate each dimension to is own place on the
vector. Whenever you have a statistic with less dimensions you get an NA
or 0 or whatever you want, on that vector position.

Regards

EJ



From p.dalgaard at biostat.ku.dk  Sun Dec  5 16:15:57 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Dec 2004 16:15:57 +0100
Subject: [R] What is the most useful way to detect nonlinearity in
	logistic regression?
In-Reply-To: <x2oeh95uhd.fsf@biostat.ku.dk>
References: <41B25AFD.7010505@csus.edu> <x2oeh95uhd.fsf@biostat.ku.dk>
Message-ID: <x2y8gcdbde.fsf@biostat.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:


> Re. the smoothed residuals, you do need to be careful about the
> smoother. Some of the "robust" ones will do precisely the wrong thing
> in this context: You really are interested in the mean, not some
> trimmed mean (which can easily amount to throwing away all your
> cases...). Here's an idea:
> 
> x <- runif(500)
> y <- rbinom(500,size=1,p=plogis(x))
> xx <- predict(loess(resid(glm(y~x,binomial))~x),se=T)
> matplot(x,cbind(xx$fit, 2*xx$se.fit, -2*xx$se.fit),pch=20)
> 
> Not sure my money isn't still on the splines, though.

Doh. You might also want to make sure that the residuals are of a type
that can be _expected_ to have mean zero. Apparently, the default
deviance residuals do not have that property, whereas response
residuals do. I did check that loess (as opposed to lowess!) does a
plain least-squares based fitting by default, but I didn't think to
check what kind of residuals I was looking at.

Serves me right for posting way beyond my bedtime...

Anyways, you're probably better off believing Frank and not me in
these matters.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From damian.betebenner at bc.edu  Sun Dec  5 16:21:46 2004
From: damian.betebenner at bc.edu (Damian Betebenner)
Date: Sun, 05 Dec 2004 10:21:46 -0500
Subject: [R] String manipulation---mixed case
Message-ID: <web-2445987@be2.bc.edu>

Hello,

Does anyone know of a "slick" way to get R to convert a string which is all upper case to a string where the first letter in each word is upper case and all others are lower case?  

I suspect the solution is to begin by parsing the string, convert the appropriate letters to upper and lower case using "toupper" and "tolower", and then to paste the pieces back together. Moreover, given the elegance and power of R, I'll bet this can be coded very tersely. Before I embarked on trying to code this, I thought I would tap the R braintrust to see if this has already been done.

Any help greatly appreciated,

Damian  

Damian Betebenner
Educational Research, Measurement & Evaluation
Lynch School of Education
Boston College
Chestnut Hill, MA 02467

(617) 552 4491



From renaud.lancelot at cirad.fr  Sun Dec  5 16:38:03 2004
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sun, 05 Dec 2004 18:38:03 +0300
Subject: [R] String manipulation---mixed case
In-Reply-To: <web-2445987@be2.bc.edu>
References: <web-2445987@be2.bc.edu>
Message-ID: <41B32B5B.9000005@cirad.fr>

Damian Betebenner a ??crit :
> Hello,
> 
> Does anyone know of a "slick" way to get R to convert a string which
> is all upper case to a string where the first letter in each word is
> upper case and all others are lower case?
> 
> I suspect the solution is to begin by parsing the string, convert the
> appropriate letters to upper and lower case using "toupper" and
> "tolower", and then to paste the pieces back together. Moreover,
> given the elegance and power of R, I'll bet this can be coded very
> tersely. Before I embarked on trying to code this, I thought I would
> tap the R braintrust to see if this has already been done.
> 
> Any help greatly appreciated,
> 
> Damian
> 
> Damian Betebenner Educational Research, Measurement & Evaluation 
> Lynch School of Education Boston College Chestnut Hill, MA 02467
> 
> (617) 552 4491
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html
> 

This question was posted a while ago and answered by Christian
Hoffmann. I wrapped it in a small function:

CapLeading <- function (string){
   fn <- function(x){
     v <- unlist(strsplit(x, split = " "))
     u <- sapply(v, function(x){
            x <- tolower(x)
            substring(x, 1, 1) <- toupper(substring(x, 1, 1))
            x})
     paste(u, collapse = " ")
     }
   sapply(string, fn)
   }

Best,

Renaud

-- 
Dr Renaud Lancelot, v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From ggrothendieck at myway.com  Sun Dec  5 16:46:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 5 Dec 2004 15:46:57 +0000 (UTC)
Subject: [R] String manipulation---mixed case
References: <web-2445987@be2.bc.edu>
Message-ID: <loom.20041205T164245-966@post.gmane.org>

Damian Betebenner <damian.betebenner <at> bc.edu> writes:

: 
: Hello,
: 
: Does anyone know of a "slick" way to get R to convert a string which is all 
upper case to a string where the first
: letter in each word is upper case and all others are lower case?  
: 
: I suspect the solution is to begin by parsing the string, convert the 
appropriate letters to upper and lower
: case using "toupper" and "tolower", and then to paste the pieces back 
together. Moreover, given the
: elegance and power of R, I'll bet this can be coded very tersely. Before I 
embarked on trying to code this, I
: thought I would tap the R braintrust to see if this has already been done.
: 

Assuming x contains your string (or a vector of such strings), try this.
The first iteration replaces each A not following a word boundary 
with a.  The second does the analogous operation with B, etc.

for(L in LETTERS) x <- gsub(paste("\\B", L, sep = ""), tolower(L), x)



From muster at gmail.com  Sun Dec  5 17:06:17 2004
From: muster at gmail.com (Terry Mu)
Date: Sun, 5 Dec 2004 11:06:17 -0500
Subject: [R] how to calculate "conditional" mean?
Message-ID: <b68812e70412050806644d0403@mail.gmail.com>

a data set like this:

year   month    day    count
2001     1            1         10
2001     1            2          11
....
2004     7            17        8
....

basically it is a count of of some numbers everyday through a few years

now I'd like to get the mean of the count for every day. 

I thought I can do this using for and ifelse,
but is there a simple way to do this? I wish a function like 
mean(count, 'only when year, month, day are equal') could do this.

thank you.



From p.dalgaard at biostat.ku.dk  Sun Dec  5 17:18:59 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Dec 2004 17:18:59 +0100
Subject: [R] String manipulation---mixed case
In-Reply-To: <41B32B5B.9000005@cirad.fr>
References: <web-2445987@be2.bc.edu> <41B32B5B.9000005@cirad.fr>
Message-ID: <x2u0r0d8gc.fsf@biostat.ku.dk>

Renaud Lancelot <renaud.lancelot at cirad.fr> writes:

> This question was posted a while ago and answered by Christian
> Hoffmann. I wrapped it in a small function:
> 
> CapLeading <- function (string){
...

Version of same:

s <- "the quick red fox jumps over the lazy brown dog"
ss <- strsplit(s, " ")[[1]]
ss1 <- sub("(.).*", "\\1", ss)
ss2 <- sub(".", "", ss)
paste(toupper(ss1), ss2, sep="", collapse=" ")



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Sun Dec  5 17:26:11 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 5 Dec 2004 16:26:11 +0000 (UTC)
Subject: [R] how to calculate "conditional" mean?
References: <b68812e70412050806644d0403@mail.gmail.com>
Message-ID: <loom.20041205T171708-626@post.gmane.org>

Terry Mu <muster <at> gmail.com> writes:

: 
: a data set like this:
: 
: year   month    day    count
: 2001     1            1         10
: 2001     1            2          11
: ....
: 2004     7            17        8
: ....
: 
: basically it is a count of of some numbers everyday through a few years
: 
: now I'd like to get the mean of the count for every day. 
: 
: I thought I can do this using for and ifelse,
: but is there a simple way to do this? I wish a function like 
: mean(count, 'only when year, month, day are equal') could do this.

You want to represent your dates as objects of the Date class
and then use tapply.  (See ?as.Date, ?Date, ?tapply, ?paste, ?with. 
Also look up ?aggregate and ?by and read about dates in R News 4/1.)  
Assuming DF is your data frame:

    Dates <- with(DF,  
        as.Date( paste(year, month, day, sep="-") )  
    )
    tapply(DF$count, Dates, mean)



From edd at debian.org  Sun Dec  5 17:35:46 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 5 Dec 2004 10:35:46 -0600
Subject: [R] Excel *.xls files, RODBC
In-Reply-To: <loom.20041205T135314-987@post.gmane.org>
References: <200412041449.iB4EnmCg008102@erdos.math.unb.ca>
	<loom.20041205T135314-987@post.gmane.org>
Message-ID: <20041205163546.GA24809@sonny.eddelbuettel.com>

On Sun, Dec 05, 2004 at 12:58:19PM +0000, Gabor Grothendieck wrote:
> Rolf Turner <rolf <at> math.unb.ca> writes:
> 
> : 
> : I gather from reading the back-issues of r-help that it should be
> : possible (modulo a number of caveats) to read an excel (yuck!) file
> : into R using RODBC.  I have obtained and installed ODBC and the RODBC
> : package, but cannot for the life of me figure out how to go about
> : it.  Can anyone give me a simple recipe?
> : 
> : I have an excel file on cdrom, say:
> : 
> : 	/mnt/cdrom/melvin.xls
> : 
> : I have started R and loaded the RODBC package.  I want to create
> : a data frame ``melvin'' by reading in /mnt/cdrom/melvin.xls.
> : What (in monosyllables --- step by step) do I do next?
> 
> The xlhtml program at
> 
>   http://freshmeat.net/projects/xlhtml/
> 
> can not only convert .xls to .html but also to .csv using something
> like:
> 
>   xlhtml -te -xc:1-10 -csv

AFAIK there are about three main strands of tools to deal with this in a
manner that is platform-independent:

-- Perl based on SpreadSheet::ReadExcel and OLE::StorageLite, which Greg has
   wrapped up nicely in gdata, a component of the gregmisc bundle
   
-- C based libraries also used in Gnumeric, and, for that matter, also as a
   loadable module for GNU Gretl -- someone ambitious could add this to the
   foreign package; this may make for a nice term project. OpenOffice may have
   its own code base.
   
-- Apache/Jakarta/POI based, which I know little of, but Whit just told us
   that he has something in the works there

I think all three of these are maintained (in fact, I look after the Perl
and Gretl ones for Debian).  Where does xlhtml fit in?  The code seems to be
C based -- it this a split of the Gnumeric code?  Have there been updates
since 2002?

Dirk

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004



From muster at gmail.com  Sun Dec  5 18:04:26 2004
From: muster at gmail.com (Terry Mu)
Date: Sun, 5 Dec 2004 12:04:26 -0500
Subject: [R] how to calculate "conditional" mean?
In-Reply-To: <loom.20041205T171708-626@post.gmane.org>
References: <b68812e70412050806644d0403@mail.gmail.com>
	<loom.20041205T171708-626@post.gmane.org>
Message-ID: <b68812e70412050904539854ef@mail.gmail.com>

thank you,

one more question:

How can I list only values for given condition?
for example, I want to see only data from febuary, i.e. month=2?

On Sun, 5 Dec 2004 16:26:11 +0000 (UTC), Gabor Grothendieck
<ggrothendieck at myway.com> wrote:
> Terry Mu <muster <at> gmail.com> writes:
> 
> :
> : a data set like this:
> 
> 
> :
> : year   month    day    count
> : 2001     1            1         10
> : 2001     1            2          11
> : ....
> : 2004     7            17        8
> : ....
> :
> : basically it is a count of of some numbers everyday through a few years
> :
> : now I'd like to get the mean of the count for every day.
> :
> : I thought I can do this using for and ifelse,
> : but is there a simple way to do this? I wish a function like
> : mean(count, 'only when year, month, day are equal') could do this.
> 
> You want to represent your dates as objects of the Date class
> and then use tapply.  (See ?as.Date, ?Date, ?tapply, ?paste, ?with.
> Also look up ?aggregate and ?by and read about dates in R News 4/1.)
> Assuming DF is your data frame:
> 
>    Dates <- with(DF,
>        as.Date( paste(year, month, day, sep="-") )
>    )
>    tapply(DF$count, Dates, mean)
> 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From muster at gmail.com  Sun Dec  5 18:18:50 2004
From: muster at gmail.com (Terry Mu)
Date: Sun, 5 Dec 2004 12:18:50 -0500
Subject: [R] how to calculate "conditional" mean?
In-Reply-To: <b68812e70412050904539854ef@mail.gmail.com>
References: <b68812e70412050806644d0403@mail.gmail.com>
	<loom.20041205T171708-626@post.gmane.org>
	<b68812e70412050904539854ef@mail.gmail.com>
Message-ID: <b68812e7041205091837f73357@mail.gmail.com>

Sorry, I didn't express my problem clearly. Your answers surely help,
but it's not what I meant to ask.

I'd like mean for every day in a year, i.e, 
mean for January 1st, 2nd, 3rd, ....
Febuary, 1st, ..., 28th, 29th(if leap year)
...

There is only one count per day, so there is no need to calculate mean
for each date.

sorry for the confusion. I appreciate your further comment.

Terry Mu



On Sun, 5 Dec 2004 12:04:26 -0500, Terry Mu <muster at gmail.com> wrote:
> thank you,
> 
> one more question:
> 
> How can I list only values for given condition?
> for example, I want to see only data from febuary, i.e. month=2?
> 
> 
> 
> On Sun, 5 Dec 2004 16:26:11 +0000 (UTC), Gabor Grothendieck
> <ggrothendieck at myway.com> wrote:
> > Terry Mu <muster <at> gmail.com> writes:
> >
> > :
> > : a data set like this:
> >
> >
> > :
> > : year   month    day    count
> > : 2001     1            1         10
> > : 2001     1            2          11
> > : ....
> > : 2004     7            17        8
> > : ....
> > :
> > : basically it is a count of of some numbers everyday through a few years
> > :
> > : now I'd like to get the mean of the count for every day.
> > :
> > : I thought I can do this using for and ifelse,
> > : but is there a simple way to do this? I wish a function like
> > : mean(count, 'only when year, month, day are equal') could do this.
> >
> > You want to represent your dates as objects of the Date class
> > and then use tapply.  (See ?as.Date, ?Date, ?tapply, ?paste, ?with.
> > Also look up ?aggregate and ?by and read about dates in R News 4/1.)
> > Assuming DF is your data frame:
> >
> >    Dates <- with(DF,
> >        as.Date( paste(year, month, day, sep="-") )
> >    )
> >    tapply(DF$count, Dates, mean)
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From ggrothendieck at myway.com  Sun Dec  5 18:29:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 5 Dec 2004 17:29:07 +0000 (UTC)
Subject: [R] how to calculate "conditional" mean?
References: <b68812e70412050806644d0403@mail.gmail.com>
	<loom.20041205T171708-626@post.gmane.org>
	<b68812e70412050904539854ef@mail.gmail.com>
	<b68812e7041205091837f73357@mail.gmail.com>
Message-ID: <loom.20041205T182721-575@post.gmane.org>

Do you mean the mean for all January firsts, the mean for all January 2nds,
etc.?

Look up ?format in addition to the commands you looked up before:

   tapply(DF$count, format(Dates, "%Y-%m"), mean)


Terry Mu <muster <at> gmail.com> writes:

: 
: Sorry, I didn't express my problem clearly. Your answers surely help,
: but it's not what I meant to ask.
: 
: I'd like mean for every day in a year, i.e, 
: mean for January 1st, 2nd, 3rd, ....
: Febuary, 1st, ..., 28th, 29th(if leap year)
: ...
: 
: There is only one count per day, so there is no need to calculate mean
: for each date.
: 
: sorry for the confusion. I appreciate your further comment.
: 
: Terry Mu
: 
: 
: On Sun, 5 Dec 2004 12:04:26 -0500, Terry Mu <muster <at> gmail.com> wrote:
: > thank you,
: > 
: > one more question:
: > 
: > How can I list only values for given condition?
: > for example, I want to see only data from febuary, i.e. month=2?
: > 
: > 
: > 
: > On Sun, 5 Dec 2004 16:26:11 +0000 (UTC), Gabor Grothendieck
: > <ggrothendieck <at> myway.com> wrote:
: > > Terry Mu <muster <at> gmail.com> writes:
: > >
: > > :
: > > : a data set like this:
: > >
: > >
: > > :
: > > : year   month    day    count
: > > : 2001     1            1         10
: > > : 2001     1            2          11
: > > : ....
: > > : 2004     7            17        8
: > > : ....
: > > :
: > > : basically it is a count of of some numbers everyday through a few years
: > > :
: > > : now I'd like to get the mean of the count for every day.
: > > :
: > > : I thought I can do this using for and ifelse,
: > > : but is there a simple way to do this? I wish a function like
: > > : mean(count, 'only when year, month, day are equal') could do this.
: > >
: > > You want to represent your dates as objects of the Date class
: > > and then use tapply.  (See ?as.Date, ?Date, ?tapply, ?paste, ?with.
: > > Also look up ?aggregate and ?by and read about dates in R News 4/1.)
: > > Assuming DF is your data frame:
: > >
: > >    Dates <- with(DF,
: > >        as.Date( paste(year, month, day, sep="-") )
: > >    )
: > >    tapply(DF$count, Dates, mean)
: > >
: > >
: > >
: > > ______________________________________________
: > > R-help <at> stat.math.ethz.ch mailing list
: > > https://stat.ethz.ch/mailman/listinfo/r-help
: > > PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html
: > >
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ggrothendieck at myway.com  Sun Dec  5 18:33:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 5 Dec 2004 17:33:23 +0000 (UTC)
Subject: [R] how to calculate "conditional" mean?
References: <b68812e70412050806644d0403@mail.gmail.com>
	<loom.20041205T171708-626@post.gmane.org>
	<b68812e70412050904539854ef@mail.gmail.com>
Message-ID: <loom.20041205T183057-269@post.gmane.org>


Place Dates in DF and use subset(DF, month == 2) in place of DF.
I think you need to reread the Introduction to R manual since you
should find this sort of question answered there.  If not the pointers
I gave you should get you started.

Terry Mu <muster <at> gmail.com> writes:

: 
: thank you,
: 
: one more question:
: 
: How can I list only values for given condition?
: for example, I want to see only data from febuary, i.e. month=2?
: 
: On Sun, 5 Dec 2004 16:26:11 +0000 (UTC), Gabor Grothendieck
: <ggrothendieck <at> myway.com> wrote:
: > Terry Mu <muster <at> gmail.com> writes:
: > 
: > :
: > : a data set like this:
: > 
: > 
: > :
: > : year   month    day    count
: > : 2001     1            1         10
: > : 2001     1            2          11
: > : ....
: > : 2004     7            17        8
: > : ....
: > :
: > : basically it is a count of of some numbers everyday through a few years
: > :
: > : now I'd like to get the mean of the count for every day.
: > :
: > : I thought I can do this using for and ifelse,
: > : but is there a simple way to do this? I wish a function like
: > : mean(count, 'only when year, month, day are equal') could do this.
: > 
: > You want to represent your dates as objects of the Date class
: > and then use tapply.  (See ?as.Date, ?Date, ?tapply, ?paste, ?with.
: > Also look up ?aggregate and ?by and read about dates in R News 4/1.)
: > Assuming DF is your data frame:
: > 
: >    Dates <- with(DF,
: >        as.Date( paste(year, month, day, sep="-") )
: >    )
: >    tapply(DF$count, Dates, mean)
: > 
: > 
: > 
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help
: > PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ggrothendieck at myway.com  Sun Dec  5 18:30:16 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 5 Dec 2004 17:30:16 +0000 (UTC)
Subject: [R] how to calculate "conditional" mean?
References: <b68812e70412050806644d0403@mail.gmail.com>
	<loom.20041205T171708-626@post.gmane.org>
	<b68812e70412050904539854ef@mail.gmail.com>
Message-ID: <loom.20041205T182939-643@post.gmane.org>

In my previous post the format should have been "%m %d".

Terry Mu <muster <at> gmail.com> writes:

: 
: thank you,
: 
: one more question:
: 
: How can I list only values for given condition?
: for example, I want to see only data from febuary, i.e. month=2?
: 
: On Sun, 5 Dec 2004 16:26:11 +0000 (UTC), Gabor Grothendieck
: <ggrothendieck <at> myway.com> wrote:
: > Terry Mu <muster <at> gmail.com> writes:
: > 
: > :
: > : a data set like this:
: > 
: > 
: > :
: > : year   month    day    count
: > : 2001     1            1         10
: > : 2001     1            2          11
: > : ....
: > : 2004     7            17        8
: > : ....
: > :
: > : basically it is a count of of some numbers everyday through a few years
: > :
: > : now I'd like to get the mean of the count for every day.
: > :
: > : I thought I can do this using for and ifelse,
: > : but is there a simple way to do this? I wish a function like
: > : mean(count, 'only when year, month, day are equal') could do this.
: > 
: > You want to represent your dates as objects of the Date class
: > and then use tapply.  (See ?as.Date, ?Date, ?tapply, ?paste, ?with.
: > Also look up ?aggregate and ?by and read about dates in R News 4/1.)
: > Assuming DF is your data frame:
: > 
: >    Dates <- with(DF,
: >        as.Date( paste(year, month, day, sep="-") )
: >    )
: >    tapply(DF$count, Dates, mean)
: > 
: > 
: > 
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help
: > PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From muster at gmail.com  Sun Dec  5 18:52:33 2004
From: muster at gmail.com (Terry Mu)
Date: Sun, 5 Dec 2004 12:52:33 -0500
Subject: [R] how to calculate "conditional" mean?
In-Reply-To: <loom.20041205T183057-269@post.gmane.org>
References: <b68812e70412050806644d0403@mail.gmail.com>
	<loom.20041205T171708-626@post.gmane.org>
	<b68812e70412050904539854ef@mail.gmail.com>
	<loom.20041205T183057-269@post.gmane.org>
Message-ID: <b68812e704120509525ffff61c@mail.gmail.com>

thanks, this time it works perfectly.

I use

tapply(DF$count, format(Dates, '%m-%d'), mean).

as Patrick mentioned. It works without as.date().

with as.date() on, it gives a result one day after the actual date.

I also got subset().

Thank you all very much. I like R and will learn more.

Terry

On Sun, 5 Dec 2004 17:33:23 +0000 (UTC), Gabor Grothendieck
<ggrothendieck at myway.com> wrote:
> 
> Place Dates in DF and use subset(DF, month == 2) in place of DF.
> I think you need to reread the Introduction to R manual since you
> should find this sort of question answered there.  If not the pointers
> I gave you should get you started.
> 
> Terry Mu <muster <at> gmail.com> writes:
> 
> : 
> : thank you,
> 
> 
> :
> : one more question:
> :
> : How can I list only values for given condition?
> : for example, I want to see only data from febuary, i.e. month=2?
> :
> : On Sun, 5 Dec 2004 16:26:11 +0000 (UTC), Gabor Grothendieck
> : <ggrothendieck <at> myway.com> wrote:
> : > Terry Mu <muster <at> gmail.com> writes:
> : >
> : > :
> : > : a data set like this:
> : >
> : >
> : > :
> : > : year   month    day    count
> : > : 2001     1            1         10
> : > : 2001     1            2          11
> : > : ....
> : > : 2004     7            17        8
> : > : ....
> : > :
> : > : basically it is a count of of some numbers everyday through a few years
> : > :
> : > : now I'd like to get the mean of the count for every day.
> : > :
> : > : I thought I can do this using for and ifelse,
> : > : but is there a simple way to do this? I wish a function like
> : > : mean(count, 'only when year, month, day are equal') could do this.
> : >
> : > You want to represent your dates as objects of the Date class
> : > and then use tapply.  (See ?as.Date, ?Date, ?tapply, ?paste, ?with.
> : > Also look up ?aggregate and ?by and read about dates in R News 4/1.)
> : > Assuming DF is your data frame:
> : >
> : >    Dates <- with(DF,
> : >        as.Date( paste(year, month, day, sep="-") )
> : >    )
> : >    tapply(DF$count, Dates, mean)
> : >
> : >
> : >
> : > ______________________________________________
> : > R-help <at> stat.math.ethz.ch mailing list
> : > https://stat.ethz.ch/mailman/listinfo/r-help
> : > PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
> : >
> :
> : ______________________________________________
> : R-help <at> stat.math.ethz.ch mailing list
> : https://stat.ethz.ch/mailman/listinfo/r-help
> : PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> :
> :
> 
> ______________________________________________
> 
> 
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sander at oomvanlieshout.net  Sun Dec  5 19:14:51 2004
From: sander at oomvanlieshout.net (Sander Oom)
Date: Sun, 05 Dec 2004 20:14:51 +0200
Subject: [R] IFELSE across large array?
In-Reply-To: <200411232355.iANNtD1G007436@tahi.mcs.vuw.ac.nz>
References: <200411232355.iANNtD1G007436@tahi.mcs.vuw.ac.nz>
Message-ID: <6.2.0.14.0.20041129222333.052972d8@www.oomvanlieshout.net>

Dear all,

Thanks to the help from the R mailing list we now have a script performing 
the required tasks, i.e. applying a mask and filter to a 3D array. See 
script below. Any further suggestions to simplify the code are very 
welcome. We are currently debugging the larger script of which this section 
is a small part.

The script could serve as a framework for many typical GIS neighborhood 
analyses, in this case the 'block majority'. The 'var.range' variable is 
derived from variography earlier in the main script.

Yours,

Sander and Alessandro

##########INPUT SECTION1##############################
numRow<-7
numCol<-8
numReal<-2
var.range<-  2.1
cellsize<- 0.25

################################################
##CALCULATE WINDOW SIZES BASED ON INPUT ABOVE#####
#window size ...minimum is 1

numCells<-round((var.range*cellsize)/2,0)
if(numCells < 1) {numCells<-1}

###################################################
library(abind)

#Sim: 0=Absent; 1=Present; 10=NA
vectSim <- c(1,1,0,0,1,1,10,1,0,10,1,1,1,0,1,1,1,1,1,0,0,1,0,0,10,0,0,
1,1,1,0,1,1,0,0,0,1,1,1,0,1,1,0,1,0,1,10,0,1,0,0,1,0,0,10,0,0,0,1,1,0,
0,1,0,1,0,0,1,0,0,1,1,1,1,1,10,1,10,1,0,1,0,1,0,0,1,1,0,10,10,1,1,0,1,
0,1,1,1,0,0,0,1,0,0,0,0,0,0,1,1,0,0)
#Mask: 10=NA; 1=DATA
vectMask <- c(10,1,10,1,1,10,1,10,1,10,10,1,10,1,10,1,1,10,1,10,1,10,10,1,10,1,
10,1,1,10,1,10,1,10,10,1,10,1,10,1,1,10,1,10,1,10,10,1,10,1,10,1,1,10,1,10)
length(vectSim)
length(vectMask)
Sim <- array(vectSim, c(numRow,numCol,numReal))
Sim[Sim==10]<-NA
Sim
Mask <- array(data=vectMask, c(numRow,numCol,numReal))
Mask

##expand array
junkcol<-array(rep(NA,numReal*numRow),c(numRow,numCells,numReal))
#add columns borders

#cols
Sim<-abind(junkcol,Sim,junkcol, along=2)
dim(Sim)[2]
junkrow<-array(rep(NA,dim(Sim)[2]),c(numCells,dim(Sim)[2],numReal))
Sim<-abind(junkrow,Sim,junkrow,along=1)

##DEFINE PORTION OF ARRAY ON WHICH MOVING WINDOW RUNS
##IT AVOIDS THE na BORDER

#maximum row and col indexes to consider are equal
# to num num rows and num cols in the original matrix

minr<-1+numCells
maxr<-dim(Sim)[1]-numCells
minc<-1+numCells
maxc<-dim(Sim)[2]-numCells

clean<-function(a,nr,r,c) {
   rmin<-row(a)[r-numCells]
   rmax<-row(a)[r+numCells]
   cmin<-col(a)[nr*(c-numCells)]
   cmax<-col(a)[nr*(c+numCells)]

   junk<-a[rmin:rmax,cmin:cmax]
   junk[numCells,numCells]<-NA
   junk<-as.vector(junk)
   sample(na.omit(junk),1)
}

for (n in 1:numReal)  {
   realiz<-Sim[1:dim(Sim)[1],1:dim(Sim)[2],n]
   rz<-realiz
   for (r in minr:maxr)  {
     for (c in minc:maxc)  {
       if(is.na(realiz[r,c]) ) {
         realiz[r,c]<-clean(a=realiz,nr=numRow,r=r,c=c)
         Sim[1:dim(Sim)[1],1:dim(Sim)[2],n]<-realiz
       }#end if
     }
   }
}##end overall loop

# Return to original array dimensions, removing extra NA
minr <- (numCells+1)
maxr <- (dim(Sim)[1]-numCells)
minc <- (numCells+1)
maxc <- (dim(Sim)[2]-numCells)

expSim <- Sim
expSim
Sim <- expSim[minr:maxr, minc:maxc, 1:dim(Sim)[3]]
Sim

# Clip simulation results using the mask
Mask

Sim[Sim==10] <- NA
Mask[Mask==10] <- NA
A <- Sim + Mask -1
A

#################################




At 01:55 2004/11/24, Ray Brownrigg wrote:
> > From: "Liaw, Andy" <andy_liaw at merck.com>
> > Date: Tue, 23 Nov 2004 12:28:48 -0500
> >
> > I'll give it half a crack:
> >
> > Steps a through c can be done via nested ifelse(), treating A and M as
> > vectors (as they really are).  Step d is the hard one.  I'd write a simple
> > Fortran code and use .Fortran() for that.
> >
> > I don't see how any of the *apply() functions can help here, as your
> > operations are element-wise, not dimension-wise.
> >
> > Andy
> >
>The original message mentions that the value 10 is actually "NODATA",
>and if one recodes 10 as NA, steps a) to c) become trivial, namely:
>
>A[A == 10] <- NA
>M[M == 10] <- NA
>return(A + M - 1)
>
>Then if step d) is performed first (i.e. appropriate values in A are
>replaced by the 'most common neighbour' [perhaps using
>round(mean(.., na.rm=T))] this still works, but would have to be repeated
>for each replication (the third dimension).
>
>Ray Brownrigg
>
> > > From: Sander Oom
> > >
> > > Dear all,
> > >
> > > As our previous email did not get any response, we try again with a
> > > reformulated question!
> > >
> > > We are trying to do something which needs an efficient loop
> > > over a huge
> > > array, possibly functions such as apply and related (tapply,
> > > lapply...?), but can't really understand syntax and examples in
> > > practice...i.e. cant' make it work.
> > >
> > > to be more specific:
> > > we are trying to apply a mask to a 3D array.
> > > By this I mean that when "overlaying" [i.e. comparing element
> > > by element]
> > > the mask on to the array the mask should change array
> > > elements according to
> > > the values of both array and mask elements
> > >
> > > the mask has two values: 1 and 10.
> > >
> > > the array elements have 3 values: 0, 1,  or 10
> > >
> > > sticking for the moment to the single 2d array case
> > >
> > > for example:
> > > [A= array]  10    0 10 1  10  0
> > >                   1   10   1 0   0 10
> > >
> > > [ M=mask]     1  10  10 1   1  1
> > >                  10    1   1  1 10 10
> > >
> > > I would like the array elements to:
> > >
> > > a) IF A(ij) !=10 and  Mij = 1
> > >               leave A(ij) unchanged
> > >
> > > b)  IF   A(ij) != 10 and M(ij) =10
> > >                 change A(ij) to M(ij) i.e mask value (10)
> > >
> > > c)IF A(ij) = 10 and M(ij) = 10
> > >                leave (Aij) unchanged
> > >
> > > d) IF A(ij) = 10 and M(ij) !=10
> > >             replace A(ij) with the majority value in the
> > > 8-neighborhood
> > >
> > > (or whatever if it is an edge element) BUT ignoring 10s in this
> > > neighborhood (i.e. with either 1 or 0, whichever is in majority)
> > >
> > > because the array is 3d I would like to repeat the thing with
> > > all the k
> > > elements (2d sub-arrays) of the array in question, using the
> > > same mask for
> > > al k elements
> > >
> > > Would you be able to suggest a strategy to do this?
> > >
> > > thanks very much
> > >
> > > Alessandro and Sander.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Sun Dec  5 19:48:20 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 5 Dec 2004 18:48:20 +0000 (UTC)
Subject: [R] Excel *.xls files, RODBC
References: <200412041449.iB4EnmCg008102@erdos.math.unb.ca>
	<loom.20041205T135314-987@post.gmane.org>
	<20041205163546.GA24809@sonny.eddelbuettel.com>
Message-ID: <loom.20041205T194731-623@post.gmane.org>

> 
: Date:   Sun, 5 Dec 2004 10:35:46 -0600 
: From:   Dirk Eddelbuettel <edd at debian.org>
: To:   Gabor Grothendieck <ggrothendieck at myway.com> 
: Cc:   <r-help at stat.math.ethz.ch> 
: Subject:   Re: [R] Excel *.xls files, RODBC 
: 
:  
: On Sun, Dec 05, 2004 at 12:58:19PM +0000, Gabor Grothendieck wrote:
: > Rolf Turner <rolf <at> math.unb.ca> writes:
: > 
: > : 
: > : I gather from reading the back-issues of r-help that it should be
: > : possible (modulo a number of caveats) to read an excel (yuck!) file
: > : into R using RODBC. I have obtained and installed ODBC and the RODBC
: > : package, but cannot for the life of me figure out how to go about
: > : it. Can anyone give me a simple recipe?
: > : 
: > : I have an excel file on cdrom, say:
: > : 
: > :      /mnt/cdrom/melvin.xls
: > : 
: > : I have started R and loaded the RODBC package. I want to create
: > : a data frame ``melvin'' by reading in /mnt/cdrom/melvin.xls.
: > : What (in monosyllables --- step by step) do I do next?
: > 
: > The xlhtml program at
: > 
: > http://freshmeat.net/projects/xlhtml/
: > 
: > can not only convert .xls to .html but also to .csv using something
: > like:
: > 
: > xlhtml -te -xc:1-10 -csv
: 
: AFAIK there are about three main strands of tools to deal with this in a
: manner that is platform-independent:
: 
: -- Perl based on SpreadSheet::ReadExcel and OLE::StorageLite, which Greg has
: wrapped up nicely in gdata, a component of the gregmisc bundle
: 
: -- C based libraries also used in Gnumeric, and, for that matter, also as a
: loadable module for GNU Gretl -- someone ambitious could add this to the
: foreign package; this may make for a nice term project. OpenOffice may have
: its own code base.
: 
: -- Apache/Jakarta/POI based, which I know little of, but Whit just told us
: that he has something in the works there
: 
: I think all three of these are maintained (in fact, I look after the Perl
: and Gretl ones for Debian). Where does xlhtml fit in? The code seems to be
: C based -- it this a split of the Gnumeric code? Have there been updates
: since 2002?
: 
: Dirk
: 
: 

Its been a while since I actually used it myself.  I
use Windows where we have lots of ways to interface to Excel.
There once was an xlhtml.org address and the home page still
exists but now its at sourceforge.

Reviewing the code or posting to the Yahoo Groups xlhml list 
(see the link on the xlhtml home page) is your best bet 
for questions. Their list seems to have been taken over by 
spam recently but there were some real messages during the 
summer so you might get a reply.   Of course, there is also
google.



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec  5 20:38:00 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 05 Dec 2004 19:38:00 -0000 (GMT)
Subject: [R] What is the most useful way to detect nonlinearity in lo
In-Reply-To: <x2y8gcdbde.fsf@biostat.ku.dk>
Message-ID: <XFMail.041205193800.Ted.Harding@nessie.mcc.ac.uk>

On 05-Dec-04 Peter Dalgaard wrote:
> Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:
>> Re. the smoothed residuals, you do need to be careful about the
>> smoother. Some of the "robust" ones will do precisely the wrong thing
>> in this context: You really are interested in the mean, not some
>> trimmed mean (which can easily amount to throwing away all your
>> cases...). Here's an idea:
>> 
>> x <- runif(500)
>> y <- rbinom(500,size=1,p=plogis(x))
>> xx <- predict(loess(resid(glm(y~x,binomial))~x),se=T)
>> matplot(x,cbind(xx$fit, 2*xx$se.fit, -2*xx$se.fit),pch=20)
>> 
>> Not sure my money isn't still on the splines, though.
> 
> Doh. You might also want to make sure that the residuals are of a type
> that can be _expected_ to have mean zero. Apparently, the default
> deviance residuals do not have that property, whereas response
> residuals do. I did check that loess (as opposed to lowess!) does a
> plain least-squares based fitting by default, but I didn't think to
> check what kind of residuals I was looking at.
> 
> Serves me right for posting way beyond my bedtime...

Hi Peter,

Yes, the above is certainly misleading (try it with 2000 instead
of 500)! But what would you suggest instead?

Thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 05-Dec-04                                       Time: 19:38:00
------------------------------ XFMail ------------------------------



From hamish_macintyre at onetel.com  Sun Dec  5 22:06:39 2004
From: hamish_macintyre at onetel.com (Hamish Macintyre)
Date: Sun, 05 Dec 2004 21:06:39 +0000
Subject: [R] Histogram with equal-counts (-probability)
Message-ID: <BDD928DF.227%hamish_macintyre@onetel.com>

I would like to use R to generate a histogram which has bars of variable bin
width with each bar having an equal number of counts. For example, if the
bin limits are the quartiles, each bar would represent 1/4 of the total
probability in the distribution.
An example of such an equal-probability histogram is presented by Nicholas
Cox at http://www.stata.com/support/faqs/graphics/histvary.html.
    
Thanks,
Hamish



From ligges at statistik.uni-dortmund.de  Sun Dec  5 22:16:47 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 05 Dec 2004 22:16:47 +0100
Subject: [R] Histogram with equal-counts (-probability)
In-Reply-To: <BDD928DF.227%hamish_macintyre@onetel.com>
References: <BDD928DF.227%hamish_macintyre@onetel.com>
Message-ID: <41B37ABF.3000308@statistik.uni-dortmund.de>

Hamish Macintyre wrote:
> I would like to use R to generate a histogram which has bars of variable bin
> width with each bar having an equal number of counts. For example, if the
> bin limits are the quartiles, each bar would represent 1/4 of the total
> probability in the distribution.
> An example of such an equal-probability histogram is presented by Nicholas
> Cox at http://www.stata.com/support/faqs/graphics/histvary.html.

So you can calculate the quartiles using the quantiles() function and 
set those quartiles as breaks in hist().

Uwe Ligges


> Thanks,
> Hamish
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Sun Dec  5 22:48:07 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 05 Dec 2004 13:48:07 -0800
Subject: [R] String manipulation---mixed case
In-Reply-To: <x2u0r0d8gc.fsf@biostat.ku.dk>
References: <web-2445987@be2.bc.edu> <41B32B5B.9000005@cirad.fr>
	<x2u0r0d8gc.fsf@biostat.ku.dk>
Message-ID: <41B38217.50303@pdf.com>

      That's great, Peter. 

      For pedestrians like me who are not quite as facile with regular 
expressions, the following seems slightly more readable: 

s <- "the quick red fox jumps over the lazy brown dog"
ss <- strsplit(s, " ")[[1]]
ss1 <- substring(ss, 1,1)
ss2 <- substring(ss, 2)
paste(toupper(ss1), ss2, sep="", collapse=" ")

[1] "The Quick Red Fox Jumps Over The Lazy Brown Dog"

      Best Wishes,
      spencer graves

Peter Dalgaard wrote:

>Renaud Lancelot <renaud.lancelot at cirad.fr> writes:
>
>  
>
>>This question was posted a while ago and answered by Christian
>>Hoffmann. I wrapped it in a small function:
>>
>>CapLeading <- function (string){
>>    
>>
>...
>
>Version of same:
>
>s <- "the quick red fox jumps over the lazy brown dog"
>ss <- strsplit(s, " ")[[1]]
>ss1 <- sub("(.).*", "\\1", ss)
>ss2 <- sub(".", "", ss)
>paste(toupper(ss1), ss2, sep="", collapse=" ")
>
>
>
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From p.dalgaard at biostat.ku.dk  Sun Dec  5 23:45:44 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Dec 2004 23:45:44 +0100
Subject: [R] What is the most useful way to detect nonlinearity in lo
In-Reply-To: <XFMail.041205193800.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041205193800.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x28y8cictj.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> >> x <- runif(500)
> >> y <- rbinom(500,size=1,p=plogis(x))
> >> xx <- predict(loess(resid(glm(y~x,binomial))~x),se=T)
> >> matplot(x,cbind(xx$fit, 2*xx$se.fit, -2*xx$se.fit),pch=20)
> >> 
> >> Not sure my money isn't still on the splines, though.
.....
> > Serves me right for posting way beyond my bedtime...
> 
> Hi Peter,
> 
> Yes, the above is certainly misleading (try it with 2000 instead
> of 500)! But what would you suggest instead?

(I did and this little computer came tumbling down...). 

Basically, I'd reconsider the type= option to residual.glm. As I said,
at least type="response" should have the right mean. Ideally, you'd
want to take advantage of the fact that the variance of the residuals
is known too, rather than have the smoother estimate it. The more I
think, the more I like the splines...


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Mon Dec  6 01:13:53 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 06 Dec 2004 00:13:53 -0000 (GMT)
Subject: [R] What is the most useful way to detect nonlinearity in lo
In-Reply-To: <x28y8cictj.fsf@biostat.ku.dk>
Message-ID: <XFMail.041206001353.Ted.Harding@nessie.mcc.ac.uk>

On 05-Dec-04 Peter Dalgaard wrote:
> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
> 
>> >> x <- runif(500)
>> >> y <- rbinom(500,size=1,p=plogis(x))
>> >> xx <- predict(loess(resid(glm(y~x,binomial))~x),se=T)
>> >> matplot(x,cbind(xx$fit, 2*xx$se.fit, -2*xx$se.fit),pch=20)
>> >> 
>> >> Not sure my money isn't still on the splines, though.
> .....
>> > Serves me right for posting way beyond my bedtime...
>> 
>> Hi Peter,
>> 
>> Yes, the above is certainly misleading (try it with 2000 instead
>> of 500)! But what would you suggest instead?
> 
> (I did and this little computer came tumbling down...). 

So did mine -- but at 5000 (which is the value I first tried):
lots of disk grinding and then it went "prprprprp" and wrote
words to the effect "Calloc cannot allocate (18790050 times 4)"
i.e. it needed 72MB, which bankrupted my 192MB baby.

2000 was OK, however, but I had plenty of time for a meal etc.
before it finished.

Which brings up that predict(loess(....)) seems to be very
memory-hungry.

> Basically, I'd reconsider the type= option to residual.glm. As I said,
> at least type="response" should have the right mean. Ideally, you'd
> want to take advantage of the fact that the variance of the residuals
> is known too, rather than have the smoother estimate it. The more I
> think, the more I like the splines...

I'll have a go at your suggestions (if I can get the syntax right ... )

Thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 06-Dec-04                                       Time: 00:13:53
------------------------------ XFMail ------------------------------



From ssim at lic.co.nz  Mon Dec  6 01:36:37 2004
From: ssim at lic.co.nz (ssim@lic.co.nz)
Date: Mon, 6 Dec 2004 13:36:37 +1300
Subject: [R] Re : LOOPS
Message-ID: <OF2773E607.BA0442C3-ONCC256F62.00019E39-CC256F62.00035A5B@livestock.org.nz>

Dear lists,

I want to construct a loop in R, but don't know how to do it. I can do it
in SAS, but I prefer in R  (which I am hoping I will off SAS for good
soon). Could anyone help me to convert the SAS codes to equivalent R codes.

Basically, the following codes were written to establish the sire gametes
or phases for daughter design for one markers two alleles.

Here are the SAS code:

do i=1 to 744;
      do j=745 to 1540;
            m[j]=0;
            if sire[j]=anml[i] then do;
                if m1[j]=m1[i]  and m2[j]=m2[i] then m[j]=0;
                else if m1[j]=m1[i] then m[j] =1;
                else if m1[j]=m2[i] then m[j==2;
                else if m2[j]=m1[i] then m[j]=1;
                else if m2[j]=m2[i] then m[j]=2;
                else m[j]=0;
            end do;
     end;
end;


Thanks Stella
___________________________________________________________________________
This message, including attachments, is confidential. If you...{{dropped}}



From ssim at lic.co.nz  Mon Dec  6 02:51:22 2004
From: ssim at lic.co.nz (ssim@lic.co.nz)
Date: Mon, 6 Dec 2004 14:51:22 +1300
Subject: [R] Re : LOOPS
Message-ID: <OFCCDBDBBD.C7E2E832-ONCC256F62.00088F51-CC256F62.000A321B@livestock.org.nz>

Thanks Peter.

Basically, I am trying to establish the line of  marker allele the progeny
has inherited from either the sire or the dam.  The progeny will share the
sires but each have different dam (ie., daughter design or half-sib model).
The SAS code I have written to identify the lines of inheritance are far
from efficient and effective.  I would think matrix or vector is the way to
go, but I don't know SAS PROC IML well enough to do that nor do I have the
package on my machine.

Example :

Sire        m1    m2    n1    n2
100         1     2     1     2

Progeny     m1    m2    n1    n2
101         3     2     1     1

So the likely sire phase (gamete)  is 2 - 1 (m2 - n1).

I hope it helps.

Thanks again. Stella
___________________________________________________________________________
This message, including attachments, is confidential. If you...{{dropped}}



From ok at cs.otago.ac.nz  Mon Dec  6 03:23:34 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Mon, 6 Dec 2004 15:23:34 +1300 (NZDT)
Subject: [R] Which FM should beginners R?  A suggestion.
Message-ID: <200412060223.iB62NYdS009566@atlas.otago.ac.nz>

We've recently had a thread on beginners and FAQs and the like.
I decided that it might be a good idea to offer a short list of
on-line help pages that beginners should read.  Goals:
 - the list short be short (I think mine is too long)
 - many of the commonest questions should find answers there (fairish)
 - I should have found the pages helpful myself (no question about that)
 - it's a beginning R topic list, not a beginning statistics topic list
You can find my first draft at

    http://www.cs.otago.ac.nz/staffpriv/ok/R-help.txt

I think something _like_ this would be a genuinely useful addition to the
R documentation.  If people send me suggestions for additions, removals,
restructuring, &c, I will try to collate them and improve this.



From andy_liaw at merck.com  Mon Dec  6 03:26:02 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 5 Dec 2004 21:26:02 -0500
Subject: [R] What is the most useful way to detect nonlinearity in
 lo
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3DA@usrymx25.merck.com>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Ted.Harding at nessie.mcc.ac.uk
> Sent: Sunday, December 05, 2004 7:14 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] What is the most useful way to detect 
> nonlinearity in lo
> 
> 
> On 05-Dec-04 Peter Dalgaard wrote:
> > (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
> > 
> >> >> x <- runif(500)
> >> >> y <- rbinom(500,size=1,p=plogis(x))
> >> >> xx <- predict(loess(resid(glm(y~x,binomial))~x),se=T)
> >> >> matplot(x,cbind(xx$fit, 2*xx$se.fit, -2*xx$se.fit),pch=20)
> >> >> 
> >> >> Not sure my money isn't still on the splines, though.
> > .....
> >> > Serves me right for posting way beyond my bedtime...
> >> 
> >> Hi Peter,
> >> 
> >> Yes, the above is certainly misleading (try it with 2000 instead
> >> of 500)! But what would you suggest instead?
> > 
> > (I did and this little computer came tumbling down...). 
> 
> So did mine -- but at 5000 (which is the value I first tried):
> lots of disk grinding and then it went "prprprprp" and wrote
> words to the effect "Calloc cannot allocate (18790050 times 4)"
> i.e. it needed 72MB, which bankrupted my 192MB baby.
> 
> 2000 was OK, however, but I had plenty of time for a meal etc.
> before it finished.
> 
> Which brings up that predict(loess(....)) seems to be very
> memory-hungry.

locfit to the rescue, perhaps?

> library(locfit)
> n <- 5000
> x <- sort(runif(n))
> y <- rbinom(n, size=1, p=plogis(x))
> system.time(xx <- predict(locfit(resid(glm(y~x, binomial))~x),
where="data",
+                           se=TRUE), gcFirst=TRUE)
[1] 0.79 0.00 0.84   NA   NA
> matplot(x, cbind(xx$fit, 2*xx$se.fit, -2*xx$se.fit), pch=20)

[The plot looks strange...]

This is on my mobile Pentium 1.6GHz w/512MB laptop.  Using loess it also ran
out of memory.  At n=2000, 
the loess route took just under 3 seconds.

Cheers,
Andy
 
> > Basically, I'd reconsider the type= option to residual.glm. 
> As I said,
> > at least type="response" should have the right mean. Ideally, you'd
> > want to take advantage of the fact that the variance of the 
> residuals
> > is known too, rather than have the smoother estimate it. The more I
> > think, the more I like the splines...
> 
> I'll have a go at your suggestions (if I can get the syntax 
> right ... )
> 
> Thanks,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 06-Dec-04                                       Time: 00:13:53
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Mon Dec  6 03:36:20 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 5 Dec 2004 21:36:20 -0500
Subject: [R] Re: Protocol for answering basic questions
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3DB@usrymx25.merck.com>

I'll really be brief this time...

> From: Tony Plate
[snip] 
> Also, I think that John Maindonald's idea of a "I am new to 
> R, where do I 
> start?" page, with a link from the posting guide, is an 
> excellent idea.

Someone mentioned a tip shown at R startup (a la S-PLUS for Windows, I
guess).  I guess someone (hint, hint) could collect a set of tips, perhaps
using Paul Johnson's page as a starting point, and make it into a contrib
package similar to the `fortune' package.  Those who likes it can have a
random one displayed at startup.  Those who don't need not bother.
 
[snip]
> [Attribution 
> to Andy Liaw, or remain anonymous?]

Umm... I did say that out in the public, so I guess not much point in hiding
now...

Best,
Andy

> As some feel that sufficient time and bandwidth has already 
> been spent on 
> this issue, if anyone has any comments on this particular 
> matter of an 
> addition to the posting guide (or FAQ), feel free to choose 
> to respond to 
> me privately, and I will summarize as appropriate.
> 
> -- Tony Plate
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From DFARRAR at vt.edu  Mon Dec  6 04:09:02 2004
From: DFARRAR at vt.edu (DFARRAR)
Date: Sun, 5 Dec 2004 22:09:02 -0500
Subject: [R] tree class in R?
Message-ID: <41C4F89A@zathras>

I am trying to store a couple numbers for each partition, in a subset of the 
partitions 
of my data set.  Of course, one can accomplish this using a binary tree.  (The 
first split is on 
inclusion/exclusion of the first object, and so on.)  I can probably simulate 
a tree using 
vectors.  (One vector gives the index of left child node, another the index of 
the right child node.)
However, it seems like there must be a useful class associated with the 
clustering 
or recursive partitioning procedures, perhaps not out there for everyone to 
see.  I poked around
on the R page and didn't see anything that clearly met my needs very directly.
 I hope I would not 
have to learn recursive partitioning in R to find what I need.



From Tom.Mulholland at dpi.wa.gov.au  Mon Dec  6 04:24:45 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Mon, 6 Dec 2004 11:24:45 +0800
Subject: [R] tree class in R?
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA46@afhex01.dpi.wa.gov.au>

Have you had a look at the rpart package? If you haven't installed it it may be worth doing so. Then you can type

require(rpart)
?rpart.object

Tom

> -----Original Message-----
> From: DFARRAR [mailto:DFARRAR at vt.edu]
> Sent: Monday, 6 December 2004 11:09 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] tree class in R?
> 
> 
> I am trying to store a couple numbers for each partition, in 
> a subset of the 
> partitions 
> of my data set.  Of course, one can accomplish this using a 
> binary tree.  (The 
> first split is on 
> inclusion/exclusion of the first object, and so on.)  I can 
> probably simulate 
> a tree using 
> vectors.  (One vector gives the index of left child node, 
> another the index of 
> the right child node.)
> However, it seems like there must be a useful class 
> associated with the 
> clustering 
> or recursive partitioning procedures, perhaps not out there 
> for everyone to 
> see.  I poked around
> on the R page and didn't see anything that clearly met my 
> needs very directly.
>  I hope I would not 
> have to learn recursive partitioning in R to find what I need.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From wt43 at cornell.edu  Mon Dec  6 05:07:31 2004
From: wt43 at cornell.edu (Janice Tse)
Date: Sun, 5 Dec 2004 23:07:31 -0500
Subject: [R] Gam() function in R
In-Reply-To: <41C4F89A@zathras>
Message-ID: <200412060407.iB647a8T016768@postoffice8.mail.cornell.edu>

Hi all,

I'm   a new user of R gam() function. I am wondering how do we decide on the
smooth function to use?
The general form is gam(y~s(x1,df=i)+s(x2,df=j).......)  , how do we decide
on the degree freedom to use for each smoother, and if we shold apply
smoother to each attribute?

Thanks!!



From andy_liaw at merck.com  Mon Dec  6 05:33:44 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 5 Dec 2004 23:33:44 -0500
Subject: [R] Gam() function in R
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3DC@usrymx25.merck.com>

Unfortunately that's not really an R question.  I recommend that you read up
on the statistical methods underneath.  One that I'd wholeheartedly
recommend is Prof. Harrell's `Regression Modeling Strategies'.

[BTW, there are now two implementations of gam() in R: one in `mgcv', which
is fairly different from that in  `gam'.  I'm guessing you're referring to
the one in `gam', but please remember to state which contributed package
you're using, along with version of R and OS.]

Cheers,
Andy

> From: Janice Tse
> 
> Hi all,
> 
> I'm   a new user of R gam() function. I am wondering how do 
> we decide on the
> smooth function to use?
> The general form is gam(y~s(x1,df=i)+s(x2,df=j).......)  , 
> how do we decide
> on the degree freedom to use for each smoother, and if we shold apply
> smoother to each attribute?
> 
> Thanks!!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Mon Dec  6 05:52:09 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 6 Dec 2004 04:52:09 +0000 (UTC)
Subject: [R] tree class in R?
References: <41C4F89A@zathras>
Message-ID: <loom.20041206T055017-259@post.gmane.org>

DFARRAR <DFARRAR <at> vt.edu> writes:

: 
: I am trying to store a couple numbers for each partition, in a subset of the 
: partitions 
: of my data set.  Of course, one can accomplish this using a binary tree.  
(The 
: first split is on 
: inclusion/exclusion of the first object, and so on.)  I can probably 
simulate 
: a tree using 
: vectors.  (One vector gives the index of left child node, another the index 
of 
: the right child node.)
: However, it seems like there must be a useful class associated with the 
: clustering 
: or recursive partitioning procedures, perhaps not out there for everyone to 
: see.  I poked around
: on the R page and didn't see anything that clearly met my needs very 
directly.
:  I hope I would not 
: have to learn recursive partitioning in R to find what I need.

Do a search of the r-help archives.  There was binary tree code posted
within the last year.



From chancs at gis.a-star.edu.sg  Mon Dec  6 06:19:53 2004
From: chancs at gis.a-star.edu.sg (CHAN Chee Seng)
Date: Mon, 6 Dec 2004 13:19:53 +0800
Subject: [R] core dump during make check when building 64-bit R on
	Solaris8/9
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0E7@BIONIC.biopolis.one-north.com>

Again, thanks for all the replies and suggestions.

At this point, I guess the only fix is NOT to use libsunperf from sun
studio 8.  If anyone really wants to use libsunperf, use sun one studio
7 or 9's instead.

I will also try and send a bug report to the sun folks (in case there
are people like me who are still stuck with Sun Studio 8 and would like
to update their buggy libsunperf), and post any findings back here.

Regards,
Chee Seng

-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: Friday, December 03, 2004 4:32 PM
To: CHAN Chee Seng
Cc: Prof Brian Ripley; Peter Dalgaard; r-help at stat.math.ethz.ch
Subject: Re: [R] core dump during make check when building 64-bit R on
Solaris8/9

"CHAN Chee Seng" <chancs at gis.a-star.edu.sg> writes:

> Can you write/send me the standalone program to show the problem so I
> can send to some Sun people that I know?

Not at this point, I was hoping you had the time.
 
> BTW, how did you run R under dbx?  (Sorry but I am not familiar with
> using dbx.)  I get this error:

R -d dbx

then "run", etc...


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From yzhang4 at pobox.une.edu.au  Mon Dec  6 06:34:59 2004
From: yzhang4 at pobox.une.edu.au (Yuandan Zhang)
Date: Mon, 6 Dec 2004 16:34:59 +1100
Subject: [R] Re : LOOPS
In-Reply-To: <OF2773E607.BA0442C3-ONCC256F62.00019E39-CC256F62.00035A5B@livestock.org.nz>
References: <OF2773E607.BA0442C3-ONCC256F62.00019E39-CC256F62.00035A5B@livestock.org.nz>
Message-ID: <20041206163459.0c750778.yzhang4@pobox.une.edu.au>

On Mon, 6 Dec 2004 13:36:37 +1300
ssim at lic.co.nz wrote:

> Dear lists,
> 
> I want to construct a loop in R, but don't know how to do it. I can do it
> in SAS, but I prefer in R  (which I am hoping I will off SAS for good
> soon). Could anyone help me to convert the SAS codes to equivalent R codes.
> 
> Basically, the following codes were written to establish the sire gametes
> or phases for daughter design for one markers two alleles.
> 
> Here are the SAS code:
> 

You may try this code. There may be better code from others.

m<-array(0, 795) #phase
for ( i in 1:744) {  # sire
  for (j in 745:1540) { # progeny
    if (sire[j] == anim[i] ) {  #check if the progeny j is sired by animal i
      if (m1[j]==m1[i]  &&  m2[j]==m2[i] ) { m[j]=0; next}
      if ( m1[j]==m1[i]) { m[j] = 1; next}
      if (m1[j]==m2[i]) { m[j==2; next}
  
.....

    }
  }
}   




> do i=1 to 744;
>       do j=745 to 1540;
>             m[j]=0;
>             if sire[j]=anml[i] then do;
>                 if m1[j]=m1[i]  and m2[j]=m2[i] then m[j]=0;
>                 else if m1[j]=m1[i] then m[j] =1;
>                 else if m1[j]=m2[i] then m[j==2;
>                 else if m2[j]=m1[i] then m[j]=1;
>                 else if m2[j]=m2[i] then m[j]=2;
>                 else m[j]=0;
>             end do;
>      end;
> end;
> 
> 
> Thanks Stella
> ___________________________________________________________________________
> This message, including attachments, is confidential. If you...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 

--
Yuandan Zhang, PhD

Animal Genetics and Breeding Unit
The University of New England
Armidale, NSW, Australia, 2351

E-mail:   yzhang4 at metz.une.edu.au
Phone:    (61) 02 6773 3786
Fax:      (61) 02 6773 3266
http://agbu.une.edu.au
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  AGBU is a joint venture of NSW Primary Industries 
  and The University of New England to undertake 
  genetic R&D for Australia's Livestock Industries           
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From wt43 at cornell.edu  Mon Dec  6 06:36:21 2004
From: wt43 at cornell.edu (Janice Tse)
Date: Mon, 6 Dec 2004 00:36:21 -0500
Subject: [R] Gam() function in R
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E3DC@usrymx25.merck.com>
Message-ID: <200412060536.iB65aO8U029853@postoffice8.mail.cornell.edu>

Thanks for the email. I will check that out....

However when I was doing this :    gam(y~s(x1)+s(x2,3), family=gaussian,
data=mydata )it gives me  the error :

"Error in terms.formula(formula, data = data) : 
        invalid model formula in ExtractVars"

What does it mean ?

Thanks
-Janice 

-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: Sunday, December 05, 2004 11:34 PM
To: 'Janice Tse'; r-help at stat.math.ethz.ch
Subject: RE: [R] Gam() function in R

Unfortunately that's not really an R question.  I recommend that you read up
on the statistical methods underneath.  One that I'd wholeheartedly
recommend is Prof. Harrell's `Regression Modeling Strategies'.

[BTW, there are now two implementations of gam() in R: one in `mgcv', which
is fairly different from that in  `gam'.  I'm guessing you're referring to
the one in `gam', but please remember to state which contributed package
you're using, along with version of R and OS.]

Cheers,
Andy

> From: Janice Tse
> 
> Hi all,
> 
> I'm   a new user of R gam() function. I am wondering how do 
> we decide on the
> smooth function to use?
> The general form is gam(y~s(x1,df=i)+s(x2,df=j).......)  , how do we 
> decide on the degree freedom to use for each smoother, and if we shold 
> apply smoother to each attribute?
> 
> Thanks!!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From jari.oksanen at oulu.fi  Mon Dec  6 09:09:47 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Mon, 06 Dec 2004 10:09:47 +0200
Subject: [R] Gam() function in R
In-Reply-To: <200412060536.iB65aO8U029853@postoffice8.mail.cornell.edu>
References: <200412060536.iB65aO8U029853@postoffice8.mail.cornell.edu>
Message-ID: <3235E28C-475E-11D9-A29C-000A95C76CA8@oulu.fi>


On 6 Dec 2004, at 7:36, Janice Tse wrote:

> Thanks for the email. I will check that out....
>
> However when I was doing this :    gam(y~s(x1)+s(x2,3),  
> family=gaussian,
> data=mydata )it gives me  the error :
>
> "Error in terms.formula(formula, data = data) :
>         invalid model formula in ExtractVars"
>
> What does it mean ?
>
When Any Liaw answered you (below), he asked you to specify which kind  
of 'gam' did you use: the one in standard package 'mgcv' or the one in  
package 'gam'. We should know this to know "what does it mean" to get   
your error message. If you used mgcv:::gam, it means that you didn't  
read it help pages which say that you should specify your model as:

gam(y ~ s(x1) + s(x2, k=3))

Further, it may be useful to read the help pages to understand what it  
means to specify k=3 and how it may influence your model. Simon Wood --  
the mgcv author -- also has a very useful article in the R Newsletter:  
see the CRAN archive. It may be really difficult to understand what you  
do when  you do mgcv:::gam unless you read this paper (it is possible,  
but hard). Simon's article specifically answers to your first question  
of deciding the smoothness, and explains how elegantly this is done in  
mgcv:::gam (gam:::gam has another set of tools and philosophy).

If you happened to use gam:::gam, then you have to look at another  
explanation.

cheers, jari oksanen

> From: Liaw, Andy [mailto:andy_liaw at merck.com]
> Sent: Sunday, December 05, 2004 11:34 PM
> To: 'Janice Tse'; r-help at stat.math.ethz.ch
> Subject: RE: [R] Gam() function in R
>
> Unfortunately that's not really an R question.  I recommend that you  
> read up
> on the statistical methods underneath.  One that I'd wholeheartedly
> recommend is Prof. Harrell's `Regression Modeling Strategies'.
>
> [BTW, there are now two implementations of gam() in R: one in `mgcv',  
> which
> is fairly different from that in  `gam'.  I'm guessing you're  
> referring to
> the one in `gam', but please remember to state which contributed  
> package
> you're using, along with version of R and OS.]
>
> Cheers,
> Andy
>
>> From: Janice Tse
>>
>> Hi all,
>>
>> I'm   a new user of R gam() function. I am wondering how do
>> we decide on the
>> smooth function to use?
>> The general form is gam(y~s(x1,df=i)+s(x2,df=j).......)  , how do we
>> decide on the degree freedom to use for each smoother, and if we shold
>> apply smoother to each attribute?
>>
>> Thanks!!
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
>
> ----------------------------------------------------------------------- 
> -----
> --
> Notice:  This e-mail message, together with any  
> attachments,...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>
--
Jari Oksanen, Oulu, Finland



From mungkle2 at hanmail.net  Mon Dec  6 09:31:05 2004
From: mungkle2 at hanmail.net (always)
Date: Mon, 06 Dec 2004 17:31:05 +0900 (KST)
Subject: [R] Use value of character vector for parameter of general
	R-function
Message-ID: <20041206173105.HM.00000000000B0Nh@wwl296.hanmail.net>



From leroy at ucsd.edu  Mon Dec  6 09:33:59 2004
From: leroy at ucsd.edu (Anthony Westerling)
Date: Mon, 6 Dec 2004 00:33:59 -0800
Subject: [R] Cocoa GUI:  pasting in R Console yields syntax error
Message-ID: <937218FB-4761-11D9-9364-000A959EA8AA@ucsd.edu>

I've recently upgraded to R-2.0.1 on a Mac running OS X 10.3+

I am using the new Cocoa-based GUI.  Everything was working well for a 
while.  In the middle of an R session, I started "suddenly" to have a 
problem where code copied from an open editor window and pasted into 
the R Console gives a syntax error.  It doesn't matter what the code 
is.  If the same exact text is typed into the console directly, I get 
no errors.

I tried quitting the R session and restarting.  The problem did not go 
away.

I tried using a different editor, instead of the built-in editor.  
After opening the file with my R code in it in AlphaX instead of the 
built-in editor, I could see that the text typed most recently ( ie, 
since the problem started) had a character that looked like an open 
square or box at the start of most lines.  I deleted these and can't 
see any other extraneous symbols in AlphaX.  However, I still get 
syntax errors when trying to paste code that was originally typed in 
using the built-in editor.  If I retype the same thing in the same file 
using AlphaX, one line below the original, then copy and paste into the 
R console, it executes without generating syntax errors.

So, it looks like something odd is going on with the built-in editor?

Anthony Westerling



From dle at aber.ac.uk  Mon Dec  6 10:23:24 2004
From: dle at aber.ac.uk (David Enot)
Date: Mon, 6 Dec 2004 09:23:24 +0000
Subject: [R] Cocoa GUI:  pasting in R Console yields syntax error
In-Reply-To: <937218FB-4761-11D9-9364-000A959EA8AA@ucsd.edu>
References: <937218FB-4761-11D9-9364-000A959EA8AA@ucsd.edu>
Message-ID: <7AEF1A0D-4768-11D9-83FC-000A95BBEB3C@aber.ac.uk>

Anthony

I faced the same problem and it took me some time to spot the origin: I 
have no clue where it can come from!( I suspect this happened when I 
moved from 1.9.0 to 2.0.0, OS X 1.3...). I know that the built in R 
editor is very handy: what I do is a "more myfile.r" on the terminal to 
check if there are weird characters and then delete then directly with 
the R editor. Not very efficient I reckon: OS X gurus may have another 
solution!

   David


On 6 Dec 2004, at 08:33, Anthony Westerling wrote:

> I've recently upgraded to R-2.0.1 on a Mac running OS X 10.3+
>
> I am using the new Cocoa-based GUI.  Everything was working well for a 
> while.  In the middle of an R session, I started "suddenly" to have a 
> problem where code copied from an open editor window and pasted into 
> the R Console gives a syntax error.  It doesn't matter what the code 
> is.  If the same exact text is typed into the console directly, I get 
> no errors.
>
> I tried quitting the R session and restarting.  The problem did not go 
> away.
>
> I tried using a different editor, instead of the built-in editor.  
> After opening the file with my R code in it in AlphaX instead of the 
> built-in editor, I could see that the text typed most recently ( ie, 
> since the problem started) had a character that looked like an open 
> square or box at the start of most lines.  I deleted these and can't 
> see any other extraneous symbols in AlphaX.  However, I still get 
> syntax errors when trying to paste code that was originally typed in 
> using the built-in editor.  If I retype the same thing in the same 
> file using AlphaX, one line below the original, then copy and paste 
> into the R console, it executes without generating syntax errors.
>
> So, it looks like something odd is going on with the built-in editor?
>
> Anthony Westerling
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ym at climpact.com  Mon Dec  6 10:48:26 2004
From: ym at climpact.com (Yves Magliulo)
Date: 06 Dec 2004 10:48:26 +0100
Subject: [R] Gam() function in R
In-Reply-To: <3235E28C-475E-11D9-A29C-000A95C76CA8@oulu.fi>
References: <200412060536.iB65aO8U029853@postoffice8.mail.cornell.edu>
	<3235E28C-475E-11D9-A29C-000A95C76CA8@oulu.fi>
Message-ID: <1102326506.18602.60.camel@new-york.climpact.net>

hi all,

this subject is very intersting for me. I'm using mgcv 0.8-9 with R
version 1.7.1. i didn't know that there was an another gam version with
package library(gam). Someone can tell me the basics differences between
them? I look for an help page on google but i only find "mgcv" help
pages.

thanks!

yves magliulo, Paris. 


Le lun 06/12/2004 ?? 09:09, Jari Oksanen a ??crit :
> On 6 Dec 2004, at 7:36, Janice Tse wrote:
> 
> > Thanks for the email. I will check that out....
> >
> > However when I was doing this :    gam(y~s(x1)+s(x2,3),  
> > family=gaussian,
> > data=mydata )it gives me  the error :
> >
> > "Error in terms.formula(formula, data = data) :
> >         invalid model formula in ExtractVars"
> >
> > What does it mean ?
> >
> When Any Liaw answered you (below), he asked you to specify which kind  
> of 'gam' did you use: the one in standard package 'mgcv' or the one in  
> package 'gam'. We should know this to know "what does it mean" to get   
> your error message. If you used mgcv:::gam, it means that you didn't  
> read it help pages which say that you should specify your model as:
> 
> gam(y ~ s(x1) + s(x2, k=3))
> 
> Further, it may be useful to read the help pages to understand what it  
> means to specify k=3 and how it may influence your model. Simon Wood --  
> the mgcv author -- also has a very useful article in the R Newsletter:  
> see the CRAN archive. It may be really difficult to understand what you  
> do when  you do mgcv:::gam unless you read this paper (it is possible,  
> but hard). Simon's article specifically answers to your first question  
> of deciding the smoothness, and explains how elegantly this is done in  
> mgcv:::gam (gam:::gam has another set of tools and philosophy).
> 
> If you happened to use gam:::gam, then you have to look at another  
> explanation.
> 
> cheers, jari oksanen
> 
> > From: Liaw, Andy [mailto:andy_liaw at merck.com]
> > Sent: Sunday, December 05, 2004 11:34 PM
> > To: 'Janice Tse'; r-help at stat.math.ethz.ch
> > Subject: RE: [R] Gam() function in R
> >
> > Unfortunately that's not really an R question.  I recommend that you  
> > read up
> > on the statistical methods underneath.  One that I'd wholeheartedly
> > recommend is Prof. Harrell's `Regression Modeling Strategies'.
> >
> > [BTW, there are now two implementations of gam() in R: one in `mgcv',  
> > which
> > is fairly different from that in  `gam'.  I'm guessing you're  
> > referring to
> > the one in `gam', but please remember to state which contributed  
> > package
> > you're using, along with version of R and OS.]
> >
> > Cheers,
> > Andy
> >
> >> From: Janice Tse
> >>
> >> Hi all,
> >>
> >> I'm   a new user of R gam() function. I am wondering how do
> >> we decide on the
> >> smooth function to use?
> >> The general form is gam(y~s(x1,df=i)+s(x2,df=j).......)  , how do we
> >> decide on the degree freedom to use for each smoother, and if we shold
> >> apply smoother to each attribute?
> >>
> >> Thanks!!
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >>
> >
> >
> > ----------------------------------------------------------------------- 
> > -----
> > --
> > Notice:  This e-mail message, together with any  
> > attachments,...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!  
> > http://www.R-project.org/posting-guide.html
> >
> --
> Jari Oksanen, Oulu, Finland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Jean-Pierre.Mueller at unil.ch  Mon Dec  6 11:47:19 2004
From: Jean-Pierre.Mueller at unil.ch (Jean-Pierre Muller)
Date: Mon, 6 Dec 2004 11:47:19 +0100
Subject: [R] Text Mining with R
In-Reply-To: <21c05c7d04120312233eb46bf4@mail.gmail.com>
References: <200412021829.31979.daniele.medri@libero.it>
	<20041203191557.043b3141.tobias.verbeke@telenet.be>
	<21c05c7d04120312233eb46bf4@mail.gmail.com>
Message-ID: <33CF3546-4774-11D9-B49D-000D93AE2752@dssp.unil.ch>

Jose,

Le 3 d??c. 04, ?? 21:23, Jose Quesada a ??crit :

> Tobias,
>
> I just created a zip file from the tar, and used the "install from
> zip" option of the Rwin console.
>
> ttda is shown in the list of installed packages. However, when I try
> "load packages", or the equivaent "library(ttda)", I get:
>
> Error in library(ttda) : 'ttda' is not a valid package -- installed < 
> 2.0.0?
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> Error in library(pkg, character.only = TRUE) :
>         'ttda' is not a valid package -- installed < 2.0.0?
>
> Do you know why?
>

I have put a link to the zip version of ttda on 
http://wwwpeople.unil.ch/jean-pierre.mueller/
I have been able to load it (using "packages -> Install package(s) from 
local zip files..."),
and to make a few tests, but i have not tested it well on a windows 
machine with R 2.0.x.,
I will not have time in the next few days. But on R for Mac OS X Aqua 
(2.0.1), I have no
problems (R CMD CHECK ttda ->  OK)  & (Installing *source* package 
'ttda' ... ->
package successfully installed ) .
HTH




-- 
Jean-Pierre M??ller
SSP / BFSH2 / UNIL / CH - 1015 Lausanne
Voice:+41 21 692 3116 / Fax:+41 21 692 3115



From bitwrit at ozemail.com.au  Tue Dec  7 18:38:00 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 8 Dec 2004 04:38:00 +1100
Subject: [R] Re: Protocol for answering basic questions
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E3DB@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E3DB@usrymx25.merck.com>
Message-ID: <20041206105137.QQQW3114.smta05.mail.ozemail.net@there>

Liaw, Andy wrote:
> ...
> Someone mentioned a tip shown at R startup (a la S-PLUS for Windows, I
> guess).  I guess someone (hint, hint) could collect a set of tips, perhaps
> using Paul Johnson's page as a starting point, and make it into a contrib
> package similar to the `fortune' package.  Those who likes it can have a
> random one displayed at startup.  Those who don't need not bother.
>
Remember, the more complete the list of tips, the less likely that you will 
want to read any given one when it appears. This is probably why I turn this 
feature off whenever it is offered.

Jim



From Ted.Harding at nessie.mcc.ac.uk  Mon Dec  6 11:40:15 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 06 Dec 2004 10:40:15 -0000 (GMT)
Subject: [R] Which FM should beginners R?  A suggestion.
In-Reply-To: <200412060223.iB62NYdS009566@atlas.otago.ac.nz>
Message-ID: <XFMail.041206104015.Ted.Harding@nessie.mcc.ac.uk>

On 06-Dec-04 Richard A. O'Keefe wrote:
> We've recently had a thread on beginners and FAQs and the like.
> I decided that it might be a good idea to offer a short list of
> on-line help pages that beginners should read.  Goals:
>  - the list short be short (I think mine is too long)
>  - many of the commonest questions should find answers there (fairish)
>  - I should have found the pages helpful myself (no question about
> that)
>  - it's a beginning R topic list, not a beginning statistics topic list
> You can find my first draft at
> 
>     http://www.cs.otago.ac.nz/staffpriv/ok/R-help.txt
> 
> I think something _like_ this would be a genuinely useful addition to
> the R documentation.  If people send me suggestions for additions,
> removals, restructuring, &c, I will try to collate them and improve
> this.

Excellent -- not just for its broad coverage of basics, but also
for the simple yet cogent organisation.

I don't think it's too long: As it stands (and it could probably
be somewhat extended) it gives a quick oversight of all the
basic resources for common use in R so that users who are
wondering "what's in R about ... " can have the list presented
to the eye in easily scanned format. And can also readily see
things they wouldn't have suspected but need to know (I've had
that experience already with your draft).

I'll try to think of suggestions, but on my experience so far
you're ahead of me already and likely to stay that way.

Thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 06-Dec-04                                       Time: 10:40:15
------------------------------ XFMail ------------------------------



From simon at stats.gla.ac.uk  Mon Dec  6 11:54:22 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Mon, 6 Dec 2004 10:54:22 +0000 (GMT)
Subject: [R] Gam() function in R
In-Reply-To: <200412060407.iB647a8T016768@postoffice8.mail.cornell.edu>
References: <200412060407.iB647a8T016768@postoffice8.mail.cornell.edu>
Message-ID: <Pine.LNX.4.58.0412061046420.16842@moon.stats.gla.ac.uk>

> I'm   a new user of R gam() function. I am wondering how do we decide on the
> smooth function to use?
> The general form is gam(y~s(x1,df=i)+s(x2,df=j).......)  , how do we decide
> on the degree freedom to use for each smoother, and if we shold apply
> smoother to each attribute?

I guess you are using gam() from package gam, in which case you probably 
need to look at the help file for step.gam. 

By default gam() in package mgcv estimates the appropriate degrees of 
freedom automatically as part of model estimation using generalized cross 
validation, (although there is an adjustable  upper limit on the range 
of degrees of freedom considered).

Package gss also has routines for fitting GAMs where the choise of df is 
fully automatic.

best,
Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From wt43 at cornell.edu  Mon Dec  6 12:06:58 2004
From: wt43 at cornell.edu (Janice Tse)
Date: Mon, 6 Dec 2004 06:06:58 -0500
Subject: [R] Gam() function in R
In-Reply-To: <Pine.LNX.4.58.0412061046420.16842@moon.stats.gla.ac.uk>
Message-ID: <200412061107.iB6B738T008663@postoffice8.mail.cornell.edu>

Thank you very much. I am using gam() from mgcv actually. You answered my
question about degree of freedom.

One more question, if I were to compare the results from gam() and glm(),
which numbers are of the greatest interest?  
What if my response variables are binary?

Thanks!
-Janice

-----Original Message-----
From: Simon Wood [mailto:simon at stats.gla.ac.uk] 
Sent: Monday, December 06, 2004 5:54 AM
To: Janice Tse
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Gam() function in R

> I'm   a new user of R gam() function. I am wondering how do we decide on
the
> smooth function to use?
> The general form is gam(y~s(x1,df=i)+s(x2,df=j).......)  , how do we 
> decide on the degree freedom to use for each smoother, and if we shold 
> apply smoother to each attribute?

I guess you are using gam() from package gam, in which case you probably
need to look at the help file for step.gam. 

By default gam() in package mgcv estimates the appropriate degrees of
freedom automatically as part of model estimation using generalized cross
validation, (although there is an adjustable  upper limit on the range of
degrees of freedom considered).

Package gss also has routines for fitting GAMs where the choise of df is
fully automatic.

best,
Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From maechler at stat.math.ethz.ch  Mon Dec  6 12:08:29 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 6 Dec 2004 12:08:29 +0100
Subject: [R] String manipulation---mixed case
In-Reply-To: <41B38217.50303@pdf.com>
References: <web-2445987@be2.bc.edu> <41B32B5B.9000005@cirad.fr>
	<x2u0r0d8gc.fsf@biostat.ku.dk> <41B38217.50303@pdf.com>
Message-ID: <16820.15789.985636.128628@gargle.gargle.HOWL>

>>>>> "Spencer" == Spencer Graves <spencer.graves at pdf.com>
>>>>>     on Sun, 05 Dec 2004 13:48:07 -0800 writes:

    Spencer> That's great, Peter. 
    Spencer> For pedestrians like me who are not quite as facile with regular 
    Spencer> expressions, the following seems slightly more readable: 

    Spencer> s <- "the quick red fox jumps over the lazy brown dog"
    Spencer> ss <- strsplit(s, " ")[[1]]
    Spencer> ss1 <- substring(ss, 1,1)
    Spencer> ss2 <- substring(ss, 2)
    Spencer> paste(toupper(ss1), ss2, sep="", collapse=" ")

    Spencer> [1] "The Quick Red Fox Jumps Over The Lazy Brown Dog"


Nice.  Since this has been asked before,
and it is something common enoguh that Emacs even has this on a
key (M-c), I think it's worth making a small example on the help
page for toupper/tolower:

## "Mixed Case" Capitalizing Function :
capitalize <- function(x) {
  ## toupper( every first letter of a word ) :
  s <- strsplit(x, " ")[[1]]
  paste(toupper(substring(s, 1,1)), substring(s, 2), sep="", collapse=" ")
}
capitalize("the quick red fox jumps over the lazy brown dog")
## ->  [1] "The Quick Red Fox Jumps Over The Lazy Brown Dog"



From henric.nilsson at statisticon.se  Mon Dec  6 12:14:14 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Mon, 06 Dec 2004 12:14:14 +0100
Subject: [R] Gam() function in R
In-Reply-To: <1102326506.18602.60.camel@new-york.climpact.net>
References: <200412060536.iB65aO8U029853@postoffice8.mail.cornell.edu>
	<3235E28C-475E-11D9-A29C-000A95C76CA8@oulu.fi>
	<1102326506.18602.60.camel@new-york.climpact.net>
Message-ID: <6.1.2.0.0.20041206112950.02d233c8@10.0.10.66>

At 10:48 2004-12-06 +0100, Yves Magliulo wrote:

>this subject is very intersting for me. I'm using mgcv 0.8-9 with R
>version 1.7.1.

You're in need of an update.

>i didn't know that there was an another gam version with
>package library(gam).

This is the 'classic' GAM implementation by Hastie & Tibshirani, discussed 
at length in Hastie & Tibshirani (1990) and in the White book.

In fact, other implementations of the GAM concept also exists. Take a look 
at the gss and assist packages; both are at CRAN, and the former is support 
software for Gu's `Smoothing Spline ANOVA Models' book. There's also the 
vgam http://www.stat.auckland.ac.nz/~yee/VGAM/ and SemiPar packages 
http://web.maths.unsw.edu.au/~wand/webspr/rsplus.html; the latter is 
support software for the `Semiparametric Regression' book by Ruppert, Wand 
and Carroll. And there's probably more out there...

>  Someone can tell me the basics differences between
>them? I look for an help page on google but i only find "mgcv" help
>pages.

Simon Wood (author of the mgcv package) has written a brief but useful 
summary: http://www.stats.gla.ac.uk/~simon/simon/mgcv_overview.html

HTH,
Henric



From maechler at stat.math.ethz.ch  Mon Dec  6 12:24:24 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 6 Dec 2004 12:24:24 +0100
Subject: [R] tree class in R?
In-Reply-To: <41C4F89A@zathras>
References: <41C4F89A@zathras>
Message-ID: <16820.16744.466155.908249@gargle.gargle.HOWL>

>>>>> "DFARRAR" == DFARRAR  <DFARRAR at vt.edu>
>>>>>     on Sun, 5 Dec 2004 22:09:02 -0500 writes:

    DFARRAR> I am trying to store a couple numbers for each
    DFARRAR> partition, in a subset of the partitions of my data
    DFARRAR> set.  Of course, one can accomplish this using a
    DFARRAR> binary tree.  (The first split is on
    DFARRAR> inclusion/exclusion of the first object, and so
    DFARRAR> on.)  I can probably simulate a tree using vectors.
    DFARRAR> (One vector gives the index of left child node,
    DFARRAR> another the index of the right child node.)
    DFARRAR> However, it seems like there must be a useful class
    DFARRAR> associated with the clustering or recursive
    DFARRAR> partitioning procedures, perhaps not out there for
    DFARRAR> everyone to see.  I poked around on the R page and
    DFARRAR> didn't see anything that clearly met my needs very
    DFARRAR> directly.

but maybe indirectly?

There's the "dendrogram" class in R, --> "?dendrogram", which
had been created with the aim to be a class that would allow
both regression/classification trees and hierarchical cluster
dendrograms to be represented.  It's not just for binary trees,
and definitely much nicer to use than a "vector simulation"
version.

    DFARRAR>   I hope I would not have to learn
    DFARRAR> recursive partitioning in R to find what I need.

(well, it may be worth your time to learn something about rpart
 anyway; but you won't want to use it's representation of trees,
 I think)

Martin Maechler, ETH Zurich



From maechler at stat.math.ethz.ch  Mon Dec  6 12:31:58 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 6 Dec 2004 12:31:58 +0100
Subject: [R] AIC, AICc, and K
In-Reply-To: <41B25FC6.7030504@pdf.com>
References: <1102203673.41b24b193a89e@webmail.uvm.edu>
	<41B25FC6.7030504@pdf.com>
Message-ID: <16820.17198.671986.73569@gargle.gargle.HOWL>

>>>>> "Spencer" == Spencer Graves <spencer.graves at pdf.com>
>>>>>     on Sat, 04 Dec 2004 17:09:26 -0800 writes:

    Spencer> I don't know the "best" way, but the following looks like it will 
    Spencer> work: 

    Spencer> tstDF <- data.frame(x=1:3, y=c(1,1,2))
    Spencer> fit0 <- lm(y~1, tstDF)
    Spencer> fitDF <- lm(y~x, tstDF)
    Spencer> AIC(fitDF,fit0)
    Spencer> df      AIC
    Spencer> fitDF  3 5.842516
    Spencer> fit0   2 8.001399

    Spencer> The function AIC with only 1 argument returns only
    Spencer> a single number.  However, given nested models, it
    Spencer> returns a data.frame with colums df and AIC.  At
    Spencer> least in this example (and I would think in all
    Spencer> other contexts as well), "df" is the K you want.
   
yes, but Benjamin would hardly want to invent a second model
just in order to call AIC() with both models and get the data
frame...

As Benjamin hoped, the "df" is part of the logLik(.) result,
and if either of you had more carefully looked at  help(logLik),
you'd have seen

>> Value:
>> 
>>      Returns an object, say 'r', of class 'logLik' which is a number
>>      with attributes, 'attr(r, "df")' (*d*egrees of *f*reedom) giving
>>      the number of parameters in the model. There's a simple 'print'
>>      method for 'logLik' objects.

More directly, you can use

    > str(unclass(logLik(fit0)))
     atomic [1:1] -2
     - attr(*, "nall")= int 3
     - attr(*, "nobs")= int 3
     - attr(*, "df")= num 2

or

    > stats:::AIC.logLik
    function (object, ..., k = 2) 
    -2 * c(object) + k * attr(object, "df")

to see how these work.

Martin Maechler, ETH Zurich




    Spencer> hope this helps. 
    Spencer> Spencer Graves

    Spencer> Benjamin M. Osborne wrote:

    >> How can I extract K (number of parameters) from an AIC
    >> calculation, both to report K itself and to calculate
    >> AICc?  I'm aware of the conversion from AIC -> AICc,
    >> where AICc = AIC + 2K(K+1)/(n-K-1), but not sure of how K
    >> is calculated or how to extract that value from either an
    >> AIC or logLik calculation.

    >> This is probably more of a basic statistics question than
    >> an R question, but I thank you for your help.

    >> -Ben Osborne
    >>



From simon at stats.gla.ac.uk  Mon Dec  6 12:41:07 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Mon, 6 Dec 2004 11:41:07 +0000 (GMT)
Subject: [R] Gam() function in R
In-Reply-To: <1102326506.18602.60.camel@new-york.climpact.net>
References: <200412060536.iB65aO8U029853@postoffice8.mail.cornell.edu>
	<3235E28C-475E-11D9-A29C-000A95C76CA8@oulu.fi>
	<1102326506.18602.60.camel@new-york.climpact.net>
Message-ID: <Pine.LNX.4.58.0412061054421.16842@moon.stats.gla.ac.uk>

> this subject is very intersting for me. I'm using mgcv 0.8-9 with R
> version 1.7.1. i didn't know that there was an another gam version with
> package library(gam). Someone can tell me the basics differences between
> them? I look for an help page on google but i only find "mgcv" help
> pages.

- I think you'd need to move to a newer version of R in order to use 
package gam, but that would also let you use a much more recent version of 
package mgcv. 

- package gam is based very closely on the GAM approach presented in 
Hastie and Tibshirani's  "Generalized Additive Models" book. Estimation is 
by back-fitting and model selection is based on step-wise regression 
methods based on approximate distributional results. A particular strength 
of this approach is that local regression smoothers (`lo()' terms) can be 
included in GAM models.

- gam in package mgcv represents GAMs using penalized regression splines. 
Estimation is by direct penalized likelihood maximization with 
integrated smoothness estimation via GCV or related criteria (there is 
also an alternative `gamm' function based on a mixed model approach). 
Strengths of the this approach are that s() terms can be functions of more 
than one variable and that tensor product smooths are available via te() 
terms - these are useful when different degrees of smoothness are 
appropriate relative to different arguments of a smooth. 

Here's an attempt at a summary of the differences:

Estimation: gam::gam based on backfitting, mgcv::gam based on direct 
penalized likelihood maximization (with smoothness estimation integrated)

Model selection: package(gam) based on stepwise regression methods. 
mgcv::gam based on integrated GCV estimation of degree of smoothness.

Smooth terms: gam::gam can represent smooth terms using a very wide range 
of scatterplot smoothers incuding loess, which is built in. mgcv::gam is 
restricted to smoothers that can be represented using basis functions and 
an associated ``wiggliness'' penalty, but these include low rank thin 
plate spline smoothers and tensor product smoothers for smooths of more 
than one variable. Both packages provide interfaces for adding new classes 
of smoother. 

Uncertainty estimation: since mgcv GAMs explicitly estimate 
coefficients for each smooth term, it is fairly straightforward to obtain 
a covariance matrix for the model coefficients, which makes further 
variance calcualtions easy. For example predictions with standard errors 
are easily obtained for predictions made with new prediction data. The 
backfitting approach makes variance calculation more difficult (e.g. at 
present s.e.s are not available from gam::predict.gam with new data)

Interface: both packages are based on Trevor Hastie's Chapter 7 of 
Chambers and Hastie. Since Trevor H. wrote package(gam) it's a closer 
implementation than package(mgcv). 

Basically, if you want integrated smoothness selection, an underlying 
parametric representation, or want smooth interactions in your models 
then mgcv is probably worth a try (but I would say that). If you want to 
use local regression smoothers and/or prefer the stepwise selection 
approach then package gam is for you. 

Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From ggrothendieck at myway.com  Mon Dec  6 12:47:10 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 6 Dec 2004 11:47:10 +0000 (UTC)
Subject: [R] String manipulation---mixed case
References: <web-2445987@be2.bc.edu> <41B32B5B.9000005@cirad.fr>
	<x2u0r0d8gc.fsf@biostat.ku.dk> <41B38217.50303@pdf.com>
	<16820.15789.985636.128628@gargle.gargle.HOWL>
Message-ID: <loom.20041206T124446-911@post.gmane.org>

Martin Maechler <maechler <at> stat.math.ethz.ch> writes:

: 
: >>>>> "Spencer" == Spencer Graves <spencer.graves <at> pdf.com>
: >>>>>     on Sun, 05 Dec 2004 13:48:07 -0800 writes:
: 
:     Spencer> That's great, Peter. 
:     Spencer> For pedestrians like me who are not quite as facile with 
regular 
:     Spencer> expressions, the following seems slightly more readable: 
: 
:     Spencer> s <- "the quick red fox jumps over the lazy brown dog"
:     Spencer> ss <- strsplit(s, " ")[[1]]
:     Spencer> ss1 <- substring(ss, 1,1)
:     Spencer> ss2 <- substring(ss, 2)
:     Spencer> paste(toupper(ss1), ss2, sep="", collapse=" ")
: 
:     Spencer> [1] "The Quick Red Fox Jumps Over The Lazy Brown Dog"
: 
: Nice.  Since this has been asked before,
: and it is something common enoguh that Emacs even has this on a
: key (M-c), I think it's worth making a small example on the help
: page for toupper/tolower:
: 
: ## "Mixed Case" Capitalizing Function :
: capitalize <- function(x) {
:   ## toupper( every first letter of a word ) :
:   s <- strsplit(x, " ")[[1]]
:   paste(toupper(substring(s, 1,1)), substring(s, 2), sep="", collapse=" ")
: }
: capitalize("the quick red fox jumps over the lazy brown dog")
: ## ->  [1] "The Quick Red Fox Jumps Over The Lazy Brown Dog"

or the following variation which is about the same length but
also (1) handles vectors of strings and (2) does not depend
on the input being lower case:


capwords <- function(s) {
	capword <- function(s) 
		paste( toupper(substring(s,1,1)), tolower(substring(s,2)), 
			sep = "", collapse = " ")
	sapply(s, capword, USE.NAMES = !is.null(names(s)))
}



From ggrothendieck at myway.com  Mon Dec  6 13:10:42 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 6 Dec 2004 12:10:42 +0000 (UTC)
Subject: [R] String manipulation---mixed case
References: <web-2445987@be2.bc.edu> <41B32B5B.9000005@cirad.fr>
	<x2u0r0d8gc.fsf@biostat.ku.dk> <41B38217.50303@pdf.com>
	<16820.15789.985636.128628@gargle.gargle.HOWL>
	<loom.20041206T124446-911@post.gmane.org>
Message-ID: <loom.20041206T130806-632@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Martin Maechler <maechler <at> stat.math.ethz.ch> writes:
: 
: : 
: : >>>>> "Spencer" == Spencer Graves <spencer.graves <at> pdf.com>
: : >>>>>     on Sun, 05 Dec 2004 13:48:07 -0800 writes:
: : 
: :     Spencer> That's great, Peter. 
: :     Spencer> For pedestrians like me who are not quite as facile with 
: regular 
: :     Spencer> expressions, the following seems slightly more readable: 
: : 
: :     Spencer> s <- "the quick red fox jumps over the lazy brown dog"
: :     Spencer> ss <- strsplit(s, " ")[[1]]
: :     Spencer> ss1 <- substring(ss, 1,1)
: :     Spencer> ss2 <- substring(ss, 2)
: :     Spencer> paste(toupper(ss1), ss2, sep="", collapse=" ")
: : 
: :     Spencer> [1] "The Quick Red Fox Jumps Over The Lazy Brown Dog"
: : 
: : Nice.  Since this has been asked before,
: : and it is something common enoguh that Emacs even has this on a
: : key (M-c), I think it's worth making a small example on the help
: : page for toupper/tolower:
: : 
: : ## "Mixed Case" Capitalizing Function :
: : capitalize <- function(x) {
: :   ## toupper( every first letter of a word ) :
: :   s <- strsplit(x, " ")[[1]]
: :   paste(toupper(substring(s, 1,1)), substring(s, 2), sep="", collapse=" ")
: : }
: : capitalize("the quick red fox jumps over the lazy brown dog")
: : ## ->  [1] "The Quick Red Fox Jumps Over The Lazy Brown Dog"
: 
: or the following variation which is about the same length but
: also (1) handles vectors of strings and (2) does not depend
: on the input being lower case:
: 
: capwords <- function(s) {
[...]

Correction.  Should be:

capwords <- function(s) {
  cap <- function(s) paste( toupper(substring(s,1,1)), 
           tolower(substring(s,2)), sep = "", collapse = " " )
  sapply(strsplit(s, split = " "), cap, USE.NAMES = !is.null(names(s)))
}



From renaud.lancelot at cirad.fr  Mon Dec  6 13:37:55 2004
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Mon, 06 Dec 2004 15:37:55 +0300
Subject: [R] AIC, AICc, and K
In-Reply-To: <16820.17198.671986.73569@gargle.gargle.HOWL>
References: <1102203673.41b24b193a89e@webmail.uvm.edu>	<41B25FC6.7030504@pdf.com>
	<16820.17198.671986.73569@gargle.gargle.HOWL>
Message-ID: <41B452A3.9030600@cirad.fr>

Martin Maechler a ??crit :
>>>>>>"Spencer" == Spencer Graves <spencer.graves at pdf.com>
>>>>>>    on Sat, 04 Dec 2004 17:09:26 -0800 writes:
> 
> 
>     Spencer> I don't know the "best" way, but the following looks like it will 
>     Spencer> work: 
> 
>     Spencer> tstDF <- data.frame(x=1:3, y=c(1,1,2))
>     Spencer> fit0 <- lm(y~1, tstDF)
>     Spencer> fitDF <- lm(y~x, tstDF)
>     Spencer> AIC(fitDF,fit0)
>     Spencer> df      AIC
>     Spencer> fitDF  3 5.842516
>     Spencer> fit0   2 8.001399
> 
>     Spencer> The function AIC with only 1 argument returns only
>     Spencer> a single number.  However, given nested models, it
>     Spencer> returns a data.frame with colums df and AIC.  At
>     Spencer> least in this example (and I would think in all
>     Spencer> other contexts as well), "df" is the K you want.
>    
> yes, but Benjamin would hardly want to invent a second model
> just in order to call AIC() with both models and get the data
> frame...
> 
> As Benjamin hoped, the "df" is part of the logLik(.) result,
> and if either of you had more carefully looked at  help(logLik),
> you'd have seen
> 
> 
>>>Value:
>>>
>>>     Returns an object, say 'r', of class 'logLik' which is a number
>>>     with attributes, 'attr(r, "df")' (*d*egrees of *f*reedom) giving
>>>     the number of parameters in the model. There's a simple 'print'
>>>     method for 'logLik' objects.
> 
> 
> More directly, you can use
> 
>     > str(unclass(logLik(fit0)))
>      atomic [1:1] -2
>      - attr(*, "nall")= int 3
>      - attr(*, "nobs")= int 3
>      - attr(*, "df")= num 2
> 
> or
> 
>     > stats:::AIC.logLik
>     function (object, ..., k = 2) 
>     -2 * c(object) + k * attr(object, "df")
> 
> to see how these work.
> 
> Martin Maechler, ETH Zurich

[snip]

Yes, but see the problem recently outlined by Prof. Ripley about nobs:

> On Sun, 31 Oct 2004, Prof Brian Ripley wrote:
> 
> 
>>> The harder task is actually to get `n', not `npar'.
>>> 
>>> length(resid(x)) may or may not include missing values, depending on the 
>>> na.action used, and will include observations with weight zero.
>>> However, logLik's "lm" method returns an attribute "nobs" that is a better 
>>> choice.
> 
> 
> But only better, as it looks like it has fallen into the first trap.
> AFAICS, if 'fit' is an lm fit, then
> 
> n = df.residual(fit) + fit$rank
> 
> 
> Quick check:
> 
> x <- rnorm(10); x[2] <- NA
> y <- rnorm(10); y[1] <- NA
> w <- c(rep(1,9), 0)
> fit <- lm(y ~x, weights = w, na.action=na.exclude)
> df.residual(fit) + fit$rank  # 7, correct
> attributes(logLik(fit)) # nall 9 nobs 9 df 3
> # but AIC fails
> length(resid(fit)) # 10
> 
> fit <- lm(y ~x, weights = w, na.action=na.omit)
> df.residual(fit) + fit$rank  # 7, correct
> attributes(logLik(fit)) # nall 7 nobs 7 df 3
> length(resid(fit)) # 8

The problem might also become a bit more complicated when dealing with 
other kind of models than lm (or weighted lm, etc).

For example, what is nobs when you are using the syntax:

cbind(success, failure) ~ covariates

in a binomial glm ? Is it the number of lines in the aggregated data 
frame, or the total number of observations ?

Best,

Renaud

-- 
Dr Renaud Lancelot, v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From aolinto_r at bignet.com.br  Mon Dec  6 13:44:43 2004
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Mon,  6 Dec 2004 10:44:43 -0200
Subject: [R] using subset
Message-ID: <1102337083.41b4543b2fe74@webmail2.bignet.com.br>

Hi,

I have doubts in using subset command. I have a list of, lets say, 15 species
and I want to make a subset with only 2 of them. I??m the command
data2 <- subset(data1, species=="sp1"|species=="sp2")

Nevertheless, when I ask for the summary (summary(data2)) the others species
names still apearing. Also I tried
data2<-data1(data1$species %in% c("sp1","sp2")),]

But I got the same result. How can I get a "clean" subset?

Thanks for any help. Best regards.

Antonio Olinto



-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br



From HDoran at air.org  Mon Dec  6 13:55:47 2004
From: HDoran at air.org (Doran, Harold)
Date: Mon, 6 Dec 2004 07:55:47 -0500
Subject: [R] using subset
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7406B7888D@dc1ex2.air.org>

Antonio:

If the variable you are subsetting is a factor with multiple levels (and
it appears you have 15), then the levels of the factor remain and will
show up in summary, but your data set will only include the rows of data
that you want. I am pretty sure subset has worked properly in your case.

Try 

> data[1:20,]

And just look at your data to see.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Antonio Olinto
Sent: Monday, December 06, 2004 7:45 AM
To: R-help
Subject: [R] using subset

Hi,

I have doubts in using subset command. I have a list of, lets say, 15
species and I want to make a subset with only 2 of them. I'm the command
data2 <- subset(data1, species=="sp1"|species=="sp2")

Nevertheless, when I ask for the summary (summary(data2)) the others
species names still apearing. Also I tried data2<-data1(data1$species
%in% c("sp1","sp2")),]

But I got the same result. How can I get a "clean" subset?

Thanks for any help. Best regards.

Antonio Olinto



-------------------------------------------------
WebMail Bignet - O seu provedor do litoral www.bignet.com.br

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From anne.piotet at urbanet.ch  Mon Dec  6 13:58:56 2004
From: anne.piotet at urbanet.ch (Anne)
Date: Mon, 6 Dec 2004 13:58:56 +0100
Subject: [R] removing NA as a level
Message-ID: <000b01c4db93$5ae26b40$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041206/e4522dc7/attachment.pl

From aolinto_r at bignet.com.br  Mon Dec  6 14:08:49 2004
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Mon,  6 Dec 2004 11:08:49 -0200
Subject: [R] RE: using subset
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7406B7888D@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7406B7888D@dc1ex2.air.org>
Message-ID: <1102338529.41b459e12fe76@webmail2.bignet.com.br>

Dear Harold, thanks for your answer.

The problem is that the levels of the factor are shown not only in the summary,
but also when I make an interaction plot.

Is it possible to create a new subset without the reference to the levels that I
will not use?

Thanks again.

Antonio

Quoting "Doran, Harold" <HDoran at air.org>:

> Antonio:
> 
> If the variable you are subsetting is a factor with multiple levels (and
> it appears you have 15), then the levels of the factor remain and will
> show up in summary, but your data set will only include the rows of data
> that you want. I am pretty sure subset has worked properly in your case.
> 
> Try 
> 
> > data[1:20,]
> 
> And just look at your data to see.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Antonio Olinto
> Sent: Monday, December 06, 2004 7:45 AM
> To: R-help
> Subject: [R] using subset
> 
> Hi,
> 
> I have doubts in using subset command. I have a list of, lets say, 15
> species and I want to make a subset with only 2 of them. I'm the command
> data2 <- subset(data1, species=="sp1"|species=="sp2")
> 
> Nevertheless, when I ask for the summary (summary(data2)) the others
> species names still apearing. Also I tried data2<-data1(data1$species
> %in% c("sp1","sp2")),]
> 
> But I got the same result. How can I get a "clean" subset?
> 
> Thanks for any help. Best regards.
> 
> Antonio Olinto
> 
> 
> 
> -------------------------------------------------
> WebMail Bignet - O seu provedor do litoral www.bignet.com.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 



-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br



From ccleland at optonline.net  Mon Dec  6 14:12:43 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 06 Dec 2004 08:12:43 -0500
Subject: [R] removing NA as a level
In-Reply-To: <000b01c4db93$5ae26b40$6c00a8c0@mtd4>
References: <000b01c4db93$5ae26b40$6c00a8c0@mtd4>
Message-ID: <41B45ACB.2020309@optonline.net>

dat$x.f <- factor(replace(dat$x.f, dat$x.f == "NA", NA))

Anne wrote:
> Dear R-helpers,
> I have a problem which I suppose is trivila, but...
> I have included NA values as factors ( (to be able to make nice printed summaries with NAs % ba category ) with the following code
> 
> dat$x.f<-factor(dat$x, exclude=NULL); levels(dat$x.f)<-c("A1","A2","A3","A4","NA"); length(dat$x.f)
> 
> Now, I want to impute the missing values. Is there a nice way to drop the NA factor instead of rewriting something of the sort 
>  dat$x.f<-factor(dat$x); levels(dat$x.f)<-c("A1","A2","A3","A4"); length(dat$x.f)?

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ozric at web.de  Mon Dec  6 14:18:58 2004
From: ozric at web.de (Christian Schulz)
Date: Mon, 06 Dec 2004 14:18:58 +0100
Subject: [R] Text Mining with R
In-Reply-To: <33CF3546-4774-11D9-B49D-000D93AE2752@dssp.unil.ch>
References: <200412021829.31979.daniele.medri@libero.it>	<20041203191557.043b3141.tobias.verbeke@telenet.be>	<21c05c7d04120312233eb46bf4@mail.gmail.com>
	<33CF3546-4774-11D9-B49D-000D93AE2752@dssp.unil.ch>
Message-ID: <41B45C42.5040405@web.de>

hi,

i'm interested in text-mining, too and so make a trial.
In Linux (suse9.2) it works fine, but in windows i can't install
the source despite off  installed  perl, tools ,htmlhelp etc..

regards, christian


Jean-Pierre Muller wrote:

> Jose,
>
> Le 3 d??c. 04, ?? 21:23, Jose Quesada a ??crit :
>
>> Tobias,
>>
>> I just created a zip file from the tar, and used the "install from
>> zip" option of the Rwin console.
>>
>> ttda is shown in the list of installed packages. However, when I try
>> "load packages", or the equivaent "library(ttda)", I get:
>>
>> Error in library(ttda) : 'ttda' is not a valid package -- installed < 
>> 2.0.0?
>>
>>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>>
>> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>> Error in library(pkg, character.only = TRUE) :
>>         'ttda' is not a valid package -- installed < 2.0.0?
>>
>> Do you know why?
>>
>
> I have put a link to the zip version of ttda on 
> http://wwwpeople.unil.ch/jean-pierre.mueller/
> I have been able to load it (using "packages -> Install package(s) 
> from local zip files..."),
> and to make a few tests, but i have not tested it well on a windows 
> machine with R 2.0.x.,
> I will not have time in the next few days. But on R for Mac OS X Aqua 
> (2.0.1), I have no
> problems (R CMD CHECK ttda ->  OK)  & (Installing *source* package 
> 'ttda' ... ->
> package successfully installed ) .
> HTH
>
>
>
>



From petr.pikal at precheza.cz  Mon Dec  6 14:20:48 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 06 Dec 2004 14:20:48 +0100
Subject: [R] using subset
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7406B7888D@dc1ex2.air.org>
Message-ID: <41B46AC0.7260.B47860@localhost>



On 6 Dec 2004 at 7:55, Doran, Harold wrote:

> Antonio:
> 
> If the variable you are subsetting is a factor with multiple levels
> (and it appears you have 15), then the levels of the factor remain and
> will show up in summary, but your data set will only include the rows
> of data that you want. I am pretty sure subset has worked properly in
> your case.
> 
> Try 
> 
> > data[1:20,]
> 
> And just look at your data to see.

Hi Antonio

If you want to get rid of factor levels in your subsetted data in 
species column,

data2$species <- factor(data2$species)

should do the trick

Cheers
Petr


> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Antonio Olinto
> Sent: Monday, December 06, 2004 7:45 AM To: R-help Subject: [R] using
> subset
> 
> Hi,
> 
> I have doubts in using subset command. I have a list of, lets say, 15
> species and I want to make a subset with only 2 of them. I'm the
> command data2 <- subset(data1, species=="sp1"|species=="sp2")
> 
> Nevertheless, when I ask for the summary (summary(data2)) the others
> species names still apearing. Also I tried data2<-data1(data1$species
> %in% c("sp1","sp2")),]
> 
> But I got the same result. How can I get a "clean" subset?
> 
> Thanks for any help. Best regards.
> 
> Antonio Olinto
> 
> 
> 
> -------------------------------------------------
> WebMail Bignet - O seu provedor do litoral www.bignet.com.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From anne.piotet at urbanet.ch  Mon Dec  6 14:29:06 2004
From: anne.piotet at urbanet.ch (Anne)
Date: Mon, 6 Dec 2004 14:29:06 +0100
Subject: [R] removing NA as a level
References: <000b01c4db93$5ae26b40$6c00a8c0@mtd4>
	<41B45ACB.2020309@optonline.net>
Message-ID: <001801c4db97$9102f010$6c00a8c0@mtd4>

Thank you! so easy and I did not think of it!

----- Original Message ----- 
From: "Chuck Cleland" <ccleland at optonline.net>
To: "Anne" <anne.piotet at urbanet.ch>
Cc: "R list" <r-help at stat.math.ethz.ch>
Sent: Monday, December 06, 2004 2:12 PM
Subject: Re: [R] removing NA as a level


> dat$x.f <- factor(replace(dat$x.f, dat$x.f == "NA", NA))
>
> Anne wrote:
> > Dear R-helpers,
> > I have a problem which I suppose is trivila, but...
> > I have included NA values as factors ( (to be able to make nice printed
summaries with NAs % ba category ) with the following code
> >
> > dat$x.f<-factor(dat$x, exclude=NULL);
levels(dat$x.f)<-c("A1","A2","A3","A4","NA"); length(dat$x.f)
> >
> > Now, I want to impute the missing values. Is there a nice way to drop
the NA factor instead of rewriting something of the sort
> >  dat$x.f<-factor(dat$x); levels(dat$x.f)<-c("A1","A2","A3","A4");
length(dat$x.f)?
>
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
>



From petr.pikal at precheza.cz  Mon Dec  6 14:31:59 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 06 Dec 2004 14:31:59 +0100
Subject: [R] removing NA as a level
In-Reply-To: <000b01c4db93$5ae26b40$6c00a8c0@mtd4>
Message-ID: <41B46D5F.31731.BEB6B1@localhost>



On 6 Dec 2004 at 13:58, Anne wrote:

> Dear R-helpers,
> I have a problem which I suppose is trivila, but...
> I have included NA values as factors ( (to be able to make nice
> printed summaries with NAs % ba category ) with the following code
> 
> dat$x.f<-factor(dat$x, exclude=NULL);
> levels(dat$x.f)<-c("A1","A2","A3","A4","NA"); length(dat$x.f)
> 
> Now, I want to impute the missing values. Is there a nice way to drop
> the NA factor instead of rewriting something of the sort 
>  dat$x.f<-factor(dat$x); levels(dat$x.f)<-c("A1","A2","A3","A4");
>  length(dat$x.f)?

Hi Anne
 
Is this 

dat$x.f[!is.na(dat$x.f), drop=T]

what you want?

?"[.factor"

Cheers
Petr


> 
> Thanks 
> 
> Anne
> 
> 
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From h.andersson at nioo.knaw.nl  Mon Dec  6 14:43:46 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Mon, 06 Dec 2004 14:43:46 +0100
Subject: [R] Modyfing PATH in Windows Installer for R
Message-ID: <cp1np3$msi$1@sea.gmane.org>

Just a small suggestion since Windows have a file system not designed 
for command line use...

Would it be possible to add the possibility of automatically 
adding/modifying the path to the R executables in the windows installer 
program?


---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From ggrothendieck at myway.com  Mon Dec  6 15:00:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 6 Dec 2004 14:00:23 +0000 (UTC)
Subject: [R] Modyfing PATH in Windows Installer for R
References: <cp1np3$msi$1@sea.gmane.org>
Message-ID: <loom.20041206T145604-547@post.gmane.org>

Henrik Andersson <h.andersson <at> nioo.knaw.nl> writes:

: 
: Just a small suggestion since Windows have a file system not designed 
: for command line use...
: 
: Would it be possible to add the possibility of automatically 
: adding/modifying the path to the R executables in the windows installer 
: program?

Your scripts can get the location of R from the Windows
registry as discussed in:

http://www.mail-archive.com/r-devel at stat.math.ethz.ch/msg05339.html



From chencheva at gmail.com  Mon Dec  6 15:12:41 2004
From: chencheva at gmail.com (Hu Chen)
Date: Mon, 6 Dec 2004 22:12:41 +0800
Subject: [R] how to get how many lines there are in a file.
Message-ID: <6f3fc9ee041206061258dcce1a@mail.gmail.com>

hi all
If I wanna get the total number of lines in a big file without reading
the file's content into R as matrix or data frame, any methods or
functions?
thanks in advance.
Regards



From bxc at steno.dk  Mon Dec  6 15:13:21 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 6 Dec 2004 15:13:21 +0100
Subject: [R] Modyfing PATH in Windows Installer for R
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FDE78@exdkba022.novo.dk>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Henrik 
> Andersson
> Sent: Monday, December 06, 2004 2:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Modyfing PATH in Windows Installer for R
> 
> 
> Just a small suggestion since Windows have a file system not designed 
> for command line use...

I would strongly disagree on that one, it is just an unusually well kept
secret.
Just open a command window in windows and type "cmd /?" and you will get
an
introduction to a lot of useful features, such as filename completion
etc. 
I run windows 2000 almost exclusively from the commnd line, and it's
just about 
as versatile as the Linux command promp, albeit with slightly different
command 
names. For some odd reasons the commands available are kept in the
windows help 
under the heading "MS-DOS commands" even though DOS is long dead. And if
you have 
loaded the utilities recommended for building R-packages you have almost
all you want.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> 
> Would it be possible to add the possibility of automatically 
> adding/modifying the path to the R executables in the windows 
> installer 
> program?
> 
> 
> ---------------------------------------------
> Henrik Andersson
> Netherlands Institute of Ecology -
> Centre for Estuarine and Marine Ecology
> P.O. Box 140
> 4400 AC Yerseke
> Phone: +31 113 577473
> h.andersson at nioo.knaw.nl http://www.nioo.knaw.nl/ppages/handersson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Dec  6 15:19:19 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Dec 2004 15:19:19 +0100
Subject: [R] RE: using subset
In-Reply-To: <1102338529.41b459e12fe76@webmail2.bignet.com.br>
References: <88EAF3512A55DF46B06B1954AEF73F7406B7888D@dc1ex2.air.org>
	<1102338529.41b459e12fe76@webmail2.bignet.com.br>
Message-ID: <x2d5xn33x4.fsf@biostat.ku.dk>

Antonio Olinto <aolinto_r at bignet.com.br> writes:

> Dear Harold, thanks for your answer.
> 
> The problem is that the levels of the factor are shown not only in the summary,
> but also when I make an interaction plot.
> 
> Is it possible to create a new subset without the reference to the levels that I
> will not use?

Just remove the levels from the factor where you don't want them:

> x1 <- dtp[dtp==0]
> x1
[1] 0 0 0 0
Levels: 0 1
> x1[drop=T]
[1] 0 0 0 0
Levels: 0


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Mon Dec  6 15:23:19 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 6 Dec 2004 15:23:19 +0100
Subject: [R] String manipulation---mixed case
In-Reply-To: <loom.20041206T130806-632@post.gmane.org>
References: <web-2445987@be2.bc.edu> <41B32B5B.9000005@cirad.fr>
	<x2u0r0d8gc.fsf@biostat.ku.dk> <41B38217.50303@pdf.com>
	<16820.15789.985636.128628@gargle.gargle.HOWL>
	<loom.20041206T124446-911@post.gmane.org>
	<loom.20041206T130806-632@post.gmane.org>
Message-ID: <16820.27479.582733.212415@gargle.gargle.HOWL>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at myway.com>
>>>>>     on Mon, 6 Dec 2004 12:10:42 +0000 (UTC) writes:

    Gabor> Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
    Gabor> : 
    Gabor> : Martin Maechler <maechler <at> stat.math.ethz.ch> writes:

           ............

    Gabor> : : Nice.  Since this has been asked before,
    Gabor> : : and it is something common enoguh that Emacs even has this on a
    Gabor> : : key (M-c), I think it's worth making a small example on the help
    Gabor> : : page for toupper/tolower:
    Gabor> : : 
    Gabor> : : ## "Mixed Case" Capitalizing Function :
    Gabor> : : capitalize <- function(x) {
    Gabor> : :   ## toupper( every first letter of a word ) :
    Gabor> : :   s <- strsplit(x, " ")[[1]]
    Gabor> : :   paste(toupper(substring(s, 1,1)), substring(s, 2), sep="", collapse=" ")
    Gabor> : : }
    Gabor> : : capitalize("the quick red fox jumps over the lazy brown dog")
    Gabor> : : ## ->  [1] "The Quick Red Fox Jumps Over The Lazy Brown Dog"
    Gabor> : 
    Gabor> : or the following variation which is about the same length but
    Gabor> : also (1) handles vectors of strings and (2) does not depend
    Gabor> : on the input being lower case:

(1) is desirable, but makes the example harder to understand.
    Many people have been lamenting about too complicated
    examples on help pages.  So, I wouldn't be sure to want the
    improvement as a help example.

(2) is not really desirable; at least not always:

   > ss <- c(first= "ABC and XYZ, now I know ..", second="AIC or BIC or AICc ?")
   > capwords(ss)
                          first                       second 
   "Abc And Xyz, Now I Know .."       "Aic Or Bic Or Aicc ?" 


    Gabor> : capwords <- function(s) {
    Gabor> [...]

    Gabor> Correction.  Should be:

    Gabor> capwords <- function(s) {
    Gabor>   cap <- function(s) paste( toupper(substring(s,1,1)), 
    Gabor>                             tolower(substring(s,2)), sep = "", collapse = " " )
    Gabor>   sapply(strsplit(s, split = " "), cap, USE.NAMES = !is.null(names(s)))
    Gabor> }

Really nice, but something that most readers of help(toupper)
won't be able to grasp..

[ and when you are really striving for a ``most-short'' one,
  you may want to know that in this case, substr(.) can be used
  instead of substring()  ;-)
]
Martin



From Sebastien.Moretti at igs.cnrs-mrs.fr  Mon Dec  6 15:30:04 2004
From: Sebastien.Moretti at igs.cnrs-mrs.fr (Sebastien Moretti)
Date: Mon, 6 Dec 2004 15:30:04 +0100
Subject: [R] barplot() options for intervals on axes
Message-ID: <200412061530.04415.Sebastien.Moretti@igs.cnrs-mrs.fr>

Hello,
I am a beginner with R. I read many tutorials and the FAQ but I cannot solve 
my problem. 
I use barplot() to view my graph. I try to get more interval marks on y axis.
I wasn't able to find options in 'help(barplot)' or 'help(par)' to do this 
with barplot().

I seek for another option to print y values on my bars like on the graph of 
the R homepage: http://www.r-project.org/

Thanks for your help.

-- 
Sebastien MORETTI
Linux User - #327894
CNRS - IGS
31 chemin Joseph Aiguier
13402 Marseille cedex 20, FRANCE
tel. +33 (0)4 91 16 44 55



From MSchwartz at MedAnalytics.com  Mon Dec  6 15:30:06 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 06 Dec 2004 08:30:06 -0600
Subject: [R] how to get how many lines there are in a file.
In-Reply-To: <6f3fc9ee041206061258dcce1a@mail.gmail.com>
References: <6f3fc9ee041206061258dcce1a@mail.gmail.com>
Message-ID: <1102343406.22293.3.camel@horizons.localdomain>

On Mon, 2004-12-06 at 22:12 +0800, Hu Chen wrote:
> hi all
> If I wanna get the total number of lines in a big file without reading
> the file's content into R as matrix or data frame, any methods or
> functions?
> thanks in advance.
> Regards

See ?readLines

You can use:

length(readLines("FileName"))

to get the number of lines read.

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Mon Dec  6 15:35:50 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 06 Dec 2004 15:35:50 +0100
Subject: [R] how to get how many lines there are in a file.
In-Reply-To: <6f3fc9ee041206061258dcce1a@mail.gmail.com>
References: <6f3fc9ee041206061258dcce1a@mail.gmail.com>
Message-ID: <41B46E46.9020201@statistik.uni-dortmund.de>

Hu Chen wrote:

> hi all
> If I wanna get the total number of lines in a big file without reading
> the file's content into R as matrix or data frame, any methods or
> functions?

You must read it in R, or how do you think should one determine the 
number of lines in a file (if you don't want to use another program)?
I'd suggest length(readLines(...)).

Uwe Ligges



> thanks in advance.
> Regards



From chris.jackson at imperial.ac.uk  Mon Dec  6 15:34:01 2004
From: chris.jackson at imperial.ac.uk (Chris Jackson)
Date: Mon, 06 Dec 2004 14:34:01 +0000
Subject: [R] Modyfing PATH in Windows Installer for R
In-Reply-To: <cp1np3$msi$1@sea.gmane.org>
References: <cp1np3$msi$1@sea.gmane.org>
Message-ID: <41B46DD9.1030101@imperial.ac.uk>

Henrik Andersson wrote:
> Just a small suggestion since Windows have a file system not designed 
> for command line use...
> 
> Would it be possible to add the possibility of automatically 
> adding/modifying the path to the R executables in the windows installer 
> program?


After asking the same question on R-devel a few months ago, I
investigated the possibility.  This would be useful to me, as it saves
the trouble of modifying the path every time a new version of R comes
out.

As far as I could tell, the installer that R for Windows uses
(InnoSetup) cannot modify the PATH out of the box, but it can do this
via a third-party add-on script.   I was doubtful whether it was worth
adding such a script to the R sources, given the small benefit obtained.

The ideal solution seems to be what's done in Mac OS X, that is, to
store a permanent path to the current R version as a symlink, and then
just changing where the symlink points to when R is upgraded.
Unfortunately Windows doesn't do symlinks.

Chris
(probably a little esoteric for R-help)
-- 
Christopher Jackson <chris.jackson at imperial.ac.uk>, Research Associate,
Department of Epidemiology and Public Health, Imperial College
School of Medicine, Norfolk Place, London W2 1PG, tel. 020 759 43371



From andy_liaw at merck.com  Mon Dec  6 15:37:22 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 6 Dec 2004 09:37:22 -0500
Subject: [R] how to get how many lines there are in a file.
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3DE@usrymx25.merck.com>

> From: Marc Schwartz
> 
> On Mon, 2004-12-06 at 22:12 +0800, Hu Chen wrote:
> > hi all
> > If I wanna get the total number of lines in a big file 
> without reading
> > the file's content into R as matrix or data frame, any methods or
> > functions?
> > thanks in advance.
> > Regards
> 
> See ?readLines
> 
> You can use:
> 
> length(readLines("FileName"))
> 
> to get the number of lines read.
> 
> HTH,
> 
> Marc Schwartz


On a system equipped with `wc' (*nix or Windows with such utilities
installed and on PATH) I would use that.  Otherwise length(count.fields())
might be a good choice.

Cheers,
Andy



From ligges at statistik.uni-dortmund.de  Mon Dec  6 15:39:14 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 06 Dec 2004 15:39:14 +0100
Subject: [R] barplot() options for intervals on axes
In-Reply-To: <200412061530.04415.Sebastien.Moretti@igs.cnrs-mrs.fr>
References: <200412061530.04415.Sebastien.Moretti@igs.cnrs-mrs.fr>
Message-ID: <41B46F12.9050108@statistik.uni-dortmund.de>

Sebastien Moretti wrote:

> Hello,
> I am a beginner with R. I read many tutorials and the FAQ but I cannot solve 
> my problem. 
> I use barplot() to view my graph. I try to get more interval marks on y axis.
> I wasn't able to find options in 'help(barplot)' or 'help(par)' to do this 
> with barplot().

See ?par and ?axis.
You might want to omit the y-axis completely at first (argument 
yaxt="n") and add one manually useing axis(2, ....).


> I seek for another option to print y values on my bars like on the graph of 
> the R homepage: http://www.r-project.org/

Click on the image and see the code how it was generated.


Uwe Ligges

> Thanks for your help.
>



From ccleland at optonline.net  Mon Dec  6 15:37:51 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 06 Dec 2004 09:37:51 -0500
Subject: [R] barplot() options for intervals on axes
In-Reply-To: <200412061530.04415.Sebastien.Moretti@igs.cnrs-mrs.fr>
References: <200412061530.04415.Sebastien.Moretti@igs.cnrs-mrs.fr>
Message-ID: <41B46EBF.6030201@optonline.net>

   Did you see the lab argument to par()?

'lab' A numerical vector of the form 'c(x, y, len)' which modifies
           the way that axes are annotated.  The values of 'x' and 'y'
           give the (approximate) number of tickmarks on the x and y
           axes and 'len' specifies the label size.  The default is
           'c(5, 5, 7)'. _Currently_, 'len' _is unimplemented_.

   You might want something like this:

par(lab=c(5,10,7))
barplot(runif(10))

Sebastien Moretti wrote:
> Hello,
> I am a beginner with R. I read many tutorials and the FAQ but I cannot solve 
> my problem. 
> I use barplot() to view my graph. I try to get more interval marks on y axis.
> I wasn't able to find options in 'help(barplot)' or 'help(par)' to do this 
> with barplot().
> 
> I seek for another option to print y values on my bars like on the graph of 
> the R homepage: http://www.r-project.org/
> 
> Thanks for your help.
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From rxg218 at psu.edu  Mon Dec  6 00:26:45 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Sun, 05 Dec 2004 18:26:45 -0500
Subject: [R] matrix of 1,0's to a data.frame of factors
Message-ID: <1102289205.4304.13.camel@localhost.localdomain>

Hi, I have an integer matrix consisting of 1's and 0's and I would like
to convert this to a data.frame where each column of the matrix becomes
a factor variable.  

Now, some columns of the matrix have only 1's or only 0's as a result
there is only 1 level for those columns in the data.frame. However it is
required that each factor have 2 levels. So my solution is:

    m <- function_returning_a_matrix()

    n <- data.frame(  apply(m,2,as.character) )
    for (i in 1:ncol(n)) {
        levels(n[,i]) <- c(1,0)
    }


When m is 118 x 1024 the loop becomes very slow. So I then tried

    n <- data.frame(  apply(m,2,as.character) )
    apply(n,2,function(x) {levels(x) <- c(1,0)})

But this does not change the levels

Can anybody point me in the right direction?

Thanks,
-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
He who is in love with himself has at least this advantage -- he won't
encounter many rivals.
-- Georg Lichtenberg, "Aphorisms"



From murdoch at stats.uwo.ca  Mon Dec  6 15:40:23 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 06 Dec 2004 09:40:23 -0500
Subject: [R] Modyfing PATH in Windows Installer for R
In-Reply-To: <cp1np3$msi$1@sea.gmane.org>
References: <cp1np3$msi$1@sea.gmane.org>
Message-ID: <ebr8r093b5c37aedijlc44sf4rsik8j0md@4ax.com>

On Mon, 06 Dec 2004 14:43:46 +0100, Henrik Andersson
<h.andersson at nioo.knaw.nl> wrote :

>Just a small suggestion since Windows have a file system not designed 
>for command line use...

Hmm?  I'm not sure what you mean by that.  As far as the command line
is concerned, Windows and Unix file systems are pretty similar.

>Would it be possible to add the possibility of automatically 
>adding/modifying the path to the R executables in the windows installer 
>program?

You mean modifying the PATH environment variable?  I think it would be
possible to do this, but I don't think it's a good idea:

 - Users who use a command line interface probably know how to do this
themselves, and others don't need it.

 - Users who use a command line interface use a large variety of
different mechanisms to set their path, and it would be hard for the R
installer to do anything helpful and not harmful to automatically
change what they've done.

For example, I set my path in F:/cygwin/etc/profile, but I may be the
only R user who uses that particular file...

Duncan Murdoch



From Sebastien.Moretti at igs.cnrs-mrs.fr  Mon Dec  6 15:45:03 2004
From: Sebastien.Moretti at igs.cnrs-mrs.fr (Sebastien Moretti)
Date: Mon, 6 Dec 2004 15:45:03 +0100
Subject: [R] barplot() options for intervals on axes
In-Reply-To: <41B46EBF.6030201@optonline.net>
References: <200412061530.04415.Sebastien.Moretti@igs.cnrs-mrs.fr>
	<41B46EBF.6030201@optonline.net>
Message-ID: <200412061545.03889.Sebastien.Moretti@igs.cnrs-mrs.fr>

>    Did you see the lab argument to par()?
>
> 'lab' A numerical vector of the form 'c(x, y, len)' which modifies
>            the way that axes are annotated.  The values of 'x' and 'y'
>            give the (approximate) number of tickmarks on the x and y
>            axes and 'len' specifies the label size.  The default is
>            'c(5, 5, 7)'. _Currently_, 'len' _is unimplemented_.
>
>    You might want something like this:
>
> par(lab=c(5,10,7))
> barplot(runif(10))
>

'lab' doesn't change my graph. It is always the same.
Maybe it doesn't work with other options I use in barplot:

barplot(y,xlab="Number of splice variants",ylab="Number of 
genes",col="red1",names.arg=x,border="red1",axes=TRUE,ylim=c(0,max(y)))



> > Hello,
> > I am a beginner with R. I read many tutorials and the FAQ but I cannot
> > solve my problem.
> > I use barplot() to view my graph. I try to get more interval marks on y
> > axis. I wasn't able to find options in 'help(barplot)' or 'help(par)' to
> > do this with barplot().
> >
> > I seek for another option to print y values on my bars like on the graph
> > of the R homepage: http://www.r-project.org/
> >
> > Thanks for your help.

-- 
Sebastien MORETTI
Linux User - #327894
CNRS - IGS
31 chemin Joseph Aiguier
13402 Marseille cedex 20, FRANCE
tel. +33 (0)4 91 16 44 55



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Dec  6 15:53:26 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 6 Dec 2004 15:53:26 +0100
Subject: [R] matrix of 1,0's to a data.frame of factors
References: <1102289205.4304.13.camel@localhost.localdomain>
Message-ID: <012c01c4dba3$578dcec0$0540210a@www.domain>

Hi Rajarshi,

try this:

mat <- sample(0:1, 20, TRUE); dim(mat) <- c(5,4)
mat[,1] <- 0; mat[,3] <- 1
#########
dat <- data.frame(mat)
dat[] <- lapply(dat, function(x) factor(x, levels=c("0","1")))
dat
lapply(dat, levels)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rajarshi Guha" <rxg218 at psu.edu>
To: "R" <r-help at stat.math.ethz.ch>
Sent: Monday, December 06, 2004 12:26 AM
Subject: [R] matrix of 1,0's to a data.frame of factors


> Hi, I have an integer matrix consisting of 1's and 0's and I would 
> like
> to convert this to a data.frame where each column of the matrix 
> becomes
> a factor variable.
>
> Now, some columns of the matrix have only 1's or only 0's as a 
> result
> there is only 1 level for those columns in the data.frame. However 
> it is
> required that each factor have 2 levels. So my solution is:
>
>    m <- function_returning_a_matrix()
>
>    n <- data.frame(  apply(m,2,as.character) )
>    for (i in 1:ncol(n)) {
>        levels(n[,i]) <- c(1,0)
>    }
>
>
> When m is 118 x 1024 the loop becomes very slow. So I then tried
>
>    n <- data.frame(  apply(m,2,as.character) )
>    apply(n,2,function(x) {levels(x) <- c(1,0)})
>
> But this does not change the levels
>
> Can anybody point me in the right direction?
>
> Thanks,
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> He who is in love with himself has at least this advantage -- he 
> won't
> encounter many rivals.
> -- Georg Lichtenberg, "Aphorisms"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Mon Dec  6 15:55:31 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 6 Dec 2004 14:55:31 +0000 (UTC)
Subject: [R] Modyfing PATH in Windows Installer for R
References: <cp1np3$msi$1@sea.gmane.org> <41B46DD9.1030101@imperial.ac.uk>
Message-ID: <loom.20041206T154505-518@post.gmane.org>

Chris Jackson <chris.jackson <at> imperial.ac.uk> writes:

: 
: Henrik Andersson wrote:
: > Just a small suggestion since Windows have a file system not designed 
: > for command line use...
: > 
: > Would it be possible to add the possibility of automatically 
: > adding/modifying the path to the R executables in the windows installer 
: > program?
: 
: After asking the same question on R-devel a few months ago, I
: investigated the possibility.  This would be useful to me, as it saves
: the trouble of modifying the path every time a new version of R comes
: out.
: 
: As far as I could tell, the installer that R for Windows uses
: (InnoSetup) cannot modify the PATH out of the box, but it can do this
: via a third-party add-on script.   I was doubtful whether it was worth
: adding such a script to the R sources, given the small benefit obtained.
: 
: The ideal solution seems to be what's done in Mac OS X, that is, to
: store a permanent path to the current R version as a symlink, and then
: just changing where the symlink points to when R is upgraded.
: Unfortunately Windows doesn't do symlinks.
: 
: Chris
: (probably a little esoteric for R-help)

Although not really recommended it actually is possible to do this in
Windows.  See the annoyances.org link in the reply at the end of:

http://www.mail-archive.com/r-devel at stat.math.ethz.ch/msg05259.html

Be warned that if you do this, deleting the Windows "symlink" will
delete the underlying folder too!  You have to disable it by 
reversing the steps you took to create it if you want to get rid
of the "symlink" without destroying the underlying folder.



From murdoch at stats.uwo.ca  Mon Dec  6 15:58:08 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 06 Dec 2004 09:58:08 -0500
Subject: [R] Modyfing PATH in Windows Installer for R
In-Reply-To: <41B46DD9.1030101@imperial.ac.uk>
References: <cp1np3$msi$1@sea.gmane.org> <41B46DD9.1030101@imperial.ac.uk>
Message-ID: <74s8r09hjq7mevjegmgo1b1tvvap7ft306@4ax.com>

On Mon, 06 Dec 2004 14:34:01 +0000, Chris Jackson
<chris.jackson at imperial.ac.uk> wrote :

>The ideal solution seems to be what's done in Mac OS X, that is, to
>store a permanent path to the current R version as a symlink, and then
>just changing where the symlink points to when R is upgraded.
>Unfortunately Windows doesn't do symlinks.

It doesn't do symlinks, but for this purpose shortcuts are pretty much
the same thing.  In the default interpreter, "shortcut.lnk" or "start
shortcut.lnk" does the same as clicking on the shortcut in Explorer.
If you don't use the default interpreter, you might want a helper
executable that provides the "start" command externally.  It's trivial
to write (if you don't want all the bells and whistles of the internal
one).  

Duncan Murdoch



From MSchwartz at MedAnalytics.com  Mon Dec  6 16:03:46 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 06 Dec 2004 09:03:46 -0600
Subject: [R] barplot() options for intervals on axes
In-Reply-To: <200412061530.04415.Sebastien.Moretti@igs.cnrs-mrs.fr>
References: <200412061530.04415.Sebastien.Moretti@igs.cnrs-mrs.fr>
Message-ID: <1102345426.22293.24.camel@horizons.localdomain>

On Mon, 2004-12-06 at 15:30 +0100, Sebastien Moretti wrote:
> Hello,
> I am a beginner with R. I read many tutorials and the FAQ but I cannot solve 
> my problem. 
> I use barplot() to view my graph. I try to get more interval marks on y axis.
> I wasn't able to find options in 'help(barplot)' or 'help(par)' to do this 
> with barplot().
> 
> I seek for another option to print y values on my bars like on the graph of 
> the R homepage: http://www.r-project.org/
> 
> Thanks for your help.

The general process of customizing the annotation of the axes for a
variety of plots is posted frequently to this e-mail list, so a search
of the archive using "axis" as the keyword yields almost 2,000 hits.
Using "axis labels" narrows that to 650, which are more relevant.

The key is to inhibit the generation of the default y axis by using the
argument 'yaxt = "n"':

Compare:

barplot(1:5)

versus

barplot(1:5, yaxt = "n")


You can then use the axis() function to customize the y axis values:

barplot(1:5, yaxt = "n")
axis(2, at = seq(0, 5, 0.25), las = 1)

help("par") provides additional information on the graphic parameters,
which are the key to these types of customizations. See ?axis for more
information on that function as well.

To your second query, the key is to note that barplot() returns the bar
midpoints, which is referenced in the Value section of ?barplot.

Thus:

mp <- barplot(1:5, yaxt = "n", ylim = c(0, 6))
axis(2, at = seq(0, 5, 0.25), las = 1)
text(mp, 1:5, labels = 1:5, pos = 3)

See ?text for more information.

Also, note that I increased the range of the y axis here to make room
for the bar text labels (primarily the final tallest bar).

Finally, there is an article in the R Help Desk section of the October
2003 R News on basic graphic operations in R, which you might find
helpful. A direct link to it is:

http://cran.r-project.org/doc/Rnews/Rnews_2003-2.pdf

HTH,

Marc Schwartz



From ggrothendieck at myway.com  Mon Dec  6 16:04:35 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 6 Dec 2004 15:04:35 +0000 (UTC)
Subject: [R] Modyfing PATH in Windows Installer for R
References: <cp1np3$msi$1@sea.gmane.org>
	<ebr8r093b5c37aedijlc44sf4rsik8j0md@4ax.com>
Message-ID: <loom.20041206T155835-669@post.gmane.org>

Duncan Murdoch <murdoch <at> stats.uwo.ca> writes:

: 
: On Mon, 06 Dec 2004 14:43:46 +0100, Henrik Andersson
: <h.andersson <at> nioo.knaw.nl> wrote :
: 
: >Just a small suggestion since Windows have a file system not designed 
: >for command line use...
: 
: Hmm?  I'm not sure what you mean by that.  As far as the command line
: is concerned, Windows and Unix file systems are pretty similar.
: 
: >Would it be possible to add the possibility of automatically 
: >adding/modifying the path to the R executables in the windows installer 
: >program?
: 
: You mean modifying the PATH environment variable?  I think it would be
: possible to do this, but I don't think it's a good idea:
: 
:  - Users who use a command line interface probably know how to do this
: themselves, and others don't need it.
: 
:  - Users who use a command line interface use a large variety of
: different mechanisms to set their path, and it would be hard for the R
: installer to do anything helpful and not harmful to automatically
: change what they've done.
: 
: For example, I set my path in F:/cygwin/etc/profile, but I may be the
: only R user who uses that particular file...

I have another suggestion.  If Rgui.exe and the other R exe's were to
check if they were in the R tree and if not look up the location in
the registry then one could just copy their Rgui.exe and other R .exe
files to any folder in their path.

This would be much less intrusive since it does not involve modifying
any system variables.  If the user does not copy them it works like it
does now.  If the user does copy them then its up to the user to be
sure that they installed R with the registry key option on which seems
fair.



From phgrosjean at sciviews.org  Mon Dec  6 16:04:56 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 6 Dec 2004 16:04:56 +0100
Subject: [R] Modyfing PATH in Windows Installer for R
In-Reply-To: <41B46DD9.1030101@imperial.ac.uk>
Message-ID: <200412061505.iB6F4x9M002768@outmx006.isp.belgacom.be>

 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chris Jackson
> Sent: Monday, December 06, 2004 3:34 PM
> To: Henrik Andersson
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Modyfing PATH in Windows Installer for R
> 
> Henrik Andersson wrote:
> > Just a small suggestion since Windows have a file system 
> not designed 
> > for command line use...
> > 
> > Would it be possible to add the possibility of automatically 
> > adding/modifying the path to the R executables in the windows 
> > installer program?
> 
> 
> After asking the same question on R-devel a few months ago, I 
> investigated the possibility.  This would be useful to me, as 
> it saves the trouble of modifying the path every time a new 
> version of R comes out.
> 
> As far as I could tell, the installer that R for Windows uses
> (InnoSetup) cannot modify the PATH out of the box, but it can do this
> via a third-party add-on script.   I was doubtful whether it was worth
> adding such a script to the R sources, given the small 
> benefit obtained.

Path and environment variables are indeed mirrored in the registry under
Windows NT. Thus, a program that can change the registry, like Inno Setup,
can also change the path. See here:
http://support.installshield.com/kb/view.asp?articleid=Q100090
Best,

Philippe Grosjean
 
> The ideal solution seems to be what's done in Mac OS X, that 
> is, to store a permanent path to the current R version as a 
> symlink, and then just changing where the symlink points to 
> when R is upgraded.
> Unfortunately Windows doesn't do symlinks.
> 
> Chris
> (probably a little esoteric for R-help)
> --
> Christopher Jackson <chris.jackson at imperial.ac.uk>, Research 
> Associate, Department of Epidemiology and Public Health, 
> Imperial College School of Medicine, Norfolk Place, London W2 
> 1PG, tel. 020 759 43371
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From RVARADHAN at JHMI.EDU  Mon Dec  6 16:21:49 2004
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Mon, 06 Dec 2004 10:21:49 -0500
Subject: [R] Computing the minimal polynomial or, at least, its degree
In-Reply-To: <41B12741.2030601@pdf.com>
Message-ID: <0I8B00CKY40DCA@jhuml1.jhmi.edu>

Dear Spencer,

Thank you very much for your help. Your solution is almost correct, but not
quite completely (it is correct for your example of A, but not in general).
The problem is that the minimal polynomial is related to the characteristic
polynomial in a not-so-straightforward manner.  The following theorem shows
the relationship between the two polynomials:

If C(\lambda), and M(\lambda) are the characteristic and minimal polynomial
of a square matrix A, respectively, and D_{n-1}(\lambda) be the greatest
common divisor of the elements of the adjoint of (\lambda I - A), then
C(\lambda) = D_{n-1}(\lambda) M(\lambda).

Is there a way to use this theorem and the "polynomial" library to compute
the minimal polynomial?

Thanks once again,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins Univerisity
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Friday, December 03, 2004 9:56 PM
To: Ravi Varadhan
Cc: r-help at r-project.org
Subject: Re: [R] Computing the minimal polynomial or, at least, its degree

	  How about the following:


library(polynom)
help(package="polynom")
A <- diag(c(1:2, 2))
eigVals <- eigen(A)$values
multEig <- table(eigVals)
k <- length(multEig)
ratPoly <- minPoly <- 1
for(i in 1:k){
   poly.i <- polynomial(c(-as.numeric(names(multEig)[i]), 1))
   minPoly <- (minPoly*poly.i)
   if(multEig[i]>1)
     ratPoly <- (ratPoly*poly.i^(multEig[i]-1))
}

 > minPoly
2 - 3*x + x^2
 > ratPoly
-2 + x
 >
	  hope this helps.  spencer graves

###############################
Spencer,

One could do this by a brute force approach. Suppose A is an nxn matrix, and
the distinct eigenvalues are: lambda_1, ..., lambda_k, with multiplicities
m_1, ..., m_k, such that they sum to n. Then the characteristic polynomial
is:
C(lambda) = \prod_i (lambda - lambda_i)^{m_i}
The minimal polynomial is given by:
M(lambda) = \prod_i (lambda - lambda_i)^{p_i},
where 1 \leq p_i \leq m_i.
So, one could run through all possible p_i, starting with the smallest
degree polynomial (within constraint), and stop when we find one that
exactly divides C(lambda).

Is there a cleverer way to do this?

Ravi.
#################################################
      Have you looked at library(polynom)?  Will that with
unique(eigen(A)$values) allow you to compute what you want?

      hope this helps.
      spencer graves

Ravi Varadhan wrote:

>Hi,
>
> 
>
>I would like to know whether there exist algorithms to compute the
>coefficients or, at least, the degree of the minimal polynomial of a square
>matrix A (over the field of complex numbers)? I don't know whether this
>would require symbolic computation.  If not, has any of the algorithms been
>implemented in R?  
>
> 
>
>Thanks very much,
>
>Ravi.
>
> 
>
>P.S.  Just for the sake of completeness, a minimal polynomial is a monic
>polynomial (whose leading coefficient is unity) of least degree, which
>divides all the annihilating polynomial of A. In particular, the minimal
>polynomial divides the characteristic polynomial.  Knowing the degree of
the
>minimal polynomial is useful in characterizing the convergence properties
of
>a certain class of numerical schemes for iteratively solving linear (and
>nonlinear) system of equations.
>
> 
>
>--------------------------------------------------------------------------
>
>Ravi Varadhan, Ph.D.
>
>Assistant Professor,  The Center on Aging and Health
>
>Division of Geriatric Medicine and Gerontology
>
>Johns Hopkins Univerisity
>
>Ph: (410) 502-2619
>
>Fax: (410) 614-9625
>
>Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu
>
>--------------------------------------------------------------------------
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From rene_27pm at yahoo.com  Mon Dec  6 16:24:53 2004
From: rene_27pm at yahoo.com (Rene Pineda)
Date: Mon, 6 Dec 2004 09:24:53 -0600 (CST)
Subject: [R] (no subject)
Message-ID: <20041206152453.8162.qmail@web41014.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041206/83774547/attachment.pl

From nhy303 at abdn.ac.uk  Mon Dec  6 16:26:40 2004
From: nhy303 at abdn.ac.uk (nhy303@abdn.ac.uk)
Date: Mon, 6 Dec 2004 15:26:40 -0000 (GMT)
Subject: [R] Missing Values
Message-ID: <1109.139.133.94.35.1102346800.squirrel@139.133.94.35>

I have just started using R for my PhD.  I am importing my data from Excel
via notepad into Word.  Unfortunately, my data has many missing values.  I
have put '.' and this allowed me to import the data into R.  However, I
now want to interpolate these missing values.  Please can someone give me
some pointers as to the method/code I could use?

Thankyou,

Lillian.



From rene_27pm at yahoo.com  Mon Dec  6 16:28:46 2004
From: rene_27pm at yahoo.com (Rene Pineda)
Date: Mon, 6 Dec 2004 09:28:46 -0600 (CST)
Subject: [R] EXPORT OUPUTS
Message-ID: <20041206152846.106.qmail@web41008.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041206/9c8a2317/attachment.pl

From murdoch at stats.uwo.ca  Mon Dec  6 16:30:13 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 06 Dec 2004 10:30:13 -0500
Subject: [R] Modyfing PATH in Windows Installer for R
In-Reply-To: <200412061505.iB6F4x9M002768@outmx006.isp.belgacom.be>
References: <41B46DD9.1030101@imperial.ac.uk>
	<200412061505.iB6F4x9M002768@outmx006.isp.belgacom.be>
Message-ID: <4iu8r093fhfccpipf1fpjksvuj5t9fsqso@4ax.com>

On Mon, 6 Dec 2004 16:04:56 +0100, "Philippe Grosjean"
<phgrosjean at sciviews.org> wrote :

>Path and environment variables are indeed mirrored in the registry under
>Windows NT. Thus, a program that can change the registry, like Inno Setup,
>can also change the path. See here:
>http://support.installshield.com/kb/view.asp?articleid=Q100090

But we still have users using Win9x versions, which use a more
DOS-like method of setting the path.  At some point we'll drop support
for them, but I don't want to do it sooner than necessary.

Duncan Murdoch



From Sebastien.Moretti at igs.cnrs-mrs.fr  Mon Dec  6 16:31:50 2004
From: Sebastien.Moretti at igs.cnrs-mrs.fr (Sebastien Moretti)
Date: Mon, 6 Dec 2004 16:31:50 +0100
Subject: [R] barplot() options for intervals on axes
In-Reply-To: <1102345426.22293.24.camel@horizons.localdomain>
References: <200412061530.04415.Sebastien.Moretti@igs.cnrs-mrs.fr>
	<1102345426.22293.24.camel@horizons.localdomain>
Message-ID: <200412061631.50383.Sebastien.Moretti@igs.cnrs-mrs.fr>

> > Hello,
> > I am a beginner with R. I read many tutorials and the FAQ but I cannot
> > solve my problem.
> > I use barplot() to view my graph. I try to get more interval marks on y
> > axis. I wasn't able to find options in 'help(barplot)' or 'help(par)' to
> > do this with barplot().
> >
> > I seek for another option to print y values on my bars like on the graph
> > of the R homepage: http://www.r-project.org/
> >
> > Thanks for your help.
>
> The general process of customizing the annotation of the axes for a
> variety of plots is posted frequently to this e-mail list, so a search
> of the archive using "axis" as the keyword yields almost 2,000 hits.
> Using "axis labels" narrows that to 650, which are more relevant.
>
> The key is to inhibit the generation of the default y axis by using the
> argument 'yaxt = "n"':
>
> Compare:
>
> barplot(1:5)
>
> versus
>
> barplot(1:5, yaxt = "n")
>
>
> You can then use the axis() function to customize the y axis values:
>
> barplot(1:5, yaxt = "n")
> axis(2, at = seq(0, 5, 0.25), las = 1)

It's exactly what I seek for !
There are too many answers in the FAQ.
So, I use 
axis(2,at=c(0,round(max(y)/8),round(max(y)/4),round(max(y)/2),round(3*max(y)/4),max(y)))
to print the vertical axe I want.

> help("par") provides additional information on the graphic parameters,
> which are the key to these types of customizations. See ?axis for more
> information on that function as well.
>
> To your second query, the key is to note that barplot() returns the bar
> midpoints, which is referenced in the Value section of ?barplot.
>
> Thus:
>
> mp <- barplot(1:5, yaxt = "n", ylim = c(0, 6))
> axis(2, at = seq(0, 5, 0.25), las = 1)
> text(mp, 1:5, labels = 1:5, pos = 3)

The text() command prints y value labels over my bars

Maybe some examples like that should be in the R manual

Thanks

> See ?text for more information.
>
> Also, note that I increased the range of the y axis here to make room
> for the bar text labels (primarily the final tallest bar).
>
> Finally, there is an article in the R Help Desk section of the October
> 2003 R News on basic graphic operations in R, which you might find
> helpful. A direct link to it is:
>
> http://cran.r-project.org/doc/Rnews/Rnews_2003-2.pdf
>
> HTH,
>
> Marc Schwartz

-- 
Sebastien MORETTI
Linux User - #327894
CNRS - IGS
31 chemin Joseph Aiguier
13402 Marseille cedex 20, FRANCE
tel. +33 (0)4 91 16 44 55



From murdoch at stats.uwo.ca  Mon Dec  6 16:36:25 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 06 Dec 2004 10:36:25 -0500
Subject: [R] Missing Values
In-Reply-To: <1109.139.133.94.35.1102346800.squirrel@139.133.94.35>
References: <1109.139.133.94.35.1102346800.squirrel@139.133.94.35>
Message-ID: <nqu8r0t50tjl3nqe6md79meobsge50htsv@4ax.com>

On Mon, 6 Dec 2004 15:26:40 -0000 (GMT), nhy303 at abdn.ac.uk wrote :

>I have just started using R for my PhD.  I am importing my data from Excel
>via notepad into Word.  Unfortunately, my data has many missing values.  I
>have put '.' and this allowed me to import the data into R.  However, I
>now want to interpolate these missing values.  Please can someone give me
>some pointers as to the method/code I could use?

The approx() function does linear interpolation.

For example:

> x <- 1:10
> y <- c(1, NA, 3, NA, NA, 2, NA, NA, NA, NA)
> 
> approx(x, y, xout = x)
$x
 [1]  1  2  3  4  5  6  7  8  9 10

$y
 [1] 1.000000 2.000000 3.000000 2.666667 2.333333 2.000000       NA
NA
 [9]       NA       NA

To get it to extrapolate those values at the end, you could try "rule
= 2", but this might not do what you want...

Duncan Murdoch



From MSchwartz at MedAnalytics.com  Mon Dec  6 16:48:44 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 06 Dec 2004 09:48:44 -0600
Subject: [R] barplot() options for intervals on axes
In-Reply-To: <200412061631.50383.Sebastien.Moretti@igs.cnrs-mrs.fr>
References: <200412061530.04415.Sebastien.Moretti@igs.cnrs-mrs.fr>
	<1102345426.22293.24.camel@horizons.localdomain>
	<200412061631.50383.Sebastien.Moretti@igs.cnrs-mrs.fr>
Message-ID: <1102348124.24131.2.camel@horizons.localdomain>

On Mon, 2004-12-06 at 16:31 +0100, Sebastien Moretti wrote:

> There are too many answers in the FAQ.

Given the discussions here of late, I suspect that there will be one or
two folks who might disagree with that statement...

;-)

Marc



From MSchwartz at MedAnalytics.com  Mon Dec  6 16:51:18 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 06 Dec 2004 09:51:18 -0600
Subject: [R] EXPORT OUPUTS
In-Reply-To: <20041206152846.106.qmail@web41008.mail.yahoo.com>
References: <20041206152846.106.qmail@web41008.mail.yahoo.com>
Message-ID: <1102348278.24131.5.camel@horizons.localdomain>

On Mon, 2004-12-06 at 09:28 -0600, Rene Pineda wrote:
> I need information about how i can exports the ouputs in latex or in
> other format...
> Thank's

See the 'xtable' package as well as the latex() function in the 'Hmisc'
package on CRAN.

HTH,

Marc Schwartz



From vito_ricci at yahoo.com  Mon Dec  6 16:53:52 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Mon, 6 Dec 2004 16:53:52 +0100 (CET)
Subject: [R] Re: (no subject)
Message-ID: <20041206155352.99626.qmail@web41211.mail.yahoo.com>

Hi,

see:

http://agec221.agecon.uiuc.edu/csiss/Rgeo/

for R-Spatial Models, are there many packages for
spatial data analysis.

Regards
Vito

you wrote:

I need information about space state models in
structural model and kalman filtering. I have a
univariate time serie and i nedd aplicate space state
model
 
Thank's

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From Sebastien.Moretti at igs.cnrs-mrs.fr  Mon Dec  6 16:59:37 2004
From: Sebastien.Moretti at igs.cnrs-mrs.fr (Sebastien Moretti)
Date: Mon, 6 Dec 2004 16:59:37 +0100
Subject: Fwd: Re: [R] barplot() options for intervals on axes
Message-ID: <200412061659.37502.Sebastien.Moretti@igs.cnrs-mrs.fr>

>There are too many answers in the FAQ.
For this topic !

> Marc Schwartz

-- 
Sebastien MORETTI
Linux User - #327894
CNRS - IGS
31 chemin Joseph Aiguier
13402 Marseille cedex 20, FRANCE
tel. +33 (0)4 91 16 44 55



From ym at climpact.com  Mon Dec  6 17:15:52 2004
From: ym at climpact.com (Yves Magliulo)
Date: 06 Dec 2004 17:15:52 +0100
Subject: [R] Missing Values
In-Reply-To: <1109.139.133.94.35.1102346800.squirrel@139.133.94.35>
References: <1109.139.133.94.35.1102346800.squirrel@139.133.94.35>
Message-ID: <1102349752.18602.93.camel@new-york.climpact.net>

about the way you import your data in R : 
if you have a large data set in EXCEL, you should rather save it
directly in EXCEL in csv format with tab or ";" separator for instance, 
leting your missing value blank or empty cells.

->then with ?read.table or ?read.csv, you should not have trouble to
import your data (see option in help pages).
na.string option will convert for you your missing value in NA.

you could also do it directly from excel to R :
cran.r-project.org/doc/manuals/R-data.pdf for more info
 

yves Magliulo, PARIS


Le lun 06/12/2004 ?? 16:26, nhy303 at abdn.ac.uk a ??crit :
> I have just started using R for my PhD.  I am importing my data from Excel
> via notepad into Word.  Unfortunately, my data has many missing values.  I
> have put '.' and this allowed me to import the data into R.  However, I
> now want to interpolate these missing values.  Please can someone give me
> some pointers as to the method/code I could use?
> 
> Thankyou,
> 
> Lillian.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From lisawang at uhnres.utoronto.ca  Mon Dec  6 17:26:42 2004
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Mon, 06 Dec 2004 11:26:42 -0500
Subject: [R] The survival rate at a certain time
Message-ID: <41B48842.EA1CCE3B@uhnres.utoronto.ca>

Hello there,

I am doing analysis on survival data. How do I pick out a probability of
survival at a chosen landmark time(for example, 3 years, 4 years) from
the result of "survfit"? or any other functions? As I know, the result
of 'survfit' only have the probabilities for all event or cencor time.

Thank you very much

Lisa Wang

Princess Margaret Hospital
Toronto, Ca

tel: 416-946-4501 ext.5201



From ym at climpact.com  Mon Dec  6 17:35:59 2004
From: ym at climpact.com (Yves Magliulo)
Date: 06 Dec 2004 17:35:59 +0100
Subject: [R] Gam() function in R
In-Reply-To: <Pine.LNX.4.58.0412061054421.16842@moon.stats.gla.ac.uk>
References: <200412060536.iB65aO8U029853@postoffice8.mail.cornell.edu>
	<3235E28C-475E-11D9-A29C-000A95C76CA8@oulu.fi>
	<1102326506.18602.60.camel@new-york.climpact.net>
	<Pine.LNX.4.58.0412061054421.16842@moon.stats.gla.ac.uk>
Message-ID: <1102350959.18602.114.camel@new-york.climpact.net>

so mgcv package is the one i need! indeed, i want integrated smoothness
selection and smooth interactions rather than stepwise selection. i have
a lot of predictor, and i use gam to select those who are "efficient"
and exclude others. (using p-value)

thanks a lot for those precious information.


Le lun 06/12/2004 ?? 12:41, Simon Wood a ??crit :
> > this subject is very intersting for me. I'm using mgcv 0.8-9 with R
> > version 1.7.1. i didn't know that there was an another gam version with
> > package library(gam). Someone can tell me the basics differences between
> > them? I look for an help page on google but i only find "mgcv" help
> > pages.
> 
> - I think you'd need to move to a newer version of R in order to use 
> package gam, but that would also let you use a much more recent version of 
> package mgcv. 
> 
> - package gam is based very closely on the GAM approach presented in 
> Hastie and Tibshirani's  "Generalized Additive Models" book. Estimation is 
> by back-fitting and model selection is based on step-wise regression 
> methods based on approximate distributional results. A particular strength 
> of this approach is that local regression smoothers (`lo()' terms) can be 
> included in GAM models.
> 
> - gam in package mgcv represents GAMs using penalized regression splines. 
> Estimation is by direct penalized likelihood maximization with 
> integrated smoothness estimation via GCV or related criteria (there is 
> also an alternative `gamm' function based on a mixed model approach). 
> Strengths of the this approach are that s() terms can be functions of more 
> than one variable and that tensor product smooths are available via te() 
> terms - these are useful when different degrees of smoothness are 
> appropriate relative to different arguments of a smooth. 
> 
> Here's an attempt at a summary of the differences:
> 
> Estimation: gam::gam based on backfitting, mgcv::gam based on direct 
> penalized likelihood maximization (with smoothness estimation integrated)
> 
> Model selection: package(gam) based on stepwise regression methods. 
> mgcv::gam based on integrated GCV estimation of degree of smoothness.
> 
> Smooth terms: gam::gam can represent smooth terms using a very wide range 
> of scatterplot smoothers incuding loess, which is built in. mgcv::gam is 
> restricted to smoothers that can be represented using basis functions and 
> an associated ``wiggliness'' penalty, but these include low rank thin 
> plate spline smoothers and tensor product smoothers for smooths of more 
> than one variable. Both packages provide interfaces for adding new classes 
> of smoother. 
> 
> Uncertainty estimation: since mgcv GAMs explicitly estimate 
> coefficients for each smooth term, it is fairly straightforward to obtain 
> a covariance matrix for the model coefficients, which makes further 
> variance calcualtions easy. For example predictions with standard errors 
> are easily obtained for predictions made with new prediction data. The 
> backfitting approach makes variance calculation more difficult (e.g. at 
> present s.e.s are not available from gam::predict.gam with new data)
> 
> Interface: both packages are based on Trevor Hastie's Chapter 7 of 
> Chambers and Hastie. Since Trevor H. wrote package(gam) it's a closer 
> implementation than package(mgcv). 
> 
> Basically, if you want integrated smoothness selection, an underlying 
> parametric representation, or want smooth interactions in your models 
> then mgcv is probably worth a try (but I would say that). If you want to 
> use local regression smoothers and/or prefer the stepwise selection 
> approach then package gam is for you. 
> 
> Simon
> 
> _____________________________________________________________________
> > Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
> >>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
> >>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814
> 
> 
>



From maustin at amgen.com  Mon Dec  6 17:40:19 2004
From: maustin at amgen.com (Austin, Matt)
Date: Mon, 6 Dec 2004 08:40:19 -0800 
Subject: [R] The survival rate at a certain time
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DC18@teal-exch.amgen.com>

You can use the summary method for a survfit object from the package
survival as in the following example:

> library(survival)
> summary( survfit( Surv(futime, fustat)~rx,data=ovarian), times=500)
Call: survfit(formula = Surv(futime, fustat) ~ rx, data = ovarian)

                rx=1 
        time       n.risk      n.event     survival      std.err lower 95%
CI 
     500.000        5.000        6.000        0.538        0.138
0.326 
upper 95% CI 
       0.891 

                rx=2 
        time       n.risk      n.event     survival      std.err lower 95%
CI 
     500.000        7.000        4.000        0.658        0.141
0.433 
upper 95% CI 
       1.000 


Matt Austin
Statistician

Amgen 
One Amgen Center Drive
M/S 24-2-C
Thousand Oaks CA 93021
(805) 447 - 7431


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Lisa Wang
> Sent: Monday, December 06, 2004 8:27 AM
> To: R-Help
> Subject: [R] The survival rate at a certain time
> 
> 
> Hello there,
> 
> I am doing analysis on survival data. How do I pick out a 
> probability of
> survival at a chosen landmark time(for example, 3 years, 4 years) from
> the result of "survfit"? or any other functions? As I know, the result
> of 'survfit' only have the probabilities for all event or cencor time.
> 
> Thank you very much
> 
> Lisa Wang
> 
> Princess Margaret Hospital
> Toronto, Ca
> 
> tel: 416-946-4501 ext.5201
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From swanson at yellowstoneresearch.org  Mon Dec  6 17:42:52 2004
From: swanson at yellowstoneresearch.org (Alan Swanson)
Date: Mon, 6 Dec 2004 09:42:52 -0700
Subject: [R] Excel *.xls files, RODBC
Message-ID: <20041206164235.A8F5F5FAAA@mail01.bridgeband.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041206/3854057d/attachment.pl

From gunter.berton at gene.com  Mon Dec  6 17:55:55 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 6 Dec 2004 08:55:55 -0800
Subject: [R] Excel *.xls files, RODBC
In-Reply-To: <41B1D349.9030100@optonline.net>
Message-ID: <200412061655.iB6GttKv023249@meitner.gene.com>

Folks: 

An additional issue is that worksheet names must conform to ODBC/SQL
standards; so, for example, Excel permits worksheet names with embedded
spaces, but RODBC does not handle them (or at least, I was not able to get
to to do so), as this is not permitted in the standards. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chuck Cleland
> Sent: Saturday, December 04, 2004 7:10 AM
> To: Rolf Turner
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Excel *.xls files, RODBC
> 
>    The following works for me under WinXP Pro to create 
> "myframe" as a 
> data frame:
> 
> library(RODBC)
> z <- odbcConnectExcel("c:/myfolder/mydata.xls")
> myframe <- sqlFetch(z, "Sheet1")
> close(z)
> 
>    Are you indicating the name of the worksheet you want within the 
> *.xls file?  I suspect there could be additional issues on a 
> non-Windows 
> OS that I don't know about.
> 
> hope this helps,
> 
> Chuck Cleland
>



From james.muller at canberra.net.au  Mon Dec  6 18:09:30 2004
From: james.muller at canberra.net.au (James Muller)
Date: Tue, 07 Dec 2004 04:09:30 +1100
Subject: [R] Excel *.xls files, RODBC
Message-ID: <41B4924A.8090404@canberra.net.au>

There is also a perl module that converts excel files to .csv on CPAN. 
It works fine for everything I've ever used it for, which is really 
simple stuff, i.e. no cells defined by functions.

steps involved:
1. go to www.cpan.org and find the package, download it
2. ensure you have the necessary setup to do things with perl, otherwise 
set them up
3. install the package
4. use it as the perl script instructs (i.e. <scriptname> --help or 
something similar)

Cheers

James



From tlumley at u.washington.edu  Mon Dec  6 18:14:27 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 6 Dec 2004 09:14:27 -0800 (PST)
Subject: [R] how to get how many lines there are in a file.
In-Reply-To: <41B46E46.9020201@statistik.uni-dortmund.de>
References: <6f3fc9ee041206061258dcce1a@mail.gmail.com>
	<41B46E46.9020201@statistik.uni-dortmund.de>
Message-ID: <Pine.A41.4.61b.0412060911440.230524@homer11.u.washington.edu>

On Mon, 6 Dec 2004, Uwe Ligges wrote:

> Hu Chen wrote:
>
>> hi all
>> If I wanna get the total number of lines in a big file without reading
>> the file's content into R as matrix or data frame, any methods or
>> functions?
>
> You must read it in R, or how do you think should one determine the number of 
> lines in a file (if you don't want to use another program)?
> I'd suggest length(readLines(...)).
>

If the file is large enough that you don't want to read the whole thing at 
once you can read it in chunks using readLines(). If all the lines are the 
same length you can find the size of the file and divide by the length of 
a line.

Also, you don't say what OS you are on.  If it isn't Windows the easiest 
thing would be to use wc.

 	-thomas



From MSchwartz at MedAnalytics.com  Mon Dec  6 18:22:12 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 06 Dec 2004 11:22:12 -0600
Subject: [R] Excel *.xls files, RODBC
In-Reply-To: <41B4924A.8090404@canberra.net.au>
References: <41B4924A.8090404@canberra.net.au>
Message-ID: <1102353732.24131.15.camel@horizons.localdomain>

On Tue, 2004-12-07 at 04:09 +1100, James Muller wrote:
> There is also a perl module that converts excel files to .csv on CPAN. 
> It works fine for everything I've ever used it for, which is really 
> simple stuff, i.e. no cells defined by functions.
> 
> steps involved:
> 1. go to www.cpan.org and find the package, download it
> 2. ensure you have the necessary setup to do things with perl, otherwise 
> set them up
> 3. install the package
> 4. use it as the perl script instructs (i.e. <scriptname> --help or 
> something similar)
> 
> Cheers
> 
> James

There is also the read.xls() function in the gdata package, which is
part of the gregmisc bundle on CRAN.

HTH,

Marc Schwartz



From andy_liaw at merck.com  Mon Dec  6 18:26:42 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 6 Dec 2004 12:26:42 -0500
Subject: [R] how to get how many lines there are in a file.
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3E0@usrymx25.merck.com>

> From: Liaw, Andy
> 
> > From: Marc Schwartz
> > 
> > On Mon, 2004-12-06 at 22:12 +0800, Hu Chen wrote:
> > > hi all
> > > If I wanna get the total number of lines in a big file 
> > without reading
> > > the file's content into R as matrix or data frame, any methods or
> > > functions?
> > > thanks in advance.
> > > Regards
> > 
> > See ?readLines
> > 
> > You can use:
> > 
> > length(readLines("FileName"))
> > 
> > to get the number of lines read.
> > 
> > HTH,
> > 
> > Marc Schwartz
> 
> 
> On a system equipped with `wc' (*nix or Windows with such utilities
> installed and on PATH) I would use that.  Otherwise 
> length(count.fields())
> might be a good choice.
> 
> Cheers,
> Andy

Marc alerted me off-list that count.fields() might spent time delimiting
fields, which is not needed for the purpose of counting lines, and suggested
using sep="\n" as a possible way to make it more efficient.  (Thanks, Marc!)

 Here are some tests on a file with 14337 lines and  8900 fields (space
delimited).

> system.time(n <- length(count.fields("hcv.ap")), gcFirst=TRUE)
[1] 48.86  0.24 49.30  0.00  0.00
> system.time(n <- length(count.fields("hcv.ap", sep="\n")), gcFirst=TRUE)
[1] 42.19  0.26 42.60  0.00  0.00
> n
[1] 14337
> system.time(n2 <- length(readLines("hcv.ap")), gcFirst=TRUE)
[1] 37.77  0.56 38.35  0.00  0.00
> n2
[1] 14337
> system.time(n3 <- scan(pipe("wc -l hcv.ap"), what=list(0, NULL))[[1]],
gcFirst=T)
Read 1 records
[1] 0.00 0.00 0.33 0.08 0.25
> n3
[1] 14337

My only concern with the readLines() approach is that it still needs to read
the entire file into memory (if I'm not mistaken), which may not be
desirable:

> system.time(obj <- readLines("hcv.ap"), gcFirst=TRUE)
[1] 36.72  0.48 37.24  0.00  0.00
> object.size(obj)/1024^2
[1] 244.6308

So it took 244+ MB just to store the text read in.  I would use a loop and
read the file in small chunks, if I really want to do it in R.

Cheers,
Andy



From f.harrell at vanderbilt.edu  Mon Dec  6 18:36:28 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 06 Dec 2004 12:36:28 -0500
Subject: [R] Gam() function in R
In-Reply-To: <1102350959.18602.114.camel@new-york.climpact.net>
References: <200412060536.iB65aO8U029853@postoffice8.mail.cornell.edu>	<3235E28C-475E-11D9-A29C-000A95C76CA8@oulu.fi>	<1102326506.18602.60.camel@new-york.climpact.net>	<Pine.LNX.4.58.0412061054421.16842@moon.stats.gla.ac.uk>
	<1102350959.18602.114.camel@new-york.climpact.net>
Message-ID: <41B4989C.6000802@vanderbilt.edu>

Yves Magliulo wrote:
> so mgcv package is the one i need! indeed, i want integrated smoothness
> selection and smooth interactions rather than stepwise selection. i have
> a lot of predictor, and i use gam to select those who are "efficient"
> and exclude others. (using p-value)

It is interesting that you use P-values but do not care that the 
strategy you use (variable selection as opposed to pre-specifying models 
or just using shrinkage) does not preserve type I error or confidence 
interval coverage probabilities in subsequent analyses with mgcv.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From mike at pathwaylearning.com  Mon Dec  6 18:38:41 2004
From: mike at pathwaylearning.com (Mike Thomas)
Date: Mon, 6 Dec 2004 11:38:41 -0600
Subject: [R] R Coders - Programmers
Message-ID: <200412061740.iB6HecuA052494@lora.pns.networktel.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041206/c55ff4fb/attachment.pl

From thpe at hhbio.wasser.tu-dresden.de  Mon Dec  6 18:49:42 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 06 Dec 2004 18:49:42 +0100
Subject: [R] one dimensional AKIMA in R?
Message-ID: <41B49BB6.8000609@hhbio.wasser.tu-dresden.de>

Hello,

I am looking for an interpolation method similar to the one-dimensional 
AKIMA interpolation as in Akima (1970). Is there already such an 
algorithm in R which I may have overlooked?

Thank you in advance

Thomas P.


H. Akima, A new method of interpolation and smooth curve fitting based 
on local procedures, J. Assoc. Comput. Mach. 17, 589-602, 1970.



From MSchwartz at MedAnalytics.com  Mon Dec  6 18:50:12 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 06 Dec 2004 11:50:12 -0600
Subject: [R] how to get how many lines there are in a file.
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E3E0@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E3E0@usrymx25.merck.com>
Message-ID: <1102355412.28601.5.camel@horizons.localdomain>

On Mon, 2004-12-06 at 12:26 -0500, Liaw, Andy wrote:

> Marc alerted me off-list that count.fields() might spent time delimiting
> fields, which is not needed for the purpose of counting lines, and suggested
> using sep="\n" as a possible way to make it more efficient.  (Thanks, Marc!)
> 
>  Here are some tests on a file with 14337 lines and  8900 fields (space
> delimited).
> 
> > system.time(n <- length(count.fields("hcv.ap")), gcFirst=TRUE)
> [1] 48.86  0.24 49.30  0.00  0.00
> > system.time(n <- length(count.fields("hcv.ap", sep="\n")), gcFirst=TRUE)
> [1] 42.19  0.26 42.60  0.00  0.00

Andy,

I suspect that the relatively modest gain to be had here is the result
of count.fields() still scanning the input buffer for the delimiting
character, even though it would occur only once per line using the
newline character. Thus, the overhead is not reduced substantially.

A scan of the source code for the .Internal function would validate
that.

Thanks for testing this.

As both you and Thomas mention, 'wc' is clearly the fastest way to go
based upon your additional figures.

Best regards,

Marc



From leroy at ucsd.edu  Mon Dec  6 18:55:49 2004
From: leroy at ucsd.edu (Anthony Westerling)
Date: Mon, 6 Dec 2004 09:55:49 -0800
Subject: [R] Cocoa GUI:  pasting in R Console yields syntax error
In-Reply-To: <7AEF1A0D-4768-11D9-83FC-000A95BBEB3C@aber.ac.uk>
References: <937218FB-4761-11D9-9364-000A959EA8AA@ucsd.edu>
	<7AEF1A0D-4768-11D9-83FC-000A95BBEB3C@aber.ac.uk>
Message-ID: <103E5121-47B0-11D9-9364-000A959EA8AA@ucsd.edu>

Thanks David.  I'm glad I'm not the only one.  I was beginning to 
wonder if I had really lost it.

Anthony

On Dec 6, 2004, at 1:23 AM, David Enot wrote:

> Anthony
>
> I faced the same problem and it took me some time to spot the origin: 
> I have no clue where it can come from!( I suspect this happened when I 
> moved from 1.9.0 to 2.0.0, OS X 1.3...). I know that the built in R 
> editor is very handy: what I do is a "more myfile.r" on the terminal 
> to check if there are weird characters and then delete then directly 
> with the R editor. Not very efficient I reckon: OS X gurus may have 
> another solution!
>
>   David
>
>
> On 6 Dec 2004, at 08:33, Anthony Westerling wrote:
>
>> I've recently upgraded to R-2.0.1 on a Mac running OS X 10.3+
>>
>> I am using the new Cocoa-based GUI.  Everything was working well for 
>> a while.  In the middle of an R session, I started "suddenly" to have 
>> a problem where code copied from an open editor window and pasted 
>> into the R Console gives a syntax error.  It doesn't matter what the 
>> code is.  If the same exact text is typed into the console directly, 
>> I get no errors.
>>
>> I tried quitting the R session and restarting.  The problem did not 
>> go away.
>>
>> I tried using a different editor, instead of the built-in editor.  
>> After opening the file with my R code in it in AlphaX instead of the 
>> built-in editor, I could see that the text typed most recently ( ie, 
>> since the problem started) had a character that looked like an open 
>> square or box at the start of most lines.  I deleted these and can't 
>> see any other extraneous symbols in AlphaX.  However, I still get 
>> syntax errors when trying to paste code that was originally typed in 
>> using the built-in editor.  If I retype the same thing in the same 
>> file using AlphaX, one line below the original, then copy and paste 
>> into the R console, it executes without generating syntax errors.
>>
>> So, it looks like something odd is going on with the built-in editor?
>>
>> Anthony Westerling
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
>



From david.whiting at ncl.ac.uk  Mon Dec  6 18:52:40 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 06 Dec 2004 17:52:40 +0000
Subject: [R] how to get how many lines there are in a file.
In-Reply-To: <Pine.A41.4.61b.0412060911440.230524@homer11.u.washington.edu>
References: <6f3fc9ee041206061258dcce1a@mail.gmail.com>
	<41B46E46.9020201@statistik.uni-dortmund.de>
	<Pine.A41.4.61b.0412060911440.230524@homer11.u.washington.edu>
Message-ID: <m2pt1njouv.fsf@192.168.57.36>


Thomas Lumley <tlumley at u.washington.edu> writes:

[...]

> If the file is large enough that you don't want to read the whole
> thing at once you can read it in chunks using readLines(). If all the
> lines are the same length you can find the size of the file and divide
> by the length of a line.
> 
> Also, you don't say what OS you are on.  If it isn't Windows the
> easiest thing would be to use wc.

Part of the reason I got into Linux was because I needed to do
(little?)  things that I found tricky to do in Windows and bit-by-bit
I accumulated various *nix tools that had been ported to DOS/Windows,
such as these found here:

http://unxutils.sourceforge.net/

So if your OS is Windows you could stick with it and still use wc
(assuming that it works in the same way as under Linux---I haven't
tested the version at this location).

Dave

-- 
David Whiting
University of Newcastle upon Tyne, UK



From ligges at statistik.uni-dortmund.de  Mon Dec  6 19:13:33 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 06 Dec 2004 19:13:33 +0100
Subject: [R] how to get how many lines there are in a file.
In-Reply-To: <m2pt1njouv.fsf@192.168.57.36>
References: <6f3fc9ee041206061258dcce1a@mail.gmail.com>	<41B46E46.9020201@statistik.uni-dortmund.de>	<Pine.A41.4.61b.0412060911440.230524@homer11.u.washington.edu>
	<m2pt1njouv.fsf@192.168.57.36>
Message-ID: <41B4A14D.1000503@statistik.uni-dortmund.de>

David Whiting wrote:

> Thomas Lumley <tlumley at u.washington.edu> writes:
> 
> [...]
> 
> 
>>If the file is large enough that you don't want to read the whole
>>thing at once you can read it in chunks using readLines(). If all the
>>lines are the same length you can find the size of the file and divide
>>by the length of a line.
>>
>>Also, you don't say what OS you are on.  If it isn't Windows the
>>easiest thing would be to use wc.
> 
> 
> Part of the reason I got into Linux was because I needed to do
> (little?)  things that I found tricky to do in Windows and bit-by-bit
> I accumulated various *nix tools that had been ported to DOS/Windows,
> such as these found here:
> 
> http://unxutils.sourceforge.net/
> 
> So if your OS is Windows you could stick with it and still use wc
> (assuming that it works in the same way as under Linux---I haven't
> tested the version at this location).

Well, the also tools required to compile R and packages under Windows 
contain wc.

Anyway, the question is whether you want to write code that performs 
best for a given problem on a given machine, or you want to write 
portable code (e.g. to be used on machines without having wc installed).

Uwe Ligges


> Dave
>



From james.muller at canberra.net.au  Mon Dec  6 19:13:31 2004
From: james.muller at canberra.net.au (James Muller)
Date: Tue, 07 Dec 2004 05:13:31 +1100
Subject: [R] convert Map shapes to logical matrices - for set operations
Message-ID: <41B4A14B.8060509@canberra.net.au>

Hi, I have a little problem.

I'm trying to do the following:
  Convert  _projected_ shapes from a Map object into logical matrices. 
That is, rasterize a shape into a logical "in-the-shape" and 
"out-of-the-shape" matrix.

What I'm trying to do is get an 'equal-area' estimate of the area of 
intersection between two overlapping shapes, without any concern about 
there being holes in the shapes. I know owin can deal with most of this, 
but for sure it doesn't support geographic projections, which will allow 
the 'equal-area' estimate to be equal.

An idea I had was to pipe the output of the maptools Map plotting 
function map into the matrix. But I really have no idea how to do that 
at this point in my R education.

Any suggestions are extremely welcome.

Cheers,

James



From jdnewmil at dcn.davis.ca.us  Mon Dec  6 19:27:21 2004
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 6 Dec 2004 10:27:21 -0800 (PST)
Subject: [R] Re: (no subject)
In-Reply-To: <20041206155352.99626.qmail@web41211.mail.yahoo.com>
Message-ID: <Pine.LNX.4.21.0412061013260.6114-100000@mirimichi.jdn.localnet>

On Mon, 6 Dec 2004, Vito Ricci wrote:

> Hi,
> 
> see:
> 
> http://agec221.agecon.uiuc.edu/csiss/Rgeo/
> 
> for R-Spatial Models, are there many packages for
> spatial data analysis.

"State space" has nothing to do with "spatial"... the "space" term comes
from linear algebra, and has to do with how the innards of a black box can
be described by vectors representing its "state".

I am new to R, so can't really address the original question in this
context, but can suggest that Scilab and Octave are two free tools with
which Kalman filtering analyses are commonly applied.

FWIW, Google did suggest
https://stat.ethz.ch/pipermail/r-sig-finance/2004q3/000070.html and linked
messages, which suggests that library(stats) is the place to look [as in
help.search("kalman")]

> Regards
> Vito
> 
> you wrote:
> 
> I need information about space state models in
> structural model and kalman filtering. I have a
> univariate time serie and i nedd aplicate space state
> model
>  
> Thank's
> 
> =====
> Diventare costruttori di soluzioni
> Became solutions' constructors
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> Top 10 reasons to become a Statistician
> 
>      1. Deviation is considered normal
>      2. We feel complete and sufficient
>      3. We are 'mean' lovers
>      4. Statisticians do it discretely and continuously
>      5. We are right 95% of the time
>      6. We can legally comment on someone's posterior distribution
>      7. We may not be normal, but we are transformable
>      8. We never have to say we are certain
>      9. We are honestly significantly different
>     10. No one wants our jobs
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k



From andy_liaw at merck.com  Mon Dec  6 20:00:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 6 Dec 2004 14:00:43 -0500
Subject: [R] how to get how many lines there are in a file.
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3E2@usrymx25.merck.com>

> From: Marc Schwartz
> 
> On Mon, 2004-12-06 at 12:26 -0500, Liaw, Andy wrote:
> 
> > Marc alerted me off-list that count.fields() might spent 
> time delimiting
> > fields, which is not needed for the purpose of counting 
> lines, and suggested
> > using sep="\n" as a possible way to make it more efficient. 
>  (Thanks, Marc!)
> > 
> >  Here are some tests on a file with 14337 lines and  8900 
> fields (space
> > delimited).
> > 
> > > system.time(n <- length(count.fields("hcv.ap")), gcFirst=TRUE)
> > [1] 48.86  0.24 49.30  0.00  0.00
> > > system.time(n <- length(count.fields("hcv.ap", 
> sep="\n")), gcFirst=TRUE)
> > [1] 42.19  0.26 42.60  0.00  0.00
> 
> Andy,
> 
> I suspect that the relatively modest gain to be had here is the result
> of count.fields() still scanning the input buffer for the delimiting
> character, even though it would occur only once per line using the
> newline character. Thus, the overhead is not reduced substantially.
> 
> A scan of the source code for the .Internal function would validate
> that.
> 
> Thanks for testing this.
> 
> As both you and Thomas mention, 'wc' is clearly the fastest way to go
> based upon your additional figures.
> 
> Best regards,
> 
> Marc

Marc,

I wrote the following function to read the file in chunks:

countLines <- function(file, chunk=1e3) {
    f <- file(file, "r")
    on.exit(close(f))
    nLines <- 0
    while((n <- length(readLines(f, chunk))) > 0) nLines <- nLines + n
    nLines
}

To my surprise:

> system.time(n4 <- countLines3("hcv.ap"), gcFirst=TRUE)
[1] 35.24  0.26 35.53  0.00  0.00
> system.time(n4 <- countLines3("hcv.ap", 1), gcFirst=TRUE)
[1] 36.10  0.32 36.43  0.00  0.00

There's almost no penalty (in time) in reading one line at a time.  One do
save quite a bit of memory, though.

Cheers,
Andy



From mzp3769 at yahoo.com  Mon Dec  6 20:01:16 2004
From: mzp3769 at yahoo.com (m p)
Date: Mon, 6 Dec 2004 11:01:16 -0800 (PST)
Subject: [R] surface fitting
Message-ID: <20041206190116.45978.qmail@web51008.mail.yahoo.com>

Hello,
I am looking for a R routine to fit a function/surface
to my data 3d data. I'd like to use the function in a
model so something like splines is not applicable.
The data are smooth and can provide a plot if helpful.
Thanks,
Mark



From petzoldt at rcs.urz.tu-dresden.de  Mon Dec  6 20:52:20 2004
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 06 Dec 2004 20:52:20 +0100
Subject: [R] surface fitting
In-Reply-To: <20041206190116.45978.qmail@web51008.mail.yahoo.com>
References: <20041206190116.45978.qmail@web51008.mail.yahoo.com>
Message-ID: <41B4B874.1060703@rcs.urz.tu-dresden.de>

m p wrote:

> Hello,
> I am looking for a R routine to fit a function/surface
> to my data 3d data. I'd like to use the function in a
> model so something like splines is not applicable.
> The data are smooth and can provide a plot if helpful.

You may try surf.ls or surf.gls from package spatial.

Thomas P.



From mhassan at scitegic.com  Mon Dec  6 21:02:12 2004
From: mhassan at scitegic.com (Moises Hassan)
Date: Mon, 6 Dec 2004 12:02:12 -0800
Subject: [R] VR package not found for R version 1.9.1
Message-ID: <830D8D4719112B418ABBC3A0EBA9581272EC36@webmail.scitegic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041206/6d8b9897/attachment.pl

From murdoch at stats.uwo.ca  Mon Dec  6 21:48:23 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 06 Dec 2004 15:48:23 -0500
Subject: [R] VR package not found for R version 1.9.1
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EC36@webmail.scitegic.com>
References: <830D8D4719112B418ABBC3A0EBA9581272EC36@webmail.scitegic.com>
Message-ID: <46h9r0pf672srkcit2v2i728akocto70sv@4ax.com>

On Mon, 6 Dec 2004 12:02:12 -0800, "Moises Hassan"
<mhassan at scitegic.com> wrote :

>I'm trying to install the VR package for version 1.9.1 but I'm getting
>the following error message:
>
> 
>
>> install.packages("VR")
>
>trying URL `http://cran.r-project.org/bin/windows/contrib/1.9/PACKAGES'
>
>Content type `text/plain; charset=iso-8859-1' length 20716 bytes
>
>opened URL
>
>downloaded 20Kb
>
> 
>
>trying URL
>`http://cran.r-project.org/bin/windows/contrib/1.9/VR_7.2-11.zip'
>
>Error in download.file(url, destfile, method, mode = "wb") : 
>
>        cannot open URL
>`http://cran.r-project.org/bin/windows/contrib/1.9/VR_7.2-11.zip'
>
>In addition: Warning message: 
>
>cannot open: HTTP status was `404 Not Found'  
>
> 
>
>Any workarounds?

The problem is that sometimes the automatic updater at CRAN deletes an
old version, then fails to download the update. 

The workarounds are to wait a day or two, to try a different version
of R (version 2.0.1 succeeds in getting VR now), or to build the
package from source (which requires a number of tools to be installed;
see R for Windows FAQ "3.1 Can I install packages into libraries in
this version?" for details).

Duncan Murdoch



From kjetil at acelerate.com  Mon Dec  6 21:11:41 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 06 Dec 2004 16:11:41 -0400
Subject: [R] String manipulation---mixed case
In-Reply-To: <16820.27479.582733.212415@gargle.gargle.HOWL>
References: <web-2445987@be2.bc.edu>
	<41B32B5B.9000005@cirad.fr>	<x2u0r0d8gc.fsf@biostat.ku.dk>
	<41B38217.50303@pdf.com>	<16820.15789.985636.128628@gargle.gargle.HOWL>	<loom.20041206T124446-911@post.gmane.org>	<loom.20041206T130806-632@post.gmane.org>
	<16820.27479.582733.212415@gargle.gargle.HOWL>
Message-ID: <41B4BCFD.7000805@acelerate.com>

Martin Maechler wrote:

>>>>>>"Gabor" == Gabor Grothendieck <ggrothendieck at myway.com>
>>>>>>            
>>>>>>
.
.
.

>    Many people have been lamenting about too complicated
>    examples on help pages. 
>
            Yes, but the help examples should be advanced too! Long time 
ago, when I used excel a little,
             the examples in the help always stopped when my questions 
started (And once I used telephony help,
              called microsoft Oslo, The question was to difficult so 
was transferred to Stockholm, and then followed half an hour with the most
             rude/unpolite Swede I've ever met (maybe even the only one) 
which used that half hour trying convincing me that the error was mine.
              Finally he understood, and a technical report with a fix 
from microsoft was faxed...)

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Mon Dec  6 21:18:05 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 06 Dec 2004 16:18:05 -0400
Subject: [R] how to get how many lines there are in a file.
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E3DE@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E3DE@usrymx25.merck.com>
Message-ID: <41B4BE7D.4090002@acelerate.com>

Liaw, Andy wrote:

>>From: Marc Schwartz
>>
>>On Mon, 2004-12-06 at 22:12 +0800, Hu Chen wrote:
>>    
>>
>>>hi all
>>>If I wanna get the total number of lines in a big file 
>>>      
>>>
>>without reading
>>    
>>
>>>the file's content into R as matrix or data frame, any methods or
>>>functions?
>>>thanks in advance.
>>>Regards
>>>      
>>>
>>See ?readLines
>>
>>You can use:
>>
>>length(readLines("FileName"))
>>
>>to get the number of lines read.
>>
>>HTH,
>>
>>Marc Schwartz
>>    
>>
>
>
>On a system equipped with `wc' (*nix or Windows with such utilities
>installed and on PATH) I would use that.  Otherwise length(count.fields())
>might be a good choice.
>
>Cheers,
>Andy
>
>  
>
For instance on a windows machine (XP) with the development tools installed:

 > list.files()
 [1] "afm"             "AUTHORS"         "bin"             "CHANGES"       
 [5] "CHANGES1"        "COPYING"         "COPYING.LIB"     "COPYRIGHTS"    
 [9] "doc"             "etc"             "FAQ"             "include"       
[13] "lib"             "library"         "MD5"             "modules"       
[17] "NEWS"            "ONEWS"           "README"          "README.packages"
[21] "README.Rterm"    "README.rw2001"   "RESOURCES"       "rw-FAQ"        
[25] "share"           "src"             "Tcl"             "THANKS"        
[29] "unins000.dat"    "unins000.exe"    "Y2K"           
 > shell("wc --help")
Usage: wc [OPTION]... [FILE]...
Print newline, word, and byte counts for each FILE, and a total line if
more than one FILE is specified.  With no FILE, or when FILE is -,
read standard input.
  -c, --bytes            print the byte counts
  -m, --chars            print the character counts
  -l, --lines            print the newline counts
  -L, --max-line-length  print the length of the longest line
  -w, --words            print the word counts
      --help     display this help and exit
      --version  output version information and exit

Report bugs to <bug-textutils at gnu.org>.
 > shell("wc --lines THANKS")
     71 THANKS
 >

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Mon Dec  6 21:45:59 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 06 Dec 2004 16:45:59 -0400
Subject: [R]time series - state space  (was: no subject)
In-Reply-To: <20041206152453.8162.qmail@web41014.mail.yahoo.com>
References: <20041206152453.8162.qmail@web41014.mail.yahoo.com>
Message-ID: <41B4C507.1050702@acelerate.com>

Rene Pineda wrote:

>I need information about space state models in structural model and kalman filtering. I have a univariate time serie and i nedd aplicate space state model
> 
>  
>
Please use an informative subject line! See
?arima
and the references therein.

See also the R Newsletter, volume 2/2, 2002, downloadable from CRAN

Kjetil

>Thank's
>
>
>
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From dm60062003 at yahoo.com  Mon Dec  6 21:58:26 2004
From: dm60062003 at yahoo.com (Derek Margetts)
Date: Mon, 6 Dec 2004 12:58:26 -0800 (PST)
Subject: [R] plot color question
Message-ID: <20041206205826.19675.qmail@web53210.mail.yahoo.com>


Thanks again Andy and Uwe for you help on my previous
post of ploting lm coef and means.  With your
direction, I was able to expand and generalize the
function to meet my requirements. 

Follow up question: Is it possible to make the points
different colors depending on which quaderant they
fall into?

The data frame contains two variables (x and y)
Quad 1 = x>mean(x) & y>mean(y) would be blue
Quad 2 = x>mean(x) & y<mean(y) would be red
Quad 3 = x<mean(x) & y>mean(y) ....
Quad 4 = x<mean(x) & y<mean(y) ....

Derek



From MSchwartz at MedAnalytics.com  Mon Dec  6 22:10:18 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 06 Dec 2004 15:10:18 -0600
Subject: [R] how to get how many lines there are in a file.
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E3E2@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E3E2@usrymx25.merck.com>
Message-ID: <1102367420.28601.24.camel@horizons.localdomain>

On Mon, 2004-12-06 at 14:00 -0500, Liaw, Andy wrote:

> Marc,
> 
> I wrote the following function to read the file in chunks:
> 
> countLines <- function(file, chunk=1e3) {
>     f <- file(file, "r")
>     on.exit(close(f))
>     nLines <- 0
>     while((n <- length(readLines(f, chunk))) > 0) nLines <- nLines + n
>     nLines
> }
> 
> To my surprise:
> 
> > system.time(n4 <- countLines3("hcv.ap"), gcFirst=TRUE)
> [1] 35.24  0.26 35.53  0.00  0.00
> > system.time(n4 <- countLines3("hcv.ap", 1), gcFirst=TRUE)
> [1] 36.10  0.32 36.43  0.00  0.00
> 
> There's almost no penalty (in time) in reading one line at a time.
> One do
> save quite a bit of memory, though.

Andy, 

I suspect that the conservation of time for reading one line at a time,
versus the larger chunks, is correlated to the use of disc caching and
"read ahead" functionality in the disk sub-system and the OS.

Thus, even though you are requesting one line to be read at a time in
your function, each physical read of the file by the disk sub-system is
in reality reading larger chunks of the file and storing that in cache
memory until needed or flushed by new data. 

So your function is taking advantage of higher speed memory to memory
transfers, versus disk to memory transfers, given the serial read nature
of the process.

As you point out however, system memory is conserved.

Best,

Marc



From kjetil at acelerate.com  Mon Dec  6 22:27:53 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 06 Dec 2004 17:27:53 -0400
Subject: [R] Text Mining with R
In-Reply-To: <41B45C42.5040405@web.de>
References: <200412021829.31979.daniele.medri@libero.it>	<20041203191557.043b3141.tobias.verbeke@telenet.be>	<21c05c7d04120312233eb46bf4@mail.gmail.com>	<33CF3546-4774-11D9-B49D-000D93AE2752@dssp.unil.ch>
	<41B45C42.5040405@web.de>
Message-ID: <41B4CED9.7020107@acelerate.com>

Christian Schulz wrote:

> hi,
>
> i'm interested in text-mining, too and so make a trial.
> In Linux (suse9.2) it works fine, but in windows i can't install
> the source despite off  installed  perl, tools ,htmlhelp etc..
>
> regards, christian
>
Just downloaded, and had no problems running  Rcmd INSTALL
on the source distribution. But you need ALL tools installed!

maybe you tried to install the "zip file for windows" on the web page,
which maybe is compiled with an old R

Kjetil


>
> Jean-Pierre Muller wrote:
>
>> Jose,
>>
>> Le 3 d??c. 04, ?? 21:23, Jose Quesada a ??crit :
>>
>>> Tobias,
>>>
>>> I just created a zip file from the tar, and used the "install from
>>> zip" option of the Rwin console.
>>>
>>> ttda is shown in the list of installed packages. However, when I try
>>> "load packages", or the equivaent "library(ttda)", I get:
>>>
>>> Error in library(ttda) : 'ttda' is not a valid package -- installed 
>>> < 2.0.0?
>>>
>>>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>>>
>>>
>>> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>>> Error in library(pkg, character.only = TRUE) :
>>>         'ttda' is not a valid package -- installed < 2.0.0?
>>>
>>> Do you know why?
>>>
>>
>> I have put a link to the zip version of ttda on 
>> http://wwwpeople.unil.ch/jean-pierre.mueller/
>> I have been able to load it (using "packages -> Install package(s) 
>> from local zip files..."),
>> and to make a few tests, but i have not tested it well on a windows 
>> machine with R 2.0.x.,
>> I will not have time in the next few days. But on R for Mac OS X Aqua 
>> (2.0.1), I have no
>> problems (R CMD CHECK ttda ->  OK)  & (Installing *source* package 
>> 'ttda' ... ->
>> package successfully installed ) .
>> HTH
>>
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From rob at fatkat.com  Mon Dec  6 23:10:05 2004
From: rob at fatkat.com (Rob Steele)
Date: Mon, 06 Dec 2004 17:10:05 -0500
Subject: [R] > 2GB memory?
Message-ID: <41B4D8BD.3020705@fatkat.com>

Can R (2.0.1) running under a 32 bit Linux use more than 2 GB of RAM?  I 
have a server with 8 GB (heh heh) but R seems to never use more than 2.  
The only documentation I've found is the man page and it's a little 
vague on memory issues.

Thanks!
Rob



From andy_liaw at merck.com  Mon Dec  6 23:26:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 6 Dec 2004 17:26:28 -0500
Subject: [R] > 2GB memory?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3E4@usrymx25.merck.com>

We have a dual Xeon box with 8GB ram, and an R process can use up to nearly
3GB of ram.

Andy

> From: Rob Steele
> 
> Can R (2.0.1) running under a 32 bit Linux use more than 2 GB 
> of RAM?  I 
> have a server with 8 GB (heh heh) but R seems to never use 
> more than 2.  
> The only documentation I've found is the man page and it's a little 
> vague on memory issues.
> 
> Thanks!
> Rob
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From brian_pfeng at yahoo.com  Mon Dec  6 23:44:33 2004
From: brian_pfeng at yahoo.com (Brian pfeng)
Date: Mon, 6 Dec 2004 16:44:33 -0600 (CST)
Subject: [R] Harmonic regression in R
Message-ID: <20041206224433.97644.qmail@web54709.mail.yahoo.com>

I' need to smooth a serie univariate with harmonic
regression, but i do not know if it exists somepeople
know in R this type of regression, not in S-plus.

Thank's 

_________________________________________________________




From francoisromain at free.fr  Mon Dec  6 22:55:57 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Mon, 06 Dec 2004 22:55:57 +0100
Subject: [R] plot color question
In-Reply-To: <20041206205826.19675.qmail@web53210.mail.yahoo.com>
References: <20041206205826.19675.qmail@web53210.mail.yahoo.com>
Message-ID: <41B4D56D.1050608@free.fr>

Hello

That would do the trick :

x <- rnorm(50)
y <- rnorm(50)
color <- rep("blue",50)
color[(x>0)&(y<0)] <-"red"
color[(x<0)&(y>0)] <-"green"
color[(x<0)&(y<0)] <-"yellow"
plot(x,y,col=color,pch=19)
abline(h=0) #just to check
abline(v=0) #just to check


Romain.

Derek Margetts a ??crit :

>Thanks again Andy and Uwe for you help on my previous
>post of ploting lm coef and means.  With your
>direction, I was able to expand and generalize the
>function to meet my requirements. 
>
>Follow up question: Is it possible to make the points
>different colors depending on which quaderant they
>fall into?
>
>The data frame contains two variables (x and y)
>Quad 1 = x>mean(x) & y>mean(y) would be blue
>Quad 2 = x>mean(x) & y<mean(y) would be red
>Quad 3 = x<mean(x) & y>mean(y) ....
>Quad 4 = x<mean(x) & y<mean(y) ....
>
>Derek
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann??e
Institut de Statistique de l'Universit?? de Paris (ISUP)
Fili??re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From brian_pfeng at yahoo.com  Mon Dec  6 23:53:41 2004
From: brian_pfeng at yahoo.com (Brian pfeng)
Date: Mon, 6 Dec 2004 16:53:41 -0600 (CST)
Subject: [R] Random Walk plus noise Model
Message-ID: <20041206225341.96186.qmail@web54707.mail.yahoo.com>

Hi, 

I need help about the example that it appears in the
book Time series the Brokwell and Davis, specificaly
the random walk plus noise in the chapter 8. I need
simulate t simulate something similar.

thanks

_________________________________________________________




From ok at cs.otago.ac.nz  Tue Dec  7 00:15:13 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 7 Dec 2004 12:15:13 +1300 (NZDT)
Subject: [R] How about a mascot for R?
Message-ID: <200412062315.iB6NFDAl020914@atlas.otago.ac.nz>

Thomas Yee <t.yee at auckland.ac.nz> wrote:
	ps. Ross has Maori origins, so a native NZ animal is a better idea
	than usual.

The native animals of New Zealand include no mammals except for
- marine mammals in the coastal waters, such as the Hector's dolphin
- a few species of bat which got blown over from Australia.
Otherwise, there are birds, reptiles, insects, and a few oddballs.
The most notable oddballs would be Tuataras, famous for their third eye.
We have some onychophorans, of which it has been said that
"Contemporary Onychophorans are able to predate organisms several times
larger than themselves" (take _that_, SAS!)....
Wetas are quite interesting; they are basically grasshoppers some of which
played the ecological role of (and are about the same size as) mice.

I suspect that only birds have the "cuddly" appeal required of a mascot.
Perhaps it's worth pointing out that Kiwis are a kind of Ratite.



From p.dalgaard at biostat.ku.dk  Tue Dec  7 00:18:39 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Dec 2004 00:18:39 +0100
Subject: [R] matrix of 1,0's to a data.frame of factors
In-Reply-To: <012c01c4dba3$578dcec0$0540210a@www.domain>
References: <1102289205.4304.13.camel@localhost.localdomain>
	<012c01c4dba3$578dcec0$0540210a@www.domain>
Message-ID: <x2653ff228.fsf@biostat.ku.dk>

"Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be> writes:

> Hi Rajarshi,
> 
> try this:
> 
> mat <- sample(0:1, 20, TRUE); dim(mat) <- c(5,4)
> mat[,1] <- 0; mat[,3] <- 1
> #########
> dat <- data.frame(mat)
> dat[] <- lapply(dat, function(x) factor(x, levels=c("0","1")))
> dat
> lapply(dat, levels)

A small style/speed  point: lapply(dat, factor, levels=0:1) will do.
  

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From srivas at ksu.edu  Tue Dec  7 00:59:51 2004
From: srivas at ksu.edu (Sivakumar Mohandass)
Date: Mon, 6 Dec 2004 17:59:51 -0600
Subject: [R] Blank eps output files
Message-ID: <200412062356.iB6NuqO7018111@mail-h12-01.cc.ksu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041206/f64eaaa8/attachment.pl

From p.dalgaard at biostat.ku.dk  Tue Dec  7 01:05:15 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Dec 2004 01:05:15 +0100
Subject: [R] Blank eps output files
In-Reply-To: <200412062356.iB6NuqO7018111@mail-h12-01.cc.ksu.edu>
References: <200412062356.iB6NuqO7018111@mail-h12-01.cc.ksu.edu>
Message-ID: <x21xe3ezwk.fsf@biostat.ku.dk>

"Sivakumar Mohandass" <srivas at ksu.edu> writes:

> Dear all,
> 
> The following commands results in a blank graph file,
> 
> postscript(file = "C:/Temp/Fig1.eps", height=4.0, width=4.0, 
> horizontal = FALSE, onefile = FALSE, paper = "special")
> 
> x11(height=3,width=3)
> par(mar=.1+c(4.5,4.5,0,0))
> 
> x <- c(10,20,30)
> y <- c(5, 7, 9)
> plot (x,y)
> dev.off()
> 
> The codes function normally without any error in the command window.
> However, the resulting files (I tried .eps, .emf and .jpeg) are all too
> small in size (< 5 mb). The .jpeg file which one can normally preview in a
> Windows OS, says it's not available for preview when double clicked upon. 
> 
> I am using R version 2.0.0. Are there any changes to the coding in this
> version? Does anyone know how to fix this?

Get rid of the x11() command. Do you an explanation why, or was it
left in accidentally?


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sundar.dorai-raj at pdf.com  Tue Dec  7 01:15:13 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 06 Dec 2004 16:15:13 -0800
Subject: [R] Blank eps output files
In-Reply-To: <200412062356.iB6NuqO7018111@mail-h12-01.cc.ksu.edu>
References: <200412062356.iB6NuqO7018111@mail-h12-01.cc.ksu.edu>
Message-ID: <41B4F611.4080107@pdf.com>



Sivakumar Mohandass wrote:

> Dear all,
> 
> The following commands results in a blank graph file,
> 
> postscript(file = "C:/Temp/Fig1.eps", height=4.0, width=4.0, 
> horizontal = FALSE, onefile = FALSE, paper = "special")
> 
> x11(height=3,width=3)
> par(mar=.1+c(4.5,4.5,0,0))
> 
> x <- c(10,20,30)
> y <- c(5, 7, 9)
> plot (x,y)
> dev.off()
> 

Your code above opens 2 devices (1=postscript, 2=x11) and only writes to 
the second. Thus nothing is ever written to nor do you ever close the 
first device. I think what you want is:

postscript(file = "C:/Temp/Fig1.eps", height=4.0, width=4.0,
            horizontal = FALSE, onefile = FALSE, paper = "special")
par(mar=.1+c(4.5,4.5,0,0))
x <- c(10,20,30)
y <- c(5, 7, 9)
plot (x,y)
dev.off()

The fact that you never closed the first device explains why you 
couldn't view it outside of R.

--sundar


> The codes function normally without any error in the command window.
> However, the resulting files (I tried .eps, .emf and .jpeg) are all too
> small in size (< 5 mb). The .jpeg file which one can normally preview in a
> Windows OS, says it's not available for preview when double clicked upon. 
> 
> I am using R version 2.0.0. Are there any changes to the coding in this
> version? Does anyone know how to fix this?
> 
> Thank you in advance,
> Shiva



From mpavlick at mail.nih.gov  Tue Dec  7 01:17:10 2004
From: mpavlick at mail.nih.gov (Mark Pavlick)
Date: Mon, 6 Dec 2004 19:17:10 -0500
Subject: [R] statistics packages for R?
Message-ID: <p06200701bddaa62a94bd@[156.40.62.30]>

						12/6
List members:

	I'm obtained the latest version of R for the Mac. I'm running 
OS 10.3.6 on a dual-processor G4.
	I'd appreciate any information about statistics training 
components that can be added to R.
I'm a novice at statistics. Are there, e.g., tutorials on statistics, 
including problem sets, that I can add to R?
	Thanks in advance for any information.

						Sincerely,

						Mark Pavlick
--



From hodgess at gator.uhd.edu  Tue Dec  7 01:42:42 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 6 Dec 2004 18:42:42 -0600
Subject: [R] akima
Message-ID: <200412070042.iB70ggI17074@gator.dt.uh.edu>

There is a contributed package called "Akima" which
has the akima function.

R Version 2.0.1
Windows

The akima is from October 2004, so it is up to date.

Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu



From hmaughan at u.arizona.edu  Tue Dec  7 02:07:04 2004
From: hmaughan at u.arizona.edu (Heather Maughan)
Date: Mon, 06 Dec 2004 18:07:04 -0700
Subject: [R] Importing module into R library
Message-ID: <BDDA5048.106%hmaughan@u.arizona.edu>

Hello,

I am basically familiar with R and am trying to import a module that someone
else has written.  I know that it must go into the R library but even after
I place the file there R doesn't recognize it.  The module is maanova,
available from the Churchill lab group for analysis of microarray data, if
anyone is familiar with it.  Any ideas/help?

Thanks,
Heather
--



From andy_liaw at merck.com  Tue Dec  7 02:18:07 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 6 Dec 2004 20:18:07 -0500
Subject: [R] Importing module into R library
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3E7@usrymx25.merck.com>

The instruction in
http://www.jax.org/staff/churchill/labsite/software/anova/rmaanova/maanova.p
df looks quite complete, so what is the exact problem you run into, if you
followed that?

BTW, it is stated that it was written for R-1.5.1, which is rather old...
If you are using R-2.0.0 or later, and are using R on Windows, you will need
to install it from source.  The R for Windows FAQ has pointers for how to do
that.

Andy

> From: Heather Maughan
> 
> Hello,
> 
> I am basically familiar with R and am trying to import a 
> module that someone
> else has written.  I know that it must go into the R library 
> but even after
> I place the file there R doesn't recognize it.  The module is maanova,
> available from the Churchill lab group for analysis of 
> microarray data, if
> anyone is familiar with it.  Any ideas/help?
> 
> Thanks,
> Heather
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Tue Dec  7 02:22:06 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 6 Dec 2004 20:22:06 -0500
Subject: [R] statistics packages for R?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3E8@usrymx25.merck.com>

Get a copy of `Introductory Statistics with R', which has a support package
`ISwR' on CRAN.  The book is well worth twice its price.

Andy

> From: Mark Pavlick
> 						12/6
> List members:
> 
> 	I'm obtained the latest version of R for the Mac. I'm running 
> OS 10.3.6 on a dual-processor G4.
> 	I'd appreciate any information about statistics training 
> components that can be added to R.
> I'm a novice at statistics. Are there, e.g., tutorials on statistics, 
> including problem sets, that I can add to R?
> 	Thanks in advance for any information.
> 
> 						Sincerely,
> 
> 						Mark Pavlick
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ok at cs.otago.ac.nz  Tue Dec  7 02:35:22 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 7 Dec 2004 14:35:22 +1300 (NZDT)
Subject: [R] Which FM should beginners R?  A suggestion.
Message-ID: <200412070135.iB71ZMxT020609@atlas.otago.ac.nz>

I've made one important change to
 http://www.cs.otago.ac.nz/staffpriv/ok/R-help.txt
and a couple of minor ones (notably qqplot).  The important one is
that I've split "elementary statistics" into "elementary descriptive
statistics and tests" and "model fitting", with rather more in the
model fitting category (but still not glm).  Should 'glm' be there?

Possibly the biggest gap is that there is nothing about time series.



From ok at cs.otago.ac.nz  Tue Dec  7 03:00:55 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 7 Dec 2004 15:00:55 +1300 (NZDT)
Subject: [R] how to get how many lines there are in a file.
Message-ID: <200412070200.iB720t98023150@atlas.otago.ac.nz>

Hu Chen asked
	> If I wanna get the total number of lines in a big file without reading
	> the file's content into R as matrix or data frame, any methods or
	> functions?
	
_Something_ must read it, but it doesn't have to be R.
On a UNIX system, you can simply do

    number.of.lines <- as.numeric(system(paste("wc -l <", file.name), TRUE))

Suppopse file.name is "massive.csv".
Then paste("wc -l <", file.name) is "wc -l < massive.csv", which is a
UNIX command to write the number of lines in massive.csv to stdout,
and system(cmd, TRUE) executes the UNIX command and returns everything
it writes to stdout as an R character vector, one element per line of
output.  In this case, there's one line of output, so one element.
Don't forget the TRUE; without it the command's standard output is not
captured, just displayed.
Finally, as.numeric turns that string into a number.

For example, on my machine,
    > as.numeric(system("wc -l <$HOME/.cshrc", TRUE))
    [1] 32

This will work in MacOS X, and you can get 'wc' for Windows, so it can be
made to work there too.

If the file is large, this is likely to be a lot faster than reading it in R.  

But the obvious question is "what happens next"?  If you want to decide
whether the amount of data is too big, then
    - false positives:  data files may contain comments, which will be
      counted by wc but don't affect the amount of memory you need
    - false negatives:  the amount of memory you need depends on the
      number (and type) of columns as well as the number of lines,
      just counting the lines may leave you thinking there is room when
      there isn't.



From dongyuan.xu at huiway.com  Tue Dec  7 04:39:30 2004
From: dongyuan.xu at huiway.com (xudongyuan)
Date: Tue, 7 Dec 2004 11:39:30 +0800
Subject: [R] how R parses expression?
Message-ID: <PHEAIHAGJCLIMJMBJFLOEELCCAAA.dongyuan.xu@huiway.com>

Hi.All and R developers:
      Since I am a beginner on R,I have some questions when I studied the source code.I wonder if anyone have time to help me?
      My question is how the R expressions change to the c code. That is when I input an expression to the GUI or from a file, how R converts the expression to the parse tree in C code(maybe it is a C function, then what is it?), and then does the sequent processes.
		thanks 
															dongyuan xu



From klealambrou at hotmail.com  Tue Dec  7 04:52:49 2004
From: klealambrou at hotmail.com (klea lambrou)
Date: Tue, 07 Dec 2004 03:52:49 +0000
Subject: [R] ANOVA
Message-ID: <BAY16-F1027D4CC9D4506F4D7C92FA6B50@phx.gbl>



From Tom.Mulholland at dpi.wa.gov.au  Tue Dec  7 04:54:21 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 7 Dec 2004 11:54:21 +0800
Subject: [R] how R parses expression?
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C922@afhex01.dpi.wa.gov.au>

I'm not really sure exactly what you are getting at. Much of R's functionality is built using the R language itself. If you are looking to interface with other languages you might want to start with the "Writing R Extensions" manual which has a section on "The R API: entry points for C code."

If you are using the windows version this manual ia available from the help menu (assuming you installed the documentation.)

Tom Mulholland

R 2.0 4-Oct-2004
Windows XP




> -----Original Message-----
> From: xudongyuan [mailto:dongyuan.xu at huiway.com]
> Sent: Tuesday, 7 December 2004 11:40 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how R parses expression?
> Importance: High
> 
> 
> Hi.All and R developers:
>       Since I am a beginner on R,I have some questions when I 
> studied the source code.I wonder if anyone have time to help me?
>       My question is how the R expressions change to the c 
> code. That is when I input an expression to the GUI or from a 
> file, how R converts the expression to the parse tree in C 
> code(maybe it is a C function, then what is it?), and then 
> does the sequent processes.
> 		thanks 
> 								
> 							dongyuan xu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From murray at math.umass.edu  Tue Dec  7 06:18:59 2004
From: murray at math.umass.edu (Murray Eisenberg)
Date: Tue, 07 Dec 2004 00:18:59 -0500
Subject: [R] ess in Windows  (newbie Q)
Message-ID: <41B53D43.6060109@math.umass.edu>

I'm a rank beginner with R, and I'm stumped at how to get it to work 
with Xemacs under Windows XP. I'm afraid there are some VERY basic 
questions I have here.  Yes, I did read 
http://ess.r-project.org/Manual/readme.html.

Both R and Xemacs are installed and working.  The current ess files are 
installed in a subdirectory of my Xemacs directory.  I added the R bin 
directory to my Windows PATH environment variable.

But when I start Xemacs and then give command

   M-x-R

what I see in the mini-buffer is

   M-x-Rd-

and it's waiting for more input.

So what could be wrong?  ....

1. Where might I find or should I put the requisite Xemacs init.el, in 
which I'm supposed to enter a line of the form

   (load "/PATH/ess-site")

to point to the ess location?  I found no such file anywhere on my 
system, so I created one in each of the _two_ HOME directories I have -- 
one pointed to by the enviroment system variable HOME and the other 
pointed to by the user environment variable HOME (which seem to be 
different -- I don't recall whether that happened automatically when I 
installed Singular or whether I did it manually).

2. And what should that file actually be called?  The ess docs say it 
should be %HOME%/.xemacs/init.el for Xemacs, but they also say to put it 
in the .emacs (or _emacs) file or default.el or site-init.el.

I'm using "init.el".

3. Did I get the form of that line correct for init.el (or whatever it 
should actually be named)?  My file ess-site.el is in

     D:\WP\XEmacs\ess-5.2.3\lisp

so the line I actually put into default.el is:

   (load "/D:/WP/XEmacs/ess-5.2.3/lisp/ess-site")

4.  Anything else I should check or do to proceed?


-- 
Murray Eisenberg                     murray at math.umass.edu
Mathematics & Statistics Dept.
Lederle Graduate Research Tower      phone 413 549-1020 (H)
University of Massachusetts                413 545-2859 (W)
710 North Pleasant Street            fax   413 545-1801
Amherst, MA 01003-9305



From murray at math.umass.edu  Tue Dec  7 06:22:33 2004
From: murray at math.umass.edu (Murray Eisenberg)
Date: Tue, 07 Dec 2004 00:22:33 -0500
Subject: [R] main R Gui window size under Windows
Message-ID: <41B53E19.7090006@math.umass.edu>

How do I get the main RGui window to open in Windows XP so that it is 
NOT maximized?  I didn't see any preference for that.
-- 
Murray Eisenberg                     murray at math.umass.edu
Mathematics & Statistics Dept.
Lederle Graduate Research Tower      phone 413 549-1020 (H)
University of Massachusetts                413 545-2859 (W)
710 North Pleasant Street            fax   413 545-1801
Amherst, MA 01003-9305



From murdoch at stats.uwo.ca  Tue Dec  7 07:33:03 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 07 Dec 2004 01:33:03 -0500
Subject: [R] how R parses expression?
In-Reply-To: <PHEAIHAGJCLIMJMBJFLOEELCCAAA.dongyuan.xu@huiway.com>
References: <PHEAIHAGJCLIMJMBJFLOEELCCAAA.dongyuan.xu@huiway.com>
Message-ID: <3ijar0tk8tkvnopfosd07cfsmjmpeqjl3d@4ax.com>

On Tue, 7 Dec 2004 11:39:30 +0800, "xudongyuan"
<dongyuan.xu at huiway.com> wrote:

>Hi.All and R developers:
>      Since I am a beginner on R,I have some questions when I studied the source code.I wonder if anyone have time to help me?
>      My question is how the R expressions change to the c code. That is when I input an expression to the GUI or from a file, how R converts the expression to the parse tree in C code(maybe it is a C function, then what is it?), and then does the sequent processes.

R expressions aren't converted to C code.  They are parsed internally
using a parser written in yacc (or bison), which converts them into R
objects.  Other parts of the interpreter work with those.

Duncan Murdoch



From murdoch at stats.uwo.ca  Tue Dec  7 07:35:27 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 07 Dec 2004 01:35:27 -0500
Subject: [R] main R Gui window size under Windows
In-Reply-To: <41B53E19.7090006@math.umass.edu>
References: <41B53E19.7090006@math.umass.edu>
Message-ID: <umjar01q4n0e2nbslir29m3lvhjnbfeo5a@4ax.com>

On Tue, 07 Dec 2004 00:22:33 -0500, Murray Eisenberg
<murray at math.umass.edu> wrote:

>How do I get the main RGui window to open in Windows XP so that it is 
>NOT maximized?  I didn't see any preference for that.

If you use the --sdi option on the command line, you'll get the SDI
interface, without any main window at all.  If you're using the MDI
interface, there's no option to make it appear un-maximized.

Duncan Murdoch



From petzoldt at rcs.urz.tu-dresden.de  Tue Dec  7 07:55:49 2004
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Tue, 07 Dec 2004 07:55:49 +0100
Subject: [R] akima
In-Reply-To: <200412070042.iB70ggI17074@gator.dt.uh.edu>
References: <200412070042.iB70ggI17074@gator.dt.uh.edu>
Message-ID: <41B553F5.4090500@rcs.urz.tu-dresden.de>

Erin Hodgess wrote:
> There is a contributed package called "Akima" which
> has the akima function.
> 
> R Version 2.0.1
> Windows
> 
> The akima is from October 2004, so it is up to date.

Yes I know. But, unfortunately package akima contains a different 
algorithm meant for "bivariate interpolation and smooth surface 
fitting", but what I wanted was an interpolation algorithm for 
univariate functions similar to splines, but which is according to the 
author "closer to a manually drawn curve than those drawn by other 
mathematical methods". (AFAIK the only disadvantage is, that the second 
derivative is not continuous.)

The FORTRAN algorithm is available on the ACM portal and in several 
other online resources and I wonder, why it is not yet available in one 
of the numerous contributed packages. If this is not the case I should 
write an R interface to it, but for a single function it would be nice 
not to write a new package, but to include it somewhere.

Thomas P.



From ironmanda at yahoo.com  Tue Dec  7 08:21:19 2004
From: ironmanda at yahoo.com (David Alyea)
Date: Mon, 6 Dec 2004 23:21:19 -0800 (PST)
Subject: [R] Installation of R-2.0.1 failure
Message-ID: <20041207072119.54831.qmail@web60404.mail.yahoo.com>

I'm trying to install R on RedHat Enterprise 3.
I installed /usr/local/bin/f2c and then in the
same dir libf2c.a.  I ran configure ok, though
there were 7 warnings.  Then I ran make and it
errored out.  Can anyone tell what's wrong?

missing link(s):  installed.packages INSTALL INSTALL
install.packages data install.packages installed.packages
INSTALL REMOVE
  library.dynam                     text    html    latex  
example
     missing link(s):  SHLIB
  license                           text    html    latex
  list                              text    html    latex  
example
  list.files                        text    html    latex  
example
  load                              text    html    latex  
example
     missing link(s):  download.file download.file
  localeconv                        text    html    latex  
example
  locales                           text    html    latex  
example
  logical                           text    html    latex
  lower.tri                         text    html    latex  
example
  ls                                text    html    latex  
example
     missing link(s):  ls.str str apropos find methods
  make.names                        text    html    latex  
example
  make.unique                       text    html    latex  
example
  manglePackageName                 text    html    latex  
example
  mapply                            text    html    latex  
example
  margin.table                      text    html    latex  
example
  mat.or.vec                        text    html    latex  
example
  match                             text    html    latex  
example
  match.arg                         text    html    latex  
example
  match.call                        text    html    latex  
example
  match.fun                         text    html    latex  
example
  matmult                           text    html    latex  
example
  matrix                            text    html    latex  
example
  maxCol                            text    html    latex  
example
  mean                              text    html    latex  
example
     missing link(s):  weighted.mean
  memory.profile                    text    html    latex  
example
  merge                             text    html    latex  
example
  missing                           text    html    latex  
example
  mode                              text    html    latex  
example
  name                              text    html    latex  
example
  names                             text    html    latex  
example
  nargs                             text    html    latex  
example
  nchar                             text    html    latex  
example
     missing link(s):  strwidth
  nlevels                           text    html    latex  
example
  noquote                           text    html    latex  
example
     missing link(s):  methods
  notyet                            text    html    latex  
example
  nrow                              text    html    latex  
example
  ns-alt                            text    html    latex  
example
  ns-dblcolon                       text    html    latex  
example
  ns-hooks                          text    html    latex
  ns-internal                       text    html    latex
  ns-load                           text    html    latex
  ns-lowlev                         text    html    latex
  ns-reflect                        text    html    latex
  ns-topenv                         text    html    latex  
example
  numeric                           text    html    latex  
example
  octmode                           text    html    latex
  on.exit                           text    html    latex  
example
  options                           text    html    latex  
example
     missing link(s):  edit help help.start bug.report
contrasts aov lm postscript postscript printCoefmat ts
dump.frames X11 update.packages pkgDepends download.file
dataentry download.file X11
  order                             text    html    latex  
example
  outer                             text    html    latex  
example
  package_version                   text    html    latex  
example
     missing link(s):  compareVersion
  parse                             text    html    latex  
example
  paste                             text    html    latex  
example
  path.expand                       text    html    latex  
example
  pmatch                            text    html    latex  
example
  polyroot                          text    html    latex  
example
     missing link(s):  uniroot
  pos.to.env                        text    html    latex  
example
  pretty                            text    html    latex  
example
     missing link(s):  axTicks
  print                             text    html    latex  
example
     missing link(s):  methods
  print.dataframe                   text    html    latex
  print.default                     text    html    latex  
example
     missing link(s):  show
  print.matrix                      text    html    latex  
example
  proc.time                         text    html    latex  
example
  prod                              text    html    latex  
example
  prop.table                        text    html    latex  
example
  pushBack                          text    html    latex  
example
  putenv                            text    html    latex  
example
  qr                                text    html    latex  
example
     missing link(s):  lm.fit lsfit
  qraux                             text    html    latex  
example
     missing link(s):  lsfit
  quit                              text    html    latex  
example
  r2dtable                          text    html    latex  
example
  range                             text    html    latex  
example
  rank                              text    html    latex  
example
  raw                               text    html    latex  
example
  rawConversion                     text    html    latex  
example
  read.table                        text    html    latex
     missing link(s):  read.fwf
  readBin                           text    html    latex  
example
  readLines                         text    html    latex  
example
  readline                          text    html    latex  
example
  real                              text    html    latex
  reg.finalizer                     text    html    latex  
example
  regex                             text    html    latex
     missing link(s):  apropos browseEnv help.search
  rep                               text    html    latex  
example
  replace                           text    html    latex
  rev                               text    html    latex  
example
     missing link(s):  dendrogram
  rle                               text    html    latex  
example
  rm                                text    html    latex  
example
  round.POSIXt                      text    html    latex  
example
  row                               text    html    latex  
example
  row.names                         text    html    latex
  rowsum                            text    html    latex  
example
     missing link(s):  aggregate
  sQuote                            text    html    latex  
example
  sample                            text    html    latex  
example
  save                              text    html    latex  
example
     missing link(s):  data
  scale                             text    html    latex  
example
     missing link(s):  par
  scan                              text    html    latex  
example
  search                            text    html    latex  
example
  seek                              text    html    latex
  seq.Date                          text    html    latex  
example
  seq.POSIXt                        text    html    latex  
example
  seq                               text    html    latex  
example
  sequence                          text    html    latex  
example
  serialize                         text    html    latex  
example
  sets                              text    html    latex  
example
  shQuote                           text    html    latex  
example
  showConnections                   text    html    latex  
example
  sign                              text    html    latex  
example
  sink                              text    html    latex  
example
     missing link(s):  capture.output
  slice.index                       text    html    latex  
example
  slotOp                            text    html    latex
  socketSelect                      text    html    latex  
example
  solve                             text    html    latex  
example
  sort                              text    html    latex  
example
  source                            text    html    latex
     missing link(s):  demo
  split                             text    html    latex  
example
  sprintf                           text    html    latex  
example
  stack                             text    html    latex  
example
     missing link(s):  lm reshape
  standardGeneric                   text    html    latex
  stop                              text    html    latex  
example
  stopifnot                         text    html    latex  
example
  strptime                          text    html    latex  
example
  strsplit                          text    html    latex  
example
  structure                         text    html    latex  
example
  strwrap                           text    html    latex  
example
  subset                            text    html    latex  
example
  substitute                        text    html    latex  
example
  substr                            text    html    latex  
example
  sum                               text    html    latex
  summary                           text    html    latex  
example
     missing link(s):  methods lm glm anova summary.glm
summary.lm
  svd                               text    html    latex  
example
  sweep                             text    html    latex  
example
  switch                            text    html    latex  
example
  sys.parent                        text    html    latex  
example
  sys.source                        text    html    latex
  system.file                       text    html    latex  
example
  system.time                       text    html    latex  
example
     missing link(s):  time
  t                                 text    html    latex  
example
  table                             text    html    latex  
example
     missing link(s):  xtabs chisq.test xtabs ftable
  tabulate                          text    html    latex  
example
  tapply                            text    html    latex  
example
     missing link(s):  aggregate
  taskCallback                      text    html    latex  
example
  taskCallbackManager               text    html    latex  
example
  taskCallbackNames                 text    html    latex  
example
  tempfile                          text    html    latex  
example
  textconnections                   text    html    latex  
example
     missing link(s):  capture.output
  tilde                             text    html    latex
     missing link(s):  formula
  toString                          text    html    latex  
example
  trace                             text    html    latex  
example
     missing link(s):  recover edit recover edit setMethod
recover
  traceback                         text    html    latex  
example
  transform                         text    html    latex  
example
  try                               text    html    latex  
example
  type.convert                      text    html    latex
  typeof                            text    html    latex  
example
  unique                            text    html    latex  
example
  unlink                            text    html    latex
  unlist                            text    html    latex  
example
     missing link(s):  lm
  unname                            text    html    latex  
example
  userhooks                         text    html    latex  
example
     missing link(s):  plot.new persp
  vector                            text    html    latex  
example
  warning                           text    html    latex  
example
  warnings                          text    html    latex  
example
  weekday.POSIXt                    text    html    latex  
example
  which                             text    html    latex  
example
  which.min                         text    html    latex  
example
  with                              text    html    latex  
example
  write                             text    html    latex  
example
  write.table                       text    html    latex  
example
  writeLines                        text    html    latex
  zMachine                          text    html    latex  
example
  zMethods                          text    html    latex
     missing link(s):  methods methods
  zScript                           text    html    latex  
example
  zip.file.extract                  text    html    latex
  zpackages                         text    html    latex  
example
  Signals                           text    html    latex
  system                            text    html    latex  
example
make[5]: Leaving directory `/root/R-2.0.1/src/library'
make[4]: *** No rule to make target `../../bin/exec/R',
needed by `base-Ex.Rout'.  Stop.
make[4]: Leaving directory `/root/R-2.0.1/tests/Examples'
make[3]: *** [test-Examples-Base] Error 2
make[3]: Leaving directory `/root/R-2.0.1/tests/Examples'
make[2]: *** [test-Examples] Error 2
make[2]: Leaving directory `/root/R-2.0.1/tests'
make[1]: *** [test-all-basics] Error 1
make[1]: Leaving directory `/root/R-2.0.1/tests'
make: *** [check] Error 2

David

=====



From ligges at statistik.uni-dortmund.de  Tue Dec  7 08:42:16 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Dec 2004 08:42:16 +0100
Subject: [R] VR package not found for R version 1.9.1
In-Reply-To: <46h9r0pf672srkcit2v2i728akocto70sv@4ax.com>
References: <830D8D4719112B418ABBC3A0EBA9581272EC36@webmail.scitegic.com>
	<46h9r0pf672srkcit2v2i728akocto70sv@4ax.com>
Message-ID: <41B55ED8.7020600@statistik.uni-dortmund.de>

Duncan Murdoch wrote:

> On Mon, 6 Dec 2004 12:02:12 -0800, "Moises Hassan"
> <mhassan at scitegic.com> wrote :
> 
> 
>>I'm trying to install the VR package for version 1.9.1 but I'm getting
>>the following error message:
>>
>>
>>
>>
>>>install.packages("VR")
>>
>>trying URL `http://cran.r-project.org/bin/windows/contrib/1.9/PACKAGES'
>>
>>Content type `text/plain; charset=iso-8859-1' length 20716 bytes
>>
>>opened URL
>>
>>downloaded 20Kb
>>
>>
>>
>>trying URL
>>`http://cran.r-project.org/bin/windows/contrib/1.9/VR_7.2-11.zip'
>>
>>Error in download.file(url, destfile, method, mode = "wb") : 
>>
>>       cannot open URL
>>`http://cran.r-project.org/bin/windows/contrib/1.9/VR_7.2-11.zip'
>>
>>In addition: Warning message: 
>>
>>cannot open: HTTP status was `404 Not Found'  
>>
>>
>>
>>Any workarounds?
> 
> 
> The problem is that sometimes the automatic updater at CRAN deletes an
> old version, then fails to download the update. 
> 
> The workarounds are to wait a day or two, to try a different version
> of R (version 2.0.1 succeeds in getting VR now), or to build the
> package from source (which requires a number of tools to be installed;
> see R for Windows FAQ "3.1 Can I install packages into libraries in
> this version?" for details).


Well, this time *my* scripts are the culprit (CRAN has mirrored very well)!
The VR bundle should not appear in the PACKAGES file, because recent 
versions of VR do not work with old versions of R.
See http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html 
and find that the "last good" binary version for R-1.9.x can be 
downloaded manually.


Uwe



> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gb at stat.umu.se  Tue Dec  7 09:10:52 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Tue, 7 Dec 2004 09:10:52 +0100
Subject: [R] read.spss: unrecognized record type
Message-ID: <20041207081052.GA7429@tal.stat.umu.se>

When reading an spss file, I get the following message:

Warning message: 
../totmorH.sav: Unrecognized record type 7, subtype 13 encountered in system file. 

What does it mean? Do I have to worry? The result looks as expected. I do
not have (easy) access to SPSS, so I cannot check exactly that I got what I
should have.

Thanks,

G??ran

And, I'm using R-2.0.1 on Debian testing (from source).
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From Tom.Mulholland at dpi.wa.gov.au  Tue Dec  7 09:34:38 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 7 Dec 2004 16:34:38 +0800
Subject: [R] read.spss: unrecognized record type
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C924@afhex01.dpi.wa.gov.au>

This has come up before you might try
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/36694.html

The bottom line is that you either have to make sure that the SPSS datasets are in a format where no warnings come up or find a way to verify that no corruption is happening. As one of the posts mentions SPSS is not well documented, so it's a hazard that can't be easily solved.

Tom

> -----Original Message-----
> From: G??ran Brostr??m [mailto:gb at stat.umu.se]
> Sent: Tuesday, 7 December 2004 4:11 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] read.spss: unrecognized record type
> 
> 
> When reading an spss file, I get the following message:
> 
> Warning message: 
> ../totmorH.sav: Unrecognized record type 7, subtype 13 
> encountered in system file. 
> 
> What does it mean? Do I have to worry? The result looks as 
> expected. I do
> not have (easy) access to SPSS, so I cannot check exactly 
> that I got what I
> should have.
> 
> Thanks,
> 
> G??ran
> 
> And, I'm using R-2.0.1 on Debian testing (from source).
> -- 
>  G??ran Brostr??m                    tel: +46 90 786 5223
>  Department of Statistics          fax: +46 90 786 6614
>  Ume?? University                   http://www.stat.umu.se/egna/gb/
>  SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From wolfram at fischer-zim.ch  Tue Dec  7 09:47:13 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Tue, 7 Dec 2004 09:47:13 +0100
Subject: [R] how to test the existence of a name in a dataframe
Message-ID: <20041207084712.GA2867@s1x.local>

I wanted to test if there exists already a name (which is
incidentally a substring of another name) in a dataframe.
I did e.g.:

> data(swiss)
> names(swiss)
[1] "Fertility"        "Agriculture"      "Examination"      "Education"       
[5] "Catholic"         "Infant.Mortality"

> ! is.null(swiss$EduX)
[1] FALSE

> ! is.null(swiss$Edu)
[1] TRUE

I did not expect to get TRUE here because ``Edu'' does not exist
as name of ``swiss''.

I did finally:
> 'Edu' %in% names(swiss)
for which I got the expected FALSE.

My question: What is the recommended way to do such a test?

Thanks - Wolfram Fischer



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Dec  7 10:06:45 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 7 Dec 2004 10:06:45 +0100
Subject: [R] how to test the existence of a name in a dataframe
References: <20041207084712.GA2867@s1x.local>
Message-ID: <010401c4dc3c$1342a620$0540210a@www.domain>

Hi Wolfram,

this behaviour is due to partial matching. Observe that

data(swiss)
swiss$Ed
swiss$Edu
swiss$Educ

I think the best way to do it is with `%in%' or `match()', i.e.,

c("Ed", "Edu", "Educ", "Education") %in% names(swiss)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Wolfram Fischer" <wolfram at fischer-zim.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, December 07, 2004 9:47 AM
Subject: [R] how to test the existence of a name in a dataframe


>I wanted to test if there exists already a name (which is
> incidentally a substring of another name) in a dataframe.
> I did e.g.:
>
>> data(swiss)
>> names(swiss)
> [1] "Fertility"        "Agriculture"      "Examination" 
> "Education"
> [5] "Catholic"         "Infant.Mortality"
>
>> ! is.null(swiss$EduX)
> [1] FALSE
>
>> ! is.null(swiss$Edu)
> [1] TRUE
>
> I did not expect to get TRUE here because ``Edu'' does not exist
> as name of ``swiss''.
>
> I did finally:
>> 'Edu' %in% names(swiss)
> for which I got the expected FALSE.
>
> My question: What is the recommended way to do such a test?
>
> Thanks - Wolfram Fischer
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tuechler at gmx.at  Tue Dec  7 10:16:22 2004
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 07 Dec 2004 10:16:22 +0100
Subject: [R] Modyfing PATH in Windows Installer for R
In-Reply-To: <4iu8r093fhfccpipf1fpjksvuj5t9fsqso@4ax.com>
References: <200412061505.iB6F4x9M002768@outmx006.isp.belgacom.be>
	<41B46DD9.1030101@imperial.ac.uk>
	<200412061505.iB6F4x9M002768@outmx006.isp.belgacom.be>
Message-ID: <3.0.6.32.20041207101622.007d68f0@pop.gmx.net>

At 10:30 06.12.2004 -0500, Duncan Murdoch wrote:
>
>But we still have users using Win9x versions, which use a more
>DOS-like method of setting the path.  At some point we'll drop support
>for them, but I don't want to do it sooner than necessary.
>
>Duncan Murdoch
>
Thank you for supporting Win9x! I would not be happy to be forced by R to
upgrade to an otherwise unnecessary version of Windows.

Heinz T??chler



From hgoehlma at gmx.de  Tue Dec  7 10:19:08 2004
From: hgoehlma at gmx.de (=?ISO-8859-1?Q?Hinrich_G=F6hlmann?=)
Date: Tue, 07 Dec 2004 10:19:08 +0100
Subject: [R] Importing vector graphics into R
Message-ID: <41B5758C.6080707@gmx.de>

Dear R users,

I know of the possibility to import bitmaps via the nice pixmap library. 
    But if you later on create a PDF it is somewhat disappointing to 
have such graphics bitmapped. Is there a trick (via maps?) to import a 
vector graphic and have them plotted onto a graph? My searching attempts 
in the searchable r-help archive did not seem to result in anything 
useful...

Cheers,
hinrich   d8-)



From blindglobe at gmail.com  Tue Dec  7 11:24:40 2004
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 7 Dec 2004 11:24:40 +0100
Subject: [R] ess in Windows (newbie Q)
In-Reply-To: <41B53D43.6060109@math.umass.edu>
References: <41B53D43.6060109@math.umass.edu>
Message-ID: <1abe3fa9041207022417b402a@mail.gmail.com>

The basic problem is that ESS is not being loaded.  To find the init
file, pull down the "Help" menu item and I believe that there is an
entry "Edit Init File" (or similar -- I don't have access to (X)Emacs
right now).

Alternatively, after starting Emacs, open up:

~/.emacs

and put it there (it will select your home directory for you).

i.e. C-x C-f ~/.emacs
or

     M-x find-file <ret> ~/.emacs

Apologies for being pedantic, if that happens to be the case.

best,
-tony



On Tue, 07 Dec 2004 00:18:59 -0500, Murray Eisenberg
<murray at math.umass.edu> wrote:
> I'm a rank beginner with R, and I'm stumped at how to get it to work
> with Xemacs under Windows XP. I'm afraid there are some VERY basic
> questions I have here.  Yes, I did read
> http://ess.r-project.org/Manual/readme.html.
> 
> Both R and Xemacs are installed and working.  The current ess files are
> installed in a subdirectory of my Xemacs directory.  I added the R bin
> directory to my Windows PATH environment variable.
> 
> But when I start Xemacs and then give command
> 
>   M-x-R
> 
> what I see in the mini-buffer is
> 
>   M-x-Rd-
> 
> and it's waiting for more input.
> 
> So what could be wrong?  ....
> 
> 1. Where might I find or should I put the requisite Xemacs init.el, in
> which I'm supposed to enter a line of the form
> 
>   (load "/PATH/ess-site")
> 
> to point to the ess location?  I found no such file anywhere on my
> system, so I created one in each of the _two_ HOME directories I have --
> one pointed to by the enviroment system variable HOME and the other
> pointed to by the user environment variable HOME (which seem to be
> different -- I don't recall whether that happened automatically when I
> installed Singular or whether I did it manually).
> 
> 2. And what should that file actually be called?  The ess docs say it
> should be %HOME%/.xemacs/init.el for Xemacs, but they also say to put it
> in the .emacs (or _emacs) file or default.el or site-init.el.
> 
> I'm using "init.el".
> 
> 3. Did I get the form of that line correct for init.el (or whatever it
> should actually be named)?  My file ess-site.el is in
> 
>     D:\WP\XEmacs\ess-5.2.3\lisp
> 
> so the line I actually put into default.el is:
> 
>   (load "/D:/WP/XEmacs/ess-5.2.3/lisp/ess-site")
> 
> 4.  Anything else I should check or do to proceed?
> 
> --
> Murray Eisenberg                     murray at math.umass.edu
> Mathematics & Statistics Dept.
> Lederle Graduate Research Tower      phone 413 549-1020 (H)
> University of Massachusetts                413 545-2859 (W)
> 710 North Pleasant Street            fax   413 545-1801
> Amherst, MA 01003-9305
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 

best,
-tony

---
A.J. Rossini
blindglobe at gmail.com



From h.andersson at nioo.knaw.nl  Tue Dec  7 11:33:52 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Tue, 07 Dec 2004 11:33:52 +0100
Subject: [R] ess in Windows  (newbie Q)
In-Reply-To: <41B53D43.6060109@math.umass.edu>
References: <41B53D43.6060109@math.umass.edu>
Message-ID: <cp411e$fh9$1@sea.gmane.org>

For you reference, there exists a separate mailing list for ESS, which 
you can find on news.gmane.org just like the R list.

Murray Eisenberg wrote:
> I'm a rank beginner with R, and I'm stumped at how to get it to work 
> with Xemacs under Windows XP. I'm afraid there are some VERY basic 
> questions I have here.  Yes, I did read 
> http://ess.r-project.org/Manual/readme.html.
> 
> Both R and Xemacs are installed and working.  The current ess files are 
> installed in a subdirectory of my Xemacs directory.  I added the R bin 
> directory to my Windows PATH environment variable.
> 
> But when I start Xemacs and then give command
> 
>   M-x-R
> 
> what I see in the mini-buffer is
> 
>   M-x-Rd-
> 
> and it's waiting for more input.
> 
> So what could be wrong?  ....
> 
> 1. Where might I find or should I put the requisite Xemacs init.el, in 
> which I'm supposed to enter a line of the form
> 
>   (load "/PATH/ess-site")
> 
> to point to the ess location?  I found no such file anywhere on my 
> system, so I created one in each of the _two_ HOME directories I have -- 
> one pointed to by the enviroment system variable HOME and the other 
> pointed to by the user environment variable HOME (which seem to be 
> different -- I don't recall whether that happened automatically when I 
> installed Singular or whether I did it manually).
> 
> 2. And what should that file actually be called?  The ess docs say it 
> should be %HOME%/.xemacs/init.el for Xemacs, but they also say to put it 
> in the .emacs (or _emacs) file or default.el or site-init.el.

I have an environment variable pointing to a folder where I put this my 
.emacs file, for Xemacs that would be in the folder .xemacs and init.el

Try Start/Run cmd.exe and then set HOME to get the location

> 
> I'm using "init.el".
> 
> 3. Did I get the form of that line correct for init.el (or whatever it 
> should actually be named)?  My file ess-site.el is in
> 
>     D:\WP\XEmacs\ess-5.2.3\lisp
> 
> so the line I actually put into default.el is:
> 
>   (load "/D:/WP/XEmacs/ess-5.2.3/lisp/ess-site")

I would try without the first slash

> 
> 4.  Anything else I should check or do to proceed?
> 
> 
See if this works, otherwise seek out the ESS mailing list!

Cheers, Henrik


---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From rgentlem at fhcrc.org  Tue Dec  7 16:13:26 2004
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Tue, 7 Dec 2004 07:13:26 -0800
Subject: [R] How about a mascot for R?
In-Reply-To: <200412062315.iB6NFDAl020914@atlas.otago.ac.nz>
References: <200412062315.iB6NFDAl020914@atlas.otago.ac.nz>
Message-ID: <8B6940B2-4862-11D9-9A40-000A95A9BA9A@fhcrc.org>


On Dec 6, 2004, at 3:15 PM, Richard A. O'Keefe wrote:

> Thomas Yee <t.yee at auckland.ac.nz> wrote:
> 	ps. Ross has Maori origins, so a native NZ animal is a better idea
> 	than usual.
>
> The native animals of New Zealand include no mammals except for
> - marine mammals in the coastal waters, such as the Hector's dolphin
> - a few species of bat which got blown over from Australia.
> Otherwise, there are birds, reptiles, insects, and a few oddballs.
> The most notable oddballs would be Tuataras, famous for their third  
> eye.
> We have some onychophorans, of which it has been said that
> "Contemporary Onychophorans are able to predate organisms several times
> larger than themselves" (take _that_, SAS!)....
> Wetas are quite interesting; they are basically grasshoppers some of  
> which
> played the ecological role of (and are about the same size as) mice.
>

Hi,
   We had a brief discussion and narrowed it to two, coincidentally  
among those named by Richard. The tuatara (there is some charm in  
associating a software product with what is essentially a slow moving  
dinosaur) and the weta (for those unaware, one might also describe it  
as a grasshopper designed by the Pentagon - these have some serious  
armor plating and a ferocious grip). Of course this is one of many  
views, kiwis, kokakos etc have lots of charm as well - and I think New  
Zealand might lay some claim to the giant squid. No need to stick with  
non-extinct things either - I suspect the dodo is up for grabs. And on  
the NZ front the Moa or the Haast eagle.

Ross and Robert

> I suspect that only birds have the "cuddly" appeal required of a  
> mascot.
> Perhaps it's worth pointing out that Kiwis are a kind of Ratite.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>
>
+----------------------------------------------------------------------- 
----+
| Robert Gentleman              phone: (206) 667-7700            |
| Head, Program in Computational Biology   fax:                          
     |
| Division of Public Health Sciences       office: M2-B865               
     |
| Fred Hutchinson Cancer Research Center   email: rgentlem at fhcrc.org     
     |
+----------------------------------------------------------------------- 
----+



From spencer.graves at pdf.com  Tue Dec  7 18:45:05 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 07 Dec 2004 09:45:05 -0800
Subject: [R] How about a mascot for R?
In-Reply-To: <8B6940B2-4862-11D9-9A40-000A95A9BA9A@fhcrc.org>
References: <200412062315.iB6NFDAl020914@atlas.otago.ac.nz>
	<8B6940B2-4862-11D9-9A40-000A95A9BA9A@fhcrc.org>
Message-ID: <41B5EC21.90104@pdf.com>

      I prefer not to consider the implications of associating ourselves 
with something extinct.  Beyond that, I'm more into reptiles that 
insects.   On the other hand, I don't care much.  I'm happy with the 
status quo and would be happy with whatever others decide. 

      Best Wishes,
      spencer graves

Robert Gentleman wrote:

>
> On Dec 6, 2004, at 3:15 PM, Richard A. O'Keefe wrote:
>
>> Thomas Yee <t.yee at auckland.ac.nz> wrote:
>>     ps. Ross has Maori origins, so a native NZ animal is a better idea
>>     than usual.
>>
>> The native animals of New Zealand include no mammals except for
>> - marine mammals in the coastal waters, such as the Hector's dolphin
>> - a few species of bat which got blown over from Australia.
>> Otherwise, there are birds, reptiles, insects, and a few oddballs.
>> The most notable oddballs would be Tuataras, famous for their third  
>> eye.
>> We have some onychophorans, of which it has been said that
>> "Contemporary Onychophorans are able to predate organisms several times
>> larger than themselves" (take _that_, SAS!)....
>> Wetas are quite interesting; they are basically grasshoppers some of  
>> which
>> played the ecological role of (and are about the same size as) mice.
>>
>
> Hi,
>   We had a brief discussion and narrowed it to two, coincidentally  
> among those named by Richard. The tuatara (there is some charm in  
> associating a software product with what is essentially a slow moving  
> dinosaur) and the weta (for those unaware, one might also describe it  
> as a grasshopper designed by the Pentagon - these have some serious  
> armor plating and a ferocious grip). Of course this is one of many  
> views, kiwis, kokakos etc have lots of charm as well - and I think 
> New  Zealand might lay some claim to the giant squid. No need to stick 
> with  non-extinct things either - I suspect the dodo is up for grabs. 
> And on  the NZ front the Moa or the Haast eagle.
>
> Ross and Robert
>
>> I suspect that only birds have the "cuddly" appeal required of a  
>> mascot.
>> Perhaps it's worth pointing out that Kiwis are a kind of Ratite.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!  
>> http://www.R-project.org/posting-guide.html
>>
>>
> +----------------------------------------------------------------------- 
> ----+
> | Robert Gentleman              phone: (206) 667-7700            |
> | Head, Program in Computational Biology   
> fax:                              |
> | Division of Public Health Sciences       office: 
> M2-B865                   |
> | Fred Hutchinson Cancer Research Center   email: 
> rgentlem at fhcrc.org         |
> +----------------------------------------------------------------------- 
> ----+
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From anne.piotet at urbanet.ch  Wed Dec  8 08:06:07 2004
From: anne.piotet at urbanet.ch (Anne)
Date: Wed, 8 Dec 2004 08:06:07 +0100
Subject: [R] ess in Windows  (newbie Q)
References: <41B53D43.6060109@math.umass.edu> <cp411e$fh9$1@sea.gmane.org>
Message-ID: <001101c4dcf4$652597f0$6c00a8c0@mtd4>

I used the instructions and init.el from John Fox
(http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/ESS/)  quite
successfully. Perhaps it can help you?
Cheers

Anne

----- Original Message ----- 
From: "Henrik Andersson" <h.andersson at nioo.knaw.nl>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, December 07, 2004 11:33 AM
Subject: Re: [R] ess in Windows (newbie Q)


> For you reference, there exists a separate mailing list for ESS, which
> you can find on news.gmane.org just like the R list.
>
> Murray Eisenberg wrote:
> > I'm a rank beginner with R, and I'm stumped at how to get it to work
> > with Xemacs under Windows XP. I'm afraid there are some VERY basic
> > questions I have here.  Yes, I did read
> > http://ess.r-project.org/Manual/readme.html.
> >
> > Both R and Xemacs are installed and working.  The current ess files are
> > installed in a subdirectory of my Xemacs directory.  I added the R bin
> > directory to my Windows PATH environment variable.
> >
> > But when I start Xemacs and then give command
> >
> >   M-x-R
> >
> > what I see in the mini-buffer is
> >
> >   M-x-Rd-
> >
> > and it's waiting for more input.
> >
> > So what could be wrong?  ....
> >
> > 1. Where might I find or should I put the requisite Xemacs init.el, in
> > which I'm supposed to enter a line of the form
> >
> >   (load "/PATH/ess-site")
> >
> > to point to the ess location?  I found no such file anywhere on my
> > system, so I created one in each of the _two_ HOME directories I have -- 
> > one pointed to by the enviroment system variable HOME and the other
> > pointed to by the user environment variable HOME (which seem to be
> > different -- I don't recall whether that happened automatically when I
> > installed Singular or whether I did it manually).
> >
> > 2. And what should that file actually be called?  The ess docs say it
> > should be %HOME%/.xemacs/init.el for Xemacs, but they also say to put it
> > in the .emacs (or _emacs) file or default.el or site-init.el.
>
> I have an environment variable pointing to a folder where I put this my
> .emacs file, for Xemacs that would be in the folder .xemacs and init.el
>
> Try Start/Run cmd.exe and then set HOME to get the location
>
> >
> > I'm using "init.el".
> >
> > 3. Did I get the form of that line correct for init.el (or whatever it
> > should actually be named)?  My file ess-site.el is in
> >
> >     D:\WP\XEmacs\ess-5.2.3\lisp
> >
> > so the line I actually put into default.el is:
> >
> >   (load "/D:/WP/XEmacs/ess-5.2.3/lisp/ess-site")
>
> I would try without the first slash
>
> >
> > 4.  Anything else I should check or do to proceed?
> >
> >
> See if this works, otherwise seek out the ESS mailing list!
>
> Cheers, Henrik
>
>
> ---------------------------------------------
> Henrik Andersson
> Netherlands Institute of Ecology -
> Centre for Estuarine and Marine Ecology
> P.O. Box 140
> 4400 AC Yerseke
> Phone: +31 113 577473
> h.andersson at nioo.knaw.nl
> http://www.nioo.knaw.nl/ppages/handersson
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From q.liu at unn.ac.uk  Wed Dec  8 11:36:48 2004
From: q.liu at unn.ac.uk (Qin Liu)
Date: Wed, 8 Dec 2004 10:36:48 -0000
Subject: [R] How to change x axes' range
Message-ID: <5B70E25BF4623948883B573159B0501044A5AF@clearwater.unn.ac.uk>

Hi, there:

When plot ann predicted results I need to indicate numbers of inputs for
each column. 

        V1       V2         V3          V4
1  86.2700  49.9380  30.7630  0.1327
2  89.5127  55.9707  33.7683  0.1186
3  91.1833  58.4670  34.5610  0.1134

matplot(t, pch = 1:4, type = "o", col = rainbow(ncol(t)),xlab = "No. of
inputs ",  ylab = "Mean of 6 Datasets", main = "xxxxxxxxxxx")

Instead of have typical x axes' range as "1, 2, 3, and 4" for each column, I
need to indicate "11 Vars, 10 Vars, 6Vars, and 4Vars" or "A, B, C, and D"

Does anybody know anything about it? I appreciate if you could help me out.

Thank you very much indeed.

Qin



From maechler at stat.math.ethz.ch  Wed Dec  8 12:01:34 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 8 Dec 2004 12:01:34 +0100
Subject: [R] mailing list time out ended now ...
Message-ID: <16822.57102.102678.705270@gargle.gargle.HOWL>

I hope you've all enjoyed the timeout and grabbed the chance to
be really productive instead of reading e-mails...  :-)

No, seriously, I do apologize very much for the practical
non-functioning of our mailing lists for about the last 17 hours
or so.  It was caused by an embarassingly simple mistake I
myself have introduced in one of our filters.  Unfortunately,
the consequence of the mistake had been very untypical and I
hadn't been alerted as in other cases
(and I *have* been productive doing other things...).

Many of the mails that you've posted during these hours are not
easy (if at all possible) to put back into the queue, so I do
ask all posters to repost their message if they haven't seen it
on R-help and it was posted approximately between 
Dec.7, 17am GMT and today
Dec.8, 10am GMT.   
--
The fact that this could happen without anyone sending an e-mail to
me privately (well, I got one to r-help-owner) seems to indicate that the
timeout has helped everyone to relax and use the extra time for
something more rewarding... ;-)

Martin Maechler, ETH Zurich



From ym at climpact.com  Wed Dec  8 12:10:19 2004
From: ym at climpact.com (Yves Magliulo)
Date: 08 Dec 2004 12:10:19 +0100
Subject: [R] How to change x axes' range
In-Reply-To: <5B70E25BF4623948883B573159B0501044A5AF@clearwater.unn.ac.uk>
References: <5B70E25BF4623948883B573159B0501044A5AF@clearwater.unn.ac.uk>
Message-ID: <1102504219.21787.22.camel@new-york.climpact.net>

matplot(.., axes=FALSE)

then custozie by yourself ayour axes 

->axis(1,at=1:4,label=c("11 Vars","10 Vars","6Vars","4Vars"))

?axis and see label option for more details

HTH, 

Yves MAGLIULO, PARIS

Le mer 08/12/2004 ?? 11:36, Qin Liu a ??crit :
> Hi, there:
> 
> When plot ann predicted results I need to indicate numbers of inputs for
> each column. 
> 
>         V1       V2         V3          V4
> 1  86.2700  49.9380  30.7630  0.1327
> 2  89.5127  55.9707  33.7683  0.1186
> 3  91.1833  58.4670  34.5610  0.1134
> 
> matplot(t, pch = 1:4, type = "o", col = rainbow(ncol(t)),xlab = "No. of
> inputs ",  ylab = "Mean of 6 Datasets", main = "xxxxxxxxxxx")
> 
> Instead of have typical x axes' range as "1, 2, 3, and 4" for each column, I
> need to indicate "11 Vars, 10 Vars, 6Vars, and 4Vars" or "A, B, C, and D"
> 
> Does anybody know anything about it? I appreciate if you could help me out.
> 
> Thank you very much indeed.
> 
> Qin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Dec  8 12:13:34 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 8 Dec 2004 06:13:34 -0500
Subject: [R] How to change x axes' range
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3F3@usrymx25.merck.com>

Add xaxt="n" in matplot(), then follow by axis(1, at=1:ncol(t),
labels=c("A", "B", ...)).

HTH,
Andy

> From: Qin Liu
> 
> Hi, there:
> 
> When plot ann predicted results I need to indicate numbers of 
> inputs for
> each column. 
> 
>         V1       V2         V3          V4
> 1  86.2700  49.9380  30.7630  0.1327
> 2  89.5127  55.9707  33.7683  0.1186
> 3  91.1833  58.4670  34.5610  0.1134
> 
> matplot(t, pch = 1:4, type = "o", col = rainbow(ncol(t)),xlab 
> = "No. of
> inputs ",  ylab = "Mean of 6 Datasets", main = "xxxxxxxxxxx")
> 
> Instead of have typical x axes' range as "1, 2, 3, and 4" for 
> each column, I
> need to indicate "11 Vars, 10 Vars, 6Vars, and 4Vars" or "A, 
> B, C, and D"
> 
> Does anybody know anything about it? I appreciate if you 
> could help me out.
> 
> Thank you very much indeed.
> 
> Qin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From h.andersson at nioo.knaw.nl  Wed Dec  8 12:18:02 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Wed, 08 Dec 2004 12:18:02 +0100
Subject: [R] How to change x axes' range
In-Reply-To: <5B70E25BF4623948883B573159B0501044A5AF@clearwater.unn.ac.uk>
References: <5B70E25BF4623948883B573159B0501044A5AF@clearwater.unn.ac.uk>
Message-ID: <cp6o08$od9$1@sea.gmane.org>

Try suppresing the axes with plot(...,xaxt='n')

and adding a custom axis with the command

axis(1,labels=LETTERS[1:5],at=1:5)

see ?axis for more info


Cheers, Henrik

Qin Liu wrote:
> Hi, there:
> 
> When plot ann predicted results I need to indicate numbers of inputs for
> each column. 
> 
>         V1       V2         V3          V4
> 1  86.2700  49.9380  30.7630  0.1327
> 2  89.5127  55.9707  33.7683  0.1186
> 3  91.1833  58.4670  34.5610  0.1134
> 
> matplot(t, pch = 1:4, type = "o", col = rainbow(ncol(t)),xlab = "No. of
> inputs ",  ylab = "Mean of 6 Datasets", main = "xxxxxxxxxxx")
> 
> Instead of have typical x axes' range as "1, 2, 3, and 4" for each column, I
> need to indicate "11 Vars, 10 Vars, 6Vars, and 4Vars" or "A, B, C, and D"
> 
> Does anybody know anything about it? I appreciate if you could help me out.
> 
> Thank you very much indeed.
> 
> Qin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From sam.kemp2 at ntlworld.com  Wed Dec  8 12:28:35 2004
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Wed, 08 Dec 2004 11:28:35 +0000
Subject: [R] memory problem
Message-ID: <41B6E563.8080604@ntlworld.com>

Hi,

I am trying to run a very computationally expensive procedure in 
R-2.0.0. and the process always gets killed after approx 8 minutes. This 
procedure calls some of my own C++ code - in case it was this code 
causing a memory leak I unload and then reload the .so file every time, 
however I still get the same problem. The procedure is run 16000 times 
and always calls the lm() function. My believe at the moment is that I 
need to allocate more memory for R - is this correct? I did try envoking 
R from the command line using R --min-vsize=1000M, however still no luck.

I have googled around and looked at the help files, but I am still 
confused about how to fix this problem.

Any insight would be greatly appeciated.

Kind Regards,

Sam.



From ym at climpact.com  Wed Dec  8 12:50:10 2004
From: ym at climpact.com (Yves Magliulo)
Date: 08 Dec 2004 12:50:10 +0100
Subject: [R] install bug with specific JPEG library by exporting CPPFLAGS
	variable
Message-ID: <1102506609.21787.32.camel@new-york.climpact.net>

Hi there,
I think I have found a small problem in the
"/my/path/R-2.0.1/src/modules/X11/MakeFile" generation.
During the configure step, I have specified a specific JPEG library by
exporting CPPFLAGS variable.
 
All compilation works well for individual files in the src/modules/X11/
directory, but when linking, the -ljpeg option doesn't work.
I obtain the following message (in french sorry):
 
------------------------------------------------------------------------------
make[4]: Entre dans le repertoire `/mnt/softs/R/R-2.0.1/src/modules/X11'
gcc -shared -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo
rotated.lo rbitmap.lo  -lSM -lICE -L/usr/X11R6/lib -lX11  -ljpeg
-lpng -lz  -lreadline -ldl -lncurses -lm
/usr/bin/ld: ne peut trouver -ljpeg
collect2: ld a retourne 1 code d'?tat d'ex?cution
make[4]: *** [R_X11.so] Erreur 1
make[4]: Quitte le repertoire `/mnt/softs/R/R-2.0.1/src/modules/X11'
make[3]: *** [R] Erreur 2
make[3]: Quitte le repertoire `/mnt/softs/R/R-2.0.1/src/modules/X11'
make[2]: *** [R] Erreur 1
make[2]: Quitte le repertoire `/mnt/softs/R/R-2.0.1/src/modules'
make[1]: *** [R] Erreur 1
make[1]: Quitte le repertoire `/mnt/softs/R/R-2.0.1/src'
make: *** [R] Erreur 1
------------------------------------------------------------------------------
 
This means that the "-L/my/jpeg/library/path" option is not added for
linking.
I re-runned the linking command with my option :
 
gcc -shared -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo
rotated.lo rbitmap.lo  -lSM -lICE -L/usr/X11R6/lib -lX11
-L/my/jpeg/library/path -ljpeg -lpng -lz  -lreadline -ldl -lncurses -lm
 
and the launched the "make" command again and it worked fine.
 
Hope it helps.



From internautem at laposte.net  Wed Dec  8 13:07:50 2004
From: internautem at laposte.net (internautem)
Date: Wed,  8 Dec 2004 13:07:50 +0100
Subject: [R] what about a mascot ?
Message-ID: <I8EKD2$406CD4FE7DDCDCD2CAA08DD0EF17AE77@laposte.net>

My friend Veslot proposed me the Raven ! 
http://www.teteamodeler.com/allopass/images/corbeau.jpg 

As a matter of fact the intelligence of this bird is
comparable to one of a monkey, although its brain is close to
a reptile brain. R is quite the same : small, compact, but so
clever. 

This is a canadian website explaining (in french) the R-aven
intelligence

http://radio-canada.ca/actualite/decouverte/reportages/2000/corvides.html


What do you think about it ? 




From Roger.Bivand at nhh.no  Wed Dec  8 13:08:26 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 8 Dec 2004 13:08:26 +0100 (CET)
Subject: [R] Importing vector graphics into R
Message-ID: <Pine.LNX.4.44.0412081306190.17157-100000@reclus.nhh.no>

On Tue, 7 Dec 2004, Hinrich G??hlmann wrote:

> Dear R users,
> 
> I know of the possibility to import bitmaps via the nice pixmap library. 
>     But if you later on create a PDF it is somewhat disappointing to 
> have such graphics bitmapped. Is there a trick (via maps?) to import a 
> vector graphic and have them plotted onto a graph? My searching attempts 
> in the searchable r-help archive did not seem to result in anything 
> useful...

No, nothing obvious. If you have an Xfig file - or convert to one from PS,
you may be able to extract the lines with their attributes by hand (the
file is just text, so you can "see" the vector graphics), and write an R
function to plot them (rescaled) onto the device if you need a single
graphical element many times. Otherwise, perhaps edit the graphics file
after R has completed its work. None of the vector map formats is easy to
use for this kind of trick, especially because you probably need
attributes on the lines (thickness, colour).

> 
> Cheers,
> hinrich   d8-)
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From Rau at demogr.mpg.de  Wed Dec  8 13:10:06 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Wed, 8 Dec 2004 13:10:06 +0100
Subject: [R] ess in Windows  (newbie Q)
Message-ID: <71E4EAD7CBE44248908143EE3C6F835317A104@poseidon.demogr.mpg.de>

Reposting... 

-----Original Message-----
From: Rau, Roland 
Sent: Tuesday, December 07, 2004 2:39 PM
To: 'murray at math.umass.edu'; r-help at stat.math.ethz.ch
Subject: RE: [R] ess in Windows (newbie Q)

Hi,

 -----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Murray Eisenberg

4.  Anything else I should check or do to proceed?

I think you have to change something in the file ess-site.el
Depending on your version of ESS, the line number may vary.
In older versions it was at about line 250.
In my current version (from 28 July 2004) it is in line 326.
You should see something like:
;;(setq-default inferior-S+5-program-name "Splus5")
;;(setq-default inferior-S+6-program-name "Splus6")
;;(setq-default inferior-R-program-name "R")	  ; unix systems
;;(setq-default inferior-R-program-name "Rterm")  ; msdos systems
the last line should be uncommented (by removing the two ";") and the
correct path should be specified.
In my case it looks like this:

(setq-default inferior-R-program-name
"U:\\R\\rw2000beta\\bin\\Rterm.exe")


Is this of any help for you?

Best,
Roland


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From Rau at demogr.mpg.de  Wed Dec  8 13:10:28 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Wed, 8 Dec 2004 13:10:28 +0100
Subject: [R] How about a mascot for R?
Message-ID: <71E4EAD7CBE44248908143EE3C6F835317A105@poseidon.demogr.mpg.de>

reposting... 

-----Original Message-----
From: Rau, Roland 
Sent: Tuesday, December 07, 2004 2:57 PM
To: r-help at stat.math.ethz.ch
Subject: RE: [R] How about a mascot for R?

Dear all,

browsing through the suggestions, I have the impression that the general
direction is towards an animal from New Zealand (I guess because of the
roots of R). But since the R Foundation is now located in Vienna,
Austria. What about a typical Austrian animal? Is there one? Maybe a
"Wolpertinger". A Wolpertinger is a fantasy animal which is a rabbit
with the antlers known from deer and some wings from a bird. In addition
to the "Austrian headquarters", another reason for such an animal which
does not exist in reality (or does it???) is that coding something with
R is sometimes so easy that it appears to be almost unreal.

What do you think about this idea?

Best,
Roland

P.S.
A picture of a Wolpertinger can be found at the bottom of the page:
http://www.einsamer-schuetze.com/krypto/fabelwesen/wolpertinger/wolpi.ht
ml




+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Dec  8 14:10:39 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Wed, 8 Dec 2004 13:10:39 +0000 (GMT)
Subject: [R] install bug with specific JPEG library by exporting CPPFLAGS
	variable
In-Reply-To: <1102506609.21787.32.camel@new-york.climpact.net>
Message-ID: <Pine.GSO.4.31.0412081306200.16761-100000@markov.stats>

On 8 Dec 2004, Yves Magliulo wrote:

> Hi there,
> I think I have found a small problem in the
> "/my/path/R-2.0.1/src/modules/X11/MakeFile" generation.
> During the configure step, I have specified a specific JPEG library by
> exporting CPPFLAGS variable.

That only specifies a set of headers, not the library.
It is up to you to ensure that the library paths you need are also
specified, e.g. in LIBS.

To be definite, this is not a bug/problem in R.

> All compilation works well for individual files in the src/modules/X11/
> directory, but when linking, the -ljpeg option doesn't work.
> I obtain the following message (in french sorry):
>
> ------------------------------------------------------------------------------
> make[4]: Entre dans le repertoire `/mnt/softs/R/R-2.0.1/src/modules/X11'
> gcc -shared -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo
> rotated.lo rbitmap.lo  -lSM -lICE -L/usr/X11R6/lib -lX11  -ljpeg
> -lpng -lz  -lreadline -ldl -lncurses -lm
> /usr/bin/ld: ne peut trouver -ljpeg
> collect2: ld a retourne 1 code d'?tat d'ex?cution

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Dec  8 14:23:05 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Wed, 8 Dec 2004 13:23:05 +0000 (GMT)
Subject: [R] install bug with specific JPEG library by exporting CPPFLAGS
	variable
In-Reply-To: <Pine.GSO.4.31.0412081306200.16761-100000@markov.stats>
Message-ID: <Pine.GSO.4.31.0412081320390.19295-100000@markov.stats>

On Wed, 8 Dec 2004, Brian D Ripley wrote:

> On 8 Dec 2004, Yves Magliulo wrote:
>
> > Hi there,
> > I think I have found a small problem in the
> > "/my/path/R-2.0.1/src/modules/X11/MakeFile" generation.
> > During the configure step, I have specified a specific JPEG library by
> > exporting CPPFLAGS variable.
>
> That only specifies a set of headers, not the library.
> It is up to you to ensure that the library paths you need are also
> specified, e.g. in LIBS.

It's best to set these in LDFLAGS: see config.site and the documentation
in R-admin.html.

> To be definite, this is not a bug/problem in R.

especially as how to do it is documented in the appropriate manual.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fciclone at bol.com.br  Wed Dec  8 14:35:14 2004
From: fciclone at bol.com.br (Alex)
Date: Wed,  8 Dec 2004 11:35:14 -0200
Subject: [R] GLMM
Message-ID: <I8EOEQ$16BE5D5CFE2C8A064D18D3AB94E9E83E@bol.com.br>

Hi all,


Could someone please tell me if we have to group data in the units with a command such "factor()" or "groupedData()" before using the functions glmmPQL or GLMM. I didn't do that and at first my results seem OK, but I'd like to solve this doubt.

Thanks in advance,

Alex

 
__________________________________________________________________________
Acabe com aquelas janelinhas que pulam na sua tela.
AntiPop-up UOL - ?? gr??tis!
http://antipopup.uol.com.br/



From kjetil at acelerate.com  Wed Dec  8 14:33:22 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 08 Dec 2004 09:33:22 -0400
Subject: [R] Strange error from R CMD INSTALL
Message-ID: <41B702A2.4070307@acelerate.com>

I am trying to install a local package and get this unexpected
error:

---------- Making package UMSA ------------
  adding build stamp to DESCRIPTION
  installing R files
  installing data files
  installing man source files
  installing indices
Error: couldn't find function "na.omit"
Execution halted

na.omit of course is in package stats, and that is listed in the
Depends field in the DESCRIPTION file.

What is happening?

(Windows XP with all development tools installed)

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From Ted.Harding at nessie.mcc.ac.uk  Wed Dec  8 14:53:12 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 08 Dec 2004 13:53:12 -0000 (GMT)
Subject: [R] Importing vector graphics into R
In-Reply-To: <Pine.LNX.4.44.0412081306190.17157-100000@reclus.nhh.no>
Message-ID: <XFMail.041208134712.Ted.Harding@nessie.mcc.ac.uk>

On 08-Dec-04 Roger Bivand wrote:
> On Tue, 7 Dec 2004, Hinrich G??hlmann wrote:
> 
>> Dear R users,
>> 
>> I know of the possibility to import bitmaps via the nice
>> pixmap library. 
>>     But if you later on create a PDF it is somewhat
>> disappointing to have such graphics bitmapped. Is there
>> a trick (via maps?) to import a vector graphic and have
>> them plotted onto a graph? My searching attempts in the
>> searchable r-help archive did not seem to result in anything 
>> useful...
> 
> No, nothing obvious. If you have an Xfig file - or convert to
> one from PS,

How does one do that? None of the tools I can find on my (Linux)
system seem to include the possibility of PS->Xfig (or any other
vector format either, except of course PDF).

> you may be able to extract the lines with their attributes by
> hand (the file is just text, so you can "see" the vector
> graphics), and write an R function to plot them (rescaled) onto
> the device if you need a single graphical element many times.
> Otherwise, perhaps edit the graphics file after R has completed
> its work. None of the vector map formats is easy to use for
> this kind of trick, especially because you probably need
> attributes on the lines (thickness, colour).

When I first saw Hinrich's post, I thought it was a hopeless
quest. Even if one had a PS file (rather than PDF), I think
it would only be feasible to write such a conversion if it
were guaranteed that the PS file used only "raw" PS language
in its simplest usage (i.e. one would be able to look out for,
and then interpret, the basic drawing commands like "moveto",
"rmoveto", "lineto", "rlineto", etc. with explicit numerical
coordinates). Unfortunately, many programs which output PS
have extended preambles in which all sorts of abbreviations
are defined to wrap up chunks of "raw" PS. So one would be
looking at writing a fully featured PS interpreter!

When starting from a PDF file, however, even though this is
in a sense "reminiscent" of PS (and may have been converted
from a PS file), nevertheless PDF is a harder format to interpret
because of its hierarchical "modular" construction (in effect
a "tree of objects"). So I would be even less optimistic about
coverting PDF to a non-PS vector format.

However, if I'm at all wrong about any of that I would be most
interested to be informatively corrected!

On the other hand, there is the possibility to convert a bitmap
to a PS file where lines and curves are drawn using vector
graphics (giving the advantage that the result is as smooth as
the resolution of the ultimate raster device allows, and also
that the resulting file may be much smaller, since it only takes
a few bytes to define a line or curve, while the corresponding
bitmap may take many).

A very useful program for this purpose is 'autotrace': see

  http://autotrace.sourceforge.net/

  "Here is a short description of currently supported formats:

      * Inputformats BMP, TGA, PNM, PPM, PGM, PBM and those
        supported by ImageMagick.
      * Exportformat Postscript, svg, xfig, swf, pstoedit, emf,
        dxf, cgm, mif, p2e and sk"

I also received the following just over a year ago on the
'autotrace' list, but the site does not respond now:

>  > What I really want, is a program like autotrace to output
>  > a list of splines or some mathematical representation of
>  > the image that I can  manipulate mathematically.
>
>  Maybe you'd like to have a look at this, to see what can be
>  achieved with ~12 KB code (Don't laugh at me ;-) This program
>  is actually used in production right now.)
>
>    http://www.mesw.de/stencil/

The "image that I can manipulate mathematically" sounds like 
the sort of thing that Hinrich is looking for!

Best wishes to all,
[autotraced signature attached (PDF -- note the size!)]


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 08-Dec-04                                       Time: 13:47:12
------------------------------ XFMail ------------------------------
-------------- next part --------------
A non-text attachment was scrubbed...
Name: teds_sig.pdf
Type: application/pdf
Size: 2222 bytes
Desc: teds_sig.pdf
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20041208/6717023f/teds_sig.pdf

From Roger.Bivand at nhh.no  Wed Dec  8 15:18:22 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 8 Dec 2004 15:18:22 +0100 (CET)
Subject: [R] Importing vector graphics into R
In-Reply-To: <XFMail.041208134712.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0412081514250.17157-100000@reclus.nhh.no>

On Wed, 8 Dec 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 08-Dec-04 Roger Bivand wrote:
> > On Tue, 7 Dec 2004, Hinrich G??hlmann wrote:
> > 
> >> Dear R users,
> >> 
> >> I know of the possibility to import bitmaps via the nice
> >> pixmap library. 
> >>     But if you later on create a PDF it is somewhat
> >> disappointing to have such graphics bitmapped. Is there
> >> a trick (via maps?) to import a vector graphic and have
> >> them plotted onto a graph? My searching attempts in the
> >> searchable r-help archive did not seem to result in anything 
> >> useful...
> > 
> > No, nothing obvious. If you have an Xfig file - or convert to
> > one from PS,
> 
> How does one do that? None of the tools I can find on my (Linux)
> system seem to include the possibility of PS->Xfig (or any other
> vector format either, except of course PDF).

http://www.pstoedit.net/pstoedit

currently maintained and fairly reliable.

> 
> > you may be able to extract the lines with their attributes by
> > hand (the file is just text, so you can "see" the vector
> > graphics), and write an R function to plot them (rescaled) onto
> > the device if you need a single graphical element many times.
> > Otherwise, perhaps edit the graphics file after R has completed
> > its work. None of the vector map formats is easy to use for
> > this kind of trick, especially because you probably need
> > attributes on the lines (thickness, colour).
> 
> When I first saw Hinrich's post, I thought it was a hopeless
> quest. Even if one had a PS file (rather than PDF), I think
> it would only be feasible to write such a conversion if it
> were guaranteed that the PS file used only "raw" PS language
> in its simplest usage (i.e. one would be able to look out for,
> and then interpret, the basic drawing commands like "moveto",
> "rmoveto", "lineto", "rlineto", etc. with explicit numerical
> coordinates). Unfortunately, many programs which output PS
> have extended preambles in which all sorts of abbreviations
> are defined to wrap up chunks of "raw" PS. So one would be
> looking at writing a fully featured PS interpreter!
> 
> When starting from a PDF file, however, even though this is
> in a sense "reminiscent" of PS (and may have been converted
> from a PS file), nevertheless PDF is a harder format to interpret
> because of its hierarchical "modular" construction (in effect
> a "tree of objects"). So I would be even less optimistic about
> coverting PDF to a non-PS vector format.
> 
> However, if I'm at all wrong about any of that I would be most
> interested to be informatively corrected!
> 
> On the other hand, there is the possibility to convert a bitmap
> to a PS file where lines and curves are drawn using vector
> graphics (giving the advantage that the result is as smooth as
> the resolution of the ultimate raster device allows, and also
> that the resulting file may be much smaller, since it only takes
> a few bytes to define a line or curve, while the corresponding
> bitmap may take many).
> 
> A very useful program for this purpose is 'autotrace': see
> 
>   http://autotrace.sourceforge.net/
> 
>   "Here is a short description of currently supported formats:
> 
>       * Inputformats BMP, TGA, PNM, PPM, PGM, PBM and those
>         supported by ImageMagick.
>       * Exportformat Postscript, svg, xfig, swf, pstoedit, emf,
>         dxf, cgm, mif, p2e and sk"
> 
> I also received the following just over a year ago on the
> 'autotrace' list, but the site does not respond now:
> 
> >  > What I really want, is a program like autotrace to output
> >  > a list of splines or some mathematical representation of
> >  > the image that I can  manipulate mathematically.
> >
> >  Maybe you'd like to have a look at this, to see what can be
> >  achieved with ~12 KB code (Don't laugh at me ;-) This program
> >  is actually used in production right now.)
> >
> >    http://www.mesw.de/stencil/
> 
> The "image that I can manipulate mathematically" sounds like 
> the sort of thing that Hinrich is looking for!
> 
> Best wishes to all,
> [autotraced signature attached (PDF -- note the size!)]
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 08-Dec-04                                       Time: 13:47:12
> ------------------------------ XFMail ------------------------------
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From jarioksa at sun3.oulu.fi  Wed Dec  8 15:21:58 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 08 Dec 2004 16:21:58 +0200
Subject: [R] Importing vector graphics into R
In-Reply-To: <XFMail.041208134712.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041208134712.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1102515718.10092.12.camel@biol102145.oulu.fi>

On Wed, 2004-12-08 at 15:53, Ted.Harding at nessie.mcc.ac.uk wrote:
> On 08-Dec-04 Roger Bivand wrote:
> > On Tue, 7 Dec 2004, Hinrich G??hlmann wrote:
> > 
> >> Dear R users,
> >> 
> >> I know of the possibility to import bitmaps via the nice
> >> pixmap library. 
> >>     But if you later on create a PDF it is somewhat
> >> disappointing to have such graphics bitmapped. Is there
> >> a trick (via maps?) to import a vector graphic and have
> >> them plotted onto a graph? My searching attempts in the
> >> searchable r-help archive did not seem to result in anything 
> >> useful...
> > 
> > No, nothing obvious. If you have an Xfig file - or convert to
> > one from PS,
> 
> How does one do that? None of the tools I can find on my (Linux)
> system seem to include the possibility of PS->Xfig (or any other
> vector format either, except of course PDF).
> 
pstoedit. May not be in standard distros, but can be compiled from the
source. Here we have even used pstoedit in post-processing eps graphs
from R. It works in some cases, but, for instance, lattice graphic was
made of polygons instead of lines, and we couldn't change line widths
for horizontal lines only in panel headers.

This is what pstoedit gives for version info:

pstoedit: version 3.33 / DLL interface 108 (build Oct 17 2003 - release
build) : Copyright (C) 1993 - 2003 Wolfgang Glunz

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From ggrothendieck at myway.com  Wed Dec  8 15:22:10 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 8 Dec 2004 14:22:10 +0000 (UTC)
Subject: [R] How about a mascot for R?
References: <200412062315.iB6NFDAl020914@atlas.otago.ac.nz>
	<8B6940B2-4862-11D9-9A40-000A95A9BA9A@fhcrc.org>
	<41B5EC21.90104@pdf.com>
Message-ID: <loom.20041208T151853-181@post.gmane.org>


Given the likely reception that newbies get on r-help, those two 
classifications, i.e. a reptile which is cold-blooeded animal or 
an insect which is an annoying pest, are not altogether off the mark but
to really capture the experience, Don Rickles should be the mascot.

Spencer Graves <spencer.graves <at> pdf.com> writes:

: 
: I prefer not to consider the implications of associating ourselves 
: with something extinct.  Beyond that, I'm more into reptiles that 
: insects.   On the other hand, I don't care much.  I'm happy with the 
: status quo and would be happy with whatever others decide. 
: 
:       Best Wishes,
:       spencer graves
: 
: Robert Gentleman wrote:
: 
: >
: > On Dec 6, 2004, at 3:15 PM, Richard A. O'Keefe wrote:
: >
: >> Thomas Yee <t.yee <at> auckland.ac.nz> wrote:
: >>     ps. Ross has Maori origins, so a native NZ animal is a better idea
: >>     than usual.
: >>
: >> The native animals of New Zealand include no mammals except for
: >> - marine mammals in the coastal waters, such as the Hector's dolphin
: >> - a few species of bat which got blown over from Australia.
: >> Otherwise, there are birds, reptiles, insects, and a few oddballs.
: >> The most notable oddballs would be Tuataras, famous for their third  
: >> eye.
: >> We have some onychophorans, of which it has been said that
: >> "Contemporary Onychophorans are able to predate organisms several times
: >> larger than themselves" (take _that_, SAS!)....
: >> Wetas are quite interesting; they are basically grasshoppers some of  
: >> which
: >> played the ecological role of (and are about the same size as) mice.
: >>
: >
: > Hi,
: >   We had a brief discussion and narrowed it to two, coincidentally  
: > among those named by Richard. The tuatara (there is some charm in  
: > associating a software product with what is essentially a slow moving  
: > dinosaur) and the weta (for those unaware, one might also describe it  
: > as a grasshopper designed by the Pentagon - these have some serious  
: > armor plating and a ferocious grip). Of course this is one of many  
: > views, kiwis, kokakos etc have lots of charm as well - and I think 
: > New  Zealand might lay some claim to the giant squid. No need to stick 
: > with  non-extinct things either - I suspect the dodo is up for grabs. 
: > And on  the NZ front the Moa or the Haast eagle.
: >
: > Ross and Robert
: >
: >> I suspect that only birds have the "cuddly" appeal required of a  
: >> mascot.
: >> Perhaps it's worth pointing out that Kiwis are a kind of Ratite.
: >>
: >> ______________________________________________
: >> R-help <at> stat.math.ethz.ch mailing list
: >> https://stat.ethz.ch/mailman/listinfo/r-help
: >> PLEASE do read the posting guide!  
: >> http://www.R-project.org/posting-guide.html
: >>
: >>
: > +----------------------------------------------------------------------- 
: > ----+
: > | Robert Gentleman              phone: (206) 667-7700            |
: > | Head, Program in Computational Biology   
: > fax:                              |
: > | Division of Public Health Sciences       office: 
: > M2-B865                   |
: > | Fred Hutchinson Cancer Research Center   email: 
: > rgentlem <at> fhcrc.org         |
: > +----------------------------------------------------------------------- 
: > ----+
: >
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help
: > PLEASE do read the posting guide! 
: > http://www.R-project.org/posting-guide.html
:



From deepayan at stat.wisc.edu  Wed Dec  8 15:25:16 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 8 Dec 2004 08:25:16 -0600
Subject: [R] GLMM
In-Reply-To: <I8EOEQ$16BE5D5CFE2C8A064D18D3AB94E9E83E@bol.com.br>
References: <I8EOEQ$16BE5D5CFE2C8A064D18D3AB94E9E83E@bol.com.br>
Message-ID: <200412080825.16732.deepayan@stat.wisc.edu>

On Wednesday 08 December 2004 07:35, Alex wrote:
> Hi all,
>
>
> Could someone please tell me if we have to group data in the units
> with a command such "factor()" or "groupedData()" before using the
> functions glmmPQL or GLMM. I didn't do that and at first my results
> seem OK, but I'd like to solve this doubt.

Factors in your model should definitely be made factors. groupedData is 
completely optional.

Deepayan



From lforzani at stat.umn.edu  Wed Dec  8 15:32:07 2004
From: lforzani at stat.umn.edu (Liliana Forzani)
Date: Wed, 8 Dec 2004 08:32:07 -0600 (CST)
Subject: [R] correlation matrix o
In-Reply-To: <1102515718.10092.12.camel@biol102145.oulu.fi>
References: <XFMail.041208134712.Ted.Harding@nessie.mcc.ac.uk>
	<1102515718.10092.12.camel@biol102145.oulu.fi>
Message-ID: <Pine.LNX.4.58.0412080830260.10734@hidden.stat.umn.edu>


Hi, I have data normal with mean 0, I was wondering how to get (using R)
the best r such that the correlation matrix of my data has the form


{r^(i-j)} where (i,j) indicate row and columm respectivly. Thanks. Liliana



From jarioksa at sun3.oulu.fi  Wed Dec  8 16:01:58 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 08 Dec 2004 17:01:58 +0200
Subject: [R] How about a mascot for R?
In-Reply-To: <71E4EAD7CBE44248908143EE3C6F835317A105@poseidon.demogr.mpg.de>
References: <71E4EAD7CBE44248908143EE3C6F835317A105@poseidon.demogr.mpg.de>
Message-ID: <1102518118.10092.20.camel@biol102145.oulu.fi>

On Wed, 2004-12-08 at 14:10, Rau, Roland wrote:

> Dear all,
> 
> browsing through the suggestions, I have the impression that the general
> direction is towards an animal from New Zealand (I guess because of the
> roots of R). But since the R Foundation is now located in Vienna,
> Austria. What about a typical Austrian animal? Is there one? Maybe a
> "Wolpertinger". A Wolpertinger is a fantasy animal which is a rabbit
> with the antlers known from deer and some wings from a bird. In addition
> to the "Austrian headquarters", another reason for such an animal which
> does not exist in reality (or does it???) is that coding something with
> R is sometimes so easy that it appears to be almost unreal.

I just wait for someone jumping off and saying this is off-topic and you
should stop posting to this list -- and I'm afraid it could happen just
at this point. However, if you accept stranger animals then the group
called Rhinogradentia gives good candidates (at least as pleasant as
Onychophora suggested previously). First, they have R in their name.
Second, they look like mascots.

The most authoritative guide to the group is:

St??mpke, H. 1957. Bau und Leben der Rhinogradentia. Gustav Fischer
Verlag, Stuttgart

The English translation is "The Snouters: Form and life of the
Rhinogrades" . The University of Chicago Press (1981).

Google will found more info for those who don't have acces to these
books.

cheers, jaRi oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From drf5n at maplepark.com  Wed Dec  8 16:10:56 2004
From: drf5n at maplepark.com (David Forrest)
Date: Wed, 8 Dec 2004 09:10:56 -0600 (CST)
Subject: [R] what about a mascot ?
In-Reply-To: <I8EKD2$406CD4FE7DDCDCD2CAA08DD0EF17AE77@laposte.net>
References: <I8EKD2$406CD4FE7DDCDCD2CAA08DD0EF17AE77@laposte.net>
Message-ID: <Pine.LNX.4.58.0412080900350.2728@maplepark.com>

On Wed, 8 Dec 2004, internautem wrote:

> My friend Veslot proposed me the Raven !
> http://www.teteamodeler.com/allopass/images/corbeau.jpg
>
> As a matter of fact the intelligence of this bird is
> comparable to one of a monkey, although its brain is close to
> a reptile brain. R is quite the same : small, compact, but so
> clever.

I like it.  It makes me think of Poe's "The Raven":

http://www.heise.de/ix/raven/Literature/Lore/TheRaven.html

Although the poem is a bit dark, I like the shared features of R and the
Raven: terse, (keyboard-) tapping, answering questions, and making
predictions.

Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Dec  8 16:09:09 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 8 Dec 2004 16:09:09 +0100
Subject: [R] correlation matrix o
References: <XFMail.041208134712.Ted.Harding@nessie.mcc.ac.uk><1102515718.10092.12.camel@biol102145.oulu.fi>
	<Pine.LNX.4.58.0412080830260.10734@hidden.stat.umn.edu>
Message-ID: <005001c4dd37$de92a9f0$0540210a@www.domain>

Hi Liliana,

how about the following:

p <- 10 # assume a 10-dim normal
H <- abs(outer(1:p, 1:p, "-")) # I think you |i-j|

library(mvtnorm)
fn <- function(rho, dat, H) -sum(log(dmvnorm(dat, sigma=rho^H)))

optimize(fn, c(-1,1), dat=rmvnorm(1000, sigma=0.5^H), H=H)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Liliana Forzani" <lforzani at stat.umn.edu>
Cc: "R-News" <r-help at stat.math.ethz.ch>
Sent: Wednesday, December 08, 2004 3:32 PM
Subject: [R] correlation matrix o


>
> Hi, I have data normal with mean 0, I was wondering how to get 
> (using R)
> the best r such that the correlation matrix of my data has the form
>
>
> {r^(i-j)} where (i,j) indicate row and columm respectivly. Thanks. 
> Liliana
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From reid_huntsinger at merck.com  Wed Dec  8 16:27:28 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 8 Dec 2004 10:27:28 -0500
Subject: [R] memory problem
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A921F@uswpmx00.merck.com>

Some additional details would help. What platform? What does the C++ code do
that might cause a memory leak? How are you calling it? 

I don't see why unloading the library would free memory even if it were
allocated outside R's memory management. Unless you're using Windows you
don't need to worry about allocating memory for R, just check that there are
no limits on memory allocation set by the administrator. 

Reid Huntsinger 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Samuel Kemp
Sent: Wednesday, December 08, 2004 6:29 AM
To: r-help at stat.math.ethz.ch
Subject: [R] memory problem


Hi,

I am trying to run a very computationally expensive procedure in 
R-2.0.0. and the process always gets killed after approx 8 minutes. This 
procedure calls some of my own C++ code - in case it was this code 
causing a memory leak I unload and then reload the .so file every time, 
however I still get the same problem. The procedure is run 16000 times 
and always calls the lm() function. My believe at the moment is that I 
need to allocate more memory for R - is this correct? I did try envoking 
R from the command line using R --min-vsize=1000M, however still no luck.

I have googled around and looked at the help files, but I am still 
confused about how to fix this problem.

Any insight would be greatly appeciated.

Kind Regards,

Sam.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Wed Dec  8 16:07:53 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 08 Dec 2004 15:07:53 -0000 (GMT)
Subject: [R] Importing vector graphics into R
In-Reply-To: <1102515718.10092.12.camel@biol102145.oulu.fi>
Message-ID: <XFMail.041208150753.Ted.Harding@nessie.mcc.ac.uk>

On 08-Dec-04 Jari Oksanen wrote:
> On Wed, 2004-12-08 at 15:53, Ted.Harding at nessie.mcc.ac.uk wrote:
>> On 08-Dec-04 Roger Bivand wrote:
>> > [...]
>> > No, nothing obvious. If you have an Xfig file - or convert to
>> > one from PS,
>> 
>> How does one do that? None of the tools I can find on my (Linux)
>> system seem to include the possibility of PS->Xfig (or any other
>> vector format either, except of course PDF).
>> 
> pstoedit. May not be in standard distros, but can be compiled from the
> source. Here we have even used pstoedit in post-processing eps graphs
> from R. It works in some cases, but, for instance, lattice graphic was
> made of polygons instead of lines, and we couldn't change line widths
> for horizontal lines only in panel headers.

Thanks to Jari Oksanen and Roger Bivand for reminding me of this.
(It turns out that I do have it on my oldest Linux -- ca. 1997 --
but not on my more recent ones. I can't remember now whether it
came with the distribution, or I installed it separately and then
forgot about it! Oh dear, I can't remember whether I forgot ... ).

According to the source website: http://www.pstoedit.net/pstoedit
it now seems to be extremely capable. I must try it!

Thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 08-Dec-04                                       Time: 15:07:53
------------------------------ XFMail ------------------------------



From jarioksa at sun3.oulu.fi  Wed Dec  8 16:38:42 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 08 Dec 2004 17:38:42 +0200
Subject: [R] How about a mascot for R?
In-Reply-To: <1102518118.10092.20.camel@biol102145.oulu.fi>
References: <71E4EAD7CBE44248908143EE3C6F835317A105@poseidon.demogr.mpg.de>
	<1102518118.10092.20.camel@biol102145.oulu.fi>
Message-ID: <1102520321.10092.28.camel@biol102145.oulu.fi>

On Wed, 2004-12-08 at 17:01, Jari Oksanen wrote:

> 
> I just wait for someone jumping off and saying this is off-topic and you
> should stop posting to this list -- and I'm afraid it could happen just
> at this point. 

Just to make it clear and to avoid misunderstanding: I was trying to
reach a passive voice with my poor English. I don't want to indicate
that anyone else but me should stop posting to this list...

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From jarioksa at sun3.oulu.fi  Wed Dec  8 16:38:43 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 08 Dec 2004 17:38:43 +0200
Subject: [R] How about a mascot for R?
In-Reply-To: <1102518118.10092.20.camel@biol102145.oulu.fi>
References: <71E4EAD7CBE44248908143EE3C6F835317A105@poseidon.demogr.mpg.de>
	<1102518118.10092.20.camel@biol102145.oulu.fi>
Message-ID: <1102520321.10092.29.camel@biol102145.oulu.fi>

On Wed, 2004-12-08 at 17:01, Jari Oksanen wrote:

> 
> I just wait for someone jumping off and saying this is off-topic and you
> should stop posting to this list -- and I'm afraid it could happen just
> at this point. 

Just to make it clear and to avoid misunderstanding: I was trying to
reach a passive voice with my poor English. I don't want to indicate
that anyone else but me should stop posting to this list...

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From abu3ammar at gmail.com  Wed Dec  8 16:38:43 2004
From: abu3ammar at gmail.com (Omar Lakkis)
Date: Wed, 8 Dec 2004 10:38:43 -0500
Subject: [R] RODBC on linux
Message-ID: <b1d3150404120807385e614871@mail.gmail.com>

I am using RODBC 1.1-2 with R 2.0.0 on a Debian box with a singel
Pentium processor. My database is Informix 7.32.
When  I run a sql query I get the table header back but no data is
selected. Please note this example:

Table "t1" has two columns, "a" and "b".  The table is populated with
three rows.

Thi si the definition of the table:
Column name          Type                                    Nulls
a                                 smallint                                yes
b                                 integer                                 yes

 
> sqlQuery(pricedb, "select * from t1")
[1] a b
<0 rows> (or 0-length row.names)
> class(a$a)
[1] "integer"
> class(a$b)
[1] "integer"

using my C code on the same box, utalizing teh same odbc installation I get: 
1    2 
3    4 
5    6

Does anyone have an idea why I get the table header but no data? 

Also, sqlTables() return no rows.

> sqlTables(pricedb)
[1] TABLE_QUALIFIER TABLE_OWNER     TABLE_NAME      TABLE_TYPE     
[5] REMARKS        
<0 rows> (or 0-length row.names)



From kjetil at acelerate.com  Wed Dec  8 16:09:49 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 08 Dec 2004 11:09:49 -0400
Subject: [R] Strange error from R CMD INSTALL
In-Reply-To: <41B702A2.4070307@acelerate.com>
References: <41B702A2.4070307@acelerate.com>
Message-ID: <41B7193D.4020903@acelerate.com>

Kjetil Brinchmann Halvorsen wrote:

> I am trying to install a local package and get this unexpected
> error:
>
> ---------- Making package UMSA ------------
>  adding build stamp to DESCRIPTION
>  installing R files
>  installing data files
>  installing man source files
>  installing indices
> Error: couldn't find function "na.omit"
> Execution halted
>
> na.omit of course is in package stats, and that is listed in the
> Depends field in the DESCRIPTION file.
>
> What is happening?
>
> (Windows XP with all development tools installed)
>
> Kjetil
>
Replying to myself with more information: 

(running from Xemacs shell)
Using a package with NO explicit calls to the function na.omit, (but 
with some handling of NA's
with other means):

C:\R\rw2001\src\gnuwin32>Rcmd INSTALL ../library/Drogas
Rcmd INSTALL ../library/Drogas


---------- Making package Drogas ------------
  adding build stamp to DESCRIPTION
  no R files in this package
  installing data files
  installing man source files
  installing indices
Error: couldn't find function "na.omit"
Execution halted
make[2]: *** [indices] Error 1
make[1]: *** [all] Error 2
make: *** [pkg-Drogas] Error 2
*** Installation of Drogas failed ***




-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Wed Dec  8 16:21:25 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 08 Dec 2004 11:21:25 -0400
Subject: [R] correlation matrix o
In-Reply-To: <Pine.LNX.4.58.0412080830260.10734@hidden.stat.umn.edu>
References: <XFMail.041208134712.Ted.Harding@nessie.mcc.ac.uk>	<1102515718.10092.12.camel@biol102145.oulu.fi>
	<Pine.LNX.4.58.0412080830260.10734@hidden.stat.umn.edu>
Message-ID: <41B71BF5.8080800@acelerate.com>

Liliana Forzani wrote:

>Hi, I have data normal with mean 0, I was wondering how to get (using R)
>the best r such that the correlation matrix of my data has the form
>
>
>{r^(i-j)} where (i,j) indicate row and columm respectivly. Thanks. Liliana
>
>  
>
Thats the correlation matrix for an autoregressive(1) process, with 
equal time increments.
So you could use arima with your regressors in xreg and ar1 structure,
or package nlme with corAR1 correlation structure.

Kjetil

>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From ripley at stats.ox.ac.uk  Wed Dec  8 17:32:10 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Wed, 8 Dec 2004 16:32:10 +0000 (GMT)
Subject: [R] RODBC on linux
In-Reply-To: <b1d3150404120807385e614871@mail.gmail.com>
Message-ID: <Pine.GSO.4.31.0412081630160.15602-100000@markov.stats>

If you look in the RODBC help pages for odbcConnect you will see this
mentioned with respect to Oracle and Sybase.  Try the fix mentioned there.

On Wed, 8 Dec 2004, Omar Lakkis wrote:

> I am using RODBC 1.1-2 with R 2.0.0 on a Debian box with a singel
> Pentium processor. My database is Informix 7.32.
> When  I run a sql query I get the table header back but no data is
> selected. Please note this example:
>
> Table "t1" has two columns, "a" and "b".  The table is populated with
> three rows.
>
> Thi si the definition of the table:
> Column name          Type                                    Nulls
> a                                 smallint                                yes
> b                                 integer                                 yes
>
>
> > sqlQuery(pricedb, "select * from t1")
> [1] a b
> <0 rows> (or 0-length row.names)
> > class(a$a)
> [1] "integer"
> > class(a$b)
> [1] "integer"
>
> using my C code on the same box, utalizing teh same odbc installation I get:
> 1    2
> 3    4
> 5    6
>
> Does anyone have an idea why I get the table header but no data?
>
> Also, sqlTables() return no rows.
>
> > sqlTables(pricedb)
> [1] TABLE_QUALIFIER TABLE_OWNER     TABLE_NAME      TABLE_TYPE
> [5] REMARKS
> <0 rows> (or 0-length row.names)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sam.kemp2 at ntlworld.com  Wed Dec  8 17:46:05 2004
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Wed, 08 Dec 2004 16:46:05 +0000
Subject: [R] memory problem
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A921F@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA31999056A921F@uswpmx00.merck.com>
Message-ID: <41B72FCD.30807@ntlworld.com>

Thanks.

Here is some more information.....

My platform is a Linux desktop.

The C++ code implements a Gamma test which is calculated by constructing 
near neighbour lists (kd-tree) - the C++ code returns the deltas and 
gammas, the R code gets the noise estimate of the data by doing a linear 
regression on these deltas and gammas. However, at the end of my C++ 
code I do delete all objects on the free store.

This Gamma test code should be called 16,000 times but R crashes after 
about 14,000, with "killed".




Huntsinger, Reid wrote:

>Some additional details would help. What platform? What does the C++ code do
>that might cause a memory leak? How are you calling it? 
>
>I don't see why unloading the library would free memory even if it were
>allocated outside R's memory management. Unless you're using Windows you
>don't need to worry about allocating memory for R, just check that there are
>no limits on memory allocation set by the administrator. 
>
>Reid Huntsinger 
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Samuel Kemp
>Sent: Wednesday, December 08, 2004 6:29 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] memory problem
>
>
>Hi,
>
>I am trying to run a very computationally expensive procedure in 
>R-2.0.0. and the process always gets killed after approx 8 minutes. This 
>procedure calls some of my own C++ code - in case it was this code 
>causing a memory leak I unload and then reload the .so file every time, 
>however I still get the same problem. The procedure is run 16000 times 
>and always calls the lm() function. My believe at the moment is that I 
>need to allocate more memory for R - is this correct? I did try envoking 
>R from the command line using R --min-vsize=1000M, however still no luck.
>
>I have googled around and looked at the help files, but I am still 
>confused about how to fix this problem.
>
>Any insight would be greatly appeciated.
>
>Kind Regards,
>
>Sam.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
>------------------------------------------------------------------------------
>
>  
>



From Robert.McGehee at geodecapital.com  Wed Dec  8 17:59:52 2004
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 8 Dec 2004 11:59:52 -0500
Subject: [R] Modulus Problem
Message-ID: <67DCA285A2D7754280D3B8E88EB5480208CA113C@MSGBOSCLB2WIN.DMN1.FMR.COM>

R users, I am having a problem with the modulus operator for large
numbers as follows,

a <- 2
n <- 561
## n is the first Carmichael number, so by Fermat's Little Theorem the
below should equal zero.

(a^(n-1) - 1) %% n
[1] 2.193172e+152
## Seems that R and Fermat disagree

## Also,
1000000000000000000 %% 11
[1] -32

This seems like a bug. Should I be avoiding integer math for large
numbers?

Thanks,
Robert

platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    0.1              
year     2004             
month    11               
day      15               
language R



From reid_huntsinger at merck.com  Wed Dec  8 18:06:29 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 8 Dec 2004 12:06:29 -0500
Subject: [R] memory problem
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9220@uswpmx00.merck.com>

OK, I would do the following:

1. Watch memory usage as the program runs, both within R and at the OS level
for the R process. I suppose your code allocates memory (eg with "new")
within the R process but the R memory manager knows nothing about it. So you
can tell where memory is leaking, if at all.

2. write a dummy .so to return constant values, doing no computation, but
using the same interface. If you still have the problem then you have a
problem with the R code.

If this got me nowhere I would start R under gdb and look around. The
"Writing R Extensions" has a section on this.

Reid

-----Original Message-----
From: Samuel Kemp [mailto:sam.kemp2 at ntlworld.com] 
Sent: Wednesday, December 08, 2004 11:46 AM
To: Huntsinger, Reid
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] memory problem


Thanks.

Here is some more information.....

My platform is a Linux desktop.

The C++ code implements a Gamma test which is calculated by constructing 
near neighbour lists (kd-tree) - the C++ code returns the deltas and 
gammas, the R code gets the noise estimate of the data by doing a linear 
regression on these deltas and gammas. However, at the end of my C++ 
code I do delete all objects on the free store.

This Gamma test code should be called 16,000 times but R crashes after 
about 14,000, with "killed".




Huntsinger, Reid wrote:

>Some additional details would help. What platform? What does the C++ code
do
>that might cause a memory leak? How are you calling it? 
>
>I don't see why unloading the library would free memory even if it were
>allocated outside R's memory management. Unless you're using Windows you
>don't need to worry about allocating memory for R, just check that there
are
>no limits on memory allocation set by the administrator. 
>
>Reid Huntsinger 
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Samuel Kemp
>Sent: Wednesday, December 08, 2004 6:29 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] memory problem
>
>
>Hi,
>
>I am trying to run a very computationally expensive procedure in 
>R-2.0.0. and the process always gets killed after approx 8 minutes. This 
>procedure calls some of my own C++ code - in case it was this code 
>causing a memory leak I unload and then reload the .so file every time, 
>however I still get the same problem. The procedure is run 16000 times 
>and always calls the lm() function. My believe at the moment is that I 
>need to allocate more memory for R - is this correct? I did try envoking 
>R from the command line using R --min-vsize=1000M, however still no luck.
>
>I have googled around and looked at the help files, but I am still 
>confused about how to fix this problem.
>
>Any insight would be greatly appeciated.
>
>Kind Regards,
>
>Sam.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>
>
>---------------------------------------------------------------------------
---
>Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
Jersey, USA 08889), and/or its affiliates (which may be known outside the
United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
Banyu) that may be confidential, proprietary copyrighted and/or legally
privileged. It is intended solely for the use of the individual or entity
named on this message.  If you are not the intended recipient, and have
received this message in error, please notify us immediately by reply e-mail
and then delete it from your system.
>---------------------------------------------------------------------------
---
>
>  
>



From tlumley at u.washington.edu  Wed Dec  8 18:44:33 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 8 Dec 2004 09:44:33 -0800 (PST)
Subject: [R] Modulus Problem
In-Reply-To: <67DCA285A2D7754280D3B8E88EB5480208CA113C@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB5480208CA113C@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <Pine.A41.4.61b.0412080939210.80648@homer10.u.washington.edu>

On Wed, 8 Dec 2004, McGehee, Robert wrote:

> R users, I am having a problem with the modulus operator for large
> numbers as follows,
>
> a <- 2
> n <- 561
> ## n is the first Carmichael number, so by Fermat's Little Theorem the
> below should equal zero.
>
> (a^(n-1) - 1) %% n
> [1] 2.193172e+152
> ## Seems that R and Fermat disagree
>
> ## Also,
> 1000000000000000000 %% 11
> [1] -32
>
> This seems like a bug. Should I be avoiding integer math for large
> numbers?

You can find out the largest representable integer from 
.Machine$integer.max, and it is probably 2^31-1.  Numbers larger than that 
are converted to double precision.  On the other hand, %% should probably 
return an error or NaN if .Machine$double.eps times the first operand is 
greater than 1.

 	-thomas



From ggrothendieck at myway.com  Wed Dec  8 18:48:22 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 8 Dec 2004 17:48:22 +0000 (UTC)
Subject: [R] Strange error from R CMD INSTALL
References: <41B702A2.4070307@acelerate.com> <41B7193D.4020903@acelerate.com>
Message-ID: <loom.20041208T184339-570@post.gmane.org>

Kjetil Brinchmann Halvorsen <kjetil <at> acelerate.com> writes:

: 
: Kjetil Brinchmann Halvorsen wrote:
: 
: > I am trying to install a local package and get this unexpected
: > error:
: >
: > ---------- Making package UMSA ------------
: >  adding build stamp to DESCRIPTION
: >  installing R files
: >  installing data files
: >  installing man source files
: >  installing indices
: > Error: couldn't find function "na.omit"
: > Execution halted
: >
: > na.omit of course is in package stats, and that is listed in the
: > Depends field in the DESCRIPTION file.
: >
: > What is happening?
: >
: > (Windows XP with all development tools installed)
: >
: > Kjetil
: >
: Replying to myself with more information: 
: 
: (running from Xemacs shell)
: Using a package with NO explicit calls to the function na.omit, (but 
: with some handling of NA's
: with other means):
: 
: C:\R\rw2001\src\gnuwin32>Rcmd INSTALL ../library/Drogas
: Rcmd INSTALL ../library/Drogas
: 
: 
: ---------- Making package Drogas ------------
:   adding build stamp to DESCRIPTION
:   no R files in this package
:   installing data files
:   installing man source files
:   installing indices
: Error: couldn't find function "na.omit"
: Execution halted
: make[2]: *** [indices] Error 1
: make[1]: *** [all] Error 2
: make: *** [pkg-Drogas] Error 2
: *** Installation of Drogas failed ***
: 

Don't know the answer but note that in your second
run it could not find any R files at all so there
is some change between the two runs.

I have found in the past that sometimes that the error 
messages in building packages are completely wrong -- 
there is a real error, its just that the error message 
does not describe it.  What I have done in the case that
I am completely lost is to remove half my package rebuild
and continuing in that way via binary search narrow it down.



From wolf.privat at gmx.de  Wed Dec  8 14:23:23 2004
From: wolf.privat at gmx.de (Andreas)
Date: Wed, 8 Dec 2004 14:23:23 +0100
Subject: [R] randomize the order of rows in a matrix or table
Message-ID: <cp6v87$dlc$1@sea.gmane.org>

Hello,

is there any function to randomize the order of rows in matrix. My dataset
for suport vector mashines is in the order first the samples of class1 and
then the samples for class2. For the training of the svm I need this dataset
randomized in its order.

regards Andreas



From andy_liaw at merck.com  Wed Dec  8 19:09:37 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 8 Dec 2004 13:09:37 -0500
Subject: [R] randomize the order of rows in a matrix or table
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3F8@usrymx25.merck.com>

mydata <- mydata[sample(nrow(mydata)), ]

Andy

> From: Andreas
> 
> Hello,
> 
> is there any function to randomize the order of rows in 
> matrix. My dataset
> for suport vector mashines is in the order first the samples 
> of class1 and
> then the samples for class2. For the training of the svm I 
> need this dataset
> randomized in its order.
> 
> regards Andreas
>



From gunter.berton at gene.com  Wed Dec  8 19:13:07 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 8 Dec 2004 10:13:07 -0800
Subject: [R] randomize the order of rows in a matrix or table
In-Reply-To: <cp6v87$dlc$1@sea.gmane.org>
Message-ID: <200412081813.iB8ID87c017835@faraday.gene.com>


?sample

as in rand.rowx <- x[sample(nrow(x)),]

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andreas
> Sent: Wednesday, December 08, 2004 5:23 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] randomize the order of rows in a matrix or table
> 
> Hello,
> 
> is there any function to randomize the order of rows in 
> matrix. My dataset
> for suport vector mashines is in the order first the samples 
> of class1 and
> then the samples for class2. For the training of the svm I 
> need this dataset
> randomized in its order.
> 
> regards Andreas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Wed Dec  8 19:15:26 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 8 Dec 2004 13:15:26 -0500
Subject: [R] randomize the order of rows in a matrix or table
In-Reply-To: <cp6v87$dlc$1@sea.gmane.org>
References: <cp6v87$dlc$1@sea.gmane.org>
Message-ID: <2292121C-4945-11D9-AD0F-000D933565E8@mail.nih.gov>

There is probably a neater solution, but....

x <- matrix(1:1000,nrow=500)
y <- runif(500)
z <- x[order(y),]

On Dec 8, 2004, at 8:23 AM, Andreas wrote:

> Hello,
>
> is there any function to randomize the order of rows in matrix. My 
> dataset
> for suport vector mashines is in the order first the samples of class1 
> and
> then the samples for class2. For the training of the svm I need this 
> dataset
> randomized in its order.
>
> regards Andreas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From adrian at atstatconsulting.com  Wed Dec  8 19:49:56 2004
From: adrian at atstatconsulting.com (Adrian Katschke)
Date: Wed, 8 Dec 2004 10:49:56 -0800 (PST)
Subject: [R] Clustering in R
Message-ID: <20041208184957.4010.qmail@web207.biz.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041208/b5d0bea0/attachment.pl

From ray at mcs.vuw.ac.nz  Wed Dec  8 20:26:22 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Thu, 9 Dec 2004 08:26:22 +1300 (NZDT)
Subject: [R] Strange error from R CMD INSTALL
Message-ID: <200412081926.iB8JQMD4023309@tahi.mcs.vuw.ac.nz>

> Date: Wed, 08 Dec 2004 09:33:22 -0400
> From: Kjetil Brinchmann Halvorsen <kjetil at acelerate.com>
> 
> I am trying to install a local package and get this unexpected
> error:
> 
> ---------- Making package UMSA ------------
>   adding build stamp to DESCRIPTION
>   installing R files
>   installing data files
>   installing man source files
>   installing indices
> Error: couldn't find function "na.omit"
> Execution halted
> 
> na.omit of course is in package stats, and that is listed in the
> Depends field in the DESCRIPTION file.
> 
Check what is in your data directory.  Does something in there use
(implicitly) na.omit?

I have seen something like this (since 2.0.0) where a .R file in the
data/ directory used a function within the package, but that function
was not 'available' at the "installing indices" phase of the
check/build.

Ray Brownrigg



From bitwrit at ozemail.com.au  Fri Dec 10 06:20:19 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 10 Dec 2004 16:20:19 +1100
Subject: [R] what about a mascot ?
In-Reply-To: <I8EKD2$406CD4FE7DDCDCD2CAA08DD0EF17AE77@laposte.net>
References: <I8EKD2$406CD4FE7DDCDCD2CAA08DD0EF17AE77@laposte.net>
Message-ID: <20041208193049.MJR4289.smta01.mail.ozemail.net@there>

internautem wrote:
> My friend Veslot proposed me the Raven !
> http://www.teteamodeler.com/allopass/images/corbeau.jpg
>
> As a matter of fact the intelligence of this bird is
> comparable to one of a monkey, although its brain is close to
> a reptile brain. R is quite the same : small, compact, but so
> clever.
>
This is a rather interesting suggestion, as tool making behavior was recently 
observed in a corvid. Also, the social interaction of many corvids is neither 
solitary nor pack, just interacting when they have a common goal. Somewhat 
like the cooperative behavior of virtual teams on the Internet. Last but not 
least, the raven is a natural for a cartoon character.

Jim



From dray at biomserv.univ-lyon1.fr  Wed Dec  8 20:37:31 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Wed, 08 Dec 2004 14:37:31 -0500
Subject: [R] Clustering in R
In-Reply-To: <20041208184957.4010.qmail@web207.biz.mail.re2.yahoo.com>
Message-ID: <5.2.1.1.0.20041208143605.01ece1b8@biomserv.univ-lyon1.fr>

see ?clustIndex in package cclust.

At 13:49 08/12/2004, Adrian Katschke wrote:
>Is there a command to get cluster criterion for the cluster methods? SAS 
>has its criterion, but I prefer to do it in R. If there is not a command 
>is there code to produce criteria to choose the number of clusters?
>
>Adrian Katschke
>Statistics Grad Student
>University of Nebraska-Lincoln
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From adrian at atstatconsulting.com  Wed Dec  8 20:44:23 2004
From: adrian at atstatconsulting.com (Adrian Katschke)
Date: Wed, 8 Dec 2004 11:44:23 -0800 (PST)
Subject: [R] XML library
Message-ID: <20041208194424.39152.qmail@web201.biz.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041208/f6bbeaef/attachment.pl

From yunming at gmail.com  Wed Dec  8 21:48:53 2004
From: yunming at gmail.com (Yunming Mu)
Date: Wed, 8 Dec 2004 14:48:53 -0600
Subject: [R] Rgui Application error help!
Message-ID: <c2f82958041208124854db91d6@mail.gmail.com>

Dear R-Users

I was trying to call a Fortran 77 subroutine from R 2.0.0 in Windows
XP. I have  Rtools, Perl and MinGW as described  in
http://www.murdoch-sutherland.com/Rtools/ with path sets.
When I use C:\R\rw2000\bin>Rcmd SHLIB test.f,
a test.dll is created without error and located in that directory, but when
I call the subroutine in R 2.0.0, R crashed and I got an error message
as follows:

Rgui.exe-Application error
the instruction at "0x5ad71531" referenced memory at "0x00000014".
The memory could not be "read".

I believe that the Fortran 77 code is causing no problem. Also,
please note that the same Fortran77 subroutine was called successfully
in Unix version of R. (test.so was created accordingly). I am
wondering whether R for Windows call Fortran subroutine in a different
way than R for Unix? Any insight would be highly appreciated!

Thanks!
Yunming Mu



From jbang at uiuc.edu  Wed Dec  8 22:07:27 2004
From: jbang at uiuc.edu (Bang)
Date: Wed, 8 Dec 2004 15:07:27 -0600
Subject: [R] Surface graph.
Message-ID: <200412082107.iB8L7Tlg003736@expredir2.cites.uiuc.edu>

What package/code could I use to create a 3-d surface graph of the predicted
values over two of the explanatory variables' coefficients?

Jim
James Bang
Department of Economics
University of Illinois 

Well I AM missing the back of my head.you COULD cut me a little slack!
-Homer Simpson



From ripley at stats.ox.ac.uk  Wed Dec  8 22:09:22 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Wed, 8 Dec 2004 21:09:22 +0000 (GMT)
Subject: [R] Strange error from R CMD INSTALL
In-Reply-To: <200412081926.iB8JQMD4023309@tahi.mcs.vuw.ac.nz>
Message-ID: <Pine.GSO.4.31.0412082103020.8915-100000@markov.stats>

On Thu, 9 Dec 2004, Ray Brownrigg wrote:

> > Date: Wed, 08 Dec 2004 09:33:22 -0400
> > From: Kjetil Brinchmann Halvorsen <kjetil at acelerate.com>
> >
> > I am trying to install a local package and get this unexpected
> > error:
> >
> > ---------- Making package UMSA ------------
> >   adding build stamp to DESCRIPTION
> >   installing R files
> >   installing data files
> >   installing man source files
> >   installing indices
> > Error: couldn't find function "na.omit"
> > Execution halted
> >
> > na.omit of course is in package stats, and that is listed in the
> > Depends field in the DESCRIPTION file.
> >
> Check what is in your data directory.  Does something in there use
> (implicitly) na.omit?
>
> I have seen something like this (since 2.0.0) where a .R file in the
> data/ directory used a function within the package, but that function
> was not 'available' at the "installing indices" phase of the
> check/build.

(INSTALL, actually.) That's a plausible explanation.  From 200update.txt
on developer.r-project.org

2) data/*.R files must be self-sufficient, and in particular not
   depend on the package or standard packages other than base.  (This
   has always been documented, but is now enforced.)

and see also `Writing R Extensions'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From isen at plantpath.wisc.edu  Wed Dec  8 22:12:26 2004
From: isen at plantpath.wisc.edu (Dr. Thomas Isenbarger)
Date: Wed, 8 Dec 2004 15:12:26 -0600
Subject: [R] similarity matrix conversion to dissimilarity
Message-ID: <DC653D1A-495D-11D9-BA63-000A9598473A@plantpath.wisc.edu>

I have a matrix of similarity scores that I want to convert into a 
matrix of dissimilarity scores so that I can apply some clustering 
methods to the data.  That is, high values in my matrix signify 
similarity and low values (zero being the lowest) signify no 
similarity.  What functions/options in R or its packages are available 
for making this kind of transformation of a matrix?

Specifically, I am a molecular biologist.  I have a set of 700+ 
nucleotide sequences i want to group into clusters based on sequence 
similarities.  There is a wide range of sequences in the set, some of 
which are homologous to other sequences in the set.  I want to use 
clustering to identify these groups.

If the sequences were related and good be trimmed to the same length, I 
would do an alignment and then use phylip (or some other distance 
method) to create a distance matrix, but since my sequences are 
unrelated and cannot be trimmed to the same length, I am at a loss for 
what to do.

For a set with so many unrelated sequences of different lengths, the 
only thing I have been able to is an all-against-all BLAST to create 
the matrix, but this gives high scores for similarities, not high 
scores for dissimilarities.  The only thought I had was to use the 
reciprocal of the BLAST score as some perverse measure of distance.

I am not subscribed to the list, so can I ask for responses directly to 
my email address?

Thank-you,
Tom Isenbarger


--
isen at plantpath.wisc.edu
thomas a isenbarger
(608) 265-0850



From Roger.Bivand at nhh.no  Wed Dec  8 22:43:32 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 8 Dec 2004 22:43:32 +0100 (CET)
Subject: [R] Surface graph.
In-Reply-To: <200412082107.iB8L7Tlg003736@expredir2.cites.uiuc.edu>
Message-ID: <Pine.LNX.4.44.0412082242010.17321-100000@reclus.nhh.no>

On Wed, 8 Dec 2004, Bang wrote:

> What package/code could I use to create a 3-d surface graph of the predicted
> values over two of the explanatory variables' coefficients?

I think you'll need the surface first, for example from interp() in the 
akima package, then visualise with persp() or perhaps rgl.surface() in 
package rgl.

> 
> Jim
> James Bang
> Department of Economics
> University of Illinois 
> 
> Well I AM missing the back of my head.you COULD cut me a little slack!
> -Homer Simpson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From MDavy at hortresearch.co.nz  Wed Dec  8 23:38:29 2004
From: MDavy at hortresearch.co.nz (Marcus Davy)
Date: Thu, 09 Dec 2004 11:38:29 +1300
Subject: [R] Is  k  equivalent to  k:k ?
Message-ID: <s1b8394b.012@hra2.marc.hort.cri.nz>


Bringing up an old topic on a small technicality.

In the help documentation on seq.

Value:

     The result is of 'mode' '"integer"' if 'from' is (numerically
     equal to an) integer and 'by' is not specified.

The arguement in seq specifying length also creates "double" which is not obvious in the wording, as "by" is not specified but
the object is double.

> typeof(seq(from=1, length=1))
[1] "double"



marcus



>>> Martin Maechler <maechler at stat.math.ethz.ch> 3/08/2004 3:15:45 AM >>>
>>>>> "Georgi" == Georgi Boshnakov <georgi.boshnakov at umist.ac.uk>
>>>>>     on Mon, 2 Aug 2004 15:46:37 +0100 writes:

    Georgi> Hi, I wonder if the following (apparent)
    Georgi> inconsistency is a bug or feature.  

a feature "of course".

    Georgi> Since scalars are simply vectors of length one I
    Georgi> would think that a and a:a produce the same result. 

Why would you?  
Have you read the documentation for ":" carefully?  

{Part this topic, namely that a numeric constant such as "4"
 is a "double" in R (but integer in S-plus since S+5.0)
 should probably finally end up in the R FAQ ...
}

Regards, Martin Maechler

    Georgi> For example,

    >> identical(4.01,4.01:4.01)
    Georgi> [1] TRUE

    Georgi> However,

    Georgi> identical(4,4:4)
    Georgi> [1] FALSE

    Georgi> and

    >> identical(4.0,4.0:4.0)
    Georgi> [1] FALSE

    Georgi> A closer look reveals that the colon operator produces objects of different class, e.g.

a closer look wouldn't have been necessary had you read and
understood the documentation.


    >> class(4)
    Georgi> [1] "numeric"
    >> class(4.0)
    Georgi> [1] "numeric"

    Georgi> but

    >> class(4:4)
    Georgi> [1] "integer"
    >> class(4.0:4.0)
    Georgi> [1] "integer"


    Georgi> Georgi Boshnakov
    Georgi> ------------------------------------------------------------------------------
    Georgi> Dr Georgi Boshnakov                                tel.: +44  (0)161 200 3684
    Georgi> Mathematics Department                           email: georgi.boshnakov at umist.ac.uk 
    Georgi> UMIST
    Georgi> P O Box 88    
    Georgi> Manchester M60 1QD
    Georgi> UK


    Georgi> [[alternative HTML version deleted]]

	    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	    do read the posting guide why you should reconfigure
	    your e-mail software

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From HDoran at air.org  Wed Dec  8 23:43:38 2004
From: HDoran at air.org (Doran, Harold)
Date: Wed, 8 Dec 2004 17:43:38 -0500
Subject: [R] similarity matrix conversion to dissimilarity
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7406CF1DCA@dc1ex2.air.org>

 Dear Sir:

I posed a similar question a few months back and received many
responses. Check the searchable archives at R Cran for those helpful
email. I did a search for 'similarity matrix' and many results were
returned.

Harold

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dr. Thomas
Isenbarger
Sent: Wednesday, December 08, 2004 4:12 PM
To: r-help at stat.math.ethz.ch
Subject: [R] similarity matrix conversion to dissimilarity

I have a matrix of similarity scores that I want to convert into a
matrix of dissimilarity scores so that I can apply some clustering
methods to the data.  That is, high values in my matrix signify
similarity and low values (zero being the lowest) signify no similarity.
What functions/options in R or its packages are available for making
this kind of transformation of a matrix?

Specifically, I am a molecular biologist.  I have a set of 700+
nucleotide sequences i want to group into clusters based on sequence
similarities.  There is a wide range of sequences in the set, some of
which are homologous to other sequences in the set.  I want to use
clustering to identify these groups.

If the sequences were related and good be trimmed to the same length, I
would do an alignment and then use phylip (or some other distance
method) to create a distance matrix, but since my sequences are
unrelated and cannot be trimmed to the same length, I am at a loss for
what to do.

For a set with so many unrelated sequences of different lengths, the
only thing I have been able to is an all-against-all BLAST to create the
matrix, but this gives high scores for similarities, not high scores for
dissimilarities.  The only thought I had was to use the reciprocal of
the BLAST score as some perverse measure of distance.

I am not subscribed to the list, so can I ask for responses directly to
my email address?

Thank-you,
Tom Isenbarger


--
isen at plantpath.wisc.edu
thomas a isenbarger
(608) 265-0850

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec  9 00:10:55 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 08 Dec 2004 23:10:55 -0000 (GMT)
Subject: [R] similarity matrix conversion to dissimilarity
In-Reply-To: <DC653D1A-495D-11D9-BA63-000A9598473A@plantpath.wisc.edu>
Message-ID: <XFMail.041208231055.Ted.Harding@nessie.mcc.ac.uk>

[replying to your personal address as well as the list; but
 I think you should subscribe to the list since this topic
 may well be pursued further]

On 08-Dec-04 Dr. Thomas Isenbarger wrote:
> I have a matrix of similarity scores that I want to convert into a 
> matrix of dissimilarity scores so that I can apply some clustering 
> methods to the data.  That is, high values in my matrix signify 
> similarity and low values (zero being the lowest) signify no 
> similarity.  What functions/options in R or its packages are available 
> for making this kind of transformation of a matrix?
> 
> Specifically, I am a molecular biologist.  I have a set of 700+ 
> nucleotide sequences i want to group into clusters based on sequence 
> similarities.  There is a wide range of sequences in the set, some of 
> which are homologous to other sequences in the set.  I want to use 
> clustering to identify these groups.
> 
> If the sequences were related and good be trimmed to the same length, I
> would do an alignment and then use phylip (or some other distance 
> method) to create a distance matrix, but since my sequences are 
> unrelated and cannot be trimmed to the same length, I am at a loss for 
> what to do.
> 
> For a set with so many unrelated sequences of different lengths, the 
> only thing I have been able to is an all-against-all BLAST to create 
> the matrix, but this gives high scores for similarities, not high 
> scores for dissimilarities.  The only thought I had was to use the 
> reciprocal of the BLAST score as some perverse measure of distance.
> 
> I am not subscribed to the list, so can I ask for responses directly to
> my email address?

Clearly any function which "inverts" the measure of "similarity"
(i.e. decreases as "similarity" increases) could be used as a
measure of dissimilarity in general. Indeed you imply as much
yourself. There is quite a wide choice ... "reciprocal" could be one.

However, reading between your lines, it seems that you do
not have a substantive interpretation for "dissimilarity".
Yet apparently you have one for "similarity". Otherwise, on
what basis do you claim that your similarity matrix expresses
*substantive* similarity?

But, if you can attach an interpretation (in some substantive
terms) to your measure of similarity, can you not then negate
the propositions that this expresses and obtain a measure of
dissimilarity? In that case, the function could be programmed
in R (though it may not be a function of your "similarity" and.
you would need to derive it from the data).

If not, why not? Or, if your measure of "similarity" in fact
does not carry a substantive interpretation, then one could
assert that any decreasing function of "similarity" could
be used, and would be as meaningful as your measure of
"similarity". Again, this can be programmed in R.

Again reading between your lines, it could be inferred that
in the situation you describe ("unrelated sequences" which
"cannot be trimmed to the same length"), while you can derive
a measure of similarity which matches established concepts
for similarity in your field, you cannot match the concepts
for dissimilarity.

If that is the case, R cannot help you with the conceptual
problem.

This may appear not helpful, but it is a sincere attempt
to clarify the issues.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 08-Dec-04                                       Time: 23:10:55
------------------------------ XFMail ------------------------------



From andy_liaw at merck.com  Thu Dec  9 00:31:01 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 8 Dec 2004 18:31:01 -0500
Subject: [R] Surface graph.
Message-ID: <3A822319EB35174CA3714066D590DCD50994E3FF@usrymx25.merck.com>

> From: Roger Bivand
> 
> On Wed, 8 Dec 2004, Bang wrote:
> 
> > What package/code could I use to create a 3-d surface graph 
> of the predicted
> > values over two of the explanatory variables' coefficients?
> 
> I think you'll need the surface first, for example from 
> interp() in the 
> akima package, then visualise with persp() or perhaps 
> rgl.surface() in 
> package rgl.

But wouldn't it be better to predict on a regular grid, and graph that, than
to interpolate an irregular set of predicted values (assuming the model is
available such that prediction at any point is possible)?

Andy
 
> > 
> > Jim
> > James Bang
> > Department of Economics
> > University of Illinois 
> > 
> > Well I AM missing the back of my head.you COULD cut me a 
> little slack!
> > -Homer Simpson
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> Roger 
> Bivand
> Economic Geography Section, Department of Economics, 
> Norwegian School of
> Economics and Business Administration, Breiviksveien 40, 
> N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> e-mail: Roger.Bivand at nhh.no
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From francoisromain at free.fr  Thu Dec  9 00:30:01 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Thu, 09 Dec 2004 00:30:01 +0100
Subject: [R] Clustering in R
In-Reply-To: <20041208184957.4010.qmail@web207.biz.mail.re2.yahoo.com>
References: <20041208184957.4010.qmail@web207.biz.mail.re2.yahoo.com>
Message-ID: <41B78E79.3040709@free.fr>

Hello,

The fpc package is filled with criterion for cluster analysis. See the 
function clusters.stats in that package.

Romain.

Adrian Katschke a ??crit :

>Is there a command to get cluster criterion for the cluster methods? SAS has its criterion, but I prefer to do it in R. If there is not a command is there code to produce criteria to choose the number of clusters?
> 
>Adrian Katschke
>Statistics Grad Student
>University of Nebraska-Lincoln
>
>  
>
-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann??e
Institut de Statistique de l'Universit?? de Paris (ISUP)
Fili??re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From ok at cs.otago.ac.nz  Thu Dec  9 01:55:50 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 9 Dec 2004 13:55:50 +1300 (NZDT)
Subject: [R] system() and file names with spaces
Message-ID: <200412090055.iB90toRk042124@atlas.otago.ac.nz>

Consider the question we had recently:  "how do I count the lines in a file
without reading it into R?"  The solution I suggested was

    as.numeric(system(paste("wc -l <", filename), TRUE))

Unfortunately, it doesn't work, or at least, not all the time.
If you already know all about that, and don't care, or already have
a solution, stop reading now.  Otherwise, let me try to undo any
harm I may have done by providing a fuller solution.

We've had several reports in this list about problems caused by Windows
file names with spaces in them.  File names with spaces are also common
in MacOS X, so common, in fact, that file name completion in a Terminal
actually works (if you have a file name "Foo Bar", and type F, o, TAB
you get Foo\ Bar).  File names with spaces are possible in other Unix
systems too, and always have been, though they are less likely.

So suppose there is a file "Foo Bar" you want to find the size of.
> file.name <- "Foo Bar"
> system(paste("wc -l <", File.name)
executes the command
   wc -l < Foo Bar
which gives you the size of Bar if there is one, or fails if there is not,
and ignores Foo (should there be one) and of course ignores "Foo Bar".

What can we do about it?  Well, we can try this:

    for.system <- function (s) gsub(" ", "\\\\ ", s)

    system(paste("wc -l <", for.system(file.name)), TRUE)

Great.  Works for files with spaces in their names.  Now we try some other
file names.  (File names like this are abundant in MacOS X.)

    file.name <- "Black & White Minstrels/1972"

	Whoops.  wc -l < Black\ &\ White\ Minstrels/1972
	forks off "wc -l <Black\ " and then tries to run
	"\ White\ Minstrels/1972".

    file.name <- "Quake(R)/scores"

	Whoops.  "Badly placed ()'s".

    file.name <- "Drunkard's walk/log-1'

	Whoops.  "Unmatched '"

So try again.

    for.system <-
	function (s) gsub("([][)(}{'\";&! \t])", "\\\\\\1", s)

    line.count <-
	function (s) as.numeric(system(paste("wc -l <", for.system(s)), TRUE))

This _still_ isn't perfect, but it is a whole lot better than the naive
version.  The major remaining problem is that the set of special characters
and the quoting mechanism need to be changed for Windows.  I _think_ the
Windows version should be something like

    for.system <- function (s) {
	i <- grep("[^-_:.A-Za-z0-9/\\\\]", s)
	s[i] <- sapply(s[i], function (s) paste("\"", s, "\"", sep=""))
	s
    }

But what if a file name contains a double quote?  Until someone tells me,
I'm just going to hope it doesn't happen.  Putting the pieces together,

f% cat >"Foo Bar"
a b c
d e
f
<EOF>


for.system <-
    if (.Platform$OS.type == "windows") {
        function (s) {
            i <- grep("[^-_:.A-Za-z0-9/\\\\]", s)
            s[i] <- sapply(s[i], function (s) paste("\"", s, "\"", sep=""))
            s
        }
    } else {
        function (s) gsub("([][)(}{'\";&! \t\n])", "\\\\\\1", s)
    }

wc <- function (s) {
    r <- scan(pipe(paste("wc <", for.system(s)), open="r"), n=3, quiet=TRUE)
    names(r) <- c("lines", "words", "chars")
    r
}

> wc("Foo Bar")
lines words chars 
    3     6    12 
> system("cp $HOME/.login Drunkard\\'s\\ Walk")
> wc("Drunkard's Walk")["chars"]
chars 
 3633 
> 

If there's already something like for.system() built into R, I'd be very
happy to know about it.  (It's a little odd that system() and pipe()
don't already support something like this; in a multi-element character
vector the first could be taken literally and the remaining ones could be
taken quoted with leading spaces.)



From wangruiwin at hotmail.com  Thu Dec  9 04:16:07 2004
From: wangruiwin at hotmail.com (Rui Wang)
Date: Wed, 8 Dec 2004 20:16:07 -0700
Subject: [R] System is computationally singular?
Message-ID: <BAY102-DAV8C837F14DB6D2A022BC9ECEB70@phx.gbl>

Hi all,

I was using the Newton-Raphson method to estimate paremeters in the model developed by my supervisor. However, when I interatively computed theta(t+1)=theta(t) - solve(H)*s (where the Hessian matrix and score vector were explicitely derived), I got the error message: Error in solve.default(H) : system is computationally singular: reciprocal condition number = 1.70568e-032. Assume my score vector and Hessian matrix were correct, could anyone give me some suggestion on how to avoid this singular situation? Thank you in advance. Maybe this question is not related to R itself, but it is kind of statistical computation problems, please forgive me to put questions here.

Rui



From David.Duffy at qimr.edu.au  Thu Dec  9 02:16:37 2004
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Thu, 9 Dec 2004 11:16:37 +1000 (EST)
Subject: [R] Re: Polychoric correlations
In-Reply-To: <200411291202.iATBir36020057@hypatia.math.ethz.ch>
References: <200411291202.iATBir36020057@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0412091051370.13058@orpheus.qimr.edu.au>

This is a bit late, but:

> About two years ago there was a thread about this which suggested that at
> that time nobody had these coefficients ready to go.
> (a) has anyone in the meanwhile programmed them?

http://www.qimr.edu.au/davidD/R/polyr.R

> (d) I appreciate this last item is not strictly an R question, but my
> intention is to use these as input into the sem package for structural
> equation models. If anyone thinks that is misguided I would be intersted to
> here.
>
> Michael Dewey
> m.dewey at iop.kcl.ac.uk

As you might know, there are various ways you can fit what in the
genetic literature is called a multifactorial threshold model (MFT ie
tetra/polychoric correlation model).  The simplest is to treat them as
Pearson correlations and use conventional SEM methods, which often works
well (esp for large dimensional problems); the AWLS method of Browne as
implemented for example in LISREL (needs large N); or full ML fitting to the
multidimensional contingency tables, which is available in programs like
Mx, or could be fitted pretty easily using mvtnorm as you suggested (Mx
uses Genz's algorithms).

You should also check the adequacy of fit of the MFT to your data, and
look at the related loglinear models.


| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From ok at cs.otago.ac.nz  Thu Dec  9 05:27:14 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 9 Dec 2004 17:27:14 +1300 (NZDT)
Subject: [R] Solaris installation problem resolved
Message-ID: <200412090427.iB94REQs044545@atlas.otago.ac.nz>

Installing the e1071 package seemed to go fine, but when I tried to
library(e1071) it didn't work.

> library(e1071)
Loading required package: class
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library "/users/local/lib/R/library/e1071/libs/e1
  ld.so.1: /users/local/lib/R/bin/exec/R: fatal: relocation error: file /users/
Error in library(e1071) : package/namespace load failed for 'e1071'

This was in R 2.0.1 on Solaris 2.9:

         _                   
platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    2                   
minor    0.1                 
year     2004                
month    11                  
day      15                  
language R                   

It turns out that the missing function is the __Crun::pure_error() function
that the Sun C++ compiler installs in a class's virtual function table for
an abstract (= 0) method.  Still I didn't get it, and patched around the
problem, only to find that operator [] new was missing.  It finally dawned
on me that something about my installation setup was the problem, not
anything to do with e1071 as such:  it just wasn't picking up the C++
runtime libraries.

If you have installed R on a Solaris box using the Sun compilers, you can
check if you made my mistake by doing

f% /users/local/bin/R CMD config SHLIB_CXXLDFLAGS
-G

Patching $INSTALL/lib/R/etc/Makeconf so that

f% /users/local/bin/R CMD config SHLIB_CXXLDFLAGS
-G /opt/SUNWspro/lib/CC4/libC.so /opt/SUNWspro/lib/libCrun.so

made the problem go away for me.  For other releases of the Sun C++
compiler these files will probably be different.  I found the names
libC.so and libCrun.so in the NOTES section of the CC manual page,
and used find /opt/SUNWspro ... to locate them.

There is almost certainly a better way to do this, but I thought someone
else might find this useful.



From lisas at salford-systems.com  Thu Dec  9 05:52:32 2004
From: lisas at salford-systems.com (Lisa Solomon)
Date: Wed, 08 Dec 2004 20:52:32 -0800
Subject: [R] 
 Data Mining Conference: Solving Real World Challenges, New York,
 March 2005
Message-ID: <41B7DA10.6010809@salford-systems.com>

Apologies for cross posting....

-----------------------------------------------------------------------------
                    Salford Systems Data Mining 2005
                      New York, March 28-30, 2005
Focusing on the Contributions of Data Mining to Solving Real World 
Challenges

     Honoring the Real-World Experiences of Data Mining Visionaries
                    Leo Breiman and Jerome Friedman

                           CONFERENCE PROGRAM
              http://www.salforddatamining.com/program.htm

------------------------------------------------------------------------------

TRACKS:
Data Mining Issues and Implementation
Real World Success Stories: Business
Real World Success Stories: Biomedical
Real World Success Stories: Environmental
Novel Methodologies

POST-CONFERENCE HANDS-ON TRAINING

Network with Data Mining Experts and Pick up Pointers from Companies, 
Research Centers and Laboratories Including:
The International Monetary Fund, American Express, Barnes and Noble, 
Visa, Pfizer, Union Bank, Wells Fargo Bank, Ciphergen, Stanford Linear 
Accelerator, Johns Hopkins Medical School and the University of 
California at Berkeley.

If you have an interest in attending this conference or the 
post-conference training, please contact Lisa Solomon:
Phone: 619-543-8880 x14, Email:  lisas at salforddatamining.com



From David.Duffy at qimr.edu.au  Thu Dec  9 07:28:51 2004
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Thu, 9 Dec 2004 16:28:51 +1000 (EST)
Subject: [R] Re: Tetrachoric and polychoric correlations, Polycor package
In-Reply-To: <200412041114.iB4B29ha000411@hypatia.math.ethz.ch>
References: <200412041114.iB4B29ha000411@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0412091619230.13524@orpheus.qimr.edu.au>

A bit late, but you might like to look at

http://www.qimr.edu.au/davidD/polyr.R

Regarding the original posters queries:

You can analyse polychoric correlations as if they were Pearson
correlations using standard software (eg sem), and this usually doesn't do
too badly, or go to AWLS (Browne) in LISREL etc, or ML analysis
of the full multidimensional contingency table using programs such as Mx,
or as you noted, mvtnorm (Mx uses Alan Genz's algorithms).

You can check model assumptions, and compare the results to those
from similar loglinear models.  For example, for a 3-way table, a single
factor model based on polychoric correlations should fit "perfectly", if
the no higher order interaction assumption is right,


| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From arv at ono.com  Thu Dec  9 09:37:23 2004
From: arv at ono.com (antonio =?iso-8859-1?q?rodr=EDguez?=)
Date: Thu, 9 Dec 2004 09:37:23 +0100
Subject: [R] Random Walk plus noise Model
In-Reply-To: <20041206225341.96186.qmail@web54707.mail.yahoo.com>
References: <20041206225341.96186.qmail@web54707.mail.yahoo.com>
Message-ID: <200412090937.23606.arv@ono.com>

take a look at this place:


http://zoonek2.free.fr/UNIX/48_R/all.html

antonio

> Hi,
>
> I need help about the example that it appears in the
> book Time series the Brokwell and Davis, specificaly
> the random walk plus noise in the chapter 8. I need
> simulate t simulate something similar.
>
> thanks




>
> _________________________________________________________
>

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From hgoehlma at gmx.de  Thu Dec  9 08:44:27 2004
From: hgoehlma at gmx.de (=?ISO-8859-1?Q?Hinrich_G=F6hlmann?=)
Date: Thu, 09 Dec 2004 08:44:27 +0100
Subject: [R] Importing vector graphics into R
In-Reply-To: <Pine.LNX.4.44.0412071840270.16498-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0412071840270.16498-100000@reclus.nhh.no>
Message-ID: <41B8025B.9020403@gmx.de>

Thanks for your suggestions!

Even though they are less than encouraging, I quickly want to give you 
the rational why I have asked this. Actually I was inspired by Paul 
Murrell's useR presentation - have a look at the very last slide of his 
presentation which you can find at
http://www.stat.auckland.ac.nz/~paul/Talks/useR2004.pdf - If only this 
kind of functionality could be generalized to any vector graphics... 
Oh, well, still pixmap gives a solution for the moment and that's ok. 
Thanks again!

Cheers,
hinrich   d8-)


Roger Bivand wrote:
> On Tue, 7 Dec 2004, Hinrich G??hlmann wrote:
> 
> 
>>Dear R users,
>>
>>I know of the possibility to import bitmaps via the nice pixmap library. 
>>    But if you later on create a PDF it is somewhat disappointing to 
>>have such graphics bitmapped. Is there a trick (via maps?) to import a 
>>vector graphic and have them plotted onto a graph? My searching attempts 
>>in the searchable r-help archive did not seem to result in anything 
>>useful...
> 
> 
> No, nothing obvious. If you have an Xfig file - or convert to one from PS,
> you may be able to extract the lines with their attributes by hand (the
> file is just text, so you can "see" the vector graphics), and write an R
> function to plot them (rescaled) onto the device if you need a single
> graphical element many times. Otherwise, perhaps edit the graphics file
> after R has completed its work. None of the vector map formats is easy to
> use for this kind of trick, especially because you probably need
> attributes on the lines (thickness, colour).
> 
> 
>>Cheers,
>>hinrich   d8-)
>>
> 
>



From maechler at stat.math.ethz.ch  Thu Dec  9 08:56:00 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 9 Dec 2004 08:56:00 +0100
Subject: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <s1b8394b.012@hra2.marc.hort.cri.nz>
References: <s1b8394b.012@hra2.marc.hort.cri.nz>
Message-ID: <16824.1296.765579.318413@gargle.gargle.HOWL>

>>>>> "Marcus" == Marcus Davy <MDavy at hortresearch.co.nz>
>>>>>     on Thu, 09 Dec 2004 11:38:29 +1300 writes:

    Marcus> Bringing up an old topic on a small technicality.

    Marcus> In the help documentation on seq()

     >> Value:

     >>   The result is of 'mode' '"integer"' if 'from' is (numerically
     >>   equal to an) integer and 'by' is not specified.

    Marcus> The arguement in seq specifying length also creates
    Marcus> "double" which is not obvious in the wording, as
    Marcus> "by" is not specified but the object is double.

    >> typeof(seq(from=1, length=1))
    Marcus> [1] "double"

yes, thank you, Marcus!

Actually the situation is a bit more complicated and I'm
currently looking at the (not yet committed) version

\value{
  The result is of \code{mode} \code{"integer"} if \code{from} is
  (numerically equal to an) integer and, e.g., only \code{to} is specified.
  % MM: to specify all the conditions doesn't seem worth, nor should the
  % code be changed just for docu.purposes; e.g. str(seq(from=1:1, to=8, by=3:3))
}

Martin


    >>>> Martin Maechler <maechler at stat.math.ethz.ch> 3/08/2004 3:15:45 AM >>>
>>>>> "Georgi" == Georgi Boshnakov <georgi.boshnakov at umist.ac.uk>
>>>>>     on Mon, 2 Aug 2004 15:46:37 +0100 writes:

    Georgi> Hi, I wonder if the following (apparent)
    Georgi> inconsistency is a bug or feature.  

    Marcus> a feature "of course".

    Georgi> Since scalars are simply vectors of length one I
    Georgi> would think that a and a:a produce the same result. 

    Marcus> Why would you?  
    Marcus> Have you read the documentation for ":" carefully?  

    Marcus> {Part this topic, namely that a numeric constant such as "4"
    Marcus> is a "double" in R (but integer in S-plus since S+5.0)
    Marcus> should probably finally end up in the R FAQ ...
    Marcus> }

    Marcus> Regards, Martin Maechler

    Georgi> For example,

    >>> identical(4.01,4.01:4.01)
    Georgi> [1] TRUE

    Georgi> However,

    Georgi> identical(4,4:4)
    Georgi> [1] FALSE

    Georgi> and

    >>> identical(4.0,4.0:4.0)
    Georgi> [1] FALSE

    Georgi> A closer look reveals that the colon operator produces objects of different class, e.g.

    Marcus> a closer look wouldn't have been necessary had you read and
    Marcus> understood the documentation.


    >>> class(4)
    Georgi> [1] "numeric"
    >>> class(4.0)
    Georgi> [1] "numeric"

    Georgi> but

    >>> class(4:4)
    Georgi> [1] "integer"
    >>> class(4.0:4.0)
    Georgi> [1] "integer"


    Georgi> Georgi Boshnakov
    Georgi> ------------------------------------------------------------------------------
    Georgi> Dr Georgi Boshnakov                                tel.: +44  (0)161 200 3684
    Georgi> Mathematics Department                           email: georgi.boshnakov at umist.ac.uk 
    Georgi> UMIST
    Georgi> P O Box 88    
    Georgi> Manchester M60 1QD
    Georgi> UK


    Georgi> [[alternative HTML version deleted]]

    Marcus> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    Marcus> do read the posting guide why you should reconfigure
    Marcus> your e-mail software

    Marcus> ______________________________________________
    Marcus> R-help at stat.math.ethz.ch mailing list
    Marcus> https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
    Marcus> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 

    Marcus> ______________________________________________________

    Marcus> The contents of this e-mail are privileged and/or confidential to the
    Marcus> named recipient and are not to be used by any other person and/or
    Marcus> organisation. If you have received this e-mail in error, please notify 
    Marcus> the sender and delete all material pertaining to this e-mail.
    Marcus> ______________________________________________________



From bhx2 at mevik.net  Thu Dec  9 08:59:02 2004
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Thu, 09 Dec 2004 08:59:02 +0100
Subject: [R] http://bugs.r-project.org down?
Message-ID: <m0y8g7j41l.fsf@bar.nemo-project.org>

I haven't been able to connect to http://bugs.r-project.org the last
few days.  Is there a problem with the site (or am I having a
problem :-) ?

-- 
Bj??rn-Helge Mevik



From manuel_gutierrez_lopez at yahoo.es  Thu Dec  9 09:09:16 2004
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Thu, 9 Dec 2004 09:09:16 +0100 (CET)
Subject: [R] test multiple objects for being equal length
Message-ID: <20041209080916.96472.qmail@web25101.mail.ukl.yahoo.com>

I could not find any help pages on How to test many
objects for being of equal length
Something like identical for more than two objects?
x<-1:6
y<-1:10
z<-3:5
## For two objects I can do:
identical(length(x),length(y))
## For more than two I currently can do:
length(unique(c(length(x),length(y),length(z))))==1

but there must be a better way.
Thanks,
M



From lecoutre at stat.ucl.ac.be  Thu Dec  9 09:17:29 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 09 Dec 2004 09:17:29 +0100
Subject: [R] test multiple objects for being equal length
In-Reply-To: <20041209080916.96472.qmail@web25101.mail.ukl.yahoo.com>
References: <20041209080916.96472.qmail@web25101.mail.ukl.yahoo.com>
Message-ID: <6.0.1.1.2.20041209091411.02065d68@stat4ux.stat.ucl.ac.be>


Hi Manuel,

First, encapsulate yoyr objects within a list. That will help you 
manipulate all them at once and ensures that the final function will work 
with whatever number of vectors.

?? ll <- list(x,y,z)
?? sapply(ll,length)
[1]  6 10  3

Then you can use your length(unique(...))==1
Another way is to use all:

?? all(sapply(ll,length)==length(ll[[1]]))
[1] FALSE

HTH,

Eric



At 09:09 9/12/2004, Manuel Gutierrez wrote:
>I could not find any help pages on How to test many
>objects for being of equal length
>Something like identical for more than two objects?
>x<-1:6
>y<-1:10
>z<-3:5
>## For two objects I can do:
>identical(length(x),length(y))
>## For more than two I currently can do:
>length(unique(c(length(x),length(y),length(z))))==1
>
>but there must be a better way.
>Thanks,
>M
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Dec  9 09:27:27 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 9 Dec 2004 09:27:27 +0100
Subject: [R] System is computationally singular?
References: <BAY102-DAV8C837F14DB6D2A022BC9ECEB70@phx.gbl>
Message-ID: <008101c4ddc8$eacb82a0$0540210a@www.domain>

Hi Rui,

when you are far from the optimum the Hessian might be a non positive 
definite matrix and thus it cannot be solved. There are some positive 
definite modifications that you could apply. For more info you could 
check at Section 3.2.4 of:

http://www.stat.wisc.edu/~mchung/teaching/stat471/stat_computing.pdf

However, if you have a function computing the minus log-likelihood and 
its derivative, then it would be easier to use `optim()', probably 
with method "BFGS" (or "CG" if you have many parameters).

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rui Wang" <wangruiwin at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, December 09, 2004 4:16 AM
Subject: [R] System is computationally singular?


> Hi all,
>
> I was using the Newton-Raphson method to estimate paremeters in the 
> model developed by my supervisor. However, when I interatively 
> computed theta(t+1)=theta(t) - solve(H)*s (where the Hessian matrix 
> and score vector were explicitely derived), I got the error message: 
> Error in solve.default(H) : system is computationally singular: 
> reciprocal condition number = 1.70568e-032. Assume my score vector 
> and Hessian matrix were correct, could anyone give me some 
> suggestion on how to avoid this singular situation? Thank you in 
> advance. Maybe this question is not related to R itself, but it is 
> kind of statistical computation problems, please forgive me to put 
> questions here.
>
> Rui
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bxc at steno.dk  Thu Dec  9 09:30:32 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Thu, 9 Dec 2004 09:30:32 +0100
Subject: [R] test multiple objects for being equal length
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FDF16@exdkba022.novo.dk>

not that its much shorter:

length( table( sapply( list(x,y,z), length ) ) ) == 1

Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Manuel 
> Gutierrez
> Sent: Thursday, December 09, 2004 9:09 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] test multiple objects for being equal length
> 
> 
> I could not find any help pages on How to test many
> objects for being of equal length
> Something like identical for more than two objects?
> x<-1:6
> y<-1:10
> z<-3:5
> ## For two objects I can do:
> identical(length(x),length(y))
> ## For more than two I currently can do: 
> length(unique(c(length(x),length(y),length(z))))==1
> 
> but there must be a better way.
> Thanks,
> M
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Thu Dec  9 09:45:08 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 9 Dec 2004 09:45:08 +0100
Subject: [R] http://bugs.r-project.org down?
In-Reply-To: <m0y8g7j41l.fsf@bar.nemo-project.org>
References: <m0y8g7j41l.fsf@bar.nemo-project.org>
Message-ID: <16824.4244.763384.412813@gargle.gargle.HOWL>

>>>>> "Bj??HM" == Bj??rn-Helge Mevik <bhx2 at mevik.net>
>>>>>     on Thu, 09 Dec 2004 08:59:02 +0100 writes:

    Bj??HM> I haven't been able to connect to http://bugs.r-project.org the last
    Bj??HM> few days.  Is there a problem with the site (or am I having a
    Bj??HM> problem :-) ?

It's hosted on a machine at Copenhagen University (close to
Peter Dalgaard).  They did "maintenance" in the main server room
just yesterday evening (CET time).  And I see the machine is
still offline as of now. However, AFAIK you shouldn't have had
problems all ``the last few days''.

Martin



From r.hankin at soc.soton.ac.uk  Thu Dec  9 09:55:50 2004
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 9 Dec 2004 08:55:50 +0000
Subject: [R] Modulus Problem
In-Reply-To: <67DCA285A2D7754280D3B8E88EB5480208CA113C@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB5480208CA113C@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <1FF00834-49C0-11D9-93E6-000A95D86AA8@soc.soton.ac.uk>

Hi
On Dec 8, 2004, at 04:59 pm, McGehee, Robert wrote:

> R users, I am having a problem with the modulus operator for large
> numbers as follows,
>
> a <- 2
> n <- 561
> ## n is the first Carmichael number, so by Fermat's Little Theorem the
> below should equal zero.
>
> (a^(n-1) - 1) %% n
> [1] 2.193172e+152
>

I don't think Fermat's Little Theorem is relevant here.  Euler's 
theorem would be though:
a^phi(561)=1 (mod 561) where phi is the totient function.

For your problem, try this:

f <- function(a,p){
   a <- as.integer(a)
   p <- as.integer(p)
   out <- as.integer(1)
   for(i in 1:p){
     out <- (out*a)%%p
   }
   return(out)
}


Then



 > f(2,561)
[1] 1
 > f(3,561)
[1] 375        (sic)
 > f(5,561)
[1] 1
 >


best

rksh

> ## Seems that R and Fermat disagree
>
> ## Also,
> 1000000000000000000 %% 11
> [1] -32
>
> Thi______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ripley at stats.ox.ac.uk  Thu Dec  9 09:58:02 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Thu, 9 Dec 2004 08:58:02 +0000 (GMT)
Subject: [R] system() and file names with spaces
In-Reply-To: <200412090055.iB90toRk042124@atlas.otago.ac.nz>
Message-ID: <Pine.GSO.4.31.0412090848500.19875-100000@markov.stats>

The normal way to do this is to quote the string, here filename.  See
?shQuote.

Your comments really are not fair: a lot of work has been put into
supporting paths containing spaces on both Windows and Unix by the R
developers (or warning that they are not supported), but not by users.
That includes researching and writing functions like shQuote.

On Thu, 9 Dec 2004, Richard A. O'Keefe wrote:

> Consider the question we had recently:  "how do I count the lines in a file
> without reading it into R?"  The solution I suggested was
>
>     as.numeric(system(paste("wc -l <", filename), TRUE))
>
> Unfortunately, it doesn't work, or at least, not all the time.
> If you already know all about that, and don't care, or already have
> a solution, stop reading now.  Otherwise, let me try to undo any
> harm I may have done by providing a fuller solution.
>
> We've had several reports in this list about problems caused by Windows
> file names with spaces in them.  File names with spaces are also common
> in MacOS X, so common, in fact, that file name completion in a Terminal
> actually works (if you have a file name "Foo Bar", and type F, o, TAB
> you get Foo\ Bar).  File names with spaces are possible in other Unix
> systems too, and always have been, though they are less likely.

That's been a feature of Unix shells with file completion (e.g. tcsh) for
at least a decade -- credit where credit is due, please.

> So suppose there is a file "Foo Bar" you want to find the size of.
> > file.name <- "Foo Bar"
> > system(paste("wc -l <", File.name)
> executes the command
>    wc -l < Foo Bar
> which gives you the size of Bar if there is one, or fails if there is not,
> and ignores Foo (should there be one) and of course ignores "Foo Bar".
>
> What can we do about it?  Well, we can try this:
>
>     for.system <- function (s) gsub(" ", "\\\\ ", s)
>
>     system(paste("wc -l <", for.system(file.name)), TRUE)
>
> Great.  Works for files with spaces in their names.  Now we try some other
> file names.  (File names like this are abundant in MacOS X.)
>
>     file.name <- "Black & White Minstrels/1972"
>
> 	Whoops.  wc -l < Black\ &\ White\ Minstrels/1972
> 	forks off "wc -l <Black\ " and then tries to run
> 	"\ White\ Minstrels/1972".
>
>     file.name <- "Quake(R)/scores"
>
> 	Whoops.  "Badly placed ()'s".
>
>     file.name <- "Drunkard's walk/log-1'
>
> 	Whoops.  "Unmatched '"
>
> So try again.
>
>     for.system <-
> 	function (s) gsub("([][)(}{'\";&! \t])", "\\\\\\1", s)
>
>     line.count <-
> 	function (s) as.numeric(system(paste("wc -l <", for.system(s)), TRUE))
>
> This _still_ isn't perfect, but it is a whole lot better than the naive
> version.  The major remaining problem is that the set of special characters
> and the quoting mechanism need to be changed for Windows.  I _think_ the
> Windows version should be something like
>
>     for.system <- function (s) {
> 	i <- grep("[^-_:.A-Za-z0-9/\\\\]", s)
> 	s[i] <- sapply(s[i], function (s) paste("\"", s, "\"", sep=""))
> 	s
>     }
>
> But what if a file name contains a double quote?  Until someone tells me,
> I'm just going to hope it doesn't happen.  Putting the pieces together,
>
> f% cat >"Foo Bar"
> a b c
> d e
> f
> <EOF>
>
>
> for.system <-
>     if (.Platform$OS.type == "windows") {
>         function (s) {
>             i <- grep("[^-_:.A-Za-z0-9/\\\\]", s)
>             s[i] <- sapply(s[i], function (s) paste("\"", s, "\"", sep=""))
>             s
>         }
>     } else {
>         function (s) gsub("([][)(}{'\";&! \t\n])", "\\\\\\1", s)
>     }
>
> wc <- function (s) {
>     r <- scan(pipe(paste("wc <", for.system(s)), open="r"), n=3, quiet=TRUE)
>     names(r) <- c("lines", "words", "chars")
>     r
> }
>
> > wc("Foo Bar")
> lines words chars
>     3     6    12
> > system("cp $HOME/.login Drunkard\\'s\\ Walk")
> > wc("Drunkard's Walk")["chars"]
> chars
>  3633
> >
>
> If there's already something like for.system() built into R, I'd be very
> happy to know about it.  (It's a little odd that system() and pipe()
> don't already support something like this; in a multi-element character
> vector the first could be taken literally and the remaining ones could be
> taken quoted with leading spaces.)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From uwe.malzahn at charite.de  Thu Dec  9 11:12:31 2004
From: uwe.malzahn at charite.de (malzahn)
Date: Thu, 9 Dec 2004 11:12:31 +0100
Subject: [R] svm- class.weights
Message-ID: <000201c4ddd7$98f00370$9b202a8d@charite.de>

ich lese meine Daten mit read.table ein, die Zielvariable hat zwei
Levels: -1 und 1; m??chte svm aufrufen mit class.weights. Beispiel:

svm.trained.1 <- svm(train.x, train.y, type = "C-classification", kernel =
"radial", cost = 0.5, class.weights= c(-1 = 0.195, 1 = 0.805), cross = 10)

so klappt es nicht, "Syntaxfehler"; setzt man die -1 und 1 in class.weights
in doppelte Hochkommata, erfolgt Absturz. Auch die Variante mit train.y als
Zeilenvektor klappt nicht. Wei?? jemand, wie der exakte Aufruf aussehen
sollte?
Vielen Dank

Uwe Malzahn
uwe.malzahn at charite.de



From p.dalgaard at biostat.ku.dk  Thu Dec  9 11:44:26 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Dec 2004 11:44:26 +0100
Subject: [R] http://bugs.r-project.org down?
In-Reply-To: <m0y8g7j41l.fsf@bar.nemo-project.org>
References: <m0y8g7j41l.fsf@bar.nemo-project.org>
Message-ID: <x2mzwnagz9.fsf@biostat.ku.dk>

bhx2 at mevik.net (Bj??rn-Helge Mevik) writes:

> I haven't been able to connect to http://bugs.r-project.org the last
> few days.  Is there a problem with the site (or am I having a
> problem :-) ?

Should be back now. A planned server power outage yesterday evening
coincided with me having to leave early and come in late, so the
machine (in my office) was down from about 15:15 yesterday and 11:25
today.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gongwuming at gmail.com  Thu Dec  9 12:04:14 2004
From: gongwuming at gmail.com (Wuming Gong)
Date: Thu, 9 Dec 2004 19:04:14 +0800
Subject: [R] How can I estimate parameters of probability distributions?
Message-ID: <24d6fd05041209030433c76705@mail.gmail.com>

Hi list, 

I have a group of data. It looks like they follow a exponential
distribution. In R, how can I esimate lamda, that is the rate in pexp,
of the distribution and can I use Kolmogorov-Smirnov for hypothesis
testing in such a situation? I have read the "8.2 Examing the
distribution of a set of data" of "An Introduction to R" but I did not
find any clues on this issue.  (The data is in the attachment).

Thanks

Wuming
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: div.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041209/f705f04e/div.txt

From vito_ricci at yahoo.com  Thu Dec  9 12:21:50 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 9 Dec 2004 12:21:50 +0100 (CET)
Subject: [R] Re:How can I estimate parameters of probability distributions?
Message-ID: <20041209112150.40224.qmail@web41206.mail.yahoo.com>

Hi,

To estimate parameters of exponential distribution you
could use the Maximun Likelihood methods. Find the
log-likelihood function and maximaze it(or minimaze
-log-likelihood function that's the same) calculating
the derivate respect to lamda.

See:

http://www.weibull.com/LifeDataWeb/maximum_likelihood_estimation_exp.htm

Another way is to estimate lamda as 1/mean of sample.

If you use KS test to test if your data belong from an
exponential distribution you're assuming that the
lamda parameter in population is rather equal to the
estimate in sample. I mean that in this way you are
testing both the kind of the distribution and its
parameter lamda.

I hope I give a little help.

Best
Vito



you wrote:

Hi list, 

I have a group of data. It looks like they follow a
exponential
distribution. In R, how can I esimate lamda, that is
the rate in pexp,
of the distribution and can I use Kolmogorov-Smirnov
for hypothesis
testing in such a situation? I have read the "8.2
Examing the
distribution of a set of data" of "An Introduction to
R" but I did not
find any clues on this issue.  (The data is in the
attachment).

Thanks

Wuming

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From carrera_porsche2000 at yahoo.com  Thu Dec  9 12:52:20 2004
From: carrera_porsche2000 at yahoo.com (=?iso-8859-1?q?gon=E7alo=20carrera?=)
Date: Thu, 9 Dec 2004 08:52:20 -0300 (ART)
Subject: [R] saving-Random-Forests
Message-ID: <20041209115220.32214.qmail@web51602.mail.yahoo.com>

Hello

I'm a new user of R environment and i??m facing some
problems when i try to save a specific random forest.
What command should i use to save the random forest?
What command should I use when invoking a previously
saved random forest?

Thanks

=====
Goncalo


	
	
		
_______________________________________________________ 

uma conta agora! http://br.info.mail.yahoo.com/



From michael.watson at bbsrc.ac.uk  Thu Dec  9 12:52:16 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 9 Dec 2004 11:52:16 -0000
Subject: [R] Create a plot legend in a new window
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E899B8@iahce2knas1.iah.bbsrc.reserved>

Hi

I have a complicated plot which has a potentially large legend.  What I
want to do is actually create the legend in a new window.  Has anyone
done this before?  I'd like to be able to create a window with just the
legend in it, and have it so the window is just the right size etc.  I'm
sure someone must have done this already?  If not, any tips would be
welcome.

Thanks
Mick



From sdavis2 at mail.nih.gov  Thu Dec  9 13:03:36 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 9 Dec 2004 07:03:36 -0500
Subject: [R] saving-Random-Forests
In-Reply-To: <20041209115220.32214.qmail@web51602.mail.yahoo.com>
References: <20041209115220.32214.qmail@web51602.mail.yahoo.com>
Message-ID: <5AF823EC-49DA-11D9-A29D-000D933565E8@mail.nih.gov>

Look at ?save and ?load.  If Rf is your random forest,

save(Rf,file='Randomforest.Rdata')
load('Randomforest.Rdata')

Sean

On Dec 9, 2004, at 6:52 AM, gon??alo carrera wrote:

> Hello
>
> I'm a new user of R environment and i??m facing some
> problems when i try to save a specific random forest.
> What command should i use to save the random forest?
> What command should I use when invoking a previously
> saved random forest?
>
> Thanks
>
> =====
> Goncalo
>
>
> 	
> 	
> 		
> _______________________________________________________
>
> uma conta agora! http://br.info.mail.yahoo.com/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From david.meyer at wu-wien.ac.at  Thu Dec  9 13:24:03 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Thu, 9 Dec 2004 13:24:03 +0100
Subject: [R] svm- class.weights
Message-ID: <20041209132403.53b98b0c.david.meyer@wu-wien.ac.at>


Uwe:

[the language of the list is English!]

Try using a *factor* for classification. The described behavior
(segfault when using class.weights with a *numeric* dependent variable)
should be fixed in the current version of e1071 (1.5-2), so please check
if you are using the latest version of e1071.

Best,
-d



From r.hankin at soc.soton.ac.uk  Thu Dec  9 13:25:16 2004
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 9 Dec 2004 12:25:16 +0000
Subject: [R] elliptic functions
Message-ID: <61D45140-49DD-11D9-93E6-000A95D86AA8@soc.soton.ac.uk>

Hi List


I'm thinking of putting together some R functionality (maybe a package)
of elliptic functions.   An elliptic function is one that is doubly 
periodic on the
complex plane and I think they are cool.

One objective of mine is to reproduce  Abramowitz and Stegun's 
beautiful diagram on
p643 (figure 18.5).

The gsl package does sn, cn, dn et seq, but before I put a huge amount 
of
effort in, has anyone else coded up any related material?

I'd be interested to hear if anyone has anything on  Weierstrass's P 
function (in particular,
computing  omega1 and omega2 from general g2 and g3)  and his
sigma functions.  Jacobi's theta functions would be good too.

--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From h.andersson at nioo.knaw.nl  Thu Dec  9 13:29:15 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Thu, 09 Dec 2004 13:29:15 +0100
Subject: [R] Create a plot legend in a new window
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E899B8@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E899B8@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <cp9ghp$2jf$1@sea.gmane.org>

par(xpd=NA) will allow you to click outside your figure and create the 
legend there:

Short example:
par(mfrow=c(1,2),xpd=NA)
plot(1:5)
legend(locator(1),pch=1,legend="TEST")


Good luck, Henrik

michael watson (IAH-C) wrote:
> Hi
> 
> I have a complicated plot which has a potentially large legend.  What I
> want to do is actually create the legend in a new window.  Has anyone
> done this before?  I'd like to be able to create a window with just the
> legend in it, and have it so the window is just the right size etc.  I'm
> sure someone must have done this already?  If not, any tips would be
> welcome.
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From simonh at pol.ac.uk  Thu Dec  9 14:06:52 2004
From: simonh at pol.ac.uk (Simon Holgate)
Date: Thu, 09 Dec 2004 13:06:52 +0000
Subject: [R] ROracle/DBI problem with dbExecStatement on RH Linux
Message-ID: <41B84DEC.3090902@pol.ac.uk>

Hi,

first thanks to Sunny Ho (and David James for the pointer) for resolving 
the problem of the empty rows returned from ROracle.

However, I have a problem still:

  > library(ROracle)
  > ora <- dbDriver("Oracle")
  > con <- dbConnect(ora, user = USER, password = PWD, dbname = DBNAME)
  > rs <- dbExecStatement(con, "select * from USER_TABLES")
 Error in .class1(object) : No direct or inherited method for function 
 "dbExecStatement" for this call

dbSendQuery/dbGetQuery work fine. I've reinstalled ROracle 0.5-5 and DBI 
0.1-8 from source on RH Linux Enterprise WS release 3 Taroon update; 
kernel 2.4.21-9.0.1; Oracle 9.2.0.1; R 1.9.1.

Any suggestions on resolving this would be much appreciated.

Cheers,

Simon



From jon.fiva at svt.ntnu.no  Thu Dec  9 14:51:59 2004
From: jon.fiva at svt.ntnu.no (Jon Hernes Fiva)
Date: Thu, 9 Dec 2004 14:51:59 +0100
Subject: [R] Spatial Probit in R
Message-ID: <696626107DDE6F41AC8650E28CB19328BC9DE5@grace.ad.svt.ntnu.no>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041209/2e1fbcd2/attachment.pl

From michael.watson at bbsrc.ac.uk  Thu Dec  9 14:57:18 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 9 Dec 2004 13:57:18 -0000
Subject: [R] Create a plot legend in a new window
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E899BE@iahce2knas1.iah.bbsrc.reserved>

Following on from this, what I want is to create a new window and fill
up the entire window with my legend.

I have tried:

plot.new()
par(mar=c(0,0,0,0))
legend()

But that still puts legend wherever I specify x and y.  So after I have
set mar to c(0,0,0,0), how do I tell R to make the legend fill the
entire window, starting at the top-left?

Thanks
Mick

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
(IAH-C)
Sent: 09 December 2004 11:52
To: R-help at stat.math.ethz.ch
Subject: [R] Create a plot legend in a new window


Hi

I have a complicated plot which has a potentially large legend.  What I
want to do is actually create the legend in a new window.  Has anyone
done this before?  I'd like to be able to create a window with just the
legend in it, and have it so the window is just the right size etc.  I'm
sure someone must have done this already?  If not, any tips would be
welcome.

Thanks
Mick

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Thu Dec  9 15:06:41 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 09 Dec 2004 15:06:41 +0100
Subject: [R] Create a plot legend in a new window
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E899B8@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41B86A01.331.18EBBCE@localhost>

Hi Michael

If on windows (what you did not tell us) use

1.	plot your graph

2.	windows() # to create new grapfic template

3.	click on it to make it active

4.	plot(1,1,type="n")

5.	place your legend somewhere on the second plot e.g. by 
legend(locator(1), ........)

BTW

help.search("window") reveals that there is ts function window, 
but also graphic function "windows". Maybe on the "window" help 
page could be mentioned link to this quite similar (by name) 
graphic function, even if it is completely unrelated.

Cheers
Petr


On 9 Dec 2004 at 11:52, michael watson (IAH-C) wrote:

> Hi
> 
> I have a complicated plot which has a potentially large legend.  What
> I want to do is actually create the legend in a new window.  Has
> anyone done this before?  I'd like to be able to create a window with
> just the legend in it, and have it so the window is just the right
> size etc.  I'm sure someone must have done this already?  If not, any
> tips would be welcome.
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From H.van.Rijn at ai.rug.nl  Thu Dec  9 15:35:50 2004
From: H.van.Rijn at ai.rug.nl (Hedderik van Rijn)
Date: Thu, 9 Dec 2004 15:35:50 +0100
Subject: [R] US 2004 Elections map
Message-ID: <9F60740C-49EF-11D9-9844-000393D7CE9A@ai.rug.nl>


even for people who didn't like the outcome of the US elections this  
year, it must have been a joy to see all the nice (and not so nice)  
graphs that were shown. As an exercise, I recreated the map shown on  
the NY-Times website [  
http://www.nytimes.com/packages/khtml/2004/11/03/politics/ 
20041103_px_ELECT_GRAPHIC.html ]

If you're interested,

   http://www.ai.rug.nl/~hedderik/R/US2004

contains the created maps, the data and the code. Note that there are  
some differences in the R map and the NYT map (most notably with  
respect to some missing data in some New England states, some  
mismatches in county names, and the representation of NYC), but I was  
quite satisfied with the map as it stands now.

   - Hedderik.



From chencheva at gmail.com  Thu Dec  9 15:39:18 2004
From: chencheva at gmail.com (Hu Chen)
Date: Thu, 9 Dec 2004 22:39:18 +0800
Subject: [R] a question about swap space, memory and read.table()
Message-ID: <6f3fc9ee041209063972fe2fea@mail.gmail.com>

Hi all 
Two computers:
one is my desktop PC, windows2000, R 1.9.1. Physical RAM 256MB, Swap
(Virtual memory)  384Mb. When I allocate a large matrix, it firstly
uses up RAM, then use swap space. In windows' task manager, the usage
of memory could exceed my physic RAM's size.
The other machine is a remote server. Windows XP, R 1.9.1 Physical RAM 2GB.
Swap space 4GB. I use "R --max-mem-size=4000M" to start R. However
when I allocate a large matrix or data frame, it uses up all RAM then
exits with a error message" cannot allocate vector of size 7812 Kb ".
The Swap space is not used at all !
What's more, I found that the read.table() function is really a waste of memory.
> ft <- read.table("filepath")
> object.size(ft)
object.size(ft)
[1] 192000692
only 192Mb.
however, in the windows task manager it shows that this process takes
nearly 800Mb memory.
I used gc() to collect garbarge. Howerver it doesn't help.
Any guys have methods to release the wasted memory?
thank you all.
Regards



From andy_liaw at merck.com  Thu Dec  9 15:39:52 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 9 Dec 2004 09:39:52 -0500
Subject: [R] test multiple objects for being equal length
Message-ID: <3A822319EB35174CA3714066D590DCD50994E404@usrymx25.merck.com>

Wrapping the suggestions into a function:

> sameLength <- function(...) {
+     n <- sapply(list(...), length)
+     all(n == n[1])
+ }
> sameLength(double(1), double(2))
[1] FALSE
> sameLength(double(1), double(1), list(x=1))
[1] TRUE

[Note that if you have lots of objects to compare, length(unique(...))==1
will not be as efficient.]

HTH,
Andy


> From: Manuel Gutierrez
> 
> I could not find any help pages on How to test many
> objects for being of equal length
> Something like identical for more than two objects?
> x<-1:6
> y<-1:10
> z<-3:5
> ## For two objects I can do:
> identical(length(x),length(y))
> ## For more than two I currently can do:
> length(unique(c(length(x),length(y),length(z))))==1
> 
> but there must be a better way.
> Thanks,
> M
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Thu Dec  9 16:24:34 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 9 Dec 2004 10:24:34 -0500
Subject: [R] US 2004 Elections map
Message-ID: <3A822319EB35174CA3714066D590DCD50994E405@usrymx25.merck.com>

Nice!  But what happen to Alaska & Hawaii? 8-)

Andy

> From: Hedderik van Rijn
> 
> even for people who didn't like the outcome of the US elections this  
> year, it must have been a joy to see all the nice (and not so nice)  
> graphs that were shown. As an exercise, I recreated the map shown on  
> the NY-Times website [  
> http://www.nytimes.com/packages/khtml/2004/11/03/politics/ 
> 20041103_px_ELECT_GRAPHIC.html ]
> 
> If you're interested,
> 
   http://www.ai.rug.nl/~hedderik/R/US2004

contains the created maps, the data and the code. Note that there are  
some differences in the R map and the NYT map (most notably with  
respect to some missing data in some New England states, some  
mismatches in county names, and the representation of NYC), but I was  
quite satisfied with the map as it stands now.

   - Hedderik.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ozric at web.de  Thu Dec  9 16:30:04 2004
From: ozric at web.de (Christian Schulz)
Date: Thu, 09 Dec 2004 16:30:04 +0100
Subject: [R] "sequence of factor settings"?
Message-ID: <41B86F7C.3050901@web.de>

Hi,

exist in any package a function which "work with sequences".

Easy example 10 factors with 2 levels, are if i'm
correct theoretical  2^10  "sequences" possible.

But i have more than 2 levels and perhaps more than 10 factors and 
interested for the top10 "sequence's"
with higehest freqs and wish to know which type of sequence it is.

Many thanks for a starting point
Christian



From michael.watson at bbsrc.ac.uk  Thu Dec  9 16:37:37 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 9 Dec 2004 15:37:37 -0000
Subject: [R] US 2004 Elections map
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E899C3@iahce2knas1.iah.bbsrc.reserved>

So, ahem, it would be pretty easy to swap the colors round and claim a
Kerry victory?
;)

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hedderik van Rijn
Sent: 09 December 2004 14:36
To: r-help at stat.math.ethz.ch
Subject: [R] US 2004 Elections map



even for people who didn't like the outcome of the US elections this  
year, it must have been a joy to see all the nice (and not so nice)  
graphs that were shown. As an exercise, I recreated the map shown on  
the NY-Times website [  
http://www.nytimes.com/packages/khtml/2004/11/03/politics/ 
20041103_px_ELECT_GRAPHIC.html ]

If you're interested,

   http://www.ai.rug.nl/~hedderik/R/US2004

contains the created maps, the data and the code. Note that there are  
some differences in the R map and the NYT map (most notably with  
respect to some missing data in some New England states, some  
mismatches in county names, and the representation of NYC), but I was  
quite satisfied with the map as it stands now.

   - Hedderik.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From isen at plantpath.wisc.edu  Thu Dec  9 16:41:00 2004
From: isen at plantpath.wisc.edu (Dr. Thomas Isenbarger)
Date: Thu, 09 Dec 2004 09:41:00 -0600
Subject: [R] more clustering questions
Message-ID: <BA1F5377-49F8-11D9-BA63-000A9598473A@plantpath.wisc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041209/7499b942/attachment.pl

From jfox at mcmaster.ca  Thu Dec  9 16:55:58 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 9 Dec 2004 10:55:58 -0500
Subject: [R] Re: Tetrachoric and polychoric correlations, Polycor package
In-Reply-To: <Pine.LNX.4.58.0412091619230.13524@orpheus.qimr.edu.au>
Message-ID: <20041209155557.QXXH1694.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear David,

Your code looks very similar to what's in the polycor package, although the
latter will also calculate quicker estimates with thresholds based on the
marginal distributions of the variables (and also polyserial correlations).
I may add WLS estimates to the sem package -- suggestions would be
appreciated.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Duffy
> Sent: Thursday, December 09, 2004 1:29 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Re: Tetrachoric and polychoric correlations, 
> Polycor package
> 
> A bit late, but you might like to look at
> 
> http://www.qimr.edu.au/davidD/polyr.R
> 
> Regarding the original posters queries:
> 
> You can analyse polychoric correlations as if they were 
> Pearson correlations using standard software (eg sem), and 
> this usually doesn't do too badly, or go to AWLS (Browne) in 
> LISREL etc, or ML analysis of the full multidimensional 
> contingency table using programs such as Mx, or as you noted, 
> mvtnorm (Mx uses Alan Genz's algorithms).
> 
> You can check model assumptions, and compare the results to 
> those from similar loglinear models.  For example, for a 
> 3-way table, a single factor model based on polychoric 
> correlations should fit "perfectly", if the no higher order 
> interaction assumption is right,
> 
> 
> | David Duffy (MBBS PhD)                                         ,-_|\
> | email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: 
> -0101  /     *
> | Epidemiology Unit, Queensland Institute of Medical Research 
>   \_,-._/
> | 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From francois.cochard at univ-tlse1.fr  Thu Dec  9 17:12:16 2004
From: francois.cochard at univ-tlse1.fr (=?iso-8859-1?Q?Fran=E7ois_Cochard?=)
Date: Thu, 9 Dec 2004 17:12:16 +0100
Subject: [R] How to test the significance of a value estimated with lme?
Message-ID: <006c01c4de09$d9fd8900$b8fefec2@cochardp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041209/5fee06f6/attachment.pl

From fm3a004 at math.uni-hamburg.de  Thu Dec  9 17:26:51 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Thu, 9 Dec 2004 17:26:51 +0100 (MET)
Subject: [R] more clustering questions
In-Reply-To: <BA1F5377-49F8-11D9-BA63-000A9598473A@plantpath.wisc.edu>
Message-ID: <Pine.GSO.3.95q.1041209171811.28707C-100000@sun11.math.uni-hamburg.de>

Dear Thomas,

the classical MDS tries to represent simultaneously
all distances as well as possible. It is based on something like a quadratic
loss function, and this means that the optimization concentrates
particularly on the adequate representation of the large distances. It seems
that the cmdscale result represents at least the 10s in the distance matrix
more properly than the solution you expected to obtain. This explains, for
example, why s5 is closer to s2 than to s1.

A more favourable method to represent the small distances properly is
Kruskal's nonmetric MDS, which is available as function isoMDS. 

Best,
Christian

On Thu, 9 Dec 2004, Dr. Thomas Isenbarger wrote:

> Sorry to bother you kind folks again with my questions.  I am trying to 
> learn as much as I can about all this, and I will admit that I don't 
> have the proper background, but I hope that someone can at least point 
> me in the correct direction.
> 
> I have created a test matrix for what I want to do:
> 
>     s1 s2 s3 s4 s5
> s1 10  5  0  8  7
> s2  5 10  0  0  5
> s3  0  0 10  0  0
> s4  8  0  0 10  0
> s5  7  5  0  0 10
> 
> this is a similarity matrix (lets call it "mini") i created to run some 
> tests.  thus, a self-against-self analysis gives a score of 10, and 
> lower scores denote lower degrees of similarity (8 denote two items 
> that are almost the same, etc).  s1 is closely related to s4 and s5, 
> but slightly more closely related to s4.  s2 is related similarly at 
> some medium level to s1 and s5.
> 
> i converted this into a dissimilarity matrix with R using
> 
> dissmini <- max(mini)-mini
> 
> this results in:
> 
>     s1 s2 s3 s4 s5
> s1  0  5 10  2  3
> s2  5  0 10 10  5
> s3 10 10  0 10 10
> s4  2 10 10  0 10
> s5  3  5 10 10  0
> 
> if I then do
> 
> plot(cmdscale(dissmini), type="n"); text(cmdscale(dissmini), 
> row.names(cmdscale(dissmini)))
> 
> I end up with a plot that shows (among other things) s2 and s5 very 
> close together, closer together than s1-s5 or s1-s2 or s1-s4.   This is 
> the opposite of what I would predict and what I want the plot to show.
> 
> If I instead use
> 
> plot(cmdscale(as.dist(dissmini)))
> 
> the plot is the same.
> 
> something like this:
> 
>                      s4
> 
>      s1
> 
> 
> s5
>   s2
> 
>                                                     s3
> 
> Thanks for your help,
> Tom Isenbarger
> 
> 
> 
> --
> isen at plantpath.wisc.edu
> thomas a isenbarger
> (608) 265-0850
> 
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From andy_liaw at merck.com  Thu Dec  9 17:27:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 9 Dec 2004 11:27:28 -0500
Subject: [R] "sequence of factor settings"?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E406@usrymx25.merck.com>

Not sure I completely understood what you want.  If you have factors f1, f2,
..., fk with number of levels n1, n2, ..., nk, then there are n1 * n2 * ...
* nk possible combinations (what you called sequences).  Are you looking for
the 10 combinations that appear most often in a data set?  If so, I would
concatenate the factors together into a character vector (each element would
be a combination), then table() that, and look for the top 10.  The names()
of the output will correspond to the combinations.

HTH,
Andy

> From: Christian Schulz
> 
> Hi,
> 
> exist in any package a function which "work with sequences".
> 
> Easy example 10 factors with 2 levels, are if i'm
> correct theoretical  2^10  "sequences" possible.
> 
> But i have more than 2 levels and perhaps more than 10 factors and 
> interested for the top10 "sequence's"
> with higehest freqs and wish to know which type of sequence it is.
> 
> Many thanks for a starting point
> Christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abu3ammar at gmail.com  Thu Dec  9 17:30:55 2004
From: abu3ammar at gmail.com (Omar Lakkis)
Date: Thu, 9 Dec 2004 11:30:55 -0500
Subject: [R] Finmetrics positions
Message-ID: <b1d31504041209083058fadc91@mail.gmail.com>

Finmetrics (in S-PLUS) has teh functions "positions" (return the
positions of an ordered data object). Is there an equivalent to it in
Remtrics?
I am applying it to teh data of a time series.



From judith.baltsar at gmx.de  Thu Dec  9 17:41:17 2004
From: judith.baltsar at gmx.de (judith.baltsar@gmx.de)
Date: Thu, 09 Dec 2004 17:41:17 +0100
Subject: [R] Scatterplot question
Message-ID: <41B88E3D.27792.704383@localhost>

Dear list members,
I have a probably simple question concerning scatterplots: I want to 
draw a plot with one X but several Y columns, so that every group 
of samples gets a different symbol. My table looks like this:

X	Y1	Y2	Y3
1	1
2	3
3	5
4		7
5		9
6		11
7			13
8			15

Simple in Excel or StarOffice, but how do I do it in R?

Thanks a lot

Judith



From drf5n at maplepark.com  Thu Dec  9 17:43:52 2004
From: drf5n at maplepark.com (David Forrest)
Date: Thu, 9 Dec 2004 10:43:52 -0600 (CST)
Subject: [R] US 2004 Elections map
In-Reply-To: <9F60740C-49EF-11D9-9844-000393D7CE9A@ai.rug.nl>
References: <9F60740C-49EF-11D9-9844-000393D7CE9A@ai.rug.nl>
Message-ID: <Pine.LNX.4.58.0412091038430.2728@maplepark.com>

On Thu, 9 Dec 2004, Hedderik van Rijn wrote:

>
> even for people who didn't like the outcome of the US elections this
> year, it must have been a joy to see all the nice (and not so nice)
> graphs that were shown. As an exercise, I recreated the map shown on
> the NY-Times website [
> http://www.nytimes.com/packages/khtml/2004/11/03/politics/
> 20041103_px_ELECT_GRAPHIC.html ]
>
> If you're interested,
>
>    http://www.ai.rug.nl/~hedderik/R/US2004
>
> contains the created maps, the data and the code. Note that there are
> some differences in the R map and the NYT map (most notably with
> respect to some missing data in some New England states, some
> mismatches in county names, and the representation of NYC), but I was
> quite satisfied with the map as it stands now.
>
>    - Hedderik.

Hi Hedderik,

I really like that -- Is there a place where R collects a portfolio of
graphics and the code to build them?  I think these sorts of things could
help demonstrate and disseminate the graphics abilities of R.

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From vito_ricci at yahoo.com  Thu Dec  9 18:03:12 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 9 Dec 2004 18:03:12 +0100 (CET)
Subject: [R] Scatterplot question
Message-ID: <20041209170312.25956.qmail@web41209.mail.yahoo.com>

Hi,

you can see: ?matplot

Best

Vito


you wrote:

Dear list members,
I have a probably simple question concerning
scatterplots: I want to 
draw a plot with one X but several Y columns, so that
every group 
of samples gets a different symbol. My table looks
like this:

X	Y1	Y2	Y3
1	1
2	3
3	5
4		7
5		9
6		11
7			13
8			15

Simple in Excel or StarOffice, but how do I do it in
R?

Thanks a lot

Judith

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From andy_liaw at merck.com  Thu Dec  9 18:05:07 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 9 Dec 2004 12:05:07 -0500
Subject: [R] Scatterplot question
Message-ID: <3A822319EB35174CA3714066D590DCD50994E407@usrymx25.merck.com>

I assume the blanks are actually NAs?  If so, try something like (say `df'
is the data frame):

matplot(df$X, df[,-1], pch=1:3)

or refer to the help page for matplot() for more options.  

Andy

> From: judith.baltsar at gmx.de
> 
> Dear list members,
> I have a probably simple question concerning scatterplots: I want to 
> draw a plot with one X but several Y columns, so that every group 
> of samples gets a different symbol. My table looks like this:
> 
> X	Y1	Y2	Y3
> 1	1
> 2	3
> 3	5
> 4		7
> 5		9
> 6		11
> 7			13
> 8			15
> 
> Simple in Excel or StarOffice, but how do I do it in R?
> 
> Thanks a lot
> 
> Judith
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abunn at whrc.org  Thu Dec  9 18:07:32 2004
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 9 Dec 2004 12:07:32 -0500
Subject: [R] Scatterplot question
In-Reply-To: <41B88E3D.27792.704383@localhost>
Message-ID: <NEBBIPHDAMMOKDKPOFFIOEHPCNAA.abunn@whrc.org>

Something like this will work:

foo.df <- data.frame(x = 1:8, y1 = c(1,3,5, NA, NA, NA, NA, NA),
                              y2 = c(NA, NA, NA, 7, 9, 11, NA, NA),
                              y3 = c(NA, NA, NA, NA, NA, NA, 13, 15))
plot(foo.df$x, foo.df$y1, ylim = c(0,20), type = "n")
points(foo.df$x, foo.df$y1, pch = 1)
points(foo.df$x, foo.df$y2, pch = 2)
points(foo.df$x, foo.df$y3, pch = 3)


HTH, Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
> judith.baltsar at gmx.de
> Sent: Thursday, December 09, 2004 11:41 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Scatterplot question
> 
> 
> Dear list members,
> I have a probably simple question concerning scatterplots: I want to 
> draw a plot with one X but several Y columns, so that every group 
> of samples gets a different symbol. My table looks like this:
> 
> X	Y1	Y2	Y3
> 1	1
> 2	3
> 3	5
> 4		7
> 5		9
> 6		11
> 7			13
> 8			15
> 
> Simple in Excel or StarOffice, but how do I do it in R?
> 
> Thanks a lot
> 
> Judith
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From mzp3769 at yahoo.com  Thu Dec  9 18:11:31 2004
From: mzp3769 at yahoo.com (m p)
Date: Thu, 9 Dec 2004 09:11:31 -0800 (PST)
Subject: [R] surf.ls
Message-ID: <20041209171131.19343.qmail@web51003.mail.yahoo.com>

Hello,
I am looking into description of surf.ls(spatial)
and see under value $beta - the coefficients.
When I use polynomial of degree 2 to fit surface
I expect to get 4 coefficients:

z = a_1 x^2 + a_2 xy + a_3 y^2 + a_4

What do beta really stand for and why do I get
$beta vector of length 6?

Thakns,
Mark



From sergei at stams.strath.ac.uk  Thu Dec  9 18:23:08 2004
From: sergei at stams.strath.ac.uk (Sergei Zuyev)
Date: Thu, 9 Dec 2004 17:23:08 +0000
Subject: [R] HTML help index generation problem with R under Windows
Message-ID: <200412091723.08225.sergei@stams.strath.ac.uk>

Hello,
I am wondering if there has been a solution to the following issue with R 
under Windows (see also thread to PR#6662 in this mailing list: 
http://tolstoy.newcastle.edu.au/R/devel/04a/0550.html )
I am using R for teaching students Statistics, so they are working with a 
university-wide installation of R. I have compiled an R-library which 
contains all the instructions and customised functions for the course and the 
students have been using it successfully under UNIX for the last 5 years. I 
keep the library in my own space which is pointed to in R_LIBS variable 
defined for the students. I myself not a sysadmin for the university network, 
so I cannot install to the system-wide library directory, but this was not a 
matter. This year, I have to (sniff-sniff) switch to Windows platform and the 
following problem arose.
The library loads fine and the text-style help files show OK, but help.start() 
reports the following error:
Error in file(f.tg, open = "w") : unable to open connection 
 In addition: Warning messages: 
 1: cannot update HTML package index in: make.packages.html(lib.loc) 
 2: cannot open file `C:\Progra~1\R\rw1071/doc/html/search/index.txt' 
The index-file is opening but does not contain links to my library.
 As far as I understand, the problem lies in the lack of write permissions for 
the system directory where R is installed. While UNIX version creates the 
html-help files in per-session way in the user's home directory, the windows 
version atempts to modify the system-wide file. 
Having possibility to display html-help would be a great benefit, as the 
students can follow the links present there which is impossible from within 
"inline" help. So my question:

Is there any work-around that problem? Would making creation of html help 
files in the users' space under Windows as under UNIX/linux resolve the 
issue? Or maybe, it's already been fixed in the recent version of R?

Thanks in advance for your help! Please, make a copy of your response to my 
email below.
-- 
=================================================================
                           Dr. Sergei ZUYEV
Statistics and Modelling Science dept., University of Strathclyde
    Livingston Tower, 26 Richmond str., Glasgow, G1 1XH, U.K.
     Tel.: +44 (0)141 548 3663    Fax:  +44 (0)141 552 2079
E-mail: sergei at stams.strath.ac.uk   http://www.stams.strath.ac.uk



From andy_liaw at merck.com  Thu Dec  9 18:23:39 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 9 Dec 2004 12:23:39 -0500
Subject: [R] a question about swap space, memory and read.table()
Message-ID: <3A822319EB35174CA3714066D590DCD50994E409@usrymx25.merck.com>

> From: Hu Chen
> 
> Hi all 
> Two computers:
> one is my desktop PC, windows2000, R 1.9.1. Physical RAM 256MB, Swap
> (Virtual memory)  384Mb. When I allocate a large matrix, it firstly
> uses up RAM, then use swap space. In windows' task manager, the usage
> of memory could exceed my physic RAM's size.
> The other machine is a remote server. Windows XP, R 1.9.1 
> Physical RAM 2GB.
> Swap space 4GB. I use "R --max-mem-size=4000M" to start R. However
> when I allocate a large matrix or data frame, it uses up all RAM then
> exits with a error message" cannot allocate vector of size 7812 Kb ".
> The Swap space is not used at all !

Please do read
http://cran.r-project.org/bin/windows/base/rw-FAQ.html#There-seems-to-be-a-l
imit-on-the-memory-it-uses_0021.

> What's more, I found that the read.table() function is really 
> a waste of memory.
> > ft <- read.table("filepath")
> > object.size(ft)
> object.size(ft)
> [1] 192000692
> only 192Mb.
> however, in the windows task manager it shows that this process takes
> nearly 800Mb memory.
> I used gc() to collect garbarge. Howerver it doesn't help.
> Any guys have methods to release the wasted memory?

This has been asked and answered on R-help many times (good candidate to add
to the FAQ?).  As an example, see:
http://tolstoy.newcastle.edu.au/R/help/04/06/1662.html

Andy



> thank you all.
> Regards
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tvdjobs at scitegic.com  Thu Dec  9 18:24:26 2004
From: tvdjobs at scitegic.com (TVD Jobs)
Date: Thu, 9 Dec 2004 09:24:26 -0800
Subject: [R] SVM and ROC plots
Message-ID: <830D8D4719112B418ABBC3A0EBA9581251342A@webmail.scitegic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041209/8b0b2b2f/attachment.pl

From p.dalgaard at biostat.ku.dk  Thu Dec  9 18:56:32 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Dec 2004 18:56:32 +0100
Subject: [R] surf.ls
In-Reply-To: <20041209171131.19343.qmail@web51003.mail.yahoo.com>
References: <20041209171131.19343.qmail@web51003.mail.yahoo.com>
Message-ID: <x2acsn9wz3.fsf@biostat.ku.dk>

m p <mzp3769 at yahoo.com> writes:

> Hello,
> I am looking into description of surf.ls(spatial)
> and see under value $beta - the coefficients.
> When I use polynomial of degree 2 to fit surface
> I expect to get 4 coefficients:
> 
> z = a_1 x^2 + a_2 xy + a_3 y^2 + a_4
> 
> What do beta really stand for and why do I get
> $beta vector of length 6?

Why do you expect a constant term but no linear terms? I'm too lazy to
verify, but it sounds like that would be your missing 2 coefficients.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Thu Dec  9 19:01:08 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 9 Dec 2004 18:01:08 +0000 (UTC)
Subject: [R] Scatterplot question
References: <41B88E3D.27792.704383@localhost>
Message-ID: <loom.20041209T185636-201@post.gmane.org>

 <judith.baltsar <at> gmx.de> writes:

: 
: Dear list members,
: I have a probably simple question concerning scatterplots: I want to 
: draw a plot with one X but several Y columns, so that every group 
: of samples gets a different symbol. My table looks like this:
: 
: X	Y1	Y2	Y3
: 1	1
: 2	3
: 3	5
: 4		7
: 5		9
: 6		11
: 7			13
: 8			15
: 
: Simple in Excel or StarOffice, but how do I do it in R?

If you already have this as a data frame or matrix then others
have already mentioned matplot.  If you are looking
for a quick way to get the data into that form in the first
place without manually padding it out with NAs then you could
represent them as time series and cbind them.  Assuming regularly
spaced time series you can use ts:

Y1 <- ts(c(1,3,5), start = 1)
Y2 <- ts(c(7,9,11), start = 4)
Y3 <- ts(c(13,15), start = 7)
Y <- cbind(Y1, Y2, Y3)

plot(Y, plot.type = "single", col = rainbow(3))
# or
matplot(1:8, Y)



From mzp3769 at yahoo.com  Thu Dec  9 19:03:47 2004
From: mzp3769 at yahoo.com (m p)
Date: Thu, 9 Dec 2004 10:03:47 -0800 (PST)
Subject: [R] surf.ls
In-Reply-To: <x2acsn9wz3.fsf@biostat.ku.dk>
Message-ID: <20041209180347.53812.qmail@web51005.mail.yahoo.com>

yes, that's true. I forgot about two linear terms.
Does anybody know what order the coefficients come
or where to find the answer?
M


--- Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> m p <mzp3769 at yahoo.com> writes:
> 
> > Hello,
> > I am looking into description of surf.ls(spatial)
> > and see under value $beta - the coefficients.
> > When I use polynomial of degree 2 to fit surface
> > I expect to get 4 coefficients:
> > 
> > z = a_1 x^2 + a_2 xy + a_3 y^2 + a_4
> > 
> > What do beta really stand for and why do I get
> > $beta vector of length 6?
> 
> Why do you expect a constant term but no linear
> terms? I'm too lazy to
> verify, but it sounds like that would be your
> missing 2 coefficients.
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej
> 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N 
>  
>  (*) \(*) -- University of Copenhagen   Denmark     
> Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)            
> FAX: (+45) 35327907
>



From das at cshl.edu  Thu Dec  9 19:14:35 2004
From: das at cshl.edu (Rajdeep Das)
Date: Thu, 9 Dec 2004 13:14:35 -0500
Subject: [R] SVM: quality of prediction
Message-ID: <000801c4de1a$f0ac34b0$6807308f@artney>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041209/dde66531/attachment.pl

From Roger.Bivand at nhh.no  Thu Dec  9 19:24:13 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 9 Dec 2004 19:24:13 +0100 (CET)
Subject: [R] surf.ls
In-Reply-To: <20041209171131.19343.qmail@web51003.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0412091916390.18125-100000@reclus.nhh.no>

On Thu, 9 Dec 2004, m p wrote:

> Hello,
> I am looking into description of surf.ls(spatial)
> and see under value $beta - the coefficients.
> When I use polynomial of degree 2 to fit surface
> I expect to get 4 coefficients:
> 
> z = a_1 x^2 + a_2 xy + a_3 y^2 + a_4
> 
> What do beta really stand for and why do I get
> $beta vector of length 6?

No, z = a_1 x^2 + a_2 xy + a_3 y^2 + a_4 + a_5 x + a_6 y,

if you like, order 2 is linear + quadratic, see p. 420 in MASS (4th 
edition), eq. 15.1:

f((x,y)) = \sum_{r+s \leq p} a_{rs} x^r y^s, 

with P = (p+1)(p+2)/2 coefficients for order p; for p=2, P=6.

> 
> Thakns,
> Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From abu3ammar at gmail.com  Thu Dec  9 19:39:59 2004
From: abu3ammar at gmail.com (Omar Lakkis)
Date: Thu, 9 Dec 2004 13:39:59 -0500
Subject: [R] POSIX
Message-ID: <b1d31504041209103941586251@mail.gmail.com>

if I have a variable of type "POSIXt"  "POSIXct" like "1969-12-31
19:00:01 EST" how can I dynamicly get the year (or years if vector)
and month?



From ripley at stats.ox.ac.uk  Thu Dec  9 21:59:12 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Thu, 9 Dec 2004 20:59:12 +0000 (GMT)
Subject: [R] POSIX
In-Reply-To: <b1d31504041209103941586251@mail.gmail.com>
Message-ID: <Pine.GSO.4.31.0412092058140.8469-100000@markov.stats>

On Thu, 9 Dec 2004, Omar Lakkis wrote:

> if I have a variable of type "POSIXt"  "POSIXct" like "1969-12-31
> 19:00:01 EST" how can I dynamicly get the year (or years if vector)
> and month?

See ?months, or any other official documentation on dates in R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MDavy at hortresearch.co.nz  Thu Dec  9 22:00:14 2004
From: MDavy at hortresearch.co.nz (Marcus Davy)
Date: Fri, 10 Dec 2004 10:00:14 +1300
Subject: [R] a question about swap space, memory and read.table()
Message-ID: <s1b973d2.041@hra2.marc.hort.cri.nz>


On a 32-bit windows (standard install or R) machine you cannot allocate more than 2Gigs
of memory,
"R --max-mem-size=2G"
or
"R --max-mem-size=2000M"

If you try to allocate more than 2G (eg 4000 Meg) I suspect it then defaults back to 1Gig. Check you memory limit with memory.limit(). There is lots of information about configuration in the windows FAQ, 
http://cran.r-project.org/bin/windows/base/rw-FAQ.html#There-seems-to-be-a-limit-on-the-memory-it-uses_0021

If you want 3Gigs per process you are going to have to modify the R executable to make it 
/LARGEADDRESSAWARE


marcus

>>> Hu Chen <chencheva at gmail.com> 10/12/2004 3:39:18 AM >>>
Hi all 
Two computers:
one is my desktop PC, windows2000, R 1.9.1. Physical RAM 256MB, Swap
(Virtual memory)  384Mb. When I allocate a large matrix, it firstly
uses up RAM, then use swap space. In windows' task manager, the usage
of memory could exceed my physic RAM's size.
The other machine is a remote server. Windows XP, R 1.9.1 Physical RAM 2GB.
Swap space 4GB. I use "R --max-mem-size=4000M" to start R. However
when I allocate a large matrix or data frame, it uses up all RAM then
exits with a error message" cannot allocate vector of size 7812 Kb ".
The Swap space is not used at all !
What's more, I found that the read.table() function is really a waste of memory.
> ft <- read.table("filepath")
> object.size(ft)
object.size(ft)
[1] 192000692
only 192Mb.
however, in the windows task manager it shows that this process takes
nearly 800Mb memory.
I used gc() to collect garbarge. Howerver it doesn't help.
Any guys have methods to release the wasted memory?
thank you all.
Regards

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From gcutler at amgen.com  Thu Dec  9 22:01:54 2004
From: gcutler at amgen.com (Gene Cutler)
Date: Thu, 9 Dec 2004 13:01:54 -0800
Subject: [R] Peak finding algorithm
In-Reply-To: <200412091117.iB9B6NsJ007188@hypatia.math.ethz.ch>
References: <200412091117.iB9B6NsJ007188@hypatia.math.ethz.ch>
Message-ID: <8E69CA3B-4A25-11D9-8F4C-000A95C91324@amgen.com>

I'm sure there must be various peak-finding algorithms out there.  Not 
knowing of any, I have written one myself*, but I thought I'd ask to 
see what's out there.

Basically, I have a 2-dimensional data set and I want to identify local 
peaks in the data, while ignoring "trivial" peaks.  My naive algorithm 
first identifies every peak and valley (point of inflection change in 
the graph), then shaves off shallow peaks and valleys based on an 
arbitrary depth parameter, then returns whatever is left.  This 
produces decent results, but, again, I'd like to know what other 
implementations are available.

(* source available on request)



From ross at biostat.ucsf.edu  Thu Dec  9 22:15:49 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 09 Dec 2004 13:15:49 -0800
Subject: [R] Calling R a la carte?
Message-ID: <1102626949.15089.211.camel@iron.libaux.ucsf.edu>

I have written an R package, mostly implemented in C++. It works.
I want to do unit tests of the C++ code, as well as higher level tests
from R.  I use the boost test framework for this, part of which is a
library which wants to be the thing that starts everything running
(i.e., main).

My code has a few invocations of R functions--just error at the moment,
I think.  The list might grow.  So I linked the test program with the R
library.

When I first tried this, my test program ended up attempting to start an
R session when I ran it.  I switched the order of the R library and the
boost test library, and all worked well (i.e., the boost test framework
got control, ran all the tests, and all passed).

That was on Linux.  I am now porting to Mac OS X, and am back to having
R start in place of my test suite (it only tries to start, because it
runs into problems).

Really I just want a little bit of R (hence the subject: R a la carte). 
The extensions document only talks about embedding a complete R session
in another program, and the same is true on previous posts I've found on
this list.

My questions:
1) Is this a good idea?  On reflection, if the error() function is ever
called it may require a context, and there will be none.  More
generally, I'm doing nothing to setup R (beyond setting environment
variables), and the library may just not be designed for that kind of
use.  Also, the test framework can cope with tests segfaulting.

2) Why is the R library taking control of my startup?  The extensions
document seems to imply that if I'm using the library I must
deliberately invoke functions from my main() to get R running.  I don't,
but it still runs.

3) What would be the best way to accomplish my goal?  I can think of
three approaches:
    a) basically continue with what I'm doing.  This obviously would
require some work around to the start up issues.  My test suite doesn't
hit any code that actually calls the R functions.
    b) Treat R as a real embedded application, setting it up correctly. 
I'm not sure if it is possible to do this while also triggering the test
suite.  In other words, the sequence would need to be
         initialize test framework
         start up R session
         perform tests
       The R session does not invoke the tests.
    c) Write a small library that fakes the R routines I need, and link
to it when testing.

Thanks.

P.S.  I originally saw this with R 1.9 on Linux, since upgraded to 2.0. 
On the Mac, it's 1.9.
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From RVARADHAN at JHMI.EDU  Thu Dec  9 22:34:33 2004
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Thu, 09 Dec 2004 16:34:33 -0500
Subject: [R] Processing and analysis of ECG signals
Message-ID: <0I8H005P159MOH@jhuml1.jhmi.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041209/74d5446e/attachment.pl

From mzp3769 at yahoo.com  Thu Dec  9 22:36:39 2004
From: mzp3769 at yahoo.com (m p)
Date: Thu, 9 Dec 2004 13:36:39 -0800 (PST)
Subject: [R] surf.ls
In-Reply-To: <Pine.LNX.4.44.0412091916390.18125-100000@reclus.nhh.no>
Message-ID: <20041209213639.28186.qmail@web51008.mail.yahoo.com>

My application of surf.ls is giving wrong results.
CAn somebody point to the problem?
Here is part of the code:

var is three columnd array filled with x,y,z values
x,y are equally spaced
zetaframe <- data.frame(x=var[,3],y=var[,2],z=var[,1])

#var is three columnd array filled with x,y,z values
#part of zetaframe looks like
2211 10.000000  7.995732 -1.4149990
2212 10.000000  8.495732 -1.5649990
2213 10.000000  8.995732 -1.7249990
2214 10.000000  9.495732 -1.8849990
2215 10.000000  9.995732 -2.0549990
2216 10.000000 10.495730 -2.2249980
2217 10.000000 10.995730 -2.3949980
2218 10.000000 11.495730 -2.5749980
2219 10.000000 11.995730 -2.7549980
2220 10.000000 12.495730 -2.9349980
2221 10.000000 12.995730 -3.1249980
2222 10.000000 13.495730 -3.3149970

zeta.kr <- surf.ls(2,
zetaframe$x,zetaframe$y,zetaframe$z)

#zeta.kr$beta =  
#-2.0909907  0.9969686 -0.5092430 -2.0151540 
0.5047758 #-0.2155143
#To plot  I fill an array with output from
#zeta.kr for same x,y grid points

k <- 0
for (i in 1:nx) {
        for (j in 1:ny) {
                k <- k+1
                xi <- var[k,3]
                yi <- var[k,2]
                zetasurf[i,j] <-
                zeta.kr$beta[1]*xi^2 +
                zeta.kr$beta[2]*xi*yi +
                zeta.kr$beta[3]*yi^2 +
                zeta.kr$beta[4] +
                zeta.kr$beta[5]*xi +
                zeta.kr$beta[6]*yi
        }
}


#When  I plot it with 

filled.contour(zetasurf,nlevels=11,
color.palette=rainbow,
xlab=xlabstring,ylab=ylabstring,cex.lab=1.2,cex.axis=1.2,
xaxs = "i", yaxs = "i", las = 1,
plot.axes={ axis(1); axis(2); points(10,10) })

# gives ridiculous results

Thanks,
MArk




--- Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Thu, 9 Dec 2004, m p wrote:
> 
> > Hello,
> > I am looking into description of surf.ls(spatial)
> > and see under value $beta - the coefficients.
> > When I use polynomial of degree 2 to fit surface
> > I expect to get 4 coefficients:
> > 
> > z = a_1 x^2 + a_2 xy + a_3 y^2 + a_4
> > 
> > What do beta really stand for and why do I get
> > $beta vector of length 6?
> 
> No, z = a_1 x^2 + a_2 xy + a_3 y^2 + a_4 + a_5 x +
> a_6 y,
> 
> if you like, order 2 is linear + quadratic, see p.
> 420 in MASS (4th 
> edition), eq. 15.1:
> 
> f((x,y)) = \sum_{r+s \leq p} a_{rs} x^r y^s, 
> 
> with P = (p+1)(p+2)/2 coefficients for order p; for
> p=2, P=6.
> 
> > 
> > Thakns,
> > Mark
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> Roger Bivand
> Economic Geography Section, Department of Economics,
> Norwegian School of
> Economics and Business Administration, Breiviksveien
> 40, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> e-mail: Roger.Bivand at nhh.no
> 
> 
>



From ggrothendieck at myway.com  Thu Dec  9 23:04:15 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 9 Dec 2004 22:04:15 +0000 (UTC)
Subject: [R] POSIX
References: <b1d31504041209103941586251@mail.gmail.com>
Message-ID: <loom.20041209T225501-337@post.gmane.org>

Omar Lakkis <abu3ammar <at> gmail.com> writes:

: 
: if I have a variable of type "POSIXt"  "POSIXct" like "1969-12-31
: 19:00:01 EST" how can I dynamicly get the year (or years if vector)
: and month?

Enter unclass(as.POSIXlt(x)) to see the components available to you
when using POSIXlt.  These include the year component which is the year
minus 1900 and the mon component which is the month number
noting that January is 0, February is 1, etc.  Also note that
the year and month can differ in different time zones so you may need
tz = "GMT" if you need it with respect to GMT time, say.

You can also use something like format(x, "%y-%m").  tz= comment
applies here too.  ?strptime lists the various % codes.

You may be better off using Date class if your granularity is year
and month or if you have regularly spaced monthly data you can use a ts 
series with frequency of 12.

The article in RNews 4/1 gives many more examples and pointers to
additional information.



From rkoenker at uiuc.edu  Thu Dec  9 23:12:47 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Thu, 9 Dec 2004 16:12:47 -0600
Subject: [R] Peak finding algorithm
In-Reply-To: <8E69CA3B-4A25-11D9-8F4C-000A95C91324@amgen.com>
References: <200412091117.iB9B6NsJ007188@hypatia.math.ethz.ch>
	<8E69CA3B-4A25-11D9-8F4C-000A95C91324@amgen.com>
Message-ID: <752B1D9E-4A2F-11D9-9CDE-000A95A7E3AA@uiuc.edu>

You might want to look at the ftnonpar package.  You haven't quite 
specified whether
you are thinking about estimating densities, or regression functions or 
some third
option, or whether 2-dimensional means: functions R -> R or functions 
R^2 -> R,
my recollection is that ftnonpar is (mostly?) about the R -> R case.

url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Dec 9, 2004, at 3:01 PM, Gene Cutler wrote:

> I'm sure there must be various peak-finding algorithms out there.  Not 
> knowing of any, I have written one myself*, but I thought I'd ask to 
> see what's out there.
>
> Basically, I have a 2-dimensional data set and I want to identify 
> local peaks in the data, while ignoring "trivial" peaks.  My naive 
> algorithm first identifies every peak and valley (point of inflection 
> change in the graph), then shaves off shallow peaks and valleys based 
> on an arbitrary depth parameter, then returns whatever is left.  This 
> produces decent results, but, again, I'd like to know what other 
> implementations are available.
>
> (* source available on request)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From lisa.pappas at hci.utah.edu  Thu Dec  9 23:14:46 2004
From: lisa.pappas at hci.utah.edu (Lisa Pappas)
Date: Thu, 9 Dec 2004 15:14:46 -0700
Subject: [R] finding the most frequent row
Message-ID: <F062093E456F8C4A9566C5CA59726C6E04518817@EMAIL.hci.utah.edu>

I am bootstrapping using a function that I have defined.  The "Statistic" of the function is an array of 10 numbers.  Therefore if I use 1000 replications,  the "t" matrix will have 1000 rows each of which is a bootstrap replicate of this 10 number array (10 columns).  Is there any easy way in R to determine which row appears the most frequently? 

Thanks,
Lisa Pappas

Huntsman Cancer Institute wishes to promote open communication while protecting confidential and/or privileged information.  If you have received this message in error, please inform the sender and delete all copies.



From nov_tao at yahoo.com  Thu Dec  9 23:20:53 2004
From: nov_tao at yahoo.com (Y. C. Tao)
Date: Thu, 9 Dec 2004 14:20:53 -0800 (PST)
Subject: [R] Results of nls
Message-ID: <20041209222053.43190.qmail@web54503.mail.yahoo.com>

This might be an easy question.

I tried to catch the results of nls, but to no avail.
For example, if I try to fit a modle y~A*x*x+B*x+C
nls will print out what A, B, and C, but how can I
store these numbers to a different variable so that I
can make use of them? I tried to "unclass" the result
of nls, but couldn't see any of the parameters.

Y C Tao



From jbang at uiuc.edu  Thu Dec  9 23:46:27 2004
From: jbang at uiuc.edu (Bang)
Date: Thu, 9 Dec 2004 16:46:27 -0600
Subject: [R] 2.01
Message-ID: <200412092246.iB9MkSpZ020428@expredir2.cites.uiuc.edu>

I recently installed the new base.

Code that I had been successfully using for a spatial model is now giving
the following message when it reads the "Y" variable in the command: 

y <- model.response(mf, "numeric")

The error it gives is: 

Error in model.response(mf, "numeric") : No direct or inherited method for
function "model.response" for this call

The code I've been using works in 1.9, not in the 2.  Is this an issue with
the program, and how can it be overcome?

Jim

James Bang
Department of Economics
University of Illinois 

Well I AM missing the back of my head.you COULD cut me a little slack!
-Homer Simpson



From p.dalgaard at biostat.ku.dk  Thu Dec  9 23:47:22 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Dec 2004 23:47:22 +0100
Subject: [R] Results of nls
In-Reply-To: <20041209222053.43190.qmail@web54503.mail.yahoo.com>
References: <20041209222053.43190.qmail@web54503.mail.yahoo.com>
Message-ID: <x2d5xjrsw5.fsf@biostat.ku.dk>

"Y. C. Tao" <nov_tao at yahoo.com> writes:

> This might be an easy question.
> 
> I tried to catch the results of nls, but to no avail.
> For example, if I try to fit a modle y~A*x*x+B*x+C
> nls will print out what A, B, and C, but how can I
> store these numbers to a different variable so that I
> can make use of them? I tried to "unclass" the result
> of nls, but couldn't see any of the parameters.

example(nls)
coef(fm3DNase1)

The coefficients *are* in the nls object, but it's a little
complicated: 

fm3DNase1$m$getPars()

which in turns picks out the stored parameters from it's own lexical
scope. If you want to "burglarize" it, try

get("internalPars",env=environment(fm3DNase1$m$getPars)) 

but in general you're supposed to use access functions like coef() so
that you don't need to know about the internals (they might change!).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ross at biostat.ucsf.edu  Thu Dec  9 23:48:48 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 09 Dec 2004 14:48:48 -0800
Subject: [R] Re: Calling R a la carte?
In-Reply-To: <1102626949.15089.211.camel@iron.libaux.ucsf.edu>
References: <1102626949.15089.211.camel@iron.libaux.ucsf.edu>
Message-ID: <1102632528.15089.241.camel@iron.libaux.ucsf.edu>

On Thu, 2004-12-09 at 13:15, Ross Boylan wrote:
> I have written an R package, mostly implemented in C++. It works.
> I want to do unit tests of the C++ code, as well as higher level tests
> from R.  I use the boost test framework for this, part of which is a
> library which wants to be the thing that starts everything running
> (i.e., main).
> 
> My code has a few invocations of R functions--just error at the moment,
> I think.  The list might grow.  So I linked the test program with the R
> library.
To clarify, the R functions I am calling are C functions, not higher
level ones.



From crevell at rice.edu  Thu Dec  9 23:59:13 2004
From: crevell at rice.edu (crevell@rice.edu)
Date: Thu, 9 Dec 2004 22:59:13 -0000
Subject: [R] Mixed effects model help
Message-ID: <20041209225913.DDBFE198A7@fungible3.mail.rice.edu>

I am attempting to set up a mixed effects model, with one factor fixed (put 
in as.factor, with three factor levels) and one factor random(put in 
as.factor, with four different batches) as well as the interactions.  I have 
tried the lme function, but R continues to crash everytime I run it...
My code is:
model_lme(y~a,random=~b+a:b)
Is this way off base?
I just need to get F statistics to do hypothesis testing.

Thanks,
Chris



From feferraz at ime.usp.br  Fri Dec 10 00:41:23 2004
From: feferraz at ime.usp.br (Fernando Henrique Ferraz P. da Rosa)
Date: Thu, 9 Dec 2004 21:41:23 -0200
Subject: [R] Mixed effects model help
In-Reply-To: <20041209225913.DDBFE198A7@fungible3.mail.rice.edu>
References: <20041209225913.DDBFE198A7@fungible3.mail.rice.edu>
Message-ID: <20041209234123.GA16667@ime.usp.br>


        This has come up less than a month ago:
https://stat.ethz.ch/pipermail/r-help/2004-November/060038.html

crevell at rice.edu writes:
> I am attempting to set up a mixed effects model, with one factor fixed (put 
> in as.factor, with three factor levels) and one factor random(put in 
> as.factor, with four different batches) as well as the interactions.  I have 
> tried the lme function, but R continues to crash everytime I run it...
> My code is:
> model_lme(y~a,random=~b+a:b)
> Is this way off base?
> I just need to get F statistics to do hypothesis testing.
> 
> Thanks,
> Chris
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
--
Fernando Henrique Ferraz P. da Rosa
http://www.ime.usp.br/~feferraz



From wuertz at itp.phys.ethz.ch  Fri Dec 10 00:43:48 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 09 Dec 2004 23:43:48 +0000
Subject: [R] Finmetrics positions
In-Reply-To: <b1d31504041209083058fadc91@mail.gmail.com>
References: <b1d31504041209083058fadc91@mail.gmail.com>
Message-ID: <41B8E334.3030707@itp.phys.ethz.ch>

Omar Lakkis wrote:

>Finmetrics (in S-PLUS) has teh functions "positions" (return the
>positions of an ordered data object). Is there an equivalent to it in
>Remtrics?
>I am applying it to teh data of a time series.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>
Yes,

have a look on the help page: seriesPositions() is the name of the function.

I used "seriesPositions()" like "seriesData()". This makes naming 
conventions consistent.

The help page tells you :

|seriesData|
|seriesPositions|
extract the |@Data| and |@position| slots from a |timeSeries| object. 
Thus, |seriesData|
returns an object of class |matrix|, and |seriesPositions| returns an 
object of class |timeDate|.


regards Diethelm Wuertz



From ok at cs.otago.ac.nz  Fri Dec 10 02:37:16 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 10 Dec 2004 14:37:16 +1300 (NZDT)
Subject: [R] Is  k  equivalent to  k:k ?
Message-ID: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>

In this discussion of seq(), can anyone explain to me _why_
seq(to=n) and seq(length=3) have different types?
In fact, it's worse than that (R2.0.1):

    > storage.mode(seq(length=0))
    [1] "integer"
    > storage.mode(seq(length=1))
    [1] "double"

If you want to pass seq(length=n) to a .C or .Fortran call,
it's not helpful that you can't tell what the type is until you know n!
It would be nice if seq(length=n) always returned the same type.
I use seq(length=n) often instead of 1:n because I'd like my code to
work when n == 0; it would make life simpler if seq(length=n) and 1:n
were the same type.

Can anyone explain to me why the arguments of seq.default are
"from", "to", "by", "length.out", "along.with"
                           ^^^^         ^^^^^
when the help page for seq documents them as
"from", "to", "by", "length", and "along"?



From ok at cs.otago.ac.nz  Fri Dec 10 03:15:11 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 10 Dec 2004 15:15:11 +1300 (NZDT)
Subject: [R] system() and file names with spaces
Message-ID: <200412100215.iBA2FB6g055341@atlas.otago.ac.nz>

Brian D Ripley <ripley at stats.ox.ac.uk> wrote:
	The normal way to do this is to quote the string, here filename.  See
	?shQuote.
	
	Your comments really are not fair:

My comments were to the effect that *I* had recommended an approach
that didn't quite work.  There's nothing unfair to R or the R team in that.

I tried to do something helpful about it.
My next comment was:

	> If there's already something like for.system() built into R, I'd be very
	> happy to know about it.

I don't see anything unfair about that either.  It's a clear suggestion
that I expected that there _was_ something like for.system() built into R.
What's unfair about hinting that I expect the problem is already solved?
I didn't say that R didn't *have* a solution, just that *I* didn't know about
it.

In retrospect, that WAS unfair.  It was unfair to me.
Because you see, I did go looking.

THERE IS NOTHING ABOUT shQuote in ?system or ?pipe.  Not in the text,
not in the examples, and not in the See Also section.

	> (It's a little odd that system() and pipe()
	> don't already support something like this;

This is the one and only comment which could in any way be taken as critical
of R or the R team.  I stand by my comment that

	> in a multi-element character
	> vector the first could be taken literally and the remaining ones could be
	> taken quoted with leading spaces.)

This would be

    old.system <- system
    system <- function (command, intern = FALSE, ignore.stderr = FALSE) {
	if (length(command > 1))
	    command <- do.call("paste", c(command[1],
                               lapply(com[2:length(command)], shQuote)))
	old.system(command, intern, ignore.stderr)
    }

or, more directly,

    system <- function (command, intern = FALSE, ignore.stderr = FALSE) {
	if (length(command > 1))
	    command <- do.call("paste", c(command[1],
			       lapply(com[2:length(command)], shQuote)))
	.Internal(system(if (ignore.stderr) paste(command, "2>/dev/null")
			 else command, intern))
    }

This would mean that something like
    system(c("mv", old.name, new.name))
would work _without_ the user having to remember to call shQuote.

This leads me to pipe().   In ?pipe we read

description: character. A description of the connection. For 'file' and
          'pipe' this is a path to the file to be opened. For 'url' it
          is a complete URL, including schemes ('http://', 'ftp://' or
          'file://').  'file' also accepts complete URLs. 

This should be

description: character. A description of the connection. For 'file' this
	  is a path to the file to be opened.  For 'pipe' it is the OS
	  command which is to be run.  For 'url' it is a complete URL,
          including schemes ('http://', 'ftp://' or 'file://').
	  'file' also accepts complete URLs. 


The 'See Also' section of ?pipe should include a paragraph:

	For 'pipe', see 'system' and 'shQuote'.

In the help page for 'system' the paragraph

     If 'intern' is 'FALSE' then the C function 'system' is used to
     invoke the command and the value returned by 'system' is the exit
     status of this function.

should be followed by a new paragraph:

     While your operating system may allow almost any string as a file
     name, the system command interpreter doesn't.  Some characters,
     such as spaces, quotation marks, and apostrophes, are likely to
     give you trouble.  You should only paste file names into a command
     directly when you are certain that they do not contain any unusual
     characters.  If they are valid R identifiers, you should have no
     trouble.  In general, you should quote file names using shQuote(),
     which knows what needs quoting for your system command interpreter
     and how to do that quoting.

Then in See Also:

     'shQuote' for quoting file names as and when necessary.

Then in Examples:

     file.name <- "What's On"
     quoted.name <- shQuote(file.name)
     system(paste("echo nothing >", quoted.name))
     system(paste("ls -l", quoted.name))
     system(paste("rm", quoted.name))

(This example has been tested.)

Had something like this already been in those help files, I would have
found shQuote when I went looking for it.



From andy_liaw at merck.com  Fri Dec 10 03:54:15 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 9 Dec 2004 21:54:15 -0500
Subject: [R] Peak finding algorithm
Message-ID: <3A822319EB35174CA3714066D590DCD50994E40A@usrymx25.merck.com>

If you are referring to something similar to LC-MS data, you might want to
take a look at the `mscalib' and/or `ppc' packages.  I haven't used those,
so don't know how relevant they are to what you have in mind.

[Searching for `peak' on Prof. Baron's R search site turned up those two
packages.]

Andy



> From: Gene Cutler
> 
> I'm sure there must be various peak-finding algorithms out 
> there.  Not 
> knowing of any, I have written one myself*, but I thought I'd ask to 
> see what's out there.
> 
> Basically, I have a 2-dimensional data set and I want to 
> identify local 
> peaks in the data, while ignoring "trivial" peaks.  My naive 
> algorithm 
> first identifies every peak and valley (point of inflection change in 
> the graph), then shaves off shallow peaks and valleys based on an 
> arbitrary depth parameter, then returns whatever is left.  This 
> produces decent results, but, again, I'd like to know what other 
> implementations are available.
> 
> (* source available on request)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Steve.Pawson at forestresearch.co.nz  Fri Dec 10 05:11:08 2004
From: Steve.Pawson at forestresearch.co.nz (Steve.Pawson@forestresearch.co.nz)
Date: Fri, 10 Dec 2004 17:11:08 +1300
Subject: [R] How to circumvent negative eigenvalues in the capscale function
Message-ID: <OF47533E1A.1B2180B9-ONCC256F66.00165FB0-CC256F66.0016FE25@forestresearch.co.nz>





Dear All

I am trying to do a partial canonical analysis of principal coordinates
using Bray-Curtis distances. The capscale addin to R appears to be the only
way of doing it, however, when I try and calculate a Bray-Curtis distance
matrix either using Capscale or Vegedist (capscale I understand uses
Vegedist anyway to calculate its distance matrix), R uses up all available
memory on the computer, stops and then comes back with errors regarding
negative eigenvalues.

I must admit to being a very very basic R user so this is starting to go
over my head. I tried using the distpcoa program of Legendre and Anderson
that can supposedly output a Bray-Curtis distance matrix corrected for the
problem of negative eigenvalues (i.e., trying to circumvent the first steps
in Capscale) but have had no success as my datamatrix is larger than what
their program can handle.

Just wondering if anyone can suggest a way of sorting what I am finding to
be a tricky little problem.

look forward to peoples thoughts.

Regards


Steve Pawson
PhD Student
School of Biological Sciences & School of Forestry, University of
Canterbury

Address: Forest Research Institute
P.O. Box 29237
Fendalton
Christchurch
Ph 03 3642949 Ext 7831


--------------------------------------------------------------------------------------------

This email is confidential and may be legally privileged.    If received in
error please destroy and immediately notify us.



From info at buyukmarket.net  Fri Dec 10 05:32:58 2004
From: info at buyukmarket.net (TANITIMDA SON NOKTA)
Date: Fri, 10 Dec 2004 06:32:58 +0200
Subject: [R] =?iso-8859-1?q?Sekt=F6r_Listesi!__Fri=2C_10_Dec_2004_06=3A32?=
	=?iso-8859-1?q?=3A58_+0200?=
Message-ID: <MC3-g4kTrmSKPHrnQ7J@mc3-info@buyukmarket.net>



From jarioksa at sun3.oulu.fi  Fri Dec 10 07:27:37 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 10 Dec 2004 08:27:37 +0200
Subject: [R] How to circumvent negative eigenvalues in the capscale
	function
In-Reply-To: <OF47533E1A.1B2180B9-ONCC256F66.00165FB0-CC256F66.0016FE25@forestresearch.co.nz>
References: <OF47533E1A.1B2180B9-ONCC256F66.00165FB0-CC256F66.0016FE25@forestresearch.co.nz>
Message-ID: <1102660056.9021.26.camel@biol102145.oulu.fi>

On Fri, 2004-12-10 at 06:11, Steve.Pawson at forestresearch.co.nz wrote:
> 

> I am trying to do a partial canonical analysis of principal coordinates
> using Bray-Curtis distances. The capscale addin to R appears to be the only
> way of doing it, however, when I try and calculate a Bray-Curtis distance
> matrix either using Capscale or Vegedist (capscale I understand uses
> Vegedist anyway to calculate its distance matrix), R uses up all available
> memory on the computer, stops and then comes back with errors regarding
> negative eigenvalues.
> 
The way to avoid negative eigenvalues is to use a ``positive
semidefinite'' dissimilarity matrix. This may sound cryptic. In simple
words: the underlying functions in capscale assume that your
dissimilarities are like (Euclidean) distances, meaning that the
shortest route between two points is a straight line, and you cannot
find a shorter route by going via a third point. This is possible with
Bray-Curtis index, and as its symptom, you get negative eigenvalues
(which are ignored in capscale, and only the dimensions with positive
eigenvalues are used). Were negative eigenvalues your problem, you could
avoid them by using another dissimilarity index with better metric
properties. Jaccard dissimilarity is rank-order similar to Bray-Curtis,
but it should be positive semidefinite.

However, I don't think think that negative eigenvalues and memory
problems are coupled. I guess that you simply have memory problems, and
negative eigenvalues are unrelated. So you need more memory or an
operating system with better memory handling. You may try with some
Linux live-cd (such as Quantian) where you can use R in Linux without
installing Linux in your hard drive.

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/



From Roger.Bivand at nhh.no  Fri Dec 10 08:45:49 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 10 Dec 2004 08:45:49 +0100 (CET)
Subject: [R] surf.ls
In-Reply-To: <20041209213639.28186.qmail@web51008.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0412100833100.18834-100000@reclus.nhh.no>

On Thu, 9 Dec 2004, m p wrote:

> My application of surf.ls is giving wrong results.
> CAn somebody point to the problem?

Your problem is of your own making: do read the book that this software 
supports, namely MASS, see ?surf.ls. I have already told you what it says 
about trend surface orders, now for some fairly standard trend surface 
numerical knowledge - the coefficients relate to coordinates that have 
been transformed to the -1,1 range, not to the input data range, because, 
when using say UTM coordinates typically for Northings in m north or south 
of the equator, y is in millions, y^2 is already very big indeed, and 
higher powers lead to meaningless fits. Try using lm() with UTM 
coordinates and you'll see what I mean. Prof. Ripley's code does this 
properly, and can be relied on to higher orders irrespective of the 
scaling of the coordinates. If you are in doubt, please read the source 
code. 

Beyond that, please do use the appropriate predict() function, which takes 
account of the change of range. Rolling your own can be good sometimes, 
but not unless you take account of all the underlying methods.

> Here is part of the code:
> 
> var is three columnd array filled with x,y,z values
> x,y are equally spaced
> zetaframe <- data.frame(x=var[,3],y=var[,2],z=var[,1])
> 
> #var is three columnd array filled with x,y,z values
> #part of zetaframe looks like
> 2211 10.000000  7.995732 -1.4149990
> 2212 10.000000  8.495732 -1.5649990
> 2213 10.000000  8.995732 -1.7249990
> 2214 10.000000  9.495732 -1.8849990
> 2215 10.000000  9.995732 -2.0549990
> 2216 10.000000 10.495730 -2.2249980
> 2217 10.000000 10.995730 -2.3949980
> 2218 10.000000 11.495730 -2.5749980
> 2219 10.000000 11.995730 -2.7549980
> 2220 10.000000 12.495730 -2.9349980
> 2221 10.000000 12.995730 -3.1249980
> 2222 10.000000 13.495730 -3.3149970
> 
> zeta.kr <- surf.ls(2,
> zetaframe$x,zetaframe$y,zetaframe$z)
> 
> #zeta.kr$beta =  
> #-2.0909907  0.9969686 -0.5092430 -2.0151540 
> 0.5047758 #-0.2155143
> #To plot  I fill an array with output from
> #zeta.kr for same x,y grid points
> 
> k <- 0
> for (i in 1:nx) {
>         for (j in 1:ny) {
>                 k <- k+1
>                 xi <- var[k,3]
>                 yi <- var[k,2]
>                 zetasurf[i,j] <-
>                 zeta.kr$beta[1]*xi^2 +
>                 zeta.kr$beta[2]*xi*yi +
>                 zeta.kr$beta[3]*yi^2 +
>                 zeta.kr$beta[4] +
>                 zeta.kr$beta[5]*xi +
>                 zeta.kr$beta[6]*yi
>         }
> }
> 
> 
> #When  I plot it with 
> 
> filled.contour(zetasurf,nlevels=11,
> color.palette=rainbow,
> xlab=xlabstring,ylab=ylabstring,cex.lab=1.2,cex.axis=1.2,
> xaxs = "i", yaxs = "i", las = 1,
> plot.axes={ axis(1); axis(2); points(10,10) })
> 
> # gives ridiculous results
> 
> Thanks,
> MArk
> 
> 
> 
> 
> --- Roger Bivand <Roger.Bivand at nhh.no> wrote:
> 
> > On Thu, 9 Dec 2004, m p wrote:
> > 
> > > Hello,
> > > I am looking into description of surf.ls(spatial)
> > > and see under value $beta - the coefficients.
> > > When I use polynomial of degree 2 to fit surface
> > > I expect to get 4 coefficients:
> > > 
> > > z = a_1 x^2 + a_2 xy + a_3 y^2 + a_4
> > > 
> > > What do beta really stand for and why do I get
> > > $beta vector of length 6?
> > 
> > No, z = a_1 x^2 + a_2 xy + a_3 y^2 + a_4 + a_5 x +
> > a_6 y,
> > 
> > if you like, order 2 is linear + quadratic, see p.
> > 420 in MASS (4th 
> > edition), eq. 15.1:
> > 
> > f((x,y)) = \sum_{r+s \leq p} a_{rs} x^r y^s, 
> > 
> > with P = (p+1)(p+2)/2 coefficients for order p; for
> > p=2, P=6.
> > 
> > > 
> > > Thakns,
> > > Mark
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > > 
> > 
> > -- 
> > Roger Bivand
> > Economic Geography Section, Department of Economics,
> > Norwegian School of
> > Economics and Business Administration, Breiviksveien
> > 40, N-5045 Bergen,
> > Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> > e-mail: Roger.Bivand at nhh.no
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From petr.pikal at precheza.cz  Fri Dec 10 09:00:34 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 10 Dec 2004 09:00:34 +0100
Subject: [R] Create a plot legend in a new window
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E899BE@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41B965B2.14597.490E54@localhost>

Hi Michael

I cannot find other option than 

1.	to go through legend code and modify it according to your 
wish (especially drawing parts)

2.	to use rect and text for making your own legend 

3. 	experiment with legend settins to produce what you want

Cheers
Petr


On 9 Dec 2004 at 13:57, michael watson (IAH-C) wrote:

> Following on from this, what I want is to create a new window and fill
> up the entire window with my legend.
> 
> I have tried:
> 
> plot.new()
> par(mar=c(0,0,0,0))
> legend()
> 
> But that still puts legend wherever I specify x and y.  So after I
> have set mar to c(0,0,0,0), how do I tell R to make the legend fill
> the entire window, starting at the top-left?
> 
> Thanks
> Mick
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
> (IAH-C) Sent: 09 December 2004 11:52 To: R-help at stat.math.ethz.ch
> Subject: [R] Create a plot legend in a new window
> 
> 
> Hi
> 
> I have a complicated plot which has a potentially large legend.  What
> I want to do is actually create the legend in a new window.  Has
> anyone done this before?  I'd like to be able to create a window with
> just the legend in it, and have it so the window is just the right
> size etc.  I'm sure someone must have done this already?  If not, any
> tips would be welcome.
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From maechler at stat.math.ethz.ch  Fri Dec 10 09:13:57 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 10 Dec 2004 09:13:57 +0100
Subject: [R] Mixed effects model help
In-Reply-To: <20041209225913.DDBFE198A7@fungible3.mail.rice.edu>
References: <20041209225913.DDBFE198A7@fungible3.mail.rice.edu>
Message-ID: <16825.23237.129499.840389@gargle.gargle.HOWL>

>>>>> "Chris" ==   <crevell at rice.edu>
>>>>>     on Thu, 9 Dec 2004 22:59:13 -0000 writes:

    Chris> I am attempting to set up a mixed effects model, with
    Chris> one factor fixed (put in as.factor, with three factor
    Chris> levels) and one factor random(put in as.factor, with
    Chris> four different batches) as well as the interactions.
    Chris> I have tried the lme function, but R continues to
    Chris> crash everytime I run it...  My code is:

    Chris> model_lme(y~a,random=~b+a:b) 

    Chris> Is this way off base?  

well, yes: if this is your code and you don't get an error or a
strong warning, your version of R is so much outdated, that you
*really* should upgrade.

[The use of "_" as assignment operator is ``prehistoric''.
 It's still valid in some Jurassic implementations of the S language...]

    Chris> I just need to get F statistics to do hypothesis testing.

Martin



From ozric at web.de  Fri Dec 10 09:24:15 2004
From: ozric at web.de (Christian Schulz)
Date: Fri, 10 Dec 2004 09:24:15 +0100
Subject: [R] "sequence of factor settings"?
In-Reply-To: <Pine.LNX.4.60.0412090828290.18806@springer.berkeley.edu>
References: <41B86F7C.3050901@web.de>
	<Pine.LNX.4.60.0412090828290.18806@springer.berkeley.edu>
Message-ID: <41B95D2F.5050406@web.de>

Phil and Andy,

many thanks for your help's!
regrads,christian


Phil Spector wrote:

> Christian -
>      Perhaps this is what you are looking for:
>
>  > dat = NULL
>  > for(i in 1:8)dat = cbind(dat,sample(1:5,size=100000,replace=TRUE))
>  > dat = data.frame(dat)
>  > tt = table(dat)
>  > tt1 = data.frame(tt)
>  > tt1 = tt1[order(tt1[,9],decreasing=TRUE),]
>  > tt1[1:10,]
>
>      Basically, converting the result from table() to a data frame
> produces a data frame with one column for each factor, and a final
> column with the counts.  Reording the data frame by that final column
> orders the sequences by their frequencies.
>
>      Hope this helps!
>
>                                        - Phil Spector
>                      Statistical Computing Facility
>                      Department of Statistics
>                      UC Berkeley
>                      spector at stat.berkeley.edu
>
>
> On Thu, 9 Dec 2004, Christian Schulz wrote:
>
>> Hi,
>>
>> exist in any package a function which "work with sequences".
>>
>> Easy example 10 factors with 2 levels, are if i'm
>> correct theoretical  2^10  "sequences" possible.
>>
>> But i have more than 2 levels and perhaps more than 10 factors and 
>> interested for the top10 "sequence's"
>> with higehest freqs and wish to know which type of sequence it is.
>>
>> Many thanks for a starting point
>> Christian
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From maechler at stat.math.ethz.ch  Fri Dec 10 09:34:11 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 10 Dec 2004 09:34:11 +0100
Subject: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>
References: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>
Message-ID: <16825.24451.885812.429876@gargle.gargle.HOWL>

I'm diverting to R-devel, where this is really more
appropriate.  Here (R-help) only a shorter version:

>>>>> "RichOK" == Richard A O'Keefe <ok at cs.otago.ac.nz>
>>>>>     on Fri, 10 Dec 2004 14:37:16 +1300 (NZDT) writes:

    RichOK> In this discussion of seq(), can anyone explain to
    RichOK> me _why_ seq(to=n) and seq(length=3) have different
    RichOK> types?  

well, the explantion isn't hard:  look at  seq.default  :-)

    RichOK> In fact, it's worse than that (R2.0.1):

    >> storage.mode(seq(length=0))
    RichOK>     [1] "integer"
    >> storage.mode(seq(length=1))
    RichOK>     [1] "double"

  { str(.) is shorter than  storage.mode(.) }

    RichOK> If you want to pass seq(length=n) to a .C or
    RichOK> .Fortran call, it's not helpful that you can't tell
    RichOK> what the type is until you know n!  It would be nice
    RichOK> if seq(length=n) always returned the same type.  I
    RichOK> use seq(length=n) often instead of 1:n because I'd
    RichOK> like my code to work when n == 0; it would make life
    RichOK> simpler if seq(length=n) and 1:n were the same type.

now if that really makes your *life* simpler, what does that
tell us about your life  ;-) :-)

For more on this, see the "R-devel" list to which this has been diverted.

Martin Maechler, ETH Zurich



From michael.watson at bbsrc.ac.uk  Fri Dec 10 10:32:54 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 10 Dec 2004 09:32:54 -0000
Subject: [R] Create a plot legend in a new window
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E899C9@iahce2knas1.iah.bbsrc.reserved>

Just for the records (and if anyone ever wants to find the "solution"),
I solved my own problem (well sort of) through imaginative use of the
barplot command:

>
barplot(rep(1,4),horiz=TRUE,names.arg=rev(c("this","is","my","legend")),
col=rev(rainbow(4)), axes=FALSE,las=2)

Produces a very nice, re-sizeable legend in a new window :-D

Mick

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
(IAH-C)
Sent: 09 December 2004 13:57
To: R-help at stat.math.ethz.ch
Subject: RE: [R] Create a plot legend in a new window


Following on from this, what I want is to create a new window and fill
up the entire window with my legend.

I have tried:

plot.new()
par(mar=c(0,0,0,0))
legend()

But that still puts legend wherever I specify x and y.  So after I have
set mar to c(0,0,0,0), how do I tell R to make the legend fill the
entire window, starting at the top-left?

Thanks
Mick

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
(IAH-C)
Sent: 09 December 2004 11:52
To: R-help at stat.math.ethz.ch
Subject: [R] Create a plot legend in a new window


Hi

I have a complicated plot which has a potentially large legend.  What I
want to do is actually create the legend in a new window.  Has anyone
done this before?  I'd like to be able to create a window with just the
legend in it, and have it so the window is just the right size etc.  I'm
sure someone must have done this already?  If not, any tips would be
welcome.

Thanks
Mick

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From thpe at hhbio.wasser.tu-dresden.de  Fri Dec 10 10:41:31 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Fri, 10 Dec 2004 10:41:31 +0100
Subject: [R] lattice graphics empty in Sweave
Message-ID: <41B96F4B.9050206@hhbio.wasser.tu-dresden.de>

Hello,

I have problems using lattice graphics together with Sweave under 
Windows XP and R 2.0.0; 2.0.1 Patched; 2.1.0 devel respectively.
Base graphics are o.k., but for lattice graphics empty files (.eps and 
.pdf) are generated.

The only workaround I found was to write and read the respective figures 
explicitely using postscript() and includegraphics{}.

Have I missed something to solve this in the canonical way?

Thomas P.

A minimal example:


=== lattice.Rnw======================================================
\documentclass[a4paper, dvips]{article}
\begin{document}

<<fig=TRUE>>=
   library(lattice)
   data(iris)
   xyplot(Sepal.Length~Sepal.Width|Species, data=iris)
@

\end{document}
=====================================================================


in R:

 > Stangle("lattice.Rnw")
 > Sweave("lattice.Rnw")
 > system("latex lattice")
 > system("dvips lattice")



From feferraz at ime.usp.br  Fri Dec 10 11:09:06 2004
From: feferraz at ime.usp.br (Fernando Henrique Ferraz P. da Rosa)
Date: Fri, 10 Dec 2004 08:09:06 -0200
Subject: [R] lattice graphics empty in Sweave
In-Reply-To: <41B96F4B.9050206@hhbio.wasser.tu-dresden.de>
References: <41B96F4B.9050206@hhbio.wasser.tu-dresden.de>
Message-ID: <20041210100906.GA18713@ime.usp.br>

Thomas Petzoldt writes:
> === lattice.Rnw======================================================
> \documentclass[a4paper, dvips]{article}
> \begin{document}
> 
> <<fig=TRUE>>=
>   library(lattice)
>   data(iris)
>   xyplot(Sepal.Length~Sepal.Width|Species, data=iris)
> @
> 
> \end{document}
> =====================================================================
> 

        This is addressed in Sweave's and R's FAQ.

http://www.ci.tuwien.ac.at/~leisch/Sweave/FAQ.html#x1-8000A.6
http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-do-lattice_002ftrellis-graphics-not-work_003f



--
Fernando Henrique Ferraz P. da Rosa
http://www.ime.usp.br/~feferraz



From dieter.menne at menne-biomed.de  Fri Dec 10 11:11:01 2004
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 10 Dec 2004 11:11:01 +0100
Subject: [R] nlme and pnlsTol
Message-ID: <INEGIMHGODBGKFPOJBBMEELHCCAA.dieter.menne@menne-biomed.de>

Dear nlme-lovers,

I do a fit fo a 3-parameter fit to physiological data with nlme:

EmptD<-deriv(~(vol+slope*t)*exp(-t/tempt)...

The approach described in Pinheiro/Bates by using nlsList
as a first approximator was somewhat unstable (many NA), but
a direct fit converges quickly and fits look good:

contr=nlmeControl(pnlsTol=0.1)
v.nlme=nlme(v~EmptD(t,vol,slope,tempt),data=acg,
          verbose=F,control=contr,
          fixed=list(tempt+slope~treat+what,vol~1),
          random=vol+tempt+slope~1,
          start=c(150,-50,-50,-5,3,5,14,3,645))

However, I had to use a rather large value of pnlsTol of 0.1 go achieve
convergence.

Questions:

1) What is the price I have to pay when using so large pnlsTol? Could I miss
any more distant optima?

2) When using an even larger value of pnlsTol=0.3, AIC is lower by 2 and
standard errors are somewhat lower. Can I trust this result?

Dieter Menne



From lami at faunalia.it  Fri Dec 10 11:43:43 2004
From: lami at faunalia.it (Leonardo Lami)
Date: Fri, 10 Dec 2004 11:43:43 +0100
Subject: [R] bar charts
Message-ID: <200412101143.44128.lami@faunalia.it>

Hi,
I'd like plot a bar graphic of a array.
I'd like to have the values of the single columns as juxtaposed bars (like in 
"barplot" with beside=TRUE) and I'd like to have the values of the single 
columns expressed  like percentage of the sum of the coloumn.
In this way all the bars are composed by the sum of the percentage of the 
values of a coloumn (with different colors) and all they have 100% value. 

Thanks
Leonardo Lami

-- 
Leonardo Lami
lami at faunalia.it            www.faunalia.it
Via Colombo 3 - 51010 Massa e Cozzile (PT), Italy   Tel: (+39)349-1310164
GPG key @: hkp://wwwkeys.pgp.net http://www.pgp.net/wwwkeys.html
https://www.biglumber.com



From lami at faunalia.it  Fri Dec 10 11:59:57 2004
From: lami at faunalia.it (Leonardo Lami)
Date: Fri, 10 Dec 2004 11:59:57 +0100
Subject: [R] Bar chart
Message-ID: <200412101159.57034.lami@faunalia.it>

Hi,
I'd like plot a bar graphic of a array.
I'd like to have the values of the single columns as juxtaposed bars (like in 
"barplot" with beside=TRUE) and I'd like to have the values of the single 
columns expressed  like percentage of the sum of the coloumn.
In this way all the bars are composed by the sum of the percentage of the 
values of a coloumn (with different colors) and all they have 100% value. 

Thanks
Leonardo Lami

-- 
Leonardo Lami
lami at faunalia.it            www.faunalia.it
Via Colombo 3 - 51010 Massa e Cozzile (PT), Italy   Tel: (+39)349-1310164
GPG key @: hkp://wwwkeys.pgp.net http://www.pgp.net/wwwkeys.html
https://www.biglumber.com



From Roger.Bivand at nhh.no  Fri Dec 10 12:14:48 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 10 Dec 2004 12:14:48 +0100 (CET)
Subject: [R] bar charts
In-Reply-To: <200412101143.44128.lami@faunalia.it>
Message-ID: <Pine.LNX.4.44.0412101210590.18937-100000@reclus.nhh.no>

On Fri, 10 Dec 2004, Leonardo Lami wrote:

> Hi,
> I'd like plot a bar graphic of a array.
> I'd like to have the values of the single columns as juxtaposed bars (like in 
> "barplot" with beside=TRUE) and I'd like to have the values of the single 
> columns expressed  like percentage of the sum of the coloumn.
> In this way all the bars are composed by the sum of the percentage of the 
> values of a coloumn (with different colors) and all they have 100% value. 

If I understand you correctly, something like:

> tab <- xtabs(breaks ~ tension + wool, data = warpbreaks)
> barplot(tab, beside=TRUE)
> colSums(tab)
  A   B 
838 682 

is what you have, and what you want is:

> barplot(apply(tab, 2, function(x) (100*x)/sum(x)), beside=TRUE)
> colSums(apply(tab, 2, function(x) (100*x)/sum(x)))
  A   B 
100 100 

You'll need a good explanatory caption, though, when the column sums are 
different.

> 
> Thanks
> Leonardo Lami
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From michael.watson at bbsrc.ac.uk  Fri Dec 10 12:47:18 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 10 Dec 2004 11:47:18 -0000
Subject: [R] Returning to normal after call to layout()
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E899D1@iahce2knas1.iah.bbsrc.reserved>

Hi

I have a function which draws a plot, using the layout() function to
divide the screen into two parts.  The function works fine, but then my
next call to plot() draws the plot in the first section of the plot I've
just drawn using layout() - whereas what I want it to do is create a new
plot.

I tried using dev.off() but that just closes the layout plot window,
which is not what I want.  So my question is - after using the layout()
function, "layout(matrix(c(1,2), 2, 1))", how do I tell R that the next
call to a plotting function should use a new window, and not the layout
window?  I've looked in ?layout but there doesn't seem to be anything in
there.

Thanks
Mick



From puneetsingh77 at yahoo.com  Fri Dec 10 12:54:15 2004
From: puneetsingh77 at yahoo.com (Puneet Singh)
Date: Fri, 10 Dec 2004 03:54:15 -0800 (PST)
Subject: [R] Porting optimisation setup from Excel Solver to R
Message-ID: <20041210115415.54419.qmail@web50509.mail.yahoo.com>

Hi all,

I am currently optimising a small portfolio I have
created as a part of my research project in Excel. I
am unable to find the appropriate package to port this
into R. My problem set up is as follows

Minimise ABS(Sum(Xi-Xi')+10*Sum(XiMi)/Mavg)

Subject to:
0 <= Xi <= 0.05
ABS(Sum(Xi)) = 0.2

where
Mi - Market Cap of Stock i
Xi - Initial weight of Stock i
Xi' - New weight of Stock i
Mavg = Average weighted market cap of portfolio.

My portfolio has a long as well as a short side,
therefore the ABS. The minimisation function is
basically a penalty on the change from initial weight
and the distance from the Average market cap. 

My problem is that I need to optimise a vector X with
the weights instead of one weight at a time... i.e. I
need to be able to change all the stocks in the
portfolio simultaneously. I am able to do this EASILY
in Excel, but have not found any straightforward
application/formulation in R.

Any help would be much appreciated.

Regards
Puneet Singh
MBA Student
Indian Institute of Management Calcutta, India


		
__________________________________ 

Send a seasonal email greeting and help others. Do good.



From petr.pikal at precheza.cz  Fri Dec 10 13:12:17 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 10 Dec 2004 13:12:17 +0100
Subject: [R] Returning to normal after call to layout()
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E899D1@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41B9A0B1.7589.12FADD2@localhost>



On 10 Dec 2004 at 11:47, michael watson (IAH-C) wrote:

> Hi
> 
> I have a function which draws a plot, using the layout() function to
> divide the screen into two parts.  The function works fine, but then
> my next call to plot() draws the plot in the first section of the plot
> I've just drawn using layout() - whereas what I want it to do is
> create a new plot.
> 
> I tried using dev.off() but that just closes the layout plot window,
> which is not what I want.  So my question is - after using the
> layout() function, "layout(matrix(c(1,2), 2, 1))", how do I tell R

Hi

what is wrong on this:

layout(matrix(c(1,2), 2, 1))
plot(1,1)
plot(1,2)
windows() #opens the new graphic device see ?device, ?windows
plot(1,3)

BTW did you read my previous post to your question about legend 
on a new window?

Cheers
Petr


> that the next call to a plotting function should use a new window, and
> not the layout window?  I've looked in ?layout but there doesn't seem
> to be anything in there.
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From michael.watson at bbsrc.ac.uk  Fri Dec 10 13:20:00 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 10 Dec 2004 12:20:00 -0000
Subject: [R] Returning to normal after call to layout()
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E899D5@iahce2knas1.iah.bbsrc.reserved>

Hi Petr

Yes thanks I did get your response and it was helpful.  However, I have
now resorted to using barplot() to draw a legend (as all I wanted was
colored boxes and text) and it works very well for my needs.

Basically, with layout() I'm not getting what I would expect, which may
be a problem with my expectation rather than anything else.  Like I say,
I have a function which draws a plot, using layout to divide the
plotting region into top and bottom.  Fine.  If the user calls my
function, and then another plotting function, I would expect the new
plotting function to overwrite the whole of the old plot - but it
doesn't, it draws the new plot in the top half of the old plot, and
clears the bottom plot.  So what I want is something which says, after
calling layout(), "OK, I'm done plotting in two different regions, I
want you to treat the current device as a single region again".

In a previous package I wrote, when using split.screen() to divide up
the plotting region, dev.off() could be used for this purpose.  But
dev.off() closes the window/plot that I created using layout().

Mick

-----Original Message-----
From: Petr Pikal [mailto:petr.pikal at precheza.cz] 
Sent: 10 December 2004 12:12
To: michael watson (IAH-C)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Returning to normal after call to layout()




On 10 Dec 2004 at 11:47, michael watson (IAH-C) wrote:

> Hi
> 
> I have a function which draws a plot, using the layout() function to 
> divide the screen into two parts.  The function works fine, but then 
> my next call to plot() draws the plot in the first section of the plot

> I've just drawn using layout() - whereas what I want it to do is 
> create a new plot.
> 
> I tried using dev.off() but that just closes the layout plot window, 
> which is not what I want.  So my question is - after using the
> layout() function, "layout(matrix(c(1,2), 2, 1))", how do I tell R

Hi

what is wrong on this:

layout(matrix(c(1,2), 2, 1))
plot(1,1)
plot(1,2)
windows() #opens the new graphic device see ?device, ?windows
plot(1,3)

BTW did you read my previous post to your question about legend 
on a new window?

Cheers
Petr


> that the next call to a plotting function should use a new window, and

> not the layout window?  I've looked in ?layout but there doesn't seem 
> to be anything in there.
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Fri Dec 10 13:29:32 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 10 Dec 2004 13:29:32 +0100
Subject: [R] Returning to normal after call to layout()
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E899D5@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41B9A4BC.1250.13F7762@localhost>



On 10 Dec 2004 at 12:20, michael watson (IAH-C) wrote:

> Hi Petr
> 
> Yes thanks I did get your response and it was helpful.  However, I
> have now resorted to using barplot() to draw a legend (as all I wanted
> was colored boxes and text) and it works very well for my needs.
> 
> Basically, with layout() I'm not getting what I would expect, which
> may be a problem with my expectation rather than anything else.  Like
> I say, I have a function which draws a plot, using layout to divide
> the plotting region into top and bottom.  Fine.  If the user calls my
> function, and then another plotting function, I would expect the new
> plotting function to overwrite the whole of the old plot - but it
> doesn't, it draws the new plot in the top half of the old plot, and
> clears the bottom plot.  So what I want is something which says, after
> calling layout(), "OK, I'm done plotting in two different regions, I
> want you to treat the current device as a single region again".
> 
> In a previous package I wrote, when using split.screen() to divide up
> the plotting region, dev.off() could be used for this purpose.  But
> dev.off() closes the window/plot that I created using layout().

Hi

from the layout help page example (shortened)

> def.par <- par(no.readonly = TRUE)
> layout(matrix(c(1,2), 2, 1))
> plot(1,3)
> plot(1,2)
> par(def.par)
> plot(1,1)
>
suppose to be exactly what you want

Cheers
Petr


> 
> Mick
> 
> -----Original Message-----
> From: Petr Pikal [mailto:petr.pikal at precheza.cz] 
> Sent: 10 December 2004 12:12
> To: michael watson (IAH-C)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Returning to normal after call to layout()
> 
> 
> 
> 
> On 10 Dec 2004 at 11:47, michael watson (IAH-C) wrote:
> 
> > Hi
> > 
> > I have a function which draws a plot, using the layout() function to
> > divide the screen into two parts.  The function works fine, but then
> > my next call to plot() draws the plot in the first section of the
> > plot
> 
> > I've just drawn using layout() - whereas what I want it to do is
> > create a new plot.
> > 
> > I tried using dev.off() but that just closes the layout plot window,
> > which is not what I want.  So my question is - after using the
> > layout() function, "layout(matrix(c(1,2), 2, 1))", how do I tell R
> 
> Hi
> 
> what is wrong on this:
> 
> layout(matrix(c(1,2), 2, 1))
> plot(1,1)
> plot(1,2)
> windows() #opens the new graphic device see ?device, ?windows
> plot(1,3)
> 
> BTW did you read my previous post to your question about legend 
> on a new window?
> 
> Cheers
> Petr
> 
> 
> > that the next call to a plotting function should use a new window,
> > and
> 
> > not the layout window?  I've looked in ?layout but there doesn't
> > seem to be anything in there.
> > 
> > Thanks
> > Mick
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Fri Dec 10 13:42:46 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Fri, 10 Dec 2004 12:42:46 +0000 (GMT)
Subject: [R] Porting optimisation setup from Excel Solver to R
In-Reply-To: <20041210115415.54419.qmail@web50509.mail.yahoo.com>
Message-ID: <Pine.GSO.4.31.0412101229050.21886-100000@markov.stats>

On Fri, 10 Dec 2004, Puneet Singh wrote:

> Hi all,
>
> I am currently optimising a small portfolio I have
> created as a part of my research project in Excel. I
> am unable to find the appropriate package to port this
> into R. My problem set up is as follows
>
> Minimise ABS(Sum(Xi-Xi')+10*Sum(XiMi)/Mavg)
>
> Subject to:
> 0 <= Xi <= 0.05
> ABS(Sum(Xi)) = 0.2

But Sum(Xi) >= 0, so the ABS is unneeded.
So I presume the minimization is over (Xi), but you have not said.

> where
> Mi - Market Cap of Stock i
> Xi - Initial weight of Stock i
> Xi' - New weight of Stock i
> Mavg = Average weighted market cap of portfolio.

I suspect you have the meanings of Xi and Xi' reversed here.

> My portfolio has a long as well as a short side,
> therefore the ABS. The minimisation function is
> basically a penalty on the change from initial weight
> and the distance from the Average market cap.

That's quite a long way from the formula you gave us.

> My problem is that I need to optimise a vector X with
> the weights instead of one weight at a time... i.e. I
> need to be able to change all the stocks in the
> portfolio simultaneously. I am able to do this EASILY
> in Excel, but have not found any straightforward
> application/formulation in R.

R is not a spreadsheet nor a general-purpose optimizer: Excel will find it
very hard to do most of the things R can do.  So why not use Excel?

It's actually easy to solve this problem theoretically, especially once
you notice that you are minimizing

ABS(0.2 + 10*Sum(XiMi)/Mavg - Sum(Xi'))

and the last term is a constant.  But I suspect you have made several
mistakes in specifying it, and will leave it to you to sort this out.

It is likely that you can set up your intended problem as a linear
program, and although R is not the obvious program to solve an LP, it does
have at least three packages that can do so.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Dec 10 13:44:36 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Fri, 10 Dec 2004 12:44:36 +0000 (GMT)
Subject: [R] Returning to normal after call to layout()
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E899D5@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.GSO.4.31.0412101243060.21886-100000@markov.stats>

On Fri, 10 Dec 2004, michael watson (IAH-C) wrote:

> Hi Petr
>
> Yes thanks I did get your response and it was helpful.  However, I have
> now resorted to using barplot() to draw a legend (as all I wanted was
> colored boxes and text) and it works very well for my needs.
>
> Basically, with layout() I'm not getting what I would expect, which may
> be a problem with my expectation rather than anything else.  Like I say,
> I have a function which draws a plot, using layout to divide the
> plotting region into top and bottom.  Fine.  If the user calls my
> function, and then another plotting function, I would expect the new
> plotting function to overwrite the whole of the old plot - but it
> doesn't, it draws the new plot in the top half of the old plot, and
> clears the bottom plot.  So what I want is something which says, after
> calling layout(), "OK, I'm done plotting in two different regions, I
> want you to treat the current device as a single region again".

Please look at the example in ?layout, which does exactly that.
Or look at ?par, especially mfrow and mfcol.  You need to reset the
appropriate par()s.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mike_saunders at umenfa.maine.edu  Fri Dec 10 14:01:33 2004
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Fri, 10 Dec 2004 08:01:33 -0500
Subject: [R] subset bug?
Message-ID: <002901c4deb8$5faf76e0$9ba76f82@CFRU0104>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041210/05bc88cc/attachment.pl

From murdoch at stats.uwo.ca  Fri Dec 10 14:48:12 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 10 Dec 2004 08:48:12 -0500
Subject: [R] 2.01
In-Reply-To: <200412092246.iB9MkSpZ020428@expredir2.cites.uiuc.edu>
References: <200412092246.iB9MkSpZ020428@expredir2.cites.uiuc.edu>
Message-ID: <04ajr0dgmtiliet9f4fg7mfss0ka5e0hut@4ax.com>

On Thu, 9 Dec 2004 16:46:27 -0600, "Bang" <jbang at uiuc.edu> wrote :

>I recently installed the new base.
>
>Code that I had been successfully using for a spatial model is now giving
>the following message when it reads the "Y" variable in the command: 
>
>y <- model.response(mf, "numeric")
>
>The error it gives is: 
>
>Error in model.response(mf, "numeric") : No direct or inherited method for
>function "model.response" for this call
>
>The code I've been using works in 1.9, not in the 2.  Is this an issue with
>the program, and how can it be overcome?

I just tried that with mf constructed according to the man page
example, and it worked with no error.  You'll need to show us how you
constructed mf before we can help with this.

Duncan Murdoch



From br44114 at yahoo.com  Fri Dec 10 14:53:36 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Fri, 10 Dec 2004 05:53:36 -0800 (PST)
Subject: [R] finding the most frequent row
Message-ID: <20041210135336.92746.qmail@web50302.mail.yahoo.com>

Here's something that works. I'm sure there are better solutions (in
particular the paste part - I couldn't figure out how to avoid typing
a[i,1], ..., a[i,10]).

a <- matrix(nrow=1000,ncol=10)
for (i in 1:1000)
	for (j in 1:10)
		a[i,j] <- sample(1:0,1)

b <- vector(mode="character")
for (i in 1:1000)
	b[i] <- paste(a[i,1],a[i,2],a[i,3],a[i,4],a[i,5],
		a[i,6],a[i,7],a[i,8],a[i,9],a[i,10],sep="")

#the most frequent row
table(b)[table(b) == max(table(b))]

HTH,
b.


-----Original Message-----
From: Lisa Pappas [mailto:lisa.pappas at hci.utah.edu]
Sent: Thursday, December 09, 2004 5:15 PM
To: r-help at stat.math.ethz.ch
Subject: [R] finding the most frequent row


I am bootstrapping using a function that I have defined.  The
"Statistic" of the function is an array of 10 numbers.  Therefore if
I use 1000 replications,  the "t" matrix will have 1000 rows each of
which is a bootstrap replicate of this 10 number array (10 columns). 
Is there any easy way in R to determine which row appears the most
frequently? 

Thanks,
Lisa Pappas

Huntsman Cancer Institute wishes to promote open communication while
protecting confidential and/or privileged information.  If you have
received this message in error, please inform the sender and delete
all copies.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Fri Dec 10 15:19:31 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 10 Dec 2004 09:19:31 -0500
Subject: [R] Returning to normal after call to layout()
In-Reply-To: <41B9A4BC.1250.13F7762@localhost>
Message-ID: <Pine.SGI.4.40.0412100916010.59635847-100000@origin.chass.utoronto.ca>

Here's tow examples that do what you want (the way I understand them).


layout(matrix(c(1,0,1,0), byrow = TRUE))
plot(10:100)
par(xpd=NA)
co <- par("usr")
legend(co[1], -10, legend=c("Hello long time no see","\n" , "and My day
was great","\n",  "How was yours"), text.width=95, col="red",,
text.col=c("red","blue", "green"),pch=c(1,NA,2,NA,3), cex=1 )

plot.new()
plot(1:10, bty="n", type="n", xlab="", ylab="", axes=F)
co1 <- par("usr")
legend(co1[1], co1[2], legend=c("Hello long time no see","\n\n\n\n\n\n" ,
"and My day was great","\n\n\n\n\n\n",  "How was yours",
"\n\n\n\n\n\n\n"), text.width=9.5, col="red",, text.col=c("red","blue",
"green"),pch=c(1,NA,2,NA,3,NA), cex=1 )



Unfortunately, to my knowledge there are no ways to set the height on
legend so I do that by including "\n" in the legend wich is the end of
line character....

Hope this help. Note if you are giving a seminar or lecture I do not think
it is appropriate to do it the second way i.e. plot on one window legend
on the other....


Hope this helps,


Jean,


On Fri, 10 Dec 2004, Petr Pikal wrote:

>
>
> On 10 Dec 2004 at 12:20, michael watson (IAH-C) wrote:
>
> > Hi Petr
> >
> > Yes thanks I did get your response and it was helpful.  However, I
> > have now resorted to using barplot() to draw a legend (as all I wanted
> > was colored boxes and text) and it works very well for my needs.
> >
> > Basically, with layout() I'm not getting what I would expect, which
> > may be a problem with my expectation rather than anything else.  Like
> > I say, I have a function which draws a plot, using layout to divide
> > the plotting region into top and bottom.  Fine.  If the user calls my
> > function, and then another plotting function, I would expect the new
> > plotting function to overwrite the whole of the old plot - but it
> > doesn't, it draws the new plot in the top half of the old plot, and
> > clears the bottom plot.  So what I want is something which says, after
> > calling layout(), "OK, I'm done plotting in two different regions, I
> > want you to treat the current device as a single region again".
> >
> > In a previous package I wrote, when using split.screen() to divide up
> > the plotting region, dev.off() could be used for this purpose.  But
> > dev.off() closes the window/plot that I created using layout().
>
> Hi
>
> from the layout help page example (shortened)
>
> > def.par <- par(no.readonly = TRUE)
> > layout(matrix(c(1,2), 2, 1))
> > plot(1,3)
> > plot(1,2)
> > par(def.par)
> > plot(1,1)
> >
> suppose to be exactly what you want
>
> Cheers
> Petr
>
>
> >
> > Mick
> >
> > -----Original Message-----
> > From: Petr Pikal [mailto:petr.pikal at precheza.cz]
> > Sent: 10 December 2004 12:12
> > To: michael watson (IAH-C)
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Returning to normal after call to layout()
> >
> >
> >
> >
> > On 10 Dec 2004 at 11:47, michael watson (IAH-C) wrote:
> >
> > > Hi
> > >
> > > I have a function which draws a plot, using the layout() function to
> > > divide the screen into two parts.  The function works fine, but then
> > > my next call to plot() draws the plot in the first section of the
> > > plot
> >
> > > I've just drawn using layout() - whereas what I want it to do is
> > > create a new plot.
> > >
> > > I tried using dev.off() but that just closes the layout plot window,
> > > which is not what I want.  So my question is - after using the
> > > layout() function, "layout(matrix(c(1,2), 2, 1))", how do I tell R
> >
> > Hi
> >
> > what is wrong on this:
> >
> > layout(matrix(c(1,2), 2, 1))
> > plot(1,1)
> > plot(1,2)
> > windows() #opens the new graphic device see ?device, ?windows
> > plot(1,3)
> >
> > BTW did you read my previous post to your question about legend
> > on a new window?
> >
> > Cheers
> > Petr
> >
> >
> > > that the next call to a plotting function should use a new window,
> > > and
> >
> > > not the layout window?  I've looked in ?layout but there doesn't
> > > seem to be anything in there.
> > >
> > > Thanks
> > > Mick
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Fri Dec 10 15:18:05 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 10 Dec 2004 15:18:05 +0100
Subject: [R] finding the most frequent row
In-Reply-To: <20041210135336.92746.qmail@web50302.mail.yahoo.com>
References: <20041210135336.92746.qmail@web50302.mail.yahoo.com>
Message-ID: <41B9B01D.1080409@statistik.uni-dortmund.de>

bogdan romocea wrote:

> Here's something that works. I'm sure there are better solutions (in
> particular the paste part - I couldn't figure out how to avoid typing
> a[i,1], ..., a[i,10]).
> 
> a <- matrix(nrow=1000,ncol=10)
> for (i in 1:1000)
> 	for (j in 1:10)
> 		a[i,j] <- sample(1:0,1)
> 
> b <- vector(mode="character")
> for (i in 1:1000)
> 	b[i] <- paste(a[i,1],a[i,2],a[i,3],a[i,4],a[i,5],
> 		a[i,6],a[i,7],a[i,8],a[i,9],a[i,10],sep="")
> 
> #the most frequent row
> table(b)[table(b) == max(table(b))]

You mean:

   a <- matrix(sample(0:1, 10000, replace = TRUE), 1000, 10)
   b <- apply(a, 1, paste, collapse="")
   tb <- table(b)
   tb[which.max(tb)]

The real problem is that paste() is slow compared to numerical 
calculations. I think there is a better solution somewhere in the 
archives (I cannot remind right now), so look up the archives, please.

Uwe Ligges





> HTH,
> b.
> 
> 
> -----Original Message-----
> From: Lisa Pappas [mailto:lisa.pappas at hci.utah.edu]
> Sent: Thursday, December 09, 2004 5:15 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] finding the most frequent row
> 
> 
> I am bootstrapping using a function that I have defined.  The
> "Statistic" of the function is an array of 10 numbers.  Therefore if
> I use 1000 replications,  the "t" matrix will have 1000 rows each of
> which is a bootstrap replicate of this 10 number array (10 columns). 
> Is there any easy way in R to determine which row appears the most
> frequently? 
> 
> Thanks,
> Lisa Pappas
> 
> Huntsman Cancer Institute wishes to promote open communication while
> protecting confidential and/or privileged information.  If you have
> received this message in error, please inform the sender and delete
> all copies.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From michael.watson at bbsrc.ac.uk  Fri Dec 10 15:19:16 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 10 Dec 2004 14:19:16 -0000
Subject: [R] cbind() and factors.
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B8CE@iahce2knas1.iah.bbsrc.reserved>

Hi

I'm seeing some "odd" behaviour with cbind().  My code is:

> cat <- read.table("cogs_category.txt", sep="\t", header=TRUE,
quote=NULL, colClasses="character")
> colnames(cat)
[1] "Code"        "Description"
> is.factor(cat$Code)
[1] FALSE
> is.factor(cat$Description)
[1] FALSE
> is.factor(rainbow(nrow(cat)))
[1] FALSE
> cat <- cbind(cat,"Color"=rainbow(nrow(cat)))
> is.factor(cat$Color)
[1] TRUE
> ?cbind

I read a text file in which has two columns, Code and Description.
Neither of these are factors.  I want to add a column of colours to the
data frame using rainbow().  The rainbow function also does not return a
factor.  However, if I cbind my data frame (which has no factors in it)
and the results of rainbow() (which is a vector, not a factor), then for
some reason the new column is a factor...??

Mick


Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7990 827831
E-mail: michael.watson at bbsrc.ac.uk



From Max_Kuhn at bd.com  Fri Dec 10 15:35:49 2004
From: Max_Kuhn at bd.com (Max_Kuhn@bd.com)
Date: Fri, 10 Dec 2004 09:35:49 -0500
Subject: [R] strange gee behavior
Message-ID: <OF0FDC15BD.ACF5922F-ON85256F66.004F13F5@bd.com>

I'm using R 1.9.1 on suse server v9 enterprise with the gee package version
4.13-10.

I have code that runs in an automated script. It uses the gee function from
the gee package. The script is run quite often without error.

I have a problem where the script locks up R when calling this function (it
starts execution and never finishes). I was able to track down the
offending data and determine that this particular dataset results in a
correlation matrix that is not positive definite and a line within gee with
eigen(z$wcor) is where things lock up . Specifically, the line within eigen

.Call("La_rs", x, only.values, if (capabilities("IEEE754")) "dsyevr" else
"dsyev", PACKAGE = "base")

is where it happens.

The issue is that I get different behavior when I run the function
"interactively" and when it's run by sourcing a script. Running at a
command line (i.e. not embedding the call within functions and sourcing)
will return a result from gee with a warning and an error code. I can
program around this pretty well. However, running the command via the
script (i.e. embedded three functions down and called by sourcing a file)
will make the system lock up. It's difficult to send the entire set of code
to try to reproduce the locking up, but here is an example that does not
produce a pd matrix:

group1 <- rep(1:10, c(3, 2, 3, 2, 4, 1, 1, 3, 4, 2))
group2 <- rep(1:10, c(3, 3, 4, 5, 2, 6, 5, 4, 3, 4))
group <- c(group1, group2)
outcome <- c(rep(FALSE, length(group1)), rep(TRUE, length(group2)))
gee(outcome ~ 1,  group, family="binomial", corstr="exchangeable").

Am I missing something obvious here? It seems like the behavior is
different depending on how the code is sent to R, but this is an
assumption. I know that there are cases where this can be the case (e.g.
printing lattice graphics). There doesn't seem to be a way to test the data
prior to running gee and try() isn't an option.

Thanks,

Max



-----------------------------------------
*******************************************************************
IMPORTANT MESSAGE FOR RECIPIENTS IN THE U.S.A.:  This message may
constitute an advertisement of a BD group's products or services  or a
solicitation of interest in them. If this is such a message and you would
like to opt out of receiving future advertisements or solicitations from
this BD group, please forward this e-mail to optoutbygroup at bd.com.
*******************************************************************  This
message (which includes any attachments) is intended only for the
designated recipient(s).  It may contain confidential or proprietary
information and may be subject to the attorney-client  privilege or other
confidentiality protections.  If you are not a designated recipient, you
may not review, use, copy or distribute this message. If you receive this
in error, please notify the sender by reply e-mail and delete this message.
Thank you.
*******************************************************************
Corporate Headquarters Mailing Address: BD (Becton, Dickinson and Company)
1 Becton Drive Franklin Lakes, NJ 07417 U.S.A.



From dray at biomserv.univ-lyon1.fr  Fri Dec 10 15:39:36 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Fri, 10 Dec 2004 09:39:36 -0500
Subject: [R] cbind() and factors.
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B8CE@iahce2knas1.iah.b
	bsrc.reserved>
Message-ID: <5.2.1.1.0.20041210093731.00beed30@biomserv.univ-lyon1.fr>

cat is a data.frame,
so cbind is use for a data.frame

and
?data.frame tell us that:

  Character variables passed to 'data.frame' are converted
      to factor columns unless protected by 'I'.

PS : it is not good ides to call your data.frame cat as there is a cat 
function.



At 09:19 10/12/2004, michael watson (IAH-C) wrote:
>Hi
>
>I'm seeing some "odd" behaviour with cbind().  My code is:
>
> > cat <- read.table("cogs_category.txt", sep="\t", header=TRUE,
>quote=NULL, colClasses="character")
> > colnames(cat)
>[1] "Code"        "Description"
> > is.factor(cat$Code)
>[1] FALSE
> > is.factor(cat$Description)
>[1] FALSE
> > is.factor(rainbow(nrow(cat)))
>[1] FALSE
> > cat <- cbind(cat,"Color"=rainbow(nrow(cat)))
> > is.factor(cat$Color)
>[1] TRUE
> > ?cbind
>
>I read a text file in which has two columns, Code and Description.
>Neither of these are factors.  I want to add a column of colours to the
>data frame using rainbow().  The rainbow function also does not return a
>factor.  However, if I cbind my data frame (which has no factors in it)
>and the results of rainbow() (which is a vector, not a factor), then for
>some reason the new column is a factor...??
>
>Mick
>
>
>Michael Watson
>Head of Informatics
>Institute for Animal Health,
>Compton Laboratory,
>Compton,
>Newbury,
>Berkshire RG20 7NN
>UK
>
>Phone : +44 (0)1635 578411 ext. 2535
>Mobile: +44 (0)7990 827831
>E-mail: michael.watson at bbsrc.ac.uk
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From rolf at math.unb.ca  Fri Dec 10 15:40:18 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Fri, 10 Dec 2004 10:40:18 -0400 (AST)
Subject: [R] cbind() and factors.
Message-ID: <200412101440.iBAEeIh1014562@erdos.math.unb.ca>


This is of the nature of an FAQ.  Data frames coerce character
vectors into factors.  If you want a character vector to stay
that way (and not become a factor) wrap in up in ``I()'':

	cat <- cbind(cat,Color=I(rainbow(nrow(cat))))

(There's no need to quote the name ``Color'' in the foregoing.)

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From jeaneid at chass.utoronto.ca  Fri Dec 10 15:59:57 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 10 Dec 2004 09:59:57 -0500
Subject: [R] finding the most frequent row
In-Reply-To: <20041210135336.92746.qmail@web50302.mail.yahoo.com>
Message-ID: <Pine.SGI.4.40.0412100951570.59635847-100000@origin.chass.utoronto.ca>

You can do the following also




X <- matrix(c(1,2,1,2,1,3,1,4), ncol=2)
Y<-unique(X)
Y[which.max(apply(Y, 1,function(i) sum(apply(matrix(1:nrow(X)),
1,function(x) identical(X[x,], i[1:2]))))),]


I do not know what your strategy is when there are multiple maxima i.e two
different  rows appear at the same frequency. The above will ge the first
row that appears to be the maximum  as an example suppose that we have

X <- matrix(c(1,2,1,2,1,3,1,4, 1, 4), ncol=2)
The above will have [2,1] as the maximum row while it should be both [2,1]
and [1,4]. To correct that try
Y[(yy <- (apply(Y, 1,function(i) sum(apply(matrix(1:nrow(X)),
1,function(x) identical(X[x,], i[1:2]))))))%in%max(yy),]


and this will give you [2,1] and [1,4] as the most frequent rows.


Hopw this help


Jean










On Fri, 10 Dec 2004, bogdan romocea wrote:

> Here's something that works. I'm sure there are better solutions (in
> particular the paste part - I couldn't figure out how to avoid typing
> a[i,1], ..., a[i,10]).
>
> a <- matrix(nrow=1000,ncol=10)
> for (i in 1:1000)
> 	for (j in 1:10)
> 		a[i,j] <- sample(1:0,1)
>
> b <- vector(mode="character")
> for (i in 1:1000)
> 	b[i] <- paste(a[i,1],a[i,2],a[i,3],a[i,4],a[i,5],
> 		a[i,6],a[i,7],a[i,8],a[i,9],a[i,10],sep="")
>
> #the most frequent row
> table(b)[table(b) == max(table(b))]
>
> HTH,
> b.
>
>
> -----Original Message-----
> From: Lisa Pappas [mailto:lisa.pappas at hci.utah.edu]
> Sent: Thursday, December 09, 2004 5:15 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] finding the most frequent row
>
>
> I am bootstrapping using a function that I have defined.  The
> "Statistic" of the function is an array of 10 numbers.  Therefore if
> I use 1000 replications,  the "t" matrix will have 1000 rows each of
> which is a bootstrap replicate of this 10 number array (10 columns).
> Is there any easy way in R to determine which row appears the most
> frequently?
>
> Thanks,
> Lisa Pappas
>
> Huntsman Cancer Institute wishes to promote open communication while
> protecting confidential and/or privileged information.  If you have
> received this message in error, please inform the sender and delete
> all copies.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dieter.menne at menne-biomed.de  Fri Dec 10 15:55:56 2004
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 10 Dec 2004 14:55:56 +0000 (UTC)
Subject: [R] cbind() and factors.
References: <8975119BCD0AC5419D61A9CF1A923E950121B8CE@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <loom.20041210T155344-809@post.gmane.org>

Probably you called the build-in rainwbow-function, which returns a string.

>str(rainbow(10))
 chr "FF0000"

Dieter Menne



From ligges at statistik.uni-dortmund.de  Fri Dec 10 16:16:45 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 10 Dec 2004 16:16:45 +0100
Subject: [R] HTML help index generation problem with R under Windows
In-Reply-To: <200412091723.08225.sergei@stams.strath.ac.uk>
References: <200412091723.08225.sergei@stams.strath.ac.uk>
Message-ID: <41B9BDDD.2060504@statistik.uni-dortmund.de>

Sergei Zuyev wrote:

> Hello,
> I am wondering if there has been a solution to the following issue with R 
> under Windows (see also thread to PR#6662 in this mailing list: 
> http://tolstoy.newcastle.edu.au/R/devel/04a/0550.html )
> I am using R for teaching students Statistics, so they are working with a 
> university-wide installation of R. I have compiled an R-library which 
> contains all the instructions and customised functions for the course and the 
> students have been using it successfully under UNIX for the last 5 years. I 
> keep the library in my own space which is pointed to in R_LIBS variable 
> defined for the students. I myself not a sysadmin for the university network, 
> so I cannot install to the system-wide library directory, but this was not a 
> matter. This year, I have to (sniff-sniff) switch to Windows platform and the 
> following problem arose.
> The library loads fine and the text-style help files show OK, but help.start() 
> reports the following error:
> Error in file(f.tg, open = "w") : unable to open connection 
>  In addition: Warning messages: 
>  1: cannot update HTML package index in: make.packages.html(lib.loc) 
>  2: cannot open file `C:\Progra~1\R\rw1071/doc/html/search/index.txt' 
> The index-file is opening but does not contain links to my library.
>  As far as I understand, the problem lies in the lack of write permissions for 
> the system directory where R is installed. While UNIX version creates the 
> html-help files in per-session way in the user's home directory, the windows 
> version atempts to modify the system-wide file. 
> Having possibility to display html-help would be a great benefit, as the 
> students can follow the links present there which is impossible from within 
> "inline" help. So my question:
> 
> Is there any work-around that problem? Would making creation of html help 
> files in the users' space under Windows as under UNIX/linux resolve the 
> issue? Or maybe, it's already been fixed in the recent version of R?
> 
> Thanks in advance for your help! Please, make a copy of your response to my 
> email below.


Two points:

a) Please upgrade, R-1.7.1 is really ancient.
b) help.start(update=FALSE) should work.

Uwe Ligges



From maechler at stat.math.ethz.ch  Fri Dec 10 17:30:35 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 10 Dec 2004 17:30:35 +0100
Subject: [R] Which FM should beginners R?  A suggestion.
In-Reply-To: <200412070135.iB71ZMxT020609@atlas.otago.ac.nz>
References: <200412070135.iB71ZMxT020609@atlas.otago.ac.nz>
Message-ID: <16825.53035.304402.646923@gargle.gargle.HOWL>

>>>>> "RichOK" == Richard A O'Keefe <ok at cs.otago.ac.nz>
>>>>>     on Tue, 7 Dec 2004 14:35:22 +1300 (NZDT) writes:

    RichOK> I've made one important change to
    RichOK> http://www.cs.otago.ac.nz/staffpriv/ok/R-help.txt

now that the server there is responsive again,...

Thank you very much, Richard!

I allow myself to be quite (too?) critical in the following.
This shouldn't deflect from the fact that your text is probably
very valuable already!

    RichOK> and a couple of minor ones (notably qqplot).  The important one is
    RichOK> that I've split "elementary statistics" into "elementary descriptive
    RichOK> statistics and tests" and "model fitting", with rather more in the
    RichOK> model fitting category (but still not glm).  Should 'glm' be there?

The importance of many of these are a matter of taste. Yes, I'd
put 'glm' there. {For my particular taste even 'MASS::rlm'}.

I'd definitely delete the "R Language Definition" from the list
of essential reading for R beginners.  This should be only for
(some of) those beginners who know real programming languages,
and now come to R "as a programming language".  Probably an
increasingly small minority of R newbies.

    RichOK> Possibly the biggest gap is that there is nothing about time series.

yes, and quite a lot about "string handling" most of which many
beginners probably have no need. I'd only keep  paste() and
system() from that list

{and as side note: I have the impression  ifelse() has been a bit
 over emphasized in the past; we've seen many user examples with
 ifelse(A,B,C)  where they should really have used
 if(A) B else C
}
Also - of course I have to say this - my favorite 'str' is  missing..

The final list of distributions is also (too) long.
Why not keep it short and refer to the list inside "An
Introduction to R"?

Another thought:
Maybe cut your list into
    ``to read in the first week''
and ``to be read during the next few months'' ?
This would make it (the first part) quite a bit less
overwhelming for the real newbie.

Thanks again for this, Richard!

Martin



From gunter.berton at gene.com  Fri Dec 10 17:32:03 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 10 Dec 2004 08:32:03 -0800
Subject: Arguments to a call-- was  [R] finding the most frequent row
In-Reply-To: <20041210135336.92746.qmail@web50302.mail.yahoo.com>
Message-ID: <200412101632.iBAGW3PY028278@hertz.gene.com>

> Here's something that works. I'm sure there are better solutions (in
> particular the paste part - I couldn't figure out how to avoid typing
> a[i,1], ..., a[i,10]).

This is a language issue that arises frequently. One standard approach is to
use do.call() as in:

... result <- do.call("paste",as.data.frame(yourmatrix), sep="")

The bit to notice here is that the "args" argument to do.call must be a
list, which is why the conversion to a data.frame (which IS a list) is used.

V&R's S PROGRAMMING book discusses this and many related issues in their
"Computing on the Language" chapter. I find it challenging, but invaluable.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From gunter.berton at gene.com  Fri Dec 10 17:50:30 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 10 Dec 2004 08:50:30 -0800
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <200412101650.iBAGoUsL006573@faraday.gene.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041210/511a96cc/attachment.pl

From gunter.berton at gene.com  Fri Dec 10 17:57:38 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 10 Dec 2004 08:57:38 -0800
Subject: [R] subset bug?
In-Reply-To: <002901c4deb8$5faf76e0$9ba76f82@CFRU0104>
Message-ID: <200412101657.iBAGvcOs020660@compton.gene.com>

The subset() Help file says

"The select argument exists only for the method for data frames ..."

So if mat is a matrix and not a data.frame, this might be the source of your
problems.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Saunders
> Sent: Friday, December 10, 2004 5:02 AM
> To: R Help
> Subject: [R] subset bug?
> 
> I ran into a problem with "subset" while working at home that 
> I am not sure is a bug or not.  I defined a custom function 
> to take a data frame of tree positions, sizes, and types 
> (containing the columns TRT, COMP, PLOT, X, Y, DBH and CON) 
> and wanted to make a stem map of the plot using different 
> symbols and colors to represent the different types of tree 
> stems.  I copied a bit of the code below, the part that gave 
> me troubles:
> 
> if(dim(subset(mat,CON==1 & DBH>11.43))[1]>0) 
> symbols(mat[mat$CON==1 &     mat$DBH>11.43,]$X,mat[mat$CON==1 
> & mat$DBH>11.43,]$Y,circles=log(mat[mat$CON==1 & 
> mat$DBH>11.43,]$DBH)*tuner,inches=FALSE,add=TRUE,fg="gray",lwd=2)
> 
> if(dim(subset(mat,CON==0 & DBH>11.43))[1]>0) 
> symbols(mat[mat$CON==0 & mat$DBH>11.43,]$X,mat[mat$CON==0 & 
> mat$DBH>11.43,]$Y,squares=log(mat[mat$CON==0 & 
> mat$DBH>11.43,]$DBH)*sqrt(pi)*tuner,inches=FALSE,add=TRUE,fg="
> gray",lwd=2)
> 
> if(dim(subset(mat,CON==1 & DBH<=11.43))[1]>0) 
> symbols(mat[mat$CON==1 & mat$DBH<=11.43,]$X,mat[mat$CON==1 & 
> mat$DBH<=11.43,]$Y,circles=log(mat[mat$CON==1 & 
> mat$DBH<=11.43,]$DBH)*tuner,inches=FALSE,add=TRUE,fg="black",lwd=2)
> 
> if(dim(subset(mat,CON==0 & DBH<=11.43))[1]>0) 
> symbols(mat[mat$CON==0 & mat$DBH<=11.43,]$X,mat[mat$CON==0 & 
> mat$DBH<=11.43,]$Y,squares=log(mat[mat$CON==0 & 
> mat$DBH<=11.43,]$DBH)*sqrt(pi)*tuner,inches=FALSE,add=TRUE,fg=
> "black",lwd=2)
> 
>  The item "tuner" just scales the symbol size.  As you can 
> see, I was using two columns to define the type of symbol 
> (CON: square vs. circle) and color (DBH: gray vs. black).  I 
> had to use the "if" statements because some of the plots did 
> not contain all combinations of tree types.  This worked fine 
> in giving me my plots.
> 
> Okay, the question I had was why I couldn't use the "subset" 
> function within the "symbols" command.  I was going to use:
> 
> subset(mat,CON==0 & DBH<=11.43,select=c(X,Y))
> 
> as the first entry into the symbols command, but it kept 
> giving me errors saying "xy.coords" could not convert it.  
> However, if I used the "plot" function with "subset" trimming 
> the data frame externally, it worked fine to plot the point 
> positions.  However, the help for "plot" and "plot.default" 
> says "xy.coords" is used within that function.  What am I 
> missing here?
> 
> Mike Saunders
> Research Assistant
> University of Maine
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mike_saunders at umenfa.maine.edu  Fri Dec 10 18:39:43 2004
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Fri, 10 Dec 2004 12:39:43 -0500
Subject: [R] subset bug?
References: <200412101657.iBAGvcOs020660@compton.gene.com>
Message-ID: <000401c4dedf$3bc66eb0$9ba76f82@CFRU0104>

Bert:

Thanks for your reply.  Actually, "mat" was a data.frame, not a matrix.

Mike


----- Original Message ----- 
From: "Berton Gunter" <gunter.berton at gene.com>
To: "'Mike Saunders'" <mike_saunders at umenfa.maine.edu>; "'R Help'" 
<r-help at stat.math.ethz.ch>
Sent: Friday, December 10, 2004 11:57 AM
Subject: RE: [R] subset bug?


> The subset() Help file says
>
> "The select argument exists only for the method for data frames ..."
>
> So if mat is a matrix and not a data.frame, this might be the source of 
> your
> problems.
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Saunders
>> Sent: Friday, December 10, 2004 5:02 AM
>> To: R Help
>> Subject: [R] subset bug?
>>
>> I ran into a problem with "subset" while working at home that
>> I am not sure is a bug or not.  I defined a custom function
>> to take a data frame of tree positions, sizes, and types
>> (containing the columns TRT, COMP, PLOT, X, Y, DBH and CON)
>> and wanted to make a stem map of the plot using different
>> symbols and colors to represent the different types of tree
>> stems.  I copied a bit of the code below, the part that gave
>> me troubles:
>>
>> if(dim(subset(mat,CON==1 & DBH>11.43))[1]>0)
>> symbols(mat[mat$CON==1 &     mat$DBH>11.43,]$X,mat[mat$CON==1
>> & mat$DBH>11.43,]$Y,circles=log(mat[mat$CON==1 &
>> mat$DBH>11.43,]$DBH)*tuner,inches=FALSE,add=TRUE,fg="gray",lwd=2)
>>
>> if(dim(subset(mat,CON==0 & DBH>11.43))[1]>0)
>> symbols(mat[mat$CON==0 & mat$DBH>11.43,]$X,mat[mat$CON==0 &
>> mat$DBH>11.43,]$Y,squares=log(mat[mat$CON==0 &
>> mat$DBH>11.43,]$DBH)*sqrt(pi)*tuner,inches=FALSE,add=TRUE,fg="
>> gray",lwd=2)
>>
>> if(dim(subset(mat,CON==1 & DBH<=11.43))[1]>0)
>> symbols(mat[mat$CON==1 & mat$DBH<=11.43,]$X,mat[mat$CON==1 &
>> mat$DBH<=11.43,]$Y,circles=log(mat[mat$CON==1 &
>> mat$DBH<=11.43,]$DBH)*tuner,inches=FALSE,add=TRUE,fg="black",lwd=2)
>>
>> if(dim(subset(mat,CON==0 & DBH<=11.43))[1]>0)
>> symbols(mat[mat$CON==0 & mat$DBH<=11.43,]$X,mat[mat$CON==0 &
>> mat$DBH<=11.43,]$Y,squares=log(mat[mat$CON==0 &
>> mat$DBH<=11.43,]$DBH)*sqrt(pi)*tuner,inches=FALSE,add=TRUE,fg=
>> "black",lwd=2)
>>
>>  The item "tuner" just scales the symbol size.  As you can
>> see, I was using two columns to define the type of symbol
>> (CON: square vs. circle) and color (DBH: gray vs. black).  I
>> had to use the "if" statements because some of the plots did
>> not contain all combinations of tree types.  This worked fine
>> in giving me my plots.
>>
>> Okay, the question I had was why I couldn't use the "subset"
>> function within the "symbols" command.  I was going to use:
>>
>> subset(mat,CON==0 & DBH<=11.43,select=c(X,Y))
>>
>> as the first entry into the symbols command, but it kept
>> giving me errors saying "xy.coords" could not convert it.
>> However, if I used the "plot" function with "subset" trimming
>> the data frame externally, it worked fine to plot the point
>> positions.  However, the help for "plot" and "plot.default"
>> says "xy.coords" is used within that function.  What am I
>> missing here?
>>
>> Mike Saunders
>> Research Assistant
>> University of Maine
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
>



From abunn at whrc.org  Fri Dec 10 20:01:20 2004
From: abunn at whrc.org (Andy Bunn)
Date: Fri, 10 Dec 2004 14:01:20 -0500
Subject: [R] Adding a polygon to a time series plot
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEJJCNAA.abunn@whrc.org>

Preamble: Sorry for being dense.

Now that that's done, here are my questions.

I want to put a polygon on a plot of a time series. I'm going to add lines
from a smooth.spline interpolation and other annotation to it. But here's
the general idea:

# Start code
     n <- 121
     dat <- rnorm(n)
     plot(dat, type="n")
     polygon(c(1:n,n:1), c(dat,c(rep(0,n))), col = "gray80", border = NA)
# End code

What if dat is a time series?

# Start code that won't parse.
     dat.ts <- ts(rnorm(n), start = 1980, frequency = 12)
     plot(dat.ts, type="n")
     polygon(...) # How do I specify x and y for the time series?
                  # Is it easier to skip the time series and add time as an
axis
# End code that won't parse

Here's a work around that is cumbersome.

# Start code
     dat.ts <- ts(rnorm(n), start = 1980, frequency = 12)
     dat <- as.vector(dat.ts)
     plot(1:n, dat, type="n", xaxt = "n", axes = FALSE, xlab = "", ylab =
"")
     polygon(c(1:n,n:1), c(dat,c(rep(0,n))), col = "gray80", border = NA)
     axis(1, at = seq(1, n, by = 12), labels = c(1980:1990))
# End code

Can I use polygon with a time series object? Or is it too awkward?

Thanks, Andy



From Weiming.Zhang at UCHSC.edu  Fri Dec 10 20:47:02 2004
From: Weiming.Zhang at UCHSC.edu (Weiming.Zhang@UCHSC.edu)
Date: Fri, 10 Dec 2004 12:47:02 -0700
Subject: [R] R/S-plus help
Message-ID: <ABEA8AB5ABE1C8478DA641618E2B6B57DBF8F9@hscex6.uchsc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041210/9572825c/attachment.pl

From andy_liaw at merck.com  Fri Dec 10 20:49:16 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 10 Dec 2004 14:49:16 -0500
Subject: [R] Adding a polygon to a time series plot
Message-ID: <3A822319EB35174CA3714066D590DCD50994E40F@usrymx25.merck.com>

> From: Andy Bunn
> 
> Preamble: Sorry for being dense.
> 
> Now that that's done, here are my questions.
> 
> I want to put a polygon on a plot of a time series. I'm going 
> to add lines
> from a smooth.spline interpolation and other annotation to 

Please note: smooth.spline() is for _smoothing_.  If you really want
interpolation, you should use spline() or splinefun().

> it. But here's
> the general idea:
> 
> # Start code
>      n <- 121
>      dat <- rnorm(n)
>      plot(dat, type="n")
>      polygon(c(1:n,n:1), c(dat,c(rep(0,n))), col = "gray80", 
> border = NA)
> # End code
> 
> What if dat is a time series?
> 
> # Start code that won't parse.
>      dat.ts <- ts(rnorm(n), start = 1980, frequency = 12)
>      plot(dat.ts, type="n")
>      polygon(...) # How do I specify x and y for the time series?
>                   # Is it easier to skip the time series and 
> add time as an
> axis
> # End code that won't parse

AFAICS, the plot will have the x-axis range equal to the time you specified,
so replace 1 with start(dat.ts) and n with end(dat.ts) ought to work, no?

HTH,
Andy

 
> Here's a work around that is cumbersome.
> 
> # Start code
>      dat.ts <- ts(rnorm(n), start = 1980, frequency = 12)
>      dat <- as.vector(dat.ts)
>      plot(1:n, dat, type="n", xaxt = "n", axes = FALSE, xlab 
> = "", ylab =
> "")
>      polygon(c(1:n,n:1), c(dat,c(rep(0,n))), col = "gray80", 
> border = NA)
>      axis(1, at = seq(1, n, by = 12), labels = c(1980:1990))
> # End code
> 
> Can I use polygon with a time series object? Or is it too awkward?
> 
> Thanks, Andy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Fri Dec 10 20:55:32 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 10 Dec 2004 14:55:32 -0500
Subject: [R] R/S-plus help
Message-ID: <3A822319EB35174CA3714066D590DCD50994E410@usrymx25.merck.com>

You've got the wrong list:  This is R-help, not S-news.  Besides, wouldn't
such questions be covered in the S-PLUS Programmer's Guide?

Andy

> From: Weiming.Zhang at uchsc.edu
> 
> Hi,
>  
> Does anybody know if S-plus Connect/C++ lib can handle 
> complex number? I need to use complex matrix.
>  
> Thanks vey much.
>  
> wz
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abunn at whrc.org  Fri Dec 10 21:05:44 2004
From: abunn at whrc.org (Andy Bunn)
Date: Fri, 10 Dec 2004 15:05:44 -0500
Subject: [R] Adding a polygon to a time series plot
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E40F@usrymx25.merck.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIGEJLCNAA.abunn@whrc.org>

Arggh! start and end. Of course. How stupid of me. And I'm not going to
interpolate with smooth.spline, but thanks for the warning. Sloppy language.

Thanks, Andy

> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com]
> Sent: Friday, December 10, 2004 2:49 PM
> To: 'Andy Bunn'; R-Help
> Subject: RE: [R] Adding a polygon to a time series plot
>
>
> > From: Andy Bunn
> >
> > Preamble: Sorry for being dense.
> >
> > Now that that's done, here are my questions.
> >
> > I want to put a polygon on a plot of a time series. I'm going
> > to add lines
> > from a smooth.spline interpolation and other annotation to
>
> Please note: smooth.spline() is for _smoothing_.  If you really want
> interpolation, you should use spline() or splinefun().
>
> > it. But here's
> > the general idea:
> >
> > # Start code
> >      n <- 121
> >      dat <- rnorm(n)
> >      plot(dat, type="n")
> >      polygon(c(1:n,n:1), c(dat,c(rep(0,n))), col = "gray80",
> > border = NA)
> > # End code
> >
> > What if dat is a time series?
> >
> > # Start code that won't parse.
> >      dat.ts <- ts(rnorm(n), start = 1980, frequency = 12)
> >      plot(dat.ts, type="n")
> >      polygon(...) # How do I specify x and y for the time series?
> >                   # Is it easier to skip the time series and
> > add time as an
> > axis
> > # End code that won't parse
>
> AFAICS, the plot will have the x-axis range equal to the time you
> specified,
> so replace 1 with start(dat.ts) and n with end(dat.ts) ought to work, no?
>
> HTH,
> Andy
>
>
> > Here's a work around that is cumbersome.
> >
> > # Start code
> >      dat.ts <- ts(rnorm(n), start = 1980, frequency = 12)
> >      dat <- as.vector(dat.ts)
> >      plot(1:n, dat, type="n", xaxt = "n", axes = FALSE, xlab
> > = "", ylab =
> > "")
> >      polygon(c(1:n,n:1), c(dat,c(rep(0,n))), col = "gray80",
> > border = NA)
> >      axis(1, at = seq(1, n, by = 12), labels = c(1980:1990))
> > # End code
> >
> > Can I use polygon with a time series object? Or is it too awkward?
> >
> > Thanks, Andy
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
> ------------------------------------------------------------------
> ------------
> Notice:  This e-mail message, together with any attachments,
> contains information of Merck & Co., Inc. (One Merck Drive,
> Whitehouse Station, New Jersey, USA 08889), and/or its affiliates
> (which may be known outside the United States as Merck Frosst,
> Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be
> confidential, proprietary copyrighted and/or legally privileged.
> It is intended solely for the use of the individual or entity
> named on this message.  If you are not the intended recipient,
> and have received this message in error, please notify us
> immediately by reply e-mail and then delete it from your system.
> ------------------------------------------------------------------
> ------------
>



From tghoward at gw.dec.state.ny.us  Fri Dec 10 21:31:07 2004
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Fri, 10 Dec 2004 15:31:07 -0500
Subject: [R] predict.randomForest
Message-ID: <s1b9c146.094@gwsmtp.DEC.STATE.NY.US>

I have a data.frame with a series of variables tagged to a binary
response ('present'/'absent').  I am trying to use randomForest to
predict present/absent in a second dataset.    After a lot a fiddling
(using two data frames, making sure data types are the same, lots of
testing with data that works such as data(iris)) I've settled on
combining all my data into one data.frame and then subset()'ing the
known present/absent portion of the data.frame for the randomForest run
and then using the other subset for the predict.   This worked with test
data, but then when I try it on a larger dataset (63,000 rows to
predict), I get this error:

Error in predict.randomForest(stsw.rf, stsw.out, type = "prob") : 
        Type of predictors in new data do not match that of the
training data.

This is the error I was getting earlier, but I thought I had solved it
by joining into one data.frame and subsetting.  The values for each
variable in the 'unknown' data (that which I want to predict) fall
within (are bound by) the values in the 'known' data.  

Does this error message have more than one meaning?

Any suggestions on how to work through this?

I am using R 2.0.1.  randomForest 4.4-2 (2004-11-02); I'm a new user to
R, but doing my best to learn as much as I can... if I'm obviously
clueless, please forgive me!

Any help would be greatly appreciated,

Thanks in advance!
Tim Howard


More background for anyone interested:
  CART (as well as many other statistical techniques) has been used for
a while to predict plant and animal distributions across a landscape.
You feed it data about places where you know the Plant to occur and not
occur and CART provides you with a tree with which you can then model
the potential distribution across your region (state, country, etc)
using GIS.
   I've heard good things about the randomForests and would like to try
to do the same thing. My biggest stumbling block is that I can't
(obviously once I realized it) get a single 'best tree' from
randomForests with which to apply my GIS models.  Or, is there any way
to extract a formula from randomForest similar to a CART or rPart tree
and apply it to a dataset outside of R?  The only solution I've been
able to come up with is bring ALL of the environmental variables into R,
have randomForest do the prediction, and the get that prediction back
into GIS. Thus my problem as I stated it above. I'm worried because my
datasets are going to be huge (100's of millions of records) when we
really get going. Should I be worried?

thanks,  Tim



From andy_liaw at merck.com  Fri Dec 10 22:16:37 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 10 Dec 2004 16:16:37 -0500
Subject: [R] predict.randomForest
Message-ID: <3A822319EB35174CA3714066D590DCD50994E412@usrymx25.merck.com>

Can you show how you called randomForest() and predict.randomForest()?  The
error message came from the code:

    if (is.data.frame(x)) {
        [changing ordered factors to numeric...]
        cat.new <- sapply(x, function(x) if (is.factor(x) && 
            !is.ordered(x)) 
            length(levels(x))
        else 1)
        [...]
        if (!all(object$forest$ncat == cat.new)) 
            stop("Type of predictors in new data do not match that of the
training data.")
    }
 
Basically, it checks whether the numbers of categories for the predictor
variables in the newdata match those used in training (1=numeric).  If you
used the formula interface, this is unlikely to happen.  But if you are
dealing with data with huge number of variables, you should avoid the
formula interface.

For predicting large data sets, I use a loop to read the data in one chunk
at a time, run the prediction on the chunk, and iterate until finish.  Works
just fine for me.

Cheers,
Andy

> From: Tim Howard
> 
> I have a data.frame with a series of variables tagged to a binary
> response ('present'/'absent').  I am trying to use randomForest to
> predict present/absent in a second dataset.    After a lot a fiddling
> (using two data frames, making sure data types are the same, lots of
> testing with data that works such as data(iris)) I've settled on
> combining all my data into one data.frame and then subset()'ing the
> known present/absent portion of the data.frame for the 
> randomForest run
> and then using the other subset for the predict.   This 
> worked with test
> data, but then when I try it on a larger dataset (63,000 rows to
> predict), I get this error:
> 
> Error in predict.randomForest(stsw.rf, stsw.out, type = "prob") : 
>         Type of predictors in new data do not match that of the
> training data.
> 
> This is the error I was getting earlier, but I thought I had solved it
> by joining into one data.frame and subsetting.  The values for each
> variable in the 'unknown' data (that which I want to predict) fall
> within (are bound by) the values in the 'known' data.  
> 
> Does this error message have more than one meaning?
> 
> Any suggestions on how to work through this?
> 
> I am using R 2.0.1.  randomForest 4.4-2 (2004-11-02); I'm a 
> new user to
> R, but doing my best to learn as much as I can... if I'm obviously
> clueless, please forgive me!
> 
> Any help would be greatly appreciated,
> 
> Thanks in advance!
> Tim Howard
> 
> 
> More background for anyone interested:
>   CART (as well as many other statistical techniques) has 
> been used for
> a while to predict plant and animal distributions across a landscape.
> You feed it data about places where you know the Plant to 
> occur and not
> occur and CART provides you with a tree with which you can then model
> the potential distribution across your region (state, country, etc)
> using GIS.
>    I've heard good things about the randomForests and would 
> like to try
> to do the same thing. My biggest stumbling block is that I can't
> (obviously once I realized it) get a single 'best tree' from
> randomForests with which to apply my GIS models.  Or, is there any way
> to extract a formula from randomForest similar to a CART or rPart tree
> and apply it to a dataset outside of R?  The only solution I've been
> able to come up with is bring ALL of the environmental 
> variables into R,
> have randomForest do the prediction, and the get that prediction back
> into GIS. Thus my problem as I stated it above. I'm worried because my
> datasets are going to be huge (100's of millions of records) when we
> really get going. Should I be worried?
> 
> thanks,  Tim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Sat Dec 11 01:42:48 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 11 Dec 2004 00:42:48 +0000 (UTC)
Subject: [R] finding the most frequent row
References: <20041210135336.92746.qmail@web50302.mail.yahoo.com>
	<Pine.SGI.4.40.0412100951570.59635847-100000@origin.chass.utoronto.ca>
Message-ID: <loom.20041211T013558-425@post.gmane.org>

Jean Eid <jeaneid <at> chass.utoronto.ca> writes:

: 
: You can do the following also
: 
: 
: X <- matrix(c(1,2,1,2,1,3,1,4), ncol=2)
: Y<-unique(X)
: Y[which.max(apply(Y, 1,function(i) sum(apply(matrix(1:nrow(X)),
: 1,function(x) identical(X[x,], i[1:2]))))),]
: 
: I do not know what your strategy is when there are multiple maxima i.e two
: different  rows appear at the same frequency. The above will ge the first
: row that appears to be the maximum  as an example suppose that we have

The two calls to apply in this last statement can be simplified 
by eliminating the indices:

Y[which.max(apply(Y, 1, function(y) sum( apply(X, 1, identical, y)))),]

Also, although less efficient, its interesting to note that
the two apply calls without the sum can be regarded as multiplying 
Y times X transpose under the identical() inner product:

# multiply x times y' under the inner product f.
# With f <- function(x,y)sum(x,y) it corresponds to ordinary 
# matrix multiplication of a times b transpose.
xyt <- function(a,b,f) apply(b,1,function(x)apply(a,1,function(y)f(x,y)))

# factoring out sum this generalized multiplication gives us:
Y[which.max(colSums( xyt(Y,X,identical) )),]



From ggrothendieck at myway.com  Sat Dec 11 06:23:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 11 Dec 2004 05:23:23 +0000 (UTC)
Subject: [R] cbind() and factors.
References: <8975119BCD0AC5419D61A9CF1A923E950121B8CE@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <loom.20041211T060644-296@post.gmane.org>

michael watson (IAH-C <michael.watson <at> bbsrc.ac.uk> writes:

: 
: Hi
: 
: I'm seeing some "odd" behaviour with cbind().  My code is:
: 
: > cat <- read.table("cogs_category.txt", sep="\t", header=TRUE,
: quote=NULL, colClasses="character")
: > colnames(cat)
: [1] "Code"        "Description"
: > is.factor(cat$Code)
: [1] FALSE
: > is.factor(cat$Description)
: [1] FALSE
: > is.factor(rainbow(nrow(cat)))
: [1] FALSE
: > cat <- cbind(cat,"Color"=rainbow(nrow(cat)))
: > is.factor(cat$Color)
: [1] TRUE
: > ?cbind
: 
: I read a text file in which has two columns, Code and Description.
: Neither of these are factors.  I want to add a column of colours to the
: data frame using rainbow().  The rainbow function also does not return a
: factor.  However, if I cbind my data frame (which has no factors in it)
: and the results of rainbow() (which is a vector, not a factor), then for
: some reason the new column is a factor...??

Others have already explained the problem and given what is likely
the best solution but here is one other idea, just in case.

You may require a data frame depending on what you want to do but
if you don't then you could alternately use a character matrix
since that won't result in any conversions to factor.

Lets call the data frame from read.table, Cat.df, and our 
matrix, Cat.m.  cat is not wrong but its confusing 
since there is a common R function called cat.  Now we can 
write the following and don't have to worry about factors:

Cat.df <- read.table(...)
# create a character matrix and cbind Colors to it
Cat.m <- cbind(as.matrix(Cat.df), Color = rainbow(nrow(Cat.df)))

If you do find you need a data frame later you can convert it back
like this:

Cat.df <- as.data.frame(Cat.m)
Cat.df[] <- Cat.m  # clobber factors with character data



From judith.baltsar at gmx.de  Sat Dec 11 10:35:39 2004
From: judith.baltsar at gmx.de (judith.baltsar@gmx.de)
Date: Sat, 11 Dec 2004 10:35:39 +0100
Subject: [R] Beginners questions on matplot and legend
Message-ID: <41BACD7B.25643.A390AE@localhost>

Dear list members,
I have a problem that is probably simple to solve but at the moment 
I have no clue at all:

I have done a simple matplot
matplot(x,y,pch=symb, xlab="Axis 1", ylab="Axis 2", main="PCA of 
plot x trait matrix")
x has the X-axis values, y has four columns of y-values and now I 
want to have a simple legend for the four symbols defined by symb. 
Easy probably, but maybe someone can give me a hint.

Thank you very much
Judith



From kjetil at acelerate.com  Fri Dec 10 23:18:29 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 10 Dec 2004 18:18:29 -0400
Subject: [R] finding the most frequent row
In-Reply-To: <20041210135336.92746.qmail@web50302.mail.yahoo.com>
References: <20041210135336.92746.qmail@web50302.mail.yahoo.com>
Message-ID: <41BA20B5.1060707@acelerate.com>

bogdan romocea wrote:

>Here's something that works. I'm sure there are better solutions (in
>particular the paste part - I couldn't figure out how to avoid typing
>a[i,1], ..., a[i,10]).
>
>a <- matrix(nrow=1000,ncol=10)
>for (i in 1:1000)
>	for (j in 1:10)
>		a[i,j] <- sample(1:0,1)
>
>b <- vector(mode="character")
>for (i in 1:1000)
>	b[i] <- paste(a[i,1],a[i,2],a[i,3],a[i,4],a[i,5],
>		a[i,6],a[i,7],a[i,8],a[i,9],a[i,10],sep="")
>  
>
Or

b <- apply(a, 1, function(x) paste(x, sep="", collapse=""))

Kjetil

>#the most frequent row
>table(b)[table(b) == max(table(b))]
>
>HTH,
>b.
>
>
>-----Original Message-----
>From: Lisa Pappas [mailto:lisa.pappas at hci.utah.edu]
>Sent: Thursday, December 09, 2004 5:15 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] finding the most frequent row
>
>
>I am bootstrapping using a function that I have defined.  The
>"Statistic" of the function is an array of 10 numbers.  Therefore if
>I use 1000 replications,  the "t" matrix will have 1000 rows each of
>which is a bootstrap replicate of this 10 number array (10 columns). 
>Is there any easy way in R to determine which row appears the most
>frequently? 
>
>Thanks,
>Lisa Pappas
>
>Huntsman Cancer Institute wishes to promote open communication while
>protecting confidential and/or privileged information.  If you have
>received this message in error, please inform the sender and delete
>all copies.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From maechler at stat.math.ethz.ch  Sat Dec 11 12:55:48 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 11 Dec 2004 12:55:48 +0100
Subject: [R] Beginners questions on matplot and legend
In-Reply-To: <41BACD7B.25643.A390AE@localhost>
References: <41BACD7B.25643.A390AE@localhost>
Message-ID: <16826.57412.714394.558527@gargle.gargle.HOWL>

>>>>> "judith" == judith baltsar <judith.baltsar at gmx.de>
>>>>>     on Sat, 11 Dec 2004 10:35:39 +0100 writes:

    judith> Dear list members, I have a problem that is probably
    judith> simple to solve but at the moment I have no clue at
    judith> all:

    judith> I have done a simple matplot matplot(x,y,pch=symb,
    judith> xlab="Axis 1", ylab="Axis 2", main="PCA of plot x
    judith> trait matrix") x has the X-axis values, y has four
    judith> columns of y-values and now I want to have a simple
    judith> legend for the four symbols defined by symb.  Easy
    judith> probably, but maybe someone can give me a hint.

yes, a hint: 

  Look at the legend() call in the iris example close to the end of

help(matplot).

or try
   par(ask=TRUE)
   example(matplot)

    judith> Thank you very much   Judith

gern geschehen!
Martin



From f.harrell at vanderbilt.edu  Sat Dec 11 14:42:35 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 11 Dec 2004 08:42:35 -0500
Subject: [R] cbind() and factors.
In-Reply-To: <loom.20041211T060644-296@post.gmane.org>
References: <8975119BCD0AC5419D61A9CF1A923E950121B8CE@iahce2knas1.iah.bbsrc.reserved>
	<loom.20041211T060644-296@post.gmane.org>
Message-ID: <41BAF94B.40101@vanderbilt.edu>

Gabor Grothendieck wrote:
> michael watson (IAH-C <michael.watson <at> bbsrc.ac.uk> writes:
> 
> : 
> : Hi
> : 
> : I'm seeing some "odd" behaviour with cbind().  My code is:
> : 
> : > cat <- read.table("cogs_category.txt", sep="\t", header=TRUE,
> : quote=NULL, colClasses="character")
> : > colnames(cat)
> : [1] "Code"        "Description"
> : > is.factor(cat$Code)
> : [1] FALSE
> : > is.factor(cat$Description)
> : [1] FALSE
> : > is.factor(rainbow(nrow(cat)))
> : [1] FALSE
> : > cat <- cbind(cat,"Color"=rainbow(nrow(cat)))
> : > is.factor(cat$Color)
> : [1] TRUE
> : > ?cbind
> : 
> : I read a text file in which has two columns, Code and Description.
> : Neither of these are factors.  I want to add a column of colours to the
> : data frame using rainbow().  The rainbow function also does not return a
> : factor.  However, if I cbind my data frame (which has no factors in it)
> : and the results of rainbow() (which is a vector, not a factor), then for
> : some reason the new column is a factor...??
> 
> Others have already explained the problem and given what is likely
> the best solution but here is one other idea, just in case.
> 
> You may require a data frame depending on what you want to do but
> if you don't then you could alternately use a character matrix
> since that won't result in any conversions to factor.
> 
> Lets call the data frame from read.table, Cat.df, and our 
> matrix, Cat.m.  cat is not wrong but its confusing 
> since there is a common R function called cat.  Now we can 
> write the following and don't have to worry about factors:
> 
> Cat.df <- read.table(...)
> # create a character matrix and cbind Colors to it
> Cat.m <- cbind(as.matrix(Cat.df), Color = rainbow(nrow(Cat.df)))
> 
> If you do find you need a data frame later you can convert it back
> like this:
> 
> Cat.df <- as.data.frame(Cat.m)
> Cat.df[] <- Cat.m  # clobber factors with character data
> 

For speed, the mApply function in the Hmisc package (used by the Hmisc 
summarize function) does looping for stratified statistical summaries by 
operating on matrices rather than data frames.   factors are converted 
to numerics, and service routines can save and restore the levels and 
other attributes.  Here is an example from the summarize help file, plus 
related examples:

# To run mApply on a data frame:
m <- mApply(asNumericMatrix(x), race, h)
# Here assume h is a function that returns a matrix similar to x
at <- subsAttr(x)  # get original attributes and storage modes
matrix2dataFrame(m, at)
# Get stratified weighted means
g <- function(y) wtd.mean(y[,1],y[,2])
summarize(cbind(y, wts), llist(sex,race), g, stat.name='y')
mApply(cbind(y,wts), llist(sex,race), g)

# Compare speed of mApply vs. by for computing
d <- data.frame(sex=sample(c('female','male'),100000,TRUE),
                 country=sample(letters,100000,TRUE),
                 y1=runif(100000), y2=runif(100000))
g <- function(x) {
   y <- c(median(x[,'y1']-x[,'y2']),
          med.sum =median(x[,'y1']+x[,'y2']))
   names(y) <- c('med.diff','med.sum')
   y
}
system.time(by(d, llist(sex=d$sex,country=d$country), g))
system.time({
              x <- asNumericMatrix(d)
              a <- subsAttr(d)
              m <- mApply(x, llist(sex=d$sex,country=d$country), g)
             })
system.time({
              x <- asNumericMatrix(d)
              summarize(x, llist(sex=d$sex, country=d$country), g)
             })

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From andy_liaw at merck.com  Sat Dec 11 16:22:03 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 11 Dec 2004 10:22:03 -0500
Subject: [R] finding the most frequent row
Message-ID: <3A822319EB35174CA3714066D590DCD50994E415@usrymx25.merck.com>

> From: Kjetil Brinchmann Halvorsen
> 
> bogdan romocea wrote:
> 
> >Here's something that works. I'm sure there are better solutions (in
> >particular the paste part - I couldn't figure out how to avoid typing
> >a[i,1], ..., a[i,10]).
> >
> >a <- matrix(nrow=1000,ncol=10)
> >for (i in 1:1000)
> >	for (j in 1:10)
> >		a[i,j] <- sample(1:0,1)
> >
> >b <- vector(mode="character")
> >for (i in 1:1000)
> >	b[i] <- paste(a[i,1],a[i,2],a[i,3],a[i,4],a[i,5],
> >		a[i,6],a[i,7],a[i,8],a[i,9],a[i,10],sep="")
> >  
> >
> Or
> 
> b <- apply(a, 1, function(x) paste(x, sep="", collapse=""))

Or, as Peter pointed out recently:

  b <- apply(a, 1, paste, sep="", collapse="")

Andy
 
> Kjetil
> 
> >#the most frequent row
> >table(b)[table(b) == max(table(b))]
> >
> >HTH,
> >b.
> >
> >
> >-----Original Message-----
> >From: Lisa Pappas [mailto:lisa.pappas at hci.utah.edu]
> >Sent: Thursday, December 09, 2004 5:15 PM
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] finding the most frequent row
> >
> >
> >I am bootstrapping using a function that I have defined.  The
> >"Statistic" of the function is an array of 10 numbers.  Therefore if
> >I use 1000 replications,  the "t" matrix will have 1000 rows each of
> >which is a bootstrap replicate of this 10 number array (10 columns). 
> >Is there any easy way in R to determine which row appears the most
> >frequently? 
> >
> >Thanks,
> >Lisa Pappas
> >
> >Huntsman Cancer Institute wishes to promote open communication while
> >protecting confidential and/or privileged information.  If you have
> >received this message in error, please inform the sender and delete
> >all copies.
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> >
> >  
> >
> 
> 
> -- 
> 
> 
> Kjetil Halvorsen.
> 
> Peace is the most effective weapon of mass construction.
>                --  Mahdi Elmandjra
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From assuncao.senra at portugalmail.com  Sat Dec 11 17:03:34 2004
From: assuncao.senra at portugalmail.com (assuncao.senra@portugalmail.com)
Date: Sat, 11 Dec 2004 16:03:34 +0000
Subject: [R] applying var(base) 
Message-ID: <1102781014.41bb1a56e3473@webmail4.portugalmail.pt>


Hello,
I have the output of loglin, the parameters of a estimated loglinear models. 
They are in list form. How can I compute the variance of these parameters?

Thanks

__________________________________________________________
Sabe quanto gasta com a sua liga????o ?? Internet?
Verifique aqui: http://acesso.portugalmail.pt/contas



From lun.li at glg.ed.ac.uk  Sat Dec 11 18:42:01 2004
From: lun.li at glg.ed.ac.uk (Lun Li)
Date: Sat, 11 Dec 2004 17:42:01 +0000
Subject: [R] Parallel computing in Splus?
Message-ID: <1102786921.41bb31698b598@staffmail.ed.ac.uk>

Dear All,

Does anyone know if there is any function or package provides parallel computing
in splus?


Thanks in advance.


Lun



From adslvdll at tpg.com.au  Sat Dec 11 20:31:04 2004
From: adslvdll at tpg.com.au (stephenc)
Date: Sun, 12 Dec 2004 06:31:04 +1100
Subject: [R] graphs - saving and multiple
Message-ID: <002301c4dfb7$f85955f0$0d01a8c0@tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041212/ef20da3e/attachment.pl

From ligges at statistik.uni-dortmund.de  Sat Dec 11 21:26:11 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 11 Dec 2004 21:26:11 +0100
Subject: [R] graphs - saving and multiple
In-Reply-To: <002301c4dfb7$f85955f0$0d01a8c0@tablet>
References: <002301c4dfb7$f85955f0$0d01a8c0@tablet>
Message-ID: <41BB57E3.206@statistik.uni-dortmund.de>

stephenc wrote:
> Hi
>  
> I am doing something like this:
>  
> hist(maximumPitch, xlab="Maximum Pitch in Hertz")
>  
> which produces a nice histogram but what do I do to get two or three,
> etc on one page?

For more than one on a page, see ?par, in particular its argument "mfrow".


> I want to save the resulting file to an eps.  I can find:
>  
>  
> postscript("ex.eps")
>  
> which I then run something like my  hist above and then 
>  
>     dev.off()    
>  
> but I don't get anything in my eps file!

Starting the device with postscript(),  plotting with hist() and closing 
with dev.off() should produce something, if you haven't done very 
strange things.

Try:
  postscript("ex.eps")
  hist(1:10)
  dev.off()

Now look into the eps file. Is it still empty? Then something is wrong 
with either your postscript interpreter or your setup of R.


Uwe Ligges



> Thanks.
>  
> Stephen 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Dec 11 21:27:32 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 11 Dec 2004 21:27:32 +0100
Subject: [R] Parallel computing in Splus?
In-Reply-To: <1102786921.41bb31698b598@staffmail.ed.ac.uk>
References: <1102786921.41bb31698b598@staffmail.ed.ac.uk>
Message-ID: <41BB5834.4050403@statistik.uni-dortmund.de>

Lun Li wrote:
> Dear All,
> 
> Does anyone know if there is any function or package provides parallel computing
> in splus?

What about asking on s-news rather than r-help? The latter is intended 
for R related questions.

Uwe Ligges

> 
> Thanks in advance.
> 
> 
> Lun
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From damian.betebenner at bc.edu  Sat Dec 11 22:54:22 2004
From: damian.betebenner at bc.edu (Damian Betebenner)
Date: Sat, 11 Dec 2004 16:54:22 -0500
Subject: [R] Paths for Shell Scripts called from R
Message-ID: <web-2803901@be1.bc.edu>

Hello list,

I suspect this is more a linux question than an R question, but I'll describe my situation in case
anyone here knows of an elegant solution.

I'm using Sweave and R to create thousands of customized reports. Within an R  loop, I have R create
a table.tex file using the CAT function which, for each iteration, creates a unique table.tex file in
a subdirectory of the directory from which the master file is sourced. I have written a shell script called
latexall that pdflatexs all the files in a given directory. I want to call latexall from within R using the SYSTEM command after I've created all my table.tex files. Later, the .pdfs of these tables are read into the
document using the includegraphics command (The reason I'm creating tables this way is for
two reasons: 1. xtable and latextable are limited, especially when one is producing complicated tables
with \multicolumn headings and \raisebox commands that make the tables look nice; and 2. I've found
that precompiling tables and then inputing them into a .tex file using the \includegraphics command
takes much of the headache out of table production in LaTeX. In particular, tables that are too wide are
easily dealt with by changing the scale factor. Float placement also seems easier to me when using
\includegraphics)

My bash shell script uses a standard for loop to loop over all the .tex file in a directory

for f in *.tex

The problem that I'm having is that I can't seem to direct latexall to the correct subdirectory without placing
latexall in the subdirectory with all the table.tex files and hard coding that subdirectory into the the shell 
script before using the SYSTEM command to call latexall.

for f in /home/directory/subdirectory/tables/*.tex  

Does anyone have an idea of how I can pass the appropriate path to my shell script from within R?

I hope this makes sense. 

Any help greatly appreciated.

Damian

Damian Betebenner
Educational Research, Measurement & Evaluation
Lynch School of Education
Boston College
Chestnut Hill, MA 02467

(617) 552 4491



From feferraz at ime.usp.br  Sat Dec 11 23:50:38 2004
From: feferraz at ime.usp.br (Fernando Henrique Ferraz P. da Rosa)
Date: Sat, 11 Dec 2004 20:50:38 -0200
Subject: [R] Paths for Shell Scripts called from R
In-Reply-To: <web-2803901@be1.bc.edu>
References: <web-2803901@be1.bc.edu>
Message-ID: <20041211225038.GA29506@ime.usp.br>

Damian Betebenner writes:
> Hello list,
> 
> I suspect this is more a linux question than an R question, but I'll describe my situation in case
> anyone here knows of an elegant solution.
> 
...
> My bash shell script uses a standard for loop to loop over all the .tex file in a directory
> 
> for f in *.tex
> 
> The problem that I'm having is that I can't seem to direct latexall to the correct subdirectory without placing
> latexall in the subdirectory with all the table.tex files and hard coding that subdirectory into the the shell 
> script before using the SYSTEM command to call latexall.
> 
> for f in /home/directory/subdirectory/tables/*.tex  
> 
> Does anyone have an idea of how I can pass the appropriate path to my shell script from within R?
> 

        You could use the $* argument in your bash script, and then pass
the argument via system(), on the call to the script. Suppose you have

$ cat pdflatexall.sh 
#!/bin/bash

for i in $1/*.tex; do
        echo pdflatex $i;
done

        In R you could call by using:

        system('pdflatexall.sh /tmp')

        Works fine here:

$ touch /tmp/bla1.tex /tmp/bla2.tex
$ R
> system('pdflatexall.sh /tmp')
pdflatex /tmp/bla1.tex
pdflatex /tmp/bla2.tex




--
Fernando Henrique Ferraz P. da Rosa
http://www.ime.usp.br/~feferraz



From p.dalgaard at biostat.ku.dk  Sun Dec 12 00:19:01 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Dec 2004 00:19:01 +0100
Subject: [R] Paths for Shell Scripts called from R
In-Reply-To: <web-2803901@be1.bc.edu>
References: <web-2803901@be1.bc.edu>
Message-ID: <x2oeh0a0ey.fsf@biostat.ku.dk>

Damian Betebenner <damian.betebenner at bc.edu> writes:

> Does anyone have an idea of how I can pass the appropriate path to
> my shell script from within R?

Anything wrong with just setting the PATH variable? E.g.

> Sys.putenv(PATH=paste(Sys.getenv("PATH"),"/home/bs/pd/scripts",sep=":"))
> system("SalesRank")
Sun Dec 12 00:10:55 CET 2004
29605


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bitwrit at ozemail.com.au  Mon Dec 13 16:42:55 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Tue, 14 Dec 2004 02:42:55 +1100
Subject: [R] graphs - saving and multiple
In-Reply-To: <41BB57E3.206@statistik.uni-dortmund.de>
References: <002301c4dfb7$f85955f0$0d01a8c0@tablet>
	<41BB57E3.206@statistik.uni-dortmund.de>
Message-ID: <20041212053908.JWCM29051.smta08.mail.ozemail.net@there>

stephenc wrote:
> ...
> but I don't get anything in my eps file!
>
If you mean you get a blank image or nothing when you import the resulting PS 
file, you may need to set onefile=FALSE.

Jim



From mmsmtp at egle.cs.rtu.lv  Sun Dec 12 09:42:25 2004
From: mmsmtp at egle.cs.rtu.lv (mmsmtp@egle.cs.rtu.lv)
Date: Sun, 12 Dec 2004 08:42:25 +0000
Subject: [R] MailMonitor Alert
Message-ID: <MMSMTP2.1102840945.37@egle.cs.rtu.lv>

Scanner: MMSMTP2.0

Problem description:

  Scanning message headers: 
    

  Body part: 1 []
    SAV sweep results:  
         

  Body part: 2 [Part-2.zip]
    SAV sweep results:  A virus was detected.
         Virus found: W32/Netsky-Z
         Virus found: W32/Netsky-Z
         
    condition: virus infection
    action taken: disinfect
    
    condition: virus disinfection failed
    action taken: delete attachment



From mmsmtp at egle.cs.rtu.lv  Sun Dec 12 09:42:25 2004
From: mmsmtp at egle.cs.rtu.lv (mmsmtp@egle.cs.rtu.lv)
Date: Sun, 12 Dec 2004 08:42:25 +0000
Subject: [R] MailMonitor Alert
Message-ID: <MMSMTP2.1102840945.34@egle.cs.rtu.lv>

Scanner: MMSMTP2.0

Problem description:

  Scanning message headers: 
    

  Body part: 1 []
    SAV sweep results:  
         

  Body part: 2 [Part-2.zip]
    SAV sweep results:  A virus was detected.
         Virus found: W32/Netsky-Z
         Virus found: W32/Netsky-Z
         
    condition: virus infection
    action taken: disinfect
    
    condition: virus disinfection failed
    action taken: delete attachment



From siewlengteng at yahoo.com  Sun Dec 12 12:15:24 2004
From: siewlengteng at yahoo.com (Siew Leng TENG)
Date: Sun, 12 Dec 2004 03:15:24 -0800 (PST)
Subject: [R] Help : generating correlation matrix with a particular structure
Message-ID: <20041212111524.26471.qmail@web50508.mail.yahoo.com>

Hi,

I would like to generate a correlation matrix with a
particular structure. For example, a 3n x 3n matrix :
A_(nxn)   aI_(nxn)  bI_(nxn)
aI_(nxn)  A_(nxn)   cI_(nxn)
aI_(nxn)  cI_(nxn)  A_(nxn)

where
- A_(nxn) is a *specified* symmetric, positive
definite nxn matrix.
- I_(nxn) is an identity matrix of order n
- a, b, c are (any) real numbers

Many attempts have been unsuccessful because a
resulting matrix with any a, b, c may not be a
positive definite one, and hence cannot qualify as a
correlation matrix. Trying to first generate a
covariance matrix however, does not guarantee a
corresponding correlation matrix with the above
structure.

My larger purpose is to use this correlation matrix to
generate multivariate normal observations from the
corresponding covariance matrix (derived via cholesky
decomposition of the cor matrix).

Greatly appreciate any comments, if this is possible
or how this can be done.

Many grateful thanks and good day,
Melinda

R-version used :
---------------
Windows version
R-1.8.1
Running on Windows XP



From p.dalgaard at biostat.ku.dk  Sun Dec 12 14:58:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Dec 2004 14:58:38 +0100
Subject: [R] Help : generating correlation matrix with a particular
	structure
In-Reply-To: <20041212111524.26471.qmail@web50508.mail.yahoo.com>
References: <20041212111524.26471.qmail@web50508.mail.yahoo.com>
Message-ID: <x2sm6bd3e9.fsf@biostat.ku.dk>

Siew Leng TENG <siewlengteng at yahoo.com> writes:

> Hi,
> 
> I would like to generate a correlation matrix with a
> particular structure. For example, a 3n x 3n matrix :
> A_(nxn)   aI_(nxn)  bI_(nxn)
> aI_(nxn)  A_(nxn)   cI_(nxn)
> aI_(nxn)  cI_(nxn)  A_(nxn)
> 
> where
> - A_(nxn) is a *specified* symmetric, positive
> definite nxn matrix.
> - I_(nxn) is an identity matrix of order n
> - a, b, c are (any) real numbers
> 
> Many attempts have been unsuccessful because a
> resulting matrix with any a, b, c may not be a
> positive definite one, and hence cannot qualify as a
> correlation matrix. Trying to first generate a
> covariance matrix however, does not guarantee a
> corresponding correlation matrix with the above
> structure.

Er, a correlation matrix *is* a covariance matrix with 1 down the
diagonal... 

You need to sort out the parametrization issues. What you're trying to
achieve is quite hard. Consider the simpler case of two blocks and
n=2; what you're asking for is a covariance matrix of the form

1 r a 0
r 1 0 a
a 0 1 r
0 a r 1

so if this is the correlation matrix of (X1,Y1,X2,Y2) you want

X1 and Y1 correlated 
X2 and Y2 correlated
X1 and X2 correlated
Y1 and Y2 correlated

but

X1 and Y2 uncorrelated
Y1 and X2 uncorrelated


One approach is to work out the conditional variance of (X2,Y2) given
(X1,Y1) and check for positive semidefiniteness. You do the math...

(Some preliminary experiments suggest that the criterion could be
abs(a)+abs(r) <= 1, but don't take my word for it)

> R-version used :
> ---------------
> Windows version
> R-1.8.1
> Running on Windows XP

You might want to upgrade, but it might not do anything for you in
this respect.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From S.G.Pickering at bath.ac.uk  Sun Dec 12 15:17:36 2004
From: S.G.Pickering at bath.ac.uk (Simon Pickering)
Date: Sun, 12 Dec 2004 14:17:36 -0000
Subject: [R] R-2.0.0 on ARM (Sharp Zaurus)
Message-ID: <200412121417.aa02020@bath.ac.uk>

Hi All,

I've recently dusted off my cross-compiling hat and decided to add R to the
OpenEmbedded build environment (this is used to build the
OpenZaurus/Familiar distributions for the Zaurus/iPAQ amongst other things -
I note there's also work underway to add the Psion 5mx etc. to
OpenEmbedded).

OpenEmbedded builds now use -soft-float which makes floating point
operations something like 10x faster than the previous GCC3.x implementation
which was itself faster than the GCC2.95 implementation of the original
Sharp and OpenZaurus ROMs. That means, for those of you with Zauruses, that
a switch from OZ3.2 (or any Sharp ROM) to the current OZ3.5.2 should produce
significant speed increases when using R; perhaps even enough to make it
usable/useful? Once I have a fully compiled version I'd be more than happy
to run some benchmarks.

Another thing to note with regards to usability, is that the OpenZaurus
distribution now also comes in the GPE (X11 based) flavour, so R should be
usable with all of its graphics. For those who want to stick with Opie
(Qtopia based), a new development, XQt - which allows X11 apps to run inside
a Qtopia/Opie window without needing to use VNC, as was previously the case
- is available and it works very well.

On to my progress...

As ever the configure script isn't overly happy cross-compiling. The problem
is the way in which LD_LIBRARY_PATH is altered (I note the FIXME comments
around this bit of code). I've patched the file to remove this for my
cross-build and it now works fine. I'm also passing ac_cv_bigendian=yes to
avoid the NA problem of old (I presume this is still necessary, I'll test
once I have a working binary).

The next issue is related to the compilation process's use of the R.bin
binary during the compilation. Obviously this doesn't work very well when
you're cross-compiling.

To try to get around this, I've created a native build of R as part of the
cross-build process. I currently get an error in R-2.0.0/src/library/base:

| make[3]: Entering directory
`/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library/base'
| building package 'base'
| mkdir -p -- ../../../library/base/demo
| mkdir -p -- ../../../library/base/man
| Error in eval(expr, envir, enclos) : may already be using lazy loading on
base
| Execution halted
| make[3]: *** [all] Error 1
| make[3]: Leaving directory
`/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library/base'
| make[2]: *** [R] Error 1
| make[2]: Leaving directory
`/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library'
| make[1]: *** [R] Error 1
| make[1]: Leaving directory
`/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src'
| make: *** [R] Error 1

Although I've no idea what it's trying to do here, my guess is that this
'lazy loading' has already been enabled on the native build (and this is
what R_EXE points to as I've patched all the Makefile.in files under
R-2.0.0/src/library), and therefore it's not happy trying to re-enable it on
the cross-build.

I'd appreciate it if someone could tell me whether my method (using the
native version of R to perform these steps) can even work (or does the ARM
version have to run these steps itself)? Assuming these steps build a
database of some sort, would it be possible to just copy across the native
built one? Would this work if placed in the correct location for the ARM
build? If the ARM version has to run these steps, could they be delayed
until it is installed and then run as a batch job?

Sorry for the long post and the many questions. I Hope this is of interest
to some of you, and that you might find a bit of time to help me out.

Many thanks,



Simon

----------------------------------------
Simon Pickering MEng
Research Officer
Materials Research Centre
Faculty of Engineering & Design
University of Bath
Bath, BA2 7AY, UK

Tel: +44 (0)1225 384802
Fax: +44 (0)1225 386928



From biolearner at 163.com  Sun Dec 12 16:24:42 2004
From: biolearner at 163.com (biolearner)
Date: Sun, 12 Dec 2004 23:24:42 +0800
Subject: [R] R
Message-ID: <41BC62BA.3050202@163.com>

R



From ggrothendieck at myway.com  Sun Dec 12 16:30:32 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 12 Dec 2004 15:30:32 +0000 (UTC)
Subject: [R] Help : generating correlation matrix with a particular
	structure
References: <20041212111524.26471.qmail@web50508.mail.yahoo.com>
Message-ID: <loom.20041212T162455-263@post.gmane.org>

Siew Leng TENG <siewlengteng <at> yahoo.com> writes:

: 
: Hi,
: 
: I would like to generate a correlation matrix with a
: particular structure. For example, a 3n x 3n matrix :
: A_(nxn)   aI_(nxn)  bI_(nxn)
: aI_(nxn)  A_(nxn)   cI_(nxn)
: aI_(nxn)  cI_(nxn)  A_(nxn)
: 
: where
: - A_(nxn) is a *specified* symmetric, positive
: definite nxn matrix.
: - I_(nxn) is an identity matrix of order n
: - a, b, c are (any) real numbers
: 
: Many attempts have been unsuccessful because a
: resulting matrix with any a, b, c may not be a
: positive definite one, and hence cannot qualify as a
: correlation matrix. Trying to first generate a
: covariance matrix however, does not guarantee a
: corresponding correlation matrix with the above
: structure.
: 
: My larger purpose is to use this correlation matrix to
: generate multivariate normal observations from the
: corresponding covariance matrix (derived via cholesky
: decomposition of the cor matrix).

This can be formulated a semidefinite programming problem.
I don't think R has any packages that do that but a google
search for "semidefinite programming" will find more info and 
some free non-R software which you could consider interfacing 
to R.



From spencer.graves at pdf.com  Sun Dec 12 17:41:20 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 12 Dec 2004 08:41:20 -0800
Subject: [R] Help : generating correlation matrix with a
	particular	structure
In-Reply-To: <loom.20041212T162455-263@post.gmane.org>
References: <20041212111524.26471.qmail@web50508.mail.yahoo.com>
	<loom.20041212T162455-263@post.gmane.org>
Message-ID: <41BC74B0.6020606@pdf.com>

      Extending what Gabor and Peter have already said, the following 
should provide a partial solution: 

patternCor3 <- function(A=diag(2), a=1:3){
# nk x nk covariance & correlation matrices
# k = length(a); abs(a) <= min(diag(A)
  minV.A <- min(diag(A))
  if(any(adj.a <- abs(a)>minV.A)){
    warning("abs(a) too large;  can't exceed",
      " min(diag(A)) = ", minV.A,
      ";  forced into that range.")
    a[adj.a] <- sign(a[adj.a])*minV.A
  }
  Aa <- kronecker(diag(3), A)
  n <- dim(A)[1]
  i1 <- n+1:n
  i2 <- n+i1
  diag(Aa[1:n, i1]) <- a[1]
  diag(Aa[i1, 1:n]) <- a[1]
  diag(Aa[1:n, i2]) <- a[2]
  diag(Aa[i2, 1:n]) <- a[2]
  diag(Aa[i1, i2]) <- a[3]
  diag(Aa[i2, i1]) <- a[3]
  s.A <- sqrt(diag(A))
  r.Aa <- (Aa/outer(rep(s.A,3), rep(s.A,3)))
  eig.Aa <- eigen(Aa)
  list(Aa=Aa, corr.Aa=r.Aa, eigen.Aa=eig.Aa)
}

      If this works, all(eigen.Aa$values>=0).  Thus, you can add a test 
for this and have something close to what you want.  You could add an 
objective function that includes these eigenvalues with, say, minimum 
adjustment of "a" and feed it to "optim" and let "optim" find a solution 
that is "closest" in whatever sense you think is useful. 

      hope this helps.  spencer graves

Gabor Grothendieck wrote:

>Siew Leng TENG <siewlengteng <at> yahoo.com> writes:
>
>: 
>: Hi,
>: 
>: I would like to generate a correlation matrix with a
>: particular structure. For example, a 3n x 3n matrix :
>: A_(nxn)   aI_(nxn)  bI_(nxn)
>: aI_(nxn)  A_(nxn)   cI_(nxn)
>: aI_(nxn)  cI_(nxn)  A_(nxn)
>: 
>: where
>: - A_(nxn) is a *specified* symmetric, positive
>: definite nxn matrix.
>: - I_(nxn) is an identity matrix of order n
>: - a, b, c are (any) real numbers
>: 
>: Many attempts have been unsuccessful because a
>: resulting matrix with any a, b, c may not be a
>: positive definite one, and hence cannot qualify as a
>: correlation matrix. Trying to first generate a
>: covariance matrix however, does not guarantee a
>: corresponding correlation matrix with the above
>: structure.
>: 
>: My larger purpose is to use this correlation matrix to
>: generate multivariate normal observations from the
>: corresponding covariance matrix (derived via cholesky
>: decomposition of the cor matrix).
>
>This can be formulated a semidefinite programming problem.
>I don't think R has any packages that do that but a google
>search for "semidefinite programming" will find more info and 
>some free non-R software which you could consider interfacing 
>to R.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From neo27 at t-online.de  Sun Dec 12 20:28:00 2004
From: neo27 at t-online.de (Mark Hempelmann)
Date: Sun, 12 Dec 2004 20:28:00 +0100
Subject: [R] firefox and R 201
Message-ID: <41BC9BC0.8060806@t-online.de>

please help.

why is it that i cannot open html help pages out of the R menu? here is what I do: using browser firefox1.0 (open 
source!), java plugin jre 150 installed, supposedly working properly. opening R201patched, html help, link:search engine 
and keywords: works properly, jre symbol appears. clicking on any link (keywords on that page): no reaction whatsoever. 
what am i doing wrong?

closing and reopening firefox won't help, since the browser then asks me to create a new profile. maybe, firefox and R 
(interacting with java) are conflicting? i couldn't find any help entry, so i am sorry if this problem was addressed 
earlier.

R forever!
viele gr????e
mark hempelmann
universit??t bielefeld



From MDavy at hortresearch.co.nz  Sun Dec 12 21:13:03 2004
From: MDavy at hortresearch.co.nz (Marcus Davy)
Date: Mon, 13 Dec 2004 09:13:03 +1300
Subject: [R] Is  k  equivalent to  k:k ?
Message-ID: <s1bd5d38.002@hra2.marc.hort.cri.nz>


Havent seen anything on R-devel yet on this topic, but if the elseif statement on line 43 of seq.default was

  else if (length.out == 0) 
    double(0)

instead of 

  else if (length.out == 0) 
    numeric(0)

then that *might* satisfy Richard A. O'Keefe's comment for length=0:n cases

 >> storage.mode(seq(length=0))
    RichOK>     [1] "integer"
 >> storage.mode(seq(length=1))
    RichOK>     [1] "double"


marcus


>>> Martin Maechler <maechler at stat.math.ethz.ch> 10/12/2004 9:34:11 PM >>>
I'm diverting to R-devel, where this is really more
appropriate.  Here (R-help) only a shorter version:

>>>>> "RichOK" == Richard A O'Keefe <ok at cs.otago.ac.nz>
>>>>>     on Fri, 10 Dec 2004 14:37:16 +1300 (NZDT) writes:

    RichOK> In this discussion of seq(), can anyone explain to
    RichOK> me _why_ seq(to=n) and seq(length=3) have different
    RichOK> types?  

well, the explantion isn't hard:  look at  seq.default  :-)

    RichOK> In fact, it's worse than that (R2.0.1):

    >> storage.mode(seq(length=0))
    RichOK>     [1] "integer"
    >> storage.mode(seq(length=1))
    RichOK>     [1] "double"

  { str(.) is shorter than  storage.mode(.) }

    RichOK> If you want to pass seq(length=n) to a .C or
    RichOK> .Fortran call, it's not helpful that you can't tell
    RichOK> what the type is until you know n!  It would be nice
    RichOK> if seq(length=n) always returned the same type.  I
    RichOK> use seq(length=n) often instead of 1:n because I'd
    RichOK> like my code to work when n == 0; it would make life
    RichOK> simpler if seq(length=n) and 1:n were the same type.

now if that really makes your *life* simpler, what does that
tell us about your life  ;-) :-)

For more on this, see the "R-devel" list to which this has been diverted.

Martin Maechler, ETH Zurich

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From THOMAS.VOLSCHO at huskymail.uconn.edu  Sun Dec 12 21:23:40 2004
From: THOMAS.VOLSCHO at huskymail.uconn.edu (Thomas W Volscho)
Date: Sun, 12 Dec 2004 15:23:40 -0500
Subject: [R] switching to Linux, suggestions?
Message-ID: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>

Dear List,
I have acquired a new desktop and wanted to put a free OS on it.  I am trying Fedora Core 1, but not sure what the best Linux OS is for using R 2.0.1?

Thank you in advance for your input,
Tom Volscho

************************************        
Thomas W. Volscho
Graduate Student
Dept. of Sociology U-2068
University of Connecticut
Storrs, CT 06269
Phone: (860) 486-3882
http://vm.uconn.edu/~twv00001



From p.dalgaard at biostat.ku.dk  Sun Dec 12 21:38:57 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Dec 2004 21:38:57 +0100
Subject: [R] firefox and R 201
In-Reply-To: <41BC9BC0.8060806@t-online.de>
References: <41BC9BC0.8060806@t-online.de>
Message-ID: <x28y83qmji.fsf@biostat.ku.dk>

neo27 at t-online.de (Mark Hempelmann) writes:

> please help.
> 
> why is it that i cannot open html help pages out of the R menu? here
> is what I do: using browser firefox1.0 (open source!), java plugin jre
> 150 installed, supposedly working properly. opening R201patched, html
> help, link:search engine and keywords: works properly, jre symbol
> appears. clicking on any link (keywords on that page): no reaction
> whatsoever. what am i doing wrong?
> 
> closing and reopening firefox won't help, since the browser then asks
> me to create a new profile. maybe, firefox and R (interacting with
> java) are conflicting? i couldn't find any help entry, so i am sorry
> if this problem was addressed earlier.

Works for me on Linux (Fedora Core 3), but another old issue is there:
Links that disappear when returning to the search page, and the search
page is not responding to pgUp/pgDown keys until after the links have
become defunct. Presumably, this means that we can add 1.4.2_06 to the
list of java versions that don't work. (and apparently, no yum
repository is carrying 1.5.0 yet, and I'm not going to mess with other
forms of installs if I can help it).

Did you check out the Windows section on
http://cran.r-project.org/doc/manuals/R-admin.html ? (Not that you
actually told us which OS you are running...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From hedderik at van-rijn.org  Sun Dec 12 21:43:58 2004
From: hedderik at van-rijn.org (Hedderik van Rijn)
Date: Sun, 12 Dec 2004 21:43:58 +0100
Subject: [R] US 2004 Elections map
In-Reply-To: <Pine.LNX.4.58.0412091038430.2728@maplepark.com>
References: <9F60740C-49EF-11D9-9844-000393D7CE9A@ai.rug.nl>
	<Pine.LNX.4.58.0412091038430.2728@maplepark.com>
Message-ID: <8C1039E7-4C7E-11D9-A801-000A956B93BA@van-rijn.org>

Hi,

I've updated the page with the US election maps, now using white 
instead of purple for counties that are 50/50 (thanks to Gregoire 
Thomas for the code). I like this graph, it relays the information a 
lot better. (Note that New England's missing counties are white as 
well.) I also removed some old code that was still around in the map.r 
file, so it now actually works when source'ing the code.

    http://www.ai.rug.nl/~hedderik/R/US2004

Moreover, some people asked me about a portfolio of graphics for R, 
e.g.:

David Forrest wrote:

> I really like that -- Is there a place where R collects a portfolio of
> graphics and the code to build them?  I think these sorts of things 
> could
> help demonstrate and disseminate the graphics abilities of R.

I would be more than willing to host such a page. If someone has graphs 
(with or without code) that s/he wants to share, please let me know, 
I'll create a page with pointers (or code/graphs itself), and will 
report back to the list.

  - Hedderik.

P.S. And for those who asked whether I forgot Alaska and Hawaii, I did 
indeed forget about Alaska, but for Hawaii you just have to look 
slightly more to the left, just left of your monitor. :-)



From ma1camoj at uco.es  Sun Dec 12 22:06:46 2004
From: ma1camoj at uco.es (Juan Antonio Caballero)
Date: Sun, 12 Dec 2004 22:06:46 +0100
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
References: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
Message-ID: <1102885606.3612.12.camel@localhost.localdomain>

El dom, 12-12-2004 a las 15:23 -0500, Thomas W Volscho escribi??:
> Dear List,
> I have acquired a new desktop and wanted to put a free OS on it.  I am trying Fedora Core 1, but not sure what the best Linux OS is for using R 2.0.1?
> 
> Thank you in advance for your input,
> Tom Volscho
> 
I used R in the susseccive Fedora Core release (at the moment I use R
2.0.0 + FC3) with satisfaction.
best,
Juan Antonio  


************************************        
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Juan Antonio Caballero Molina   /"\    ASCII Ribbon Campaign
Universidad de C??rdoba          \ /    Respect for open standards
Looking for fine software        X     No HTML/RTF in email
and/or writing?                 / \    No M$ Word docs in email
http://counter.li.org                  Linux user number 346272



From brian_pfeng at yahoo.com  Sun Dec 12 22:21:52 2004
From: brian_pfeng at yahoo.com (Brian pfeng)
Date: Sun, 12 Dec 2004 15:21:52 -0600 (CST)
Subject: [R] makeARIMA() for SARIMA models
Message-ID: <20041212212152.71223.qmail@web54702.mail.yahoo.com>

I am carrying out my  project titulation in Models of 
state space and Kalman filtering. I was commended to
carry out the applications in R, what implies that I
am beginner in him. I have not had big problems, the
program is very flexible. I modeled a time of serie
with a SARIMA (3,1,2) (1,1,2) and I am very good. The
problem is now to apply Kalman, specifically with
KalmanLike (), KalmanForecast (), KalmanSmooth(),
KalmanRun(). For I use it before makeARIMA () but I
don't understand and i don't know to include the
seasonal coefficients. Then take the nuisance of
writing him to request their help. In fact I want to
know if I can include the seasonal part with the
intruction makeARIMA () and as making it, so that this
I surrender the representation in the space of the
states and I can to use Kalman.  
Thank's



From br44114 at yahoo.com  Sun Dec 12 22:29:13 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Sun, 12 Dec 2004 13:29:13 -0800 (PST)
Subject: [R] errors when trying to rename data frame columns
Message-ID: <20041212212913.91592.qmail@web50309.mail.yahoo.com>

Dear R users,

I need to rename the columns of a series of data frames. The names of
the data frames and those of the columns need to be pulled from some
vectors. I tried a couple of things but only got errors. What am I
missing?

#---create data frame
dframes <- c("a","b","c")
assign(dframes[2],data.frame(11:20,21:30))

#---rename the columns
cols <- c("one","two")

> names(get(dframes[2])) <- cols
Error: couldn't find function "get<-"
> assign(dframes[2],data.frame(cols[1]=11:20,cols[2]=21:30))
Error: syntax error
> labels(get(dframes[2]))[[2]] <- cols
Error: couldn't find function "labels<-"

I'm using R 2.0.0 on Windows XP.

Thank you,
b.



From lederer at trium.de  Sun Dec 12 22:48:15 2004
From: lederer at trium.de (lederer@trium.de)
Date: Sun, 12 Dec 2004 22:48:15 +0100 (CET)
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
References: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
Message-ID: <49121.193.158.170.127.1102888095.squirrel@193.158.170.127>

Hi,

recently, i installed Gentoo in addition to SuSE on my Laptop.
In order to see whether it was worth the effort, i did a small
benchmark for R under SuSE and Gentoo.
I guess, that the benchmarks under SuSE are also approximately
valid for other binary distributions.

As a consequence, depending on your Unix/Linux experience,
i would recommend Gentoo for optimal performance.
If you are new to Linux and want to avoid the relatively complicated
Gentoo setup, i recommend that you should least compile R from source,
which should be easy also for a Linux newbie.

The benchmark (see below for the script) consisted of
i) Generating random normals and plotting density plots.
ii) Cox model
iii) Inverting random 200x200 matrices.

I used different versions of R, since 2.0.1 is not yet included in the
Gentoo portage tree. Here are my results:

A) SuSE 9.2, R 2.0.1 (from i586 rpm):
                 benchmark cpu.user cpu.system
1        benchmark.density   222.22       6.76
11      benchmark.survival   133.69       0.27
12 benchmark.linearalgebra   365.25       3.64

B) R 1.9.0 compiled under SuSE 9.2, without additional CFLAGS
(i.e. using CFLAGS from the configure script):
                 benchmark cpu.user cpu.system
1        benchmark.density   217.31       6.12
11      benchmark.survival   101.77       0.14
12 benchmark.linearalgebra   165.49       3.34

C) R 1.9.0 compiled under SuSE 9.2, using the same CFLAGS
as in Gentoo (see below for my CFLAGS):
                 benchmark cpu.user cpu.system
1        benchmark.density   199.16       5.96
11      benchmark.survival    94.26       0.15
12 benchmark.linearalgebra   159.17       4.93

D) R 1.9.0-r1 under Gentoo, using the CFLAGS for the whole system,
not just for R.
                 benchmark cpu.user cpu.system
1        benchmark.density   176.08       6.10
11      benchmark.survival    84.20       0.14
12 benchmark.linearalgebra   134.72       6.54

My CFLAGS (for a centrino) are:
CFLAGS="-pipe -O3 -march=pentium4 -mmmx -msse -msse2 -mfpmath=sse,387
-maccumulate-outgoing-args -mno-align-stringops -fomit-frame-pointer
-ffast-math -fsched-spec-load -fprefetch-loop-arrays -ftracer
-fmove-all-movables"

Question to the Gurus: Would it be allowed, to use
-funsafe-math-optimizations?


Here is my benchmark script:

--------------------------------------------------
require(survival)

benchmark.density <- function()
{
    for (i in 1:1000)
    {
        x <- rnorm(100000)
        plot(density(x), type="l", xlim=c(-10,10), main=i)
    }
}

benchmark.survival <- function()
{
    for (i in 1:1000)
    {
        time <- c(rexp(800, 1), rexp(800, 0.8), rexp(800, 0.9),
                  rexp(800, 0.7), rexp(800, 0.5))
        time <- round(time, digits=2)  # introduce ties
        event <- as.integer(time <= 1)
        time[time > 1] <- 1
        group <- c(rep(0, 800), rep(1, 800), rep(2, 800),
                   rep(3, 800), rep(4, 800))
        plot(survfit(Surv(time,event) ~ group), xlim=c(0,1))
        title(main=i)
        dummy <- coxph(Surv(time,event) ~ group)
    }
}


benchmark.linearalgebra <- function()
{
    for (i in 1:1000)
    {
        A <- matrix(rnorm(200*200), nrow=200, ncol=200)
        AI <- solve(A)
        residual <- A %*% AI - diag(1, 200)
        hist(residual, main=i)
    }
}


my.benchmark <- function(func)
{
   funcname <- (as.character(sys.call()[[2]]))
   cat(funcname, "\n")
   times <- system.time(func())
   return(data.frame(benchmark=funcname,
                     cpu.user=times[1],
                     cpu.system=times[2]))
}

result <- my.benchmark(benchmark.density)
result <- rbind(result, my.benchmark(benchmark.survival))
result <- rbind(result, my.benchmark(benchmark.linearalgebra))

sink("benchmark.out")
cat(Sys.info(), "\n")
print(result)
sink()
--------------------------------------------------------------

Christian


> Dear List,
> I have acquired a new desktop and wanted to put a free OS on it.  I am
> trying Fedora Core 1, but not sure what the best Linux OS is for using R
> 2.0.1?
>
> Thank you in advance for your input,
> Tom Volscho
>
> ************************************
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From james.holtman at convergys.com  Sun Dec 12 23:03:31 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Sun, 12 Dec 2004 17:03:31 -0500
Subject: [R] 'object.size' takes a long time to return a value
Message-ID: <OF47D64FED.49226196-ON85256F68.00790E53@nd.convergys.com>





I was using 'object.size' to see how much memory a list was taking up.
After executing the command, I had thought that my computer had locked up.
After further testing, I determined that it was taking 241 seconds for
object.size to return a value.

I did notice in the release notes that 'object.size' did take longer when
the list contained character vectors.  Is the time that it is taking
'object.size' to return a value to be expected for such a list?

Much better results were obtained when the character vectors were converted
to factors.


######  Results from the testing  ###################
> str(x.1)
List of 10
 $ : chr [1:227299] "sadc" "sar" "date" "ksh" ...
 $ : chr [1:227299] "aprperf" "aprperf" "aprperf" "aprperf" ...
 $ : num [1:227299] 23 23 0 23 23 0 0 0 0 23 ...
 $ : num [1:227299] 0 0 0 0 0 0 0 0 0 0 ...
 $ : num [1:227299] 3600 3600 0.01 3600 3600 0.01 0.01 0.01 0.01 3600 ...
 $ : num [1:227299] 0.01 0 0.01 0 0.01 0 0.01 0 0 0.01 ...
 $ : num [1:227299] 0 0 0 0 0 0 0 0 0 0 ...
 $ : num [1:227299] 0.01 0 0.01 0 0.01 0 0.01 0 0 0.01 ...
 $ : num [1:227299] 62608 67968    29 10208 13128 ...
 $ : num [1:227299] 0 1 0 0 1 0 0 0 0 0 ...

# takes a long time (241 seconds) to report the size
> gc();system.time(print(object.size(x.1)))
          used (Mb) gc trigger  (Mb)
Ncells  711007 19.0    2235810  59.8
Vcells 5191294 39.7   14409257 110.0
[1] 34154972
[1] 241.07   0.00 241.08     NA     NA

# trying list of 1000
> x.2 <- list.subset(x.1, 1:1000);gc();system.time(print(object.size(x.2)))
          used (Mb) gc trigger  (Mb)
Ncells  711006 19.0    2235810  59.8
Vcells 4300288 32.9   14409257 110.0
[1] 145860
[1] 0.01 0.00 0.01   NA   NA

# trying list of 10,000
> x.2 <- list.subset(x.1,
1:10000);gc();system.time(print(object.size(x.2)))
          used (Mb) gc trigger  (Mb)
Ncells  711006 19.0    2235810  59.8
Vcells 4381288 33.5   14409257 110.0
[1] 1491948
[1] 0.28 0.00 0.28   NA   NA

# list of 40,000
> x.2 <- list.subset(x.1,
1:40000);gc();system.time(print(object.size(x.2)))
          used (Mb) gc trigger  (Mb)
Ncells  711006 19.0    2235810  59.8
Vcells 4651288 35.5   14409257 110.0
[1] 5988460
[1] 7.15 0.00 7.15   NA   NA

# list of 60,000
> x.2 <- list.subset(x.1,
1:60000);gc();system.time(print(object.size(x.2)))
          used (Mb) gc trigger  (Mb)
Ncells  711006 19.0    2235810  59.8
Vcells 4831288 36.9   14409257 110.0
[1] 9001556
[1] 17.33  0.00 17.32    NA    NA

# list of 100,000
> x.2 <- list.subset(x.1,
1:100000);gc();system.time(print(object.size(x.2)))
          used (Mb) gc trigger  (Mb)
Ncells  711006 19.0    2235810  59.8
Vcells 5191288 39.7   14409257 110.0
[1] 15044780
[1] 51.85  0.00 51.86    NA    NA

# list structure of the last object
> str(x.2)
List of 10
 $ : chr [1:100000] "sadc" "sar" "date" "ksh" ...
 $ : chr [1:100000] "aprperf" "aprperf" "aprperf" "aprperf" ...
 $ : num [1:100000] 23 23 0 23 23 0 0 0 0 23 ...
 $ : num [1:100000] 0 0 0 0 0 0 0 0 0 0 ...
 $ : num [1:100000] 3600 3600 0.01 3600 3600 0.01 0.01 0.01 0.01 3600 ...
 $ : num [1:100000] 0.01 0 0.01 0 0.01 0 0.01 0 0 0.01 ...
 $ : num [1:100000] 0 0 0 0 0 0 0 0 0 0 ...
 $ : num [1:100000] 0.01 0 0.01 0 0.01 0 0.01 0 0 0.01 ...
 $ : num [1:100000] 62608 67968    29 10208 13128 ...
 $ : num [1:100000] 0 1 0 0 1 0 0 0 0 0 ...

# with the first two items on the list converted to factors,
#     'object.size' performs a lot better
> str(x.1)
List of 10
 $ : Factor w/ 175 levels "#bpbkar","#bpcd",..: 132 133 60 93 13 160 60 84
60 132 ...
 $ : Factor w/ 8 levels "apra3g","aprperf",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ : num [1:227299] 23 23 0 23 23 0 0 0 0 23 ...
 $ : num [1:227299] 0 0 0 0 0 0 0 0 0 0 ...
 $ : num [1:227299] 3600 3600 0.01 3600 3600 0.01 0.01 0.01 0.01 3600 ...
 $ : num [1:227299] 0.01 0 0.01 0 0.01 0 0.01 0 0 0.01 ...
 $ : num [1:227299] 0 0 0 0 0 0 0 0 0 0 ...
 $ : num [1:227299] 0.01 0 0.01 0 0.01 0 0.01 0 0 0.01 ...
 $ : num [1:227299] 62608 67968    29 10208 13128 ...
 $ : num [1:227299] 0 1 0 0 1 0 0 0 0 0 ...
> system.time(print(object.size(x.1)))  # now it is fast
[1] 16374176
[1]  0  0  0 NA NA

> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.1
year     2004
month    11
day      15
language R
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929
--
"NOTICE:  The information contained in this electronic mail ...{{dropped}}



From beniltoncarvalho at hotmail.com  Sun Dec 12 23:08:52 2004
From: beniltoncarvalho at hotmail.com (Benilton Carvalho)
Date: Sun, 12 Dec 2004 17:08:52 -0500
Subject: [R] Multiv. NR
Message-ID: <BAY22-F3250F48B65DC88034841B1DAAA0@phx.gbl>

Do you know if R has any multiv. Newton-Raphson routine implemented?



From m.abdolell at utoronto.ca  Sun Dec 12 23:10:23 2004
From: m.abdolell at utoronto.ca (Mohamed Abdolell)
Date: Sun, 12 Dec 2004 17:10:23 -0500
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
Message-ID: <ALEJLOAANHOJCNBLGOFBIEOHCAAA.m.abdolell@utoronto.ca>

Have you tried Qunatian?  http://dirk.eddelbuettel.com/quantian.html

Not only does it have R 2.0.1, but it's got a whole bunch of other programs
already installed, including emacs, TeX, kile, and many more apps.

It's debian and based on clusterKnoppix.

You can run it "live" or install it to hard drive.

- Mohamed


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Thomas W Volscho
Sent: 12 December 2004 15:24
To: r-help at stat.math.ethz.ch
Subject: [R] switching to Linux, suggestions?


Dear List,
I have acquired a new desktop and wanted to put a free OS on it.  I am
trying Fedora Core 1, but not sure what the best Linux OS is for using R
2.0.1?

Thank you in advance for your input,
Tom Volscho

************************************
Thomas W. Volscho
Graduate Student
Dept. of Sociology U-2068
University of Connecticut
Storrs, CT 06269
Phone: (860) 486-3882
http://vm.uconn.edu/~twv00001

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Sun Dec 12 23:20:46 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Dec 2004 23:20:46 +0100
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <49121.193.158.170.127.1102888095.squirrel@193.158.170.127>
References: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
	<49121.193.158.170.127.1102888095.squirrel@193.158.170.127>
Message-ID: <x2acsjupj5.fsf@biostat.ku.dk>

lederer at trium.de writes:

> D) R 1.9.0-r1 under Gentoo, using the CFLAGS for the whole system,
> not just for R.
>                  benchmark cpu.user cpu.system
> 1        benchmark.density   176.08       6.10
> 11      benchmark.survival    84.20       0.14
> 12 benchmark.linearalgebra   134.72       6.54
> 
> My CFLAGS (for a centrino) are:
> CFLAGS="-pipe -O3 -march=pentium4 -mmmx -msse -msse2 -mfpmath=sse,387
> -maccumulate-outgoing-args -mno-align-stringops -fomit-frame-pointer
> -ffast-math -fsched-spec-load -fprefetch-loop-arrays -ftracer
> -fmove-all-movables"
> 
> Question to the Gurus: Would it be allowed, to use
> -funsafe-math-optimizations?

Not if you want R to pass its own selftests....

Notice also that for linear algebra, the use of an optimized blas
(Goto, ATLAS) will have a much stronger influence than compiler
options. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sfalcon at fhcrc.org  Sun Dec 12 23:35:55 2004
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Sun, 12 Dec 2004 14:35:55 -0800
Subject: [R] Parallel computing in Splus?
In-Reply-To: <1102786921.41bb31698b598@staffmail.ed.ac.uk>
References: <1102786921.41bb31698b598@staffmail.ed.ac.uk>
Message-ID: <2FD3B8CB-4C8E-11D9-80A9-000D933A3A9E@fhcrc.org>


On Dec 11, 2004, at 9:42 AM, Lun Li wrote:
> Does anyone know if there is any function or package provides parallel 
> computing
> in splus?
>

Consider using R and take a look at the SNOW package.



From wuertz at itp.phys.ethz.ch  Mon Dec 13 00:21:42 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 12 Dec 2004 23:21:42 +0000
Subject: [R] Re: [R-sig-finance] dates and times on Windows for fMetrics
In-Reply-To: <41BC19FF.2020800@bigpond.com>
References: <41BC19FF.2020800@bigpond.com>
Message-ID: <41BCD286.8080207@itp.phys.ethz.ch>

# Here is the solution:


require(fBasics)

# Be sure that R is running in time zone GMT.
# Set your Windows environment variable to "GMT"
# Your PC Windows clock can still run in any other time zone!
# My clock is now running in Zurich in Europe.


Date    = c("2003-10-09", "2003-10-10", "2003-10-13", "2003-10-14")
 Open   = c(1.27, 1.25, 1.27, 1.29)
 High   = c(1.28, 1.28, 1.29, 1.29)
 Low    = c(1.25, 1.25, 1.27, 1.27)
 Close  = c(1.25, 1.27, 1.28, 1.27)
 Volume = c(152810, 111338, 243843, 180211)
Data    = data.frame(Open, High, Low, Close, Volume)


# In which time zone are your data recorded?
zone = "Australia/Sydney"
# At what local time have your data been recorded?
# Say, 16:00:00 local time "Australia/Sydney" when the exchange closes?
Time = "16:00:00"
# At which Financial Center you like to use your Data?
FinCenter = "Australia/Sydney"


# Make a timeSeries Object:
sydney.ts = timeSeries(
   data = Data,
   charvec = paste(Date, Time),
   units = c("Open", "High", "Low", "Close", "Volume"),
   zone = "Australia/Sydney",
   FinCenter = "Australia/Sydney")

sydney.ts
# You should get:
#                     Open High  Low Close Volume
# 2003-10-09 16:00:00 1.27 1.28 1.25  1.25 152810
# 2003-10-10 16:00:00 1.25 1.28 1.25  1.27 111338
# 2003-10-13 16:00:00 1.27 1.29 1.27  1.28 243843
# 2003-10-14 16:00:00 1.29 1.29 1.27  1.27 180211


# Now, you are living at your "FinCenter" in Adelaide,
# but the data were recorded in the time "zone" of Sydney:

adelaide.ts = timeSeries(
   data = Data,
   charvec = paste(Date, Time),
   units = c("Open", "High", "Low", "Close", "Volume"),
   zone = "Australia/Sydney",
   FinCenter = "Australia/Adelaide")
adelaide.ts

# Or, you are living in Melbourne:

melbourne.ts = timeSeries(
   data = Data,
   charvec = paste(Date, Time),
   units = c("Open", "High", "Low", "Close", "Volume"),
   zone = "Australia/Sydney",
   FinCenter = "Australia/Melbourne") 
melbourne.ts


# Why does it fail for Perth?
# Have a look on the tail of the DST rules for Sydney:
tail(Sydney())
# You get:
#                  Sydney offSet
# 123 2028-03-25 16:00:00  36000
# 124 2028-10-28 16:00:00  39600
# 125 2029-03-24 16:00:00  36000
# 126 2029-10-27 16:00:00  39600
# 127 2030-03-30 16:00:00  36000
# 128 2030-10-26 16:00:00  39600

# Now for Perth:
tail(Perth())
# You get:
#                  Perth offSet
# 8  1974-10-26 18:00:00  32400
# 9  1975-03-01 18:00:00  28800
# 10 1983-10-29 18:00:00  32400
# 11 1984-03-03 18:00:00  28800
# 12 1991-11-16 18:00:00  32400
# 13 1992-02-29 18:00:00  28800

# OOPS ...
# The DST rules are missing after 1992.
# A quick repair:
# Let's assume that the DST dates are the same as for Sydney:
# and the offset 2 hours (120 Minutes) earlier:

rm(Perth)
PERTH <<- Perth
Perth = function() {
    Perth = paste(substring(as.character(Sydney()[52:128,1]), 1, 10), 
"18:00:00")
    offSet = Sydney()[52:128,2] - 2*60*60
    rbind(PERTH(), data.frame(Perth, offSet))
}

# Try the complete Perth():
Perth()


perth.ts = timeSeries(
   data = Data,
   charvec = paste(Date, Time),
   units = c("Open", "High", "Low", "Close", "Volume"),
   zone = "Australia/Sydney",
   FinCenter = "Australia/Perth") 
perth.ts

# You get:
#                     Open High  Low Close Volume
# 2003-10-09 14:00:00 1.27 1.28 1.25  1.25 152810
# 2003-10-10 14:00:00 1.25 1.28 1.25  1.27 111338
# 2003-10-13 14:00:00 1.27 1.29 1.27  1.28 243843
# 2003-10-14 14:00:00 1.29 1.29 1.27  1.27 180211

 

# Is that right, there are 2 hours difference from Perth to Sydney?



# Note there are some other FinCenters which are not up to date.
# The list will be checked and updated with the next version of Rmetrics.


# Regards
# Diethelm Wuertz







Tom Mulholland wrote:

> First things first
>
> R       "R version 2.0.1, 2004-11-15"
> OS.type "windows"
> GUI     "Rgui"
>
> I thought that I had the time and date stuff nearly under control. I 
> don't get the ubiquitous "GMT" warning although I'm not sure that the 
> way I have done it is correct. I use a batch file to invoke R
>
> set TZ=GMT
> rgui.exe
>
> I have a dataset that I use called tempdata
>
> > str(tempdata)
> `data.frame':   300 obs. of  7 variables:
>  $ date     : chr  "2003/10/09" "2003/10/10" "2003/10/13" "2003/10/14" 
> ...
>  $ Open     : num  1.27 1.25 1.27 1.29 1.27 1.28 1.32 1.35 1.35 1.34 ...
>  $ High     : num  1.28 1.28 1.29 1.29 1.29 1.31 1.35 1.37 1.37 1.34 ...
>  $ Low      : num  1.25 1.25 1.27 1.27 1.27 1.28 1.31 1.32 1.33 1.32 ...
>  $ Close    : num  1.25 1.27 1.28 1.27 1.28 1.31 1.35 1.35 1.34 1.33 ...
>  $ Volume   : int  152810 111338 243843 180211 159147 386021 270289 
> 690343 574630 314740 ...
>  $ dateposix:`POSIXct', format: chr  "2003-10-09" "2003-10-10" 
> "2003-10-13" "2003-10-14" ...
>
> I use the POSIXct in my own home-made plots. In playing with Fmetrics 
> I naturaly wanted to create a time series
>
> This works
> ts = timeSeries(tempdata[,2:6], charvec = tempdata[,1],format = 
> "%Y/%m/%d",FinCenter = "Australia/Sydney")
>
> Although if I set myFinCenter to "Australia/Perth" it fails. (See 
> below for structure)
>
> while
> ts = timeSeries(tempdata[,2:6], charvec = tempdata[,1],format = 
> "%Y/%m/%d",FinCenter = "Australia/Perth") fails with
>
> Error in if (timeTest == 0) iso.format = "%Y-%m-%d" :
>         missing value where TRUE/FALSE needed
>
> Ive looked at the function but I'm missing something.
>
> Any ideas would be much appreciated
>
>
> > str(ts)
> Formal class 'timeSeries' [package "fBasics"] with 7 slots
>   ..@ Data         : num [1:300, 1:5] 1.27 1.25 1.27 1.29 1.27 1.28 
> 1.32 1.35 1.35 1.34 ...
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : chr [1:300] "2003-10-09 10:00:00" "2003-10-10 10:00:00" 
> "2003-10-13 10:00:00" "2003-10-14 10:00:00" ...
>   .. .. ..$ : chr [1:5] "TS.1" "TS.2" "TS.3" "TS.4" ...
>   ..@ positions    : chr [1:300] "2003-10-09 10:00:00" "2003-10-10 
> 10:00:00" "2003-10-13 10:00:00" "2003-10-14 10:00:00" ...
>   ..@ format       : chr "%Y-%m-%d %H:%M:%S"
>   ..@ FinCenter    : chr "Australia/Sydney"
>   ..@ units        : chr [1:5] "TS.1" "TS.2" "TS.3" "TS.4" ...
>   ..@ title        : chr "Time Series Object"
>   ..@ documentation: chr "Created at Australia/Sydney 2004-12-12 
> 19:52:35"
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>



From tmulholland at bigpond.com  Mon Dec 13 00:57:57 2004
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Mon, 13 Dec 2004 07:57:57 +0800
Subject: [R] Re: [R-sig-finance] dates and times on Windows for fMetrics
In-Reply-To: <41BCD286.8080207@itp.phys.ethz.ch>
References: <41BC19FF.2020800@bigpond.com> <41BCD286.8080207@itp.phys.ethz.ch>
Message-ID: <41BCDB05.40803@bigpond.com>



Diethelm Wuertz wrote:

...
> # OOPS ...
> # The DST rules are missing after 1992.
...

We don't have daylight saving anymore,(the running joke here is that it 
fades the curtains too quickly) and it's been that way for at least a 
decade. So I don't think there are any missing rules.

I'm just on my way out to work so I'll fully digest the message once I 
get home.

Thanks.

Tom



From yzhang4 at pobox.une.edu.au  Mon Dec 13 01:12:48 2004
From: yzhang4 at pobox.une.edu.au (Yuandan Zhang)
Date: Mon, 13 Dec 2004 11:12:48 +1100
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
References: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
Message-ID: <20041213111248.0fa8f2e4.yzhang4@pobox.une.edu.au>


I have both desktop and laptop with Fedora core 3. R 2.0.1 are working well on all PCs. 

There is a problem with Fedora core 3 on HP nx500 laptop. The display resolution is 1024x768 according to HP's description. However, during the installation, there were only two options on display resolution "800x600" "640x480".

I tried various ways, but can't set it to 1024x768. any one have fix for this.

Yuandan


On Sun, 12 Dec 2004 15:23:40 -0500
Thomas W Volscho <THOMAS.VOLSCHO at huskymail.uconn.edu> wrote:

> Dear List,
> I have acquired a new desktop and wanted to put a free OS on it.  I am trying Fedora Core 1, but not sure what the best Linux OS is for using R 2.0.1?
> 
> Thank you in advance for your input,
> Tom Volscho
> 
> ************************************        
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wuertz at itp.phys.ethz.ch  Mon Dec 13 01:25:24 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon, 13 Dec 2004 00:25:24 +0000
Subject: [R] Re: [R-sig-finance] dates and times on Windows for fMetrics
In-Reply-To: <41BCDB05.40803@bigpond.com>
References: <41BC19FF.2020800@bigpond.com> <41BCD286.8080207@itp.phys.ethz.ch>
	<41BCDB05.40803@bigpond.com>
Message-ID: <41BCE174.7040208@itp.phys.ethz.ch>

Thanks for this information.

I will take care about DST in Australia for the next Rmetrics release.

Thanks Diethelm




Tom Mulholland wrote:

>
>
> Diethelm Wuertz wrote:
>
> ...
>
>> # OOPS ...
>> # The DST rules are missing after 1992.
>
> ...
>
> We don't have daylight saving anymore,(the running joke here is that 
> it fades the curtains too quickly) and it's been that way for at least 
> a decade. So I don't think there are any missing rules.
>
> I'm just on my way out to work so I'll fully digest the message once I 
> get home.
>
> Thanks.
>
> Tom
>



From xmeng at capitalbio.com  Mon Dec 13 03:10:51 2004
From: xmeng at capitalbio.com (=?gb2312?B?w8/QwA==?=)
Date: Mon, 13 Dec 2004 10:10:51 +0800
Subject: [R] about vsn
Message-ID: <302903851.22573@capitalbio.com>

Hello sir:
As to the "variance stabilization" method which is applied in the "vsn"package under R environment,here's a question about the ratio of the 2 channels:
After applying vsn,we can get new_cy5 and new_cy3 intensity according to the original cy5 and cy3 intensity respectively. And the new_cy5 new_cy3 are similar with ln(intensity).

But how can I calculate the ratio=cy5/cy3 ?

If the new_cy5 and new_cy3 is log2(intensity),the ratio equals  to 2^(log2cy5-log2cy3),but as to vsn,how to get the ratio?
Since the vsn transformation is similar to ln transformation when the intensity is high, I wanna know how high the intensity is so that I can use ln instead of vsn?

Thanks a lot!

my best regards!



From Benjamin.Osborne at uvm.edu  Mon Dec 13 03:39:48 2004
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Sun, 12 Dec 2004 21:39:48 -0500
Subject: [R] AIC, glm, lognormal distribution
Message-ID: <1102905588.41bd00f438371@webmail.uvm.edu>

I'm attempting to do model selection with AIC, using a glm and a lognormal
distribution, but:

fit1<-glm(BA~Year,data=pdat.sp1.65.04, family=gaussian(link="log"))

## gives the same result as either of the following:
fit1<-glm(BA~Year,data=pdat.sp1.65.04, family=gaussian)
fit1<-lm(BA~Year,data=pdat.sp1.65.04)

fit1
#Coefficients:
#(Intercept)     Year2004
#    -1.6341      -0.2741

#Degrees of Freedom: 84 Total (i.e. Null);  83 Residual
#Null Deviance:      1.521
#Residual Deviance: 1.476        AIC: -97.31


fit1<-glm(BA~Year,data=pdat.sp1.65.04, family=quasi(link="log"))
# also gives the same result but returns AIC: NA


## Is it possible to model a lognormal distribution without having to transform
## the data themselves?  (i.e.:

fit1<-lm(log(BA)~Year,data=pdat.sp1.65.04)



Thanks in advance,
Ben Osborne

--
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440



From Bill.Venables at csiro.au  Mon Dec 13 04:18:12 2004
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Mon, 13 Dec 2004 14:18:12 +1100
Subject: [R] AIC, glm, lognormal distribution
Message-ID: <B998A44C8986644EA8029CFE6396A9240AB044@exqld2-bne.qld.csiro.au>

Benjamin,

A couple of points:

> fit <- glm(y ~ x, gaussian(link = "log"))

does NOT fit a model with a lognormal response distribution.  It fits a
non-linear regression model with an ordinary gaussian response
distribution.  This model has constant variance, whereas the lognormal
model (which you would fit by transforming the response) has constant
coefficient of variation.  You would transform the response for two
reasons, namely it should linearize the relationship between
(transformed) response and predictors AND it should change a constant CV
into homoscedasticity, or constant variance.  This latter property as
important as the first, usually.  You should not think of a glm with log
link as a kind of handy alternative to a log-transformed regression as
they are in reality very different models.

Second point: you claim that the calls

> fitA <- glm(y ~ x, gaussian(link = "log"))
> fitB <- glm(y ~ x, gaussian)
> fitC <- lm(y ~ x)

give identical results.  This is NOT true for me on R 2.0.1 (Windows),
so you may care to check that, (although fitB and fitC are fully
equivalent, of course).

When you sort out the model you really need to use, you may find stepAIC
in the MASS library useful as one tool for model selection, or at least
for steps towards that generally rather complex goal.

Bill Venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Benjamin M.
Osborne
Sent: Monday, 13 December 2004 12:40 PM
To: r-help at stat.math.ethz.ch
Subject: [R] AIC, glm, lognormal distribution


I'm attempting to do model selection with AIC, using a glm and a
lognormal
distribution, but:

fit1<-glm(BA~Year,data=pdat.sp1.65.04, family=gaussian(link="log"))

## gives the same result as either of the following:
fit1<-glm(BA~Year,data=pdat.sp1.65.04, family=gaussian)
fit1<-lm(BA~Year,data=pdat.sp1.65.04)

fit1
#Coefficients:
#(Intercept)     Year2004
#    -1.6341      -0.2741

#Degrees of Freedom: 84 Total (i.e. Null);  83 Residual
#Null Deviance:      1.521
#Residual Deviance: 1.476        AIC: -97.31


fit1<-glm(BA~Year,data=pdat.sp1.65.04, family=quasi(link="log"))
# also gives the same result but returns AIC: NA


## Is it possible to model a lognormal distribution without having to
transform
## the data themselves?  (i.e.:

fit1<-lm(log(BA)~Year,data=pdat.sp1.65.04)



Thanks in advance,
Ben Osborne

--
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Mon Dec 13 04:55:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 13 Dec 2004 03:55:43 +0000 (UTC)
Subject: [R] errors when trying to rename data frame columns
References: <20041212212913.91592.qmail@web50309.mail.yahoo.com>
Message-ID: <loom.20041213T041758-22@post.gmane.org>

bogdan romocea <br44114 <at> yahoo.com> writes:

: 
: Dear R users,
: 
: I need to rename the columns of a series of data frames. The names of
: the data frames and those of the columns need to be pulled from some
: vectors. I tried a couple of things but only got errors. What am I
: missing?
: 
: #---create data frame
: dframes <- c("a","b","c")
: assign(dframes[2],data.frame(11:20,21:30))
: 
: #---rename the columns
: cols <- c("one","two")
: 

   df <- data.frame(11:20, 21:30)
   names(df) <- cols
   assign(dframes[2], df)

At the expense of some complexity you can do it all in one 
assign like this:

   assign(dframes[2], as.data.frame(
            mapply("{", cols, list(11:20, 21:30), SIMPLIFY = FALSE)  
   ))

Another possibility is to paste together, as a character string,
the names<- statement and then parse and eval it:

   assign(dframes[2], data.frame(11:20, 21:30))
   eval(parse(text = paste("names(", ") <- cols", sep = dframes[2])))


: > names(get(dframes[2])) <- cols
: Error: couldn't find function "get<-"
: > assign(dframes[2],data.frame(cols[1]=11:20,cols[2]=21:30))
: Error: syntax error
: > labels(get(dframes[2]))[[2]] <- cols
: Error: couldn't find function "labels<-"
: 
: I'm using R 2.0.0 on Windows XP.



From klealambrou at hotmail.com  Mon Dec 13 07:02:40 2004
From: klealambrou at hotmail.com (klea lambrou)
Date: Mon, 13 Dec 2004 06:02:40 +0000
Subject: [R] correlations
Message-ID: <BAY16-F15275E78CFD14DF378F177A6AB0@phx.gbl>



From ligges at statistik.uni-dortmund.de  Mon Dec 13 08:57:03 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 13 Dec 2004 08:57:03 +0100
Subject: [R] Multiv. NR
In-Reply-To: <BAY22-F3250F48B65DC88034841B1DAAA0@phx.gbl>
References: <BAY22-F3250F48B65DC88034841B1DAAA0@phx.gbl>
Message-ID: <41BD4B4F.1050301@statistik.uni-dortmund.de>

Benilton Carvalho wrote:
> Do you know if R has any multiv. Newton-Raphson routine implemented?

Not exactly, but see ?optim for a similar method.

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Dec 13 09:03:28 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 13 Dec 2004 09:03:28 +0100
Subject: [R] errors when trying to rename data frame columns
In-Reply-To: <20041212212913.91592.qmail@web50309.mail.yahoo.com>
References: <20041212212913.91592.qmail@web50309.mail.yahoo.com>
Message-ID: <41BD4CD0.1070802@statistik.uni-dortmund.de>

bogdan romocea wrote:

> Dear R users,
> 
> I need to rename the columns of a series of data frames. The names of
> the data frames and those of the columns need to be pulled from some
> vectors. I tried a couple of things but only got errors. What am I
> missing?
> 
> #---create data frame
> dframes <- c("a","b","c")
> assign(dframes[2],data.frame(11:20,21:30))
> 
> #---rename the columns
> cols <- c("one","two")
> 
> 
>>names(get(dframes[2])) <- cols
> 
> Error: couldn't find function "get<-"

It tells you that there is not function "get<-": you cannot assign into 
something you are calling with get()!


>>assign(dframes[2],data.frame(cols[1]=11:20,cols[2]=21:30))
> 
> Error: syntax error

Note the "[1]" in the name!



>>labels(get(dframes[2]))[[2]] <- cols

Same as above.


What you are really going to do is:


   dframes <- c("a","b","c")
   cols <- c("one","two")
   df <- data.frame(11:20, 21:30)
   names(df) <- cols
   assign(dframes[2], df)


Uwe Ligges


> Error: couldn't find function "labels<-"
> 
> I'm using R 2.0.0 on Windows XP.
> 
> Thank you,
> b.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From fredrik.bg.lundgren at bredband.net  Mon Dec 13 09:56:04 2004
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Mon, 13 Dec 2004 09:56:04 +0100
Subject: [R] random effect in logistisk regression
Message-ID: <000301c4e0f1$94059330$5f9d72d5@Larissa>

Dear R-gurus,

Where can I find a simple logistic regression model which can handle 
random effects as well among all the R-packages?

With thansk in advance

Fredrik Lundgren
Norrk??ping, Sweden



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Dec 13 10:05:33 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 13 Dec 2004 10:05:33 +0100
Subject: [R] random effect in logistisk regression
References: <000301c4e0f1$94059330$5f9d72d5@Larissa>
Message-ID: <009001c4e0f2$e75129e0$0540210a@www.domain>

Hi Fredrik,

look at functions `glmmPQL' (package MASS) and GLMM (package lme4)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>
To: "R-help" <r-help at stat.math.ethz.ch>
Sent: Monday, December 13, 2004 9:56 AM
Subject: [R] random effect in logistisk regression


> Dear R-gurus,
>
> Where can I find a simple logistic regression model which can handle 
> random effects as well among all the R-packages?
>
> With thansk in advance
>
> Fredrik Lundgren
> Norrk??ping, Sweden
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From fredrik.bg.lundgren at bredband.net  Mon Dec 13 10:30:46 2004
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Mon, 13 Dec 2004 10:30:46 +0100
Subject: [R] bootstrap package
Message-ID: <000f01c4e0f6$6cbc6880$5f9d72d5@Larissa>

Hello R'ers

In previous versions of R (I now use 2.0.1) there was a package 
"bootstrap". I wrote some programs that depended heavily on this package 
but can unfortunately not find it on Cran today. Is there any way to 
find an older version of "bootstrap" and use it with the new version - 
2.0.1?

Sincerely Fredrik Lundgren
Norrk??ping
Sweden



From rachelpearce at msn.com  Mon Dec 13 10:37:19 2004
From: rachelpearce at msn.com (Rachel Pearce)
Date: Mon, 13 Dec 2004 09:37:19 -0000
Subject: [R] Percentages in contingency tables *warning trivial question*
Message-ID: <BAY4-DAV32DC76FAB8254FD186DF01D4AB0@phx.gbl>

I hesitate to post this question in the light of recent threads, indeed
I have hesitated for several weeks, however I have come to a full stop
and really need some help if I am going to progress. I am a new user of
R for medical statistics. I have attempted to read all the relevant
documents, but would welcome any suggestions as to what I have missed.

I am trying to contruct "table 1" type contingency (mostly) tables. I
would like to include percentages, thus:

		Cases		Controls	Total
		N	%	N	%	N	%
Total		50	100	50	100	100	100


Sex: M	23 	46	27	54	50	50

etc...

I hesitate even more to mention it here, but I am thinking of something
along the lines of PROC TABULATE in SAS.

The closest I have found in the documentation I have read so far is an
example given in the help for "addmargins":

	Bee <- sample( c("Hum","Buzz"), 177, replace=TRUE )
	Sea <- sample( c("White","Black","Red","Dead"), 177,
replace=TRUE )
	...
	# Weird function needed to return the N when computing
percentages
	sqsm <- function( x ) sum( x )^2/100
	B <- table(Sea, Bee)
	round(sweep(addmargins(B, 1, list(list(All=sum, N=sqsm))), 2,
	apply( B, 2, sum )/100, "/" ), 1)
	round(sweep(addmargins(B, 2, list(list(All=sum, N=sqsm))), 1,
	apply(B, 1, sum )/100, "/"), 1)

.. Which introduced me to "sweep" and maybe could be extended to do
what I want. But I don't like using mysterious "weird" functions.

I recently found Paul Johnson's Rtips where:
http://www.ku.edu/~pauljohn/R/Rtips.html#6.1 mentioned the function
prop.table, which is also close to what I want. But how to show Ns and
percentages im the same table? 

I wondered if there were a function which does this already. Or perhaps
I should just write one for myself? Or should I not be trying to do this
in R in the first place and go back to Excel (I no longer have access to
SAS)? Please, NO! Or perhaps I am looking for the wrong thing in the
manuals? 

I have followed recent advice to look at Frank E Harrell's detailed
tabulation code, but this seems to produce many errors on my system and
with my version of R (see below). I do not have access to LaTeX
(apologies for incorrect typography). I can provide details of the
errors if it turns out that the answer to my question is RTFM by Prof
Harrell.

I would like to add my two pennorth to the debate about "trivial"
questions, of which I assume this is one. I believe that a very large
amount of what is hard about learning R on one's own with documentation
but without a real person, is a matter of vocabulary. I only found sweep
and prop.table by chance since neither of them are indexed by words like
"proportion" or "percentage" which is what I had been looking for.
Similarly I still do not know exactly what "sweep" does, since I have
never heard this verb used in a mathematical / statistical context, and
the help on sweep states that what it does is sweep. I have experienced
many similar examples in the last few weeks. This is not to say that
there is anything wrong with the help on these functions nor with the
help in general, but what R does not have is an extensive indexing
system by synonyms and uses. It is largely for reasons like this, I
believe, that trivial questions continue to be asked. If one does not
know the name of the function to do "verb" and one has tried "verb" and
the synonyms which spring to mind and drawn a blank, where to next? 

Another reason for difficulty is that while a function may exist to do
something, it is sometimes hard to find the package where it is
contained, e.g. Frank Harrell's functions seem to be in a package called
Hmisc which is not listed in the drop-down box for "load package".

System and version information:

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R     

Rachel Pearce

British Society of Blood and Marrow Tranplantation



From ccleland at optonline.net  Mon Dec 13 11:47:17 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 13 Dec 2004 05:47:17 -0500
Subject: [R] Percentages in contingency tables *warning trivial question*
In-Reply-To: <BAY4-DAV32DC76FAB8254FD186DF01D4AB0@phx.gbl>
References: <BAY4-DAV32DC76FAB8254FD186DF01D4AB0@phx.gbl>
Message-ID: <41BD7335.8000707@optonline.net>

   You might want to look at CrossTable() in the gmodels package of the 
gregmisc bundle.  For example:

 > library(gmodels)
 > sex <- as.factor(sample(c("Male", "Female"), 100, replace=TRUE))
 > case <- as.factor(sample(c("Case", "Control"), 100, replace=TRUE))
 > CrossTable(sex, case)

    Cell Contents
|-----------------|
|               N |
|   N / Row Total |
|   N / Col Total |
| N / Table Total |
|-----------------|

Total Observations in Table:  100

              | case
          sex |      Case |   Control | Row Total |
-------------|-----------|-----------|-----------|
       Female |        21 |        29 |        50 |
              |     0.420 |     0.580 |     0.500 |
              |     0.420 |     0.580 |           |
              |     0.210 |     0.290 |           |
-------------|-----------|-----------|-----------|
         Male |        29 |        21 |        50 |
              |     0.580 |     0.420 |     0.500 |
              |     0.580 |     0.420 |           |
              |     0.290 |     0.210 |           |
-------------|-----------|-----------|-----------|
Column Total |        50 |        50 |       100 |
              |     0.500 |     0.500 |           |
-------------|-----------|-----------|-----------|

Rachel Pearce wrote:
> I hesitate to post this question in the light of recent threads, indeed
> I have hesitated for several weeks, however I have come to a full stop
> and really need some help if I am going to progress. I am a new user of
> R for medical statistics. I have attempted to read all the relevant
> documents, but would welcome any suggestions as to what I have missed.
> 
> I am trying to contruct "table 1" type contingency (mostly) tables. I
> would like to include percentages, thus:
> 
> 		Cases		Controls	Total
> 		N	%	N	%	N	%
> Total		50	100	50	100	100	100
> 
> 
> Sex: M	23 	46	27	54	50	50
> 
> etc...
> 
> I hesitate even more to mention it here, but I am thinking of something
> along the lines of PROC TABULATE in SAS.
> 
> The closest I have found in the documentation I have read so far is an
> example given in the help for "addmargins":
> 
> 	Bee <- sample( c("Hum","Buzz"), 177, replace=TRUE )
> 	Sea <- sample( c("White","Black","Red","Dead"), 177,
> replace=TRUE )
> 	...
> 	# Weird function needed to return the N when computing
> percentages
> 	sqsm <- function( x ) sum( x )^2/100
> 	B <- table(Sea, Bee)
> 	round(sweep(addmargins(B, 1, list(list(All=sum, N=sqsm))), 2,
> 	apply( B, 2, sum )/100, "/" ), 1)
> 	round(sweep(addmargins(B, 2, list(list(All=sum, N=sqsm))), 1,
> 	apply(B, 1, sum )/100, "/"), 1)
> 
> .. Which introduced me to "sweep" and maybe could be extended to do
> what I want. But I don't like using mysterious "weird" functions.
> 
> I recently found Paul Johnson's Rtips where:
> http://www.ku.edu/~pauljohn/R/Rtips.html#6.1 mentioned the function
> prop.table, which is also close to what I want. But how to show Ns and
> percentages im the same table? 
> 
> I wondered if there were a function which does this already. Or perhaps
> I should just write one for myself? Or should I not be trying to do this
> in R in the first place and go back to Excel (I no longer have access to
> SAS)? Please, NO! Or perhaps I am looking for the wrong thing in the
> manuals? 
> 
> I have followed recent advice to look at Frank E Harrell's detailed
> tabulation code, but this seems to produce many errors on my system and
> with my version of R (see below). I do not have access to LaTeX
> (apologies for incorrect typography). I can provide details of the
> errors if it turns out that the answer to my question is RTFM by Prof
> Harrell.
> 
> I would like to add my two pennorth to the debate about "trivial"
> questions, of which I assume this is one. I believe that a very large
> amount of what is hard about learning R on one's own with documentation
> but without a real person, is a matter of vocabulary. I only found sweep
> and prop.table by chance since neither of them are indexed by words like
> "proportion" or "percentage" which is what I had been looking for.
> Similarly I still do not know exactly what "sweep" does, since I have
> never heard this verb used in a mathematical / statistical context, and
> the help on sweep states that what it does is sweep. I have experienced
> many similar examples in the last few weeks. This is not to say that
> there is anything wrong with the help on these functions nor with the
> help in general, but what R does not have is an extensive indexing
> system by synonyms and uses. It is largely for reasons like this, I
> believe, that trivial questions continue to be asked. If one does not
> know the name of the function to do "verb" and one has tried "verb" and
> the synonyms which spring to mind and drawn a blank, where to next? 
> 
> Another reason for difficulty is that while a function may exist to do
> something, it is sometimes hard to find the package where it is
> contained, e.g. Frank Harrell's functions seem to be in a package called
> Hmisc which is not listed in the drop-down box for "load package".
> 
> System and version information:
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R     
> 
> Rachel Pearce
> 
> British Society of Blood and Marrow Tranplantation
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From maechler at stat.math.ethz.ch  Mon Dec 13 12:20:48 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 13 Dec 2004 12:20:48 +0100
Subject: [R] 'object.size' takes a long time to return a value
In-Reply-To: <OF47D64FED.49226196-ON85256F68.00790E53@nd.convergys.com>
References: <OF47D64FED.49226196-ON85256F68.00790E53@nd.convergys.com>
Message-ID: <16829.31504.325876.705673@gargle.gargle.HOWL>

>>>>> "james" == james holtman <james.holtman at convergys.com>
>>>>>     on Sun, 12 Dec 2004 17:03:31 -0500 writes:

    james> I was using 'object.size' to see how much memory a
    james> list was taking up.  After executing the command, I
    james> had thought that my computer had locked up.  After
    james> further testing, I determined that it was taking 241
    james> seconds for object.size to return a value.

    james> I did notice in the release notes that 'object.size'
    james> did take longer when the list contained character
    james> vectors.  Is the time that it is taking 'object.size'
    james> to return a value to be expected for such a list?

yes, partly its expected to take longer than for others,
but, actually, it does take longer than I would have expected,
even after starting to think about it:

Every element of your character vector is a string which is
coded ``as a vector of bytes with a string terminator'' 
(simplification).  To find a string length, i.e., what the R
function  nchar() also does,  "one" has to read all character up
to the string terminator.  That's much slower than just
using the hard coded fact that an integer is 4 bytes or a double
is 8.

    james> Much better results were obtained when the character
    james> vectors were converted to factors.

yes; since your factor only had a dozen or at most 175 levels;
and only these are character; the factor *data* are integers.

However, what I say above does not explain everything about
the slowness of object.size( <character> ).
We would have to go into the C code and the exact implementation
of object.size() to see the reason - and think about possible
improvements.

BTW: Note that R saves memory when character elements are
     "shared"; e.g., for me (on 64-bit Linux, 2.0.1patched),

  > object.size(rep("abcedfghijklmn", 3))
  [1] 152
  > object.size(c("abcedfghijklmn", "ABCEDFGHIJKLMN", "ABCEDFGHijklmn"))
  [1] 296


Here is some code to experiment further
which slowly constructs character vectors where (I think)
no "sharing" takes place:

rChar <- function(n, m, ch.set = c(LETTERS,letters))
{
    ## Purpose: create random character vector
    ## ----------------------------------------------------------------------
    ## Arguments: n: length of vector
    ##            m: "average" string length
    ## ----------------------------------------------------------------------
    ## Author: Martin Maechler, Date: 13 Dec 2004, 11:35
    sapply(rpois(n, lambda=m),
           function(m) paste(sample(ch.set, size=m), collapse=""))
}

lc <- rChar(1e5, 4)# already takes several seconds on a fast machine

## This is on 64-bit [AMD Athlon(tm) 64 Processor 2800+] "lynne":
system.time(print(object.size(lc)))
## [1] 7240464
## [1] 2.11 0.00 2.14 0.00 0.00

system.time(print(sum(nchar(lc)))) # which is **MUCH** faster
## [1] 399461
## [1] 0.02 0.00 0.02 0.00 0.00

## but still quite slower
system.time(print(for(i in 1:10)sn <- sum(nchar(lc))))## 0.10
## than
lx <- rnorm(1e5)
system.time(print(for(i in 1:10)os <- object.size(lx)))## 0.01

##------------


Note that if we continue this topic, it should probably be moved
to R-devel, since it's getting technical and about R internals
(in coded in C).

--
Martin Maechler, ETH Zurich



From p.dalgaard at biostat.ku.dk  Mon Dec 13 12:23:05 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Dec 2004 12:23:05 +0100
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <20041213111248.0fa8f2e4.yzhang4@pobox.une.edu.au>
References: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
	<20041213111248.0fa8f2e4.yzhang4@pobox.une.edu.au>
Message-ID: <x2zn0iphly.fsf@biostat.ku.dk>

Yuandan Zhang <yzhang4 at pobox.une.edu.au> writes:

> I have both desktop and laptop with Fedora core 3. R 2.0.1 are working well on all PCs. 
> 
> There is a problem with Fedora core 3 on HP nx500 laptop. The display resolution is 1024x768 according to HP's description. However, during the installation, there were only two options on display resolution "800x600" "640x480".
> 
> I tried various ways, but can't set it to 1024x768. any one have fix for this.

You may want to set the bits per pixel first (you can reconfigure the
screen via the control panel as far as I recall). Anyways, this works
fine on other laptops, and is not at all R related, so you should
probably ask elsewhere...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From kjetil at acelerate.com  Mon Dec 13 12:37:29 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 13 Dec 2004 07:37:29 -0400
Subject: [R] bootstrap package
In-Reply-To: <000f01c4e0f6$6cbc6880$5f9d72d5@Larissa>
References: <000f01c4e0f6$6cbc6880$5f9d72d5@Larissa>
Message-ID: <41BD7EF9.5050706@acelerate.com>

Fredrik Lundgren wrote:

> Hello R'ers
>
> In previous versions of R (I now use 2.0.1) there was a package 
> "bootstrap". I wrote some programs that depended heavily on this 
> package but can unfortunately not find it on Cran today. Is there any 
> way to find an older version of "bootstrap" and use it with the new 
> version - 2.0.1?

Bootstrap should have been in the "Orphaned" subdierctory on CRAN, but 
is'nt there?
What happened?
It can be found in the "Archive" subdirectory, thoug.

Kjetil

>
> Sincerely Fredrik Lundgren
> Norrk??ping
> Sweden
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From bxc at steno.dk  Mon Dec 13 12:36:48 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 13 Dec 2004 12:36:48 +0100
Subject: [R] Percentages in contingency tables *warning trivial question*
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FDF98@exdkba022.novo.dk>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rachel Pearce
> Sent: Monday, December 13, 2004 10:37 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Percentages in contingency tables *warning 
> trivial question*
> 
> 
> I hesitate to post this question in the light of recent 
> threads, indeed I have hesitated for several weeks, however I 
> have come to a full stop and really need some help if I am 
> going to progress. I am a new user of R for medical 
> statistics. I have attempted to read all the relevant 
> documents, but would welcome any suggestions as to what I have missed.
> 
> I am trying to contruct "table 1" type contingency (mostly) 
> tables. I would like to include percentages, thus:
> 
> 		Cases		Controls	Total
> 		N	%	N	%	N	%
> Total		50	100	50	100	100	100
> 
> 
> Sex: M	23 	46	27	54	50	50
> 
> etc...
> 
> I hesitate even more to mention it here, but I am thinking of 
> something along the lines of PROC TABULATE in SAS.

This is one of the holes in the tabulation features in R.
The simplest feature needed in the one in addmargins, but
tabulation is still rudimentary in R.

I'm afraid that what you want would reqire:

1. Make the table of counts
2. Make the table of percentages by sweeping out a margin
   ( i.e. take the margin and divide the entire table by that,
     - sweeping is just the generalization of this; use any
     desired function instesd of "/" )
3. Define a new table with an extra dimension (c("N","pct")) and
   fill in the two original tables there.

The last step is necessary in the absence of a generalized cbind/rbind
for tables/arrays.

Please correct me if such a thing exists. If it does, it should be
referenced under "see also" in the help page for cbind.

The weird example in addmargins only covers the case where a table of
percentages is wanted with a margin of total counts, not the general
problem.

Somebody should sit down a write a reasonable tabulation feature for R,
but the problem in itself is complcated, so the syntax is likely to be
arcane. For example, take a look at the syntax for proc tabulate in SAS,
which is very strange, but given the features it covers (which are all 
desirable) it is difficult to come up with something simpler.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------

> The closest I have found in the documentation I have read so 
> far is an example given in the help for "addmargins":
> 
> 	Bee <- sample( c("Hum","Buzz"), 177, replace=TRUE )
> 	Sea <- sample( c("White","Black","Red","Dead"), 177, 
> replace=TRUE )
> 	...
> 	# Weird function needed to return the N when computing 
> percentages
> 	sqsm <- function( x ) sum( x )^2/100
> 	B <- table(Sea, Bee)
> 	round(sweep(addmargins(B, 1, list(list(All=sum, N=sqsm))), 2,
> 	apply( B, 2, sum )/100, "/" ), 1)
> 	round(sweep(addmargins(B, 2, list(list(All=sum, N=sqsm))), 1,
> 	apply(B, 1, sum )/100, "/"), 1)
> 
> .. Which introduced me to "sweep" and maybe could be extended 
> to do what I want. But I don't like using mysterious "weird" 
> functions.
> 
> I recently found Paul Johnson's Rtips where: 
> http://www.ku.edu/~pauljohn/R/Rtips.html#6.1 mentioned the 
> function prop.table, which is also close to what I want. But 
> how to show Ns and percentages im the same table? 
> 
> I wondered if there were a function which does this already. 
> Or perhaps I should just write one for myself? Or should I 
> not be trying to do this in R in the first place and go back 
> to Excel (I no longer have access to SAS)? Please, NO! Or 
> perhaps I am looking for the wrong thing in the manuals? 
> 
> I have followed recent advice to look at Frank E Harrell's 
> detailed tabulation code, but this seems to produce many 
> errors on my system and with my version of R (see below). I 
> do not have access to LaTeX (apologies for incorrect 
> typography). I can provide details of the errors if it turns 
> out that the answer to my question is RTFM by Prof Harrell.
> 
> I would like to add my two pennorth to the debate about 
> "trivial" questions, of which I assume this is one. I believe 
> that a very large amount of what is hard about learning R on 
> one's own with documentation but without a real person, is a 
> matter of vocabulary. I only found sweep and prop.table by 
> chance since neither of them are indexed by words like 
> "proportion" or "percentage" which is what I had been looking 
> for. Similarly I still do not know exactly what "sweep" does, 
> since I have never heard this verb used in a mathematical / 
> statistical context, and the help on sweep states that what 
> it does is sweep. I have experienced many similar examples in 
> the last few weeks. This is not to say that there is anything 
> wrong with the help on these functions nor with the help in 
> general, but what R does not have is an extensive indexing 
> system by synonyms and uses. It is largely for reasons like 
> this, I believe, that trivial questions continue to be asked. 
> If one does not know the name of the function to do "verb" 
> and one has tried "verb" and the synonyms which spring to 
> mind and drawn a blank, where to next? 
> 
> Another reason for difficulty is that while a function may 
> exist to do something, it is sometimes hard to find the 
> package where it is contained, e.g. Frank Harrell's functions 
> seem to be in a package called Hmisc which is not listed in 
> the drop-down box for "load package".
> 
> System and version information:
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R     
> 
> Rachel Pearce
> 
> British Society of Blood and Marrow Tranplantation
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Dec 13 13:02:09 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Mon, 13 Dec 2004 12:02:09 +0000 (GMT)
Subject: [R] bootstrap package
In-Reply-To: <41BD7EF9.5050706@acelerate.com>
Message-ID: <Pine.GSO.4.31.0412131155000.11283-100000@markov.stats>

On Mon, 13 Dec 2004, Kjetil Brinchmann Halvorsen wrote:

> Fredrik Lundgren wrote:
>
> > Hello R'ers
> >
> > In previous versions of R (I now use 2.0.1) there was a package
> > "bootstrap". I wrote some programs that depended heavily on this
> > package but can unfortunately not find it on Cran today. Is there any
> > way to find an older version of "bootstrap" and use it with the new
> > version - 2.0.1?
>
> Bootstrap should have been in the "Orphaned" subdierctory on CRAN, but
> is'nt there?
> What happened?

Things are not done retrospectively.  Package bootstrap (and multiv) was
orphaned before the "Orphaned" subdirectory was instituted.

More a question of `what didn't happen' ....

> It can be found in the "Archive" subdirectory, thoug.

The fact that it is no longer in the main area indicates that it probably
does not pass R CMD check with 2.0.x.

Note to Fredrik Lundgren: it is open to you to take over as maintainer:
see Orphaned/README.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Herbert_Desson at jltgroup.com  Mon Dec 13 13:24:10 2004
From: Herbert_Desson at jltgroup.com (Herbert_Desson@jltgroup.com)
Date: Mon, 13 Dec 2004 12:24:10 -0000
Subject: [R] Re: Help : generating correlation matrix with a particular
Message-ID: <E624774E77218D4497DCD28E325D78640A64A266@uk-lon-email01.uk.group.local>

************************************************************
Important: We would  draw your  attention to the  notices at 
the bottom of this  e-mail, particularly  before opening and 
reviewing any file attachment(s).
************************************************************

Here is some code we have used.

a<-array(c(1,.9,.7,.9,1,.3,.7,.3,1),dim=c(3,3))
a
s<-eigen(a)$vectors
l<-diag(eigen(a)$values)
l[l<0]<-0
b<-s%*%sqrt(l)
for(i in 1:nrow(b)){b[i,]<-b[i,]/sqrt(sum(b[i,]^2))}
ap<-b%*%t(b)
ap


It is based on a paper by Rebonato etal that formerly was at
www.rebonato.com/correlationmatrix.pdf.
Unfortunately the website has disappeared.

Best regards,
Herb


Herbert G. Desson, ACAS, MAAA

Actuary
JLT Risk Solutions
6 Crutched Friars
London EC3N 2PH

phone:  +44 (0)20 7528 4702
fax:       +44 (0)20 7558 3785




>Message: 2
>Date: 12 Dec 2004 14:58:38 +0100
>From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>Subject: Re: [R] Help : generating correlation matrix with a
>	particular	structure
>To: Siew Leng TENG <siewlengteng at yahoo.com>
>Cc: r-help at stat.math.ethz.ch
>Message-ID: <x2sm6bd3e9.fsf at biostat.ku.dk>
>Content-Type: text/plain; charset=us-ascii
>
>Siew Leng TENG <siewlengteng at yahoo.com> writes:
>
>> Hi,
>> 
>> I would like to generate a correlation matrix with a
>> particular structure. For example, a 3n x 3n matrix :
>> A_(nxn)   aI_(nxn)  bI_(nxn)
>> aI_(nxn)  A_(nxn)   cI_(nxn)
>> aI_(nxn)  cI_(nxn)  A_(nxn)
>> 
>> where
>> - A_(nxn) is a *specified* symmetric, positive
>> definite nxn matrix.
>> - I_(nxn) is an identity matrix of order n
>> - a, b, c are (any) real numbers
>> 
>> Many attempts have been unsuccessful because a
>> resulting matrix with any a, b, c may not be a
>> positive definite one, and hence cannot qualify as a
>> correlation matrix. Trying to first generate a
>> covariance matrix however, does not guarantee a
>> corresponding correlation matrix with the above
>> structure.
>
>Er, a correlation matrix *is* a covariance matrix with 1 down the
>diagonal... 
>
>You need to sort out the parametrization issues. What you're trying to
>achieve is quite hard. Consider the simpler case of two blocks and
>n=2; what you're asking for is a covariance matrix of the form
>
>1 r a 0
>r 1 0 a
>a 0 1 r
>0 a r 1
>
>so if this is the correlation matrix of (X1,Y1,X2,Y2) you want
>
>X1 and Y1 correlated 
>X2 and Y2 correlated
>X1 and X2 correlated
>Y1 and Y2 correlated
>
>but
>
>X1 and Y2 uncorrelated
>Y1 and X2 uncorrelated
>
>
>One approach is to work out the conditional variance of (X2,Y2) given
>(X1,Y1) and check for positive semidefiniteness. You do the math...
>
>(Some preliminary experiments suggest that the criterion could be
>abs(a)+abs(r) <= 1, but don't take my word for it)
>
>> R-version used :
>> ---------------
>> Windows version
>> R-1.8.1
>> Running on Windows XP
>
>You might want to upgrade, but it might not do anything for you in
>this respect.
>
>-- 
>   O__  ---- Peter Dalgaard             Blegdamsvej 3  
>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



************************************************************
JLT Risk Solutions Ltd
6 Crutched Friars, London EC3N 2PH. Co Reg No 1536540
Tel: (44) (0)20 7528 4000   Fax: (44) (0)20 7528 4500
http://www.jltgroup.com
Lloyd's Broker.  Regulated by the General Insurance
Standards Council
------------------------------------------------------------
The content of this e-mail (including any attachments) as 
received may not be the same as sent. If you consider that 
the content is material to the formation or performance of 
a contract or you are otherwise relying upon its accuracy, 
you should consider requesting a copy be sent by facsimile 
or normal mail.  In any event, please check this message
and any identified file attachment(s) upon receipt and notify
the sender immediately if there is any manifest transmission
error, omission or corruption. This does not change or reduce
any party's duty of utmost good faith when contracting for
insurance or reinsurance. The information in this e-mail is 
confidential and may be legally privileged. If you are not 
the intended recipient, please notify the sender immediately 
and then delete this e-mail entirely - you must not retain, 
copy, distribute or use this e-mail for any purpose or 
disclose any of its content to others.

Opinions, conclusions and other information in this e-mail 
that do not relate to the official business of JLT Risk 
Solutions Ltd shall be understood as neither given nor 
endorsed by it.  Please note we intercept and monitor 
incoming / outgoing e-mail and therefore you should neither 
expect nor intend any e-mail to be private in nature.

We have checked this e-mail for viruses and other harmful 
components and believe but not guarantee it virus-free prior 
to leaving our computer system.  However, you should satisfy 
yourself that it is free from harmful components, as we do 
not accept responsibility for any loss or damage it may 
cause to your computer systems.



From christoph.lehmann at gmx.ch  Mon Dec 13 13:27:11 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 13 Dec 2004 13:27:11 +0100
Subject: [R] classification for huge datasets: SVM yields memory troubles
Message-ID: <41BD8A9F.4040509@gmx.ch>

Hi
I have a matrix with 30 observations and roughly 30000 variables, each 
obs belongs to one of two groups. With svm and slda I get into memory 
troubles ('cannot allocate vector of size' roughly 2G). PCA LDA runs 
fine. Are there any way to use the memory issue withe SVM's? Or can you 
recommend any other classification method for such huge datasets?


P.S. I run suse 9.1 on a 2G RAM PIV machine.
thanks for a hint

Christoph



From thpe at hhbio.wasser.tu-dresden.de  Mon Dec 13 13:54:11 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 13 Dec 2004 13:54:11 +0100
Subject: [R] bootstrap package
In-Reply-To: <000f01c4e0f6$6cbc6880$5f9d72d5@Larissa>
References: <000f01c4e0f6$6cbc6880$5f9d72d5@Larissa>
Message-ID: <41BD90F3.9000301@hhbio.wasser.tu-dresden.de>

Fredrik Lundgren wrote:
> Hello R'ers
> 
> In previous versions of R (I now use 2.0.1) there was a package 
> "bootstrap". I wrote some programs that depended heavily on this package 
> but can unfortunately not find it on Cran today. Is there any way to 
> find an older version of "bootstrap" and use it with the new version - 
> 2.0.1?

I had the same problem some weeks ago and found the sourcecode at

http://cran.r-project.org/src/contrib/Archive/

I downloaded the most recent version bootstrap_1.0-12.tar.gz and 
compiled and installed it on R 2.0.0/Windows. There are some warnings 
during compilation (Codoc mismatches) and at runtime (multi-argument 
returns are deprecated...) and it *still* works.

Thomas Petzoldt



From sergei at stams.strath.ac.uk  Mon Dec 13 13:59:33 2004
From: sergei at stams.strath.ac.uk (Sergei Zuyev)
Date: Mon, 13 Dec 2004 12:59:33 +0000
Subject: [R] HTML help index generation problem with R under Windows
In-Reply-To: <41B9BDDD.2060504@statistik.uni-dortmund.de>
References: <200412091723.08225.sergei@stams.strath.ac.uk>
	<41B9BDDD.2060504@statistik.uni-dortmund.de>
Message-ID: <200412131259.33474.sergei@stams.strath.ac.uk>

On Friday 10th of December 2004 15:16, Uwe Ligges wrote:
> Sergei Zuyev wrote:
> > Hello,
> > I am wondering if there has been a solution to the following issue with R
> > under Windows (see also thread to PR#6662 in this mailing list:
> > http://tolstoy.newcastle.edu.au/R/devel/04a/0550.html )
> > I am using R for teaching students Statistics, so they are working with a
> > university-wide installation of R. I have compiled an R-library which
> > contains all the instructions and customised functions for the course and
> > the students have been using it successfully under UNIX for the last 5
> > years. I keep the library in my own space which is pointed to in R_LIBS
> > variable defined for the students. I myself not a sysadmin for the
> > university network, so I cannot install to the system-wide library
> > directory, but this was not a matter. This year, I have to (sniff-sniff)
> > switch to Windows platform and the following problem arose.
> > The library loads fine and the text-style help files show OK, but
> > help.start() reports the following error:
> > Error in file(f.tg, open = "w") : unable to open connection
> >  In addition: Warning messages:
> >  1: cannot update HTML package index in: make.packages.html(lib.loc)
> >  2: cannot open file `C:\Progra~1\R\rw1071/doc/html/search/index.txt'
> > The index-file is opening but does not contain links to my library.
> >  As far as I understand, the problem lies in the lack of write
> > permissions for the system directory where R is installed. While UNIX
> > version creates the html-help files in per-session way in the user's home
> > directory, the windows version atempts to modify the system-wide file.
> > Having possibility to display html-help would be a great benefit, as the
> > students can follow the links present there which is impossible from
> > within "inline" help. So my question:
> >
> > Is there any work-around that problem? Would making creation of html help
> > files in the users' space under Windows as under UNIX/linux resolve the
> > issue? Or maybe, it's already been fixed in the recent version of R?
> >
> > Thanks in advance for your help! Please, make a copy of your response to
> > my email below.
>
> Two points:
>
> a) Please upgrade, R-1.7.1 is really ancient.
> b) help.start(update=FALSE) should work.
>
> Uwe Ligges

Thanks, Uwe, for your response!
a) I'll try to convince our IT people to upgrade, but they are reluctant to do 
that often. I must confess, I undestand them - so many times I had to 
re-compile and re-install packages after R-upgrades... This indeed might 
create a havoc for other users in the middle of the teaching term.

b) This indeed opens an html help but does not fix the problem - my package is 
not showing up in the help. This could be expected since the index file is 
unchanged from its canonical state.

Any more ideas, please?
-- 
=================================================================
                           Dr. Sergei ZUYEV
Statistics and Modelling Science dept., University of Strathclyde
    Livingston Tower, 26 Richmond str., Glasgow, G1 1XH, U.K.
     Tel.: +44 (0)141 548 3663    Fax:  +44 (0)141 552 2079
E-mail: sergei at stams.strath.ac.uk   http://www.stams.strath.ac.uk



From ashelton at albany.edu  Mon Dec 13 16:48:15 2004
From: ashelton at albany.edu (Anne Shelton)
Date: Mon, 13 Dec 2004 10:48:15 -0500
Subject: [R] (no subject)
Message-ID: <63D883CB0A1B204EB053673DD882CB5305356261@email.albany.edu>


I have installed Parallel R on a LINUX Cluster and I am trying to initiate
the Parallel engine function in pR but I am receiving this error

	Error in namespaceExport(ns, exports) : undefined exports: fixPre1.8
	> StartPE(2)
 	chr [1:2] "/usr/local/lib/R/library/base/exec/pRBatch.R" "NULL"
	Failed to create scheduler thread.
	Failed to create threads.
	Error in StartPE(2) : Failed to initialize the parallel engine
	In addition: Warning message: 
	package methods in options("defaultPackages") was not found 
	> 
	MPI process rank 0 (n0, p23798) caught a SIGSEGV.
	>
--------------------------------------------------------------------	
	One of the processes started by mpirun has exited with a nonzero
exit
	code.  This typically indicates that the process finished in error.
	If your process did not finish in error, be sure to include a
"return
	0" or "exit(0)" in your C code before exiting the application.

	PID 23804 failed on node n0 (127.0.0.1) due to signal 15.


Any insight on this error would be helpful.



From maechler at stat.math.ethz.ch  Mon Dec 13 17:03:36 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 13 Dec 2004 17:03:36 +0100
Subject: [R] Re: Help : generating correlation matrix with a particular
In-Reply-To: <E624774E77218D4497DCD28E325D78640A64A266@uk-lon-email01.uk.group.local>
References: <E624774E77218D4497DCD28E325D78640A64A266@uk-lon-email01.uk.group.local>
Message-ID: <16829.48472.506417.852872@gargle.gargle.HOWL>

>>>>> "Herbert" == Herbert Desson <Herbert_Desson at jltgroup.com>
>>>>>     on Mon, 13 Dec 2004 12:24:10 -0000 writes:


    Herbert> Here is some code we have used.

    Herbert> a<-array(c(1,.9,.7,.9,1,.3,.7,.3,1),dim=c(3,3))
    Herbert> a
    Herbert> s<-eigen(a)$vectors
    Herbert> l<-diag(eigen(a)$values)
    Herbert> l[l<0]<-0
    Herbert> b<-s%*%sqrt(l)
    Herbert> for(i in 1:nrow(b)){b[i,]<-b[i,]/sqrt(sum(b[i,]^2))}
    Herbert> ap<-b%*%t(b)
    Herbert> ap

This code does the same thing as my (simplistic, but slightly more
general) function posdefify()  in package "sfsmisc" :

  a <- matrix(c(1,.9,.7,.9,1,.3,.7,.3,1), 3)

  install.packages("sfsmisc")
  library(sfsmisc)

  posdefify(a)

gives
          [,1]      [,2]      [,3]
[1,] 1.0000000 0.8940242 0.6963190
[2,] 0.8940242 1.0000000 0.3009691
[3,] 0.6963190 0.3009691 1.0000000


    Herbert> It is based on a paper by Rebonato etal that formerly was at
    Herbert> www.rebonato.com/correlationmatrix.pdf.
    Herbert> Unfortunately the website has disappeared.

The idea is very simple and has been re-invented many times as
far as I know.

More sophisticated methods for "posdefiying" a matrix exist in
other places. Given symmetrix matrix A, they try to find the
matrix Ap, positive definite, such  ||A - Ap||  is minimal.
The eigen-value based simple solution that you've used above
and I've also coded in posdefify(), is not the same one would
get for `usual' matrix norms  || . || 

[[NB:  posdefify() also has a 2nd method the implementation of
       which has an embarassing bug. The next version of
       sfsmisc, due in a day or two, will have it fixed.
]]

Does anyone know of rigorous mathematical results in this
regard?

Martin Maechler, ETH Zurich



From wantia at ifi.unizh.ch  Mon Dec 13 16:59:48 2004
From: wantia at ifi.unizh.ch (Jan Wantia)
Date: Mon, 13 Dec 2004 16:59:48 +0100
Subject: [R] lists within a list / data-structure problem
Message-ID: <41BDBC74.9070308@ifi.unizh.ch>

Dear all,

this is a rather basic question; i am not sure how to structure my data
well:
I want to extraxt various measures from my raw-data. These measures are
of different sizes, so I decided to store them in a list, like:

run1 <- list(Dom = (my_vector), mean = (my_single_number))

I can do that in a for loop for 40 runs, ending up with 40 lists: run1,
run2, ..., run40.
To have all the measurements neatly together I thought of making another
list, containing  40 sub-lists:

 > ALL <- list(run1, run2,..., run40)
 > ALL
[[1]]
[[1]]$Dom
[1] "my_vector"

[[1]]$mean
[1] "my_single_number"


[[2]]
[[2]]$Dom
[1] "my_vector"

[[2]]$mean
[1] "my_single_number"

...

1) This may be a bit clumsy as I have to type all the sub-list's names
in by hand in order to produce my ALL-list: Is there a better way?

2) I have problems of addressing the data now. I can easily access any
single value; for example, for the second component of the second sub- list:

 > ALL[[2]][[2]]
[1] "my_single_number",

but: how could I get the second component of all sub-lists, to plot, for
example, all the $mean in one plot? For a matrix, mat[,2] would give me
the whole second column, but
ALL[[]][[2]]
does not return all the second components.

I feel that 'lapply' might help me here, but I could not figure out
exactly how to use it, and it always comes down to the problem of how to
correctly address the components in the sublists.

Or is there maybe a smarter way to do that instead of using a list of lists?

Any hint would be warmly appreciated!

Jan
(R 2.0.1 on windows XP)

-- 

______________________________________________________

Jan Wantia
Deptartment of Informatics, University of Zurich
Andreasstr. 15
CH 8050 Zurich
Switzerland

Tel.:    +41 (0) 1 635 4315
Fax:     +41 (0) 1 635 45 07
email: wantia at ifi.unizh.ch



From dethlef at math.aau.dk  Mon Dec 13 17:10:29 2004
From: dethlef at math.aau.dk (Claus Dethlefsen)
Date: Mon, 13 Dec 2004 17:10:29 +0100
Subject: [R] Advice on parsing formulae
In-Reply-To: <200412131105.iBDB4GHF031431@hypatia.math.ethz.ch>
Message-ID: <000401c4e12e$43ad1ba0$9230e182@peano>

Dear list

I would like to be able to group terms in a formula using a function that I
will call tvar(), eg. the formula

Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))

where x,u and v are numeric and A and B are factors - binary, say.

As output, I want the model.matrix as if tvar had not been there at all. In
addition, I would like to have information on the grouping, as a vector as
long as ncol( model.matrix ) with zeros corresponding to terms outside tvar
and with an index grouping the terms inside each tvar(). In the (sick)
example:


> model.matrix(Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3)))
   (Intercept)     z     u B2 poly(v, 3)1 poly(v, 3)2 poly(v, 3)3  x:A1
x:A2
1            1 -1.55 -1.03  0       0.160      -0.350      -0.281  0.66
0.00
2            1 -1.08  0.55  0      -0.164      -0.211       0.340  0.91
0.00
3            1  0.29 -0.26  0      -0.236      -0.073       0.311 -1.93
0.00
4            1 -1.11  0.96  0       0.222      -0.285      -0.385 -0.23
0.00
5            1  0.43 -0.76  1      -0.434       0.515      -0.532  0.22
0.00

I would like the vector

c(0,1,0,2,3,3,3,4,4)

pointing to the tvar-grouped terms.

Thus what I would like, looks a bit like the 'assign' attribute of the
model.matrix() output. I have not figured out a way of doing this in a nice
way and would like some help, please.

I hope somebody can help me (or point the manual-pages I should read),

Best, 

Claus Dethlefsen
----------------------------------------------------------------------------
---
Assistant Professor, Claus Dethlefsen, Ph.D.
mailto:dethlef at math.auc.dk, http://www.math.auc.dk/~dethlef
Dpt. of Mathematical Sciences, Aalborg University
Fr. Bajers Vej 7G, 9220 Aalborg East
Denmark

-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From Allan at STATS.uct.ac.za  Mon Dec 13 17:14:41 2004
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Mon, 13 Dec 2004 18:14:41 +0200
Subject: [R] R: functions problem
Message-ID: <41BDBFF1.E489A08A@STATS.uct.ac.za>

hi all


how can i see the code inside a particular function? i know one can
simply type the function, eg ls, but sometimes when this is done one
will get "UseMethod("some function name"). (One could also use "body"
but i have the same problem in this case. )How does one see the code in
this case?

From Allan at STATS.uct.ac.za  Mon Dec 13 17:16:57 2004
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Mon, 13 Dec 2004 18:16:57 +0200
Subject: [R] R: optim problems and neural networks
Message-ID: <41BDC079.4ECE4DE6@STATS.uct.ac.za>

hi all

i have two questions:

1. i have been having problems with using the "optim" function. I have
tried to define the matrix containing the parameters as a matrix, ie not
simply p*1. The function does not allow this. Am i correct? 

2. in the code below i have tried to calculate the weights of a basic
feed forward neural network (I know that i could use the nnet package.).
the aim is to learn how to code and become familiar with the language.
note that the code still has to be generalised and does not allow for
skip layers. No error checking on the parameter estimates are included
yet. when i run the above code, i get the following error, "Error in
fn(par, ...) : recursive default argument reference ". why does this
happen? As a simple case, use x<-c(1,2,3) and y<-2+3*x. I know that
there is no errors. I simply want the code to work at this stage.




NNET<-function(p=2,size=1,skip=T,x,y)
{

p<-p

x<-x
x<-as.matrix(x)
x<-cbind(rep(1,nrow(x)),x)

y<-y

size<-size
skip<-skip
betas<-matrix(1:((p+1)*size),nrow=1+p,ncol=size)

f3<-function (betas,x,y,p=p) 
{

	#b1<-matrix(betas,nrow=p,ncol=size)

	#the input to hidden layer section
	hl<-x%*%betas[1:p,]

	hl_act<-(exp(hl)-1)/(1+exp(hl))

	#b2<-matrix(betas[(1+p*size):((1+p)*size)],nrow=size)

	#the hidden layer to output section
	ol<-hl_act%*%betas[p+1,]

	# the square of the residuals
	e<-(y-ol)

	fit_nn<-(ol)

	# the sums of squares / 2
	E<-sum(e*e)/2
}


optim(c(rnorm((p+1)*size)),f3,x=x,y=y,hessian=T,method=c("BFGS"))
}

NNET(p=2,x=x,y=y)

From tom_woody at swissinfo.org  Mon Dec 13 17:22:21 2004
From: tom_woody at swissinfo.org (=?ISO-8859-15?Q?Thomas_Sch=F6nhoff?=)
Date: Mon, 13 Dec 2004 17:22:21 +0100
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
References: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
Message-ID: <41BDC1BD.8060708@swissinfo.org>

Hello,

Thomas W Volscho schrieb:
> Dear List,
> I have acquired a new desktop and wanted to put a free OS on it.  I am trying Fedora Core 1, but not sure what the best Linux OS is for using R 2.0.1?

R is developed on Linux, so there shouldn't be too much restrictions 
on whatever distri you're going to use!
If running a new shiny Desktop you might consider other aspects than 
using R, which should run with every Linux distribution.
If you're looking for something comfortable to install and maintain 
you might give Quatian a try 
(http://dirk.eddelbuettel.com/quantian.html). This is a somewhat 
modified Knoppix trimmed to statistical analysis (not entirely, though)


regards

Thomas



From michael.watson at bbsrc.ac.uk  Mon Dec 13 17:31:08 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 13 Dec 2004 16:31:08 -0000
Subject: [R] R: functions problem
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B8D1@iahce2knas1.iah.bbsrc.reserved>

Sometimes it's in <function_name>.default  Eg:

> barplot
function (height, ...) 
UseMethod("barplot")
<environment: namespace:graphics>

> barplot.default
function (height, width = 1, space = NULL, names.arg = NULL, 
    legend.text = NULL, beside = FALSE, horiz = FALSE, density = NULL, 
    angle = 45, col = NULL, border = par("fg"), main = NULL, 
    sub = NULL, xlab = NULL, ylab = NULL, xlim = NULL, ylim = NULL, 
    xpd = TRUE, axes = TRUE, axisnames = TRUE, cex.axis = par("cex.axis"), 
    cex.names = par("cex.axis"), inside = TRUE, plot = TRUE, 
    axis.lty = 0, offset = 0, ...) 
{
    if (!missing(inside)) 
        .NotYetUsed("inside", error = FALSE)
    if (missing(space)) 
        space <- if (is.matrix(height) && beside) 
            c(0, 1)

etc etc

Mick

-----Original Message-----
From:	r-help-bounces at stat.math.ethz.ch on behalf of Clark Allan
Sent:	Mon 12/13/2004 4:14 PM
To:	r-help at stat.math.ethz.ch
Cc:	
Subject:	[R] R: functions problem
hi all


how can i see the code inside a particular function? i know one can
simply type the function, eg ls, but sometimes when this is done one
will get "UseMethod("some function name"). (One could also use "body"
but i have the same problem in this case. )How does one see the code in
this case?



From gunter.berton at gene.com  Mon Dec 13 17:44:36 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 13 Dec 2004 08:44:36 -0800
Subject: [R] lists within a list / data-structure problem
In-Reply-To: <41BDBC74.9070308@ifi.unizh.ch>
Message-ID: <200412131644.iBDGibXH000505@meitner.gene.com>

Jan:

One thing to keep in mind: A list is vector. So vector-type operations like
c(), "[", etc. work on lists, too (but be careful). Some comments inline
below that I hope will be helpful. A good reference on the S language is
V&R's S PROGRAMMING, which I recommend highly.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jan Wantia
> Sent: Monday, December 13, 2004 8:00 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] lists within a list / data-structure problem
> 
> Dear all,
> 
> this is a rather basic question; i am not sure how to 
> structure my data
> well:
> I want to extraxt various measures from my raw-data. These 
> measures are
> of different sizes, so I decided to store them in a list, like:
> 
> run1 <- list(Dom = (my_vector), mean = (my_single_number))
> 
> I can do that in a for loop for 40 runs, ending up with 40 
> lists: run1,
> run2, ..., run40.
> To have all the measurements neatly together I thought of 
> making another
> list, containing  40 sub-lists:
> 

As you found, this is clumsy. The usual way to do this is to put the results
into a *single* list as follows:

## Contruct the empty list with 40 components:
out<-vector("list", 40)
### loop 
... ## do the calculations
out[[i]] <- list(Dom = yourvec,mean=yournumb)
...

>  > ALL <- list(run1, run2,..., run40)
>  > ALL
> [[1]]
> [[1]]$Dom
> [1] "my_vector"
> 
> [[1]]$mean
> [1] "my_single_number"
> 
> 
> [[2]]
> [[2]]$Dom
> [1] "my_vector"
> 
> [[2]]$mean
> [1] "my_single_number"
> 
> ...
> 
> 1) This may be a bit clumsy as I have to type all the sub-list's names
> in by hand in order to produce my ALL-list: Is there a better way?
> 
> 2) I have problems of addressing the data now. I can easily access any
> single value; for example, for the second component of the 
> second sub- list:

out[[i]] is the ith component of the list, i.e. the ith 2 component list
containing the result of the ith loop. So out[[i]][[1]] is yourvec for the
ith loop and out[[i]][[2]] is yournumb. These can be abbreciated as
out[[c(i,1]] and out[[c(i,2)]]

> 
>  > ALL[[2]][[2]]
> [1] "my_single_number",
> 
> but: how could I get the second component of all sub-lists, 
> to plot, for
> example, all the $mean in one plot? For a matrix, mat[,2] 
> would give me
> the whole second column, but
> ALL[[]][[2]]
> does not return all the second components.
> 
> I feel that 'lapply' might help me here, but I could not figure out
> exactly how to use it, and it always comes down to the 
> problem of how to
> correctly address the components in the sublists.
> 
> Or is there maybe a smarter way to do that instead of using a 
> list of lists?
> 
> Any hint would be warmly appreciated!
> 
> Jan
> (R 2.0.1 on windows XP)
> 
> -- 
> 
> ______________________________________________________
> 
> Jan Wantia
> Deptartment of Informatics, University of Zurich
> Andreasstr. 15
> CH 8050 Zurich
> Switzerland
> 
> Tel.:    +41 (0) 1 635 4315
> Fax:     +41 (0) 1 635 45 07
> email: wantia at ifi.unizh.ch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From james.holtman at convergys.com  Mon Dec 13 17:46:19 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Mon, 13 Dec 2004 11:46:19 -0500
Subject: [R] lists within a list / data-structure problem
Message-ID: <OFC472CC2C.E0DC3146-ON85256F69.005BD628@nd.convergys.com>





construct you list in the loop:

x.all <- list()  # initialize
for (i in 1:limit){
      .......
      x.all[[i]] <- result.list
}

now you want to name them, e.g., run1 ....

names(x.all) <- paste('run', seq(length(x.all)), sep='')


To access, you can do    x.all$run1$Dom


To extract all the 'Dom's

lapply(x.all, function(x) x$Dom)

HTH
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Jan Wantia                                                                                                           
                      <wantia at ifi.unizh.ch>        To:       r-help at stat.math.ethz.ch                                                      
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  [R] lists within a list / data-structure problem                              
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      12/13/2004 10:59                                                                                                     
                                                                                                                                           
                                                                                                                                           




Dear all,

this is a rather basic question; i am not sure how to structure my data
well:
I want to extraxt various measures from my raw-data. These measures are
of different sizes, so I decided to store them in a list, like:

run1 <- list(Dom = (my_vector), mean = (my_single_number))

I can do that in a for loop for 40 runs, ending up with 40 lists: run1,
run2, ..., run40.
To have all the measurements neatly together I thought of making another
list, containing  40 sub-lists:

 > ALL <- list(run1, run2,..., run40)
 > ALL
[[1]]
[[1]]$Dom
[1] "my_vector"

[[1]]$mean
[1] "my_single_number"


[[2]]
[[2]]$Dom
[1] "my_vector"

[[2]]$mean
[1] "my_single_number"

...

1) This may be a bit clumsy as I have to type all the sub-list's names
in by hand in order to produce my ALL-list: Is there a better way?

2) I have problems of addressing the data now. I can easily access any
single value; for example, for the second component of the second sub-
list:

 > ALL[[2]][[2]]
[1] "my_single_number",

but: how could I get the second component of all sub-lists, to plot, for
example, all the $mean in one plot? For a matrix, mat[,2] would give me
the whole second column, but
ALL[[]][[2]]
does not return all the second components.

I feel that 'lapply' might help me here, but I could not figure out
exactly how to use it, and it always comes down to the problem of how to
correctly address the components in the sublists.

Or is there maybe a smarter way to do that instead of using a list of
lists?

Any hint would be warmly appreciated!

Jan
(R 2.0.1 on windows XP)

--

______________________________________________________

Jan Wantia
Deptartment of Informatics, University of Zurich
Andreasstr. 15
CH 8050 Zurich
Switzerland

Tel.:    +41 (0) 1 635 4315
Fax:     +41 (0) 1 635 45 07
email: wantia at ifi.unizh.ch

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Mon Dec 13 17:47:51 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 13 Dec 2004 12:47:51 -0400
Subject: [R] bootstrap package
In-Reply-To: <000f01c4e0f6$6cbc6880$5f9d72d5@Larissa>
References: <000f01c4e0f6$6cbc6880$5f9d72d5@Larissa>
Message-ID: <41BDC7B7.5040901@acelerate.com>

Fredrik Lundgren wrote:

> Hello R'ers
>
> In previous versions of R (I now use 2.0.1) there was a package 
> "bootstrap". I wrote some programs that depended heavily on this 
> package but can unfortunately not find it on Cran today. Is there any 
> way to find an older version of "bootstrap" and use it with the new 
> version - 2.0.1?
>
I  just uploaded to CRAN a version of bootstrap which
passes R CMC check under rw2001

Kjetil

> Sincerely Fredrik Lundgren
> Norrk??ping
> Sweden
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From gunter.berton at gene.com  Mon Dec 13 17:49:23 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 13 Dec 2004 08:49:23 -0800
Subject: [R] R: functions problem
In-Reply-To: <41BDBFF1.E489A08A@STATS.uct.ac.za>
Message-ID: <200412131649.iBDGnNuK029213@faraday.gene.com>

Two issues are at play here, S3 methods and namespaces. You need to read up
on both. But, in brief, to get an ** exported ** S3 method you need to type:
method.class, e.g. 
>summary.lm
If the method is ** not** exported, you need the ::: operator or look at
?getAnywhere .

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Clark Allan
> Sent: Monday, December 13, 2004 8:15 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R: functions problem
> 
> hi all
> 
> 
> how can i see the code inside a particular function? i know one can
> simply type the function, eg ls, but sometimes when this is done one
> will get "UseMethod("some function name"). (One could also use "body"
> but i have the same problem in this case. )How does one see 
> the code in
> this case?
> 

From apjaworski at mmm.com  Mon Dec 13 17:58:45 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Mon, 13 Dec 2004 10:58:45 -0600
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <41BDC1BD.8060708@swissinfo.org>
Message-ID: <OFBFED9B98.1846C630-ON86256F69.005D2CC7-86256F69.005D452E@mmm.com>






The newest "production" release of Fedora is Core 3.  I would use this one
if I were you.

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             Thomas Sch??nhoff                                              
             <tom_woody at swissi                                             
             nfo.org>                                                   To 
             Sent by:                  R User-Liste                        
             r-help-bounces at st         <r-help at stat.math.ethz.ch>          
             at.math.ethz.ch                                            cc 
                                                                           
                                                                   Subject 
             12/13/2004 10:22          Re: [R] switching to Linux,         
             AM                        suggestions?                        
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Hello,

Thomas W Volscho schrieb:
> Dear List,
> I have acquired a new desktop and wanted to put a free OS on it.  I am
trying Fedora Core 1, but not sure what the best Linux OS is for using R
2.0.1?

R is developed on Linux, so there shouldn't be too much restrictions
on whatever distri you're going to use!
If running a new shiny Desktop you might consider other aspects than
using R, which should run with every Linux distribution.
If you're looking for something comfortable to install and maintain
you might give Quatian a try
(http://dirk.eddelbuettel.com/quantian.html). This is a somewhat
modified Knoppix trimmed to statistical analysis (not entirely, though)


regards

Thomas

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sergei at stams.strath.ac.uk  Mon Dec 13 17:59:58 2004
From: sergei at stams.strath.ac.uk (Sergei Zuyev)
Date: Mon, 13 Dec 2004 16:59:58 +0000
Subject: [R] HTML help index generation problem with R under Windows
In-Reply-To: <41BDA73D.9000505@statistik.uni-dortmund.de>
References: <200412091723.08225.sergei@stams.strath.ac.uk>
	<200412131259.33474.sergei@stams.strath.ac.uk>
	<41BDA73D.9000505@statistik.uni-dortmund.de>
Message-ID: <200412131659.59035.sergei@stams.strath.ac.uk>

On ?????????????????????? 13 ?"??????????? 2004 14:29, you wrote:
> >
> > Thanks, Uwe, for your response!
> > a) I'll try to convince our IT people to upgrade, but they are reluctant
> > to do that often. I must confess, I undestand them - so many times I had
> > to re-compile and re-install packages after R-upgrades... This indeed
> > might create a havoc for other users in the middle of the teaching term.
>
> So time to convince the IT people that you get write access to the R
> directory. Just tell them you are volunteering to maintain the
> department's R installation.

OK, this might work for myself, but what about students? As I understand every 
USER of my library will have the same problem, so everyone should have a 
write access to R installation. If this is so, no sysadmin will ever subscibe 
to that...

> > b) This indeed opens an html help but does not fix the problem - my
> > package is not showing up in the help. This could be expected since the
> > index file is unchanged from its canonical state.
> >
> > Any more ideas, please?
>
> Well, if you need to update the index, you have to install R into a
> directory you have got write access to, either network or local drive....

But under UNIX/Linux the index is owned by the user, so no such problem 
arises. Why not in Windows?

Cheers,
Sergei

-- 
=================================================================
                           Dr. Sergei ZUYEV
Statistics and Modelling Science dept., University of Strathclyde
    Livingston Tower, 26 Richmond str., Glasgow, G1 1XH, U.K.
     Tel.: +44 (0)141 548 3663    Fax:  +44 (0)141 552 2079
E-mail: sergei at stams.strath.ac.uk   http://www.stams.strath.ac.uk



From br44114 at yahoo.com  Mon Dec 13 18:05:54 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 13 Dec 2004 09:05:54 -0800 (PST)
Subject: [R] switching to Linux, suggestions?
Message-ID: <20041213170554.39637.qmail@web50305.mail.yahoo.com>

Before choosing a GNU/Linux distribution look into the package
management issue. 
http://distrowatch.com/
I would suggest that you avoid all RPM-based distributions (Mandrake,
Fedora, SuSE), and consider Debian (+ those based on it) & the
source-based distributions (such as Gentoo). I've been using Mandrake
for a couple of years but got tired of RPM. 

HTH,
b.


-----Original Message-----
From: Thomas W Volscho [mailto:THOMAS.VOLSCHO at huskymail.uconn.edu]
Sent: Sunday, December 12, 2004 3:24 PM
To: r-help at stat.math.ethz.ch
Subject: [R] switching to Linux, suggestions?


Dear List,
I have acquired a new desktop and wanted to put a free OS on it.  I
am trying Fedora Core 1, but not sure what the best Linux OS is for
using R 2.0.1?

Thank you in advance for your input,
Tom Volscho

************************************        
Thomas W. Volscho
Graduate Student
Dept. of Sociology U-2068
University of Connecticut
Storrs, CT 06269
Phone: (860) 486-3882
http://vm.uconn.edu/~twv00001

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



		
__________________________________ 

Dress up your holiday email, Hollywood style. Learn more.



From Herbert_Desson at jltgroup.com  Mon Dec 13 18:19:38 2004
From: Herbert_Desson at jltgroup.com (Herbert_Desson@jltgroup.com)
Date: Mon, 13 Dec 2004 17:19:38 -0000
Subject: [R] Re: Help : generating correlation matrix with a particula r
Message-ID: <E624774E77218D4497DCD28E325D78640A64A26D@uk-lon-email01.uk.group.local>


************************************************************
Important: We would  draw your  attention to the  notices at
the bottom of this  e-mail, particularly  before opening and
reviewing any file attachment(s).
************************************************************


Martin,

Thank you for letting us know about posdefify.  It does do exactly what the
Rebonato paper recommended and gives the same result as our code, but it
will be much better behaved in the wild than ours will.

BTW Troels Ring [tring at gvdnet.dk]  found the Rebonato paper at
http://www.quarchome.com/correlationmatrix.pdf

Thank you Troels.

Best regards,
Herb

Herbert G. Desson, ACAS, MAAA

Actuary
JLT Risk Solutions
6 Crutched Friars
London EC3N 2PH

phone:  +44 (0)20 7528 4702
fax:       +44 (0)20 7558 3785



-----Original Message-----


************************************************************
JLT Risk Solutions Ltd
6 Crutched Friars, London EC3N 2PH. Co Reg No 1536540
Tel: (44) (0)20 7528 4000   Fax: (44) (0)20 7528 4500
http://www.jltgroup.com
Lloyd's Broker.  Regulated by the General Insurance
Standards Council
------------------------------------------------------------
The content of this e-mail (including any attachments) as 
received may not be the same as sent. If you consider that 
the content is material to the formation or performance of 
a contract or you are otherwise relying upon its accuracy, 
you should consider requesting a copy be sent by facsimile 
or normal mail.  In any event, please check this message
and any identified file attachment(s) upon receipt and notify
the sender immediately if there is any manifest transmission
error, omission or corruption. This does not change or reduce
any party's duty of utmost good faith when contracting for
insurance or reinsurance. The information in this e-mail is 
confidential and may be legally privileged. If you are not 
the intended recipient, please notify the sender immediately 
and then delete this e-mail entirely - you must not retain, 
copy, distribute or use this e-mail for any purpose or 
disclose any of its content to others.

Opinions, conclusions and other information in this e-mail 
that do not relate to the official business of JLT Risk 
Solutions Ltd shall be understood as neither given nor 
endorsed by it.  Please note we intercept and monitor 
incoming / outgoing e-mail and therefore you should neither 
expect nor intend any e-mail to be private in nature.

We have checked this e-mail for viruses and other harmful 
components and believe but not guarantee it virus-free prior 
to leaving our computer system.  However, you should satisfy 
yourself that it is free from harmful components, as we do 
not accept responsibility for any loss or damage it may 
cause to your computer systems.
************************************************************

From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
Sent: 13 December 2004 16:04
To: Herbert_Desson at jltgroup.com
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Re: Help : generating correlation matrix with a
particular


>>>>> "Herbert" == Herbert Desson <Herbert_Desson at jltgroup.com>
>>>>>     on Mon, 13 Dec 2004 12:24:10 -0000 writes:


    Herbert> Here is some code we have used.

    Herbert> a<-array(c(1,.9,.7,.9,1,.3,.7,.3,1),dim=c(3,3))
    Herbert> a
    Herbert> s<-eigen(a)$vectors
    Herbert> l<-diag(eigen(a)$values)
    Herbert> l[l<0]<-0
    Herbert> b<-s%*%sqrt(l)
    Herbert> for(i in 1:nrow(b)){b[i,]<-b[i,]/sqrt(sum(b[i,]^2))}
    Herbert> ap<-b%*%t(b)
    Herbert> ap

This code does the same thing as my (simplistic, but slightly more
general) function posdefify()  in package "sfsmisc" :

  a <- matrix(c(1,.9,.7,.9,1,.3,.7,.3,1), 3)

  install.packages("sfsmisc")
  library(sfsmisc)

  posdefify(a)

gives
          [,1]      [,2]      [,3]
[1,] 1.0000000 0.8940242 0.6963190
[2,] 0.8940242 1.0000000 0.3009691
[3,] 0.6963190 0.3009691 1.0000000


    Herbert> It is based on a paper by Rebonato etal that formerly was at
    Herbert> www.rebonato.com/correlationmatrix.pdf.
    Herbert> Unfortunately the website has disappeared.

The idea is very simple and has been re-invented many times as
far as I know.

More sophisticated methods for "posdefiying" a matrix exist in
other places. Given symmetrix matrix A, they try to find the
matrix Ap, positive definite, such  ||A - Ap||  is minimal.
The eigen-value based simple solution that you've used above
and I've also coded in posdefify(), is not the same one would
get for `usual' matrix norms  || . || 

[[NB:  posdefify() also has a 2nd method the implementation of
       which has an embarassing bug. The next version of
       sfsmisc, due in a day or two, will have it fixed.
]]

Does anyone know of rigorous mathematical results in this
regard?

Martin Maechler, ETH Zurich



From Brandon.J.Whitcher at gsk.com  Mon Dec 13 18:21:33 2004
From: Brandon.J.Whitcher at gsk.com (Brandon.J.Whitcher@gsk.com)
Date: Mon, 13 Dec 2004 17:21:33 +0000
Subject: [R] multivariate permutation tests
Message-ID: <OFCCD00EEB.C6A5C873-ON80256F69.005E1A1C-80256F69.005F6EDC@sb.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041213/9371fa19/attachment.pl

From doktora at gmail.com  Mon Dec 13 18:25:01 2004
From: doktora at gmail.com (doktora v)
Date: Mon, 13 Dec 2004 12:25:01 -0500
Subject: [R] Calling R from a non-X shell script to plot?
Message-ID: <3398909b04121309256597d218@mail.gmail.com>

I am trying to run R from an apache C++ module in a shell script to
plot some data to display it in apache later. I get the error
(reported in apache's logs):

Xlib: connection to ":0.0" refused by server
Xlib: No protocol specified
Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,  :
  unable to start device PNG
In addition: Warning message:
unable to open connection to X11 display`'
Execution halted

Is anyone familiar with this (i.e. running R from a non-X
environment)? Is there  a way to get around this? I've seen some stuff
about virtual devices,  but have no idea if it works or where to
start. If there is a simpler solution, please let me know.

-- doktora


I run the following shell script from the apache module using execve:
------------------------------------------------------
DISPLAY=:0.0
export DISPLAY

if $R --vanilla --quiet < $rfile >$log ; then
  echo "Success"
else
  echo "There were errors in $0, see $log"
fi
------------------------------------------------------

And $rfile contains this:
------------------------------------------------------
png(filename = "/tmp/plot.png", width = 510, height = 360);
plot(<some graph>)
dev.off()



From doktora at gmail.com  Mon Dec 13 18:28:26 2004
From: doktora at gmail.com (doktora v)
Date: Mon, 13 Dec 2004 12:28:26 -0500
Subject: [R] Moving standard deviation?
Message-ID: <3398909b04121309287421dca3@mail.gmail.com>

Is there a simple function in R to get a moving standard deviation
(i.e. for the last x samples)?

My goal is to plot bollinger bands around a moving average for price
data. I use kernel smoothing for the moving average.

cheers and thanks!
over and out
-- doktora



From ligges at statistik.uni-dortmund.de  Mon Dec 13 18:31:24 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 13 Dec 2004 18:31:24 +0100
Subject: [R] HTML help index generation problem with R under Windows
In-Reply-To: <200412131659.59035.sergei@stams.strath.ac.uk>
References: <200412091723.08225.sergei@stams.strath.ac.uk>	<200412131259.33474.sergei@stams.strath.ac.uk>	<41BDA73D.9000505@statistik.uni-dortmund.de>
	<200412131659.59035.sergei@stams.strath.ac.uk>
Message-ID: <41BDD1EC.8090905@statistik.uni-dortmund.de>

Sergei Zuyev wrote:

> On ????????? 2004 14:29, you wrote:
> 
>>>Thanks, Uwe, for your response!
>>>a) I'll try to convince our IT people to upgrade, but they are reluctant
>>>to do that often. I must confess, I undestand them - so many times I had
>>>to re-compile and re-install packages after R-upgrades... This indeed
>>>might create a havoc for other users in the middle of the teaching term.
>>
>>So time to convince the IT people that you get write access to the R
>>directory. Just tell them you are volunteering to maintain the
>>department's R installation.
> 
> 
> OK, this might work for myself, but what about students? As I understand every 
> USER of my library will have the same problem, so everyone should have a 
> write access to R installation. If this is so, no sysadmin will ever subscibe 
> to that...

No, if you have regenarted the index, everybody can use it, given the 
installation has taken place on the network share R is located on.


>>>b) This indeed opens an html help but does not fix the problem - my
>>>package is not showing up in the help. This could be expected since the
>>>index file is unchanged from its canonical state.
>>>
>>>Any more ideas, please?
>>
>>Well, if you need to update the index, you have to install R into a
>>directory you have got write access to, either network or local drive....
> 
> 
> But under UNIX/Linux the index is owned by the user, so no such problem 
> arises. Why not in Windows?

The index lives in the R installation directory, but not in a user's 
subdirectory, the strategy is completely different here.

Uwe Ligges


> 
> Cheers,
> Sergei
>



From doktora at gmail.com  Mon Dec 13 18:35:55 2004
From: doktora at gmail.com (doktora v)
Date: Mon, 13 Dec 2004 12:35:55 -0500
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <20041213170554.39637.qmail@web50305.mail.yahoo.com>
References: <20041213170554.39637.qmail@web50305.mail.yahoo.com>
Message-ID: <3398909b04121309356055abe8@mail.gmail.com>

I'm using SUSE with success on intel laptop and AMD desktop. You get
the best of both worlds: rpm and source. I can easily get the rpm
packages i need, and compile on my own the things that i can spend
time on (such as R 2.0.1 -- compiles out of the box on suse).

BTW, I'm looking to switch to Mac platform. Anyone had any experience
with that? I'm expecting on a power G4 laptop later this week.... hope
R behaves...

-- doktora


On Mon, 13 Dec 2004 09:05:54 -0800 (PST), bogdan romocea
<br44114 at yahoo.com> wrote:
> Before choosing a GNU/Linux distribution look into the package
> management issue.
> http://distrowatch.com/
> I would suggest that you avoid all RPM-based distributions (Mandrake,
> Fedora, SuSE), and consider Debian (+ those based on it) & the
> source-based distributions (such as Gentoo). I've been using Mandrake
> for a couple of years but got tired of RPM.
> 
> HTH,
> b.
> 
> 
> -----Original Message-----
> From: Thomas W Volscho [mailto:THOMAS.VOLSCHO at huskymail.uconn.edu]
> Sent: Sunday, December 12, 2004 3:24 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] switching to Linux, suggestions?
> 
> Dear List,
> I have acquired a new desktop and wanted to put a free OS on it.  I
> am trying Fedora Core 1, but not sure what the best Linux OS is for
> using R 2.0.1?
> 
> Thank you in advance for your input,
> Tom Volscho
> 
> ************************************
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> __________________________________
> 
> Dress up your holiday email, Hollywood style. Learn more.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From doktora at gmail.com  Mon Dec 13 18:48:35 2004
From: doktora at gmail.com (doktora v)
Date: Mon, 13 Dec 2004 12:48:35 -0500
Subject: [R] R 2.0.1 and RODBC install problem
Message-ID: <3398909b04121309484bbcb3bf@mail.gmail.com>

Hi all, 

I try to install the RODBC to no avail. Where is sql.h and sqltext.h
supposed to come from? I don't have such files anywhere on my hdd!

After downloading latest RODBC.tar.gz, I do 
R CMD INSTALL RODBC 

and get the following:

* Installing *source* package 'RODBC' ...
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking for library containing SQLTables... -lodbc
configure: creating ./config.status
config.status: creating src/Makevars
** libs
gcc -I/usr/local/lib/R/include  -I/usr/local/include   -fPIC  -g -O2
-c RODBC.c -o RODBC.o
RODBC.c:23:17: sql.h: No such file or directory
RODBC.c:24:20: sqlext.h: No such file or directory
RODBC.c:44: error: parse error before "SQLCHAR"
RODBC.c:44: warning: no semicolon at end of struct or union
RODBC.c:45: warning: data definition has no type or storage class
RODBC.c:46: error: parse error before "DataType"

[... goes on like this for a while, throughtout RODBC.c ...]

make: *** [RODBC.o] Error 1
ERROR: compilation failed for package 'RODBC'
** Removing '/usr/local/lib/R/library/RODBC'
** Restoring previous '/usr/local/lib/R/library/RODBC'



From baron at psych.upenn.edu  Mon Dec 13 18:52:58 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 13 Dec 2004 12:52:58 -0500
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <20041213170554.39637.qmail@web50305.mail.yahoo.com>
References: <20041213170554.39637.qmail@web50305.mail.yahoo.com>
Message-ID: <20041213175258.GA11944@psych>

Fedora uses yum as well as rpm.  I haven't installed an RPM in
months, except for R.  Yum is great.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From doktora at gmail.com  Mon Dec 13 18:53:39 2004
From: doktora at gmail.com (doktora v)
Date: Mon, 13 Dec 2004 12:53:39 -0500
Subject: [R] Switching to Mac, suggestions? (was switching to linux)
Message-ID: <3398909b04121309532f885ed7@mail.gmail.com>

I'm looking to switch to Mac platform. Anyone had any experience
with that? I'm expecting on a power G4 laptop later this week.... hope
R behaves...

-- dok



From andy_liaw at merck.com  Mon Dec 13 18:57:47 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 13 Dec 2004 12:57:47 -0500
Subject: [R] Calling R from a non-X shell script to plot?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E41B@usrymx25.merck.com>

See
http://cran.us.r-project.org/doc/FAQ/R-FAQ.html#How-do-I-produce-PNG-graphic
s-in-batch-mode_003f

Andy

> From: doktora v
> 
> I am trying to run R from an apache C++ module in a shell script to
> plot some data to display it in apache later. I get the error
> (reported in apache's logs):
> 
> Xlib: connection to ":0.0" refused by server
> Xlib: No protocol specified
> Error in X11(paste("png::", filename, sep = ""), width, 
> height, pointsize,  :
>   unable to start device PNG
> In addition: Warning message:
> unable to open connection to X11 display`'
> Execution halted
> 
> Is anyone familiar with this (i.e. running R from a non-X
> environment)? Is there  a way to get around this? I've seen some stuff
> about virtual devices,  but have no idea if it works or where to
> start. If there is a simpler solution, please let me know.
> 
> -- doktora
> 
> 
> I run the following shell script from the apache module using execve:
> ------------------------------------------------------
> DISPLAY=:0.0
> export DISPLAY
> 
> if $R --vanilla --quiet < $rfile >$log ; then
>   echo "Success"
> else
>   echo "There were errors in $0, see $log"
> fi
> ------------------------------------------------------
> 
> And $rfile contains this:
> ------------------------------------------------------
> png(filename = "/tmp/plot.png", width = 510, height = 360);
> plot(<some graph>)
> dev.off()
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From doktora at gmail.com  Mon Dec 13 19:03:02 2004
From: doktora at gmail.com (doktora v)
Date: Mon, 13 Dec 2004 13:03:02 -0500
Subject: [R] Calling R from a non-X shell script to plot?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E41B@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E41B@usrymx25.merck.com>
Message-ID: <3398909b04121310035fefbb46@mail.gmail.com>

Thank, 
bitmap produces huge and empty files. it could be my ghostscript, but
there is no way to really tell, it's the latest version.... will try
Xvfb....


On Mon, 13 Dec 2004 12:57:47 -0500, Liaw, Andy <andy_liaw at merck.com> wrote:
> See
> http://cran.us.r-project.org/doc/FAQ/R-FAQ.html#How-do-I-produce-PNG-graphic
> s-in-batch-mode_003f
> 
> Andy
> 
> > From: doktora v
> >
> > I am trying to run R from an apache C++ module in a shell script to
> > plot some data to display it in apache later. I get the error
> > (reported in apache's logs):
> >
> > Xlib: connection to ":0.0" refused by server
> > Xlib: No protocol specified
> > Error in X11(paste("png::", filename, sep = ""), width,
> > height, pointsize,  :
> >   unable to start device PNG
> > In addition: Warning message:
> > unable to open connection to X11 display`'
> > Execution halted
> >
> > Is anyone familiar with this (i.e. running R from a non-X
> > environment)? Is there  a way to get around this? I've seen some stuff
> > about virtual devices,  but have no idea if it works or where to
> > start. If there is a simpler solution, please let me know.
> >
> > -- doktora
> >
> >
> > I run the following shell script from the apache module using execve:
> > ------------------------------------------------------
> > DISPLAY=:0.0
> > export DISPLAY
> >
> > if $R --vanilla --quiet < $rfile >$log ; then
> >   echo "Success"
> > else
> >   echo "There were errors in $0, see $log"
> > fi
> > ------------------------------------------------------
> >
> > And $rfile contains this:
> > ------------------------------------------------------
> > png(filename = "/tmp/plot.png", width = 510, height = 360);
> > plot(<some graph>)
> > dev.off()
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From spencer.graves at pdf.com  Mon Dec 13 19:04:59 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 13 Dec 2004 10:04:59 -0800
Subject: [R] Moving standard deviation?
In-Reply-To: <3398909b04121309287421dca3@mail.gmail.com>
References: <3398909b04121309287421dca3@mail.gmail.com>
Message-ID: <41BDD9CB.2000407@pdf.com>

      A search for "moving standard deviation" at www.r-project.org -> 
search -> "R site search" just produced 7 matches.  Please look at those 
and let us know if none of those help you (and what you tried that 
didn't work). 

      spencer graves

doktora v wrote:

>Is there a simple function in R to get a moving standard deviation
>(i.e. for the last x samples)?
>
>My goal is to plot bollinger bands around a moving average for price
>data. I use kernel smoothing for the moving average.
>
>cheers and thanks!
>over and out
>-- doktora
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From machud at intellektik.informatik.tu-darmstadt.de  Mon Dec 13 18:20:51 2004
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Mon, 13 Dec 2004 18:20:51 +0100 (CET)
Subject: [R] Friedman test for replicated blocked data
Message-ID: <Pine.LNX.4.58.0412131818510.9730@kika.intellektik.informatik.tu-darmstadt.de>

Hi,

I would need to extend the Friedman test to a replicated
design. Currently the function: friedman.test(y, ...) only works for
unreplicated designs.

I found in Conover 1999 "Practical Nonparamteric statistics" an
extension of the formula to my case.

Nevertheless, other sources, like Sheskin 2000 "Parametric and
Nonparametric statistical Procedures" and Daniel 1990 "Applied
nonparametric statistics" give a different formula from that of Conover
in the unreplicated case. Since they do not provide the extension of
this formula to the replicated case I would like to know if someone
could give me a reference where I could find such extension. The formula
is indeed very simple:

F= z \sqrt{bk(k+1)/6}

where z is a quantile from the normal distribution, b the number of
blocks and k the number of treatments.

Unfortunately, the sources cited do not provide indications from where
the formula comes from.


Thank you for consideration,


Greetings,


Marco


----------------------------------
Marco Chiarandini,
Technische Universitaet Darmstadt



From krcabrer at epm.net.co  Mon Dec 13 19:22:46 2004
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Mon, 13 Dec 2004 13:22:46 -0500
Subject: [R] Problem with graphic window in LINUX
In-Reply-To: <20041212053908.JWCM29051.smta08.mail.ozemail.net@there>
References: <002301c4dfb7$f85955f0$0d01a8c0@tablet>
	<41BB57E3.206@statistik.uni-dortmund.de>
	<20041212053908.JWCM29051.smta08.mail.ozemail.net@there>
Message-ID: <41BDDDF6.7070403@epm.net.co>


Hi R users:

I got a problem when I make graphics in LINUX, the graphic window
is static, I can resize it, but I can not move it (there is no frame),
*  what should I configure in LINUX to make them work again?
* Is it posible to  have a history list of the graphics like in W2K, so 
I  can see them again?
Thank you for your help

Kenneth



From abu3ammar at gmail.com  Mon Dec 13 19:27:06 2004
From: abu3ammar at gmail.com (Omar Lakkis)
Date: Mon, 13 Dec 2004 13:27:06 -0500
Subject: [R] timeSeries to its error
Message-ID: <b1d3150404121310275cbaec42@mail.gmail.com>

I have a timeSeries that I want to convert to its. When I use as.its()
I get the error:

Error in checkSlotAssignment(object, name, value) : 
        Assignment of an object of class "NULL" is not valid for slot
"units" in an object of class "timeSeries"; is(value, "character") is
not TRUE

When I constructed the timeSeries object I did not use the units param
in the constructor because I do not know what it does.



From rxg218 at psu.edu  Mon Dec 13 19:31:40 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 13 Dec 2004 13:31:40 -0500
Subject: [R] Calling R from a non-X shell script to plot?
In-Reply-To: <3398909b04121309256597d218@mail.gmail.com>
References: <3398909b04121309256597d218@mail.gmail.com>
Message-ID: <1102962700.13504.25.camel@blue.chem.psu.edu>

On Mon, 2004-12-13 at 12:25 -0500, doktora v wrote:
> I am trying to run R from an apache C++ module in a shell script to
> plot some data to display it in apache later. I get the error
> (reported in apache's logs):
> 
> Xlib: connection to ":0.0" refused by server
> Xlib: No protocol specified
> Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,  :
>   unable to start device PNG
> In addition: Warning message:
> unable to open connection to X11 display`'
> Execution halted
> 
> Is anyone familiar with this (i.e. running R from a non-X
> environment)? Is there  a way to get around this? I've seen some stuff
> about virtual devices,  but have no idea if it works or where to
> start. If there is a simpler solution, please let me know.

I use the following when I need to plot a graphic by calling R from a
PHP script on my webserver

bitmap(file=plotfile, height=7, width=7)
par(pch=19, col='black', cex=1.5)
plot(log(1/boxsize), log(bv), ylab="log(box count)", xlab="log(1/box
size)")

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
A list is only as strong as its weakest link.
-- Don Knuth



From rxg218 at psu.edu  Mon Dec 13 19:32:23 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 13 Dec 2004 13:32:23 -0500
Subject: [R] Calling R from a non-X shell script to plot?
In-Reply-To: <3398909b04121309256597d218@mail.gmail.com>
References: <3398909b04121309256597d218@mail.gmail.com>
Message-ID: <1102962744.13504.28.camel@blue.chem.psu.edu>

On Mon, 2004-12-13 at 12:25 -0500, doktora v wrote:
> I am trying to run R from an apache C++ module in a shell script to
> plot some data to display it in apache later. I get the error
> (reported in apache's logs):
> 
> Xlib: connection to ":0.0" refused by server
> Xlib: No protocol specified
> Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,  :
>   unable to start device PNG
> In addition: Warning message:
> unable to open connection to X11 display`'
> Execution halted

I use the following when I need to plot a graphic by calling R from a
PHP script on my webserver

bitmap(file=plotfile, height=7, width=7)
par(pch=19, col='black', cex=1.5)
plot(log(1/boxsize), log(bv), ylab="log(box count)", xlab="log(1/box
size)")

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Science kind of takes the fun out of the portent business.
-Hobbes



From tplate at acm.org  Mon Dec 13 19:44:21 2004
From: tplate at acm.org (Tony Plate)
Date: Mon, 13 Dec 2004 11:44:21 -0700
Subject: [R] Percentages in contingency tables *warning trivial question*
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E9027FDF98@exdkba022.novo.dk
 >
References: <0ABD88905D18E347874E0FB71C0B29E9027FDF98@exdkba022.novo.dk>
Message-ID: <6.1.0.6.2.20041213114127.06164848@mailhost.blackmesacapital.com>

The 'abind' function in the 'abind' package is a generalized binding 
functions for arrays.  (I've never tried it with tables.)

At Monday 04:36 AM 12/13/2004, BXC (Bendix Carstensen) wrote:
>[...snip...]
>The last step is necessary in the absence of a generalized cbind/rbind
>for tables/arrays.
>
>Please correct me if such a thing exists. If it does, it should be
>referenced under "see also" in the help page for cbind.



From doktora at gmail.com  Mon Dec 13 19:45:31 2004
From: doktora at gmail.com (doktora v)
Date: Mon, 13 Dec 2004 13:45:31 -0500
Subject: [R] Moving standard deviation?
In-Reply-To: <41BDD9CB.2000407@pdf.com>
References: <3398909b04121309287421dca3@mail.gmail.com>
	<41BDD9CB.2000407@pdf.com>
Message-ID: <3398909b04121310457b718d7d@mail.gmail.com>

I have tried there but didn't find anything useful. Most of the
matches are for functions which take a std dev input, and the "moving"
part of the query relates to something else (like moving average in
the qcc package).

Anyway, it's not too difficult to create the function, but I was
wondering if anyone had done it before. Efficiency is a concideration,
naturally.

I'll post what i come up with...

cheers
dok


On Mon, 13 Dec 2004 10:04:59 -0800, Spencer Graves
<spencer.graves at pdf.com> wrote:
>       A search for "moving standard deviation" at www.r-project.org ->
> search -> "R site search" just produced 7 matches.  Please look at those
> and let us know if none of those help you (and what you tried that
> didn't work).
> 
>       spencer graves
> 
> doktora v wrote:
> 
> >Is there a simple function in R to get a moving standard deviation
> >(i.e. for the last x samples)?
> >
> >My goal is to plot bollinger bands around a moving average for price
> >data. I use kernel smoothing for the moving average.
> >
> >cheers and thanks!
> >over and out
> >-- doktora
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> 
> --
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
>



From jari.oksanen at oulu.fi  Mon Dec 13 19:47:53 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Mon, 13 Dec 2004 20:47:53 +0200
Subject: [R] Switching to Mac, suggestions? (was switching to linux)
In-Reply-To: <3398909b04121309532f885ed7@mail.gmail.com>
References: <3398909b04121309532f885ed7@mail.gmail.com>
Message-ID: <7ED537C0-4D37-11D9-B26D-000A95C76CA8@oulu.fi>


On 13 Dec 2004, at 19:53, doktora v wrote:

> I'm looking to switch to Mac platform. Anyone had any experience
> with that? I'm expecting on a power G4 laptop later this week.... hope
> R behaves...

I have been a Linux user since 1999, and I got my first ever Mac (iBook 
G4 laptop) last December. There is just as little to comment on MacOS X 
as there is to comment on flavours of Linux distros: there is no large 
difference as regards to R. I still prefer emacs & ess as a shell (but 
you can get some kind of real emacs in Mac as well), but MacOS X/ R is 
more of an eye candy (though I find it really hard to get any real use 
for transparent windows in R: I still prefer to see what I type instead 
of looking the background through the text). As regards to R, it is 
just the same if you have any brand of Linux or MacOS X or even a 
fringe system like Windows. The differences are somewhere else than in 
R.

By the way, Ubuntu "GNU"/Linux works nicely in Mac, with blas who knows 
about the vector processor in G4.

cheers, jazza
--
Jari Oksanen, Oulu, Finland



From doktora at gmail.com  Mon Dec 13 19:52:56 2004
From: doktora at gmail.com (doktora v)
Date: Mon, 13 Dec 2004 13:52:56 -0500
Subject: [R] Calling R from a non-X shell script to plot?
In-Reply-To: <1102962744.13504.28.camel@blue.chem.psu.edu>
References: <3398909b04121309256597d218@mail.gmail.com>
	<1102962744.13504.28.camel@blue.chem.psu.edu>
Message-ID: <3398909b04121310525b12673d@mail.gmail.com>

So, it turns out that using Xvfb is quite simple and works great (fast).

To summ up: if you are trying to plot a png file from a non-X environment, 
run Xvfb, like so (for example):

Xvfb :9 -screen 0 800x600x16&

This will run a virtual X server 9 with screen 0.
In your shell script (or whatever other environment you are using),
set the DISPLAY environment variable to :9.0:
DISPLAY=:9.0
export DISPLAY

presto 
-- dok


On Mon, 13 Dec 2004 13:32:23 -0500, Rajarshi Guha <rxg218 at psu.edu> wrote:
> On Mon, 2004-12-13 at 12:25 -0500, doktora v wrote:
> > I am trying to run R from an apache C++ module in a shell script to
> > plot some data to display it in apache later. I get the error
> > (reported in apache's logs):
> >
> > Xlib: connection to ":0.0" refused by server
> > Xlib: No protocol specified
> > Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,  :
> >   unable to start device PNG
> > In addition: Warning message:
> > unable to open connection to X11 display`'
> > Execution halted
> 
> I use the following when I need to plot a graphic by calling R from a
> PHP script on my webserver
> 
> bitmap(file=plotfile, height=7, width=7)
> par(pch=19, col='black', cex=1.5)
> plot(log(1/boxsize), log(bv), ylab="log(box count)", xlab="log(1/box
> size)")
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Science kind of takes the fun out of the portent business.
> -Hobbes
> 
>



From cstrato at aon.at  Mon Dec 13 20:02:43 2004
From: cstrato at aon.at (cstrato)
Date: Mon, 13 Dec 2004 20:02:43 +0100
Subject: [R] How to duplicate rows in dataframe?
In-Reply-To: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
References: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
Message-ID: <41BDE753.2020201@aon.at>

Dear all:

I have the following (simple?) problem:
Consider a dataframe where the first column contains
integers used as index, e.g.
    index
     24
     13
     46
     32

Now I have the following vector used to sort the dataframe:
    x <- c(13,24,32,46)
Sorting the dataframe can be done by using order.

However consider the following vector:
    x <- c(13,32,13,24,46,24,24)
Now I want to get the dataframe in the order of the rows
defined in x, i.e. the dataframe contains duplicate rows.
One way to achieve this would be to use rbind in a for-loop.

My question is:
Is there an easier and - more important - faster way to
obtain the dataframe as defined in x?

Thank you in advance.
Best regards
Christian
_._._._._._._._._._._._._._._._
C.h.i.s.t.i.a.n S.t.r.a.t.o.w.a
V.i.e.n.n.a       A.u.s.t.r.i.a
_._._._._._._._._._._._._._._._



From mikem at salter-point.com  Mon Dec 13 20:19:01 2004
From: mikem at salter-point.com (Mike Meyer)
Date: Mon, 13 Dec 2004 11:19:01 -0800
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <20041213175258.GA11944@psych>
References: <20041213170554.39637.qmail@web50305.mail.yahoo.com>
	<20041213175258.GA11944@psych>
Message-ID: <20041213111901.6f0a5268.mikem@Salter-Point.com>

You have installed RPM's, just under the covers.  YUM is just a smart front-end to installing rpms.  Regards, --Mike


On Mon, 13 Dec 2004 12:52:58 -0500
Jonathan Baron <baron at psych.upenn.edu> wrote:

> Fedora uses yum as well as rpm.  I haven't installed an RPM in
> months, except for R.  Yum is great.
> 
> Jon
> -- 
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron
> R search page: http://finzi.psych.upenn.edu/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 

Mike Meyer,  Seattle WA



From ripley at stats.ox.ac.uk  Mon Dec 13 20:24:16 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Mon, 13 Dec 2004 19:24:16 +0000 (GMT)
Subject: [R] R 2.0.1 and RODBC install problem
In-Reply-To: <3398909b04121309484bbcb3bf@mail.gmail.com>
Message-ID: <Pine.GSO.4.31.0412131922130.795-100000@markov.stats>

>From the DESCRIPTION file:

SystemRequirements:   An ODBC driver manager and drivers. See README.

PLEASE do as it asks.

On Mon, 13 Dec 2004, doktora v wrote:

> Hi all,
>
> I try to install the RODBC to no avail. Where is sql.h and sqltext.h
> supposed to come from? I don't have such files anywhere on my hdd!

[...]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please do as we ask.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sfalcon at fhcrc.org  Mon Dec 13 20:34:35 2004
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 13 Dec 2004 11:34:35 -0800
Subject: [R] Calling R from a non-X shell script to plot?
In-Reply-To: <3398909b04121309256597d218@mail.gmail.com>
References: <3398909b04121309256597d218@mail.gmail.com>
Message-ID: <20041213193435.GC22142@queenbee.fhcrc.org>

On Mon, Dec 13, 2004 at 12:25:01PM -0500, doktora v wrote:
> Is anyone familiar with this (i.e. running R from a non-X
> environment)? Is there  a way to get around this? I've seen some stuff
> about virtual devices,  but have no idea if it works or where to
> start. If there is a simpler solution, please let me know.

I've used Xvfb in this situation.  After installing Xvfb, you can do
something like this:

Xvfb :15&
export DISPLAY=localhost:15
# Run R 

+ seth



From sundar.dorai-raj at pdf.com  Mon Dec 13 20:56:48 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 13 Dec 2004 13:56:48 -0600
Subject: [R] Moving standard deviation?
In-Reply-To: <3398909b04121310457b718d7d@mail.gmail.com>
References: <3398909b04121309287421dca3@mail.gmail.com>	<41BDD9CB.2000407@pdf.com>
	<3398909b04121310457b718d7d@mail.gmail.com>
Message-ID: <41BDF400.2090103@pdf.com>

You could use ?running in the gtools package (part of the gregmisc bundle).

--sundar

doktora v wrote:

> I have tried there but didn't find anything useful. Most of the
> matches are for functions which take a std dev input, and the "moving"
> part of the query relates to something else (like moving average in
> the qcc package).
> 
> Anyway, it's not too difficult to create the function, but I was
> wondering if anyone had done it before. Efficiency is a concideration,
> naturally.
> 
> I'll post what i come up with...
> 
> cheers
> dok
> 
> 
> On Mon, 13 Dec 2004 10:04:59 -0800, Spencer Graves
> <spencer.graves at pdf.com> wrote:
> 
>>      A search for "moving standard deviation" at www.r-project.org ->
>>search -> "R site search" just produced 7 matches.  Please look at those
>>and let us know if none of those help you (and what you tried that
>>didn't work).
>>
>>      spencer graves
>>
>>doktora v wrote:
>>
>>
>>>Is there a simple function in R to get a moving standard deviation
>>>(i.e. for the last x samples)?
>>>
>>>My goal is to plot bollinger bands around a moving average for price
>>>data. I use kernel smoothing for the moving average.
>>>
>>>cheers and thanks!
>>>over and out
>>>-- doktora
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>--
>>Spencer Graves, PhD, Senior Development Engineer
>>O:  (408)938-4420;  mobile:  (408)655-4567
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From abu3ammar at gmail.com  Mon Dec 13 21:10:08 2004
From: abu3ammar at gmail.com (Omar Lakkis)
Date: Mon, 13 Dec 2004 15:10:08 -0500
Subject: [R] read attribute
Message-ID: <b1d31504041213121078be240a@mail.gmail.com>

How can I get a single attribute value of an object ?

I jhave the tiemSeries object
> ts1
                    Open
2003-10-09 02:00:00 1.27
2003-10-10 02:00:00 1.25
2003-10-13 02:00:00 1.27
2003-10-14 02:00:00 1.29

When I unclass ts1 I get:
> unclass(ts1)
list()
attr(,"Data")
                    Open
2003-10-09 02:00:00 1.27
2003-10-10 02:00:00 1.25
2003-10-13 02:00:00 1.27
2003-10-14 02:00:00 1.29
attr(,"positions")
[1] "2003-10-09 02:00:00" "2003-10-10 02:00:00" "2003-10-13 02:00:00"
[4] "2003-10-14 02:00:00"
attr(,"format")
[1] "%Y-%m-%d %H:%M:%S"
attr(,"FinCenter")
[1] "Zurich"
attr(,"units")
[1] "Open"
attr(,"title")
[1] "Time Series Object"
attr(,"documentation")
[1] "Created at Zurich 2004-12-13 15:13:05"

How can I get teh value of the attribute positions?



From olafm at tako.de  Mon Dec 13 21:19:49 2004
From: olafm at tako.de (Olaf Mersmann)
Date: Mon, 13 Dec 2004 21:19:49 +0100
Subject: [R] How to duplicate rows in dataframe?
In-Reply-To: <41BDE753.2020201@aon.at>
References: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
	<41BDE753.2020201@aon.at>
Message-ID: <41BDF965.1010100@tako.de>

Hi Christian,

cstrato wrote:
> Dear all:
> 
*snip*
> 
> However consider the following vector:
>    x <- c(13,32,13,24,46,24,24)
> Now I want to get the dataframe in the order of the rows
> defined in x, i.e. the dataframe contains duplicate rows.
> One way to achieve this would be to use rbind in a for-loop.
> 
> My question is:
> Is there an easier and - more important - faster way to
> obtain the dataframe as defined in x?

?unique
?sort

HTH
Olaf



From gunter.berton at gene.com  Mon Dec 13 21:22:58 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 13 Dec 2004 12:22:58 -0800
Subject: [R] How to duplicate rows in dataframe?
In-Reply-To: <41BDE753.2020201@aon.at>
Message-ID: <200412132023.iBDKMxgb003251@hertz.gene.com>

?match

ix<-df$index ## just for clarity

df[match(x,ix),]

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of cstrato
> Sent: Monday, December 13, 2004 11:03 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to duplicate rows in dataframe?
> 
> Dear all:
> 
> I have the following (simple?) problem:
> Consider a dataframe where the first column contains
> integers used as index, e.g.
>     index
>      24
>      13
>      46
>      32
> 
> Now I have the following vector used to sort the dataframe:
>     x <- c(13,24,32,46)
> Sorting the dataframe can be done by using order.
> 
> However consider the following vector:
>     x <- c(13,32,13,24,46,24,24)
> Now I want to get the dataframe in the order of the rows
> defined in x, i.e. the dataframe contains duplicate rows.
> One way to achieve this would be to use rbind in a for-loop.
> 
> My question is:
> Is there an easier and - more important - faster way to
> obtain the dataframe as defined in x?
> 
> Thank you in advance.
> Best regards
> Christian
> _._._._._._._._._._._._._._._._
> C.h.i.s.t.i.a.n S.t.r.a.t.o.w.a
> V.i.e.n.n.a       A.u.s.t.r.i.a
> _._._._._._._._._._._._._._._._
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From james.holtman at convergys.com  Mon Dec 13 21:22:59 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Mon, 13 Dec 2004 15:22:59 -0500
Subject: [R] How to duplicate rows in dataframe?
Message-ID: <OF2D190C4A.EB399065-ON85256F69.006FF2A6@nd.convergys.com>





> x.1 <- data.frame(a=1:5, b=1:5)
> x.1
  a b
1 1 1
2 2 2
3 3 3
4 4 4
5 5 5
> x.1[c(1,2,2,2,3,3,4,4,5,4,3,2,1),]
    a b
1   1 1
2   2 2
2.1 2 2
2.2 2 2
3   3 3
3.1 3 3
4   4 4
4.1 4 4
5   5 5
4.2 4 4
3.2 3 3
2.3 2 2
1.1 1 1
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      cstrato                                                                                                              
                      <cstrato at aon.at>             To:       r-help at stat.math.ethz.ch                                                      
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  [R] How to duplicate rows in dataframe?                                       
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      12/13/2004 14:02                                                                                                     
                                                                                                                                           
                                                                                                                                           




Dear all:

I have the following (simple?) problem:
Consider a dataframe where the first column contains
integers used as index, e.g.
    index
     24
     13
     46
     32

Now I have the following vector used to sort the dataframe:
    x <- c(13,24,32,46)
Sorting the dataframe can be done by using order.

However consider the following vector:
    x <- c(13,32,13,24,46,24,24)
Now I want to get the dataframe in the order of the rows
defined in x, i.e. the dataframe contains duplicate rows.
One way to achieve this would be to use rbind in a for-loop.

My question is:
Is there an easier and - more important - faster way to
obtain the dataframe as defined in x?

Thank you in advance.
Best regards
Christian
_._._._._._._._._._._._._._._._
C.h.i.s.t.i.a.n S.t.r.a.t.o.w.a
V.i.e.n.n.a       A.u.s.t.r.i.a
_._._._._._._._._._._._._._._._

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From br44114 at yahoo.com  Mon Dec 13 21:26:46 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 13 Dec 2004 12:26:46 -0800 (PST)
Subject: [R] Moving standard deviation?
Message-ID: <20041213202646.6242.qmail@web50309.mail.yahoo.com>

A simple for loop does the job. Why not write your own function?

movsd <- function(series,lag)
{
movingsd <- vector(mode="numeric")
for (i in lag:length(series))
	{
	movingsd[i] <- sd(series[(i-lag+1):i])
	}
assign("movingsd",movingsd,.GlobalEnv)
}

This is very efficient: it takes (much) less time to write from
scratch than to look for an existing function.

HTH,
b.


-----Original Message-----
From: doktora v 
Sent: Monday, December 13, 2004 1:46 PM
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Moving standard deviation?


I have tried there but didn't find anything useful. Most of the
matches are for functions which take a std dev input, and the
"moving"
part of the query relates to something else (like moving average in
the qcc package).

Anyway, it's not too difficult to create the function, but I was
wondering if anyone had done it before. Efficiency is a
concideration,
naturally.

I'll post what i come up with...

cheers
dok


On Mon, 13 Dec 2004 10:04:59 -0800, Spencer Graves
<spencer.graves at pdf.com> wrote:
>       A search for "moving standard deviation" at www.r-project.org
->
> search -> "R site search" just produced 7 matches.  Please look at
those
> and let us know if none of those help you (and what you tried that
> didn't work).
> 
>       spencer graves
> 
> doktora v wrote:
> 
> >Is there a simple function in R to get a moving standard deviation
> >(i.e. for the last x samples)?
> >
> >My goal is to plot bollinger bands around a moving average for
price
> >data. I use kernel smoothing for the moving average.
> >
> >cheers and thanks!
> >over and out
> >-- doktora
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
> >
> 
> --
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



		
__________________________________ 

Jazz up your holiday email with celebrity designs. Learn more.



From isen at plantpath.wisc.edu  Mon Dec 13 21:27:56 2004
From: isen at plantpath.wisc.edu (Thomas Isenbarger)
Date: Mon, 13 Dec 2004 14:27:56 -0600
Subject: [R] UPGMA
Message-ID: <79407C23-4D45-11D9-A577-000A9598473A@plantpath.wisc.edu>

R-help folks:

Thanks in the past for your help.  I have another question that I hope 
has a simple answer.  I have searched the R home pages and the R-help 
archives with no hits.  How can I cluster data in R using UPGMA?

I am not subscribed to the list (can't keep up with all the traffic!), 
so I would appreciate it if you could email directly to me (or to the 
list and to me).

Thanks,
Tom Isenbarger
--
isen at plantpath.wisc.edu
thomas a isenbarger
(608) 265-0850



From wolf.privat at gmx.de  Mon Dec 13 21:56:22 2004
From: wolf.privat at gmx.de (Andreas)
Date: Mon, 13 Dec 2004 21:56:22 +0100
Subject: [R] classification for huge datasets: SVM yields memory troubles
References: <41BD8A9F.4040509@gmx.ch>
Message-ID: <cpl0ti$57a$1@sea.gmane.org>

Hi,

I'm a beginner in the SVM-module but I have seen there is a parameter called
:
cachesize #cache memory in MB (default 40)

please let me know if this parameter solved your problem, I might get the
same number of samples in the near future.

regards Andreas

"Christoph Lehmann" <christoph.lehmann at gmx.ch> schrieb im Newsbeitrag
news:41BD8A9F.4040509 at gmx.ch...
> Hi
> I have a matrix with 30 observations and roughly 30000 variables, each
> obs belongs to one of two groups. With svm and slda I get into memory
> troubles ('cannot allocate vector of size' roughly 2G). PCA LDA runs
> fine. Are there any way to use the memory issue withe SVM's? Or can you
> recommend any other classification method for such huge datasets?
>
>
> P.S. I run suse 9.1 on a 2G RAM PIV machine.
> thanks for a hint
>
> Christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From tvandaelen at scitegic.com  Mon Dec 13 22:19:53 2004
From: tvandaelen at scitegic.com (Ton van Daelen)
Date: Mon, 13 Dec 2004 13:19:53 -0800
Subject: [R] Problem tuning an SVM
Message-ID: <830D8D4719112B418ABBC3A0EBA9581251343A@webmail.scitegic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041213/185b1f0f/attachment.pl

From cstrato at aon.at  Mon Dec 13 22:31:43 2004
From: cstrato at aon.at (cstrato)
Date: Mon, 13 Dec 2004 22:31:43 +0100
Subject: [R] How to duplicate rows in dataframe?
In-Reply-To: <200412132023.iBDKMxgb003251@hertz.gene.com>
References: <200412132023.iBDKMxgb003251@hertz.gene.com>
Message-ID: <41BE0A3F.1030906@aon.at>

Thank you all (Olaf Mersmann, Berton Gunter, James Holtman,
Peter Aspach) for your fast reply.

The solutions:
    df[match(x,ix),]   and
    df[c(1,2,2,3,3),]
are exactly what I was looking for.

Best regards
Christian

Berton Gunter wrote:

> ?match
> 
> ix<-df$index ## just for clarity
> 
> df[match(x,ix),]
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>  
>  
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of cstrato
>>Sent: Monday, December 13, 2004 11:03 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] How to duplicate rows in dataframe?
>>
>>Dear all:
>>
>>I have the following (simple?) problem:
>>Consider a dataframe where the first column contains
>>integers used as index, e.g.
>>    index
>>     24
>>     13
>>     46
>>     32
>>
>>Now I have the following vector used to sort the dataframe:
>>    x <- c(13,24,32,46)
>>Sorting the dataframe can be done by using order.
>>
>>However consider the following vector:
>>    x <- c(13,32,13,24,46,24,24)
>>Now I want to get the dataframe in the order of the rows
>>defined in x, i.e. the dataframe contains duplicate rows.
>>One way to achieve this would be to use rbind in a for-loop.
>>
>>My question is:
>>Is there an easier and - more important - faster way to
>>obtain the dataframe as defined in x?
>>
>>Thank you in advance.
>>Best regards
>>Christian
>>_._._._._._._._._._._._._._._._
>>C.h.i.s.t.i.a.n S.t.r.a.t.o.w.a
>>V.i.e.n.n.a       A.u.s.t.r.i.a
>>_._._._._._._._._._._._._._._._
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> 
>



From mail at joeconway.com  Mon Dec 13 22:51:00 2004
From: mail at joeconway.com (Joe Conway)
Date: Mon, 13 Dec 2004 13:51:00 -0800
Subject: [R] Calling R from a non-X shell script to plot?
In-Reply-To: <20041213193435.GC22142@queenbee.fhcrc.org>
References: <3398909b04121309256597d218@mail.gmail.com>
	<20041213193435.GC22142@queenbee.fhcrc.org>
Message-ID: <41BE0EC4.4050206@joeconway.com>

Seth Falcon wrote:
> On Mon, Dec 13, 2004 at 12:25:01PM -0500, doktora v wrote:
> 
>>Is anyone familiar with this (i.e. running R from a non-X
>>environment)? Is there  a way to get around this? I've seen some stuff
>>about virtual devices,  but have no idea if it works or where to
>>start. If there is a simpler solution, please let me know.
> 
> I've used Xvfb in this situation.  After installing Xvfb, you can do
> something like this:
> 
> Xvfb :15&
> export DISPLAY=localhost:15
> # Run R 

FWIW, here's what I've used in the past for an Xvfb init script:

8<------------------------------------
#!/bin/bash
#
# syslog        Starts Xvfb.
#
#
# chkconfig: 2345 12 88
# description: Xvfb is a facility that applications requiring an X frame 
buffer \
# can use in place of actually running X on the server

# Source function library.
. /etc/init.d/functions

[ -f /usr/X11R6/bin/Xvfb ] || exit 0

XVFB="/usr/X11R6/bin/Xvfb :5 -screen 0 1024x768x16"

RETVAL=0

umask 077

start() {
         echo -n $"Starting Xvfb: "
         $XVFB&
         RETVAL=$?
         echo_success
         echo
         [ $RETVAL = 0 ] && touch /var/lock/subsys/Xvfb
         return $RETVAL
}
stop() {
         echo -n $"Shutting down Xvfb: "
         killproc Xvfb
         RETVAL=$?
         echo
         [ $RETVAL = 0 ] && rm -f /var/lock/subsys/Xvfb
         return $RETVAL
}
restart() {
         stop
         start
}

case "$1" in
   start)
         start
         ;;
   stop)
         stop
         ;;
   restart|reload)
         restart
         ;;
   condrestart)
         [ -f /var/lock/subsys/Xvfb ] && restart || :
         ;;
   *)
         echo $"Usage: $0 {start|stop|restart|condrestart}"
         exit 1
esac

exit $RETVAL
8<-----------------------------------------

HTH,

Joe



From doktora at gmail.com  Mon Dec 13 23:22:10 2004
From: doktora at gmail.com (doktora v)
Date: Mon, 13 Dec 2004 17:22:10 -0500
Subject: [R] R 2.0.1 and RODBC install problem
In-Reply-To: <Pine.GSO.4.31.0412131922130.795-100000@markov.stats>
References: <3398909b04121309484bbcb3bf@mail.gmail.com>
	<Pine.GSO.4.31.0412131922130.795-100000@markov.stats>
Message-ID: <3398909b04121314221c295177@mail.gmail.com>

Thank you for pointing me in the right direction. 
As I believe I had done my effort to find the answer prior to posting,
and had a driver manager installed (unixODBC 2.2.8) and the database
odbc drivers (psqlodbc 7.2.4).

However, it might suffice to notice, for future reference (shall
anyone encounter this problem), that the odbc driver manager needs to
be installed with its development module.
Mine wasn't and, consequently, didn't have the header files (sql.h and
sqlext.h) installed.

-- dok

On Mon, 13 Dec 2004 19:24:16 +0000 (GMT), Brian D Ripley
<ripley at stats.ox.ac.uk> wrote:
> From the DESCRIPTION file:
> 
> SystemRequirements:   An ODBC driver manager and drivers. See README.
> 
> PLEASE do as it asks.
> 
> On Mon, 13 Dec 2004, doktora v wrote:
> 
> > Hi all,
> >
> > I try to install the RODBC to no avail. Where is sql.h and sqltext.h
> > supposed to come from? I don't have such files anywhere on my hdd!
> 
> [...]
> 
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> Please do as we ask.
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From gunter.berton at gene.com  Mon Dec 13 23:23:08 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 13 Dec 2004 14:23:08 -0800
Subject: [R] classification for huge datasets: SVM yields memory troubles
In-Reply-To: <cpl0ti$57a$1@sea.gmane.org>
Message-ID: <200412132223.iBDMN8c3001809@compton.gene.com>

" I have a matrix with 30 observations and roughly 30000 
variables, ... <snipped>" 

Comment: This is ** not ** a "huge" data set -- it is a tiny one with a
large number of covariates. The difference is: If it were truly huge, SVM
and/or LDA or ... might actually be able to produce useful results. With so
few data and so many variables, it is hard to see how any approach that one
uses is not simply a fancy random number generator.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andreas
> Sent: Monday, December 13, 2004 12:56 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] classification for huge datasets: SVM yields 
> memory troubles
> 
> Hi,
> 
> I'm a beginner in the SVM-module but I have seen there is a 
> parameter called
> :
> cachesize #cache memory in MB (default 40)
> 
> please let me know if this parameter solved your problem, I 
> might get the
> same number of samples in the near future.
> 
> regards Andreas
> 
> "Christoph Lehmann" <christoph.lehmann at gmx.ch> schrieb im Newsbeitrag
> news:41BD8A9F.4040509 at gmx.ch...
> > Hi
> > I have a matrix with 30 observations and roughly 30000 
> variables, each
> > obs belongs to one of two groups. With svm and slda I get 
> into memory
> > troubles ('cannot allocate vector of size' roughly 2G). PCA LDA runs
> > fine. Are there any way to use the memory issue withe 
> SVM's? Or can you
> > recommend any other classification method for such huge datasets?
> >
> >
> > P.S. I run suse 9.1 on a 2G RAM PIV machine.
> > thanks for a hint
> >
> > Christoph
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

From doktora at gmail.com  Mon Dec 13 23:23:39 2004
From: doktora at gmail.com (doktora v)
Date: Mon, 13 Dec 2004 17:23:39 -0500
Subject: [R] Calling R from a non-X shell script to plot?
In-Reply-To: <41BE0EC4.4050206@joeconway.com>
References: <3398909b04121309256597d218@mail.gmail.com>
	<20041213193435.GC22142@queenbee.fhcrc.org>
	<41BE0EC4.4050206@joeconway.com>
Message-ID: <3398909b041213142346196ddd@mail.gmail.com>

Ah, that's great. I was halfway on my way writing this out... 
Thanks, much appreciated!

-- dok


On Mon, 13 Dec 2004 13:51:00 -0800, Joe Conway <mail at joeconway.com> wrote:
> Seth Falcon wrote:
> > On Mon, Dec 13, 2004 at 12:25:01PM -0500, doktora v wrote:
> >
> >>Is anyone familiar with this (i.e. running R from a non-X
> >>environment)? Is there  a way to get around this? I've seen some stuff
> >>about virtual devices,  but have no idea if it works or where to
> >>start. If there is a simpler solution, please let me know.
> >
> > I've used Xvfb in this situation.  After installing Xvfb, you can do
> > something like this:
> >
> > Xvfb :15&
> > export DISPLAY=localhost:15
> > # Run R
> 
> FWIW, here's what I've used in the past for an Xvfb init script:
> 
> 8<------------------------------------
> #!/bin/bash
> #
> # syslog        Starts Xvfb.
> #
> #
> # chkconfig: 2345 12 88
> # description: Xvfb is a facility that applications requiring an X frame
> buffer \
> # can use in place of actually running X on the server
> 
> # Source function library.
> . /etc/init.d/functions
> 
> [ -f /usr/X11R6/bin/Xvfb ] || exit 0
> 
> XVFB="/usr/X11R6/bin/Xvfb :5 -screen 0 1024x768x16"
> 
> RETVAL=0
> 
> umask 077
> 
> start() {
>          echo -n $"Starting Xvfb: "
>          $XVFB&
>          RETVAL=$?
>          echo_success
>          echo
>          [ $RETVAL = 0 ] && touch /var/lock/subsys/Xvfb
>          return $RETVAL
> }
> stop() {
>          echo -n $"Shutting down Xvfb: "
>          killproc Xvfb
>          RETVAL=$?
>          echo
>          [ $RETVAL = 0 ] && rm -f /var/lock/subsys/Xvfb
>          return $RETVAL
> }
> restart() {
>          stop
>          start
> }
> 
> case "$1" in
>    start)
>          start
>          ;;
>    stop)
>          stop
>          ;;
>    restart|reload)
>          restart
>          ;;
>    condrestart)
>          [ -f /var/lock/subsys/Xvfb ] && restart || :
>          ;;
>    *)
>          echo $"Usage: $0 {start|stop|restart|condrestart}"
>          exit 1
> esac
> 
> exit $RETVAL
> 8<-----------------------------------------
> 
> HTH,
> 
> Joe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Dec 13 23:59:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Dec 2004 23:59:40 +0100
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <20041213175258.GA11944@psych>
References: <20041213170554.39637.qmail@web50305.mail.yahoo.com>
	<20041213175258.GA11944@psych>
Message-ID: <x23by9ztwj.fsf@biostat.ku.dk>

Jonathan Baron <baron at psych.upenn.edu> writes:

> Fedora uses yum as well as rpm.  I haven't installed an RPM in
> months, except for R.  Yum is great.

Actually, Martyn set up CRAN as a yum repository. I have

$ more /etc/yum.repos.d/R.repo
[R]
name=CRAN Fedora $releasever - $basearch
baseurl=http://cran.r-project.org/bin/linux/redhat/fc$releasever/$basearch/
enabled=1
gpgcheck=1

whereafter I could just say "yum install R", and in due course I
presume "yum update R".

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Tue Dec 14 00:20:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Dec 2004 00:20:07 +0100
Subject: [R] Problem with graphic window in LINUX
In-Reply-To: <41BDDDF6.7070403@epm.net.co>
References: <002301c4dfb7$f85955f0$0d01a8c0@tablet>
	<41BB57E3.206@statistik.uni-dortmund.de>
	<20041212053908.JWCM29051.smta08.mail.ozemail.net@there>
	<41BDDDF6.7070403@epm.net.co>
Message-ID: <x2y8g1yee0.fsf@biostat.ku.dk>

Kenneth Cabrera <krcabrer at epm.net.co> writes:

> Hi R users:
> 
> I got a problem when I make graphics in LINUX, the graphic window
> is static, I can resize it, but I can not move it (there is no frame),
> *  what should I configure in LINUX to make them work again?
> * Is it posible to  have a history list of the graphics like in W2K,
> so I  can see them again?
> Thank you for your help
> 
> Kenneth

The first issue is not really Linux per se. It's a weird interaction
between R, X, and the window manager. Happens to me occasionally with
FC3, never did with RH8 on the same hardware. On FC3, you can
right-click the edge and select "move" from the menu, or press Alt-F7
to move windows around if you don't have a top border to grab. Someone
with enough time on his hands needs to look into this with a debugger.

The second issue: Yes, it is a SMOP ("simple matter of programming");
volunteers?  (I suspect that the hardest bit is figuring out how to
attach a menu to the raw-X11 plot window. The display list is there
already and we use it when resizing, etc.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Tue Dec 14 00:23:08 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Dec 2004 00:23:08 +0100
Subject: [R] read attribute
In-Reply-To: <b1d31504041213121078be240a@mail.gmail.com>
References: <b1d31504041213121078be240a@mail.gmail.com>
Message-ID: <x2u0qpye8z.fsf@biostat.ku.dk>

Omar Lakkis <abu3ammar at gmail.com> writes:

> > unclass(ts1)
> list()
....
> attr(,"positions")
> [1] "2003-10-09 02:00:00" "2003-10-10 02:00:00" "2003-10-13 02:00:00"
> [4] "2003-10-14 02:00:00"
..........
> 
> How can I get teh value of the attribute positions?

The output is providing a rather strong hint: attr(ts1, "positions")

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wolf.privat at gmx.de  Tue Dec 14 00:42:02 2004
From: wolf.privat at gmx.de (Andreas)
Date: Tue, 14 Dec 2004 00:42:02 +0100
Subject: [R] Farmating printed Numbers and Text
Message-ID: <cpl9c0$qag$1@sea.gmane.org>

Hello,

is there any function to print (using "cat" or "print" etc.) a table of
values row per row in a exact order, even if the value has 3 digit and in
the next row eg. 5 digits? This means I want to print a number with 3 digits
with 2 spaces in front and a number with 4 digits only with one space in
front.

regards Andreas



From gunter.berton at gene.com  Tue Dec 14 01:41:13 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 13 Dec 2004 16:41:13 -0800
Subject: [R] Farmating printed Numbers and Text
In-Reply-To: <cpl9c0$qag$1@sea.gmane.org>
Message-ID: <200412140041.iBE0fD5j021522@compton.gene.com>


Well..., since no one has responded, try ?format  

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andreas
> Sent: Monday, December 13, 2004 3:42 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Farmating printed Numbers and Text
> 
> Hello,
> 
> is there any function to print (using "cat" or "print" etc.) 
> a table of
> values row per row in a exact order, even if the value has 3 
> digit and in
> the next row eg. 5 digits? This means I want to print a 
> number with 3 digits
> with 2 spaces in front and a number with 4 digits only with 
> one space in
> front.
> 
> regards Andreas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andrewr at uidaho.edu  Tue Dec 14 01:51:51 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Tue, 14 Dec 2004 11:51:51 +1100
Subject: [R] Farmating printed Numbers and Text
In-Reply-To: <200412140041.iBE0fD5j021522@compton.gene.com>
References: <cpl9c0$qag$1@sea.gmane.org>
	<200412140041.iBE0fD5j021522@compton.gene.com>
Message-ID: <20041214005151.GU594@uidaho.edu>

Or ?sprintf

Andrew

On Mon, Dec 13, 2004 at 04:41:13PM -0800, Berton Gunter wrote:
> 
> Well..., since no one has responded, try ?format  
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andreas
> > Sent: Monday, December 13, 2004 3:42 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Farmating printed Numbers and Text
> > 
> > Hello,
> > 
> > is there any function to print (using "cat" or "print" etc.) 
> > a table of
> > values row per row in a exact order, even if the value has 3 
> > digit and in
> > the next row eg. 5 digits? This means I want to print a 
> > number with 3 digits
> > with 2 spaces in front and a number with 4 digits only with 
> > one space in
> > front.
> > 
> > regards Andreas
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From Tom.Mulholland at dpi.wa.gov.au  Tue Dec 14 02:28:33 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 14 Dec 2004 09:28:33 +0800
Subject: [R] read attribute
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C931@afhex01.dpi.wa.gov.au>

Since I'm not sure what you are trying to achieve would ts1 at positions get what you wanted without unclassing the object. I only ask this because my first instinct when I started using the Rmetrics package was to try and use ts1$positions which only gives you a null value.

Tom

> -----Original Message-----
> From: Omar Lakkis [mailto:abu3ammar at gmail.com]
> Sent: Tuesday, 14 December 2004 4:10 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] read attribute
> 
> 
> How can I get a single attribute value of an object ?
> 
> I jhave the tiemSeries object
> > ts1
>                     Open
> 2003-10-09 02:00:00 1.27
> 2003-10-10 02:00:00 1.25
> 2003-10-13 02:00:00 1.27
> 2003-10-14 02:00:00 1.29
> 
> When I unclass ts1 I get:
> > unclass(ts1)
> list()
> attr(,"Data")
>                     Open
> 2003-10-09 02:00:00 1.27
> 2003-10-10 02:00:00 1.25
> 2003-10-13 02:00:00 1.27
> 2003-10-14 02:00:00 1.29
> attr(,"positions")
> [1] "2003-10-09 02:00:00" "2003-10-10 02:00:00" "2003-10-13 02:00:00"
> [4] "2003-10-14 02:00:00"
> attr(,"format")
> [1] "%Y-%m-%d %H:%M:%S"
> attr(,"FinCenter")
> [1] "Zurich"
> attr(,"units")
> [1] "Open"
> attr(,"title")
> [1] "Time Series Object"
> attr(,"documentation")
> [1] "Created at Zurich 2004-12-13 15:13:05"
> 
> How can I get teh value of the attribute positions?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Tue Dec 14 02:47:03 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 14 Dec 2004 09:47:03 +0800
Subject: [R] Moving standard deviation?
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA4D@afhex01.dpi.wa.gov.au>

Thanks for the hint on qcc. As for bollinger bands a generic porcess was put up by Gabor Grothendieck.

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/22367.html

You might also like to have a look at the Rmetrics (fBasics, fSeries etc),
in particular RollingAnalysis {fSeries}

Tom

> -----Original Message-----
> From: doktora v [mailto:doktora at gmail.com]
> Sent: Tuesday, 14 December 2004 2:46 AM
> To: Spencer Graves
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Moving standard deviation?
> 
> 
> I have tried there but didn't find anything useful. Most of the
> matches are for functions which take a std dev input, and the "moving"
> part of the query relates to something else (like moving average in
> the qcc package).
> 
> Anyway, it's not too difficult to create the function, but I was
> wondering if anyone had done it before. Efficiency is a concideration,
> naturally.
> 
> I'll post what i come up with...
> 
> cheers
> dok
> 
> 
> On Mon, 13 Dec 2004 10:04:59 -0800, Spencer Graves
> <spencer.graves at pdf.com> wrote:
> >       A search for "moving standard deviation" at 
www.r-project.org ->
> search -> "R site search" just produced 7 matches.  Please look at those
> and let us know if none of those help you (and what you tried that
> didn't work).
> 
>       spencer graves
> 
> doktora v wrote:
> 
> >Is there a simple function in R to get a moving standard deviation
> >(i.e. for the last x samples)?
> >
> >My goal is to plot bollinger bands around a moving average for price
> >data. I use kernel smoothing for the moving average.
> >
> >cheers and thanks!
> >over and out
> >-- doktora
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> 
> --
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MDavy at hortresearch.co.nz  Tue Dec 14 04:26:33 2004
From: MDavy at hortresearch.co.nz (Marcus Davy)
Date: Tue, 14 Dec 2004 16:26:33 +1300
Subject: [R] Farmating printed Numbers and Text
Message-ID: <s1bf144c.072@hra2.marc.hort.cri.nz>


Or ?formatC, e.g.

> tmp <- c(1,10,100,1000,10000)

> formatC(tmp, digits=4, format="d")
[1] "    1" "   10" "  100" " 1000" "10000"

> sapply(tmp, function(x){sprintf("%5.0f", as.double(x))})
[1] "    1" "   10" "  100" " 1000" "10000"

 
marcus

>>> Andrew Robinson <andrewr at uidaho.edu> 14/12/2004 1:51:51 PM >>>
Or ?sprintf

Andrew

On Mon, Dec 13, 2004 at 04:41:13PM -0800, Berton Gunter wrote:
> 
> Well..., since no one has responded, try ?format  
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andreas
> > Sent: Monday, December 13, 2004 3:42 PM
> > To: r-help at stat.math.ethz.ch 
> > Subject: [R] Farmating printed Numbers and Text
> > 
> > Hello,
> > 
> > is there any function to print (using "cat" or "print" etc.) 
> > a table of
> > values row per row in a exact order, even if the value has 3 
> > digit and in
> > the next row eg. 5 digits? This means I want to print a 
> > number with 3 digits
> > with 2 spaces in front and a number with 4 digits only with 
> > one space in
> > front.
> > 
> > regards Andreas
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu 
PO Box 441133                        W : http://www.uidaho.edu/~andrewr 
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu 
No statement above necessarily represents my employer's opinion.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From sunny.ho at gmail.com  Tue Dec 14 06:53:30 2004
From: sunny.ho at gmail.com (Sunny Ho)
Date: Tue, 14 Dec 2004 13:53:30 +0800
Subject: [R] ROracle/DBI problem with dbExecStatement on RH Linux
In-Reply-To: <200412101123.iBABB6Kw015242@hypatia.math.ethz.ch>
References: <200412101123.iBABB6Kw015242@hypatia.math.ethz.ch>
Message-ID: <ba0addaf04121321531c08f756@mail.gmail.com>

Hi Simon,

I believe you are not using the right syntax for dbExecStatement(). In
particular, you should first use dbPrepareStatement, then
dbExecStatement to execute a "prepared" statement. You may do a "?
dbExecStatement" to read their usage. After some experimenting, I
found that using dbSendQuery is much easier for the 'select' operation
(the name 'Query' may imply that it is written just for the purpose of
'select'). For other operations (insert & delete, etc), then
dbPrepareStatement & dbExecStatement could be more appropriate.

My 2 cents, it is worth the effort to read the ROracle.pdf that comes
with the ROracle package.

Sunny Ho (sunny.ho at gmail.com)

> 
> Date: Thu, 09 Dec 2004 13:06:52 +0000
> From: Simon Holgate <simonh at pol.ac.uk>
> Subject: [R] ROracle/DBI problem with dbExecStatement on RH Linux
> To: r-help at stat.math.ethz.ch
> Message-ID: <41B84DEC.3090902 at pol.ac.uk>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> 
> Hi,
> 
> first thanks to Sunny Ho (and David James for the pointer) for resolving
> the problem of the empty rows returned from ROracle.
> 
> However, I have a problem still:
> 
>   > library(ROracle)
>   > ora <- dbDriver("Oracle")
>   > con <- dbConnect(ora, user = USER, password = PWD, dbname = DBNAME)
>   > rs <- dbExecStatement(con, "select * from USER_TABLES")
>  Error in .class1(object) : No direct or inherited method for function
>  "dbExecStatement" for this call
> 
> dbSendQuery/dbGetQuery work fine. I've reinstalled ROracle 0.5-5 and DBI
> 0.1-8 from source on RH Linux Enterprise WS release 3 Taroon update;
> kernel 2.4.21-9.0.1; Oracle 9.2.0.1; R 1.9.1.
> 
> Any suggestions on resolving this would be much appreciated.
> 
> Cheers,
> 
> Simon
>



From mike_saunders at umenfa.maine.edu  Mon Dec 13 21:37:28 2004
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Mon, 13 Dec 2004 15:37:28 -0500
Subject: [R] How to duplicate rows in dataframe?
References: <1990cf519911fd.19911fd1990cf5@huskymail.uconn.edu>
	<41BDE753.2020201@aon.at>
Message-ID: <001901c4e153$8fba8f80$9ba76f82@CFRU0104>

Christian:

You should be able to do this with the command:

dataframe[order(dataframe$x),]

See order or sort in the R help for optional arguments that can customize 
the way you want to deal with ties.

Mike


----- Original Message ----- 
From: "cstrato" <cstrato at aon.at>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, December 13, 2004 2:02 PM
Subject: [R] How to duplicate rows in dataframe?


> Dear all:
>
> I have the following (simple?) problem:
> Consider a dataframe where the first column contains
> integers used as index, e.g.
>    index
>     24
>     13
>     46
>     32
>
> Now I have the following vector used to sort the dataframe:
>    x <- c(13,24,32,46)
> Sorting the dataframe can be done by using order.
>
> However consider the following vector:
>    x <- c(13,32,13,24,46,24,24)
> Now I want to get the dataframe in the order of the rows
> defined in x, i.e. the dataframe contains duplicate rows.
> One way to achieve this would be to use rbind in a for-loop.
>
> My question is:
> Is there an easier and - more important - faster way to
> obtain the dataframe as defined in x?
>
> Thank you in advance.
> Best regards
> Christian
> _._._._._._._._._._._._._._._._
> C.h.i.s.t.i.a.n S.t.r.a.t.o.w.a
> V.i.e.n.n.a       A.u.s.t.r.i.a
> _._._._._._._._._._._._._._._._
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jarioksa at sun3.oulu.fi  Tue Dec 14 09:46:00 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 14 Dec 2004 10:46:00 +0200
Subject: [R] Switching to Mac, suggestions? (was switching to linux)
In-Reply-To: <3398909b04121309532f885ed7@mail.gmail.com>
References: <3398909b04121309532f885ed7@mail.gmail.com>
Message-ID: <1103013959.17862.103.camel@biol102145.oulu.fi>

On Mon, 2004-12-13 at 19:53, doktora v wrote:
> I'm looking to switch to Mac platform. Anyone had any experience
> with that? I'm expecting on a power G4 laptop later this week.... hope
> R behaves...
> 
Still one comment on speed. I once (and, actually, just now) had to
analyse a big data set of some 1100 observations using various
multivariate methods, among them isoMDS of MASS and eigenvector methods
in vegan library. I made a testsuite of typical analysis sequence for
this very special data set. So it is non-general, but something that
matters to me. I have run this data set on crippled (=Celeron) i686
under Linux and Windows, and on G4 (iBook and iMac) under MacOS X,
Yellowdog Linux 3 and Ubuntu GNU/Linux 4.10. It may be daring to say
something about G4 performance based on this special case, but this
doesn't stop me from saying. For my all sequence, G4 with MacOS X is
somewhat faster compared to cpu speed than Celeron, but not nearly as
much as advertised. There were some procedures that run slower per MHz
than Celeron (isoMDS). However, MacOS X comes with G4-optimized blas, so
that eigenvector based analysis was faster: 800 MHz iBook run like 1400
MHz Celeron, and 1000MHz iMac run like 1700 MHz Celeron. I guess the
boost depends on time you spend in blas. Otherwise you may count that
your G4 cpu cycles equal i686 cpu cycles, and you are slower since you
can get faster Intel chips. Vector processor (AltiVec) may be handy, but
most functions can't use without very tedious and ugly code optimized by
hand. I've seen claims that gcc 3.4 has some automatic G4 optimization.
If this is true, you may get some advantage with G4.

G5 is a different issue.

Yellowdog Linux 3 didn't have G4-optimized blas, and it was really slow.
Actually, 800 MHz iBook run like a 500 MHz Celeron in a blas-heavy
analysis. YD3 was so old that I couldn't build an optimized blas without
extensive upgrading (gcc, glibc etc), and I really wasn't motivated for
that. You can get a G4-optimized blas for Ubuntu GNU/Linux and with that
it runs just as fast as MacOS X.

BTW, this test matter in the sense that I have to run these analyses,
and they take an observable amount of time. The test suite run in 800MHz
iBook in 1600 secs, and in in  2GHZ Celeron in 700 secs. We are not
talking about millisecond boosts but about going to lunch or sitting by
your computer.

Another efficiency issue in Mac is that graphics are superb in Mac. The
default plot (quartz) is small but sharp. It used to scale instantly
when you changed its size, but this deteriorated in 2.0 series.

cheers, jari oksanen

-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From maechler at stat.math.ethz.ch  Tue Dec 14 10:09:58 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 14 Dec 2004 10:09:58 +0100
Subject: [R] UPGMA
In-Reply-To: <79407C23-4D45-11D9-A577-000A9598473A@plantpath.wisc.edu>
References: <79407C23-4D45-11D9-A577-000A9598473A@plantpath.wisc.edu>
Message-ID: <16830.44518.250118.111826@gargle.gargle.HOWL>

>>>>> "Thomas" == Thomas Isenbarger <isen at plantpath.wisc.edu>
>>>>>     on Mon, 13 Dec 2004 14:27:56 -0600 writes:

    Thomas> R-help folks:

    Thomas> Thanks in the past for your help.  I have another
    Thomas> question that I hope has a simple answer.  I have
    Thomas> searched the R home pages and the R-help archives
    Thomas> with no hits.  How can I cluster data in R using
    Thomas> UPGMA?

library(cluster)
?agnes		# uses method="average" aka UPGMA by default

or
?hclust		# has  method="average" as well.


I've added a \concept{} to agnes such that from the next version
of "cluster", 
   help.search("UPGMA clustering") 
will find agnes().

Martin Maechler, ETH Zurich



From siegfried.gonzi at stud.uni-graz.at  Tue Dec 14 10:00:53 2004
From: siegfried.gonzi at stud.uni-graz.at (Siegfried Gonzi)
Date: Tue, 14 Dec 2004 10:00:53 +0100
Subject: [R] Switching to Mac, suggestions? (was switching to linux)
References: <3398909b04121309532f885ed7@mail.gmail.com>
	<1103013959.17862.103.camel@biol102145.oulu.fi>
Message-ID: <41BEABC5.3020103@stud.uni-graz.at>

Jari Oksanen wrote:

>On Mon, 2004-12-13 at 19:53, doktora v wrote:
>
>>I'm looking to switch to Mac platform. Anyone had any experience
>>with that? I'm expecting on a power G4 laptop later this week.... hope
>>R behaves...
>>
>Still one comment on speed. I once (and, actually, just now) had to
>analyse a big data set of some 1100 observations using various
>multivariate methods, among them isoMDS of MASS and eigenvector methods
>in vegan library. I made a testsuite of typical analysis sequence for
>this very special data set. So it is non-general, but something that
>matters to me. I have run this data set on crippled (=Celeron) i686
>under Linux and Windows, and on G4 (iBook and iMac) under MacOS X,
>Yellowdog Linux 3 and Ubuntu GNU/Linux 4.10. It may be daring to say
>something about G4 performance based on this special case, but this
>doesn't stop me from saying. For my all sequence, G4 with MacOS X is
>somewhat faster compared to cpu speed than Celeron, but not nearly as
>much as advertised. There were some procedures that run slower per MHz
>than Celeron (isoMDS). However, MacOS X comes with G4-optimized blas, so
>that eigenvector based analysis was faster: 800 MHz iBook run like 1400
>MHz Celeron, and 1000MHz iMac run like 1700 MHz Celeron. I guess the
>boost depends on time you spend in blas. Otherwise you may count that
>your G4 cpu cycles equal i686 cpu cycles, and you are slower since you
>can get faster Intel chips. Vector processor (AltiVec) may be handy, but
>most functions can't use without very tedious and ugly code optimized by
>hand. I've seen claims that gcc 3.4 has some automatic G4 optimization.
>If this is true, you may get some advantage with G4.
>

Hello:

It is not directly related to your problem. However, the G4 falls far 
behind when speaking of "trigonometric functions". I once had the case 
where my old Celeron 1000 MHz laptop (SuSE LInux 8) outperformed --by a 
factor of two -- my ibook G4 800MHz any time.

At that time I had to perform off-line calculations. The code was based 
on Bigloo which by itself called external C functions.

I posted my observations (note: you can also test the G4/MAC OSX  
'trig-functions' by means of the 'Coyote Gulch benchmark', though, my 
experience was based on a real-life problem) to some Macintosh mailing 
lists. As it turned out my posts have been deleted by the administrator 
due to the fact that people insist that the G4 is the last invention 
since bread-and-butter.

There was only one professor from a Japanese technical university who 
took me for serious and we had some private communications.

One thing which I haven't figured out yet: the altivec libraries are 
tailored to single precion only - right?

Regards,
Siegfried Gonzi
PS: No, I would never trade my ibook for any other laptop.
PSS: And one should quikcly try to forget all the rumors about: "Mac OSX 
actually is slow". I happen to use an ibook 800MHz and 640 MB RAM; and 
"Panther" OSX 10.3.5 is damn fast and snappy.
PSSS: There exists a library from Motorola for the old 68k proecessor; 
that library adresses some of tthe trig-slow-performance issues.



From siegfried.gonzi at stud.uni-graz.at  Tue Dec 14 10:21:18 2004
From: siegfried.gonzi at stud.uni-graz.at (Siegfried Gonzi)
Date: Tue, 14 Dec 2004 10:21:18 +0100
Subject: [R] Switching to Mac, suggestions? (was switching to linux)
References: <3398909b04121309532f885ed7@mail.gmail.com>
	<7ED537C0-4D37-11D9-B26D-000A95C76CA8@oulu.fi>
Message-ID: <41BEB08E.4080108@stud.uni-graz.at>

Jari Oksanen wrote:

>
> On 13 Dec 2004, at 19:53, doktora v wrote:
>
>> I'm looking to switch to Mac platform. Anyone had any experience
>> with that? I'm expecting on a power G4 laptop later this week.... hope
>> R behaves...
>
>
> I have been a Linux user since 1999, and I got my first ever Mac 
> (iBook G4 laptop) last December. There is just as little to comment on 
> MacOS X as there is to comment on flavours of Linux distros: there is 
> no large difference as regards to R. I still prefer emacs & ess as a 
> shell (but you can get some kind of real emacs in Mac as well), but 
> MacOS X/ R is more of an eye candy (though I find it really hard to 
> get any real use for transparent windows in R: I still prefer to see 
> what I type instead of looking the background through the text). As 
> regards to R, it is just the same if you have any brand of Linux or 
> MacOS X or even a fringe system like Windows. The differences are 
> somewhere else than in R.
>
> By the way, Ubuntu "GNU"/Linux works nicely in Mac, with blas who 
> knows about the vector processor in G4. 


Hello:

When relating Mac OSX to Linux there is one thing always worth to note: 
'virtual desktops'. I have seen some mad Mac addicts who neglect the 
fact that 'virtual desktops' are one of the most useful things among X11 
environments. Expose comes nowhere near though!

Typically virtual desktops on the Mac are an add-on feature and cost money.

I prefer to use "Desktop Manager 0.5.1 by Rich Wareham', though, it is 
beta but it is damn stable -- at least in my case. The beta version was 
free but I think I should start my plan of sending the great guy behind 
the project at least a good cigar:

http://wsmanager.sourceforge.net/

 I happen to have engaged typically 6 to 8 virtual desktops at the same 
time.

And when we are at that: one should try to install the Emacs from 
source. There is also an Emas version tailored to "Carbon" out there, 
but the X11 Emacs is more what one is used to comming from Unix or Linux.

That is all what one needs on Mac OSX.

Regards,
S. Gonzi



From siegfried.gonzi at stud.uni-graz.at  Tue Dec 14 10:30:07 2004
From: siegfried.gonzi at stud.uni-graz.at (Siegfried Gonzi)
Date: Tue, 14 Dec 2004 10:30:07 +0100
Subject: [R] switching to Linux, suggestions?
References: <20041213170554.39637.qmail@web50305.mail.yahoo.com>
	<3398909b04121309356055abe8@mail.gmail.com>
Message-ID: <41BEB29F.2030405@stud.uni-graz.at>

doktora v wrote:

>I'm using SUSE with success on intel laptop and AMD desktop. You get
>the best of both worlds: rpm and source. I can easily get the rpm
>packages i need, and compile on my own the things that i can spend
>time on (such as R 2.0.1 -- compiles out of the box on suse).
>
>BTW, I'm looking to switch to Mac platform. Anyone had any experience
>with that? I'm expecting on a power G4 laptop later this week.... hope
>R behaves...
>

Hello:

There is one issue about SuSE Linux: The "Professional version" and the 
"Standard distribution". The professional version cost a tad more.

The advantage of the professional version: you get always the header 
files too in some cases. I once had SuSE Linux 8 on my old Celeron 
laptop. At that time I tried to install "Numerics" on Python. But with 
no avail because the "Standard SuSE distribution" lacks some additional 
header files and you get always the bare minimum only.

That said: the normal SuSE distribution will always let you aft-install 
all the things you need.

Regards,
S. Gonzi
PS: I hope I am not saying somthing outragiuous wrong now: but there 
exists a free Fortran 95 compiler from INTEL for Linux. As far as I know 
it is the one and only free Fortran 95 compiler out there (okay gnu g95 
is on its way). However, it is hard to get INTEL Fortran 95 running on 
Debian Linux a colleague told me. I for myslef can only say that I had 
had no problems in installing Fortran 95 from INTEL on SuSE.



From maechler at stat.math.ethz.ch  Tue Dec 14 11:46:47 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 14 Dec 2004 11:46:47 +0100
Subject: [R] Moving standard deviation?
In-Reply-To: <20041213202646.6242.qmail@web50309.mail.yahoo.com>
References: <20041213202646.6242.qmail@web50309.mail.yahoo.com>
Message-ID: <16830.50327.185258.989256@gargle.gargle.HOWL>

>>>>> "bogdan" == bogdan romocea <br44114 at yahoo.com>
>>>>>     on Mon, 13 Dec 2004 12:26:46 -0800 (PST) writes:

    bogdan> A simple for loop does the job. Why not write your own function?

     movsd <- function(series,lag)
     {
     movingsd <- vector(mode="numeric")
     for (i in lag:length(series))
	     {
	     movingsd[i] <- sd(series[(i-lag+1):i])
	     }
     assign("movingsd",movingsd,.GlobalEnv)
     }

    bogdan> This is very efficient: it takes (much) less time to write from
    bogdan> scratch than to look for an existing function.

yes, it's fine, but the assign() line is really something so
much not-recommendable `` I can't let go through '' :

Your  movsd() provides a function with ``side effect'' : it
"suddenly" creates a global variable 'movingsd' {overwriting
another one, if there was one} instead of doing
the most natural thing for an S (or R) function:  *return* it's
computation:

So, please, use instead something like

     movsd <- function(series,lag)
     {
	msd <- vector(mode="numeric")
	for (i in lag:length(series))
	{
		msd[i] <- sd(series[(i-lag+1):i])
	}
	msd
     }

and then things like

    sy1.3 <- movsd(y1, 3)
    sy1.5 <- movsd(y1, 5)
    sy2.5 <- movsd(y2, 5)

etc.



From debian_list at web.de  Tue Dec 14 12:23:00 2004
From: debian_list at web.de (Martin Wegmann)
Date: Tue, 14 Dec 2004 12:23:00 +0100
Subject: [R] sort() leaves row names unaffected
Message-ID: <200412141223.00437.debian_list@web.de>

Hello, 

I wonder if I ran into a bug. If I do 

summary(df1$X1) -> df1.y

df1.y
a  b   c   d  e
[1,] 50.74627 8.955224 17.91045 19.40299 2.985075

sort(df1.y) 
       a  b   c   d  e
[1,] 2.985075 8.955224 17.91045 19.40299 50.74627

my numbers are sorted but do not anymore correspond to the rownames. 
For me it is counterintuitive that solely the numbers are sorted and not the 
names. Is there a way to sort names + numbers or is this behaviour of sort() 
unintended?

Martin

R 2.0.1-1 debian reposit.



From tang_chalmers at hotmail.com  Tue Dec 14 12:30:31 2004
From: tang_chalmers at hotmail.com (jing tang)
Date: Tue, 14 Dec 2004 12:30:31 +0100
Subject: [R] Several questions in R
Message-ID: <BAY21-F28FAA17F410C29D61B911A83AC0@phx.gbl>

Hi,
I have several small question in R,
1) How to display all the variables in current workspace?
2) How to write a long command in two lines. Suppose one command line is 
long to be put within one line.
Thanks!
tang



From sdavis2 at mail.nih.gov  Tue Dec 14 12:43:07 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 14 Dec 2004 06:43:07 -0500
Subject: [R] Several questions in R
In-Reply-To: <BAY21-F28FAA17F410C29D61B911A83AC0@phx.gbl>
References: <BAY21-F28FAA17F410C29D61B911A83AC0@phx.gbl>
Message-ID: <5291E71A-4DC5-11D9-A4BA-000D933565E8@mail.nih.gov>


On Dec 14, 2004, at 6:30 AM, jing tang wrote:

> Hi,
> I have several small question in R,
> 1) How to display all the variables in current workspace?

ls()

> 2) How to write a long command in two lines. Suppose one command line 
> is long to be put within one line.

Just hit return in most cases.  R is smart enough to know if you are in 
the middle of a function body.

Also, it is important to read the manual, An Introduction to R:

http://cran.r-project.org/manuals.html

Sean



From arv at ono.com  Tue Dec 14 13:44:26 2004
From: arv at ono.com (antonio =?iso-8859-15?q?rodr=EDguez?=)
Date: Tue, 14 Dec 2004 13:44:26 +0100
Subject: [R] Several questions in R
In-Reply-To: <BAY21-F28FAA17F410C29D61B911A83AC0@phx.gbl>
References: <BAY21-F28FAA17F410C29D61B911A83AC0@phx.gbl>
Message-ID: <200412141344.26492.arv@ono.com>

El Martes, 14 de Diciembre de 2004 12:30, jing tang escribi??:
> Hi,
> I have several small question in R,
> 1) How to display all the variables in current workspace?

ls()

> 2) How to write a long command in two lines. Suppose one command line is
> long to be put within one line.

just push 'Enter' key, a '+' will appear so you can continue

antonio

> Thanks!
> tang
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From wuertz at itp.phys.ethz.ch  Tue Dec 14 12:47:16 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Tue, 14 Dec 2004 11:47:16 +0000
Subject: [R] Moving standard deviation?
In-Reply-To: <16830.50327.185258.989256@gargle.gargle.HOWL>
References: <20041213202646.6242.qmail@web50309.mail.yahoo.com>
	<16830.50327.185258.989256@gargle.gargle.HOWL>
Message-ID: <41BED2C4.8070201@itp.phys.ethz.ch>


# Another possibility;

# Have a look on Rmetrics: www.rmetrics.org


# Rmetrics package "fSeries"  has functions to perform rolling analysis:

# FUNCTION:                 DESCRIPTION:
#  rollFun                   Compute Rolling Function Value
#   rollMean                  Compute Rolling Mean
#   rollVar                   Compute Rolling Variance
#   rollMin                   Compute Rolling Minimum
#   rollMax                   Compute Rolling Maximum

# e.g.

rollFun =
function(x, n, FUN, ...)
{   # A function implemented by Diethelm Wuertz

    # Description:
    #   Compute rolling function value

    # FUNCTION:
   
    # Transform:
    x = as.vector(x)
   
    # Roll FUN:
    start = 1
    end = length(x)-n+1
    m = x[start:end]
    for (i in 2:n) {
        start = start + 1
        end = end + 1
        m = cbind(m, x[start:end])}
   
    # Result:
    ans = apply(m, MARGIN = 1, FUN = FUN, ...)
   
    # Return value:
    ans
}


and for the variance:


rollVar  =
function(x, n = 9, trim = TRUE, unbiased = TRUE, na.rm = FALSE)
{   # A function implemented by Diethelm Wuertz

    # Description:
    #   Compute rolling variance

    # FUNCTION:
   
    # Transform:
    x = as.vector(x)
   
    # Roll Var:
    if (na.rm) x = as.vector(na.omit(x))
    rvar = rollFun(x = x, n = n, FUN = var)
    if (!unbiased) rvar = (rvar * (n-1))/n
    if (!trim) rvar = c(rep(NA, (n-1)), rvar)
   
    # Return Value:
    rvar
}


# Additional Arguments:

# trim, unbiased and  na.rm

# try for a vector
x = rnorm(100)
sqrt(rollVar(x, n = 9))
sqrt(rollVar(x, n = 9, trim = FALSE))

# It also works for R's time series objects "ts"
# (for any object which can be transformed in a
#      vector by the function as.vector)
# x.ts = as.ts(x)
sqrt(rollVar(x.ts, 9))
sqrt(rollVar(x.ts, 9, trim = FALSE))

# It works for any function ...
# Try also:
rollFun(x, n=9, FUN=sd)


I have also written functions for the rolling analysis of Rmetrics' S4
timeSeries objects, performing the analysis on equidistant time scales
daily business time scales or any other more complex intraday time
scales for time series data collected from financial markets  in
different time zones and with different DST rules, (not on the running
 index).

Functions for the rolling analysis of financial market data are not 
difficult
to write using Rmetrics timeSeries and Financial Center concepts.
They will be published in one of the next Rmetrics releases.


Regards Diethelm Wuertz




Martin Maechler wrote:

>>>>>>"bogdan" == bogdan romocea <br44114 at yahoo.com>
>>>>>>    on Mon, 13 Dec 2004 12:26:46 -0800 (PST) writes:
>>>>>>            
>>>>>>
>
>    bogdan> A simple for loop does the job. Why not write your own function?
>
>     movsd <- function(series,lag)
>     {
>     movingsd <- vector(mode="numeric")
>     for (i in lag:length(series))
>	     {
>	     movingsd[i] <- sd(series[(i-lag+1):i])
>	     }
>     assign("movingsd",movingsd,.GlobalEnv)
>     }
>
>    bogdan> This is very efficient: it takes (much) less time to write from
>    bogdan> scratch than to look for an existing function.
>
>yes, it's fine, but the assign() line is really something so
>much not-recommendable `` I can't let go through '' :
>
>Your  movsd() provides a function with ``side effect'' : it
>"suddenly" creates a global variable 'movingsd' {overwriting
>another one, if there was one} instead of doing
>the most natural thing for an S (or R) function:  *return* it's
>computation:
>
>So, please, use instead something like
>
>     movsd <- function(series,lag)
>     {
>	msd <- vector(mode="numeric")
>	for (i in lag:length(series))
>	{
>		msd[i] <- sd(series[(i-lag+1):i])
>	}
>	msd
>     }
>
>and then things like
>
>    sy1.3 <- movsd(y1, 3)
>    sy1.5 <- movsd(y1, 5)
>    sy2.5 <- movsd(y2, 5)
>
>etc.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From murdoch at stats.uwo.ca  Tue Dec 14 12:55:40 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 14 Dec 2004 06:55:40 -0500
Subject: [R] Several questions in R
In-Reply-To: <BAY21-F28FAA17F410C29D61B911A83AC0@phx.gbl>
References: <BAY21-F28FAA17F410C29D61B911A83AC0@phx.gbl>
Message-ID: <kuktr0dp1t8o38nsgus8mmimthlut55amr@4ax.com>

On Tue, 14 Dec 2004 12:30:31 +0100, "jing tang"
<tang_chalmers at hotmail.com> wrote:

>Hi,
>I have several small question in R,
>1) How to display all the variables in current workspace?

ls() or objects()

>2) How to write a long command in two lines. Suppose one command line is 
>long to be put within one line.

Just continue on the next line, making sure that the starting line is
syntactically incomplete.  For example

  x +
  y

is a single statement "x + y", whereas

 x
 + y

is two statements.  You can do this by having unbalanced parentheses
in the first line, or ending it with an operator, or maybe other ways
(but it's too early in the morning for me to think of them).

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Tue Dec 14 12:55:42 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 14 Dec 2004 12:55:42 +0100
Subject: [R] sort() leaves row names unaffected
In-Reply-To: <200412141223.00437.debian_list@web.de>
References: <200412141223.00437.debian_list@web.de>
Message-ID: <41BED4BE.8090301@statistik.uni-dortmund.de>

Martin Wegmann wrote:

> Hello, 
> 
> I wonder if I ran into a bug. If I do 
> 
> summary(df1$X1) -> df1.y
> 
> df1.y
> a  b   c   d  e
> [1,] 50.74627 8.955224 17.91045 19.40299 2.985075
> 
> sort(df1.y) 
>        a  b   c   d  e
> [1,] 2.985075 8.955224 17.91045 19.40299 50.74627

Note: You are sorting a matrix rather than a vector!

> my numbers are sorted but do not anymore correspond to the rownames. 

You mean colnames, not rownames!

> For me it is counterintuitive that solely the numbers are sorted and not the 
> names. Is there a way to sort names + numbers or is this behaviour of sort() 
> unintended?

Don't use sort() on matrices, use order() instead, as in:

  x[,order(x)]

Uwe Ligges


> Martin
> 
> R 2.0.1-1 debian reposit.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Antigen_BIVV_MAILS at stat.math.ethz.ch  Tue Dec 14 12:57:50 2004
From: Antigen_BIVV_MAILS at stat.math.ethz.ch (Antigen_BIVV_MAILS@stat.math.ethz.ch)
Date: 14 Dec 2004 12:57:50 +0100
Subject: [R] Antigen forwarded attachment
Message-ID: <BIVV-MAILSb6KZmcK8r00000029@BIVV-MAILS.bivv.local>

The entire message "[R] Several questions in R", originally sent to you by r-help-bounces at stat.math.ethz.ch (r-help-bounces at stat.math.ethz.ch), has been forwarded to you from the Antigen Quarantine area.
This message may have been re-scanned by Antigen and handled according to the appropriate scan job's settings.



<<Entire Message.eml>>
-------------- next part --------------
An embedded message was scrubbed...
From: "jing tang" <tang_chalmers at hotmail.com>
Subject: [R] Several questions in R
Date: Tue, 14 Dec 2004 12:30:31 +0100
Size: 2878
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041214/998eac95/attachment.mht

From Antigen_BIVV_MAILS at stat.math.ethz.ch  Tue Dec 14 12:58:58 2004
From: Antigen_BIVV_MAILS at stat.math.ethz.ch (Antigen_BIVV_MAILS@stat.math.ethz.ch)
Date: 14 Dec 2004 12:58:58 +0100
Subject: [R] Antigen forwarded attachment
Message-ID: <BIVV-MAILSoygXLsxpT00000035@BIVV-MAILS.bivv.local>

The entire message "Re: [R] Several questions in R", originally sent to you by r-help-bounces at stat.math.ethz.ch (r-help-bounces at stat.math.ethz.ch), has been forwarded to you from the Antigen Quarantine area.
This message may have been re-scanned by Antigen and handled according to the appropriate scan job's settings.



<<Entire Message.eml>>
-------------- next part --------------
An embedded message was scrubbed...
From: Sean Davis <sdavis2 at mail.nih.gov>
Subject: Re: [R] Several questions in R
Date: Tue, 14 Dec 2004 06:43:07 -0500
Size: 3375
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041214/ab10d63e/attachment.mht

From ligges at statistik.uni-dortmund.de  Tue Dec 14 12:59:29 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 14 Dec 2004 12:59:29 +0100
Subject: [R] Several questions in R
In-Reply-To: <BAY21-F28FAA17F410C29D61B911A83AC0@phx.gbl>
References: <BAY21-F28FAA17F410C29D61B911A83AC0@phx.gbl>
Message-ID: <41BED5A1.5090309@statistik.uni-dortmund.de>

jing tang wrote:

> Hi,
> I have several small question in R,
> 1) How to display all the variables in current workspace?

See ?ls

> 2) How to write a long command in two lines. Suppose one command line is 
> long to be put within one line.

Just do it at a place where the command is not syntactically complete, 
or use {}.




> Thanks!
> tang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

Please do as you are asked above, and in particular please do read 
documentation.

Uwe Ligges



From chencheva at gmail.com  Tue Dec 14 13:56:07 2004
From: chencheva at gmail.com (Hu Chen)
Date: Tue, 14 Dec 2004 20:56:07 +0800
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <41BEB29F.2030405@stud.uni-graz.at>
References: <20041213170554.39637.qmail@web50305.mail.yahoo.com>
	<3398909b04121309356055abe8@mail.gmail.com>
	<41BEB29F.2030405@stud.uni-graz.at>
Message-ID: <6f3fc9ee04121404561f1034e@mail.gmail.com>

haha....
debian vs rpm again... everywhere..

On Tue, 14 Dec 2004 10:30:07 +0100, Siegfried Gonzi
<siegfried.gonzi at stud.uni-graz.at> wrote:
> doktora v wrote:
> 
> >I'm using SUSE with success on intel laptop and AMD desktop. You get
> >the best of both worlds: rpm and source. I can easily get the rpm
> >packages i need, and compile on my own the things that i can spend
> >time on (such as R 2.0.1 -- compiles out of the box on suse).
> >
> >BTW, I'm looking to switch to Mac platform. Anyone had any experience
> >with that? I'm expecting on a power G4 laptop later this week.... hope
> >R behaves...
> >
> 
> Hello:
> 
> There is one issue about SuSE Linux: The "Professional version" and the
> "Standard distribution". The professional version cost a tad more.
> 
> The advantage of the professional version: you get always the header
> files too in some cases. I once had SuSE Linux 8 on my old Celeron
> laptop. At that time I tried to install "Numerics" on Python. But with
> no avail because the "Standard SuSE distribution" lacks some additional
> header files and you get always the bare minimum only.
> 
> That said: the normal SuSE distribution will always let you aft-install
> all the things you need.
> 
> Regards,
> S. Gonzi
> PS: I hope I am not saying somthing outragiuous wrong now: but there
> exists a free Fortran 95 compiler from INTEL for Linux. As far as I know
> it is the one and only free Fortran 95 compiler out there (okay gnu g95
> is on its way). However, it is hard to get INTEL Fortran 95 running on
> Debian Linux a colleague told me. I for myslef can only say that I had
> had no problems in installing Fortran 95 from INTEL on SuSE.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Tue Dec 14 13:17:40 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 14 Dec 2004 13:17:40 +0100
Subject: [R] sort() leaves row names unaffected
In-Reply-To: <200412141223.00437.debian_list@web.de>
Message-ID: <41BEE7F4.24621.4F0B5A@localhost>

Hi

I am not sure how you did get such result of sort and summary

Summary usually gives

> summary(titr$kys)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.000   1.050   1.250   1.143   1.369   2.000 

and sorting it

> y<-summary(titr$kys)
> y[1]<-5
> y[3]<-10
> y
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  5.000   1.050  10.000   1.143   1.369   2.000 
> sort(y)
1st Qu.    Mean 3rd Qu.    Max.    Min.  Median 
  1.050   1.143   1.369   2.000   5.000  10.000 

as well as

> x<-rnorm(5)*5
> names(x)<-letters[1:5]
> x
         a          b          c          d          e 
-3.8849080  1.9393499  1.8456569 -2.5334288  0.6304933 
> sort(x)
         a          d          e          c          b 
-3.8849080 -2.5334288  0.6304933  1.8456569  1.9393499 


On 14 Dec 2004 at 12:23, Martin Wegmann wrote:

Didn't you redefine something?

Cheers
Petr


> Hello, 
> 
> I wonder if I ran into a bug. If I do 
> 
> summary(df1$X1) -> df1.y
> 
> df1.y
> a  b   c   d  e
> [1,] 50.74627 8.955224 17.91045 19.40299 2.985075
> 
> sort(df1.y) 
>        a  b   c   d  e
> [1,] 2.985075 8.955224 17.91045 19.40299 50.74627
> 
> my numbers are sorted but do not anymore correspond to the rownames.
> For me it is counterintuitive that solely the numbers are sorted and
> not the names. Is there a way to sort names + numbers or is this
> behaviour of sort() unintended?
> 
> Martin
> 
> R 2.0.1-1 debian reposit.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From KINGSEBI at gmx.net  Tue Dec 14 14:14:59 2004
From: KINGSEBI at gmx.net (Sebastian Kaiser)
Date: Tue, 14 Dec 2004 14:14:59 +0100 (MET)
Subject: [R] increase thr range in R
Message-ID: <26416.1103030099@www35.gmx.net>

Hello Everybody in order to get some needed results out of my function i
need to get my besselI function evaluated at some values which normally gave
Inf or 0 (expon.scaled NAN) back. So I would like to increase the range in R
from approxamittly 1e+320 to aabout 1e+500 or something like that. Is there
any possibility or pacckage to do this easily?
Thank You
Sebastian Kaiser
Institut for Statistics in Munich Germany

-- 
Neue Internetadresse:
http:\\www.stat.uni-muenchen.de/~kaiser

"Vielleicht kann ich die Wahrheit finden, indem ich die
L??gen vergleiche." (Leo Trotzki 1879-1940)



From br44114 at yahoo.com  Tue Dec 14 14:30:03 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Tue, 14 Dec 2004 05:30:03 -0800 (PST)
Subject: [R] sort() leaves row names unaffected
Message-ID: <20041214133003.53432.qmail@web50303.mail.yahoo.com>

I asked the same question a few weeks ago. See
http://tolstoy.newcastle.edu.au/R/help/04/11/6775.html


-----Original Message-----
From: Martin Wegmann
Sent: Tuesday, December 14, 2004 6:23 AM
To: r-help at stat.math.ethz.ch
Subject: [R] sort() leaves row names unaffected


Hello, 

I wonder if I ran into a bug. If I do 

summary(df1$X1) -> df1.y

df1.y
a  b   c   d  e
[1,] 50.74627 8.955224 17.91045 19.40299 2.985075

sort(df1.y) 
       a  b   c   d  e
[1,] 2.985075 8.955224 17.91045 19.40299 50.74627

my numbers are sorted but do not anymore correspond to the rownames. 
For me it is counterintuitive that solely the numbers are sorted and
not the 
names. Is there a way to sort names + numbers or is this behaviour of
sort() 
unintended?

Martin

R 2.0.1-1 debian reposit.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



		
__________________________________ 

Dress up your holiday email, Hollywood style. Learn more.



From andy_liaw at merck.com  Tue Dec 14 14:33:18 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 14 Dec 2004 08:33:18 -0500
Subject: [R] sort() leaves row names unaffected
Message-ID: <3A822319EB35174CA3714066D590DCD50994E420@usrymx25.merck.com>

The problem, I think, boils down to the following:

> x <- matrix(c(3, 1, 2), ncol=3, dimnames=list(NULL, letters[1:3]))
> x
     a b c
[1,] 3 1 2
> sort(x)
     a b c
[1,] 1 2 3
> y <- rbind(x, c(1,1,2))
> y
     a b c
[1,] 3 1 2
[2,] 1 1 2
> sort(y)
     a b c
[1,] 1 1 2
[2,] 1 2 3

I.e., I'm guessing your `df1.y' is a matrix with one row and column names
`a' through `e'.  When you sort() a matrix, sort() simply treat it as a
vector (as a matrix in R is just a vector with a dim attribute) and sort
that (by stacking columns).  The output is sorted, but with the original
dimension and dimnames as the input.

Perhaps sort() should be made generic with methods for
matrix/array/data.frame, but one needs to think about what the logical thing
to do if no other argument is given...  In any case, it shouldn't give you
the result you're expecting (at least not by default): you are expecting the
result to be sorted columns, which hardly makes sense if there are more than
one row.

Andy

> From: Martin Wegmann
> 
> Hello, 
> 
> I wonder if I ran into a bug. If I do 
> 
> summary(df1$X1) -> df1.y
> 
> df1.y
> a  b   c   d  e
> [1,] 50.74627 8.955224 17.91045 19.40299 2.985075
> 
> sort(df1.y) 
>        a  b   c   d  e
> [1,] 2.985075 8.955224 17.91045 19.40299 50.74627
> 
> my numbers are sorted but do not anymore correspond to the rownames. 
> For me it is counterintuitive that solely the numbers are 
> sorted and not the 
> names. Is there a way to sort names + numbers or is this 
> behaviour of sort() 
> unintended?
> 
> Martin
> 
> R 2.0.1-1 debian reposit.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Tue Dec 14 14:41:12 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 14 Dec 2004 08:41:12 -0500
Subject: [R] sort() leaves row names unaffected
Message-ID: <3A822319EB35174CA3714066D590DCD50994E421@usrymx25.merck.com>

I don't see how this is the `same' question at all.  The object Martin has
doesn't look like a data frame (can you tell why?), so I believe is
unrelated to what you asked before.

Andy

ps: Why is this copied to debian_list at web.de?


> From: bogdan romocea
> 
> I asked the same question a few weeks ago. See
> http://tolstoy.newcastle.edu.au/R/help/04/11/6775.html
> 
> 
> -----Original Message-----
> From: Martin Wegmann
> Sent: Tuesday, December 14, 2004 6:23 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] sort() leaves row names unaffected
> 
> 
> Hello, 
> 
> I wonder if I ran into a bug. If I do 
> 
> summary(df1$X1) -> df1.y
> 
> df1.y
> a  b   c   d  e
> [1,] 50.74627 8.955224 17.91045 19.40299 2.985075
> 
> sort(df1.y) 
>        a  b   c   d  e
> [1,] 2.985075 8.955224 17.91045 19.40299 50.74627
> 
> my numbers are sorted but do not anymore correspond to the rownames. 
> For me it is counterintuitive that solely the numbers are sorted and
> not the 
> names. Is there a way to sort names + numbers or is this behaviour of
> sort() 
> unintended?
> 
> Martin
> 
> R 2.0.1-1 debian reposit.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> 		
> __________________________________ 
> 
> Dress up your holiday email, Hollywood style. Learn more.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From plummer at iarc.fr  Tue Dec 14 14:43:22 2004
From: plummer at iarc.fr (Martyn Plummer)
Date: Tue, 14 Dec 2004 14:43:22 +0100
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <x23by9ztwj.fsf@biostat.ku.dk>
References: <20041213170554.39637.qmail@web50305.mail.yahoo.com>
	<20041213175258.GA11944@psych>  <x23by9ztwj.fsf@biostat.ku.dk>
Message-ID: <1103031802.3693.8.camel@nemo>

On Mon, 2004-12-13 at 23:59 +0100, Peter Dalgaard wrote:
> Jonathan Baron <baron at psych.upenn.edu> writes:
> 
> > Fedora uses yum as well as rpm.  I haven't installed an RPM in
> > months, except for R.  Yum is great.
> 
> Actually, Martyn set up CRAN as a yum repository. I have
> 
> $ more /etc/yum.repos.d/R.repo
> [R]
> name=CRAN Fedora $releasever - $basearch
> baseurl=http://cran.r-project.org/bin/linux/redhat/fc$releasever/$basearch/
> enabled=1
> gpgcheck=1
> 
> whereafter I could just say "yum install R", and in due course I
> presume "yum update R".

You can also configure "up2date" - the slightly annoying pulsating red
icon on the panel - to get automatic notification of updates from CRAN.
Instructions are in the ReadMe file in the same directory as the Fedora
Core 3 RPMS.

Martyn



From kjetil at acelerate.com  Tue Dec 14 01:17:54 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 13 Dec 2004 20:17:54 -0400
Subject: [R] lists within a list / data-structure problem
In-Reply-To: <200412131644.iBDGibXH000505@meitner.gene.com>
References: <200412131644.iBDGibXH000505@meitner.gene.com>
Message-ID: <41BE3132.5070503@acelerate.com>

Berton Gunter wrote:

>Jan:
>
>One thing to keep in mind: A list is vector. So vector-type operations like
>c(), "[", etc. work on lists, too (but be careful). Some comments inline
>below that I hope will be helpful. A good reference on the S language is
>V&R's S PROGRAMMING, which I recommend highly.
>
>  
>
You can also have a matrix of lists.

Kjetil


>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
> 
> 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jan Wantia
>>Sent: Monday, December 13, 2004 8:00 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] lists within a list / data-structure problem
>>
>>Dear all,
>>
>>this is a rather basic question; i am not sure how to 
>>structure my data
>>well:
>>I want to extraxt various measures from my raw-data. These 
>>measures are
>>of different sizes, so I decided to store them in a list, like:
>>
>>run1 <- list(Dom = (my_vector), mean = (my_single_number))
>>
>>I can do that in a for loop for 40 runs, ending up with 40 
>>lists: run1,
>>run2, ..., run40.
>>To have all the measurements neatly together I thought of 
>>making another
>>list, containing  40 sub-lists:
>>
>>    
>>
>
>As you found, this is clumsy. The usual way to do this is to put the results
>into a *single* list as follows:
>
>## Contruct the empty list with 40 components:
>out<-vector("list", 40)
>### loop 
>... ## do the calculations
>out[[i]] <- list(Dom = yourvec,mean=yournumb)
>...
>
>  
>
>> > ALL <- list(run1, run2,..., run40)
>> > ALL
>>[[1]]
>>[[1]]$Dom
>>[1] "my_vector"
>>
>>[[1]]$mean
>>[1] "my_single_number"
>>
>>
>>[[2]]
>>[[2]]$Dom
>>[1] "my_vector"
>>
>>[[2]]$mean
>>[1] "my_single_number"
>>
>>...
>>
>>1) This may be a bit clumsy as I have to type all the sub-list's names
>>in by hand in order to produce my ALL-list: Is there a better way?
>>
>>2) I have problems of addressing the data now. I can easily access any
>>single value; for example, for the second component of the 
>>second sub- list:
>>    
>>
>
>out[[i]] is the ith component of the list, i.e. the ith 2 component list
>containing the result of the ith loop. So out[[i]][[1]] is yourvec for the
>ith loop and out[[i]][[2]] is yournumb. These can be abbreciated as
>out[[c(i,1]] and out[[c(i,2)]]
>
>  
>
>> > ALL[[2]][[2]]
>>[1] "my_single_number",
>>
>>but: how could I get the second component of all sub-lists, 
>>to plot, for
>>example, all the $mean in one plot? For a matrix, mat[,2] 
>>would give me
>>the whole second column, but
>>ALL[[]][[2]]
>>does not return all the second components.
>>
>>I feel that 'lapply' might help me here, but I could not figure out
>>exactly how to use it, and it always comes down to the 
>>problem of how to
>>correctly address the components in the sublists.
>>
>>Or is there maybe a smarter way to do that instead of using a 
>>list of lists?
>>
>>Any hint would be warmly appreciated!
>>
>>Jan
>>(R 2.0.1 on windows XP)
>>
>>-- 
>>
>>______________________________________________________
>>
>>Jan Wantia
>>Deptartment of Informatics, University of Zurich
>>Andreasstr. 15
>>CH 8050 Zurich
>>Switzerland
>>
>>Tel.:    +41 (0) 1 635 4315
>>Fax:     +41 (0) 1 635 45 07
>>email: wantia at ifi.unizh.ch
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From plaia at unipa.it  Tue Dec 14 14:57:50 2004
From: plaia at unipa.it (Antonella Plaia)
Date: Tue, 14 Dec 2004 14:57:50 +0100
Subject: [R] correlation in lme4
Message-ID: <JFECKBNPHCAGDIGMHLCNGEKCCEAA.plaia@unipa.it>

Dear all,
I have tried to consider a correlation structure in lme (package lme4), but
without success.
I have used something like:

> risul<-lme(y~x+ z , data=mydata, random=~ x | g, correlation = corAR1())

but the result is the same as:

> risul<-lme(y~x+ z , data=mydata, random=~ x | g).

Can anybody help me?

Antonella


**************************************************
Prof. Antonella Plaia
Dipartimento di Scienze Statistiche e Matematiche "S. Vianelli"
Universit?? di Palermo
Viale delle Scienze 90128 Palermo ITALIA
Tel. +39 091 6626244 Fax +39 091 485726
http://dssm.unipa.it/plaia
e-mail:plaia at unipa.it

From MSchwartz at MedAnalytics.com  Tue Dec 14 15:13:45 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 14 Dec 2004 08:13:45 -0600
Subject: [R] Slashdot thread references R/Quantian and another new R/S-PLUS
	book pending
Message-ID: <1103033626.12742.22.camel@horizons.localdomain>

Greetings all,

Just a quick heads up that there is a Slashdot thread this morning on
Open Source Math Software for Education, which includes references to R
and Dirk's Quantian distribution (among others). The thread is at:

http://ask.slashdot.org/askslashdot/04/12/13/2355258.shtml?tid=185&tid=4

The OP's query was targeted for high school to first year college age
students, but as is typical with Slashdot, the scope of the dialog tends
to wander....


Also, I was reading the latest issue of Amstat News and noted that there
is a new book from Springer on R/S-PLUS pending by Professor Brian
Everitt called "An R And S-Plus Companion To Multivariate Analysis". The
book appears to be due to be release next month. The link for the
Springer site is:

http://www.springeronline.com/sgw/cda/frontpage/0,11855,4-40109-22-34953445-0,00.html

and the Amazon.com link is:

http://www.amazon.com/exec/obidos/tg/detail/-/1852338822

Professor Everitt's prior related work, co-authored with Dr. Sophia
Rabe-Hesketh, is "Analyzing Medical Data Using S-PLUS" also from
Springer, which along with the other related books that I have, provided
a solid reference when I started using R a few years ago.

HTH,

Marc Schwartz



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Dec 14 15:43:44 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 14 Dec 2004 15:43:44 +0100
Subject: [R] correlation in lme4
References: <JFECKBNPHCAGDIGMHLCNGEKCCEAA.plaia@unipa.it>
Message-ID: <008a01c4e1eb$4f8336e0$0540210a@www.domain>

Hi Antonella,

I think that lme4 does not have yet corStruct classes available, at 
least for the currect version for Windows which I have. So I'd suggest 
you to use the nlme package.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Antonella Plaia" <plaia at unipa.it>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, December 14, 2004 2:57 PM
Subject: [R] correlation in lme4


> Dear all,
> I have tried to consider a correlation structure in lme (package 
> lme4), but
> without success.
> I have used something like:
>
>> risul<-lme(y~x+ z , data=mydata, random=~ x | g, correlation = 
>> corAR1())
>
> but the result is the same as:
>
>> risul<-lme(y~x+ z , data=mydata, random=~ x | g).
>
> Can anybody help me?
>
> Antonella
>
>
> **************************************************
> Prof. Antonella Plaia
> Dipartimento di Scienze Statistiche e Matematiche "S. Vianelli"
> Universit?? di Palermo
> Viale delle Scienze 90128 Palermo ITALIA
> Tel. +39 091 6626244 Fax +39 091 485726
> http://dssm.unipa.it/plaia
> e-mail:plaia at unipa.it
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From halldor at vedur.is  Tue Dec 14 16:01:17 2004
From: halldor at vedur.is (=?ISO-8859-1?Q?Halldor_Bj=F6rnsson?=)
Date: Tue, 14 Dec 2004 15:01:17 +0000
Subject: [R] plot with dates
Message-ID: <41BF003D.6010004@vedur.is>


Hi,
I am trying to understand the behaviour of the plot function.
If I have

  novdate <- as.Date("2001/11/1") + (0:29)
  y <- 1:30
  b <- data.frame(novdate,y)

then plot(b$novdate,b$y) will produce a plot where the x-ticmarks are 
given as dates (Nov 04, Nov 09 etc), but plot(b) will produce a plot
where the x-tickmars are integer values (#day since Jan 1st 1970)

In the first case plot is getting a an x-vector of class Date, and 
y-vector of class integer.  In the second case plot gets an object of 
class data.frame (but with components of class Date and integer).


I am new to R so I may be wrong about this, but it seems to me that
different plotting methods are invoked. methods(plot) yields a list of
plot methods, but I cannot access most of them.

Is there a way to guide plot(b) to using the method used by 
plot(b$novdate,b$y), - or is that a bad idea?...

Sincerely,
Halld??r


------------------------------------------
Halldor Bjornsson   (halldor at vedur.is)
Vedurstofa Islands (Icelandic Met. Office)
Bustadavegur 9, IS-150, Reykjavik, Iceland



From chencheva at gmail.com  Tue Dec 14 15:57:28 2004
From: chencheva at gmail.com (Hu Chen)
Date: Tue, 14 Dec 2004 22:57:28 +0800
Subject: [R] amd 64 ??
Message-ID: <6f3fc9ee041214065751d35ff3@mail.gmail.com>

Hi, I got a amd 64 + debian/pure64 system.
R is installed by apt-get so it's no doubt that R is for 64bit.
My question is, for those packages, for example, ipred, it's difficult
to find in which official debian packages they are. How could I
install them? if I used install.packages in R, the packges I get are
64 bit or 32 bit?
Thank you all.



From ripley at stats.ox.ac.uk  Tue Dec 14 16:11:43 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Tue, 14 Dec 2004 15:11:43 +0000 (GMT)
Subject: [R] plot with dates
In-Reply-To: <41BF003D.6010004@vedur.is>
Message-ID: <Pine.GSO.4.31.0412141506210.16090-100000@markov.stats>

On Tue, 14 Dec 2004, [ISO-8859-1] Halldor Bjrnsson wrote:

>
> Hi,
> I am trying to understand the behaviour of the plot function.
> If I have
>
>   novdate <- as.Date("2001/11/1") + (0:29)
>   y <- 1:30
>   b <- data.frame(novdate,y)
>
> then plot(b$novdate,b$y) will produce a plot where the x-ticmarks are
> given as dates (Nov 04, Nov 09 etc), but plot(b) will produce a plot
> where the x-tickmars are integer values (#day since Jan 1st 1970)
>
> In the first case plot is getting a an x-vector of class Date, and
> y-vector of class integer.  In the second case plot gets an object of
> class data.frame (but with components of class Date and integer).
>
>
> I am new to R so I may be wrong about this, but it seems to me that
> different plotting methods are invoked. methods(plot) yields a list of
> plot methods, but I cannot access most of them.

You would be able to had you read the help page for methods().  E.g. use
getS3method("plot", "data.frame")  Or try ?plot.data.frame.

> Is there a way to guide plot(b) to using the method used by
> plot(b$novdate,b$y), - or is that a bad idea?...

It's the natural way to do so.  The plot method for data frames is really
designed for a quick look at multi-column numeric data frames.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Dec 14 16:21:06 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Dec 2004 16:21:06 +0100
Subject: [R] amd 64 ??
In-Reply-To: <6f3fc9ee041214065751d35ff3@mail.gmail.com>
References: <6f3fc9ee041214065751d35ff3@mail.gmail.com>
Message-ID: <x2wtvkvrbx.fsf@biostat.ku.dk>

Hu Chen <chencheva at gmail.com> writes:

> Hi, I got a amd 64 + debian/pure64 system.
> R is installed by apt-get so it's no doubt that R is for 64bit.
> My question is, for those packages, for example, ipred, it's difficult
> to find in which official debian packages they are. How could I
> install them? if I used install.packages in R, the packges I get are
> 64 bit or 32 bit?

If they work, they're 64 bit...

install.packages build from source, so you should be OK provided your
toolchain is complete.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From efg at stowers-institute.org  Tue Dec 14 17:00:25 2004
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 14 Dec 2004 10:00:25 -0600
Subject: [R] increase thr range in R
References: <26416.1103030099@www35.gmx.net>
Message-ID: <cpn2n0$5qo$1@sea.gmane.org>

IEEE floating point provides a "double-extended" type often called a long
double.  The exact size of a double-extended can vary but the minimum
specified by the spec is 80 bits.  A PC with the IEEE extended type gives an
approximate range from 3.4E-4932 to 1.1E4932, which would give you the range
you want.  Other architectures, such as a Tru64 alpha, give an even larger
extended.

Reference:  See the "IEEE Standard" in
"What Every Computer Scientist Should Know About Floating-Point Arithmetic"
http://docs.sun.com/source/806-3568/ncg_goldberg.html

The "note" under R's ?numeric help says "All real numbers are stored in
double precision format," so R apparently does not yet support the extended
type.

str(.Machine) in R gives a sizeof.longdouble, but I don't know how to use
such a longdouble in native R -- perhaps someone else could enlighten us.  C
or C++ (or other languges) would support the long double type and the
computations you'd like to do.

efg
Earl F. Glynn
Stowers Institute for Medical Research

=========
"Sebastian Kaiser" <KINGSEBI at gmx.net> wrote in message
news:26416.1103030099 at www35.gmx.net...
Hello Everybody in order to get some needed results out of my function i
need to get my besselI function evaluated at some values which normally gave
Inf or 0 (expon.scaled NAN) back. So I would like to increase the range in R
from approxamittly 1e+320 to aabout 1e+500 or something like that. Is there
any possibility or pacckage to do this easily?
Thank You
Sebastian Kaiser
Institut for Statistics in Munich Germany



From spencer.graves at pdf.com  Tue Dec 14 17:56:21 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 14 Dec 2004 08:56:21 -0800
Subject: [R] increase thr range in R
In-Reply-To: <cpn2n0$5qo$1@sea.gmane.org>
References: <26416.1103030099@www35.gmx.net> <cpn2n0$5qo$1@sea.gmane.org>
Message-ID: <41BF1B35.7090400@pdf.com>

Sebastian: 

      I routinely deal with situations like that in one of two ways: 

      (1) Can you work with log(besselI)?  If yes, that should solve the 
problem. 

      (2) What do you do with the numbers returned from besselI?  I 
assume they are later used to compute numbers in a more sensible range 
by division or differencing.  There are many asymptotic approximations, 
etc., that should work quite well to simplify the computations in those 
extreme cases. 

      hope this helps.  spencer graves    

Earl F. Glynn wrote:

>IEEE floating point provides a "double-extended" type often called a long
>double.  The exact size of a double-extended can vary but the minimum
>specified by the spec is 80 bits.  A PC with the IEEE extended type gives an
>approximate range from 3.4E-4932 to 1.1E4932, which would give you the range
>you want.  Other architectures, such as a Tru64 alpha, give an even larger
>extended.
>
>Reference:  See the "IEEE Standard" in
>"What Every Computer Scientist Should Know About Floating-Point Arithmetic"
>http://docs.sun.com/source/806-3568/ncg_goldberg.html
>
>The "note" under R's ?numeric help says "All real numbers are stored in
>double precision format," so R apparently does not yet support the extended
>type.
>
>str(.Machine) in R gives a sizeof.longdouble, but I don't know how to use
>such a longdouble in native R -- perhaps someone else could enlighten us.  C
>or C++ (or other languges) would support the long double type and the
>computations you'd like to do.
>
>efg
>Earl F. Glynn
>Stowers Institute for Medical Research
>
>=========
>"Sebastian Kaiser" <KINGSEBI at gmx.net> wrote in message
>news:26416.1103030099 at www35.gmx.net...
>Hello Everybody in order to get some needed results out of my function i
>need to get my besselI function evaluated at some values which normally gave
>Inf or 0 (expon.scaled NAN) back. So I would like to increase the range in R
>from approxamittly 1e+320 to aabout 1e+500 or something like that. Is there
>any possibility or pacckage to do this easily?
>Thank You
>Sebastian Kaiser
>Institut for Statistics in Munich Germany
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From ggrothendieck at myway.com  Tue Dec 14 18:41:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 14 Dec 2004 12:41:57 -0500 (EST)
Subject: [R] plot with dates
Message-ID: <20041214174157.0DAB739F3@mprdmxin.myway.com>



Halldor Bjrnsson <halldor <at> vedur.is> writes:

: 
: Hi,
: I am trying to understand the behaviour of the plot function.
: If I have
: 
:   novdate <- as.Date("2001/11/1") + (0:29)
:   y <- 1:30
:   b <- data.frame(novdate,y)
: 
: then plot(b$novdate,b$y) will produce a plot where the x-ticmarks are 
: given as dates (Nov 04, Nov 09 etc), but plot(b) will produce a plot
: where the x-tickmars are integer values (#day since Jan 1st 1970)
: 
: In the first case plot is getting a an x-vector of class Date, and 
: y-vector of class integer.  In the second case plot gets an object of 
: class data.frame (but with components of class Date and integer).
: 
: I am new to R so I may be wrong about this, but it seems to me that
: different plotting methods are invoked. methods(plot) yields a list of
: plot methods, but I cannot access most of them.

If you issue the command plot(x, ...whatever...) it will
wind up calling plot.foo if class(x) is "foo".  You can find
the possibilities via:

methods(plot)

To view the starred ones on the output from the above
you have to do somethingk like this assuming you want
to look at the source for plot.acf:

stats:::plot.acf

(since plot.acf is from the stats package).

: 
: Is there a way to guide plot(b) to using the method used by 
: plot(b$novdate,b$y), - or is that a bad idea?...
: 
: Sincerely,
: Halldr
: 


This really looks like a time series so you probably really want
to treat it that way, not as a data frame.

library(zoo)
z <- zoo(y, novdate)
plot(z)



From andy_liaw at merck.com  Tue Dec 14 19:19:52 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 14 Dec 2004 13:19:52 -0500
Subject: [R] make check-all failed on SLES8 for x86_64
Message-ID: <3A822319EB35174CA3714066D590DCD50994E424@usrymx25.merck.com>

Hi all,

I've only now noticed that on our Opteron boxes running Suse Enterprise
Server 8 that R-2.0.1 did not pass make chek-all.  It bombed in
tests/lapack.R at:

> eigenok <- function(A, E, Eps = 1000 * .Machine$double.eps) {
    V <- E$vect
    lam <- E$values
    stopifnot(abs(A %*% V - V %*% diag(lam)) < Eps,  .... [TRUNCATED] 

> Ceigenok <- function(A, E, Eps = 1000 * .Machine$double.eps) {
    V <- E$vect
    lam <- E$values
    stopifnot(Mod(A %*% V - V %*% diag(lam)) < Eps, .... [TRUNCATED] 

> sm <- cbind(1, 3:1, 1:3)

> eigenok(sm, eigen(sm))
Error: abs(A %*% V - V %*% diag(lam)) < Eps is not TRUE

This is using:
~/R-2.0.1> gcc -v
Reading specs from /opt/gcc33/lib64/gcc-lib/x86_64-suse-linux/3.3/specs
Configured with: ../configure --enable-threads=posix --prefix=/opt/gcc33
--with-local-prefix=/usr/local --infodir=/opt/gcc33/share/info
--mandir=/opt/gcc33/share/man --libdir=/opt/gcc33/lib64
--enable-languages=c,c++,f77,objc,java,ada --disable-checking
--enable-libgcj --with-gxx-include-dir=/opt/gcc33/include/g++
--with-slibdir=/lib64 --with-system-zlib --enable-shared
--enable-__cxa_atexit x86_64-suse-linux
Thread model: posix
gcc version 3.3 20030312 (prerelease) (SuSE Linux)

compiled with the following setting:

R is now configured for x86_64-unknown-linux-gnu

  Source directory:          .
  Installation directory:    /usr/local

  C compiler:                gcc  -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2

  Interfaces supported:      X11, tcltk
  External libraries:        readline
  Additional capabilities:   PNG, JPEG
  Options enabled:           R profiling

  Recommended packages:      yes

Actually, I've only noticed this when the samething happens on a RH-based
box (same hardware), using:
$ gcc -v
Reading specs from /usr/lib/gcc-lib/x86_64-redhat-linux/3.2.3/specs
Configured with: ../configure --prefix=/usr --mandir=/usr/share/man
--infodir=/usr/share/info --enable-shared --enable-threads=posix
--disable-checking --with-system-zlib --enable-__cxa_atexit
--enable-languages=c,c++,objc,java,f77 --host=x86_64-redhat-linux
Thread model: posix
gcc version 3.2.3 20030502 (Red Hat Linux 3.2.3-24)

I'd very much appreciate any pointers!

Best,
Andy


Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
andy_liaw <at> merck.com          732-594-0820



From rpeng at jhsph.edu  Tue Dec 14 19:54:58 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 14 Dec 2004 13:54:58 -0500
Subject: [R] make check-all failed on SLES8 for x86_64
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E424@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E424@usrymx25.merck.com>
Message-ID: <41BF3702.2070707@jhsph.edu>

Martin Maechler brought this up a while back (and added the regression 
test).  I believe it was a compiler problem and upgrading to gcc 3.4.1 
fixed it.

See the thread starting here:

https://stat.ethz.ch/pipermail/r-devel/2004-July/030260.html

-roger

Liaw, Andy wrote:
> Hi all,
> 
> I've only now noticed that on our Opteron boxes running Suse Enterprise
> Server 8 that R-2.0.1 did not pass make chek-all.  It bombed in
> tests/lapack.R at:
> 
> 
>>eigenok <- function(A, E, Eps = 1000 * .Machine$double.eps) {
> 
>     V <- E$vect
>     lam <- E$values
>     stopifnot(abs(A %*% V - V %*% diag(lam)) < Eps,  .... [TRUNCATED] 
> 
> 
>>Ceigenok <- function(A, E, Eps = 1000 * .Machine$double.eps) {
> 
>     V <- E$vect
>     lam <- E$values
>     stopifnot(Mod(A %*% V - V %*% diag(lam)) < Eps, .... [TRUNCATED] 
> 
> 
>>sm <- cbind(1, 3:1, 1:3)
> 
> 
>>eigenok(sm, eigen(sm))
> 
> Error: abs(A %*% V - V %*% diag(lam)) < Eps is not TRUE
> 
> This is using:
> ~/R-2.0.1> gcc -v
> Reading specs from /opt/gcc33/lib64/gcc-lib/x86_64-suse-linux/3.3/specs
> Configured with: ../configure --enable-threads=posix --prefix=/opt/gcc33
> --with-local-prefix=/usr/local --infodir=/opt/gcc33/share/info
> --mandir=/opt/gcc33/share/man --libdir=/opt/gcc33/lib64
> --enable-languages=c,c++,f77,objc,java,ada --disable-checking
> --enable-libgcj --with-gxx-include-dir=/opt/gcc33/include/g++
> --with-slibdir=/lib64 --with-system-zlib --enable-shared
> --enable-__cxa_atexit x86_64-suse-linux
> Thread model: posix
> gcc version 3.3 20030312 (prerelease) (SuSE Linux)
> 
> compiled with the following setting:
> 
> R is now configured for x86_64-unknown-linux-gnu
> 
>   Source directory:          .
>   Installation directory:    /usr/local
> 
>   C compiler:                gcc  -g -O2
>   C++ compiler:              g++  -g -O2
>   Fortran compiler:          g77  -g -O2
> 
>   Interfaces supported:      X11, tcltk
>   External libraries:        readline
>   Additional capabilities:   PNG, JPEG
>   Options enabled:           R profiling
> 
>   Recommended packages:      yes
> 
> Actually, I've only noticed this when the samething happens on a RH-based
> box (same hardware), using:
> $ gcc -v
> Reading specs from /usr/lib/gcc-lib/x86_64-redhat-linux/3.2.3/specs
> Configured with: ../configure --prefix=/usr --mandir=/usr/share/man
> --infodir=/usr/share/info --enable-shared --enable-threads=posix
> --disable-checking --with-system-zlib --enable-__cxa_atexit
> --enable-languages=c,c++,objc,java,f77 --host=x86_64-redhat-linux
> Thread model: posix
> gcc version 3.2.3 20030502 (Red Hat Linux 3.2.3-24)
> 
> I'd very much appreciate any pointers!
> 
> Best,
> Andy
> 
> 
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300     
> Merck Research Labs           Rahway, NJ 07065
> andy_liaw <at> merck.com          732-594-0820
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From p.dalgaard at biostat.ku.dk  Tue Dec 14 20:02:42 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Dec 2004 20:02:42 +0100
Subject: [R] make check-all failed on SLES8 for x86_64
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E424@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E424@usrymx25.merck.com>
Message-ID: <x2y8g0u2i5.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> Hi all,
> 
> I've only now noticed that on our Opteron boxes running Suse Enterprise
> Server 8 that R-2.0.1 did not pass make chek-all.  It bombed in
> tests/lapack.R at:
> 
> > eigenok <- function(A, E, Eps = 1000 * .Machine$double.eps) {
>     V <- E$vect
>     lam <- E$values
>     stopifnot(abs(A %*% V - V %*% diag(lam)) < Eps,  .... [TRUNCATED] 
> 
> > Ceigenok <- function(A, E, Eps = 1000 * .Machine$double.eps) {
>     V <- E$vect
>     lam <- E$values
>     stopifnot(Mod(A %*% V - V %*% diag(lam)) < Eps, .... [TRUNCATED] 
> 
> > sm <- cbind(1, 3:1, 1:3)
> 
> > eigenok(sm, eigen(sm))
> Error: abs(A %*% V - V %*% diag(lam)) < Eps is not TRUE
> 
> This is using:
> ~/R-2.0.1> gcc -v
> Reading specs from /opt/gcc33/lib64/gcc-lib/x86_64-suse-linux/3.3/specs
> Configured with: ../configure --enable-threads=posix --prefix=/opt/gcc33
> --with-local-prefix=/usr/local --infodir=/opt/gcc33/share/info
> --mandir=/opt/gcc33/share/man --libdir=/opt/gcc33/lib64
> --enable-languages=c,c++,f77,objc,java,ada --disable-checking
> --enable-libgcj --with-gxx-include-dir=/opt/gcc33/include/g++
> --with-slibdir=/lib64 --with-system-zlib --enable-shared
> --enable-__cxa_atexit x86_64-suse-linux
> Thread model: posix
> gcc version 3.3 20030312 (prerelease) (SuSE Linux)
> 
> compiled with the following setting:
> 
> R is now configured for x86_64-unknown-linux-gnu
> 
>   Source directory:          .
>   Installation directory:    /usr/local
> 
>   C compiler:                gcc  -g -O2
>   C++ compiler:              g++  -g -O2
>   Fortran compiler:          g77  -g -O2
> 
>   Interfaces supported:      X11, tcltk
>   External libraries:        readline
>   Additional capabilities:   PNG, JPEG
>   Options enabled:           R profiling
> 
>   Recommended packages:      yes
> 
> Actually, I've only noticed this when the samething happens on a RH-based
> box (same hardware), using:
> $ gcc -v
> Reading specs from /usr/lib/gcc-lib/x86_64-redhat-linux/3.2.3/specs
> Configured with: ../configure --prefix=/usr --mandir=/usr/share/man
> --infodir=/usr/share/info --enable-shared --enable-threads=posix
> --disable-checking --with-system-zlib --enable-__cxa_atexit
> --enable-languages=c,c++,objc,java,f77 --host=x86_64-redhat-linux
> Thread model: posix
> gcc version 3.2.3 20030502 (Red Hat Linux 3.2.3-24)
> 
> I'd very much appreciate any pointers!

Hmm. Not the first time we see eigen() in that role... First thing to
check is of course whether it is a case of "Eps too low" or true
miscomputation. For the record, it is not happening for me with SuSE
9.1 on Opteron:

> sm <- cbind(1, 3:1, 1:3)
> A <- sm
> E <- eigen(sm)
> V <- E$vect
> lam <- E$values
> abs(A %*% V - V %*% diag(lam))
             [,1]         [,2]         [,3]
[1,] 0.000000e+00 1.110223e-16 3.354246e-16
[2,] 2.220446e-15 8.326673e-17 2.712063e-16
[3,] 8.881784e-16 0.000000e+00 3.267174e-16
> .Machine$double.eps
[1] 2.220446e-16

pd at linux:~> gcc -v
Reading specs from /usr/lib64/gcc-lib/x86_64-suse-linux/3.3.3/specs
Configured with: ../configure --enable-threads=posix --prefix=/usr
--with-local-prefix=/usr/local --infodir=/usr/share/info
--mandir=/usr/share/man --enable-languages=c,c++,f77,objc,java,ada
--disable-checking --libdir=/usr/lib64 --enable-libgcj
--with-gxx-include-dir=/usr/include/g++ --with-slibdir=/lib64
--with-system-zlib --enable-shared --enable-__cxa_atexit
x86_64-suse-linux
Thread model: posix
gcc version 3.3.3 (SuSE Linux)


Compile settings similar to yours, except that I have -O3.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From umalvarez at fata.unam.mx  Tue Dec 14 20:15:52 2004
From: umalvarez at fata.unam.mx (Ulises M. Alvarez)
Date: Tue, 14 Dec 2004 13:15:52 -0600
Subject: [R] switching to Linux, suggestions?
Message-ID: <1103051752.4020.8.camel@josku.erikson.edu>

A little bit late, but I have to tell...

Take a look at Ubuntu

http://www.ubuntulinux.org

Is both comfortable to install and maintain; plus easy to expand.



From umalvarez at fata.unam.mx  Tue Dec 14 20:23:46 2004
From: umalvarez at fata.unam.mx (Ulises M. Alvarez)
Date: Tue, 14 Dec 2004 13:23:46 -0600
Subject: [R] switching to Linux, suggestions?
In-Reply-To: <3398909b04121309356055abe8@mail.gmail.com>
References: <20041213170554.39637.qmail@web50305.mail.yahoo.com>
	<3398909b04121309356055abe8@mail.gmail.com>
Message-ID: <1103052226.4020.16.camel@josku.erikson.edu>

Again, I have to tell...

Take a look at Ubuntu
http://www.ubuntulinux.org/

When I get tired of MAC OSX, I installed the ppc version of Ubuntu on my
IBook. It's great and it's based on Debian. 


On Mon, 2004-12-13 at 12:35 -0500, doktora v wrote:
> I'm using SUSE with success on intel laptop and AMD desktop. You get
> the best of both worlds: rpm and source. I can easily get the rpm
> packages i need, and compile on my own the things that i can spend
> time on (such as R 2.0.1 -- compiles out of the box on suse).
> 
> BTW, I'm looking to switch to Mac platform. Anyone had any experience
> with that? I'm expecting on a power G4 laptop later this week.... hope
> R behaves...
> 
> -- doktora
> 
> 
> On Mon, 13 Dec 2004 09:05:54 -0800 (PST), bogdan romocea
> <br44114 at yahoo.com> wrote:
> > Before choosing a GNU/Linux distribution look into the package
> > management issue.
> > http://distrowatch.com/
> > I would suggest that you avoid all RPM-based distributions (Mandrake,
> > Fedora, SuSE), and consider Debian (+ those based on it) & the
> > source-based distributions (such as Gentoo). I've been using Mandrake
> > for a couple of years but got tired of RPM.
> > 
> > HTH,
> > b.
> > 
> > 
> > -----Original Message-----
> > From: Thomas W Volscho [mailto:THOMAS.VOLSCHO at huskymail.uconn.edu]
> > Sent: Sunday, December 12, 2004 3:24 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] switching to Linux, suggestions?
> > 
> > Dear List,
> > I have acquired a new desktop and wanted to put a free OS on it.  I
> > am trying Fedora Core 1, but not sure what the best Linux OS is for
> > using R 2.0.1?
> > 
> > Thank you in advance for your input,
> > Tom Volscho
> > 
> > ************************************
> > Thomas W. Volscho
> > Graduate Student
> > Dept. of Sociology U-2068
> > University of Connecticut
> > Storrs, CT 06269
> > Phone: (860) 486-3882
> > http://vm.uconn.edu/~twv00001
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > __________________________________
> > 
> > Dress up your holiday email, Hollywood style. Learn more.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Dec 14 20:35:47 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 14 Dec 2004 14:35:47 -0500
Subject: [R] linear regression: evaluating the result Q
Message-ID: <3A822319EB35174CA3714066D590DCD50994E427@usrymx25.merck.com>

It looks just like the classical F-test for lack-of-fit, using estimate of
`pure errors' from replicates, doesn't it?  This should be in most applied
regression books.  The power (i.e., probability of finding lack-of-fit when
it exists) of such tests will depend on the data.

Andy

> From: RenE J.V. Bertin
> 
> Hello,
> 
> I'd like to come back to this question I obtained some 
> valuable help with a while ago.
> 
> I just came across a paper applying a seemingly rather 
> clever/elegant technique to assess the extent to which a 
> linear fit is appropriate, given once data. These authors 
> apply an ANOVA to the residuals, and take a NS result as an 
> indication that the fitted relationship is indeed 
> (sufficiently) linear.
> 
> But is this a clever/elegant technique, and is it good and robust?
> A rather pathological example where it fails (I think):
> 
> ##> kk<-data.frame( ordered(factor( rep( 1:25, each=11))), 
> ordered(factor(rep( 0:10, 25))), sin( pi*jitter(rep(0:10,25))) )
> ##> names(kk)<-c("s","x","y")
> ##> summary( aov(y~x+Error(s), data=kk) )
> 
> Error: s
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 24  2.592   0.108               
> 
> Error: Within
>            Df Sum Sq Mean Sq F value Pr(>F)
> x          10  1.174   0.117   0.974  0.467
> Residuals 240 28.924   0.121               
> 
> (it doesn't fail when using a cosine instead of a sine, of course).
> 
> And if so, before I reinvent the wheel in implementing it 
> myself: is anyone here aware of an existing implementation of 
> a test that does just that?
> 
> Thanks,
> RenE Bertin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From drcarbon at gmail.com  Tue Dec 14 20:41:23 2004
From: drcarbon at gmail.com (Dr Carbon)
Date: Tue, 14 Dec 2004 14:41:23 -0500
Subject: [R] drawing a rectangle through multiple plots
Message-ID: <e89bb7ac041214114113dd8be7@mail.gmail.com>

How do I draw a rectangle across multiple plots on a device?

E.g.,

def.par <- par(no.readonly = TRUE)
par(mfrow = c(3, 1))
plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
rect(2, -4, 3, 4)
par(def.par)

I want the rectangle to extend across the whole device. How do I get
at those coordinates? Is this a case where I should be using grid or
gridBase?

Thanks.



From ssim at lic.co.nz  Tue Dec 14 20:43:41 2004
From: ssim at lic.co.nz (ssim@lic.co.nz)
Date: Wed, 15 Dec 2004 08:43:41 +1300
Subject: [R] Re : Save result in a For Loop
Message-ID: <OF7C0A1636.782F369A-ONCC256F6A.006C2BC5-CC256F6A.006C5E3F@livestock.org.nz>

Hiya,

I have been struggling to save the result from the FOR loop. What is the
best way to do it, as I need the result to merge with another dataset for
further analysis ?

for (dd in ((M-10):M)){
+ dist<-(32-dd)
+ r<-1/2*(1-exp(-2*dist/100))
+ map<-c(dd,round(r,4))
+ print(map)
+ next
+ }

Thanks. Stella
___________________________________________________________________________
This message, including attachments, is confidential. If you...{{dropped}}



From do_joly at yahoo.ca  Tue Dec 14 20:43:52 2004
From: do_joly at yahoo.ca (Damien Joly)
Date: Tue, 14 Dec 2004 11:43:52 -0800 (PST)
Subject: [R] r-mathlib and ruby-rmathlib
Message-ID: <20041214194352.47628.qmail@web14311.mail.yahoo.com>

Hi all,

I am trying to set up r-mathlib so I can access R
functions from Ruby using ruby-rmathlib, and for the
life of me I can't find an rpm or source code for
r-mathlib (I can find debian packages ...).  I'm using
Red Hat Enterprise Linux WS (V 3) (EM_64 bit
processor) and R version 1.9.1.

Thanks for any help,

Damien



From julianno.sambatti at cirad.fr  Tue Dec 14 22:09:29 2004
From: julianno.sambatti at cirad.fr (Julianno Sambatti)
Date: Tue, 14 Dec 2004 16:09:29 -0500
Subject: [R] Multivariate multipl regression
Message-ID: <003001c4e221$33fb4580$91a9ddc3@selection7035>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041214/7409302c/attachment.pl

From jost at cict.fr  Tue Dec 14 21:21:33 2004
From: jost at cict.fr (Christian Jost)
Date: Tue, 14 Dec 2004 21:21:33 +0100
Subject: [R] looking for Chebyshev I low pass filter
Message-ID: <a06002012bde4fb2e25af@[130.120.104.141]>

I am looking for a simple implementation of a Chebyshev type I 
low-pass filter in R. Searching through the R web site I only found 
similar requests but never an answer pointing to a package providing 
such standard filters (Chebyshev, Butterworth). Any idea where I 
might find this?

Thanks, Christian.



From gunter.berton at gene.com  Tue Dec 14 21:23:19 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 14 Dec 2004 12:23:19 -0800
Subject: [R] Multivariate multipl regression
In-Reply-To: <003001c4e221$33fb4580$91a9ddc3@selection7035>
Message-ID: <200412142023.iBEKNJVO004151@volta.gene.com>

?aov or ?manova

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Julianno Sambatti
> Sent: Tuesday, December 14, 2004 1:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Multivariate multipl regression
> 
> I am trying to find Multivariate Multiple Regression 
> documentation for R. Could somebody please indicate me some, 
> if available at all.
> 
> Thanks
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

From br44114 at yahoo.com  Tue Dec 14 21:44:52 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Tue, 14 Dec 2004 12:44:52 -0800 (PST)
Subject: [R] Re : Save result in a For Loop
Message-ID: <20041214204452.48338.qmail@web50310.mail.yahoo.com>

Not sure if it's the best way, but you could do it this way:
all.results <- vector(mode="numeric")
for (i in 1:100)
	{
	...
	this.run <- ...
	all.results <- c(all.results,this.run)
	}
At this point all.results contains the values of this.run from the
whole loop. If this.run is not a vector/number but a data frame look
at rbind/cbind.

Or, create a vector/matrix first and then populate it from the for
loop:
all.results <- vector()/matrix()/data.frame()
for (i in 1:100)
  for(j ...)
	{
	...
	all.results[i] <- this.run  ,OR
	all.results[i,] <- this.run  , OR
	all.results[i,j] <- this.run
	}

HTH,
b.


-----Original Message-----
From: ssim at lic.co.nz [mailto:ssim at lic.co.nz]
Sent: Tuesday, December 14, 2004 2:44 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Re : Save result in a For Loop


Hiya,

I have been struggling to save the result from the FOR loop. What is
the
best way to do it, as I need the result to merge with another dataset
for
further analysis ?

for (dd in ((M-10):M)){
+ dist<-(32-dd)
+ r<-1/2*(1-exp(-2*dist/100))
+ map<-c(dd,round(r,4))
+ print(map)
+ next
+ }

Thanks. Stella
___________________________________________________________________________
This message, including attachments, is confidential. If\ yo...{{dropped}}



From cberry at tajo.ucsd.edu  Tue Dec 14 20:04:15 2004
From: cberry at tajo.ucsd.edu (Chuck Berry)
Date: Tue, 14 Dec 2004 19:04:15 +0000 (UTC)
Subject: [R] Advice on parsing formulae
References: <200412131105.iBDB4GHF031431@hypatia.math.ethz.ch>
	<000401c4e12e$43ad1ba0$9230e182@peano>
Message-ID: <loom.20041214T194953-451@post.gmane.org>

Claus Dethlefsen <dethlef <at> math.aau.dk> writes: 
 
>  
> Dear list 
>  
> I would like to be able to group terms in a formula using a function that I 
> will call tvar(), eg. the formula 
>  
> Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3)) 
>  
> where x,u and v are numeric and A and B are factors - binary, say. 
>  
> As output, I want the model.matrix as if tvar had not been there at all. In 
> addition, I would like to have information on the grouping, as a vector as 
> long as ncol( model.matrix ) with zeros corresponding to terms outside tvar 
> and with an index grouping the terms inside each tvar(). In the (sick) 
 
 
Since you want to single out terms with 'tvar' in them, something like 
 
tvar.terms <- terms( your.formula.above, specials = "tvar" ) 
 
will likely be what you want. You'll want to study 
 
    ?terms 
    ?terms.object 
 
and then 
 
    print( tvar.terms ) 
 
And likely 'deparse' will help, too. 
 
> example: 
[rest deleted]



From p.murrell at auckland.ac.nz  Tue Dec 14 22:01:04 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 15 Dec 2004 10:01:04 +1300
Subject: [R] drawing a rectangle through multiple plots
References: <e89bb7ac041214114113dd8be7@mail.gmail.com>
Message-ID: <41BF5490.5020004@stat.auckland.ac.nz>

Hi


Dr Carbon wrote:
> How do I draw a rectangle across multiple plots on a device?
> 
> E.g.,
> 
> def.par <- par(no.readonly = TRUE)
> par(mfrow = c(3, 1))
> plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
> plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
> plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
> rect(2, -4, 3, 4)
> par(def.par)
> 
> I want the rectangle to extend across the whole device. How do I get
> at those coordinates? Is this a case where I should be using grid or
> gridBase?


You could use grid and gridBase (and grid.moveto() and grid.line.to)), 
but if your real example is as straightforward as this toy one, then you 
can "fake it" by drawing overlapping lines deliberately beyond the 
extent of each plot to create what looks like a rectangle across the 
three plots --- par(xpd=NA) means that the lines are not clipped to each 
plot region ...

par(mfrow = c(3, 1))
par(xpd=NA)
plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
# "top" of rectangle
lines(c(2, 2, 3, 3), c(-10, 4, 4, -10))
plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
# "sides" of rectangle
lines(c(2, 2), c(-10, 10))
lines(c(3, 3), c(-10, 10))
plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
# "bottom" of rectangle
lines(c(2, 2, 3, 3), c(10, -4, -4, 10))

HTH

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From andy_liaw at merck.com  Tue Dec 14 22:45:23 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 14 Dec 2004 16:45:23 -0500
Subject: [R] make check-all failed on SLES8 for x86_64
Message-ID: <3A822319EB35174CA3714066D590DCD50994E429@usrymx25.merck.com>

Thanks for reminding me of that thread, Roger.  I've forgotten all about
it...

Strangely, several people reported no problem with optimized BLAS, but I
tried linking against the latest version (0.96, threaded), and got the same
failure.  I will try to see if I can upgrade GCC.

Best,
Andy

> From: Roger D. Peng 
> 
> Martin Maechler brought this up a while back (and added the 
> regression 
> test).  I believe it was a compiler problem and upgrading to 
> gcc 3.4.1 
> fixed it.
> 
> See the thread starting here:
> 
> https://stat.ethz.ch/pipermail/r-devel/2004-July/030260.html
> 
> -roger
> 
> Liaw, Andy wrote:
> > Hi all,
> > 
> > I've only now noticed that on our Opteron boxes running 
> Suse Enterprise
> > Server 8 that R-2.0.1 did not pass make chek-all.  It bombed in
> > tests/lapack.R at:
> > 
> > 
> >>eigenok <- function(A, E, Eps = 1000 * .Machine$double.eps) {
> > 
> >     V <- E$vect
> >     lam <- E$values
> >     stopifnot(abs(A %*% V - V %*% diag(lam)) < Eps,  .... 
> [TRUNCATED] 
> > 
> > 
> >>Ceigenok <- function(A, E, Eps = 1000 * .Machine$double.eps) {
> > 
> >     V <- E$vect
> >     lam <- E$values
> >     stopifnot(Mod(A %*% V - V %*% diag(lam)) < Eps, .... 
> [TRUNCATED] 
> > 
> > 
> >>sm <- cbind(1, 3:1, 1:3)
> > 
> > 
> >>eigenok(sm, eigen(sm))
> > 
> > Error: abs(A %*% V - V %*% diag(lam)) < Eps is not TRUE
> > 
> > This is using:
> > ~/R-2.0.1> gcc -v
> > Reading specs from 
> /opt/gcc33/lib64/gcc-lib/x86_64-suse-linux/3.3/specs
> > Configured with: ../configure --enable-threads=posix 
> --prefix=/opt/gcc33
> > --with-local-prefix=/usr/local --infodir=/opt/gcc33/share/info
> > --mandir=/opt/gcc33/share/man --libdir=/opt/gcc33/lib64
> > --enable-languages=c,c++,f77,objc,java,ada --disable-checking
> > --enable-libgcj --with-gxx-include-dir=/opt/gcc33/include/g++
> > --with-slibdir=/lib64 --with-system-zlib --enable-shared
> > --enable-__cxa_atexit x86_64-suse-linux
> > Thread model: posix
> > gcc version 3.3 20030312 (prerelease) (SuSE Linux)
> > 
> > compiled with the following setting:
> > 
> > R is now configured for x86_64-unknown-linux-gnu
> > 
> >   Source directory:          .
> >   Installation directory:    /usr/local
> > 
> >   C compiler:                gcc  -g -O2
> >   C++ compiler:              g++  -g -O2
> >   Fortran compiler:          g77  -g -O2
> > 
> >   Interfaces supported:      X11, tcltk
> >   External libraries:        readline
> >   Additional capabilities:   PNG, JPEG
> >   Options enabled:           R profiling
> > 
> >   Recommended packages:      yes
> > 
> > Actually, I've only noticed this when the samething happens 
> on a RH-based
> > box (same hardware), using:
> > $ gcc -v
> > Reading specs from /usr/lib/gcc-lib/x86_64-redhat-linux/3.2.3/specs
> > Configured with: ../configure --prefix=/usr --mandir=/usr/share/man
> > --infodir=/usr/share/info --enable-shared --enable-threads=posix
> > --disable-checking --with-system-zlib --enable-__cxa_atexit
> > --enable-languages=c,c++,objc,java,f77 --host=x86_64-redhat-linux
> > Thread model: posix
> > gcc version 3.2.3 20030502 (Red Hat Linux 3.2.3-24)
> > 
> > I'd very much appreciate any pointers!
> > 
> > Best,
> > Andy
> > 
> > 
> > Andy Liaw, PhD
> > Biometrics Research      PO Box 2000, RY33-300     
> > Merck Research Labs           Rahway, NJ 07065
> > andy_liaw <at> merck.com          732-594-0820
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> Roger D. 
> Peng
> http://www.biostat.jhsph.edu/~rpeng/
> 
>



From john.maindonald at anu.edu.au  Wed Dec 15 00:59:07 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Wed, 15 Dec 2004 10:59:07 +1100
Subject: [R] classification for huge datasets: SVM yields memory troubles
In-Reply-To: <200412141109.iBEB8uB8020027@hypatia.math.ethz.ch>
References: <200412141109.iBEB8uB8020027@hypatia.math.ethz.ch>
Message-ID: <23DE54A8-4E2C-11D9-B6E2-000A95CDA0F2@anu.edu.au>

While it is true that the large number of variables relative to
the number of observations restricts what can be inferred,
the situation is not as hopeless as Bert seems to suggest.
If it were, attempts at the analysis of expression array data
would be a waste to time.  Methods developed to that
general area may well be relevant to other data where the
number of variables is similarly far larger than the number
of observations.

See Ambroise, C. and Mclachlan, G.J. 2002.  Selection bias
in gene extraction on the basis of microarray gene-expression
data.  PNAS 99: 6562--6566.

This discusses some of the literature on the use of SVMs.

The selection bias that these authors discuss also affects
plots, even principal components and other ordination-base
plots where features have been selected on the basis of their
ability to separate into known groups.  I have draft versions
of code that addresses this selection bias as it affects the
plotting of graphs, which (along a paper that has been
submitted for inclusion in a conference proceedings) I am
happy to make available to anyone who wants to experiment.

Another good place to look, as a starting point, may be
Gordon Smyth's LIMMA User's Guide.  This can be a bit
hard to find. With limma installed, type help.start().
After some time a browser window should open. Click on
Packages | limma | Overview | LIMMA User's Guide (pdf)

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

On 14 Dec 2004, at 10:09 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Berton Gunter <gunter.berton at gene.com>
> Date: 14 December 2004 9:23:08 AM
> To: "'Andreas'" <wolf.privat at gmx.de>, <r-help at stat.math.ethz.ch>
> Cc: Subject: RE: [R] classification for huge datasets: SVM yields 
> memory troubles
>
>
> " I have a matrix with 30 observations and roughly 30000
> variables, ... <snipped>"
>
> Comment: This is ** not ** a "huge" data set -- it is a tiny one with a
> large number of covariates. The difference is: If it were truly huge, 
> SVM
> and/or LDA or ... might actually be able to produce useful results. 
> With so
> few data and so many variables, it is hard to see how any approach 
> that one
> uses is not simply a fancy random number generator.
>
John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From arrayprofile at yahoo.com  Wed Dec 15 01:14:30 2004
From: arrayprofile at yahoo.com (array chip)
Date: Tue, 14 Dec 2004 16:14:30 -0800 (PST)
Subject: [R] using Hmisc and Design library
Message-ID: <20041215001430.88575.qmail@web40806.mail.yahoo.com>

Hi, I encountered a weird problem when using the
Design and Hmisc problem. I have 2 data frame called
"a" and "b", both have 3 columns: "time", "status" and
"scores", a sample of the data frame is like:

data frame "a":
  time status scores 
1   21      1  99.61
2   38      0 101.11
3   51      0 100.62
4   48      0  87.52
5   78      0  97.18

data frame "b":
  time status scores 
1   27      0 -126.7
2   24      0 -135.6
3   30      0 -139.5
4   49      0 -137.6
5   27      0 -136.9

when I ran the following using data frame "a",
everything is ok.

> library(Hmisc,T);library(Design,T)
> dd <- datadist(a)
> options(datadist='dd')
> fit <- cph(Surv(time,status) ~ scores,
data=a,surv=T, x=T, y=T)
> fit
Cox Proportional Hazards Model

cph(formula = Surv(time, status) ~ scores, data =
data.combined.testing.set.scores, x = T, y = T,
	surv = T)

 Obs Events Model L.R. d.f.      P Score Score P    R2

  85     31       7.47    1 0.0063  7.21  0.0072 0.088


         coef se(coef)    z       p 
scores 0.0766   0.0287 2.67 0.00752


but when I ran the same script using data frame "b",
even though no error message was produced, no fit
object was generated:

> rm(fit)
> library(Hmisc,T);library(Design,T)
> dd <- datadist(a)
> options(datadist='dd')
> fit <- cph(Surv(time,status) ~ scores,
data=a,surv=T, x=T, y=T)
> fit
Problem: Object "fit" not found, while calling
subroutine S_agsurv2 
Use traceback() to see the call stack

can anyone tell me why? no missing value is present in
either data frame.

Thanks



From arrayprofile at yahoo.com  Wed Dec 15 01:27:33 2004
From: arrayprofile at yahoo.com (array chip)
Date: Tue, 14 Dec 2004 16:27:33 -0800 (PST)
Subject: [R] Re: [S] using Hmisc and Design library
In-Reply-To: <20041215001430.88575.qmail@web40806.mail.yahoo.com>
Message-ID: <20041215002733.93966.qmail@web40804.mail.yahoo.com>

actually data frame "b" has 177 rows, the script ran
ok on the first 166 rows as a subset, but started to
break down if subset of the first 177 rows were used
as the dataset, or the first 166 rows plus 168th row,
....

the data in those rows are:
    time status scores 
165  172      0 -123.3
166  105      0 -138.4
167  166      0 -128.8
168  140      0 -114.2
169  163      0 -117.0
170  141      0 -115.8

please advise!


--- array chip <arrayprofile at yahoo.com> wrote:

> Hi, I encountered a weird problem when using the
> Design and Hmisc problem. I have 2 data frame called
> "a" and "b", both have 3 columns: "time", "status"
> and
> "scores", a sample of the data frame is like:
> 
> data frame "a":
>   time status scores 
> 1   21      1  99.61
> 2   38      0 101.11
> 3   51      0 100.62
> 4   48      0  87.52
> 5   78      0  97.18
> 
> data frame "b":
>   time status scores 
> 1   27      0 -126.7
> 2   24      0 -135.6
> 3   30      0 -139.5
> 4   49      0 -137.6
> 5   27      0 -136.9
> 
> when I ran the following using data frame "a",
> everything is ok.
> 
> > library(Hmisc,T);library(Design,T)
> > dd <- datadist(a)
> > options(datadist='dd')
> > fit <- cph(Surv(time,status) ~ scores,
> data=a,surv=T, x=T, y=T)
> > fit
> Cox Proportional Hazards Model
> 
> cph(formula = Surv(time, status) ~ scores, data =
> data.combined.testing.set.scores, x = T, y = T,
> 	surv = T)
> 
>  Obs Events Model L.R. d.f.      P Score Score P   
> R2
> 
>   85     31       7.47    1 0.0063  7.21  0.0072
> 0.088
> 
> 
>          coef se(coef)    z       p 
> scores 0.0766   0.0287 2.67 0.00752
> 
> 
> but when I ran the same script using data frame "b",
> even though no error message was produced, no fit
> object was generated:
> 
> > rm(fit)
> > library(Hmisc,T);library(Design,T)
> > dd <- datadist(a)
> > options(datadist='dd')
> > fit <- cph(Surv(time,status) ~ scores,
> data=a,surv=T, x=T, y=T)
> > fit
> Problem: Object "fit" not found, while calling
> subroutine S_agsurv2 
> Use traceback() to see the call stack
> 
> can anyone tell me why? no missing value is present
> in
> either data frame.
> 
> Thanks
> 
> 
> __________________________________ 
>
--------------------------------------------------------------------
> This message was distributed by
> s-news at lists.biostat.wustl.edu.  To
> unsubscribe send e-mail to
> s-news-request at lists.biostat.wustl.edu with

> 



		
__________________________________ 

Dress up your holiday email, Hollywood style. Learn more.



From andy_liaw at merck.com  Wed Dec 15 01:41:00 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 14 Dec 2004 19:41:00 -0500
Subject: [R] Re : Save result in a For Loop
Message-ID: <3A822319EB35174CA3714066D590DCD50994E42B@usrymx25.merck.com>

Is there anything wrong with vectorized calculation?  E.g.,

dd <- (M-10):M
r <- 0.5 * (1 - exp(-2 * (32 - dd) / 100))
round(r, 4)

Andy

> From: bogdan romocea
> 
> Not sure if it's the best way, but you could do it this way:
> all.results <- vector(mode="numeric")
> for (i in 1:100)
> 	{
> 	...
> 	this.run <- ...
> 	all.results <- c(all.results,this.run)
> 	}
> At this point all.results contains the values of this.run from the
> whole loop. If this.run is not a vector/number but a data frame look
> at rbind/cbind.
> 
> Or, create a vector/matrix first and then populate it from the for
> loop:
> all.results <- vector()/matrix()/data.frame()
> for (i in 1:100)
>   for(j ...)
> 	{
> 	...
> 	all.results[i] <- this.run  ,OR
> 	all.results[i,] <- this.run  , OR
> 	all.results[i,j] <- this.run
> 	}
> 
> HTH,
> b.
> 
> 
> -----Original Message-----
> From: ssim at lic.co.nz [mailto:ssim at lic.co.nz]
> Sent: Tuesday, December 14, 2004 2:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Re : Save result in a For Loop
> 
> 
> Hiya,
> 
> I have been struggling to save the result from the FOR loop. What is
> the
> best way to do it, as I need the result to merge with another dataset
> for
> further analysis ?
> 
> for (dd in ((M-10):M)){
> + dist<-(32-dd)
> + r<-1/2*(1-exp(-2*dist/100))
> + map<-c(dd,round(r,4))
> + print(map)
> + next
> + }
> 
> Thanks. Stella
> ______________________________________________________________
> _____________
> This message, including attachments, is confidential. If\\...{{dropped}}



From arrayprofile at yahoo.com  Wed Dec 15 02:02:25 2004
From: arrayprofile at yahoo.com (array chip)
Date: Tue, 14 Dec 2004 17:02:25 -0800 (PST)
Subject: [R] Re: [S] using Hmisc and Design library
In-Reply-To: <OJEJLDFGCAPPJALLCOABMEPDFFAA.Robert.Balshaw@syreon.com>
Message-ID: <20041215010225.14682.qmail@web40828.mail.yahoo.com>

sorry, I had a typo there, it's datadist(b) for the
analysis of data frame "b".

--- Robert Balshaw <Robert.Balshaw at syreon.com> wrote:

> Not sure if this will help, but did you mean to use
> datadist(a) for
> the analysis of B?
> 
> Rob
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf
> Of array chip
> > Sent: Tuesday, December 14, 2004 4:28 PM
> > To: s-news at lists.biostat.wustl.edu;
> r-help at stat.math.ethz.ch
> > Subject: [R] Re: [S] using Hmisc and Design
> library
> >
> >
> > actually data frame "b" has 177 rows, the script
> ran
> > ok on the first 166 rows as a subset, but started
> to
> > break down if subset of the first 177 rows were
> used
> > as the dataset, or the first 166 rows plus 168th
> row,
> > ....
> >
> > the data in those rows are:
> >     time status scores
> > 165  172      0 -123.3
> > 166  105      0 -138.4
> > 167  166      0 -128.8
> > 168  140      0 -114.2
> > 169  163      0 -117.0
> > 170  141      0 -115.8
> >
> > please advise!
> >
> >
> > --- array chip <arrayprofile at yahoo.com> wrote:
> >
> > > Hi, I encountered a weird problem when using the
> > > Design and Hmisc problem. I have 2 data frame
> called
> > > "a" and "b", both have 3 columns: "time",
> "status"
> > > and
> > > "scores", a sample of the data frame is like:
> > >
> > > data frame "a":
> > >   time status scores
> > > 1   21      1  99.61
> > > 2   38      0 101.11
> > > 3   51      0 100.62
> > > 4   48      0  87.52
> > > 5   78      0  97.18
> > >
> > > data frame "b":
> > >   time status scores
> > > 1   27      0 -126.7
> > > 2   24      0 -135.6
> > > 3   30      0 -139.5
> > > 4   49      0 -137.6
> > > 5   27      0 -136.9
> > >
> > > when I ran the following using data frame "a",
> > > everything is ok.
> > >
> > > > library(Hmisc,T);library(Design,T)
> > > > dd <- datadist(a)
> > > > options(datadist='dd')
> > > > fit <- cph(Surv(time,status) ~ scores,
> > > data=a,surv=T, x=T, y=T)
> > > > fit
> > > Cox Proportional Hazards Model
> > >
> > > cph(formula = Surv(time, status) ~ scores, data
> =
> > > data.combined.testing.set.scores, x = T, y = T,
> > > 	surv = T)
> > >
> > >  Obs Events Model L.R. d.f.      P Score Score P
> > > R2
> > >
> > >   85     31       7.47    1 0.0063  7.21  0.0072
> > > 0.088
> > >
> > >
> > >          coef se(coef)    z       p
> > > scores 0.0766   0.0287 2.67 0.00752
> > >
> > >
> > > but when I ran the same script using data frame
> "b",
> > > even though no error message was produced, no
> fit
> > > object was generated:
> > >
> > > > rm(fit)
> > > > library(Hmisc,T);library(Design,T)
> > > > dd <- datadist(a)
> > > > options(datadist='dd')
> > > > fit <- cph(Surv(time,status) ~ scores,
> > > data=a,surv=T, x=T, y=T)
> > > > fit
> > > Problem: Object "fit" not found, while calling
> > > subroutine S_agsurv2
> > > Use traceback() to see the call stack
> > >
> > > can anyone tell me why? no missing value is
> present
> > > in
> > > either data frame.
> > >
> > > Thanks
> > >
> > >
> > >
> > >
> > >
> > > __________________________________
> > >
> >
>
----------------------------------------------------------
> > ----------
> > > This message was distributed by
> > > s-news at lists.biostat.wustl.edu.  To
> > > unsubscribe send e-mail to
> > > s-news-request at lists.biostat.wustl.edu with
> >
> > >
> >
> >
> >
> >
> > __________________________________
> >
> > Dress up your holiday email, Hollywood style.
> Learn more.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 



		
__________________________________ 

Jazz up your holiday email with celebrity designs. Learn more.



From corey.bradshaw at cdu.edu.au  Wed Dec 15 02:14:14 2004
From: corey.bradshaw at cdu.edu.au (Corey Bradshaw)
Date: Wed, 15 Dec 2004 10:44:14 +0930
Subject: [R] adding perspectives to existing persp plots
Message-ID: <65EB8C57DA514942B1EC55E8DA89FE550151E988@mail.site.ntu.edu.au>

I've created a perspective plot using 'persp' in the graphics package.
I'd like to add a second plane of z values to the existing plot, but I
cannot seem to do this using 'persp'. Is there an analogue to 'lines' or
'points' for perspectives?

Corey.
corey.bradshaw at cdu.edu.au



From kerryrekky at yahoo.com  Wed Dec 15 05:22:53 2004
From: kerryrekky at yahoo.com (Kerry Bush)
Date: Tue, 14 Dec 2004 20:22:53 -0800 (PST)
Subject: [R] how to fit a weighted logistic regression?
Message-ID: <20041215042253.80165.qmail@web51802.mail.yahoo.com>

I tried lrm in library(Design) but there is always
some error message. Is this function really doing the
weighted logistic regression as maximizing the
following likelihood:

\sum w_i*(y_i*\beta*x_i-log(1+exp(\beta*x_i)))

Does anybody know a better way to fit this kind of
model in R?

FYI: one example of getting error message is like:
> x=runif(10,0,3)
> y=c(rep(0,5),rep(1,5))
> w=rep(1/10,10)
> fit=lrm(y~x,weights=w)
Warning message: 
currently weights are ignored in model validation and
bootstrapping lrm fits in: lrm(y ~ x, weights = w) 

although the model can be fit, the above output
warning makes me uncomfortable. Can anybody explain
about it a little bit?

Best wishes,
Feixia



From xiwu at uoguelph.ca  Wed Dec 15 06:07:04 2004
From: xiwu at uoguelph.ca (ximing wu)
Date: Wed, 15 Dec 2004 00:07:04 -0500
Subject: [R] How to generate random number from grouped data?
Message-ID: <5.1.0.14.2.20041214235635.022ce6e0@staff.mail.uoguelph.ca>

Hi there, I need help on the following problem. Any suggestions will be 
much appreciated.

Let x be an i.i.d. random sample of a continuous variable. For some fixed 
numbers a1<a2<a3<...<ak, we don't observe x, but some grouped information:
1) the frequency of x falls in each interval (a1, a2], (a2, a3], (a3, a4],...
2) the conditional mean of x within each interval, mu1, mu2, mu3,...

Is there a way to generate a random sample based on the grouped information 
on frequency and mean of each interval?

Thanks for the help.

Ximing



From francoisromain at free.fr  Wed Dec 15 08:00:10 2004
From: francoisromain at free.fr (francoisromain@free.fr)
Date: Wed, 15 Dec 2004 08:00:10 +0100
Subject: [R] (sans objet)
Message-ID: <1103094010.41bfe0fa4be94@imp2-q.free.fr>


Hello,

Just look at the examples in ?persp.
There is a function called trans3d defined in it that will traduce your 3D
coordinates to 2D, and so you will be able to draw lines with lines function.

Romain.

Corey Bradshaw a ??crit :

>I've created a perspective plot using 'persp' in the graphics package.
>I'd like to add a second plane of z values to the existing plot, but I
>cannot seem to do this using 'persp'. Is there an analogue to 'lines' or
>'points' for perspectives?
>
>Corey.
>corey.bradshaw at cdu.edu.au
>
--
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann??e
Institut de Statistique de l'Universit?? de Paris (ISUP)
Fili??re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From corey.bradshaw at cdu.edu.au  Wed Dec 15 08:10:06 2004
From: corey.bradshaw at cdu.edu.au (Corey Bradshaw)
Date: Wed, 15 Dec 2004 16:40:06 +0930
Subject: [R] RE: adding perspectives to existing persp plots
Message-ID: <65EB8C57DA514942B1EC55E8DA89FE550151EA40@mail.site.ntu.edu.au>

Thanks, Romain. I've certainly used that to draw lines and points in the plots produced by 'persp'; however, my problem is that I need to incorporate an entirely new z function (not just a plane) onto the same plot (i.e., using the same x and y values).

Corey

-----Original Message-----
From: francoisromain at free.fr [mailto:francoisromain at free.fr] 
Sent: Wednesday, December 15, 2004 4:30 PM
To: Corey Bradshaw
Cc: R-help at stat.math.ethz.ch
Subject: 


Hello,

Just look at the examples in ?persp.
There is a function called trans3d defined in it that will traduce your 3D
coordinates to 2D, and so you will be able to draw lines with lines function.

Romain.

Corey Bradshaw a ??crit :

>I've created a perspective plot using 'persp' in the graphics package.
>I'd like to add a second plane of z values to the existing plot, but I
>cannot seem to do this using 'persp'. Is there an analogue to 'lines' or
>'points' for perspectives?
>
>Corey.
>corey.bradshaw at cdu.edu.au
>
--
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann??e
Institut de Statistique de l'Universit?? de Paris (ISUP)
Fili??re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From Michael.Wolf at bezreg-muenster.nrw.de  Wed Dec 15 08:42:09 2004
From: Michael.Wolf at bezreg-muenster.nrw.de (Wolf, Michael)
Date: Wed, 15 Dec 2004 08:42:09 +0100
Subject: [R] Problem with German special characters
Message-ID: <9E00F1C36CEF614CA4AA6CC2587B594D4C211B@EXCHANGE2.harz.bezreg-muenster.nrw.de>

Dear list!

When using the German special characters I didn't see all characters in the correct way. Let's take the command

ff <- "??????????????"   # (for those ones who can't see this in the correct way \"a, \"??, \"??
                  # \ss (?), \"A, \"O and \"U in LaTeX commands

'ff' will show the output text "\344\366????\304\326\334" on Rconsole. So only "??" ("\u) and "??" (\ss) are shown in the correct way. The other characters are coded in the "\" form.

I found out that this problem doesn't occur when exporting 'ff' to a text file with 'writeLines'. When opening this file with a text editor you can see all characters in the correct way.

What can I do to get the correct output of the special characters?

Thanks for your help in advance!

Mit freundlichen Gr????en

Dr. Michael Wolf
Bezirksregierung M??nster
Dezernat 61
Domplatz 1-3    48161 M??nster
Tel.:   ++ 49 (02 51) / 4 11 - 17 95
Fax.:   ++ 49 (02 51) / 4 11 - 8 17 95
E-Mail: michael.wolf at bezreg-muenster.nrw.de



From Michael.Wolf at bezreg-muenster.nrw.de  Wed Dec 15 09:13:10 2004
From: Michael.Wolf at bezreg-muenster.nrw.de (Wolf, Michael)
Date: Wed, 15 Dec 2004 09:13:10 +0100
Subject: AW: [R] Problem with German special characters
Message-ID: <9E00F1C36CEF614CA4AA6CC2587B594D4C211F@EXCHANGE2.harz.bezreg-muenster.nrw.de>

Dear Prof. Ripley,

thanks for your help. Everything is working fine!

With best regards

Michael Wolf 

-----Urspr??ngliche Nachricht-----
Von: Brian D Ripley [mailto:ripley at stats.ox.ac.uk] 
Gesendet: Mittwoch, 15. Dezember 2004 09:01
An: Wolf, Michael
Betreff: Re: [R] Problem with German special characters

Please do look in the list archives: this is a Windows bug worked around a while back. You need to get the R-patched version of R.


On Wed, 15 Dec 2004, Wolf, Michael wrote:

> Dear list!
>
> When using the German special characters I didn't see all characters 
> in the correct way. Let's take the command
>
> ff <- "??????????????"   # (for those ones who can't see this in the correct way \"a, \"??, \"??
>                   # \ss (?), \"A, \"O and \"U in LaTeX commands
>
> 'ff' will show the output text "\344\366????\304\326\334" on Rconsole. So only "??" ("\u) and "??" (\ss) are shown in the correct way. The other characters are coded in the "\" form.
>
> I found out that this problem doesn't occur when exporting 'ff' to a text file with 'writeLines'. When opening this file with a text editor you can see all characters in the correct way.
>
> What can I do to get the correct output of the special characters?
>
> Thanks for your help in advance!
>
> Mit freundlichen Gr????en
>
> Dr. Michael Wolf
> Bezirksregierung M??nster
> Dezernat 61
> Domplatz 1-3    48161 M??nster
> Tel.:   ++ 49 (02 51) / 4 11 - 17 95
> Fax.:   ++ 49 (02 51) / 4 11 - 8 17 95
> E-Mail: michael.wolf at bezreg-muenster.nrw.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From 0034058 at fudan.edu.cn  Tue Dec 14 16:02:45 2004
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 14 Dec 2004 23:02:45 +0800
Subject: [R] can R do the goodman modified multiple regression method?
Message-ID: <0I8P00KV8W4FVT@mail.fudan.edu.cn>

the method is described in the article:goodman leo A.,a modified multiple regression approch to analysis of dischotomous variables",american sociological review 33(hebruary):28-46

thank you in advance:)



From petr.pikal at precheza.cz  Wed Dec 15 09:29:56 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 15 Dec 2004 09:29:56 +0100
Subject: [R] adding perspectives to existing persp plots
In-Reply-To: <65EB8C57DA514942B1EC55E8DA89FE550151E988@mail.site.ntu.edu.au>
Message-ID: <41C00414.19385.312675@localhost>

Hi Corey

Did you try to use par(new=TRUE) before ploting the second 
persp graph?

Cheers
Petr

On 15 Dec 2004 at 10:44, Corey Bradshaw wrote:

> I've created a perspective plot using 'persp' in the graphics package.
> I'd like to add a second plane of z values to the existing plot, but I
> cannot seem to do this using 'persp'. Is there an analogue to 'lines'
> or 'points' for perspectives?
> 
> Corey.
> corey.bradshaw at cdu.edu.au
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From buhard at cephb.fr  Wed Dec 15 09:40:48 2004
From: buhard at cephb.fr (BUHARD O)
Date: Wed, 15 Dec 2004 09:40:48 +0100
Subject: [R] question about surf.gls(spatial package) with NA/NaN/inf values
References: <1101655292.2321.0.camel@localhost.localdomain>
	<200411281401.13351.deepayan@stat.wisc.edu>
Message-ID: <001801c4e281$c97568b0$4f05030a@IBMINSERM>

Hi all,

I need to fit a trend surface to a 3D-dataset with the surf.gls() function
but my data contain missing values. There is just 2 NA in a total of 360
data points but I can't suppresse the variable associated with these NA
values to the analysis. The problem is surf.gls can't manage with Na/NaN/inf
values, so I wanted to know if a similar function in R can do it. I precise
that there's no replicate in the (preliminary test) data so I can't estimate
these values
them easily.

Thanks a lot for help

Regards

BUHARD Olivier



From jmoreira at fe.up.pt  Wed Dec 15 11:35:44 2004
From: jmoreira at fe.up.pt (jmoreira@fe.up.pt)
Date: Wed, 15 Dec 2004 10:35:44 +0000
Subject: [R] Best datatype for time-series with irregular ocurrencies 
Message-ID: <1103106944.41c0138098b9c@webmail.fe.up.pt>

Hello,

Does someone has experience working with time-series where the ocurrencies do 
not happen at regular time spaces? In one day I can have 20 ocurrencies and in 
the following day I can have the double. ts datatype must have regular time 
spaces. What is the best way to put the data if I want to use forecasting 
methods as Holt-Winters, NN or SVM? To put data and time in different 
variables? If in one variable it would be of Date datatype?

Thank you for any help.

Joao


-------------------------------------------------
Joao Mendes Moreira
Faculdade de Engenharia da Universidade do Porto
DEMEGI / GEIN



From Heather.Turner at warwick.ac.uk  Wed Dec 15 11:41:02 2004
From: Heather.Turner at warwick.ac.uk (Heather Turner)
Date: Wed, 15 Dec 2004 10:41:02 +0000
Subject: [R] Advice on parsing formulae
Message-ID: <s1c014c6.095@liberator.csv.warwick.ac.uk>

I think this will do what you want:

# Need this function to remove spaces from term labels later on
> removeSpace <-  function(string) gsub("[[:space:]]", "", string)

# Specify which terms are in a "tvar" group
# (could remove spaces separately)
> tvar <- unname(sapply(c("x:A", "z", "B", "poly(v,3)"), removeSpace))

# Use terms to get term labels from formula
> formula <- Y ~ 1 + x:A + z + u + B + poly(v,3)
> term.labels <- unname(sapply(attr(terms(formula), "term.labels"), removeSpace))
> tvar
[1] "x:A"       "z"         "B"         "poly(v,3)"
> term.labels
[1] "z"         "u"         "B"         "poly(v,3)" "x:A"

# Get assign variable for parameters
# (You would use first two lines, but I don't have data so defined assign variable myself)
> #X <- model.matrix(formula)
> #pAssign <- attr(X, "assign")
> pAssign <- c(0,1,2,3,4,4,4,5,5)  

# Define "tvarAssign"
> tvarAssign <- match(pAssign, sort(match(tvar, term.labels)))
> tvarAssign[is.na(tvarAssign)] <- 0
> tvarAssign
[1] 0 1 0 2 3 3 3 4 4

HTH

Heather

Mrs H Turner
Research Assistant
Dept. of Statistics
University of Warwick

>>> "Claus Dethlefsen" <dethlef at math.aau.dk> 12/13/04 04:10pm >>>
Dear list

I would like to be able to group terms in a formula using a function that I
will call tvar(), eg. the formula

Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))

where x,u and v are numeric and A and B are factors - binary, say.

As output, I want the model.matrix as if tvar had not been there at all. In
addition, I would like to have information on the grouping, as a vector as
long as ncol( model.matrix ) with zeros corresponding to terms outside tvar
and with an index grouping the terms inside each tvar(). In the (sick)
example:


> model.matrix(Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3)))
   (Intercept)     z     u B2 poly(v, 3)1 poly(v, 3)2 poly(v, 3)3  x:A1
x:A2
1            1 -1.55 -1.03  0       0.160      -0.350      -0.281  0.66
0.00
2            1 -1.08  0.55  0      -0.164      -0.211       0.340  0.91
0.00
3            1  0.29 -0.26  0      -0.236      -0.073       0.311 -1.93
0.00
4            1 -1.11  0.96  0       0.222      -0.285      -0.385 -0.23
0.00
5            1  0.43 -0.76  1      -0.434       0.515      -0.532  0.22
0.00

I would like the vector

c(0,1,0,2,3,3,3,4,4)

pointing to the tvar-grouped terms.

Thus what I would like, looks a bit like the 'assign' attribute of the
model.matrix() output. I have not figured out a way of doing this in a nice
way and would like some help, please.

I hope somebody can help me (or point the manual-pages I should read),

Best, 

Claus Dethlefsen
----------------------------------------------------------------------------
---
Assistant Professor, Claus Dethlefsen, Ph.D.
mailto:dethlef at math.auc.dk, http://www.math.auc.dk/~dethlef 
Dpt. of Mathematical Sciences, Aalborg University
Fr. Bajers Vej 7G, 9220 Aalborg East
Denmark

-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From cg.pettersson at evp.slu.se  Wed Dec 15 11:58:34 2004
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Wed, 15 Dec 2004 11:58:34 +0100
Subject: [R] MIME decoding in Mozilla Thunderbird
Message-ID: <41C018DA.1090306@evp.slu.se>

Hello all!
I recently switched mail program to Mozilla Thunderbird, running on W2k.
Everything works fine, except the MIME-digests from this list. The 
decoding doesn??t work properly.

I had contact with Martin Maechler some time ago, and he suggested a try 
on this list for ideas on how to do the decoding in Windows, even if 
it??s not a proper R-question.

Outlook do the proper job on the MIME, but using that is to go a bit 
far! Or?

/CG

-- 
CG Pettersson MSci. PhD.Stud.
Swedish University of Agricultural Sciences (SLU)
Dep. of Ecology and Crop production sciences (EVP).
http://www.slu.se/
cg.pettersson at evp.slu.se



From dmb at mrc-dunn.cam.ac.uk  Wed Dec 15 12:37:00 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 15 Dec 2004 11:37:00 +0000 (GMT)
Subject: [R] Massive clustering job?
Message-ID: <Pine.LNX.4.21.0412151112310.23621-100000@mail.mrc-dunn.cam.ac.uk>


Hi, 

I have ~40,000 rows in a database, each of which contains an id column and
20 additional columns of count data.

I want to cluster the rows based on these count vectors.

Their are ~1.6 billion possible 'distances' between pairs of vectors
(cells in my distance matrix), so I need to do something smart.

Can R somehow handle this?

My first thought was to index the database with something that makes
nearest neighbour lookup more efficient, and then use single linkage
clustering. Is this kind of index implemented in R (by default when using
single linkage)?

Also 'grouping' identical vectors is very easy. I tried making groups more
fuzzy by using a hashing function over the count vectors, but my hash was
too crude. Any way to do fuzzy grouping in R which scales well?

For example, removing identical vectors gives me ~30,000 rows (and ~900
million pairs of distances). As an example of how fast I can group, the
above query took 0.13 seconds in mysql (using an index over every element
in the vector). However, if I tried to calculate a distance between every
pair of non identical vectors (lets say I can calculate ~1000 eutlidian
distances per second) it would take me ~10 days just to calculate the
distance matrix.

Sorry for all the information. Any suggestions on how to cluster such a
huge dataset (using R) would be appreciated.

Cheers,
Dan.



From ggrothendieck at myway.com  Wed Dec 15 13:02:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 15 Dec 2004 12:02:58 +0000 (UTC)
Subject: [R] Best datatype for time-series with irregular ocurrencies
References: <1103106944.41c0138098b9c@webmail.fe.up.pt>
Message-ID: <loom.20041215T125125-697@post.gmane.org>

 <jmoreira <at> fe.up.pt> writes:

: Does someone has experience working with time-series where the ocurrencies 
do 
: not happen at regular time spaces? In one day I can have 20 ocurrencies and 
in 
: the following day I can have the double. ts datatype must have regular time 
: spaces. What is the best way to put the data if I want to use forecasting 
: methods as Holt-Winters, NN or SVM? To put data and time in different 
: variables? If in one variable it would be of Date datatype?

There are several packages that can accommodate irregularly spaced time series.
Regarding your specific requirements, zoo is the only one that supports 
the Date class as the time variable.  Also, the upcoming version of zoo 
(not yet on CRAN) has support for e1071::svm and nnet::nnet .

There is a summary of the various irregular time series classes/packages
here:

https://stat.ethz.ch/pipermail/r-sig-finance/2004q4/000210.html



From fm3a004 at math.uni-hamburg.de  Wed Dec 15 13:16:35 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Wed, 15 Dec 2004 13:16:35 +0100 (MET)
Subject: [R] Massive clustering job?
In-Reply-To: <Pine.LNX.4.21.0412151112310.23621-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.GSO.3.95q.1041215130857.2065A-100000@sun11.math.uni-hamburg.de>

Dear Dan,

I would think about transforming your columns in such a way (square
root, log?) that methods operating on n*p matrices and assuming
roughly elliptical within-clusters distributions such as kmeans or
clara, or, after dimension reduction, EMclust or fixmahal can be applied.
Maybe you can even do that on untransformed data (take a look at the
variable-wise distributions or 2-d scatterplots). 
You do not need a distance matrix then.

Christian

On Wed, 15 Dec 2004, Dan Bolser wrote:

> 
> Hi, 
> 
> I have ~40,000 rows in a database, each of which contains an id column and
> 20 additional columns of count data.
> 
> I want to cluster the rows based on these count vectors.
> 
> Their are ~1.6 billion possible 'distances' between pairs of vectors
> (cells in my distance matrix), so I need to do something smart.
> 
> Can R somehow handle this?
> 
> My first thought was to index the database with something that makes
> nearest neighbour lookup more efficient, and then use single linkage
> clustering. Is this kind of index implemented in R (by default when using
> single linkage)?
> 
> Also 'grouping' identical vectors is very easy. I tried making groups more
> fuzzy by using a hashing function over the count vectors, but my hash was
> too crude. Any way to do fuzzy grouping in R which scales well?
> 
> For example, removing identical vectors gives me ~30,000 rows (and ~900
> million pairs of distances). As an example of how fast I can group, the
> above query took 0.13 seconds in mysql (using an index over every element
> in the vector). However, if I tried to calculate a distance between every
> pair of non identical vectors (lets say I can calculate ~1000 eutlidian
> distances per second) it would take me ~10 days just to calculate the
> distance matrix.
> 
> Sorry for all the information. Any suggestions on how to cluster such a
> huge dataset (using R) would be appreciated.
> 
> Cheers,
> Dan.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From wuertz at itp.phys.ethz.ch  Wed Dec 15 14:02:47 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Wed, 15 Dec 2004 13:02:47 +0000
Subject: [R] Best datatype for time-series with irregular ocurrencies
In-Reply-To: <1103106944.41c0138098b9c@webmail.fe.up.pt>
References: <1103106944.41c0138098b9c@webmail.fe.up.pt>
Message-ID: <41C035F7.8020403@itp.phys.ethz.ch>

jmoreira at fe.up.pt wrote:

>Hello,
>
>Does someone has experience working with time-series where the ocurrencies do 
>not happen at regular time spaces? 
>

package fBasics from Rmetrics (www.rmetrics.org)  has S4 timeDate and 
timeSeries objects similar
to those in SPlus for irregular time series manipulations and 
investigations.

package its is another option

Regards Diethelm Wuertz

>In one day I can have 20 ocurrencies and in 
>the following day I can have the double. ts datatype must have regular time 
>spaces. What is the best way to put the data if I want to use forecasting 
>methods as Holt-Winters, NN or SVM? To put data and time in different 
>variables? If in one variable it would be of Date datatype?
>
>Thank you for any help.
>
>Joao
>
>
>-------------------------------------------------
>Joao Mendes Moreira
>Faculdade de Engenharia da Universidade do Porto
>DEMEGI / GEIN
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From Achim.Zeileis at wu-wien.ac.at  Wed Dec 15 14:23:40 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 15 Dec 2004 14:23:40 +0100 (CET)
Subject: [R] Best datatype for time-series with irregular ocurrencies
In-Reply-To: <41C035F7.8020403@itp.phys.ethz.ch>
References: <1103106944.41c0138098b9c@webmail.fe.up.pt>
	<41C035F7.8020403@itp.phys.ethz.ch>
Message-ID: <Pine.LNX.4.58.0412151418190.4211@thorin.ci.tuwien.ac.at>

On Wed, 15 Dec 2004, Diethelm Wuertz wrote:

> jmoreira at fe.up.pt wrote:
>
> >Hello,
> >
> >Does someone has experience working with time-series where the ocurrencies do
> >not happen at regular time spaces?
> >
>
> package fBasics from Rmetrics (www.rmetrics.org)  has S4 timeDate and
> timeSeries objects similar
> to those in SPlus for irregular time series manipulations and
> investigations.
>
> package its is another option

Just to be complete: in addition to
  "its" in package its
  "timeSeries" in fBasics
which are S4 classes, there are
  "irts" in package tseries
  "zoo" in package zoo
which are S3 classes for irregularly spaced observations. "its" is
probably the most mature, "timeSeries" is - as Diethelm said - similar to
the S-PLUS implementation and "zoo" has the advantage that time
information can be of (almost) arbitrary class.
The development version of zoo is due for release next week or so,
contact me off-list if you want the current version.

Best,
Z

> Regards Diethelm Wuertz
>
> >In one day I can have 20 ocurrencies and in
> >the following day I can have the double. ts datatype must have regular time
> >spaces. What is the best way to put the data if I want to use forecasting
> >methods as Holt-Winters, NN or SVM? To put data and time in different
> >variables? If in one variable it would be of Date datatype?
> >
> >Thank you for any help.
> >
> >Joao
> >
> >
> >-------------------------------------------------
> >Joao Mendes Moreira
> >Faculdade de Engenharia da Universidade do Porto
> >DEMEGI / GEIN
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From plxadh at nottingham.ac.uk  Wed Dec 15 14:51:56 2004
From: plxadh at nottingham.ac.uk (Andrew Higginson)
Date: Wed, 15 Dec 2004 13:51:56 +0000
Subject: [R] repeated measures with Poisson
Message-ID: <s1c0419d.058@ccw0m1.nottingham.ac.uk>


I'm pretty new to R and I have a stats problem that's got me baffled. I'm trying to carry out a repeated measures test on Poission distributed data, where half the subjects in each block (4 blocks) were treated, and half were controls. Measurements were carried out before and after the treatment. There was another 2-level factor included. The trouble is I can't take averages, and have to include the identity of each subject in each block. 

Can anyone help?

Andy Higginson


This message has been scanned but we cannot guarantee that it and any
attachments are free from viruses or other damaging content: you are
advised to perform your own checks.  Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.



From j.van_den_hoff at fz-rossendorf.de  Wed Dec 15 14:56:28 2004
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Wed, 15 Dec 2004 14:56:28 +0100
Subject: [R] german umlaut problem under MacOS
In-Reply-To: <Pine.LNX.4.21.0412151112310.23621-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0412151112310.23621-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41C0428C.3040801@fz-rossendorf.de>

I did not find this in the archive (hope it isn't there...):

the current release of R (2.0.1) for MacOS (10.3.6) seems not to handle
german special characters like '??' correctly:


 > f <- '??'

can be entered at the prompt, but echoing the variable yields

[1] "\303\274"  (I think the unicode of the character)

and inserting, for instance

text(1,2,f)

in some plot seems to insert two characters (?????) (probably an 
interpretation of the first and second group of the unicode?).

I believe, this is a R problem or is there a simple configuration switch?


thanks

joerg



From rolf at math.unb.ca  Wed Dec 15 15:12:00 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 15 Dec 2004 10:12:00 -0400 (AST)
Subject: [R] how to fit a weighted logistic regression?
Message-ID: <200412151412.iBFEC0LB004900@erdos.math.unb.ca>

I was going to say ``Why not just use glm()?'', but when I tried the
example given in the original message I got a different but similarly
nervous-making warning:

	Warning in eval(expr, envir, enclos) : non-integer #successes
	in a binomial glm!

Looking into the code I found that the warning originates in
binomial()$initialize in the lines:

	m <- weights * y
        if (any(abs(m - round(m)) > 0.001)) 
            warning("non-integer #successes in a binomial glm!")

I also noticed that if y is given as a two column matrix (successes,
and failures) then the check for non-integer values in y gets done
without multiplying anything by the weights, and so y passes the
check and no warning is issued.  I.e.

	f1 <- glm(y~x,weights=w,family=binomial)

causes a warning, but

	f2 <- glm(cbind(y,1-y)~x,weights=w,family=binomial)

does not.  The fits f1 and f2 appear to be the same, although they
differ in the number of iterations, and by an order of e-8 in the
coefficients and the scaled and unscaled covariance.

So is that warning which arises in the ``f1'' case actually
appropriate?

					cheers,

						Rolf Turner
						rolf at math.unb.ca

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===

Original message:

> I tried lrm in library(Design) but there is always some error
> message. Is this function really doing the weighted logistic
> regression as maximizing the following likelihood:
> 
> \sum w_i*(y_i*\beta*x_i-log(1+exp(\beta*x_i)))
> 
> Does anybody know a better way to fit this kind of model in R?
> 
> FYI: one example of getting error message is like:
> > x=runif(10,0,3)
> > y=c(rep(0,5),rep(1,5))
> > w=rep(1/10,10)
> > fit=lrm(y~x,weights=w)
> Warning message: 
> currently weights are ignored in model validation and
> bootstrapping lrm fits in: lrm(y ~ x, weights = w) 
> 
> although the model can be fit, the above output
> warning makes me uncomfortable. Can anybody explain
> about it a little bit?
> 
> Best wishes,
> Feixia



From matthew_wiener at merck.com  Wed Dec 15 15:25:16 2004
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 15 Dec 2004 09:25:16 -0500
Subject: [R] Massive clustering job?
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04993C8B@uswsmx03.merck.com>

It sounds like "clara" in package cluster might help.

Regards,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
Sent: Wednesday, December 15, 2004 6:37 AM
To: R mailing list
Subject: [R] Massive clustering job?



Hi, 

I have ~40,000 rows in a database, each of which contains an id column and
20 additional columns of count data.

I want to cluster the rows based on these count vectors.

Their are ~1.6 billion possible 'distances' between pairs of vectors
(cells in my distance matrix), so I need to do something smart.

Can R somehow handle this?

My first thought was to index the database with something that makes
nearest neighbour lookup more efficient, and then use single linkage
clustering. Is this kind of index implemented in R (by default when using
single linkage)?

Also 'grouping' identical vectors is very easy. I tried making groups more
fuzzy by using a hashing function over the count vectors, but my hash was
too crude. Any way to do fuzzy grouping in R which scales well?

For example, removing identical vectors gives me ~30,000 rows (and ~900
million pairs of distances). As an example of how fast I can group, the
above query took 0.13 seconds in mysql (using an index over every element
in the vector). However, if I tried to calculate a distance between every
pair of non identical vectors (lets say I can calculate ~1000 eutlidian
distances per second) it would take me ~10 days just to calculate the
distance matrix.

Sorry for all the information. Any suggestions on how to cluster such a
huge dataset (using R) would be appreciated.

Cheers,
Dan.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Wed Dec 15 11:53:12 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 15 Dec 2004 04:53:12 -0600
Subject: [R] how to fit a weighted logistic regression?
In-Reply-To: <20041215042253.80165.qmail@web51802.mail.yahoo.com>
References: <20041215042253.80165.qmail@web51802.mail.yahoo.com>
Message-ID: <41C01798.7080900@vanderbilt.edu>

Kerry Bush wrote:
> I tried lrm in library(Design) but there is always
> some error message. Is this function really doing the
> weighted logistic regression as maximizing the
> following likelihood:
> 
> \sum w_i*(y_i*\beta*x_i-log(1+exp(\beta*x_i)))
> 
> Does anybody know a better way to fit this kind of
> model in R?
> 
> FYI: one example of getting error message is like:
> 
>>x=runif(10,0,3)
>>y=c(rep(0,5),rep(1,5))
>>w=rep(1/10,10)
>>fit=lrm(y~x,weights=w)
> 
> Warning message: 
> currently weights are ignored in model validation and
> bootstrapping lrm fits in: lrm(y ~ x, weights = w) 
> 
> although the model can be fit, the above output
> warning makes me uncomfortable. Can anybody explain
> about it a little bit?

The message means exactly what it says.  Model validation in Design 
currently cannot incorporate weights for lrm.  Everything else is OK.

Frank Harrell

> 
> Best wishes,
> Feixia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From deepayan at stat.wisc.edu  Wed Dec 15 15:44:07 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 15 Dec 2004 08:44:07 -0600
Subject: [R] RE: adding perspectives to existing persp plots
In-Reply-To: <65EB8C57DA514942B1EC55E8DA89FE550151EA40@mail.site.ntu.edu.au>
References: <65EB8C57DA514942B1EC55E8DA89FE550151EA40@mail.site.ntu.edu.au>
Message-ID: <200412150844.07585.deepayan@stat.wisc.edu>

On Wednesday 15 December 2004 01:10, Corey Bradshaw wrote:
> Thanks, Romain. I've certainly used that to draw lines and points in
> the plots produced by 'persp'; however, my problem is that I need to
> incorporate an entirely new z function (not just a plane) onto the
> same plot (i.e., using the same x and y values).

If the surfaces are non-intersecting, you might be able to use 
'wireframe' from the lattice package. See the second example 
in ?wireframe.

Deepayan



From Roger.Bivand at nhh.no  Wed Dec 15 15:43:17 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 15 Dec 2004 15:43:17 +0100 (CET)
Subject: [R] question about surf.gls(spatial package) with 
	NA/NaN/infvalues
In-Reply-To: <001801c4e281$c97568b0$4f05030a@IBMINSERM>
Message-ID: <Pine.LNX.4.44.0412151538240.1491-100000@reclus.nhh.no>

On Wed, 15 Dec 2004, BUHARD O wrote:

> Hi all,
> 
> I need to fit a trend surface to a 3D-dataset with the surf.gls()
> function but my data contain missing values. There is just 2 NA in a
> total of 360 data points but I can't suppresse the variable associated
> with these NA values to the analysis. The problem is surf.gls can't
> manage with Na/NaN/inf values, so I wanted to know if a similar function
> in R can do it. I precise that there's no replicate in the (preliminary
> test) data so I can't estimate these values them easily.
> 

Either the z values are missing, and you can use the fit on the 358 points 
to predict them, or one or other of the point coordinates are missing, so 
you don't know where the affected z values were observed. You can fit once 
you omit just those incomplete observations, I think. 

> Thanks a lot for help
> 
> Regards
> 
> BUHARD Olivier
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From Allan at STATS.uct.ac.za  Wed Dec 15 15:48:51 2004
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Wed, 15 Dec 2004 16:48:51 +0200
Subject: [R] R: optimisation
Message-ID: <41C04ED3.9C90BD4F@STATS.uct.ac.za>

hi all

other than optim, optimise, and some other related optimisation
functions are there any optimisation packages in R?

From f.harrell at vanderbilt.edu  Wed Dec 15 16:01:04 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 15 Dec 2004 09:01:04 -0600
Subject: [R] MIME decoding in Mozilla Thunderbird
In-Reply-To: <41C018DA.1090306@evp.slu.se>
References: <41C018DA.1090306@evp.slu.se>
Message-ID: <41C051B0.4060406@vanderbilt.edu>

CG Pettersson wrote:
> Hello all!
> I recently switched mail program to Mozilla Thunderbird, running on W2k.
> Everything works fine, except the MIME-digests from this list. The 
> decoding doesn??t work properly.
> 
> I had contact with Martin Maechler some time ago, and he suggested a try 
> on this list for ideas on how to do the decoding in Windows, even if 
> it??s not a proper R-question.
> 
> Outlook do the proper job on the MIME, but using that is to go a bit 
> far! Or?
> 
> /CG
> 

Thunderbird, which is an otherwise wonderful mail client, does not work 
for r-help digests.  The reason is that if you receive 100 messages in a 
day, Thunderbird inefficiently handles all the mime 'attachments', and 
navigating them all is incredibly slow.  I didn't have the decoding 
problem you mentioned though.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From i.visser at uva.nl  Wed Dec 15 16:15:35 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Wed, 15 Dec 2004 16:15:35 +0100
Subject: [R] R: optimisation
In-Reply-To: <41C04ED3.9C90BD4F@STATS.uct.ac.za>
Message-ID: <BDE613A7.BA7F%i.visser@uva.nl>

There's also nlm for non-linear newton type minimization,
best, ingmar

On 12/15/04 3:48 PM, "Clark Allan" <Allan at STATS.uct.ac.za> wrote:

> hi all
> 
> other than optim, optimise, and some other related optimisation
> functions are there any optimisation packages in R?
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From cg.pettersson at evp.slu.se  Wed Dec 15 16:43:38 2004
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Wed, 15 Dec 2004 16:43:38 +0100
Subject: [R] MIME decoding in Mozilla Thunderbird
In-Reply-To: <41C051B0.4060406@vanderbilt.edu>
References: <41C018DA.1090306@evp.slu.se> <41C051B0.4060406@vanderbilt.edu>
Message-ID: <41C05BAA.9060002@evp.slu.se>

Ok, the time consumption could be a problem, I find it quite okay though.

My problem is to reach the actual messages.

If I use the option File/Attechments/Open, the window that opens just 
contain a list of all headers from the last 24 hours (from which I just 
choosed one - as I thought)

If I on the other hand use File/Attechments/Save, I can save the choosen 
    message as an .eml file. Why can??t I open it?

I realise this question should be on the Thunderbird list, but I have a 
feeling the coding in the MIME-digests of this list are unusually advanced.

Thanks
/CG



Frank E Harrell Jr wrote:
> CG Pettersson wrote:
> 
>> Hello all!
>> I recently switched mail program to Mozilla Thunderbird, running on W2k.
>> Everything works fine, except the MIME-digests from this list. The 
>> decoding doesn??t work properly.
>>
>> I had contact with Martin Maechler some time ago, and he suggested a 
>> try on this list for ideas on how to do the decoding in Windows, even 
>> if it??s not a proper R-question.
>>
>> Outlook do the proper job on the MIME, but using that is to go a bit 
>> far! Or?
>>
>> /CG
>>
> 
> Thunderbird, which is an otherwise wonderful mail client, does not work 
> for r-help digests.  The reason is that if you receive 100 messages in a 
> day, Thunderbird inefficiently handles all the mime 'attachments', and 
> navigating them all is incredibly slow.  I didn't have the decoding 
> problem you mentioned though.
> 


-- 
CG Pettersson MSci. PhD.Stud.
Swedish University of Agricultural Sciences (SLU)
Dep. of Ecology and Crop production sciences (EVP).
http://www.slu.se/
cg.pettersson at evp.slu.se



From ripley at stats.ox.ac.uk  Wed Dec 15 16:29:36 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Wed, 15 Dec 2004 15:29:36 +0000 (GMT)
Subject: [R] R does not support UTF-8 (was german umlaut problem under
	MacOS)
In-Reply-To: <41C0428C.3040801@fz-rossendorf.de>
Message-ID: <Pine.GSO.4.31.0412151519420.4791-100000@markov.stats>

You wrote your mail in UTF-8.  R does not support UTF-8, and that is both
documented and announced on startup in such a locale (at least on OSes
with standard-conforming implementations):

gannet% env LANG=en_GB.utf8 R

R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
...
WARNING: UTF-8 locales are not currently supported

Solution: do not use an unsupported locale.


On Wed, 15 Dec 2004, joerg van den hoff wrote:

> I did not find this in the archive (hope it isn't there...):
>
> the current release of R (2.0.1) for MacOS (10.3.6) seems not to handle
> german special characters like '' correctly:

I get two characters (Atilde quarter) here.

>  > f <- ''
>
> can be entered at the prompt, but echoing the variable yields

You mean printing the contents, I presume.

> [1] "\303\274"  (I think the unicode of the character)
>
> and inserting, for instance
>
> text(1,2,f)
>
> in some plot seems to insert two characters () (probably an
> interpretation of the first and second group of the unicode?).
>
> I believe, this is a R problem or is there a simple configuration switch?
>
>
> thanks
>
> joerg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Wed Dec 15 16:34:20 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 15 Dec 2004 10:34:20 -0500
Subject: [R] can R do the goodman modified multiple regression method?
In-Reply-To: <0I8P00KV8W4FVT@mail.fudan.edu.cn>
Message-ID: <20041215153420.TRQ1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear ronggui,

If memory serves me right, this is a relatively early presentation for
sociologists of loglinear models for contingency tables (in which all the
variables are dichotomous). You can fit such a model in R as a generalised
linear model using glm() or via the loglin() function. The latter, which
works by iterative proportional fitting, will be more similar in approach to
what's in the Goodman paper (but both produce ML estimates).

I hope that this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ronggui
> Sent: Tuesday, December 14, 2004 10:03 AM
> To: R-help
> Subject: [R] can R do the goodman modified multiple regression method?
> 
> the method is described in the article:goodman leo A.,a 
> modified multiple regression approch to analysis of 
> dischotomous variables",american sociological review 
> 33(hebruary):28-46
> 
> thank you in advance:)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Wed Dec 15 16:46:30 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 15 Dec 2004 16:46:30 +0100
Subject: [R] MIME decoding in Mozilla Thunderbird
In-Reply-To: <41C05BAA.9060002@evp.slu.se>
References: <41C018DA.1090306@evp.slu.se> <41C051B0.4060406@vanderbilt.edu>
	<41C05BAA.9060002@evp.slu.se>
Message-ID: <16832.23638.186698.701855@gargle.gargle.HOWL>

>>>>> "CG" == CG Pettersson <cg.pettersson at evp.slu.se>
>>>>>     on Wed, 15 Dec 2004 16:43:38 +0100 writes:

    CG> Ok, the time consumption could be a problem, I find it quite okay though.
    CG> My problem is to reach the actual messages.

    CG> If I use the option File/Attechments/Open, the window that opens just 
    CG> contain a list of all headers from the last 24 hours (from which I just 
    CG> choosed one - as I thought)

    CG> If I on the other hand use File/Attechments/Save, I can save the choosen 
    CG> message as an .eml file. Why can??t I open it?

    CG> I realise this question should be on the Thunderbird list, but I have a 
    CG> feeling the coding in the MIME-digests of this list are unusually advanced.

I don't think so.

AFAIK there are thousands of mailman-operated mailing lists out
there, and all of them use the same MIME digestification.

Consequently, I now think that you could pose this question / problem
on the 'mailman-users' mailing list as well and may get other
suggestions. (When you do, please indicate that we use "mailman
2.1.5" which is the latest released version).

Martin


    CG> Frank E Harrell Jr wrote:
    >> CG Pettersson wrote:
    >> 
    >>> Hello all!
    >>> I recently switched mail program to Mozilla Thunderbird, running on W2k.
    >>> Everything works fine, except the MIME-digests from this list. The 
    >>> decoding doesn??t work properly.
    >>> 
    >>> I had contact with Martin Maechler some time ago, and he suggested a 
    >>> try on this list for ideas on how to do the decoding in Windows, even 
    >>> if it??s not a proper R-question.
    >>> 
    >>> Outlook do the proper job on the MIME, but using that is to go a bit 
    >>> far! Or?
    >>> 
    >>> /CG
    >>> 
    >> 
    >> Thunderbird, which is an otherwise wonderful mail client, does not work 
    >> for r-help digests.  The reason is that if you receive 100 messages in a 
    >> day, Thunderbird inefficiently handles all the mime 'attachments', and 
    >> navigating them all is incredibly slow.  I didn't have the decoding 
    >> problem you mentioned though.



From pburns at pburns.seanet.com  Wed Dec 15 16:48:35 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 15 Dec 2004 15:48:35 +0000
Subject: [R] R: optimisation
In-Reply-To: <41C04ED3.9C90BD4F@STATS.uct.ac.za>
References: <41C04ED3.9C90BD4F@STATS.uct.ac.za>
Message-ID: <41C05CD3.10109@pburns.seanet.com>

You don't say what sort of optimisation you have in mind.  If you
are looking for something that will handle "non-standard" problems,
you can have a look at 'genopt' from S Poetry.


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Clark Allan wrote:

>hi all
>
>other than optim, optimise, and some other related optimisation
>functions are there any optimisation packages in R?
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From j.van_den_hoff at fz-rossendorf.de  Wed Dec 15 18:13:35 2004
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Wed, 15 Dec 2004 18:13:35 +0100
Subject: [R] R does not support UTF-8 (was german umlaut problem under
	MacOS)
In-Reply-To: <Pine.GSO.4.31.0412151519420.4791-100000@markov.stats>
References: <Pine.GSO.4.31.0412151519420.4791-100000@markov.stats>
Message-ID: <41C070BF.8030705@fz-rossendorf.de>

Brian D Ripley wrote:
> You wrote your mail in UTF-8.  R does not support UTF-8, and that is both
> documented and announced on startup in such a locale (at least on OSes
> with standard-conforming implementations):

thanks for clarifying this point.

nevertheless:

1. the mail was (on purpose) sent in utf-8 to transport correctly the 
output from the R command window (i.e. the GUI provided with the macOS 
port). it is _this_ GUI (sorry for not explaining this correctly in the 
first place) where the problem occurs. I'm not using (knowingly at 
least) utf-8.
when starting the same binary from the command line in a terminal (where 
I generally use ISO Latin 1 encoding) it is perfectly possible to get 
the special characters into variables and into plots.

2. the OS is macos 10.3, i.e. essentially FreeBSD derivative and 
hopefully conforms to the standardsbu  R on startup in the GUI gives only:
========cut=============

R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.


R>
========cut=============
i.e. no announcement whatsoever concerning missing utf-8 support, 
despite the fact that following input is interpreted in such a way.

so, probably this is more a question to the maintainers of the macOS 
port:_where_ did R (when startet with the GUI) get the notion that it 
should interpret keyboard input as utf-8?  can I change this (it's not 
in the preferences, for instance)?

> 
> gannet% env LANG=en_GB.utf8 R
> 
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
> ...
> WARNING: UTF-8 locales are not currently supported
> 
> Solution: do not use an unsupported locale.
> 
> 
> On Wed, 15 Dec 2004, joerg van den hoff wrote:
> 
> 
>>I did not find this in the archive (hope it isn't there...):
>>
>>the current release of R (2.0.1) for MacOS (10.3.6) seems not to handle
>>german special characters like '??' correctly:
> 
> 
> I get two characters (Atilde quarter) here.
> 
> 
>> > f <- '??'
>>
>>can be entered at the prompt, but echoing the variable yields
> 
> 
> You mean printing the contents, I presume.
yes ("shell speak").
> 
> 
>>[1] "\303\274"  (I think the unicode of the character)
>>
>>and inserting, for instance
>>
>>text(1,2,f)
>>
>>in some plot seems to insert two characters (?????) (probably an
>>interpretation of the first and second group of the unicode?).
>>
>>I believe, this is a R problem or is there a simple configuration switch?
>>
>>
>>thanks
>>
>>joerg
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 

regards,

joerg



From dederderian at micron.com  Wed Dec 15 18:37:34 2004
From: dederderian at micron.com (dederderian@micron.com)
Date: Wed, 15 Dec 2004 10:37:34 -0700
Subject: [R] backspace key doesn't work correctly
Message-ID: <E6F64B42266D654B80A0F7F4B98212A502D69342@ntxboimbx03.micron.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041215/d60ecc14/attachment.pl

From rrouzier at mdanderson.org  Wed Dec 15 18:46:24 2004
From: rrouzier at mdanderson.org (rrouzier@mdanderson.org)
Date: Wed, 15 Dec 2004 11:46:24 -0600
Subject: [R] (no subject)
Message-ID: <OF50827248.B2B1B81F-ON86256F6B.0061206D-86256F6B.0061A19C@mdacc.tmc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041215/89f5cd1e/attachment.pl

From sblay at sfu.ca  Wed Dec 15 19:57:08 2004
From: sblay at sfu.ca (Sigal Blay)
Date: Wed, 15 Dec 2004 10:57:08 -0800
Subject: [R] ERROR: installing package indices failed
In-Reply-To: <41A0C022.4090409@statistik.uni-dortmund.de>
References: <20041120205712.GA13035@stawlmihq.cs.sfu.ca>
	<41A0C022.4090409@statistik.uni-dortmund.de>
Message-ID: <20041215185707.GB11993@sfu.ca>

Turns out that this problem is a propagation of a bug from a
Depends package - gdata doesn't import stats in its NAMESPACE
file, although it makes use of objects defined in stats like 
reorder and na.omit.

I'm curious as to the reason why 'R CMD check' did not detect
this at the source package, and instead did not pass my new  
package. Something for you mighty R developers to consider?  

Thanks,
Sigal Blay

On Sun, Nov 21, 2004 at 05:19:46PM +0100, Uwe Ligges wrote:
> Sigal Blay wrote:
> >gregmisc is installed yet the problem persist.
> >I installed gregmisc using 
> >install.packages(c("combinat","gregmisc","genetics"),lib='/home/sblay/lib')
> >(on the same library path where I am trying to install LDheatmap)
> >
> 
> Have you set the environment variable R_LIBS appropriately?
> 
> Uwe Ligges
> 
> 
> >>installed.packages(lib='/home/sblay/lib')
> >
> >          Package     LibPath           Version Priority Bundle   
> >combinat  "combinat"  "/home/sblay/lib" "0.0-5" NA       NA       
> >gdata     "gdata"     "/home/sblay/lib" "2.0.0" NA       
> >"gregmisc"
> >genetics  "genetics"  "/home/sblay/lib" "1.1.1" NA       NA       
> >gmodels   "gmodels"   "/home/sblay/lib" "2.0.0" NA       
> >"gregmisc"
> >gplots    "gplots"    "/home/sblay/lib" "2.0.0" NA       
> >"gregmisc"
> >gtools    "gtools"    "/home/sblay/lib" "2.0.0" NA       
> >"gregmisc"
> >LDheatmap "LDheatmap" "/home/sblay/lib" "1.0"   NA       NA       
> >
> >...
> >
> >
> >
> >>I am developing a package named LDehatmap.
> >>It depends on the "genetics" package
> >>and includes two data files and a demo file.
> >>When I'm trying to install it, I get the following messages:
> >>
> >>* Installing *source* package 'LDheatmap' ...
> >>** R
> >>** data
> >>** demo
> >>** help
> >>
> >>>>>Building/Updating help pages for package 'LDheatmap'
> >>
> >>   Formats: text html latex example
> >>LDheatmap                text    html    latex   example
> >>ldheatmap                text    html    latex   example
> >>Error: object 'reorder' not found whilst loading namespace 
> >>'gdata'
> >>Error: package 'gdata' could not be loaded
> >>Execution halted
> >>ERROR: installing package indices failed
> >>
> >>Any ideas?
> >
> >
> >Yes.  You do not have gdata (part of gregmisc) installed, and 
> >genetics depends on it.  How did you get genetics installed? A 
> >binary install?
> >
> >Install gregmisc ....
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed Dec 15 20:07:08 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 15 Dec 2004 11:07:08 -0800
Subject: [R] AUC for logistic regression [was: (no subject)]
In-Reply-To: <OF50827248.B2B1B81F-ON86256F6B.0061206D-86256F6B.0061A19C@mdacc.tmc.edu>
References: <OF50827248.B2B1B81F-ON86256F6B.0061206D-86256F6B.0061A19C@mdacc.tmc.edu>
Message-ID: <41C08B5C.8030907@pdf.com>

      What's AUC?  If you mean AIC (Akaike Information Criterion), and 
if you fit logistic regression using "glm", the help file says that glm 
returns an object of class "glm", which is a list containing among other 
things an attribute aic.  For example, suppose you fit a model as follows: 

      fit <- glm(y~x, famil=binomial()...)

      Then fit$aic returns the AIC. 

      You may also wish to consider anova and anova.glm. 

      hope this helps.  spencer graves

rrouzier at mdanderson.org wrote:

>Dear R-helper,
>
>I would like to compare the AUC of two logistic regression models (same 
>population). Is it possible with R ?
>
>Thank you
>
>Roman Rouzier
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From andy_liaw at merck.com  Wed Dec 15 20:25:06 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 15 Dec 2004 14:25:06 -0500
Subject: [R] AUC for logistic regression [was: (no subject)]
Message-ID: <3A822319EB35174CA3714066D590DCD50994E42D@usrymx25.merck.com>

My guess is `area under the ROC curve'.  There's the roc package in
BioConductor that I believe can compute this.

Andy


> From: Spencer Graves
> 
>       What's AUC?  If you mean AIC (Akaike Information 
> Criterion), and 
> if you fit logistic regression using "glm", the help file 
> says that glm 
> returns an object of class "glm", which is a list containing 
> among other 
> things an attribute aic.  For example, suppose you fit a 
> model as follows: 
> 
>       fit <- glm(y~x, famil=binomial()...)
> 
>       Then fit$aic returns the AIC. 
> 
>       You may also wish to consider anova and anova.glm. 
> 
>       hope this helps.  spencer graves
> 
> rrouzier at mdanderson.org wrote:
> 
> >Dear R-helper,
> >
> >I would like to compare the AUC of two logistic regression 
> models (same 
> >population). Is it possible with R ?
> >
> >Thank you
> >
> >Roman Rouzier
> >	[[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >  
> >
> 
> -- 
> 
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From arrayprofile at yahoo.com  Wed Dec 15 20:43:17 2004
From: arrayprofile at yahoo.com (array chip)
Date: Wed, 15 Dec 2004 11:43:17 -0800 (PST)
Subject: [R] using Hmisc and Design library
Message-ID: <20041215194317.76573.qmail@web40822.mail.yahoo.com>

Hi all,

I encountered a weird problem when using the
Design and Hmisc libraries in S-Plus (it worked well
in R). I have a data frame called "b", which
has 3 columns: "time", "status" and
"scores", a sample of the data frame is like:

data frame "b":
  time status scores 
1   27      0 -126.7
2   24      0 -135.6
3   30      0 -139.5
4   49      0 -137.6
5   27      0 -136.9

when I ran the following script using this data frame,
even though no error message was produced, no fit
object was generated:

> library(Hmisc,T);library(Design,T)
> dd <- datadist(b)
> options(datadist='dd')
> fit <- cph(Surv(time,status) ~ scores,
data=b,surv=T, x=T, y=T)
> fit
Problem: Object "fit" not found, while calling
subroutine S_agsurv2 
Use traceback() to see the call stack

actually data frame "b" has 177 rows, the script ran
ok on the first 166 rows as a subset, but started to
break down if subset of the first 177 rows were used
as the input, or the first 166 rows plus 168th row,
....

the data in those rows in "b" are:
    time status scores 
165  172      0 -123.3
166  105      0 -138.4
167  166      0 -128.8
168  140      0 -114.2
169  163      0 -117.0
170  141      0 -115.8

Additionally, even if I only ran the script on the
first 166 rows, I still can't generate a plot:

> dd <- datadist(bbb[1:166,])
> options(datadist='dd')
> fit <- cph(Surv(time,status) ~ scores,
data=bbb[1:166,],surv=T, x=T, y=T)
> fit

Cox Proportional Hazards Model

cph(formula = Surv(time, status) ~ scores, data =
bbb[1:166,  ], x = T, y = T, surv = T)

 Obs Events Model L.R. d.f. P Score Score P    R2 
 166     36      29.08    1 0 35.37       0 0.182


        coef se(coef)    z         p 
scores 0.102   0.0172 5.94 2.91e-009

> plot(fit, scores=seq(-140, -100, by=1),
time=36,fun=function(x) 1-x,xlim=c(-140,
-100),ylim=c(0,1),lwd=3,xlab='Scores',ylab='Probability
at 3 Years')

no error message, but only a blank graph window is
produced.

can anyone please tell me why this happens only in
S-Plus, but not in R? no missing value is present in
either data frame. the data "b" is attached in case
you need to run the script.

Thanks very much!






		
__________________________________ 


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: b.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041215/8b4952f7/b.txt

From gunter.berton at gene.com  Wed Dec 15 20:41:27 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 15 Dec 2004 11:41:27 -0800
Subject: [R] AUC for logistic regression [was: (no subject)]
In-Reply-To: <41C08B5C.8030907@pdf.com>
Message-ID: <200412151941.iBFJfRiV028340@hertz.gene.com>


AUC means "Area Under the Curve" and is a common summary statistic for
repeated measures experiments (e.g. repeated measurements of serum
concentration of a drug in an individual)in PK/PD studies
(pharmacokinetic/pharmacodynamic).

I think the poster may actually mean "nonlinear regression for logistic
models" rather than "logistic regression," which, of course, has a different
statistical meaning. Hence nonlinear regression modeling for repeated
measures is probably the issue here, for which nlme() is appropriate, I
think. Alternatively, and perhaps somewhat less statistically desirable
(though perhaps fairly standard in PK/PD modeling), one can use nls() or
perhaps nlsList() to fit each individual's curve and compute the AUC's. So
in any case, the answer appears to be "yes, you can use R" perhaps with
add-ons to do what you want.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Wednesday, December 15, 2004 11:07 AM
> To: rrouzier at mdanderson.org
> Cc: R-help at lists.R-project.org
> Subject: Re: [R] AUC for logistic regression [was: (no subject)]
> 
>       What's AUC?  If you mean AIC (Akaike Information 
> Criterion), and 
> if you fit logistic regression using "glm", the help file 
> says that glm 
> returns an object of class "glm", which is a list containing 
> among other 
> things an attribute aic.  For example, suppose you fit a 
> model as follows: 
> 
>       fit <- glm(y~x, famil=binomial()...)
> 
>       Then fit$aic returns the AIC. 
> 
>       You may also wish to consider anova and anova.glm. 
> 
>       hope this helps.  spencer graves
> 
> rrouzier at mdanderson.org wrote:
> 
> >Dear R-helper,
> >
> >I would like to compare the AUC of two logistic regression 
> models (same 
> >population). Is it possible with R ?
> >
> >Thank you
> >
> >Roman Rouzier
> >	[[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >  
> >
> 
> -- 
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From v39k9 at unb.ca  Wed Dec 15 20:46:41 2004
From: v39k9 at unb.ca (Joe Nocera)
Date: Wed, 15 Dec 2004 15:46:41 -0400
Subject: [R] AUC for logistic regression [was: (no subject)]
In-Reply-To: <41C08B5C.8030907@pdf.com>
References: <OF50827248.B2B1B81F-ON86256F6B.0061206D-86256F6B.0061A19C@mdacc.tmc.edu>
	<41C08B5C.8030907@pdf.com>
Message-ID: <1103140001.41c094a15a561@webmail.unb.ca>

I believe that Roman is referring to AUC as the "Area Under Curve" from a Receiver
Operating Characteristic.  

If this indeed your quantity of interest - it can be calculated in R.  You can download
code at:

http://www.bioconductor.org/repository/release1.5/package/Win32/
and/or
http://biostat.ku.dk/~bxc/SPE/library/

Check out the archives - I'm sure there is more there if you search "ROC" instead.

Cheers,
Joe

Quoting Spencer Graves <spencer.graves at pdf.com>:

>       What's AUC?  If you mean AIC (Akaike Information Criterion), and 
> if you fit logistic regression using "glm", the help file says that glm 
> returns an object of class "glm", which is a list containing among other 
> things an attribute aic.  For example, suppose you fit a model as follows: 
> 
>       fit <- glm(y~x, famil=binomial()...)
> 
>       Then fit$aic returns the AIC. 
> 
>       You may also wish to consider anova and anova.glm. 
> 
>       hope this helps.  spencer graves
> 
> rrouzier at mdanderson.org wrote:
> 
> >Dear R-helper,
> >
> >I would like to compare the AUC of two logistic regression models (same 
> >population). Is it possible with R ?
> >
> >Thank you
> >
> >Roman Rouzier
> >	[[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >  
> >
> 
> -- 
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Joseph J. Nocera
Ph.D. Candidate
NB Coop. Fish & Wildlife Research Unit
Biology Department - Univ. New Brunswick
Fredericton, NB
Canada   E3B 6E1
tel: (902) 679-5733

"Why does it have to be spiders?  Why can't it be 'follow the butterflies'"?!
    - Ron Weasley, Harry Potter & The Chamber of Secrets



From ross at biostat.ucsf.edu  Wed Dec 15 21:05:15 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 15 Dec 2004 12:05:15 -0800
Subject: [R] main() in libR?
Message-ID: <20041215200515.GL20200@wheat.boylan.org>

libR seems to include a main() function.  Should it?

I'm hoping this simpler question will get more of a response than my
previous post about R a la carte :)

The evidence that libR includes main() is two-fold.  First, when I use
nm, I see the _main defined.  Second, when I run a program linked to
libR, the main R shell takes over, even though I have my own main (in
another library).

Looking at the sources, it seems the only possible contributor of
main() is src/main/Rmain.c.  I wouldn't expect this in the library,
and the documentation of the library (in "R Extensions") says you must
supply your own main.

I have tried to follow through the Makefile's, but the spot the
library is built has not jumped out at me.

I've encountered this with number of generations of R on a number of
platforms (Debian Linux and OS X).

Thanks.



From nov_tao at yahoo.com  Wed Dec 15 21:14:30 2004
From: nov_tao at yahoo.com (Y. C. Tao)
Date: Wed, 15 Dec 2004 12:14:30 -0800 (PST)
Subject: [R] which
Message-ID: <20041215201431.75426.qmail@web54503.mail.yahoo.com>

Why the last "which" in the following example doesn't
work? Is there a simple way to identify the indices of
array elements that meet multiple criteria?

Thanks.

YC Tao

> x<-1:10
> which(x<5)
[1] 1 2 3 4
> which(x>2)
[1]  3  4  5  6  7  8  9 10
> which(x<5 && x>2)
numeric(0)




		
__________________________________ 

Dress up your holiday email, Hollywood style. Learn more.



From andy_liaw at merck.com  Wed Dec 15 21:22:04 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 15 Dec 2004 15:22:04 -0500
Subject: [R] which
Message-ID: <3A822319EB35174CA3714066D590DCD50994E430@usrymx25.merck.com>

Because you have one too many `&'.

Andy

> From: Y. C. Tao
> 
> Why the last "which" in the following example doesn't
> work? Is there a simple way to identify the indices of
> array elements that meet multiple criteria?
> 
> Thanks.
> 
> YC Tao
> 
> > x<-1:10
> > which(x<5)
> [1] 1 2 3 4
> > which(x>2)
> [1]  3  4  5  6  7  8  9 10
> > which(x<5 && x>2)
> numeric(0)
> 
> __________________________________ 
> 
> Dress up your holiday email, Hollywood style. Learn more.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tura at centroin.com.br  Wed Dec 15 21:30:37 2004
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Wed, 15 Dec 2004 18:30:37 -0200
Subject: [R] AUC for logistic regression [was: (no subject)]
In-Reply-To: <41C08B5C.8030907@pdf.com>
References: <OF50827248.B2B1B81F-ON86256F6B.0061206D-86256F6B.0061A19C@mdacc.tmc.edu>
	<41C08B5C.8030907@pdf.com>
Message-ID: <6.1.2.0.2.20041215182424.04c2a070@centroin.com.br>

At 17:07 15/12/2004, Spencer Graves wrote:

>>Dear R-helper,
>>
>>I would like to compare the AUC of two logistic regression models (same 
>>population). Is it possible with R ?
>>
>>Thank you
>>
>>Roman Rouzier


Roman

If I understand your question You have 2 ROC curve from same dataset. In 
this case you can use a routine create for me :

seROC<-function(AUC,na,nn){
a<-AUC
q1<-a/(2-a)
q2<-(2*a^2)/(1+a)
se<-sqrt((a*(1-a)+(na-1)*(q1-a^2)+(nn-1)*(q2-a^2))/(nn*na))
se
}


cROC<-function(AUC1,na1,nn1,AUC2,na2,nn2,r){
se1<-seROC(AUC1,na1,nn1)
se2<-seROC(AUC2,na2,nn2)
sed<-sqrt(se1^2+se2^2-2*r*se1*se2)
zad<-(AUC1-AUC2)/sed
p<-dnorm(zad)
a<-list(zad,p)
a
}

The first function (seROC) calculate teh standart error of ROC curve, the 
second function (cROC) compare ROC curves .

The parameters:

AUC - area under curve
na - number of positives results
nn - number total tests (positives +negatives)
r - correlation of two numeric variables

Best wishes





Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil 


-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From f.harrell at vanderbilt.edu  Wed Dec 15 21:53:09 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 15 Dec 2004 14:53:09 -0600
Subject: [R] AUC for logistic regression [was: (no subject)]
In-Reply-To: <1103140001.41c094a15a561@webmail.unb.ca>
References: <OF50827248.B2B1B81F-ON86256F6B.0061206D-86256F6B.0061A19C@mdacc.tmc.edu>	<41C08B5C.8030907@pdf.com>
	<1103140001.41c094a15a561@webmail.unb.ca>
Message-ID: <41C0A435.6000603@vanderbilt.edu>

Joe Nocera wrote:
> I believe that Roman is referring to AUC as the "Area Under Curve" from a Receiver
> Operating Characteristic.  
> 
> If this indeed your quantity of interest - it can be calculated in R.  You can download
> code at:
> 
> http://www.bioconductor.org/repository/release1.5/package/Win32/
> and/or
> http://biostat.ku.dk/~bxc/SPE/library/
> 
> Check out the archives - I'm sure there is more there if you search "ROC" instead.
> 
> Cheers,
> Joe
> 
> Quoting Spencer Graves <spencer.graves at pdf.com>:
> 
> 
>>      What's AUC?  If you mean AIC (Akaike Information Criterion), and 
>>if you fit logistic regression using "glm", the help file says that glm 
>>returns an object of class "glm", which is a list containing among other 
>>things an attribute aic.  For example, suppose you fit a model as follows: 
>>
>>      fit <- glm(y~x, famil=binomial()...)
>>
>>      Then fit$aic returns the AIC. 
>>
>>      You may also wish to consider anova and anova.glm. 
>>
>>      hope this helps.  spencer graves
>>
>>rrouzier at mdanderson.org wrote:
>>
>>
>>>Dear R-helper,
>>>
>>>I would like to compare the AUC of two logistic regression models (same 
>>>population). Is it possible with R ?
>>>
>>>Thank you
>>>
>>>Roman Rouzier
>>>	[[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>> 
>>>
>>
>>-- 
>>Spencer Graves, PhD, Senior Development Engineer
>>O:  (408)938-4420;  mobile:  (408)655-4567
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Joseph J. Nocera

AUC is standard output in the lrm function in the Design package (the "C 
Index").  validate.lrm computes the overfitting-corrected C index.


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From minhan.science at gmail.com  Wed Dec 15 21:55:07 2004
From: minhan.science at gmail.com (Min-Han Tan)
Date: Wed, 15 Dec 2004 15:55:07 -0500
Subject: [R] hclust and heatmap - slightly different dendrograms?
Message-ID: <7902152a04121512556a2eba3a@mail.gmail.com>

Good afternoon,

I ran heatmap and hclust on the same matrix x (strictly, I ran
heatmap(x), and hclust(dist(t(x))), and realized that the two
dendrograms were slightly different, in that the left-right
arrangement of one pair of subclusters (columns) was reversed in the
two functions (but all individual columns were grouped correctly).

Looking through the code for heatmap as a most definite nonexpert, it
seems to me that hclust is also invoked by heatmap.

> heatmap
function (x, Rowv = NULL, Colv = if (symm) "Rowv" else NULL, 
    distfun = dist, hclustfun = hclust, add.expr, symm = FALSE,
...

hcr <- hclustfun(distfun(x))
            ddr <- as.dendrogram(hcr)

....
hcc <- hclustfun(distfun(if (symm) 
                x
            else t(x)))
            ddc <- as.dendrogram(hcc)


I understand it is possible to add Rowv=NA and order the samples as
per hclust, but I'm just wondering if there is a reason for this
observation. Any pointers would be very much appreciated.

Thanks!

Min-Han Tan



From ccleland at optonline.net  Wed Dec 15 22:04:54 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 15 Dec 2004 16:04:54 -0500
Subject: [R] which
In-Reply-To: <20041215201431.75426.qmail@web54503.mail.yahoo.com>
References: <20041215201431.75426.qmail@web54503.mail.yahoo.com>
Message-ID: <41C0A6F6.5050703@optonline.net>

 > which(x < 5 & x > 2)
[1] 3 4

 From the help page for Logical Operators:

?"!"

      '&' and '&&' indicate logical AND and '|' and '||' indicate
      logical OR.  The shorter form performs elementwise comparisons in
      much the same way as arithmetic operators.  The longer form
      evaluates left to right examining only the first element of each
      vector.  Evaluation proceeds only until the result is determined.
      The longer form is appropriate for programming control-flow and
      typically preferred in 'if' clauses.

Y. C. Tao wrote:
> Why the last "which" in the following example doesn't
> work? Is there a simple way to identify the indices of
> array elements that meet multiple criteria?
> 
> Thanks.
> 
> YC Tao
> 
> 
>>x<-1:10
>>which(x<5)
> 
> [1] 1 2 3 4
> 
>>which(x>2)
> 
> [1]  3  4  5  6  7  8  9 10
> 
>>which(x<5 && x>2)
> 
> numeric(0)

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From damian.cirelli at maine.edu  Wed Dec 15 22:31:02 2004
From: damian.cirelli at maine.edu (=?ISO-8859-1?Q?Dami=E1n_Cirelli?=)
Date: Wed, 15 Dec 2004 16:31:02 -0500
Subject: [R] TukeyHSD & Covariates
Message-ID: <41C0AD16.6030704@maine.edu>

Dear R gurus,
I have the following model:

appcov.aov <- aov(yield ~ prevyield + trt + block)

where prevyield is a continuous numeric covariate and trt and block are 
factors (yes, I did factor()!)
Now, when I do a TukeyHSD, my diff's are all screwed up!
For instance:
treatment mean for treatmen "E" is 277.25 and for treatment "O" is 
279.5, so I figure the diff O-E should be 2.25, but TukeyHSD says:

          diff         lwr        upr
O-E -50.817101 -84.8112057 -16.822996

So I wonder where is that -50.8 coming from???

Anybody have a clue?

Thanks a lot!

PS: it works if I take prevyield (the covariate) out of the model, but 
the point is I need to analyse it with the covariate.
Thanks again



From sdavis2 at mail.nih.gov  Wed Dec 15 22:46:58 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 15 Dec 2004 16:46:58 -0500
Subject: [R] hclust and heatmap - slightly different dendrograms?
References: <7902152a04121512556a2eba3a@mail.gmail.com>
Message-ID: <000a01c4e2ef$9a415310$7d75f345@WATSON>

Hierarchical clustering does NOT give an ordering for the clusters.  It only 
gives the clustering, so order is not invariant.

Sean
----- Original Message ----- 
From: "Min-Han Tan" <minhan.science at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, December 15, 2004 3:55 PM
Subject: [R] hclust and heatmap - slightly different dendrograms?


> Good afternoon,
>
> I ran heatmap and hclust on the same matrix x (strictly, I ran
> heatmap(x), and hclust(dist(t(x))), and realized that the two
> dendrograms were slightly different, in that the left-right
> arrangement of one pair of subclusters (columns) was reversed in the
> two functions (but all individual columns were grouped correctly).
>
> Looking through the code for heatmap as a most definite nonexpert, it
> seems to me that hclust is also invoked by heatmap.
>
>> heatmap
> function (x, Rowv = NULL, Colv = if (symm) "Rowv" else NULL,
>    distfun = dist, hclustfun = hclust, add.expr, symm = FALSE,
> ...
>
> hcr <- hclustfun(distfun(x))
>            ddr <- as.dendrogram(hcr)
>
> ....
> hcc <- hclustfun(distfun(if (symm)
>                x
>            else t(x)))
>            ddc <- as.dendrogram(hcc)
>
>
> I understand it is possible to add Rowv=NA and order the samples as
> per hclust, but I'm just wondering if there is a reason for this
> observation. Any pointers would be very much appreciated.
>
> Thanks!
>
> Min-Han Tan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Wed Dec 15 22:54:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Dec 2004 22:54:51 +0100
Subject: [R] TukeyHSD & Covariates
In-Reply-To: <41C0AD16.6030704@maine.edu>
References: <41C0AD16.6030704@maine.edu>
Message-ID: <x2sm67fcr8.fsf@biostat.ku.dk>

Dami??n Cirelli <damian.cirelli at maine.edu> writes:

> Dear R gurus,
> I have the following model:
> 
> appcov.aov <- aov(yield ~ prevyield + trt + block)
> 
> where prevyield is a continuous numeric covariate and trt and block
> are factors (yes, I did factor()!)
> Now, when I do a TukeyHSD, my diff's are all screwed up!
> For instance:
> treatment mean for treatmen "E" is 277.25 and for treatment "O" is
> 279.5, so I figure the diff O-E should be 2.25, but TukeyHSD says:
> 
>           diff         lwr        upr
> O-E -50.817101 -84.8112057 -16.822996
> 
> So I wonder where is that -50.8 coming from???
> 
> Anybody have a clue?
> 
> Thanks a lot!
> 
> PS: it works if I take prevyield (the covariate) out of the model, but
> the point is I need to analyse it with the covariate.
> Thanks again

If the covariate level differs between the treatment groups, then the
difference in the covariate-adjusted means could well differ quite a
bit from the unadjusted difference. What happens if you do

summary(lm(yield ~ prevyield + trt + block))

(Not sure I'm happy about using the HSD procedure with an unbalanced
design, btw.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From damian.cirelli at maine.edu  Wed Dec 15 23:02:08 2004
From: damian.cirelli at maine.edu (=?ISO-8859-1?Q?Dami=E1n_Cirelli?=)
Date: Wed, 15 Dec 2004 17:02:08 -0500
Subject: [R] TukeyHSD & Covariates
In-Reply-To: <x2sm67fcr8.fsf@biostat.ku.dk>
References: <41C0AD16.6030704@maine.edu> <x2sm67fcr8.fsf@biostat.ku.dk>
Message-ID: <41C0B460.7000006@maine.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041215/0d484e25/attachment.pl

From damian.cirelli at maine.edu  Wed Dec 15 23:15:20 2004
From: damian.cirelli at maine.edu (=?ISO-8859-1?Q?Dami=E1n_Cirelli?=)
Date: Wed, 15 Dec 2004 17:15:20 -0500
Subject: [R] TukeyHSD & Covariates
In-Reply-To: <x2sm67fcr8.fsf@biostat.ku.dk>
References: <41C0AD16.6030704@maine.edu> <x2sm67fcr8.fsf@biostat.ku.dk>
Message-ID: <41C0B778.5040807@maine.edu>

I have interactions where I shouldn't so nevermind, I'm a dumb ass.

Thanks again



From jens_hainmueller at ksg05.harvard.edu  Thu Dec 16 01:50:48 2004
From: jens_hainmueller at ksg05.harvard.edu (Jens Hainmueller)
Date: Wed, 15 Dec 2004 19:50:48 -0500
Subject: [R] help with multiple imputation using imp.mix
Message-ID: <HCECJPLNNGBBJIOJMJJAMEMPCNAA.jens_hainmueller@ksg05.harvard.edu>

I am desperately trying to impute missing data using 'imp.mix' but always
run into this yucky error message to which I cannot find the solution. It's
the first time I am using mix and I'm trying really hard to understand, but
there's just this one step I don't get...perhaps someone knows the answer?

Thanks!
Jens

My code runs:

data<-read.table('http://www.courses.fas.harvard.edu/~gov2001/Data/immigrati
on.dat',header=TRUE)
library(mix)
rngseed(12345678)
# Preare data for imputation
gender1<-c()
 gender1<-as.integer(data$gender)
 gender1[gender1==1]<-2
 gender1[gender1==0]<-1
 data$gender<-gender1
x<-cbind(data$gender,data$ipip,data$ideol,data$prtyid, data$wage1992)
colnames(x)<-c("gender","ipip", "ideol", "prtyid","wage")
# start imputation
s <- prelim.mix(x,4)
thetahat <- em.mix(s)

And here comes the error message:

> newtheta <- da.mix(s,thetahat, steps=100,showits=TRUE)
Steps of Data Augmentation:
1...Error in da.mix(s, thetahat, steps = 100, showits = TRUE) :
        Improper posterior--empty cells
> imp.mix(s, newtheta, x)



From dongyuan.xu at huiway.com  Thu Dec 16 03:12:45 2004
From: dongyuan.xu at huiway.com (xudongyuan)
Date: Thu, 16 Dec 2004 10:12:45 +0800
Subject: [R] how R outputs?
Message-ID: <PHEAIHAGJCLIMJMBJFLOKELLCAAA.dongyuan.xu@huiway.com>

Hi.All and R developers:
	When I look into the R source code, I have a question.Since R has its own data structure(i.e. SEXP),how does it convert the result to the normal output after it has computed? For example,when I input,
>abs(-3)
I learned that in R's execution, the expression is parsed to a parse tree,and becomes a SEXP list. After "eval" function, the result is still a SEXP. But R outputs:
[1] 3
The output is normal.So my question is how R makes its SEXP result into the normal result.Where can I find the place R makes this convertion in R's source code?Can anyone help me?
		thanks
																dongyuan xu



From corey.bradshaw at cdu.edu.au  Thu Dec 16 04:40:42 2004
From: corey.bradshaw at cdu.edu.au (Corey Bradshaw)
Date: Thu, 16 Dec 2004 13:10:42 +0930
Subject: [R] RE: adding perspectives to existing persp plots
Message-ID: <65EB8C57DA514942B1EC55E8DA89FE550151EACB@mail.site.ntu.edu.au>


Hi there,

Turns out it was a simply 'par' problem

I just needed to establish par(new = TRUE) after the first 'persp' plot
and prior to coding the 2nd 'persp' plot.

Thanks for your suggestions.

Corey


-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
Sent: Thursday, December 16, 2004 12:14 AM
To: r-help at stat.math.ethz.ch
Cc: Corey Bradshaw
Subject: Re: [R] RE: adding perspectives to existing persp plots

On Wednesday 15 December 2004 01:10, Corey Bradshaw wrote:
> Thanks, Romain. I've certainly used that to draw lines and points in
> the plots produced by 'persp'; however, my problem is that I need to
> incorporate an entirely new z function (not just a plane) onto the
> same plot (i.e., using the same x and y values).

If the surfaces are non-intersecting, you might be able to use 
'wireframe' from the lattice package. See the second example 
in ?wireframe.

Deepayan



From vasileios_p at yahoo.gr  Thu Dec 16 09:29:51 2004
From: vasileios_p at yahoo.gr (vasilis pappas)
Date: Thu, 16 Dec 2004 08:29:51 +0000 (GMT)
Subject: [R] 3 questions
Message-ID: <20041216082951.53553.qmail@web51409.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041216/8121d66a/attachment.pl

From dethlef at math.aau.dk  Thu Dec 16 09:33:46 2004
From: dethlef at math.aau.dk (Claus Dethlefsen)
Date: Thu, 16 Dec 2004 09:33:46 +0100
Subject: [R] Advice on parsing formulae
In-Reply-To: <s1c014c6.095@liberator.csv.warwick.ac.uk>
Message-ID: <002f01c4e349$f5a00800$9230e182@peano>

Thank you for the advice. I have now boiled my problem down to the
following:

How do I create fm2 from fm1 ?

fm1 <-  Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))
fm2 <-  Y ~ 1 + x:A + z + u + B + poly(v, 3)

Thus, how do I simply remove tvar( * ) from a formula? Do I have to write a
function to parse the string and then re-create the formula? Is there an
easy way of doing this?


When my above problem is solved, I can (with the help from Heather Turner
and Chuck Berry) do the following

## define som data
x <- z <- u <- v <- rnorm(5)
A <- B <- factor( rep( c(1,2), c(3,2) ) )

## define my formula fm1 and manually create fm2.
fm1 <-  Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))
fm2 <-  Y ~ 1 + x:A + z + u + B + poly(v, 3)

## extract the term.labels from fm2, make the design matrix and extract
'assign'
term.labels <- unname(sapply(attr(terms(fm2), "term.labels"),
                             removeSpace))
X <- model.matrix(fm2,keep.order=TRUE)
pAssign <- attr(X, "assign")

## Now, extract the tvar-terms from fm1
tvar.terms <- terms( fm1, specials = "tvar",keep.order=TRUE )
idx <- attr(tvar.terms,"specials")$tvar
if (attr(tvar.terms,"intercept")) idx <- idx -1
tvar <- attr(terms(fm2,keep.order=TRUE),"term.labels")[idx]
tvar <- unname( sapply( tvar, removeSpace) )

## Finally, combine the information to get the vector I asked for
tvarAssign <- match(pAssign, sort(match(tvar, term.labels)))
tvarAssign[is.na(tvarAssign)] <- 0

 

> -----Original Message-----
> From: Heather Turner [mailto:Heather.Turner at warwick.ac.uk] 
> Sent: 15. december 2004 11:41
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Advice on parsing formulae
> 
> 
> I think this will do what you want:
> 
> # Need this function to remove spaces from term labels later on
> > removeSpace <-  function(string) gsub("[[:space:]]", "", string)
> 
> # Specify which terms are in a "tvar" group
> # (could remove spaces separately)
> > tvar <- unname(sapply(c("x:A", "z", "B", "poly(v,3)"), removeSpace))
> 
> # Use terms to get term labels from formula
> > formula <- Y ~ 1 + x:A + z + u + B + poly(v,3)
> > term.labels <- unname(sapply(attr(terms(formula), 
> "term.labels"), removeSpace))
> > tvar
> [1] "x:A"       "z"         "B"         "poly(v,3)"
> > term.labels
> [1] "z"         "u"         "B"         "poly(v,3)" "x:A"
> 
> # Get assign variable for parameters
> # (You would use first two lines, but I don't have data so 
> defined assign variable myself)
> > #X <- model.matrix(formula)
> > #pAssign <- attr(X, "assign")
> > pAssign <- c(0,1,2,3,4,4,4,5,5)  
> 
> # Define "tvarAssign"
> > tvarAssign <- match(pAssign, sort(match(tvar, term.labels)))
> > tvarAssign[is.na(tvarAssign)] <- 0
> > tvarAssign
> [1] 0 1 0 2 3 3 3 4 4
> 
> HTH
> 
> Heather
> 
> Mrs H Turner
> Research Assistant
> Dept. of Statistics
> University of Warwick
> 
> >>> "Claus Dethlefsen" <dethlef at math.aau.dk> 12/13/04 04:10pm >>>
> Dear list
> 
> I would like to be able to group terms in a formula using a 
> function that I
> will call tvar(), eg. the formula
> 
> Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))
> 
> where x,u and v are numeric and A and B are factors - binary, say.
> 
> As output, I want the model.matrix as if tvar had not been 
> there at all. In
> addition, I would like to have information on the grouping, 
> as a vector as
> long as ncol( model.matrix ) with zeros corresponding to 
> terms outside tvar
> and with an index grouping the terms inside each tvar(). In the (sick)
> example:
> 
> 
> > model.matrix(Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + 
> tvar(poly(v,3)))
>    (Intercept)     z     u B2 poly(v, 3)1 poly(v, 3)2 poly(v, 
> 3)3  x:A1
> x:A2
> 1            1 -1.55 -1.03  0       0.160      -0.350      
> -0.281  0.66
> 0.00
> 2            1 -1.08  0.55  0      -0.164      -0.211       
> 0.340  0.91
> 0.00
> 3            1  0.29 -0.26  0      -0.236      -0.073       
> 0.311 -1.93
> 0.00
> 4            1 -1.11  0.96  0       0.222      -0.285      
> -0.385 -0.23
> 0.00
> 5            1  0.43 -0.76  1      -0.434       0.515      
> -0.532  0.22
> 0.00
> 
> I would like the vector
> 
> c(0,1,0,2,3,3,3,4,4)
> 
> pointing to the tvar-grouped terms.
> 
> Thus what I would like, looks a bit like the 'assign' attribute of the
> model.matrix() output. I have not figured out a way of doing 
> this in a nice
> way and would like some help, please.
> 
> I hope somebody can help me (or point the manual-pages I should read),
> 
> Best, 
> 
> Claus Dethlefsen
> --------------------------------------------------------------
> Assistant Professor, Claus Dethlefsen, Ph.D.
> mailto:dethlef at math.auc.dk, http://www.math.auc.dk/~dethlef 
> Dpt. of Mathematical Sciences, Aalborg University
> Fr. Bajers Vej 7G, 9220 Aalborg East
> Denmark
> 

-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From Uwe.Ligges at R-project.org  Thu Dec 16 08:41:25 2004
From: Uwe.Ligges at R-project.org (Uwe Ligges)
Date: Thu, 16 Dec 2004 08:41:25 +0100
Subject: [R] [R-pkgs] Building of Windows binary packages for R-1.9.x has
	been stopped
Message-ID: <41C13C25.9000009@R-project.org>

Dear useRs,

automatical building of contributed Windows binary packages for R-1.9.x
has been stopped.
The last known good versions of contributed packages are available in
the repository: These may be outdated versions, because new versions of
many packages already require R>=2.0.0.

Upgrading to R-2.0.1 is recommended.

Uwe Ligges

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From tura at centroin.com.br  Thu Dec 16 10:00:50 2004
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Thu, 16 Dec 2004 07:00:50 -0200
Subject: [R] AUC for logistic regression [was: (no subject)]
In-Reply-To: <20041215222143.90395.qmail@web51807.mail.yahoo.com>
References: <6.1.2.0.2.20041215182424.04c2a070@centroin.com.br>
	<20041215222143.90395.qmail@web51807.mail.yahoo.com>
Message-ID: <6.1.2.0.2.20041216065102.04c114c0@centroin.com.br>

At 20:21 15/12/2004, Kerry Bush wrote:

>Your formula is only a trapzoidal approximation to the
>theoretical AUC. It might be downward biased. I am
>also wondering if there is a funcion in R that can
>compute the MLE estimate for AUC.

Kerry I agree with you. But I dont Know compute the MLE estimate for AUC 
and its standatr error.
Second I dont know MLE(AUC)/SE is normal so I dont konw wich test use.
Third I dont konw compute the correction is necessary because the test in 
same population.

If You explain me this topics I upgrade the funciotn with pleasure.

But same with a possible downward bias the function solve Roman problem...

Thanks in advance

Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil 


-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From p.dalgaard at biostat.ku.dk  Thu Dec 16 10:22:27 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Dec 2004 10:22:27 +0100
Subject: [R] Advice on parsing formulae
In-Reply-To: <002f01c4e349$f5a00800$9230e182@peano>
References: <002f01c4e349$f5a00800$9230e182@peano>
Message-ID: <x2k6rifvho.fsf@biostat.ku.dk>

"Claus Dethlefsen" <dethlef at math.aau.dk> writes:

> Thank you for the advice. I have now boiled my problem down to the
> following:
> 
> How do I create fm2 from fm1 ?
> 
> fm1 <-  Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))
> fm2 <-  Y ~ 1 + x:A + z + u + B + poly(v, 3)
> 
> Thus, how do I simply remove tvar( * ) from a formula? Do I have to write a
> function to parse the string and then re-create the formula? 

Eek, no!!

> Is there an
> easy way of doing this?

Not really. Recursively descend the parse tree and replace calls to
tvar with its argument. Probably ends with one of those 10-line
functions that take hours to get right... Something like this would go
in the middle

if (is.call(e))
  if (e[[1]]==as.name("tvar")) 
    e[[2]]
  else
    for (i in 2:length(e)) 
       e[[i]] <- myfun(e[[i]])
else
  e

If you don't mind getting left with a couple of extra parentheses,
this seems to work:

> eval(substitute(substitute(fm1,list(tvar=as.name("("))),list(fm1=fm1)))
Y ~ 1 + (x:A) + (z) + u + (B) + (poly(v, 3))

(Note to self: we need a substitute() variant that takes a variable,
not a literal as the first argument.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Philippe.Grosjean at umh.ac.be  Thu Dec 16 10:43:51 2004
From: Philippe.Grosjean at umh.ac.be (Philippe Grosjean)
Date: Thu, 16 Dec 2004 10:43:51 +0100
Subject: [R] Detecting incomplete commands
Message-ID: <200412160946.iBG9kmIA1228854@hedwig1.umh.ac.be>

Hello,

I need a similar behaviour as with the prompt: asking to complete incomplete
R command with eval(parse(text = ....))
Is it a way to make the difference between an illegal and an incomplete R
command in a string?
For instance:

> parse(text="ls()")
expression(ls())

This is fine!

> parse(text="ls(")
Error in parse(file, n, text, prompt) : parse error

This is an incomplete command, and I want to ask to the user for further
input (multiline command)

> parse(text="ls())")
Error in parse(file, n, text, prompt) : parse error

This is clearly an illegal command. Could I make a distinction with the
previous case here?
Best,

Philippe Grosjean

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    web:   http://www.umh.ac.be/~econum
 ) ) ) ) )
..............................................................



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec 16 10:51:42 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 16 Dec 2004 09:51:42 -0000 (GMT)
Subject: [R] help with multiple imputation using imp.mix
In-Reply-To: <HCECJPLNNGBBJIOJMJJAMEMPCNAA.jens_hainmueller@ksg05.harvard.edu>
Message-ID: <XFMail.041216095142.Ted.Harding@nessie.mcc.ac.uk>

Hi Jens,

On 16-Dec-04 Jens Hainmueller wrote:
> I am desperately trying to impute missing data using
> 'imp.mix' but always run into this yucky error message
> to which I cannot find the solution.
> It's the first time I am using mix and I'm trying really
> hard to understand, but there's just this one step I don't
> get...perhaps someone knows the answer?
> 
> Thanks!
> Jens
> 
> My code runs:
> 
> data<-read.table('http://www.courses.fas.harvard.edu/~gov2001/Data/immig
> rati
> on.dat',header=TRUE)
> library(mix)
> rngseed(12345678)
># Preare data for imputation
> gender1<-c()
>  gender1<-as.integer(data$gender)
>  gender1[gender1==1]<-2
>  gender1[gender1==0]<-1
>  data$gender<-gender1
> x<-cbind(data$gender,data$ipip,data$ideol,data$prtyid, data$wage1992)
> colnames(x)<-c("gender","ipip", "ideol", "prtyid","wage")
># start imputation
> s <- prelim.mix(x,4)
> thetahat <- em.mix(s)
> 
> And here comes the error message:
> 
>> newtheta <- da.mix(s,thetahat, steps=100,showits=TRUE)
> Steps of Data Augmentation:
> 1...Error in da.mix(s, thetahat, steps = 100, showits = TRUE) :
>         Improper posterior--empty cells
>> imp.mix(s, newtheta, x)

This is my first shot, basically somewhat of a guess since
I don;t have details of your data.

It looks as though you have categorical variables
  "gender","ipip", "ideol", "prtyid"
(at least I hope so -- 'mix' requires you to put all the
categoricals first) and one "continuous" variable "wage".
Am I correct? (specifically for "ipip" whose nature I can't
guess, while I can for the others).

The thing to note is that, by default, 'mix' will create
category cells using all possible factorial combinations of
the levels of your categorical variables.
So you could end up with a large number of category cells.
E.g. if there are 2 levels for "gender", 4 for "ipip",
5 for "ideol" and 6 for "partyid", then 'mix' will create
2x4x5x6 = 240 distinct category cells of data, and will fit
a separate mean (or multivariate mean, depending on how
many continuous variables you have) for each category cell,
and a common variance (or covariance matrix) for all such
cells. It will also estimate the multinomial distribution
over the (e.g. 240) category cells. This is the "unrestricted
model" corresponding to all possible degrees of interaction
between the categoricals, and is what is adopted when (as
you did) you use 'em.mix' followed by 'da.mix'.

Then, when it comes to imputation, it puts a Dirichlet prior
on the multinomial for category cells and a multivariate normal
prior on the vector means and a Wishart prior on the covariance
matrix for the MV normal distribution of the continuous
variables. Then it samples from the joint multinomial x multivariate
distribution for the observations which has been randomly
chosen according to these priors.

Now, if it happens that because of the large number of category
cells there are several of these empty in your data, then the
above process can fail leading to such error messages.

One way round this is to restrict the categorical model, so as
to decrease the degrees of interaction between the categoricals.
To do this, you use 'ecm.mix' followed by 'dabipf.mix', using
the parameters "design" and "margins" to specify your restricted
model.

NB It can be tricky to get this right!

You could experiment to see if the "empty cell" problem described
above is what is causing your problems, by trying imputation
using fewer categorical variables (e.g. 2 at a time instead of
all 4) with the simple 'em.mix' and 'da.mix' before tangling
with the more complicated issues arising from 'ecm.mix' and
'dabipf.mix'. The results of this may not be definitive, but
could be useful in locating where the problem lies.

I hope this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 16-Dec-04                                       Time: 09:51:42
------------------------------ XFMail ------------------------------



From dieter.menne at menne-biomed.de  Thu Dec 16 11:25:04 2004
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 16 Dec 2004 10:25:04 +0000 (UTC)
Subject: [R] 3 questions
References: <20041216082951.53553.qmail@web51409.mail.yahoo.com>
Message-ID: <loom.20041216T112243-600@post.gmane.org>

 
> 1) I have constracted a function that returns an output, which runs in a while
( condition ){ run function }
> loop. I would like to know if there is a way to get the outputs in different 
windows, every time the function
> runs, so as to compare easier the results.

Try windows(). 
But better don't use it, R has much better tools to compare outputs; trellis' 
plots (?lattice) main use is to make nicely comparable output.

Dieter



From ramasamy at cancer.org.uk  Thu Dec 16 12:33:31 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 16 Dec 2004 11:33:31 +0000
Subject: [R] 3 questions
In-Reply-To: <20041216082951.53553.qmail@web51409.mail.yahoo.com>
References: <20041216082951.53553.qmail@web51409.mail.yahoo.com>
Message-ID: <1103196811.3228.42.camel@ndmpc126.orc.ox.ac.uk>

Please learn to wrap your emails at about 72 characters. 

See below for other comments.

On Thu, 2004-12-16 at 08:29, vasilis pappas wrote:
> Hello R users,
>    
>    I have three questions and I would be grateful if someone could give me an answer to each of these.
>  
> 1) I have constracted a function that returns an output, which runs in a while( condition ){ run function } loop. I would like to know if there is a way to get the outputs in different windows, every time the function runs, so as to compare easier the results.

This depends on what you mean by 'output'. If you mean graphs, then you
can use x11() if you have X Window systems or windows() if you are  in
Windows OS.

 windows()
 plot(1:10)
 windows()
 plot(rnorm(1000))

Or you could use the mfrow option in par() to split the plotting window

 par(mfrow=c(1,2)
 plot(1:10)
 plot(rnorm(1000))

If your output is not graphical, then you can save to a file and compare
them or save it to a list/matrix in R.

> 2) In my function I make use of the Rcmdr package. But every time I use my  function and the Rcmdr package loads, I get the R-commander window and I have to delete it every time. So I wonder if there is a way to avoid R-commander window every time I load the Rcmdr package.
>  
> 3) My final question is if there is a function in R that computes the Mallow's Cp statistic. I have already found AIC and BIC but not Mallow's Cp. Furthermore I found leaps() function but it is not exactly what I want.

There is a function called mle.cp in the package wle. Not sure if this
does what you want.

> Thank you for you attention.I will looking forward for some answers.
>  
> 
> 
> 
> ---------------------------------
> 
>    @yahoo.gr
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kjetil at acelerate.com  Thu Dec 16 12:39:36 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 16 Dec 2004 07:39:36 -0400
Subject: [R] R: optimisation
In-Reply-To: <41C05CD3.10109@pburns.seanet.com>
References: <41C04ED3.9C90BD4F@STATS.uct.ac.za>
	<41C05CD3.10109@pburns.seanet.com>
Message-ID: <41C173F8.6040709@acelerate.com>

Patrick Burns wrote:

> You don't say what sort of optimisation you have in mind.  If you
> are looking for something that will handle "non-standard" problems,
> you can have a look at 'genopt' from S Poetry.
>
Some CRAN packages:
gafit
seao, seao.gui
lpSolve
linprog
minpack.lm
rgenoud
(at least)

Kjetil Halvorsen


>
> Patrick Burns
>
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Clark Allan wrote:
>
>> hi all
>>
>> other than optim, optimise, and some other related optimisation
>> functions are there any optimisation packages in R?
>>
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From S.G.Pickering at bath.ac.uk  Thu Dec 16 12:54:39 2004
From: S.G.Pickering at bath.ac.uk (Simon Pickering)
Date: Thu, 16 Dec 2004 11:54:39 -0000
Subject: [R] Lazy-loading db setup in the R build process
Message-ID: <200412161154.aa24428@bath.ac.uk>

Hi All,

I have read the article on lazy-loading in the September R news letter, and
think I have at least a vague grasp on what is happening.

Am I right in thinking that, assuming I were using the same packages, I
could copy the .rdb & .rdx files from one installation of R (2.0.0) to
another? 

I ask this as I'm trying to cross-compile R (for ARM), and need to use R
itself to perform the lazy-loading db setup (and probably other things) as
part of the build process. Therefore I build a native version of R (x86) and
pass the path of the native R binary as R_EXE in the arm cross-build
Makefiles (in src/library and its sub-directories).

This fails with the following error:

| make[3]: Entering directory
`/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library/base'
| building package 'base'
| mkdir -p -- ../../../library/base/demo mkdir -p -- 
| ../../../library/base/man Error in eval(expr, envir, enclos) : may 
| already be using lazy loading on base Execution halted
| make[3]: *** [all] Error 1
| make[3]: Leaving directory
`/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library/base'
| make[2]: *** [R] Error 1
| make[2]: Leaving directory
`/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library'
| make[1]: *** [R] Error 1
| make[1]: Leaving directory
`/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src'
| make: *** [R] Error 1

My guess is that as the native version has already been built (and enabled
lazy-loading, etc.), it's not happy that I'm trying to enable it again.
Might this be the case?

If so I can see a couple of possible options and I wonder if someone could
comment on what may or may not be possible to remedy the problem:

* Run the native binary without loading its various database files (is this
is possible)

* Delete the native database files so they can't be used and will be rebuilt
for the arm version without complaint

* Simply copy the database files from the native build to the appropriate
locations in the cross-build (after patching the Makefiles to remove the
references to R_EXE), assuming that they should be portable across
architectures?

Many thanks,



Simon

----------------------------------------
Simon Pickering MEng
Research Officer
Materials Research Centre
Faculty of Engineering & Design
University of Bath
Bath, BA2 7AY, UK

Tel: +44 (0)1225 384802
Fax: +44 (0)1225 386928



From stefan.sobernig at wu-wien.ac.at  Thu Dec 16 12:59:06 2004
From: stefan.sobernig at wu-wien.ac.at (Stefan Sobernig)
Date: Thu, 16 Dec 2004 12:59:06 +0100
Subject: [R] PL/R calls fail 
Message-ID: <6.1.2.0.2.20041216125901.01a0c368@isis.wu-wien.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041216/a32af974/attachment.pl

From michael.watson at bbsrc.ac.uk  Thu Dec 16 13:32:08 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 16 Dec 2004 12:32:08 -0000
Subject: [R] Incorrect permissions to edit database package
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89A08@iahce2knas1.iah.bbsrc.reserved>

Hi

I am running R 2.0 on Suse Linux 8.2.  I get an error message when
loading a library:

"Incorrect permissions to edit the package database,
/usr/lib/R/library/liblisting.Rda: save.locLib(locLibList, curLib)"

However, when I look at that file, the user I am running R as has r and
w permissions on it....

So firstly - any ideas why I get this error message?  And secondly, does
it matter that I am getting it?  I don't get it if I run R as root
(obviously) but I don't really want to be doing that (obviously)

Cheers
Mick



From p.dalgaard at biostat.ku.dk  Thu Dec 16 13:58:01 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Dec 2004 13:58:01 +0100
Subject: [R] Lazy-loading db setup in the R build process
In-Reply-To: <200412161154.aa24428@bath.ac.uk>
References: <200412161154.aa24428@bath.ac.uk>
Message-ID: <x23by6flie.fsf@biostat.ku.dk>

Simon Pickering <S.G.Pickering at bath.ac.uk> writes:

> Hi All,
> 
> I have read the article on lazy-loading in the September R news letter, and
> think I have at least a vague grasp on what is happening.
> 
> Am I right in thinking that, assuming I were using the same packages, I
> could copy the .rdb & .rdx files from one installation of R (2.0.0) to
> another? 
> 
> I ask this as I'm trying to cross-compile R (for ARM), and need to use R
> itself to perform the lazy-loading db setup (and probably other things) as
> part of the build process. Therefore I build a native version of R (x86) and
> pass the path of the native R binary as R_EXE in the arm cross-build
> Makefiles (in src/library and its sub-directories).
> 
> This fails with the following error:
> 
> | make[3]: Entering directory
> `/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library/base'
> | building package 'base'
> | mkdir -p -- ../../../library/base/demo mkdir -p -- 
> | ../../../library/base/man Error in eval(expr, envir, enclos) : may 
> | already be using lazy loading on base Execution halted
> | make[3]: *** [all] Error 1
> | make[3]: Leaving directory
> `/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library/base'
> | make[2]: *** [R] Error 1
> | make[2]: Leaving directory
> `/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library'
> | make[1]: *** [R] Error 1
> | make[1]: Leaving directory
> `/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src'
> | make: *** [R] Error 1
> 
> My guess is that as the native version has already been built (and enabled
> lazy-loading, etc.), it's not happy that I'm trying to enable it again.
> Might this be the case?
> 
> If so I can see a couple of possible options and I wonder if someone could
> comment on what may or may not be possible to remedy the problem:
> 
> * Run the native binary without loading its various database files (is this
> is possible)
> 
> * Delete the native database files so they can't be used and will be rebuilt
> for the arm version without complaint
> 
> * Simply copy the database files from the native build to the appropriate
> locations in the cross-build (after patching the Makefiles to remove the
> references to R_EXE), assuming that they should be portable across
> architectures?

The last one might work (try it and see). A little experiment,
replacing Opteron base.rdb/rdx files with i386 counterparts indicated
no immediate (!) ill effects.

The (first) thing that seems to bite your cross compile is in
src/library/base/makebasedb.R:

    baseFileBase <- file.path(.Library,"base","R","base")

    if (file.info(baseFileBase)["size"] < 20000) # crude heuristic
        stop("may already be using lazy loading on base");

in which .Library refers to the running R process, and you probably
want the cross-build library instead. I'm not sure that is the only
place that needs changing though. (It *looks* like the tools for
non-base packages take an argument that would allow the build library
to be distinct from the the one used by the running R).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jonathan.white at oeaw.ac.at  Thu Dec 16 14:18:07 2004
From: jonathan.white at oeaw.ac.at (Jonathan White)
Date: Thu, 16 Dec 2004 14:18:07 +0100
Subject: [R] (D)COM "unknown (internal) error"
Message-ID: <41C18B0F.4050407@oeaw.ac.at>

Hello All,

I'm getting the error message "unknown (internal) error" when evaluating 
the following string

    source('j:/tmp/test.r')

via the (D)COM server. The script test.r is run and everything works, 
but Evaluate still returns this error. I can avoid the error by using 
EvaluateNoReturn, but would prefer to use Evaluate. I'm running R 2.0.1 
and (D)COM 1.35 on WindowsXP.

I've tested it in our own DCOM implementation and using the RExcel 
interface, and get the same error message in both cases.

Thanks in advance for any input

Jonnie White



From Heather.Turner at warwick.ac.uk  Thu Dec 16 14:23:17 2004
From: Heather.Turner at warwick.ac.uk (Heather Turner)
Date: Thu, 16 Dec 2004 13:23:17 +0000
Subject: [R] Advice on parsing formulae
Message-ID: <s1c18c53.015@liberator.csv.warwick.ac.uk>

Okay, well you could define a tvar function as follows
tvar <- function(term) term

Then stick to fm1
X <- model.matrix(fm1,keep.order=TRUE)
pAssign <- attr(X, "assign")

tvar.terms <- terms( fm1, specials = "tvar",keep.order=TRUE )
idx <- attr(tvar.terms,"specials")$tvar
if (attr(tvar.terms,"response")) idx <- idx -1
all.labels <- attr(tvar.terms, "term.labels")
tvar.labels <- all.labels[idx]
tvarAssign <- match(pAssign, match(tvar.labels, all.labels))
tvarAssign[is.na(tvarAssign)] <- 0

I think that the specials attributes is an index of the variables attribute, hence I have replaced
if (attr(tvar.terms,"intercept")) idx <- idx -1
with
if (attr(tvar.terms,"response")) idx <- idx -1
check e.g.
fm1 <-  Y ~  tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))
fm1 <-  ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))

Heather

From: 	"Claus Dethlefsen" <dethlef at math.aau.dk>
To:	"'Heather Turner'" <Heather.Turner at warwick.ac.uk>, <r-help at stat.math.ethz.ch>
Date: 	12/16/04 8:33am
Subject: 	RE: [R] Advice on parsing formulae

Thank you for the advice. I have now boiled my problem down to the
following:

How do I create fm2 from fm1 ?

fm1 <-  Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))
fm2 <-  Y ~ 1 + x:A + z + u + B + poly(v, 3)

Thus, how do I simply remove tvar( * ) from a formula? Do I have to write a
function to parse the string and then re-create the formula? Is there an
easy way of doing this?


When my above problem is solved, I can (with the help from Heather Turner
and Chuck Berry) do the following

## define som data
x <- z <- u <- v <- rnorm(5)
A <- B <- factor( rep( c(1,2), c(3,2) ) )

## define my formula fm1 and manually create fm2.
fm1 <-  Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))
fm2 <-  Y ~ 1 + x:A + z + u + B + poly(v, 3)

## extract the term.labels from fm2, make the design matrix and extract
'assign'
term.labels <- unname(sapply(attr(terms(fm2), "term.labels"),
                             removeSpace))
X <- model.matrix(fm2,keep.order=TRUE)
pAssign <- attr(X, "assign")

## Now, extract the tvar-terms from fm1
tvar.terms <- terms( fm1, specials = "tvar",keep.order=TRUE )
idx <- attr(tvar.terms,"specials")$tvar
if (attr(tvar.terms,"intercept")) idx <- idx -1
tvar <- attr(terms(fm2,keep.order=TRUE),"term.labels")[idx]
tvar <- unname( sapply( tvar, removeSpace) )

## Finally, combine the information to get the vector I asked for
tvarAssign <- match(pAssign, sort(match(tvar, term.labels)))
tvarAssign[is.na(tvarAssign)] <- 0

Mrs H Turner
Research Assistant
Dept. of Statistics
University of Warwick



From ripley at stats.ox.ac.uk  Thu Dec 16 14:31:58 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Thu, 16 Dec 2004 13:31:58 +0000 (GMT)
Subject: [R] Lazy-loading db setup in the R build process
In-Reply-To: <x23by6flie.fsf@biostat.ku.dk>
Message-ID: <Pine.GSO.4.31.0412161328250.25306-100000@markov.stats>

On 16 Dec 2004, Peter Dalgaard wrote:

> Simon Pickering <S.G.Pickering at bath.ac.uk> writes:
>
> > Hi All,
> >
> > I have read the article on lazy-loading in the September R news letter, and
> > think I have at least a vague grasp on what is happening.
> >
> > Am I right in thinking that, assuming I were using the same packages, I
> > could copy the .rdb & .rdx files from one installation of R (2.0.0) to
> > another?
> >
> > I ask this as I'm trying to cross-compile R (for ARM), and need to use R
> > itself to perform the lazy-loading db setup (and probably other things) as
> > part of the build process. Therefore I build a native version of R (x86) and
> > pass the path of the native R binary as R_EXE in the arm cross-build
> > Makefiles (in src/library and its sub-directories).
> >
> > This fails with the following error:
> >
> > | make[3]: Entering directory
> > `/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library/base'
> > | building package 'base'
> > | mkdir -p -- ../../../library/base/demo mkdir -p --
> > | ../../../library/base/man Error in eval(expr, envir, enclos) : may
> > | already be using lazy loading on base Execution halted
> > | make[3]: *** [all] Error 1
> > | make[3]: Leaving directory
> > `/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library/base'
> > | make[2]: *** [R] Error 1
> > | make[2]: Leaving directory
> > `/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src/library'
> > | make[1]: *** [R] Error 1
> > | make[1]: Leaving directory
> > `/home/simon/dev/bk/build/tmp/work/r-2.0.0-r0/R-2.0.0/src'
> > | make: *** [R] Error 1
> >
> > My guess is that as the native version has already been built (and enabled
> > lazy-loading, etc.), it's not happy that I'm trying to enable it again.
> > Might this be the case?
> >
> > If so I can see a couple of possible options and I wonder if someone could
> > comment on what may or may not be possible to remedy the problem:
> >
> > * Run the native binary without loading its various database files (is this
> > is possible)

No.

> > * Delete the native database files so they can't be used and will be rebuilt
> > for the arm version without complaint

No.

> > * Simply copy the database files from the native build to the appropriate
> > locations in the cross-build (after patching the Makefiles to remove the
> > references to R_EXE), assuming that they should be portable across
> > architectures?
>
> The last one might work (try it and see). A little experiment,
> replacing Opteron base.rdb/rdx files with i386 counterparts indicated
> no immediate (!) ill effects.

That should certainly work: I would expect them to be identical for most
packages.

> The (first) thing that seems to bite your cross compile is in
> src/library/base/makebasedb.R:
>
>     baseFileBase <- file.path(.Library,"base","R","base")
>
>     if (file.info(baseFileBase)["size"] < 20000) # crude heuristic
>         stop("may already be using lazy loading on base");
>
> in which .Library refers to the running R process, and you probably
> want the cross-build library instead. I'm not sure that is the only
> place that needs changing though. (It *looks* like the tools for
> non-base packages take an argument that would allow the build library
> to be distinct from the the one used by the running R).

Take a look at cross-compiling (Windows under Linux).  We have failed to
make lazy loading of base work under cross-compilation, but do this for
all other packages.

I thought about endian differences (which I gather you have here), but I
don't think they matter as we use XDR format for the databases.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec 16 14:38:31 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Thu, 16 Dec 2004 13:38:31 +0000 (GMT)
Subject: [R] Incorrect permissions to edit database package
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A08@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.GSO.4.31.0412161333340.25306-100000@markov.stats>

On Thu, 16 Dec 2004, michael watson (IAH-C) wrote:

> I am running R 2.0 on Suse Linux 8.2.  I get an error message when
> loading a library:

Do you mean loading a *package* or a dynamic/shared library?  And which
one?

> "Incorrect permissions to edit the package database,
> /usr/lib/R/library/liblisting.Rda: save.locLib(locLibList, curLib)"
>
> However, when I look at that file, the user I am running R as has r and
> w permissions on it....
>
> So firstly - any ideas why I get this error message?  And secondly, does
> it matter that I am getting it?  I don't get it if I run R as root
> (obviously) but I don't really want to be doing that (obviously)

That file is not part of R, but from the BioC reposTools package.
Please ask about problems with BioC packages on the appropriate list.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Thu Dec 16 14:46:36 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 16 Dec 2004 08:46:36 -0500
Subject: [R] how R outputs?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E436@usrymx25.merck.com>

If R is run in interactive mode; i.e., interactive() == TRUE, whatever
function calls that produced R objects gets printed (by calling the
appropriate print() method; known as auto-printing), unless the object is
returned invisibly (i.e., wrapped in invisible()).  As an example, if you
put

abs(-3)

into a file, then run that through R CMD BATCH, you will not see the output
printed.  Also, auto-printing is off inside loops; e.g.,

for (I in 1:10) abs(-3)

will not show any output, even if you run it in an interactive session.

Andy

> From: xudongyuan
> 
> Hi.All and R developers:
> 	When I look into the R source code, I have a 
> question.Since R has its own data structure(i.e. SEXP),how 
> does it convert the result to the normal output after it has 
> computed? For example,when I input,
> >abs(-3)
> I learned that in R's execution, the expression is parsed to 
> a parse tree,and becomes a SEXP list. After "eval" function, 
> the result is still a SEXP. But R outputs:
> [1] 3
> The output is normal.So my question is how R makes its SEXP 
> result into the normal result.Where can I find the place R 
> makes this convertion in R's source code?Can anyone help me?
> 		thanks
> 								
> 								
> dongyuan xu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From f.harrell at vanderbilt.edu  Thu Dec 16 14:52:08 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 16 Dec 2004 07:52:08 -0600
Subject: [R] Advice on parsing formulae
In-Reply-To: <002f01c4e349$f5a00800$9230e182@peano>
References: <002f01c4e349$f5a00800$9230e182@peano>
Message-ID: <41C19308.8000202@vanderbilt.edu>

Claus Dethlefsen wrote:
> Thank you for the advice. I have now boiled my problem down to the
> following:
> 
> How do I create fm2 from fm1 ?
> 
> fm1 <-  Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))
> fm2 <-  Y ~ 1 + x:A + z + u + B + poly(v, 3)
> 
> Thus, how do I simply remove tvar( * ) from a formula? Do I have to write a
> function to parse the string and then re-create the formula? Is there an
> easy way of doing this?
  . . .

The Design package does manipulations of this sort to "understand" the 
model design to generate automatic tests of nonlinearity, pooled main 
effect + interaction tests, etc.  Related to your first issue, Design 
interprets a formula to find the "innermost" variable which is used on 
axes when plotting predicted values, and for other purposes.

In Function.Design you will see code like

TL <- attr(terms(object),"term.labels")
#Get inner transformations
from <- c('asis(*)','pol(*)','lsp(*)','rcs(*)','catg(*)','scored(*)',
   'strat(*)','matrx(*)','I(*)')
to   <- rep('*',9)

trans <- paste("h(",sedit(TL[ac!=9], from, to),")",sep="")
#change wrapping function to h()
h <- function(x,...) deparse(substitute(x))
for(i in (1:pm)) trans[i] <- eval(parse(text=trans[i]))

This may indirectly give you an idea, or you might see if the Design 
package does what you need in general [it doesn't support orthogonal 
polynomials, which I no longer find useful; it supports regular 
polynomials and regression splines].

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From p.dalgaard at biostat.ku.dk  Thu Dec 16 14:56:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Dec 2004 14:56:53 +0100
Subject: [R] Lazy-loading db setup in the R build process
In-Reply-To: <Pine.GSO.4.31.0412161328250.25306-100000@markov.stats>
References: <Pine.GSO.4.31.0412161328250.25306-100000@markov.stats>
Message-ID: <x2y8fye47u.fsf@biostat.ku.dk>

Brian D Ripley <ripley at stats.ox.ac.uk> writes:

> On 16 Dec 2004, Peter Dalgaard wrote:
> 
> > Simon Pickering <S.G.Pickering at bath.ac.uk> writes:
> >
>  > <stuff & more stuff>

[...]
> Take a look at cross-compiling (Windows under Linux).  We have failed to
> make lazy loading of base work under cross-compilation, but do this for
> all other packages.

Just curious: Was that "failed" as in "didn't quite get it right" or as
in "hit an unexpected obstacle"?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Thu Dec 16 15:12:58 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 16 Dec 2004 09:12:58 -0500
Subject: [R] 3 questions
In-Reply-To: <20041216082951.53553.qmail@web51409.mail.yahoo.com>
Message-ID: <20041216141301.PYAW1694.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Vasilis,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of vasilis pappas
> Sent: Thursday, December 16, 2004 3:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] 3 questions
. . .
>  
> 2) In my function I make use of the Rcmdr package. But every 
> time I use my  function and the Rcmdr package loads, I get 
> the R-commander window and I have to delete it every time. So 
> I wonder if there is a way to avoid R-commander window every 
> time I load the Rcmdr package.
>  

You could remove the .onAttach() function from the package, which calls
Commander() to start the Rcmdr GUI. Because Commander() performs a variety
of initialisations, however, your function may not work properly in its
absence. It's not possible to know whether you'll run into problems without
knowing what you're using in the Rcmdr package. 

I hope that this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox



From thpe at hhbio.wasser.tu-dresden.de  Thu Dec 16 15:17:55 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 16 Dec 2004 15:17:55 +0100
Subject: [R] Detecting incomplete commands
In-Reply-To: <200412160946.iBG9kmIA1228854@hedwig1.umh.ac.be>
References: <200412160946.iBG9kmIA1228854@hedwig1.umh.ac.be>
Message-ID: <41C19913.3080905@hhbio.wasser.tu-dresden.de>

Philippe Grosjean wrote:
> Hello,
> 
> I need a similar behaviour as with the prompt: asking to complete incomplete
> R command with eval(parse(text = ....))
> Is it a way to make the difference between an illegal and an incomplete R
> command in a string?
> For instance:
> 
> 
>>parse(text="ls()")
> 
> expression(ls())
> 
> This is fine!
> 
> 
>>parse(text="ls(")
> 
> Error in parse(file, n, text, prompt) : parse error
> 
> This is an incomplete command, and I want to ask to the user for further
> input (multiline command)

Hello Phillipe,

why not using something like this:

 > ok<-try(parse(text="ls("), silent=TRUE)
 > ok
[1] "Error in parse(file, n, text, prompt) : parse error\n"
attr(,"class")
[1] "try-error"

and then check if "ok" is an expression:

 > is.expression(ok)
FALSE


Thomas P.



From phgrosjean at sciviews.org  Thu Dec 16 15:50:00 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 16 Dec 2004 15:50:00 +0100
Subject: [R] Detecting incomplete commands
In-Reply-To: <41C19913.3080905@hhbio.wasser.tu-dresden.de>
Message-ID: <200412161453.iBGEqwJs1065176@hedwig1.umh.ac.be>

Thank you Thomas for your answer. This is of course what I intend to do
(indeed, to use a tryCatch(), instead of I try(). However, I would like to
have a different behaviour depending if the synthax is incorrect "ls())" for
instance, or incomplete "ls(" for instance. Indeed, exactly like the
distinction made at the command line:

> ls()) # This generates an error
Error: syntax error
> ls( # This is not an error, but a multiline command
+ 

If you use parse() within a try(), you got the same error message in both
cases: "Error in parse(file, n, text, prompt) : parse error". So, it is not
possible to discriminate between the two situation in this context. Is it a
way to cope with that?

Best,

Philippe Grosjean 
  

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Petzoldt
> Sent: Thursday, December 16, 2004 3:18 PM
> To: phgrosjean at sciviews.org
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Detecting incomplete commands
> 
> Philippe Grosjean wrote:
> > Hello,
> > 
> > I need a similar behaviour as with the prompt: asking to complete 
> > incomplete R command with eval(parse(text = ....)) Is it a 
> way to make 
> > the difference between an illegal and an incomplete R command in a 
> > string?
> > For instance:
> > 
> > 
> >>parse(text="ls()")
> > 
> > expression(ls())
> > 
> > This is fine!
> > 
> > 
> >>parse(text="ls(")
> > 
> > Error in parse(file, n, text, prompt) : parse error
> > 
> > This is an incomplete command, and I want to ask to the user for 
> > further input (multiline command)
> 
> Hello Phillipe,
> 
> why not using something like this:
> 
>  > ok<-try(parse(text="ls("), silent=TRUE)  > ok [1] "Error 
> in parse(file, n, text, prompt) : parse error\n"
> attr(,"class")
> [1] "try-error"
> 
> and then check if "ok" is an expression:
> 
>  > is.expression(ok)
> FALSE
> 
> 
> Thomas P.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From mail at joeconway.com  Thu Dec 16 16:05:52 2004
From: mail at joeconway.com (Joe Conway)
Date: Thu, 16 Dec 2004 07:05:52 -0800
Subject: [R] PL/R calls fail
In-Reply-To: <6.1.2.0.2.20041216125901.01a0c368@isis.wu-wien.ac.at>
References: <6.1.2.0.2.20041216125901.01a0c368@isis.wu-wien.ac.at>
Message-ID: <41C1A450.4060103@joeconway.com>

Stefan Sobernig wrote:
> I am currently trying to create a development environment
> including PostgreSQL 8.0.0rc1, R 2.0.1 and PL/R on a system running Fedora 
> Cora 1.
> So far, I have suceeded in setting up PostgreSQL and R as
> a shared library - unfortunately I have not been able to link these
> two spheres by adding the PostgreSQL add-on PL/R due to
> some mysterious probs.

This is an inappropriate list for such a PL/R specific question. Please 
sign up for the PL/R list here:
   http://gborg.postgresql.org/mailman/listinfo/plr-general

Thanks,

Joe



From ggrothendieck at myway.com  Thu Dec 16 15:42:15 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 16 Dec 2004 14:42:15 +0000 (UTC)
Subject: [R] Advice on parsing formulae
References: <s1c014c6.095@liberator.csv.warwick.ac.uk>
	<002f01c4e349$f5a00800$9230e182@peano>
Message-ID: <loom.20041216T153812-671@post.gmane.org>

Claus Dethlefsen <dethlef <at> math.aau.dk> writes:

: 
: Thank you for the advice. I have now boiled my problem down to the
: following:
: 
: How do I create fm2 from fm1 ?
: 
: fm1 <-  Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))
: fm2 <-  Y ~ 1 + x:A + z + u + B + poly(v, 3)
: 
: Thus, how do I simply remove tvar( * ) from a formula? Do I have to write a
: function to parse the string and then re-create the formula? Is there an
: easy way of doing this?


If its ok to leave in some extra parentheses you could 
1. convert it to a character string, fm1.str and then
2. use gsub to replace all occurrences of "tvar" with "" and 
   then convert it back to a formula:

fm1.str <- paste( as.character(fm1)[c(2,1,3)], collapse = "" )
fm2 <- as.formula( gsub("tvar", "", fm1.str) )



From p.dalgaard at biostat.ku.dk  Thu Dec 16 16:20:59 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Dec 2004 16:20:59 +0100
Subject: [R] Detecting incomplete commands
In-Reply-To: <200412161453.iBGEqwJs1065176@hedwig1.umh.ac.be>
References: <200412161453.iBGEqwJs1065176@hedwig1.umh.ac.be>
Message-ID: <x2u0qme0bo.fsf@biostat.ku.dk>

"Philippe Grosjean" <phgrosjean at sciviews.org> writes:

> Thank you Thomas for your answer. This is of course what I intend to do
> (indeed, to use a tryCatch(), instead of I try(). However, I would like to
> have a different behaviour depending if the synthax is incorrect "ls())" for
> instance, or incomplete "ls(" for instance. Indeed, exactly like the
> distinction made at the command line:
> 
> > ls()) # This generates an error
> Error: syntax error
> > ls( # This is not an error, but a multiline command
> + 
> 
> If you use parse() within a try(), you got the same error message in both
> cases: "Error in parse(file, n, text, prompt) : parse error". So, it is not
> possible to discriminate between the two situation in this context. Is it a
> way to cope with that?

Here's one idea:

> grep("line 2",try(parse(textConnection("ls)")),silent=T))
numeric(0)
> grep("line 2",try(parse(textConnection("ls(")),silent=T))
[1] 1


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jinss at hkusua.hku.hk  Thu Dec 16 16:33:47 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Thu, 16 Dec 2004 23:33:47 +0800
Subject: [R] partial linear model
Message-ID: <20041216153347.GA26405@S127.localdomain>

Dear all,

Are there any packages can estimate the partial linear
model.  Or any one can give me any suggestions.

Many thanks in advance.


              Jin



From mike_saunders at umenfa.maine.edu  Thu Dec 16 16:40:00 2004
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Thu, 16 Dec 2004 10:40:00 -0500
Subject: [R] nls question
Message-ID: <002b01c4e385$80b8b280$9ba76f82@CFRU0104>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041216/4fa16110/attachment.pl

From jmf at atmosinfo.com  Thu Dec 16 16:41:08 2004
From: jmf at atmosinfo.com (Jeffrey Freedman)
Date: Thu, 16 Dec 2004 10:41:08 -0500
Subject: [R] Problem with postscript graphics device driver
Message-ID: <E7D34F84-4F78-11D9-98EB-000A27B459A6@atmosinfo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041216/b1cd2436/attachment.pl

From andy_liaw at merck.com  Thu Dec 16 16:40:39 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 16 Dec 2004 10:40:39 -0500
Subject: [R] partial linear model
Message-ID: <3A822319EB35174CA3714066D590DCD50994E43C@usrymx25.merck.com>

What kinds of terms do you have in mind for the non-linear parts?  gam() in
both mgcv and gam will accommodate linear terms, so they ought to work, no?

Andy

> From: Jin Shusong
> 
> Dear all,
> 
> Are there any packages can estimate the partial linear
> model.  Or any one can give me any suggestions.
> 
> Many thanks in advance.
> 
> 
>               Jin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Scott.Waichler at pnl.gov  Thu Dec 16 16:58:34 2004
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Thu, 16 Dec 2004 07:58:34 -0800
Subject: [R] Customizing axis labels with alternating in lattice
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A011D47A7@pnlmse35.pnl.gov>


I would like to have complete control over which plots and which
axes have labels in a lattice figure.  I tried lots of vectors for the
alternating setting in scales, but none worked.  Here is what I
would like to do for a 2 col x 3 row figure, where the x designates
a location I would like to have axis labels:

      ___________
    x |_____|____|
      |_____|____|x
    x |_____|____|
         x     x

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From vito.muggeo at giustizia.it  Thu Dec 16 17:25:35 2004
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 16 Dec 2004 17:25:35 +0100
Subject: R: [R] partial linear model
References: <20041216153347.GA26405@S127.localdomain>
Message-ID: <002701c4e38b$e23e27a0$5c13070a@PROCGEN>

Dear Jin,
if you mean `conditional linearity', (i.e. given the nonlinear parameter,
the model is linear) you can use nls() with algorithm = "plinear". See ?nls
Alternatively, if your model has just one nonlinear parameter th, say, I
think you can write the objective function (for instance the logLik)
depending on th and use optimize() to search for the optimum; Then fit your
model assuming th known (and ignoring its (co)variability.. ). Something
like:

fn<-function(th,y,X){
#the deviance function depending on th
#th: nonlinear parameter to be estimated
#y: the response
#X: the design matrix
o<-glm(y~X+_someKnownFunction(th)_+..)
o$dev
}

#search the optimum
ob<-optimize(fn,..

th1<-ob$minimum #(or ob$maximum)
o<-glm(y~X+_someKnownFunction(th1)_+..) #fit the model assuming th=th1
*known*

Hope this helps,
vito muggeo


----- Original Message -----
From: Jin Shusong <jinss at hkusua.hku.hk>
To: R Help <r-help at stat.math.ethz.ch>
Sent: Thursday, December 16, 2004 4:33 PM
Subject: [R] partial linear model


> Dear all,
>
> Are there any packages can estimate the partial linear
> model.  Or any one can give me any suggestions.
>
> Many thanks in advance.
>
>
>               Jin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Thu Dec 16 17:19:42 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 16 Dec 2004 10:19:42 -0600
Subject: [R] Customizing axis labels with alternating in lattice
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A011D47A7@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A011D47A7@pnlmse35.pnl.gov>
Message-ID: <200412161019.42348.deepayan@stat.wisc.edu>

On Thursday 16 December 2004 09:58, Waichler, Scott R wrote:
> I would like to have complete control over which plots and which
> axes have labels in a lattice figure.  I tried lots of vectors for
> the alternating setting in scales, but none worked.  Here is what I
> would like to do for a 2 col x 3 row figure, where the x designates a
> location I would like to have axis labels:
>
>       ___________
>     x |_____|____|
>       |_____|____|x
>     x |_____|____|
>          x     x

This should be pretty basic:

scales = list(x = list(alternating = FALSE), 
              y = list(alternating = TRUE))

Deepayan



From ripley at stats.ox.ac.uk  Thu Dec 16 17:18:03 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Thu, 16 Dec 2004 16:18:03 +0000 (GMT)
Subject: [R] Problem with postscript graphics device driver
In-Reply-To: <E7D34F84-4F78-11D9-98EB-000A27B459A6@atmosinfo.com>
Message-ID: <Pine.GSO.4.31.0412161613170.13204-100000@markov.stats>

If you mean R 2.0.1, your installation is broken.  Here's a quick check of
where it should be:

> ls(asNamespace("grDevices"), all=TRUE)
 [1] ".PSenv"               ".Quartzenv"           ".X11env"
 [4] ".__NAMESPACE__."      ".__S3MethodsTable__." ".packageName"
....

On Thu, 16 Dec 2004, Jeffrey Freedman wrote:

> I recently installed R version 2.01 for OS X (version 10.3.6). I now
> get the following error message when I try to use the postscript
> function:
>
> Error in get(name.opt, envir = envir) : Object ".PSenv" not found
>
> This did not happen in previous versions of R (i.e. 1.9). I do not get
> this error message when using the pdf graphics device driver.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec 16 17:21:50 2004
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Thu, 16 Dec 2004 16:21:50 +0000 (GMT)
Subject: [R] Detecting incomplete commands
In-Reply-To: <x2u0qme0bo.fsf@biostat.ku.dk>
Message-ID: <Pine.GSO.4.31.0412161618520.13204-100000@markov.stats>

On 16 Dec 2004, Peter Dalgaard wrote:

> "Philippe Grosjean" <phgrosjean at sciviews.org> writes:
>
> > Thank you Thomas for your answer. This is of course what I intend to do
> > (indeed, to use a tryCatch(), instead of I try(). However, I would like to
> > have a different behaviour depending if the synthax is incorrect "ls())" for
> > instance, or incomplete "ls(" for instance. Indeed, exactly like the
> > distinction made at the command line:
> >
> > > ls()) # This generates an error
> > Error: syntax error
> > > ls( # This is not an error, but a multiline command
> > +
> >
> > If you use parse() within a try(), you got the same error message in both
> > cases: "Error in parse(file, n, text, prompt) : parse error". So, it is not
> > possible to discriminate between the two situation in this context. Is it a
> > way to cope with that?
>
> Here's one idea:
>
> > grep("line 2",try(parse(textConnection("ls)")),silent=T))
> numeric(0)
> > grep("line 2",try(parse(textConnection("ls(")),silent=T))
> [1] 1

This is a lot easier at C level: see <R_ext/Parse.h> and `Writing R
Extensions'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dirk.enzmann at jura.uni-hamburg.de  Thu Dec 16 17:45:53 2004
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Thu, 16 Dec 2004 17:45:53 +0100
Subject: [R] Percentages in contingency tables *warning trivial question*
Message-ID: <41C1BBC1.9080107@jura.uni-hamburg.de>

Being still unsatisfied with the CrossTable() function I modified the 
code so that the function will create an output similar to the SPSS 
procedure CROSSTABS. Most probably the code will not meet most R 
programmers' standards, perhaps someone else is willing to optimize it. 
Unfortunately, as an R beginner I am not able to write a documentation 
file (perhaps someone is willing to put some effort in it, too)- the 
parameters that can be used can be found next to "function".

Including the function code here would cause nasty line breaks, you can 
find it at

http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Software/crosstabs.r

Dirk

At Mon, 13 Dec 2004 05:47:17 -0500 Chuck Cleland 
<ccleland at optonline.net> wrote:

(snip)
 > You might want to look at CrossTable() in the gmodels package
 > of the gregmisc bundle.
(snip)

-- 
*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Schlueterstr. 28
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From MSchwartz at MedAnalytics.com  Thu Dec 16 18:46:14 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 16 Dec 2004 11:46:14 -0600
Subject: [R] Percentages in contingency tables *warning trivial question*
In-Reply-To: <41C1BBC1.9080107@jura.uni-hamburg.de>
References: <41C1BBC1.9080107@jura.uni-hamburg.de>
Message-ID: <1103219174.6114.14.camel@horizons.localdomain>

On Thu, 2004-12-16 at 17:45 +0100, Dirk Enzmann wrote:
> Being still unsatisfied with the CrossTable() function I modified the 
> code so that the function will create an output similar to the SPSS 
> procedure CROSSTABS. Most probably the code will not meet most R 
> programmers' standards, perhaps someone else is willing to optimize it. 
> Unfortunately, as an R beginner I am not able to write a documentation 
> file (perhaps someone is willing to put some effort in it, too)- the 
> parameters that can be used can be found next to "function".
> 
> Including the function code here would cause nasty line breaks, you can 
> find it at
> 
> http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Software/crosstabs.r
> 
> Dirk

Dirk,

As the author of the CrossTable() function, I appreciate your additions
to the functionality of the code.

I will save your e-mail and code and look to incorporate it into the
main CrossTable() function as soon as time permits. I will include an
appropriate attribution to you for your contribution in the source code
and help files.

It is my long term goal to enhance the scope of the function with
additional outputs (as I have mentioned here and elsewhere previously),
but other work related issues have precluded me from getting back to
them sooner. 

The risk of the inclusion of these additional features is that the
function becomes something of a behemoth, given all the options for
outputs. That approach is contrary to general R philosophy. However,
this type of broad formatted cross-tabulation functionality seems to be
an issue that comes up frequently. The lack of this was my initial
reason for creating the function based upon my own requirements. Having
been a former SAS user, I tended to fashion the function on the output
of SAS PROC FREQ.

I will pass on the updated files to Greg for inclusion in his bundle as
soon as I have completed the additions and testing.

Best regards and thanks again.

Marc Schwartz



From gregory.r.warnes at pfizer.com  Thu Dec 16 19:04:29 2004
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Thu, 16 Dec 2004 13:04:29 -0500
Subject: [R] SAS or R software
Message-ID: <915D2D65A9986440A277AC5C98AA466F3F6646@groamrexm02.amer.pfizer.com>



> - Output delivery system (ODS):  *Every* piece of SAS output is an 
> output object that can be captured as
> a dataset, rendered in RTF, LaTeX, HTML, PDF, etc. with a relatively 
> simple mechanism (including graphs)
>     ods pdf file='mystuff.pdf'';
>       << any sas stuff>>
>     ods pdf close;

R now has this ability as well via the "sinkplot" and "textplot" commands
provided by the "gplots" package.

-G



LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From samsonite at nyu.edu  Thu Dec 16 19:09:16 2004
From: samsonite at nyu.edu (Samuel A Mati)
Date: Thu, 16 Dec 2004 13:09:16 -0500
Subject: [R] PHP MySQL and R
Message-ID: <bcedfebcdaf0.bcdaf0bcedfe@nyu.edu>

Hello everyone, this is my first post.  Nice to meet all of you.  I am 
having some troubles using R in combination with PHP and MySQL.  I 
would appreciate any assistance very much!  This is kind of long, so if 
you'd like a shorter version let me know.

I am working on a project that takes a list of points (inputted via the 
web, and stored into a MySQL DB), and runs an R script on them.  The 
user receives and e-mail when the script is complete, and can view the 
output (which, ideally, will be stored in a database and formatted for 
the web by PHP)

What is the best way to do this?  

As of now, I have it so the user can upload the list of points and they 
are stored in a database. (see below)  When the user requests a job to 
be run, that is, for the server to use R to process the data, an entry 
is added to the jobs table.

Every x seconds, a daemon (written in PHP) looks at the "jobs" table 
and looks to see if any are in the "processing" state.  If none are, 
this means the server is free to run a script... so the daemon chooses 
a job to be run. (and sets its status to processing)

At this point this information is available:
The R script that needs to be run.
The Dataset ID

-----------
Problem #1:
How should I call R so that it runs the script, lets call it "bla.R" on 
the points stored in a MySQL database?  Do I have PHP create a 
temporary file, call the R script with that filename as an argument, 
and have R just do table.read("temp.txt")?
-----------

-----------
Problem #2:
The R script just runs linear regressions on the data.  I'd like to 
take only SOME of the data outputted by the "summary" function.  Let's 
say we have a simple linear regression on the X and Y points:
fit <- lm(X~Y)

How can I get R to output something that can be easily split apart and 
stored into a DB?  I want the following values:

-Residuals Min
-Residuals 1Q
-Residuals Median
-Residuals 3Q
-Residuals Max
-The Residual Standard Error
-Multiple R-Squared
-Adjusted R-Squared
-F-Statistic
-etc..etc..

How can this be acheived?
-------------



====Database Structure====

A list of points is called a Dataset.

We have a table called "Datasets" which simply holds all the Datasets:

DATASETS
id
title

and a table "Data" which holds all the points of all the datasets:

DATA
id
ds_id
x
y
lagged

The points of a Dataset can be found from this query: "SELECT 
x,y,lagged FROM DATA WHERE ds_id=(whatever dataset)"


and the table "jobs"

JOBS
id
script (which r script to run)
dataset (which dataset to use)
status (queued, processing, or completed)

=======================


Thank you all so much for helping me out.. I appreciate it very much 
and am looking forward to figuring this out!
-Sam



From christian_mora at vtr.net  Thu Dec 16 19:11:36 2004
From: christian_mora at vtr.net (christian_mora@vtr.net)
Date: Thu, 16 Dec 2004 14:11:36 -0400
Subject: [R] nls question
In-Reply-To: <002b01c4e385$80b8b280$9ba76f82@CFRU0104>
Message-ID: <4178B9F1000D98F8@hudson.vtr.net>

Mike
nlsList from nlme library can fit nonlinear models for dataset grouped by
some specification, e.g. by specie in your case
Regards
Christian

>-- Mensaje Original --
>From: "Mike Saunders" <mike_saunders at umenfa.maine.edu>
>To: "R Help" <r-help at stat.math.ethz.ch>
>Date: Thu, 16 Dec 2004 10:40:00 -0500
>Subject: [R] nls question
>
>
>Just a quick question.  Is there a way to easily specify factor levels
in
>a function definition within nls?  For example, I am trying to fit a 3
parameter,
>nonlinear Weibull function to tree height and I would like to have results
>by species (or down the road a bit, by plot).  I am hoping there is someway
>to do it easily, similar to the "gas example" in the ANCOVA chapter in
Modern
>Applied Statistics with S, and not running it with a "by" function or making
>a very complex function statement with all factor levels combinations.
>
>Can someone point me in the right direction?
>
>Thanks,
>Mike
>
>Mike Saunders
>Research Assistant
>Forest Ecosystem Research Program
>Department of Forest Ecosystem Sciences
>University of Maine
>Orono, ME  04469
>207-581-2763 (O)
>207-581-4257 (F)
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From takahama at andrew.cmu.edu  Thu Dec 16 19:12:29 2004
From: takahama at andrew.cmu.edu (Satoshi Takahama)
Date: Thu, 16 Dec 2004 13:12:29 -0500
Subject: [R] na.omit argument in gls()
Message-ID: <41C1D00D.7000000@andrew.cmu.edu>

Hello everyone,

I was wondering if someone could tell me what happens when you use 
na.omit in the gls() function in library(nlme) when you assume a 
correlation structure for the errors:

gls(Y ~ X1 + X2 + X3, data=sample.data, na.action=na.omit, 
correlation=corARMA(p=1))

Is na.omit is an appropriate argument to use in this case? There is 
considerable information I could find about imputing missing data - 
which I have done to determine that the autocorrelation is approximately 
AR(1) - but I have not found any literature on whether it is then 
acceptable to omit the imputed data and just use observed measurements 
for generalized least squares after the correlation structure has been 
determined. Any help would be very much appreciated.

Thank you very much!

Satoshi

___
Satoshi Takahama
Dept. of Chemical Engineering
Carnegie Mellon University
Pittsburgh, PA 15213



From Scott.Waichler at pnl.gov  Thu Dec 16 19:57:07 2004
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Thu, 16 Dec 2004 10:57:07 -0800
Subject: [R] Unsightly lines from x*y grid in levelplot with Postscript
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A011D4A10@pnlmse35.pnl.gov>


When I generate levelplot figures with Postscript, the lines from
the underlying grid (i.e., the x and y in z ~ x* y)
show up as faint white lines in the output.  If I have a dense grid,
the lines wash out the contour colors.  I can avoid the problem if
I use png instead, but that brings up other problems.  Any
ideas for how I can get Postscript to not show those lines around
the cells?

Scott

Scott Waichler, Senior Research Scientist
Pacific Northwest National Laboratory
MSIN K9-36
P.O. Box 999
Richland, WA   99352    USA
509-372-4423 (voice)
509-372-6089 (fax)
scott.waichler at pnl.gov
http://hydrology.pnl.gov



From greg.snow at ihc.com  Thu Dec 16 19:57:25 2004
From: greg.snow at ihc.com (Greg Snow)
Date: Thu, 16 Dec 2004 11:57:25 -0700
Subject: [R] drawing a rectangle through multiple plots
Message-ID: <s1c1782e.089@lp-msg1.co.ihc.com>

Attached is a file with the definition of a function (cnvrt.coords)
that
can just be sourced into R.  This function will convert between the
different coordinate systems.  The basic usage for your problem is:

1. draw first plot
2. convert the y-value for the top of your rectangle from the current
    user coordinates to device coordinates
3. draw the second and third plots
4. turn off clipping so that the rectangle can be drawn accross plots
5. convert the number from step 2 into the current user coordinates
6. draw the rectangle using the points in the current usr coordinates 
    (using the doubly converted number for the top).

Here is an example similar to yours:

par(mfrow=c(3,1))

# first plot
plot(1:10, rnorm(10),ylim=c(-4,4),xlab='')

### get the y coordinate of 4 in device units
tmp.y <- cnvrt.coords(x=NA,y=4)$dev$y

# middle plot
plot(1:10, rnorm(10), ylim=c(-4,4),xlab='')

#bottom plot
plot(1:10, rnorm(10), ylim=c(-4,4))

### turn off clipping (alow drawn lines to cross all plots)
par(xpd=NA)

### convert top to current usr coordinates
tmp.y2 <- cnvrt.coords(x=NA, y=tmp.y, input='dev')$usr$y

# draw rectangle
rect(2.5, -4, 4.5, tmp.y2)


Hope this helps,



Greg Snow, Ph.D.
Statistical Data Center
greg.snow at ihc.com
(801) 408-8111

>>> Dr Carbon <drcarbon at gmail.com> 12/14/04 12:41PM >>>
How do I draw a rectangle across multiple plots on a device?

E.g.,

def.par <- par(no.readonly = TRUE)
par(mfrow = c(3, 1))
plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
plot(1:10, rnorm(10), ylim = c(-4,4), type = "l")
rect(2, -4, 3, 4)
par(def.par)

I want the rectangle to extend across the whole device. How do I get
at those coordinates? Is this a case where I should be using grid or
gridBase?

Thanks.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

From deepayan at stat.wisc.edu  Thu Dec 16 20:38:07 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 16 Dec 2004 13:38:07 -0600
Subject: [R] Unsightly lines from x*y grid in levelplot with Postscript
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A011D4A10@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A011D4A10@pnlmse35.pnl.gov>
Message-ID: <200412161338.07140.deepayan@stat.wisc.edu>

On Thursday 16 December 2004 12:57, Waichler, Scott R wrote:
> When I generate levelplot figures with Postscript, the lines from
> the underlying grid (i.e., the x and y in z ~ x* y)
> show up as faint white lines in the output.  If I have a dense grid,
> the lines wash out the contour colors.  I can avoid the problem if
> I use png instead, but that brings up other problems.  Any
> ideas for how I can get Postscript to not show those lines around
> the cells?

I'm not sure what you mean. Could you send me an example (off-list)? 

One typical problem with gv is that some artifacts show up (which may 
not be what you describe) when the display is anti-aliased. Try turning 
that off (by pressing 'a') and see if that improves the display. 

Deepayan



From p.murrell at auckland.ac.nz  Thu Dec 16 20:40:20 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 17 Dec 2004 08:40:20 +1300
Subject: [R] Unsightly lines from x*y grid in levelplot with Postscript
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A011D4A10@pnlmse35.pnl.gov>
Message-ID: <41C1E4A4.7060707@stat.auckland.ac.nz>

Hi


Waichler, Scott R wrote:
> When I generate levelplot figures with Postscript, the lines from
> the underlying grid (i.e., the x and y in z ~ x* y)
> show up as faint white lines in the output.  If I have a dense grid,
> the lines wash out the contour colors.  I can avoid the problem if
> I use png instead, but that brings up other problems.  Any
> ideas for how I can get Postscript to not show those lines around
> the cells?


I think it's ghostview (or whatever viewer you're using) that's showing 
the lines (not postscript).  You don't get the lines when you print do 
you?

Try turning off antialiasing.

If you produce PDF, it's possible to just turn off antialiasing for 
"line art", so the text doesn't go chunky.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From das at cshl.edu  Thu Dec 16 20:48:07 2004
From: das at cshl.edu (Rajdeep Das)
Date: Thu, 16 Dec 2004 14:48:07 -0500
Subject: [R] reading svm function in e1071
Message-ID: <011201c4e3a8$2a61f450$6807308f@artney>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041216/3bd8d944/attachment.pl

From andy_liaw at merck.com  Thu Dec 16 21:01:55 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 16 Dec 2004 15:01:55 -0500
Subject: [R] reading svm function in e1071
Message-ID: <3A822319EB35174CA3714066D590DCD50994E43F@usrymx25.merck.com>

It's in the R FAQ.  Try getAnywhere("predict.svm").

Andy

> From: Rajdeep Das
> 
> Hi,
> 
> If I try to read the codes of functions in e1071 package, it 
> gives me following error message.
> 
> >library(e1071)
> 
> > svm
> function (x, ...)
> UseMethod("svm")
> <environment: namespace:e1071>
> 
> > predict.svm
> Error: Object "predict.svm" not found
> >
> 
> Can someone help me on this how to read the codes of the 
> functions in the e1071 package?
> 
> Thanks.
> 
> 
> Raj
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From rpeng at jhsph.edu  Thu Dec 16 21:22:32 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 16 Dec 2004 15:22:32 -0500
Subject: [R] reading svm function in e1071
In-Reply-To: <011201c4e3a8$2a61f450$6807308f@artney>
References: <011201c4e3a8$2a61f450$6807308f@artney>
Message-ID: <41C1EE88.6030202@jhsph.edu>

The predict method for class `svm' is hidden in a namespace.  Try

getS3method("predict", "svm")

-roger

Rajdeep Das wrote:
> Hi,
> 
> If I try to read the codes of functions in e1071 package, it gives me following error message.
> 
> 
>>library(e1071)
> 
> 
>>svm
> 
> function (x, ...)
> UseMethod("svm")
> <environment: namespace:e1071>
> 
>>predict.svm
> 
> Error: Object "predict.svm" not found
> 
> 
> Can someone help me on this how to read the codes of the functions in the e1071 package?
> 
> Thanks.
> 
> 
> Raj
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From Scott.Waichler at pnl.gov  Thu Dec 16 21:52:10 2004
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Thu, 16 Dec 2004 12:52:10 -0800
Subject: FW: [R] Unsightly lines from x*y grid in levelplot with Postscript
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A011D4A3A@pnlmse35.pnl.gov>

 
> One typical problem with gv is that some artifacts show up (which may 
> not be what you describe) when the display is anti-aliased. Try 
> turning that off (by pressing 'a') and see if that improves the 
> display.

> If you produce PDF, it's possible to just turn off antialiasing for 
> "line art", so the text doesn't go chunky.

Sorry, I should have said this is a Ghostview thing.

A summary of the suggestions from Deepayan and Paul:

Pressing "a" does fix the problem in Ghostview by turning off the
anti-aliasing.  It can also be done from the menu under "State."
 
In Adobe Acrobat Reader 5.0, on the Edit menu, in the Preferences 
dialog, in the Display panel, there are three smoothing checkboxes
(one for text, one for line art, and one for images).

None of this matters for printing, where the white lines will not
show up.

--Scott Waichler



From dingjun_cn at yahoo.com  Thu Dec 16 22:40:32 2004
From: dingjun_cn at yahoo.com (Jun Ding)
Date: Thu, 16 Dec 2004 13:40:32 -0800 (PST)
Subject: [R] counting numbers without replicates in a vector
Message-ID: <20041216214032.76735.qmail@web80105.mail.yahoo.com>

Hi,
I am just wondering if there is an easy way to count
in a numeric vector how many numbers don't have
replicates. 
For example, 
a=c(1,1,2,2,3,4,5), how can I know there are three
numbers (3, 4 and 5) without replicates?

Thank you!

Jun


=====



From tlumley at u.washington.edu  Thu Dec 16 22:47:16 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 16 Dec 2004 13:47:16 -0800 (PST)
Subject: [R] Advice on parsing formulae
In-Reply-To: <002f01c4e349$f5a00800$9230e182@peano>
References: <002f01c4e349$f5a00800$9230e182@peano>
Message-ID: <Pine.A41.4.61b.0412161329010.264232@homer09.u.washington.edu>

On Thu, 16 Dec 2004, Claus Dethlefsen wrote:

> Thank you for the advice. I have now boiled my problem down to the
> following:
>
> How do I create fm2 from fm1 ?
>
> fm1 <-  Y ~ 1 + tvar(x:A) + tvar(z) + u + tvar(B) + tvar(poly(v,3))
> fm2 <-  Y ~ 1 + x:A + z + u + B + poly(v, 3)
>
> Thus, how do I simply remove tvar( * ) from a formula? Do I have to write a
> function to parse the string and then re-create the formula? Is there an
> easy way of doing this?

One way is to process the formula recursively:

no.tvar<-function(formula){
 	process<-function(expr){
            if (length(expr)==1)
              return(expr)
 	   if(length(expr)==2)
              if ( expr[[1]]==as.name("tvar"))
               return(expr[[2]])
              else return(expr)
            expr[[2]]<-process(expr[[2]])
            expr[[3]]<-process(expr[[3]])
            return(expr)
          }
   formula[[3]]<-process(formula[[3]])
   formula 
}

> no.tvar(fm1)
Y ~ 1 + x:A + z + u + B + poly(v, 3)

This is not the more common sort of problem where you actually want to 
remove terms from a formula (eg Error() in aov, cluster() and strata() in 
coxph).  You want to modify them in place.


 	-thomas



From sundar.dorai-raj at pdf.com  Thu Dec 16 22:50:24 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 16 Dec 2004 15:50:24 -0600
Subject: [R] counting numbers without replicates in a vector
In-Reply-To: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
References: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
Message-ID: <41C20320.8010906@pdf.com>



Jun Ding wrote:

> Hi,
> I am just wondering if there is an easy way to count
> in a numeric vector how many numbers don't have
> replicates. 
> For example, 
> a=c(1,1,2,2,3,4,5), how can I know there are three
> numbers (3, 4 and 5) without replicates?
> 

How about using ?table:

tab <- table(a)
unq <- names(tab)[tab == 1]

Then use as.numeric(unq) to convert the names to numbers if needed.

--sundar



From spencer.graves at pdf.com  Thu Dec 16 22:53:32 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 16 Dec 2004 13:53:32 -0800
Subject: [R] counting numbers without replicates in a vector
In-Reply-To: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
References: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
Message-ID: <41C203DC.6030001@pdf.com>

      Have you considered something like the following: 

 > a
[1] 1 1 2 2 3 4 5
 > sum(table(a)==1)
[1] 3     

      hope this helps.  spencer graves

Jun Ding wrote:

>Hi,
>I am just wondering if there is an easy way to count
>in a numeric vector how many numbers don't have
>replicates. 
>For example, 
>a=c(1,1,2,2,3,4,5), how can I know there are three
>numbers (3, 4 and 5) without replicates?
>
>Thank you!
>
>Jun
>
>
>=====
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From baron at psych.upenn.edu  Thu Dec 16 22:55:18 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 16 Dec 2004 16:55:18 -0500
Subject: [R] counting numbers without replicates in a vector
In-Reply-To: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
References: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
Message-ID: <20041216215518.GA14546@psych>

On 12/16/04 13:40, Jun Ding wrote:
 Hi,
 I am just wondering if there is an easy way to count
 in a numeric vector how many numbers don't have
 replicates.
 For example,
 a=c(1,1,2,2,3,4,5), how can I know there are three
 numbers (3, 4 and 5) without replicates?

Take a look at unique()

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From gunter.berton at gene.com  Thu Dec 16 22:57:20 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 16 Dec 2004 13:57:20 -0800
Subject: [R] counting numbers without replicates in a vector
In-Reply-To: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
Message-ID: <200412162157.iBGLvKTQ009218@hertz.gene.com>

?duplicated and ?unique might be of interest to you ...

But I think an easier way is:

z<-table(a)
length(z)-sum(z>1)

This gives you the count. 
names(z)[z==1]   gives you the actual values

(The quotes can be removed by explicitly calling print with argument
quote=FALSE)

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jun Ding
> Sent: Thursday, December 16, 2004 1:41 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] counting numbers without replicates in a vector
> 
> Hi,
> I am just wondering if there is an easy way to count
> in a numeric vector how many numbers don't have
> replicates. 
> For example, 
> a=c(1,1,2,2,3,4,5), how can I know there are three
> numbers (3, 4 and 5) without replicates?
> 
> Thank you!
> 
> Jun
> 
> 
> =====
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From yshao at wadsworth.org  Thu Dec 16 23:01:07 2004
From: yshao at wadsworth.org (Yu Shao)
Date: Thu, 16 Dec 2004 17:01:07 -0500
Subject: [R] counting numbers without replicates in a vector
In-Reply-To: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
References: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
Message-ID: <41C205A3.7050407@wadsworth.org>

table will probably do. The following is probably is what you want:

 > a <- c(1,1,2,2,3,4,5)
 > a.tab <- table (a)
 > a.tab
a
1 2 3 4 5
2 2 1 1 1
 > as.vector(which (a.tab == 1)) 
[1] 3 4 5


Cheers,
Yu

Jun Ding wrote:

>Hi,
>I am just wondering if there is an easy way to count
>in a numeric vector how many numbers don't have
>replicates. 
>For example, 
>a=c(1,1,2,2,3,4,5), how can I know there are three
>numbers (3, 4 and 5) without replicates?
>
>Thank you!
>
>Jun
>
>
>=====
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From ray at mcs.vuw.ac.nz  Thu Dec 16 23:06:50 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Fri, 17 Dec 2004 11:06:50 +1300 (NZDT)
Subject: [R] counting numbers without replicates in a vector
Message-ID: <200412162206.iBGM6oW1026691@tahi.mcs.vuw.ac.nz>

> I am just wondering if there is an easy way to count
> in a numeric vector how many numbers don't have
> replicates. 
> For example, 
> a=c(1,1,2,2,3,4,5), how can I know there are three
> numbers (3, 4 and 5) without replicates?
> 
How about:
length(table(a)[table(a) == 1])

?
Ray Brownrigg



From MSchwartz at MedAnalytics.com  Thu Dec 16 23:10:09 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 16 Dec 2004 16:10:09 -0600
Subject: [R] counting numbers without replicates in a vector
In-Reply-To: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
References: <20041216214032.76735.qmail@web80105.mail.yahoo.com>
Message-ID: <1103235009.12600.0.camel@horizons.localdomain>

On Thu, 2004-12-16 at 13:40 -0800, Jun Ding wrote:
> Hi,
> I am just wondering if there is an easy way to count
> in a numeric vector how many numbers don't have
> replicates. 
> For example, 
> a=c(1,1,2,2,3,4,5), how can I know there are three
> numbers (3, 4 and 5) without replicates?
> 
> Thank you!
> 
> Jun

How about:

> as.vector(which(table(a) == 1))
[1] 3 4 5

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Thu Dec 16 23:42:50 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Dec 2004 23:42:50 +0100
Subject: [R] counting numbers without replicates in a vector
In-Reply-To: <200412162206.iBGM6oW1026691@tahi.mcs.vuw.ac.nz>
References: <200412162206.iBGM6oW1026691@tahi.mcs.vuw.ac.nz>
Message-ID: <x2hdmleufp.fsf@biostat.ku.dk>

Ray Brownrigg <ray at mcs.vuw.ac.nz> writes:

> > I am just wondering if there is an easy way to count
> > in a numeric vector how many numbers don't have
> > replicates. 
> > For example, 
> > a=c(1,1,2,2,3,4,5), how can I know there are three
> > numbers (3, 4 and 5) without replicates?
> > 
> How about:
> length(table(a)[table(a) == 1])

Also, probably inefficient, but rather neat:

> setdiff(a,a[duplicated(a)])
[1] 3 4 5


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tvandaelen at scitegic.com  Fri Dec 17 01:03:33 2004
From: tvandaelen at scitegic.com (Ton van Daelen)
Date: Thu, 16 Dec 2004 16:03:33 -0800
Subject: [R] Problem with SVM and scaling
Message-ID: <830D8D4719112B418ABBC3A0EBA95812C67A61@webmail.scitegic.com>

Hi all -
 
I am running into a problem with the SVM() method when applying it to data sets that have descriptors with zero variance. Here is the sequence of events:

1. I split my data set with 512 descriptors in a training and test set
2. I build an SVM model for the training set. Out of 512 descriptors, 500 have zero variance which I discard before calling the SVM method
3. For the test set, 8 descriptors have zero variance, which I discard too
4. predict.svm() then fails, because it tries to scale using two vectors of different size (500 and 504)

Is there a way to get around this?

Thanks - Ton



From Meredith.Briggs at team.telstra.com  Fri Dec 17 01:30:12 2004
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Fri, 17 Dec 2004 11:30:12 +1100
Subject: [R] (no subject)
Message-ID: <3B5823541A25D311B3B90008C7F9056410E35B4D@ntmsg0092.corpmail.telstra.com.au>



From jinss at hkusua.hku.hk  Fri Dec 17 03:02:15 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Fri, 17 Dec 2004 10:02:15 +0800 (EAT)
Subject: R: [R] partial linear model
In-Reply-To: <002701c4e38b$e23e27a0$5c13070a@PROCGEN>
References: <20041216153347.GA26405@S127.localdomain>
	<002701c4e38b$e23e27a0$5c13070a@PROCGEN>
Message-ID: <20041217.100215.46109420.jinss@hkusua.hku.hk>

From: "Vito Muggeo" <vito.muggeo at giustizia.it>
Subject: R: [R] partial linear model
Date: Thu, 16 Dec 2004 17:25:35 +0100


> Dear Jin,
> if you mean `conditional linearity', (i.e. given the nonlinear parameter,
> the model is linear) you can use nls() with algorithm = "plinear". See ?nls
> Alternatively, if your model has just one nonlinear parameter th, say, I
> think you can write the objective function (for instance the logLik)
> depending on th and use optimize() to search for the optimum; Then fit your
> model assuming th known (and ignoring its (co)variability.. ). Something
> like:
> 
> fn<-function(th,y,X){
> #the deviance function depending on th
> #th: nonlinear parameter to be estimated
> #y: the response
> #X: the design matrix
> o<-glm(y~X+_someKnownFunction(th)_+..)
> o$dev
> }
> 
> #search the optimum
> ob<-optimize(fn,..
> 
> th1<-ob$minimum #(or ob$maximum)
> o<-glm(y~X+_someKnownFunction(th1)_+..) #fit the model assuming th=th1
> *known*
> 
> Hope this helps,
> vito muggeo
> 
> 
> ----- Original Message -----
> From: Jin Shusong <jinss at hkusua.hku.hk>
> To: R Help <r-help at stat.math.ethz.ch>
> Sent: Thursday, December 16, 2004 4:33 PM
> Subject: [R] partial linear model
> 
> 
> > Dear all,
> >
> > Are there any packages can estimate the partial linear
> > model.  Or any one can give me any suggestions.
> >
> > Many thanks in advance.
> >
> >
> >               Jin
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
Dear Muggeo,

You suggestion is so useful and by following you idea, I
have written my own program.  Thank you very much.


        Jin



From andy_liaw at merck.com  Fri Dec 17 03:21:45 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 16 Dec 2004 21:21:45 -0500
Subject: [R] counting numbers without replicates in a vector
Message-ID: <3A822319EB35174CA3714066D590DCD50994E442@usrymx25.merck.com>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Thursday, December 16, 2004 5:43 PM
> To: Ray Brownrigg
> Cc: dingjun_cn at yahoo.com; r-help at stat.math.ethz.ch
> Subject: Re: [R] counting numbers without replicates in a vector
> 
> 
> Ray Brownrigg <ray at mcs.vuw.ac.nz> writes:
> 
> > > I am just wondering if there is an easy way to count
> > > in a numeric vector how many numbers don't have
> > > replicates. 
> > > For example, 
> > > a=c(1,1,2,2,3,4,5), how can I know there are three
> > > numbers (3, 4 and 5) without replicates?
> > > 
> > How about:
> > length(table(a)[table(a) == 1])
> 
> Also, probably inefficient, but rather neat:
> 
> > setdiff(a,a[duplicated(a)])
> [1] 3 4 5
 
Probably not (inefficient):

> x <- sample(1:5e4, 2e5, replace=TRUE)
> system.time(ans1 <- which(table(x) == 1), gcFirst=TRUE)
[1] 1.08 0.00 1.10   NA   NA
> system.time(ans2 <- setdiff(x, x[duplicated(x)]), gcFirst=TRUE)
[1] 0.23 0.02 0.26   NA   NA
> setdiff(ans2, as.numeric(names(ans1)))
numeric(0)

Best,
Andy

 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Andrew.Ward at qsa.qld.edu.au  Fri Dec 17 03:32:56 2004
From: Andrew.Ward at qsa.qld.edu.au (Andrew Ward)
Date: Fri, 17 Dec 2004 12:32:56 +1000
Subject: [R] RE: How about a mascot for R?
Message-ID: <86EED55AE819274B8308B62A225457D8016B866C@exch1.qsa.local>

 
Apologies for adding to this discussion so late.

When the idea of a mascot was first raised, I
recalled how in describing (and spelling) R to
people in seminars, etc I would sometimes say,
in my best pirate voice, "That's R, as in ARRRRRR!".

The connection to something or someone criminal
is perhaps not desirable, but I'm sure that there
is someone in the R community who could be a model
for the logo (long beard certainly, eye-patch or
wooden leg even better). And then clicking on the
logo would play the accompanying sound. I think
this has plenty of appeal.

;-)

Regards,

Andrew C. Ward,                
Senior Analyst (Quantitative), Tel: +61 7 3864 0439
Queensland Studies Authority,  Fax: +61 7 3229 3318
295 Ann Street,
Brisbane Qld 4000, Australia 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This email (including any attached files) is for the intended 
recipient(s) only. If you received this email by mistake, please, 
as a courtesy, tell the sender, then delete this email.

The views and opinions are the originator's and do not necessarily 
reflect those of the Queensland Studies Authority. All reasonable 
precautions have been taken to ensure that this email contained no 
viruses at the time it was sent.



From MSchwartz at MedAnalytics.com  Fri Dec 17 04:10:10 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 16 Dec 2004 21:10:10 -0600
Subject: [R] RE: How about a mascot for R?
In-Reply-To: <86EED55AE819274B8308B62A225457D8016B866C@exch1.qsa.local>
References: <86EED55AE819274B8308B62A225457D8016B866C@exch1.qsa.local>
Message-ID: <1103253010.15397.8.camel@horizons.localdomain>

On Fri, 2004-12-17 at 12:32 +1000, Andrew Ward wrote:
>  Apologies for adding to this discussion so late.
> 
> When the idea of a mascot was first raised, I
> recalled how in describing (and spelling) R to
> people in seminars, etc I would sometimes say,
> in my best pirate voice, "That's R, as in ARRRRRR!".
> 
> The connection to something or someone criminal
> is perhaps not desirable, but I'm sure that there
> is someone in the R community who could be a model
> for the logo (long beard certainly, eye-patch or
> wooden leg even better). And then clicking on the
> logo would play the accompanying sound. I think
> this has plenty of appeal.
> 
> ;-)


Perhaps we could get Johnny Depp (aka Jack Sparrow)?

http://www.imdb.com/gallery/ss/0325980/Ss/0325980/PiratesCN-36-6.jpg

Or perhaps Jason Isaacs (aka Captain Hook)?

http://www.imdb.com/gallery/ss/0316396/Ss/0316396/5576_D137_00042.JPG

;-)

Marc



From sundar.dorai-raj at pdf.com  Fri Dec 17 04:56:53 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 16 Dec 2004 21:56:53 -0600
Subject: [R] RE: How about a mascot for R?
In-Reply-To: <1103253010.15397.8.camel@horizons.localdomain>
References: <86EED55AE819274B8308B62A225457D8016B866C@exch1.qsa.local>
	<1103253010.15397.8.camel@horizons.localdomain>
Message-ID: <41C25905.2080700@pdf.com>



Marc Schwartz wrote:

> On Fri, 2004-12-17 at 12:32 +1000, Andrew Ward wrote:
> 
>> Apologies for adding to this discussion so late.
>>
>>When the idea of a mascot was first raised, I
>>recalled how in describing (and spelling) R to
>>people in seminars, etc I would sometimes say,
>>in my best pirate voice, "That's R, as in ARRRRRR!".
>>
>>The connection to something or someone criminal
>>is perhaps not desirable, but I'm sure that there
>>is someone in the R community who could be a model
>>for the logo (long beard certainly, eye-patch or
>>wooden leg even better). And then clicking on the
>>logo would play the accompanying sound. I think
>>this has plenty of appeal.
>>
>>;-)
> 
> 
> 
> Perhaps we could get Johnny Depp (aka Jack Sparrow)?
> 
> http://www.imdb.com/gallery/ss/0325980/Ss/0325980/PiratesCN-36-6.jpg
> 
> Or perhaps Jason Isaacs (aka Captain Hook)?
> 
> http://www.imdb.com/gallery/ss/0316396/Ss/0316396/5576_D137_00042.JPG
> 
> ;-)
> 
> Marc
> 

Or Keith Richards (aka Keith Richards)?

http://images.google.com/images?sourceid=mozclient&ie=utf-8&oe=utf-8&q=keith+richards

--sundar



From michael.beer at unifr.ch  Fri Dec 17 09:33:00 2004
From: michael.beer at unifr.ch (BEER Michael)
Date: Fri, 17 Dec 2004 09:33:00 +0100
Subject: [R] PHP MySQL and R
Message-ID: <24D0F1947691984E89F3151A7DC314DD8D8369@EXCHANGE2.unifr.ch>

> -----------
> Problem #1:
> ...
> -----------

Consider using the library "RMySQL" (available at
http://stat.bell-labs.com/RS-DBI/download/) for directly accessing your
MySQL database from R.

--- bla.R ---
library("RMySQL")
con <- dbConnect(dbDriver("MySQL"), 
	  group = "<your group name as defined in $HOME/.my.cnf>")
res <- dbSendQuery(con, 
	  "SELECT x,y,lagged FROM DATA WHERE ds_id=(whatever dataset)")
userdata <- fetch(res, n = -1)
...
--- ***** ---

The only parameter you need to pass to your script from PHP would thus
be the current dataset ID. Maybe someone else can advise you on how to
do this in an elegant way.

 
> -----------
> Problem #2:
> ...
> -------------

Paste the appropriate values of summary.lm into an SQL statement
("INSERT INTO ...") and send this command to your database using
dbSendQuery(...) again.

Yours, Michael.



From froemke at bioinf.uni-hannover.de  Fri Dec 17 10:57:08 2004
From: froemke at bioinf.uni-hannover.de (Cornelia Froemke)
Date: Fri, 17 Dec 2004 10:57:08 +0100
Subject: [R] combined list boxes / tcltk
Message-ID: <5.2.0.9.0.20041217105152.01ba7e88@mail.bioinf.uni-hannover.de>

Dear list members,

I would like to combine two list boxes with tcltk. The source code runs 
fine - but when I choose in the upper box an item, the selected item in the 
lower box disappears and vice versa. Could anybody help me?

This is the source code:

##############
require(tcltk)

choose.variables<-tktoplevel()

choose.group<-tklistbox(choose.variables,height=4,selectmode="single",background="white")
tkgrid(tklabel(choose.variables,text="Select group variable"))
tkgrid(choose.group)
all.variables1 <- c("group1","group2","group3","group4")

for (i in (1:4))
{tkinsert(choose.group,"end",all.variables1[i])}
tkselection.set(choose.group,0)

choose.endpoint<-tklistbox(choose.variables,height=4,selectmode="single",background="white")
tkgrid(tklabel(choose.variables,text="Select endpoint variable"))
tkgrid(choose.endpoint)
all.variables2 <- c("endpoint1","endpoint2","endpoint3","endpoint4")

for (j in (1:4))
{tkinsert(choose.endpoint,0,all.variables2[j])}
tkselection.set(choose.endpoint,0)

OnOK <- function()
{groupChoice <- all.variables1[as.numeric(tkcurselection(choose.group))+1]
  endpointChoice <- 
all.variables2[as.numeric(tkcurselection(choose.endpoint))+1]
  tkdestroy(choose.variables)
  msg <- c(groupChoice, endpointChoice)
  tkmessageBox(message=msg)}

OK.but <-tkbutton(choose.variables,text="   OK   ",command=OnOK)
tkgrid(OK.but)
tkfocus(choose.variables)

##############

Thanks in advance,
Cornelia

******************************************************
Dipl.Ing.agr.
Cornelia Froemke
Lehrgebiet Bioinformatik
Fachbereich Gartenbau
Universitaet Hannover
Herrenhaeuser Str. 2
D-30419 Hannover

Tel.: 0511 762-5821
mailto:froemke at bioinf.uni-hannover.de



From Gronwald at econ.uni-hamburg.de  Fri Dec 17 11:13:47 2004
From: Gronwald at econ.uni-hamburg.de (Marc Gronwald)
Date: Fri, 17 Dec 2004 11:13:47 +0100
Subject: [R] VAR-Estimation
Message-ID: <001401c4e421$18a97930$8d7f6486@2131IWWT>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041217/610b86b0/attachment.pl

From fhduan at gmail.com  Fri Dec 17 11:47:54 2004
From: fhduan at gmail.com (Frank Duan)
Date: Fri, 17 Dec 2004 05:47:54 -0500
Subject: [R] How to interpret and modify "plot.svm"?
Message-ID: <3b9172310412170247590e362b@mail.gmail.com>

Dear R people,

I am trying to plot the results from running svm in library(e1071). I
use plot.svm. After searching through the help archives and FAQ, I
still have several questions:

1.  In default, crosses indicate support vectors. But why are there
two colors of crosses? What do they represent?

2. I want to draw a white-gray colored plot and modify the different
colored crosses or circles by different shaped points. Could anyone
give me a hint?

3. Is it possible for me to draw a "hyperplane" on the plot?

4. What is the algorithm to plot the contour region?

Thank you very much,

Frank



From c18g at zfn.uni-bremen.de  Fri Dec 17 11:47:58 2004
From: c18g at zfn.uni-bremen.de (Patrick Hausmann)
Date: Fri, 17 Dec 2004 11:47:58 +0100
Subject: [R] reshape and split
Message-ID: <1103280478.41c2b95e618f9@www2.zfn.uni-bremen.de>

Dear R-users,

I am trying to reshape the DF "dat2" in the "long" format,
but can't figure out how to use the "split"-option:

> dat2
   a.1995.z  b.1995.z  a.1996.z                    var
1 100.00000 100.00000 100.00000 Neue Anlagen insgesamt
2  40.09904  23.60890  40.88960      Neue Ausr??stungen
3  59.90096  76.39110  59.11040            Neue Bauten

This should be the result:

region	time	wz	value	var
a	1995	z	100	Neue Anlagen insgesamt
a	1995	z	40.09904	Neue Ausr??stungen
a	1995	z	59.90096	Neue Bauten
b	1995	z	100	Neue Anlagen insgesamt
b	1995	z	23.60890	Neue Ausr??stungen
b	1995	z	76.39110	Neue Bauten
a	1996	z	100	Neue Anlagen insgesamt
a	1996	z	40.88960	Neue Ausr??stungen
a	1996	z	59.11040	Neue Bauten

# Not working, sure "[.]" is missing
reshape(dat2, dir="long", varying=1:3, split=list(regexp="[0-9][a-z]",
include=T))

Thanks for any help!
Patrick



From maechler at stat.math.ethz.ch  Fri Dec 17 12:18:21 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 17 Dec 2004 12:18:21 +0100
Subject: [R] drawing a rectangle through multiple plots
In-Reply-To: <s1c1782e.089@lp-msg1.co.ihc.com>
References: <s1c1782e.089@lp-msg1.co.ihc.com>
Message-ID: <16834.49277.627108.683230@gargle.gargle.HOWL>

Hi Greg,

to attach files in e-mails for R-help, you must use the
"text/plain" MIME type; if you can't do this with your e-mail software, 
(besides considering to change your e-mail software :-)
- either put the file up for FTP or HTTP and only publish the URL
  on R-help
- or ``cut & paste'' into your message
  (with the well know "wrap around problems for people who like
  to have source files with lines that extend to more than about
  70 characters).

Regards,
Martin Maechler, R list maintainer, ETH Zurich



From andy_liaw at merck.com  Fri Dec 17 12:32:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Dec 2004 06:32:09 -0500
Subject: [R] How to interpret and modify "plot.svm"?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E443@usrymx25.merck.com>

David can give the definitive answers, but I'll give my take on this...

> From: Frank Duan
> 
> Dear R people,
> 
> I am trying to plot the results from running svm in library(e1071). I
> use plot.svm. After searching through the help archives and FAQ, I
> still have several questions:
> 
> 1.  In default, crosses indicate support vectors. But why are there
> two colors of crosses? What do they represent?

SVs are part of the training data, and as such, have class labels, right?
 
> 2. I want to draw a white-gray colored plot and modify the different
> colored crosses or circles by different shaped points. Could anyone
> give me a hint?

Supposedly possible with the ... argument, but it's not clear how to do
that, unless you read the code (which you can, of course).  Looks like some
graphical parameters are hardwired in there, so you might need to modify it
to your own liking.
 
> 3. Is it possible for me to draw a "hyperplane" on the plot?

Not sure what you mean.  You are looking at a 2D subspace.  How do you want
to draw hyperplane on such subspace?
 
> 4. What is the algorithm to plot the contour region?

Again, you can look at the code.  It does the logical thing: predict() on a
regular grid and plot that.

Andy
 
> Thank you very much,
> 
> Frank
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From kjetil at acelerate.com  Fri Dec 17 13:08:31 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 17 Dec 2004 08:08:31 -0400
Subject: [R] VAR-Estimation
In-Reply-To: <001401c4e421$18a97930$8d7f6486@2131IWWT>
References: <001401c4e421$18a97930$8d7f6486@2131IWWT>
Message-ID: <41C2CC3F.3030106@acelerate.com>

Marc Gronwald wrote:

>Hi,
>
>I want to estimate a VAR-model and calculate the impulse response function and a variance decomposition. I am familiar with standard R-functions, like e.g. arima. But I have not found equivalent functions to estimate a VAR-modell.
>
>Are there any functions available or which R-package can I use to solve my problem?
>
>Thank you for your help.
>
>Marc Gronwald
>University of Hamburg
>Department of Economics
>Von-Melle-Park 5
>
>D-20146 Hamburg
>
>GERMANY
>
>Fon: +49 (0)40 482325547
>Fax: +49 (0)40 482325546
>Mail: gronwald at econ.uni-hamburg.de
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>
You can have a look at package (bundle) dse.

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From r.hankin at soc.soton.ac.uk  Fri Dec 17 13:18:17 2004
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 17 Dec 2004 12:18:17 +0000
Subject: [R] take precisely one named argument
Message-ID: <BBA4E3BF-5025-11D9-946F-000A95D86AA8@soc.soton.ac.uk>

Hi

I want a function that takes precisely one named argument and no
unnamed arguments. The named argument must be one of "a" or "b".

If "a" is supplied, return "a".  If "b" is supplied, return 2*b.
That is, the desired behaviour is:

R> f(a=4)   #return 4
R> f(b=33)  #return 66
R> f(5)      #error
R> f(a=3,b=5)   #error
R> f(a=3,q=3)   #error
R> f(q=3)       #error

The following function is intended to implement this:

f <- function(a=NULL, b=NULL){
   if(!xor(is.null(a), is.null(b))){stop("specify exactly one of a and 
b")}
   if(is.null(a)){return(2*b)}else{return(a)}
}


It almost works, but  f(6) returns 6 (and should be an error).

What is the best way to accomplish my desired behaviour?

--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From bxc at steno.dk  Fri Dec 17 13:46:18 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 17 Dec 2004 13:46:18 +0100
Subject: [R] take precisely one named argument
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE11F@exdkba022.novo.dk>

specify:

f <- function(...,a=NULL,b=NULL) {...etc

Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
> Sent: Friday, December 17, 2004 1:18 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] take precisely one named argument
> 
> 
> Hi
> 
> I want a function that takes precisely one named argument and 
> no unnamed arguments. The named argument must be one of "a" or "b".
> 
> If "a" is supplied, return "a".  If "b" is supplied, return 
> 2*b. That is, the desired behaviour is:
> 
> R> f(a=4)   #return 4
> R> f(b=33)  #return 66
> R> f(5)      #error
> R> f(a=3,b=5)   #error
> R> f(a=3,q=3)   #error
> R> f(q=3)       #error
> 
> The following function is intended to implement this:
> 
> f <- function(a=NULL, b=NULL){
>    if(!xor(is.null(a), is.null(b))){stop("specify exactly one 
> of a and 
> b")}
>    if(is.null(a)){return(2*b)}else{return(a)}
> }
> 
> 
> It almost works, but  f(6) returns 6 (and should be an error).
> 
> What is the best way to accomplish my desired behaviour?
> 
> --
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From david.meyer at wu-wien.ac.at  Fri Dec 17 13:47:00 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Fri, 17 Dec 2004 13:47:00 +0100
Subject: [R] Problem with SVM and scaling
Message-ID: <20041217134700.7ba426b0.david.meyer@wu-wien.ac.at>


Ton:

Does preprocessing (scaling, removing constant variables, etc.) "by
hand" of the whole data set *before* splitting resolve things?

You will need the same variable structure in the training and the test
set anyway; scaling is just the first code part that fails on your
data...

g,
-d

-----

Hi all -
_
I am running into a problem with the SVM() method when applying it to
data sets that have descriptors with zero variance. Here is the sequence
of events:

1. I split my data set with 512 descriptors in a training and test set
2. I build an SVM model for the training set. Out of 512 descriptors,
500 have zero variance which I discard before calling the SVM method
3. For the test set, 8 descriptors have zero variance, which I discard
too
4. predict.svm() then fails, because it tries to scale using two vectors
of different size (500 and 504)

Is there a way to get around this?

-- 
Dr. David Meyer
Department of Information Systems

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From Ted.Harding at nessie.mcc.ac.uk  Fri Dec 17 13:56:37 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 17 Dec 2004 12:56:37 -0000 (GMT)
Subject: [R] take precisely one named argument
In-Reply-To: <BBA4E3BF-5025-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <XFMail.041217125637.Ted.Harding@nessie.mcc.ac.uk>

On 17-Dec-04 Robin Hankin wrote:
> Hi
> I want a function that takes precisely one named argument and no
> unnamed arguments. The named argument must be one of "a" or "b".
> 
> If "a" is supplied, return "a".  If "b" is supplied, return 2*b.
> That is, the desired behaviour is:
> 
> R> f(a=4)   #return 4
> R> f(b=33)  #return 66
> R> f(5)      #error
> R> f(a=3,b=5)   #error
> R> f(a=3,q=3)   #error
> R> f(q=3)       #error
> 
> The following function is intended to implement this:
> 
  f <- function(a=NULL, b=NULL){
     if(!xor(is.null(a), is.null(b))){
       stop("specify exactly one of a and b")
  }
     if(is.null(a)){return(2*b)}else{return(a)}
  }
> 
> 
> It almost works, but  f(6) returns 6 (and should be an error).
> 
> What is the best way to accomplish my desired behaviour?

I don't know the *best* way (expert R anatomists will know ... )
but the following dirty handed modification seems to do what
you want:

  f <- function(z=NULL, a=NULL, b=NULL){
     if(!is.null(z)){
       stop("usage: f(a=...) or f(b=...)")
     }
     if(!xor(is.null(a), is.null(b))){
       stop("specify exactly one of a and b")
     }
     if(is.null(a)){return(2*b)}else{return(a)}
  }

(This traps attempts to use f() with an un-named argument).

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 17-Dec-04                                       Time: 12:56:37
------------------------------ XFMail ------------------------------



From andy_liaw at merck.com  Fri Dec 17 14:16:04 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Dec 2004 08:16:04 -0500
Subject: [R] take precisely one named argument
Message-ID: <3A822319EB35174CA3714066D590DCD50994E444@usrymx25.merck.com>

Here's my attempt:

> f <- function(...) {
+     argList <- list(...)
+     if (length(argList) != 1) stop("Wrong number of arguments!")
+     if (length(names(argList)) != 1 || ! names(argList) %in% c("a", "b"))
+         stop("Wrong name for argument!")
+     x <- argList[[1]] * if (names(argList) == "a") 1 else 2
+     x
+ }
> f(a=6)
Error in f(6) : Wrong name for argument!
> f(b=6)
[1] 6
> f(c=6)
[1] 12
> f(c=6)
Error in f(c = 6) : Wrong name for argument!

HTH,
Andy


> From: Robin Hankin
> 
> Hi
> 
> I want a function that takes precisely one named argument and no
> unnamed arguments. The named argument must be one of "a" or "b".
> 
> If "a" is supplied, return "a".  If "b" is supplied, return 2*b.
> That is, the desired behaviour is:
> 
> R> f(a=4)   #return 4
> R> f(b=33)  #return 66
> R> f(5)      #error
> R> f(a=3,b=5)   #error
> R> f(a=3,q=3)   #error
> R> f(q=3)       #error
> 
> The following function is intended to implement this:
> 
> f <- function(a=NULL, b=NULL){
>    if(!xor(is.null(a), is.null(b))){stop("specify exactly one 
> of a and 
> b")}
>    if(is.null(a)){return(2*b)}else{return(a)}
> }
> 
> 
> It almost works, but  f(6) returns 6 (and should be an error).
> 
> What is the best way to accomplish my desired behaviour?
> 
> --
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From joseclaudio.faria at terra.com.br  Fri Dec 17 14:19:38 2004
From: joseclaudio.faria at terra.com.br (=?ISO-8859-1?Q?Jos=E9_Cl=E1udio_Faria?=)
Date: Fri, 17 Dec 2004 10:19:38 -0300
Subject: [R] Doubts about chi-square distribution
Message-ID: <41C2DCEA.6030007@terra.com.br>

Dear list,

For educational purposes I have been working with the script below.

I have a observation:

   line 31

   #CScal[i] = (amo^2)  # IT IS WRONG, I KNOW, BUT IT MAKE R TO CRASHES!

   I'm thinking this is a possible bug in the R!


And I have a couple of doubts:

1) line 51

    #curve(dchisq(x, n-1), add = T, col = 'red'

    I think that it is correct, but the function no match with the observed 
distribution.

    curve(dchisq(x, n), add = T, col = 'red') matches, so, what is wrong?

2) Plot

    I would like to distance the Ylabel from the left limit of the screen, is it 
possible?

Many thanks,

-- 
Jos?? Cl??udio Faria
Brasil/Bahia/UESC/DCET
Estat??stica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br


# Title   : Origin chi-square distribution
# Author  : Jos?? Cl??udio Faria
# Date    : 16/12/2004
# Version : v1
#-------------------------------------------------------------------------------

   #---------------------- Begin informations -----------------------------------

   # Populational parameters
   Mpop   = 0        # Mean
   Vpop   = 1        # Variance
   N      = 10000    # Size

   # Sampling
   n      = 10       # size of the sample
   sr     = 10000    # sampling repetition

   # Plot parameter
   nchist = 150

   #---------------------- End informations -------------------------------------

   CScal = numeric();  # CScal = Chi-square calculated

   pop = rnorm(N, Mpop, sqrt(Vpop))  # pop~N(Mpop,Vpop)

   for (i in 1:sr)
   {
     amo = sample(pop, n, replace = TRUE)
     #====================================================================
     #CScal[i] = (amo^2)  # IT IS WRONG, I KNOW, BUT IT MAKE R TO CRASHES!
     #====================================================================
     CScal[i] = sum(amo^2)
   }

   win.graph(w = 6, h = 7)
   split.screen(c(2,1))

   screen(1)
   hist(CScal, breaks = nchist, col = 'gray', main = 'Histogram',
        xlab = NULL, ylab = 'Absolute frequence', font.lab = 2, font = 2)
   mtext(expression(chi^2==sum(amo^2)), side = 3, col = 'red', font = 2)

   screen(2)
   hist(CScal, probability = T, breaks = nchist, col = 'gray', main = 'Density',
        xlab = expression(chi^2), ylab = expression(f(chi^2)), font.lab = 2, 
font = 2)
   mtext(expression(chi^2==sum(amo^2)), side = 3, col = "red", font = 2)

   x = CScal
   #===========================================
   # I THINK THIS WAY IS CORRECT (n-1)
   #curve(dchisq(x, n-1), add = T, col = 'red')
   #===========================================
   curve(dchisq(x, n), add = T, col = 'red')


   cat('\nPopulation:'); cat('\n')
   cat('\tMean =', Mpop); cat('\n')
   cat('\tVariance =', Vpop); cat('\n')
   cat('\tSize (N) =', N); cat('\n')
   cat('\nSample:'); cat('\n')
   cat('\tSize (n) =', n, '->', (n - 1), 'df'); cat('\n')
   cat('\nSampling:'); cat('\n')
   cat('\tRepetitions =', sr); cat('\n\n')



From andy_liaw at merck.com  Fri Dec 17 14:23:37 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Dec 2004 08:23:37 -0500
Subject: [R] take precisely one named argument
Message-ID: <3A822319EB35174CA3714066D590DCD50994E445@usrymx25.merck.com>

> From: Liaw, Andy
> Here's my attempt:
> 
> > f <- function(...) {
> +     argList <- list(...)
> +     if (length(argList) != 1) stop("Wrong number of arguments!")
> +     if (length(names(argList)) != 1 || ! names(argList) 
> %in% c("a", "b"))
> +         stop("Wrong name for argument!")
> +     x <- argList[[1]] * if (names(argList) == "a") 1 else 2
> +     x
> + }
> > f(a=6)
> Error in f(6) : Wrong name for argument!
> > f(b=6)
> [1] 6
> > f(c=6)
> [1] 12

This obviously _looks_ wrong, but that's only because I was running R under
ESS, and up-arrowed to edit the commands.  The correct transcript for this
part ought to look like:

> f(a=6)
[1] 6
> f(b=12)
[1] 24
> f(b=6)
[1] 12
> f(c=6)
Error in f(c = 6) : Wrong name for argument!

Andy

> > f(c=6)
> Error in f(c = 6) : Wrong name for argument!
> 
> HTH,
> Andy
> 
> 
> > From: Robin Hankin
> > 
> > Hi
> > 
> > I want a function that takes precisely one named argument and no
> > unnamed arguments. The named argument must be one of "a" or "b".
> > 
> > If "a" is supplied, return "a".  If "b" is supplied, return 2*b.
> > That is, the desired behaviour is:
> > 
> > R> f(a=4)   #return 4
> > R> f(b=33)  #return 66
> > R> f(5)      #error
> > R> f(a=3,b=5)   #error
> > R> f(a=3,q=3)   #error
> > R> f(q=3)       #error
> > 
> > The following function is intended to implement this:
> > 
> > f <- function(a=NULL, b=NULL){
> >    if(!xor(is.null(a), is.null(b))){stop("specify exactly one 
> > of a and 
> > b")}
> >    if(is.null(a)){return(2*b)}else{return(a)}
> > }
> > 
> > 
> > It almost works, but  f(6) returns 6 (and should be an error).
> > 
> > What is the best way to accomplish my desired behaviour?
> > 
> > --
> > Robin Hankin
> > Uncertainty Analyst
> > Southampton Oceanography Centre
> > European Way, Southampton SO14 3ZH, UK
> >   tel  023-8059-7743
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From nicolas.deig at epfl.ch  Fri Dec 17 14:28:19 2004
From: nicolas.deig at epfl.ch (NICOLAS DEIG)
Date: Fri, 17 Dec 2004 14:28:19 +0100
Subject: [R] (no subject)
Message-ID: <32fd3e6f.3e6f32fd@imap.epfl.ch>

hello,

i am encoutering problems with a function of R concerning the
classification trees. 

In the library {tree}, when I use the function "tree" to grow a
classification tree it should give me an object of class "tree". Then I
should be allowed to use this "tree" object in the function "cv.tree"
that  requires an object of class "tree" in order to run a
cross-validation experiment. 
There is the problem, when I do this the following message appears:
"Error in as.data.frame.default(data) : can't coerce function into a
data.frame". 
So instead of considering the object as an  object of class tree the
function "cv.tree" handles it like a data.frame.

Does anyone knows why or how to work on a class "tree" object so that
"cv.tree" can handle it?

Thanks in advance...
Nicolas Deig



From andy_liaw at merck.com  Fri Dec 17 14:46:12 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Dec 2004 08:46:12 -0500
Subject: [R] (no subject)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E446@usrymx25.merck.com>



> -----Original Message-----
> From: NICOLAS DEIG
> 
> hello,
> 
> i am encoutering problems with a function of R concerning the
> classification trees. 
> 
> In the library {tree}, when I use the function "tree" to grow a
         ^^^^^^^

That should be "package"...

> classification tree it should give me an object of class 
> "tree". Then I
> should be allowed to use this "tree" object in the function "cv.tree"
> that  requires an object of class "tree" in order to run a
> cross-validation experiment. 
> There is the problem, when I do this the following message appears:
> "Error in as.data.frame.default(data) : can't coerce function into a
> data.frame". 
> So instead of considering the object as an  object of class tree the
> function "cv.tree" handles it like a data.frame.
> 
> Does anyone knows why or how to work on a class "tree" object so that
> "cv.tree" can handle it?

Please show how you did it the the error you got.  I can't reproduce the
problem you described:

> library(tree)
> iris.tree <- tree(Species ~ ., iris)
> iris.cvtree <- cv.tree(iris.tree)
> iris.cvtree
$size
[1] 6 5 4 3 2 1

$dev
[1]  52.33779  45.94317  46.28027  63.43450 151.84608 335.57375

$k
[1]       -Inf   4.228650   4.717398  15.957916  95.676543 190.954250

$method
[1] "deviance"

attr(,"class")
[1] "prune"         "tree.sequence"

Andy


 
> Thanks in advance...
> Nicolas Deig
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jfox at mcmaster.ca  Fri Dec 17 14:48:04 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 17 Dec 2004 08:48:04 -0500
Subject: [R] combined list boxes / tcltk
In-Reply-To: <5.2.0.9.0.20041217105152.01ba7e88@mail.bioinf.uni-hannover.de>
Message-ID: <20041217134804.GKTB1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Cornelia,

Add the argument  exportselection=FALSE  to tklistbox().

I hope this hleps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Cornelia Froemke
> Sent: Friday, December 17, 2004 4:57 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] combined list boxes / tcltk
> 
> Dear list members,
> 
> I would like to combine two list boxes with tcltk. The source 
> code runs fine - but when I choose in the upper box an item, 
> the selected item in the lower box disappears and vice versa. 
> Could anybody help me?
> 
> This is the source code:
> 
> ##############
> require(tcltk)
> 
> choose.variables<-tktoplevel()
> 
> choose.group<-tklistbox(choose.variables,height=4,selectmode="
> single",background="white")
> tkgrid(tklabel(choose.variables,text="Select group variable"))
> tkgrid(choose.group)
> all.variables1 <- c("group1","group2","group3","group4")
> 
> for (i in (1:4))
> {tkinsert(choose.group,"end",all.variables1[i])}
> tkselection.set(choose.group,0)
> 
> choose.endpoint<-tklistbox(choose.variables,height=4,selectmod
> e="single",background="white")
> tkgrid(tklabel(choose.variables,text="Select endpoint variable"))
> tkgrid(choose.endpoint)
> all.variables2 <- c("endpoint1","endpoint2","endpoint3","endpoint4")
> 
> for (j in (1:4))
> {tkinsert(choose.endpoint,0,all.variables2[j])}
> tkselection.set(choose.endpoint,0)
> 
> OnOK <- function()
> {groupChoice <- 
> all.variables1[as.numeric(tkcurselection(choose.group))+1]
>   endpointChoice <-
> all.variables2[as.numeric(tkcurselection(choose.endpoint))+1]
>   tkdestroy(choose.variables)
>   msg <- c(groupChoice, endpointChoice)
>   tkmessageBox(message=msg)}
> 
> OK.but <-tkbutton(choose.variables,text="   OK   ",command=OnOK)
> tkgrid(OK.but)
> tkfocus(choose.variables)
> 
> ##############
> 
> Thanks in advance,
> Cornelia
> 
> ******************************************************
> Dipl.Ing.agr.
> Cornelia Froemke
> Lehrgebiet Bioinformatik
> Fachbereich Gartenbau
> Universitaet Hannover
> Herrenhaeuser Str. 2
> D-30419 Hannover
> 
> Tel.: 0511 762-5821
> mailto:froemke at bioinf.uni-hannover.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Dec 17 14:53:02 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Dec 2004 14:53:02 +0100
Subject: [R] Doubts about chi-square distribution
In-Reply-To: <41C2DCEA.6030007@terra.com.br>
References: <41C2DCEA.6030007@terra.com.br>
Message-ID: <x2llbxkp4x.fsf@biostat.ku.dk>

Jos?? Cl??udio Faria <joseclaudio.faria at terra.com.br> writes:

> Dear list,
> 
> For educational purposes I have been working with the script below.
> 
> I have a observation:
> 
>    line 31
> 
>    #CScal[i] = (amo^2)  # IT IS WRONG, I KNOW, BUT IT MAKE R TO CRASHES!
> 
>    I'm thinking this is a possible bug in the R!

Possibly, but ***WHICH R?*** 

I see a segmentation fault on 1.9.1 on i386 but an error message with
2.0.1 on Opteron.
 
(We did remember to tell you to include full version information in
the posting guide, did we not?)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Fri Dec 17 14:47:34 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 17 Dec 2004 13:47:34 -0000 (GMT)
Subject: [R] take precisely one named argument
In-Reply-To: <XFMail.041217125637.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.041217134734.Ted.Harding@nessie.mcc.ac.uk>

On 17-Dec-04 Ted Harding wrote:
> I don't know the *best* way (expert R anatomists will know ... )
> but the following dirty handed modification seems to do what
> you want:
> 
>   f <- function(z=NULL, a=NULL, b=NULL){
>      if(!is.null(z)){
>        stop("usage: f(a=...) or f(b=...)")
>      }
>      if(!xor(is.null(a), is.null(b))){
>        stop("specify exactly one of a and b")
>      }
>      if(is.null(a)){return(2*b)}else{return(a)}
>   }
> 
> (This traps attempts to use f() with an un-named argument).
> 
> Ted.

Playing around with the above shows that not only does it
give the correct response for correct usage, e.g. f(a=3) or f(b=3),
but, serendipitously, it gives appropriate responses for just
about any way you could think of using it wrongly:

> f(3)
Error in f(3) : usage: f(a=...) or f(b=...)

> f(a=2,b=3)
Error in f(a = 2, b = 3) : specify exactly one of a and b

> f(d=3)
Error in f(d = 3) : unused argument(s) (d ...)

> f(a=2,d=3)
Error in f(a = 2, d = 3) : unused argument(s) (d ...)

etc.

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 17-Dec-04                                       Time: 13:47:34
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Fri Dec 17 15:26:37 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Dec 2004 15:26:37 +0100
Subject: [R] take precisely one named argument
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E9027FE11F@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E9027FE11F@exdkba022.novo.dk>
Message-ID: <x2hdmlknky.fsf@biostat.ku.dk>

"BXC (Bendix Carstensen)" <bxc at steno.dk> writes:

> specify:
> 
> f <- function(...,a=NULL,b=NULL) {...etc

Or (variant of same)

f <- function(..., a, b) {
    if (nargs() != 1 || length(list(...)))
          stop("precisely one of a=x or b=y must be given")
    else
          if (missing(a)) 2*b else a
}


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From paterijk at hotmail.com  Fri Dec 17 15:34:13 2004
From: paterijk at hotmail.com (Pat Meyer)
Date: Fri, 17 Dec 2004 14:34:13 +0000
Subject: [R] Matrix and rownames problem
Message-ID: <BAY1-F27E70BE76CE450D7814A06D5AF0@phx.gbl>

Hi,
I'm quite new to R, so excuse me if this problem has a simple solution.

I'm working with an array, lets say

i <- array(c(1:3,3:1), dim=c(3,2))

Then I want to give the rows and the columns names:

rownames(i)<-c("a","b","c")
colnames(i)<-c("d","e")

The result is given below:

  d e
a 1 3
b 2 2
c 3 1

Here comes my problem. When I'm taking a submatrix

j<-i[1,1:2]

the result should be (for me) an array of one line, and two colums. Here's 
the result:

d e
1 3

When I want to access the rownames of j, it returns NULL. I want it to be 
"a".

On the other side, if I take a submatrix 2x2, there is no problem.

In my problem, rownames(j) must return the name of the extracted row. So I 
don't understand why a 1x2 array is not a normal array.

Could someone help me with this?

Thanx in advance,

Patrick



From fisher at plessthan.com  Fri Dec 17 15:45:36 2004
From: fisher at plessthan.com (Dennis Fisher)
Date: Fri, 17 Dec 2004 06:45:36 -0800
Subject: [R] Is the page number in a document created with "pdf()"
	accessible?
Message-ID: <501D335A-503A-11D9-957A-0011242E1C5E@plessthan.com>

When I create pdf documents in R using pdf(), each page has text in the 
outer margin indicating the page number.  I track page numbers in a 
cumbersome manner.  Is the page number in a multi-page document tracked 
internally by par() or some other function?

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-8864)
Fax: 1-415-564-2220
www.PLessThan.com



From jfox at mcmaster.ca  Fri Dec 17 15:46:50 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 17 Dec 2004 09:46:50 -0500
Subject: [R] Matrix and rownames problem
In-Reply-To: <BAY1-F27E70BE76CE450D7814A06D5AF0@phx.gbl>
Message-ID: <20041217144658.CRBQ1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Patrick,

By default, when indexing returns an array dimension of 1, the corresponding
coordinate is dropped. Try  j <- i[1, 1:2, drop=FALSE], and see ?"[".

I hope this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pat Meyer
> Sent: Friday, December 17, 2004 9:34 AM
> To: r-help at stat.math.ethz.ch
> Cc: patrick.meyer at internet.lu
> Subject: [R] Matrix and rownames problem
> 
> Hi,
> I'm quite new to R, so excuse me if this problem has a simple 
> solution.
> 
> I'm working with an array, lets say
> 
> i <- array(c(1:3,3:1), dim=c(3,2))
> 
> Then I want to give the rows and the columns names:
> 
> rownames(i)<-c("a","b","c")
> colnames(i)<-c("d","e")
> 
> The result is given below:
> 
>   d e
> a 1 3
> b 2 2
> c 3 1
> 
> Here comes my problem. When I'm taking a submatrix
> 
> j<-i[1,1:2]
> 
> the result should be (for me) an array of one line, and two 
> colums. Here's the result:
> 
> d e
> 1 3
> 
> When I want to access the rownames of j, it returns NULL. I 
> want it to be "a".
> 
> On the other side, if I take a submatrix 2x2, there is no problem.
> 
> In my problem, rownames(j) must return the name of the 
> extracted row. So I don't understand why a 1x2 array is not a 
> normal array.
> 
> Could someone help me with this?
> 
> Thanx in advance,
> 
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Fri Dec 17 15:56:07 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 17 Dec 2004 08:56:07 -0600
Subject: [R] Matrix and rownames problem
In-Reply-To: <BAY1-F27E70BE76CE450D7814A06D5AF0@phx.gbl>
References: <BAY1-F27E70BE76CE450D7814A06D5AF0@phx.gbl>
Message-ID: <1103295367.15397.26.camel@horizons.localdomain>

On Fri, 2004-12-17 at 14:34 +0000, Pat Meyer wrote:
> Hi,
> I'm quite new to R, so excuse me if this problem has a simple solution.

It does. It's in FAQ 7.5 "Why do my matrices lose dimensions?"

> I'm working with an array, lets say
> 
> i <- array(c(1:3,3:1), dim=c(3,2))
> 
> Then I want to give the rows and the columns names:
> 
> rownames(i)<-c("a","b","c")
> colnames(i)<-c("d","e")
> 
> The result is given below:
> 
>   d e
> a 1 3
> b 2 2
> c 3 1
> 
> Here comes my problem. When I'm taking a submatrix
> 
> j<-i[1,1:2]

If you just want the first row, you can use:

j <- i[1, ]

> the result should be (for me) an array of one line, and two colums. Here's 
> the result:
> 
> d e
> 1 3
> 
> When I want to access the rownames of j, it returns NULL. I want it to be 
> "a".
> 
> On the other side, if I take a submatrix 2x2, there is no problem.
> 
> In my problem, rownames(j) must return the name of the extracted row. So I 
> don't understand why a 1x2 array is not a normal array.
> 
> Could someone help me with this?
> 
> Thanx in advance,

As per the FAQ referenced above, use:

> j <- i[1, , drop = FALSE]

> j
  d e
a 1 3


HTH,

Marc Schwartz



From saurin_jani at yahoo.com  Fri Dec 17 15:59:26 2004
From: saurin_jani at yahoo.com (Saurin Jani)
Date: Fri, 17 Dec 2004 06:59:26 -0800 (PST)
Subject: [R] How can I take anti log of log base 2 values in R
Message-ID: <20041217145926.81274.qmail@web41124.mail.yahoo.com>

Hi,

I am using R for microarray data anlaysis. When I
normalize my data, it converts all my data in to log
base 2 values. how can I convert back to log base
10..is there any function in R which  I can use or how
can I take anti  log. or is there any function in R
for antilog. 


Please let me know,..if anyone knows..

Thank you so much,
Saurin

=====
Saurin's WebWorld: http://hawkmail.monmouth.edu/~s0451884



From bxc at steno.dk  Fri Dec 17 16:23:03 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 17 Dec 2004 16:23:03 +0100
Subject: [R] How can I take anti log of log base 2 values in R
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE12F@exdkba022.novo.dk>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Saurin Jani
> Sent: Friday, December 17, 2004 3:59 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How can I take anti log of log base 2 values in R
> 
> 
> Hi,
> 
> I am using R for microarray data anlaysis. When I
> normalize my data, it converts all my data in to log
> base 2 values. how can I convert back to log base
> 10..is there any function in R which  I can use or how
> can I take anti  log. or is there any function in R
> for antilog. 

This is a math question, from a very early course in analysis:

Any two logarithms are proportional, so:

log10(x) = K log2(x) for any x

In particular for x=2, and since log2(2)=1:

log10(2) = K

so we have in general:

log10(x) = log10(2) * log2(x)

The antilog for the natural log is exp(x), 
for the base 2 log it is 2^x and 
for the base 10 log it is 10^x.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------

> Please let me know,..if anyone knows..
> 
> Thank you so much,
> Saurin
> 
> =====
> Saurin's WebWorld: http://hawkmail.monmouth.edu/~s0451884
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Fri Dec 17 16:31:38 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Dec 2004 10:31:38 -0500
Subject: [R] Doubts about chi-square distribution
Message-ID: <3A822319EB35174CA3714066D590DCD50994E448@usrymx25.merck.com>

> From: Peter Dalgaard
> 
> Jos?? Cl??udio Faria <joseclaudio.faria at terra.com.br> writes:
> 
> > Dear list,
> > 
> > For educational purposes I have been working with the script below.
> > 
> > I have a observation:
> > 
> >    line 31
> > 
> >    #CScal[i] = (amo^2)  # IT IS WRONG, I KNOW, BUT IT MAKE 
> R TO CRASHES!
> > 
> >    I'm thinking this is a possible bug in the R!
> 
> Possibly, but ***WHICH R?*** 

It happens with R-2.0.1 pre-compiled binary from CRAN for Windows, on XPPro
SP2.  Weird...

BTW, I'd recommend Jose to do something like:

simCS <- function(Mpop=0, Vpop=1, N=1e4, n=10, sr=1e4, nchist=150) {
    pop = rnorm(N, Mpop, sqrt(Vpop))  # pop~N(Mpop,Vpop)
    CScal <- replicate(sr, sum(sample(pop, n, replace=TRUE)^2))
    [insert the rest of the code here]
}

Andy



> I see a segmentation fault on 1.9.1 on i386 but an error message with
> 2.0.1 on Opteron.
>  
> (We did remember to tell you to include full version information in
> the posting guide, did we not?)
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Fri Dec 17 16:31:52 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 17 Dec 2004 15:31:52 +0000 (UTC)
Subject: [R] take precisely one named argument
References: <BBA4E3BF-5025-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <loom.20041217T161825-325@post.gmane.org>

Robin Hankin <r.hankin <at> soc.soton.ac.uk> writes:

: 
: Hi
: 
: I want a function that takes precisely one named argument and no
: unnamed arguments. The named argument must be one of "a" or "b".
: 
: If "a" is supplied, return "a".  If "b" is supplied, return 2*b.
: That is, the desired behaviour is:
: 
: R> f(a=4)   #return 4
: R> f(b=33)  #return 66
: R> f(5)      #error
: R> f(a=3,b=5)   #error
: R> f(a=3,q=3)   #error
: R> f(q=3)       #error
: 
: The following function is intended to implement this:
: 
: f <- function(a=NULL, b=NULL){
:    if(!xor(is.null(a), is.null(b))){stop("specify exactly one of a and 
: b")}
:    if(is.null(a)){return(2*b)}else{return(a)}
: }
: 
: It almost works, but  f(6) returns 6 (and should be an error).
: 
: What is the best way to accomplish my desired behaviour?

Here is one way to do it.  nm are the names (where the [-1] removes
the function name).  The ... traps any arg that is not a or b and the 
stopifnot conditions ensure that exactly one of a and b are specified.  


ff <- function(..., a = 0, b = 0) {
	nm <- names(match.call()[-1])
	stopifnot(length(nm) == 1, nm %in% c("a", "b"))
	a+2*b
}

One thing to watch out for is that if



From MSchwartz at MedAnalytics.com  Fri Dec 17 16:33:14 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 17 Dec 2004 09:33:14 -0600
Subject: [R] Is the page number in a document created with "pdf()"
	accessible?
In-Reply-To: <501D335A-503A-11D9-957A-0011242E1C5E@plessthan.com>
References: <501D335A-503A-11D9-957A-0011242E1C5E@plessthan.com>
Message-ID: <1103297594.15397.42.camel@horizons.localdomain>

On Fri, 2004-12-17 at 06:45 -0800, Dennis Fisher wrote:
> When I create pdf documents in R using pdf(), each page has text in the 
> outer margin indicating the page number.  I track page numbers in a 
> cumbersome manner.  Is the page number in a multi-page document tracked 
> internally by par() or some other function?

I am confused. Is the behavior of pdf() on Macs (I presume you are using
a Mac from your e-mail headers) different than on Linux?

If I create a single pdf() or postscript() document that contains
multiple plots using "onefile = TRUE", I do not get automatically
generated page numbers on each plot.

Are you embedding or reading the R generated PDF file in some other
application that reads the PDF file page sequence and then appends the
page number?

FWIW, a scan of the code in .../src/main/devPS.c, which I believe is
shared by the PDF device, indicates that the page counter variable
'pageno' is hard coded to start at 0 and increment by one for each new
page. Thus, it is not affected by other 'options' or 'pars'.

Unless I am missing something or the pdf() function behaves differently
on a Mac, I am not sure what and how you are seeing what you do.

In a vacuum, I would recommend that you create only one R plot per PDF
file, which would then enable you to control the page numbering in your
other application or to perhaps use pdflatex for greater control.

HTH,

Marc Schwartz



From michael.watson at bbsrc.ac.uk  Fri Dec 17 16:41:38 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 17 Dec 2004 15:41:38 -0000
Subject: [R] If it's not a data.frame, matrix or vector, what is it?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89A38@iahce2knas1.iah.bbsrc.reserved>

Hi

Forgive my ignorance.  I am selecting a column of a data.frame using the
column name, and I want to know what the resulting column "is".  My data
frame is called "submin" and the column name is held in a variable
called "display.gname" Eg:

> is.data.frame(submin)
[1] TRUE
> is.data.frame(submin[,display.gname])
[1] FALSE
> is.matrix(submin[,display.gname])
[1] FALSE
> is.vector(submin[,display.gname])
[1] FALSE
> length(submin[,display.gname])
[1] 4622

So if it's not a data.frame, a matrix or a vector, what is "it"?

Thanks (and sorry if this is in the FAQ)

Mick



From andy_liaw at merck.com  Fri Dec 17 16:54:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Dec 2004 10:54:54 -0500
Subject: [R] If it's not a data.frame, matrix or vector, what is it?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E449@usrymx25.merck.com>

What does str(submin$display.gname) say?

Andy

> From: michael watson (IAH-C)
> 
> Hi
> 
> Forgive my ignorance.  I am selecting a column of a 
> data.frame using the
> column name, and I want to know what the resulting column 
> "is".  My data
> frame is called "submin" and the column name is held in a variable
> called "display.gname" Eg:
> 
> > is.data.frame(submin)
> [1] TRUE
> > is.data.frame(submin[,display.gname])
> [1] FALSE
> > is.matrix(submin[,display.gname])
> [1] FALSE
> > is.vector(submin[,display.gname])
> [1] FALSE
> > length(submin[,display.gname])
> [1] 4622
> 
> So if it's not a data.frame, a matrix or a vector, what is "it"?
> 
> Thanks (and sorry if this is in the FAQ)
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From michael.watson at bbsrc.ac.uk  Fri Dec 17 17:13:13 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 17 Dec 2004 16:13:13 -0000
Subject: [R] If it's not a data.frame, matrix or vector, what is it?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B8E3@iahce2knas1.iah.bbsrc.reserved>

OK I'm definitely being a bit ignorant (so please accept my apologies)
of R and it's internal data storage.

When I created the data.frame, I did use I() with this column - the
original data being a vector created simply with "vector()".  It's clear
now that I didn't realise exactly what this was doing (all I wanted was
for it to NOT be converted to a factor).  

> str(submin[,display.gname])
Class 'AsIs'  chr [1:4622] "" "" "" "" ...
> is.array(submin[,display.gname])
[1] FALSE
> class(submin[,display.gname])
[1] "AsIs"

But I am still confused; I program mainly in perl and java, and this
"thing" - it's not a vector, an array, a matrix or a data.frame - so
what is "it"? :-S

Thanks 

Mick


-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: 17 December 2004 15:55
To: michael watson (IAH-C); r-help at stat.math.ethz.ch
Subject: RE: [R] If it's not a data.frame, matrix or vector, what is it?


What does str(submin$display.gname) say?

Andy

> From: michael watson (IAH-C)
> 
> Hi
> 
> Forgive my ignorance.  I am selecting a column of a
> data.frame using the
> column name, and I want to know what the resulting column 
> "is".  My data
> frame is called "submin" and the column name is held in a variable
> called "display.gname" Eg:
> 
> > is.data.frame(submin)
> [1] TRUE
> > is.data.frame(submin[,display.gname])
> [1] FALSE
> > is.matrix(submin[,display.gname])
> [1] FALSE
> > is.vector(submin[,display.gname])
> [1] FALSE
> > length(submin[,display.gname])
> [1] 4622
> 
> So if it's not a data.frame, a matrix or a vector, what is "it"?
> 
> Thanks (and sorry if this is in the FAQ)
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------
------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ggrothendieck at myway.com  Fri Dec 17 17:16:26 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 17 Dec 2004 16:16:26 +0000 (UTC)
Subject: [R] take precisely one named argument
References: <BBA4E3BF-5025-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
	<loom.20041217T161825-325@post.gmane.org>
Message-ID: <loom.20041217T171040-530@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Robin Hankin <r.hankin <at> soc.soton.ac.uk> writes:
: 
: : 
: : Hi
: : 
: : I want a function that takes precisely one named argument and no
: : unnamed arguments. The named argument must be one of "a" or "b".
: : 
: : If "a" is supplied, return "a".  If "b" is supplied, return 2*b.
: : That is, the desired behaviour is:
: : 
: : R> f(a=4)   #return 4
: : R> f(b=33)  #return 66
: : R> f(5)      #error
: : R> f(a=3,b=5)   #error
: : R> f(a=3,q=3)   #error
: : R> f(q=3)       #error
: : 
: : The following function is intended to implement this:
: : 
: : f <- function(a=NULL, b=NULL){
: :    if(!xor(is.null(a), is.null(b))){stop("specify exactly one of a and 
: : b")}
: :    if(is.null(a)){return(2*b)}else{return(a)}
: : }
: : 
: : It almost works, but  f(6) returns 6 (and should be an error).
: : 
: : What is the best way to accomplish my desired behaviour?
: 
: Here is one way to do it.  nm are the names (where the [-1] removes
: the function name).  The ... traps any arg that is not a or b and the 
: stopifnot conditions ensure that exactly one of a and b are specified.  
: 
: ff <- function(..., a = 0, b = 0) {
: 	nm <- names(match.call()[-1])
: 	stopifnot(length(nm) == 1, nm %in% c("a", "b"))
: 	a+2*b
: }
: 
: One thing to watch out for is that if

Seems this got cut off somehow.

I was going to mention that while the above works, if the variables
are aa and bb rather than a and b then ... results in only exact matches
working.  That is a or b will be trapped by ... so only aa written
exactly and bb written exactly will match the aa and bb args.

Also, one could replace c("a", "b") with names(formals()[-1]) which
would make it easier to change variable names or add more variable
names in the future since the condition would not change.



From andy_liaw at merck.com  Fri Dec 17 17:26:37 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Dec 2004 11:26:37 -0500
Subject: [R] If it's not a data.frame, matrix or vector, what is it?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E44A@usrymx25.merck.com>

It may be educational to read the help pages of functions that you're using,
namely ?I (or even ?AsIs) and ?as.vector.  ?I says it simply prepend "AsIs"
to the class attribute of the object.  ?as.vector says it returns FALSE if
the object has any attributes except names (which it does in this case: the
class).  Note that str() does tell you that it's `chr', or character.  You
can unclass() it to see what it actually is.

HTH,
Andy


> From: michael watson (IAH-C) 
> 
> 
> OK I'm definitely being a bit ignorant (so please accept my apologies)
> of R and it's internal data storage.
> 
> When I created the data.frame, I did use I() with this column - the
> original data being a vector created simply with "vector()".  
> It's clear
> now that I didn't realise exactly what this was doing (all I 
> wanted was
> for it to NOT be converted to a factor).  
> 
> > str(submin[,display.gname])
> Class 'AsIs'  chr [1:4622] "" "" "" "" ...
> > is.array(submin[,display.gname])
> [1] FALSE
> > class(submin[,display.gname])
> [1] "AsIs"
> 
> But I am still confused; I program mainly in perl and java, and this
> "thing" - it's not a vector, an array, a matrix or a data.frame - so
> what is "it"? :-S
> 
> Thanks 
> 
> Mick
> 
> 
> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com] 
> Sent: 17 December 2004 15:55
> To: michael watson (IAH-C); r-help at stat.math.ethz.ch
> Subject: RE: [R] If it's not a data.frame, matrix or vector, 
> what is it?
> 
> 
> What does str(submin$display.gname) say?
> 
> Andy
> 
> > From: michael watson (IAH-C)
> > 
> > Hi
> > 
> > Forgive my ignorance.  I am selecting a column of a
> > data.frame using the
> > column name, and I want to know what the resulting column 
> > "is".  My data
> > frame is called "submin" and the column name is held in a variable
> > called "display.gname" Eg:
> > 
> > > is.data.frame(submin)
> > [1] TRUE
> > > is.data.frame(submin[,display.gname])
> > [1] FALSE
> > > is.matrix(submin[,display.gname])
> > [1] FALSE
> > > is.vector(submin[,display.gname])
> > [1] FALSE
> > > length(submin[,display.gname])
> > [1] 4622
> > 
> > So if it's not a data.frame, a matrix or a vector, what is "it"?
> > 
> > Thanks (and sorry if this is in the FAQ)
> > 
> > Mick
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --------------------------------------------------------------
> ----------
> ------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From ggrothendieck at myway.com  Fri Dec 17 17:48:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 17 Dec 2004 16:48:23 +0000 (UTC)
Subject: [R] take precisely one named argument
References: <XFMail.041217125637.Ted.Harding@nessie.mcc.ac.uk>
	<XFMail.041217134734.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <loom.20041217T174332-718@post.gmane.org>

 <Ted.Harding <at> nessie.mcc.ac.uk> writes:

: 
: On 17-Dec-04 Ted Harding wrote:
: > I don't know the *best* way (expert R anatomists will know ... )
: > but the following dirty handed modification seems to do what
: > you want:
: > 
: >   f <- function(z=NULL, a=NULL, b=NULL){
: >      if(!is.null(z)){
: >        stop("usage: f(a=...) or f(b=...)")
: >      }
: >      if(!xor(is.null(a), is.null(b))){
: >        stop("specify exactly one of a and b")
: >      }
: >      if(is.null(a)){return(2*b)}else{return(a)}
: >   }
: > 
: > (This traps attempts to use f() with an un-named argument).
: > 
: > Ted.
: 
: Playing around with the above shows that not only does it
: give the correct response for correct usage, e.g. f(a=3) or f(b=3),
: but, serendipitously, it gives appropriate responses for just
: about any way you could think of using it wrongly:
: 
: > f(3)
: Error in f(3) : usage: f(a=...) or f(b=...)
: 
: > f(a=2,b=3)
: Error in f(a = 2, b = 3) : specify exactly one of a and b
: 
: > f(d=3)
: Error in f(d = 3) : unused argument(s) (d ...)
: 
: > f(a=2,d=3)
: Error in f(a = 2, d = 3) : unused argument(s) (d ...)
: 



Here is a minor reduction of the above:

ff <- function(z, a = 0, b = 0) {
	stopifnot(missing(z), xor(missing(a), missing(b)))
	a+2*b
}



From suzette at sdac.harvard.edu  Fri Dec 17 17:50:29 2004
From: suzette at sdac.harvard.edu (Suzette Blanchard)
Date: Fri, 17 Dec 2004 11:50:29 -0500 (EST)
Subject: [R] reading the seed from a simulation
Message-ID: <Pine.GSO.4.40.0412171145450.11929-100000@sdac.harvard.edu>


Greetings,

	I have a simulation of a nonlinear model that
is failing.  But it does not fail til way into the simulation.
I would like to look at the run that is failing
and maybe I could if I could capture the seed for the
failing run.  The help file on set.seed says you can do it
but when I tried

rs<-.Random.seed
print(paste("rs",rs,sep=" "))

I got 626 of them so I don't know how to identify the right
one.  Please can you help?

Thank you,
Suzette

=================================
Suzette Blanchard, Ph.D.
UCSD-PPRU



From ripley at stats.ox.ac.uk  Fri Dec 17 18:24:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Dec 2004 17:24:10 +0000 (GMT)
Subject: [R] Is the page number in a document created with "pdf()"
	accessible?
In-Reply-To: <501D335A-503A-11D9-957A-0011242E1C5E@plessthan.com>
References: <501D335A-503A-11D9-957A-0011242E1C5E@plessthan.com>
Message-ID: <Pine.LNX.4.61.0412171717570.7579@gannet.stats>

On Fri, 17 Dec 2004, Dennis Fisher wrote:

> When I create pdf documents in R using pdf(), each page has text in the outer 
> margin indicating the page number.

It neither has an outer margin nor anything there, unless you added code 
to do this.  I guess your PDF viewer is adding this.

> I track page numbers in a cumbersome 
> manner.  Is the page number in a multi-page document tracked internally by 
> par() or some other function?

It's in the PDF file (part of the definition of PDF) and is tracked by the 
device driver.

See ?plot.new / ?grid.newpage: you could use hooks to do the counting.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From denson at usc.edu  Fri Dec 17 18:30:46 2004
From: denson at usc.edu (Tom Denson)
Date: Fri, 17 Dec 2004 09:30:46 -0800
Subject: [R] Factor analysis with dichotomous variables
Message-ID: <0I8V000OBNBBUQ90@msg-mx1.usc.edu>

Hello,

I would like to conduct an exploratory factor analysis with dichotomous
data. Do any R routines exist for this purpose? I recall reading something
about methods with tetrachoric correlations.

Any help would be appreciated.

Best,
Tom Denson
Department of Psychology
University of Southern California



From tplate at blackmesacapital.com  Fri Dec 17 18:39:59 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Fri, 17 Dec 2004 10:39:59 -0700
Subject: [R] reading the seed from a simulation
In-Reply-To: <Pine.GSO.4.40.0412171145450.11929-100000@sdac.harvard.edu>
References: <Pine.GSO.4.40.0412171145450.11929-100000@sdac.harvard.edu>
Message-ID: <6.1.0.6.2.20041217103055.0616daa0@mailhost.blackmesacapital.com>

With most modern random number generators you can't capture the current 
state in a single 32-bit integer.  (I suspect the .Random.seed you are 
seeing is the state contained in 625 integers).

The easiest way to run reproducible simulations is to explicitly set the 
seed, using an integer, before each run.  Then it's easy to put the random 
number generator into the same state again, e.g.:

for (sim.num in 1:100) {
   set.seed(sim.num)
   ... run simulation ...
}

If you can't do this, you can record the value of .Random.seed prior to the 
simulation, and then when you want to reproduce that simulation again, set 
.Random.seed to that value, e.g.:

 > set.seed(1)
 > sample(1:100, 5)
[1] 27 37 57 89 20
 > sample(1:100, 5)
[1] 90 94 65 62  6
 > set.seed(1)
 > sample(1:100, 5)
[1] 27 37 57 89 20
 > saved.seed <- .Random.seed
 > sample(1:100, 5)
[1] 90 94 65 62  6
 > .Random.seed <- saved.seed
 > sample(1:100, 5)
[1] 90 94 65 62  6
 >

This is not guaranteed to work with all random-number generators; see the 
NOTE section in ?set.seed

-- Tony Plate


At Friday 09:50 AM 12/17/2004, Suzette Blanchard wrote:

>Greetings,
>
>         I have a simulation of a nonlinear model that
>is failing.  But it does not fail til way into the simulation.
>I would like to look at the run that is failing
>and maybe I could if I could capture the seed for the
>failing run.  The help file on set.seed says you can do it
>but when I tried
>
>rs<-.Random.seed
>print(paste("rs",rs,sep=" "))
>
>I got 626 of them so I don't know how to identify the right
>one.  Please can you help?
>
>Thank you,
>Suzette
>
>=================================
>Suzette Blanchard, Ph.D.
>UCSD-PPRU
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From david.meyer at wu-wien.ac.at  Fri Dec 17 18:46:07 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Fri, 17 Dec 2004 18:46:07 +0100
Subject: [R] How to interpret and modify "plot.svm"?
Message-ID: <20041217184607.0bc72fd4.david.meyer@wu-wien.ac.at>

Frank:


> Dear R people,

> I am trying to plot the results from running svm in library(e1071). I
> use plot.svm. After searching through the help archives and FAQ, I
> still have several questions:

> 1.  In default, crosses indicate support vectors. But why are there
> two colors of crosses? What do they represent?

The colors represent the classes of the data points. The help page
admittedly doesn't tell you this and deserves improvement.

> 2. I want to draw a white-gray colored plot and modify the different
> colored crosses or circles by different shaped points. Could anyone
> give me a hint?

I just added three arguments to plot.svm() that allow customizing of the
plot symbols. The contour region is controlled by the parameters of the
filled.contour() function used in plot.svm(), so you will need to add
the color.palette argument to plot.svm (which subsequently will be
passed to filled.contour()).

> 3. Is it possible for me to draw a "hyperplane" on the plot?

You can add arbitrary objects to the plot (try lines()); but plot.svm()
doesn't compute the boundaries.

> 4. What is the algorithm to plot the contour region?

see filled.contour(). The input is determined by a grid of predicted
values.

Best,
-d


> Thank you very much,

> Frank

-- 
Dr. David Meyer
Department of Information Systems

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From paterijk at hotmail.com  Fri Dec 17 18:52:17 2004
From: paterijk at hotmail.com (Pat Meyer)
Date: Fri, 17 Dec 2004 17:52:17 +0000
Subject: [R] Union of list elements
Message-ID: <BAY1-F24A33609008F029E0B1935D5AF0@phx.gbl>



From paterijk at hotmail.com  Fri Dec 17 18:58:58 2004
From: paterijk at hotmail.com (Pat Meyer)
Date: Fri, 17 Dec 2004 17:58:58 +0000
Subject: [R] Union of list elements
Message-ID: <BAY1-F407164259F04E3C8FBFF3BD5AF0@phx.gbl>


Hi,

First of all, let me thank you all for replying so rapidly to my first 
question on this list. It was very very helpfull... and I'm learning R 
faster and faster.

I just encountered a second problem, which may also have a simple solution.

Here it is:

In my program, a vector is a set of objects.

I was looking for a way to store these sets in a big object. I chose to 
store them in a list.

So now I have a list of vectors which looks as follows:

[[1]]
[1] "a1" "a3" "a4"

[[2]]
[1] "a1" "a4" "a5"

[[3]]
[1] "a1" "a5" "a6"

Then comes a crucial step where I may have to add the vector ("a3", "a1", 
"a4") in this list.

But as you can see, this set is already present (at position 1 of my list). 
So it should not be added. If I do a systematic concatenation, at the end, I 
have a list with too many vectors (where some elements of my list represent 
the same set).

So what I would like to do is a type of Union, but I can't find a way to do 
it. Unions work only on vectors, and not a vector and a list of vectors.

Can anyone help me with this?

I thank you very much in advance

Patrick



From HDoran at air.org  Fri Dec 17 19:07:08 2004
From: HDoran at air.org (Doran, Harold)
Date: Fri, 17 Dec 2004 13:07:08 -0500
Subject: [R] Factor analysis with dichotomous variables
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7406E76B5F@dc1ex2.air.org>

You can use factanal to do the analysis. The polychor() package will
give you polychorics. You can then the do the factor analysis on this
correlation matrix. 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tom Denson
Sent: Friday, December 17, 2004 12:31 PM
To: R-help at stat.math.ethz.ch
Subject: [R] Factor analysis with dichotomous variables

Hello,

I would like to conduct an exploratory factor analysis with dichotomous
data. Do any R routines exist for this purpose? I recall reading
something about methods with tetrachoric correlations.

Any help would be appreciated.

Best,
Tom Denson
Department of Psychology
University of Southern California

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Fri Dec 17 19:12:38 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 17 Dec 2004 18:12:38 +0000 (UTC)
Subject: [R] Union of list elements
References: <BAY1-F407164259F04E3C8FBFF3BD5AF0@phx.gbl>
Message-ID: <loom.20041217T191130-730@post.gmane.org>

Pat Meyer <paterijk <at> hotmail.com> writes:

: 
: Hi,
: 
: First of all, let me thank you all for replying so rapidly to my first 
: question on this list. It was very very helpfull... and I'm learning R 
: faster and faster.
: 
: I just encountered a second problem, which may also have a simple solution.
: 
: Here it is:
: 
: In my program, a vector is a set of objects.
: 
: I was looking for a way to store these sets in a big object. I chose to 
: store them in a list.
: 
: So now I have a list of vectors which looks as follows:
: 
: [[1]]
: [1] "a1" "a3" "a4"
: 
: [[2]]
: [1] "a1" "a4" "a5"
: 
: [[3]]
: [1] "a1" "a5" "a6"
: 
: Then comes a crucial step where I may have to add the vector ("a3", "a1", 
: "a4") in this list.
: 
: But as you can see, this set is already present (at position 1 of my list). 
: So it should not be added. If I do a systematic concatenation, at the end, I 
: have a list with too many vectors (where some elements of my list represent 
: the same set).
: 
: So what I would like to do is a type of Union, but I can't find a way to do 
: it. Unions work only on vectors, and not a vector and a list of vectors.

L <- list(c("a1","a3","a4"), c("a1","a4","a5"), c("a1","a5","a6"))
newentry <- c("a3", "a1", "a4")
if (!any(sapply(L, setequal, newentry))) L <- c(L, list(newentry))



From andy_liaw at merck.com  Fri Dec 17 19:19:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Dec 2004 13:19:28 -0500
Subject: [R] Union of list elements
Message-ID: <3A822319EB35174CA3714066D590DCD50994E44B@usrymx25.merck.com>

All the ways of doing such things (that I know of) in R only work on atomic
objects, so one way to do it is, again, concatenate the vectors into one
string, then do the comparison:

> lst = list(c("a1", "a3", "a4"), c("a1", "a4", "a5"), c("a1", "a5", "a6"))
> lst.vec <- sapply(lst, paste, collapse=":")
> lst.vec
[1] "a1:a3:a4" "a1:a4:a5" "a1:a5:a6"
> x <- lst[[1]]
> paste(x, collapse=":") %in% lst.vec
[1] TRUE

Then you can "grow" the list by something like:

if (!paste(x, collapse=":") %in% lst.vec) lst <- c(lst, list(x))

[BTW, note that this only works if order matters; i.e., c("a1", "a3", "a4")
not equal to c("a4", "a3", "a1").]

HTH,
Andy

> From: Pat Meyer
> 
> Hi,
> 
> First of all, let me thank you all for replying so rapidly to 
> my first 
> question on this list. It was very very helpfull... and I'm 
> learning R 
> faster and faster.
> 
> I just encountered a second problem, which may also have a 
> simple solution.
> 
> Here it is:
> 
> In my program, a vector is a set of objects.
> 
> I was looking for a way to store these sets in a big object. 
> I chose to 
> store them in a list.
> 
> So now I have a list of vectors which looks as follows:
> 
> [[1]]
> [1] "a1" "a3" "a4"
> 
> [[2]]
> [1] "a1" "a4" "a5"
> 
> [[3]]
> [1] "a1" "a5" "a6"
> 
> Then comes a crucial step where I may have to add the vector 
> ("a3", "a1", 
> "a4") in this list.
> 
> But as you can see, this set is already present (at position 
> 1 of my list). 
> So it should not be added. If I do a systematic 
> concatenation, at the end, I 
> have a list with too many vectors (where some elements of my 
> list represent 
> the same set).
> 
> So what I would like to do is a type of Union, but I can't 
> find a way to do 
> it. Unions work only on vectors, and not a vector and a list 
> of vectors.
> 
> Can anyone help me with this?
> 
> I thank you very much in advance
> 
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Fri Dec 17 19:24:52 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Dec 2004 13:24:52 -0500
Subject: [R] Union of list elements
Message-ID: <3A822319EB35174CA3714066D590DCD50994E44C@usrymx25.merck.com>

> From: Gabor Grothendieck
> 
> Pat Meyer <paterijk <at> hotmail.com> writes:
> 
> : 
> : Hi,
> : 
> : First of all, let me thank you all for replying so rapidly 
> to my first 
> : question on this list. It was very very helpfull... and I'm 
> learning R 
> : faster and faster.
> : 
> : I just encountered a second problem, which may also have a 
> simple solution.
> : 
> : Here it is:
> : 
> : In my program, a vector is a set of objects.
> : 
> : I was looking for a way to store these sets in a big 
> object. I chose to 
> : store them in a list.
> : 
> : So now I have a list of vectors which looks as follows:
> : 
> : [[1]]
> : [1] "a1" "a3" "a4"
> : 
> : [[2]]
> : [1] "a1" "a4" "a5"
> : 
> : [[3]]
> : [1] "a1" "a5" "a6"
> : 
> : Then comes a crucial step where I may have to add the 
> vector ("a3", "a1", 
> : "a4") in this list.
> : 
> : But as you can see, this set is already present (at 
> position 1 of my list). 
> : So it should not be added. If I do a systematic 
> concatenation, at the end, I 
> : have a list with too many vectors (where some elements of 
> my list represent 
> : the same set).
> : 
> : So what I would like to do is a type of Union, but I can't 
> find a way to do 
> : it. Unions work only on vectors, and not a vector and a 
> list of vectors.
> 
> L <- list(c("a1","a3","a4"), c("a1","a4","a5"), c("a1","a5","a6"))
> newentry <- c("a3", "a1", "a4")
> if (!any(sapply(L, setequal, newentry))) L <- c(L, list(newentry))

Cool, Gabor!  This works even if the sets have the same elements in
different orders.  Much better than what I had.

Best,
Andy



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From xiaoliu at jhmi.edu  Fri Dec 17 19:58:04 2004
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Fri, 17 Dec 2004 13:58:04 -0500
Subject: [R] package.skeleton()
Message-ID: <36b16c907ece.41c2e5ec@jhmimail.jhmi.edu>

Hi, R people:

I generated a package using package.skeleton().  But I can not load it using library().

> package.skeleton("RDIPcor", list = c("ROCAUC.i", "cor.i"), path = "/home/xiao")
Creating directories ...
Creating DESCRIPTION ...
Creating READMEs ...
Saving functions and data ...
Making help files ...
Done.
Further steps are described in /home/xiao/RDIPcor/README 
> library(RDIPcor, lib.loc = "/home/xiao")
Error in testRversion(descfields) : This package has not been installed properly
 See the Note in ?library

My R1.8.0 is under LINUX.

Thanks in advance

Xiao



From patrick.meyer at internet.lu  Fri Dec 17 20:04:41 2004
From: patrick.meyer at internet.lu (Patrick Meyer)
Date: Fri, 17 Dec 2004 20:04:41 +0100
Subject: [R] Union of list elements
Message-ID: <41C32DC9.9010104@internet.lu>

Thank you Gabor.

But I have a problem with the beginning of my algorithm, where the list 
you call L is empty... then the code breaks down... It says:

"Error in any(...,na.rm = na.rm) : incorrect argument type"

How can I handle this?

Thank you very much for your help

Patrick



From pburns at pburns.seanet.com  Fri Dec 17 20:14:46 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 17 Dec 2004 19:14:46 +0000
Subject: [R] Union of list elements
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E44C@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E44C@usrymx25.merck.com>
Message-ID: <41C33026.4080807@pburns.seanet.com>

Andy, I don't think that you should be so quick to put yourself down.
Your solution just needs a 'sort' put in it so that order doesn't matter.

With Andy's solution the object can now be a character vector rather
than a list and 'match' can be used for testing new items:

newitems <- unique(newitems)
test.result <- match(newitems, values, nomatch=NA)
values <- c(values, newitems[is.na(test.result)])

I think both solutions have their place, and the usage will determine
which is more efficient (where efficiency includes how well it fits in
as well as time and memory use).

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Liaw, Andy wrote:

>>From: Gabor Grothendieck
>>
>>Pat Meyer <paterijk <at> hotmail.com> writes:
>>
>>: 
>>: Hi,
>>: 
>>: First of all, let me thank you all for replying so rapidly 
>>to my first 
>>: question on this list. It was very very helpfull... and I'm 
>>learning R 
>>: faster and faster.
>>: 
>>: I just encountered a second problem, which may also have a 
>>simple solution.
>>: 
>>: Here it is:
>>: 
>>: In my program, a vector is a set of objects.
>>: 
>>: I was looking for a way to store these sets in a big 
>>object. I chose to 
>>: store them in a list.
>>: 
>>: So now I have a list of vectors which looks as follows:
>>: 
>>: [[1]]
>>: [1] "a1" "a3" "a4"
>>: 
>>: [[2]]
>>: [1] "a1" "a4" "a5"
>>: 
>>: [[3]]
>>: [1] "a1" "a5" "a6"
>>: 
>>: Then comes a crucial step where I may have to add the 
>>vector ("a3", "a1", 
>>: "a4") in this list.
>>: 
>>: But as you can see, this set is already present (at 
>>position 1 of my list). 
>>: So it should not be added. If I do a systematic 
>>concatenation, at the end, I 
>>: have a list with too many vectors (where some elements of 
>>my list represent 
>>: the same set).
>>: 
>>: So what I would like to do is a type of Union, but I can't 
>>find a way to do 
>>: it. Unions work only on vectors, and not a vector and a 
>>list of vectors.
>>
>>L <- list(c("a1","a3","a4"), c("a1","a4","a5"), c("a1","a5","a6"))
>>newentry <- c("a3", "a1", "a4")
>>if (!any(sapply(L, setequal, newentry))) L <- c(L, list(newentry))
>>    
>>
>
>Cool, Gabor!  This works even if the sets have the same elements in
>different orders.  Much better than what I had.
>
>Best,
>Andy
>
>
>
>  
>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From andy_liaw at merck.com  Fri Dec 17 20:19:29 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Dec 2004 14:19:29 -0500
Subject: [R] Union of list elements
Message-ID: <3A822319EB35174CA3714066D590DCD50994E44D@usrymx25.merck.com>

Initialize the list with the first vector, instead of an emtry list, if you
can.

Andy

> From: Patrick Meyer
> 
> Thank you Gabor.
> 
> But I have a problem with the beginning of my algorithm, 
> where the list 
> you call L is empty... then the code breaks down... It says:
> 
> "Error in any(...,na.rm = na.rm) : incorrect argument type"
> 
> How can I handle this?
> 
> Thank you very much for your help
> 
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From v39k9 at unb.ca  Fri Dec 17 20:27:41 2004
From: v39k9 at unb.ca (Joe Nocera)
Date: Fri, 17 Dec 2004 15:27:41 -0400
Subject: [R] behaviour of BIC and AICc code
Message-ID: <1103311661.41c3332d1fbe8@webmail.unb.ca>

Dear R-helpers

I have generated a suite of GLMs.  To select the best model for each set, I am using the
meta-analysis approach of de Luna and Skouras (Scand J Statist 30:113-128).  Simply
put, I am calculating AIC, AICc, BIC, etc., and then using whichever criterion
minimizes APE (Accumulated Prediction Error from cross-validations on all model sets)
to select models.

My problem arises where I have noticed my rankings from BIC and AICc are exactly
inverse.  I fear this behaviour is a result of my coding as follows:

I calculate BIC from sample size:
stepAIC (mymodel.glm, k=log(n))

I then calculate AICc by:
stepAIC (mymodel.glm, k=2*sum(mymodel.glm$prior.weights)/(sum(mymodel$prior.weights) -
length(coef(mymodel.glm))-1)).

I base these calculations for:
BIC on Venables and Ripley's MASS ("...Only k=2 gives the genuine AIC: k = log(n) is
sometimes referred to as BIC or SBC."...)
; and for AICc from that AICc = AIC + ((2K*(K+1))/(n-K-1))
	
Is this behaviour expected?  Or is the coding off?  I could find no reference to this
problem in the archives here, nor at S-news.

Cheers,
Joe

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Joseph J. Nocera
Ph.D. Candidate
Biology Department - Univ. New Brunswick
Fredericton, NB
Canada   E3B 6E1
tel: (902) 679-5733



From tlumley at u.washington.edu  Fri Dec 17 21:01:34 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 17 Dec 2004 12:01:34 -0800 (PST)
Subject: [R] package.skeleton()
In-Reply-To: <36b16c907ece.41c2e5ec@jhmimail.jhmi.edu>
References: <36b16c907ece.41c2e5ec@jhmimail.jhmi.edu>
Message-ID: <Pine.A41.4.61b.0412171157120.225516@homer06.u.washington.edu>

On Fri, 17 Dec 2004, XIAO LIU wrote:

> Hi, R people:
>
> I generated a package using package.skeleton().  But I can not load it 
> using library().

It looks as though you didn't do anything except run package.skeleton(). 
As the output of package.skeleton() and the help page suggest, you need 
further steps, and it tells you where to find a description of these 
steps.

At a minimum you need to edit the help files and INSTALL the package 
before you can use it.

 	-thomas


>> package.skeleton("RDIPcor", list = c("ROCAUC.i", "cor.i"), path = "/home/xiao")
> Creating directories ...
> Creating DESCRIPTION ...
> Creating READMEs ...
> Saving functions and data ...
> Making help files ...
> Done.
> Further steps are described in /home/xiao/RDIPcor/README
>> library(RDIPcor, lib.loc = "/home/xiao")
> Error in testRversion(descfields) : This package has not been installed properly
> See the Note in ?library
>
> My R1.8.0 is under LINUX.
>
> Thanks in advance
>
> Xiao
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ggrothendieck at myway.com  Fri Dec 17 21:04:08 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 17 Dec 2004 20:04:08 +0000 (UTC)
Subject: [R] Union of list elements
References: <41C32DC9.9010104@internet.lu>
Message-ID: <loom.20041217T210150-538@post.gmane.org>

Patrick Meyer <patrick.meyer <at> internet.lu> writes:

: 
: Thank you Gabor.
: 
: But I have a problem with the beginning of my algorithm, where the list 
: you call L is empty... then the code breaks down... It says:
: 
: "Error in any(...,na.rm = na.rm) : incorrect argument type"
: 
: How can I handle this?
: 

Good point.  We can rescue it by doing the simplification ourself
using unlist(lapply(...)) instead of sapply(...):

L <- list()
newentry <- c("a3", "a1", "a4")
if (!any(unlist(lapply(L, setequal, newentry)))) L <- c(L, list(newentry))



From zhang at dq.fct.unl.pt  Fri Dec 17 21:09:38 2004
From: zhang at dq.fct.unl.pt (zhang qingyou)
Date: Sat, 18 Dec 2004 04:09:38 +0800
Subject: [R] font size (library stats)
Message-ID: <000601c4e474$562dd500$235e010a@Qingyou>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041218/4c9603ce/attachment.pl

From ggrothendieck at myway.com  Fri Dec 17 21:14:36 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 17 Dec 2004 20:14:36 +0000 (UTC)
Subject: [R] reshape and split
References: <1103280478.41c2b95e618f9@www2.zfn.uni-bremen.de>
Message-ID: <loom.20041217T210809-67@post.gmane.org>

Patrick Hausmann <c18g <at> zfn.uni-bremen.de> writes:

: 
: Dear R-users,
: 
: I am trying to reshape the DF "dat2" in the "long" format,
: but can't figure out how to use the "split"-option:
: 
: > dat2
:    a.1995.z  b.1995.z  a.1996.z                    var
: 1 100.00000 100.00000 100.00000 Neue Anlagen insgesamt
: 2  40.09904  23.60890  40.88960      Neue Ausr??stungen
: 3  59.90096  76.39110  59.11040            Neue Bauten
: 
: This should be the result:
: 
: region	time	wz	value	var
: a	1995	z	100	Neue Anlagen insgesamt
: a	1995	z	40.09904	Neue Ausr??stungen
: a	1995	z	59.90096	Neue Bauten
: b	1995	z	100	Neue Anlagen insgesamt
: b	1995	z	23.60890	Neue Ausr??stungen
: b	1995	z	76.39110	Neue Bauten
: a	1996	z	100	Neue Anlagen insgesamt
: a	1996	z	40.88960	Neue Ausr??stungen
: a	1996	z	59.11040	Neue Bauten
: 
: # Not working, sure "[.]" is missing
: reshape(dat2, dir="long", varying=1:3, split=list(regexp="[0-9][a-z]",
: include=T))


Its easiest to just code times= and v.names= arguments directly rather
than use split= to try to guess them (at which point reworking it into
the form you want is straight forward):
 
nm3 <- names(dat2)[1:3]
dat2.long <- reshape(dat2, dir = "long", 
		varying = list(nm3), times = nm3, v.names = "value")

# rework into desired form
with(dat2.long, 
	data.frame(
		region = substr(time,1,1),
		time = substr(time,3,6), 
		wz = substr(time,8,8), 
		value = value, 
		var = var
	)
)



From patrick.meyer at internet.lu  Fri Dec 17 21:43:47 2004
From: patrick.meyer at internet.lu (Patrick Meyer)
Date: Fri, 17 Dec 2004 21:43:47 +0100
Subject: [R] Union of list elements
Message-ID: <41C34503.10100@internet.lu>

It does not exactly do what it is meant to... Instead of breaking down, 
my code is looping forever now... I think I will have a deeper look at 
it tomorrow...

In fact, I have two lists: let's say L and N.

An elements (or set) of N must be added to L, if and only if it is not a 
subset of an element of L.

I think that the code you wrote does that, for each element of separately.

Couldn't this be a solution: add all the elements of N to L, and 
afterwards eliminate the sets which are present more than once? How 
could one eliminate these sets?

Thanx for your help...

P.



From accamb01 at louisville.edu  Fri Dec 17 23:11:57 2004
From: accamb01 at louisville.edu (Alexander C Cambon)
Date: Fri, 17 Dec 2004 17:11:57 -0500
Subject: [R] SAS or R software
Message-ID: <s1c31367.099@gwise.louisville.edu>

I apologize for adding this so late to the "SAS or R software " thread.
This is a question, not a reply, but it seems to me to fit in well with
the subject of this thread.

I would like to know anyone's experiences in the following two areas
below.  I should add I have no experience myself in these areas:

1) Migrating from SAS to R in the choice of statistical software used
for FDA  reporting.

 (For example, was there more effort involved in areas of
documentation, revision tracking,  or validation of software codes?)

2) Migrating from SAS to R in the choice of statistical software used
for NIH reporting  (or other US or non-US) government agencies) .

I find myself using R more and more and being continually amazed by its
breadth of capabilities, though I have not tried ordering pizza yet. I
use SAS, S-Plus, and, more recently, R for survival analysis and
recurrent events in clinical trials.

Alex Cambon
Biostatistician
School of Public Health and Information Sciences
University of Louisville



From accamb01 at louisville.edu  Fri Dec 17 23:11:57 2004
From: accamb01 at louisville.edu (Alexander C Cambon)
Date: Fri, 17 Dec 2004 17:11:57 -0500
Subject: [R] SAS or R software
Message-ID: <s1c31367.000@gwise.louisville.edu>

I apologize for adding this so late to the "SAS or R software " thread.
This is a question, not a reply, but it seems to me to fit in well with
the subject of this thread.

I would like to know anyone's experiences in the following two areas
below.  I should add I have no experience myself in these areas:

1) Migrating from SAS to R in the choice of statistical software used
for FDA  reporting.

 (For example, was there more effort involved in areas of
documentation, revision tracking,  or validation of software codes?)

2) Migrating from SAS to R in the choice of statistical software used
for NIH reporting  (or other US or non-US) government agencies) .

I find myself using R more and more and being continually amazed by its
breadth of capabilities, though I have not tried ordering pizza yet. I
use SAS, S-Plus, and, more recently, R for survival analysis and
recurrent events in clinical trials.

Alex Cambon
Biostatistician
School of Public Health and Information Sciences
University of Louisville



From mchaudha at jhsph.edu  Fri Dec 17 23:38:52 2004
From: mchaudha at jhsph.edu (Mohammad A. Chaudhary)
Date: Fri, 17 Dec 2004 17:38:52 -0500
Subject: [R] Confidence Intervals from Bootstrap Replications 
Message-ID: <7B4C4F3BD3C32243B0FC170251843990B60310@XCH-VN02.sph.ad.jhsph.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041217/575ea0c5/attachment.pl

From gunter.berton at gene.com  Fri Dec 17 23:53:50 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 17 Dec 2004 14:53:50 -0800
Subject: [R] Confidence Intervals from Bootstrap Replications 
In-Reply-To: <7B4C4F3BD3C32243B0FC170251843990B60310@XCH-VN02.sph.ad.jhsph.edu>
Message-ID: <200412172253.iBHMrolr027711@volta.gene.com>


See , e.g. section 8.3 "The two-sample problem" of Efron and Tibshirani's AN
INTRODUCTION TO THE BOOTSTRAP. It makes it clear there why one just
bootstrap samples independently from the two separate samples.

The "strata" argument of boot() in the boot package allows one to do such
independent sampling and use the capabilities of that function.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Mohammad A. Chaudhary
> Sent: Friday, December 17, 2004 2:39 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Confidence Intervals from Bootstrap Replications 
> 
> Hi All:
> 
> I have to compute bootstrap confidence intervals, the statistic
> (incremental cost effectiveness ratio) is computed from two samples
> (intervention and control) of different sizes. All the bootstrap
> functions that I have seen use one dataset as argument. I may go ahead
> and get the desired number of bootstrap replications 
> separately. I would
> appreciate if you could point me to a source of a bootstrap 
> function (if
> available) that takes the B bootstrap replications and other 
> descriptive
> statistics and can get me the confidence intervals. Please 
> write me if I
> have not been clear in explaining my problem. Regards,
> 
> Ashraf  
> 
>  
> 
> ___________________________________
> 
> M. Ashraf Chaudhary, Ph.D.
> 
> Associate Scientist/Biostatistician
> 
> Department of International Health
> 
> Disease Prevention and Control Program
> 
> Johns Hopkins University Bloomberg School of Public Health
> 
> 615 North Wolfe Street, Room W5506
> 
> Baltimore MD 21205
> 
>  
> 
> mchaudha at jhsph.edu 
> 
> Phone: (410) 502-0741/Fax: (410) 502-6733
> 
> http://faculty.jhsph.edu/?F=Mohammad&L=Chaudhary
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From f.harrell at vanderbilt.edu  Sat Dec 18 00:04:27 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 17 Dec 2004 17:04:27 -0600
Subject: [R] SAS or R software
In-Reply-To: <s1c31367.099@gwise.louisville.edu>
References: <s1c31367.099@gwise.louisville.edu>
Message-ID: <41C365FB.9080106@vanderbilt.edu>

Alexander C Cambon wrote:
> I apologize for adding this so late to the "SAS or R software " thread.
> This is a question, not a reply, but it seems to me to fit in well with
> the subject of this thread.
> 
> I would like to know anyone's experiences in the following two areas
> below.  I should add I have no experience myself in these areas:
> 
> 1) Migrating from SAS to R in the choice of statistical software used
> for FDA  reporting.
> 
>  (For example, was there more effort involved in areas of
> documentation, revision tracking,  or validation of software codes?)

FDA has no requirements.  They accept Minitab and even accept Excel. 
Requirements are to be a good statistician doing quality reproducible 
work for its own sake.


> 
> 2) Migrating from SAS to R in the choice of statistical software used
> for NIH reporting  (or other US or non-US) government agencies) .

No issues.

Frank

> 
> I find myself using R more and more and being continually amazed by its
> breadth of capabilities, though I have not tried ordering pizza yet. I
> use SAS, S-Plus, and, more recently, R for survival analysis and
> recurrent events in clinical trials.
> 
> Alex Cambon
> Biostatistician
> School of Public Health and Information Sciences
> University of Louisville


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From dmb at mrc-dunn.cam.ac.uk  Sat Dec 18 00:09:13 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Fri, 17 Dec 2004 23:09:13 +0000 (GMT)
Subject: [R] Massive clustering job?
In-Reply-To: <45AAE6FD142DCB43A38C00A11FF5DF3E04993C8B@uswsmx03.merck.com>
Message-ID: <Pine.LNX.4.21.0412172308420.28767-100000@mail.mrc-dunn.cam.ac.uk>

On Wed, 15 Dec 2004, Wiener, Matthew wrote:

>It sounds like "clara" in package cluster might help.

Cheers, this looks just the ticket. How should I choose k though?

Dan.


>
>Regards,
>
>Matt Wiener
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
>Sent: Wednesday, December 15, 2004 6:37 AM
>To: R mailing list
>Subject: [R] Massive clustering job?
>
>
>
>Hi, 
>
>I have ~40,000 rows in a database, each of which contains an id column and
>20 additional columns of count data.
>
>I want to cluster the rows based on these count vectors.
>
>Their are ~1.6 billion possible 'distances' between pairs of vectors
>(cells in my distance matrix), so I need to do something smart.
>
>Can R somehow handle this?
>
>My first thought was to index the database with something that makes
>nearest neighbour lookup more efficient, and then use single linkage
>clustering. Is this kind of index implemented in R (by default when using
>single linkage)?
>
>Also 'grouping' identical vectors is very easy. I tried making groups more
>fuzzy by using a hashing function over the count vectors, but my hash was
>too crude. Any way to do fuzzy grouping in R which scales well?
>
>For example, removing identical vectors gives me ~30,000 rows (and ~900
>million pairs of distances). As an example of how fast I can group, the
>above query took 0.13 seconds in mysql (using an index over every element
>in the vector). However, if I tried to calculate a distance between every
>pair of non identical vectors (lets say I can calculate ~1000 eutlidian
>distances per second) it would take me ~10 days just to calculate the
>distance matrix.
>
>Sorry for all the information. Any suggestions on how to cluster such a
>huge dataset (using R) would be appreciated.
>
>Cheers,
>Dan.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
>------------------------------------------------------------------------------
>



From bates at stat.wisc.edu  Sat Dec 18 00:38:21 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 17 Dec 2004 17:38:21 -0600
Subject: [R] SAS or R software
In-Reply-To: <s1c31367.099@gwise.louisville.edu>
References: <s1c31367.099@gwise.louisville.edu>
Message-ID: <41C36DED.1020709@stat.wisc.edu>

Alexander C Cambon wrote:
> I apologize for adding this so late to the "SAS or R software " thread.
> This is a question, not a reply, but it seems to me to fit in well with
> the subject of this thread.
> 
> I would like to know anyone's experiences in the following two areas
> below.  I should add I have no experience myself in these areas:
> 
> 1) Migrating from SAS to R in the choice of statistical software used
> for FDA  reporting.
> 
>  (For example, was there more effort involved in areas of
> documentation, revision tracking,  or validation of software codes?)

This brings up a question that I have often asked but have never had 
answered.  If someone asks me if R is "validated" I usually respond "by 
whom and for what?".  There seems to be an belief that the FDA validates 
software as acceptable for use in the analysis of data for a submission 
to the FDA.  However I have never met anyone who can describe to me 
exactly what this entails.  So I can't say if R is "validated" because I 
don't know what that means.

As I understand it the FDA does not certify or validate software as 
providing "correct" or acceptable answers.  I have been told that what 
the FDA requires is that the software used to produce the results quoted 
in a submission should be auditable.  That is, the FDA must be able to 
check exactly how the numerical results were produced, should they wish 
to do so.  This can be tricky for proprietary software because typically 
the group making the submission does not have access to the source code 
so there has to be a delicate three-way negotiation on the extent to 
which the software vendor will reveal their source code.  However, 
revealing source code not a difficult issue in the open source world. 
Representatives of the FDA (or anyone else, for that matter) can read 
the source code any time they want to.  In fact they are encouraged to 
do so.

So if the standard is "auditable" I don't think you get much more 
auditable than R is.



From ggrothendieck at myway.com  Sat Dec 18 01:16:04 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 18 Dec 2004 00:16:04 +0000 (UTC)
Subject: [R] Union of list elements
References: <41C34503.10100@internet.lu>
Message-ID: <loom.20041218T010652-929@post.gmane.org>

Patrick Meyer <patrick.meyer <at> internet.lu> writes:

: 
: It does not exactly do what it is meant to... Instead of breaking down, 
: my code is looping forever now... I think I will have a deeper look at 
: it tomorrow...
: 
: In fact, I have two lists: let's say L and N.
: 
: An elements (or set) of N must be added to L, if and only if it is not a 
: subset of an element of L.
: 
: I think that the code you wrote does that, for each element of separately.

If both arguments are lists, not a list and vector as you previously
described, then just lapply over the previous formula (here encapsulated
in the function) using a second outer lapply:

L <- c(L, N[unlist(lapply(N, function(x) 
               !any(unlist(lapply(L, setequal, x)))))])

: 
: Couldn't this be a solution: add all the elements of N to L, and 
: afterwards eliminate the sets which are present more than once? How 
: could one eliminate these sets?
: 

Yes but it would involve unnecessary comparisons between pairs in
L as well as unnecessary comparisons between pairs in N.



From MSchwartz at MedAnalytics.com  Sat Dec 18 01:19:28 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 17 Dec 2004 18:19:28 -0600
Subject: [R] SAS or R software
In-Reply-To: <s1c31367.099@gwise.louisville.edu>
References: <s1c31367.099@gwise.louisville.edu>
Message-ID: <1103329168.7851.35.camel@horizons.localdomain>

On Fri, 2004-12-17 at 17:11 -0500, Alexander C Cambon wrote: 
> I apologize for adding this so late to the "SAS or R software "
> thread.
> This is a question, not a reply, but it seems to me to fit in well
> with
> the subject of this thread.
> 
> I would like to know anyone's experiences in the following two areas
> below.  I should add I have no experience myself in these areas:
> 
> 1) Migrating from SAS to R in the choice of statistical software used
> for FDA  reporting.

You will find that to be a non-issue from the FDA's perspective. This
has been discussed here with some frequency.  If you search the archives
you will find comments from Frank Harrell and others.

The FDA does not and cannot endorse a particular software product. Nor
does it validate any statistical software for a specific purpose. They
do need to be able to reproduce the results, which means they need to
know what software product was used, which version and on what platform,
etc.

The SAS XPORT Transport Format (which is openly defined and documented),
has been used for the transfer of data sets and has been available in
many statistical products.

There have been a variety of activities (CDISC, HL-7, etc) regarding the
electronic submission of data to the FDA. Some additional information is
here:

http://www.fda.gov/cder/regulatory/ersr/default.htm

and here:

http://www.cdisc.org/news/index.html

Any other issues impacting the selection of a particular statistical
application are more likely to be political within your working
environment and FUD. 

As you are likely aware, other statistically relevant issues are
contained in various ICH guidance documents regarding GCP considerations
and principles for clinical trials:

http://www.ich.org/UrlGrpServer.jser?@_ID=475&@_TEMPLATE=272


Keep in mind also that one big advantage R has (in my mind) is the use
of Sweave for the reproducible generation of reports, which to an extent
are self-documenting. 


>  (For example, was there more effort involved in areas of
> documentation, revision tracking,  or validation of software codes?

Since the FDA's role with computer software and validation has been
raised before, the following documents cover many of these areas. The
list is not meant to be exhaustive, but should give a flavor in this
domain.

There are specific guidance documents by the FDA pertaining to software
that is contained in a medical device (ie. the firmware in a pacemaker
or medical monitoring equipment) or is used to develop a medical device.
The current guidance in this case is here:

http://www.fda.gov/cdrh/comp/guidance/938.html

Other guidance pertains to 21 CFR 11, which addresses data management
systems used for clinical trials and covers issues such as electronic
signatures, audit trails and the like. A guidance document for that is
here:

http://www.fda.gov/cder/guidance/5667fnl.htm

Keep in mind, from a perspective standpoint, that even MS Excel and
Access can be made to be 21 CFR 11 compliant and there are companies
whose business is focused on just that task.

There is also a general guidance document for computer systems used in
clinical trials here:

http://www.fda.gov/ora/compliance_ref/bimo/ffinalcct.htm

Though it is to be superseded by a draft document here:

http://www.fda.gov/cder/guidance/6032dft.htm 


> 2) Migrating from SAS to R in the choice of statistical software used
> for NIH reporting  (or other US or non-US) government agencies) .

Same here to my knowledge.

As I was typing this, I see Frank just responded.

I also just noted Doug's post, so perhaps some of the above information
will be helpful in clarifying some of his questions as well.

I believe that the above is factually correct, but if someone knows
anything to not be so, please correct me.

HTH,

Marc Schwartz



From maustin at amgen.com  Sat Dec 18 01:49:27 2004
From: maustin at amgen.com (Austin, Matt)
Date: Fri, 17 Dec 2004 16:49:27 -0800
Subject: [R] SAS or R software
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DCBE@teal-exch.amgen.com>

One point that is missing in this discussion is ease of review by the
statistician at the FDA.  As a statistician in clinical trials, you want to
make it as easy as possible for your colleague at the FDA to do their job,
so you put the programs in a format that they are more likely to find
useful.  As more reviewing statisticians are familiar with SAS than with
other statistical packages/languages, I feel more comfortable sending a
submission in SAS. 

Reviewers do use the programs in a filing that were written for creation of
data sets and analysis to check for correct variable definitions and
appropriate analyses.  If the programs are written in a package/language
that the reviewer understands they can get their job done easier.

Given this, I have used S-Plus in regulatory work where it was clearly
stronger--at some point I hope to use R where it adds a clear benefit.  Of
course 95% of the statistical analyses that are performed for regulatory
submission can probably be done just as well with any of the major
statistical package/languages available.

$0.02

--Matt  

Matt Austin
Statistician

Amgen 
One Amgen Center Drive
M/S 24-2-C
Thousand Oaks CA 93021
(805) 447 - 7431




> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Marc Schwartz
> Sent: Friday, December 17, 2004 16:19 PM
> To: Alexander C Cambon
> Cc: R-Help; Douglas Bates
> Subject: Re: [R] SAS or R software
> 
> 
> On Fri, 2004-12-17 at 17:11 -0500, Alexander C Cambon wrote: 
> > I apologize for adding this so late to the "SAS or R software "
> > thread.
> > This is a question, not a reply, but it seems to me to fit in well
> > with
> > the subject of this thread.
> > 
> > I would like to know anyone's experiences in the following two areas
> > below.  I should add I have no experience myself in these areas:
> > 
> > 1) Migrating from SAS to R in the choice of statistical 
> software used
> > for FDA  reporting.
> 
> You will find that to be a non-issue from the FDA's perspective. This
> has been discussed here with some frequency.  If you search 
> the archives
> you will find comments from Frank Harrell and others.
> 
> The FDA does not and cannot endorse a particular software product. Nor
> does it validate any statistical software for a specific purpose. They
> do need to be able to reproduce the results, which means they need to
> know what software product was used, which version and on 
> what platform,
> etc.
> 
> The SAS XPORT Transport Format (which is openly defined and 
> documented),
> has been used for the transfer of data sets and has been available in
> many statistical products.
> 
> There have been a variety of activities (CDISC, HL-7, etc) 
> regarding the
> electronic submission of data to the FDA. Some additional 
> information is
> here:
> 
> http://www.fda.gov/cder/regulatory/ersr/default.htm
> 
> and here:
> 
> http://www.cdisc.org/news/index.html
> 
> Any other issues impacting the selection of a particular statistical
> application are more likely to be political within your working
> environment and FUD. 
> 
> As you are likely aware, other statistically relevant issues are
> contained in various ICH guidance documents regarding GCP 
> considerations
> and principles for clinical trials:
> 
> http://www.ich.org/UrlGrpServer.jser?@_ID=475&@_TEMPLATE=272
> 
> 
> Keep in mind also that one big advantage R has (in my mind) is the use
> of Sweave for the reproducible generation of reports, which 
> to an extent
> are self-documenting. 
> 
> 
> >  (For example, was there more effort involved in areas of
> > documentation, revision tracking,  or validation of software codes?
> 
> Since the FDA's role with computer software and validation has been
> raised before, the following documents cover many of these areas. The
> list is not meant to be exhaustive, but should give a flavor in this
> domain.
> 
> There are specific guidance documents by the FDA pertaining 
> to software
> that is contained in a medical device (ie. the firmware in a pacemaker
> or medical monitoring equipment) or is used to develop a 
> medical device.
> The current guidance in this case is here:
> 
> http://www.fda.gov/cdrh/comp/guidance/938.html
> 
> Other guidance pertains to 21 CFR 11, which addresses data management
> systems used for clinical trials and covers issues such as electronic
> signatures, audit trails and the like. A guidance document for that is
> here:
> 
> http://www.fda.gov/cder/guidance/5667fnl.htm
> 
> Keep in mind, from a perspective standpoint, that even MS Excel and
> Access can be made to be 21 CFR 11 compliant and there are companies
> whose business is focused on just that task.
> 
> There is also a general guidance document for computer systems used in
> clinical trials here:
> 
> http://www.fda.gov/ora/compliance_ref/bimo/ffinalcct.htm
> 
> Though it is to be superseded by a draft document here:
> 
> http://www.fda.gov/cder/guidance/6032dft.htm 
> 
> 
> > 2) Migrating from SAS to R in the choice of statistical 
> software used
> > for NIH reporting  (or other US or non-US) government agencies) .
> 
> Same here to my knowledge.
> 
> As I was typing this, I see Frank just responded.
> 
> I also just noted Doug's post, so perhaps some of the above 
> information
> will be helpful in clarifying some of his questions as well.
> 
> I believe that the above is factually correct, but if someone knows
> anything to not be so, please correct me.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ys03165003 at student.ecnu.edu.cn  Sat Dec 18 04:14:16 2004
From: ys03165003 at student.ecnu.edu.cn (ys03165003@student.ecnu.edu.cn)
Date: Sat, 18 Dec 2004 11:14:16 +0800
Subject: [R] For help
Message-ID: <303339656.19267@student.ecnu.edu.cn>

Hi

During using the R(vision 2.0.1), I meet a problem. I would like to do the 

Multiple Correspondence Analysis, but when I use the "< mca(lf, nf = 2, abbrev = 

FALSE)", the sentence "Error: couldn't find function 'mca'" will appear. So, 

please tell me how can I use the "mca()", thanks!

                         Any help appreciated.... 

                                                                  jeff



From lordsutch at gmail.com  Sat Dec 18 05:07:54 2004
From: lordsutch at gmail.com (Chris Lawrence)
Date: Fri, 17 Dec 2004 22:07:54 -0600
Subject: [R] Factor analysis with dichotomous variables
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7406E76B5F@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7406E76B5F@dc1ex2.air.org>
Message-ID: <e2e0e3d30412172007bba2096@mail.gmail.com>

On Fri, 17 Dec 2004 13:07:08 -0500, Doran, Harold <HDoran at air.org> wrote:
> You can use factanal to do the analysis. The polychor() package will
> give you polychorics. You can then the do the factor analysis on this
> correlation matrix.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tom Denson
> Sent: Friday, December 17, 2004 12:31 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Factor analysis with dichotomous variables
> 
> Hello,
> 
> I would like to conduct an exploratory factor analysis with dichotomous
> data. Do any R routines exist for this purpose? I recall reading
> something about methods with tetrachoric correlations.
> 
> Any help would be appreciated.

You may also want to consider the routines in MCMCpack
(MCMCordfactanal and MCMCmixfactanal), depending on your application.


Chris
-- 
Chris Lawrence - http://blog.lordsutch.com/



From jinss at hkusua.hku.hk  Sat Dec 18 05:17:34 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Sat, 18 Dec 2004 12:17:34 +0800
Subject: [R] For help
In-Reply-To: <UAWOUVYXGJQQAWISGRMISKDNFQCB.ys03165003@student.ecnu.edu.cn>
References: <UAWOUVYXGJQQAWISGRMISKDNFQCB.ys03165003@student.ecnu.edu.cn>
Message-ID: <20041218041734.GA20063@S127.localdomain>

On Sat, Dec 18, 2004 at 11:14:16AM +0800, ys03165003 at student.ecnu.edu.cn wrote:
> Hi
> 
> During using the R(vision 2.0.1), I meet a problem. I would like to do the 
> 
> Multiple Correspondence Analysis, but when I use the "< mca(lf, nf = 2, abbrev = 
> 
> FALSE)", the sentence "Error: couldn't find function 'mca'" will appear. So, 
> 
> please tell me how can I use the "mca()", thanks!
> 
>                          Any help appreciated.... 
> 
>                                                                   jeff
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
Dear Jeff,

  You should load the library MASS before you call mca.

> require(MASS)

 can load the library.
-- 


                      Jin



From spencer.graves at pdf.com  Sat Dec 18 05:48:18 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 17 Dec 2004 20:48:18 -0800
Subject: [R] For help
In-Reply-To: <303339656.19267@student.ecnu.edu.cn>
References: <303339656.19267@student.ecnu.edu.cn>
Message-ID: <41C3B692.3030101@pdf.com>

      By requesting 'help.search("mca")', I learned that mca was in 
"library(MASS)".  Did you try "library(MASS)" before your mca command? 

      hope this helps. 
      spencer graves
p.s.  Using 'help.search' was suggested by the posting guide, 
"www.R-project.org/posting-guide.html".

ys03165003 at student.ecnu.edu.cn wrote:

>Hi
>
>During using the R(vision 2.0.1), I meet a problem. I would like to do the 
>
>Multiple Correspondence Analysis, but when I use the "< mca(lf, nf = 2, abbrev = 
>
>FALSE)", the sentence "Error: couldn't find function 'mca'" will appear. So, 
>
>please tell me how can I use the "mca()", thanks!
>
>                         Any help appreciated.... 
>
>                                                                  jeff
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From patrick.meyer at internet.lu  Sat Dec 18 08:51:11 2004
From: patrick.meyer at internet.lu (Patrick Meyer)
Date: Sat, 18 Dec 2004 08:51:11 +0100
Subject: [R] Union of list elements
Message-ID: <41C3E16F.9090103@internet.lu>

Thanx to all of you who helped me with my sets problem. ;-)

Patrick



From henric.nilsson at statisticon.se  Sat Dec 18 14:10:40 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Sat, 18 Dec 2004 14:10:40 +0100
Subject: [R] SAS or R software
In-Reply-To: <1103329168.7851.35.camel@horizons.localdomain>
References: <s1c31367.099@gwise.louisville.edu>
	<1103329168.7851.35.camel@horizons.localdomain>
Message-ID: <41C42C50.5080503@statisticon.se>

Marc Schwartz said the following on 2004-12-18 01:19:

> As you are likely aware, other statistically relevant issues are
> contained in various ICH guidance documents regarding GCP considerations
> and principles for clinical trials:
> 
> http://www.ich.org/UrlGrpServer.jser?@_ID=475&@_TEMPLATE=272

ICH E9 states that (p. 27):
"The computer software used for data management and statistical analysis 
should be reliable, and documentation of appropriate software testing 
procedures should be available."

Some commercial software vendors (SAS, Insightful, and StatSoft) offer 
white papers stating that their software can work within an 21 CFR Part 
11 compliant system.

http://www.sas.com/industry/pharma/develop/papers.html

http://www.insightful.com/industry/pharm/21cfr_part11_Final.pdf

http://www.statsoft.com/support/whitepapers/pdf/STATISTICA_CFR.pdf

Some commercial vendors (SAS and Insightful) also offers tools for 
validation of the installation and operation of the software. SAS has

http://support.sas.com/documentation/installcenter/common/91/ts1m3/qualification_tools_guide.pdf

and S-PLUS has validate().

As a statistical consultant working within the pharamceutical industry, 
I think that our clients find the white papers being some kind of 
quality seal. It signals that someone has actually thought about the 
issues involved, written a document about it, and even stated that it 
can be done. Of course, there's a lot of FUD going on here. But if our 
lives can be made simpler by producing similar white papers and QA 
tools, why not?

(But for some people, only SAS will do:
Last week we were audited on behalf of a client. One of the specific 
issues discussed were validation and the Part 11 compliance of S-PLUS. 
In this specific trial, data are to be transferred from Oracle Clinical 
-> SAS -> SPLUS, and they auditors were really worried about the first 
and last link of that chain. Finally, they suggested using only SAS... 
And in this particular case, Part 11 is really a non-issue since 
physical records exists (i.e. case report forms) and all final S-PLUS 
output and code will also be stored physically (i.e. print-outs) -- no 
need for electronic signatures here!)

> There is also a general guidance document for computer systems used in
> clinical trials here:
> 
> http://www.fda.gov/ora/compliance_ref/bimo/ffinalcct.htm
> 
> Though it is to be superseded by a draft document here:
> 
> http://www.fda.gov/cder/guidance/6032dft.htm 

 From the introduction (p. 2):
"This document provides guidance about computerized systems that are 
used to create, modify, maintain, archive, retrieve, or transmit 
clinical data required to be maintained and/or submitted to the Food and 
Drug Administration (FDA)"

The `retrieve' part is certainly applicable. If we regard R as 
off-the-shelf software, the guidance says (p. 11):
"For most off-the-shelf software, the design level validation will have 
already been done by the company that wrote the software. Given the 
importance of ensuring valid clinical trial data, FDA suggests that the 
sponsor or contract research organization (CRO) have documentation
(either original validation documents or on-site vendor audit documents) 
of this design level validation by the vendor and would itself have 
performed functional testing (e.g., by use of test data sets) and 
researched known software limitations, problems, and defect corrections. 
Detailed documentation of any additional validation efforts performed by 
the sponsor or CRO will preserve the findings of these efforts.

In the special case of database and spreadsheet software that is: (1) 
purchased off-the-shelf, (2) designed for and widely used for general 
purposes, (3) unmodified, and (4) not being used for direct entry of 
data, the sponsor or contract research organization may not have 
documentation of design level validation. FDA suggests that the sponsor 
or contract research organization perform functional testing (e.g., by 
use of test data sets) and research known software limitations,
problems, and defect corrections.

In the case of off-the-shelf software, we recommend that the following 
be available to the Agency on request:

* A written design specification that describes what the software is 
intended to do and how it is intended to do it;

* A written test plan based on the design specification, including both 
structural and functional analysis; and

* Test results and an evaluation of how these results demonstrate that 
the predetermined design specification has been met."

I think the guidance is quite clear here. We must prove to the FDA, at 
their wish, that the software used is working properly. In order to do 
this, we seem to need documents describing the development process and 
the QA tools used by R Core. An idea of what we'll need may be found in 
the `Computer Systems Validation in Clinical Research - A Practical 
Guide (Edition 1)' at

http://www.acdm.org.uk/public/publications/publications.htm

Especially section 2.4, 5 + subsections, 8 + subsections, and 9.7 + 
subsections seem relevant. (I've ordered the 2nd edition, but it hasn't 
arrived yet.)


Henric



From jeaneid at chass.utoronto.ca  Sat Dec 18 14:26:12 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Sat, 18 Dec 2004 08:26:12 -0500
Subject: [R] Union of list elements
In-Reply-To: <41C3E16F.9090103@internet.lu>
Message-ID: <Pine.SGI.4.40.0412180820400.846780-100000@origin.chass.utoronto.ca>


Can't you turn the lists into data frames issue unique and
force them back to lists. Here's the code:


L <- list(c("a1","a3","a4"), c("a1","a4","a5"), c("a1","a5","a6"))
M <- list(c("a1","a3","a4"), c("a2","a4","a5"), c("a1","a5","a6"), c("a7",
"a1", "a4"))
LL <- as.data.frame(I(L))
MM <- as.data.frame(I(M))
X <- as.list(unique(rbind(LL,MM)))

You have an issue here that which needs to be addressed;
the ordering of the elements within each list. Is this suppose to be
informative? i.e. do you care that you have ("a1", "a2", "a3") versus
("a2", "a3", "a1"). If not the above solution has to be redesigned. as an
example:

L <- list(c("a1","a3","a4"), c("a1","a4","a5"), c("a1","a5","a6"))
M <- list(c("a1","a3","a4"), c("a2","a4","a5"), c("a1","a6","a5"))
LL <- as.data.frame(I(L))
MM <- as.data.frame(I(M))
X <- as.list(unique(rbind(LL,MM)))

Above will generate ("a1","a6","a5"), and ("a1","a5","a6") as two
distinct rows. However, sorting them first will give you what you want
(again if this is uninformative)

L <- lapply(L, sort)
M <- lapply(M,sort)
LL <- as.data.frame(I(L))
MM <- as.data.frame(I(M))
X <- as.list(unique(rbind(LL,MM)))


P.S In your last email you stated that you have two sets .... I think
list and you want the unique sets of list N, however
what you have is a huge list that you cannot issue the command unique.
That is why you are creating L and that is why what Gabor suggested does
not  work initially because the list L is starting empty. But if that is
the  case all you have to do is turn the list into data.frame issue unique
and turn it back to a list


M <- list(c("a1","a3","a4"), c("a2","a4","a5"), c("a1","a6","a5"),
c("a1","a6","a5"))
MM <- as.data.frame(I(M))
X <- as.list(unique(MM))


Hope this helps,


Jean,

On Sat, 18 Dec 2004, Patrick Meyer wrote:

> Thanx to all of you who helped me with my sets problem. ;-)
>
> Patrick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From hellik at web.de  Sat Dec 18 14:31:06 2004
From: hellik at web.de (Helmut Kudrnovsky)
Date: Sat, 18 Dec 2004 14:31:06 +0100
Subject: [R] variables - data-structure
Message-ID: <5.1.0.14.0.20041218134413.00a0e140@pop3.web.de>

dear R-friends,

i`ve got a large dataset of  vegetation-samples with about 500 
variables(=species) in the following format:

1 spec1
1 spec23
1 spec54
1 spec63
2 spec1
2 spec2
2 spec253
2 spec300
2 spec423
3 spec20
3 spec88
3 spec121
3 spec200
3 spec450
.
.

this means:  sample 1 (grassland) with the species (=spec) 1, 23, 54, 63

is it possible to get a following data-structure for further analysis?

		1	2	3	......
spec1		1	1	0
spec2		0	1	0
spec3
...
spec253	0	1	0
...
spec450	0	0	1

with thanks from the snowy tirol
helli



From f.harrell at vanderbilt.edu  Sat Dec 18 14:55:58 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 18 Dec 2004 07:55:58 -0600
Subject: [R] SAS or R software
In-Reply-To: <1103329168.7851.35.camel@horizons.localdomain>
References: <s1c31367.099@gwise.louisville.edu>
	<1103329168.7851.35.camel@horizons.localdomain>
Message-ID: <41C436EE.8000903@vanderbilt.edu>

Marc Schwartz wrote:
> On Fri, 2004-12-17 at 17:11 -0500, Alexander C Cambon wrote: 
> 
>>I apologize for adding this so late to the "SAS or R software "
>>thread.
>>This is a question, not a reply, but it seems to me to fit in well
>>with
>>the subject of this thread.
>>
>>I would like to know anyone's experiences in the following two areas
>>below.  I should add I have no experience myself in these areas:
>>
>>1) Migrating from SAS to R in the choice of statistical software used
>>for FDA  reporting.
> 
> 
> You will find that to be a non-issue from the FDA's perspective. This
> has been discussed here with some frequency.  If you search the archives
> you will find comments from Frank Harrell and others.
> 
> The FDA does not and cannot endorse a particular software product. Nor
> does it validate any statistical software for a specific purpose. They
> do need to be able to reproduce the results, which means they need to
> know what software product was used, which version and on what platform,
> etc.
> 
> The SAS XPORT Transport Format (which is openly defined and documented),
> has been used for the transfer of data sets and has been available in
> many statistical products.
> 
> There have been a variety of activities (CDISC, HL-7, etc) regarding the
> electronic submission of data to the FDA. Some additional information is
> here:
> 
> http://www.fda.gov/cder/regulatory/ersr/default.htm
> 
> and here:
> 
> http://www.cdisc.org/news/index.html
> 
> Any other issues impacting the selection of a particular statistical
> application are more likely to be political within your working
> environment and FUD. 
> 
> As you are likely aware, other statistically relevant issues are
> contained in various ICH guidance documents regarding GCP considerations
> and principles for clinical trials:
> 
> http://www.ich.org/UrlGrpServer.jser?@_ID=475&@_TEMPLATE=272
> 
> 
> Keep in mind also that one big advantage R has (in my mind) is the use
> of Sweave for the reproducible generation of reports, which to an extent
> are self-documenting. 
> 
> 
> 
>> (For example, was there more effort involved in areas of
>>documentation, revision tracking,  or validation of software codes?
> 
> 
> Since the FDA's role with computer software and validation has been
> raised before, the following documents cover many of these areas. The
> list is not meant to be exhaustive, but should give a flavor in this
> domain.
> 
> There are specific guidance documents by the FDA pertaining to software
> that is contained in a medical device (ie. the firmware in a pacemaker
> or medical monitoring equipment) or is used to develop a medical device.
> The current guidance in this case is here:
> 
> http://www.fda.gov/cdrh/comp/guidance/938.html
> 
> Other guidance pertains to 21 CFR 11, which addresses data management
> systems used for clinical trials and covers issues such as electronic
> signatures, audit trails and the like. A guidance document for that is
> here:
> 
> http://www.fda.gov/cder/guidance/5667fnl.htm
> 
> Keep in mind, from a perspective standpoint, that even MS Excel and
> Access can be made to be 21 CFR 11 compliant and there are companies
> whose business is focused on just that task.
> 
> There is also a general guidance document for computer systems used in
> clinical trials here:
> 
> http://www.fda.gov/ora/compliance_ref/bimo/ffinalcct.htm
> 
> Though it is to be superseded by a draft document here:
> 
> http://www.fda.gov/cder/guidance/6032dft.htm 
> 
> 
> 
>>2) Migrating from SAS to R in the choice of statistical software used
>>for NIH reporting  (or other US or non-US) government agencies) .
> 
> 
> Same here to my knowledge.
> 
> As I was typing this, I see Frank just responded.
> 
> I also just noted Doug's post, so perhaps some of the above information
> will be helpful in clarifying some of his questions as well.
> 
> I believe that the above is factually correct, but if someone knows
> anything to not be so, please correct me.
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> 

In addition to the excellent points made by Marc, Doug, and Matt, I want 
to expand on the revision tracking point originally raised by Alexander. 
  We use CVS for all pharmaceutical industry work.  Besides allowing two 
statisticians working on each project to mirror each other's data and 
code (for backup when one is out and a pressing question is asked), the 
revision control and commented change tracking of CVS has proven to work 
incredibly well in this arena.

The one area where we use SAS for pharmaceutical industry work is 
running SAS PROC EXPORT to convert data to cvs format for importing with 
the Hmisc package's sasxport.get function (see 
http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/SASexportHowto). 
We found that reading binary SAS transport format datasets in R or with 
Stat/Transfer was not reliable enough.  We have a freely available SAS 
macro that runs PROC EXPORT in a loop to get all datasets in a data 
library, with metadata.  That way any SAS exporting errors can be blamed 
on SAS.  Ironically there is a bug in PROC EXPORT.  When a character 
field has an unmatched quote in it, the CSV file can result in an odd 
number of quotes for the field.  sasxport.get checks the number of 
records imported against the number reported by PROC CONTENTS, so this 
problem is easily detected and corrected with emacs.

Note that with literally billions of dollars at their disposal, SAS 
didn't take the time to really write a procedure for PROC EXPORT.  Like 
the R sas.get function, it generates voluminous SAS DATA step code to do 
the work.

Regarding CDISC, the SAS transport format that is now accepted by FDA is 
deficient because there is no place for certain metadata (e.g., units of 
measurement, value labels are remote from the datasets, variable names 
are truncated to 8 characters).  The preferred format for CDISC will 
become XML.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From bates at stat.wisc.edu  Sat Dec 18 14:54:51 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 18 Dec 2004 07:54:51 -0600
Subject: [R] variables - data-structure
In-Reply-To: <5.1.0.14.0.20041218134413.00a0e140@pop3.web.de>
References: <5.1.0.14.0.20041218134413.00a0e140@pop3.web.de>
Message-ID: <41C436AB.1090707@stat.wisc.edu>

Helmut Kudrnovsky wrote:
> dear R-friends,
> 
> i`ve got a large dataset of  vegetation-samples with about 500 
> variables(=species) in the following format:
> 
> 1 spec1
> 1 spec23
> 1 spec54
> 1 spec63
> 2 spec1
> 2 spec2
> 2 spec253
> 2 spec300
> 2 spec423
> 3 spec20
> 3 spec88
> 3 spec121
> 3 spec200
> 3 spec450
> .
> .
> 
> this means:  sample 1 (grassland) with the species (=spec) 1, 23, 54, 63
> 
> is it possible to get a following data-structure for further analysis?
> 
>         1    2    3    ......
> spec1        1    1    0
> spec2        0    1    0
> spec3
> ...
> spec253    0    1    0
> ...
> spec450    0    0    1

It appears that you want to form a crosstabulation.  If so, you can use 
the table or xtabs functions.  I suggest you look at the documentation 
and examples of those and see if they will be suitable.



From jfox at mcmaster.ca  Sat Dec 18 15:01:00 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 18 Dec 2004 09:01:00 -0500
Subject: [R] variables - data-structure
In-Reply-To: <5.1.0.14.0.20041218134413.00a0e140@pop3.web.de>
Message-ID: <20041218140059.PZFL1694.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Helmut,

How about table(species, sample)?

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Helmut 
> Kudrnovsky
> Sent: Saturday, December 18, 2004 8:31 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] variables - data-structure
> 
> dear R-friends,
> 
> i`ve got a large dataset of  vegetation-samples with about 500
> variables(=species) in the following format:
> 
> 1 spec1
> 1 spec23
> 1 spec54
> 1 spec63
> 2 spec1
> 2 spec2
> 2 spec253
> 2 spec300
> 2 spec423
> 3 spec20
> 3 spec88
> 3 spec121
> 3 spec200
> 3 spec450
> .
> .
> 
> this means:  sample 1 (grassland) with the species (=spec) 1, 
> 23, 54, 63
> 
> is it possible to get a following data-structure for further analysis?
> 
> 		1	2	3	......
> spec1		1	1	0
> spec2		0	1	0
> spec3
> ...
> spec253	0	1	0
> ...
> spec450	0	0	1
> 
> with thanks from the snowy tirol
> helli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Sat Dec 18 15:03:12 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 18 Dec 2004 08:03:12 -0600
Subject: [R] SAS or R software
In-Reply-To: <41C42C50.5080503@statisticon.se>
References: <s1c31367.099@gwise.louisville.edu>
	<1103329168.7851.35.camel@horizons.localdomain>
	<41C42C50.5080503@statisticon.se>
Message-ID: <41C438A0.5070309@vanderbilt.edu>

Henric Nilsson wrote:
> Marc Schwartz said the following on 2004-12-18 01:19:
> 
>> As you are likely aware, other statistically relevant issues are
>> contained in various ICH guidance documents regarding GCP considerations
>> and principles for clinical trials:
>>
>> http://www.ich.org/UrlGrpServer.jser?@_ID=475&@_TEMPLATE=272
> 
> 
> ICH E9 states that (p. 27):
> "The computer software used for data management and statistical analysis 
> should be reliable, and documentation of appropriate software testing 
> procedures should be available."
> 
> Some commercial software vendors (SAS, Insightful, and StatSoft) offer 
> white papers stating that their software can work within an 21 CFR Part 
> 11 compliant system.
> 
> http://www.sas.com/industry/pharma/develop/papers.html
> 
> http://www.insightful.com/industry/pharm/21cfr_part11_Final.pdf
> 
> http://www.statsoft.com/support/whitepapers/pdf/STATISTICA_CFR.pdf
> 
> Some commercial vendors (SAS and Insightful) also offers tools for 
> validation of the installation and operation of the software. SAS has
> 
> http://support.sas.com/documentation/installcenter/common/91/ts1m3/qualification_tools_guide.pdf 
> 
> 
> and S-PLUS has validate().
> 
> As a statistical consultant working within the pharamceutical industry, 
> I think that our clients find the white papers being some kind of 
> quality seal. It signals that someone has actually thought about the 
> issues involved, written a document about it, and even stated that it 
> can be done. Of course, there's a lot of FUD going on here. But if our 
> lives can be made simpler by producing similar white papers and QA 
> tools, why not?
> 
> (But for some people, only SAS will do:
> Last week we were audited on behalf of a client. One of the specific 
> issues discussed were validation and the Part 11 compliance of S-PLUS. 
> In this specific trial, data are to be transferred from Oracle Clinical 
> -> SAS -> SPLUS, and they auditors were really worried about the first 
> and last link of that chain. Finally, they suggested using only SAS... 
> And in this particular case, Part 11 is really a non-issue since 
> physical records exists (i.e. case report forms) and all final S-PLUS 
> output and code will also be stored physically (i.e. print-outs) -- no 
> need for electronic signatures here!)
> 
>> There is also a general guidance document for computer systems used in
>> clinical trials here:
>>
>> http://www.fda.gov/ora/compliance_ref/bimo/ffinalcct.htm
>>
>> Though it is to be superseded by a draft document here:
>>
>> http://www.fda.gov/cder/guidance/6032dft.htm 
> 
> 
>  From the introduction (p. 2):
> "This document provides guidance about computerized systems that are 
> used to create, modify, maintain, archive, retrieve, or transmit 
> clinical data required to be maintained and/or submitted to the Food and 
> Drug Administration (FDA)"
> 
> The `retrieve' part is certainly applicable. 
> 
...
> Henric
> 

That is not clear.  And since FDA allows submissions using Excel, with 
not even an audit trail, and with known major statistical computing 
errors in Excel, I am fairly certain that it is not applicable or at the 
least is not enforced  in any meaningful way.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From p.dalgaard at biostat.ku.dk  Sat Dec 18 15:25:06 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Dec 2004 15:25:06 +0100
Subject: [R] variables - data-structure
In-Reply-To: <5.1.0.14.0.20041218134413.00a0e140@pop3.web.de>
References: <5.1.0.14.0.20041218134413.00a0e140@pop3.web.de>
Message-ID: <x2vfazwunx.fsf@biostat.ku.dk>

Helmut Kudrnovsky <hellik at web.de> writes:

> dear R-friends,
> 
> i`ve got a large dataset of  vegetation-samples with about 500
> variables(=species) in the following format:
> 
> 1 spec1
> 1 spec23
> 1 spec54
> 1 spec63
> 2 spec1
> 2 spec2
> 2 spec253
> 2 spec300
> 2 spec423
> 3 spec20
> 3 spec88
> 3 spec121
> 3 spec200
> 3 spec450
> .
> .
> 
> this means:  sample 1 (grassland) with the species (=spec) 1, 23, 54, 63
> 
> is it possible to get a following data-structure for further analysis?
> 
> 		1	2	3	......
> spec1		1	1	0
> spec2		0	1	0
> spec3
> ...
> spec253	0	1	0
> ...
> spec450	0	0	1
> 
> with thanks from the snowy tirol
> helli

Should be fairly easy. You could for instance generate a
table(species,area) - with a few complications if the same combination
can occur more than once. Or use matrix indexing

M <- matrix(0,nspec,narea)
M[cbind(species,area)] <- 1

Upon reading, the sort order of the species may be a little
problematic:

dd <- read.table(stdin())

0: 1 spec1
1: 1 spec23
2: 1 spec54
3: 1 spec63
4: 2 spec1
6: 2 spec2
7: 2 spec253
8: 2 spec300
9: 2 spec423
10: 3 spec20
11: 3 spec88
12: 3 spec121
13: 3 spec200
14: 3 spec450
15: 
# ctrl-D terminates input

names(dd) <- c("area","species")
with(dd, table(species,area))

         area
species   1 2 3
  spec1   1 1 0
  spec121 0 0 1
  spec2   0 1 0
  spec20  0 0 1
  spec200 0 0 1
  spec23  1 0 0
  spec253 0 1 0
  spec300 0 1 0
  spec423 0 1 0
  spec450 0 0 1
  spec54  1 0 0
  spec63  1 0 0
  spec88  0 0 1

To fix up, use something like

 specn <- paste("spec", 
                sort(as.numeric(substring(levels(dd$species),5))),
                sep="")
 dd <- transform(dd, species=factor(species,levels=specn))
 with(dd, table(species,area))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From henric.nilsson at statisticon.se  Sat Dec 18 16:24:58 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Sat, 18 Dec 2004 16:24:58 +0100
Subject: [R] SAS or R software
In-Reply-To: <41C438A0.5070309@vanderbilt.edu>
References: <s1c31367.099@gwise.louisville.edu>
	<1103329168.7851.35.camel@horizons.localdomain>
	<41C42C50.5080503@statisticon.se> <41C438A0.5070309@vanderbilt.edu>
Message-ID: <41C44BCA.3010001@statisticon.se>

Frank E Harrell Jr said the following on 2004-12-18 15:03:

> That is not clear.

Perhaps. And I think this is the issue. From the clients' perspective, 
not a single FDA document states that you can use other software than 
SAS. They haven't really thought about the fact that there isn't any FDA 
documents encouraging the use of SAS for statistical analyses.

I don't think that the real problem is convincing regulatory authorities 
that R (or any other (open-source) software for that matter) is 
operating adequately. But clients and auditors seems to reason along the 
lines of "rather being safe than sorry" and "nobody's ever been critized 
for using SAS". From their perspective, when we propose using `some 
other' software they start thinking that it perhaps may jeopardize their 
trial results (and, all to often, "but doesn't FDA require SAS?").

How to fight this? I don't know. Right now I'm thinking, "If you can 
beat 'em, join 'em" and that the way of proving that `some other' 
software works is through having similar documents and tools as the 
commercial vendors.

>  And since FDA allows submissions using Excel, with 
> not even an audit trail, and with known major statistical computing 
> errors in Excel, I am fairly certain that it is not applicable or at the 
> least is not enforced  in any meaningful way.

The general preconception seems to be that neither SAS nor Excel needs 
validation. E.g. the British guideline referenced in my previous email 
states on p. 12 that
"It is generally considered that there is no requirement for validation 
of commercial hardware and established operating systems or for packages 
such as the SAS system, Oracle and MS Excel, as entities in their own 
right. However, most are configurable systems and so need adequate 
control of installation and their configuration parameters."

Luckily for Excel, not a single word about precision and adequacy...


Henric



From baron at psych.upenn.edu  Sat Dec 18 16:48:35 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 18 Dec 2004 10:48:35 -0500
Subject: [R] SAS or R software
In-Reply-To: <41C44BCA.3010001@statisticon.se>
References: <s1c31367.099@gwise.louisville.edu>
	<1103329168.7851.35.camel@horizons.localdomain>
	<41C42C50.5080503@statisticon.se> <41C438A0.5070309@vanderbilt.edu>
	<41C44BCA.3010001@statisticon.se>
Message-ID: <20041218154835.GA25150@psych>

There were two earlier threads on this topic:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/17554.html

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/10706.html

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From bates at stat.wisc.edu  Sat Dec 18 17:00:31 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 18 Dec 2004 10:00:31 -0600
Subject: [R] SAS XPORT data format [was Re: SAS or R software]
In-Reply-To: <41C436EE.8000903@vanderbilt.edu>
References: <s1c31367.099@gwise.louisville.edu>	<1103329168.7851.35.camel@horizons.localdomain>
	<41C436EE.8000903@vanderbilt.edu>
Message-ID: <41C4541F.6010002@stat.wisc.edu>

Frank E Harrell Jr wrote:

... much discussion deleted ...

> Regarding CDISC, the SAS transport format that is now accepted by FDA is 
> deficient because there is no place for certain metadata (e.g., units of 
> measurement, value labels are remote from the datasets, variable names 
> are truncated to 8 characters).  The preferred format for CDISC will 
> become XML.

Since you brought up the SAS XPORT data format I have to respond with my 
usual rant about it.

<rant>
When it comes to the SAS XPORT data format those are at best third or 
fourth order deficiencies in the metadata.  The first order deficiency 
in the metadata is that it does not contain the number of records in a 
data set.  In this format a file can contain more than one data set and 
  a data set consists of an unknown number of fixed-length records. 
Because of the potential of more than one data set you can't just read 
to the end of the file or use the file size and the record size to 
calculate the number of records.  You must read through the file 
examining each group of 80 characters (Why 80 characters?  Those of us 
who remember punched cards can tell you why.) and for each such group 
try to determine if this is the beginning of another record in the 
current data set or the beginning of a new data set.  How is the 
beginning of a new data set indicated - by a magic string of characters. 
  What if, either perversely or accidently, this magic string of 
characters were included as a text field at the beginning of a record? 
You wouldn't be able to tell if you have a new record or a new data set.

Even better than that, there are situations in which the number of 
records in a data set is not well-defined due to the requirement of 
padding the last 80 character group with blanks.  (After all when you 
create a punch card deck from your data set you want to get an integer 
number of punched cards.) For example, if you are writing an odd number 
of 40 character records then you must pad the last 80 character group 
with blanks.  When reading this data set how can you distinguish the odd 
number of records padded with blanks from an even number of records in 
which the last record happened to be all blanks?  You can't.

When I first encountered this, I thought that I must not understand the 
format properly.  I thought that SAS (and, through SAS, the FDA) 
couldn't really be using a format in which the number of records in a 
data set can be ambiguous.  This would mean that the operations of 
writing the XPORT data set and reading it are not guaranteed to be 
inverses.  I started reading material on the SAS web site and discovered 
that SAS indeed was aware of this problem and had a solution - users 
should not create data sets that exhibit this abiguity.  That's it. 
Their solution is "don't do that".
</rant>

I think that replacing the SAS XPORT data format with XML will be a step 
forward.



From jmoreira at fe.up.pt  Sat Dec 18 17:54:49 2004
From: jmoreira at fe.up.pt (jmoreira@fe.up.pt)
Date: Sat, 18 Dec 2004 16:54:49 +0000
Subject: [R] erro in SVM (packsge "e1071")
Message-ID: <1103388889.41c460d9a963e@webmail.fe.up.pt>

Hello,

I am using SVM under e1071 package for nu-regression with 18 parameters. The 
variables are ordered factors, factors, date or numeric datatypes. I use the 
linear kernel.
It gives the following error that I cannot solve. I tryed debug, browser and 
all that stuff, but no way.
The error is:

Error in get(ctr, mode = "function", envir = parent.frame())(levels(x),  : 
        Orthogonal polynomials cannot be represented accurately enough for 236 
degrees of freedom

I use the nu parameter. However, reading ?svm help it says "parameter needed 
for 'nu-classification' and 'one-classification'". Does not say anything about 
nu-regression. It is an omission in the ?svm help page? Or am I 
notundestanding something?

I believe it has something to do with the calculus of the eigenvalues. Anyway 
how can I overpass this problem? Increasing the training data (is around 900 
records)?

Thanks for any help

Joao



From ggrothendieck at myway.com  Sat Dec 18 19:53:48 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 18 Dec 2004 18:53:48 +0000 (UTC)
Subject: [R] variables - data-structure
References: <5.1.0.14.0.20041218134413.00a0e140@pop3.web.de>
Message-ID: <loom.20041218T192206-305@post.gmane.org>

Helmut Kudrnovsky <hellik <at> web.de> writes:

: 
: dear R-friends,
: 
: i`ve got a large dataset of  vegetation-samples with about 500 
: variables(=species) in the following format:
: 
: 1 spec1
: 1 spec23
: 1 spec54
: 1 spec63
: 2 spec1
: 2 spec2
: 2 spec253
: 2 spec300
: 2 spec423
: 3 spec20
: 3 spec88
: 3 spec121
: 3 spec200
: 3 spec450
: .
: .
: 
: this means:  sample 1 (grassland) with the species (=spec) 1, 23, 54, 63
: 
: is it possible to get a following data-structure for further analysis?
: 
: 		1	2	3	......
: spec1		1	1	0
: spec2		0	1	0
: spec3
: ...
: spec253	0	1	0
: ...
: spec450	0	0	1
: 
: with thanks from the snowy tirol
: helli



If your intention is to use this as a community matrix with the R
vegan package then I think you require the transpose of the above,
namely (assuming DF contains your data frame):

comm <- with(DF, table(sample, species)) 

library(vegan)
diversity(comm)
specaccum(comm, method = "random")

etc.



From ksartor at montana.edu  Sun Dec 19 00:43:08 2004
From: ksartor at montana.edu (Karla Sartor)
Date: Sat, 18 Dec 2004 16:43:08 -0700
Subject: [R] Sums of sq in car package Anova function
Message-ID: <41C4C08C.9010408@montana.edu>

Hello R users,

I am trying to run a three factor ANOVA on a data set with unequal 
sample sizes.

I fit the data to a 'lm' object and used the Anova function from the 
'car' package with the 'type=III' option to get type III sums of 
squares.  I also set the contrast coding option to 'options(contrasts = 
c("contr.sum", "contr.poly"))' as cautioned in Jon Fox's book "An R and 
S-plus Companion to Applied Regression'.

Is there anything else that I need to consider when using the type III 
option with the Anova function?

When I run the same data set in SPSS with General Linear Model and type 
III  sums of squares, the sums of squares are different enough that one 
of the main effect terms is significant in the R table and not in the 
SPSS table.  I found a similar discrepancy with a different data set, 
only SPSS showed a significant interaction effect while, while the 
'Anova' function did not.

I also compared the results from SPSS those from the 'anova' function in 
the base package, and the results are nearly identical.  I would expect 
the two methods with type III sums of squares to be more similar, does 
anyone have any ideas as to why that was not the case?  I am hoping to 
not go back to SPSS at this point, so am trying to decide which of the 
two R functions is most appropriate for me (and defensible, considering 
the unequal sample sizes).

Thank you in advance for any ideas you may have!

Karla

Karla Sartor
Montana State University - LRES
ksartor at montana.edu



From f.harrell at vanderbilt.edu  Sun Dec 19 02:24:16 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 18 Dec 2004 19:24:16 -0600
Subject: [R] SAS or R software
In-Reply-To: <41C44BCA.3010001@statisticon.se>
References: <s1c31367.099@gwise.louisville.edu>
	<1103329168.7851.35.camel@horizons.localdomain>
	<41C42C50.5080503@statisticon.se> <41C438A0.5070309@vanderbilt.edu>
	<41C44BCA.3010001@statisticon.se>
Message-ID: <41C4D840.3080202@vanderbilt.edu>

Henric Nilsson wrote:
> Frank E Harrell Jr said the following on 2004-12-18 15:03:
> 
>> That is not clear.
> 
> 
> Perhaps. And I think this is the issue. From the clients' perspective, 
> not a single FDA document states that you can use other software than 
> SAS. They haven't really thought about the fact that there isn't any FDA 
> documents encouraging the use of SAS for statistical analyses.

Right.  This reminds me of the worst movie of all time, Plan 9 From 
Outer Space, in which the psychic Creskin closes the movie by saying 
"Can you prove that this DIDN'T happen?".

> 
> I don't think that the real problem is convincing regulatory authorities 
> that R (or any other (open-source) software for that matter) is 
> operating adequately. But clients and auditors seems to reason along the 
> lines of "rather being safe than sorry" and "nobody's ever been critized 
> for using SAS". From their perspective, when we propose using `some 
> other' software they start thinking that it perhaps may jeopardize their 
> trial results (and, all to often, "but doesn't FDA require SAS?").

Yes that is the hurdle.

> 
> How to fight this? I don't know. Right now I'm thinking, "If you can 
> beat 'em, join 'em" and that the way of proving that `some other' 
> software works is through having similar documents and tools as the 
> commercial vendors.

With the job market for statisticians being excellent, I've often 
wondered why clinical statisticians in industry are so often timid. 
Statisticians need to show strength and stamina, along with good 
teaching skills, on this issue.

> 
>>  And since FDA allows submissions using Excel, with not even an audit 
>> trail, and with known major statistical computing errors in Excel, I 
>> am fairly certain that it is not applicable or at the least is not 
>> enforced  in any meaningful way.
> 
> 
> The general preconception seems to be that neither SAS nor Excel needs 
> validation. E.g. the British guideline referenced in my previous email 
> states on p. 12 that
> "It is generally considered that there is no requirement for validation 
> of commercial hardware and established operating systems or for packages 
> such as the SAS system, Oracle and MS Excel, as entities in their own 
> right. However, most are configurable systems and so need adequate 
> control of installation and their configuration parameters."

This makes me wonder about the British system.  Have they not seen the 
serious calculation errors documented to be in Excel?

> 
> Luckily for Excel, not a single word about precision and adequacy...

Right.  Thanks for your note Henric -Frank

> 
> 
> Henric
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From sway at tanox.com  Sun Dec 19 03:54:47 2004
From: sway at tanox.com (Shawn Way)
Date: Sat, 18 Dec 2004 20:54:47 -0600
Subject: [R] SAS or R software
Message-ID: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088B59414@houston.tanox.net>

I've seen multiple comments about MS Excel's precision and accuracy.
Can you please point me in the right direction in locating information
about these?

Thank you very much,

Shawn Way, PE
Engineering Manager
Tanox, Inc.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frank E Harrell
Jr
Sent: Saturday, December 18, 2004 7:24 PM
To: Henric Nilsson
Cc: R-Help; Douglas Bates; MSchwartz at medanalytics.com; Alexander C
Cambon; Jim Garrett
Subject: Re: [R] SAS or R software


Henric Nilsson wrote:
> Frank E Harrell Jr said the following on 2004-12-18 15:03:
> 
>> That is not clear.
> 
> 
> Perhaps. And I think this is the issue. From the clients' perspective,
> not a single FDA document states that you can use other software than 
> SAS. They haven't really thought about the fact that there isn't any
FDA 
> documents encouraging the use of SAS for statistical analyses.

Right.  This reminds me of the worst movie of all time, Plan 9 From 
Outer Space, in which the psychic Creskin closes the movie by saying 
"Can you prove that this DIDN'T happen?".

> 
> I don't think that the real problem is convincing regulatory 
> authorities
> that R (or any other (open-source) software for that matter) is 
> operating adequately. But clients and auditors seems to reason along
the 
> lines of "rather being safe than sorry" and "nobody's ever been
critized 
> for using SAS". From their perspective, when we propose using `some 
> other' software they start thinking that it perhaps may jeopardize
their 
> trial results (and, all to often, "but doesn't FDA require SAS?").

Yes that is the hurdle.

> 
> How to fight this? I don't know. Right now I'm thinking, "If you can
> beat 'em, join 'em" and that the way of proving that `some other' 
> software works is through having similar documents and tools as the 
> commercial vendors.

With the job market for statisticians being excellent, I've often 
wondered why clinical statisticians in industry are so often timid. 
Statisticians need to show strength and stamina, along with good 
teaching skills, on this issue.

> 
>>  And since FDA allows submissions using Excel, with not even an audit
>> trail, and with known major statistical computing errors in Excel, I 
>> am fairly certain that it is not applicable or at the least is not 
>> enforced  in any meaningful way.
> 
> 
> The general preconception seems to be that neither SAS nor Excel needs
> validation. E.g. the British guideline referenced in my previous email

> states on p. 12 that
> "It is generally considered that there is no requirement for
validation 
> of commercial hardware and established operating systems or for
packages 
> such as the SAS system, Oracle and MS Excel, as entities in their own 
> right. However, most are configurable systems and so need adequate 
> control of installation and their configuration parameters."

This makes me wonder about the British system.  Have they not seen the 
serious calculation errors documented to be in Excel?

> 
> Luckily for Excel, not a single word about precision and adequacy...

Right.  Thanks for your note Henric -Frank

> 
> 
> Henric
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt
University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tchur at optushome.com.au  Sun Dec 19 04:30:12 2004
From: tchur at optushome.com.au (Tim Churches)
Date: Sun, 19 Dec 2004 14:30:12 +1100
Subject: [R] SAS or R software
In-Reply-To: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088B59414@houston.tanox.net>
References: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088B59414@houston.tanox.net>
Message-ID: <41C4F5C4.2070702@optushome.com.au>

Shawn Way wrote:

>I've seen multiple comments about MS Excel's precision and accuracy.
>Can you please point me in the right direction in locating information
>about these?
>  
>
As always, Google is your friend, but see for example 
http://www.nwpho.org.uk/sadb/Poisson%20CI%20in%20spreadsheets.pdf

Tim C



From MSchwartz at MedAnalytics.com  Sun Dec 19 06:44:56 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 18 Dec 2004 23:44:56 -0600
Subject: [R] SAS or R software
In-Reply-To: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088B59414@houston.tanox.net>
References: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088B59414@houston.tanox.net>
Message-ID: <1103435096.7851.94.camel@horizons.localdomain>

On Sat, 2004-12-18 at 20:54 -0600, Shawn Way wrote:
> I've seen multiple comments about MS Excel's precision and accuracy.
> Can you please point me in the right direction in locating information
> about these?
> 
> Thank you very much,

There was an exchange on this last year and I posted some links to
articles and web sites that have further information. The post in
question is:

https://stat.ethz.ch/pipermail/r-help/2003-June/033094.html

In that same thread, issues were also raised regarding the rounding of
numbers and Excel's implementation of the IEEE 754 standard. There was a
comparison between Excel, Gnumeric and OO.org's Calc with respect to
this issue and I posted some results of that comparison here:

https://stat.ethz.ch/pipermail/r-help/2003-June/033364.html

The above post provided some further insight into why Excel and
unfortunately, Calc, behave the way that they do with numbers "close to
zero". As far as I know, the same behavior is still in place today with
the current versions of each application.

HTH,

Marc Schwartz



From ys03165003 at student.ecnu.edu.cn  Sun Dec 19 09:00:45 2004
From: ys03165003 at student.ecnu.edu.cn (ys03165003@student.ecnu.edu.cn)
Date: Sun, 19 Dec 2004 16:00:45 +0800
Subject: [R] how to make the matrix as factors
Message-ID: <303443245.16312@student.ecnu.edu.cn>

Hi All,

Thanks for you help, I have loaded the library MASS to call the mca. But when I
want to do the mca, there is another problem, for example
--------
--------
> leaf <- read.table("C:/Documents and Settings/wxh-c//1.txt",
+ col.names=c("size","texture"),header=TRUE)
> leaf
    size texture
1      3       3
2      2       2
3      3       2
4      3       2
5      3       2
6      3       3
7      3       2
8      3       2
9      3       2
> attach(leaf)
> require(MASS)
Loading required package: MASS 
[1] TRUE
> new.mca <- mca(leaf)
Error in mca(leaf) : All variables must be factors
---------
So, what should I do to make the matrix as factor. Waiting for you answers.
Thanks!

                                                                jeff



From pierre.bady at univ-lyon1.fr  Sun Dec 19 10:52:00 2004
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Sun, 19 Dec 2004 10:52:00 +0100 (CET)
Subject: [R] how to make the matrix as factors
In-Reply-To: <303443245.16312@student.ecnu.edu.cn>
References: <303443245.16312@student.ecnu.edu.cn>
Message-ID: <1103449920.41c54f40737f1@webmail.univ-lyon1.fr>


Hi,

The argument df of the function 'mca?? requires only a data.frame (not matrix)
containing only factors.


# Example:
?data.frame

library(MASS)
?mca

# we  construct the matrix.
size <- c(3,2,3,3,3,3,3,3,3)
texture <- c(2,3,2,2,2,2,2,2,2)
leaf <- cbind(size,texture)

new.mca <- mca(leaf)

# Error in mca(leaf) : All variables must be factors

# now, we construct a data.frame.
dleaf <- as.data.frame(leaf)
summary(dleaf)

# we transform each variable into a factor. 
dleaf[,2] <- as.factor(dleaf[,2])
dleaf[,1] <- as.factor(dleaf[,1])
summary(dleaf)
new.mca <- mca(dleaf)
new.mca
#

cheers,


P.BADY





En r??ponse ?? ys03165003 at student.ecnu.edu.cn:

> Hi All,
> 
> Thanks for you help, I have loaded the library MASS to call the mca. But
> wen I
> want to do the mca, there is another problem, for example
> --------
> --------
> > leaf <- read.table("C:/Documents and Settings/wxh-c/????????/1.txt",
> + col.names=c("size","texture"),header=TRUE)
> > leaf
>     size texture
> 1      3       3
> 2      2       2
> 3      3       2
> 4      3       2
> 5      3       2
> 6      3       3
> 7      3       2
> 8      3       2
> 9      3       2
> > attach(leaf)
> > require(MASS)
> Loading required package: MASS 
> [1] TRUE
> > new.mca <- mca(leaf)
> Error in mca(leaf) : All variables must be factors
> ---------
> So, what should I do to make the matrix as factor. Waiting for you
> answers.
> Thanks!
> 
>                                                                 jeff
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 



---------------------------------------------------------
Pierre BADY     <??)))><
Universit?? Claude Bernard Lyon 1
UMR CNRS 5023, LEHF
bat Alphonse Forel
43 boulevard du 11 novembre 1918 
F-69622 VILLEURBANNE CEDEX 
FRANCE
TEL : +33 (0)4 72 44 62 34 
FAX : +33 (0)4 72 43 28 92 
MEL : pierre.bady at univ-lyon1.fr
http://limnologie.univ-lyon1.fr



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 19 11:00:48 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 19 Dec 2004 10:00:48 -0000 (GMT)
Subject: [R] SAS or R software
In-Reply-To: <41C4D840.3080202@vanderbilt.edu>
Message-ID: <XFMail.041219100048.Ted.Harding@nessie.mcc.ac.uk>

On 19-Dec-04 Frank E Harrell Jr wrote:
> Henric Nilsson wrote:
>> 
>> How to fight this? I don't know. Right now I'm thinking, "If you can 
>> beat 'em, join 'em" and that the way of proving that `some other' 
>> software works is through having similar documents and tools as the 
>> commercial vendors.
> 
> With the job market for statisticians being excellent, I've often 
> wondered why clinical statisticians in industry are so often timid. 
> Statisticians need to show strength and stamina, along with good 
> teaching skills, on this issue.

Because, I fear (and I don't have good documentation on it but I
do have quite a strong impression), that such is not what their
employers and managers see as their role and function.

Other may wish to comment ...

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 19-Dec-04                                       Time: 10:00:48
------------------------------ XFMail ------------------------------



From pburns at pburns.seanet.com  Sun Dec 19 11:38:39 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sun, 19 Dec 2004 10:38:39 +0000
Subject: [R] Problems with Excel (was SAS or R software)
In-Reply-To: <41C4F5C4.2070702@optushome.com.au>
References: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088B59414@houston.tanox.net>
	<41C4F5C4.2070702@optushome.com.au>
Message-ID: <41C55A2F.9000200@pburns.seanet.com>

I'm in the process of researching problems with Excel.  The references
given by Tim and Marc seem to lead to discussions of most of the problems
with statistical procedures in Excel.  The executive summary is that if
it is in Excel and it looks like statistics, then avoid it.

However, there are some other issues as well.  Here are three:

1)  For those of us used to S, Excel gives a non-intuitive result for:

           -2^4

I wonder how many formulas are in existence that have unintended results
due to this.

2)  When numbers in scientific notation are written to ascii (csv or 
txt), Excel
decides that some of the digits are unnecessary and doesn't write them 
to the
file.  As far as I can tell the number of significant digits that you 
get is arbitrary
and capricious.  Apparently Microsoft thinks of this as a feature -- I'm 
not sure
what they think the up-side is.  If you want all of your digits, make 
sure that none
of the numbers are displayed in scientific notation.  (Yes, what is 
written depends
on what is displayed, but it is not WYSIWYG.)

3)  In the olden days if there were a blank cell in a range of cells on 
which a
function operated (a mean perhaps), then Excel decided that the blank meant
zero.  Microsoft changed this so that blank cells are ignored 
(apparently a good
thing).  However, if you reference a blank cell, it is then counted as zero.

Exercise:  In the first column put numbers in the first few rows, but 
leave one
cell blank.  below that do the average of the cells (including the 
blank).  Now
in cell B1 put "=A1"  and copy this formula down column B.  You will get 
two
different means.


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Tim Churches wrote:

> Shawn Way wrote:
>
>> I've seen multiple comments about MS Excel's precision and accuracy.
>> Can you please point me in the right direction in locating information
>> about these?
>>  
>>
> As always, Google is your friend, but see for example 
> http://www.nwpho.org.uk/sadb/Poisson%20CI%20in%20spreadsheets.pdf
>
> Tim C
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>



From fredrik.bg.lundgren at bredband.net  Sun Dec 19 11:43:18 2004
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Sun, 19 Dec 2004 11:43:18 +0100
Subject: [R]
Message-ID: <004c01c4e5b7$8d04e2a0$5f9d72d5@Larissa>

test - please disregard
Fredrik Lundgren



From kretschmer at kaufbach.delug.de  Sun Dec 19 12:59:17 2004
From: kretschmer at kaufbach.delug.de (Andreas Kretschmer,,,)
Date: Sun, 19 Dec 2004 12:59:17 +0100
Subject: [R] Can I calculate the area of a polygon?
Message-ID: <20041219115917.GA7547@kaufbach.delug.de>

Hello,

I'm verry new in R.

My Problem:

I have a PostgreSQL-Server with installed support for the R-Language.
And now I have a table with a Polygon like this:

test_db=# select * from geo where id=1;
 id |             koerper
----+---------------------------------
  1 | ((0,0),(0,2),(2,2),(3,3),(5,0))
(1 row)

Now, I need to know the area of this object. Please, can you tell me, if
it possible to use the installed plr - Language zu calculate the area?

I have some experience to draw diagramms from Postgres via the
R-language, but I'm a absolute novice to use R to solve problems like
this.


Regards, Andreas
-- 
Diese Message wurde erstellt mit freundlicher Unterst??tzung eines freilau-
fenden Pinguins aus artgerechter Freilandhaltung.   Er ist garantiert frei
von Micro$oft'schen Viren. (#97922 http://counter.li.org)     GPG 7F4584DA
Was, Sie wissen nicht, wo Kaufbach ist? Hier: N 51.05082??, E 13.56889?? ;-)



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 19 13:34:37 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 19 Dec 2004 12:34:37 -0000 (GMT)
Subject: [R] SAS or R software
In-Reply-To: <41C4F5C4.2070702@optushome.com.au>
Message-ID: <XFMail.041219123437.Ted.Harding@nessie.mcc.ac.uk>

On 19-Dec-04 Tim Churches wrote:
> Shawn Way wrote:
> 
>>I've seen multiple comments about MS Excel's precision and accuracy.
>>Can you please point me in the right direction in locating information
>>about these?
>>  
>>
> As always, Google is your friend, but see for example 
> http://www.nwpho.org.uk/sadb/Poisson%20CI%20in%20spreadsheets.pdf

There is a huge literature on this topic, some of it published
in journals, much of it floating around on the web and in the
archives of mailing lists.

Tim's reference above is interesting, but only one example.
The McCullough and Wilson reference given there, though now
somewhat dated, identifies many of the classic problems.
Googling on

  mccullough wilson excel

will throw up a host of followups.

Informed statistical comment on the problems of Excel encountered
by serious users can be found by browsing in the mailing list
ASSUME (Association of Statistics Specialists Using Microsoft Excel):

  http://www.jiscmail.ac.uk/lists/assume.html

The most recent serious issue reported there is the RAND() bug:
see ASSUME archives for Dec 2003 followed up in the March 2004
archives. The latter point to a statement from Microsoft:

  SYMPTOMS
  When you use the RAND function in Microsoft Office Excel 2003,
  the RAND function may return negative numbers.
  CAUSE
  This problem may occur when you try to use many random numbers,
  and you update the RAND function multiple times. For example,
  this problem may occur when you update your Excel worksheet by
  pressing F9 ten times or more.
  RESOLUTION
  This problem is fixed in the Microsoft Excel 2003 Hotfix Package
  that is dated February 29, 2004. 

  http://support.microsoft.com/default.aspx?scid=kb;en-us;834520

  (and the deeper you probe in this, the worse it gets).


While using Excel for statistics has some limited value in the
context of initiating to statistics students whose IT experience
is limited to exposure to courses on Excel and Word, and the
teacher wants to build on such experience, I think that Excel
should never be used for serious statistical work, for several
reasons.

1. The many reported (and some allegedly fixed) bugs in
   calculation and algorithms necessarily provoke suspicion
   that others still exist or may have been introduced. One
   simply cannot trust the results without checking.

2. Too many things can be done silently and invisibly, "behind
   the spreadsheet", by Excel. Changes to data and differences
   between what is shown on the spreadsheet and what goes into
   exported files can arise without the user being aware of them.

   A particularly frightening example is the "sort" disaster
   reported to ASSUME (8 Dec 2003) by Allan Reese.

3. Excel has some value as a straightforward data entry pad.
   However, I have seen far too many cases where sloppy usage
   has led to the resulting spreadsheet containing "information"
   which is either superfluous or wrong, in ways which would
   not be obvious to the user.

   For example, a "missing data" cell, if blank, may be interpreted
   as having value zero. Some people enter "." for missing data,
   but often are not consistent. If inadvertently a space is
   entered in a cell outside the intended row/column range of the
   data (or, I suspect, even if the spreadsheet cursor wanders
   outside the range) then when the sheet is exported (e.g. as
   "CSV") these extra rows and columns will be included.

   In one case I received an Excel spredsheet with hundreds of
   such extra rows and dozens of extra columns, together with
   dozens of cases where " " and "." had been used inconsistently,
   all this over and over on each of about 6 "worksheet" pages;
   not to mention data in the wrong columns etc. It took about 4
   days of continuous work to clean this up.

   To be frank, for entering complex data the discipline enforced
   by a properly designed Data Entry Form in a database package
   would avoid such problems altogether, and such should be used.
   The illusion of success that Excel gives the user is a most
   treacherous danger and frankly I simply do not, in the first
   instance, trust data in a spreadsheet.

4. The use of formulae in cells to generate cell values can
   cause all sorts of problems. One to especially watch out for
   is that a formula may have been wrongly or inappropriately
   "copied" from one column to another or from one worksheet
   to another. You can of course check this by moving the cell
   cursor to such cells and noting what the formula is, but as
   you can imagine this is a horribly uhpleasant process (and
   by the way take care that you don't inadvertently alter it
   while you're doing this!). Also see Allan Reese's "sort"
   disaster above, which was formula-induced.

I could go on. I've written at length already because many
readers of R-help may be in situations where they necessarily
receive data in Excel files, or have to use Excel themselves,
and may not yet have become aware of the risks. So I'm writing
as a warning to them: Don't trust Excel, but if you must use it
then check everything, make sure it's what it should be, and
make sure that it stays that way when the spreadsheet is
accessed (as in (3) or (4) above, things may change invisibly).

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 19-Dec-04                                       Time: 12:34:37
------------------------------ XFMail ------------------------------



From david.meyer at wu-wien.ac.at  Sun Dec 19 14:23:07 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Sun, 19 Dec 2004 14:23:07 +0100
Subject: [R] erro in SVM (packsge "e1071")
Message-ID: <20041219142307.00c8336a.david.meyer@wu-wien.ac.at>

Joao:

The reported error message is not from e1071.
How *exactly* did you call svm()?

As to the documentation of the nu parameter: yes, this is an omission,
of course, nu is used in nu-regression as well; thanks for pointing this
out.

best,
David

---------

Hello,

I am using SVM under e1071 package for nu-regression with 18 parameters.
The 
variables are ordered factors, factors, date or numeric datatypes. I use
the 
linear kernel.
It gives the following error that I cannot solve. I tryed debug, browser
and 
all that stuff, but no way.
The error is:

Error in get(ctr, mode = "function", envir = parent.frame())(levels(x), 
: 
        Orthogonal polynomials cannot be represented accurately enough
for 236 
degrees of freedom

I use the nu parameter. However, reading ?svm help it says "parameter
needed 
for 'nu-classification' and 'one-classification'". Does not say anything
about 
nu-regression. It is an omission in the ?svm help page? Or am I 
notundestanding something?

I believe it has something to do with the calculus of the eigenvalues.
Anyway 
how can I overpass this problem? Increasing the training data (is around
900 
records)?

Thanks for any help

Joao




-- 
Dr. David Meyer
Department of Information Systems

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From f.harrell at vanderbilt.edu  Sun Dec 19 14:38:53 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 19 Dec 2004 08:38:53 -0500
Subject: [R] SAS or R software
In-Reply-To: <XFMail.041219123437.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041219123437.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41C5846D.5050401@vanderbilt.edu>

(Ted Harding) wrote:
> On 19-Dec-04 Tim Churches wrote:
> 
>>Shawn Way wrote:
>>
>>
>>>I've seen multiple comments about MS Excel's precision and accuracy.
>>>Can you please point me in the right direction in locating information
>>>about these?
>>> 
>>>
>>
>>As always, Google is your friend, but see for example 
>>http://www.nwpho.org.uk/sadb/Poisson%20CI%20in%20spreadsheets.pdf
. . .

Also see http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/ExcelProblems
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ggrothendieck at myway.com  Sun Dec 19 14:41:10 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 19 Dec 2004 13:41:10 +0000 (UTC)
Subject: [R] how to make the matrix as factors
References: <303443245.16312@student.ecnu.edu.cn>
	<1103449920.41c54f40737f1@webmail.univ-lyon1.fr>
Message-ID: <loom.20041219T143603-800@post.gmane.org>


Note that an easy way to get them all to be factors is to add the
appropriate colClasses= argument to your read.table call:

   leaf <- read.table(...whatever...,  colClasses = "factor")
   mca(leaf)

See ?read.table for more about the colClasses= argument.

Pierre BADY <pierre.bady <at> univ-lyon1.fr> writes:

: 
: Hi,
: 
: The argument df of the function ?'mca?? requires only a data.frame (not matrix)
: containing only factors.
: 
: # Example:
: ?data.frame
: 
: library(MASS)
: ?mca
: 
: # we  construct the matrix.
: size <- c(3,2,3,3,3,3,3,3,3)
: texture <- c(2,3,2,2,2,2,2,2,2)
: leaf <- cbind(size,texture)
: 
: new.mca <- mca(leaf)
: 
: # Error in mca(leaf) : All variables must be factors
: 
: # now, we construct a data.frame.
: dleaf <- as.data.frame(leaf)
: summary(dleaf)
: 
: # we transform each variable into a factor. 
: dleaf[,2] <- as.factor(dleaf[,2])
: dleaf[,1] <- as.factor(dleaf[,1])
: summary(dleaf)
: new.mca <- mca(dleaf)
: new.mca
: #
: 
: cheers,
: 
: P.BADY
: 
: 
: En r??ponse ?  ys03165003 <at> student.ecnu.edu.cn:
: 
: > Hi All,
: > 
: > Thanks for you help, I have loaded the library MASS to call the mca. But
: > wen I
: > want to do the mca, there is another problem, for example
: > --------
: > --------
: > > leaf <- read.table("C:/Documents and Settings/wxh-c/???????/1.txt",
: > + col.names=c("size","texture"),header=TRUE)
: > > leaf
: >     size texture
: > 1      3       3
: > 2      2       2
: > 3      3       2
: > 4      3       2
: > 5      3       2
: > 6      3       3
: > 7      3       2
: > 8      3       2
: > 9      3       2
: > > attach(leaf)
: > > require(MASS)
: > Loading required package: MASS 
: > [1] TRUE
: > > new.mca <- mca(leaf)
: > Error in mca(leaf) : All variables must be factors
: > ---------
: > So, what should I do to make the matrix as factor. Waiting for you
: > answers.
: > Thanks!
: > 
: >                                                                 jeff
: > 
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help
: > PLEASE do read the posting guide!
: > http://www.R-project.org/posting-guide.html
: > 
: > 
: 
: ---------------------------------------------------------
: Pierre BADY     <??)))><
: Universit?? Claude Bernard Lyon 1
: UMR CNRS 5023, LEHF
: bat Alphonse Forel
: 43 boulevard du 11 novembre 1918 
: F-69622 VILLEURBANNE CEDEX 
: FRANCE
: TEL : +33 (0)4 72 44 62 34 
: FAX : +33 (0)4 72 43 28 92 
: MEL : pierre.bady <at> univ-lyon1.fr
: http://limnologie.univ-lyon1.fr
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 19 14:40:32 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 19 Dec 2004 13:40:32 -0000 (GMT)
Subject: [R] Can I calculate the area of a polygon?
In-Reply-To: <20041219115917.GA7547@kaufbach.delug.de>
Message-ID: <XFMail.041219134032.Ted.Harding@nessie.mcc.ac.uk>

On 19-Dec-04 Andreas Kretschmer,,, wrote:
> Hello,
> I'm verry new in R.

Welcome!

> My Problem:
> 
> I have a PostgreSQL-Server with installed support for the R-Language.
> And now I have a table with a Polygon like this:
> 
> test_db=# select * from geo where id=1;
>  id |             koerper
> ----+---------------------------------
>   1 | ((0,0),(0,2),(2,2),(3,3),(5,0))
> (1 row)
> 
> Now, I need to know the area of this object. Please, can you tell me,
> if it possible to use the installed plr - Language zu calculate the
> area?
> 
> I have some experience to draw diagramms from Postgres via the
> R-language, but I'm a absolute novice to use R to solve problems like
> this.

Not sure what you mean by "the installed plr - Language", but for
calculating areas og polygons you may find the following function
useful.
Example based on the coordinates ((0,0),(0,2),(2,2),(3,3),(5,0))
you give above.

Let X be the matrix of the coordinates:

  > X
       [,1] [,2]
  [1,]    0    0
  [2,]    0    2
  [3,]    2    2
  [4,]    3    3
  [5,]    5    0


Function "area":

  area<-function(X){
    X<-rbind(X,X[1,])
    x<-X[,1]; y<-X[,2]; lx<-length(x)
    -sum((x[2:lx]-x[1:lx-1])*(y[2:lx]+y[1:lx-1]))/2
  }

  > area(X)
  [1] -9.5

Note that this function generates a "signed area": positive
if the bounding contour is "counter-clockwise" in the order
of the vertices as given in X, negative if clockwise.

The principle of the function is that it first closes the
polygon by repeating the first point, and then (in effect)
calculates the contour integral

  Int -y*dx

round the contour of the polygon (equivalent to Int x*dy).

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 19-Dec-04                                       Time: 13:40:32
------------------------------ XFMail ------------------------------



From numero.primo at tele2.it  Sun Dec 19 15:24:03 2004
From: numero.primo at tele2.it (Landini Massimiliano)
Date: Sun, 19 Dec 2004 15:24:03 +0100
Subject: [R] Different graph type can coexisti??
Message-ID: <h93bs0pnh977men0e3qgtatdvgiv3936ce@4ax.com>

Please consider a data frame

A	B	C	D
1	4	5	0
2	3	2	75
3	4	1	84
4	5	1	90
5	3	0	100

Is there a way to plot column B and C as barplot *and* D as line on the same
graph??

R-2.0.1 powered by Mandrake 10.1


-------------------------------------------------------------------------------------------------------------------------
Landini dr. Massimiliano
Tel. mob. (+39) 347 140 11 94
Tel./Fax. (+39) 051 762 196
e-mail: numero (dot) primo (at) tele2 (dot) it
-------------------------------------------------------------------------------------------------------------------------
Legge di Hanggi: Pi?? stupida ?? la tua ricerca, pi?? verr?? letta e approvata.
Corollario alla Legge di Hanggi: Pi?? importante ?? la tua ricerca, meno verr??
capita.



From kretschmer at kaufbach.delug.de  Sun Dec 19 15:41:32 2004
From: kretschmer at kaufbach.delug.de (Andreas Kretschmer,,,)
Date: Sun, 19 Dec 2004 15:41:32 +0100
Subject: [R] Can I calculate the area of a polygon?
In-Reply-To: <XFMail.041219134032.Ted.Harding@nessie.mcc.ac.uk>
References: <20041219115917.GA7547@kaufbach.delug.de>
	<XFMail.041219134032.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20041219144132.GA1286@kaufbach.delug.de>

am  Sun, dem 19.12.2004, um 13:40:32 -0000 mailte Ted Harding folgendes:
> Not sure what you mean by "the installed plr - Language", but for

The Database-System PostgreSQL is ready to extend with external
features. As example: pl/r is a procedural language. So can I share data
between the database and R.

,----[  lynx --dump  plr-overview.html  ]
| Chapter 1. Overview
|
|    PL/R is a loadable procedural language that enables you to write
|    PostgreSQL functions and triggers in the [6]R programming language.
|    PL/R offers most (if not all) of the capabilities a function writer
|    has in the R language.
`----



> calculating areas og polygons you may find the following function
> useful.
> Example based on the coordinates ((0,0),(0,2),(2,2),(3,3),(5,0))
> you give above.
> 
> Let X be the matrix of the coordinates:
> 
>   > X
>        [,1] [,2]
>   [1,]    0    0
>   [2,]    0    2
>   [3,]    2    2
>   [4,]    3    3
>   [5,]    5    0
> 
> 
> Function "area":
> 
>   area<-function(X){
>     X<-rbind(X,X[1,])
>     x<-X[,1]; y<-X[,2]; lx<-length(x)
>     -sum((x[2:lx]-x[1:lx-1])*(y[2:lx]+y[1:lx-1]))/2
>   }
> 
>   > area(X)
>   [1] -9.5

Danke, Merci, thank you.

> Hoping this helps,

Yes.


Regards, Andreas
-- 
Diese Message wurde erstellt mit freundlicher Unterst??tzung eines freilau-
fenden Pinguins aus artgerechter Freilandhaltung.   Er ist garantiert frei
von Micro$oft'schen Viren. (#97922 http://counter.li.org)     GPG 7F4584DA
Was, Sie wissen nicht, wo Kaufbach ist? Hier: N 51.05082??, E 13.56889?? ;-)



From mkimpel at iupui.edu  Sun Dec 19 15:57:43 2004
From: mkimpel at iupui.edu (Kimpel, Mark W)
Date: Sun, 19 Dec 2004 09:57:43 -0500
Subject: [R] limma, FDR, and p.adjust
Message-ID: <2E6C5260C7C387449A96DF46EE76313C01961EE0@iu-mssg-mbx02.exchange.iu.edu>

I am posting this to both R and BioC communities because I believe there
is a lot of confusion on this topic in both communities (having searched
the mail archives of both) and I am hoping that someone will have
information that can be shared with both communities.

I have seen countless questions on the BioC list regarding limma
(Bioconductor) and its calculation of FDR. Some of them involved
misunderstandings or confusions regarding across which tests the FDR
"correction" is being applied. My question is more fundamental and
involves how the FDR method is implemented at the level of "p.adjust"
(package: stats).

I have reread the paper by Benjamini and Hochberg (1995) and nowhere in
their paper do they actually "adjust" p values; rather, they develop
criteria by which an appropriate p value maximum is chosen such that FDR
is expected to be below a certain threshold. 

To try to get a better handle on this, I wrote the following simple
script to generate a list of random p values, and view it before and
after apply p.adjust (method=fdr). 
 
rn<-abs(rnorm(100, 0.5, 0.33))
rn<-rn[order(rn)]
rn<-rn[1:80]
rn
p.adj<-p.adjust(rn, method="fdr")
p.adj

As you can see after running the code, the p values are truly being
adjusted, but for what FDR? If I set my p value at 0.05, does that mean
my FDR is 5%? I have been told by someone that is the case but,
normally, when discussing FDR, q values are reported or just one p value
is reported--the threshold for a set FDR. The p.adjust documentation is
unclear.

For the R developers, I can understand how one would want to include FDR
procedures in p.adjust, but I wonder, given the numerous FDR algorithms
now available, if it would be best to formulate an FDR.select function
that would be option to p.adjust and itself incorporate more recent FDR
procedures than the one proposed by Benjamini and Hochberg in 1995.
(Benjamini himself has a newer one). Some of these may currently be
available as add-on packages but they are not standardized regarding I&O
and this makes it difficult for developers to incorporate them into
packages such as limma.

So those are my questions and suggestions, 

Thanks,

Mark W. Kimpel MD



From jfox at mcmaster.ca  Sun Dec 19 16:12:05 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 19 Dec 2004 10:12:05 -0500
Subject: [R] Sums of sq in car package Anova function
In-Reply-To: <41C4C08C.9010408@montana.edu>
Message-ID: <20041219151205.XQHP1694.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Karla,

I suggested last night that you send me further information, but decided
this morning to try out a reproducible example of my own:

> set.seed(12345)
> A <- factor(sample(c("a1", "a2", "a3"), 100, replace=TRUE))
> B <- factor(sample(c("b1", "b2"), 100, replace=TRUE))
> C <- factor(sample(c("c1", "c2", "c3"), 100, replace=TRUE))
> mu <- array(1:18, c(3,2,3))
> a <- as.numeric(A)
> b <- as.numeric(B)
> c <- as.numeric(C)
> y <- mu[cbind(a,b,c)] + rnorm(100)
> mod <- lm(y ~ A*B*C)
> library(car)
> options(contrasts=c("contr.sum", "contr.poly"))
> Anova(mod, type="II")
Anova Table (Type II tests)

Response: y
           Sum Sq Df   F value    Pr(>F)    
A           65.88  2   38.4098 1.696e-12 ***
B          196.47  1  229.0775 < 2.2e-16 ***
C         2441.00  2 1423.0809 < 2.2e-16 ***
A:B          0.22  2    0.1259    0.8819    
A:C          6.92  4    2.0174    0.0996 .  
B:C          0.87  2    0.5095    0.6027    
A:B:C        2.89  4    0.8432    0.5018    
Residuals   70.33 82                        
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> Anova(mod, type="III")
Anova Table (Type III tests)

Response: y
            Sum Sq Df   F value    Pr(>F)    
(Intercept) 7830.2  1 9129.8959 < 2.2e-16 ***
A             55.7  2   32.4913 4.059e-11 ***
B            189.5  1  221.0076 < 2.2e-16 ***
C           2124.0  2 1238.2549 < 2.2e-16 ***
A:B            0.2  2    0.0942    0.9102    
A:C            5.9  4    1.7323    0.1507    
B:C            0.6  2    0.3417    0.7115    
A:B:C          2.9  4    0.8432    0.5018    
Residuals     70.3 82                        
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 


I don't have a working copy of SPSS anymore, but here's what SAS does with
this example:

      Source                      DF      Type II SS     Mean Square    F
Value    Pr > F

      A                            2       65.884048       32.942024
38.41    <.0001
      B                            1      196.467384      196.467384
229.08    <.0001
      A*B                          2        0.215883        0.107942
0.13    0.8819
      C                            2     2440.998718     1220.499359
1423.08    <.0001
      A*C                          4        6.920872        1.730218
2.02    0.0996
      B*C                          2        0.873945        0.436973
0.51    0.6027
      A*B*C                        4        2.892820        0.723205
0.84    0.5018


      Source                      DF     Type III SS     Mean Square    F
Value    Pr > F

      A                            2       55.732128       27.866064
32.49    <.0001
      B                            1      189.546201      189.546201
221.01    <.0001
      A*B                          2        0.161608        0.080804
0.09    0.9102
      C                            2     2123.968177     1061.984089
1238.25    <.0001
      A*C                          4        5.942845        1.485711
1.73    0.1507
      B*C                          2        0.586168        0.293084
0.34    0.7115
      A*B*C                        4        2.892820        0.723205
0.84    0.5018

So, as you can see, the results check.

It's hard to know what to make of this without more information about what
you did. Much as I'm not an admirer of SPSS, I doubt whether it computes
type-III sums of squares incorrectly, so I suspect something wrong with
either your SPSS commands or your R commands.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Karla Sartor
> Sent: Saturday, December 18, 2004 6:43 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Sums of sq in car package Anova function
> 
> Hello R users,
> 
> I am trying to run a three factor ANOVA on a data set with 
> unequal sample sizes.
> 
> I fit the data to a 'lm' object and used the Anova function 
> from the 'car' package with the 'type=III' option to get type 
> III sums of squares.  I also set the contrast coding option 
> to 'options(contrasts = c("contr.sum", "contr.poly"))' as 
> cautioned in Jon Fox's book "An R and S-plus Companion to 
> Applied Regression'.
> 
> Is there anything else that I need to consider when using the 
> type III option with the Anova function?
> 
> When I run the same data set in SPSS with General Linear 
> Model and type III  sums of squares, the sums of squares are 
> different enough that one of the main effect terms is 
> significant in the R table and not in the SPSS table.  I 
> found a similar discrepancy with a different data set, only 
> SPSS showed a significant interaction effect while, while the 
> 'Anova' function did not.
> 
> I also compared the results from SPSS those from the 'anova' 
> function in the base package, and the results are nearly 
> identical.  I would expect the two methods with type III sums 
> of squares to be more similar, does anyone have any ideas as 
> to why that was not the case?  I am hoping to not go back to 
> SPSS at this point, so am trying to decide which of the two R 
> functions is most appropriate for me (and defensible, 
> considering the unequal sample sizes).
> 
> Thank you in advance for any ideas you may have!
> 
> Karla
> 
> Karla Sartor
> Montana State University - LRES
> ksartor at montana.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From bob.ohara at helsinki.fi  Sun Dec 19 16:12:29 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Sun, 19 Dec 2004 17:12:29 +0200
Subject: [R] Different graph type can coexisti??
In-Reply-To: <h93bs0pnh977men0e3qgtatdvgiv3936ce@4ax.com>
References: <h93bs0pnh977men0e3qgtatdvgiv3936ce@4ax.com>
Message-ID: <41C59A5D.5030200@helsinki.fi>

Landini Massimiliano wrote:

>Please consider a data frame
>
>A	B	C	D
>1	4	5	0
>2	3	2	75
>3	4	1	84
>4	5	1	90
>5	3	0	100
>
>Is there a way to plot column B and C as barplot *and* D as line on the same
>graph??
>
>  
>
It's not totally clear to me how you want to plot B and C, but the 
following code, and ?barplot should get you what you want:

Bars=matrix(cbind(B,C), nrow=2, byrow=T)

PLOT=barplot(Bars, ylim=c(0,100))
lines(PLOT,D, col=2)
axis(1)

The only tricky thing is dealing with the way R scales the x-axis, but 
fortunately the function barplot() returns the mid-points: hence this 
construction.

Bob

-- 
Bob O'Hara
Department of Mathematics and Statistics
P.O. Box 68 (Gustaf H??llstr??min katu 2b)
FIN-00014 University of Helsinki
Finland

Telephone: +358-9-191 51479
Mobile: +358 50 599 0540
Fax:  +358-9-191 51400
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: www.jnr-eeb.org



From ksartor at montana.edu  Sun Dec 19 17:32:14 2004
From: ksartor at montana.edu (Karla Sartor)
Date: Sun, 19 Dec 2004 09:32:14 -0700
Subject: [R] Sums of sq in car package Anova function
In-Reply-To: <20041219151205.XQHP1694.tomts36-srv.bellnexxia.net@JohnDesktop8300>
References: <20041219151205.XQHP1694.tomts36-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <41C5AD0E.4060307@montana.edu>

John,

Thank very much for your help.  I think that I have figured out my 
problem.  The levels of one of my factors are "1" and "0".  While this 
didn't matter with the 'anova()' function, is does seem to alter the 
results with the 'Anova' function.  When I changed the levels to 
letters, the tables matched my SPSS output.  As for why the type III 
test in SPSS was nearly identical to the 'anova' function, my unequal 
sample sizes were not drastically different so changing to type III must 
not have changed the results very much?  That was all I could come up 
with at the time.

Here is the code I used:

options(contrasts = c("contr.sum", "contr.poly"))
require(car)

GH = read.table("GH.txt", header =T)
GH.sub = subset(GH, GH$sp=="C")
attach(GH.sub)

biomass= log10(GH.sub$tot.bio)
GH.sub.fit = lm(biomass~am*nbr*barr, data=GH.sub)
print(Anova(GH.sub.fit, type='III'))

I get this with "1" and "0" factor levels:

Anova Table (Type III tests)

Response: biomass
                 Sum Sq  Df   F value        Pr(>F)   
(Intercept) 51.943   1     3725.4324 < 2.2e-16 ***
am             2.403    1     172.3630   < 2.2e-16 ***
nbr            0.779    3      18.6347     4.434e-10 ***
barr           0.078    1      5.5803       0.01968 * 
am:nbr       0.018    3      0.4284       0.73296   
am:barr      0.039    1      2.7826       0.09775 . 
nbr:barr      0.044    3     1.0606        0.36834   
am:nbr:barr 0.022    3     0.5208        0.66873   
Residuals    1.771 127   


And this with letter factor levels:

Anova Table (Type III tests)

Response: biomass
                    Sum Sq  Df   F value          Pr(>F)   
(Intercept)    75.371   1     5405.7202     < 2e-16 ***
am                2.403     1     172.3630     < 2e-16 ***
nbr               1.482     3      35.4357     < 2e-16 ***
barr              0.040     1      2.8410       0.09434 . 
am:nbr          0.018     3      0.4284       0.73296   
am:barr         0.039     1      2.7826       0.09775 . 
nbr:barr         0.051     3      1.2167      0.30643   
am:nbr:barr    0.022     3     0.5208       0.66873   
Residuals       1.771 127                     
---

SPSS gives: 

Tests of Between-Subjects Effects
Dependent Variable: lot10.tot.bio
Source                    Type III                 df    Mean Square    
F             Sig.
                                Sum of Squares   
Corrected Model    4.002(a)                 15       .267           
19.133      .000
Intercept                  75.371                     1    75.371       
   5405.720  .000
am                           2.403                       1    2.403      
      172.363    .000
nbr                           1.482                       3    .494   
          35.436       .000
barr                           .040                        1    .040    
        .841            .094
am * nbr                    018                         3    .006        
    .428            .733
am * barr                   .039                        1    .039        
     2.783         .098
nbr * barr                  .051                        3    .017        
     1.217         .306
am * nbr * barr         .022                        3     .007         
    .521           .669
Error                        1.771                      127  .014        
   
Total                        80.796                    143               
Corrected Total        5.772                      142               
a    R Squared = .693 (Adjusted R Squared = .657)


Am I missing something else?  I don't know the best way to post the data 
set, so I will send it to John and maybe he can post it if it is of 
interest.

Thanks again!

Karla

Karla Sartor
Montana State University - LRES
ksartor at montana.edu



 


John Fox wrote:

>Dear Karla,
>
>I suggested last night that you send me further information, but decided
>this morning to try out a reproducible example of my own:
>
>  
>
>>set.seed(12345)
>>A <- factor(sample(c("a1", "a2", "a3"), 100, replace=TRUE))
>>B <- factor(sample(c("b1", "b2"), 100, replace=TRUE))
>>C <- factor(sample(c("c1", "c2", "c3"), 100, replace=TRUE))
>>mu <- array(1:18, c(3,2,3))
>>a <- as.numeric(A)
>>b <- as.numeric(B)
>>c <- as.numeric(C)
>>y <- mu[cbind(a,b,c)] + rnorm(100)
>>mod <- lm(y ~ A*B*C)
>>library(car)
>>options(contrasts=c("contr.sum", "contr.poly"))
>>Anova(mod, type="II")
>>    
>>
>Anova Table (Type II tests)
>
>Response: y
>           Sum Sq Df   F value    Pr(>F)    
>A           65.88  2   38.4098 1.696e-12 ***
>B          196.47  1  229.0775 < 2.2e-16 ***
>C         2441.00  2 1423.0809 < 2.2e-16 ***
>A:B          0.22  2    0.1259    0.8819    
>A:C          6.92  4    2.0174    0.0996 .  
>B:C          0.87  2    0.5095    0.6027    
>A:B:C        2.89  4    0.8432    0.5018    
>Residuals   70.33 82                        
>---
>Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
>  
>
>>Anova(mod, type="III")
>>    
>>
>Anova Table (Type III tests)
>
>Response: y
>            Sum Sq Df   F value    Pr(>F)    
>(Intercept) 7830.2  1 9129.8959 < 2.2e-16 ***
>A             55.7  2   32.4913 4.059e-11 ***
>B            189.5  1  221.0076 < 2.2e-16 ***
>C           2124.0  2 1238.2549 < 2.2e-16 ***
>A:B            0.2  2    0.0942    0.9102    
>A:C            5.9  4    1.7323    0.1507    
>B:C            0.6  2    0.3417    0.7115    
>A:B:C          2.9  4    0.8432    0.5018    
>Residuals     70.3 82                        
>---
>Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
>
>
>I don't have a working copy of SPSS anymore, but here's what SAS does with
>this example:
>
>      Source                      DF      Type II SS     Mean Square    F
>Value    Pr > F
>
>      A                            2       65.884048       32.942024
>38.41    <.0001
>      B                            1      196.467384      196.467384
>229.08    <.0001
>      A*B                          2        0.215883        0.107942
>0.13    0.8819
>      C                            2     2440.998718     1220.499359
>1423.08    <.0001
>      A*C                          4        6.920872        1.730218
>2.02    0.0996
>      B*C                          2        0.873945        0.436973
>0.51    0.6027
>      A*B*C                        4        2.892820        0.723205
>0.84    0.5018
>
>
>      Source                      DF     Type III SS     Mean Square    F
>Value    Pr > F
>
>      A                            2       55.732128       27.866064
>32.49    <.0001
>      B                            1      189.546201      189.546201
>221.01    <.0001
>      A*B                          2        0.161608        0.080804
>0.09    0.9102
>      C                            2     2123.968177     1061.984089
>1238.25    <.0001
>      A*C                          4        5.942845        1.485711
>1.73    0.1507
>      B*C                          2        0.586168        0.293084
>0.34    0.7115
>      A*B*C                        4        2.892820        0.723205
>0.84    0.5018
>
>So, as you can see, the results check.
>
>It's hard to know what to make of this without more information about what
>you did. Much as I'm not an admirer of SPSS, I doubt whether it computes
>type-III sums of squares incorrectly, so I suspect something wrong with
>either your SPSS commands or your R commands.
>
>I hope this helps,
> John
>
>--------------------------------
>John Fox
>Department of Sociology
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>905-525-9140x23604
>http://socserv.mcmaster.ca/jfox 
>-------------------------------- 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Karla Sartor
>>Sent: Saturday, December 18, 2004 6:43 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Sums of sq in car package Anova function
>>
>>Hello R users,
>>
>>I am trying to run a three factor ANOVA on a data set with 
>>unequal sample sizes.
>>
>>I fit the data to a 'lm' object and used the Anova function 
>>from the 'car' package with the 'type=III' option to get type 
>>III sums of squares.  I also set the contrast coding option 
>>to 'options(contrasts = c("contr.sum", "contr.poly"))' as 
>>cautioned in Jon Fox's book "An R and S-plus Companion to 
>>Applied Regression'.
>>
>>Is there anything else that I need to consider when using the 
>>type III option with the Anova function?
>>
>>When I run the same data set in SPSS with General Linear 
>>Model and type III  sums of squares, the sums of squares are 
>>different enough that one of the main effect terms is 
>>significant in the R table and not in the SPSS table.  I 
>>found a similar discrepancy with a different data set, only 
>>SPSS showed a significant interaction effect while, while the 
>>'Anova' function did not.
>>
>>I also compared the results from SPSS those from the 'anova' 
>>function in the base package, and the results are nearly 
>>identical.  I would expect the two methods with type III sums 
>>of squares to be more similar, does anyone have any ideas as 
>>to why that was not the case?  I am hoping to not go back to 
>>SPSS at this point, so am trying to decide which of the two R 
>>functions is most appropriate for me (and defensible, 
>>considering the unequal sample sizes).
>>
>>Thank you in advance for any ideas you may have!
>>
>>Karla
>>
>>Karla Sartor
>>Montana State University - LRES
>>ksartor at montana.edu
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>
>  
>



From sdutky at terpalum.umd.edu  Sun Dec 19 17:48:38 2004
From: sdutky at terpalum.umd.edu (Steve Dutky)
Date: Sun, 19 Dec 2004 11:48:38 -0500
Subject: [R] Using hexbin to Eliminate Indiscernible Points from a Plot 
References: <85a76feb.b04b82da.819d300@ms08.mrf.mail.rcn.net>
Message-ID: <001901c4e5ea$969b88e0$01010101@cable.rcn.com>

Dear all,
I have installed hexbin 1.1-4 package and cobbled together 
the following function, cull, to eliminate xy points closer 
than a specified resolution.   While it appears to reduce the 
congestion of points on a plot, I am surprised to see 
overlapping points remain.

I would appreciate any insight as to what may cause this and 
as to whether a better way to reduce plot congestion may exist.

Thanks, Steve Dutky
sdutky at terpalum.umd.edu
+1-301-545-4113

"cull"<-function(x,y,PAR=par(),resolution=PAR$cin[1]/2) {
#
# returns a logical vector of points that are discernible at 
# the specified resolution.

# requires that either that PAR argument be passed explicity 
# as named list of "cin" and "pin" parameters or that active 
# graphics device already sized for plotting x and y

# PARS$pin == width X height of plot areas in inches
   bins<-ceiling(PAR$pin/resolution)[1]
   shape<-PAR$pin[2]/PAR$pin[1]

   if (missing(y))  #assume x has two columns
      !duplicated(hexbin(x[,1],x[,2],xbins=bins,shape=shape,IDs=T)@cID)
   else
      !duplicated(hexbin(x,y,xbins=bins,shape=shape,IDs=T)@cID)


}

#example:
library(hexbin)
m<-cbind(rnorm(10000),rnorm(10000))
plot(apply(m,2,FUN=range))
k<-cull(m, resolution=9/72)
plot(m[k,])



From apiszcz at solarrain.com  Sun Dec 19 18:08:18 2004
From: apiszcz at solarrain.com (Al Piszcz)
Date: Sun, 19 Dec 2004 12:08:18 -0500 (EST)
Subject: [R] Plotting and Decluttering Text
Message-ID: <Pine.LNX.4.61.0412191149210.20207@l1>


Goal: adjust overlapping 'text' items in plot so they are legible

I have located 'space' as one possible technique for decluttering
text positions on a plot. I would rather not jitter the plot point, just
the text nearby.

Are there other techniques or functions available that declutters a plot 
using 'text'?

Perhaps an arrow or a line to the plot mark could be used to
lead to the text item.

The following will demonstrate my concern. If you do not see overlap
use a larger value for plot density. The plot I am attempting with the
real data has a density of 64 providing space adjacent to the point.
In the real data there are few clusters that have tighter spacing
than produced in the example below. A plotdensity value of 150 or so
will give a better visualization of the issue.

Thanks.

# EXAMPLE VIEW
plotdensity<-150
x1<-rnorm(plotdensity)
y1<-rnorm(plotdensity)
plot(x1,y1)
textLabels <- as.character(1:plotdensity)
text(x1,y1,textLabels,pos=1)



==
I have also reviewed sunflowerplot.



From jfox at mcmaster.ca  Sun Dec 19 18:25:32 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 19 Dec 2004 12:25:32 -0500
Subject: [R] Sums of sq in car package Anova function
In-Reply-To: <41C5AD0E.4060307@montana.edu>
Message-ID: <20041219172532.GEA1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Karla,

If indeed one of your factors has levels "0" and "1", that wouldn't matter
at all, but if it is a numeric variable with values 0 and 1 (rather than a
factor) then that would make a difference to the linear model that's fit to
the data. The difference doesn't affect the sequential ("type-I") sums of
squares produced by anova() but it does affect some of the type-III sums of
squares produced by Anova().

Anyway, I'm glad that you found the error.

Regards,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Karla Sartor
> Sent: Sunday, December 19, 2004 11:32 AM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Sums of sq in car package Anova function
> 
> John,
> 
> Thank very much for your help.  I think that I have figured 
> out my problem.  The levels of one of my factors are "1" and 
> "0".  While this didn't matter with the 'anova()' function, 
> is does seem to alter the results with the 'Anova' function.  
> When I changed the levels to letters, the tables matched my 
> SPSS output.  As for why the type III test in SPSS was nearly 
> identical to the 'anova' function, my unequal sample sizes 
> were not drastically different so changing to type III must 
> not have changed the results very much?  That was all I could 
> come up with at the time.
> 
> Here is the code I used:
> 
> options(contrasts = c("contr.sum", "contr.poly"))
> require(car)
> 
> GH = read.table("GH.txt", header =T)
> GH.sub = subset(GH, GH$sp=="C")
> attach(GH.sub)
> 
> biomass= log10(GH.sub$tot.bio)
> GH.sub.fit = lm(biomass~am*nbr*barr, data=GH.sub) 
> print(Anova(GH.sub.fit, type='III'))
> 
> I get this with "1" and "0" factor levels:
> 
> Anova Table (Type III tests)
> 
> Response: biomass
>                  Sum Sq  Df   F value        Pr(>F)   
> (Intercept) 51.943   1     3725.4324 < 2.2e-16 ***
> am             2.403    1     172.3630   < 2.2e-16 ***
> nbr            0.779    3      18.6347     4.434e-10 ***
> barr           0.078    1      5.5803       0.01968 * 
> am:nbr       0.018    3      0.4284       0.73296   
> am:barr      0.039    1      2.7826       0.09775 . 
> nbr:barr      0.044    3     1.0606        0.36834   
> am:nbr:barr 0.022    3     0.5208        0.66873   
> Residuals    1.771 127   
> 
> 
> And this with letter factor levels:
> 
> Anova Table (Type III tests)
> 
> Response: biomass
>                     Sum Sq  Df   F value          Pr(>F)   
> (Intercept)    75.371   1     5405.7202     < 2e-16 ***
> am                2.403     1     172.3630     < 2e-16 ***
> nbr               1.482     3      35.4357     < 2e-16 ***
> barr              0.040     1      2.8410       0.09434 . 
> am:nbr          0.018     3      0.4284       0.73296   
> am:barr         0.039     1      2.7826       0.09775 . 
> nbr:barr         0.051     3      1.2167      0.30643   
> am:nbr:barr    0.022     3     0.5208       0.66873   
> Residuals       1.771 127                     
> ---
> 
> SPSS gives: 
> 
> Tests of Between-Subjects Effects
> Dependent Variable: lot10.tot.bio
> Source                    Type III                 df    Mean 
> Square    
> F             Sig.
>                                 Sum of Squares   
> Corrected Model    4.002(a)                 15       .267           
> 19.133      .000
> Intercept                  75.371                     1    
> 75.371       
>    5405.720  .000
> am                           2.403                       1    
> 2.403      
>       172.363    .000
> nbr                           1.482                       3    .494   
>           35.436       .000
> barr                           .040                        1  
>   .040    
>         .841            .094
> am * nbr                    018                         3    
> .006        
>     .428            .733
> am * barr                   .039                        1    
> .039        
>      2.783         .098
> nbr * barr                  .051                        3    
> .017        
>      1.217         .306
> am * nbr * barr         .022                        3     
> .007         
>     .521           .669
> Error                        1.771                      127  
> .014        
>    
> Total                        80.796                    143    
>            
> Corrected Total        5.772                      142               
> a    R Squared = .693 (Adjusted R Squared = .657)
> 
> 
> Am I missing something else?  I don't know the best way to 
> post the data set, so I will send it to John and maybe he can 
> post it if it is of interest.
> 
> Thanks again!
> 
> Karla
> 
> Karla Sartor
> Montana State University - LRES
> ksartor at montana.edu
> 
> 
> 
>  
> 
> 
> John Fox wrote:
> 
> >Dear Karla,
> >
> >I suggested last night that you send me further information, but 
> >decided this morning to try out a reproducible example of my own:
> >
> >  
> >
> >>set.seed(12345)
> >>A <- factor(sample(c("a1", "a2", "a3"), 100, replace=TRUE)) B <- 
> >>factor(sample(c("b1", "b2"), 100, replace=TRUE)) C <- 
> >>factor(sample(c("c1", "c2", "c3"), 100, replace=TRUE)) mu <- 
> >>array(1:18, c(3,2,3)) a <- as.numeric(A) b <- as.numeric(B) c <- 
> >>as.numeric(C) y <- mu[cbind(a,b,c)] + rnorm(100) mod <- 
> lm(y ~ A*B*C)
> >>library(car)
> >>options(contrasts=c("contr.sum", "contr.poly")) Anova(mod, 
> type="II")
> >>    
> >>
> >Anova Table (Type II tests)
> >
> >Response: y
> >           Sum Sq Df   F value    Pr(>F)    
> >A           65.88  2   38.4098 1.696e-12 ***
> >B          196.47  1  229.0775 < 2.2e-16 ***
> >C         2441.00  2 1423.0809 < 2.2e-16 ***
> >A:B          0.22  2    0.1259    0.8819    
> >A:C          6.92  4    2.0174    0.0996 .  
> >B:C          0.87  2    0.5095    0.6027    
> >A:B:C        2.89  4    0.8432    0.5018    
> >Residuals   70.33 82                        
> >---
> >Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> >  
> >
> >>Anova(mod, type="III")
> >>    
> >>
> >Anova Table (Type III tests)
> >
> >Response: y
> >            Sum Sq Df   F value    Pr(>F)    
> >(Intercept) 7830.2  1 9129.8959 < 2.2e-16 ***
> >A             55.7  2   32.4913 4.059e-11 ***
> >B            189.5  1  221.0076 < 2.2e-16 ***
> >C           2124.0  2 1238.2549 < 2.2e-16 ***
> >A:B            0.2  2    0.0942    0.9102    
> >A:C            5.9  4    1.7323    0.1507    
> >B:C            0.6  2    0.3417    0.7115    
> >A:B:C          2.9  4    0.8432    0.5018    
> >Residuals     70.3 82                        
> >---
> >Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> >
> >
> >I don't have a working copy of SPSS anymore, but here's what 
> SAS does 
> >with this example:
> >
> >      Source                      DF      Type II SS     
> Mean Square    F
> >Value    Pr > F
> >
> >      A                            2       65.884048       32.942024
> >38.41    <.0001
> >      B                            1      196.467384      196.467384
> >229.08    <.0001
> >      A*B                          2        0.215883        0.107942
> >0.13    0.8819
> >      C                            2     2440.998718     1220.499359
> >1423.08    <.0001
> >      A*C                          4        6.920872        1.730218
> >2.02    0.0996
> >      B*C                          2        0.873945        0.436973
> >0.51    0.6027
> >      A*B*C                        4        2.892820        0.723205
> >0.84    0.5018
> >
> >
> >      Source                      DF     Type III SS     
> Mean Square    F
> >Value    Pr > F
> >
> >      A                            2       55.732128       27.866064
> >32.49    <.0001
> >      B                            1      189.546201      189.546201
> >221.01    <.0001
> >      A*B                          2        0.161608        0.080804
> >0.09    0.9102
> >      C                            2     2123.968177     1061.984089
> >1238.25    <.0001
> >      A*C                          4        5.942845        1.485711
> >1.73    0.1507
> >      B*C                          2        0.586168        0.293084
> >0.34    0.7115
> >      A*B*C                        4        2.892820        0.723205
> >0.84    0.5018
> >
> >So, as you can see, the results check.
> >
> >It's hard to know what to make of this without more 
> information about 
> >what you did. Much as I'm not an admirer of SPSS, I doubt whether it 
> >computes type-III sums of squares incorrectly, so I suspect 
> something 
> >wrong with either your SPSS commands or your R commands.
> >
> >I hope this helps,
> > John
> >
> >--------------------------------
> >John Fox
> >Department of Sociology
> >McMaster University
> >Hamilton, Ontario
> >Canada L8S 4M4
> >905-525-9140x23604
> >http://socserv.mcmaster.ca/jfox
> >--------------------------------
> >
> >  
> >
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Karla Sartor
> >>Sent: Saturday, December 18, 2004 6:43 PM
> >>To: r-help at stat.math.ethz.ch
> >>Subject: [R] Sums of sq in car package Anova function
> >>
> >>Hello R users,
> >>
> >>I am trying to run a three factor ANOVA on a data set with unequal 
> >>sample sizes.
> >>
> >>I fit the data to a 'lm' object and used the Anova function 
> from the 
> >>'car' package with the 'type=III' option to get type III sums of 
> >>squares.  I also set the contrast coding option to 
> 'options(contrasts 
> >>= c("contr.sum", "contr.poly"))' as cautioned in Jon Fox's 
> book "An R 
> >>and S-plus Companion to Applied Regression'.
> >>
> >>Is there anything else that I need to consider when using 
> the type III 
> >>option with the Anova function?
> >>
> >>When I run the same data set in SPSS with General Linear Model and 
> >>type III  sums of squares, the sums of squares are different enough 
> >>that one of the main effect terms is significant in the R table and 
> >>not in the SPSS table.  I found a similar discrepancy with 
> a different 
> >>data set, only SPSS showed a significant interaction effect while, 
> >>while the 'Anova' function did not.
> >>
> >>I also compared the results from SPSS those from the 'anova' 
> >>function in the base package, and the results are nearly 
> identical.  I 
> >>would expect the two methods with type III sums of squares 
> to be more 
> >>similar, does anyone have any ideas as to why that was not 
> the case?  
> >>I am hoping to not go back to SPSS at this point, so am trying to 
> >>decide which of the two R functions is most appropriate for me (and 
> >>defensible, considering the unequal sample sizes).
> >>
> >>Thank you in advance for any ideas you may have!
> >>
> >>Karla
> >>
> >>Karla Sartor
> >>Montana State University - LRES
> >>ksartor at montana.edu
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From numero.primo at tele2.it  Sun Dec 19 20:46:46 2004
From: numero.primo at tele2.it (Landini Massimiliano)
Date: Sun, 19 Dec 2004 20:46:46 +0100
Subject: [R] Homogeneity of variance tests between more than 2 samples (long)
Message-ID: <7kjbs017one6c37l6heq4r5486hqo8rjkt@4ax.com>

Dear all
a couple of months ago i've found threads regard test that verify AnOVa
assumption on homogeneity of variances.  Prof. Ripley advice LDA / QDA
procedures, many books (and many proprietary programs) advice Hartley's F_max,
Cochran's minimum/maximum variance ratio (only balanced experiments), K^2
Bartlett's test, Levene's test.

Morton B. Brown and  Alan B. Forsythe in a 1974 article wrote about "Robust test
for the equality of variances" (editet by Journal of the American Statistical
Association Vol. 69, pp.: 364-367) "...the common F-ratio and Bartlett??s test
are very sensitive to the assumption that the underlying populations are from a
Gaussian distribution. When the underlying distributions are nonnormal, these
tests can have an actual size several times larger than their nominal level of
significance...."

Peter Armitage in  Statistical Methods in Medical Research ( Blackwell
Scientific Publication, 1971, page. 212) "...Bartlett's test maybe is less
useful than it seems; motif are two: first F test is very sensitive to the
nonnormality; second, in samples with few data, true variances must differ in
considerable manner before there is a wise/reasonable probability to obtain
results significant. In other word, even if M/C ratio is NOT significant,
estimated  variances and true variances can differ in substantial manner. If
eventually differences in true variances had weight in further analysis, is more
clever admit differences, even if tests give a non significant result..."

So, I'm asking at gurus which is best behaviour, which test they use or teach.

-------------------------------------------------------------------------------------------------------------------------
Landini dr. Massimiliano
Tel. mob. (+39) 347 140 11 94
Tel./Fax. (+39) 051 762 196
e-mail: numero (dot) primo (at) tele2 (dot) it
-------------------------------------------------------------------------------------------------------------------------
Legge di Hanggi: Pi?? stupida ?? la tua ricerca, pi?? verr?? letta e approvata.
Corollario alla Legge di Hanggi: Pi?? importante ?? la tua ricerca, meno verr??
capita.



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 19 20:44:19 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 19 Dec 2004 19:44:19 -0000 (GMT)
Subject: [R] PBIB datataset
Message-ID: <XFMail.041219194419.Ted.Harding@nessie.mcc.ac.uk>

I'm looking at Pinheiro & Bates "Mixed-Effects Models in
S and S-PLUS" at the moment. Several datasets are used,
one of which is called "PBIB" (a partially balanced
incomplete block design).

All the other datasets can be found somewhere or other in R.

However, I cannot locate PBIB, and it does not seem to
be mentioned in the latest edition of the R Full Reference
Manual.

I Have Been Good. I Have Searched The Arcives.

There was a similar query back on 16 July 2002 from E. Corda;
it seems this got no response (that I could find).

So: Is it available? If not, is there a particular reason
(e.g. copyright)?

If not available on CRAN, is there an alternative source?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 19-Dec-04                                       Time: 19:44:19
------------------------------ XFMail ------------------------------



From maustin at amgen.com  Sun Dec 19 21:12:42 2004
From: maustin at amgen.com (Austin, Matt)
Date: Sun, 19 Dec 2004 12:12:42 -0800
Subject: [R] PBIB datataset
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DCC0@teal-exch.amgen.com>

Is it the PBIB dataset in the SASmixed package?  I don't have my copy of the
text at home.

--Matt

> library(SASmixed)
Loading required package: lme4 

Attaching package 'lme4':


        The following object(s) are masked from package:nlme :

         Alfalfa Assay bdf BodyWeight Cefamandole Dialyzer Earthquake
ergoStool Fatigue Gasoline getCovariateFormula getResponseFormula Glucose
Glucose2 Gun IGF lmeControl Machines MathAchieve MathAchSchool Meat Milk
Muscle Nitrendipene Oats Orthodont Ovary Oxboys Oxide PBG Phenobarb Pixel
Quinidine Rail RatPupWeight Relaxin Remifentanil Soybean Spruce
Tetracycline1 Tetracycline2 Wafer Wheat Wheat2 

Warning message: 
package 'SASmixed' was built under R version 2.0.1 
> data(PBIB)
> PBIB
   response Treatment Block
1       2.4        15     1
2       2.5         9     1
3       2.6         1     1
4       2.0        13     1
5       2.7         5     2

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
> Ted.Harding at nessie.mcc.ac.uk
> Sent: Sunday, December 19, 2004 11:44 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] PBIB datataset
> 
> 
> I'm looking at Pinheiro & Bates "Mixed-Effects Models in
> S and S-PLUS" at the moment. Several datasets are used,
> one of which is called "PBIB" (a partially balanced
> incomplete block design).
> 
> All the other datasets can be found somewhere or other in R.
> 
> However, I cannot locate PBIB, and it does not seem to
> be mentioned in the latest edition of the R Full Reference
> Manual.
> 
> I Have Been Good. I Have Searched The Arcives.
> 
> There was a similar query back on 16 July 2002 from E. Corda;
> it seems this got no response (that I could find).
> 
> So: Is it available? If not, is there a particular reason
> (e.g. copyright)?
> 
> If not available on CRAN, is there an alternative source?
> 
> With thanks,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 19-Dec-04                                       Time: 19:44:19
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 19 22:46:09 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 19 Dec 2004 21:46:09 -0000 (GMT)
Subject: [R] Homogeneity of variance tests between more than 2 sample
In-Reply-To: <7kjbs017one6c37l6heq4r5486hqo8rjkt@4ax.com>
Message-ID: <XFMail.041219214609.Ted.Harding@nessie.mcc.ac.uk>

On 19-Dec-04 Landini Massimiliano wrote:
> Dear all
> a couple of months ago i've found threads regard test that
> verify AnOVa assumption on homogeneity of variances.  Prof.
> Ripley advice LDA / QDA procedures, many books (and many
> proprietary programs) advice Hartley's F_max, Cochran's
> minimum/maximum variance ratio (only balanced experiments),
> K^2 Bartlett's test, Levene's test.
> 
> Morton B. Brown and  Alan B. Forsythe in a 1974 article wrote
> about "Robust test for the equality of variances" (editet by
> Journal of the American Statistical Association Vol. 69, pp.:
> 364-367) "...the common F-ratio and Bartlett??s test are very
> sensitive to the assumption that the underlying populations
> are from a Gaussian distribution. When the underlying distributions
> are nonnormal, these tests can have an actual size several times
> larger than their nominal level of significance...."
> 
> Peter Armitage in  Statistical Methods in Medical Research
> ( Blackwell Scientific Publication, 1971, page. 212)
> "...Bartlett's > test maybe is less useful than it seems; motif
> are two: first F test is very sensitive to the nonnormality;
> second, in samples with few data, true variances must differ
> in considerable manner before there is a wise/reasonable probability
> to obtain results significant. In other word, even if M/C ratio
> is NOT significant, estimated  variances and true variances can
> differ in substantial manner. If eventually differences in true
> variances had weight in further analysis, is more clever admit
> differences, even if tests give a non significant result..."
> 
> So, I'm asking at gurus which is best behaviour, which test they use or
> teach.

It's true that Bartlett's test tends to be a better test of
normality of distribution than of homogeneity of variance.

It's also true that with small numbers of data these tests
are not powerful (though in that case you cannot hope for
much anyway).

For non-normal data, there's something of a question as to
what is meant (or, perhaps more accurately, what is intended
to be meant) by homogeneity of variance, as a test preliminary
to an analysis of variance.

It is possible to consider distribution-free approaches to
this mind of question.

One of Tukey's sneakiest inventions was the application of
the Mann-Whitney test (usually seen as a test of identity
of distribution against location-shift types of alternative,
more accurately against alternatives like "P(X<u) > P(Y<u)")
to test similarity of dispersion.

The trick: given X1 , ... , Xm and Y1 , ... , Yn, pool them
and sort the result as Z1 < Z2 < ... < ZN where N = m + n.

Now take the Z's in the order

  Z[1] , Z[N] , Z[2] , Z[N-1] , Z[3] , Z[N-2] , ....

i.e. work inwards from the ends, alternately from each end.

Note, as you proceed, whether each Z is an X or a Y.
You thus get a sequence of Xs and Ys. Then sum the number
of pairs (X,Y) in this sequence where the X occurs earlier
than the Y.

This sum, under the null hypothesis of identity of distribution,
has the Mann-Whitney distribution (just like its usual version),
and it is sensitive to differences of dispersion (e.g. if the
distribution of X is more dispersed than the distribution of Y,
then the Xs will be found earlier in the sequence since they
lie further out than the Ys and so will be counted in first
by the above method).

No doubt, just as there are distribution-free extensions of
procedures like Mann-Whitney to several samples ("nonparametric
ANOVA"), so such a procedure could be applied to test equality
of "dispersions" for several samples, and no doubt it has been
done.

However, I've not made use of such a procedure myself, so I
have to leave it to others to report details.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 19-Dec-04                                       Time: 21:46:09
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 19 22:16:05 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 19 Dec 2004 21:16:05 -0000 (GMT)
Subject: [R] PBIB datataset
In-Reply-To: <E7D5AB4811D20B489622AABA9C53859104E0DCC0@teal-exch.amgen.com>
Message-ID: <XFMail.041219211605.Ted.Harding@nessie.mcc.ac.uk>

Thanks, Austin, I think that probably clears it up (see below).

On 19-Dec-04 Austin, Matt wrote:
> Is it the PBIB dataset in the SASmixed package?  I don't have
> my copy of the text at home.
> 
> --Matt
> 
>> library(SASmixed)
> Loading required package: lme4 
> 
> Attaching package 'lme4':
> 
>         The following object(s) are masked from package:nlme :
> 
>          Alfalfa Assay bdf BodyWeight Cefamandole Dialyzer
> Earthquake ergoStool Fatigue Gasoline getCovariateFormula
> getResponseFormula Glucose Glucose2 Gun IGF lmeControl Machines
> MathAchieve MathAchSchool Meat Milk Muscle Nitrendipene Oats
> Orthodont Ovary Oxboys Oxide PBG Phenobarb Pixel Quinidine Rail
> RatPupWeight Relaxin Remifentanil Soybean Spruce Tetracycline1
> Tetracycline2 Wafer Wheat Wheat2 

The dataset names in the above masked objects are the entire
list of datasets in P&B except for CO2, ChickWeight, DNase,
Indometh, Loblolly, Orange and Theoph, and also PBIB. All of
these except PBIB can be found elsewhere, but (as you show
below) PBIB can be found in SASmixed and so completes the party.

However, SASmixed itself cannot be found the the R Full Reference
Manual (of 25 Nov 2004) either ... the dates on the current
versions of lme3 and SASmixed are 2004-12-16 and 2004-12-15
respectively. Now that I try it (today), the "R Site Search" of
Jonathan Baron does bring it up.

Thanks for helping to clarify this!
Ted.

> 
> Warning message: 
> package 'SASmixed' was built under R version 2.0.1 
>> data(PBIB)
>> PBIB
>    response Treatment Block
> 1       2.4        15     1
> 2       2.5         9     1
> 3       2.6         1     1
> 4       2.0        13     1
> 5       2.7         5     2
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
>> Ted.Harding at nessie.mcc.ac.uk
>> Sent: Sunday, December 19, 2004 11:44 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] PBIB datataset
>> 
>> 
>> I'm looking at Pinheiro & Bates "Mixed-Effects Models in
>> S and S-PLUS" at the moment. Several datasets are used,
>> one of which is called "PBIB" (a partially balanced
>> incomplete block design).
>> 
>> All the other datasets can be found somewhere or other in R.
>> 
>> However, I cannot locate PBIB, and it does not seem to
>> be mentioned in the latest edition of the R Full Reference
>> Manual.
>> 
>> I Have Been Good. I Have Searched The Arcives.
>> 
>> There was a similar query back on 16 July 2002 from E. Corda;
>> it seems this got no response (that I could find).
>> 
>> So: Is it available? If not, is there a particular reason
>> (e.g. copyright)?
>> 
>> If not available on CRAN, is there an alternative source?
>> 
>> With thanks,
>> Ted.
>> 
>> 
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
>> Date: 19-Dec-04                                       Time: 19:44:19
>> ------------------------------ XFMail ------------------------------
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 19-Dec-04                                       Time: 21:16:05
------------------------------ XFMail ------------------------------



From mchaudha at jhsph.edu  Mon Dec 20 00:54:50 2004
From: mchaudha at jhsph.edu (Ashraf Chaudhary)
Date: Sun, 19 Dec 2004 18:54:50 -0500
Subject: [R] Confidence Intervals from Bootstrap Replications 
In-Reply-To: <200412172253.iBHMrolr027711@volta.gene.com>
Message-ID: <200412192354.iBJNsvk4013988@hypatia.math.ethz.ch>

Thank you for the response. The suggestions are helpful. I would look into
the capabilities of the boot function. The solution in 8.3 is the last
resort that I was trying to avoid. Regards,
Ashraf

-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com] 
Sent: Friday, December 17, 2004 5:54 PM
To: 'Mohammad A. Chaudhary'; r-help at stat.math.ethz.ch
Subject: RE: [R] Confidence Intervals from Bootstrap Replications 


See , e.g. section 8.3 "The two-sample problem" of Efron and Tibshirani's AN
INTRODUCTION TO THE BOOTSTRAP. It makes it clear there why one just
bootstrap samples independently from the two separate samples.

The "strata" argument of boot() in the boot package allows one to do such
independent sampling and use the capabilities of that function.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Mohammad A. Chaudhary
> Sent: Friday, December 17, 2004 2:39 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Confidence Intervals from Bootstrap Replications 
> 
> Hi All:
> 
> I have to compute bootstrap confidence intervals, the statistic
> (incremental cost effectiveness ratio) is computed from two samples
> (intervention and control) of different sizes. All the bootstrap
> functions that I have seen use one dataset as argument. I may go ahead
> and get the desired number of bootstrap replications 
> separately. I would
> appreciate if you could point me to a source of a bootstrap 
> function (if
> available) that takes the B bootstrap replications and other 
> descriptive
> statistics and can get me the confidence intervals. Please 
> write me if I
> have not been clear in explaining my problem. Regards,
> 
> Ashraf  
> 
>  
> 
> ___________________________________
> 
> M. Ashraf Chaudhary, Ph.D.
> 
> Associate Scientist/Biostatistician
> 
> Department of International Health
> 
> Disease Prevention and Control Program
> 
> Johns Hopkins University Bloomberg School of Public Health
> 
> 615 North Wolfe Street, Room W5506
> 
> Baltimore MD 21205
> 
>  
> 
> mchaudha at jhsph.edu 
> 
> Phone: (410) 502-0741/Fax: (410) 502-6733
> 
> http://faculty.jhsph.edu/?F=Mohammad&L=Chaudhary
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Dec 20 01:11:20 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Dec 2004 01:11:20 +0100
Subject: [R] Homogeneity of variance tests between more than 2 sample
In-Reply-To: <XFMail.041219214609.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041219214609.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x27jndltg7.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> For non-normal data, there's something of a question as to
> what is meant (or, perhaps more accurately, what is intended
> to be meant) by homogeneity of variance, as a test preliminary
> to an analysis of variance.

Yes... If you use the test as a preliminary to an ANOVA, which largely
depends on second order properties, I think it is reasonable to assume
that you really mean to compare the variances. It's always been a
mystery to me why SPSS prefers the Levene test, which tests whether
the mean absolute deviation is identical, which is a pretty obviously
not the same thing, unless you assume something like the distributions
being scaled versions of eachother.

The Tukey procedure that you outline below would seem to have
something of the same issue: If two distributions have the same
variance but different kurtosis, you'll get the heavy-tailed one
occurring before the light-tailed one in that scheme, then a region
where the light-tailed distribution dominates and finally a region
where the heavy tailed distribution dominates again (think of a
uniform distribution and a normal distribution with the same
variance). It is hard to tell whether the M-W test is biased one way
or the other, but it probably will not have a M-W distribution.

Notice also, btw, that R has several dispersion tests in standard
package "stats", including fligner.test() and ansari.test().
 
> It is possible to consider distribution-free approaches to
> this mind of question.
> 
> One of Tukey's sneakiest inventions was the application of
> the Mann-Whitney test (usually seen as a test of identity
> of distribution against location-shift types of alternative,
> more accurately against alternatives like "P(X<u) > P(Y<u)")
> to test similarity of dispersion.
> 
> The trick: given X1 , ... , Xm and Y1 , ... , Yn, pool them
> and sort the result as Z1 < Z2 < ... < ZN where N = m + n.
> 
> Now take the Z's in the order
> 
>   Z[1] , Z[N] , Z[2] , Z[N-1] , Z[3] , Z[N-2] , ....
> 
> i.e. work inwards from the ends, alternately from each end.
> 
> Note, as you proceed, whether each Z is an X or a Y.
> You thus get a sequence of Xs and Ys. Then sum the number
> of pairs (X,Y) in this sequence where the X occurs earlier
> than the Y.
> 
> This sum, under the null hypothesis of identity of distribution,
> has the Mann-Whitney distribution (just like its usual version),
> and it is sensitive to differences of dispersion (e.g. if the
> distribution of X is more dispersed than the distribution of Y,
> then the Xs will be found earlier in the sequence since they
> lie further out than the Ys and so will be counted in first
> by the above method).
> 
> No doubt, just as there are distribution-free extensions of
> procedures like Mann-Whitney to several samples ("nonparametric
> ANOVA"), so such a procedure could be applied to test equality
> of "dispersions" for several samples, and no doubt it has been
> done.
> 
> However, I've not made use of such a procedure myself, so I
> have to leave it to others to report details.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ross at biostat.ucsf.edu  Mon Dec 20 01:43:09 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Sun, 19 Dec 2004 16:43:09 -0800
Subject: [R] Re: main() in libR? only in 1.9
In-Reply-To: <20041215200515.GL20200@wheat.boylan.org>
References: <20041215200515.GL20200@wheat.boylan.org>
Message-ID: <1103503389.2091.21.camel@iron.libaux.ucsf.edu>

On Wed, 2004-12-15 at 12:05, Ross Boylan wrote:
> libR seems to include a main() function.  Should it?
I think I've tracked this down, and it seems to be specific to R 1.9. 
The 2.0 libR does not include main.

I believe main is present in 1.9 because the link line for libR includes
a reference to "../unix/*.lo".  One of those file is the object for
system.c, which includes main.

On reviewing "Writing R Extensions" for 1.9.1, I note there is no
reference in it to an independent programming calling libR.

I saw this with 1.9.1 on Debian and Mac OS X.



From tiago17 at socrates.Berkeley.EDU  Mon Dec 20 02:04:29 2004
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Mon, 20 Dec 2004 01:04:29 +0000
Subject: [R] muliple plots with pairs (matrix of scatter plots)
Message-ID: <p06100501bdebd4eff241@[83.132.29.100]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041220/7ca4ff4c/attachment.pl

From loesljrg at verizon.net  Mon Dec 20 02:34:12 2004
From: loesljrg at verizon.net (JRG)
Date: Sun, 19 Dec 2004 20:34:12 -0500
Subject: [R] Homogeneity of variance tests between more than 2 sample
In-Reply-To: <x27jndltg7.fsf@biostat.ku.dk>
References: <XFMail.041219214609.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41C5E5C4.7499.244A7D8@localhost>

On 20 Dec 2004 at 1:11, Peter Dalgaard wrote:

> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
> 
> > For non-normal data, there's something of a question as to
> > what is meant (or, perhaps more accurately, what is intended
> > to be meant) by homogeneity of variance, as a test preliminary
> > to an analysis of variance.
> 
> Yes... If you use the test as a preliminary to an ANOVA, which largely
> depends on second order properties, I think it is reasonable to assume
> that you really mean to compare the variances. It's always been a
> mystery to me why SPSS prefers the Levene test, which tests whether
> the mean absolute deviation is identical, which is a pretty obviously
> not the same thing, unless you assume something like the distributions
> being scaled versions of eachother.

I don't use SPSS, but "Levene's test" is often taken to mean "an ANOVA on absolute deviations from cell medians".  Carroll & 
Schneider (1985) show that such a test has the correct asymptotic level, and that it works pretty well as a test of equality of 
scale for non-normal distributions.  They also show that the same test but with residuals taken from cell means rather than medians 
has the correct asymptotic level only if the populations are symmetric.  I don't know which version SPSS has implemented.

Carroll, R. J., & Schneider, H. (1985).  A note on Levene's tests for equality of variances.  Statistics & Probability Letters, 3, 
191--194.

---JRG


John R. Gleason

Syracuse University
430 Huntington Hall                      Voice:   315-443-3107
Syracuse, NY 13244-2340  USA             FAX:     315-443-4085

PGP public key at keyservers



From bates at stat.wisc.edu  Mon Dec 20 03:06:16 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 19 Dec 2004 20:06:16 -0600
Subject: [R] PBIB datataset
In-Reply-To: <XFMail.041219211605.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041219211605.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41C63398.70101@stat.wisc.edu>

(Ted Harding) wrote:
> Thanks, Austin, I think that probably clears it up (see below).
> 
> On 19-Dec-04 Austin, Matt wrote:
> 
>>Is it the PBIB dataset in the SASmixed package?  I don't have
>>my copy of the text at home.
>>
>>--Matt
>>
>>
>>>library(SASmixed)
>>
>>Loading required package: lme4 
>>
>>Attaching package 'lme4':
>>
>>        The following object(s) are masked from package:nlme :
>>
>>         Alfalfa Assay bdf BodyWeight Cefamandole Dialyzer
>>Earthquake ergoStool Fatigue Gasoline getCovariateFormula
>>getResponseFormula Glucose Glucose2 Gun IGF lmeControl Machines
>>MathAchieve MathAchSchool Meat Milk Muscle Nitrendipene Oats
>>Orthodont Ovary Oxboys Oxide PBG Phenobarb Pixel Quinidine Rail
>>RatPupWeight Relaxin Remifentanil Soybean Spruce Tetracycline1
>>Tetracycline2 Wafer Wheat Wheat2 
> 
> 
> The dataset names in the above masked objects are the entire
> list of datasets in P&B except for CO2, ChickWeight, DNase,
> Indometh, Loblolly, Orange and Theoph, and also PBIB. All of
> these except PBIB can be found elsewhere, but (as you show
> below) PBIB can be found in SASmixed and so completes the party.
> 
> However, SASmixed itself cannot be found the the R Full Reference
> Manual (of 25 Nov 2004) either ... the dates on the current
> versions of lme3 and SASmixed are 2004-12-16 and 2004-12-15
> respectively. Now that I try it (today), the "R Site Search" of
> Jonathan Baron does bring it up.
> 
> Thanks for helping to clarify this!
> Ted.

This is a new version of SASmixed that was uploaded a couple of days 
ago.  I changed it so that the fits are done with the lme4 version of 
lme.  It should be faster and more reliable than the version of lme in 
the nlme package.

This version of SASmixed has a vignette that provides comparative 
analyses in lme for the examples in "SAS System for Mixed Models".  The 
specification of models in the new lme is occasionally different from 
the older specification.  Don't pay too much attention to the textual 
descriptions - look at the examples in the appendices.  I haven't 
finished rewriting the textual description from an old, old version.

Those who (like me) cringe at the way that models with crossed random 
effects needed to be specified in the old lme may find it interesting 
that the Demand example now specifies the model fit as
Demand> fm1Demand <- lme(log(d) ~ log(y) + log(rd) + log(rt) +
     log(rs), data = Demand, random = list(State = ~1, Year = ~1))

Demand> summary(fm1Demand)
Linear mixed-effects model fit by REML
Fixed: log(d) ~ log(y) + log(rd) + log(rt) + log(rs)
  Data: Demand
        AIC       BIC   logLik
  -224.1653 -205.4148 120.0826

Random effects:
  Groups   Name        Variance   Std.Dev.
  Year     (Intercept) 0.00026465 0.016268
  State    (Intercept) 0.02948900 0.171724
  Residual             0.00111705 0.033422
# of obs: 77, groups: Year, 11; State, 7

Fixed effects:
              Estimate Std. Error DF t value  Pr(>|t|)
(Intercept) -1.284043   0.723423 72 -1.7750  0.080132 .
log(y)       1.069806   0.103925 72 10.2941 8.553e-16 ***
log(rd)     -0.295342   0.052463 72 -5.6296 3.265e-07 ***
log(rt)      0.039882   0.027889 72  1.4300  0.157034
log(rs)     -0.326739   0.114385 72 -2.8565  0.005595 **
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Correlation of Fixed Effects:
         (Intr) log(y) lg(rd) lg(rt)
log(y)  -0.976
log(rd)  0.383 -0.227
log(rt)  0.077 -0.062 -0.337
log(rs)  0.444 -0.600 -0.270 -0.323


The lme4 and nlme packages should not be loaded simultaneously.  Use one 
or the other but not both.



From MDavy at hortresearch.co.nz  Mon Dec 20 06:14:39 2004
From: MDavy at hortresearch.co.nz (Marcus Davy)
Date: Mon, 20 Dec 2004 18:14:39 +1300
Subject: [R] Re: [BioC] limma, FDR, and p.adjust
Message-ID: <s1c716bd.037@hra2.marc.hort.cri.nz>


Mark,
there is a fdr website link via Yoav Benjamini's homepage which is: http://www.math.tau.ac.il/%7Eroee/index.htm
On it you can download an S-Plus function (under the downloads link) which calculates the false discovery rate threshold alpha level using stepup, stepdown, dependence methods etc. 
Some changes are required to the plotting code when porting it to R. I removed the xaxs="s" arguement on line 80. The fdr function requires a list of p-values as input, a Q-value (*expected* false discovery rate control at level Q) and a required method of fdr controlling procedure.

> As you can see after running the code, the p values are truly being
> adjusted, but for what FDR? If I set my p value at 0.05, does that mean
> my FDR is 5%? I have been told by someone that is the case but,
> normally, when discussing FDR, q values are reported or just one p value
> is reported--the threshold for a set FDR. The p.adjust documentation is
> unclear.

The p.adjust function appears to be using the "stepup" fdr controlling procedure when method="fdr" is specified. It adjusts the 
p-values so that FDR control is at the desired level alpha over the entire range (0,1), which gives the same result as specifying a 
Q-value in the fdr function itself calculating a false discovery rate threshold alpha level so that FDR<=Q.

So it adjusts for all FDR desired levels. If your p-value threhold is 0.05 then the expected proportion of false discoveries is 5%.

e.g.

n   <- 1000
pi0 <- 0.5
x <- rnorm(n, mean=c(rep(0, each=n*pi0), rep(3, each=n - (n*pi0))))
p <- 2*pnorm( -abs(x))
p <- sort(round(p,3))

p.adjusted <- p.adjust(p, method="fdr")

# Controlling fdr at Q, and p.adjust at level alpha
Qvalue <- alpha <- 0.05
  
threshold <- fdr(p, Q=Qvalue, method="stepup")     # fdr function available from the website link above
threshold

plot(p, p.adjusted)
abline(v=threshold, lty=2)
abline(h=alpha, lty=2)

> # Stepup FDR control at Q=0.05
> sum(p <= threshold)
[1] 372
> 
> # p.adjust(ed) p-values at level alpha=0.05
> sum(p.adjusted <= alpha)
[1] 372

Simultaneously modifying Qvalue, and alpha above to a different expected proportion of false discoveries should still produce identically sized rejected lists.

Hope that helps.

marcus



>>> "Kimpel, Mark W" <mkimpel at iupui.edu> 20/12/2004 3:57:43 AM >>>
I am posting this to both R and BioC communities because I believe there
is a lot of confusion on this topic in both communities (having searched
the mail archives of both) and I am hoping that someone will have
information that can be shared with both communities.

I have seen countless questions on the BioC list regarding limma
(Bioconductor) and its calculation of FDR. Some of them involved
misunderstandings or confusions regarding across which tests the FDR
"correction" is being applied. My question is more fundamental and
involves how the FDR method is implemented at the level of "p.adjust"
(package: stats).

I have reread the paper by Benjamini and Hochberg (1995) and nowhere in
their paper do they actually "adjust" p values; rather, they develop
criteria by which an appropriate p value maximum is chosen such that FDR
is expected to be below a certain threshold. 

To try to get a better handle on this, I wrote the following simple
script to generate a list of random p values, and view it before and
after apply p.adjust (method=fdr). 
 
rn<-abs(rnorm(100, 0.5, 0.33))
rn<-rn[order(rn)]
rn<-rn[1:80]
rn
p.adj<-p.adjust(rn, method="fdr")
p.adj

As you can see after running the code, the p values are truly being
adjusted, but for what FDR? If I set my p value at 0.05, does that mean
my FDR is 5%? I have been told by someone that is the case but,
normally, when discussing FDR, q values are reported or just one p value
is reported--the threshold for a set FDR. The p.adjust documentation is
unclear.

For the R developers, I can understand how one would want to include FDR
procedures in p.adjust, but I wonder, given the numerous FDR algorithms
now available, if it would be best to formulate an FDR.select function
that would be option to p.adjust and itself incorporate more recent FDR
procedures than the one proposed by Benjamini and Hochberg in 1995.
(Benjamini himself has a newer one). Some of these may currently be
available as add-on packages but they are not standardized regarding I&O
and this makes it difficult for developers to incorporate them into
packages such as limma.

So those are my questions and suggestions, 

Thanks,

Mark W. Kimpel MD

_______________________________________________
Bioconductor mailing list
Bioconductor at stat.math.ethz.ch 
https://stat.ethz.ch/mailman/listinfo/bioconductor 

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From bernd.weiss at uni-koeln.de  Mon Dec 20 07:11:31 2004
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Mon, 20 Dec 2004 07:11:31 +0100
Subject: [R] Sweave and LaTeX beamer class
Message-ID: <41C67B23.5857.229E6D@localhost>

Hi,

has anyonne experienced problems between the LaTeX beamer class and 
Sweave? The following code does not work properly:
#################################
\documentclass{beamer}

\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ngerman}

\begin{document}
\frame{
  \frametitle{test}
test
<<>>=
 1+1
@ 
}
\end{document}
#################################

Below is the error code:
#################################
loading : Context Support Macros / PDF (2004.10.26)
) (d:\programme\texmf\tex\latex\hyperref\nameref.sty) (testset.out)
(testset.out) (testset.nav) 
(d:\programme\texmf\tex\latex\ae\t1aett.fd)
Runaway argument?
> 1 + 1 \end {Sinput} \begin {Soutput} [1] 2 \end {Soutput} \end 
{Sch\ETC.
! Paragraph ended before \FV at BeginScanning was complete.
<to be read again>
                   \par
l.26 }

? x
No pages of output.
Transcript written on testset.log.
#################################

Thanks in advance,

Bernd



From blindglobe at gmail.com  Mon Dec 20 08:22:35 2004
From: blindglobe at gmail.com (A.J. Rossini)
Date: Mon, 20 Dec 2004 08:22:35 +0100
Subject: [R] SAS or R software
In-Reply-To: <41C42C50.5080503@statisticon.se>
References: <s1c31367.099@gwise.louisville.edu>
	<1103329168.7851.35.camel@horizons.localdomain>
	<41C42C50.5080503@statisticon.se>
Message-ID: <1abe3fa904121923225600070c@mail.gmail.com>

All good points; in my current organization there seem to be 3 hurdles
that need to be crossed.  Most are internal issues, but all related to
conservative interpretation of Part 11.

1. Qualifications:  installation, operational, and performance.  R
clearly satisfies the first and third, the second perhaps needs
someone in R core or similar (i.e. consultant, etc)  needs to provide
the OQ.

2. Statistical results as derived variables (i.e. data).  If so, then
Part 11 can apply, if not it might not.

3.  Removing the "Open Source" moniker (which gets legal people really
upset) and treating R as quality vendor-supplied code under a novel
licensing scheme which has source available and for which a business
case can be made.  Back in the old days (i.e. when I was in high
school in the 80s), our school mini computers had source for the OSs
available, and for most critical vendor or contractor suppiled
software, we had source.  In fact, it was standard!

Anyway, I'm slowly working on these issues internally.  At somepoint,
there will be a breakthrough at one pharma, making it easier for the
rest.  Right now my issue is how to deal with Clinical QA, the
equivalent group is a nightmare bureaucracy to work through, I'm sure,
at most large pharmas.

best,
-toniy



On Sat, 18 Dec 2004 14:10:40 +0100, Henric Nilsson
<henric.nilsson at statisticon.se> wrote:
> Marc Schwartz said the following on 2004-12-18 01:19:
> 
> > As you are likely aware, other statistically relevant issues are
> > contained in various ICH guidance documents regarding GCP considerations
> > and principles for clinical trials:
> >
> > http://www.ich.org/UrlGrpServer.jser?@_ID=475&@_TEMPLATE=272
> 
> ICH E9 states that (p. 27):
> "The computer software used for data management and statistical analysis
> should be reliable, and documentation of appropriate software testing
> procedures should be available."
> 
> Some commercial software vendors (SAS, Insightful, and StatSoft) offer
> white papers stating that their software can work within an 21 CFR Part
> 11 compliant system.
> 
> http://www.sas.com/industry/pharma/develop/papers.html
> 
> http://www.insightful.com/industry/pharm/21cfr_part11_Final.pdf
> 
> http://www.statsoft.com/support/whitepapers/pdf/STATISTICA_CFR.pdf
> 
> Some commercial vendors (SAS and Insightful) also offers tools for
> validation of the installation and operation of the software. SAS has
> 
> http://support.sas.com/documentation/installcenter/common/91/ts1m3/qualification_tools_guide.pdf
> 
> and S-PLUS has validate().
> 
> As a statistical consultant working within the pharamceutical industry,
> I think that our clients find the white papers being some kind of
> quality seal. It signals that someone has actually thought about the
> issues involved, written a document about it, and even stated that it
> can be done. Of course, there's a lot of FUD going on here. But if our
> lives can be made simpler by producing similar white papers and QA
> tools, why not?
> 
> (But for some people, only SAS will do:
> Last week we were audited on behalf of a client. One of the specific
> issues discussed were validation and the Part 11 compliance of S-PLUS.
> In this specific trial, data are to be transferred from Oracle Clinical
> -> SAS -> SPLUS, and they auditors were really worried about the first
> and last link of that chain. Finally, they suggested using only SAS...
> And in this particular case, Part 11 is really a non-issue since
> physical records exists (i.e. case report forms) and all final S-PLUS
> output and code will also be stored physically (i.e. print-outs) -- no
> need for electronic signatures here!)
> 
> > There is also a general guidance document for computer systems used in
> > clinical trials here:
> >
> > http://www.fda.gov/ora/compliance_ref/bimo/ffinalcct.htm
> >
> > Though it is to be superseded by a draft document here:
> >
> > http://www.fda.gov/cder/guidance/6032dft.htm
> 
> From the introduction (p. 2):
> "This document provides guidance about computerized systems that are
> used to create, modify, maintain, archive, retrieve, or transmit
> clinical data required to be maintained and/or submitted to the Food and
> Drug Administration (FDA)"
> 
> The `retrieve' part is certainly applicable. If we regard R as
> off-the-shelf software, the guidance says (p. 11):
> "For most off-the-shelf software, the design level validation will have
> already been done by the company that wrote the software. Given the
> importance of ensuring valid clinical trial data, FDA suggests that the
> sponsor or contract research organization (CRO) have documentation
> (either original validation documents or on-site vendor audit documents)
> of this design level validation by the vendor and would itself have
> performed functional testing (e.g., by use of test data sets) and
> researched known software limitations, problems, and defect corrections.
> Detailed documentation of any additional validation efforts performed by
> the sponsor or CRO will preserve the findings of these efforts.
> 
> In the special case of database and spreadsheet software that is: (1)
> purchased off-the-shelf, (2) designed for and widely used for general
> purposes, (3) unmodified, and (4) not being used for direct entry of
> data, the sponsor or contract research organization may not have
> documentation of design level validation. FDA suggests that the sponsor
> or contract research organization perform functional testing (e.g., by
> use of test data sets) and research known software limitations,
> problems, and defect corrections.
> 
> In the case of off-the-shelf software, we recommend that the following
> be available to the Agency on request:
> 
> * A written design specification that describes what the software is
> intended to do and how it is intended to do it;
> 
> * A written test plan based on the design specification, including both
> structural and functional analysis; and
> 
> * Test results and an evaluation of how these results demonstrate that
> the predetermined design specification has been met."
> 
> I think the guidance is quite clear here. We must prove to the FDA, at
> their wish, that the software used is working properly. In order to do
> this, we seem to need documents describing the development process and
> the QA tools used by R Core. An idea of what we'll need may be found in
> the `Computer Systems Validation in Clinical Research - A Practical
> Guide (Edition 1)' at
> 
> http://www.acdm.org.uk/public/publications/publications.htm
> 
> Especially section 2.4, 5 + subsections, 8 + subsections, and 9.7 +
> subsections seem relevant. (I've ordered the 2nd edition, but it hasn't
> arrived yet.)
> 
> 
> Henric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 

best,
-tony

---
A.J. Rossini
blindglobe at gmail.com



From david.whiting at ncl.ac.uk  Mon Dec 20 08:35:23 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 20 Dec 2004 07:35:23 +0000
Subject: [R] Sweave and LaTeX beamer class
In-Reply-To: <41C67B23.5857.229E6D@localhost>
References: <41C67B23.5857.229E6D@localhost>
Message-ID: <m2d5x51kxw.fsf@192.168.57.36>


Hi Bernd,

I think it is because you are trying to place a chunk of R code within
a LaTeX command. I'm not sure that Sweave will be able to handle
that. In situations analogous to this I process the R chunk earlier in
my document and create an object that I then access in the LaTeX chunk
using \Sexpr{}.  Re-working your example might be something like this
(untested):

#################################
\documentclass{beamer}

\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ngerman}

\begin{document}

<<>>=
x <- 1+1
@ 

\frame{
  \frametitle{test}
test \Sexpr{x}}
\end{document}
#################################


Dave

"Bernd Weiss" <bernd.weiss at uni-koeln.de> writes:

> Hi,
> 
> has anyonne experienced problems between the LaTeX beamer class and 
> Sweave? The following code does not work properly:
> #################################
> \documentclass{beamer}
> 
> \usepackage[latin1]{inputenc}
> \usepackage[T1]{fontenc}
> \usepackage{ngerman}
> 
> \begin{document}
> \frame{
>   \frametitle{test}
> test
> <<>>=
>  1+1
> @ 
> }
> \end{document}
> #################################
> 
> Below is the error code:
> #################################
> loading : Context Support Macros / PDF (2004.10.26)
> ) (d:\programme\texmf\tex\latex\hyperref\nameref.sty) (testset.out)
> (testset.out) (testset.nav) 
> (d:\programme\texmf\tex\latex\ae\t1aett.fd)
> Runaway argument?
> > 1 + 1 \end {Sinput} \begin {Soutput} [1] 2 \end {Soutput} \end 
> {Sch\ETC.
> ! Paragraph ended before \FV at BeginScanning was complete.
> <to be read again>
>                    \par
> l.26 }
> 
> ? x
> No pages of output.
> Transcript written on testset.log.
> #################################
> 
> Thanks in advance,
> 
> Bernd
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
David Whiting
University of Newcastle upon Tyne, UK



From r-help at stat.math.ethz.ch  Mon Dec 20 09:17:12 2004
From: r-help at stat.math.ethz.ch (Laura)
Date: Mon, 20 Dec 2004 09:17:12 +0100
Subject: [R] Re: a web about Pollas Y Gays?
Message-ID: <337788bba75a8ed8ca3b6e85547f6f05@localhost.localdomain>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041220/62cac7f2/attachment.pl

From bernd.weiss at uni-koeln.de  Mon Dec 20 09:33:23 2004
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Mon, 20 Dec 2004 09:33:23 +0100
Subject: [R] Sweave and LaTeX beamer class -- SOLVED!
In-Reply-To: <41C67B23.5857.229E6D@localhost>
Message-ID: <41C69C63.16029.A4806D@localhost>

On 20 Dec 2004 at 7:11, Bernd Weiss wrote:

> Hi,
> 
> has anyonne experienced problems between the LaTeX beamer class and
> Sweave? The following code does not work properly:
> ################################# \documentclass{beamer}
> 
> \usepackage[latin1]{inputenc}
> \usepackage[T1]{fontenc}
> \usepackage{ngerman}
> 
> \begin{document}
> \frame{
>   \frametitle{test}
> test
> <<>>=
>  1+1
> @ 
> }
> \end{document}
> #################################


The following code works as expected! 

It is important to use a frame-environment and the "fragile"-option.

\begin{frame}[fragile]
...
\end{frame}

##################################

\documentclass{beamer}


\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{ngerman}

\begin{document}

\begin{frame}[fragile]
  \frametitle{test}
test
<<>>=
 1+1
@ 
\end{frame}
\end{document}

####################################



From mkondrin at hppi.troitsk.ru  Mon Dec 20 15:47:41 2004
From: mkondrin at hppi.troitsk.ru (mkondrin@hppi.troitsk.ru)
Date: Mon, 20 Dec 2004 17:47:41 +0300
Subject: [R] Increased execution speed of R2.0.1?
Message-ID: <41C6E60D.8040109@hppi.troitsk.ru>

Hello!
I have just upgraded from the R 1.9.1 to the latest version and 
installed a couple of packages. I as a rule import only base package 
with this couple of packages at startup. I was very impressed with the 
speed R2.0.1 does now run (it seems to me I have 50% gain over 1.9.1 
version). My question: is this due to "lazy-loading" mechanism (it looks 
unlikely to me as IMHO lazy-loading conserves memory usage, not cpu 
cycles) or are packages gots byte-compiled during installation (it is 
more likely taking into account binary *.rdx files in R/library/... 
tree). There used to be an alpha verision of byte-compiler for R - is it 
now included into "official" version of R?
Thank you.



From mkondrin at hppi.troitsk.ru  Mon Dec 20 15:49:55 2004
From: mkondrin at hppi.troitsk.ru (mkondrin@hppi.troitsk.ru)
Date: Mon, 20 Dec 2004 17:49:55 +0300
Subject: [R] Increased execution speed of R2.0.1?
Message-ID: <41C6E693.6030107@hppi.troitsk.ru>

Hello!
I have just upgraded from the R 1.9.1 to the latest version and
installed a couple of packages. I as a rule import only base package
with this couple of packages at startup. I was very impressed with the
speed R2.0.1 does now run (it seems to me I have 50% gain over 1.9.1
version). My question: is this due to "lazy-loading" mechanism (it looks
unlikely to me as IMHO lazy-loading conserves memory usage, not cpu
cycles) or are packages gots byte-compiled during installation (it is
more likely taking into account binary *.rdx files in R/library/...
tree). There used to be an alpha verision of byte-compiler for R - is it
now included into "official" version of R?
Thank you.



From narinder.singh at diagenic.com  Mon Dec 20 11:07:58 2004
From: narinder.singh at diagenic.com (narinder.singh)
Date: Mon, 20 Dec 2004 11:07:58 +0100
Subject: [R] Normalization based on PM values only
In-Reply-To: <s1c716bd.037@hra2.marc.hort.cri.nz>
References: <s1c716bd.037@hra2.marc.hort.cri.nz>
Message-ID: <20041220094326.M53597@webpost.it-as.no>


I have been asked to look some data generated on affy-platform. Due to 
biological reasons the data owners want to look at PM values only, i.e ignore 
the MM values. Hence my question is:

What is the best option available for normalizing affy data using the PM 
intensities only.

I appologize if this question has been asked and answered on this group 
earlier. 

Narinder S. Sahni



From stecalza at tiscali.it  Mon Dec 20 11:10:58 2004
From: stecalza at tiscali.it (stecalza@tiscali.it)
Date: Mon, 20 Dec 2004 11:10:58 +0100
Subject: [R] Sweave and LaTeX beamer class
In-Reply-To: <41C67B23.5857.229E6D@localhost>
Message-ID: <418226DA000CF9AA@mail-2.tiscali.it>

I compiled your code exactly as posted but had no problem at all!! I'm using
Debian-GNU/Linux & Emacs, so possibly this has to do with your editor

Ste

>-- Messaggio Originale --
>From: "Bernd Weiss" <bernd.weiss at uni-koeln.de>
>To: r-help at stat.math.ethz.ch
>Date: Mon, 20 Dec 2004 07:11:31 +0100
>Subject: [R] Sweave and LaTeX beamer class
>
>
>Hi,
>
>has anyonne experienced problems between the LaTeX beamer class and 
>Sweave? The following code does not work properly:
>#################################
>\documentclass{beamer}
>
>\usepackage[latin1]{inputenc}
>\usepackage[T1]{fontenc}
>\usepackage{ngerman}
>
>\begin{document}
>\frame{
>  \frametitle{test}
>test
><<>>=
> 1+1
>@ 
>}
>\end{document}
>#################################
>
>Below is the error code:
>#################################
>loading : Context Support Macros / PDF (2004.10.26)
>) (d:\programme\texmf\tex\latex\hyperref\nameref.sty) (testset.out)
>(testset.out) (testset.nav) 
>(d:\programme\texmf\tex\latex\ae\t1aett.fd)
>Runaway argument?
>> 1 + 1 \end {Sinput} \begin {Soutput} [1] 2 \end {Soutput} \end 
>{Sch\ETC.
>! Paragraph ended before \FV at BeginScanning was complete.
><to be read again>
>                   \par
>l.26 }
>
>? x
>No pages of output.
>Transcript written on testset.log.
>#################################
>
>Thanks in advance,
>
>Bernd
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


__________________________________________________________________
Tiscali Adsl 2 Mega Free: l'adsl piu' veloce e' gratis!
Naviga libero dai costi fissi con Tiscali Adsl 2 Mega Free, l'adsl Free
piu' veloce in Italia. In piu', se ti abboni entro il 7 gennaio 2005,
navighi gratis fino al 31 marzo. E il costo di adesione e' GRATIS.
http://abbonati.tiscali.it/adsl/



From R-user at zutt.org  Mon Dec 20 11:47:40 2004
From: R-user at zutt.org (R user)
Date: Mon, 20 Dec 2004 11:47:40 +0100
Subject: [R] evaluate expression on several dataframe columns
Message-ID: <1103539660.19543.29.camel@dutiih.st.ewi.tudelft.nl>

Hi R-users,

I have a collection of dataframes and know how to build
a string that refers to it, in this example, name_infra_alg_inc.
Then, I have a character string yval, which the user can select
from a drop down list. It contains the column names of the
dataframes.

assign(paste(name_infra_alg_inc, "ci", sep="."),
  ci(get(name_infra_alg_inc)[[yval]], confidence=0.95))

My problem is that I sometimes want to combine columns.
For example, if there are columns A, B and C.
Would it be possible that yval has the value "A+B*C" and
then call some sort of evaluate function?
Maybe I could attach the dataframe and then call some function,
I don't know how to figure this out, so hopefully someone can help me.

Thanks in advance



From ligges at statistik.uni-dortmund.de  Mon Dec 20 12:17:53 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 20 Dec 2004 12:17:53 +0100
Subject: [R] evaluate expression on several dataframe columns
In-Reply-To: <1103539660.19543.29.camel@dutiih.st.ewi.tudelft.nl>
References: <1103539660.19543.29.camel@dutiih.st.ewi.tudelft.nl>
Message-ID: <41C6B4E1.4080404@statistik.uni-dortmund.de>

R user wrote:

> Hi R-users,
> 
> I have a collection of dataframes and know how to build
> a string that refers to it, in this example, name_infra_alg_inc.
> Then, I have a character string yval, which the user can select
> from a drop down list. It contains the column names of the
> dataframes.
> 
> assign(paste(name_infra_alg_inc, "ci", sep="."),
>   ci(get(name_infra_alg_inc)[[yval]], confidence=0.95))
> 
> My problem is that I sometimes want to combine columns.
> For example, if there are columns A, B and C.
> Would it be possible that yval has the value "A+B*C" and
> then call some sort of evaluate function?
> Maybe I could attach the dataframe and then call some function,
> I don't know how to figure this out, so hopefully someone can help me.
> 
> Thanks in advance


Anonymous R user,

I really don't know why are you going to do so, but you can do:

A <- 1
B <- 2
C <- 3
x <- expression(A+B*C)
eval(x)

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From paul.hewson at plymouth.ac.uk  Mon Dec 20 12:17:31 2004
From: paul.hewson at plymouth.ac.uk (Paul Hewson)
Date: Mon, 20 Dec 2004 11:17:31 -0000
Subject: [R] Producing "Editable" Graphs for PowerPoint
Message-ID: <52A8091888A23F47A013223014B6E9FE64BE83@03-CSEXCH.uopnet.plymouth.ac.uk>

Hello,

(apologies, I'm not entirely sure whether this question is about R or my
limitations with PowerPoint).   I've submitted a paper (which has been
accepted) but the journal now require me to submit graphs that are
"editable in PowerPoint".   I would be grateful for suggestions as to
how I should do this.

The best route seems to be to copy-and-paste the figures from the
windows() device as a metafile.   However, on converting the graphs to
an editable format and "ungrouping" them, it appears that every line,
tickmark, dot and dash is treated as a separate entity by PowerPoint
(for example even the horizontal and vertical parts of "+" are
separated).   Alternatively, reading in the files from a saved metafile
is even more problematic: having used par(new = TRUE) I have a set of
layered graphs and all but the last one dissapear once I open the
graphic for editing.

Is this normal for a figure "editable by PowerPoint" or am I doing
something horribly wrong somewhere?    

Many thanks

Paul




-=-=-=-=-=-=-=-=-=-=-=-=
Paul Hewson
Lecturer in Statistics
School of Mathematics and Statistics
University of Plymouth
Drake Circus
Plymouth PL4 8AA

tel (01752) 232778
fax (01752) 232780

email paul.hewson at plymouth.ac.uk
www.plymouth.ac.uk



From ligges at statistik.uni-dortmund.de  Mon Dec 20 12:50:26 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 20 Dec 2004 12:50:26 +0100
Subject: [R] Increased execution speed of R2.0.1?
In-Reply-To: <41C6E693.6030107@hppi.troitsk.ru>
References: <41C6E693.6030107@hppi.troitsk.ru>
Message-ID: <41C6BC82.1080800@statistik.uni-dortmund.de>

mkondrin at hppi.troitsk.ru wrote:

> Hello!
> I have just upgraded from the R 1.9.1 to the latest version and
> installed a couple of packages. I as a rule import only base package
> with this couple of packages at startup. I was very impressed with the
> speed R2.0.1 does now run (it seems to me I have 50% gain over 1.9.1
> version). My question: is this due to "lazy-loading" mechanism (it looks

Yes.

> unlikely to me as IMHO lazy-loading conserves memory usage, not cpu
> cycles) or are packages gots byte-compiled during installation (it is
> more likely taking into account binary *.rdx files in R/library/...
> tree). There used to be an alpha verision of byte-compiler for R - is it
> now included into "official" version of R?

No.

Uwe Ligges

> Thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Dec 20 12:57:16 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 20 Dec 2004 12:57:16 +0100
Subject: [R] muliple plots with pairs (matrix of scatter plots)
In-Reply-To: <p06100501bdebd4eff241@[83.132.29.100]>
References: <p06100501bdebd4eff241@[83.132.29.100]>
Message-ID: <41C6BE1C.7060603@statistik.uni-dortmund.de>

Tiago R Magalhaes wrote:

> I am trying to make a graph with 4 scatter matrixes plots and 
> couldn't do it. While trying to find a solution for this I also came 
> across the idea of giving different values to the same argument for 
> each of the lower and upper function but couldn't do it. (Examplified 
> below with the col argument). The first problem of plotting 4 scatter 
> matrixes in a graph is a problem of real interest for me at this 
> point. The second problem is a matter of curiosity.
> 
> I am using a Mac PowerBook G4 with OS 10.3.7 and R 2.0.1
> 
> 
> Problem 1)
> x=data.frame(a=sample(1:100, 50), b=sample(1:100, 50),c=sample(1:100, 
> 50),d=sample(1:100, 50))
> x.list=vector('list',4)
> for (j in 1:4) x.list[[j]]=x
> 
> #produces a graph with four plots:
> layout(matrix(c(1,3,2,4),2,2))
> for (j in seq(x)){
> 	plot(x.list[[j]][1:2])
> 	}
> 
> # But unfortunately the following produces a new plot everytime:
> layout(matrix(c(1,3,2,4),2,2))
> for (j in seq(x)){
> 	pairs(x.list[[j]])
> 	}
> #Maybe pairs can't be used to produce a graph with multiple plots?

Yes, it uses similar constructs to put multiple plots together.

You might want to use packages grid and gridBase to set something up 
using viewports.


> Problem 2)
> I wanted to plot upper and lower panels with different colours. I 
> tried what I thougth was obvious, giving the col argument inside of 
> each function:
> pairs (x, lower.panel=points(x, col=2), upper.panel=points(x, 
> col=3))# only the diagonal is plotted
> pairs (x, lower.panel=points(x), upper.panel=points(x))  #once again 
> only the diagonal is plotted
> pairs (x, lower.panel=points, upper.panel=points)  #both panels are 
> plotted, actually, this is simlar to pairs(x)
> 
> #Another little observation that I thought was strange:
> pairs(x, lower.panel=points(x), col=2)# colours the outside box red
> pairs(x, lower.panel=points(x), col=2, xaxt='n', yaxt='n')# does what I want
> pairs(x, lower.panel=points(x, col=2))# doesn't change the colour but 
> works otherwise


You have to specify a function, e.g. an unnamed one such as
    function(x, y) points(x, y, col=2)
rather than a function call such as just
    points(x)

Hence we get:

pairs(x, lower.panel = function(x, y) points(x, y, col=2),
          upper.panel = function(x, y) points(x, y, col=3))



Uwe Ligges


> Thank you very much for any comments and help
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Mon Dec 20 13:26:09 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 20 Dec 2004 07:26:09 -0500
Subject: [R] Producing "Editable" Graphs for PowerPoint
In-Reply-To: <52A8091888A23F47A013223014B6E9FE64BE83@03-CSEXCH.uopnet.plymouth.ac.uk>
References: <52A8091888A23F47A013223014B6E9FE64BE83@03-CSEXCH.uopnet.plymouth.ac.uk>
Message-ID: <41C6C4E1.2050205@vanderbilt.edu>

Paul Hewson wrote:
> Hello,
> 
> (apologies, I'm not entirely sure whether this question is about R or my
> limitations with PowerPoint).   I've submitted a paper (which has been
> accepted) but the journal now require me to submit graphs that are
> "editable in PowerPoint".   I would be grateful for suggestions as to
> how I should do this.
> 
> The best route seems to be to copy-and-paste the figures from the
> windows() device as a metafile.   However, on converting the graphs to
> an editable format and "ungrouping" them, it appears that every line,
> tickmark, dot and dash is treated as a separate entity by PowerPoint
> (for example even the horizontal and vertical parts of "+" are
> separated).   Alternatively, reading in the files from a saved metafile
> is even more problematic: having used par(new = TRUE) I have a set of
> layered graphs and all but the last one dissapear once I open the
> graphic for editing.
> 
> Is this normal for a figure "editable by PowerPoint" or am I doing
> something horribly wrong somewhere?    
> 
> Many thanks
> 
> Paul
> 
> 
> 
> 
> -=-=-=-=-=-=-=-=-=-=-=-=
> Paul Hewson
> Lecturer in Statistics
> School of Mathematics and Statistics
> University of Plymouth
> Drake Circus
> Plymouth PL4 8AA
> 
> tel (01752) 232778
> fax (01752) 232780
> 
> email paul.hewson at plymouth.ac.uk
> www.plymouth.ac.uk

I had hoped that journals engaged in reproducible research by now.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From michael.watson at bbsrc.ac.uk  Mon Dec 20 14:46:55 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 20 Dec 2004 13:46:55 -0000
Subject: [R] Memory problem with jpeg() and wide jpegs
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89A55@iahce2knas1.iah.bbsrc.reserved>

Hi

I have been creating very, very long jpeg images for the last two weeks
using jpeg().  All of a sudden, and I mean that, it's stopped working -
I've not changed a thing!  The error message I get is:

> jpeg("out.jpg",width=50000,height=480, quality=100)
Error in devga(paste("jpeg:", quality, ":", filename, sep = ""), width,
: 
        unable to start device devga
In addition: Warning message: 
Unable to allocate bitmap 

I have plenty of disk space in the place I want to create the jpeg, and
if I reduce it from "width=50000" to "width=5000", then it works no
problem, which suggests it is a memory problem.  If I close R and
re-open R the problem does not go away.  And as I said above, I have
been using this code for weeks with no problem, then I change nothing,
and start getting this error message.

I'm running R 2.0.1 on Windows XP.  I'm going to restart Windows and see
if that helps, but if re-starting windows does help, is there an
explanation as to where all the memory R used to be able to get at
disappeared to in the current session?

Cheers
Mick



From Mike.Prager at noaa.gov  Mon Dec 20 15:05:49 2004
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Mon, 20 Dec 2004 09:05:49 -0500
Subject: [R] Producing "Editable" Graphs for PowerPoint
In-Reply-To: <52A8091888A23F47A013223014B6E9FE64BE83@03-CSEXCH.uopnet.plymouth.ac.uk>
References: <52A8091888A23F47A013223014B6E9FE64BE83@03-CSEXCH.uopnet.plymouth.ac.uk>
Message-ID: <41C6DC3D.6000400@noaa.gov>

Paul--

It is hard to fight the tide, even when it contains sewage, but (you 
probably agree) this is an idiotic requirement.  Not only does this 
journal want to change your work (which journals do much too much), they 
also want you to help them use of low-class tools to do so.

You could try asking the journal to tell you what changes they feel are 
needed & promising to make them yourself.  That's what I would do, but 
there's no guarantee it will succeed.

An alternative would be to make EPS files from R and send them to them. 
The XP version (and presumably above) of MSPP can import and edit EPS 
files. If the journal doesn't like the *way* it edits them, well, what 
can you do?  Maybe that will allow you to gently suggest that their 
requirement is all wet.

We need somehow to restore the roles of authors and journals.  Authors 
are supposed to write the stuff, and journals figure how to get it into 
print.  Somewhere along the line, journals have taken on all the 
prerogatives of collaborators.  It seems to me that is a conflict of 
interest.

Good luck!

Regards,
...Mike



Paul Hewson wrote:

>Hello,
>
>(apologies, I'm not entirely sure whether this question is about R or my
>limitations with PowerPoint).   I've submitted a paper (which has been
>accepted) but the journal now require me to submit graphs that are
>"editable in PowerPoint".   I would be grateful for suggestions as to
>how I should do this.
>
>The best route seems to be to copy-and-paste the figures from the
>windows() device as a metafile.   However, on converting the graphs to
>an editable format and "ungrouping" them, it appears that every line,
>tickmark, dot and dash is treated as a separate entity by PowerPoint
>(for example even the horizontal and vertical parts of "+" are
>separated).   Alternatively, reading in the files from a saved metafile
>is even more problematic: having used par(new = TRUE) I have a set of
>layered graphs and all but the last one dissapear once I open the
>graphic for editing.
>
>Is this normal for a figure "editable by PowerPoint" or am I doing
>something horribly wrong somewhere?    
>
>Many thanks
>
>Paul
>
>
>  
>

-- 
Michael H. Prager, Ph.D.
Population Dynamics Team
NOAA Center for Coastal Habitat and Fisheries Research
NMFS Southeast Fisheries Science Center
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/



From ligges at statistik.uni-dortmund.de  Mon Dec 20 15:23:23 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 20 Dec 2004 15:23:23 +0100
Subject: [R] Memory problem with jpeg() and wide jpegs
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A55@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89A55@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41C6E05B.3010107@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> Hi
> 
> I have been creating very, very long jpeg images for the last two weeks
> using jpeg().  All of a sudden, and I mean that, it's stopped working -
> I've not changed a thing!  The error message I get is:
> 
> 
>>jpeg("out.jpg",width=50000,height=480, quality=100)
> 
> Error in devga(paste("jpeg:", quality, ":", filename, sep = ""), width,
> : 
>         unable to start device devga
> In addition: Warning message: 
> Unable to allocate bitmap 
> 
> I have plenty of disk space in the place I want to create the jpeg, and
> if I reduce it from "width=50000" to "width=5000", then it works no
> problem, which suggests it is a memory problem.  If I close R and
> re-open R the problem does not go away.  And as I said above, I have
> been using this code for weeks with no problem, then I change nothing,
> and start getting this error message.

I don't believe it has worked. Such a huge one does not work for me even 
on a rather big machine.
If it had worked: On the same platform, OS, R version?
I think you have to contribute a patch in order to get such a huge jpeg.

Uwe Ligges


> I'm running R 2.0.1 on Windows XP.  I'm going to restart Windows and see
> if that helps, but if re-starting windows does help, is there an
> explanation as to where all the memory R used to be able to get at
> disappeared to in the current session?
 >
> Cheers
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mdeasnds at fs1.ser.man.ac.uk  Mon Dec 20 14:56:48 2004
From: mdeasnds at fs1.ser.man.ac.uk (Neil Shephard)
Date: Mon, 20 Dec 2004 14:56:48 +0100
Subject: [R] Re: limma, FDR, and p.adjust
Message-ID: <41C6E82F.27935.363987@localhost>

In response to Mark Kimpel's query about FDR implementations in R I thought it might 
be pertinent to mention the qvalue package written and maintained by Alan Dabney and 
John Storey and obtainable from http://faculty.washington.edu/~jstorey/qvalue/ (this is 
also mirrored in the packages section of http://cran.r-project.org/).

I don't think it really address' the problems I/O problems originally queried, but thought I 
would mention it as no one else has.

HTH's

Neil

Neil Shephard
Genetics Statistician
ARC Epidemiology Unit, University of Manchester
neil.shephard at man.ac.uk
neil.shephard at mindless.com

"If your result needs a statistician then you should design a better experiment" - 
Ernest Rutherford



From tlumley at u.washington.edu  Mon Dec 20 16:41:37 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 20 Dec 2004 07:41:37 -0800 (PST)
Subject: [R] Homogeneity of variance tests between more than 2 sample
In-Reply-To: <41C5E5C4.7499.244A7D8@localhost>
References: <XFMail.041219214609.Ted.Harding@nessie.mcc.ac.uk>
	<41C5E5C4.7499.244A7D8@localhost>
Message-ID: <Pine.A41.4.61b.0412200736150.158154@homer07.u.washington.edu>

On Sun, 19 Dec 2004, JRG wrote:

> On 20 Dec 2004 at 1:11, Peter Dalgaard wrote:
>>> Yes... If you use the test as a preliminary to an ANOVA, which largely
>> depends on second order properties, I think it is reasonable to assume
>> that you really mean to compare the variances. It's always been a
>> mystery to me why SPSS prefers the Levene test, which tests whether
>> the mean absolute deviation is identical, which is a pretty obviously
>> not the same thing, unless you assume something like the distributions
>> being scaled versions of eachother.
>
> I don't use SPSS, but "Levene's test" is often taken to mean "an ANOVA 
> on absolute deviations from cell medians".  Carroll & Schneider (1985) 
> show that such a test has the correct asymptotic level, and that it 
> works pretty well as a test of equality of scale for non-normal 
> distributions.

Yes, but that is the level under the strong null of equality of 
distributions, not under the null hypothesis that variances are equal (for 
possibly non-equal distributions).  Peter's point (and perhaps Ted's 
earlier point) is that the equal mean absolute deviation from the median 
is not the same as equal variance so a test can't possibly be valid for 
both.


 	-thomas



From das at cshl.edu  Mon Dec 20 17:05:19 2004
From: das at cshl.edu (Rajdeep Das)
Date: Mon, 20 Dec 2004 11:05:19 -0500
Subject: [R] feature selection for SVM
Message-ID: <000e01c4e6ad$b4bef280$6500a8c0@artney>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041220/1625c8a3/attachment.pl

From michael.watson at bbsrc.ac.uk  Mon Dec 20 17:16:49 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 20 Dec 2004 16:16:49 -0000
Subject: [R] Memory problem with jpeg() and wide jpegs
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B8EC@iahce2knas1.iah.bbsrc.reserved>

Uwe, I had to smile at your comments - the code works on my 512Mb laptop....


-----Original Message-----
From:	Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
Sent:	Mon 12/20/2004 2:23 PM
To:	michael watson (IAH-C)
Cc:	R-help at stat.math.ethz.ch
Subject:	Re: [R] Memory problem with jpeg() and wide jpegs
michael watson (IAH-C) wrote:

> Hi
> 
> I have been creating very, very long jpeg images for the last two weeks
> using jpeg().  All of a sudden, and I mean that, it's stopped working -
> I've not changed a thing!  The error message I get is:
> 
> 
>>jpeg("out.jpg",width=50000,height=480, quality=100)
> 
> Error in devga(paste("jpeg:", quality, ":", filename, sep = ""), width,
> : 
>         unable to start device devga
> In addition: Warning message: 
> Unable to allocate bitmap 
> 
> I have plenty of disk space in the place I want to create the jpeg, and
> if I reduce it from "width=50000" to "width=5000", then it works no
> problem, which suggests it is a memory problem.  If I close R and
> re-open R the problem does not go away.  And as I said above, I have
> been using this code for weeks with no problem, then I change nothing,
> and start getting this error message.

I don't believe it has worked. Such a huge one does not work for me even 
on a rather big machine.
If it had worked: On the same platform, OS, R version?
I think you have to contribute a patch in order to get such a huge jpeg.

Uwe Ligges


> I'm running R 2.0.1 on Windows XP.  I'm going to restart Windows and see
> if that helps, but if re-starting windows does help, is there an
> explanation as to where all the memory R used to be able to get at
> disappeared to in the current session?
 >
> Cheers
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From buser at stat.math.ethz.ch  Mon Dec 20 17:20:22 2004
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 20 Dec 2004 17:20:22 +0100
Subject: [R] Friedman test for replicated blocked data
In-Reply-To: <Pine.LNX.4.58.0412131818510.9730@kika.intellektik.informatik.tu-darmstadt.de>
References: <Pine.LNX.4.58.0412131818510.9730@kika.intellektik.informatik.tu-darmstadt.de>
Message-ID: <16838.64454.719879.71785@gargle.gargle.HOWL>

Hi Marco

> I would need to extend the Friedman test to a replicated
> design. Currently the function: friedman.test(y, ...) only works for
> unreplicated designs.

You'll find something about this topic in:
 
Myles Hollander & Douglas A. Wolfe (1999), _Nonparametric
statistical methods_. 2nd edition, John Wiley & Sons. Chapter 7 

In Chapter 7.9 there is an extension to the balanced replicated
design. Notice also the comments 70 and 74 about the
motivation of the test and the approximation of the test
statistic and comment 77 for an extension to the general case
(unbalanced). 
They give further references for more details, too. 

Hope this will help you

Christoph

-- 
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/



From ripley at stats.ox.ac.uk  Mon Dec 20 18:37:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 20 Dec 2004 17:37:23 +0000 (GMT)
Subject: [R] Memory problem with jpeg() and wide jpegs
In-Reply-To: <41C6E05B.3010107@statistik.uni-dortmund.de>
References: <8975119BCD0AC5419D61A9CF1A923E95E89A55@iahce2knas1.iah.bbsrc.reserved>
	<41C6E05B.3010107@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0412201726420.26642@gannet.stats>

It is also several times greater than the limit of human perception, being 
several feet long at printing resolutions that need a magnifying glass 
to see.

This is Windows and the limit is in the graphics card: mine is able to do 
this but I suspect you need a 128Mb card (that jpeg is of itself about 
90Mb).  However, most viewers (including PhotoShop) will barf on such a 
large jpeg.


On Mon, 20 Dec 2004, Uwe Ligges wrote:

> michael watson (IAH-C) wrote:
>
>> Hi
>> 
>> I have been creating very, very long jpeg images for the last two weeks
>> using jpeg().  All of a sudden, and I mean that, it's stopped working -
>> I've not changed a thing!  The error message I get is:
>> 
>> 
>>> jpeg("out.jpg",width=50000,height=480, quality=100)
>> 
>> Error in devga(paste("jpeg:", quality, ":", filename, sep = ""), width,
>> :         unable to start device devga
>> In addition: Warning message: Unable to allocate bitmap 
>> I have plenty of disk space in the place I want to create the jpeg, and
>> if I reduce it from "width=50000" to "width=5000", then it works no
>> problem, which suggests it is a memory problem.  If I close R and
>> re-open R the problem does not go away.  And as I said above, I have
>> been using this code for weeks with no problem, then I change nothing,
>> and start getting this error message.
>
> I don't believe it has worked. Such a huge one does not work for me even on a 
> rather big machine.
> If it had worked: On the same platform, OS, R version?
> I think you have to contribute a patch in order to get such a huge jpeg.
>
> Uwe Ligges
>
>
>> I'm running R 2.0.1 on Windows XP.  I'm going to restart Windows and see
>> if that helps, but if re-starting windows does help, is there an
>> explanation as to where all the memory R used to be able to get at
>> disappeared to in the current session?
>>
>> Cheers
>> Mick
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spratap at assembla.com  Mon Dec 20 18:47:29 2004
From: spratap at assembla.com (Seshasayanan Pratap)
Date: Mon, 20 Dec 2004 12:47:29 -0500
Subject: [R] Interest in commercial support for R,
	R-metrics and related packages
Message-ID: <200412201747.iBKHlhP22013@assembla.apolloservers.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041220/ce95e2c0/attachment.pl

From spencer.graves at pdf.com  Mon Dec 20 19:28:34 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 20 Dec 2004 10:28:34 -0800
Subject: [R] Interest in commercial support for R,	R-metrics and related
	packages
In-Reply-To: <200412201747.iBKHlhP22013@assembla.apolloservers.com>
References: <200412201747.iBKHlhP22013@assembla.apolloservers.com>
Message-ID: <41C719D2.6070302@pdf.com>

      A representative from Spotfire (www.spotfire.com) told me they can 
link to S-Plus and R, and they use those links to produce custom code 
for customers.  By the General Public License (GPL), Spotfire is 
required to offer under the GPL the code for how they link to R, but 
they don't have to make publicly available the code for their core 
product.  Also, I don't think they have to make publicly available the 
code they write for customers as long as they are selling consulting 
services and not software. 

      This may or may not help you, since Spotfire does not focus 
primarily on financial services. 

      spencer graves

Seshasayanan Pratap wrote:

>I am a partner at Assembla, a software services group that helps companies
>make use of open source techniques and software. We have been asked by a
>company in the financial services sector to provide support for their use of
>R, Rmetrics and possibly SciViews. 
>
>I am trying to locate other companies in the financial services industry
>that might be interested in commercial support for R, Rmetrics and/or other
>R packages used for financial analysis. I would like to understand what type
>of services - such as technical support, custom development, versions tuned
>for financial analysis, user conferences/workshops, etc - would be
>interesting to users of R. 
>
>Thank you. 
>
>Sesha Pratap 
>
> 
><http://ps3pub.psteering.com/assembla/discussion/discussion_create.jsp?objec
>tId=fs000080000g1asihvp0000000&parent=fs000080000g1sc50c0g000000> Reply
>
>
> 
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From gunter.berton at gene.com  Mon Dec 20 19:38:12 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 20 Dec 2004 10:38:12 -0800
Subject: [R] Dose of Reality re: SAS vs R
In-Reply-To: <1abe3fa904121923225600070c@mail.gmail.com>
Message-ID: <200412201838.iBKIcChF027113@compton.gene.com>

R folks:

I appreciate and have learned from the recent "SAS vs R" and "Bad Excel
Calculations" threads. Not only civil, but even at times erudite,
discussion. So I apologize for the lateness of this remark and hope it isn't
redundant or trivial.

To those who may wonder why SAS is so dominant in the clinical arena despite
(better) alternatives: INERTIA. That is:

1) There is a huge infrastructure of SAS code already in place for
regulatory submissions and SAS programmers to maintain and enlarge it. As a
practical matter, it is hard to imagine a large organization simply chucking
this and starting afresh. Clearly, change -- if were to occur at all --
would have to be slow and incremental.

2) From my experience at presentations of recent biostatistics PhD's, for
most, their education continues to promulgate the use of SAS in
clinical/regulatory settings, undoubtedly due to 1).

3) As has already been noted, most existing FDA regulators -- statisticians
and clinicians alike -- are familiar with SAS, and therefore submissions
with other software (like R) might delay or complicate the review process.
We statisticians are not the biggest dogs in this arena, after all.

Reality bites! So R users must persevere.

-- Bert Gunter



From br44114 at yahoo.com  Mon Dec 20 19:52:25 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 20 Dec 2004 10:52:25 -0800 (PST)
Subject: [R] faster row by row data frame processing
Message-ID: <20041220185225.44669.qmail@web50303.mail.yahoo.com>

Dear R users,

I have a data frame with a few thousand rows and several hundred
numeric columns (plus a date column). For each row (day), I want to
assign +/- 1 to the highest X absolute values, 0 to the other values,
and save all that in a separate data frame. 

I have a working solution (below), however I find it rather slow. Is
there something I could do to increase the speed? (The code is
CPU-bound; Pentium 4 @ 2.4 GHz, 512 MB RAM, Win XP, R 2.0.0.)

Thank you,
b.


#all is the original data frame (date + a number of columns)
#set up the output data frame
DailyTopN <- data.frame(all[1,1],matrix(ncol=ncol(all)-1))
names(DailyTopN) <- names(all)
top <- 20
for (i in 1:1000)	#the rows to be processed
	{
	#data frame row as vector
	onerow <- na.omit(as.matrix(all[i,][2:ncol(all)])[1, ])
	#select the 'top' highest absolute values
	r <- rank(abs(onerow),ties.method="random")
	selected <- names(r[which(r <= top)])
	#set +/-1 for the highest absolute values, 0 for the others
	DailyTopN[i,selected] <- 1 * sign(all[i,selected])
	DailyTopN[i,1] <- all[i,1]	#add the date
	}
DailyTopN[is.na(DailyTopN)] <- 0
rownames(DailyTopN) <- 1:nrow(DailyTopN)



From jmoreira at fe.up.pt  Mon Dec 20 19:52:30 2004
From: jmoreira at fe.up.pt (=?iso-8859-1?Q?Jo=E3o_Mendes_Moreira?=)
Date: Mon, 20 Dec 2004 18:52:30 -0000
Subject: [R] erro in SVM (packsge "e1071")
References: <20041219142307.00c8336a.david.meyer@wu-wien.ac.at>
Message-ID: <013c01c4e6c5$0f238b70$5e7aa8c0@FEUPsig.fe.up.pt>

The way I call SVM is:

i <- (-2)
j <- 4
learner='svm'
learner.pars=list(Duracao ~ ., data=orig.data,
   scale=c(FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE,
    FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,
    FALSE, FALSE, FALSE),
   type='nu-regression', kernel='linear',
   cost=2^(2*i), nu=j/10)
learner.pars$data <- orig.data[begin.test.pos:(test.pos-1),]

model <- do.call(learner,learner.pars)

The variables begin.test.pos and test.pos are windexes for orig.data and are 
working well. in this case begin.test.pos = 1 and test.pos = 875.

orig.data is a data.frame where the second, third and seventh parameters are 
numeric. The first parameter  is a date and all the others are factors (some 
of them ordered). The ordered factors are: Dia Semana (week day), DiaAno 
(day of the year), DiaMes (day of the month), SemanaAno (week of the year) 
and SemanaMes (week of the month).
The first two lines of the orig.data data.frame are:
         Data       InicioViagem Duracao     DiaSemana  TipoDia EpocaEscolar 
DiasDesdeUltPagamento DiaAno DiaMes FluxoEntrada FluxoSaida
13 2004-01-01        25056    3220        quinta-feira   feriado 
normal                     9                        1             1 
normal           fsp4
9  2004-01-01        28554    2866         quinta-feira   feriado 
normal                     9                        1             1 
normal           fsp4
            Modelo                  Motorista SemanaAno SemanaMes Servico
13    Mercedes_O530_N     10701         1             1 
10597
9      Mercedes_O530_N     11292         1             1 
10597

I am using sliding window with 30 days (around 900 records) for training.
The error is in the svm function. May be because SVM uses other functions, 
but it happens when I run svm.

Thanks a lot for the help

Joao
_______________________________________________
FEUP - Engineering Faculty, Porto University
Engineering and Industrial Management group
Tel.: +351 22 508 1639
Fax: +351 22 508 1538

----- Original Message ----- 
From: "David Meyer" <david.meyer at wu-wien.ac.at>
To: <jmoreira at fe.up.pt>
Cc: <r-help at stat.math.ethz.ch>
Sent: Sunday, December 19, 2004 1:23 PM
Subject: Re: [R] erro in SVM (packsge "e1071")


> Joao:
>
> The reported error message is not from e1071.
> How *exactly* did you call svm()?
>
> As to the documentation of the nu parameter: yes, this is an omission,
> of course, nu is used in nu-regression as well; thanks for pointing this
> out.
>
> best,
> David
>
> ---------
>
> Hello,
>
> I am using SVM under e1071 package for nu-regression with 18 parameters.
> The
> variables are ordered factors, factors, date or numeric datatypes. I use
> the
> linear kernel.
> It gives the following error that I cannot solve. I tryed debug, browser
> and
> all that stuff, but no way.
> The error is:
>
> Error in get(ctr, mode = "function", envir = parent.frame())(levels(x),
> :
>        Orthogonal polynomials cannot be represented accurately enough
> for 236
> degrees of freedom
>
> I use the nu parameter. However, reading ?svm help it says "parameter
> needed
> for 'nu-classification' and 'one-classification'". Does not say anything
> about
> nu-regression. It is an omission in the ?svm help page? Or am I
> notundestanding something?
>
> I believe it has something to do with the calculus of the eigenvalues.
> Anyway
> how can I overpass this problem? Increasing the training data (is around
> 900
> records)?
>
> Thanks for any help
>
> Joao
>
>
>
>
> -- 
> Dr. David Meyer
> Department of Information Systems
>
> Vienna University of Economics and Business Administration
> Augasse 2-6, A-1090 Wien, Austria, Europe
> Fax: +43-1-313 36x746
> Tel: +43-1-313 36x4393
> HP:  http://wi.wu-wien.ac.at/~meyer/
>



From Whit.Armstrong at tudor.com  Mon Dec 20 20:37:51 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Mon, 20 Dec 2004 14:37:51 -0500
Subject: [R] faster row by row data frame processing
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF042B3D9B@tudor.com>

Something like this perhaps?

x <- matrix(rnorm(1000),ncol=10)
y <- t(apply(abs(x),1,rank,ties.method="first"))

thresh <- 8
x[y>thresh] <- sign(x[y>thresh])
x[y<=thresh] <- 0


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> Sent: Monday, December 20, 2004 1:52 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] faster row by row data frame processing
> 
> 
> Dear R users,
> 
> I have a data frame with a few thousand rows and several 
> hundred numeric columns (plus a date column). For each row 
> (day), I want to assign +/- 1 to the highest X absolute 
> values, 0 to the other values, and save all that in a 
> separate data frame. 
> 
> I have a working solution (below), however I find it rather 
> slow. Is there something I could do to increase the speed? 
> (The code is CPU-bound; Pentium 4 @ 2.4 GHz, 512 MB RAM, Win 
> XP, R 2.0.0.)
> 
> Thank you,
> b.
> 
> 
> #all is the original data frame (date + a number of columns) 
> #set up the output data frame DailyTopN <- 
> data.frame(all[1,1],matrix(ncol=ncol(all)-1))
> names(DailyTopN) <- names(all)
> top <- 20
> for (i in 1:1000)	#the rows to be processed
> 	{
> 	#data frame row as vector
> 	onerow <- na.omit(as.matrix(all[i,][2:ncol(all)])[1, ])
> 	#select the 'top' highest absolute values
> 	r <- rank(abs(onerow),ties.method="random")
> 	selected <- names(r[which(r <= top)])
> 	#set +/-1 for the highest absolute values, 0 for the others
> 	DailyTopN[i,selected] <- 1 * sign(all[i,selected])
> 	DailyTopN[i,1] <- all[i,1]	#add the date
> 	}
> DailyTopN[is.na(DailyTopN)] <- 0
> rownames(DailyTopN) <- 1:nrow(DailyTopN)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Mon Dec 20 21:06:47 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 20 Dec 2004 15:06:47 -0500
Subject: [R] Interest in commercial support for R, R-metrics and
	relat ed packages
Message-ID: <3A822319EB35174CA3714066D590DCD50994E455@usrymx25.merck.com>

Spotfire communicates with R and S-PLUS via SOAP (in R's case, via Greg
Warnes' RSOAP server).  I would guess that `linking' at that level with GPL
software does not make the code on the Spotfire side GPL.  The two systems
don't even have to live on the same computer (and most likely, don't).

Just my $0.02...

Andy

> From: Spencer Graves
> 
>       A representative from Spotfire (www.spotfire.com) told 
> me they can 
> link to S-Plus and R, and they use those links to produce custom code 
> for customers.  By the General Public License (GPL), Spotfire is 
> required to offer under the GPL the code for how they link to R, but 
> they don't have to make publicly available the code for their core 
> product.  Also, I don't think they have to make publicly 
> available the 
> code they write for customers as long as they are selling consulting 
> services and not software. 
> 
>       This may or may not help you, since Spotfire does not focus 
> primarily on financial services. 
> 
>       spencer graves
> 
> Seshasayanan Pratap wrote:
> 
> >I am a partner at Assembla, a software services group that 
> helps companies
> >make use of open source techniques and software. We have 
> been asked by a
> >company in the financial services sector to provide support 
> for their use of
> >R, Rmetrics and possibly SciViews. 
> >
> >I am trying to locate other companies in the financial 
> services industry
> >that might be interested in commercial support for R, 
> Rmetrics and/or other
> >R packages used for financial analysis. I would like to 
> understand what type
> >of services - such as technical support, custom development, 
> versions tuned
> >for financial analysis, user conferences/workshops, etc - would be
> >interesting to users of R. 
> >
> >Thank you. 
> >
> >Sesha Pratap 
> >
> > 
> ><http://ps3pub.psteering.com/assembla/discussion/discussion_c
reate.jsp?objec
>tId=fs000080000g1asihvp0000000&parent=fs000080000g1sc50c0g000000> Reply
>
>
> 
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spratap at assembla.com  Mon Dec 20 21:27:45 2004
From: spratap at assembla.com (Seshasayanan Pratap)
Date: Mon, 20 Dec 2004 15:27:45 -0500
Subject: [R] Interest in commercial support for R,
	R-metrics and related packages
In-Reply-To: <41C719D2.6070302@pdf.com>
Message-ID: <200412202028.iBKKRvP32271@assembla.apolloservers.com>

Thanks for the tip about Spotfire.  We are trying to locate companies that
would be interested in joining a collaborative effort to fund modifications,
enhancements and customizations to R.  Some efforts, such as improving
performance for large datasets, would be released under the GPL.  Other
efforts, such as a better GUI for financial analysts using R, are likely to
be released under a shared code license to members of the collaborative. In
situations where we are developing custom code or shared code, we will
probably use a linking scheme similar to Spotfire.


-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Monday, December 20, 2004 1:29 PM
To: Seshasayanan Pratap
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Interest in commercial support for R, R-metrics and related
packages

      A representative from Spotfire (www.spotfire.com) told me they can 
link to S-Plus and R, and they use those links to produce custom code 
for customers.  By the General Public License (GPL), Spotfire is 
required to offer under the GPL the code for how they link to R, but 
they don't have to make publicly available the code for their core 
product.  Also, I don't think they have to make publicly available the 
code they write for customers as long as they are selling consulting 
services and not software. 

      This may or may not help you, since Spotfire does not focus 
primarily on financial services. 

      spencer graves

Seshasayanan Pratap wrote:

>I am a partner at Assembla, a software services group that helps companies
>make use of open source techniques and software. We have been asked by a
>company in the financial services sector to provide support for their use
of
>R, Rmetrics and possibly SciViews. 
>
>I am trying to locate other companies in the financial services industry
>that might be interested in commercial support for R, Rmetrics and/or other
>R packages used for financial analysis. I would like to understand what
type
>of services - such as technical support, custom development, versions tuned
>for financial analysis, user conferences/workshops, etc - would be
>interesting to users of R. 
>
>Thank you. 
>
>Sesha Pratap 
>
> 
><http://ps3pub.psteering.com/assembla/discussion/discussion_create.jsp?obje
c
>tId=fs000080000g1asihvp0000000&parent=fs000080000g1sc50c0g000000> Reply
>
>
> 
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From r.ghezzo at staff.mcgill.ca  Mon Dec 20 21:45:11 2004
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Mon, 20 Dec 2004 15:45:11 -0500
Subject: [R] problems with limma
Message-ID: <1103575511.41c739d7085e4@webmail.mcgill.ca>

I try to send this message To Gordon Smyth at smyth at vehi,edu.au but it bounced
back, so here it is to r-help

I am trying to use limma, just downloaded it from CRAN. I use R 2.0.1 on Win XP
see the following:
> library(RODBC)
> chan1 <- odbcConnectExcel("D:/Data/mgc/Chips/Chips4.xls")
> dd <- sqlFetch(chan1,"Raw")   # all data  12000
> #
> nzw <- cbind(dd$NZW1C,dd$NZW2C,dd$NZW3C,dd$NZW1T,dd$NZW2T,dd$NZW3T)
> akr <- cbind(dd$AKR1C,dd$AKR2C,dd$AKR3C,dd$AKR1T,dd$AKR2T,dd$AKR3T)
> bas <- cbind(dd$NZW1C,dd$NZW2C,dd$NZW3C,dd$AKR1C,dd$AKR2C,dd$AKR3C)
> #
>  design<-matrix(c(1,1,1,1,1,1,0,0,0,1,1,1),ncol=2)
>  fit1 <- lmFit(nzw,design)
>  fit1 <- eBayes(fit1)
>  topTable(fit1,adjust="fdr",number=5)
              M         t      P.Value         B
12222  3679.480 121.24612 7.828493e-06 -4.508864
1903   3012.405 118.32859 7.828493e-06 -4.508866
9068   1850.232  92.70893 1.178902e-05 -4.508889
10635  2843.534  91.99336 1.178902e-05 -4.508890
561   18727.858  90.17085 1.178902e-05 -4.508893
> #
>  fit2 <- lmFit(akr,design)
>  fit2 <- eBayes(fit2)
>  topTable(fit2,adjust="fdr",number=5)
              M        t      P.Value         B
88     1426.738 80.48058 5.839462e-05 -4.510845
1964  36774.167 73.05580 5.839462e-05 -4.510861
5854   7422.578 68.60316 5.839462e-05 -4.510874
11890  1975.316 66.54480 5.839462e-05 -4.510880
9088   2696.952 64.16343 5.839462e-05 -4.510889
> #
>  fit3 <- lmFit(bas,design)
>  fit3 <- eBayes(fit3)
>  topTable(fit3,adjust="fdr",number=5)
             M         t      P.Value         B
6262  1415.088 100.78933 2.109822e-05 -4.521016
5660  1913.479  96.40903 2.109822e-05 -4.521020
11900 4458.489  94.30738 2.109822e-05 -4.521022
9358  1522.330  80.46641 3.346749e-05 -4.521041
11773 1784.483  73.76620 3.346749e-05 -4.521053
> #    Now lets do all together in Anova
> #
>  all <- cbind(nzw,akr)
>  ts <- c(1,1,1,2,2,2,3,3,3,4,4,4)
>  ts <- as.factor(ts)
>  levels(ts) <- c("nzwC","nzwT","akrC","akrT")
>  design <- model.matrix(~0+ts)
>  colnames(design) <- levels(ts)
>  fit4 <- lmFit(all,design)
>  cont.matrix <- makeContrasts(
+      Baseline = akrC - nzwC,
+      NZW_Smk = nzwT - nzwC,
+      AKR_Smk = akrT - akrC,
+      Diff = (akrT - akrC) - (nzwT - nzwC),
+      levels=design)
>   fit42 <- contrasts.fit(fit4,cont.matrix)
>   fit42 <- eBayes(fit42)
> #
>   topTable(fit42,coef="Baseline",adjust="fdr",number=5)
               M         t     P.Value         B
3189    942.0993  13.57485 0.004062283 -4.528799
8607   2634.1826  11.23476 0.006913442 -4.530338
10242  -942.2860 -10.99253 0.006913442 -4.530551
283    -609.0831 -10.79354 0.006913442 -4.530735
3224  -1564.2572 -10.19429 0.008089034 -4.531351
----------------------------------------------------
------------- Shouldn't this be equal to fit1 above?
----------------------------------------------------
>   topTable(fit42,coef="NZW_Smk",adjust="fdr",number=5)
             M         t   P.Value         B
7724 -246.5956 -8.687324 0.1615395 -4.591133
1403 -307.8660 -7.063312 0.4066814 -4.591363
3865 -253.4899 -6.585582 0.4598217 -4.591457
3032 -509.2413 -5.841901 0.8294166 -4.591640
2490 -240.3259 -5.338679 0.9997975 -4.591795
----------------------------------------------------
------------- Shouldn't this be equal to fit2 above?
------------- The P.Value are unreal!!
----------------------------------------------------
>   topTable(fit42,coef="AKR_Smk",adjust="fdr",number=5)
             M        t  P.Value         B
11547 151.6622 6.380978 0.917470 -4.595085
12064 324.0851 6.337235 0.917470 -4.595085
6752  964.5478 5.858994 0.952782 -4.595086
10251 152.7587 5.339843 0.952782 -4.595087
1440  189.6056 4.933151 0.952782 -4.595089
----------------------------------------------------
------------- Shouldn't this be equal to fit3 above?
------------- The P.Value are unreal!!
----------------------------------------------------
>   topTable(fit42,coef="Diff",adjust="fdr",number=5)
              M         t   P.Value         B
7724   302.6892  7.540195 0.4102211 -4.593201
1403   419.4962  6.805495 0.4102211 -4.593265
10251  270.5269  6.686796 0.4102211 -4.593277
3270   409.8391  6.414966 0.4192042 -4.593307
10960 -511.4711 -5.469247 0.9652171 -4.593435
> #
>
So the results I get from just pairwise comparisons are very significant, but
when I try the Anova way, the significance completely dissapears.
Am I doing something completely wrong?
This is data from Affimetrix mouse chips.
Thanks for any help
Heberto Ghezzo
Ph.D.
Meakins-Christie Labs
McGill University
Montreal - Canada



From michael.watson at bbsrc.ac.uk  Mon Dec 20 22:14:31 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 20 Dec 2004 21:14:31 -0000
Subject: [R] Memory problem with jpeg() and wide jpegs
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B8F3@iahce2knas1.iah.bbsrc.reserved>

Surprisingly, Internet Explorer (which due to it's browser dominance will be the main, but not sole, purveyor of the images I create) loads this image up in a few seconds and allows the user to scroll along it very nicely.  The image is really for demonstration purposes only, and many people who use the package I am writing (if there are any) will create much smaller images of the object in question.  However, I like the fact that R can draw a whole genome in about 10 seconds, don't you? ;-)

Mick


-----Original Message-----
From:	Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent:	Mon 12/20/2004 5:37 PM
To:	Uwe Ligges
Cc:	michael watson (IAH-C); R-help at stat.math.ethz.ch
Subject:	Re: [R] Memory problem with jpeg() and wide jpegs
It is also several times greater than the limit of human perception, being 
several feet long at printing resolutions that need a magnifying glass 
to see.

This is Windows and the limit is in the graphics card: mine is able to do 
this but I suspect you need a 128Mb card (that jpeg is of itself about 
90Mb).  However, most viewers (including PhotoShop) will barf on such a 
large jpeg.


On Mon, 20 Dec 2004, Uwe Ligges wrote:

> michael watson (IAH-C) wrote:
>
>> Hi
>> 
>> I have been creating very, very long jpeg images for the last two weeks
>> using jpeg().  All of a sudden, and I mean that, it's stopped working -
>> I've not changed a thing!  The error message I get is:
>> 
>> 
>>> jpeg("out.jpg",width=50000,height=480, quality=100)
>> 
>> Error in devga(paste("jpeg:", quality, ":", filename, sep = ""), width,
>> :         unable to start device devga
>> In addition: Warning message: Unable to allocate bitmap 
>> I have plenty of disk space in the place I want to create the jpeg, and
>> if I reduce it from "width=50000" to "width=5000", then it works no
>> problem, which suggests it is a memory problem.  If I close R and
>> re-open R the problem does not go away.  And as I said above, I have
>> been using this code for weeks with no problem, then I change nothing,
>> and start getting this error message.
>
> I don't believe it has worked. Such a huge one does not work for me even on a 
> rather big machine.
> If it had worked: On the same platform, OS, R version?
> I think you have to contribute a patch in order to get such a huge jpeg.
>
> Uwe Ligges
>
>
>> I'm running R 2.0.1 on Windows XP.  I'm going to restart Windows and see
>> if that helps, but if re-starting windows does help, is there an
>> explanation as to where all the memory R used to be able to get at
>> disappeared to in the current session?
>>
>> Cheers
>> Mick
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From michael.watson at bbsrc.ac.uk  Mon Dec 20 22:28:00 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 20 Dec 2004 21:28:00 -0000
Subject: [R] problems with limma
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B8F5@iahce2knas1.iah.bbsrc.reserved>

Could you give a bit more detail about your experimental design?  You're using affy, so you're working with single channel data - so nzw, akr and bas all have six arrays?


-----Original Message-----
From:	r-help-bounces at stat.math.ethz.ch on behalf of r.ghezzo at staff.mcgill.ca
Sent:	Mon 12/20/2004 8:45 PM
To:	r-help at stat.math.ethz.ch
Cc:	
Subject:	[R] problems with limma
I try to send this message To Gordon Smyth at smyth at vehi,edu.au but it bounced
back, so here it is to r-help

I am trying to use limma, just downloaded it from CRAN. I use R 2.0.1 on Win XP
see the following:
> library(RODBC)
> chan1 <- odbcConnectExcel("D:/Data/mgc/Chips/Chips4.xls")
> dd <- sqlFetch(chan1,"Raw")   # all data  12000
> #
> nzw <- cbind(dd$NZW1C,dd$NZW2C,dd$NZW3C,dd$NZW1T,dd$NZW2T,dd$NZW3T)
> akr <- cbind(dd$AKR1C,dd$AKR2C,dd$AKR3C,dd$AKR1T,dd$AKR2T,dd$AKR3T)
> bas <- cbind(dd$NZW1C,dd$NZW2C,dd$NZW3C,dd$AKR1C,dd$AKR2C,dd$AKR3C)
> #
>  design<-matrix(c(1,1,1,1,1,1,0,0,0,1,1,1),ncol=2)
>  fit1 <- lmFit(nzw,design)
>  fit1 <- eBayes(fit1)
>  topTable(fit1,adjust="fdr",number=5)
              M         t      P.Value         B
12222  3679.480 121.24612 7.828493e-06 -4.508864
1903   3012.405 118.32859 7.828493e-06 -4.508866
9068   1850.232  92.70893 1.178902e-05 -4.508889
10635  2843.534  91.99336 1.178902e-05 -4.508890
561   18727.858  90.17085 1.178902e-05 -4.508893
> #
>  fit2 <- lmFit(akr,design)
>  fit2 <- eBayes(fit2)
>  topTable(fit2,adjust="fdr",number=5)
              M        t      P.Value         B
88     1426.738 80.48058 5.839462e-05 -4.510845
1964  36774.167 73.05580 5.839462e-05 -4.510861
5854   7422.578 68.60316 5.839462e-05 -4.510874
11890  1975.316 66.54480 5.839462e-05 -4.510880
9088   2696.952 64.16343 5.839462e-05 -4.510889
> #
>  fit3 <- lmFit(bas,design)
>  fit3 <- eBayes(fit3)
>  topTable(fit3,adjust="fdr",number=5)
             M         t      P.Value         B
6262  1415.088 100.78933 2.109822e-05 -4.521016
5660  1913.479  96.40903 2.109822e-05 -4.521020
11900 4458.489  94.30738 2.109822e-05 -4.521022
9358  1522.330  80.46641 3.346749e-05 -4.521041
11773 1784.483  73.76620 3.346749e-05 -4.521053
> #    Now lets do all together in Anova
> #
>  all <- cbind(nzw,akr)
>  ts <- c(1,1,1,2,2,2,3,3,3,4,4,4)
>  ts <- as.factor(ts)
>  levels(ts) <- c("nzwC","nzwT","akrC","akrT")
>  design <- model.matrix(~0+ts)
>  colnames(design) <- levels(ts)
>  fit4 <- lmFit(all,design)
>  cont.matrix <- makeContrasts(
+      Baseline = akrC - nzwC,
+      NZW_Smk = nzwT - nzwC,
+      AKR_Smk = akrT - akrC,
+      Diff = (akrT - akrC) - (nzwT - nzwC),
+      levels=design)
>   fit42 <- contrasts.fit(fit4,cont.matrix)
>   fit42 <- eBayes(fit42)
> #
>   topTable(fit42,coef="Baseline",adjust="fdr",number=5)
               M         t     P.Value         B
3189    942.0993  13.57485 0.004062283 -4.528799
8607   2634.1826  11.23476 0.006913442 -4.530338
10242  -942.2860 -10.99253 0.006913442 -4.530551
283    -609.0831 -10.79354 0.006913442 -4.530735
3224  -1564.2572 -10.19429 0.008089034 -4.531351
----------------------------------------------------
------------- Shouldn't this be equal to fit1 above?
----------------------------------------------------
>   topTable(fit42,coef="NZW_Smk",adjust="fdr",number=5)
             M         t   P.Value         B
7724 -246.5956 -8.687324 0.1615395 -4.591133
1403 -307.8660 -7.063312 0.4066814 -4.591363
3865 -253.4899 -6.585582 0.4598217 -4.591457
3032 -509.2413 -5.841901 0.8294166 -4.591640
2490 -240.3259 -5.338679 0.9997975 -4.591795
----------------------------------------------------
------------- Shouldn't this be equal to fit2 above?
------------- The P.Value are unreal!!
----------------------------------------------------
>   topTable(fit42,coef="AKR_Smk",adjust="fdr",number=5)
             M        t  P.Value         B
11547 151.6622 6.380978 0.917470 -4.595085
12064 324.0851 6.337235 0.917470 -4.595085
6752  964.5478 5.858994 0.952782 -4.595086
10251 152.7587 5.339843 0.952782 -4.595087
1440  189.6056 4.933151 0.952782 -4.595089
----------------------------------------------------
------------- Shouldn't this be equal to fit3 above?
------------- The P.Value are unreal!!
----------------------------------------------------
>   topTable(fit42,coef="Diff",adjust="fdr",number=5)
              M         t   P.Value         B
7724   302.6892  7.540195 0.4102211 -4.593201
1403   419.4962  6.805495 0.4102211 -4.593265
10251  270.5269  6.686796 0.4102211 -4.593277
3270   409.8391  6.414966 0.4192042 -4.593307
10960 -511.4711 -5.469247 0.9652171 -4.593435
> #
>
So the results I get from just pairwise comparisons are very significant, but
when I try the Anova way, the significance completely dissapears.
Am I doing something completely wrong?
This is data from Affimetrix mouse chips.
Thanks for any help
Heberto Ghezzo
Ph.D.
Meakins-Christie Labs
McGill University
Montreal - Canada

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From david.meyer at wu-wien.ac.at  Mon Dec 20 22:41:32 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Mon, 20 Dec 2004 22:41:32 +0100
Subject: [R] erro in SVM (packsge "e1071")
In-Reply-To: <013c01c4e6c5$0f238b70$5e7aa8c0@FEUPsig.fe.up.pt>
References: <20041219142307.00c8336a.david.meyer@wu-wien.ac.at>
	<013c01c4e6c5$0f238b70$5e7aa8c0@FEUPsig.fe.up.pt>
Message-ID: <20041220224132.5bff5fa3.david.meyer@wu-wien.ac.at>

So the error occurs during a call to model.matrix() from svm() because
of the polynomial contrasts--do you get the same error using, e.g.,
lm()?

best,
David

> The way I call SVM is:
> 
> i <- (-2)
> j <- 4
> learner='svm'
> learner.pars=list(Duracao ~ ., data=orig.data,
>    scale=c(FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE,
>     FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,
>     FALSE, FALSE, FALSE),
>    type='nu-regression', kernel='linear',
>    cost=2^(2*i), nu=j/10)
> learner.pars$data <- orig.data[begin.test.pos:(test.pos-1),]
> 
> model <- do.call(learner,learner.pars)
> 
> The variables begin.test.pos and test.pos are windexes for orig.data
> and are working well. in this case begin.test.pos = 1 and test.pos =
> 875.
> 
> orig.data is a data.frame where the second, third and seventh
> parameters are numeric. The first parameter  is a date and all the
> others are factors (some of them ordered). The ordered factors are:
> Dia Semana (week day), DiaAno (day of the year), DiaMes (day of the
> month), SemanaAno (week of the year) and SemanaMes (week of the
> month). The first two lines of the orig.data data.frame are:
>          Data       InicioViagem Duracao     DiaSemana  TipoDia
>          EpocaEscolar 
> DiasDesdeUltPagamento DiaAno DiaMes FluxoEntrada FluxoSaida
> 13 2004-01-01        25056    3220        quinta-feira   feriado 
> normal                     9                        1             1 
> normal           fsp4
> 9  2004-01-01        28554    2866         quinta-feira   feriado 
> normal                     9                        1             1 
> normal           fsp4
>             Modelo                  Motorista SemanaAno SemanaMes
>             Servico
> 13    Mercedes_O530_N     10701         1             1 
> 10597
> 9      Mercedes_O530_N     11292         1             1 
> 10597
> 
> I am using sliding window with 30 days (around 900 records) for
> training. The error is in the svm function. May be because SVM uses
> other functions, but it happens when I run svm.
> 
> Thanks a lot for the help
> 
> Joao
> _______________________________________________
> FEUP - Engineering Faculty, Porto University
> Engineering and Industrial Management group
> Tel.: +351 22 508 1639
> Fax: +351 22 508 1538
> 
> ----- Original Message ----- 
> From: "David Meyer" <david.meyer at wu-wien.ac.at>
> To: <jmoreira at fe.up.pt>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Sunday, December 19, 2004 1:23 PM
> Subject: Re: [R] erro in SVM (packsge "e1071")
> 
> 
> > Joao:
> >
> > The reported error message is not from e1071.
> > How *exactly* did you call svm()?
> >
> > As to the documentation of the nu parameter: yes, this is an
> > omission, of course, nu is used in nu-regression as well; thanks for
> > pointing this out.
> >
> > best,
> > David
> >
> > ---------
> >
> > Hello,
> >
> > I am using SVM under e1071 package for nu-regression with 18
> > parameters. The
> > variables are ordered factors, factors, date or numeric datatypes. I
> > use the
> > linear kernel.
> > It gives the following error that I cannot solve. I tryed debug,
> > browser and
> > all that stuff, but no way.
> > The error is:
> >
> > Error in get(ctr, mode = "function", envir =
> > parent.frame())(levels(x),:
> >        Orthogonal polynomials cannot be represented accurately
> >        enough
> > for 236
> > degrees of freedom
> >
> > I use the nu parameter. However, reading ?svm help it says
> > "parameter needed
> > for 'nu-classification' and 'one-classification'". Does not say
> > anything about
> > nu-regression. It is an omission in the ?svm help page? Or am I
> > notundestanding something?
> >
> > I believe it has something to do with the calculus of the
> > eigenvalues. Anyway
> > how can I overpass this problem? Increasing the training data (is
> > around 900
> > records)?
> >
> > Thanks for any help
> >
> > Joao
> >
> >
> >
> >
> > -- 
> > Dr. David Meyer
> > Department of Information Systems
> >
> > Vienna University of Economics and Business Administration
> > Augasse 2-6, A-1090 Wien, Austria, Europe
> > Fax: +43-1-313 36x746
> > Tel: +43-1-313 36x4393
> > HP:  http://wi.wu-wien.ac.at/~meyer/
> > 
> 
> 


-- 
Dr. David Meyer
Department of Information Systems

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From Ted.Harding at nessie.mcc.ac.uk  Mon Dec 20 22:29:07 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 20 Dec 2004 21:29:07 -0000 (GMT)
Subject: [R] PBIB datataset
In-Reply-To: <41C63398.70101@stat.wisc.edu>
Message-ID: <XFMail.041220212907.Ted.Harding@nessie.mcc.ac.uk>

On 20-Dec-04 Douglas Bates wrote:
> This is a new version of SASmixed that was uploaded a couple of days 
> ago.  I changed it so that the fits are done with the lme4 version of 
> lme.  It should be faster and more reliable than the version of lme in 
> the nlme package.
> 
> This version of SASmixed has a vignette that provides comparative 
> analyses in lme for the examples in "SAS System for Mixed Models".  The
> specification of models in the new lme is occasionally different from 
> the older specification.  Don't pay too much attention to the textual 
> descriptions - look at the examples in the appendices.  I haven't 
> finished rewriting the textual description from an old, old version.
> 
> Those who (like me) cringe at the way that models with crossed random 
> effects needed to be specified in the old lme may find it interesting 
> that the Demand example now specifies the model fit as
> Demand> fm1Demand <- lme(log(d) ~ log(y) + log(rd) + log(rt) +
>      log(rs), data = Demand, random = list(State = ~1, Year = ~1))
> 
> [...]
> 
> The lme4 and nlme packages should not be loaded simultaneously.
> Use one or the other but not both.

Doug,
Thanks a lot for these clarifications. This still leaves me with
a question or two.

Suppose I want (as I do) to work through Pinheiro & Bates, example
by example. Some datasets are here (lme4), others there (SASmixed,
nlme), etc.

Does this mean I need to start afresh, loading just the package
with the dataset currently of interest?

I'm a touch confused about the distribution (a) of datasets,
(b) of the R functions needed to analyse them according to
Pinheiro & Bates, over the packages! If I load the package
containing dataset A, will it include the functions used in
P&B for that dataset? Or might I need to load another one
as well? and, if so, might I encounter the sort of clash
you hint at above?

(I know I could probably check it in detail in each case as
it arises, but I guess you know your way arond all this better
than someone who hasn't been there yet).

With thanks (and congratulations on what looks like a very
well conceived book on the topic),
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 20-Dec-04                                       Time: 21:29:07
------------------------------ XFMail ------------------------------



From deepayan at stat.wisc.edu  Mon Dec 20 23:11:01 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 20 Dec 2004 16:11:01 -0600
Subject: [R] PBIB datataset
In-Reply-To: <XFMail.041220212907.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041220212907.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200412201611.01159.deepayan@stat.wisc.edu>

On Monday 20 December 2004 15:29, Ted Harding wrote:

[...]

> > The lme4 and nlme packages should not be loaded simultaneously.
> > Use one or the other but not both.
>
> Doug,
> Thanks a lot for these clarifications. This still leaves me with
> a question or two.
>
> Suppose I want (as I do) to work through Pinheiro & Bates, example
> by example. Some datasets are here (lme4), others there (SASmixed,
> nlme), etc.
>
> Does this mean I need to start afresh, loading just the package
> with the dataset currently of interest?

No, you can do 

> data(PBIB, package = "SASmixed")

which loads the dataset without loading the package. If you are using 
nlme, this will give you (essentially) a plain data frame (i.e., it 
won't be a "groupedData" object in the nlme sense). 

Deepayan



From paul.livingstone at aerostructures.com.au  Mon Dec 20 23:30:12 2004
From: paul.livingstone at aerostructures.com.au (Paul Livingstone)
Date: Tue, 21 Dec 2004 09:30:12 +1100
Subject: [R] why use profile likelihood for Box Cox transformation?
Message-ID: <ECD3B76DBDD151439FD9A554D18229B1095254@KRUSTY_II.aerostructures.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041221/944910c2/attachment.pl

From p.murrell at auckland.ac.nz  Mon Dec 20 23:32:39 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 21 Dec 2004 11:32:39 +1300
Subject: [R] Importing vector graphics into R
References: <Pine.LNX.4.44.0412071840270.16498-100000@reclus.nhh.no>
	<41B8025B.9020403@gmx.de>
Message-ID: <41C75307.7010601@stat.auckland.ac.nz>

Hi

Sorry to join this thread late.  As a couple of people have pointed out, 
no general solution exists currently.  It's an interesting problem 
though and something that I have experimented with in a couple of ways 
in the past.  I have written down some thoughts about the issues and 
described a couple of experiments;  it's all very rough at this stage 
and would need a lot of polishing and packaging to be widely useful, but 
you might want to take a look at 
http://www.stat.auckland.ac.nz/~paul/R/Import/import.html
and particularly the further link
http://www.stat.auckland.ac.nz/~paul/R/Import/importvector.html

Paul


Hinrich G??hlmann wrote:
> Thanks for your suggestions!
> 
> Even though they are less than encouraging, I quickly want to give you 
> the rational why I have asked this. Actually I was inspired by Paul 
> Murrell's useR presentation - have a look at the very last slide of his 
> presentation which you can find at
> http://www.stat.auckland.ac.nz/~paul/Talks/useR2004.pdf - If only this 
> kind of functionality could be generalized to any vector graphics... Oh, 
> well, still pixmap gives a solution for the moment and that's ok. Thanks 
> again!
> 
> Cheers,
> hinrich   d8-)
> 
> 
> Roger Bivand wrote:
> 
>> On Tue, 7 Dec 2004, Hinrich G??hlmann wrote:
>>
>>
>>> Dear R users,
>>>
>>> I know of the possibility to import bitmaps via the nice pixmap 
>>> library.    But if you later on create a PDF it is somewhat 
>>> disappointing to have such graphics bitmapped. Is there a trick (via 
>>> maps?) to import a vector graphic and have them plotted onto a graph? 
>>> My searching attempts in the searchable r-help archive did not seem 
>>> to result in anything useful...
>>
>>
>>
>> No, nothing obvious. If you have an Xfig file - or convert to one from 
>> PS,
>> you may be able to extract the lines with their attributes by hand (the
>> file is just text, so you can "see" the vector graphics), and write an R
>> function to plot them (rescaled) onto the device if you need a single
>> graphical element many times. Otherwise, perhaps edit the graphics file
>> after R has completed its work. None of the vector map formats is easy to
>> use for this kind of trick, especially because you probably need
>> attributes on the lines (thickness, colour).
>>
>>
>>> Cheers,
>>> hinrich   d8-)
>>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From bates at stat.wisc.edu  Mon Dec 20 23:37:56 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 20 Dec 2004 16:37:56 -0600
Subject: [R] PBIB datataset
In-Reply-To: <XFMail.041220212907.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041220212907.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41C75444.4040508@stat.wisc.edu>

(Ted Harding) wrote:
> On 20-Dec-04 Douglas Bates wrote:
> 
>>This is a new version of SASmixed that was uploaded a couple of days 
>>ago.  I changed it so that the fits are done with the lme4 version of 
>>lme.  It should be faster and more reliable than the version of lme in 
>>the nlme package.
>>
>>This version of SASmixed has a vignette that provides comparative 
>>analyses in lme for the examples in "SAS System for Mixed Models".  The
>>specification of models in the new lme is occasionally different from 
>>the older specification.  Don't pay too much attention to the textual 
>>descriptions - look at the examples in the appendices.  I haven't 
>>finished rewriting the textual description from an old, old version.
>>
>>Those who (like me) cringe at the way that models with crossed random 
>>effects needed to be specified in the old lme may find it interesting 
>>that the Demand example now specifies the model fit as
>>Demand> fm1Demand <- lme(log(d) ~ log(y) + log(rd) + log(rt) +
>>     log(rs), data = Demand, random = list(State = ~1, Year = ~1))
>>
>>[...]
>>
>>The lme4 and nlme packages should not be loaded simultaneously.
>>Use one or the other but not both.
> 
> 
> Doug,
> Thanks a lot for these clarifications. This still leaves me with
> a question or two.
> 
> Suppose I want (as I do) to work through Pinheiro & Bates, example
> by example. Some datasets are here (lme4), others there (SASmixed,
> nlme), etc.
> 
> Does this mean I need to start afresh, loading just the package
> with the dataset currently of interest?
> 
> I'm a touch confused about the distribution (a) of datasets,
> (b) of the R functions needed to analyse them according to
> Pinheiro & Bates, over the packages! If I load the package
> containing dataset A, will it include the functions used in
> P&B for that dataset? Or might I need to load another one
> as well? and, if so, might I encounter the sort of clash
> you hint at above?
> 
> (I know I could probably check it in detail in each case as
> it arises, but I guess you know your way arond all this better
> than someone who hasn't been there yet).
> 
> With thanks (and congratulations on what looks like a very
> well conceived book on the topic),
> Ted.

If you want to work through P&B you should use the nlme package. The 
only data set mentioned in the book and not available in the nlme 
package is PBIB which, as you have discovered, is in SASmixed.  The data 
sets in SASmixed (and in the lme4 package) are data frames (as opposed 
to groupedData objects).  That is, these data sets don't depend upon 
classes defined in the package.  Hence you could attach the SASmixed 
package, save the PBIB data set to a file, restart R, attach the nlme 
package and then load the PBIB data from the file that you just saved. 
Then you would have a complete set of data sets for use with the nlme 
package.

By the way, you can save yourself some typing by checking the 'scripts' 
directory in the installed package.  It contains R scripts to reproduce 
(most of) the analyses in our book.

The reason that I have to say "most of" is because the examples in the 
book were done with S-PLUS (version 3.4, I believe) and sometimes the 
results in R are different from those in S-PLUS.   These models are fit 
using numerical optimizers and the numerical optimizer code in S-PLUS is 
different from the optimizer code in R.  On this particular set of 
problems the optimizer used by ms() function in S-PLUS tends to perform 
better than the optimizers in either the nlm() or optim() functions in R.

After we finished that book I continued to work on computational methods 
for linear mixed effects models.  The lme4 package is the fruit of that 
development.  Superficially the lme function in the lme4 package looks 
very much like the one in the nlme package but internally they are very 
different.  The most obvious difference is the use of S4 classes and 
methods in lme4, as opposed to the S3 classes and methods in nlme. 
However, there are many, many other differences in the representation 
and the computational methods.

The lme4 package will eventually replace the nlme package but currently 
it does not implement nlme or gls or gnls functions so I don't want to 
replace the nlme package.  Because both these packages define an lme 
function and classes but the classes are incompatible, R will get very 
confused if you load both packages in the same session.

As mentioned above, the design of the lme function in the lme4 package 
is very different from that in the nlme package.  It is more general in 
some ways and less general in others.  The big gains are the ability to 
handle crossed and partially crossed random effects in a much cleaner 
way than could be done in the nlme package.  The lme4 package uses a 
(well actually two but I plan to reduce it to one) sparse matrix 
representation of the lme model and data that makes it possible to fit 
such models faster and more reliably than in the past.



From p.murrell at auckland.ac.nz  Mon Dec 20 23:39:18 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 21 Dec 2004 11:39:18 +1300
Subject: [R] font size (library stats)
References: <000601c4e474$562dd500$235e010a@Qingyou>
Message-ID: <41C75496.8040203@stat.auckland.ac.nz>

Hi


zhang qingyou wrote:
 > Hello! With library stats I use command "plot" and get a cluster
 > dendrogram. But I have more than 250 labels and the labels were
 > superposed on cluster dendrogram. So what option should I use to zoom
 > out the characters of the labels.


Look at ?plot.dendrogram and especially the nodePar argument.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From hb at maths.lth.se  Tue Dec 21 00:21:02 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 21 Dec 2004 00:21:02 +0100
Subject: [R] problems with limma
In-Reply-To: <1103575511.41c739d7085e4@webmail.mcgill.ca>
Message-ID: <001601c4e6ea$9334e6f0$650040d5@hblaptop>

It probably bounces because the email address is incorrect (it should be a
'w', not a 'v'). /Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> r.ghezzo at staff.mcgill.ca
> Sent: Monday, December 20, 2004 9:45 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] problems with limma
> 
> 
> I try to send this message To Gordon Smyth at 
> smyth at vehi,edu.au but it bounced back, so here it is to r-help
> 
> I am trying to use limma, just downloaded it from CRAN. I use 
> R 2.0.1 on Win XP see the following:
> > library(RODBC)
> > chan1 <- odbcConnectExcel("D:/Data/mgc/Chips/Chips4.xls")
> > dd <- sqlFetch(chan1,"Raw")   # all data  12000
> > #
> > nzw <- cbind(dd$NZW1C,dd$NZW2C,dd$NZW3C,dd$NZW1T,dd$NZW2T,dd$NZW3T)
> > akr <- cbind(dd$AKR1C,dd$AKR2C,dd$AKR3C,dd$AKR1T,dd$AKR2T,dd$AKR3T)
> > bas <- cbind(dd$NZW1C,dd$NZW2C,dd$NZW3C,dd$AKR1C,dd$AKR2C,dd$AKR3C)
> > #
> >  design<-matrix(c(1,1,1,1,1,1,0,0,0,1,1,1),ncol=2)
> >  fit1 <- lmFit(nzw,design)
> >  fit1 <- eBayes(fit1)
> >  topTable(fit1,adjust="fdr",number=5)
>               M         t      P.Value         B
> 12222  3679.480 121.24612 7.828493e-06 -4.508864
> 1903   3012.405 118.32859 7.828493e-06 -4.508866
> 9068   1850.232  92.70893 1.178902e-05 -4.508889
> 10635  2843.534  91.99336 1.178902e-05 -4.508890
> 561   18727.858  90.17085 1.178902e-05 -4.508893
> > #
> >  fit2 <- lmFit(akr,design)
> >  fit2 <- eBayes(fit2)
> >  topTable(fit2,adjust="fdr",number=5)
>               M        t      P.Value         B
> 88     1426.738 80.48058 5.839462e-05 -4.510845
> 1964  36774.167 73.05580 5.839462e-05 -4.510861
> 5854   7422.578 68.60316 5.839462e-05 -4.510874
> 11890  1975.316 66.54480 5.839462e-05 -4.510880
> 9088   2696.952 64.16343 5.839462e-05 -4.510889
> > #
> >  fit3 <- lmFit(bas,design)
> >  fit3 <- eBayes(fit3)
> >  topTable(fit3,adjust="fdr",number=5)
>              M         t      P.Value         B
> 6262  1415.088 100.78933 2.109822e-05 -4.521016
> 5660  1913.479  96.40903 2.109822e-05 -4.521020
> 11900 4458.489  94.30738 2.109822e-05 -4.521022
> 9358  1522.330  80.46641 3.346749e-05 -4.521041
> 11773 1784.483  73.76620 3.346749e-05 -4.521053
> > #    Now lets do all together in Anova
> > #
> >  all <- cbind(nzw,akr)
> >  ts <- c(1,1,1,2,2,2,3,3,3,4,4,4)
> >  ts <- as.factor(ts)
> >  levels(ts) <- c("nzwC","nzwT","akrC","akrT")
> >  design <- model.matrix(~0+ts)
> >  colnames(design) <- levels(ts)
> >  fit4 <- lmFit(all,design)
> >  cont.matrix <- makeContrasts(
> +      Baseline = akrC - nzwC,
> +      NZW_Smk = nzwT - nzwC,
> +      AKR_Smk = akrT - akrC,
> +      Diff = (akrT - akrC) - (nzwT - nzwC),
> +      levels=design)
> >   fit42 <- contrasts.fit(fit4,cont.matrix)
> >   fit42 <- eBayes(fit42)
> > #
> >   topTable(fit42,coef="Baseline",adjust="fdr",number=5)
>                M         t     P.Value         B
> 3189    942.0993  13.57485 0.004062283 -4.528799
> 8607   2634.1826  11.23476 0.006913442 -4.530338
> 10242  -942.2860 -10.99253 0.006913442 -4.530551
> 283    -609.0831 -10.79354 0.006913442 -4.530735
> 3224  -1564.2572 -10.19429 0.008089034 -4.531351
> ----------------------------------------------------
> ------------- Shouldn't this be equal to fit1 above?
> ----------------------------------------------------
> >   topTable(fit42,coef="NZW_Smk",adjust="fdr",number=5)
>              M         t   P.Value         B
> 7724 -246.5956 -8.687324 0.1615395 -4.591133
> 1403 -307.8660 -7.063312 0.4066814 -4.591363
> 3865 -253.4899 -6.585582 0.4598217 -4.591457
> 3032 -509.2413 -5.841901 0.8294166 -4.591640
> 2490 -240.3259 -5.338679 0.9997975 -4.591795
> ----------------------------------------------------
> ------------- Shouldn't this be equal to fit2 above?
> ------------- The P.Value are unreal!!
> ----------------------------------------------------
> >   topTable(fit42,coef="AKR_Smk",adjust="fdr",number=5)
>              M        t  P.Value         B
> 11547 151.6622 6.380978 0.917470 -4.595085
> 12064 324.0851 6.337235 0.917470 -4.595085
> 6752  964.5478 5.858994 0.952782 -4.595086
> 10251 152.7587 5.339843 0.952782 -4.595087
> 1440  189.6056 4.933151 0.952782 -4.595089
> ----------------------------------------------------
> ------------- Shouldn't this be equal to fit3 above?
> ------------- The P.Value are unreal!!
> ----------------------------------------------------
> >   topTable(fit42,coef="Diff",adjust="fdr",number=5)
>               M         t   P.Value         B
> 7724   302.6892  7.540195 0.4102211 -4.593201
> 1403   419.4962  6.805495 0.4102211 -4.593265
> 10251  270.5269  6.686796 0.4102211 -4.593277
> 3270   409.8391  6.414966 0.4192042 -4.593307
> 10960 -511.4711 -5.469247 0.9652171 -4.593435
> > #
> >
> So the results I get from just pairwise comparisons are very 
> significant, but when I try the Anova way, the significance 
> completely dissapears. Am I doing something completely wrong? 
> This is data from Affimetrix mouse chips. Thanks for any help 
> Heberto Ghezzo Ph.D. Meakins-Christie Labs McGill University 
> Montreal - Canada
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From MSchwartz at MedAnalytics.com  Tue Dec 21 00:21:12 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 20 Dec 2004 17:21:12 -0600
Subject: [R] Dose of Reality re: SAS vs R
In-Reply-To: <200412201838.iBKIcChF027113@compton.gene.com>
References: <200412201838.iBKIcChF027113@compton.gene.com>
Message-ID: <1103584872.10887.113.camel@horizons.localdomain>

On Mon, 2004-12-20 at 10:38 -0800, Berton Gunter wrote:
> R folks:
> 
> I appreciate and have learned from the recent "SAS vs R" and "Bad Excel
> Calculations" threads. Not only civil, but even at times erudite,
> discussion. So I apologize for the lateness of this remark and hope it isn't
> redundant or trivial.
> 
> To those who may wonder why SAS is so dominant in the clinical arena despite
> (better) alternatives: INERTIA. That is:
> 
> 1) There is a huge infrastructure of SAS code already in place for
> regulatory submissions and SAS programmers to maintain and enlarge it. As a
> practical matter, it is hard to imagine a large organization simply chucking
> this and starting afresh. Clearly, change -- if were to occur at all --
> would have to be slow and incremental.
> 
> 2) From my experience at presentations of recent biostatistics PhD's, for
> most, their education continues to promulgate the use of SAS in
> clinical/regulatory settings, undoubtedly due to 1).
> 
> 3) As has already been noted, most existing FDA regulators -- statisticians
> and clinicians alike -- are familiar with SAS, and therefore submissions
> with other software (like R) might delay or complicate the review process.
> We statisticians are not the biggest dogs in this arena, after all.
> 
> Reality bites! So R users must persevere.

Since the notion of inertia was raised by Bert, for those interested in
at least one theory on the adoption of technology and product life
cycles (if one considers R as a software technology), the book "Crossing
the Chasm" by Geoffrey Moore might be of interest.

The Amazon.com link is:

http://www.amazon.com/exec/obidos/tg/detail/-/0066620023

and a very brief Wikipedia overview is here, with a diagram:

http://en.wikipedia.org/wiki/Crossing_the_Chasm

In many respects, the general and increasing adoption of open source
applications fits the theory well. One might consider the growth of
Linux and more recent specific examples of applications such as Firefox
and Thunderbird as replacements for Internet Explorer and Outlook
Express (anybody see the two page Firefox ad in the New York Times).

The potential impact of this particular theory, with respect to change,
was importantly noted when the National Academy of Sciences' Institute
of Medicine published a book as part of their Health Care Quality
Initiative, calling it "Crossing the Quality Chasm: A New Health System
for the 21st Century":

http://www.iom.edu/focuson.asp?id=8089

Another book, which I think dovetails with Moore's book, is "Only the
Paranoid Survive" by Andy Grove, the Chairman of the Board at Intel. In
some cases, the catalyst for crossing the chasm might be a shift in
marketplace dynamics, which sees a market leader falter when they fail
to effectively react to the shift, enabling a new company, technology or
product to take the leadership position.

Grove calls these situations "strategic inflection points", with a
meaning taken from the mathematical term. If the company properly reacts
to the shift, they experience new positive growth possibly under a
substantially altered business model. If they fail to react, they begin
a slide downhill, possibly to never recover or regain their dominance.

The Amazon.com link for Grove's book is:

http://www.amazon.com/exec/obidos/tg/detail/-/0385483821

HTH,

Marc Schwartz



From p.murrell at auckland.ac.nz  Tue Dec 21 00:47:44 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 21 Dec 2004 12:47:44 +1300
Subject: [R] muliple plots with pairs (matrix of scatter plots)
References: <p06100501bdebd4eff241@[83.132.29.100]>
	<41C6BE1C.7060603@statistik.uni-dortmund.de>
Message-ID: <41C764A0.9040007@stat.auckland.ac.nz>

Hi


Uwe Ligges wrote:
> Tiago R Magalhaes wrote:
> 
>> I am trying to make a graph with 4 scatter matrixes plots and couldn't 
>> do it. While trying to find a solution for this I also came across the 
>> idea of giving different values to the same argument for each of the 
>> lower and upper function but couldn't do it. (Examplified below with 
>> the col argument). The first problem of plotting 4 scatter matrixes in 
>> a graph is a problem of real interest for me at this point. The second 
>> problem is a matter of curiosity.
>>
>> I am using a Mac PowerBook G4 with OS 10.3.7 and R 2.0.1
>>
>>
>> Problem 1)
>> x=data.frame(a=sample(1:100, 50), b=sample(1:100, 50),c=sample(1:100, 
>> 50),d=sample(1:100, 50))
>> x.list=vector('list',4)
>> for (j in 1:4) x.list[[j]]=x
>>
>> #produces a graph with four plots:
>> layout(matrix(c(1,3,2,4),2,2))
>> for (j in seq(x)){
>>     plot(x.list[[j]][1:2])
>>     }
>>
>> # But unfortunately the following produces a new plot everytime:
>> layout(matrix(c(1,3,2,4),2,2))
>> for (j in seq(x)){
>>     pairs(x.list[[j]])
>>     }
>> #Maybe pairs can't be used to produce a graph with multiple plots?
> 
> 
> Yes, it uses similar constructs to put multiple plots together.
> 
> You might want to use packages grid and gridBase to set something up 
> using viewports.



I don't think that's going to work either -- pairs() makes some pretty 
strong assumptions that it is the only plot on the page.

One possible way to go is to use splom() instead from the lattice 
package.  For example (using your data from above) ...

splom(~ x)

... and lattice plots can be embedded in grid viewports easily, for 
example ...

grid.newpage()
pushViewport(viewport(layout=grid.layout(2, 2)))
for (j in seq(x)) {
   row <- (j - 1) %/% 2 + 1
   col <- (j - 1) %% 2 + 1
   pushViewport(viewport(layout.pos.col=col,
                         layout.pos.row=row))
   print(splom(~ x.list[[j]]), newpage=FALSE)
   popViewport()
}
popViewport()

... you may need to fiddle with the splom() args to get them looking how 
you want them.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.dalgaard at biostat.ku.dk  Tue Dec 21 00:53:22 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Dec 2004 00:53:22 +0100
Subject: [R] why use profile likelihood for Box Cox transformation?
In-Reply-To: <ECD3B76DBDD151439FD9A554D18229B1095254@KRUSTY_II.aerostructures.com.au>
References: <ECD3B76DBDD151439FD9A554D18229B1095254@KRUSTY_II.aerostructures.com.au>
Message-ID: <x2fz20v85p.fsf@biostat.ku.dk>

"Paul Livingstone" <paul.livingstone at aerostructures.com.au> writes:

> The alternative model, Y^lambda = a + bX + e, has been explored
> before by non-statistician colleagues. But instead of using boxcox
> and maximising the profile likelihood, the model has been twisted,
> shuffled, differenced and logged, to get
> 
> ln(dY/dX) = A + B.ln(Y) + E
> 
> and lambda ( =f(B) ) estimated via LS regression. Note: RHS contains
> Y, not X. This relationship has some physical justification.
> 
> 
> I assume that these two approaches are not equivalent, is this correct?  

Correct.

> I assume the Box Cox approach (profile likelihood) is better, is
> this correct and why?

This is sort of similar to the issue of output least squares vs.
system least squares in inverse problems theory. 

If what you have is a relation between Y and x and (only) Y is
measured with errors, you'd be getting a bias towards zero in the
estimated B by using the "shuffled" equation.

Then again, it's not really obvious that Box-Cox is right either
because it mixes up the functional relation and the error
chacteristics. Y^lambda should be linear in X _and_ have normally
distributed errors with a constant variance. You might need one lambda
to linearize and another to stabilize the variance.
 
If the errors really enter at the systems level (you have a stochastic
differential equation), it's a different story altogether!
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gblevins at mn.rr.com  Tue Dec 21 01:03:30 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Mon, 20 Dec 2004 18:03:30 -0600
Subject: [R] How to display each symbol in a different color using plot with
	summary.formula.reverse
Message-ID: <014d01c4e6f0$8156b7a0$aaca5e18@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041220/2efbc74d/attachment.pl

From josh8912 at yahoo.com  Tue Dec 21 01:31:55 2004
From: josh8912 at yahoo.com (JJ)
Date: Mon, 20 Dec 2004 16:31:55 -0800 (PST)
Subject: [R] question on  using "warning.expression"
Message-ID: <20041221003155.20287.qmail@web51704.mail.yahoo.com>

Hello:
I would like to flag certain warnings to make them
into errors.  I thought I could do this by giving
warning.expression a function to evaluate, in which I
could check the warning message and call stop if I
want.

I tried this:

options(warning.expression=expression(myfunction()))

but it does not appear to actually call myfunction. 
The function myfunction instructs that if the warning
has a certain messsage, then stop() is called.  

Can anyone help with the proper syntax for
warning.expression?  Or is there a better way to do
what I want?

Thanks,
John



From josh8912 at yahoo.com  Tue Dec 21 03:46:18 2004
From: josh8912 at yahoo.com (JJ)
Date: Mon, 20 Dec 2004 18:46:18 -0800 (PST)
Subject: [R] hang-up during nlme call
Message-ID: <20041221024618.75333.qmail@web51707.mail.yahoo.com>

Hello:
I recently asked the list a question regarding
warning.expression.  This is a different statement of
the problem.  

I am doing a large simulation experiment using nlme
and most of the data realizations run fine.  The
simulations stall, however, on a few particularly
noisy data sets.  What happens is that the nlme
function initially calls the nlm function and some
code in there generates an endless number of warning
messages.  These are: "Warning: Singular precision
matrix in level ...".  Becuse of this, my simulation
is caught in an endless loop and after quite a while R
crashes.

I do use try(nlme(...)), so if I could find a way to
flag these warning and make them into errors, such as
by calling stop(), then my code could move on to the
next realizaton of the simulation.  Some warnings are
produced that are not endless or otherwise
problematic, so I do not want to cause an error for
every warning.  

Does anyone have any ideas as to how to identify
warning messages and call stop() or otherwise induce
an error if the warnings are of a particular type?  

John



From gerifalte28 at hotmail.com  Tue Dec 21 03:54:19 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Tue, 21 Dec 2004 02:54:19 +0000
Subject: [R] How to display each symbol in a different color using plot
	withsummary.formula.reverse
In-Reply-To: <014d01c4e6f0$8156b7a0$aaca5e18@glblpyirxqz5lp>
Message-ID: <BAY103-F4048571CFD31A2AF9DCF12A6A30@phx.gbl>

Hi Greg

Try the argument "col" within plot.  Using your example you could try:
s <- summary(n10 ~ n13, method="reverse", test=T)
col <- c("black", "maroon", red")
plot(s, dotsize=1.2, col =col[YourData[,n10]] ,cex.labels=.7, cex.axis=.5, 
cex.main=.5, which="categorical")

I hope that this helps

Francisco

PS: In the future try to show your data structure and levels in the example, 
it makes it easier for us to understand what you want to do.


>From: "Greg Blevins" <gblevins at mn.rr.com>
>To: "R-Help" <r-help at stat.math.ethz.ch>
>Subject: [R] How to display each symbol in a different color using plot 
>withsummary.formula.reverse
>Date: Mon, 20 Dec 2004 18:03:30 -0600
>
>Dear R Masters,
>
>I have searched high and low (the help archives and my various R reference 
>material and help files) for a solution to what appears to me to be quite a 
>simple problem.  In the following syntax, variable n10 has three levels.  I 
>would like the symbols that appear in the graph for these three levels to 
>be different colors.  The best I have been able to do is to have the Key 
>display three colors, but the symbols in the graph only show up in black 
>and white.  Any suggestions?
>
>par(cex=.8)
>s <- summary(n10 ~ n13, method="reverse", test=T)
>plot(s, dotsize=1.2, cex.labels=.7, cex.axis=.5, cex.main=.5, 
>which="categorical")
>Key(locator(1))
>
>R version 2.0.1
>Windows XP
>
>Thank you,
>
>Greg Blevins
>The Market Solutions Group
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Tue Dec 21 05:47:26 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 21 Dec 2004 04:47:26 +0000 (UTC)
Subject: [R] hang-up during nlme call
References: <20041221024618.75333.qmail@web51707.mail.yahoo.com>
Message-ID: <loom.20041221T054425-879@post.gmane.org>

JJ <josh8912 <at> yahoo.com> writes:

: 
: Hello:
: I recently asked the list a question regarding
: warning.expression.  This is a different statement of
: the problem.  
: 
: I am doing a large simulation experiment using nlme
: and most of the data realizations run fine.  The
: simulations stall, however, on a few particularly
: noisy data sets.  What happens is that the nlme
: function initially calls the nlm function and some
: code in there generates an endless number of warning
: messages.  These are: "Warning: Singular precision
: matrix in level ...".  Becuse of this, my simulation
: is caught in an endless loop and after quite a while R
: crashes.
: 
: I do use try(nlme(...)), so if I could find a way to
: flag these warning and make them into errors, such as
: by calling stop(), then my code could move on to the
: next realizaton of the simulation.  Some warnings are
: produced that are not endless or otherwise
: problematic, so I do not want to cause an error for
: every warning.  
: 
: Does anyone have any ideas as to how to identify
: warning messages and call stop() or otherwise induce
: an error if the warnings are of a particular type?  

See ?withCallingHandlers .  Here is an example which raises an
error for an "X" warning but not other sorts of warnings.  

f <- function(x) if (x$message == "X") stop("X")
withCallingHandlers({
	print(10)
	warning("Y")  # this warning does not result in stop
	print(20)
	warning("X")  # this warning results in stop
	print(30)
}, warning = f)



From r.hankin at soc.soton.ac.uk  Tue Dec 21 10:01:32 2004
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 21 Dec 2004 09:01:32 +0000
Subject: [R] Im(z) <-
Message-ID: <E8B7F120-532E-11D9-946F-000A95D86AA8@soc.soton.ac.uk>


Hello everybody

If

R> a <- 1i+(1:10)

I would like to say

R> Im(a) <- 1:10
R> a
  [1]  1+ 1i  2+ 2i  3+ 3i  4+ 4i  5+ 5i  6+ 6i  7+ 7i  8+ 8i  9+ 9i 
10+10i


But there does not seem to be a "Im<-" function.

How about this:

"Im<-" <- function(x,value){
   if(is.complex(value)){stop("value must be real")}
   if(all(value==0)){
     return(Re(x))
   } else {
     return(Re(x)+1i*value)
   }
}



comments anyone?  can I infer that the above is a bad idea from the
absence of a "Im<-"() function in R?



--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From xmeng at capitalbio.com  Tue Dec 21 10:45:25 2004
From: xmeng at capitalbio.com (=?gb2312?B?w8/QwA==?=)
Date: Tue, 21 Dec 2004 17:45:25 +0800
Subject: [R] about colnames
Message-ID: <303622325.19594@capitalbio.com>

Hello sir:
If there's a data frame(with name "df"):
a  b  c d  e   f 
1 10 12 20 30  100
2 3  15 16 40  200
..

If I wanna change the last 3 colunm names"d" "e" "f" respectively into "x" "y" "z"respectively,the following is what I do:
colnames(df[4:6])<-c("x","y","z")
But no change to the colnames of df.



And if I wanna change all the colnames:
colnames(df)<-letters[1:6]
All the colnames have been changed.

So I wanna know the reason,and how to change the last 3 colnames of df.

Thanks a lot!



From michael.watson at bbsrc.ac.uk  Tue Dec 21 10:23:33 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 21 Dec 2004 09:23:33 -0000
Subject: [R] about colnames
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89A58@iahce2knas1.iah.bbsrc.reserved>

Try

colnames(df)[4:6] <- c("x","y","z")

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
xmeng at capitalbio.com
Sent: 21 December 2004 09:45
To: r-help at stat.math.ethz.ch
Subject: [R] about colnames


Hello sir:
If there's a data frame(with name "df"):
a  b  c d  e   f 
1 10 12 20 30  100
2 3  15 16 40  200
..

If I wanna change the last 3 colunm names"d" "e" "f" respectively into
"x" "y" "z"respectively,the following is what I do:
colnames(df[4:6])<-c("x","y","z")
But no change to the colnames of df.



And if I wanna change all the colnames: colnames(df)<-letters[1:6] All
the colnames have been changed.

So I wanna know the reason,and how to change the last 3 colnames of df.

Thanks a lot!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From nicolas.deig at epfl.ch  Tue Dec 21 11:00:54 2004
From: nicolas.deig at epfl.ch (NICOLAS DEIG)
Date: Tue, 21 Dec 2004 11:00:54 +0100
Subject: [R] (no subject)
Message-ID: <cb166d8f.6d8fcb16@imap.epfl.ch>

hello,

I am encoutering problems with a function of R. 
The function is for classification trees. 

I am working on datas of this kind:

       X1    X2    X3    X4    X5   class
1   2.092 1.902 2.779 2.944 1.946       1

for 200 observations and 4 differents classes.

What i would like to do is grow a tree with the function "tree" and then
use the result in the function "cv.tree" in order to ran cross
validation experiment. 
> library(tree)
> Z<-tree(V1~X1,data)
> W<-cv.tree(Z)
Error in as.data.frame.default(data) : can't coerce function into a
data.frame


There is the problem, "cv.tree" won t consider the resluts of function
"tree" as an object of class tree, it just considers this object as a
data frame. 
Can anyone eyplain me why or already encoutered this kind of problem?

THanks in advance,
Nicolas



From d.firth at warwick.ac.uk  Tue Dec 21 11:23:55 2004
From: d.firth at warwick.ac.uk (David Firth)
Date: Tue, 21 Dec 2004 10:23:55 +0000
Subject: [R] link to a vignette/overview document in .Rd file?
Message-ID: <6B6561FB-533A-11D9-97EB-000A95A6625E@warwick.ac.uk>

Is there a reliable way to link to a vignette or other overview 
document, which has been included in the "inst/doc" directory of a 
source package, within a package help file in Rd format?

or maybe link to an index of such documents?

David

Professor David Firth
Dept of Statistics
University of Warwick
Coventry CV4 7AL
United Kingdom

Voice: +44 (0)247 657 2581
Fax:   +44 (0)247 652 4532
Web:   http://www.warwick.ac.uk/go/dfirth



From numero.primo at tele2.it  Tue Dec 21 11:51:37 2004
From: numero.primo at tele2.it (Landini Massimiliano)
Date: Tue, 21 Dec 2004 11:51:37 +0100
Subject: [R] muliple plots with pairs (matrix of scatter plots)
In-Reply-To: <41C764A0.9040007@stat.auckland.ac.nz>
References: <p06100501bdebd4eff241@[83.132.29.100]>
	<41C6BE1C.7060603@statistik.uni-dortmund.de>
	<41C764A0.9040007@stat.auckland.ac.nz>
Message-ID: <vuvfs0talmkocbkkhfesn0s510bo5e2vrr@4ax.com>

On Tue, 21 Dec 2004 12:47:44 +1300, you wrote:

|=[:o)  Hi
|=[:o)  
|=[:o)  
|=[:o)  Uwe Ligges wrote:
|=[:o)  > Tiago R Magalhaes wrote:
|=[:o)  > 
|=[:o)  >> I am trying to make a graph with 4 scatter matrixes plots and couldn't 
|=[:o)  >> do it. While trying to find a solution for this I also came across the 
|=[:o)  >> idea of giving different values to the same argument for each of the 
|=[:o)  >> lower and upper function but couldn't do it. (Examplified below with 
|=[:o)  >> the col argument). The first problem of plotting 4 scatter matrixes in 
|=[:o)  >> a graph is a problem of real interest for me at this point. The second 
|=[:o)  >> problem is a matter of curiosity.
|=[:o)  >>
|=[:o)  >> I am using a Mac PowerBook G4 with OS 10.3.7 and R 2.0.1
|=[:o)  >>
|=[:o)  >>
|=[:o)  >> Problem 1)
|=[:o)  >> x=data.frame(a=sample(1:100, 50), b=sample(1:100, 50),c=sample(1:100, 
|=[:o)  >> 50),d=sample(1:100, 50))
|=[:o)  >> x.list=vector('list',4)
|=[:o)  >> for (j in 1:4) x.list[[j]]=x
|=[:o)  >>
|=[:o)  >> #produces a graph with four plots:
|=[:o)  >> layout(matrix(c(1,3,2,4),2,2))
|=[:o)  >> for (j in seq(x)){
|=[:o)  >>     plot(x.list[[j]][1:2])
|=[:o)  >>     }
|=[:o)  >>
|=[:o)  >> # But unfortunately the following produces a new plot everytime:
|=[:o)  >> layout(matrix(c(1,3,2,4),2,2))
|=[:o)  >> for (j in seq(x)){

	par(new=TRUE) 

|=[:o)  >>     pairs(x.list[[j]])
|=[:o)  >>     }
|=[:o)  >> #Maybe pairs can't be used to produce a graph with multiple plots?
|=[:o)  > 
|=[:o)  > 
|=[:o)  > Yes, it uses similar constructs to put multiple plots together.
|=[:o)  > 
|=[:o)  > You might want to use packages grid and gridBase to set something up 
|=[:o)  > using viewports.
|=[:o)  
|=[:o)  
|=[:o)  
|=[:o)  I don't think that's going to work either -- pairs() makes some pretty 
|=[:o)  strong assumptions that it is the only plot on the page.
|=[:o)  
|=[:o)  One possible way to go is to use splom() instead from the lattice 
|=[:o)  package.  For example (using your data from above) ...
|=[:o)  
|=[:o)  splom(~ x)
|=[:o)  
|=[:o)  ... and lattice plots can be embedded in grid viewports easily, for 
|=[:o)  example ...
|=[:o)  
|=[:o)  grid.newpage()
|=[:o)  pushViewport(viewport(layout=grid.layout(2, 2)))
|=[:o)  for (j in seq(x)) {
|=[:o)     row <- (j - 1) %/% 2 + 1
|=[:o)     col <- (j - 1) %% 2 + 1
|=[:o)     pushViewport(viewport(layout.pos.col=col,
|=[:o)                           layout.pos.row=row))
|=[:o)     print(splom(~ x.list[[j]]), newpage=FALSE)
|=[:o)     popViewport()
|=[:o)  }
|=[:o)  popViewport()
|=[:o)  
|=[:o)  ... you may need to fiddle with the splom() args to get them looking how 
|=[:o)  you want them.
|=[:o)  
|=[:o)  Paul


-------------------------------------------------------------------------------------------------------------------------
Landini dr. Massimiliano
Tel. mob. (+39) 347 140 11 94
Tel./Fax. (+39) 051 762 196
e-mail: numero (dot) primo (at) tele2 (dot) it
-------------------------------------------------------------------------------------------------------------------------
Legge di Hanggi: Pi?? stupida ?? la tua ricerca, pi?? verr?? letta e approvata.
Corollario alla Legge di Hanggi: Pi?? importante ?? la tua ricerca, meno verr??
capita.



From Rau at demogr.mpg.de  Tue Dec 21 12:00:44 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 21 Dec 2004 12:00:44 +0100
Subject: [R] No Graphics Window, Mandrake 10.1
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6150589@HERMES.demogr.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041221/cb883627/attachment.pl

From Rau at demogr.mpg.de  Tue Dec 21 12:01:24 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 21 Dec 2004 12:01:24 +0100
Subject: [R] Increased execution speed of R2.0.1?
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E615058A@HERMES.demogr.mpg.de>

Dear R-Community, 

mkondrin at hppi.troitsk.ru wrote:

> tree). There used to be an alpha verision of byte-compiler for R - is
it
> now included into "official" version of R?

Is this byte-compiler publicly available (even if it is "only" an
alpha-version)?

Thanks,
Roland

I was searching the R-help archives. The only references on
byte-compilation referred to (a) the Emacs package ESS and (b) to XLisp.


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From michael.watson at bbsrc.ac.uk  Tue Dec 21 12:05:56 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 21 Dec 2004 11:05:56 -0000
Subject: [R] Creating a vector of colours that are as different from one
	another as possible
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89A5F@iahce2knas1.iah.bbsrc.reserved>

Hi

I want to create a vector of colors that are as different from one
another as possible.  ?rainbow states "Conceptually, all of these
functions actually use (parts of) a line cut out of the 3-dimensional
color space...".  This suggests to me that the resulting colors are all
placed on this "line" and are equi-distant along it.  The resulting
color palette is a range of colours where adjacent colours are actually
quite similar, especially when n (the number of colours) is high.

Conceptually I guess what I want is colors from a 3D polygon in 3D
colour space, where the number of vertices in the polygon is n,
resulting in a color palette where the colors are all quite different
from one another.  Is this possible or am I talking crap? (I've only had
one coffee this morning)

Thanks in advance
Mick



From mkondrin at hppi.troitsk.ru  Tue Dec 21 23:23:12 2004
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Tue, 21 Dec 2004 14:23:12 -0800
Subject: [R] Increased execution speed of R2.0.1?
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E615058A@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E615058A@HERMES.demogr.mpg.de>
Message-ID: <41C8A250.4090901@hppi.troitsk.ru>

Rau, Roland wrote:

>Is this byte-compiler publicly available (even if it is "only" an
>alpha-version)?
>
>  
>
Hello!
Compiler is on the home page of Luke Tierney
http://www.stat.uiowa.edu/~luke/R/compiler/
Good luck!



From Philippe.Hupe at curie.fr  Tue Dec 21 12:46:00 2004
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Tue, 21 Dec 2004 12:46:00 +0100
Subject: [R] aggregate and median
Message-ID: <41C80CF8.7050000@curie.fr>

I am trying to use the function aggregate with the median function but I 
get the following error:

Error in FUN(X[[1]], ...) : Argument "INDEX"

When I replace median by mean, it works perfectly

Can someone tell me where the problem comes from?

Thx


I am running R 2.0.0 on SunOS  5.9

-- 
Philippe Hup??
UMR 144 - Service Bioinformatique
Institut Curie
Laboratoire de Transfert (4??me ??tage)
26 rue d'Ulm
75005 Paris - France
 	
Email :  Philippe.Hupe at curie.fr
T??l :	 +33 (0)1 44 32 42 75
Fax :  	 +33 (0)1 42 34 65 28



From andy_liaw at merck.com  Tue Dec 21 13:50:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 21 Dec 2004 07:50:43 -0500
Subject: [R] No Graphics Window, Mandrake 10.1
Message-ID: <3A822319EB35174CA3714066D590DCD50994E45A@usrymx25.merck.com>

You might need to install devel package(s) for X, which contain header files
that are needed.  When configure finished, do you see `X11' among the final
output?  If not, that may very well be the problem.

Andy

> From: Rau, Roland
> 
> Dear all,
> 
> I installed Mandrakelinux 10.1 (community) on my notebook. Before, I
> used Mandrakelinux 9.1 (and later 10.0 as an update).
> Now I wanted to compile, as before, R.
> In the beginning, nothing worked but it seemed to be the 
> problem of GCC
> 3.4.1 which was shipped with my distribution. After downloading and
> compiling GCC 3.4.3 everything went smoothly during the compilation of
> R.
> 
> The problem I have now is:
> if I want to plot something, there is no window graphics popping up.
> For example if I write
> plot(1,1)
> nothing happens.
> 
> Then I tried:
> X11()
> which resulted in:
> Error in X11(): X11 is not available
> 
> Did somebody have similar problems? I guess there is nothing 
> wrong with
> R but with my installation because I tried:
> - R-2.0.1.tar.gz
> - R-devel_2004-12.20.tar.gz
> - R-2.0.0-1mdk.i586.rpm
> All of them behaved completely the same in the respect.
> 
> I don't know whether it is required or not during the 
> ./configure
> step to specify
> ./configure --with-x
> I tried with and without but it did not change anything.
> 
> I also did 
> make check
> after 'make' and before 'make install' but I could not detect anything
> for this problem.
> 
> Any hints and suggestions are appreciated.
> 
> Thanks,
> Roland
> 
> The last version I tried was the following version:
> > version
>          _                           
> platform i686-pc-linux-gnu           
> arch     i686                        
> os       linux-gnu                   
> system   i686, linux-gnu             
> status   Under development (unstable)
> major    2                           
> minor    1.0                         
> year     2004                        
> month    12                          
> day      20                          
> language R                           
> > 
> 
> 
> 
> +++++
> This mail has been sent through the MPI for Demographic 
> Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From haynesm at cfr.nichd.nih.gov  Tue Dec 21 14:09:43 2004
From: haynesm at cfr.nichd.nih.gov (Haynes, Maurice (NIH/NICHD))
Date: Tue, 21 Dec 2004 08:09:43 -0500
Subject: [R] R code for var-cov matrix given variances and correlations
Message-ID: <6000BB14AFA9A741BC2315A598837ED5166F4B30@nihexchange4.nih.gov>

Dear list members,

Where can I find code for computing the p*p variance-covariance 
matrix given a vector of p variances (ordered varA, varB, ..., 
varp) and a vector of all possible correlations (ordered corAB, 
corAC, ..., corp-1,p)?

I know that the covariance between 2 variables is equal to the 
product of their correlation and their standard deviations: 
corAB * varA^.5 * varB^.5
and so:
covAB <- function(corAB, varA, varB) {
    corAB * varA^.5 * varB^.5
    }

If the vector of variances were
var.vec <- c(14, 12, 7) 
and the vector of correlations were 
cor.vec <- c(.4, .2, .5), 
then the vector of covariances would be:
> covAB(c(.4, .2, .5),c(14, 14, 12), c(12, 7, 7))
[1] 5.184593 1.979899 4.582576
>
and the variance-covariance matrix with covariances rounded to 
the first decimal place would be:
> vmat <- matrix(c(14, 5.2, 2.0, 5.2, 12, 4.6, 2.0, 4.6, 7),
+ nrow=3)
> vmat
     [,1] [,2] [,3]
[1,] 14.0  5.2  2.0
[2,]  5.2 12.0  4.6
[3,]  2.0  4.6  7.0
> 

So the question is: How can I generate a p*p variance-covariance
matrix from a vector of variances and a vector of correlations
without resorting to a construction like:
    vmat <- matrix(rep(0, p*p), nrow=p)
    if (p == 2) {
        vmat[1,1] <- var[1]
        vmat[1,2] <- cor[1] * (var[1]^.5 * var[2]^.5)
        vmat[2,1] <- cor[1] * (var[2]^.5 * var[1]^.5)
        vmat[2,2] <- var[2]
        }
    if (p == 3) {
        vmat[1,1] <- var[1]
        vmat[1,2] <- cor[1] * (var[1]^.5 * var[2]^.5)
        vmat[1,3] <- cor[2] * (var[1]^.5 * var[3]^.5)
        vmat[2,1] <- cor[1] * (var[2]^.5 * var[1]^.5)
        vmat[2,2] <- var[2]
        vmat[2,3] <- cor[3] * (var[2]^.5 * var[3]^.5)
        vmat[3,1] <- cor[2] * (var[3]^.5 * var[1]^.5)
        vmat[3,2] <- cor[3] * (var[3]^.5 * var[2]^.5)
        vmat[3,3] <- var[3]
        }
and so forth?

Thanks,

Maurice Haynes
National Institute of Child Health and Human Development
Child and Family Research Section
6705 Rockledge Drive, Suite 8030
Bethesda, MD  20892
Voice: 301-496-8180
Fax: 301-496-2766
E-mail: mh192j at nih.gov



From petr.pikal at precheza.cz  Tue Dec 21 14:10:51 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 21 Dec 2004 14:10:51 +0100
Subject: [R] aggregate and median
In-Reply-To: <41C80CF8.7050000@curie.fr>
Message-ID: <41C82EEB.15211.1522771@localhost>



On 21 Dec 2004 at 12:46, Philippe Hup?? wrote:

> I am trying to use the function aggregate with the median function but
> I get the following error:
> 
> Error in FUN(X[[1]], ...) : Argument "INDEX"
> 
> When I replace median by mean, it works perfectly

Hi Philippe

I suppose that you have some typo in your aggregate construction 
or you redefined median.

> aggregate(x, list(rrr), mean)
   Group.1           x
1        1 -0.19455580
2        2 -0.06877719
3        3 -0.47657192
4        4 -0.41082682
5        5  1.27739323
6        6  1.15004620
7        7 -0.40064292
8        8 -0.02360514
9        9 -0.24954037
10      10  0.13480356
11      11  0.24179472
> aggregate(x, list(rrr), median)
   Group.1           x
1        1 -0.19455580
2        2 -0.06877719
3        3 -0.47657192
4        4 -0.41082682
5        5  1.27739323
6        6  1.15004620
7        7 -0.40064292
8        8 -0.02360514
9        9 -0.24954037
10      10  0.13480356
11      11  0.24179472

Works for me as supposed. Or do you do something completely 
defferent?

Cheers
Petr

> 
> Can someone tell me where the problem comes from?
> 
> Thx
> 
> 
> I am running R 2.0.0 on SunOS  5.9
> 
> -- 
> Philippe Hup??
> UMR 144 - Service Bioinformatique
> Institut Curie
> Laboratoire de Transfert (4??me ??tage)
> 26 rue d'Ulm
> 75005 Paris - France
> 
> Email :  Philippe.Hupe at curie.fr
> T??l :	 +33 (0)1 44 32 42 75
> Fax :  	 +33 (0)1 42 34 65 28
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec 21 14:02:10 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 21 Dec 2004 13:02:10 -0000 (GMT)
Subject: [R] Creating a vector of colours that are as different from 
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A5F@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <XFMail.041221130210.Ted.Harding@nessie.mcc.ac.uk>

On 21-Dec-04 michael watson \(IAH-C\) wrote:
> Hi
> 
> I want to create a vector of colors that are as different
> from one another as possible.  ?rainbow states "Conceptually,
> all of these functions actually use (parts of) a line cut out
> of the 3-dimensional color space...".  This suggests to me
> that the resulting colors are all placed on this "line" and
> are equi-distant along it.  The resulting color palette is
> a range of colours where adjacent colours are actually quite
> similar, especially when n (the number of colours) is high.
> 
> Conceptually I guess what I want is colors from a 3D polygon
> in 3D colour space, where the number of vertices in the polygon
> is n, resulting in a color palette where the colors are all
> quite different from one another.  Is this possible or am I
> talking crap? (I've only had one coffee this morning)

One is not enough, by a long way, in my experience ...

How large is n? It's not easy to select more than a few clearly
distinct colours. Also, "distinct" is context-dependent, because:

What will be the spatial relationships of the different colours
in your output? You can successfully have fairly similar
colours adjacent to each other, since the contrast is more
obvious when they're adjacent. However, if you want to use
colours to track identity and difference across scttered points
or patches, then you need bigger separations between colours,
since you want to be able to see easily that patch "A" here is
of the same kind as patch "A" there and different from patch "B"
somwehere else, when mingled with patches of other kinds.

And size matters. Big patches of similar colour (as on a map)
can look quite distinct, while the same colours used to plot
filled circular blobs on a graph might be barely distinguishable,
and totally undistinguishable if used to plot coloured "."s
or "+"s.

It depends too on what you will be using to render the colours.
Monitor screens vary in their aility to render different
colours distinctly, and so do colour printers.

It's all very psycho-visual and success usually requires
experimentation!

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 21-Dec-04                                       Time: 13:02:10
------------------------------ XFMail ------------------------------



From r.ghezzo at staff.mcgill.ca  Tue Dec 21 14:18:05 2004
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Tue, 21 Dec 2004 08:18:05 -0500
Subject: [R] help with limma
Message-ID: <1103635085.41c8228d7c1cb@webmail.mcgill.ca>

Follow up on my previous e-mail
I am using Affys nzwC etc. are single columns vectors length 12000
then nzw,akr,bas are 12000 by 6 matrices
Thanks again for any help, now I resend the e-mail to Gordon with the correct
address I hope.
Heberto Ghezzo
McGill - Canada



From f.harrell at vanderbilt.edu  Tue Dec 21 14:25:42 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 21 Dec 2004 07:25:42 -0600
Subject: [R] How to display each symbol in a different color using plot
	with	summary.formula.reverse
In-Reply-To: <014d01c4e6f0$8156b7a0$aaca5e18@glblpyirxqz5lp>
References: <014d01c4e6f0$8156b7a0$aaca5e18@glblpyirxqz5lp>
Message-ID: <41C82456.5010407@vanderbilt.edu>

Greg Blevins wrote:
> Dear R Masters,
> 
> I have searched high and low (the help archives and my various R reference material and help files) for a solution to what appears to me to be quite a simple problem.  In the following syntax, variable n10 has three levels.  I would like the symbols that appear in the graph for these three levels to be different colors.  The best I have been able to do is to have the Key display three colors, but the symbols in the graph only show up in black and white.  Any suggestions?
> 
> par(cex=.8)
> s <- summary(n10 ~ n13, method="reverse", test=T)
> plot(s, dotsize=1.2, cex.labels=.7, cex.axis=.5, cex.main=.5, which="categorical")
> Key(locator(1))
> 
> R version 2.0.1
> Windows XP
> 
> Thank you,
> 
> Greg Blevins
> The Market Solutions Group

Greg,

These kinds of questions are probably best directed at package maintainers.

This would be an enhancement to the dotchart2 function used by 
plot.summary.formula.reverse, and the addition of a new argument to it 
from plot....  I have found symbols (esp. circle vs. triangle) to be 
more effective for this purpose so I am not motivated to work on this 
very soon but would consider it.  -Frank



-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From p.dalgaard at biostat.ku.dk  Tue Dec 21 14:34:59 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Dec 2004 14:34:59 +0100
Subject: [R] R code for var-cov matrix given variances and correlations
In-Reply-To: <6000BB14AFA9A741BC2315A598837ED5166F4B30@nihexchange4.nih.gov>
References: <6000BB14AFA9A741BC2315A598837ED5166F4B30@nihexchange4.nih.gov>
Message-ID: <x2y8fru64c.fsf@biostat.ku.dk>

"Haynes, Maurice (NIH/NICHD)" <haynesm at cfr.nichd.nih.gov> writes:

> If the vector of variances were
> var.vec <- c(14, 12, 7) 
> and the vector of correlations were 
> cor.vec <- c(.4, .2, .5), 
> then the vector of covariances would be:
> > covAB(c(.4, .2, .5),c(14, 14, 12), c(12, 7, 7))
> [1] 5.184593 1.979899 4.582576
> >
> and the variance-covariance matrix with covariances rounded to 
> the first decimal place would be:
> > vmat <- matrix(c(14, 5.2, 2.0, 5.2, 12, 4.6, 2.0, 4.6, 7),
> + nrow=3)
> > vmat
>      [,1] [,2] [,3]
> [1,] 14.0  5.2  2.0
> [2,]  5.2 12.0  4.6
> [3,]  2.0  4.6  7.0
> > 


# First fill in the correlation matrix:

V <- matrix(NA,3,3)
diag(V) <- 1
V[lower.tri(V)] <-  c(.4, .2, .5)
V[upper.tri(V)] <- t(V)[upper.tri(V)]

# then scale rows and columns

D <- diag(sqrt( c(14, 12, 7)))
D %*% V %*% D

# or, more efficiently

s <- sqrt( c(14, 12, 7))
sweep(sweep(V,1,s,"*"),2,s,"*")




-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From petr.pikal at precheza.cz  Tue Dec 21 14:37:15 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 21 Dec 2004 14:37:15 +0100
Subject: [R] Creating a vector of colours that are as different from
	one	another as possible
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A5F@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41C8351B.15875.16A55EF@localhost>



On 21 Dec 2004 at 11:05, michael watson (IAH-C) wrote:

> Hi
> 
> I want to create a vector of colors that are as different from one
> another as possible.  ?rainbow states "Conceptually, all of these
> functions actually use (parts of) a line cut out of the 3-dimensional
> color space...".  This suggests to me that the resulting colors are
> all placed on this "line" and are equi-distant along it.  The
> resulting color palette is a range of colours where adjacent colours
> are actually quite similar, especially when n (the number of colours)
> is high.
> 
> Conceptually I guess what I want is colors from a 3D polygon in 3D
> colour space, where the number of vertices in the polygon is n,
> resulting in a color palette where the colors are all quite different
> from one another.  Is this possible or am I talking crap? (I've only
> had one coffee this morning)

Hi Micheal

With increased number of colors you always end with neighbour 
colors quite similar. If I understand the rainbow function correctly 
it sets saturation to 1 (maximum) value to 1 (maximum) and divide 
the third component hue to equally spaced intervals to get the most 
different colours from given range of hues.

You can also experiment with hsv() function

> plot(1:11, col = hsv(h = seq(0,1,.1), s=1, v=0)) #all black
> plot(1:11, col = hsv(h = seq(0,1,.1), s=1, v=1)) # different colours

You can imagine hue as a circle from 0 to 360 and if you want to 
have neighbouring colours to be the most different you have to 
choose them from oposit parts of a circle e.g. 0,180 or 90,270.

So

barvy<-rainbow(12)
vyber<-c(1,7,3,9,5,11,2,8,4,10,6,12)
plot(1:12,col=rainbow(12)[vyber])

will give you a sequence of 12 colours which are by my opinion 
most different.

Cheers
Petr


> 
> Thanks in advance
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From d.firth at warwick.ac.uk  Tue Dec 21 14:38:07 2004
From: d.firth at warwick.ac.uk (David Firth)
Date: Tue, 21 Dec 2004 13:38:07 +0000
Subject: [R] R code for var-cov matrix given variances and correlations
In-Reply-To: <6000BB14AFA9A741BC2315A598837ED5166F4B30@nihexchange4.nih.gov>
References: <6000BB14AFA9A741BC2315A598837ED5166F4B30@nihexchange4.nih.gov>
Message-ID: <8C6B585E-5355-11D9-97EB-000A95A6625E@warwick.ac.uk>

On 21 Dec, 2004, at 13:09, Haynes, Maurice (NIH/NICHD) wrote:

> Dear list members,
>
> Where can I find code for computing the p*p variance-covariance
> matrix given a vector of p variances (ordered varA, varB, ...,
> varp) and a vector of all possible correlations (ordered corAB,
> corAC, ..., corp-1,p)?

Something like this (not tested):

         sd.vec <- sqrt(var.vec)
         n <- length(sd.vec)
         cor.mat <- matrix(1, n, n)
         for (i in 2:n){
             for (j in 1:i-1){
                 cor.mat[i,j] <- cor.vec[(i-1)*(i-2)/2 + j]
                 cor.mat[j,i] <- cor.mat[i,j]}}
         cov.mat <- cor.mat * outer(sd.vec, sd.vec)

I hope that helps.

David


>
> I know that the covariance between 2 variables is equal to the
> product of their correlation and their standard deviations:
> corAB * varA^.5 * varB^.5
> and so:
> covAB <- function(corAB, varA, varB) {
>     corAB * varA^.5 * varB^.5
>     }
>
> If the vector of variances were
> var.vec <- c(14, 12, 7)
> and the vector of correlations were
> cor.vec <- c(.4, .2, .5),
> then the vector of covariances would be:
>> covAB(c(.4, .2, .5),c(14, 14, 12), c(12, 7, 7))
> [1] 5.184593 1.979899 4.582576
>>
> and the variance-covariance matrix with covariances rounded to
> the first decimal place would be:
>> vmat <- matrix(c(14, 5.2, 2.0, 5.2, 12, 4.6, 2.0, 4.6, 7),
> + nrow=3)
>> vmat
>      [,1] [,2] [,3]
> [1,] 14.0  5.2  2.0
> [2,]  5.2 12.0  4.6
> [3,]  2.0  4.6  7.0
>>
>
> So the question is: How can I generate a p*p variance-covariance
> matrix from a vector of variances and a vector of correlations
> without resorting to a construction like:
>     vmat <- matrix(rep(0, p*p), nrow=p)
>     if (p == 2) {
>         vmat[1,1] <- var[1]
>         vmat[1,2] <- cor[1] * (var[1]^.5 * var[2]^.5)
>         vmat[2,1] <- cor[1] * (var[2]^.5 * var[1]^.5)
>         vmat[2,2] <- var[2]
>         }
>     if (p == 3) {
>         vmat[1,1] <- var[1]
>         vmat[1,2] <- cor[1] * (var[1]^.5 * var[2]^.5)
>         vmat[1,3] <- cor[2] * (var[1]^.5 * var[3]^.5)
>         vmat[2,1] <- cor[1] * (var[2]^.5 * var[1]^.5)
>         vmat[2,2] <- var[2]
>         vmat[2,3] <- cor[3] * (var[2]^.5 * var[3]^.5)
>         vmat[3,1] <- cor[2] * (var[3]^.5 * var[1]^.5)
>         vmat[3,2] <- cor[3] * (var[3]^.5 * var[2]^.5)
>         vmat[3,3] <- var[3]
>         }
> and so forth?
>
> Thanks,
>
> Maurice Haynes
> National Institute of Child Health and Human Development
> Child and Family Research Section
> 6705 Rockledge Drive, Suite 8030
> Bethesda, MD  20892
> Voice: 301-496-8180
> Fax: 301-496-2766
> E-mail: mh192j at nih.gov
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec 21 14:54:45 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 21 Dec 2004 13:54:45 -0000 (GMT)
Subject: [R] R code for var-cov matrix given variances and correlatio
In-Reply-To: <6000BB14AFA9A741BC2315A598837ED5166F4B30@nihexchange4.nih.gov>
Message-ID: <XFMail.041221135445.Ted.Harding@nessie.mcc.ac.uk>

On 21-Dec-04 Haynes, Maurice (NIH/NICHD) wrote:
> Dear list members,
> 
> Where can I find code for computing the p*p variance-covariance 
> matrix given a vector of p variances (ordered varA, varB, ..., 
> varp) and a vector of all possible correlations (ordered corAB, 
> corAC, ..., corp-1,p)?
> [...]

example:

> X<-cbind(rnorm(10),rnorm(10))
> cov(X)
           [,1]       [,2]
[1,] 0.90141614 0.09364018
[2,] 0.09364018 0.88476825

> cor(X)
          [,1]      [,2]
[1,] 1.0000000 0.1048540
[2,] 0.1048540 1.0000000

> vars<-apply(X,2,var)
> vars
[1] 0.9014161 0.8847682

> diag(sqrt(vars))%*%cor(X)%*%diag(sqrt(vars))
           [,1]       [,2]
[1,] 0.90141614 0.09364018
[2,] 0.09364018 0.88476825

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 21-Dec-04                                       Time: 13:54:45
------------------------------ XFMail ------------------------------



From ozric at web.de  Tue Dec 21 15:02:10 2004
From: ozric at web.de (christian schulz)
Date: Tue, 21 Dec 2004 15:02:10 +0100
Subject: [R] RMySQL_0.5.5 / SUSE9.2
Message-ID: <200412211502.11190.ozric@web.de>

Hi,

where is my mistake?

i have done follow comand-lines.

export PKG_LIBS="-L/usr/lib/myslq"
export PKG_CPPFLAGS="-I/usr/include/mysql"
R CMD INSTALL  RMySQL_0.5-5.tar.gz

i'm using R-2.0.1 , suse 9.2, 5.0.0-alpha-standard , and after a succefuel 
compilaition i get  when im type in R library(RMySQL):

Loading required package: DBI
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library 
"/usr/local/lib/R/library/RMySQL/libs/RMySQL.so":
  /usr/local/lib/R/library/RMySQL/libs/RMySQL.so: undefined symbol: 
mysql_field_count
Error in library(RMySQL) : .First.lib failed for 'RMySQL'

P.S.
Perhaps MySQL 5.0.0-alpha cause the error, but in the past it works
under linux.........?

many thanks & 
regards, christian



From dj at research.bell-labs.com  Tue Dec 21 16:05:42 2004
From: dj at research.bell-labs.com (David James)
Date: Tue, 21 Dec 2004 10:05:42 -0500
Subject: [R] RMySQL_0.5.5 / SUSE9.2
In-Reply-To: <200412211502.11190.ozric@web.de>;
	from ozric@web.de on Tue, Dec 21, 2004 at 03:02:10PM +0100
References: <200412211502.11190.ozric@web.de>
Message-ID: <20041221100542.A937@jessie.research.bell-labs.com>

Hi,

I'm not sure where is/are your mistake(s).  But you could try finding
whether you installed the MySQL shared libraries (as of 4.1.8 there
are separate RPMs for these), and/or whether you need to add to your
LD_LIBRARY_PATH the directory where these reside.

Regards,

--
David


christian schulz wrote:
> Hi,
> 
> where is my mistake?
> 
> i have done follow comand-lines.
> 
> export PKG_LIBS="-L/usr/lib/myslq"
> export PKG_CPPFLAGS="-I/usr/include/mysql"
> R CMD INSTALL  RMySQL_0.5-5.tar.gz
> 
> i'm using R-2.0.1 , suse 9.2, 5.0.0-alpha-standard , and after a succefuel 
> compilaition i get  when im type in R library(RMySQL):
> 
> Loading required package: DBI
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library 
> "/usr/local/lib/R/library/RMySQL/libs/RMySQL.so":
>   /usr/local/lib/R/library/RMySQL/libs/RMySQL.so: undefined symbol: 
> mysql_field_count
> Error in library(RMySQL) : .First.lib failed for 'RMySQL'
> 
> P.S.
> Perhaps MySQL 5.0.0-alpha cause the error, but in the past it works
> under linux.........?
> 
> many thanks & 
> regards, christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From nassar at noos.fr  Tue Dec 21 16:23:38 2004
From: nassar at noos.fr (Naji)
Date: Tue, 21 Dec 2004 16:23:38 +0100
Subject: [R] R COCOA- Mac users
Message-ID: <BDEDFE8A.6EF%nassar@noos.fr>

Hi all,

I've downloaded 
http://cran.cict.fr/bin/macosx/R-2.0.1.dmg
Installed all the packages (except Aqua) (no error message occured)

In my application folder, I find R only and I'm having the following message
when processing R :

Error in rm(rlibs) : can't remove variables from base namespace

R :
Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1
(2004-11-15), ISBN 3-900051-07-0

Is it possible to forward a check list in order to get R Cocoa Gui running?


Best wishes for the new year
Naji



From ashelton at albany.edu  Tue Dec 21 16:43:55 2004
From: ashelton at albany.edu (Anne Shelton)
Date: Tue, 21 Dec 2004 10:43:55 -0500
Subject: [R] Need Parallel R help
Message-ID: <63D883CB0A1B204EB053673DD882CB5305356281@email.albany.edu>

I have installed Parallel R on a LINUX Cluster and I am trying to initiate
the Parallel engine function in pR but I am receiving this error

	Error in namespaceExport(ns, exports) : undefined exports: fixPre1.8
	> StartPE(2)
 	chr [1:2] "/usr/local/lib/R/library/base/exec/pRBatch.R" "NULL"
	Failed to create scheduler thread.
	Failed to create threads.
	Error in StartPE(2) : Failed to initialize the parallel engine
	In addition: Warning message: 
	package methods in options("defaultPackages") was not found 
	> 
	MPI process rank 0 (n0, p23798) caught a SIGSEGV.
	>
--------------------------------------------------------------------	
	One of the processes started by mpirun has exited with a nonzero
exit
	code.  This typically indicates that the process finished in error.
	If your process did not finish in error, be sure to include a
"return
	0" or "exit(0)" in your C code before exiting the application.

	PID 23804 failed on node n0 (127.0.0.1) due to signal 15.


Any insight on this error would be helpful.



From tlumley at u.washington.edu  Tue Dec 21 17:04:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 21 Dec 2004 08:04:17 -0800 (PST)
Subject: [R] Creating a vector of colours that are as different from one
	another as possible
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A5F@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89A5F@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.A41.4.61b.0412210747030.105582@homer03.u.washington.edu>

On Tue, 21 Dec 2004, michael watson (IAH-C) wrote:
>
> Conceptually I guess what I want is colors from a 3D polygon in 3D
> colour space, where the number of vertices in the polygon is n,
> resulting in a color palette where the colors are all quite different
> from one another.  Is this possible or am I talking crap? (I've only had
> one coffee this morning)
>

It depends on whether you need the colors to be different colors or 
whether lightness and saturation differences are ok.  For example, a cube 
with edges parallel to the axes in RGB space will have quite strong 
lightness differences, and will probably have visible saturation 
differences (depending on exactly which cube it is).  Often this is a Bad 
Thing.


It's hard to get a large number of colours that are all obviously 
different.  The ColorBrewer palettes (which are optimised for map 
coloring) go up to 11, but some of these are sets of light/dark pairs. If 
you wanted small plotting symbols it would be even more difficult, since 
blue-yellow distinctions are less visible in small things and since you 
probably want higher saturation.

 	-thomas

 	-thomas



From maechler at stat.math.ethz.ch  Tue Dec 21 17:21:43 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 21 Dec 2004 17:21:43 +0100
Subject: [R] R code for var-cov matrix given variances and correlations
In-Reply-To: <x2y8fru64c.fsf@biostat.ku.dk>
References: <6000BB14AFA9A741BC2315A598837ED5166F4B30@nihexchange4.nih.gov>
	<x2y8fru64c.fsf@biostat.ku.dk>
Message-ID: <16840.19863.431686.368447@gargle.gargle.HOWL>

>>>>> "PD" == Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>>>>>     on 21 Dec 2004 14:34:59 +0100 writes:

    PD> "Haynes, Maurice (NIH/NICHD)" <haynesm at cfr.nichd.nih.gov> writes:
    >> If the vector of variances were
    >> var.vec <- c(14, 12, 7) 
    >> and the vector of correlations were 
    >> cor.vec <- c(.4, .2, .5), 
    >> then the vector of covariances would be:
    >> > covAB(c(.4, .2, .5),c(14, 14, 12), c(12, 7, 7))
    >> [1] 5.184593 1.979899 4.582576
    >> >
    >> and the variance-covariance matrix with covariances rounded to 
    >> the first decimal place would be:
    >> > vmat <- matrix(c(14, 5.2, 2.0, 5.2, 12, 4.6, 2.0, 4.6, 7),
    >> +                 nrow=3)
    >> > vmat
    >> [,1] [,2] [,3]
    >> [1,] 14.0  5.2  2.0
    >> [2,]  5.2 12.0  4.6
    >> [3,]  2.0  4.6  7.0
    >> > 


    PD> # First fill in the correlation matrix:

    PD> V <- matrix(NA,3,3)
    PD> diag(V) <- 1
    PD> V[lower.tri(V)] <-  c(.4, .2, .5)
    PD> V[upper.tri(V)] <- t(V)[upper.tri(V)]

    PD> # then scale rows and columns

    PD> D <- diag(sqrt( c(14, 12, 7)))
    PD> D %*% V %*% D

    PD> # or, more efficiently

    PD> s <- sqrt( c(14, 12, 7))
    PD> sweep(sweep(V,1,s,"*"),2,s,"*")

and others give similar answers, not as nice as the sweep one.
While I leave the exact comparisons and even more slick
alternatives to Gabor (:-),
yet another alternative is to look at 
  cov2cor() 	   
which does  Cov |--> Cor, i.e., the inverse of what the original
poster wanted.
The source of that function, e.g. in
    https://svn.r-project.org/R/trunk/src/library/stats/R/cor.R
is

cov2cor <- function(V)
{
    ## Purpose: Covariance matrix |--> Correlation matrix -- efficiently
    ## ----------------------------------------------------------------------
    ## Arguments: V: a covariance matrix (i.e. symmetric and positive definite)
    ## ----------------------------------------------------------------------
    ## Author: Martin Maechler, Date: 12 Jun 2003, 11:50
    p <- (d <- dim(V))[1]
    if(!is.numeric(V) || length(d) != 2 || p != d[2])
	stop("`V' is not a square numeric matrix")
    Is <- sqrt(1/diag(V)) # diag( 1/sigma_i )
    if(any(!is.finite(Is)))
	warning("diagonal has non-finite entries")
    r <- V # keep dimnames
    r[] <- Is * V * rep(Is, each = p)
    ##	== D %*% V %*% D  where D = diag(Is)
    r[cbind(1:p,1:p)] <- 1 # exact in diagonal
    r
}

[ Note that this is the *real source* including comments, 
  contrary to what you get when you just type   'cov2cor' in R ]

What I've wanted to expose here are the lines

    Is <- sqrt(1/diag(V)) # diag( 1/sigma_i )
    ..
    r <- V # keep dimnames
    r[] <- Is * V * rep(Is, each = p)

which shows ``how to'' compute  Diag %*% Matrix %*% Diag,
efficiently, probably a bit more even than Peter's nice double
use of sweep(.., *)  {and still keeping dimnames}.

Martin



From jmoreira at fe.up.pt  Tue Dec 21 17:44:05 2004
From: jmoreira at fe.up.pt (jmoreira@fe.up.pt)
Date: Tue, 21 Dec 2004 16:44:05 +0000
Subject: [R] erro in SVM (packsge "e1071")
In-Reply-To: <20041220224132.5bff5fa3.david.meyer@wu-wien.ac.at>
References: <20041219142307.00c8336a.david.meyer@wu-wien.ac.at>
	<013c01c4e6c5$0f238b70$5e7aa8c0@FEUPsig.fe.up.pt>
	<20041220224132.5bff5fa3.david.meyer@wu-wien.ac.at>
Message-ID: <1103647445.41c852d560c25@webmail.fe.up.pt>

With lm it works.
With ppr and nnet it gives exactly the same error as in svm.
I did:
lm(Duracao ~ ., data= orig.data[1:874,])
ppr(Duracao ~ ., nterms=3, data=orig.data[1:874,], 
       na.action=na.omit, max.terms=6, optlevel=3, 
       sm.method='supsmu', bass=5, span=3/10)
nnet(Duracao ~ ., size = 3, data=orig.data[1:874,], 
    na.action=na.omit, rang = 2, decay = 5^(2*0), 
    maxit = 4000)

Error in get(ctr, mode = "function", envir = parent.frame())(levels(x),  : 
        Orthogonal polynomials cannot be represented accurately enough for 236 
degrees of freedom

Joao

Citando David Meyer <david.meyer at wu-wien.ac.at>:

> So the error occurs during a call to model.matrix() from svm() because
> of the polynomial contrasts--do you get the same error using, e.g.,
> lm()?
> 
> best,
> David
> 
> > The way I call SVM is:
> > 
> > i <- (-2)
> > j <- 4
> > learner='svm'
> > learner.pars=list(Duracao ~ ., data=orig.data,
> >    scale=c(FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE,
> >     FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,
> >     FALSE, FALSE, FALSE),
> >    type='nu-regression', kernel='linear',
> >    cost=2^(2*i), nu=j/10)
> > learner.pars$data <- orig.data[begin.test.pos:(test.pos-1),]
> > 
> > model <- do.call(learner,learner.pars)
> > 
> > The variables begin.test.pos and test.pos are windexes for orig.data
> > and are working well. in this case begin.test.pos = 1 and test.pos =
> > 875.
> > 
> > orig.data is a data.frame where the second, third and seventh
> > parameters are numeric. The first parameter  is a date and all the
> > others are factors (some of them ordered). The ordered factors are:
> > Dia Semana (week day), DiaAno (day of the year), DiaMes (day of the
> > month), SemanaAno (week of the year) and SemanaMes (week of the
> > month). The first two lines of the orig.data data.frame are:
> >          Data       InicioViagem Duracao     DiaSemana  TipoDia
> >          EpocaEscolar 
> > DiasDesdeUltPagamento DiaAno DiaMes FluxoEntrada FluxoSaida
> > 13 2004-01-01        25056    3220        quinta-feira   feriado 
> > normal                     9                        1             1 
> > normal           fsp4
> > 9  2004-01-01        28554    2866         quinta-feira   feriado 
> > normal                     9                        1             1 
> > normal           fsp4
> >             Modelo                  Motorista SemanaAno SemanaMes
> >             Servico
> > 13    Mercedes_O530_N     10701         1             1 
> > 10597
> > 9      Mercedes_O530_N     11292         1             1 
> > 10597
> > 
> > I am using sliding window with 30 days (around 900 records) for
> > training. The error is in the svm function. May be because SVM uses
> > other functions, but it happens when I run svm.
> > 
> > Thanks a lot for the help
> > 
> > Joao
> > _______________________________________________
> > FEUP - Engineering Faculty, Porto University
> > Engineering and Industrial Management group
> > Tel.: +351 22 508 1639
> > Fax: +351 22 508 1538
> > 
> > ----- Original Message ----- 
> > From: "David Meyer" <david.meyer at wu-wien.ac.at>
> > To: <jmoreira at fe.up.pt>
> > Cc: <r-help at stat.math.ethz.ch>
> > Sent: Sunday, December 19, 2004 1:23 PM
> > Subject: Re: [R] erro in SVM (packsge "e1071")
> > 
> > 
> > > Joao:
> > >
> > > The reported error message is not from e1071.
> > > How *exactly* did you call svm()?
> > >
> > > As to the documentation of the nu parameter: yes, this is an
> > > omission, of course, nu is used in nu-regression as well; thanks for
> > > pointing this out.
> > >
> > > best,
> > > David
> > >
> > > ---------
> > >
> > > Hello,
> > >
> > > I am using SVM under e1071 package for nu-regression with 18
> > > parameters. The
> > > variables are ordered factors, factors, date or numeric datatypes. I
> > > use the
> > > linear kernel.
> > > It gives the following error that I cannot solve. I tryed debug,
> > > browser and
> > > all that stuff, but no way.
> > > The error is:
> > >
> > > Error in get(ctr, mode = "function", envir =
> > > parent.frame())(levels(x),:
> > >        Orthogonal polynomials cannot be represented accurately
> > >        enough
> > > for 236
> > > degrees of freedom
> > >
> > > I use the nu parameter. However, reading ?svm help it says
> > > "parameter needed
> > > for 'nu-classification' and 'one-classification'". Does not say
> > > anything about
> > > nu-regression. It is an omission in the ?svm help page? Or am I
> > > notundestanding something?
> > >
> > > I believe it has something to do with the calculus of the
> > > eigenvalues. Anyway
> > > how can I overpass this problem? Increasing the training data (is
> > > around 900
> > > records)?
> > >
> > > Thanks for any help
> > >
> > > Joao
> > >
> > >
> > >
> > >
> > > -- 
> > > Dr. David Meyer
> > > Department of Information Systems
> > >
> > > Vienna University of Economics and Business Administration
> > > Augasse 2-6, A-1090 Wien, Austria, Europe
> > > Fax: +43-1-313 36x746
> > > Tel: +43-1-313 36x4393
> > > HP:  http://wi.wu-wien.ac.at/~meyer/
> > > 
> > 
> > 
> 
> 
> -- 
> Dr. David Meyer
> Department of Information Systems
> 
> Vienna University of Economics and Business Administration
> Augasse 2-6, A-1090 Wien, Austria, Europe
> Fax: +43-1-313 36x746 
> Tel: +43-1-313 36x4393
> HP:  http://wi.wu-wien.ac.at/~meyer/
>



From josh8912 at yahoo.com  Tue Dec 21 19:36:14 2004
From: josh8912 at yahoo.com (JJ)
Date: Tue, 21 Dec 2004 10:36:14 -0800 (PST)
Subject: [R] hang-up during nlme call
Message-ID: <20041221183614.81103.qmail@web51703.mail.yahoo.com>

Excellent Gabor.  Thanks.  I had seen the
"withCallingHandlers" function, but I did not
understand how it was to be used.  Thanks for the
example.
John



From spencer.graves at pdf.com  Tue Dec 21 19:51:49 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 21 Dec 2004 10:51:49 -0800
Subject: [R] Creating a vector of colours that are as different from
In-Reply-To: <XFMail.041221130210.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041221130210.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41C870C5.5050202@pdf.com>

      There should be a caveat with psycho-visual experimentation:  
Tufte (1983, p. 183) says that 5-10 percent of viewers are color 
deficient or color blind.  He (pp. 153-154) argues strongly against 
"color puzzles" that look pretty but are extremely difficult to decode.  
He says, "Shades of gray provide an easily comprehended order to the 
data measures", in some cases better than "the visually more spectacular 
color".  Cleveland (1993, pp. 264-265) suggests color encoding he calls 
THVL (two hues, varying lightness).  His example ranges from 100% cyan 
(red) in steps of 20% to 20% cyan then switches to 20% magenta (blue) in 
steps of 20% to 100% magenta.  These are selected in part because they 
are fairly distinct even for people with modest color blindness. 

      Perhaps others can comment on more recent research and how this 
relates to the color brewer. 

      In a related area, I learned years ago that substantial portions 
of the public (especially those over 40) do not have eyesight corrected 
to 20-20 and can't read PowerPoint slides with type smaller than 25-28 
point, unless they are in a very small room or are otherwise right on 
top of the slides.  The painful part for me was that they would rarely 
tell me they couldn't read my slides.  I had to guess from the questions 
they asked or didn't ask. 

      hope this helps.  spencer graves

REFERENCES: 

Edward R. Tufte (1983) The Display of Quantitative Information (Chesire, 
CT:  Graphics Press) 

William S. Cleveland (1993) Visualizing Data (Murray Hill, NJ:  AT&T 
Bell Labs)

(Ted Harding) wrote:

>On 21-Dec-04 michael watson \(IAH-C\) wrote:
>  
>
>>Hi
>>
>>I want to create a vector of colors that are as different
>>from one another as possible.  ?rainbow states "Conceptually,
>>all of these functions actually use (parts of) a line cut out
>>of the 3-dimensional color space...".  This suggests to me
>>that the resulting colors are all placed on this "line" and
>>are equi-distant along it.  The resulting color palette is
>>a range of colours where adjacent colours are actually quite
>>similar, especially when n (the number of colours) is high.
>>
>>Conceptually I guess what I want is colors from a 3D polygon
>>in 3D colour space, where the number of vertices in the polygon
>>is n, resulting in a color palette where the colors are all
>>quite different from one another.  Is this possible or am I
>>talking crap? (I've only had one coffee this morning)
>>    
>>
>
>One is not enough, by a long way, in my experience ...
>
>How large is n? It's not easy to select more than a few clearly
>distinct colours. Also, "distinct" is context-dependent, because:
>
>What will be the spatial relationships of the different colours
>in your output? You can successfully have fairly similar
>colours adjacent to each other, since the contrast is more
>obvious when they're adjacent. However, if you want to use
>colours to track identity and difference across scttered points
>or patches, then you need bigger separations between colours,
>since you want to be able to see easily that patch "A" here is
>of the same kind as patch "A" there and different from patch "B"
>somwehere else, when mingled with patches of other kinds.
>
>And size matters. Big patches of similar colour (as on a map)
>can look quite distinct, while the same colours used to plot
>filled circular blobs on a graph might be barely distinguishable,
>and totally undistinguishable if used to plot coloured "."s
>or "+"s.
>
>It depends too on what you will be using to render the colours.
>Monitor screens vary in their aility to render different
>colours distinctly, and so do colour printers.
>
>It's all very psycho-visual and success usually requires
>experimentation!
>
>Cheers,
>Ted.
>
>
>--------------------------------------------------------------------
>E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
>Date: 21-Dec-04                                       Time: 13:02:10
>------------------------------ XFMail ------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From avneetaquarian at yahoo.com  Tue Dec 21 21:06:47 2004
From: avneetaquarian at yahoo.com (avneet singh)
Date: Tue, 21 Dec 2004 12:06:47 -0800 (PST)
Subject: [R] lm regression: estimate of a categorical variable without being
	broken into levels
Message-ID: <20041221200647.97407.qmail@web14922.mail.yahoo.com>

Hello:

I am new to R and am going through the growing pains,
wonder if you could help alleviate some.

I wished to find the estimate for a categorical
variable without it being broken into levels but dont
know how to.

if I use the following example:

$>data(iris)
$>g=lm(Sepal.Length~.,iris)
$>summary(g)

I get the estimate of the categorical variable
"Species" broken up into levels.

$>anova(g)

Now i get values pertaining to the variable "Species"
without the levels being broken but i do not get the
estimate.

Question: Is there a formulae which provides the
values that "summary" provides but doesnt break up the
categorical variable into levels?

Thank you


=====
There are lies, damned lies and statistics. 
~Mark Twain



From mi2kelgrum at yahoo.com  Tue Dec 21 21:17:30 2004
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Tue, 21 Dec 2004 12:17:30 -0800 (PST)
Subject: [R] scheduling R tasks under windows
Message-ID: <20041221201730.12058.qmail@web60210.mail.yahoo.com>

I'm trying to schedule R tasks in Windows Server 2003.
I can run the following from the DOS prompt without
any difficulty:

c:\Reports>c:\r\rw2001\bin\rterm.exe --no-restore
--no-save <test.R> test.out

where test.r has two lines: library(tools);
Sweave("rlr.Rnw").

When I try to run the same from the task scheduler, I
fill in the dialogue box as follows:

Run:	c:\r\rw2001\bin\rterm.exe --no-restore --no-save
<test.R> test.out

Start in:	c:\Reports

Which opens Rterm, but is preceded by "ARGUMENT
'<test.R>' __ignored__" and "ARGUMENT 'test.out'
__ignored__"

Anyone know what I'm doing wrong?

Mikkel



From Achim.Zeileis at wu-wien.ac.at  Tue Dec 21 21:42:26 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 21 Dec 2004 21:42:26 +0100
Subject: [R] lm regression: estimate of a categorical variable without
	being broken into levels
In-Reply-To: <20041221200647.97407.qmail@web14922.mail.yahoo.com>
References: <20041221200647.97407.qmail@web14922.mail.yahoo.com>
Message-ID: <20041221214226.265db3f5.Achim.Zeileis@wu-wien.ac.at>

On Tue, 21 Dec 2004 12:06:47 -0800 (PST) avneet singh wrote:

> Hello:
> 
> I am new to R and am going through the growing pains,
> wonder if you could help alleviate some.
> 
> I wished to find the estimate for a categorical
> variable without it being broken into levels but dont
> know how to.
> 
> if I use the following example:
> 
> $>data(iris)
> $>g=lm(Sepal.Length~.,iris)
> $>summary(g)
> 
> I get the estimate of the categorical variable
> "Species" broken up into levels.
> 
> $>anova(g)
> 
> Now i get values pertaining to the variable "Species"
> without the levels being broken but i do not get the
> estimate.

There is not "the estimate", there are two estimates (as reported in the
summary above), hence are 2 Df associated with Species.

> Question: Is there a formulae which provides the
> values that "summary" provides but doesnt break up the
> categorical variable into levels?

It is not at all clear to me what you really want. It is the nature of a
categorical variable to fall into different categories (aka levels). If
you want to treat it as if it were numeric, you can transform it to a
numeric variable (not that it would make much sense in this example).

It is also possible that what you are looking for are different
contrasts. See ?contrasts or the corresponding section in MASS4 (the
book).

hth,
Z

> Thank you
> 
> 
> =====
> There are lies, damned lies and statistics. 
> ~Mark Twain
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From br44114 at yahoo.com  Tue Dec 21 21:43:47 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Tue, 21 Dec 2004 12:43:47 -0800 (PST)
Subject: [R] scheduling R tasks under windows
Message-ID: <20041221204348.62758.qmail@web50303.mail.yahoo.com>

Save the command(s) in a batch (.bat) file, and then run the .bat
file from the task scheduler.


-----Original Message-----
From: Mikkel Grum [mailto:mi2kelgrum at yahoo.com]
Sent: Tuesday, December 21, 2004 3:18 PM
To: RHelp
Subject: [R] scheduling R tasks under windows


I'm trying to schedule R tasks in Windows Server 2003.
I can run the following from the DOS prompt without
any difficulty:

c:\Reports>c:\r\rw2001\bin\rterm.exe --no-restore
--no-save <test.R> test.out

where test.r has two lines: library(tools);
Sweave("rlr.Rnw").

When I try to run the same from the task scheduler, I
fill in the dialogue box as follows:

Run:	c:\r\rw2001\bin\rterm.exe --no-restore --no-save
<test.R> test.out

Start in:	c:\Reports

Which opens Rterm, but is preceded by "ARGUMENT
'<test.R>' __ignored__" and "ARGUMENT 'test.out'
__ignored__"

Anyone know what I'm doing wrong?

Mikkel

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Tue Dec 21 21:34:47 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 21 Dec 2004 16:34:47 -0400
Subject: [R] Problems with Excel (was SAS or R software)
In-Reply-To: <41C55A2F.9000200@pburns.seanet.com>
References: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088B59414@houston.tanox.net>	<41C4F5C4.2070702@optushome.com.au>
	<41C55A2F.9000200@pburns.seanet.com>
Message-ID: <41C888E7.4000308@acelerate.com>

Patrick Burns wrote:

> I'm in the process of researching problems with Excel.  The references
> given by Tim and Marc seem to lead to discussions of most of the problems
> with statistical procedures in Excel.  The executive summary is that if
> it is in Excel and it looks like statistics, then avoid it.
>
> However, there are some other issues as well.  Here are three:
>
> 1)  For those of us used to S, Excel gives a non-intuitive result for:
>
>           -2^4

Yes. However, OO Calc gives the same result as Excel (Office 2003). Are 
they trying to
heavily to mimic Excel?

>
> I wonder how many formulas are in existence that have unintended results
> due to this.
>
> 2)  When numbers in scientific notation are written to ascii (csv or 
> txt), Excel
> decides that some of the digits are unnecessary and doesn't write them 
> to the
> file.  As far as I can tell the number of significant digits that you 
> get is arbitrary
> and capricious.  Apparently Microsoft thinks of this as a feature -- 
> I'm not sure
> what they think the up-side is.  If you want all of your digits, make 
> sure that none
> of the numbers are displayed in scientific notation.  (Yes, what is 
> written depends
> on what is displayed, but it is not WYSIWYG.)

Tried this to with OO Calc. All were written with 14 significant digits. 
However, test was very
fast and needs replcation investing some more time.

>
> 3)  In the olden days if there were a blank cell in a range of cells 
> on which a
> function operated (a mean perhaps), then Excel decided that the blank 
> meant
> zero.  Microsoft changed this so that blank cells are ignored 
> (apparently a good
> thing).  However, if you reference a blank cell, it is then counted as 
> zero.
>
> Exercise:  In the first column put numbers in the first few rows, but 
> leave one
> cell blank.  below that do the average of the cells (including the 
> blank).  Now
> in cell B1 put "=A1"  and copy this formula down column B.  You will 
> get two
> different means.
>
Yes. Done, but also on OP Calc with *exactly* the same result. 

Does one need to investigate OO Calc in the same way as has been done
with Excel?

Kjetil

>
> Patrick Burns
>
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Tim Churches wrote:
>
>> Shawn Way wrote:
>>
>>> I've seen multiple comments about MS Excel's precision and accuracy.
>>> Can you please point me in the right direction in locating information
>>> about these?
>>>  
>>>
>> As always, Google is your friend, but see for example 
>> http://www.nwpho.org.uk/sadb/Poisson%20CI%20in%20spreadsheets.pdf
>>
>> Tim C
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Tue Dec 21 21:53:37 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 21 Dec 2004 16:53:37 -0400
Subject: [R] PBIB datataset
In-Reply-To: <XFMail.041220212907.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041220212907.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41C88D51.2000005@acelerate.com>

(Ted Harding) wrote:

>On 20-Dec-04 Douglas Bates wrote:
>  
>
>>This is a new version of SASmixed that was uploaded a couple of days 
>>ago.  I changed it so that the fits are done with the lme4 version of 
>>lme.  It should be faster and more reliable than the version of lme in 
>>the nlme package.
>>
>>This version of SASmixed has a vignette that provides comparative 
>>analyses in lme for the examples in "SAS System for Mixed Models".  The
>>specification of models in the new lme is occasionally different from 
>>the older specification.  Don't pay too much attention to the textual 
>>descriptions - look at the examples in the appendices.  I haven't 
>>finished rewriting the textual description from an old, old version.
>>
>>Those who (like me) cringe at the way that models with crossed random 
>>effects needed to be specified in the old lme may find it interesting 
>>that the Demand example now specifies the model fit as
>>Demand> fm1Demand <- lme(log(d) ~ log(y) + log(rd) + log(rt) +
>>     log(rs), data = Demand, random = list(State = ~1, Year = ~1))
>>
>>[...]
>>
>>The lme4 and nlme packages should not be loaded simultaneously.
>>Use one or the other but not both.
>>    
>>
>
>Doug,
>Thanks a lot for these clarifications. This still leaves me with
>a question or two.
>
>Suppose I want (as I do) to work through Pinheiro & Bates, example
>by example. Some datasets are here (lme4), others there (SASmixed,
>nlme), etc.
>
>Does this mean I need to start afresh, loading just the package
>with the dataset currently of interest?
>  
>
I think it is enough to do
library(nlme)
<much work>
detach(pos=2)
library(lme4)
.
.
.
Is that right?

Kjetil

>I'm a touch confused about the distribution (a) of datasets,
>(b) of the R functions needed to analyse them according to
>Pinheiro & Bates, over the packages! If I load the package
>containing dataset A, will it include the functions used in
>P&B for that dataset? Or might I need to load another one
>as well? and, if so, might I encounter the sort of clash
>you hint at above?
>
>(I know I could probably check it in detail in each case as
>it arises, but I guess you know your way arond all this better
>than someone who hasn't been there yet).
>
>With thanks (and congratulations on what looks like a very
>well conceived book on the topic),
>Ted.
>
>
>--------------------------------------------------------------------
>E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
>Date: 20-Dec-04                                       Time: 21:29:07
>------------------------------ XFMail ------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From nxt7 at psu.edu  Tue Dec 21 22:47:33 2004
From: nxt7 at psu.edu (NATALIA F TCHETCHERINA)
Date: Tue, 21 Dec 2004 16:47:33 -0500 (EST)
Subject: [R] (no subject)
Message-ID: <200412212147.iBLLlX315076@webmail1.cac.psu.edu>

Hello,
I am working with maQualityPlots.
I would like to change colors of graphs of 'Spatial:Rank(M-Raw)' and
'Spatial:Rank(M-Norm)' from blue-yellow to green-red.I think I need to change
col=NULL in :

 maQualityPlots(mrawObj, headerInfo = "", save = TRUE, dev = "png", col=NULL,
badspotfunction=NULL, controlId = c("ID", "Name"), DEBUG = FALSE, ...)
 My question is: Where I can find information about the vector 'col' that would
be helpful to change col=NULL?
or may be I have to change 

 qpImage(nbgraw, xvar = "maM", main = "Spatial: Rank(M-Raw)")?

Sincerely, Natalia.



From smyth at wehi.EDU.AU  Tue Dec 21 23:18:04 2004
From: smyth at wehi.EDU.AU (Gordon K Smyth)
Date: Wed, 22 Dec 2004 09:18:04 +1100 (EST)
Subject: Fwd: [R] problems with limma
In-Reply-To: <1103634718.41c8211ec5038@webmail.mcgill.ca>
References: <1103634718.41c8211ec5038@webmail.mcgill.ca>
Message-ID: <3996.220.236.75.62.1103667484.squirrel@220.236.75.62>

On Wed, December 22, 2004 12:11 am, r.ghezzo at staff.mcgill.ca said:
> ----- Forwarded message from r.ghezzo at staff.mcgill.ca -----
>     Date: Mon, 20 Dec 2004 15:45:11 -0500
>     From: r.ghezzo at staff.mcgill.ca
> Reply-To: r.ghezzo at staff.mcgill.ca
>  Subject: [R] problems with limma
>       To: r-help at stat.math.ethz.ch
>
> I try to send this message To Gordon Smyth at smyth at vehi,edu.au but it bounced
> back, so here it is to r-help

Questions about limma should be sent to the Bioconductor mailing list (the address is given in the
introduction of the Limma User's Guide for example).

> I am trying to use limma, just downloaded it from CRAN. I use R 2.0.1 on Win XP
> see the following:
>> library(RODBC)
>> chan1 <- odbcConnectExcel("D:/Data/mgc/Chips/Chips4.xls")
>> dd <- sqlFetch(chan1,"Raw")   # all data  12000
>> #
>> nzw <- cbind(dd$NZW1C,dd$NZW2C,dd$NZW3C,dd$NZW1T,dd$NZW2T,dd$NZW3T)
>> akr <- cbind(dd$AKR1C,dd$AKR2C,dd$AKR3C,dd$AKR1T,dd$AKR2T,dd$AKR3T)
>> bas <- cbind(dd$NZW1C,dd$NZW2C,dd$NZW3C,dd$AKR1C,dd$AKR2C,dd$AKR3C)
>> #
>>  design<-matrix(c(1,1,1,1,1,1,0,0,0,1,1,1),ncol=2)
>>  fit1 <- lmFit(nzw,design)
>>  fit1 <- eBayes(fit1)
>>  topTable(fit1,adjust="fdr",number=5)
>               M         t      P.Value         B
> 12222  3679.480 121.24612 7.828493e-06 -4.508864
> 1903   3012.405 118.32859 7.828493e-06 -4.508866
> 9068   1850.232  92.70893 1.178902e-05 -4.508889
> 10635  2843.534  91.99336 1.178902e-05 -4.508890
> 561   18727.858  90.17085 1.178902e-05 -4.508893
>> #
>>  fit2 <- lmFit(akr,design)
>>  fit2 <- eBayes(fit2)
>>  topTable(fit2,adjust="fdr",number=5)
>               M        t      P.Value         B
> 88     1426.738 80.48058 5.839462e-05 -4.510845
> 1964  36774.167 73.05580 5.839462e-05 -4.510861
> 5854   7422.578 68.60316 5.839462e-05 -4.510874
> 11890  1975.316 66.54480 5.839462e-05 -4.510880
> 9088   2696.952 64.16343 5.839462e-05 -4.510889
>> #
>>  fit3 <- lmFit(bas,design)
>>  fit3 <- eBayes(fit3)
>>  topTable(fit3,adjust="fdr",number=5)
>              M         t      P.Value         B
> 6262  1415.088 100.78933 2.109822e-05 -4.521016
> 5660  1913.479  96.40903 2.109822e-05 -4.521020
> 11900 4458.489  94.30738 2.109822e-05 -4.521022
> 9358  1522.330  80.46641 3.346749e-05 -4.521041
> 11773 1784.483  73.76620 3.346749e-05 -4.521053
>> #    Now lets do all together in Anova
>> #
>>  all <- cbind(nzw,akr)
>>  ts <- c(1,1,1,2,2,2,3,3,3,4,4,4)
>>  ts <- as.factor(ts)
>>  levels(ts) <- c("nzwC","nzwT","akrC","akrT")
>>  design <- model.matrix(~0+ts)
>>  colnames(design) <- levels(ts)
>>  fit4 <- lmFit(all,design)
>>  cont.matrix <- makeContrasts(
> +      Baseline = akrC - nzwC,
> +      NZW_Smk = nzwT - nzwC,
> +      AKR_Smk = akrT - akrC,
> +      Diff = (akrT - akrC) - (nzwT - nzwC),
> +      levels=design)
>>   fit42 <- contrasts.fit(fit4,cont.matrix)
>>   fit42 <- eBayes(fit42)
>> #
>>   topTable(fit42,coef="Baseline",adjust="fdr",number=5)
>                M         t     P.Value         B
> 3189    942.0993  13.57485 0.004062283 -4.528799
> 8607   2634.1826  11.23476 0.006913442 -4.530338
> 10242  -942.2860 -10.99253 0.006913442 -4.530551
> 283    -609.0831 -10.79354 0.006913442 -4.530735
> 3224  -1564.2572 -10.19429 0.008089034 -4.531351
> ----------------------------------------------------
> ------------- Shouldn't this be equal to fit1 above?
> ----------------------------------------------------
>>   topTable(fit42,coef="NZW_Smk",adjust="fdr",number=5)
>              M         t   P.Value         B
> 7724 -246.5956 -8.687324 0.1615395 -4.591133
> 1403 -307.8660 -7.063312 0.4066814 -4.591363
> 3865 -253.4899 -6.585582 0.4598217 -4.591457
> 3032 -509.2413 -5.841901 0.8294166 -4.591640
> 2490 -240.3259 -5.338679 0.9997975 -4.591795
> ----------------------------------------------------
> ------------- Shouldn't this be equal to fit2 above?
> ------------- The P.Value are unreal!!
> ----------------------------------------------------
>>   topTable(fit42,coef="AKR_Smk",adjust="fdr",number=5)
>              M        t  P.Value         B
> 11547 151.6622 6.380978 0.917470 -4.595085
> 12064 324.0851 6.337235 0.917470 -4.595085
> 6752  964.5478 5.858994 0.952782 -4.595086
> 10251 152.7587 5.339843 0.952782 -4.595087
> 1440  189.6056 4.933151 0.952782 -4.595089
> ----------------------------------------------------
> ------------- Shouldn't this be equal to fit3 above?
> ------------- The P.Value are unreal!!
> ----------------------------------------------------
>>   topTable(fit42,coef="Diff",adjust="fdr",number=5)
>               M         t   P.Value         B
> 7724   302.6892  7.540195 0.4102211 -4.593201
> 1403   419.4962  6.805495 0.4102211 -4.593265
> 10251  270.5269  6.686796 0.4102211 -4.593277
> 3270   409.8391  6.414966 0.4192042 -4.593307
> 10960 -511.4711 -5.469247 0.9652171 -4.593435
>> #
>>
> So the results I get from just pairwise comparisons are very significant, but
> when I try the Anova way, the significance completely dissapears.
> Am I doing something completely wrong?

Your commands look correct but your data looks crazy.  The M-values are supposed to be log-fold
changes, and you're getting values in the 10s of thousands.  Perhaps you are trying analyse data
on the raw intensity scale and have huge differences between arrays and groups or very large
outliers.  Note that the B-statistics are telling you that there is absolutely no differential
expression throughout.  Warning bells should go up when you see such large t-statistics with such
small B-statistics.  Please look at your data and do some quality assessment.  At very least you
probably need to log-transform.

Your 4-group approach should give the same M-values as the 2-group approach, but the standard
errors and t-statistics will change because your error estimates change.

Gordon

> This is data from Affimetrix mouse chips.
> Thanks for any help
> Heberto Ghezzo
> Ph.D.
> Meakins-Christie Labs
> McGill University
> Montreal - Canada



From wolf.privat at gmx.de  Tue Dec 21 23:29:11 2004
From: wolf.privat at gmx.de (Andreas)
Date: Tue, 21 Dec 2004 23:29:11 +0100
Subject: [R] Rgui.exe - Error while tuning svm
Message-ID: <cqa83g$731$1@sea.gmane.org>

Hello,

if I try to tune my svm with the code:

Tune <- tune.svm(Data.Train, Class.Train, type="C-classification",
kernel="radial", gamma = 2^(-1:1), cost = 2^(2:4))

i get a windows Messagebox with a error in the application "Rgui.exe" and
the message: "Die Anweisung in 0x6c48174d verweist auf Speicher 0x00000000.
Der Vorgang "read" konnte nicht auf dem Speicher ausgef??hrt werden. ....."

is this a known error, or is there any mistake in my code?

regards Andreas



From p.connolly at hortresearch.co.nz  Tue Dec 21 23:38:52 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 22 Dec 2004 11:38:52 +1300
Subject: [R] Removing trailing spaces
Message-ID: <20041221223852.GW3840@hortresearch.co.nz>

Some years ago when I used S-PLUS, I seem to remember, there was a
discussion about a simple method of removing trailing spaces from
vector elements.  I'd be fairly sure the same would work in R, but I
can't find any mention of anything like it in the R archives or with
help.search().

There are ways I could do it with substring(), but I seem to remember
there was something more elegant.  I can find the strip function in
the clim.pact package, but that trims everything after the first space
which is not useful in this case.

Someone please refresh my memory.

TIA

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From gunter.berton at gene.com  Wed Dec 22 00:11:24 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 21 Dec 2004 15:11:24 -0800
Subject: [R] Removing trailing spaces
In-Reply-To: <20041221223852.GW3840@hortresearch.co.nz>
Message-ID: <200412212311.iBLNBOSs020831@hertz.gene.com>


Use regular expressions: ?regexpr

e.g. if myvec is your original vector of strings

notrlblanks<-sub('[[:blank:]]+?','',myvec)


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Patrick Connolly
> Sent: Tuesday, December 21, 2004 2:39 PM
> To: R-help
> Subject: [R] Removing trailing spaces
> 
> Some years ago when I used S-PLUS, I seem to remember, there was a
> discussion about a simple method of removing trailing spaces from
> vector elements.  I'd be fairly sure the same would work in R, but I
> can't find any mention of anything like it in the R archives or with
> help.search().
> 
> There are ways I could do it with substring(), but I seem to remember
> there was something more elegant.  I can find the strip function in
> the clim.pact package, but that trims everything after the first space
> which is not useful in this case.
> 
> Someone please refresh my memory.
> 
> TIA
> 
> -- 
> Patrick Connolly
> HortResearch
> Mt Albert
> Auckland
> New Zealand 
> Ph: +64-9 815 4200 x 7188
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> ~.~.~.~.~
> I have the world`s largest collection of seashells. I keep it on all
> the beaches of the world ... Perhaps you`ve seen it.  
> ---Steven Wright 
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> ~.~.~.~.~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From MSchwartz at MedAnalytics.com  Wed Dec 22 00:19:07 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 21 Dec 2004 17:19:07 -0600
Subject: [R] Removing trailing spaces
In-Reply-To: <20041221223852.GW3840@hortresearch.co.nz>
References: <20041221223852.GW3840@hortresearch.co.nz>
Message-ID: <1103671147.4757.29.camel@horizons.localdomain>

On Wed, 2004-12-22 at 11:38 +1300, Patrick Connolly wrote:
> Some years ago when I used S-PLUS, I seem to remember, there was a
> discussion about a simple method of removing trailing spaces from
> vector elements.  I'd be fairly sure the same would work in R, but I
> can't find any mention of anything like it in the R archives or with
> help.search().
> 
> There are ways I could do it with substring(), but I seem to remember
> there was something more elegant.  I can find the strip function in
> the clim.pact package, but that trims everything after the first space
> which is not useful in this case.
> 
> Someone please refresh my memory.
> 
> TIA

See the final examples in ?sub

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Wed Dec 22 00:22:11 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 21 Dec 2004 17:22:11 -0600
Subject: [R] Problems with Excel (was SAS or R software)
In-Reply-To: <41C888E7.4000308@acelerate.com>
References: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088B59414@houston.tanox.net>
	<41C4F5C4.2070702@optushome.com.au>
	<41C55A2F.9000200@pburns.seanet.com>
	<41C888E7.4000308@acelerate.com>
Message-ID: <1103671331.4757.31.camel@horizons.localdomain>

On Tue, 2004-12-21 at 16:34 -0400, Kjetil Brinchmann Halvorsen wrote:
> Patrick Burns wrote:
> 
> > I'm in the process of researching problems with Excel.  The references
> > given by Tim and Marc seem to lead to discussions of most of the problems
> > with statistical procedures in Excel.  The executive summary is that if
> > it is in Excel and it looks like statistics, then avoid it.
> >
> > However, there are some other issues as well.  Here are three:
> >
> > 1)  For those of us used to S, Excel gives a non-intuitive result for:
> >
> >           -2^4
> 
> Yes. However, OO Calc gives the same result as Excel (Office 2003). Are 
> they trying to
> heavily to mimic Excel?

Yes. As I mentioned in the thread on Gnumeric over on r-devel, OO.org
seems to focus more on ensuring behavior consistent with Excel to a
fault rather than improving upon it. This includes the so-called "close
to zero adjustments" that I referred to previously in this thread.

This is my opinion alone, but presumably this is driven by the need to
be able to be a "drop in" replacement for Excel. So, as long as under-
informed Excel users are not aware of these issues, they could happily
use Calc without experiencing any differences in behavior and not be
confused (or burned) by unexpected results.

What is interesting, is that if you try to use that same formula in
Gnumeric, it will calculate 16, but if you go back and review the
formula, it will automatically have parens around the "-2" so that the
formula looks like "=(-2)^4" in the cell, instead of the R/S like
negation of the formula (ie. "-(2^4)"). You can explicitly put that
formula into a cell and get -16 in Gnumeric.

> > I wonder how many formulas are in existence that have unintended results
> > due to this.
> >
> > 2)  When numbers in scientific notation are written to ascii (csv or 
> > txt), Excel
> > decides that some of the digits are unnecessary and doesn't write them 
> > to the
> > file.  As far as I can tell the number of significant digits that you 
> > get is arbitrary
> > and capricious.  Apparently Microsoft thinks of this as a feature -- 
> > I'm not sure
> > what they think the up-side is.  If you want all of your digits, make 
> > sure that none
> > of the numbers are displayed in scientific notation.  (Yes, what is 
> > written depends
> > on what is displayed, but it is not WYSIWYG.)
> 
> Tried this to with OO Calc. All were written with 14 significant digits. 
> However, test was very
> fast and needs replcation investing some more time.
> 
> >
> > 3)  In the olden days if there were a blank cell in a range of cells 
> > on which a
> > function operated (a mean perhaps), then Excel decided that the blank 
> > meant
> > zero.  Microsoft changed this so that blank cells are ignored 
> > (apparently a good
> > thing).  However, if you reference a blank cell, it is then counted as 
> > zero.
> >
> > Exercise:  In the first column put numbers in the first few rows, but 
> > leave one
> > cell blank.  below that do the average of the cells (including the 
> > blank).  Now
> > in cell B1 put "=A1"  and copy this formula down column B.  You will 
> > get two
> > different means.
> >
> Yes. Done, but also on OP Calc with *exactly* the same result. 

Same with Gnumeric here unfortunately. It would appear that the behavior
is the result of pasting the blank cell as a "0" instead of as a blank.
Thus, the average() function sees the "0" as a legit value and of course
increases the sample size by 1.

I just filed a bug with the Gnumeric folks on this.

> Does one need to investigate OO Calc in the same way as has been done
> with Excel?

Yes, absolutely.


Marc



From tmulholland at bigpond.com  Wed Dec 22 00:26:35 2004
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Wed, 22 Dec 2004 07:26:35 +0800
Subject: [R] Removing trailing spaces
In-Reply-To: <20041221223852.GW3840@hortresearch.co.nz>
References: <20041221223852.GW3840@hortresearch.co.nz>
Message-ID: <41C8B12B.9030700@bigpond.com>

I generally use sub like so sub(" *$","",vector)


 > x <- c("this one   ","and that one ","plus this")
 > sub(" *$","",x)
[1] "this one"     "and that one" "plus this"
 >

Tom

Patrick Connolly wrote:
> Some years ago when I used S-PLUS, I seem to remember, there was a
> discussion about a simple method of removing trailing spaces from
> vector elements.  I'd be fairly sure the same would work in R, but I
> can't find any mention of anything like it in the R archives or with
> help.search().
> 
> There are ways I could do it with substring(), but I seem to remember
> there was something more elegant.  I can find the strip function in
> the clim.pact package, but that trims everything after the first space
> which is not useful in this case.
> 
> Someone please refresh my memory.
> 
> TIA
> 


-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From p.connolly at hortresearch.co.nz  Wed Dec 22 00:26:37 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 22 Dec 2004 12:26:37 +1300
Subject: [R] Removing trailing spaces
In-Reply-To: <1103671147.4757.29.camel@horizons.localdomain>
References: <20041221223852.GW3840@hortresearch.co.nz> 
	<1103671147.4757.29.camel@horizons.localdomain>
Message-ID: <20041221232637.GX3840@hortresearch.co.nz>

On Tue, 21-Dec-2004 at 05:19PM -0600, Marc Schwartz wrote:

|> On Wed, 2004-12-22 at 11:38 +1300, Patrick Connolly wrote:
|> > Some years ago when I used S-PLUS, I seem to remember, there was a
|> > discussion about a simple method of removing trailing spaces from
|> > vector elements.  I'd be fairly sure the same would work in R, but I
|> > can't find any mention of anything like it in the R archives or with
|> > help.search().
|> > 
|> > There are ways I could do it with substring(), but I seem to remember
|> > there was something more elegant.  I can find the strip function in
|> > the clim.pact package, but that trims everything after the first space
|> > which is not useful in this case.
|> > 
|> > Someone please refresh my memory.
|> > 
|> > TIA
|> 
|> See the final examples in ?sub

Yes.  Thanks.  I knew it was something very simple. 

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From p.dalgaard at biostat.ku.dk  Wed Dec 22 00:36:47 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Dec 2004 00:36:47 +0100
Subject: [R] Removing trailing spaces
In-Reply-To: <20041221223852.GW3840@hortresearch.co.nz>
References: <20041221223852.GW3840@hortresearch.co.nz>
Message-ID: <x2llbri5ps.fsf@biostat.ku.dk>

Patrick Connolly <p.connolly at hortresearch.co.nz> writes:

> Some years ago when I used S-PLUS, I seem to remember, there was a
> discussion about a simple method of removing trailing spaces from
> vector elements.  I'd be fairly sure the same would work in R, but I
> can't find any mention of anything like it in the R archives or with
> help.search().
> 
> There are ways I could do it with substring(), but I seem to remember
> there was something more elegant.  I can find the strip function in
> the clim.pact package, but that trims everything after the first space
> which is not useful in this case.
> 
> Someone please refresh my memory.

Like this?

> x <- c("a b ", "dfhskf   ", "FGF", "IUOI UOUO UOUO  ")
> sub(" *$", "", x)
[1] "a b"            "dfhskf"         "FGF"            "IUOI UOUO UOUO"


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Wed Dec 22 00:42:32 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 21 Dec 2004 18:42:32 -0500
Subject: [R] Rgui.exe - Error while tuning svm
In-Reply-To: <cqa83g$731$1@sea.gmane.org>
References: <cqa83g$731$1@sea.gmane.org>
Message-ID: <rvchs05q3u1lgjqlrc43pkpbp87oj3qvkm@4ax.com>

On Tue, 21 Dec 2004 23:29:11 +0100, "Andreas" <wolf.privat at gmx.de>
wrote:

>Hello,
>
>if I try to tune my svm with the code:
>
>Tune <- tune.svm(Data.Train, Class.Train, type="C-classification",
>kernel="radial", gamma = 2^(-1:1), cost = 2^(2:4))
>
>i get a windows Messagebox with a error in the application "Rgui.exe" and
>the message: "Die Anweisung in 0x6c48174d verweist auf Speicher 0x00000000.
>Der Vorgang "read" konnte nicht auf dem Speicher ausgef??hrt werden. ....."
>
>is this a known error, or is there any mistake in my code?

That looks like dereferencing a null pointer.  It's likely in the
e1071 code, not actually in Rgui.

Duncan Murdoch



From andy_liaw at merck.com  Wed Dec 22 02:02:34 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 21 Dec 2004 20:02:34 -0500
Subject: [R] (no subject)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E45D@usrymx25.merck.com>

What you've shown us looks inconsistent:  The data frame you show has
variables X1 through X5 and `class'.  If this is a data frame named `data',
why do you call tree() with the formula V1 ~ X1?  Where is V1?

`data' is the name of a built-in R function.  Try using some other name for
the data frame.  Also, give us the output of traceback() after the error
occur.  That should help.

Andy

> From: NICOLAS DEIG
> 
> hello,
> 
> I am encoutering problems with a function of R. 
> The function is for classification trees. 
> 
> I am working on datas of this kind:
> 
>        X1    X2    X3    X4    X5   class
> 1   2.092 1.902 2.779 2.944 1.946       1
> 
> for 200 observations and 4 differents classes.
> 
> What i would like to do is grow a tree with the function 
> "tree" and then
> use the result in the function "cv.tree" in order to ran cross
> validation experiment. 
> > library(tree)
> > Z<-tree(V1~X1,data)
> > W<-cv.tree(Z)
> Error in as.data.frame.default(data) : can't coerce function into a
> data.frame
> 
> 
> There is the problem, "cv.tree" won t consider the resluts of function
> "tree" as an object of class tree, it just considers this object as a
> data frame. 
> Can anyone eyplain me why or already encoutered this kind of problem?
> 
> THanks in advance,
> Nicolas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From yunt at u.washington.edu  Wed Dec 22 02:17:54 2004
From: yunt at u.washington.edu (UW Email (yunt))
Date: Tue, 21 Dec 2004 17:17:54 -0800
Subject: [R] how do I get my data in matrix form?
Message-ID: <004701c4e7c4$10d1aec0$aa795f80@son.washington.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041221/08ec5c3c/attachment.pl

From jorelien at scimetrika.com  Wed Dec 22 02:30:05 2004
From: jorelien at scimetrika.com (Jean G. Orelien)
Date: Tue, 21 Dec 2004 20:30:05 -0500
Subject: [R] GAM: Getting standard errors from the parametric terms in a GAM
	model
Message-ID: <NC-TDA03qMoZmi3xlDi000005be@mail.nctda.org>

I am new to R.  I'm using the function GAM and wanted to get standard errors
and p-values for the parametric terms (I fitted a semi-parametric models).
Using the function anova() on the object from GAM, I only get  p-values for
the nonparametric terms.

 

Does anyone know if and how to get standard errors for the parametric terms?

 

Thanks.

 

Jean G. Orelien



 


From ccleland at optonline.net  Wed Dec 22 02:30:58 2004
From: ccleland at optonline.net (ccleland@optonline.net)
Date: Tue, 21 Dec 2004 20:30:58 -0500
Subject: [R] how do I get my data in matrix form?
Message-ID: <9902779923d1.9923d1990277@optonline.net>

You probably want something like this:

library(foreign)
mydata <- read.spss("c:\\myfolder\\mydata.sav", to.data.frame=TRUE)

----- Original Message -----
From: "UW Email (yunt)" <yunt at u.washington.edu>
Date: Tuesday, December 21, 2004 8:17 pm
Subject: [R] how do I get my data in matrix form?

> Hi,
> I am trying to convert the SPSS data that I imported using the 
> package foreign into a matrix form in R. I tried using 
> write.table() and cbind() but the output still looks bizzarre. I 
> have about 104 columns and 21143 rows. Is the size of the the data 
> causing a problem? 
> Here was my approach
> mydata<-("file location",max=0)
> mydata
> cbind(mydata) #doesnt work well
> write.table(mydata) #distorted output
> I also tried data.frame(mydata) #but this gives me only 4 columns 
> (though properly arranged)
> 
> Is there a better way to resolve this problem? 
> Thanks
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From jorelien at scimetrika.com  Wed Dec 22 02:37:23 2004
From: jorelien at scimetrika.com (Jean G. Orelien)
Date: Tue, 21 Dec 2004 20:37:23 -0500
Subject: [R] GAM: Overfitting
Message-ID: <NC-TDA030ng8os5Rmln000005bf@mail.nctda.org>

I am analyzing particulate matter data (PM10) on a small data set (147
observations).  I fitted a semi-parametric model and am worried about
overfitting.  How can one check for model fit in GAM?

 

Jean G. Orelien



 


From Achim.Zeileis at wu-wien.ac.at  Wed Dec 22 02:38:41 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 22 Dec 2004 02:38:41 +0100
Subject: [R] how do I get my data in matrix form?
In-Reply-To: <004701c4e7c4$10d1aec0$aa795f80@son.washington.edu>
References: <004701c4e7c4$10d1aec0$aa795f80@son.washington.edu>
Message-ID: <20041222023841.2ef67373.Achim.Zeileis@wu-wien.ac.at>

On Tue, 21 Dec 2004 17:17:54 -0800 UW Email \(yunt\) wrote:

> Hi,
> I am trying to convert the SPSS data that I imported using the package
> foreign into a matrix form in R. I tried using write.table() and
> cbind() but the output still looks bizzarre. I have about 104 columns
> and 21143 rows. Is the size of the the data causing a problem? Here
> was my approach
> mydata<-("file location",max=0)

which gives a syntax error...

I guess you meant something like

R> mydata <- read.spss("foo")

which gives you a "list". If you want a "data.frame" instead (which is
probably better than a matrix, you can say

R> mydata <- as.data.frame(mydata)

or instead you can say

R> mydata <- read.spss("foo", to.data.frame = TRUE)

in the first place!
If you really want a "matrix" then, you can say

R> mydata <- as.matrix(mydata)

Look at the man pages for more info.
Z

> mydata
> cbind(mydata) #doesnt work well
> write.table(mydata) #distorted output
> I also tried data.frame(mydata) #but this gives me only 4 columns
> (though properly arranged)
> 
> Is there a better way to resolve this problem? 
> Thanks
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From f.harrell at vanderbilt.edu  Wed Dec 22 04:25:39 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 21 Dec 2004 21:25:39 -0600
Subject: [R] GAM: Overfitting
In-Reply-To: <NC-TDA030ng8os5Rmln000005bf@mail.nctda.org>
References: <NC-TDA030ng8os5Rmln000005bf@mail.nctda.org>
Message-ID: <41C8E933.1070100@vanderbilt.edu>

Jean G. Orelien wrote:
> I am analyzing particulate matter data (PM10) on a small data set (147
> observations).  I fitted a semi-parametric model and am worried about
> overfitting.  How can one check for model fit in GAM?
> 
>  
> 
> Jean G. Orelien

It's good to separate 'model fit' (or lack of fit) from 'overfitting'. 
Overfitting can cause the model fit to appear to be excellent, but there 
is still a huge problem.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From fhduan at gmail.com  Wed Dec 22 05:39:32 2004
From: fhduan at gmail.com (Frank Duan)
Date: Tue, 21 Dec 2004 23:39:32 -0500
Subject: [R] How to interpret and modify "plot.svm"?
In-Reply-To: <20041217184607.0bc72fd4.david.meyer@wu-wien.ac.at>
References: <20041217184607.0bc72fd4.david.meyer@wu-wien.ac.at>
Message-ID: <3b91723104122120391b4a74c@mail.gmail.com>

Thank you, Dr. Meyer. I updated the e1071 package but still can't find
the other three arguments for plot.svm. In addition, I can plot a
gray-colored contour region by adding the argument "col = c(gray(0.2),
gray(0.8))". But I failed to change those colored "x" or "o" points
into the shapes I want. Basically, I don't want to have any color in
the plot. Could you give me a hint how to do that?

The code I am using to plot the gray-contour figure is:

m <- svm(****, data = mydata)
plot(m, mydata, col = c(gray(0.2), gray(0.8)))

Thank you very much and Merry Xmas.

Frank


On Fri, 17 Dec 2004 18:46:07 +0100, David Meyer
<david.meyer at wu-wien.ac.at> wrote:
> Frank:
> 
> > Dear R people,
> 
> > I am trying to plot the results from running svm in library(e1071). I
> > use plot.svm. After searching through the help archives and FAQ, I
> > still have several questions:
> 
> > 1.  In default, crosses indicate support vectors. But why are there
> > two colors of crosses? What do they represent?
> 
> The colors represent the classes of the data points. The help page
> admittedly doesn't tell you this and deserves improvement.
> 
> > 2. I want to draw a white-gray colored plot and modify the different
> > colored crosses or circles by different shaped points. Could anyone
> > give me a hint?
> 
> I just added three arguments to plot.svm() that allow customizing of the
> plot symbols. The contour region is controlled by the parameters of the
> filled.contour() function used in plot.svm(), so you will need to add
> the color.palette argument to plot.svm (which subsequently will be
> passed to filled.contour()).
> 
> > 3. Is it possible for me to draw a "hyperplane" on the plot?
> 
> You can add arbitrary objects to the plot (try lines()); but plot.svm()
> doesn't compute the boundaries.
> 
> > 4. What is the algorithm to plot the contour region?
> 
> see filled.contour(). The input is determined by a grid of predicted
> values.
> 
> Best,
> -d
> 
> > Thank you very much,
> 
> > Frank
> 
> --
> Dr. David Meyer
> Department of Information Systems
> 
> Vienna University of Economics and Business Administration
> Augasse 2-6, A-1090 Wien, Austria, Europe
> Fax: +43-1-313 36x746
> Tel: +43-1-313 36x4393
> HP:  http://wi.wu-wien.ac.at/~meyer/
>



From ealaca at ucdavis.edu  Wed Dec 22 07:36:53 2004
From: ealaca at ucdavis.edu (Emilio A. Laca)
Date: Tue, 21 Dec 2004 22:36:53 -0800
Subject: [R] Complex lme model
Message-ID: <BDEE5605.3AB5%ealaca@ucdavis.edu>

I would like to request help for coding the lme for the following:

Soils from 14 Regions (fixed) were sampled every Year (fixed) during three
years. Each Year each Region was sampled at several Points (random), and two
Cores (random) were taken at each Point. Each Core was split into five
Depths (fixed, crossed with all other factors) and C (response) was measured
at each Depth of each Core of each Point of each Region in each Year.

First, I want to assume that Points were randomly selected each year, and I
want to test an AR(1) over Depth.

Second, I want to assume that Points were selected only once during the
first year. Each near, new cores were randomly selected from each of the
original points. I would like to test and AR(1) for year.

Help will be greatly appreciated and knowledge will be passed on to other
students. Thank you.

PS:  lme(C ~ Region*Year*Depth, data=soil, random=~1|Region/Point/Core) just
returned:

Error in MEEM(object, conLin, control$niterEM) :
    Singularity in backsolve at level 0, block 1


-- 
Emilio A. Laca     
One Shields Avenue, 2306 PES Building
Agronomy and Range Science                    ealaca at ucdavis.edu
University of California                      fax: (530) 752-4361
Davis, California  95616                            (530) 754-4083



From buser at stat.math.ethz.ch  Wed Dec 22 08:42:15 2004
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 22 Dec 2004 08:42:15 +0100
Subject: [R] Random intercept model with time-dependent
	covariates, results different from SAS 
Message-ID: <16841.9559.123981.491533@gargle.gargle.HOWL>

Answering on a mail from
>From     Keith Wong <keithw_at_med.usyd.edu.au>
Date     Sun 04 Jul 2004 - 17:21:36 EST
Subject  [R] Random intercept model with time-dependent
         covariates, results different from SAS  

Hi all

I've got a question about the degrees of freedom in a mixed model,
calculated with lme from the lme4 package. 
Since I've no access to the original data of Keith Wong, I generated a
dataframe with the same structure (but balanced) by

> set.seed(1)
> dat <- data.frame(y = rnorm(50), group = rep(c("a","b"), each = 25),
                  time = factor(rep(1:5,10)), w = rnorm(50), z = rnorm(50),
                  id = factor(rep(1:10, each = 5)))
> str(dat)

The subject id is nested in group. We have 5 repeated measures for each
subject. 

Surly there is no meaningfull interpretation for the results but for
demonstration it will do well.

I do the model without the covariates:

> library(nlme)
> options(contrasts = c("contr.SAS", "contr.poly")) 
> g2 <- lme(y ~ time+group+time:group, data = dat, random = ~ 1 | id)
> anova(g2)

Analysis of Variance Table
            numDF denDF   F-value p-value
(Intercept)     1    32 0.6340507  0.4317
time            4    32 0.1103619  0.9780
group           1     8 0.2924309  0.6034
time:group      4    32 0.4614766  0.7634

I quit R and restart it and now use library(lme4)

> library(lme4)
> options(contrasts = c("contr.SAS", "contr.poly")) 
> g2 <- lme(y ~ time+group+time:group, data = dat, random = ~ 1 | id)
> anova(g2)

Analysis of Variance Table
           Df Sum Sq Mean Sq  Denom F value Pr(>F)
time        4  0.351   0.088 40.000  0.1104 0.9782
group       1  0.233   0.233 40.000  0.2925 0.5916
time:group  4  1.468   0.367 40.000  0.4615 0.7635

I get other degrees of freedom for the denominator. How can I tell lme that
id is nested in the fixed factor group to get 8 as denominator DF for
group? In my example the difference is small (I've generated the data
randomly) but in other more meaningfull analysis the different DF can
change the results.

I Use R 2.0.1 under Linux.
Package:       nlme
Version:       3.1-53
Package:       lme4
Version:       0.6-11

In the original message the example was also calculated with SAS and there
was the problem of the degrees of freedom for group, too, but the syntax
wasn't correct. One must specify

> PROC MIXED;
> CLASS id time group;
> MODEL y = time group time*group /solution;
> RANDOM int /subject = id(group);
> RUN;

It is important to tell SAS that the subject id is nested in group and you
can do this by using 
        subject = id(group)
instead of
	subject = id
Then you will get the correct degrees of freedom for the test of group:

Type 3 Tests of Fixed Effects

	    Num	      Den	
Effect	     DF	       DF	F Value		Pr > F

time	      4	       32	   0.11		0.9780
group	      1	        8	   0.29		0.6033
time*group    4	       32	   0.46		0.7634


Thanks in advance for an answer. I'm interested if I must use the lme
syntax in another way for the lme4 package.

Regards,

Christoph Buser



From simon at stats.gla.ac.uk  Wed Dec 22 11:49:35 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Wed, 22 Dec 2004 10:49:35 +0000 (GMT)
Subject: [R] GAM: Getting standard errors from the parametric terms in
	a GAM model
In-Reply-To: <NC-TDA03qMoZmi3xlDi000005be@mail.nctda.org>
References: <NC-TDA03qMoZmi3xlDi000005be@mail.nctda.org>
Message-ID: <Pine.LNX.4.58.0412221047300.11026@moon.stats.gla.ac.uk>

summary.gam and anova.gam in package mgcv will report standard errors and 
p-values for parametric terms, as well as smooth terms, for a gam fitted 
by function gam from package mgcv. 

Simon

> I am new to R.  I'm using the function GAM and wanted to get standard errors
> and p-values for the parametric terms (I fitted a semi-parametric models).
> Using the function anova() on the object from GAM, I only get  p-values for
> the nonparametric terms.
> 
>  
> 
> Does anyone know if and how to get standard errors for the parametric terms?
> 
>  
> 
> Thanks.
> 
>  
> 
> Jean G. Orelien
> 
> 
> 
>  
> 
>



From angelare at to.infn.it  Wed Dec 22 11:57:53 2004
From: angelare at to.infn.it (Angela Re)
Date: Wed, 22 Dec 2004 11:57:53 +0100
Subject: [R] fitting data
Message-ID: <41C95331.9060709@to.infn.it>

Good  morning,
I'd like to know some detailed sources to understand fitting 
experimental data distribution in R. Can you help me?
Thank you, Angela



From simon at stats.gla.ac.uk  Wed Dec 22 12:08:17 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Wed, 22 Dec 2004 11:08:17 +0000 (GMT)
Subject: [R] GAM: Overfitting
In-Reply-To: <NC-TDA030ng8os5Rmln000005bf@mail.nctda.org>
References: <NC-TDA030ng8os5Rmln000005bf@mail.nctda.org>
Message-ID: <Pine.LNX.4.58.0412221054000.11026@moon.stats.gla.ac.uk>

> I am analyzing particulate matter data (PM10) on a small data set (147
> observations).  I fitted a semi-parametric model and am worried about
> overfitting.  How can one check for model fit in GAM?

- Keeping a random subset of the data as a validation set,  fitting 
to the remaining data and then comparing the R^2/ proportion deviance explained 
on fit set and validation set is usually quite diagnostic. If the fit data 
are much better predicted than the validation data, then you probably have 
over-fitting. 

- If your response is treated as Poisson then scale parameter estimates 
<<1 are also diagnostic, but only if you are not expecting overdispersion, 
of course. 

- If you use gam from package mgcv then, by default, model 
effective degrees of freedom are estimated from your data by GCV or an 
approximation to AIC. mgcv::gam allows you to increase the penalty on each 
model degree of freedom in these criteria, via gam argument `gamma'. Some 
work by Kim and Gu (2004, J.Roy.Statist.Soc.B) suggests that gamma around 
1.4 can be a sensible choise for surpressing overfitting, without 
much of a degredation in MSE performance.
 

best,
Simon



From anne.piotet at urbanet.ch  Wed Dec 22 13:06:02 2004
From: anne.piotet at urbanet.ch (Anne)
Date: Wed, 22 Dec 2004 13:06:02 +0100
Subject: [R] ordering levels
Message-ID: <000c01c4e81e$9cd51650$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041222/0d81169c/attachment.pl

From anne.piotet at urbanet.ch  Wed Dec 22 13:10:41 2004
From: anne.piotet at urbanet.ch (Anne)
Date: Wed, 22 Dec 2004 13:10:41 +0100
Subject: [R] RE ordering levels
Message-ID: <002301c4e81f$43046c10$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041222/da177013/attachment.pl

From ccleland at optonline.net  Wed Dec 22 13:13:39 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 22 Dec 2004 07:13:39 -0500
Subject: [R] ordering levels
In-Reply-To: <000c01c4e81e$9cd51650$6c00a8c0@mtd4>
References: <000c01c4e81e$9cd51650$6c00a8c0@mtd4>
Message-ID: <41C964F3.9080003@optonline.net>

testf <- ordered(testf, levels=c("red", "blue", "white"))

?ordered

Anne wrote:
> Hello!
> I would like to know if there is a simple way to reorder levels of a given factor.Let's say  that the  vector
> testf<-factor(c("red","red","red","blue","blue","white")) 
>   levels(testf)  : blue red white 
> 
> should have reordered levels  such as 
>  levels(testf)  : red blue  white 
> 
>  (this is for presentation purposes)
> I guess I'm looking for a generalized version of "relevel"...
> 
> Thanks 
> 
> Anne
> 
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From bxc at steno.dk  Wed Dec 22 13:24:45 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 22 Dec 2004 13:24:45 +0100
Subject: [R] ordering levels
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE20D@exdkba022.novo.dk>

In:

http://biostat.ku.dk/~bxc/SPE/library/

you will find a zip of the Lexis package that contains the
function Relevel, which has precise this (and other) features.

Bendix 
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anne
> Sent: Wednesday, December 22, 2004 1:06 PM
> To: R list
> Subject: [R] ordering levels
> 
> 
> Hello!
> I would like to know if there is a simple way to reorder 
> levels of a given factor.Let's say  that the  vector
> testf<-factor(c("red","red","red","blue","blue","white")) 
>   levels(testf)  : blue red white 
> 
> should have reordered levels  such as 
>  levels(testf)  : red blue  white 
> 
>  (this is for presentation purposes)
> I guess I'm looking for a generalized version of "relevel"...
> 
> Thanks 
> 
> Anne
> 
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From Achim.Zeileis at wu-wien.ac.at  Wed Dec 22 13:27:52 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 22 Dec 2004 13:27:52 +0100
Subject: [R] ordering levels
In-Reply-To: <000c01c4e81e$9cd51650$6c00a8c0@mtd4>
References: <000c01c4e81e$9cd51650$6c00a8c0@mtd4>
Message-ID: <20041222132752.166a34f0.Achim.Zeileis@wu-wien.ac.at>

On Wed, 22 Dec 2004 13:06:02 +0100 Anne wrote:

> Hello!
> I would like to know if there is a simple way to reorder levels of a
> given factor.Let's say  that the  vector
> testf<-factor(c("red","red","red","blue","blue","white")) 
>   levels(testf)  : blue red white 
> 
> should have reordered levels  such as 
>  levels(testf)  : red blue  white 

You can do

testf <- factor(as.character(testf), levels = c("red", "blue", "white"))

or simply create testf with the right ordering of levels in the first
place.

hth,
Z

>  (this is for presentation purposes)
> I guess I'm looking for a generalized version of "relevel"...
> 
> Thanks 
> 
> Anne
> 
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From bxc at steno.dk  Wed Dec 22 13:38:05 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 22 Dec 2004 13:38:05 +0100
Subject: [R] ordering levels: I was wrong
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE211@exdkba022.novo.dk>

I was wrong about needing the Relevel from the Lexis package.

The default verson of relevel does the job of reshuffling levels
in any desired order, albeit with a warning (which comes from the
fact that apparently only a single number had been anticipated by 
the designer):

> testf <- factor( sample( letters[1:4], 100, replace=T ) )
> table( testf, newf=relevel( testf, ref=c(3,2,1,4) ) )
     newf
testf c  b  a  d 
    a  0  0 21  0
    b  0 21  0  0
    c 32  0  0  0
    d  0  0  0 26
Warning message: 
the condition has length > 1 and only the first element will be used in:
if (is.na(ref)) stop("ref must be an existing level") 

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anne
> Sent: Wednesday, December 22, 2004 1:06 PM
> To: R list
> Subject: [R] ordering levels
> 
> 
> Hello!
> I would like to know if there is a simple way to reorder 
> levels of a given factor.Let's say  that the  vector
> testf<-factor(c("red","red","red","blue","blue","white")) 
>   levels(testf)  : blue red white 
> 
> should have reordered levels  such as 
>  levels(testf)  : red blue  white 
> 
>  (this is for presentation purposes)
> I guess I'm looking for a generalized version of "relevel"...
> 
> Thanks 
> 
> Anne
> 
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Dec 22 13:40:50 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 22 Dec 2004 13:40:50 +0100 (CET)
Subject: [R] RE ordering levels
In-Reply-To: <002301c4e81f$43046c10$6c00a8c0@mtd4>
References: <002301c4e81f$43046c10$6c00a8c0@mtd4>
Message-ID: <Pine.LNX.4.51.0412221332220.9748@artemis.imbe.med.uni-erlangen.de>

On Wed, 22 Dec 2004, Anne wrote:

> Sorry, sorry....
> of course
> levels(testf)[c(2,1,3)]
>
> will do the job
>

Anne,

note that `red' is coded as `blue' now:

R> testf <- factor(c("red","red","red","blue","blue","white"))
R> testf
[1] red   red   red   blue  blue  white
Levels: blue red white
R> levels(testf) <- levels(testf)[c(2,1,3)]
R> testf
[1] blue  blue  blue  red   red   white
Levels: red blue white

so you changed the data, not just the ordering of the levels. If there is
a reasonable ordering available, an ordered factor contructed via

ordered(c("red","red","red","blue","blue","white"), levels = c("red", "blue", "white"))
[1] red   red   red   blue  blue  white
Levels: red < blue < white

will do (the example here is silly, of course). But I guess you are
interested in a different ordering because of
some plotting problem or because of the specification of `baseline' levels
in linear models. For the latter one, have a deeper look at the `base'
argument of the `contr.treatment' function.

Best,

Torsten

> My excuses to all
>
> Anne
>
> PS I will meditate the following saying
> "la parole est d'argent et le silence est d'or"
>
> BONNES FETES A TOUS
> SEASONAL GREETINGS
>
>
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From Ted.Harding at nessie.mcc.ac.uk  Wed Dec 22 13:37:30 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 22 Dec 2004 12:37:30 -0000 (GMT)
Subject: [R] ordering levels
In-Reply-To: <000c01c4e81e$9cd51650$6c00a8c0@mtd4>
Message-ID: <XFMail.041222123730.Ted.Harding@nessie.mcc.ac.uk>

On 22-Dec-04 Anne wrote:
> Hello!
> I would like to know if there is a simple way to reorder levels
> of a given factor.Let's say  that the  vector
> testf<-factor(c("red","red","red","blue","blue","white")) 
>   levels(testf)  : blue red white 
> 
> should have reordered levels  such as 
>  levels(testf)  : red blue  white 
> 
>  (this is for presentation purposes)
> I guess I'm looking for a generalized version of "relevel"...

Hi Anne,

Look at ?factor and note the default

   levels = sort(unique.default(x), na.last = TRUE)

in

   factor(x, levels = sort(unique.default(x), na.last = TRUE),
          labels = levels, exclude = NA, ordered = is.ordered(x))

You can change the order of levels by changing the "levels = ...",
as in

> factor(c("1.2nd","2.1st","3.4th","4.3rd"))
[1] 1.2nd 2.1st 3.4th 4.3rd
Levels: 1.2nd 2.1st 3.4th 4.3rd

> factor(c("1.2nd","2.1st","3.4th","4.3rd"),
         levels=c("2.1st","1.2nd","4.3rd","3.4th"))
[1] 1.2nd 2.1st 3.4th 4.3rd
Levels: 2.1st 1.2nd 4.3rd 3.4th

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 22-Dec-04                                       Time: 12:37:30
------------------------------ XFMail ------------------------------



From michael.gray at somerville.oxford.ac.uk  Wed Dec 22 13:55:27 2004
From: michael.gray at somerville.oxford.ac.uk (Michael Gray)
Date: Wed, 22 Dec 2004 12:55:27 +0000 (GMT)
Subject: [R] downloading R
Message-ID: <20041222125527.186902A0E8@webmail222.herald.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041222/c31f0182/attachment.pl

From anne.piotet at urbanet.ch  Wed Dec 22 14:22:57 2004
From: anne.piotet at urbanet.ch (Anne)
Date: Wed, 22 Dec 2004 14:22:57 +0100
Subject: [R] RE ordering levels
References: <002301c4e81f$43046c10$6c00a8c0@mtd4>
Message-ID: <007301c4e829$5bae0190$6c00a8c0@mtd4>

Thanks to all of you; i should be able now to get around the problem!

Anne


----- Original Message ----- 
From: "Anne" <anne.piotet at urbanet.ch>
To: "R list" <r-help at stat.math.ethz.ch>
Sent: Wednesday, December 22, 2004 1:10 PM
Subject: [R] RE ordering levels


> Sorry, sorry....
> of course
> levels(testf)[c(2,1,3)]
>
> will do the job
>
> My excuses to all
>
> Anne
>
> PS I will meditate the following saying
> "la parole est d'argent et le silence est d'or"
>
> BONNES FETES A TOUS
> SEASONAL GREETINGS
>
>
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From subianto at cs.uu.nl  Wed Dec 22 14:26:01 2004
From: subianto at cs.uu.nl (Muhammad Subianto)
Date: Wed, 22 Dec 2004 14:26:01 +0100
Subject: [R] Make a table
Message-ID: <41C975E9.7050804@cs.uu.nl>

Dear useR,

I have a dataset like this below,
 > prevRND.dat   <- read.table("C:\\workdir\\prevRND.txt",
+                           header=FALSE,  # No header.
+                           col.names = c("X","Y","Z"),
+                           sep = ",")
 > prevRND.dat
   X Y        Z
1 A A 0.950933
2 A B 0.143600
3 A C 0.956133
4 B A 0.000533
5 B B 0.986467
6 B C 0.032066
7 C A 0.005333
8 C B 0.000000
9 C C 0.009266

How can I make that data above as table,

	        Y
X    A          B         C
A  0.950933 0.143600 0.956133
B  0.000533 0.986467 0.032066
C  0.005333 0.000000 0.009266

I cannot use table() or ftable() functions because the 3rd column (Z) is 
probability. Are there any function to make a table as I want?

Kind regards,
Muhammad Subianto



From francoisromain at free.fr  Wed Dec 22 14:57:31 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Wed, 22 Dec 2004 14:57:31 +0100
Subject: [R] weighted kernel density estimation
Message-ID: <41C97D4B.8060803@free.fr>

Dear wizaRds,

I use the MASS::kde2d function to estimate density of the two first 
principal components. I do that to have a graphic visualisation of a 
"group structure" in my dataset. So far, no problem.

But i would like to estimate that density using weights according to the 
COS?? values that tells me if my observation is well represented on the 
factorial plan 1-2. I would like to use (1) instead of (2) where the w_i 
depends on the COS??, i hacked the MASS::kde2d function to do that.

\hat{f}_n(x) = \frac{1}{n|H|} \sum_{i=1}^n w_i \times K\left[H^{-1}(x-X_i)\right]          (1)

\hat{f}_n(x) = \frac{1}{n|H|} \sum_{i=1}^n K\left[H^{-1}(x-X_i)\right]                     (2)


I have uploaded to http://addictedtor.free.fr/testkde2dw the files :
- plan12.pdf             : my factorial plan 1-2 (size of representation 
of the observations are proportionnal to the COS??)
- persp.pdf               : the estimation without the "weights"
- perspW.pdf            : the estimation with the "weights"

My questions are : As anyone done that ? Do i have right to do that ?

Thanks a lot.


See my kde2dw function below :

kde2dw <- function (x, y, h, n = 25, lims = c(range(x), range(y)), w) 
{
# w is a weight vector, you must have sum(w)=length(x)

    nx <- length(x)
    if (length(y) != nx) 
        stop("Data vectors must be the same length")

    gx <- seq(lims[1], lims[2], length = n)
    gy <- seq(lims[3], lims[4], length = n)
    if (missing(h)) h <- c(bandwidth.nrd(x), bandwidth.nrd(y))
    if(missing(w))  w <- rep(1,nx)

    h <- h/4
    ax <- outer(gx, x, "-")/h[1] 
    ay <- outer(gy, y, "-")/h[2] 


    z <- matrix(dnorm(ax), n, nx) %*% diag(w) %*% t(matrix(dnorm(ay), n, nx)) / (nx * h[1] * h[2])

    return(list(x = gx, y = gy, z = z))
}

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann??e
Institut de Statistique de l'Universit?? de Paris (ISUP)
Fili??re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From ripley at stats.ox.ac.uk  Wed Dec 22 15:07:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 22 Dec 2004 14:07:08 +0000 (GMT)
Subject: [R] downloading R
In-Reply-To: <20041222125527.186902A0E8@webmail222.herald.ox.ac.uk>
References: <20041222125527.186902A0E8@webmail222.herald.ox.ac.uk>
Message-ID: <Pine.LNX.4.61.0412221402210.3074@gannet.stats>

On Wed, 22 Dec 2004, Michael Gray wrote:

> I have tried to download the latest version of the R program 
> (rw2001.exe) from your website www.r-project.org onto my computer with

Unlikely, as it is not there.  I suspect it was from a CRAN mirror.

> Windows XP but with no success. Upon completion of the download (of the 
> base distribution), I try to open the program and a error box appears 
> saying 'the setup files are corrupted. Please obtain a new copy of the 
> program'. I am unsure as to what the problem is and how to proceed. I 
> would be grateful for any help, thank you for your assistance,

It means exactly what it says.  Your download is corrupted: we supply 
md5sums for you to verify that the download is correct: please check them.

It might be that a local cache has a corrupted copy, so you might want to 
try another mirror or refreshing the cache (if you can).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From apjaworski at mmm.com  Wed Dec 22 15:11:24 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Wed, 22 Dec 2004 08:11:24 -0600
Subject: [R] Make a table
In-Reply-To: <41C975E9.7050804@cs.uu.nl>
Message-ID: <OFA8317E7C.07DE690C-ON86256F72.004D94E0-86256F72.004DF2FD@mmm.com>






Muhammad,

Try

tapply(prevRND.dat$Z, list(X=prevRND.dat$X, Y=prevRND.dat$Y), mean)

Cheers,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             Muhammad Subianto                                             
             <subianto at cs.uu.n                                             
             l>                                                         To 
             Sent by:                  r-help at stat.math.ethz.ch            
             r-help-bounces at st                                          cc 
             at.math.ethz.ch                                               
                                                                   Subject 
                                       [R] Make a table                    
             12/22/2004 07:26                                              
             AM                                                            
                                                                           
                                                                           
                                                                           
                                                                           




Dear useR,

I have a dataset like this below,
 > prevRND.dat   <- read.table("C:\\workdir\\prevRND.txt",
+                           header=FALSE,  # No header.
+                           col.names = c("X","Y","Z"),
+                           sep = ",")
 > prevRND.dat
   X Y        Z
1 A A 0.950933
2 A B 0.143600
3 A C 0.956133
4 B A 0.000533
5 B B 0.986467
6 B C 0.032066
7 C A 0.005333
8 C B 0.000000
9 C C 0.009266

How can I make that data above as table,

                     Y
X    A          B         C
A  0.950933 0.143600 0.956133
B  0.000533 0.986467 0.032066
C  0.005333 0.000000 0.009266

I cannot use table() or ftable() functions because the 3rd column (Z) is
probability. Are there any function to make a table as I want?

Kind regards,
Muhammad Subianto

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From angelare at to.infn.it  Wed Dec 22 15:12:48 2004
From: angelare at to.infn.it (Angela Re)
Date: Wed, 22 Dec 2004 15:12:48 +0100
Subject: [R] how to fit in R
Message-ID: <41C980E0.7020207@to.infn.it>

Good morning,
in my work I need to study data distributions and so I  need to fit the 
experimental distribution by theoretical curves such as normal, Poison, 
binomial and so on.  I'd like to know, given a vector of data, for example

x<-rnorm (1000, 10)

if they follow a normal distribution. I'd like to do a fit (to estimate 
the parameters of the theoretical distribution) and then a goodness test.
Can you suggest me any R package or manuals about this issue? The 
documentation on the R-guide isn't sufficient to me.
Thank you of your help, Angela



From bxc at steno.dk  Wed Dec 22 15:13:31 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 22 Dec 2004 15:13:31 +0100
Subject: [R] Make a table
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE217@exdkba022.novo.dk>

try:

tapply( Z, list( X, Y ), mean )

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Muhammad Subianto
> Sent: Wednesday, December 22, 2004 2:26 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Make a table
> 
> 
> Dear useR,
> 
> I have a dataset like this below,
>  > prevRND.dat   <- read.table("C:\\workdir\\prevRND.txt",
> +                           header=FALSE,  # No header.
> +                           col.names = c("X","Y","Z"),
> +                           sep = ",")
>  > prevRND.dat
>    X Y        Z
> 1 A A 0.950933
> 2 A B 0.143600
> 3 A C 0.956133
> 4 B A 0.000533
> 5 B B 0.986467
> 6 B C 0.032066
> 7 C A 0.005333
> 8 C B 0.000000
> 9 C C 0.009266
> 
> How can I make that data above as table,
> 
> 	        Y
> X    A          B         C
> A  0.950933 0.143600 0.956133
> B  0.000533 0.986467 0.032066
> C  0.005333 0.000000 0.009266
> 
> I cannot use table() or ftable() functions because the 3rd 
> column (Z) is 
> probability. Are there any function to make a table as I want?
> 
> Kind regards,
> Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From MSchwartz at MedAnalytics.com  Wed Dec 22 15:14:28 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 22 Dec 2004 08:14:28 -0600
Subject: [R] Make a table
In-Reply-To: <41C975E9.7050804@cs.uu.nl>
References: <41C975E9.7050804@cs.uu.nl>
Message-ID: <1103724869.15858.10.camel@horizons.localdomain>

On Wed, 2004-12-22 at 14:26 +0100, Muhammad Subianto wrote:
> Dear useR,
> 
> I have a dataset like this below,
>  > prevRND.dat   <- read.table("C:\\workdir\\prevRND.txt",
> +                           header=FALSE,  # No header.
> +                           col.names = c("X","Y","Z"),
> +                           sep = ",")
>  > prevRND.dat
>    X Y        Z
> 1 A A 0.950933
> 2 A B 0.143600
> 3 A C 0.956133
> 4 B A 0.000533
> 5 B B 0.986467
> 6 B C 0.032066
> 7 C A 0.005333
> 8 C B 0.000000
> 9 C C 0.009266
> 
> How can I make that data above as table,
> 
> 	        Y
> X    A          B         C
> A  0.950933 0.143600 0.956133
> B  0.000533 0.986467 0.032066
> C  0.005333 0.000000 0.009266
> 
> I cannot use table() or ftable() functions because the 3rd column (Z) is 
> probability. Are there any function to make a table as I want?

How about:

> xtabs(Z ~ X + Y, data = prevRND.dat)
   Y
X   A        B        C       
  A 0.950933 0.143600 0.956133
  B 0.000533 0.986467 0.032066
  C 0.005333 0.000000 0.009266

HTH,

Marc Schwartz



From subianto at cs.uu.nl  Wed Dec 22 15:35:50 2004
From: subianto at cs.uu.nl (Muhammad Subianto)
Date: Wed, 22 Dec 2004 15:35:50 +0100
Subject: [R] Make a table
In-Reply-To: <41C975E9.7050804@cs.uu.nl>
References: <41C975E9.7050804@cs.uu.nl>
Message-ID: <41C98646.20600@cs.uu.nl>

Thanks to all of you.
That's what I want.

Best wishes,
Muhammad Subianto


Try

tapply(prevRND.dat$Z, list(X=prevRND.dat$X, Y=prevRND.dat$Y), mean)

__________________________________
Andy Jaworskitry:


try:

tapply( Z, list( X, Y ), mean )

----------------------
Bendix Carstensen
How about:


>> xtabs(Z ~ X + Y, data = prevRND.dat)
>  
>
   Y
X   A        B        C       
  A 0.950933 0.143600 0.956133
  B 0.000533 0.986467 0.032066
  C 0.005333 0.000000 0.009266

HTH,

Marc Schwartz



From Achim.Zeileis at wu-wien.ac.at  Wed Dec 22 16:10:28 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 22 Dec 2004 16:10:28 +0100
Subject: [R] how to fit in R
In-Reply-To: <41C980E0.7020207@to.infn.it>
References: <41C980E0.7020207@to.infn.it>
Message-ID: <20041222161028.71249807.Achim.Zeileis@wu-wien.ac.at>

On Wed, 22 Dec 2004 15:12:48 +0100 Angela Re wrote:

> Good morning,
> in my work I need to study data distributions and so I  need to fit
> the experimental distribution by theoretical curves such as normal,
> Poison, binomial and so on.  I'd like to know, given a vector of data,
> for example
> 
> x<-rnorm (1000, 10)
> 
> if they follow a normal distribution. I'd like to do a fit (to
> estimate the parameters of the theoretical distribution) and then a
> goodness test.

For fitting distributions (via ML) look at
  ?fitdistr
in package MASS. Goodness-of-fit testing after the estimation of
parameters might not be straightforward, but there are several solutions
for testing normality in package nortest. Also look at ?shapiro.test and
?goodfit in package vcd. 

All purpose gof tests are also available, see ?ks.test and ?chisq.test.
Z

> Can you suggest me any R package or manuals about this issue? The 
> documentation on the R-guide isn't sufficient to me.
> Thank you of your help, Angela
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From br44114 at yahoo.com  Wed Dec 22 16:30:04 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 22 Dec 2004 07:30:04 -0800 (PST)
Subject: [R] how to fit in R
Message-ID: <20041222153004.74151.qmail@web50307.mail.yahoo.com>

See
http://www.statsoft.com/textbook/stdisfit.html
There are several approaches you can use - Chi-square, Q-Q plots, P-P
plots, various tests (Kolmogorov-Smirnov, Shapiro-Wilks' W) etc. 

HTH,
b.


-----Original Message-----
From: Angela Re
Sent: Wednesday, December 22, 2004 9:13 AM
To: r-help at stat.math.ethz.ch
Subject: [R] how to fit in R


Good morning,
in my work I need to study data distributions and so I  need to fit
the 
experimental distribution by theoretical curves such as normal,
Poison, 
binomial and so on.  I'd like to know, given a vector of data, for
example

x<-rnorm (1000, 10)

if they follow a normal distribution. I'd like to do a fit (to
estimate 
the parameters of the theoretical distribution) and then a goodness
test.
Can you suggest me any R package or manuals about this issue? The 
documentation on the R-guide isn't sufficient to me.
Thank you of your help, Angela

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From angelare at to.infn.it  Wed Dec 22 16:33:52 2004
From: angelare at to.infn.it (Angela Re)
Date: Wed, 22 Dec 2004 16:33:52 +0100
Subject: [R] MASS installation
Message-ID: <41C993E0.6010604@to.infn.it>

Good evening,
I need to fit experimental distributions by theoretical ones. I know 
that it is possible in the package MASS but I don't know how to install 
it. Can you help me?
Thank you, Angela



From michael.watson at bbsrc.ac.uk  Wed Dec 22 16:45:23 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 22 Dec 2004 15:45:23 -0000
Subject: [R] Preformatted text in Rd files
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B900@iahce2knas1.iah.bbsrc.reserved>

A quickie - how do I include preformatted text within the "details" section of an .Rd file?

Thanks
Mick



From Achim.Zeileis at wu-wien.ac.at  Wed Dec 22 17:11:04 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 22 Dec 2004 17:11:04 +0100
Subject: [R] MASS installation
In-Reply-To: <41C993E0.6010604@to.infn.it>
References: <41C993E0.6010604@to.infn.it>
Message-ID: <20041222171104.553049fa.Achim.Zeileis@wu-wien.ac.at>

On Wed, 22 Dec 2004 16:33:52 +0100 Angela Re wrote:

> Good evening,
> I need to fit experimental distributions by theoretical ones. I know 
> that it is possible in the package MASS but I don't know how to
> install it. Can you help me?

Please do read the basic manuals such as "An Introduction to R" and also
look at the FAQs. 

Hint: Packages can be installed with install.packages() but you might
want to check if MASS is already installed...
Z


> Thank you, Angela
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From vito_ricci at yahoo.com  Wed Dec 22 17:16:03 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 22 Dec 2004 17:16:03 +0100 (CET)
Subject: [R] Re:MASS installation
Message-ID: <20041222161603.26104.qmail@web41214.mail.yahoo.com>

Dear Angela,

to install MASS package you can choose installing from
CRAN.
Go to menu Packages--> Install package(s) from CRAN;
appears a window (Select) select MASS; in this way
MASS will be downloaded - you need to be connected
with Internet, preferibly ADSL, it'll be faster;
look in Packages-->Load package appears a window
(Select) see in MASS has been already installed and it
needs only to be loaded. Choose MASS and it'll be
loaded. Alternatively from command line type:

library(MASS)

Regards
Vito



You wrote:

Good evening,
I need to fit experimental distributions by
theoretical ones. I know 
that it is possible in the package MASS but I don't
know how to install 
it. Can you help me?
Thank you, Angela

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From tlumley at u.washington.edu  Wed Dec 22 17:26:37 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 22 Dec 2004 08:26:37 -0800 (PST)
Subject: [R] GAM: Getting standard errors from the parametric terms in
	a GAM model
In-Reply-To: <NC-TDA03qMoZmi3xlDi000005be@mail.nctda.org>
References: <NC-TDA03qMoZmi3xlDi000005be@mail.nctda.org>
Message-ID: <Pine.A41.4.61b.0412220818280.384708@homer10.u.washington.edu>

On Tue, 21 Dec 2004, Jean G. Orelien wrote:

> I am new to R.  I'm using the function GAM and wanted to get standard errors
> and p-values for the parametric terms (I fitted a semi-parametric models).
> Using the function anova() on the object from GAM, I only get  p-values for
> the nonparametric terms.
>
>
>
> Does anyone know if and how to get standard errors for the parametric terms?
>

If you mean gam() in the gam package then, yes, someone does but it hasn't 
been included in the package yet.  It is described in the current issue of 
JASA.  Code for S-PLUS is supposed to be at
   http://www.ihapss.jsph.edu/software/
but that is currently not working.

 	-thomas



From Achim.Zeileis at wu-wien.ac.at  Wed Dec 22 17:53:13 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 22 Dec 2004 17:53:13 +0100
Subject: [R] Re:MASS installation
In-Reply-To: <20041222161603.26104.qmail@web41214.mail.yahoo.com>
References: <20041222161603.26104.qmail@web41214.mail.yahoo.com>
Message-ID: <20041222175313.200a4f29.Achim.Zeileis@wu-wien.ac.at>

On Wed, 22 Dec 2004 17:16:03 +0100 (CET) Vito Ricci wrote:

> Dear Angela,
> 
> to install MASS package you can choose installing from
> CRAN.
> Go to menu Packages--> Install package(s) from CRAN;

This is under Windows... Angela hasn't told us the OS she's using,
though.

> appears a window (Select) select MASS; in this way

The chances of finding MASS there are rather low. You might be able to
see the VR bundle though.

> MASS will be downloaded - you need to be connected
> with Internet, preferibly ADSL, it'll be faster;
> look in Packages-->Load package appears a window
> (Select) see in MASS has been already installed and it

It is a recommended package and should therefore be part of every binary
distribution. In short: it is almost surely installed on Angela's
machine.
Z

> needs only to be loaded. Choose MASS and it'll be
> loaded. Alternatively from command line type:
> 
> library(MASS)
> 
> Regards
> Vito
> 
> 
> 
> You wrote:
> 
> Good evening,
> I need to fit experimental distributions by
> theoretical ones. I know 
> that it is possible in the package MASS but I don't
> know how to install 
> it. Can you help me?
> Thank you, Angela
> 
> =====
> Diventare costruttori di soluzioni
> Became solutions' constructors
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> Top 10 reasons to become a Statistician
> 
>      1. Deviation is considered normal
>      2. We feel complete and sufficient
>      3. We are 'mean' lovers
>      4. Statisticians do it discretely and continuously
>      5. We are right 95% of the time
>      6. We can legally comment on someone's posterior distribution
>      7. We may not be normal, but we are transformable
>      8. We never have to say we are certain
>      9. We are honestly significantly different
>     10. No one wants our jobs
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese 
> http://www.modugno.it/archivio/palese/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From sue at xlsolutions-corp.com  Wed Dec 22 19:08:46 2004
From: sue at xlsolutions-corp.com (sue@xlsolutions-corp.com)
Date: Wed, 22 Dec 2004 11:08:46 -0700
Subject: [R] Course***R/S-plus Fundamentals and Programming Techniques @ 4
	locations, January 2005
Message-ID: <20041222180846.23818.qmail@webmail02.mesa1.secureserver.net>

Happy New Year!

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to 
announce  2-day "R/S-plus Fundamentals and Programming 
Techniques".

****Chicago, IL ---------------------------- January 13th-14th, 2005
****San Francisco, CA ---------------------- January 13th-14th, 2005

****Washington, DC ------------------------- January 27th-28th, 2005
****Boston, MA   --------------------------- January 27th-28th, 2005


Reserve your seat now at the early bird rates! Payment due AFTER 
the class.


Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a 
broad spectrum of topics, from reading raw data to a comparison of R 
and S. We will learn the essentials of data manipulation, graphical 
visualization and R/S-plus programming. We will explore statistical 
data analysis tools,including graphics with data sets. How to enhance 
your plots. We will perform basic statistics and fit linear regression
models. Participants are encouraged to bring data for interactive 
sessions


With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com 
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm 
Please let us know if you and your colleagues are interested in this 
classto take advantage of group discount. Register now to secure your 
seat! Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com 
elvis at xlsolutions-corp.com



From ihaka at stat.auckland.ac.nz  Wed Dec 22 20:04:19 2004
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Thu, 23 Dec 2004 08:04:19 +1300
Subject: [R] Creating a vector of colours that are as different from one
	another as possible
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A5F@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89A5F@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41C9C533.1030805@stat.auckland.ac.nz>

michael watson (IAH-C) wrote:
> Hi
> 
> I want to create a vector of colors that are as different from one
> another as possible.  ?rainbow states "Conceptually, all of these
> functions actually use (parts of) a line cut out of the 3-dimensional
> color space...".  This suggests to me that the resulting colors are all
> placed on this "line" and are equi-distant along it.  The resulting
> color palette is a range of colours where adjacent colours are actually
> quite similar, especially when n (the number of colours) is high.
> 
> Conceptually I guess what I want is colors from a 3D polygon in 3D
> colour space, where the number of vertices in the polygon is n,
> resulting in a color palette where the colors are all quite different
> from one another.  Is this possible or am I talking crap? (I've only had
> one coffee this morning)

First, you want to be using a color space where distance corresponds
to perception of similarity/difference.  There are a number of
perceptually based spaces which approximate this (CIELUV, CIELAB
and Munsell).  You could put your points in one of these spaces
and them move them about until they are as far from each other
as possible.

Second, it's generally considered bad practice to use more that
six colors in a single display.  Given the approximate nature of
the uniformity of the perceptual color spaces you could consider
placing this many points "by hand."

How much sense this makes depends on what it is you are trying to do.

(I found reading some of what Munsell had to say pretty informative.)

-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From David.Brahm at geodecapital.com  Wed Dec 22 21:11:28 2004
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Wed, 22 Dec 2004 15:11:28 -0500
Subject: [R] outer(-x, x, pmin) cannot allocate
Message-ID: <6AF7541D27821A4BAB515245C1A2FEED03776B50@MSGBOSCLC2WIN.DMN1.FMR.COM>

R> x <- 0. + 1:8000
R> y <- outer(-x, x, pmin)
Error: cannot allocate vector of size 1000000 Kb

Why does R need to allocate a gigabyte to create an 8000 x 8000 matrix?
It doesn't have any trouble with outer(-x, x, "+").  Thanks.

-- David Brahm (brahm at alum.mit.edu)

Version:
 platform = i686-pc-linux-gnu
 arch = i686
 os = linux-gnu
 system = i686, linux-gnu
 status = 
 major = 2
 minor = 0.1
 year = 2004
 month = 11
 day = 15
 language = R



From niels.waller at vanderbilt.edu  Wed Dec 22 21:45:20 2004
From: niels.waller at vanderbilt.edu (Niels Waller)
Date: Wed, 22 Dec 2004 14:45:20 -0600
Subject: [R] Creating packages in windoze: *** [indices] Error 1
Message-ID: <200412222045.iBMKjJv5016196@imap3.mail.vanderbilt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041222/61216b84/attachment.pl

From p.dalgaard at biostat.ku.dk  Wed Dec 22 23:24:18 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Dec 2004 23:24:18 +0100
Subject: [R] outer(-x, x, pmin) cannot allocate
In-Reply-To: <6AF7541D27821A4BAB515245C1A2FEED03776B50@MSGBOSCLC2WIN.DMN1.FMR.COM>
References: <6AF7541D27821A4BAB515245C1A2FEED03776B50@MSGBOSCLC2WIN.DMN1.FMR.COM>
Message-ID: <x2acs6j7jh.fsf@biostat.ku.dk>

"Brahm, David" <David.Brahm at geodecapital.com> writes:

> R> x <- 0. + 1:8000
> R> y <- outer(-x, x, pmin)
> Error: cannot allocate vector of size 1000000 Kb
> 
> Why does R need to allocate a gigabyte to create an 8000 x 8000 matrix?
> It doesn't have any trouble with outer(-x, x, "+").  Thanks.

Hmm, it does run the process size to over 5GB on systems that do not
crash. An 8000x8000 matrix of doubles takes about 0.5 GB, and the way
outer() works, it expands both arguments to vectors of the same length
as the result.

So needing a GB is not too surprising, but something does seem to be a
bit memory hungry. Looking inside pmin, it is pretty clear that it
wasn't built for memory efficiency...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gunter.berton at gene.com  Wed Dec 22 23:30:29 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 22 Dec 2004 14:30:29 -0800
Subject: [R] outer(-x, x, pmin) cannot allocate
In-Reply-To: <6AF7541D27821A4BAB515245C1A2FEED03776B50@MSGBOSCLC2WIN.DMN1.FMR.COM>
Message-ID: <200412222230.iBMMUTw5020558@hertz.gene.com>

David:

In general, this is not a good question to ask, as one needs to go into the
bowels of R to find an answer.

But note that 8000 x 8000 x 4 bytes for double precision  = 256 mb. Now look
at the code of outer(). Two vectors of this size are created = 512mb. Then
copies of these must be created to be passed into pmin, I believe, as R
passes by value. That's 1gb.

My guess is that "+", as an internal function, avoids the final doubling.

Corrections/clarifications by knowledgeable R experts cheerfully welcomed.
I'm on thin ice here.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Brahm, David
> Sent: Wednesday, December 22, 2004 12:11 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] outer(-x, x, pmin) cannot allocate
> 
> R> x <- 0. + 1:8000
> R> y <- outer(-x, x, pmin)
> Error: cannot allocate vector of size 1000000 Kb
> 
> Why does R need to allocate a gigabyte to create an 8000 x 
> 8000 matrix?
> It doesn't have any trouble with outer(-x, x, "+").  Thanks.
> 
> -- David Brahm (brahm at alum.mit.edu)
> 
> Version:
>  platform = i686-pc-linux-gnu
>  arch = i686
>  os = linux-gnu
>  system = i686, linux-gnu
>  status = 
>  major = 2
>  minor = 0.1
>  year = 2004
>  month = 11
>  day = 15
>  language = R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rpeng at jhsph.edu  Wed Dec 22 23:48:19 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 22 Dec 2004 17:48:19 -0500
Subject: [R] outer(-x, x, pmin) cannot allocate
In-Reply-To: <200412222230.iBMMUTw5020558@hertz.gene.com>
References: <200412222230.iBMMUTw5020558@hertz.gene.com>
Message-ID: <41C9F9B3.5040001@jhsph.edu>

Actually, I believe it's 8 bytes for double precision.

-roger

Berton Gunter wrote:
> David:
> 
> In general, this is not a good question to ask, as one needs to go into the
> bowels of R to find an answer.
> 
> But note that 8000 x 8000 x 4 bytes for double precision  = 256 mb. Now look
> at the code of outer(). Two vectors of this size are created = 512mb. Then
> copies of these must be created to be passed into pmin, I believe, as R
> passes by value. That's 1gb.
> 
> My guess is that "+", as an internal function, avoids the final doubling.
> 
> Corrections/clarifications by knowledgeable R experts cheerfully welcomed.
> I'm on thin ice here.
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>  
>  
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Brahm, David
>>Sent: Wednesday, December 22, 2004 12:11 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] outer(-x, x, pmin) cannot allocate
>>
>>R> x <- 0. + 1:8000
>>R> y <- outer(-x, x, pmin)
>>Error: cannot allocate vector of size 1000000 Kb
>>
>>Why does R need to allocate a gigabyte to create an 8000 x 
>>8000 matrix?
>>It doesn't have any trouble with outer(-x, x, "+").  Thanks.
>>
>>-- David Brahm (brahm at alum.mit.edu)
>>
>>Version:
>> platform = i686-pc-linux-gnu
>> arch = i686
>> os = linux-gnu
>> system = i686, linux-gnu
>> status = 
>> major = 2
>> minor = 0.1
>> year = 2004
>> month = 11
>> day = 15
>> language = R
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From tlumley at u.washington.edu  Thu Dec 23 00:06:16 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 22 Dec 2004 15:06:16 -0800 (PST)
Subject: [R] outer(-x, x, pmin) cannot allocate
In-Reply-To: <x2acs6j7jh.fsf@biostat.ku.dk>
References: <6AF7541D27821A4BAB515245C1A2FEED03776B50@MSGBOSCLC2WIN.DMN1.FMR.COM>
	<x2acs6j7jh.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.61b.0412221504300.98750@homer04.u.washington.edu>

On Wed, 22 Dec 2004, Peter Dalgaard wrote:

> "Brahm, David" <David.Brahm at geodecapital.com> writes:
>
>> R> x <- 0. + 1:8000
>> R> y <- outer(-x, x, pmin)
>> Error: cannot allocate vector of size 1000000 Kb
>>
>> Why does R need to allocate a gigabyte to create an 8000 x 8000 matrix?
>> It doesn't have any trouble with outer(-x, x, "+").  Thanks.
>
> Hmm, it does run the process size to over 5GB on systems that do not
> crash. An 8000x8000 matrix of doubles takes about 0.5 GB, and the way
> outer() works, it expands both arguments to vectors of the same length
> as the result.
>
> So needing a GB is not too surprising, but something does seem to be a
> bit memory hungry. Looking inside pmin, it is pretty clear that it
> wasn't built for memory efficiency...
>

It's not just needing a Gb that was surprising, but needing a single 
object of size 1Gb.  However, that is because pmin() cbind()s its 
arguments together, giving a 8000x8000x2 array:  8000x8000x2x8=1Gb.

 	-thomas



From ggrothendieck at myway.com  Thu Dec 23 00:34:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 22 Dec 2004 23:34:02 +0000 (UTC)
Subject: [R] Creating packages in windoze: *** [indices] Error 1
References: <200412222045.iBMKjJv5016196@imap3.mail.vanderbilt.edu>
Message-ID: <loom.20041223T003027-412@post.gmane.org>

Niels Waller <niels.waller <at> vanderbilt.edu> writes:

: 
: Dear R community, 
: 
: I am running R 2.0.1 on a Windoze XP OS.  I recently upgraded from R 1.9x to
: 2.0.1 and I am currently 
: upgrading a my personal function packages. My other packages compiled
: without 
: a hitch but I am having a difficult time with my largest package.  
: Can someone please help me with the following error message  
: "Error in parse(file, n, text, prompt) : syntax error on line 1)"
: 
: I have searched the "Creating R packages" 
: manual but have been unable to determine how to solve my problem. 
: 
: Thank you in advance for any and all help.
: 
: Niels Waller
: Vanderbilt University
: 
: H:\R\rw2001\src\gnuwin32>rcmd build --binary --use-zip taxon
: * checking for file 'taxon/DESCRIPTION' ... OK
: installing R.css in h:/TEMP/Rbuild.1020
: 
: ---------- Making package taxon ------------
:   adding build stamp to DESCRIPTION
:   installing R files
:   installing inst files
:   installing data files
:   installing man source files
:   installing indices
: Error in parse(file, n, text, prompt) : syntax error on line 1
: Execution halted
: make[2]: *** [indices] Error 1
: make[1]: *** [all] Error 2
: make: *** [pkg-taxon] Error 2
: *** Installation of taxon failed ***
: 
: Removing 'h:/TEMP/Rbuild.1020/taxon'
:  ERROR
: * installation failed
: 
: * building 'taxon_2.0-0.zip'
:         zip warning: name not matched: taxon
: 
: zip error: Nothing to do! (try: zip -r9X
: H:/R/rw2001/src/gnuwin32/taxon_2.0-0.zip . -i taxon)
: 
: H:\R\rw2001\src\gnuwin32>echo rcmd rd2dvi.sh --pdf taxon
: rcmd rd2dvi.sh --pdf taxon
: H:\R\rw2001\src\gnuwin32>
:

I have found that the messages are not necessarily informative of what
the problem is.  I suggest you try sourcing each of your source files into
R to see if it chokes on any of them.

If that does not work take half out of your package and rebuild and
continue using binary search until you have narrowed it down to the 
culprit.



From p.dalgaard at biostat.ku.dk  Thu Dec 23 00:33:16 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Dec 2004 00:33:16 +0100
Subject: [R] outer(-x, x, pmin) cannot allocate
In-Reply-To: <200412222230.iBMMUTw5020558@hertz.gene.com>
References: <200412222230.iBMMUTw5020558@hertz.gene.com>
Message-ID: <x2652tkiwz.fsf@biostat.ku.dk>

Berton Gunter <gunter.berton at gene.com> writes:

> David:
> 
> In general, this is not a good question to ask, as one needs to go into the
> bowels of R to find an answer.
> 
> But note that 8000 x 8000 x 4 bytes for double precision  = 256 mb. Now look
> at the code of outer(). Two vectors of this size are created = 512mb. Then
> copies of these must be created to be passed into pmin, I believe, as R
> passes by value. That's 1gb.
> 
> My guess is that "+", as an internal function, avoids the final doubling.
> 
> Corrections/clarifications by knowledgeable R experts cheerfully welcomed.
> I'm on thin ice here.

Well it's 8 bytes to a double, not 4...   If you look inside pmin,
you'll see the first couple of lines saying:

    elts <- list(...)
    mmm <- as.vector(elts[[1]])
    has.na <- FALSE
    for (each in elts[-1]) {
        work <- cbind(mmm, as.vector(each))
        nas <- is.na(work)

which by my counts takes about 6 copies (2 in "elts", 1 in "mmm", 1 in
"elts[-1]", 2 in "work") in addition to the 2 input vectors + a
logical vector of the same length of "work". And that is before
actually operating on anything! You can never be quite sure that the
copying actually takes place since R tries to do virtual copies if it
can, but the empirical data suggests that it does get to something
like 10 or 11 copies in total.

However, somewhat surprisingly, this doesn't help a whole lot:

  x <- 0. + 1:8000
  mypmin <- function(x,y)ifelse(x<y,x,y)
  y <- outer(-x, x, mypmin)

This version seems a little better, but still crosses the 3 GB line
(hmm, rm(y) had probably saved half a GB):
  
  mypmin <- function(x,y) {ix <- x<y; y[ix] <- x[ix]; y}

The "+" variant runs in 1.5 GB which would seem to be the smallest
you can hope for.
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gcutler at amgen.com  Thu Dec 23 01:09:29 2004
From: gcutler at amgen.com (Gene Cutler)
Date: Wed, 22 Dec 2004 16:09:29 -0800
Subject: [R] Get rid of space padding
In-Reply-To: <200412221129.iBMBKsBH002811@hypatia.math.ethz.ch>
References: <200412221129.iBMBKsBH002811@hypatia.math.ethz.ch>
Message-ID: <E9DD4DE6-5476-11D9-AC31-000A95C91324@amgen.com>

I'm currently using the below function from some library (MASS?) for
writing my data out to file.  I'm using it instead of plain old "write"
because it does buffering.  The problem that I'm having is that the
numbers are space padded, but I need true tab-delineated files.  It
looks like the spaces are coming from 'format', but I don't see
an option for format to not pad numbers, the closest I see has to do
with stripping spaces from strings.  Am I missing something obvious?


write.matrix <- function (x, file = "", sep = "\t", blocksize=2000)
{
     x <- as.matrix(x)
     p <- ncol(x)
     cn <- colnames(x)
     if (!missing(blocksize) && blocksize > 0) {
         cat(cn, file = file, sep = c(rep(sep, p - 1), "\n"))
         nlines <- 0
         nr <- nrow(x)
         while (nlines < nr) {
             nb <- min(blocksize, nr - nlines)
             cat(format(t(x[nlines + (1:nb), ])), file = file,
                 append = TRUE, sep = c(rep(sep, p - 1), "\n"))
             nlines <- nlines + nb
         }
     }
     else cat(c(cn, format(t(x))), file = file, sep = c(rep(sep, p - 1), 
"\n"))
}


Thanks.


-- Gene



From p.dalgaard at biostat.ku.dk  Thu Dec 23 01:37:00 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Dec 2004 01:37:00 +0100
Subject: [R] Get rid of space padding
In-Reply-To: <E9DD4DE6-5476-11D9-AC31-000A95C91324@amgen.com>
References: <200412221129.iBMBKsBH002811@hypatia.math.ethz.ch>
	<E9DD4DE6-5476-11D9-AC31-000A95C91324@amgen.com>
Message-ID: <x2k6r9voib.fsf@biostat.ku.dk>

Gene Cutler <gcutler at amgen.com> writes:

> I'm currently using the below function from some library (MASS?) for
> writing my data out to file.  I'm using it instead of plain old "write"
> because it does buffering.  The problem that I'm having is that the
> numbers are space padded, but I need true tab-delineated files.  It
> looks like the spaces are coming from 'format', but I don't see
> an option for format to not pad numbers, the closest I see has to do
> with stripping spaces from strings.  Am I missing something obvious?

write.table(), maybe?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec 23 02:00:33 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 23 Dec 2004 01:00:33 -0000 (GMT)
Subject: [R] Get rid of space padding
In-Reply-To: <E9DD4DE6-5476-11D9-AC31-000A95C91324@amgen.com>
Message-ID: <XFMail.041223010033.Ted.Harding@nessie.mcc.ac.uk>

On 23-Dec-04 Gene Cutler wrote:
> I'm currently using the below function from some library (MASS?) for
> writing my data out to file.  I'm using it instead of plain old "write"
> because it does buffering.  The problem that I'm having is that the
> numbers are space padded, but I need true tab-delineated files.  It
> looks like the spaces are coming from 'format', but I don't see
> an option for format to not pad numbers, the closest I see has to do
> with stripping spaces from strings.  Am I missing something obvious?
> 
> 
> write.matrix <- function (x, file = "", sep = "\t", blocksize=2000)
> {
>      x <- as.matrix(x)
>      p <- ncol(x)
>      cn <- colnames(x)
>      if (!missing(blocksize) && blocksize > 0) {
>          cat(cn, file = file, sep = c(rep(sep, p - 1), "\n"))
>          nlines <- 0
>          nr <- nrow(x)
>          while (nlines < nr) {
>              nb <- min(blocksize, nr - nlines)
>              cat(format(t(x[nlines + (1:nb), ])), file = file,
>                  append = TRUE, sep = c(rep(sep, p - 1), "\n"))
>              nlines <- nlines + nb
>          }
>      }
>      else cat(c(cn, format(t(x))), file = file,
>               sep = c(rep(sep, p - 1), "\n"))
> }

I think this may depend on your operating system. I just tried
your function as defined above:

> x<-rnorm(1000)
> x<-cbind(x,x,x)
> write.matrix(x,file="temp.write")

and then:

$ od -c temp.write | less
0000000   x  \t   x  \t   x  \n       0   .   3   8   7   7   4   7   9
0000020   2   1  \t       0   .   3   8   7   7   4   7   9   2   1  \t
0000040       0   .   3   8   7   7   4   7   9   2   1  \n   -   0   .
0000060   7   8   9   3   7   9   5   5   4  \t   -   0   .   7   8   9
0000100   3   7   9   5   5   4  \t   -   0   .   7   8   9   3   7   9
0000120   5   5   4  \n   -   1   .   3   3   0   4   9   1   1   8   9
0000140  \t   -   1   .   3   3   0   4   9   1   1   8   9  \t   -   1
[etc]

so, for me, the tabs are coming through as such.

(R-1.8.0, RH9 Linux)

What gives you the information that "\t" has expanded to spaces?
Often, writing a file out to a display, or importing it into an
editor (though you should be able to turn this off) expands tabs
to the appropriate numbers of spaces. So you may be seeing spaces
when the underlying file uses tabs. However, there are occasional
true spaces above where the number is not negative (which is what
arranges the line-up between negative and positive numbers).
Maybe this is your trouble:

$ less temp.write
x       x       x
 0.387747921     0.387747921     0.387747921
-0.789379554    -0.789379554    -0.789379554
-1.330491189    -1.330491189    -1.330491189
[etc.] (the tabs "\t" line up the "-"s; for positive numbers
an extra " " is needed).

Since 'format' is for "pretty-printing" it seems likely that
you will get such interspersed spacings whatever you do.
Why do you need to use format?

I just tried a version of your function in which each occurrence
of "format(...)" is replaced by "...", i.e. delete "format(" and ")".
It seems to work:

> write.matrix(x,file="temp.write")

$ less temp.write
x       x       x
0.387747920949271       0.387747920949271       0.387747920949271
-0.789379554483419      -0.789379554483419      -0.789379554483419
-1.33049118922334       -1.33049118922334       -1.33049118922334

$ od -c temp.write
0000000   x  \t   x  \t   x  \n   0   .   3   8   7   7   4   7   9   2
0000020   0   9   4   9   2   7   1  \t   0   .   3   8   7   7   4   7
0000040   9   2   0   9   4   9   2   7   1  \t   0   .   3   8   7   7
0000060   4   7   9   2   0   9   4   9   2   7   1  \n   -   0   .   7

so now it's just "\t" with no spaces. Of course this is  now printing
more decimal places, but you can control this with 'round'.

Please state: Your R version, your OS, and in what context you
are seeing spaces rather than tabs --  and why it matters!

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 23-Dec-04                                       Time: 01:00:33
------------------------------ XFMail ------------------------------



From gcutler at amgen.com  Thu Dec 23 02:19:53 2004
From: gcutler at amgen.com (Gene Cutler)
Date: Wed, 22 Dec 2004 17:19:53 -0800
Subject: [R] Get rid of space padding
In-Reply-To: <XFMail.041223010033.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041223010033.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <BF983CD6-5480-11D9-AC31-000A95C91324@amgen.com>


On Dec 22, 2004, at 5:00 PM, (Ted Harding) wrote:
>
> so, for me, the tabs are coming through as such.
>
> (R-1.8.0, RH9 Linux)
>
> What gives you the information that "\t" has expanded to spaces?
> Often, writing a file out to a display, or importing it into an
> editor (though you should be able to turn this off) expands tabs
>


I am getting tabs, but the values are being padded out to the tabs.
Here is one sample line with spaces replaced by '.' and tabs replaced
by '^'.

5...................^45597241............^16734145............^2.7128169 
8016131e-06^0.622804755173039...^0.91743119266055....^GB-4858-1- 
A.........^GB-4873-1-A.........

This is with R 2.0.1 both on Mandrake linux and Mac OS X.

Also, I know it's not an issue of a text editor altering the data as  
these file get
read in by perl scripts and R as well as my text editor of choice.



From br44114 at yahoo.com  Thu Dec 23 05:37:14 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 22 Dec 2004 20:37:14 -0800 (PST)
Subject: [R] combination of scatterplot and image graph
Message-ID: <20041223043714.33824.qmail@web50302.mail.yahoo.com>

Dear R users,

I'm interested in a combination of a scatterplot and an image graph.
I have two large vectors. Because in the scatterplot some areas are
sparsely and others densely populated, I want to see the points, and
I also want their color to be changed based on their density (similar
to a heat map). Is there a function that can do that?

Thank you,
b.




		
__________________________________ 

Send a seasonal email greeting and help others. Do good.



From deepayan at stat.wisc.edu  Thu Dec 23 06:02:22 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 22 Dec 2004 23:02:22 -0600
Subject: [R] combination of scatterplot and image graph
In-Reply-To: <20041223043714.33824.qmail@web50302.mail.yahoo.com>
References: <20041223043714.33824.qmail@web50302.mail.yahoo.com>
Message-ID: <200412222302.22211.deepayan@stat.wisc.edu>

On Wednesday 22 December 2004 22:37, bogdan romocea wrote:
> Dear R users,
>
> I'm interested in a combination of a scatterplot and an image graph.
> I have two large vectors. Because in the scatterplot some areas are
> sparsely and others densely populated, I want to see the points, and
> I also want their color to be changed based on their density (similar
> to a heat map). Is there a function that can do that?

Perhaps not exactly that, but you might be inerested in the hexbin 
package. For example, 

library(hexbin)
plot(hexbin(x = rnorm(5000), y = rnorm(5000)))

Deepayan



From yinglu at princeton.edu  Wed Dec 22 20:41:19 2004
From: yinglu at princeton.edu (Ying Lu)
Date: Wed, 22 Dec 2004 14:41:19 -0500
Subject: [R] [R-pkgs] eco 1.0-1 released
Message-ID: <200412221941.iBMJfJGo012380@smtpserver2.Princeton.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041222/1ee380dd/attachment.pl

From Tom.Mulholland at dpi.wa.gov.au  Thu Dec 23 08:46:17 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 23 Dec 2004 15:46:17 +0800
Subject: [R] How do you generate multiple sequences
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA56@afhex01.dpi.wa.gov.au>

I thought it would be quite simple to generate a number of sequences, but my only answer is to loop and this doesn't seem in keeping with R's capabilities.

In short I have the start and finish positions of several (well thousands probably) sequences. So I am looking at how do I take

x <- c(2,6,13,20)
y <- c(4,10,16,24)

 and produce a list with four components with the appropriate sequence

Something like
[[1]]
[1] 2 3 4

[[2]]
 [1] 6 7 8 9 10

[[3]]
 [1] 13 14 15 16

[[4]]
 [1] 20 21 22 23 24

So can someone point me in the direction of the ******* obvious



Tom Mulholland
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R



From ligges at statistik.uni-dortmund.de  Thu Dec 23 08:56:12 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 23 Dec 2004 08:56:12 +0100
Subject: [R] Preformatted text in Rd files
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B900@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B900@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41CA7A1C.7010101@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:
> A quickie - how do I include preformatted text within the "details" section of an .Rd file?
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Please read the manual "Writing R Extensions". In that manual, please 
look for "preformatted" (what you are asking for) and you will certainly 
find it in Section 2.3 ("Marking text").

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Thu Dec 23 09:05:29 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 23 Dec 2004 09:05:29 +0100
Subject: [R] How do you generate multiple sequences
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA56@afhex01.dpi.wa.gov.au>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA56@afhex01.dpi.wa.gov.au>
Message-ID: <41CA7C49.1030309@statistik.uni-dortmund.de>

Mulholland, Tom wrote:

> I thought it would be quite simple to generate a number of sequences, but my only answer is to loop and this doesn't seem in keeping with R's capabilities.
> 
> In short I have the start and finish positions of several (well thousands probably) sequences. So I am looking at how do I take
> 
> x <- c(2,6,13,20)
> y <- c(4,10,16,24)
> 
>  and produce a list with four components with the appropriate sequence
> 
> Something like
> [[1]]
> [1] 2 3 4
> 
> [[2]]
>  [1] 6 7 8 9 10
> 
> [[3]]
>  [1] 13 14 15 16
> 
> [[4]]
>  [1] 20 21 22 23 24
> 
> So can someone point me in the direction of the ******* obvious

See ?seq and ?mapply:

mapply(seq, x, y)


Uwe Ligges



> 
> 
> Tom Mulholland
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.0            
> year     2004           
> month    10             
> day      04             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec 23 09:46:17 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 23 Dec 2004 08:46:17 -0000 (GMT)
Subject: [R] Get rid of space padding
In-Reply-To: <BF983CD6-5480-11D9-AC31-000A95C91324@amgen.com>
Message-ID: <XFMail.041223084617.Ted.Harding@nessie.mcc.ac.uk>

On 23-Dec-04 Gene Cutler wrote:
> 
> On Dec 22, 2004, at 5:00 PM, (Ted Harding) wrote:
>>
>> so, for me, the tabs are coming through as such.
>>
>> (R-1.8.0, RH9 Linux)
>>
>> What gives you the information that "\t" has expanded to spaces?
>> Often, writing a file out to a display, or importing it into an
>> editor (though you should be able to turn this off) expands tabs
>>
> 
> 
> I am getting tabs, but the values are being padded out to the tabs.
> Here is one sample line with spaces replaced by '.' and tabs replaced
> by '^'.
> 
> 5...................^45597241............^16734145............^2.7128169
> 8016131e-06^0.622804755173039...^0.91743119266055....^GB-4858-1- 
> A.........^GB-4873-1-A.........
> 
> This is with R 2.0.1 both on Mandrake linux and Mac OS X.
> 
> Also, I know it's not an issue of a text editor altering the data as  
> these file get read in by perl scripts and R as well as my text editor
> of choice.

OK Gene, getting a clue here. I can reproduce similar behaviour
using your version of write.matrix by seting some elements to
character variables:

  x<-matrix(rnorm(30),ncol=3)
  x[1,1]<-"A"
  x[2,2]<-"B"
  x[3,3]<-"C"
  write.matrix(x,file="temp.write")

A                       0.0855822398994265      1.02493287358937  
2.17769486851001        B                       -0.310203876654049
-1.46891720382270       -0.756931913255919      C                 
0.177454935470461       -1.06532248163526       -0.413338129170855

and 'od -c temp.write' gives:

0000000   A                                                            
0000020          \t   0   .   0   8   5   5   8   2   2   3   9   8   9
0000040   9   4   2   6   5  \t   1   .   0   2   4   9   3   2   8   7
0000060   3   5   8   9   3   7          \n   2   .   1   7   7   6   9
0000100   4   8   6   8   5   1   0   0   1          \t   B            
0000120                                                          \t   -
0000140   0   .   3   1   0   2   0   3   8   7   6   6   5   4   0   4
0000160   9  \n   -   1   .   4   6   8   9   1   7   2   0   3   8   2
0000200   2   7   0      \t   -   0   .   7   5   6   9   3   1   9   1
0000220   3   2   5   5   9   1   9  \t   C                            
0000240                                          \n   0   .   1   7   7
0000260   4   5   4   9   3   5   4   7   0   4   6   1      \t   -   1
0000300   .   0   6   5   3   2   2   4   8   1   6   3   5   2   6    
0000320  \t   -   0   .   4   1   3   3   3   8   1   2   9   1   7   0
0000340   8   5   5  \n   -   0   .   8   2   4   0   5   9   9   6   4

where, as in your example, "short" results are padded out to
the position of the next tab with spaces.

However, when (as I suggested last time) I modify your function
'write.matrix' so as to remove occurrences of "format("...")"
(leaving only  the ... ) then it seems to be OK.

Now the first few lines of 'cat temp.write' are

A       0.0855822398994265      1.02493287358937
2.17769486851001        B       -0.310203876654049
-1.46891720382270       -0.756931913255919      C
0.177454935470461       -1.06532248163526       -0.413338129170855

and 'od -c temp.write' gives

0000000   A  \t   0   .   0   8   5   5   8   2   2   3   9   8   9   9
0000020   4   2   6   5  \t   1   .   0   2   4   9   3   2   8   7   3
0000040   5   8   9   3   7  \n   2   .   1   7   7   6   9   4   8   6
0000060   8   5   1   0   0   1  \t   B  \t   -   0   .   3   1   0   2
0000100   0   3   8   7   6   6   5   4   0   4   9  \n   -   1   .   4
0000120   6   8   9   1   7   2   0   3   8   2   2   7   0  \t   -   0
0000140   .   7   5   6   9   3   1   9   1   3   2   5   5   9   1   9
0000160  \t   C  \n   0   .   1   7   7   4   5   4   9   3   5   4   7
0000200   0   4   6   1  \t   -   1   .   0   6   5   3   2   2   4   8
0000220   1   6   3   5   2   6  \t   -   0   .   4   1   3   3   3   8

so that all the spaces have now disappeared, leaving only tabs.

The revised definition of "write.matrix" is:

write.matrix <- function (x, file = "", sep = "\t", blocksize=2000)
{
     x <- as.matrix(x)
     p <- ncol(x)
     cn <- colnames(x)
     if (!missing(blocksize) && blocksize > 0) { 
         cat(cn, file = file, sep = c(rep(sep, p - 1), "\n"))
         nlines <- 0 
         nr <- nrow(x)
         while (nlines < nr) { 
             nb <- min(blocksize, nr - nlines)
             cat(t(x[nlines + (1:nb), ]), file = file,
                 append = TRUE, sep = c(rep(sep, p - 1), "\n"))
             nlines <- nlines + nb
         }
     } 
     else cat(c(cn, t(x)), file = file,
              sep = c(rep(sep, p - 1), "\n"))
}

Hence, back to my earlier question: Why do you need "format" in
your function? This is what is generating the effect which you
don't want!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 23-Dec-04                                       Time: 08:46:17
------------------------------ XFMail ------------------------------



From francoisromain at free.fr  Thu Dec 23 09:59:30 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Thu, 23 Dec 2004 09:59:30 +0100
Subject: [R] combination of scatterplot and image graph
In-Reply-To: <20041223043714.33824.qmail@web50302.mail.yahoo.com>
References: <20041223043714.33824.qmail@web50302.mail.yahoo.com>
Message-ID: <41CA88F2.8040406@free.fr>

Hello,

you can fit a density using MASS::kde2d and then do a contour plot 
(?contour) and scatterplot your data :

x <- rnorm(500)
y <- rnorm(500)
require(MASS)
d <- kde2d(x,y,n=50)
image(d)
contour(d,add=T)
points(x,y,pch=20)

Romain.

bogdan romocea a ??crit :

>Dear R users,
>
>I'm interested in a combination of a scatterplot and an image graph.
>I have two large vectors. Because in the scatterplot some areas are
>sparsely and others densely populated, I want to see the points, and
>I also want their color to be changed based on their density (similar
>to a heat map). Is there a function that can do that?
>
>Thank you,
>b.
>  
>
-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann??e
Institut de Statistique de l'Universit?? de Paris (ISUP)
Fili??re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From liuqincn at yahoo.com  Thu Dec 23 10:05:43 2004
From: liuqincn at yahoo.com (liu qin)
Date: Thu, 23 Dec 2004 01:05:43 -0800 (PST)
Subject: [R] Re: How to calculate Ln(x) value and Exp(Ln(x)) value in R?
Message-ID: <20041223090543.41190.qmail@web12306.mail.yahoo.com>

 Hi, there:
 
 Does anybody know how to calculate Log value as Ln()
 and Exp( ) value in R please?
 
 Many thanks
 
 Qin



From david.meyer at wu-wien.ac.at  Thu Dec 23 10:07:58 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Thu, 23 Dec 2004 10:07:58 +0100
Subject: [R] How to interpret and modify "plot.svm"?
Message-ID: <20041223100758.06dfb618.david.meyer@wu-wien.ac.at>


> I updated the e1071 package but still can't find
> the other three arguments for plot.svm. 

It's in e1071 since version 1.5-3. (current version: 1.5-4).

> In addition, I can plot a
> gray-colored contour region by adding the argument "col = c(gray(0.2),
> gray(0.8))". But I failed to change those colored "x" or "o" points
> into the shapes I want. Basically, I don't want to have any color in
> the plot. Could you give me a hint how to do that?

Look at the example on the help page for plot.svm().

Best,
David.



From david.meyer at wu-wien.ac.at  Thu Dec 23 10:13:21 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Thu, 23 Dec 2004 10:13:21 +0100
Subject: [R] Rgui.exe - Error while tuning svm
Message-ID: <20041223101321.7d125cd0.david.meyer@wu-wien.ac.at>

> If I try to tune my svm with the code:

> Tune <- tune.svm(Data.Train, Class.Train, type="C-classification",
> kernel="radial", gamma = 2^(-1:1), cost = 2^(2:4))

> i get a windows Messagebox with a error in the application "Rgui.exe"
> and the message: "Die Anweisung in 0x6c48174d verweist auf Speicher
> 0x00000000. Der Vorgang "read" konnte nicht auf dem Speicher
> ausgef_hrt werden. ....."

Which version of e1071 are you using?
There has been a memory leak problem until 1.5-1 which could very well
cause this null pointer exception...

best,
David

-- 
Dr. David Meyer
Department of Information Systems

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From ligges at statistik.uni-dortmund.de  Thu Dec 23 10:41:14 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 23 Dec 2004 10:41:14 +0100
Subject: [R] Re: How to calculate Ln(x) value and Exp(Ln(x)) value in
 R?
In-Reply-To: <20041223090543.41190.qmail@web12306.mail.yahoo.com>
References: <20041223090543.41190.qmail@web12306.mail.yahoo.com>
Message-ID: <41CA92BA.4000409@statistik.uni-dortmund.de>

liu qin wrote:

>  Hi, there:
>  
>  Does anybody know how to calculate Log value as Ln()
>  and Exp( ) value in R please?


Do you know how to read the documentation and the posting guide? Please
do so before posting!

See ?log and ?exp

Uwe Ligges




>  Many thanks
>  
>  Qin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Thu Dec 23 11:07:47 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 23 Dec 2004 11:07:47 +0100
Subject: [R] Re: How to calculate Ln(x) value and Exp(Ln(x)) value in R?
In-Reply-To: <20041223090543.41190.qmail@web12306.mail.yahoo.com>
Message-ID: <41CAA703.24312.C0BADE@localhost>



On 23 Dec 2004 at 1:05, liu qin wrote:

>  Hi, there:
> 
>  Does anybody know how to calculate Log value as Ln()
>  and Exp( ) value in R please?

Is something wrong with

log(x)
log2(x)
exp(x)

Cheers
Petr


> 
>  Many thanks
> 
>  Qin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From r.alberts at rug.nl  Thu Dec 23 20:28:31 2004
From: r.alberts at rug.nl (Rudi Alberts)
Date: 23 Dec 2004 11:28:31 -0800
Subject: [R] subsetting a data.frame to the 'unique' of a column
Message-ID: <1103830111.1910.13.camel@gbic04>

Hi,

I often run into this problem:
I have a data.frame with one column containing entries that are not
unique. What I then want is a subset of the data.frame in which
the entries in that column have become the 'unique' of the original
column. 
Normally I program around it by taking the unique of the column and
making a new data.frame with it and filling the rest of the data.

(By the way, when moving to the smaller data.frame for example 5 rows
with the same value in that column will be replaced by one row for that
value. I don't mind which of the rows now..)


something like this, however, this gives me the complete df.

df[df$colname %in% unique(df$colname),]

or this, which doesnt work

df[df$colname == unique(df$colname),]


regards, R. Alberts



From p.dalgaard at biostat.ku.dk  Thu Dec 23 12:01:47 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Dec 2004 12:01:47 +0100
Subject: [R] subsetting a data.frame to the 'unique' of a column
In-Reply-To: <1103830111.1910.13.camel@gbic04>
References: <1103830111.1910.13.camel@gbic04>
Message-ID: <x24qidjn1g.fsf@biostat.ku.dk>

Rudi Alberts <r.alberts at rug.nl> writes:


> something like this, however, this gives me the complete df.
> 
> df[df$colname %in% unique(df$colname),]
> 
> or this, which doesnt work
> 
> df[df$colname == unique(df$colname),]


 df[!duplicated(df$colname),]


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gb at tal.stat.umu.se  Thu Dec 23 12:14:59 2004
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu, 23 Dec 2004 12:14:59 +0100
Subject: [R] subsetting a data.frame to the 'unique' of a column
In-Reply-To: <1103830111.1910.13.camel@gbic04>
References: <1103830111.1910.13.camel@gbic04>
Message-ID: <20041223111459.GA28726@tal.stat.umu.se>

On Thu, Dec 23, 2004 at 11:28:31AM -0800, Rudi Alberts wrote:
> Hi,
> 
> I often run into this problem:
> I have a data.frame with one column containing entries that are not
> unique. What I then want is a subset of the data.frame in which
> the entries in that column have become the 'unique' of the original
> column. 
> Normally I program around it by taking the unique of the column and
> making a new data.frame with it and filling the rest of the data.
> 
> (By the way, when moving to the smaller data.frame for example 5 rows
> with the same value in that column will be replaced by one row for that
> value. I don't mind which of the rows now..)
> 
> 
> something like this, however, this gives me the complete df.
> 
> df[df$colname %in% unique(df$colname),]
> 
> or this, which doesnt work
> 
> df[df$colname == unique(df$colname),]
> 
 Use 'duplicated':

> df[!duplicated(df$colname), ]

-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From Achim.Zeileis at wu-wien.ac.at  Thu Dec 23 13:37:25 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 23 Dec 2004 13:37:25 +0100 (CET)
Subject: [R] [R-pkgs] zoo 0.9-1
Message-ID: <Pine.LNX.4.58.0412231329140.23821@thorin.ci.tuwien.ac.at>

Dear useRs,

a new and much improved version of the zoo package for indexed totally
ordered observations (such as irregular time series) is available from
CRAN. It allows indexing observations with time/index vectors of arbitrary
class and extends many of the standard generic functions also available
for "ts" objects. Additionally, it allows conversion from/to other
(irregular) time series classes such as "its" and "irts". A vignette that
explains the features of zoo is also available in the package. For further
details see also the list of changes below.

Seasonal greetings,
Z


Changes in Version 0.9-1

  o new generic functions ORDER() and MATCH() (with
    order() and match() as the default) so that zoo()
    can handle  arbitrary index/time classes when
    suitable methods for the generic function c(),
    length(), order(), match() and subsetting [, are
    supplied.

  o improved printing of "zoo" objects and added
    a summary() method.

  o extended coercion functionality to and from
    "zoo" objects. "its" objects can be coerced to
    "zoo" and vice versa. "zoo" objects can be
    coerced to vector, matrix, data.frame or list.

  o added functionality to extract/assign to
    the coredata() of a "zoo" object.

  o added/improved functionality to extract/assign
    to the window() of a "zoo" object.

  o added/improved functionality to extract/assign
    to the index() or time() of a "zoo" object.

  o added lag(), diff(), start(), end(), head(),
    tail() methods.

  o improved plot.zoo() by more flexible expansion
    of plotting parameters such as col, lty and pch.

  o added a cbind() method for "zoo" objects (almost
    synonymous with merge())

  o NA handling for "zoo" objects via na.omit(),
    na.contiguous(), na.approx() and na.locf().

  o na.locf() generic function with default method (suitable
    for "zoo" objects) which implements Last Observation
    Carried Forward.

  o na.approx() generic function with default method (suitable
    for "zoo" objects) which implements elimination of NAs
    by interpolation.

  o added mathematical methods: group generic functions
    for "zoo" objects, t(), cumsum(), cumprod(), cummin(),
    and cummax().

  o added model.frame.AsIs and model.frame.zoo to support
    regression based on zoo objects, in particular with lm()
    (but also many other regression functions).

  o Zero length vector zoo objects may have non-zero index vectors
    intended to be used in merge to extend zoo objects. zoo
    changed to enable the creation of such objects by omitting
    first argument.

  o added a vignette explaining the new features

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From kjetil at acelerate.com  Thu Dec 23 14:56:49 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 23 Dec 2004 09:56:49 -0400
Subject: [R] Strange error from R CMD INSTALL
In-Reply-To: <Pine.GSO.4.31.0412082103020.8915-100000@markov.stats>
References: <Pine.GSO.4.31.0412082103020.8915-100000@markov.stats>
Message-ID: <41CACEA1.8030308@acelerate.com>

Brian D Ripley wrote:

>On Thu, 9 Dec 2004, Ray Brownrigg wrote:
>
>  
>
>>>Date: Wed, 08 Dec 2004 09:33:22 -0400
>>>From: Kjetil Brinchmann Halvorsen <kjetil at acelerate.com>
>>>
>>>I am trying to install a local package and get this unexpected
>>>error:
>>>
>>>---------- Making package UMSA ------------
>>>  adding build stamp to DESCRIPTION
>>>  installing R files
>>>  installing data files
>>>  installing man source files
>>>  installing indices
>>>Error: couldn't find function "na.omit"
>>>Execution halted
>>>
>>>na.omit of course is in package stats, and that is listed in the
>>>Depends field in the DESCRIPTION file.
>>>
>>>      
>>>
>>Check what is in your data directory.  Does something in there use
>>(implicitly) na.omit?
>>
>>I have seen something like this (since 2.0.0) where a .R file in the
>>data/ directory used a function within the package, but that function
>>was not 'available' at the "installing indices" phase of the
>>check/build.
>>    
>>
>
>(INSTALL, actually.) That's a plausible explanation.  From 200update.txt
>on developer.r-project.org
>
>  
>
That looked like a plausible explication, but is not the culprit here. I 
used Gabor's divide and conquer
strategy,  and found that by eliminating some files from the data  
subdirectory the error disappears and the
build completes. all of this files involves  SPSS .sav   files read with 
read.spss from package foreign
(I have latest version installed), and thereafter some data 
transformations.

However, moving the inflicted files to new test data only packages, they 
build without problems!

And, there is still left in my data subdirectory some SPSS .sav files 
which not makes problems.

Any ideas?

Kjetil

>2) data/*.R files must be self-sufficient, and in particular not
>   depend on the package or standard packages other than base.  (This
>   has always been documented, but is now enforced.)
>
>and see also `Writing R Extensions'.
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From baron at psych.upenn.edu  Thu Dec 23 16:02:00 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 23 Dec 2004 10:02:00 -0500
Subject: [R] searching Jonathan Baron's R Site
Message-ID: <20041223150200.GA4691@psych>

First, my site will be down December 27-28 because of a network
upgrade at Penn.  It will also be down at least one day before
that, while I upgrade the operating system.  (And another day
some time in January because of a planned power outage.)

Second, I have replaced the search engine in my R site:
http://finzi.psych.upenn.edu/

I am now using Namazu instead of HtDig.  The direct link to the
search page is
http://finzi.psych.upenn.edu/nmz.html

Namazu has capabilities that HtDig does not have, such as
wildcard searches.  (On the down side, its phrase searching works
fine for two word phrases, but a search for "A B C" will actually
produce something like "A B" or "B C", so it will give you more
than you want.  But HtDig's phrase searching seems completely
broken, so this is actually an improvement.)

The HtDig search engine is still there, but I do not plan to
update it anymore.  And it may stop working completely when I
upgrade from Fedora Core 2 to FC3 (as it almost did after the
last upgrade).

Here is a modification of Andy Liaw's search function.  I think
it works now, but I haven't tested it extensively.  It needs some
documentation.  But I'll wait to do that until I make a few more
changes in the site (in particular, collapsing all the mail
before 2002 into one index - without affecting existing links).

This function runs from within R and opens a web page with the
results of the search.  For example,

RSiteSearch("multiple imputation")

Jon
--

RSiteSearch <- function(string,
                        restrict=c("Rhelp02a","Rhelp01","Rhelp00","functions","docs"),
                        format="normal", sortby="score", matchesPerPage=10) {
    string <- paste("http://finzi.psych.upenn.edu/cgi-bin/namazu.cgi?query=",
                    gsub(" ", "+", string), sep="")
    mpp <- paste("max=", matchesPerPage, sep="")

    format <- charmatch(format, c("normal", "short"))
    if (format == 0) stop("format must be either normal or short")
    format <- paste("result=", switch(format, "normal", "short"), sep="")

    sortby <- charmatch(sortby, c("score", "date:late", "date:early",
                                  "field:subject:ascending", "field:subject:decending",
                                  "field:from:ascending", "field:from:decending",
                                  "field:size:ascending", "field:size:decending",
                                  "field:uri:ascending", "field:uri:decending",))
    if (sortby == 0) stop("wrong sortby specified")
    sortby <- paste("sort=",
                    switch(sortby, "score", "date:late", "date:early",
                                  "field:subject:ascending", "field:subject:decending",
                                  "field:from:ascending", "field:from:decending",
                                  "field:size:ascending", "field:size:decending",
                                  "field:uri:ascending", "field:uri:decending"),
                    sep="")

    res <- ""
    if ("Rhelp02a" %in% restrict) res <- "idxname=Rhelp02a"
    if ("Rhelp01" %in% restrict) res <- paste(res,"idxname=Rhelp01",sep="&")
    if ("Rhelp00" %in% restrict) res <- paste(res,"idxname=Rhelp00",sep="&")
    if ("Rhelpold" %in% restrict) res <- paste(res,"idxname=Rhelpold",sep="&")
    if ("docs" %in% restrict) res <- paste(res,"idxname=docs",sep="&")
    if ("functions" %in% restrict) res <- paste(res,"idxname=functions",sep="&")
    if (res=="") res <- paste("idxname=Rhelp02a&idxname=Rhelp01&idxname=Rhelp00&",
                              "idxname=functions&idxname=docs",sep="")
    res <- sub("^&+","",res)

    qstring <- paste(string, mpp, format, sortby, res, sep="&")
    browseURL(qstring)
    invisible(qstring)
}

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From vladutu at musc.edu  Thu Dec 23 17:32:27 2004
From: vladutu at musc.edu (Liviu M Vladutu)
Date: Thu, 23 Dec 2004 11:32:27 -0500
Subject: [R] tcltk problem
Message-ID: <1103819547.41caf31b7c036@webmail.musc.edu>



Hi all,
I have R Version 2.1.0 installed on a box running Redhat Fedora Core 2.
When I try:
library()
it shows the package tcltk.

But if I try:
library(tcltk) I get the same error message like in this thread:
http://tolstoy.newcastle.edu.au/R/help/01c/3418.html

The same, tcl and tk are installed in /usr/lib; if I try:
capabilities("tcltk")

I get:
tcltk
FALSE

I cant' find tcltk.so
Any suggestions please?
Thx in advance and Merry Xmas!
Liviu



From spencer.graves at pdf.com  Thu Dec 23 18:06:16 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 23 Dec 2004 09:06:16 -0800
Subject: [R] subsetting a data.frame to the 'unique' of a column
In-Reply-To: <20041223111459.GA28726@tal.stat.umu.se>
References: <1103830111.1910.13.camel@gbic04>
	<20041223111459.GA28726@tal.stat.umu.se>
Message-ID: <41CAFB08.3060706@pdf.com>

      What about "aggregate"? 

 DF <- data.frame(a=c(1,1,2), b=1:3, c=letters[1:3])
 aggregate(DF[2:3], DF[1], function(x)x[1])
  a b c
1 1 1 1
2 2 3 3

      hope this helps.  spencer graves

G??ran Brostr??m wrote:

>On Thu, Dec 23, 2004 at 11:28:31AM -0800, Rudi Alberts wrote:
>  
>
>>Hi,
>>
>>I often run into this problem:
>>I have a data.frame with one column containing entries that are not
>>unique. What I then want is a subset of the data.frame in which
>>the entries in that column have become the 'unique' of the original
>>column. 
>>Normally I program around it by taking the unique of the column and
>>making a new data.frame with it and filling the rest of the data.
>>
>>(By the way, when moving to the smaller data.frame for example 5 rows
>>with the same value in that column will be replaced by one row for that
>>value. I don't mind which of the rows now..)
>>
>>
>>something like this, however, this gives me the complete df.
>>
>>df[df$colname %in% unique(df$colname),]
>>
>>or this, which doesnt work
>>
>>df[df$colname == unique(df$colname),]
>>
>>    
>>
> Use 'duplicated':
>
>  
>
>>df[!duplicated(df$colname), ]
>>    
>>
>
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From MSchwartz at MedAnalytics.com  Thu Dec 23 18:32:05 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 23 Dec 2004 11:32:05 -0600
Subject: [R] tcltk problem
In-Reply-To: <1103819547.41caf31b7c036@webmail.musc.edu>
References: <1103819547.41caf31b7c036@webmail.musc.edu>
Message-ID: <1103823125.14780.17.camel@horizons.localdomain>

On Thu, 2004-12-23 at 11:32 -0500, Liviu M Vladutu wrote:
> 
> Hi all,
> I have R Version 2.1.0 installed on a box running Redhat Fedora Core 2.

Are you really running 2.1.0 (which is an unreleased development
version) or are you running 2.0.1, which is the present released
version?

What does the banner indicate when you first start R?

How did you install R (compile from source or use one of Martyn's RPMS)?

Also, please run the following command from a console and post the
results back:

rpm -q tcl tcl-devel


> When I try:
> library()
> it shows the package tcltk.
> 
> But if I try:
> library(tcltk) I get the same error message like in this thread:
> http://tolstoy.newcastle.edu.au/R/help/01c/3418.html
> 
> The same, tcl and tk are installed in /usr/lib; if I try:
> capabilities("tcltk")
> 
> I get:
> tcltk
> FALSE

As is indicated in the above post you referenced, this suggests that you
compiled from source, but R did not locate the requisite files during
the configure process.

> I cant' find tcltk.so

Run the following command in a console and post the results back:

locate tcltk.so

> Any suggestions please?
> Thx in advance and Merry Xmas!
> Liviu

Please provide the above information and we can help further.

Marc Schwartz



From dmb at mrc-dunn.cam.ac.uk  Thu Dec 23 18:38:21 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 23 Dec 2004 17:38:21 +0000 (GMT)
Subject: [R] Question about creating error bars
Message-ID: <Pine.LNX.4.21.0412231647430.20516-100000@mail.mrc-dunn.cam.ac.uk>


I have data that looks (very roughly) like this...

Declarative: 
Several 'groups', each group with a very variable number of
data points associated.


Procedural:
v.1 <- c(rep(50,1), rep(5,5), rep(2,10))  # Set up 
v.2 <- c('a','b','c','d','e','f','g','h', # the
         'i','j','k','l','m','n','o','p') # groups
v.3 <- rep(v.2,v.1)                       # here.
v.4 <- rnorm(length(v.3))                 # Simulate data.
v.5 <- tapply(v.4,v.3,mean)               # My analysis.

plot(v.5)

As the number of data points in a group gets smaller, so the variance of
the mean value for that group goes up. I would like to bootstrap some
error bars to show roughly how variable the value for each group is. Here
we have a normal distribution, but mostly my data is binary (i.e. each
group has a different number of (nearly) binary observations). The groups
are ordered, and I want to see any trend in my data accross the groups.

Dan.



From dfs at research.att.com  Thu Dec 23 18:41:43 2004
From: dfs at research.att.com (Deborah Swayne)
Date: Thu, 23 Dec 2004 12:41:43 -0500
Subject: [R] Creating a vector of colours that are as different from
In-Reply-To: <41C870C5.5050202@pdf.com>
References: <XFMail.041221130210.Ted.Harding@nessie.mcc.ac.uk>
	<41C870C5.5050202@pdf.com>
Message-ID: <16843.855.140577.773107@fry.research.att.com>

Spencer Graves writes:
 >       There should be a caveat with psycho-visual experimentation:  
 > Tufte (1983, p. 183) says that 5-10 percent of viewers are color 
 > deficient or color blind. 

The vischeck.com web site provides examples that show color-normal
people what the world looks like to people with different color
deficiencies; I found it fascinating.  It even allows you to submit
your own graphics to get an idea what they might look like to others.
Now I might not have to bother my friend down the hall so often ...

Debby



From gunter.berton at gene.com  Thu Dec 23 18:47:41 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 23 Dec 2004 09:47:41 -0800
Subject: [R] subsetting a data.frame to the 'unique' of a column
In-Reply-To: <41CAFB08.3060706@pdf.com>
Message-ID: <200412231747.iBNHlfoB025189@meitner.gene.com>

Spencer's solution is considerably more inefficient then using duplicated()
and subscripting: in a small example with 3 columns and 10000 rows, it took
5 times as long on my Windows setup.

The reason is that aggregate() is basically a wrapper for tapply and tapply
basically loops in R. duplicated() loops in C (and uses hashing, I believe).

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Thursday, December 23, 2004 9:06 AM
> To: G??ran Brostr??m
> Cc: Rudi Alberts; r-help at stat.math.ethz.ch
> Subject: Re: [R] subsetting a data.frame to the 'unique' of a column
> 
>       What about "aggregate"? 
> 
>  DF <- data.frame(a=c(1,1,2), b=1:3, c=letters[1:3])
>  aggregate(DF[2:3], DF[1], function(x)x[1])
>   a b c
> 1 1 1 1
> 2 2 3 3
> 
>       hope this helps.  spencer graves
> 
> G??ran Brostr??m wrote:
> 
> >On Thu, Dec 23, 2004 at 11:28:31AM -0800, Rudi Alberts wrote:
> >  
> >
> >>Hi,
> >>
> >>I often run into this problem:
> >>I have a data.frame with one column containing entries that are not
> >>unique. What I then want is a subset of the data.frame in which
> >>the entries in that column have become the 'unique' of the original
> >>column. 
> >>Normally I program around it by taking the unique of the column and
> >>making a new data.frame with it and filling the rest of the data.
> >>
> >>(By the way, when moving to the smaller data.frame for 
> example 5 rows
> >>with the same value in that column will be replaced by one 
> row for that
> >>value. I don't mind which of the rows now..)
> >>
> >>
> >>something like this, however, this gives me the complete df.
> >>
> >>df[df$colname %in% unique(df$colname),]
> >>
> >>or this, which doesnt work
> >>
> >>df[df$colname == unique(df$colname),]
> >>
> >>    
> >>
> > Use 'duplicated':
> >
> >  
> >
> >>df[!duplicated(df$colname), ]
> >>    
> >>
> >
> >  
> >
> 
> -- 
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Thu Dec 23 18:58:58 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 23 Dec 2004 09:58:58 -0800
Subject: [R] subsetting a data.frame to the 'unique' of a column
In-Reply-To: <200412231747.iBNHlfoB025189@meitner.gene.com>
References: <200412231747.iBNHlfoB025189@meitner.gene.com>
Message-ID: <41CB0762.3020702@pdf.com>

      Thanks, Bert, for the correction.  Moreover, I see now that mine 
didn't even give an acceptable answer, converting levels "a" and "c" of 
the factor DF$c to 1 and 3.  I confess I didn't read the documentation 
before replying.  Here is "duplicate with my example case: 

 > DF[!duplicated(DF$a), ]
  a b c
1 1 1 a
3 2 3 c

      Thanks again for the correction.  spencer graves

Berton Gunter wrote:

>Spencer's solution is considerably more inefficient then using duplicated()
>and subscripting: in a small example with 3 columns and 10000 rows, it took
>5 times as long on my Windows setup.
>
>The reason is that aggregate() is basically a wrapper for tapply and tapply
>basically loops in R. duplicated() loops in C (and uses hashing, I believe).
>
>Cheers,
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
> 
> 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>>Sent: Thursday, December 23, 2004 9:06 AM
>>To: G??ran Brostr??m
>>Cc: Rudi Alberts; r-help at stat.math.ethz.ch
>>Subject: Re: [R] subsetting a data.frame to the 'unique' of a column
>>
>>      What about "aggregate"? 
>>
>> DF <- data.frame(a=c(1,1,2), b=1:3, c=letters[1:3])
>> aggregate(DF[2:3], DF[1], function(x)x[1])
>>  a b c
>>1 1 1 1
>>2 2 3 3
>>
>>      hope this helps.  spencer graves
>>
>>G??ran Brostr??m wrote:
>>
>>    
>>
>>>On Thu, Dec 23, 2004 at 11:28:31AM -0800, Rudi Alberts wrote:
>>> 
>>>
>>>      
>>>
>>>>Hi,
>>>>
>>>>I often run into this problem:
>>>>I have a data.frame with one column containing entries that are not
>>>>unique. What I then want is a subset of the data.frame in which
>>>>the entries in that column have become the 'unique' of the original
>>>>column. 
>>>>Normally I program around it by taking the unique of the column and
>>>>making a new data.frame with it and filling the rest of the data.
>>>>
>>>>(By the way, when moving to the smaller data.frame for 
>>>>        
>>>>
>>example 5 rows
>>    
>>
>>>>with the same value in that column will be replaced by one 
>>>>        
>>>>
>>row for that
>>    
>>
>>>>value. I don't mind which of the rows now..)
>>>>
>>>>
>>>>something like this, however, this gives me the complete df.
>>>>
>>>>df[df$colname %in% unique(df$colname),]
>>>>
>>>>or this, which doesnt work
>>>>
>>>>df[df$colname == unique(df$colname),]
>>>>
>>>>   
>>>>
>>>>        
>>>>
>>>Use 'duplicated':
>>>
>>> 
>>>
>>>      
>>>
>>>>df[!duplicated(df$colname), ]
>>>>   
>>>>
>>>>        
>>>>
>>> 
>>>
>>>      
>>>
>>-- 
>>Spencer Graves, PhD, Senior Development Engineer
>>O:  (408)938-4420;  mobile:  (408)655-4567
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From gcutler at amgen.com  Thu Dec 23 19:00:12 2004
From: gcutler at amgen.com (Gene Cutler)
Date: Thu, 23 Dec 2004 10:00:12 -0800
Subject: [R] Get rid of space padding
In-Reply-To: <XFMail.041223084617.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041223084617.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <7DA9D526-550C-11D9-9854-000A95C91324@amgen.com>


On Dec 23, 2004, at 12:46 AM, (Ted Harding) wrote:
>
> However, when (as I suggested last time) I modify your function
> 'write.matrix' so as to remove occurrences of "format("...")"
> (leaving only  the ... ) then it seems to be OK.
>

Thanks, Ted.  'format' was the problem.  I didn't write the write.matrix
function, I just assumed 'format' was in there for a good reason.



From gunter.berton at gene.com  Thu Dec 23 19:20:23 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 23 Dec 2004 10:20:23 -0800
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <200412231820.iBNIKN4E011734@volta.gene.com>

 
[NOTE: This is a periodically posted (~ once/month) message for new R users.
Please let me know by private email if you object to this as a "waste" of
space]


Newbies (and others!) may find the R Reference Card made available by  Tom
Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  useful. It
categorizes and organizes a bunch of R's (S's) basic, most used functions so
that they can be easily found. For example, paste() is under the "Strings"
heading and expand.grid() is under "Data Creation." For newbies struggling
to find the right R function as well as veterans who can't quite remember
the function name, it's very handy.
 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From ripley at stats.ox.ac.uk  Thu Dec 23 19:26:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 23 Dec 2004 18:26:31 +0000 (GMT)
Subject: [R] Creating packages in windoze: *** [indices] Error 1
In-Reply-To: <200412222045.iBMKjJv5016196@imap3.mail.vanderbilt.edu>
References: <200412222045.iBMKjJv5016196@imap3.mail.vanderbilt.edu>
Message-ID: <Pine.LNX.4.61.0412231821430.24823@gannet.stats>

My guess is that the syntax error is in a data file.  Please read

http://developer.r-project.org/200update.txt

and check that you can actually load all your data files (something that
was unchecked before).

On Wed, 22 Dec 2004, Niels Waller wrote:

> I am running R 2.0.1 on a Windoze XP OS.  I recently upgraded from R 1.9x to
> 2.0.1 and I am currently
> upgrading a my personal function packages. My other packages compiled
> without
> a hitch but I am having a difficult time with my largest package.
> Can someone please help me with the following error message
> "Error in parse(file, n, text, prompt) : syntax error on line 1)"
>
> I have searched the "Creating R packages"
> manual but have been unable to determine how to solve my problem.
>
> Thank you in advance for any and all help.
>
> Niels Waller
> Vanderbilt University
>
>
> H:\R\rw2001\src\gnuwin32>rcmd build --binary --use-zip taxon

Please try INSTALLing first, as `Writing R Extensions' suggests.

> * checking for file 'taxon/DESCRIPTION' ... OK
> installing R.css in h:/TEMP/Rbuild.1020
>
>
> ---------- Making package taxon ------------
>  adding build stamp to DESCRIPTION
>  installing R files
>  installing inst files
>  installing data files
>  installing man source files
>  installing indices
> Error in parse(file, n, text, prompt) : syntax error on line 1
> Execution halted
> make[2]: *** [indices] Error 1
> make[1]: *** [all] Error 2
> make: *** [pkg-taxon] Error 2
> *** Installation of taxon failed ***

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From baron at psych.upenn.edu  Thu Dec 23 19:30:05 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 23 Dec 2004 13:30:05 -0500
Subject: [R] R Reference Card (especially useful for Newbies)
In-Reply-To: <200412231820.iBNIKN4E011734@volta.gene.com>
References: <200412231820.iBNIKN4E011734@volta.gene.com>
Message-ID: <20041223183005.GB12106@psych>

On 12/23/04 10:20, Berton Gunter wrote:
 Newbies (and others!) may find the R Reference Card made available by  Tom
 Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf
 useful.

There are two other reference cards, and all three are linked
from my R site (below).  [You might add these to your reminder
announcement.]

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From tlumley at u.washington.edu  Thu Dec 23 19:34:47 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 23 Dec 2004 10:34:47 -0800 (PST)
Subject: [R] Creating a vector of colours that are as different from
In-Reply-To: <16843.855.140577.773107@fry.research.att.com>
References: <XFMail.041221130210.Ted.Harding@nessie.mcc.ac.uk>
	<41C870C5.5050202@pdf.com>
	<16843.855.140577.773107@fry.research.att.com>
Message-ID: <Pine.A41.4.61b.0412231034040.50004@homer11.u.washington.edu>

On Thu, 23 Dec 2004, Deborah Swayne wrote:

> Spencer Graves writes:
> >       There should be a caveat with psycho-visual experimentation:
> > Tufte (1983, p. 183) says that 5-10 percent of viewers are color
> > deficient or color blind.
>
> The vischeck.com web site provides examples that show color-normal
> people what the world looks like to people with different color
> deficiencies; I found it fascinating.  It even allows you to submit
> your own graphics to get an idea what they might look like to others.
> Now I might not have to bother my friend down the hall so often ...
>

The `dichromat' package in R also does this.

 	-thomas



From MSchwartz at MedAnalytics.com  Thu Dec 23 19:57:23 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 23 Dec 2004 12:57:23 -0600
Subject: [R] tcltk problem
In-Reply-To: <1103823125.14780.17.camel@horizons.localdomain>
References: <1103819547.41caf31b7c036@webmail.musc.edu>
	<1103823125.14780.17.camel@horizons.localdomain>
Message-ID: <1103828243.14780.35.camel@horizons.localdomain>

On Thu, 2004-12-23 at 11:32 -0600, Marc Schwartz wrote:
> On Thu, 2004-12-23 at 11:32 -0500, Liviu M Vladutu wrote:
> > 
> > Hi all,
> > I have R Version 2.1.0 installed on a box running Redhat Fedora Core 2.
> 
> Are you really running 2.1.0 (which is an unreleased development
> version) or are you running 2.0.1, which is the present released
> version?
> 
> What does the banner indicate when you first start R?

As a quick follow up here, this thread has been moved to r-devel under
the same subject as Liviu confirmed that 2.1.0 is indeed what was
installed.

Marc



From burak_kutlu at yahoo.com  Thu Dec 23 22:36:26 2004
From: burak_kutlu at yahoo.com (burak kutlu)
Date: Thu, 23 Dec 2004 13:36:26 -0800 (PST)
Subject: [R] how to ignore t.test error message
Message-ID: <20041223213626.60885.qmail@web80902.mail.scd.yahoo.com>


Hello,
I was wondering if there is a way to ignore the error
message you get when some of the data means you
compare are constant in some lines of your data frame.
I'd like to go ahead with t.test and get the
calculated p-values anyway in such a case.
Thanks
-burak



From bitwrit at ozemail.com.au  Sat Dec 25 09:26:48 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Sat, 25 Dec 2004 19:26:48 +1100
Subject: [R] combination of scatterplot and image graph
In-Reply-To: <20041223043714.33824.qmail@web50302.mail.yahoo.com>
References: <20041223043714.33824.qmail@web50302.mail.yahoo.com>
Message-ID: <20041223224226.XLEO21478.smta11.mail.ozemail.net@there>

bogdan romocea wrote:
> Dear R users,
>
> I'm interested in a combination of a scatterplot and an image graph.
> I have two large vectors. Because in the scatterplot some areas are
> sparsely and others densely populated, I want to see the points, and
> I also want their color to be changed based on their density (similar
> to a heat map). Is there a function that can do that?
>
This looked much like a function in the "plotrix" package, and I found it was 
quite easy to modify one of the functions to do what I think you want. Try 
installing "plotrix" and sourcing this function.

color.scale<-function(x,redrange,greenrange,bluerange){
 ncolors<-length(x)
 if(length(redrange) > 1) reds<-rescale(x,redrange)
 else reds<-rep(redrange,ncolors)
 if(length(greenrange) > 1) greens<-rescale(x,greenrange)
 else greens<-rep(greenrange,ncolors)
 if(length(bluerange) > 1) blues<-rescale(x,bluerange)
 else blues<-rep(bluerange,ncolors)
 colormatrix<-cbind(reds,greens,blues)
 colvec<-apply(colormatrix,1,rgb.to.hex,scale.up)
 return(colvec)
}

Then:

x<-rnorm(20)
y<-rnorm(20)
densities<-apply(as.matrix(dist(cbind(x,y))),mean))
plot(x,y,col=color.scale(densities,c(0,255),0,c(255,0))

I've made up the color endpoints, of course. This looks so useful that I will 
include it in the next version of plotrix.

Jim



From p.drake at beatson.gla.ac.uk  Fri Dec 24 11:09:40 2004
From: p.drake at beatson.gla.ac.uk (Paul JH Drake)
Date: Fri, 24 Dec 2004 10:09:40 +0000
Subject: [R] Sorting problem
Message-ID: <1103882978.5784.6.camel@G4.site>

Hi
I'm using R 2.0 in SuSE 9.2.

When I plot data as a boxplot, the boxes appear on the plot in
alphabetical order (of group) rather than the order in which they appear
in the data. So far, the only thing I can do to fix this is to prefix
the group labels with a,b,c...etc to trick R into plotting them in the
right order.

Can sorting be turned off?
How should I address this sensibly?

Thanks in advance

Paul



From ggrothendieck at myway.com  Fri Dec 24 11:43:37 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 24 Dec 2004 05:43:37 -0500 (EST)
Subject: [R] Sorting problem
Message-ID: <20041224104337.3AC84399E@mprdmxin.myway.com>


From:   Paul JH Drake <p.drake at beatson.gla.ac.uk>
> 
> When I plot data as a boxplot, the boxes appear on the plot in
> alphabetical order (of group) rather than the order in which they appear
> in the data. So far, the only thing I can do to fix this is to prefix
> the group labels with a,b,c...etc to trick R into plotting them in the
> right order.
> 
> Can sorting be turned off?
> How should I address this sensibly?

Actually they are shown in order of the levels of the factor.
e.g. to show them in reverse order:

attach(InsectSprays)
spray <- factor( as.character(spray), level = rev(levels(spray)) )
boxplot(count ~ spray)



From ripley at stats.ox.ac.uk  Fri Dec 24 12:17:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Dec 2004 11:17:46 +0000 (GMT)
Subject: [R] Sorting problem
In-Reply-To: <1103882978.5784.6.camel@G4.site>
References: <1103882978.5784.6.camel@G4.site>
Message-ID: <Pine.LNX.4.61.0412241106120.18685@gannet.stats>

On Fri, 24 Dec 2004, Paul JH Drake wrote:

> I'm using R 2.0 in SuSE 9.2.

There is no such version, BTW: please see the posting guide for hints on 
supplying the sort of information we need (and an example would have 
helped a lot here).

> When I plot data as a boxplot, the boxes appear on the plot in
> alphabetical order (of group) rather than the order in which they appear
> in the data.

I'm guessing you used the formula interface to boxplot(): if so they 
appear in the order of the levels of the factor.  Otherwise they appear in 
the order the groups are supplied to boxplot().

> So far, the only thing I can do to fix this is to prefix the group 
> labels with a,b,c...etc to trick R into plotting them in the right 
> order.
>
> Can sorting be turned off?

No, as it is not turned on!

> How should I address this sensibly?

I need to be guessing, again!  Create your group as a factor with the 
levels in the order you want.  You don't tell us how you created it, but 
see ?factor.  In particular, if you did not supply a factor but something 
which was coerced to a factor, supply a factor.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Fri Dec 24 12:59:33 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 24 Dec 2004 11:59:33 -0000 (GMT)
Subject: [R] Sorting problem
In-Reply-To: <20041224104337.3AC84399E@mprdmxin.myway.com>
Message-ID: <XFMail.041224115149.Ted.Harding@nessie.mcc.ac.uk>

On 24-Dec-04 Gabor Grothendieck wrote:
> 
> From:   Paul JH Drake <p.drake at beatson.gla.ac.uk>
>> 
>> When I plot data as a boxplot, the boxes appear on the plot in
>> alphabetical order (of group) rather than the order in which they
>> appear
>> in the data. So far, the only thing I can do to fix this is to prefix
>> the group labels with a,b,c...etc to trick R into plotting them in the
>> right order.
>> 
>> Can sorting be turned off?
>> How should I address this sensibly?
> 
> Actually they are shown in order of the levels of the factor.
> e.g. to show them in reverse order:
> 
> attach(InsectSprays)
> spray <- factor( as.character(spray), level = rev(levels(spray)) )
> boxplot(count ~ spray)

Applying boxplot to data constructed as follows:

  X<-list(C=rnorm(10),B=rnorm(20),A=rnorm(30))
  boxplot(X)

gives the plots in the order "C", "B", "A", so here they indeed
come out in the order of the data.

But suppose I had wanted them in the order "B", "A", "C", say.
How would Gabor's suggestion (or similar) be applied to this case?
 
(OK I'm being thick, maybe; but I can't see how to relate the
one to the other!)

Ted.



From ripley at stats.ox.ac.uk  Fri Dec 24 13:26:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Dec 2004 12:26:25 +0000 (GMT)
Subject: [R] Sorting problem
In-Reply-To: <XFMail.041224115149.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041224115149.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0412241221160.32303@gannet.stats>

On Fri, 24 Dec 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 24-Dec-04 Gabor Grothendieck wrote:
>>
>> From:   Paul JH Drake <p.drake at beatson.gla.ac.uk>
>>>
>>> When I plot data as a boxplot, the boxes appear on the plot in
>>> alphabetical order (of group) rather than the order in which they
>>> appear
>>> in the data. So far, the only thing I can do to fix this is to prefix
>>> the group labels with a,b,c...etc to trick R into plotting them in the
>>> right order.
>>>
>>> Can sorting be turned off?
>>> How should I address this sensibly?
>>
>> Actually they are shown in order of the levels of the factor.

That's making lots of unsubstantiated assumptions, including that the 
formula interface to boxplot() was used: we do not even know that 
boxplot() was used (rather than plot.factor or bwplot or ...).

>> e.g. to show them in reverse order:
>>
>> attach(InsectSprays)
>> spray <- factor( as.character(spray), level = rev(levels(spray)) )
>> boxplot(count ~ spray)
>
> Applying boxplot to data constructed as follows:
>
>  X<-list(C=rnorm(10),B=rnorm(20),A=rnorm(30))
>  boxplot(X)
>
> gives the plots in the order "C", "B", "A", so here they indeed
> come out in the order of the data.
>
> But suppose I had wanted them in the order "B", "A", "C", say.
> How would Gabor's suggestion (or similar) be applied to this case?
>
> (OK I'm being thick, maybe; but I can't see how to relate the
> one to the other!)

They are not related.  You need to reorder the elements in your list:

boxplot(X[c("B", "A", "C")])

The help page for boxplot in R-devel explains both approaches.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Fri Dec 24 13:31:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 24 Dec 2004 07:31:58 -0500 (EST)
Subject: [R] Sorting problem
Message-ID: <20041224123158.119E4397B@mprdmxin.myway.com>


> From:   <Ted.Harding at nessie.mcc.ac.uk>
>  
> On 24-Dec-04 Gabor Grothendieck wrote:
> > 
> > From: Paul JH Drake <p.drake at beatson.gla.ac.uk>
> >> 
> >> When I plot data as a boxplot, the boxes appear on the plot in
> >> alphabetical order (of group) rather than the order in which they
> >> appear
> >> in the data. So far, the only thing I can do to fix this is to prefix
> >> the group labels with a,b,c...etc to trick R into plotting them in the
> >> right order.
> >> 
> >> Can sorting be turned off?
> >> How should I address this sensibly?
> > 
> > Actually they are shown in order of the levels of the factor.
> > e.g. to show them in reverse order:
> > 
> > attach(InsectSprays)
> > spray <- factor( as.character(spray), level = rev(levels(spray)) )
> > boxplot(count ~ spray)
> 
> Applying boxplot to data constructed as follows:
> 
> X<-list(C=rnorm(10),B=rnorm(20),A=rnorm(30))
> boxplot(X)
> 
> gives the plots in the order "C", "B", "A", so here they indeed
> come out in the order of the data.
> 
> But suppose I had wanted them in the order "B", "A", "C", say.
> How would Gabor's suggestion (or similar) be applied to this case?

boxplot(X[c("B","C","A")])



From p.drake at beatson.gla.ac.uk  Fri Dec 24 13:47:32 2004
From: p.drake at beatson.gla.ac.uk (Paul JH Drake)
Date: Fri, 24 Dec 2004 12:47:32 +0000
Subject: [R] Sorting problem
In-Reply-To: <Pine.LNX.4.61.0412241106120.18685@gannet.stats>
References: <1103882978.5784.6.camel@G4.site>
	<Pine.LNX.4.61.0412241106120.18685@gannet.stats>
Message-ID: <1103892451.5784.16.camel@G4.site>

Hi
Sorry if my post wasn't clear.

R version: 2.0.0
OS: SuSE 9.1
Pentium IV laptop

I have been creating boxplots as follows:
boxplot(gene~Group) [additional options left out for simplicity]

and the problem was that the boxplots were plotted in alphabetical
order, rather than the order in which I wanted them to be plotted.

Thanks for your suggestions - now that I know a little more about
factors, I have been able to arrange the order of the levels of the
factor "Group" and all is well!

Thanks for your patience and help

Paul



From baron at psych.upenn.edu  Fri Dec 24 13:55:59 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 24 Dec 2004 07:55:59 -0500
Subject: [R] searching Jonathan Baron's R Site - revised function
In-Reply-To: <20041223150200.GA4691@psych>
References: <20041223150200.GA4691@psych>
Message-ID: <20041224125559.GA2686@psych>

Documentation coming soon.
--
# Written by Andy Liaw and modified by Jonathan Baron 12/24/2004.
RSiteSearch <- function(string,
                        restrict=c("Rhelp02a","Rhelp01","functions","docs"),
                        format="normal", sortby="score", matchesPerPage=20) {
    string <- paste("http://finzi.psych.upenn.edu/cgi-bin/namazu.cgi?query=",
                    gsub(" ", "+", string), sep="")
    mpp <- paste("max=", matchesPerPage, sep="")

    format <- charmatch(format, c("normal", "short"))
    if (format == 0) stop("format must be either normal or short")
    format <- paste("result=", switch(format, "normal", "short"), sep="")

    sortby <- charmatch(sortby, c("score", "date:late", "date:early",
                                  "field:subject:ascending", "field:subject:decending",
                                  "field:from:ascending", "field:from:decending",
                                  "field:size:ascending", "field:size:decending",
                                  "field:uri:ascending", "field:uri:decending",))
    if (sortby == 0) stop("wrong sortby specified")
    sortby <- paste("sort=",
                    switch(sortby, "score", "date:late", "date:early",
                                  "field:subject:ascending", "field:subject:decending",
                                  "field:from:ascending", "field:from:decending",
                                  "field:size:ascending", "field:size:decending",
                                  "field:uri:ascending", "field:uri:decending"),
                    sep="")

    res <- ""
    if ("Rhelp02a" %in% restrict) res <- "idxname=Rhelp02a"
    if ("Rhelp01" %in% restrict) res <- paste(res,"idxname=Rhelp01",sep="&")
    if ("docs" %in% restrict) res <- paste(res,"idxname=docs",sep="&")
    if ("functions" %in% restrict) res <- paste(res,"idxname=functions",sep="&")
    if (res=="") {print("Using defaults: Rhelp 2002-; functions; docs.")
      res <- paste("idxname=Rhelp02a&idxname=functions&idxname=docs",sep="")}
    res <- sub("^&+","",res)

    qstring <- paste(string, mpp, format, sortby, res, sep="&")
    browseURL(qstring)
    invisible(qstring)
}

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From ligges at statistik.uni-dortmund.de  Fri Dec 24 13:59:06 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 24 Dec 2004 13:59:06 +0100
Subject: [R] how to ignore t.test error message
In-Reply-To: <20041223213626.60885.qmail@web80902.mail.scd.yahoo.com>
References: <20041223213626.60885.qmail@web80902.mail.scd.yahoo.com>
Message-ID: <41CC129A.1010902@statistik.uni-dortmund.de>

burak kutlu wrote:
> Hello,
> I was wondering if there is a way to ignore the error
> message you get when some of the data means you
> compare are constant in some lines of your data frame.

What does the error message say exactly?
Do you have an example?

Even with the degenerated case
   t.test(c(0, 0), c(0, 0))
I don't get any error message.


> I'd like to go ahead with t.test 

See ?try

> and get the calculated p-values anyway in such a case.

And what is the p-value in such a case?

I guess I haven't understood the problem correctly, so please be more 
specific!

Uwe Ligges


> Thanks
> -burak
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Fri Dec 24 14:22:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 24 Dec 2004 08:22:02 -0500 (EST)
Subject: [R] Sorting problem
Message-ID: <20041224132202.D1BEE3986@mprdmxin.myway.com>


>  page took 6.48 seconds   home | my page | my email 
>   .   
>  
>  
>  
>  
> 
>  email     
>  
>  
>  
> 
> Mail Addresses Calendar Notepad ggrothendieck at myway.com sign out 
> << Hide Folders Check Messages Compose Message POP Accounts | Mail Preferences | Help 
>  
>  
> Folders 
> Inbox 
> Drafts 
> Sent 
> Trash (Empty) 
> Bulk Mail (Empty) 
>  
>  
>  
> My Folders edit 
> R 
> Rcom 
> Rtmp 
> Saved 
> this 
> Tx 
> wcd 
> zoo 
>  
>  
>  
> Spam Tools info 
>    Spam Filter Level:
>  OffLowMediumMedium-highHigh 
> My Block List 
> Image Filter 
> Custom Filters 
>  
>  
>  
>  
>  
>  
> Now Live: 
> 
>  125MB Free Storage Upgrade  
>  
>  
>  
> 
>  Send/Receive 10MB emails!  
>  
>  
>  
> 
>             < Prev Next >   Back to Inbox Print View  Full Header 
>  
>     As AttachmentAs Inline Text Move to Folder----- Folders ------InboxDraftsSentTrashBulk Mail---- My Folders ----RRcomRtmpSavedthisTxwcdzoo
>  
>  
> 
>  Message is not flagged. [ Flag for Follow Up ] 
> 
> From:   Gabor Grothendieck <ggrothendieck at myway.com>
> 
> > From: <Ted.Harding at nessie.mcc.ac.uk>
> > 
> > On 24-Dec-04 Gabor Grothendieck wrote:
> > > 
> > > From: Paul JH Drake <p.drake at beatson.gla.ac.uk>
> > >> 
> > >> When I plot data as a boxplot, the boxes appear on the plot in
> > >> alphabetical order (of group) rather than the order in which they
> > >> appear
> > >> in the data. So far, the only thing I can do to fix this is to prefix
> > >> the group labels with a,b,c...etc to trick R into plotting them in the
> > >> right order.
> > >> 
> > >> Can sorting be turned off?
> > >> How should I address this sensibly?
> > > 
> > > Actually they are shown in order of the levels of the factor.
> > > e.g. to show them in reverse order:
> > > 
> > > attach(InsectSprays)
> > > spray <- factor( as.character(spray), level = rev(levels(spray)) )
> > > boxplot(count ~ spray)
> > 
> > Applying boxplot to data constructed as follows:
> > 
> > X<-list(C=rnorm(10),B=rnorm(20),A=rnorm(30))
> > boxplot(X)
> > 
> > gives the plots in the order "C", "B", "A", so here they indeed
> > come out in the order of the data.
> > 
> > But suppose I had wanted them in the order "B", "A", "C", say.
> > How would Gabor's suggestion (or similar) be applied to this case?
> 
> boxplot(X[c("B","C","A")])
> 

It occurred to me that maybe your question was not how to reorder
them if they are in list form but how to convert the list form
to the formula form.  If that was it then:

Xu <- unlist(X)
g <- factor(rep(names(X), sapply(X, length)))  
boxplot(Xu ~ g)



From ggrothendieck at myway.com  Fri Dec 24 14:24:50 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 24 Dec 2004 08:24:50 -0500 (EST)
Subject: [R] Sorting problem
Message-ID: <20041224132450.7016A3992@mprdmxin.myway.com>


[sorry. I had some email problems on my last one.  here it
is again.]

From:   Gabor Grothendieck <ggrothendieck at myway.com>
> 
> > From: <Ted.Harding at nessie.mcc.ac.uk>
> > 
> > On 24-Dec-04 Gabor Grothendieck wrote:
> > > 
> > > From: Paul JH Drake <p.drake at beatson.gla.ac.uk>
> > >> 
> > >> When I plot data as a boxplot, the boxes appear on the plot in
> > >> alphabetical order (of group) rather than the order in which they
> > >> appear
> > >> in the data. So far, the only thing I can do to fix this is to prefix
> > >> the group labels with a,b,c...etc to trick R into plotting them in the
> > >> right order.
> > >> 
> > >> Can sorting be turned off?
> > >> How should I address this sensibly?
> > > 
> > > Actually they are shown in order of the levels of the factor.
> > > e.g. to show them in reverse order:
> > > 
> > > attach(InsectSprays)
> > > spray <- factor( as.character(spray), level = rev(levels(spray)) )
> > > boxplot(count ~ spray)
> > 
> > Applying boxplot to data constructed as follows:
> > 
> > X<-list(C=rnorm(10),B=rnorm(20),A=rnorm(30))
> > boxplot(X)
> > 
> > gives the plots in the order "C", "B", "A", so here they indeed
> > come out in the order of the data.
> > 
> > But suppose I had wanted them in the order "B", "A", "C", say.
> > How would Gabor's suggestion (or similar) be applied to this case?
> 
> boxplot(X[c("B","C","A")])
> 

It occurred to me that maybe your question was not how to reorder
them if they are in list form but how to convert the list form
to the formula form.  If that was it then:

Xu <- unlist(X)
g <- factor(rep(names(X), sapply(X, length)))  # add levels= arg to reorder
boxplot(Xu ~ g)



From ggrothendieck at myway.com  Fri Dec 24 14:36:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 24 Dec 2004 08:36:43 -0500 (EST)
Subject: [R] Sorting problem
Message-ID: <20041224133643.4E75C396B@mprdmxin.myway.com>


From:   Gabor Grothendieck <ggrothendieck at myway.com>
> 
> [sorry. I had some email problems on my last one. here it
> is again.]
> 
> From: Gabor Grothendieck <ggrothendieck at myway.com>
> > 
> > > From: <Ted.Harding at nessie.mcc.ac.uk>
> > > 
> > > On 24-Dec-04 Gabor Grothendieck wrote:
> > > > 
> > > > From: Paul JH Drake <p.drake at beatson.gla.ac.uk>
> > > >> 
> > > >> When I plot data as a boxplot, the boxes appear on the plot in
> > > >> alphabetical order (of group) rather than the order in which they
> > > >> appear
> > > >> in the data. So far, the only thing I can do to fix this is to prefix
> > > >> the group labels with a,b,c...etc to trick R into plotting them in the
> > > >> right order.
> > > >> 
> > > >> Can sorting be turned off?
> > > >> How should I address this sensibly?
> > > > 
> > > > Actually they are shown in order of the levels of the factor.
> > > > e.g. to show them in reverse order:
> > > > 
> > > > attach(InsectSprays)
> > > > spray <- factor( as.character(spray), level = rev(levels(spray)) )
> > > > boxplot(count ~ spray)
> > > 
> > > Applying boxplot to data constructed as follows:
> > > 
> > > X<-list(C=rnorm(10),B=rnorm(20),A=rnorm(30))
> > > boxplot(X)
> > > 
> > > gives the plots in the order "C", "B", "A", so here they indeed
> > > come out in the order of the data.
> > > 
> > > But suppose I had wanted them in the order "B", "A", "C", say.
> > > How would Gabor's suggestion (or similar) be applied to this case?
> > 
> > boxplot(X[c("B","C","A")])
> > 
> 
> It occurred to me that maybe your question was not how to reorder
> them if they are in list form but how to convert the list form
> to the formula form. If that was it then:
> 
> Xu <- unlist(X)
> g <- factor(rep(names(X), sapply(X, length))) # add levels= arg to reorder
> boxplot(Xu ~ g)

or even shorter:

boxplot(values ~ ind, stack(X))



From Ted.Harding at nessie.mcc.ac.uk  Fri Dec 24 19:16:06 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 24 Dec 2004 18:16:06 -0000 (GMT)
Subject: [R] Sorting problem
In-Reply-To: <20041224133643.4E75C396B@mprdmxin.myway.com>
Message-ID: <XFMail.041224181606.Ted.Harding@nessie.mcc.ac.uk>

On 24-Dec-04 Gabor Grothendieck wrote:
>> From: Gabor Grothendieck <ggrothendieck at myway.com>
>> > > From: <Ted.Harding at nessie.mcc.ac.uk>
>> > > On 24-Dec-04 Gabor Grothendieck wrote:
>> > > > Actually they are shown in order of the levels of the factor.
>> > > > e.g. to show them in reverse order:
>> > > > 
>> > > > attach(InsectSprays)
>> > > > spray<-factor(as.character(spray),level=rev(levels(spray)))
>> > > > boxplot(count ~ spray)
>> > > 
>> > > Applying boxplot to data constructed as follows:
>> > > 
>> > > X<-list(C=rnorm(10),B=rnorm(20),A=rnorm(30))
>> > > boxplot(X)
>> > > 
>> > > gives the plots in the order "C", "B", "A", so here they indeed
>> > > come out in the order of the data.
>> > > 
>> > > But suppose I had wanted them in the order "B", "A", "C", say.
>> > > How would Gabor's suggestion (or similar) be applied to this case?
>> > 
>> > boxplot(X[c("B","C","A")])
>> > 
>> 
>> It occurred to me that maybe your question was not how to reorder
>> them if they are in list form but how to convert the list form
>> to the formula form. If that was it then:

Yes, I was interested in how to relate the two forms.

>> Xu <- unlist(X)
>> g <- factor(rep(names(X), sapply(X, length)))
>> # add levels= arg to reorder
>> boxplot(Xu ~ g)

And thanks for this manoeuvre! But ...

> or even shorter:
> 
> boxplot(values ~ ind, stack(X))

... Wow!! I always admire neat footwork, but, Gabor, how many
    feet have you got? This is like watching the 3-card trick!

Happy Christmas!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 24-Dec-04                                       Time: 18:16:05
------------------------------ XFMail ------------------------------



From ericpante at hotmail.com  Sat Dec 25 16:43:18 2004
From: ericpante at hotmail.com (Eric Pante)
Date: Sat, 25 Dec 2004 15:43:18 +0000
Subject: [R] RE: R-help Digest, Vol 22, Issue 24
In-Reply-To: <200412241111.iBOB5axS025950@hypatia.math.ethz.ch>
Message-ID: <BAY2-F408C29A2E958C0D831D4DCBCA70@phx.gbl>



From e.leuven at uva.nl  Sat Dec 25 23:29:40 2004
From: e.leuven at uva.nl (Edwin Leuven)
Date: Sat, 25 Dec 2004 23:29:40 +0100
Subject: [R] xlim in lattice
Message-ID: <41CDE9D4.3040506@uva.nl>

dear all,

i do the following (using ecdf from Hmisc, but that in turn uses lattice):

ecdf(~stp | year*abil, group = bonus )

where year*abil defines 2x2 = 4 groups (and i've put year first so that 
years are columns since i want to manipulate the x-axis)

the scale of stp is very different for the 2 years, and i want 
xlim=c(0,60) when year=1, and xlim=c(0,280) when year=2.

i've been trying to solve this seemingly trivial problem for the last 3 
hours and still have no clue.

does anyone know how to achieve this?

edwin



From deepayan at stat.wisc.edu  Sun Dec 26 01:02:15 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 25 Dec 2004 18:02:15 -0600
Subject: [R] xlim in lattice
In-Reply-To: <41CDE9D4.3040506@uva.nl>
References: <41CDE9D4.3040506@uva.nl>
Message-ID: <200412251802.15428.deepayan@stat.wisc.edu>

On Saturday 25 December 2004 16:29, Edwin Leuven wrote:
> dear all,
>
> i do the following (using ecdf from Hmisc, but that in turn uses
> lattice):
>
> ecdf(~stp | year*abil, group = bonus )
>
> where year*abil defines 2x2 = 4 groups (and i've put year first so
> that years are columns since i want to manipulate the x-axis)
>
> the scale of stp is very different for the 2 years, and i want
> xlim=c(0,60) when year=1, and xlim=c(0,280) when year=2.
>
> i've been trying to solve this seemingly trivial problem for the last
> 3 hours and still have no clue.
>
> does anyone know how to achieve this?

Well, going as far as 

ecdf(~stp | year*abil, group = bonus,
     scales = 
     list(x = list(relation = "free", 
          limits = list(c(0, 60), c(0, 280), c(0, 60), c(0, 280)))))

should be easy enough. Unfortunately, it's not as easy to get rid of the 
labels from the first row. It's still possible, if you are desperate 
enough, but it's not very clean:

ecdf(~stp | year*abil, group = bonus,
     scales = 
     list(x = list(relation = "free", 
          limits = list(c(0, 60), c(0, 280), c(0, 60), c(0, 280)),
          at = list(TRUE, TRUE, NULL, NULL))),
     par.settings = list(layout.heights = list(axis.panel = c(1, 0))))

(this is assuming R 2.0.x)

Deepayan



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 26 11:31:15 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 26 Dec 2004 10:31:15 -0000 (GMT)
Subject: [R] Prosodic/phonetic analysis with R
Message-ID: <XFMail.041226103115.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I'm interested in looking (in a beginner's amateurish way)
at prosodic/phonetic analysis of recorded speech.

In particular I would want to use R to formulate and
evaluate specific models.

So I would like to ask R people for their recommendations
for a program which would

a) Take as input a sound file in one of the common formats
   (".wav", ".au")

b) perform at least basic phonetic analysis (formants, F0,
   spectrograms, ... )

b) Save to file a representation of the basic phonetic
   analysis which it has done, in a form which R can readily
   import and use.

I've come across 'praat':

   http://www.praat.org

which seems to offer a spohisticated range of analyses, though
it's not too clear from the above URL what can be saved and
in what format.

I'd be grateful for any comments on using 'praat' with R,
and/or for any suggestions for other software with commments.
I only want to hear about software which (like 'praat') is
GPL or similar.

Thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 26-Dec-04                                       Time: 10:31:14
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Sun Dec 26 12:09:26 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 26 Dec 2004 12:09:26 +0100
Subject: [R] Prosodic/phonetic analysis with R
In-Reply-To: <XFMail.041226103115.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041226103115.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41CE9BE6.6020004@statistik.uni-dortmund.de>

(Ted Harding) wrote:

> Hi Folks,
> 
> I'm interested in looking (in a beginner's amateurish way)
> at prosodic/phonetic analysis of recorded speech.
> 
> In particular I would want to use R to formulate and
> evaluate specific models.
> 
> So I would like to ask R people for their recommendations
> for a program which would
> 
> a) Take as input a sound file in one of the common formats
>    (".wav", ".au")

Ted,

see package tuneR for reading Wave files.


> b) perform at least basic phonetic analysis (formants, F0,
>    spectrograms, ... )

For F0 and spectograms see also tuneR.


"Formants" is a bit more tricky. We tried some analyses, but since the 
definition of a formant is still not completly clear to me, we haven't 
provided anything for formants in the package yet. Do you know some good 
literature that gives a somwhat precise definition? At least musicias 
only talk about something like "raised" areas in the periodogram, which 
is not very helpful given the missing definition of "raised".

Uwe



> b) Save to file a representation of the basic phonetic
>    analysis which it has done, in a form which R can readily
>    import and use.
> 
> I've come across 'praat':
> 
>    http://www.praat.org
> 
> which seems to offer a spohisticated range of analyses, though
> it's not too clear from the above URL what can be saved and
> in what format.
> 
> I'd be grateful for any comments on using 'praat' with R,
> and/or for any suggestions for other software with commments.
> I only want to hear about software which (like 'praat') is
> GPL or similar.
> 
> Thanks,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 26-Dec-04                                       Time: 10:31:14
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From shravan.vasishth at gmail.com  Sun Dec 26 13:02:05 2004
From: shravan.vasishth at gmail.com (Shravan Vasishth)
Date: Sun, 26 Dec 2004 13:02:05 +0100
Subject: [R] Prosodic/phonetic analysis with R
In-Reply-To: <200412261120.iBQB4ow8006523@hypatia.math.ethz.ch>
References: <200412261120.iBQB4ow8006523@hypatia.math.ethz.ch>
Message-ID: <8b1813aa04122604024879f772@mail.gmail.com>

You might want to look at EMU:

http://emu.sourceforge.net/

Praat has a scripting language associated with it, their web page
describes it, and others have tutorials on Praat scripting (just
search on Google). People I know in this area use EMU as well as
Praat.  From your description of your needs the Praat scripting
language is probably the starting point for you.

On Sun, 26 Dec 2004 12:20:45 +0100, r-help-request at stat.math.ethz.ch 

> From: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Subject: [R] Prosodic/phonetic analysis with R
> To: r-help at stat.math.ethz.ch
> Message-ID: <XFMail.041226103115.Ted.Harding at nessie.mcc.ac.uk>
> Content-Type: text/plain; charset=iso-8859-1
> 
> Hi Folks,
> 
> I'm interested in looking (in a beginner's amateurish way)
> at prosodic/phonetic analysis of recorded speech.
> 
> In particular I would want to use R to formulate and
> evaluate specific models.
> 
> So I would like to ask R people for their recommendations
> for a program which would
> 
> a) Take as input a sound file in one of the common formats
>    (".wav", ".au")
> 
> b) perform at least basic phonetic analysis (formants, F0,
>    spectrograms, ... )
> 
> b) Save to file a representation of the basic phonetic
>    analysis which it has done, in a form which R can readily
>    import and use.
> 
> I've come across 'praat':
> 
>    http://www.praat.org
> 
> which seems to offer a spohisticated range of analyses, though
> it's not too clear from the above URL what can be saved and
> in what format.
> 
> I'd be grateful for any comments on using 'praat' with R,
> and/or for any suggestions for other software with commments.
> I only want to hear about software which (like 'praat') is
> GPL or similar.
> 
> Thanks,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 26-Dec-04                                       Time: 10:31:14
> ------------------------------ XFMail ------------------------------

-- 
Prof. Dr. Shravan Vasishth   Tel (direct): +49-(0)331-977-2457
Juniorprofessor:        Empirical Methods in Syntax Research
Institute for Linguistics, Potsdam University, PO Box 601553
14415 Potsdam, Germany  Tel (secretary): -2016; Fax: -2087
http://www.purl.oclc.org/NET/vasishth



From fren2 at yahoo.com  Sun Dec 26 13:11:20 2004
From: fren2 at yahoo.com (Frederic renaud)
Date: Sun, 26 Dec 2004 04:11:20 -0800 (PST)
Subject: [R] Un peu d'aide
Message-ID: <20041226121120.6098.qmail@web51809.mail.yahoo.com>

Bonjour,
Je voudrais faire un petit programme sous R dans
lequel je ferais intervenir des parametres que
j'initialiserais dans R directement ou dans un fichier
txt(des naturels, reels et caracteres). N'auriez vous
pas des programmes deja fait qui reprennent ces
techniques? 
De plus, je voudrais pouvoir travailler avec la
distribution exp (rexp) de parametres: moyenne=1 et
variance=1. J'ai un petit prob pour introduire la
variance.
D'avance Merci
Fred



From tobias.verbeke at telenet.be  Sun Dec 26 13:54:57 2004
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Sun, 26 Dec 2004 12:54:57 +0000
Subject: [R] Un peu d'aide
In-Reply-To: <20041226121120.6098.qmail@web51809.mail.yahoo.com>
References: <20041226121120.6098.qmail@web51809.mail.yahoo.com>
Message-ID: <20041226125457.17528927.tobias.verbeke@telenet.be>

On Sun, 26 Dec 2004 04:11:20 -0800 (PST)
Frederic renaud <fren2 at yahoo.com> wrote:

> Bonjour,
> Je voudrais faire un petit programme sous R dans
> lequel je ferais intervenir des parametres que
> j'initialiserais dans R directement ou dans un fichier
> txt(des naturels, reels et caracteres). N'auriez vous
> pas des programmes deja fait qui reprennent ces
> techniques? 

Salut, 

La liste R-help est anglophone. En bas de ce message 
figure l'URL du posting guide, qui pourrait te guider
vers la documentation (qui est fort riche).

Il n'y a aucun probl??me ?? lire des donn??es depuis
un fichier. Voir la fonction read.table ou scan
Il y a un manuel sp??cifique pour l'importation
et l'exportation des donn??es (qui se trouve tr??s
probablement sur ton syst??me, mais que tu peux
t??l??charger ??galement depuis CRAN).

> De plus, je voudrais pouvoir travailler avec la
> distribution exp (rexp) de parametres: moyenne=1 et
> variance=1. J'ai un petit prob pour introduire la
> variance.

Je crois qu'il n'y qu'un seul param??tre pour
cette distribution ("scale parameter b > 0"). 
La moyenne est b, la variance est b^2.

Le param??tre utilis?? en R est le dit 'hazard rate'
lambda = 1/b. Pour une moyenne = 1 et variance = 1
il suffit d'introduire l'argument rate = 1.

HTH,
Tobias

> D'avance Merci
> Fred
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From receiver.ml_lab at laposte.net  Sun Dec 26 14:05:02 2004
From: receiver.ml_lab at laposte.net (Nooks L. Affro)
Date: Sun, 26 Dec 2004 14:05:02 +0100
Subject: [R] Un peu d'aide
In-Reply-To: <20041226121120.6098.qmail@web51809.mail.yahoo.com>
References: <20041226121120.6098.qmail@web51809.mail.yahoo.com>
Message-ID: <20041226130501.GA13889@charmed>

On Sun, Dec 26, 2004 at 04:11:20AM -0800, Frederic renaud wrote:
> Bonjour,

Bonjour,


> Je voudrais faire un petit programme sous R dans
> lequel je ferais intervenir des parametres que
> j'initialiserais dans R directement ou dans un fichier
> txt(des naturels, reels et caracteres). N'auriez vous
> pas des programmes deja fait qui reprennent ces
> techniques? 

Pour une introduction a R, en francais dans le texte, voir le document :

http://statwww.epfl.ch/davison/teaching/ProbStat/20022003/rdebuts_fr.pdf


> De plus, je voudrais pouvoir travailler avec la
> distribution exp (rexp) de parametres: moyenne=1 et
> variance=1. J'ai un petit prob pour introduire la
> variance.

Si je me rappelle bien pour une loi exponentielle , la moyenne et la
variance sont identiques au parametre (taux) de la loi (usuellement not'e
\lambda dans la litterature). Il est donc inutile de specifier les deux. Dans R ce parametre est le taux ('rate' en anglais). 

utiliser la commande help(rexp) pour plus de details et des exemples.

> D'avance Merci
> Fred
> 

Nooks,



From sgunnste at jhsph.edu  Sun Dec 26 14:20:17 2004
From: sgunnste at jhsph.edu (=?ISO-8859-1?Q?Sn=E6bj=F6rn_Gunnsteinsson?=)
Date: Sun, 26 Dec 2004 13:20:17 +0000
Subject: [R] error in gregmisc package
Message-ID: <E2ABA52C-5740-11D9-A12A-000393D3EA82@jhsph.edu>

Hi,

    I just installed the gregmisc bundle. When I do library(gdata)  (or 
library(gtools), library(gmodels)) everything works fine.

When I do library(gplots) I get

      Error in parse(file, n, text, prompt) : syntax error on line 1850
      Error in library(gplots) : package/namespace load failed

I'm using R-1.7.0 on Mac OS 10.3.

Can anybody help me with this? Could I be doing something wrong or is 
there
an error in the package (related to R-1.7.0, or Mac OS X or in general)?

Best, Snaebjorn Gunnsteinsson.



From kjetil at acelerate.com  Sun Dec 26 14:27:59 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 26 Dec 2004 09:27:59 -0400
Subject: [R] error in gregmisc package
In-Reply-To: <E2ABA52C-5740-11D9-A12A-000393D3EA82@jhsph.edu>
References: <E2ABA52C-5740-11D9-A12A-000393D3EA82@jhsph.edu>
Message-ID: <41CEBC5F.9030809@acelerate.com>

Sn??bj??rn Gunnsteinsson wrote:

> Hi,
>
>    I just installed the gregmisc bundle. When I do library(gdata)  (or 
> library(gtools), library(gmodels)) everything works fine.
>
> When I do library(gplots) I get
>
>      Error in parse(file, n, text, prompt) : syntax error on line 1850
>      Error in library(gplots) : package/namespace load failed
>
> I'm using R-1.7.0 on Mac OS 10.3.

That's very old and probably does'nt know about namesapces?

Kjetil

>
> Can anybody help me with this? Could I be doing something wrong or is 
> there
> an error in the package (related to R-1.7.0, or Mac OS X or in general)?
>
> Best, Snaebjorn Gunnsteinsson.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From ripley at stats.ox.ac.uk  Sun Dec 26 14:48:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 26 Dec 2004 13:48:54 +0000 (GMT)
Subject: [R] error in gregmisc package
In-Reply-To: <41CEBC5F.9030809@acelerate.com>
References: <E2ABA52C-5740-11D9-A12A-000393D3EA82@jhsph.edu>
	<41CEBC5F.9030809@acelerate.com>
Message-ID: <Pine.LNX.4.61.0412261338290.5719@gannet.stats>

On Sun, 26 Dec 2004, Kjetil Brinchmann Halvorsen wrote:

> Sn?bj?rn Gunnsteinsson wrote:
>
>>    I just installed the gregmisc bundle. When I do library(gdata)  (or 
>> library(gtools), library(gmodels)) everything works fine.
>> 
>> When I do library(gplots) I get
>> 
>>      Error in parse(file, n, text, prompt) : syntax error on line 1850
>>      Error in library(gplots) : package/namespace load failed
>> 
>> I'm using R-1.7.0 on Mac OS 10.3.
>
> That's very old and probably does'nt know about namesapces?

It does: they were introduced for packages other than base in 1.7.0.
However, that line is

     m[[1]] <- graphics:::plot.formula

and the ::: operator was introduced in 1.8.0 and package `graphics' in 
1.9.0.

>> Can anybody help me with this? Could I be doing something wrong or is there
>> an error in the package (related to R-1.7.0, or Mac OS X or in general)?

gregmisc should have a dependence on R>=1.9.0 (at least).  It won't be the 
only package that does not have such a dependence, including many of those 
introduced since 1.7.0 came out (over two years ago) as people tend not to 
test on past versions of R.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 26 15:02:13 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 26 Dec 2004 14:02:13 -0000 (GMT)
Subject: [R] Prosodic/phonetic analysis with R
In-Reply-To: <41CE9BE6.6020004@statistik.uni-dortmund.de>
Message-ID: <XFMail.041226140213.Ted.Harding@nessie.mcc.ac.uk>

On 26-Dec-04 Uwe Ligges wrote:
> (Ted Harding) wrote:
>> So I would like to ask R people for their recommendations
>> for a program which would
>> 
>> a) Take as input a sound file in one of the common formats
>>    (".wav", ".au")
> 
> Ted,
> 
> see package tuneR for reading Wave files.
> 
> 
>> b) perform at least basic phonetic analysis (formants, F0,
>>    spectrograms, ... )
> 
> For F0 and spectograms see also tuneR.

Thanks for the pointer to tuneR, Uwe. I've had a look at the
reference manual, and it does seem to be primarily oriented
towards analysis of musical data. I'm not so much interested
in getting the raw sound file into R and then doing basic
frequency-type analysis on this, as in working on the output
of a program which can apply phonetic expertise to the file
and then present the characteristics of the phonetic analysis
to R for further analysis.

> "Formants" is a bit more tricky. We tried some analyses, but
> since the definition of a formant is still not completly clear
> to me, we haven't provided anything for formants in the package
> yet.
> 
> Do you know some good literature that gives a somwhat precise
> definition? At least musicias only talk about something like
> "raised" areas in the periodogram, which is not very helpful
> given the missing definition of "raised".

Well, I'm only a beginner! I could agree with your summary from
what I have read so far. The account I have seen so far which
best combines general accessibility with apparent technical
throughness is the on-line Britannica article "Phonetics":

http://www.britannica.com/eb/print?tocId=9108587&fullArticle=true

and the following is a relevant quote:

   In summary, speech sounds are fairly well defined by nine
   acoustic factors. The first three factors include the
   frequencies of the first three formants; these are responsible
   for the major part of the information in speech. Characterizing
   the vocal tract shape, these formant frequencies specify vowels,
   nasals, laterals, and the transitional movements in voiced
   consonants. The frequencies of the fourth and higher formants
   do not vary significantly. The fourth factor is the fundamental
   frequency--roughly speaking, the pitch--of the larynx pulse in
   voiced sounds, and the fifth, the amplitude--roughly speaking,
   the loudness--of the larynx pulse. These last two factors
   account for suprasegmental information; e.g., variations in
   stress and intonation. They also distinguish between voiced
   and voiceless sounds, in that the latter have no larynx pulse
   amplitude. The centre frequency of the high-frequency hissing
   noises in voiceless sounds constitutes the sixth acoustic factor,
   and the seventh is the amplitude of these high-frequency noises.
   These two factors characterize the major differences among
   voiceless sounds. In more accurate descriptions it would be
   necessary to specify more than just the centre frequency of
   the noise in fricative sounds. The eighth and ninth factors
   include the amplitudes of the second and third formants relative
   to the first formant; the amplitudes of the formants as a whole
   are determined by the larynx pulse amplitude. These latter
   factors are the least important in that they convey only
   supplementary information about nasals and laterals.

Earlier in the article it is stated that

  "The resonant frequencies of the vocal tract are known as
   the formants."

but one has to read through the whole thing before the richer
implications of this start to become apparent.

The advantage of software like 'praat' is that phonetic experts
have incorporated their understanding -- much clearer than I'm
likely to achieve from the above -- into the software!

I'm also grateful to Shravan Vasishth for responding with the
suggestion of EMU. This seems at first sight to be less sophisticated
than 'praat', though with what looks like a useful repertoire
of "primitives" -- from its description:

  "EMU is a collection of software tools for the creation,
   manipulation and analysis of speech databases. At the core
   of EMU is a database search engine which allows queries based
   on the sequential and hierarchical structure of the annotations."

It has the immediate advantage that it comes with facilities
for direct linkage to S-Plus and R. Clearly worth looking into,
but I don't know yet whether it would do enough of the dirty
work for me!

Thanks, Uwe and Shravan! If I get anywhere useful, I'll report
back to the list.

All best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 26-Dec-04                                       Time: 14:02:13
------------------------------ XFMail ------------------------------



From sgunnste at jhsph.edu  Sun Dec 26 15:09:00 2004
From: sgunnste at jhsph.edu (=?ISO-8859-1?Q?Sn=E6bj=F6rn_Gunnsteinsson?=)
Date: Sun, 26 Dec 2004 14:09:00 +0000
Subject: [R] error in gregmisc package
In-Reply-To: <Pine.LNX.4.61.0412261338290.5719@gannet.stats>
References: <E2ABA52C-5740-11D9-A12A-000393D3EA82@jhsph.edu>
	<41CEBC5F.9030809@acelerate.com>
	<Pine.LNX.4.61.0412261338290.5719@gannet.stats>
Message-ID: <B08F3E08-5747-11D9-A12A-000393D3EA82@jhsph.edu>

Thank you very much.

    I'll update my R version and see if things run more smoothly.

Best, Snaebjorn.

On Dec 26, 2004, at 1:48 PM, Prof Brian Ripley wrote:

> On Sun, 26 Dec 2004, Kjetil Brinchmann Halvorsen wrote:
>
>> Sn??bj??rn Gunnsteinsson wrote:
>>
>>>    I just installed the gregmisc bundle. When I do library(gdata)  
>>> (or library(gtools), library(gmodels)) everything works fine.
>>> When I do library(gplots) I get
>>>      Error in parse(file, n, text, prompt) : syntax error on line 
>>> 1850
>>>      Error in library(gplots) : package/namespace load failed
>>> I'm using R-1.7.0 on Mac OS 10.3.
>>
>> That's very old and probably does'nt know about namesapces?
>
> It does: they were introduced for packages other than base in 1.7.0.
> However, that line is
>
>     m[[1]] <- graphics:::plot.formula
>
> and the ::: operator was introduced in 1.8.0 and package `graphics' in 
> 1.9.0.
>
>>> Can anybody help me with this? Could I be doing something wrong or 
>>> is there
>>> an error in the package (related to R-1.7.0, or Mac OS X or in 
>>> general)?
>
> gregmisc should have a dependence on R>=1.9.0 (at least).  It won't be 
> the only package that does not have such a dependence, including many 
> of those introduced since 1.7.0 came out (over two years ago) as 
> people tend not to test on past versions of R.
>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From klaus_thul at yahoo.com  Sun Dec 26 16:14:42 2004
From: klaus_thul at yahoo.com (Klaus Thul)
Date: Sun, 26 Dec 2004 23:14:42 +0800
Subject: [R] Problem with stepAIC, "invalid second argument"
Message-ID: <1104074082.7626.13.camel@localhost>

Dear all,

am I doing something wrong, or is this a bug (R 2.0.1 & 1.9.1,
linux&windows):

    > library(MASS)
    > df <- data.frame(x = rnorm(100), y = rnorm(100), z = rnorm(100))
    > f <- lm(x ~ y + z, data = df)
    > stepAIC(f, scope = list(upper = ~y * z, lower = ~1))
    Start:  AIC= -16.56
     x ~ y + z
    Error in eval(expr, envir, enclos) : invalid second argument
    >


I can avoid the problem doing the following, but in my application I do
not want to use attach / detach because of possible name conflicts:

    > attach(df)
    > f <- lm(x ~ y + z)
    > stepAIC(f, scope = list(upper = ~y * z, lower = ~1))
    Start:  AIC= 23.55
     x ~ y + z

    ...

    Call:
    lm(formula = x ~ 1)

    Coefficients:
    (Intercept)
        -0.1972
    > detach(df)


Thanks for any help,
Klaus Thul



From ripley at stats.ox.ac.uk  Sun Dec 26 16:56:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 26 Dec 2004 15:56:04 +0000 (GMT)
Subject: [R] Problem with stepAIC, "invalid second argument"
In-Reply-To: <1104074082.7626.13.camel@localhost>
References: <1104074082.7626.13.camel@localhost>
Message-ID: <Pine.LNX.4.61.0412261551380.7360@gannet.stats>

Try ?df to see what you did wrong!

Always use conflicts() before posting to see what you have overridden.
R normally allows you to get away with masking objects, but in this case 
it is the R object that is masking yours.


On Sun, 26 Dec 2004, Klaus Thul wrote:

> am I doing something wrong, or is this a bug (R 2.0.1 & 1.9.1,
> linux&windows):
>
>    > library(MASS)
>    > df <- data.frame(x = rnorm(100), y = rnorm(100), z = rnorm(100))
>    > f <- lm(x ~ y + z, data = df)
>    > stepAIC(f, scope = list(upper = ~y * z, lower = ~1))
>    Start:  AIC= -16.56
>     x ~ y + z
>    Error in eval(expr, envir, enclos) : invalid second argument

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 26 22:10:44 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 26 Dec 2004 21:10:44 -0000 (GMT)
Subject: [R] Prosodic/phonetic analysis with R
In-Reply-To: <XFMail.041226140213.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.041226211044.Ted.Harding@nessie.mcc.ac.uk>

On 26-Dec-04 Ted Harding wrote:
> The advantage of software like 'praat' is that phonetic experts
> have incorporated their understanding -- much clearer than I'm
> likely to achieve from the above -- into the software!

Reporting back on first impressions of 'praat' -- see:

  http://www.praat.org

The software can certainly extract a great variety of different
kinds of phonetic information from speech recorded as a sound
file. Furthermore (though it took some digging around with demos
and such to get to it) you can write out the data for these
parameters to text files in a reasonably transparent format,
which it is more (sometimes) or less (sometimes) easy to edit
into files which can be read straight into R.

In some cases (e.g. the listing of F0) it is a simple two-column
table of time and F0, easily converted into a CSV and then in R
into a time series.

In other cases (e.g. the listing for formants) the file is more
highly structured and in a hierarchical format, and more
sophisticated edting and processing is needed in order to make
it directly readable into R. Nevertheless, it's not obscure and
is quite do-able.

So, for the simple-minded things I'm interested in learning about,
it looks as though 'praat' is a tool which can be quite easily
used in conjunction with R. If any of you are interested in
this kind of work, I'd recommend it as a good place to start.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 26-Dec-04                                       Time: 21:10:44
------------------------------ XFMail ------------------------------



From fren2 at yahoo.com  Mon Dec 27 01:56:46 2004
From: fren2 at yahoo.com (Frederic renaud)
Date: Sun, 26 Dec 2004 16:56:46 -0800 (PST)
Subject: [R] vector
Message-ID: <20041227005646.35183.qmail@web51802.mail.yahoo.com>

Hi!
Can you help me? I would like to make a vector from p
pre-determined vector. The parameter p can change.
I've try with ?c with unsuccess! An idee?

Sorry for my english!
Thanks
Fred



From spencer.graves at pdf.com  Mon Dec 27 03:34:00 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 26 Dec 2004 18:34:00 -0800
Subject: [R] vector
In-Reply-To: <20041227005646.35183.qmail@web51802.mail.yahoo.com>
References: <20041227005646.35183.qmail@web51802.mail.yahoo.com>
Message-ID: <41CF7498.4020107@pdf.com>

      1.  Don't worry about your english.  For most of the people who 
follow this list, English is a second language. 

      2.  It would help if you provided an example of what you tried 
that didn't work. 

      3.  Does the following answer your question: 

 > a <- 1:2
 > b <- 3:4
 > (d <- c(a, b))
[1] 1 2 3 4
 >

      If not, please check the posting guide 
"http://www.R-project.org/posting-guide.html".  It may help you answer 
your question yourself faster than we can.  If that fails, it may 
increase the chances that we actually answer your real question. 

      Bon chance.
      spencer graves

Frederic renaud wrote:

>Hi!
>Can you help me? I would like to make a vector from p
>pre-determined vector. The parameter p can change.
>I've try with ?c with unsuccess! An idee?
>
>Sorry for my english!
>Thanks
>Fred
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From andrea.nesi at prato.unifi.it  Mon Dec 27 08:49:11 2004
From: andrea.nesi at prato.unifi.it (Andrea Nesi)
Date: Mon, 27 Dec 2004 08:49:11 +0100 (CET)
Subject: [R] Richiesta informazioni
Message-ID: <1104133751.41cfbe779ab84@mail.prato.unifi.it>

Sono Andrea Nesi, studente di Ingegneria dell??Universit?? di Firenze. Sto 
facendo il tirocinio nell??Ufficio di Protezione Civile del Comune di Prato, 
dove sto cercando di effettuare una statistica sui dati idrometrici rilevati, 
relativi ad una serie storica di 12 anni (per ogni anno ?? in mio possesso un 
file txt di valori idrometrici delle letture ogni 15 minuti). Scopo del lavoro 
?? quello di verificare le altezze relative ad una portata con tempo di ritorno 
biennale e quinquennale (che gi?? esistono ma appaiono sottostimate) in 
ottemperanza della c.d. legge Bertolaso.  
Per questo lavoro ho deciso di utilizzare il software R, ma le mie conoscenze 
del programma sono, in pratica, nulle. E?? da un po?? di tempo che lavoro alla 
manipolazione dei dati in  mio possesso, ma anche utilizzando le dispense che 
ho trovato non riesco a giungere ad una soluzione definitiva del problema. Non 
avendo purtroppo nessuno che mi possa dare aiuto sul programma, ho pensato di 
chiedere a lei se pu?? fornirmi dei riferimenti riguardo testi che possano 
essere utili al mio lavoro, o qualsiasi altra informazione (comandi, lavori gi?? 
effettuati sul tema, etc.) attinente il mio studio in R, se lei ne conosce 
qualcuna.
Ringraziandola molto per la disponibilit??, se mi pu?? rispondere le sarei molto 
grato.

Andrea Nesi



From andrea.nesi at prato.unifi.it  Mon Dec 27 08:49:11 2004
From: andrea.nesi at prato.unifi.it (Andrea Nesi)
Date: Mon, 27 Dec 2004 08:49:11 +0100 (CET)
Subject: [R] Richiesta informazioni
Message-ID: <1104133751.41cfbe779ab84@mail.prato.unifi.it>


Sono Andrea Nesi, studente di Ingegneria dell??Universit?? di Firenze. Sto 
facendo il tirocinio nell??Ufficio di Protezione Civile del Comune di Prato, 
dove sto cercando di effettuare una statistica sui dati idrometrici rilevati, 
relativi ad una serie storica di 12 anni (per ogni anno ?? in mio possesso un 
file txt di valori idrometrici delle letture ogni 15 minuti). Scopo del lavoro 
?? quello di verificare le altezze relative ad una portata con tempo di ritorno 
biennale e quinquennale (che gi?? esistono ma appaiono sottostimate) in 
ottemperanza della c.d. legge Bertolaso.  
Per questo lavoro ho deciso di utilizzare il software R, ma le mie conoscenze 
del programma sono, in pratica, nulle. E?? da un po?? di tempo che lavoro alla 
manipolazione dei dati in  mio possesso, ma anche utilizzando le dispense che 
ho trovato non riesco a giungere ad una soluzione definitiva del problema. Non 
avendo purtroppo nessuno che mi possa dare aiuto sul programma, ho pensato di 
chiedere a lei se pu?? fornirmi dei riferimenti riguardo testi che possano 
essere utili al mio lavoro, o qualsiasi altra informazione (comandi, lavori gi?? 
effettuati sul tema, etc.) attinente il mio studio in R, se lei ne conosce 
qualcuna.
Ringraziandola molto per la disponibilit??, se mi pu?? rispondere le sarei molto 
grato.

Andrea Nesi



From andrea.nesi at prato.unifi.it  Mon Dec 27 08:52:39 2004
From: andrea.nesi at prato.unifi.it (Andrea Nesi)
Date: Mon, 27 Dec 2004 08:52:39 +0100 (CET)
Subject: [R] Richiesta informazioni
Message-ID: <1104133959.41cfbf47453df@mail.prato.unifi.it>

Sono Andrea Nesi, studente di Ingegneria dell??Universit?? di Firenze. Sto 
facendo il tirocinio nell??Ufficio di Protezione Civile del Comune di Prato, 
dove sto cercando di effettuare una statistica sui dati idrometrici rilevati, 
relativi ad una serie storica di 12 anni (per ogni anno ?? in mio possesso un 
file txt di valori idrometrici delle letture ogni 15 minuti). Scopo del lavoro 
?? quello di verificare le altezze relative ad una portata con tempo di ritorno 
biennale e quinquennale (che gi?? esistono ma appaiono sottostimate) in 
ottemperanza della c.d. legge Bertolaso.  
Per questo lavoro ho deciso di utilizzare il software R, ma le mie conoscenze 
del programma sono, in pratica, nulle. E?? da un po?? di tempo che lavoro alla 
manipolazione dei dati in  mio possesso, ma anche utilizzando le dispense che 
ho trovato non riesco a giungere ad una soluzione definitiva del problema. Non 
avendo purtroppo nessuno che mi possa dare aiuto sul programma, ho pensato di 
chiedere a lei se pu?? fornirmi dei riferimenti riguardo testi che possano 
essere utili al mio lavoro, o qualsiasi altra informazione (comandi, lavori gi?? 
effettuati sul tema, etc.) attinente il mio studio in R, se lei ne conosce 
qualcuna.
Ringraziandola molto per la disponibilit??, se mi pu?? rispondere le sarei molto 
grato.

Andrea Nesi



From Stefano.Guazzetti at ausl.re.it  Mon Dec 27 09:13:57 2004
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Mon, 27 Dec 2004 09:13:57 +0100
Subject: R: [R] Richiesta informazioni
Message-ID: <B8A1EED732379B44A7E59D22E82E4442365391@IMHOTEP.ausl.org>

Dear Andrea. 
Your request seems vague. And also: be careful, even if some of
the people reading the list understand italian you are expected 
to post your requests in english. 
In short: read the FAQ and at least "An introduction to R". 
But you seem to be lucky, on the CRAN you will find a document
by Vito Ricci, in italian, about time series analysis:
http://cran.r-project.org/doc/contrib/Ricci-ts-italian.pdf ,
that you could find useful for your purposes.

Best whishes, Stefano

Caro Andrea.
Anzitutto stai attento che la lingua ufficiale della lista ?? 
l'inglese. In breve: leggi le FAQ ed almeno "An introduction
to R". Sei fortunato: sul cran sembra esserci un manuale sulla 
analisi delle serie storiche son R in italiano, contribuito da 
Vito Ricci:
http://cran.r-project.org/doc/contrib/Ricci-ts-italian.pdf 

Tanti auguri, Stefano

   >-----Messaggio originale-----
   >Da: r-help-bounces at stat.math.ethz.ch
   >[mailto:r-help-bounces at stat.math.ethz.ch]Per conto di Andrea Nesi
   >Inviato: luned?? 27 dicembre 2004 8.49
   >A: r-help at stat.math.ethz.ch; 
   >UNEXPECTED_DATA_AFTER_ADDRESS at .SYNTAX-ERROR
   >Oggetto: [R] Richiesta informazioni
   >
   >
   >Sono Andrea Nesi, studente di Ingegneria dell'Universit?? di 
   >Firenze. Sto 
   >facendo il tirocinio nell'Ufficio di Protezione Civile del 
   >Comune di Prato, 
   >dove sto cercando di effettuare una statistica sui dati 
   >idrometrici rilevati, 
   >relativi ad una serie storica di 12 anni (per ogni anno ?? 
   >in mio possesso un 
   >file txt di valori idrometrici delle letture ogni 15 
   >minuti). Scopo del lavoro 
   >?? quello di verificare le altezze relative ad una portata 
   >con tempo di ritorno 
   >biennale e quinquennale (che gi?? esistono ma appaiono 
   >sottostimate) in 
   >ottemperanza della c.d. legge Bertolaso.  
   >Per questo lavoro ho deciso di utilizzare il software R, ma 
   >le mie conoscenze 
   >del programma sono, in pratica, nulle. E' da un po' di 
   >tempo che lavoro alla 
   >manipolazione dei dati in  mio possesso, ma anche 
   >utilizzando le dispense che 
   >ho trovato non riesco a giungere ad una soluzione 
   >definitiva del problema. Non 
   >avendo purtroppo nessuno che mi possa dare aiuto sul 
   >programma, ho pensato di 
   >chiedere a lei se pu?? fornirmi dei riferimenti riguardo 
   >testi che possano 
   >essere utili al mio lavoro, o qualsiasi altra informazione 
   >(comandi, lavori gi?? 
   >effettuati sul tema, etc.) attinente il mio studio in R, se 
   >lei ne conosce 
   >qualcuna.
   >Ringraziandola molto per la disponibilit??, se mi pu?? 
   >rispondere le sarei molto 
   >grato.
   >
   >Andrea Nesi
   >
   >______________________________________________
   >R-help at stat.math.ethz.ch mailing list
   >https://stat.ethz.ch/mailman/listinfo/r-help
   >PLEASE do read the posting guide! 
   >http://www.R-project.org/posting-guide.html
   >



From bxc at steno.dk  Mon Dec 27 11:09:13 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 27 Dec 2004 11:09:13 +0100
Subject: [R] vector
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE248@exdkba022.novo.dk>

If you have all your vectors in a list

vl <- list( p1, p2, p3)

the the following should do the trick:

res <- numeric(0)
for( i in 1:length(vl) ) res <- c( res, vl[[i]] ) 

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frederic renaud
> Sent: Monday, December 27, 2004 1:57 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] vector
> 
> 
> Hi!
> Can you help me? I would like to make a vector from p 
> pre-determined vector. The parameter p can change. I've try 
> with ?c with unsuccess! An idee?
> 
> Sorry for my english!
> Thanks
> Fred
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Dec 27 11:32:37 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Dec 2004 11:32:37 +0100
Subject: [R] vector
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E9027FE248@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E9027FE248@exdkba022.novo.dk>
Message-ID: <x2wtv4av5m.fsf@biostat.ku.dk>

"BXC (Bendix Carstensen)" <bxc at steno.dk> writes:

> If you have all your vectors in a list
> 
> vl <- list( p1, p2, p3)
> 
> the the following should do the trick:
> 
> res <- numeric(0)
> for( i in 1:length(vl) ) res <- c( res, vl[[i]] ) 

Or,

  do.call("c", vl)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gb at tal.stat.umu.se  Mon Dec 27 11:39:34 2004
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Mon, 27 Dec 2004 11:39:34 +0100
Subject: [R] vector
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E9027FE248@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E9027FE248@exdkba022.novo.dk>
Message-ID: <20041227103934.GA20505@tal.stat.umu.se>

On Mon, Dec 27, 2004 at 11:09:13AM +0100, BXC (Bendix Carstensen) wrote:
> If you have all your vectors in a list
> 
> vl <- list( p1, p2, p3)
> 
> the the following should do the trick:
> 
> res <- numeric(0)
> for( i in 1:length(vl) ) res <- c( res, vl[[i]] ) 

or, shorter, and faster I would guess,

res <- unlist(v1)


> Bendix Carstensen
> ----------------------
> Bendix Carstensen
> Senior Statistician
> Steno Diabetes Center
> Niels Steensens Vej 2
> DK-2820 Gentofte
> Denmark
> tel: +45 44 43 87 38
> mob: +45 30 75 87 38
> fax: +45 44 43 07 06
> bxc at steno.dk
> www.biostat.ku.dk/~bxc
> ----------------------
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frederic renaud
> > Sent: Monday, December 27, 2004 1:57 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] vector
> > 
> > 
> > Hi!
> > Can you help me? I would like to make a vector from p 
> > pre-determined vector. The parameter p can change. I've try 
> > with ?c with unsuccess! An idee?
> > 
> > Sorry for my english!
> > Thanks
> > Fred
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read 
> > the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From anne at wjh.harvard.edu  Mon Dec 27 11:55:29 2004
From: anne at wjh.harvard.edu (Anne G)
Date: Mon, 27 Dec 2004 05:55:29 -0500 (EST)
Subject: [R] splot.screen: multiple plots
Message-ID: <Pine.SOL.4.30.0412270540150.25734-100000@wjh1.wjh.harvard.edu>

I have multiple conditions that I would like to plot in a
grid. To save space, I don't need the ticks, labels etc on
the plots which are not on the outside.

I tried split.screen, but if it is clearer to use, it seems
pretty rigid since it decides of each screen size apriori
while I need more space for the left column and top or
bottom row screens than for the other plots.

axis(1,tick=FALSE) does not seem to do anything, asp=1 does
not seem to do anything either. par(pty="s") gave it a
somewhate square aspect, But there is a lot of empty space
which must have to do with margins, but setting the margins
did not seem to change anything.

I think I have seen examples like mine in a book, but it
might have been a book on Splus. Do you know where I can
find an example of what I am looking for.

thanks
Anne
----------------------------------------------------------
here is a simplified example with the same data for all
plots

X<-c(1.00000  3.63168  6.44916 10.17572 20.47440)
Y<-c(0.26 0.56 0.80 0.92 0.88)

numFreq<-7
numdist<-8

coefind <- 1
quartz(display = "", width =15, height = 10, pointsize = 9,
            family = "Helvetica", antialias = TRUE,
autorefresh = TRUE)
par(pty="s")
split.screen(c(numdist,numFreq))

for (inddist in 1:numdist)
	{
	for (indFreq in 1:numFreq)
		{
		indscreen<- indFreq+(inddist-1)*numFreq

			screen(indscreen)
#			mar<-c(1,1,1,0)

			plot(X,Y,
log="x",col="red",xlim=c(1,100),ylim=c(0,1),lab=5, xlab="",
ylab="")
			axis(1,tick=FALSE)
			axis(2,tick=FALSE)

			if( inddist==1)
				{
				title(main=sprintf('Freq =
%d',indFreq));
				}

			if( indFreq==1)
				{
				axis(2, tick=TRUE)
				title(ylab=sprintf('dist =
%d',inddist));
				}
 			if (inddist==8)
				{
				axis(1,tick=TRUE)
				title(xlab="perc face")
				}

			coefind <- coefind+1
		}
	}
close.screen(all = TRUE)    # exit split-screen mode



From bxc at steno.dk  Mon Dec 27 12:10:19 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 27 Dec 2004 12:10:19 +0100
Subject: [R] splot.screen: multiple plots
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE24A@exdkba022.novo.dk>

You probably want to use something like

par( mar=c(0,0,0,0), oma=c(3,3,2,3) )

which will put all your plots next to each other with no space between
them
and with a reasonable margin outside ("oma").

Take a print of the (rather long) help page for "par", and bring it with
you 
wherever you go, so youcan study it in waiting lines, traffic jams,
trains etc.
It's boring to learn but useful to know all those weird arguments...

best
Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anne G
> Sent: Monday, December 27, 2004 11:55 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] splot.screen: multiple plots
> 
> 
> I have multiple conditions that I would like to plot in a
> grid. To save space, I don't need the ticks, labels etc on
> the plots which are not on the outside.
> 
> I tried split.screen, but if it is clearer to use, it seems 
> pretty rigid since it decides of each screen size apriori 
> while I need more space for the left column and top or bottom 
> row screens than for the other plots.
> 
> axis(1,tick=FALSE) does not seem to do anything, asp=1 does
> not seem to do anything either. par(pty="s") gave it a 
> somewhate square aspect, But there is a lot of empty space 
> which must have to do with margins, but setting the margins 
> did not seem to change anything.
> 
> I think I have seen examples like mine in a book, but it
> might have been a book on Splus. Do you know where I can
> find an example of what I am looking for.
> 
> thanks
> Anne
> ----------------------------------------------------------
> here is a simplified example with the same data for all
> plots
> 
> X<-c(1.00000  3.63168  6.44916 10.17572 20.47440)
> Y<-c(0.26 0.56 0.80 0.92 0.88)
> 
> numFreq<-7
> numdist<-8
> 
> coefind <- 1
> quartz(display = "", width =15, height = 10, pointsize = 9,
>             family = "Helvetica", antialias = TRUE,
> autorefresh = TRUE)
> par(pty="s")
> split.screen(c(numdist,numFreq))
> 
> for (inddist in 1:numdist)
> 	{
> 	for (indFreq in 1:numFreq)
> 		{
> 		indscreen<- indFreq+(inddist-1)*numFreq
> 
> 			screen(indscreen)
> #			mar<-c(1,1,1,0)
> 
> 			plot(X,Y,
> log="x",col="red",xlim=c(1,100),ylim=c(0,1),lab=5, xlab="",
> ylab="")
> 			axis(1,tick=FALSE)
> 			axis(2,tick=FALSE)
> 
> 			if( inddist==1)
> 				{
> 				title(main=sprintf('Freq =
> %d',indFreq));
> 				}
> 
> 			if( indFreq==1)
> 				{
> 				axis(2, tick=TRUE)
> 				title(ylab=sprintf('dist =
> %d',inddist));
> 				}
>  			if (inddist==8)
> 				{
> 				axis(1,tick=TRUE)
> 				title(xlab="perc face")
> 				}
> 
> 			coefind <- coefind+1
> 		}
> 	}
> close.screen(all = TRUE)    # exit split-screen mode
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From fren2 at yahoo.com  Mon Dec 27 12:18:53 2004
From: fren2 at yahoo.com (Frederic renaud)
Date: Mon, 27 Dec 2004 03:18:53 -0800 (PST)
Subject: [R] list(0) to integer
Message-ID: <20041227111853.56569.qmail@web51806.mail.yahoo.com>

Hello
I've another question :-)
I would like to transform a list to a integer.
I must be sure that the number entered by the user is
an integer! Thus, I've made :


repeat{
  cat("Effectif des populations (integer):")
  n<-scan("",n=1,what=list(0),quiet=TRUE)

  if (is.integer(n[[1]])==TRUE) break
  print("L'effectif des population doit etre un
entier")
  }

That doesn't work of course but I've no idea to do
this. How verify that n[[1]] is an integer an
transform them as an integer (as.integer(n) doesn't
work!) Someone can help me? Thanks!
Fred



From tobias.verbeke at telenet.be  Mon Dec 27 14:23:25 2004
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Mon, 27 Dec 2004 13:23:25 +0000
Subject: [R] list(0) to integer
In-Reply-To: <20041227111853.56569.qmail@web51806.mail.yahoo.com>
References: <20041227111853.56569.qmail@web51806.mail.yahoo.com>
Message-ID: <20041227132325.5036218b.tobias.verbeke@telenet.be>

On Mon, 27 Dec 2004 03:18:53 -0800 (PST)
Frederic renaud <fren2 at yahoo.com> wrote:

> Hello
> I've another question :-)
> I would like to transform a list to a integer.
> I must be sure that the number entered by the user is
> an integer! Thus, I've made :
> 
> 
> repeat{
>   cat("Effectif des populations (integer):")
>   n<-scan("",n=1,what=list(0),quiet=TRUE)
> 
>   if (is.integer(n[[1]])==TRUE) break
>   print("L'effectif des population doit etre un
> entier")
>   }

if (!is.integer(unlist(n))) stop("L'effectif ...")

You don't need `== TRUE' (and I guess you meant `== FALSE')
An error action can be executed using stop("some error message").
`break' breaks out of the loop (ie goes to the first statement
after the loop), so the print statement cannot be executed.

Are you sure you need n to be a list ?

HTH,
Tobias

>  That doesn't work of course but I've no idea to do
> this. How verify that n[[1]] is an integer an
> transform them as an integer (as.integer(n) doesn't
> work!) 

> Someone can help me? Thanks!
> Fred
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From das at cshl.edu  Mon Dec 27 19:52:32 2004
From: das at cshl.edu (Rajdeep Das)
Date: Mon, 27 Dec 2004 13:52:32 -0500
Subject: [R] classification using logistic regression
Message-ID: <001e01c4ec45$38d8fe20$6500a8c0@artney>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041227/3e9363e8/attachment.pl

From burak_kutlu at yahoo.com  Mon Dec 27 19:53:59 2004
From: burak_kutlu at yahoo.com (burak kutlu)
Date: Mon, 27 Dec 2004 10:53:59 -0800 (PST)
Subject: [R] how to ignore t.test error message
In-Reply-To: <41CC129A.1010902@statistik.uni-dortmund.de>
Message-ID: <20041227185359.31299.qmail@web80904.mail.scd.yahoo.com>

Hi 
Although t.test does not give an error with
t.test(c(0, 0), c(0, 0));

if you try doing that with any number other than 0, 

> t.test(c(1,1),c(1,1))

(R gives the following error)


Error in t.test.default(c(1, 1), c(1, 1)) : 
	data are essentially constant

The logic in insisting is that my data has different
columns, and I'd like to tolerate if in one of the
columns, the data fail to pass the t.test.

Thanks
Burak

--- Uwe Ligges <ligges at statistik.uni-dortmund.de>
wrote:

> burak kutlu wrote:
> > Hello,
> > I was wondering if there is a way to ignore the
> error
> > message you get when some of the data means you
> > compare are constant in some lines of your data
> frame.
> 
> What does the error message say exactly?
> Do you have an example?
> 
> Even with the degenerated case
>    t.test(c(0, 0), c(0, 0))
> I don't get any error message.
> 
> 
> > I'd like to go ahead with t.test 
> 
> See ?try
> 
> > and get the calculated p-values anyway in such a
> case.
> 
> And what is the p-value in such a case?
> 
> I guess I haven't understood the problem correctly,
> so please be more 
> specific!
> 
> Uwe Ligges
> 
> 
> > Thanks
> > -burak
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Mon Dec 27 20:08:01 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 27 Dec 2004 14:08:01 -0500
Subject: [R] how to ignore t.test error message
Message-ID: <3A822319EB35174CA3714066D590DCD50994E46F@usrymx25.merck.com>

See ?try (as Uwe said) or ?tryCatch.

Andy

> From: burak kutlu
> 
> Hi 
> Although t.test does not give an error with
> t.test(c(0, 0), c(0, 0));
> 
> if you try doing that with any number other than 0, 
> 
> > t.test(c(1,1),c(1,1))
> 
> (R gives the following error)
> 
> 
> Error in t.test.default(c(1, 1), c(1, 1)) : 
> 	data are essentially constant
> 
> The logic in insisting is that my data has different
> columns, and I'd like to tolerate if in one of the
> columns, the data fail to pass the t.test.
> 
> Thanks
> Burak
> 
> --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> wrote:
> 
> > burak kutlu wrote:
> > > Hello,
> > > I was wondering if there is a way to ignore the
> > error
> > > message you get when some of the data means you
> > > compare are constant in some lines of your data
> > frame.
> > 
> > What does the error message say exactly?
> > Do you have an example?
> > 
> > Even with the degenerated case
> >    t.test(c(0, 0), c(0, 0))
> > I don't get any error message.
> > 
> > 
> > > I'd like to go ahead with t.test 
> > 
> > See ?try
> > 
> > > and get the calculated p-values anyway in such a
> > case.
> > 
> > And what is the p-value in such a case?
> > 
> > I guess I haven't understood the problem correctly,
> > so please be more 
> > specific!
> > 
> > Uwe Ligges
> > 
> > 
> > > Thanks
> > > -burak
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Kevin.Wang at maths.anu.edu.au  Mon Dec 27 21:09:02 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Tue, 28 Dec 2004 07:09:02 +1100 (EST)
Subject: [R] classification using logistic regression
In-Reply-To: <001e01c4ec45$38d8fe20$6500a8c0@artney>
References: <001e01c4ec45$38d8fe20$6500a8c0@artney>
Message-ID: <Pine.GSO.4.58.0412280705520.17873@yin>

Hi,

On Mon, 27 Dec 2004, Rajdeep Das wrote:

> I would like to do classification using logistic regression. Which R package can I use?

Have you tried glm() function?

> Also is there any package for feature selection for logistic regression based method?

Do you mean model selection methods like forward selection?  If so, try
step()

HTH,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From ernesto at ipimar.pt  Tue Dec 28 01:09:23 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 28 Dec 2004 00:09:23 +0000
Subject: [R] RandomFields: Controling seed with GaussRF
Message-ID: <1104192563.3559.12.camel@moria.ipimar.pt>

Hi,

I'm using RF to simulate a correlated variable with GaussRF

set.seed=1
GaussRF(sim.kfinegrid, grid=F, model="exponential",
param=c(0,0.5,0,0.2))

However when I simulate again using the same random seed I get different
results.

> set.seed=1 
> summary(GaussRF(sim.kfinegrid, grid=F, model="exponential",
param=c(0,0.5,0,0.2)))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
-2.3110 -0.9726 -0.3831 -0.3048  0.2378  2.0100
> set.seed=1 
> summary(GaussRF(sim.kfinegrid, grid=F, model="exponential",
param=c(0,0.5,0,0.2)))
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
-1.83400 -0.33260  0.10340  0.06004  0.47310  1.68300

How can I control the seed with GaussRF ?

Regards

EJ



From das at cshl.edu  Tue Dec 28 01:26:17 2004
From: das at cshl.edu (Rajdeep Das)
Date: Mon, 27 Dec 2004 19:26:17 -0500
Subject: [R] random numbers within a given range
Message-ID: <004301c4ec73$d90600e0$6500a8c0@artney>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041227/48cdbf54/attachment.pl

From p.dalgaard at biostat.ku.dk  Tue Dec 28 01:30:02 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Dec 2004 01:30:02 +0100
Subject: [R] RandomFields: Controling seed with GaussRF
In-Reply-To: <1104192563.3559.12.camel@moria.ipimar.pt>
References: <1104192563.3559.12.camel@moria.ipimar.pt>
Message-ID: <x27jn3e039.fsf@biostat.ku.dk>

Ernesto Jardim <ernesto at ipimar.pt> writes:

> set.seed=1
> GaussRF(sim.kfinegrid, grid=F, model="exponential",
> param=c(0,0.5,0,0.2))
> 
> However when I simulate again using the same random seed I get different
> results.
> 
> > set.seed=1 
> > summary(GaussRF(sim.kfinegrid, grid=F, model="exponential",
....
> How can I control the seed with GaussRF ?

Surely not by assigning values to set.seed - it is a function! 

Look at ?set.seed and pay attention to the Usage: section.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ernesto at ipimar.pt  Tue Dec 28 01:39:15 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 28 Dec 2004 00:39:15 +0000
Subject: [R] RandomFields: Controling seed with GaussRF
In-Reply-To: <x27jn3e039.fsf@biostat.ku.dk>
References: <1104192563.3559.12.camel@moria.ipimar.pt>
	<x27jn3e039.fsf@biostat.ku.dk>
Message-ID: <1104194355.3559.15.camel@moria.ipimar.pt>

On Tue, 2004-12-28 at 00:30, Peter Dalgaard wrote:
> Ernesto Jardim <ernesto at ipimar.pt> writes:
> 
> > set.seed=1
> > GaussRF(sim.kfinegrid, grid=F, model="exponential",
> > param=c(0,0.5,0,0.2))
> > 
> > However when I simulate again using the same random seed I get different
> > results.
> > 
> > > set.seed=1 
> > > summary(GaussRF(sim.kfinegrid, grid=F, model="exponential",
> ....
> > How can I control the seed with GaussRF ?
> 
> Surely not by assigning values to set.seed - it is a function! 
> 
> Look at ?set.seed and pay attention to the Usage: section.
> 

You're correct ! Sorry. It's a problem of the interface between the
computer and the chair :)

EJ



From p.dalgaard at biostat.ku.dk  Tue Dec 28 01:34:37 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Dec 2004 01:34:37 +0100
Subject: [R] random numbers within a given range
In-Reply-To: <004301c4ec73$d90600e0$6500a8c0@artney>
References: <004301c4ec73$d90600e0$6500a8c0@artney>
Message-ID: <x23bxrdzvm.fsf@biostat.ku.dk>

"Rajdeep Das" <das at cshl.edu> writes:

> Is there a way to get random numbers within a given range?

Yes. What distribution? If uniform, see help(Uniform).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dkannan41 at yahoo.com  Tue Dec 28 02:00:19 2004
From: dkannan41 at yahoo.com (duraikannan sundaramoorthi)
Date: Mon, 27 Dec 2004 17:00:19 -0800 (PST)
Subject: [R] plots
Message-ID: <20041228010019.24354.qmail@web40910.mail.yahoo.com>

Is there a way to save plots and use in a word
document. Thanks for your help in advance.
durai


		
__________________________________ 

Send holiday email and support a worthy cause. Do good.



From Charles.Annis at StatisticalEngineering.com  Tue Dec 28 02:27:00 2004
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 27 Dec 2004 20:27:00 -0500
Subject: [R] plots
In-Reply-To: <20041228010019.24354.qmail@web40910.mail.yahoo.com>
Message-ID: <200412280124.iBS1On7D002767@hypatia.math.ethz.ch>

Yes.  Assuming you are using R in Windows, right click on the plot, and
choose Save as metafile, or more expeditiously, Copy as metafile, and then
paste directly into your WORD document.



Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of duraikannan
sundaramoorthi
Sent: Monday, December 27, 2004 8:00 PM
To: r-help at stat.math.ethz.ch
Subject: [R] plots

Is there a way to save plots and use in a word
document. Thanks for your help in advance.
durai


		
__________________________________ 

Send holiday email and support a worthy cause. Do good.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dmb at mrc-dunn.cam.ac.uk  Tue Dec 28 02:26:28 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 28 Dec 2004 01:26:28 +0000 (GMT)
Subject: [R] Question about creating error bars
In-Reply-To: <Pine.LNX.4.21.0412231647430.20516-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.21.0412280126020.10273-100000@mail.mrc-dunn.cam.ac.uk>


On Thu, 23 Dec 2004, Dan Bolser wrote:

>
>I have data that looks (very roughly) like this...
>
>Declarative: 
>Several 'groups', each group with a very variable number of
>data points associated.
>
>
>Procedural:
>v.1 <- c(rep(50,1), rep(5,5), rep(2,10))  # Set up 
>v.2 <- c('a','b','c','d','e','f','g','h', # the
>         'i','j','k','l','m','n','o','p') # groups
>v.3 <- rep(v.2,v.1)                       # here.
>v.4 <- rnorm(length(v.3))                 # Simulate data.
>v.5 <- tapply(v.4,v.3,mean)               # My analysis.
>
>plot(v.5)
>
>As the number of data points in a group gets smaller, so the variance of
>the mean value for that group goes up. I would like to bootstrap some
>error bars to show roughly how variable the value for each group is. Here
>we have a normal distribution, but mostly my data is binary (i.e. each
>group has a different number of (nearly) binary observations). The groups
>are ordered, and I want to see any trend in my data accross the groups.
>
>Dan.

Nothing I can do to help pal, sorry.


>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bxc at steno.dk  Tue Dec 28 08:15:30 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Tue, 28 Dec 2004 08:15:30 +0100
Subject: [R] plots
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE24F@exdkba022.novo.dk>

For exact contol of height and width, you may want to have a look at:

?win.metafile

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> duraikannan sundaramoorthi
> Sent: Tuesday, December 28, 2004 2:00 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] plots
> 
> 
> Is there a way to save plots and use in a word
> document. Thanks for your help in advance.
> durai
> 
> 
> 		
> __________________________________ 
> 
> Send holiday email and support a worthy cause. Do good.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From tfliao at uiuc.edu  Tue Dec 28 10:05:46 2004
From: tfliao at uiuc.edu (Tim F Liao)
Date: Tue, 28 Dec 2004 03:05:46 -0600
Subject: [R] plots
Message-ID: <5910bbac.b5b12de4.8199300@expms6.cites.uiuc.edu>

You can copy (right click then choose metafile) and paste 
into Word easily, though sometimes you may have problem 
sizing the graph, depending on how it's produced.  However, 
single framed graphs work quite straightforwardly.

Tim Liao

---- Original message ----
>Date: Mon, 27 Dec 2004 17:00:19 -0800 (PST)
>From: duraikannan sundaramoorthi <dkannan41 at yahoo.com>  
>Subject: [R] plots  
>To: r-help at stat.math.ethz.ch
>
>Is there a way to save plots and use in a word
>document. Thanks for your help in advance.
>durai
>
>
>		
>__________________________________ 
>
>Send holiday email and support a worthy cause. Do good.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-
project.org/posting-guide.html



From fredrik.bg.lundgren at bredband.net  Tue Dec 28 11:02:02 2004
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Tue, 28 Dec 2004 11:02:02 +0100
Subject: [R] editing a package directly from R
Message-ID: <000501c4ecc4$47723a30$5f9d72d5@Larissa>

Dear R,

I have made a package 'myfuncs' of some home made functions and 
installed it without problems. It contains no C or fortran code and no 
dynamic link library. Is there a way to edit this package directly from 
R (or Xemacs with ESS) or do I always have to edit the source and then 
reinstall the package? I have a weak memory that such editing could be 
done in S-Plus 2000 or 6.1?
Windows XP, R 2.0.1, Xemacs 21.4.13, and ESS 5.2.2

Thanks for any help

Fredrik Lundgren



From michael.gray at somerville.oxford.ac.uk  Tue Dec 28 12:47:37 2004
From: michael.gray at somerville.oxford.ac.uk (Michael Gray)
Date: Tue, 28 Dec 2004 11:47:37 +0000 (GMT)
Subject: [R] R: repeat loops
Message-ID: <20041228114737.42CFBF4BE@webmail220.herald.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041228/04f8ae5e/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Dec 28 13:12:49 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 28 Dec 2004 13:12:49 +0100
Subject: [R] editing a package directly from R
In-Reply-To: <000501c4ecc4$47723a30$5f9d72d5@Larissa>
References: <000501c4ecc4$47723a30$5f9d72d5@Larissa>
Message-ID: <41D14DC1.80902@statistik.uni-dortmund.de>

Fredrik Lundgren wrote:

> Dear R,
> 
> I have made a package 'myfuncs' of some home made functions and 
> installed it without problems. It contains no C or fortran code and no 
> dynamic link library. Is there a way to edit this package directly from 
> R (or Xemacs with ESS) or do I always have to edit the source and then 
> reinstall the package? 

The latter.

Uwe Ligges

> I have a weak memory that such editing could be 
> done in S-Plus 2000 or 6.1?
> Windows XP, R 2.0.1, Xemacs 21.4.13, and ESS 5.2.2
> 
> Thanks for any help
> 
> Fredrik Lundgren
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Tue Dec 28 14:55:16 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 28 Dec 2004 14:55:16 +0100
Subject: [R] editing a package directly from R
In-Reply-To: <41D14DC1.80902@statistik.uni-dortmund.de>
References: <000501c4ecc4$47723a30$5f9d72d5@Larissa>
	<41D14DC1.80902@statistik.uni-dortmund.de>
Message-ID: <16849.26052.4473.499440@stat.math.ethz.ch>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Tue, 28 Dec 2004 13:12:49 +0100 writes:

    UweL> Fredrik Lundgren wrote:
    >> Dear R,
    >> 
    >> I have made a package 'myfuncs' of some home made
    >> functions and installed it without problems. It contains
    >> no C or fortran code and no dynamic link library. Is
    >> there a way to edit this package directly from R (or
    >> Xemacs with ESS) or do I always have to edit the source
    >> and then reinstall the package?

    UweL> The latter.

yes, edit the source by all means, but
no, you don't have to reinstall the package every time if you
are working on the package itself!

I do +- the following (on Unix; Windows should be close: use
		      'Rcmd' instead of "R CMD"):
1) a) If you are using a namespace, rename the "NAMESPACE" file to
      "NAMESPACE-" (e.g.), "R CMD check myfuncs" which installs
      a "no-namespace version" of the package into myfuncs.Rcheck
      {{and now gives warnings for all `namespace - private'
        objects that are not documented}}
   b) otherwise, just 'R CMD check myfuncs'

2) library(myfuncs, lib = "<correct_path_to>/myfuncs.Rcheck")
  
   You have all your current definitions available in *R*
   {assuming ESS}.

3) Now open the >> source << file you want to work on, say
    ....../myfuncs/R/onefun.R
   and use ESS (e.g. C-c C-l) to load the file/ or just the
   changed code into your R session.

   Test out your changes / improvements etc,
   always editing the source and sending its current contents to R.

4) Once you think you have ``done a batch'' (whatever that will mean).
   Run again  "R CMD check myfuncs" which uses your new changed
   source but also runs all other examples and the tests in
   myfuncs/tests/*.R  etc.

5) detach("package:myfuncs")  and   `` go to 2) ''

6) If you are finished with your changes, you'll want to rename
   "NAMESPACE-" back to "NAMESPACE".
    {you will typically be reminded because of 'R CMD check's warnings}.

Martin Maechler, ETH Zurich

    >> I have a weak memory that such editing could be done in
    >> S-Plus 2000 or 6.1?  Windows XP, R 2.0.1, Xemacs 21.4.13,
    >> and ESS 5.2.2
    >> 
    >> Thanks for any help
    >> 
    >> Fredrik Lundgren



From ales.ziberna at guest.arnes.si  Tue Dec 28 14:54:24 2004
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Tue, 28 Dec 2004 14:54:24 +0100
Subject: [R] R: repeat loops
References: <20041228114737.42CFBF4BE@webmail220.herald.ox.ac.uk>
Message-ID: <005201c4ece4$ee40f6b0$1a09f9c2@ales>

The mistakes in your function are:
1. the functions repeat and break do not have any arguments
2. the break function should be used insed the repear loop (or if statment 
with break function should be inside the repeat loop)

Correcting for these mistakes I corrected your function. It is at the end of 
my reply.

I also belive that it might be better to use while function.
Look at ?Control (if you haven't already)

I hope this helps!

Ale??

sim2.dat<-function(n,theta){
 u<-runif(n)
 k<-1
 P<--(1-theta)/log(theta)
 F<-P

 for(j in 1:n){
  repeat {
   P<-k*(1-theta)*P/(k+1)
   k<-k+1
   F<-F+P
   if(u[j]<F){break}
  }
 u[j]<-k
 }
 u
}



----- Original Message ----- 
From: "Michael Gray" <michael.gray at somerville.oxford.ac.uk>
To: <R-help at stat.math.ethz.ch>
Sent: Tuesday, December 28, 2004 12:47 PM
Subject: [R] R: repeat loops


To whoever this may concern

I am trying to write a repeat loop and can't make out from the documentation 
on the website how exactly to construct the repeat, break structure of the 
loop. Below is the function sim2.dat that I am trying to create, in which 
firstly I create n random uniform(0,1) numbers. From there I assign values 
to k, P and F. From then on for each random number u[j] of u, I try to set 
up a repeat loop so that if u[j]>=F I alter P, k, and F and then compare 
u[j] to F again. I want to stop the loop when u[j]<F and then assign the 
present value of k to u[j]. However my effort below seems to fail! Any help 
would be much appreciated, either with respect to repeat loops or better 
ways to conquer my problem. Thank you for your time,

Regards,

Michael Gray

R text:
sim2.dat<-function(n,theta){
+ u<-runif(n)
+ k<-1
+ P<--(1-theta)/log(theta)
+ F<-P
+ for(j in 1:n){repeat{if(u[j]>=F){P<-k*(1-theta)*P/(k+1)
+ k<-k+1
+ F<-F+P}}
+ if(u[j]<F){break(
+ repeat{if(u[j]>=F){P<-k*(1-theta)*P/(k+1)
+ k<-k+1
+ F<-F+P}})
+ u[j]<-k}}
+ u}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From giljustino at yahoo.com.br  Tue Dec 28 17:46:10 2004
From: giljustino at yahoo.com.br (Gilvan Justino)
Date: Tue, 28 Dec 2004 13:46:10 -0300 (ART)
Subject: [R] Developing R classes
Message-ID: <20041228164610.85427.qmail@web50406.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041228/808350c6/attachment.pl

From andy_liaw at merck.com  Tue Dec 28 18:00:32 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 28 Dec 2004 12:00:32 -0500
Subject: [R] Developing R classes
Message-ID: <3A822319EB35174CA3714066D590DCD50994E472@usrymx25.merck.com>

Fritz's lecture at useR! 2004 may also be a good starting point:
http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Leisch.pdf

Andy

> From: Gilvan Justino
> 
> Hi,
>  
> I??m trying to write some R classes but I din??t find 
> documentation enought to develop them. I read there is 2 ways 
> to write classes, using S3 ou S4 models. And it seems that S4 
> is the best model, so I thing I should use this one. 
>  
> I??m new user of R and I??m searched on the net some 
> information about creating new classes. I found this document:
> http://www.biostat.harvard.edu/courses/individual/bio271/lectu
res/L11/S4Objects.pdf
If someone knows some docs about creating our own classes, could please,
post its url at here ?
 
More one question... It seems that some documents about S can be applyed to
R. How do you know if you can use this documentation using R ? I??m asking
this because some stufs I was looking for I found more explanation using S
and not R. Am I right or I??m looking in the wrong place ?
 
Thanks a lot!
 
__________________________________________________


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Tue Dec 28 18:17:22 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 28 Dec 2004 12:17:22 -0500 (EST)
Subject: [R] Developing R classes
Message-ID: <20041228171722.851EB12D65@mprdmxin.myway.com>



Chambers book, Programming with Data, has a lot about S4.

Note that S4 has more features but S3 is simpler and 
has higher performance so its not all one way.  Also, they
are related so if you learn the simpler S3 first it will
make it easier to learn S4 later.  

I suggest you download and read the source code for
package `zoo' for S3 and package `its' for S4.  Both packages
define irregular time series classes.

Date:   Tue, 28 Dec 2004 13:46:10 -0300 (ART) 
From:   Gilvan Justino <giljustino at yahoo.com.br>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] Developing R classes 

 
Hi,

Im trying to write some R classes but I dint find documentation enought to develop them. I read there is 2 ways to write classes, using S3 ou S4 models. And it seems that S4 is the best model, so I thing I should use this one. 

Im new user of R and Im searched on the net some information about creating new classes. I found this document:
http://www.biostat.harvard.edu/courses/individual/bio271/lectures/L11/S4Objects.pdf
If someone knows some docs about creating our own classes, could please, post its url at here ?

More one question... It seems that some documents about S can be applyed to R. How do you know if you can use this documentation using R ? Im asking this because some stufs I was looking for I found more explanation using S and not R. Am I right or Im looking in the wrong place ?

Thanks a lot!



From rvalliant at survey.umd.edu  Tue Dec 28 18:24:27 2004
From: rvalliant at survey.umd.edu (Richard Valliant)
Date: Tue, 28 Dec 2004 12:24:27 -0500
Subject: [R] lost association for .RData files
Message-ID: <s1d15089.028@SURVEYGWIA.UMD.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041228/fa9ab1c2/attachment.pl

From 2knortonex at cdacouncil.org  Tue Dec 28 18:32:51 2004
From: 2knortonex at cdacouncil.org (2knortonex)
Date: Tue, 28 Dec 2004 12:32:51 -0500
Subject: [R] Norton AntiVirus detected and quarantined a virus in a message
	you sent.
Message-ID: <656D7FA872131D4B85661286CE2B042ADCDAC8@ex2k.cdacouncil.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041228/af036663/attachment.pl

From tlumley at u.washington.edu  Tue Dec 28 18:38:50 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 28 Dec 2004 09:38:50 -0800 (PST)
Subject: [R] Developing R classes
In-Reply-To: <20041228164610.85427.qmail@web50406.mail.yahoo.com>
References: <20041228164610.85427.qmail@web50406.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0412280937420.25188@homer03.u.washington.edu>

On Tue, 28 Dec 2004, Gilvan Justino wrote:

> Hi,
>
> Im trying to write some R classes but I dint find documentation 
> enought to develop them. I read there is 2 ways to write classes, using 
> S3 ou S4 models. And it seems that S4 is the best model, so I thing I 
> should use this one.

There's a small example in the R Newsletter, Volume 1, 2004.

 	-thomas

From ggrothendieck at myway.com  Tue Dec 28 18:48:46 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 28 Dec 2004 12:48:46 -0500 (EST)
Subject: [R] lost association for .RData files
Message-ID: <20041228174846.94ED412D62@mprdmxin.myway.com>


You can change file associations by going into Windows Explorer
(hold down Windows key and press E) and choosing Tools | Options.
Take the File Types tab.  Now, select RDATA from the long list 
and click on Advanced and Edit and enter the string:

   "C:\Program Files\R\rw2001\bin\RGui.exe" "%1"

in the box labelled Application used to perform the action.

Date:   Tue, 28 Dec 2004 12:24:27 -0500 
From:   Richard Valliant <rvalliant at survey.umd.edu>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] lost association for .RData files 

 
Somehow I have lost the correct file association for .RData files. They
are now associated with a text file editor. When I right click on any
.RData file and try to change the association, R is not listed as a
choice of program. I browse to c:\Program Files\R\rw2001\bin\Rgui.exe
and select that file to open .RData, but Windows does not then place
Rgui.exe in the list of programs to use. I can open any .RData file by
dragging it onto the R desktop icon, but cannot double-click any .RData
file to open it.

This problem has persisted through several uninstalls of old versions
and upgrades to new versions of R. I found no mention of the problem in
the mail archives.

Windows XP
R 2.0.1

Thanks,
Richard Valliant



From anthony at stat.sdu.dk  Tue Dec 28 18:57:04 2004
From: anthony at stat.sdu.dk (Anthony Gichangi)
Date: Tue, 28 Dec 2004 18:57:04 +0100
Subject: [R] Varying  x-axes ticks
Message-ID: <000601c4ed06$a435ea40$cb83e182@hq.skylab.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041228/cbf308cf/attachment.pl

From das at cshl.edu  Tue Dec 28 19:22:30 2004
From: das at cshl.edu (Rajdeep Das)
Date: Tue, 28 Dec 2004 13:22:30 -0500
Subject: [R] glm vs multinom
Message-ID: <000a01c4ed0a$31845af0$6500a8c0@artney>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041228/7e57c352/attachment.pl

From thchung at tgen.org  Tue Dec 28 19:51:27 2004
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Tue, 28 Dec 2004 11:51:27 -0700
Subject: [R] Configuration of memory usage
Message-ID: <BDF6F93F.214D%thchung@tgen.org>

Hi, all;

I know there has been a lot of discussions on memory usage in R.
However, I have some odd situation here. Basically, I have a rare
opportunity to run R in a system with 64GB memory without any limit on
memory usage for any person or process. However, I encountered the memory
problem error message like this:

Error: cannot allocate vector of size 594075 Kb

I got this error message while I was trying to apply dChip preprocessing
procedures for 150 Affymetrix U133v2 chips that has > 22,000 probe sets on
them. The actual codes I ran was like this:

> Data <- ReadAffy(filenames = paste(HOME, "CelData/", fname, sep=""))
> mem.limits()
nsize vsize
   NA    NA
> gc()
           used  (Mb) gc trigger   (Mb)
Ncells   530216  14.2     899071   24.1
Vcells 76196137 581.4  243983468 1861.5
> eset <- expresso(Data, normalize.method="invariantset", bg.correct=FALSE, pmc\
orrect.method="pmonly", summary.method="liwong")
normalization: invariantset
PM/MM correction : pmonly
expression values: liwong
normalizing...Error: cannot allocate vector of size 594075 Kb
> gc()
           used  (Mb) gc trigger   (Mb)
Ncells   797983  21.4    1710298   45.7
Vcells 76716811 585.4  305954068 2334.3
> object.size(Data)
[1] 608355664
> memory.profile()
    NILSXP     SYMSXP    LISTSXP     CLOSXP     ENVSXP    PROMSXP    LANGSXP
         1      30484     372373       4845        420        180     127274
SPECIALSXP BUILTINSXP    CHARSXP     LGLSXP                           INTSXP
       203       1168     111434       5296          0          0      44649
   REALSXP    CPLXSXP     STRSXP     DOTSXP     ANYSXP     VECSXP    EXPRSXP
     13382          9      60173          0          0      26002          0
  BCODESXP  EXTPTRSXP WEAKREFSXP
         0        106          0

Although I have no idea of memory allocation in R, apparently something's
wrong with this. The memory problem must have nothing to do with physical
memory. My question is this. Is this memory problem due to some non-optimal
configuration of memory usage? If so, then what will be the optimal
configuration for this? If not, then there must be problems on actual
implementations of functions I used here, right? The reason I am asking this
is that, according to the reference manual, the error message I got can be
brought up by roughly three reasons. First, when the system is unable to
provide the R requested memory. Second, when the requested memory size
exceeds the address-space limit for a process. Finally, when the length of a
vector is larger than 2^31-1. I wonder the problem has anything to do with
the third case. (If so, then I think I am hopeless unless the internal
implementations change...)



From rgentlem at fhcrc.org  Tue Dec 28 20:05:38 2004
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Tue, 28 Dec 2004 11:05:38 -0800
Subject: [R] Developing R classes
In-Reply-To: <Pine.A41.4.61b.0412280937420.25188@homer03.u.washington.edu>
References: <20041228164610.85427.qmail@web50406.mail.yahoo.com>
	<Pine.A41.4.61b.0412280937420.25188@homer03.u.washington.edu>
Message-ID: <760967E3-5903-11D9-A592-000D933DC9FE@fhcrc.org>

And some notes on the Bioconductor developer page.

On Dec 28, 2004, at 9:38 AM, Thomas Lumley wrote:

> On Tue, 28 Dec 2004, Gilvan Justino wrote:
>
>> Hi,
>>
>> I??m trying to write some R classes but I din??t find documentation  
>> enought to develop them. I read there is 2 ways to write classes,  
>> using S3 ou S4 models. And it seems that S4 is the best model, so I  
>> thing I should use this one.
>
> There's a small example in the R Newsletter, Volume 1, 2004.
>
> 	-thomas______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
+----------------------------------------------------------------------- 
----------------+
| Robert Gentleman              phone: (206) 667-7700                    
          |
| Head, Program in Computational Biology   fax:  (206) 667-1319   |
| Division of Public Health Sciences       office: M2-B865               
       |
| Fred Hutchinson Cancer Research Center                                 
          |
| email: rgentlem at fhcrc.org                                              
                          |
+----------------------------------------------------------------------- 
----------------+



From ashelton at albany.edu  Tue Dec 28 20:08:52 2004
From: ashelton at albany.edu (Anne Shelton)
Date: Tue, 28 Dec 2004 14:08:52 -0500
Subject: [R] (no subject)
Message-ID: <63D883CB0A1B204EB053673DD882CB5305356294@email.albany.edu>



Anne Shelton
ITS Research IT Group
University at Albany
1400 Washington Ave. - MSC-100
Albany, NY 12222
 
Office: 437-4525  Cell: 788-8634



From p.dalgaard at biostat.ku.dk  Tue Dec 28 20:14:16 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Dec 2004 20:14:16 +0100
Subject: [R] Configuration of memory usage
In-Reply-To: <BDF6F93F.214D%thchung@tgen.org>
References: <BDF6F93F.214D%thchung@tgen.org>
Message-ID: <x2brcez14n.fsf@biostat.ku.dk>

Tae-Hoon Chung <thchung at tgen.org> writes:

> Hi, all;
> 
> I know there has been a lot of discussions on memory usage in R.
> However, I have some odd situation here. Basically, I have a rare
> opportunity to run R in a system with 64GB memory without any limit on
> memory usage for any person or process. However, I encountered the memory
> problem error message like this:
> 
> Error: cannot allocate vector of size 594075 Kb
....
> Although I have no idea of memory allocation in R, apparently something's
> wrong with this. The memory problem must have nothing to do with physical
> memory. My question is this. Is this memory problem due to some non-optimal
> configuration of memory usage? If so, then what will be the optimal
> configuration for this? If not, then there must be problems on actual
> implementations of functions I used here, right? The reason I am asking this
> is that, according to the reference manual, the error message I got can be
> brought up by roughly three reasons. First, when the system is unable to
> provide the R requested memory. Second, when the requested memory size
> exceeds the address-space limit for a process. Finally, when the length of a
> vector is larger than 2^31-1. 

Hmm, the length issue should not kick in before the length exceeds 2
billion or so and you are not beyond 75 or 150 million (counting 8 or
4 bytes per elements).

> I wonder the problem has anything to do with
> the third case. (If so, then I think I am hopeless unless the internal
> implementations change...)

Well, revolutionaries often find themselves just below the cutting
edge...

Just a sanity check: this is using a 64-bit compiled R on a 64-bit
operating system, right?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Tue Dec 28 20:53:45 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Dec 2004 11:53:45 -0800
Subject: [R] Varying  x-axes ticks
In-Reply-To: <000601c4ed06$a435ea40$cb83e182@hq.skylab.dk>
References: <000601c4ed06$a435ea40$cb83e182@hq.skylab.dk>
Message-ID: <41D1B9C9.306@pdf.com>

      Please consider the following: 

plot(0:1, axes=F)
axis(1)

      If you read the help page for "axis" and work the examples there, 
you may be able to figure it out.  Hint:  Develop a function, say, fn, 
to shrink 25-104 to 25-50 leaving x < 25 unchanged.  Then the following 
should work for you: 

plot(fn(x), y, axes=F) # where x and y are the numbers you want to plot. 
axis(1, fn(xlabs), xlabs) # where xlabs = numbers for which you want 
tick marks and labels
axis(2)

      I did NOT TEST this particular piece of code, but I've done things 
like this in the past, and they worked. 

      hope this helps. 
      spencer graves

Anthony Gichangi wrote:

>Dear R Users,
>I have the following situations to plot. 
>
>x-values (times)      :  range from 0 to 104           (in weeks)
>y-values (hazards)  :  range from 0 to  0.5
>
>Now I want to make a plot of time versus hazards such
>that  in the time axis (i.e x-axis) the values 0-24 occupies
>half of the axis and the values 25-104 ocupies the rest
>of the axis. 
>
>In other words I want to emphasize on first 24 weeks so that
>the features of the hazard are more pronounced for these 
>values.
>
>Anybody with ideas how I can accomplish this ?
>
>
>Thanks in advance
>
>
>With kind regards
>
>Anthony
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From ripley at stats.ox.ac.uk  Tue Dec 28 21:42:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Dec 2004 20:42:13 +0000 (GMT)
Subject: [R] lost association for .RData files
In-Reply-To: <s1d15089.028@SURVEYGWIA.UMD.EDU>
References: <s1d15089.028@SURVEYGWIA.UMD.EDU>
Message-ID: <Pine.LNX.4.61.0412282040290.22196@gannet.stats>

It is an optional part of the install: did you check the option?
If so, it still works in R 2.0.1.

On Tue, 28 Dec 2004, Richard Valliant wrote:

> Somehow I have lost the correct file association for .RData files. They
> are now associated with a text file editor. When I right click on any
> .RData file and try to change the association, R is not listed as a
> choice of program. I browse to c:\Program Files\R\rw2001\bin\Rgui.exe
> and select that file to open .RData, but Windows does not then place
> Rgui.exe in the list of programs to use.  I can open any .RData file by
> dragging it onto the R desktop icon, but cannot double-click any .RData
> file to open it.
>
> This problem has persisted through several uninstalls of old versions
> and upgrades to new versions of R. I found no mention of the problem in
> the mail archives.
>
> Windows XP
> R 2.0.1

> Thanks,
> Richard Valliant


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Dec 28 21:43:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Dec 2004 20:43:59 +0000 (GMT)
Subject: [R] glm vs multinom
In-Reply-To: <000a01c4ed0a$31845af0$6500a8c0@artney>
References: <000a01c4ed0a$31845af0$6500a8c0@artney>
Message-ID: <Pine.LNX.4.61.0412282042420.22196@gannet.stats>

On Tue, 28 Dec 2004, Rajdeep Das wrote:

> I am doing two class classification using logistic regression. I 
> realized that I can either use "glm" function or "multinom" function. I 
> know "multinom" is used for multiclass classification. But if I was it 
> for binary classification, I was wondering if there is an difference in 
> the results compared to "glm" results.

Why wonder?  Try it and see.
If you use both correctly, the differences will be small.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Dec 28 21:52:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Dec 2004 20:52:51 +0000 (GMT)
Subject: [R] Configuration of memory usage
In-Reply-To: <BDF6F93F.214D%thchung@tgen.org>
References: <BDF6F93F.214D%thchung@tgen.org>
Message-ID: <Pine.LNX.4.61.0412282044390.22196@gannet.stats>

Your lack of knowledge extends to the R posting guide: please consult it
before posting.

1) Do not post to two lists!  I've removed the BioC list.

2) Do tell us your system details.  Looks like you have a 32-bit version 
of R (from the size of the Ncells), and you need a 64-bit version to make 
use of more than about 3Gb, so your results seem completely consistent 
with the limits of your build of R (rather than of R).


On Tue, 28 Dec 2004, Tae-Hoon Chung wrote:

> Hi, all;
>
> I know there has been a lot of discussions on memory usage in R.
> However, I have some odd situation here. Basically, I have a rare
> opportunity to run R in a system with 64GB memory without any limit on
> memory usage for any person or process. However, I encountered the memory
> problem error message like this:
>
> Error: cannot allocate vector of size 594075 Kb
>
> I got this error message while I was trying to apply dChip preprocessing
> procedures for 150 Affymetrix U133v2 chips that has > 22,000 probe sets on
> them. The actual codes I ran was like this:
>
>> Data <- ReadAffy(filenames = paste(HOME, "CelData/", fname, sep=""))
>> mem.limits()
> nsize vsize
>   NA    NA
>> gc()
>           used  (Mb) gc trigger   (Mb)
> Ncells   530216  14.2     899071   24.1
> Vcells 76196137 581.4  243983468 1861.5
>> eset <- expresso(Data, normalize.method="invariantset", bg.correct=FALSE, pmc\
> orrect.method="pmonly", summary.method="liwong")
> normalization: invariantset
> PM/MM correction : pmonly
> expression values: liwong
> normalizing...Error: cannot allocate vector of size 594075 Kb
>> gc()
>           used  (Mb) gc trigger   (Mb)
> Ncells   797983  21.4    1710298   45.7
> Vcells 76716811 585.4  305954068 2334.3
>> object.size(Data)
> [1] 608355664
>> memory.profile()
>    NILSXP     SYMSXP    LISTSXP     CLOSXP     ENVSXP    PROMSXP    LANGSXP
>         1      30484     372373       4845        420        180     127274
> SPECIALSXP BUILTINSXP    CHARSXP     LGLSXP                           INTSXP
>       203       1168     111434       5296          0          0      44649
>   REALSXP    CPLXSXP     STRSXP     DOTSXP     ANYSXP     VECSXP    EXPRSXP
>     13382          9      60173          0          0      26002          0
>  BCODESXP  EXTPTRSXP WEAKREFSXP
>         0        106          0
>
> Although I have no idea of memory allocation in R, apparently something's
> wrong with this. The memory problem must have nothing to do with physical
> memory. My question is this. Is this memory problem due to some non-optimal
> configuration of memory usage? If so, then what will be the optimal
> configuration for this? If not, then there must be problems on actual
> implementations of functions I used here, right? The reason I am asking this
> is that, according to the reference manual, the error message I got can be
> brought up by roughly three reasons. First, when the system is unable to
> provide the R requested memory. Second, when the requested memory size
> exceeds the address-space limit for a process. Finally, when the length of a
> vector is larger than 2^31-1. I wonder the problem has anything to do with
> the third case. (If so, then I think I am hopeless unless the internal
> implementations change...)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From arshia22 at yahoo.com  Tue Dec 28 22:19:08 2004
From: arshia22 at yahoo.com (ebashi)
Date: Tue, 28 Dec 2004 13:19:08 -0800 (PST)
Subject: [R] R&PHP
Message-ID: <20041228211908.38961.qmail@web81001.mail.yahoo.com>

Dear friends;

Does anyone have any idea how to connect PHP & R and
using Mysql as DB.

Sincerely,
Sean



From tfliao at uiuc.edu  Tue Dec 28 22:26:01 2004
From: tfliao at uiuc.edu (Tim F Liao)
Date: Tue, 28 Dec 2004 15:26:01 -0600
Subject: [R] lost association for .RData files
Message-ID: <b0520fa5.b5f4f2f2.8199700@expms6.cites.uiuc.edu>

If you right click the file icon of any of the RData files 
in Windows Explorer, the click Open With, and then Browse 
and Choose the R executable file icon in the R folder under 
Program Files (and make sure "Always Use This Program" is 
chosen), then you should be able to get the default back.

Tim Liao
Professor of Sociology & Statistics
University of Illinois

---- Original message ----
>Date: Tue, 28 Dec 2004 12:24:27 -0500
>From: "Richard Valliant" <rvalliant at survey.umd.edu>  
>Subject: [R] lost association for .RData files  
>To: <r-help at stat.math.ethz.ch>
>
>Somehow I have lost the correct file association for .RData 
files. They
>are now associated with a text file editor. When I right 
click on any
>.RData file and try to change the association, R is not 
listed as a
>choice of program. I browse to c:\Program Files\R\rw2001
\bin\Rgui.exe
>and select that file to open .RData, but Windows does not 
then place
>Rgui.exe in the list of programs to use.  I can open 
any .RData file by
>dragging it onto the R desktop icon, but cannot double-
click any .RData
>file to open it.
> 
>This problem has persisted through several uninstalls of 
old versions
>and upgrades to new versions of R. I found no mention of 
the problem in
>the mail archives.
> 
>Windows XP
>R 2.0.1
> 
>Thanks,
>Richard Valliant
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-
project.org/posting-guide.html



From Cary.Miller at uchsc.edu  Tue Dec 28 22:37:03 2004
From: Cary.Miller at uchsc.edu (Cary Miller)
Date: Tue, 28 Dec 2004 14:37:03 -0700 (MST)
Subject: [R] error installing 2.0.1 '.install_package_description'
Message-ID: <Pine.LNX.4.44.0412281430030.26304-100000@tobor.uchsc.edu>

Dear R,
  I am attempting to install 2.0.1 on a Debian system and getting an error
that appeared for people installing 2.0.0.

Warning message:
package seems to be using lazy loading already in:
makeLazyLoading("tools")
Error in get(x, envir, mode, inherits) : variable
".install_package_description" was not found
Execution halted


platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    9.0


?????
  Thanks

//=\ Cary Miller                                    //=\
\=// Center for Computational Pharmacology          \=//
//=\ University of Colorado Health Sciences Center  //=\
\=// http://compbio.uchsc.edu/Hunter_lab/Miller     \=//
//=\ (303) 724-0263                                 //=\



From thills at mail.utexas.edu  Tue Dec 28 23:47:12 2004
From: thills at mail.utexas.edu (thomas hills)
Date: Tue, 28 Dec 2004 16:47:12 -0600
Subject: [R] read.table from a list of filenames
In-Reply-To: <200412281109.iBSB7uvx019408@hypatia.math.ethz.ch>
Message-ID: <69F6E851-5922-11D9-84F6-000393DC4A86@mail.utexas.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041228/1c89dfc1/attachment.pl

From andy_liaw at merck.com  Tue Dec 28 23:55:46 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 28 Dec 2004 17:55:46 -0500
Subject: [R] read.table from a list of filenames
Message-ID: <3A822319EB35174CA3714066D590DCD50994E473@usrymx25.merck.com>

The solution is in section 7.21 of the R FAQ.  BTW, `rf' is a built-in R
function for generating random numbers from an F distribution, so better use
some other name.

Andy

> From: thomas hills
> 
> I am wondering if it is possible to read.table repeatedly from a list 
> of file names into a new list of table names.
> 
> For example:
> 
> filenames <- list.files()
> 
> then with a function like
> 
> rf <- function(i) {
> word??(filename[i]) <- read.table(filenames[i]) }
> 
> I can't seem to find a function like word?? that will be the 
> object of 
> another operation.   If this worked, then I could repeat it for the 
> length of filenames.
> 
> Also, even the following function seems to give me an error, but I 
> don't yet know why.
> 
> rf <- function(nam, i) {  nam <- read.table(filenames[i]) }
> 
> 
> Any help would be very much appreciated.
> 
> Thanks,
> Thomas
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tlumley at u.washington.edu  Wed Dec 29 00:05:38 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 28 Dec 2004 15:05:38 -0800 (PST)
Subject: [R] read.table from a list of filenames
In-Reply-To: <69F6E851-5922-11D9-84F6-000393DC4A86@mail.utexas.edu>
References: <69F6E851-5922-11D9-84F6-000393DC4A86@mail.utexas.edu>
Message-ID: <Pine.A41.4.61b.0412281500540.25188@homer03.u.washington.edu>

On Tue, 28 Dec 2004, thomas hills wrote:

> I am wondering if it is possible to read.table repeatedly from a list
> of file names into a new list of table names.
>
> For example:
>
> filenames <- list.files()
>
> then with a function like
>
> rf <- function(i) {
> word??(filename[i]) <- read.table(filenames[i]) }

lapply(filenames, read.table)

will do what I think you want to do.

[It is possible to assign each file to a variable whose name is given by
  another variable, and FAQ 7.21 tells you how, but that probably isn't a
  good idea.]

 	-thomas



From p.dalgaard at biostat.ku.dk  Wed Dec 29 00:06:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Dec 2004 00:06:14 +0100
Subject: [R] read.table from a list of filenames
In-Reply-To: <69F6E851-5922-11D9-84F6-000393DC4A86@mail.utexas.edu>
References: <69F6E851-5922-11D9-84F6-000393DC4A86@mail.utexas.edu>
Message-ID: <x2oegexbtl.fsf@biostat.ku.dk>

thomas hills <thills at mail.utexas.edu> writes:

> I am wondering if it is possible to read.table repeatedly from a list 
> of file names into a new list of table names.
> 
> For example:
> 
> filenames <- list.files()
> 
> then with a function like
> 
> rf <- function(i) {
> word??(filename[i]) <- read.table(filenames[i]) }
> 
> I can't seem to find a function like word?? that will be the object of 
> another operation.   If this worked, then I could repeat it for the 
> length of filenames.
> 
> Also, even the following function seems to give me an error, but I 
> don't yet know why.
> 
> rf <- function(nam, i) {  nam <- read.table(filenames[i]) }
> 
> 
> Any help would be very much appreciated.

Something like

listoftables <- lapply(filenames, read.table)
names(listoftables) <- filenames

might be what you are looking for.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From edd at debian.org  Wed Dec 29 01:03:40 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 28 Dec 2004 18:03:40 -0600
Subject: [R] error installing 2.0.1 '.install_package_description'
In-Reply-To: <Pine.LNX.4.44.0412281430030.26304-100000@tobor.uchsc.edu>
References: <Pine.LNX.4.44.0412281430030.26304-100000@tobor.uchsc.edu>
Message-ID: <20041229000340.GA9547@sonny.eddelbuettel.com>

On Tue, Dec 28, 2004 at 02:37:03PM -0700, Cary Miller wrote:
> Dear R,
>   I am attempting to install 2.0.1 on a Debian system and getting an error
> that appeared for people installing 2.0.0.
> 
> Warning message:
> package seems to be using lazy loading already in:
> makeLazyLoading("tools")
> Error in get(x, envir, mode, inherits) : variable
> ".install_package_description" was not found
> Execution halted

Could you please tell us with more precision
-- what you were trying to install, and
-- how you were attempting it?

The error message is non-informative.

Dirk


> 
> 
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    9.0
> 
> 
> ?????
>   Thanks
> 
> //=\ Cary Miller                                    //=\
> \=// Center for Computational Pharmacology          \=//
> //=\ University of Colorado Health Sciences Center  //=\
> \=// http://compbio.uchsc.edu/Hunter_lab/Miller     \=//
> //=\ (303) 724-0263                                 //=\
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004



From HDoran at air.org  Wed Dec 29 01:03:40 2004
From: HDoran at air.org (Doran, Harold)
Date: Tue, 28 Dec 2004 19:03:40 -0500
Subject: [R] gls model and matrix operations
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044E2A@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041228/52f19c79/attachment.pl

From Cary.Miller at uchsc.edu  Wed Dec 29 01:54:02 2004
From: Cary.Miller at uchsc.edu (Cary Miller)
Date: Tue, 28 Dec 2004 17:54:02 -0700 (MST)
Subject: [R] .install_package_description error
Message-ID: <Pine.LNX.4.44.0412281723170.26381-100000@tobor.uchsc.edu>

Hi R,
  Apologies for the earlier non-informative error message.  I am
installing R 2.0.1 from source on a Linux debian system.  Configure works
fine.  Then I run make and get an error that was reported by several
people when installing R 2.0.0.  Here is the error message

> Warning message:
> package seems to be using lazy loading already in:
> makeLazyLoading("tools")
> Error in get(x, envir, mode, inherits) : variable
> ".install_package_description" was not found
> Execution halted

In October this same error message appeared in several messages and was
apparently fixed by a change in src/library/tools/R/admin.R to the 'Built'
function.  The 2.0.1 version appears to contain those changes.  Am I
missing something?

  --Thanks

//=\ Cary Miller
\=// Center for Computational Pharmacology
//=\ University of Colorado Health Sciences Center
\=// http://compbio.uchsc.edu/Hunter_lab/Miller
//=\ (303) 724-0263



From edd at debian.org  Wed Dec 29 03:07:59 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 28 Dec 2004 20:07:59 -0600
Subject: [R] .install_package_description error
In-Reply-To: <Pine.LNX.4.44.0412281723170.26381-100000@tobor.uchsc.edu>
References: <Pine.LNX.4.44.0412281723170.26381-100000@tobor.uchsc.edu>
Message-ID: <20041229020759.GA10389@sonny.eddelbuettel.com>

On Tue, Dec 28, 2004 at 05:54:02PM -0700, Cary Miller wrote:
> Hi R,
>   Apologies for the earlier non-informative error message.  I am
> installing R 2.0.1 from source on a Linux debian system.  Configure works

You could use the fallback strategy of employing our pre-built packages:

	$ apt-get install r-base

They do contain just about feature there is: 

edd at homebud:~> R

R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
[...]
> capabilities()
    jpeg      png    tcltk      X11    GNOME     libz http/ftp  sockets
    TRUE     TRUE     TRUE     TRUE    FALSE     TRUE     TRUE     TRUE
  libxml     fifo   cledit  IEEE754    bzip2     PCRE
    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
>                        

so that wouldn't be a reason to roll your own, but you may of course have
other excellent reasons to.

> fine.  Then I run make and get an error that was reported by several
> people when installing R 2.0.0.  Here is the error message
> 
> > Warning message:
> > package seems to be using lazy loading already in:
> > makeLazyLoading("tools")
> > Error in get(x, envir, mode, inherits) : variable
> > ".install_package_description" was not found
> > Execution halted

In the course of looking after the Debian R packages, I tend to build R
fairly regularly, and I don't think I've seen this error.  I can only guess
that you may be mixing an old (pre-2.0.0) and a new installation, which
would obviously be a bad idea at the best of times, and especially when
binary compatibility 

> In October this same error message appeared in several messages and was
> apparently fixed by a change in src/library/tools/R/admin.R to the 'Built'

I either must have missed that, or forgotten about it.

> function.  The 2.0.1 version appears to contain those changes.  Am I
> missing something?

Not necessarily.  You could try to poke around in the debian/ directory of
the sources themselves to see our configure options (with a minor version
skew).  The very detailed buildd logs at

	http://buildd.debian.org/build.php?pkg=r-base

contain some useful info too, if you know where to look.

Let me know (off-line, maybe ?) if I can help further.

Dirk

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004



From f.harrell at vanderbilt.edu  Wed Dec 29 04:16:42 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 28 Dec 2004 21:16:42 -0600
Subject: [R] Developing R classes
In-Reply-To: <20041228171722.851EB12D65@mprdmxin.myway.com>
References: <20041228171722.851EB12D65@mprdmxin.myway.com>
Message-ID: <41D2219A.6010603@vanderbilt.edu>

Gabor Grothendieck wrote:
> 
> Chambers book, Programming with Data, has a lot about S4.
> 
> Note that S4 has more features but S3 is simpler and 
> has higher performance so its not all one way.  Also, they
> are related so if you learn the simpler S3 first it will
> make it easier to learn S4 later.  
> 
> I suggest you download and read the source code for
> package `zoo' for S3 and package `its' for S4.  Both packages
> define irregular time series classes.

I agree with Gabor.  S3 is much easier and faster to implement and has 
flexibility advantages (e.g., adding new elements to fit objects for 
debugging).

Frank

> 
> Date:   Tue, 28 Dec 2004 13:46:10 -0300 (ART) 
> From:   Gilvan Justino <giljustino at yahoo.com.br>
> To:   <r-help at stat.math.ethz.ch> 
> Subject:   [R] Developing R classes 
> 
>  
> Hi,
> 
> I??m trying to write some R classes but I din??t find documentation enought to develop them. I read there is 2 ways to write classes, using S3 ou S4 models. And it seems that S4 is the best model, so I thing I should use this one. 
> 
> I??m new user of R and I??m searched on the net some information about creating new classes. I found this document:
> http://www.biostat.harvard.edu/courses/individual/bio271/lectures/L11/S4Objects.pdf
> If someone knows some docs about creating our own classes, could please, post its url at here ?
> 
> More one question... It seems that some documents about S can be applyed to R. How do you know if you can use this documentation using R ? I??m asking this because some stufs I was looking for I found more explanation using S and not R. Am I right or I??m looking in the wrong place ?
> 
> Thanks a lot!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From thills at mail.utexas.edu  Wed Dec 29 04:27:21 2004
From: thills at mail.utexas.edu (thomas hills)
Date: Tue, 28 Dec 2004 21:27:21 -0600
Subject: [R] using get() in assign()
In-Reply-To: <200412281109.iBSB7uvx019408@hypatia.math.ethz.ch>
Message-ID: <8D1E8E95-5949-11D9-84F6-000393DC4A86@mail.utexas.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041228/53a34088/attachment.pl

From spencer.graves at pdf.com  Wed Dec 29 05:47:45 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Dec 2004 20:47:45 -0800
Subject: [R] using get() in assign()
In-Reply-To: <8D1E8E95-5949-11D9-84F6-000393DC4A86@mail.utexas.edu>
References: <8D1E8E95-5949-11D9-84F6-000393DC4A86@mail.utexas.edu>
Message-ID: <41D236F1.9030203@pdf.com>

      Consider the following: 

 > changeNames <- function(dfName,
+          newNames=c("A1", "B1"), pos.=1){
+   DF. <- get(dfName)
+   names(DF.) <- newNames
+   assign(dfName, DF., pos=pos.)
+   "done"
+ }
 >
 > DF <- data.frame(a=1, b=2)
 > changeNames("DF")
[1] "done"
 > DF
  A1 B1
1  1  2
 >
      There's probably a way to do this with "do.call", but the above 
seems to work. 

      hope this helps.  spencer graves

thomas hills wrote:

>I'm trying to rename the columns in a list of data.frames using the 
>following...
>
>for(i in 1:length(filenames)) {
>assign(names(get(filenames[i])), c("name", "infood", "time") ) }
>
>R returns no errors, but the names are unchanged in the data.frames.
>
>The original names were things like
>
> > names(get(filenames[2]))
>[1] "Tc45w4.V1" "Tc45w4.V2" "Tc45w4.V3"
>
>after the above procedure they are still those names.
>
>Ideas appreciated.  Thanks.
>
>Thomas
>
>
>
>	[[alternative text/enriched version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From p.dalgaard at biostat.ku.dk  Wed Dec 29 10:03:44 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Dec 2004 10:03:44 +0100
Subject: [R] using get() in assign()
In-Reply-To: <8D1E8E95-5949-11D9-84F6-000393DC4A86@mail.utexas.edu>
References: <8D1E8E95-5949-11D9-84F6-000393DC4A86@mail.utexas.edu>
Message-ID: <x2sm5pxyq7.fsf@biostat.ku.dk>

thomas hills <thills at mail.utexas.edu> writes:

> I'm trying to rename the columns in a list of data.frames using the 
> following...
> 
> for(i in 1:length(filenames)) {
> assign(names(get(filenames[i])), c("name", "infood", "time") ) }
> 
> R returns no errors, but the names are unchanged in the data.frames.
> 
> The original names were things like
> 
>  > names(get(filenames[2]))
> [1] "Tc45w4.V1" "Tc45w4.V2" "Tc45w4.V3"
> 
> after the above procedure they are still those names.
> 
> Ideas appreciated.  Thanks.

assign() takes a character string as its first argument, so you
probably now have a variable called "Tc45w4.V1" (it's a semi-bug that
it doesn't barf when passed a character vector of length > 1). What
you seem to want is

for(i in 1:length(filenames)) {
  t <- get(filenames[i])
  names(t) <-  c("name", "infood", "time")
  assign(filenames[i], t)
}

It is easier with a list

for (i in seq(along=listoftables)) 
   names(listoftables[[i]]) <- c("name", "infood", "time")

but *not* "for (i in listoftables) names(i) <- ..." because you'd be
changing the names of a copy of the actual elements.

I suspect the jury is still out on whether it is good programming
practice to do 

listoftables <- lapply(listoftables, "names<-", c("name", "infood", "time"))

but it is tempting in cases like this. (The awkward bit is that for
performance reasons, assignment functions -- names<-, [<-, etc. --
might not leave their argument unchanged.)
 
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From obuhard at yahoo.fr  Wed Dec 29 11:46:18 2004
From: obuhard at yahoo.fr (Olivier BUHARD)
Date: Wed, 29 Dec 2004 11:46:18 +0100 (CET)
Subject: [R] about image(graphics) function
Message-ID: <20041229104618.62923.qmail@web25706.mail.ukl.yahoo.com>

Hi all,

is it possible to modify the way a graph obtained
through image(graphics) is filled, I mean starting
filling the graphical matrix by row from the
upper-left rather than by the lower-left cell... In
many cases, it can be usefull to have a representation
of the data spatialy corresponding to a real support,
as it is the case with the function image(marray) from
Bioconductor packages, which fills the graphical
matrix by row from upper-left, but just handles
marrayRaw or marrayNorm objects. Of course, I could
reorder the matrix, but it's heavier than with an
already available function if it exists...

Thanks a lot for help

BUHARD Olivier



From sdavis2 at mail.nih.gov  Wed Dec 29 11:58:25 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 29 Dec 2004 05:58:25 -0500
Subject: [R] about image(graphics) function
In-Reply-To: <20041229104618.62923.qmail@web25706.mail.ukl.yahoo.com>
References: <20041229104618.62923.qmail@web25706.mail.ukl.yahoo.com>
Message-ID: <8FFDFBB5-5988-11D9-AC2C-000D933565E8@mail.nih.gov>

Buhard,

The simplest way to do this is to make a new function, call it 
my.image, for example, that does the reordering automatically.  Then, 
you can replace calls to image with calls to my.image.

my.image(mat,...) {
   mat2 <- mat[nrow(mat):1,]
   image(mat2,...)
}

I haven't tested the above, but that is the idea.  Note how the '...' 
allows you to pass any other parameters unchanged to image.

Hope this helps.

Sean

On Dec 29, 2004, at 5:46 AM, Olivier BUHARD wrote:

> Hi all,
>
> is it possible to modify the way a graph obtained
> through image(graphics) is filled, I mean starting
> filling the graphical matrix by row from the
> upper-left rather than by the lower-left cell... In
> many cases, it can be usefull to have a representation
> of the data spatialy corresponding to a real support,
> as it is the case with the function image(marray) from
> Bioconductor packages, which fills the graphical
> matrix by row from upper-left, but just handles
> marrayRaw or marrayNorm objects. Of course, I could
> reorder the matrix, but it's heavier than with an
> already available function if it exists...
>
> Thanks a lot for help
>
> BUHARD Olivier
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Wed Dec 29 13:00:05 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 29 Dec 2004 07:00:05 -0500 (EST)
Subject: [R] using get() in assign()
Message-ID: <20041229120005.4BD7112CD6@mprdmxin.myway.com>



Its probably easier to keep your data frames in a list 
and to just specify the col.names= argument on 
read.table when you read them in:

cn <- c("name", "infood", "time")
L <- sapply(filenames, read.table, col.names = cn, simplify = FALSE)

You can add other read.table arguments, if you need them, 
in the same way that col.names= was added above.

Note that we used sapply(...whatever..., simplify = FALSE)
instead of lapply since that sapply will label the components
of list L with the filenames whereas a lapply will not.  


Date:   Tue, 28 Dec 2004 21:27:21 -0600 
From:   thomas hills <thills at mail.utexas.edu>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] using get() in assign() 

 
I'm trying to rename the columns in a list of data.frames using the 
following...

for(i in 1:length(filenames)) {
assign(names(get(filenames[i])), c("name", "infood", "time") ) }

R returns no errors, but the names are unchanged in the data.frames.

The original names were things like

> names(get(filenames[2]))
[1] "Tc45w4.V1" "Tc45w4.V2" "Tc45w4.V3"

after the above procedure they are still those names.

Ideas appreciated. Thanks.

Thomas



From Dax42 at web.de  Wed Dec 29 14:17:35 2004
From: Dax42 at web.de (dax42)
Date: Wed, 29 Dec 2004 14:17:35 +0100
Subject: [R] numeric(0)
Message-ID: <0125F7F0-599C-11D9-83AD-000393883D7E@web.de>

Dear all,

I am trying to calculate a score for a string sequence consisting of 
the following four letters: ACGT.
I have got a matrix giving the scores for each pair of letters.
So for example the string ACCT has got the pairs: AC, CC and CT.

The matrix has got the following form:
names<-c("A","C","G","T");
mscore<-matrix(0,4,4);
rownames(mscore)<-names;
colnames(mscore)<-names;

So for the first example pair above I could get the score contained in 
the matrix with the following code:
 >> mscore["A","C"]

I am now trying to sum up all the scores with the following code:

score<-0;

for(j in 1:length(sequence)-1){
	score<-score+mscore[sequence[j],sequence[j+1]];
}

where sequence is the string sequence.
Unfortunately, it does not work and I get the following error message:

Error in "[<-"(`*tmp*`, 1, i, value = numeric(0)) :
	nothing to replace with

What does this mean? Strangely, the command
"print(score+mscore[sequence[j],sequence[j+1]])"
works, so it really is the assignment which won't work. Why is that?
I am running R 2.0 series (GUI version) on Mac OSX 10.3.7.

Any suggestions are welcome.
Thanks a lot,
Dax



From bxc at steno.dk  Wed Dec 29 14:47:29 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 29 Dec 2004 14:47:29 +0100
Subject: [R] numeric(0)
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE268@exdkba022.novo.dk>

You would probably be better off without the loop, for example:

ns <- length( sequence )
num.seq <- match( sequence, names )
scores <- mscore[cbind(num.seq[-1],num.seq[-ns])]
sum( scores )

I have used the fact that if you index a matrix with
a two-column,matrix ( here, cbind( , ) ), you select the
corresponding elements of the matrix, see ?"["
But it only works if the matrix is numeric.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of dax42
> Sent: Wednesday, December 29, 2004 2:18 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] numeric(0)
> 
> 
> Dear all,
> 
> I am trying to calculate a score for a string sequence consisting of 
> the following four letters: ACGT.
> I have got a matrix giving the scores for each pair of 
> letters. So for example the string ACCT has got the pairs: 
> AC, CC and CT.
> 
> The matrix has got the following form: 
> names<-c("A","C","G","T"); mscore<-matrix(0,4,4); 
> rownames(mscore)<-names; colnames(mscore)<-names;
> 
> So for the first example pair above I could get the score 
> contained in 
> the matrix with the following code:
>  >> mscore["A","C"]
> 
> I am now trying to sum up all the scores with the following code:
> 
> score<-0;
> 
> for(j in 1:length(sequence)-1){
> 	score<-score+mscore[sequence[j],sequence[j+1]];
> }
> 
> where sequence is the string sequence.
> Unfortunately, it does not work and I get the following error message:
> 
> Error in "[<-"(`*tmp*`, 1, i, value = numeric(0)) :
> 	nothing to replace with
> 
> What does this mean? Strangely, the command 
> "print(score+mscore[sequence[j],sequence[j+1]])"
> works, so it really is the assignment which won't work. Why 
> is that? I am running R 2.0 series (GUI version) on Mac OSX 10.3.7.
> 
> Any suggestions are welcome.
> Thanks a lot,
> Dax
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Wed Dec 29 14:45:25 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 29 Dec 2004 13:45:25 -0000 (GMT)
Subject: [R] numeric(0)
In-Reply-To: <0125F7F0-599C-11D9-83AD-000393883D7E@web.de>
Message-ID: <XFMail.041229134525.Ted.Harding@nessie.mcc.ac.uk>

On 29-Dec-04 dax42 wrote:
> I am now trying to sum up all the scores with the following code:
> 
> score<-0;
> 
> for(j in 1:length(sequence)-1){
>       score<-score+mscore[sequence[j],sequence[j+1]];
> }
> 
> where sequence is the string sequence.
> Unfortunately, it does not work and I get the following error message:
> 
> Error in "[<-"(`*tmp*`, 1, i, value = numeric(0)) :
>       nothing to replace with

You've fallen into a classic trap: 1:length(sequence)-1 does not
mean what one might naturally expect.

> sequence<-(1:10)
> length(sequence)
[1] 10
> 1:length(sequence)-1
 [1] 0 1 2 3 4 5 6 7 8 9
> 1:(length(sequence)-1)
[1] 1 2 3 4 5 6 7 8 9

In other words, "1:length(sequence)" is constructed first, then
1 is subtracted from every element. As a result, you tried to
read from sequence[0], which isn't there.

However, you can make it evaluate "length(sequence)-1" before
constructing 1:(length(sequence)-1), by using the parantheses
to force precedence.

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 29-Dec-04                                       Time: 13:45:25
------------------------------ XFMail ------------------------------



From sdavis2 at mail.nih.gov  Wed Dec 29 14:50:30 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 29 Dec 2004 08:50:30 -0500
Subject: [R] numeric(0)
In-Reply-To: <0125F7F0-599C-11D9-83AD-000393883D7E@web.de>
References: <0125F7F0-599C-11D9-83AD-000393883D7E@web.de>
Message-ID: <9A4FD3F8-59A0-11D9-AC2C-000D933565E8@mail.nih.gov>


On Dec 29, 2004, at 8:17 AM, dax42 wrote:

> Dear all,
>
> I am trying to calculate a score for a string sequence consisting of 
> the following four letters: ACGT.
> I have got a matrix giving the scores for each pair of letters.
> So for example the string ACCT has got the pairs: AC, CC and CT.
>
> The matrix has got the following form:
> names<-c("A","C","G","T");
> mscore<-matrix(0,4,4);
> rownames(mscore)<-names;
> colnames(mscore)<-names;
>
> So for the first example pair above I could get the score contained in 
> the matrix with the following code:
> >> mscore["A","C"]
>
> I am now trying to sum up all the scores with the following code:
>
> score<-0;
>
> for(j in 1:length(sequence)-1){
> 	score<-score+mscore[sequence[j],sequence[j+1]];
> }
>

Is sequence a string?  If so, you will probably need to make some 
modifications like these:

 > sequence <- 'ACGT'
 > sequence[1]
[1] "ACGT"
 > substr(sequence,1,1)
[1] "A"
 > substr(sequence,2,1)
[1] ""
 > substr(sequence,1,2)
[1] "AC"
 > substr(sequence,2,2)
[1] "C"
 > substr(sequence,3,3)
[1] "G"
 > mscore <- matrix(runif(16),4,4)
 > names <- c('A','C','G','T')
 > rownames(mscore) <- names
 > colnames(mscore) <- names
 > mscore
           A         C         G          T
A 0.6200289 0.6324337 0.1895207 0.28253473
C 0.5026072 0.6552428 0.7978809 0.43131540
G 0.1669823 0.8808445 0.6021024 0.01563101
T 0.4184646 0.9620714 0.7723088 0.33045464
 > mscore[substr(sequence,1,1),substr(sequence,2,2)]
[1] 0.6324337
 > score <- 0
 > for (j in 1:(nchar(sequence)-1)) {score <- score+ 
mscore[substr(sequence,j,j),substr(sequence,j+1,j+1)]}
 > score
[1] 1.445946

Hope this helps.  I imagine this isn't the most efficient way of 
solving this problem, though.

Sean



From andy_liaw at merck.com  Wed Dec 29 14:54:34 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 29 Dec 2004 08:54:34 -0500
Subject: [R] numeric(0)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E475@usrymx25.merck.com>

`names' is a builtin R function for extracting names of elements in a
vector, or names of components in a list.  If I use

nm <- c("A","C","G","T")
rownames(mscore) <- colnames(mscore) <- nm

I don't get any error.

Andy

> From: dax42
> 
> Dear all,
> 
> I am trying to calculate a score for a string sequence consisting of 
> the following four letters: ACGT.
> I have got a matrix giving the scores for each pair of letters.
> So for example the string ACCT has got the pairs: AC, CC and CT.
> 
> The matrix has got the following form:
> names<-c("A","C","G","T");
> mscore<-matrix(0,4,4);
> rownames(mscore)<-names;
> colnames(mscore)<-names;
> 
> So for the first example pair above I could get the score 
> contained in 
> the matrix with the following code:
>  >> mscore["A","C"]
> 
> I am now trying to sum up all the scores with the following code:
> 
> score<-0;
> 
> for(j in 1:length(sequence)-1){
> 	score<-score+mscore[sequence[j],sequence[j+1]];
> }
> 
> where sequence is the string sequence.
> Unfortunately, it does not work and I get the following error message:
> 
> Error in "[<-"(`*tmp*`, 1, i, value = numeric(0)) :
> 	nothing to replace with
> 
> What does this mean? Strangely, the command
> "print(score+mscore[sequence[j],sequence[j+1]])"
> works, so it really is the assignment which won't work. Why is that?
> I am running R 2.0 series (GUI version) on Mac OSX 10.3.7.
> 
> Any suggestions are welcome.
> Thanks a lot,
> Dax
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dave at kanecap.com  Wed Dec 29 15:06:37 2004
From: dave at kanecap.com (David Kane)
Date: Wed, 29 Dec 2004 09:06:37 -0500
Subject: [R] Developing R classes
In-Reply-To: <20041228164610.85427.qmail@web50406.mail.yahoo.com>
References: <20041228164610.85427.qmail@web50406.mail.yahoo.com>
Message-ID: <16850.47597.746218.332517@gargle.gargle.HOWL>

Gilvan Justino writes:
 > Hi,
 >  
 > I??m trying to write some R classes but I din??t find documentation
 > enought to develop them. 

I have faced a similar problem. Here is the summary of what I have
been able to find (including some of the responses that you have
already received) with a focus on S4 classes since some people assert
that all new development should be done with them.

1) I think that the three best free resources for learning about S4
classes are (in the order that I recommend you read them):

http://www.omegahat.org/RSMethods/Intro.pdf (More or less Section 1.6
from The Green Book.)

http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/S4Objects.pdf (I
think that this version of the talk is more up-to-date than the one
located at the Bioconductor site at
http://www.bioconductor.org/develPage/guidelines/programming/S4Objects.pdf)

@Article{Rnews:Lumley:2004b,
  author       = {Thomas Lumley},
  title	       = {Programmers' Niche: A Simple Class, in {S3} and {S4}},
  journal      = {R News},
  year	       = 2004,
  volume       = 4,
  number       = 1,
  pages	       = {33--36},
  month	       = {June},
  url	       = {http://CRAN.R-project.org/doc/Rnews/},
}


2) If you want to get serious with S4 classes, you may need to
purchase "Programming with Data" by John Chambers (aka The Green Book)
and/or "S Programming" by Venable and Ripley. I own both, but am
interested in finding material that is similarly comprehensive but
free. Many of the people (read: students) that I urge to use R do not
have the sort of budgets for buying books that I am lucky enough to
have.

3) I found all of these references to be worth a read through. 

http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Leisch.pdf
http://www.biostat.jhsph.edu/~rpeng/R-classes-scope.pdf
http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/Methods.pdf
http://www.molgen.mpg.de/~wolski/Robject/Extending.pdf

Hope that helps,

Dave Kane

PS. I can't access the stat.auckland site right now but have been able
to in the past.



From ggrothendieck at myway.com  Wed Dec 29 15:29:34 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 29 Dec 2004 14:29:34 +0000 (UTC)
Subject: [R] numeric(0)
References: <0125F7F0-599C-11D9-83AD-000393883D7E@web.de>
Message-ID: <loom.20041229T152814-870@post.gmane.org>

> 
: Date:   Wed, 29 Dec 2004 14:17:35 +0100 
: From:   dax42 <Dax42 at web.de>
: [ Add to Address Book | Block Address | Report as Spam ] 
: To:   <r-help at stat.math.ethz.ch> 
: Subject:   [R] numeric(0) 
: 
:  
: Dear all,
: 
: I am trying to calculate a score for a string sequence consisting of 
: the following four letters: ACGT.
: I have got a matrix giving the scores for each pair of letters.
: So for example the string ACCT has got the pairs: AC, CC and CT.
: 
: The matrix has got the following form:
: names<-c("A","C","G","T");
: mscore<-matrix(0,4,4);
: rownames(mscore)<-names;
: colnames(mscore)<-names;
: 
: So for the first example pair above I could get the score contained in 
: the matrix with the following code:
: >> mscore["A","C"]
: 
: I am now trying to sum up all the scores with the following code:
: 
: score<-0;
: 
: for(j in 1:length(sequence)-1){
:      score<-score+mscore[sequence[j],sequence[j+1]];
: }

The above code sutracts 1 from the vector 1:length(sequence) giving
a vector that starts at 0.  I think you meant 

	1:(length(sequence)-1)
: 
: where sequence is the string sequence.
: Unfortunately, it does not work and I get the following error message:
: 
: Error in "[<-"(`*tmp*`, 1, i, value = numeric(0)) :
:      nothing to replace with
: 
: What does this mean? Strangely, the command
: "print(score+mscore[sequence[j],sequence[j+1]])"
: works, so it really is the assignment which won't work. Why is that?
: I am running R 2.0 series (GUI version) on Mac OSX 10.3.7.
: 
: Any suggestions are welcome.

Try this.  (The first line is to ensure that levels that don't
appear still get counted.)

f <- factor(sequence, levels = names)
sum(table(f[-length(f)], f[-1]) * mscore)



From HDoran at air.org  Wed Dec 29 16:02:58 2004
From: HDoran at air.org (Doran, Harold)
Date: Wed, 29 Dec 2004 10:02:58 -0500
Subject: [R] gls model and matrix operations
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044E31@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041229/0165d37c/attachment.pl

From Dax42 at web.de  Wed Dec 29 16:32:36 2004
From: Dax42 at web.de (dax42)
Date: Wed, 29 Dec 2004 16:32:36 +0100
Subject: [R] numeric(0)
In-Reply-To: <XFMail.041229134525.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041229134525.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <DDF8AAF2-59AE-11D9-83AD-000393883D7E@web.de>

Hi,

it's true, simply adding the parenthesis does the trick.
Anyways, I am trying to do it without the loop now.

Thanks for helping out!
Cheers, Dax

Am 29.12.2004 um 14:45 schrieb (Ted Harding):

> On 29-Dec-04 dax42 wrote:
>> I am now trying to sum up all the scores with the following code:
>>
>> score<-0;
>>
>> for(j in 1:length(sequence)-1){
>>       score<-score+mscore[sequence[j],sequence[j+1]];
>> }
>>
>> where sequence is the string sequence.
>> Unfortunately, it does not work and I get the following error message:
>>
>> Error in "[<-"(`*tmp*`, 1, i, value = numeric(0)) :
>>       nothing to replace with
>
> You've fallen into a classic trap: 1:length(sequence)-1 does not
> mean what one might naturally expect.
>
>> sequence<-(1:10)
>> length(sequence)
> [1] 10
>> 1:length(sequence)-1
>  [1] 0 1 2 3 4 5 6 7 8 9
>> 1:(length(sequence)-1)
> [1] 1 2 3 4 5 6 7 8 9
>
> In other words, "1:length(sequence)" is constructed first, then
> 1 is subtracted from every element. As a result, you tried to
> read from sequence[0], which isn't there.
>
> However, you can make it evaluate "length(sequence)-1" before
> constructing 1:(length(sequence)-1), by using the parantheses
> to force precedence.
>
> Cheers,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 29-Dec-04                                       Time: 13:45:25
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Wed Dec 29 17:34:56 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 29 Dec 2004 16:34:56 -0000 (GMT)
Subject: [R] numeric(0)
In-Reply-To: <XFMail.041229134525.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.041229163456.Ted.Harding@nessie.mcc.ac.uk>

On 29-Dec-04 Ted Harding wrote:
> You've fallen into a classic trap: 1:length(sequence)-1 does not
> mean what one might naturally expect.
> 
>> sequence<-(1:10)
>> length(sequence)
> [1] 10
>> 1:length(sequence)-1
>  [1] 0 1 2 3 4 5 6 7 8 9
>> 1:(length(sequence)-1)
> [1] 1 2 3 4 5 6 7 8 9
> 
> In other words, "1:length(sequence)" is constructed first, then
> 1 is subtracted from every element. As a result, you tried to
> read from sequence[0], which isn't there.
> 
> However, you can make it evaluate "length(sequence)-1" before
> constructing 1:(length(sequence)-1), by using the parantheses
> to force precedence.

This got me wondering. Hence:

  > 1:2-4
  [1] -3 -2

as above. Similarly of course 1:2+4.

Likewise

  > 1:2*4
  [1] 4 8

and similarly of course

  > 1:2/4
  [1] 0.25 0.50

However:

  > 1:2^4
  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16

and not

  [1]  1 16

which has to be forced by "(1:2)^4".

So the precedence rules seem to be:

A:  Unary -
B:  ^
C:  :
D:  * /
E:  + -

The position of ":" in this surprises me, intuitively.
I can see reasons why it would be natural for ":" to come
below "Unary -" -- this makes sense of the natural "-2:2",
for instance, so that this gives "-2 -1 0 1 2" and not "-2".

However, I don't see why it was chosen to make "1:2^4" mean
"1:(2^4)" and not "(1:2)^4". My intuitive expectation is that
this should follow the pattern of 1:2*4 or 1:2+4. All three
are, after all, simply binary arithmetic operators. So I'd
have expected to find ":" at position "B", above "^" at "C",
and not between "^" and "*".

What's the best place to look for the details on operator
precedence and the like in R?

[And, just to pre-empt the suggestion: I'm very much in the
 habit of using parentheses to force precedence in all cases
 where I'm not absolutely sure of the outcome ... ]

Thanks, and best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 29-Dec-04                                       Time: 16:34:56
------------------------------ XFMail ------------------------------



From ggrothendieck at myway.com  Wed Dec 29 18:32:12 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 29 Dec 2004 12:32:12 -0500 (EST)
Subject: [R] numeric(0)
Message-ID: <20041229173212.1B4D5396F@mprdmxin.myway.com>


From:   <Ted.Harding at nessie.mcc.ac.uk>

> What's the best place to look for the details on operator
> precedence and the like in R?

Check out the language reference manual in the directory
   .../R/rw2001/doc/manual/r-lang.pdf .

On Windows its also accessible from R via 
   Help | Manuals | R Language

The manuals can also be found online.  e.g. Google for 
   r-lang "infix and prefix"
to find the relevant section.

By the way, even without this problem seq is better than : in 
programs as it has better boundary behavior.  Consider 1:n vs.
seq(length = n).  For n=0 the : operator actually generates the
vector c(1,0) whereas the seq solution gives a zero length
vector, as desired.  Even seq has problems unless you use
length= .

The : operator is mainly useful if you are typing throwaway 
code directly into R.



From david.whiting at ncl.ac.uk  Wed Dec 29 18:36:47 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 29 Dec 2004 17:36:47 +0000
Subject: [R] numeric(0)
In-Reply-To: <XFMail.041229163456.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041229163456.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <m2oegd9fbk.fsf@192.168.57.36>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:


[...]
> 
> What's the best place to look for the details on operator
> precedence and the like in R?

?Syntax
?Arithmetic

-- 
David Whiting
University of Newcastle upon Tyne, UK



From p.dalgaard at biostat.ku.dk  Wed Dec 29 18:46:33 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Dec 2004 18:46:33 +0100
Subject: [R] numeric(0)
In-Reply-To: <XFMail.041229163456.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041229163456.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2oegdxaiu.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> A:  Unary -
> B:  ^
> C:  :
> D:  * /
> E:  + -
> 
> The position of ":" in this surprises me, intuitively.
> I can see reasons why it would be natural for ":" to come
> below "Unary -" -- this makes sense of the natural "-2:2",
> for instance, so that this gives "-2 -1 0 1 2" and not "-2".
> 
> However, I don't see why it was chosen to make "1:2^4" mean
> "1:(2^4)" and not "(1:2)^4". My intuitive expectation is that
> this should follow the pattern of 1:2*4 or 1:2+4. All three
> are, after all, simply binary arithmetic operators. So I'd
> have expected to find ":" at position "B", above "^" at "C",
> and not between "^" and "*".

Well, ^ traditionally has higher precedence than unary - (-2^4 is -16)
and as you just argued, - has higher precedence than :, so by
transitivity...

Apart from that, you may want to consider model formulas like a:(b+c+d)^2
 
> What's the best place to look for the details on operator
> precedence and the like in R?

?Syntax has it, and ?Arithmetic points you there. The true authority
is gram.y in the sources (in the sense that it defines what happens,
not just says what ought to happen).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Wed Dec 29 18:54:36 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Dec 2004 18:54:36 +0100
Subject: [R] numeric(0)
In-Reply-To: <XFMail.041229134525.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041229134525.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2k6r1xa5f.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 29-Dec-04 dax42 wrote:
> > for(j in 1:length(sequence)-1){
...

> You've fallen into a classic trap: 1:length(sequence)-1 does not
> mean what one might naturally expect.

It might be a good time to warn that 1:(length(sequence)-1) does not
always mean what you might naturally expect either.
seq(length=length(sequence)-1) is safer, and you may in addition want
to safeguard against the case length(sequence)==0.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From hb at maths.lth.se  Wed Dec 29 19:22:01 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 29 Dec 2004 19:22:01 +0100
Subject: [R] about image(graphics) function
In-Reply-To: <8FFDFBB5-5988-11D9-AC2C-000D933565E8@mail.nih.gov>
Message-ID: <007901c4edd3$4b8a1740$270040d5@hblaptop>

See "Code example explaining image()" on
http://www.maths.lth.se/help/R/image/.

Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
> Sent: Wednesday, December 29, 2004 11:58 AM
> To: r-help
> Subject: Re: [R] about image(graphics) function
> 
> 
> Buhard,
> 
> The simplest way to do this is to make a new function, call it 
> my.image, for example, that does the reordering automatically.  Then, 
> you can replace calls to image with calls to my.image.
> 
> my.image(mat,...) {
>    mat2 <- mat[nrow(mat):1,]
>    image(mat2,...)
> }
> 
> I haven't tested the above, but that is the idea.  Note how the '...' 
> allows you to pass any other parameters unchanged to image.
> 
> Hope this helps.
> 
> Sean
> 
> On Dec 29, 2004, at 5:46 AM, Olivier BUHARD wrote:
> 
> > Hi all,
> >
> > is it possible to modify the way a graph obtained
> > through image(graphics) is filled, I mean starting
> > filling the graphical matrix by row from the
> > upper-left rather than by the lower-left cell... In
> > many cases, it can be usefull to have a representation
> > of the data spatialy corresponding to a real support,
> > as it is the case with the function image(marray) from Bioconductor 
> > packages, which fills the graphical matrix by row from 
> upper-left, but 
> > just handles marrayRaw or marrayNorm objects. Of course, I could
> > reorder the matrix, but it's heavier than with an
> > already available function if it exists...
> >
> > Thanks a lot for help
> >
> > BUHARD Olivier
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From das at cshl.edu  Wed Dec 29 19:32:56 2004
From: das at cshl.edu (Rajdeep Das)
Date: Wed, 29 Dec 2004 13:32:56 -0500
Subject: [R] predict.glm
Message-ID: <001e01c4edd4$d0a4afc0$6500a8c0@artney>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041229/bc1804fa/attachment.pl

From rhodesju at ohsu.edu  Wed Dec 29 21:49:56 2004
From: rhodesju at ohsu.edu (Justin Rhodes)
Date: Wed, 29 Dec 2004 12:49:56 -0800
Subject: [R] Question about setting tick mark limits in Plot
Message-ID: <s1d2a804.037@ohsu.edu>

Dear R-list,

I am having trouble getting the Plot function to set the tick marks the
way I want, and I am not sure what I am doing wrong.  

Below is the code that I wrote, and I am not sure why it doesn't work. 
p is a vector of length 2198, containing numbers between 0 and 240.  q
is a vector of ones, the same length as p.  I want the x-axis to have
tick marks every 30 min and I want the last tick mark to be at 240 min. 
When I run the program it only plots 5 tick marks starting at 0 and
ending at 200,  though the data extend to 240 min.  I am very confused. 
Thanks for any help.  I really appreciate it.  I have wasted too much
time trying to figure this out on my own.



par(xaxp=c(0,240,8))
par(mar=c(13,1,13,1))

plot(p,q, pch="|", yaxt="n", main=subject,xlab="Minutes", ylab="")



Take care,



Justin Rhodes
Behavioral Neuroscience
Oregon Health & Science University
VA Medical Center (R & D 12)
3710 SW US Veterans Hospital Rd
Portland, OR  97239
Phone: (503) 220-8262 extn 54392
Fax: (503) 721-1029
E-mail: rhodesju at ohsu.edu



From merser at image.dk  Wed Dec 29 22:11:10 2004
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Wed, 29 Dec 2004 22:11:10 +0100
Subject: [R] setting tabstop
Message-ID: <000301c4edea$efb3ba80$8b00a8c0@IBM>

hi

is it possible to change tabstop (default 8) in the GUI ?

regards soren

i'm using
    winxp sp2
    R ver. 2.0.1



From jfox at mcmaster.ca  Wed Dec 29 22:42:22 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 29 Dec 2004 16:42:22 -0500
Subject: [R] predict.glm
In-Reply-To: <001e01c4edd4$d0a4afc0$6500a8c0@artney>
Message-ID: <20041229214222.PUSU1899.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Raj,

I'm not sure that I'm interpreting your question properly, but I'll give it
a shot: When you use a two-level factor as the response variable in a
binomial GLM, the first level is taken to represent "failure" and the second
"success." See ?glm for details. The default order of levels is
alphabetical, so unless you did something to order the levels differently,
"Male" would correspond to success and "Female" to failure. Thus the fitted
probability would be the probability of "Male."

Is that what you wanted to know?

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rajdeep Das
> Sent: Wednesday, December 29, 2004 1:33 PM
> To: R Help Mailing List
> Subject: [R] predict.glm
> 
> Hi All,
> 
> Sorry for this is a very naive question. 
> 
> I am trying to do binary classification (male vs female)  
> using glm using following data:
> 
> X1    X2    X3    Class
> 2.3    4.5    2.1    Male
> 0.9    3.2   1.6     Male
> 1.7    1.8    2.6    Feamle
> 
> 
> 
> I am trying to use predict.glm for prediction with 
> type="respose" which gives the predicted probabilities as per 
> documentation. 
> 
> My question is: which of the two classes does this 
> probability corresponds to? My understanding is that it is 
> the probability of the class that each of the new data has. 
> Is that correct?
> 
> Thanks.
> 
> Raj
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed Dec 29 22:54:23 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 29 Dec 2004 13:54:23 -0800
Subject: [R] Question about setting tick mark limits in Plot
In-Reply-To: <s1d2a804.037@ohsu.edu>
References: <s1d2a804.037@ohsu.edu>
Message-ID: <41D3278F.1030103@pdf.com>

How about the following: 

p <- seq(0, 240, len=2198)
q. <- rep(1, len=2198)

plot(p,q., pch="|", yaxt="n", xaxt="n",
     xlab="Minutes", ylab="")
axis(1, seq(0, 240, 30))

      Note:  "q" is a system function, so I changed it to "q.". 

      hope this helps.  spencer graves

Justin Rhodes wrote:

>Dear R-list,
>
>I am having trouble getting the Plot function to set the tick marks the
>way I want, and I am not sure what I am doing wrong.  
>
>Below is the code that I wrote, and I am not sure why it doesn't work. 
>p is a vector of length 2198, containing numbers between 0 and 240.  q
>is a vector of ones, the same length as p.  I want the x-axis to have
>tick marks every 30 min and I want the last tick mark to be at 240 min. 
>When I run the program it only plots 5 tick marks starting at 0 and
>ending at 200,  though the data extend to 240 min.  I am very confused. 
>Thanks for any help.  I really appreciate it.  I have wasted too much
>time trying to figure this out on my own.
>
>
>
>par(xaxp=c(0,240,8))
>par(mar=c(13,1,13,1))
>
>plot(p,q, pch="|", yaxt="n", main=subject,xlab="Minutes", ylab="")
>
>
>
>Take care,
>
>
>
>Justin Rhodes
>Behavioral Neuroscience
>Oregon Health & Science University
>VA Medical Center (R & D 12)
>3710 SW US Veterans Hospital Rd
>Portland, OR  97239
>Phone: (503) 220-8262 extn 54392
>Fax: (503) 721-1029
>E-mail: rhodesju at ohsu.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From datkins at u.washington.edu  Wed Dec 29 22:55:55 2004
From: datkins at u.washington.edu (datkins@u.washington.edu)
Date: Wed, 29 Dec 2004 13:55:55 -0800 (PST)
Subject: [R] Discrepancy between intervals.lme and coef.lme
Message-ID: <Pine.LNX.4.43.0412291355550.24783@hymn03.u.washington.edu>


I'm using R on Windows v2.0.1 with the nlme package (v3.1-53) and am finding some unexpected discrepancies in the output of intervals.lme and coef.lme.  I've included a toy dataset at the end, but briefly, the data are longitudinal data from couples in marital therapy.  Each spouse's relationship satisfaction is measured 4 times; I've fit both linear and quadratic models to the change over time.  The quadratic fits show the discrpancies.

Here's the call to lmList, coef, and intervals:

tmp.lis1 <- lmList(dv ~ time + I(time^2)| id/sex, data = tmp.df,
                      na.action = na.omit)
coef(tmp.lis1)
intervals(tmp.lis1)

Here is the coef() output:
           (Intercept)       time  I(time^2)
1/Husband       89.60  11.100000  -2.500000
1/Wife          69.80   5.300000  -0.500000
2/Husband       49.00   8.833333  -2.833333
2/Wife          45.00  28.666667  -9.666667
3/Husband       96.00  -6.000000         NA
3/Wife          60.00  19.000000         NA
4/Husband       70.00  48.500000 -16.500000
4/Wife          92.00  14.500000  -4.500000
5/Husband       75.00  43.500000 -14.500000
5/Wife          87.00  37.000000 -14.000000
6/Husband       66.75   1.250000  -1.250000
6/Wife          66.15  12.150000  -2.750000
7/Husband       92.75   6.750000  -0.750000
7/Wife          82.35  17.850000  -3.250000
8/Husband       76.15 -25.350000  11.750000
8/Wife         100.50 -12.000000   6.000000

And just the (Intercept) portion of the intervals() output:
, , (Intercept)

                lower       est.       upper
1/Husband 72.4335719 89.6000000 106.7664281
1/Wife    52.6335719 69.8000000  86.9664281
2/Husband        NaN 49.0000000         NaN
2/Wife           NaN 45.0000000         NaN
3/Husband        NaN 96.0000000         NaN
3/Wife           NaN        NaN         NaN
4/Husband        NaN        NaN         NaN
4/Wife           NaN        NaN         NaN
5/Husband        NaN        NaN         NaN
5/Wife           NaN        NaN         NaN
6/Husband -3.8551591 -0.7453560   2.3644471
6/Wife    -3.2306740 -1.4468675   0.3369390
7/Husband -2.1707917 -0.1916630   1.7874658
7/Wife    -2.4667397 -1.0766253   0.3134891
8/Husband  4.0996388  4.5693563   5.0390738
8/Wife     0.9368527  1.7888544   2.6408560

Notice that the intercept estimates for couples 6-8 are wildly different between the coef() and intervals() output.  Granted, fitting an intercept, slope, and quadratic to 4 data points doesn't leave much for an error term, but it seems like the intercept coefficients ought to be the same.  If the quadratic is dropped, there is no discrepancy between coef() and intevals(), so perhaps this is related to the complexity of the model vs. sparseness of data?

Any insights appreciated (data below).

Dave
-- 
Dave Atkins, PhD
datkins at u.washington.edu

tmp.df <- data.frame(id = as.factor(rep(1:8, each=8)),
                      sex = factor(rep(0:1, each=4, length.out=64), 0:1,
                                   c("Husband","Wife")),
                      time = rep(0:3, length.out=64),
                      dv = c(92, 91, 109, 98, 70, 74, 79, 81, 49, 55, NA,
                         50, 45, 64, NA, 44, NA, 90, 84, NA, NA, 79, 98, NA,
                         70, 102, 101, NA, 92, 102, 103, NA, 75, 104, 104,
                         NA, 87, 110, 105, NA, 66, 69, 62, 60, 67, 73, 82,
                         77, 91, 104, 98, 108, 81, 101, 101, 108, 75, 66,
                         69, 107, 102, 90, 105, 117))



From p.dalgaard at biostat.ku.dk  Wed Dec 29 23:40:48 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Dec 2004 23:40:48 +0100
Subject: [R] Discrepancy between intervals.lme and coef.lme
In-Reply-To: <Pine.LNX.4.43.0412291355550.24783@hymn03.u.washington.edu>
References: <Pine.LNX.4.43.0412291355550.24783@hymn03.u.washington.edu>
Message-ID: <x27jn0ybgv.fsf@biostat.ku.dk>

datkins at u.washington.edu writes:

> I'm using R on Windows v2.0.1 with the nlme package (v3.1-53) and am finding some unexpected discrepancies in the output of intervals.lme and coef.lme.  I've included a toy dataset at the end, but briefly, the data are longitudinal data from couples in marital therapy.  Each spouse's relationship satisfaction is measured 4 times; I've fit both linear and quadratic models to the change over time.  The quadratic fits show the discrpancies.
> 
> Here's the call to lmList, coef, and intervals:
> 
> tmp.lis1 <- lmList(dv ~ time + I(time^2)| id/sex, data = tmp.df,
>                       na.action = na.omit)
> coef(tmp.lis1)
> intervals(tmp.lis1)
> 
> Here is the coef() output:
>            (Intercept)       time  I(time^2)
> 1/Husband       89.60  11.100000  -2.500000
> 1/Wife          69.80   5.300000  -0.500000
> 2/Husband       49.00   8.833333  -2.833333
> 2/Wife          45.00  28.666667  -9.666667
> 3/Husband       96.00  -6.000000         NA
> 3/Wife          60.00  19.000000         NA
> 4/Husband       70.00  48.500000 -16.500000
> 4/Wife          92.00  14.500000  -4.500000
> 5/Husband       75.00  43.500000 -14.500000
> 5/Wife          87.00  37.000000 -14.000000
> 6/Husband       66.75   1.250000  -1.250000
> 6/Wife          66.15  12.150000  -2.750000
> 7/Husband       92.75   6.750000  -0.750000
> 7/Wife          82.35  17.850000  -3.250000
> 8/Husband       76.15 -25.350000  11.750000
> 8/Wife         100.50 -12.000000   6.000000
> 
> And just the (Intercept) portion of the intervals() output:
> , , (Intercept)
> 
>                 lower       est.       upper
> 1/Husband 72.4335719 89.6000000 106.7664281
> 1/Wife    52.6335719 69.8000000  86.9664281
> 2/Husband        NaN 49.0000000         NaN
> 2/Wife           NaN 45.0000000         NaN
> 3/Husband        NaN 96.0000000         NaN
> 3/Wife           NaN        NaN         NaN
> 4/Husband        NaN        NaN         NaN
> 4/Wife           NaN        NaN         NaN
> 5/Husband        NaN        NaN         NaN
> 5/Wife           NaN        NaN         NaN
> 6/Husband -3.8551591 -0.7453560   2.3644471
> 6/Wife    -3.2306740 -1.4468675   0.3369390
> 7/Husband -2.1707917 -0.1916630   1.7874658
> 7/Wife    -2.4667397 -1.0766253   0.3134891
> 8/Husband  4.0996388  4.5693563   5.0390738
> 8/Wife     0.9368527  1.7888544   2.6408560
> 
> Notice that the intercept estimates for couples 6-8 are wildly different between the coef() and intervals() output.  Granted, fitting an intercept, slope, and quadratic to 4 data points doesn't leave much for an error term, but it seems like the intercept coefficients ought to be the same.  If the quadratic is dropped, there is no discrepancy between coef() and intevals(), so perhaps this is related to the complexity of the model vs. sparseness of data?
> 
> Any insights appreciated (data below).


Looks like the sort of things that happens if parts of code don't take
notice of pivoting caused by singularities in the model. BTW: This
wouldn't look like an issue with the lme methods, but with lmList
counterparts. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jo_brandt at web.de  Wed Dec 29 23:51:54 2004
From: jo_brandt at web.de (Johanna BRANDT)
Date: Wed, 29 Dec 2004 23:51:54 +0100
Subject: [R] GEE with own link function
Message-ID: <41D3350A.5060803@web.de>

Hello,

I want to fit a GEE with a user-defined link function.

For the user-defined link-function I still read 
http://finzi.psych.upenn.edu/R/Rhelp01/archive/6555.html and 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/25727.html.

Only for testing purposes I added a new link function 
(corlogit) in make.link (as well as in binomial) with 
exactly the same code as logit before using my code.

I tried it with glm() and it works without any problem (I 
get the same results using 'binomial(link="logit")' or 
'binomial(link="corlogit")').


But with library(geepack) (Version: 0.2-10) I get different 
results for logit and test.logit as link functions. First 
there was an error, so that I had to modify 'geese.fit' (I 
only added test.logit where I found logit).

With library(gee) (Version: 4.13-10) R exits without error 
when fitting a GEE with binomial(link="corlogit"). Because 
of the error 'unkown link' I modified 'gee' the same way as 
for geepack().


I never changed the original application files, but I wrote 
"own programmes" (using the original code and just adding 
corlogit in the list of the link functions). Are there any 
other functions where I had to add or modify something? What 
else can I do?

Thank you for your help!
Johanna Brandt


(I'm using R 2.0.1 under Windows 2000)


## Example for geese() from the R-Help #####################


I took the example from the help:
 > data(ohio)
 > summary(geese(resp ~ age + smoke + age:smoke, id=id, 
data=ohio,
+              family=binomial(link="logit"), corstr="exch", 
scale.fix=TRUE))

Call:
geese(formula = resp ~ age + smoke + age:smoke, id = id, 
data = ohio,
     family = binomial(link = "logit"), scale.fix = TRUE, 
corstr = "exch")

Mean Model:
  Mean Link:                 logit
  Variance to Mean Relation: binomial

  Coefficients:
                estimate     san.se        wald          p
(Intercept) -1.90049529 0.11908698 254.6859841 0.00000000
age         -0.14123592 0.05820089   5.8888576 0.01523698
smoke        0.31382583 0.18575838   2.8541747 0.09113700
age:smoke    0.07083184 0.08852946   0.6401495 0.42365667

Scale is fixed.

Correlation Model:
  Correlation Structure:     exch
  Correlation Link:          identity

  Estimated Correlation Parameters:
       estimate     san.se     wald p
alpha 0.354531 0.03582698 97.92378 0

Returned Error Value:    0
Number of clusters:   537   Maximum cluster size: 4

 >
 > ## Korrigiert
 > summary(fit.korr <- geese(resp ~ age + smoke + age:smoke, 
id=id, data=ohio,
+              family=binomial(link="corlogit"), 
corstr="exch", scale.fix=TRUE))

Call:
geese(formula = resp ~ age + smoke + age:smoke, id = id, 
data = ohio,
     family = binomial(link = "corlogit"), scale.fix = TRUE, 
corstr = "exch")

Mean Model:
  Mean Link:                 corlogit
  Variance to Mean Relation: binomial

  Coefficients:
                estimate     san.se        wald          p
(Intercept) -1.12581067 0.06344341 314.8891093 0.00000000
age         -0.07680433 0.03128947   6.0252497 0.01410264
smoke        0.17083868 0.10162807   2.8258236 0.09275930
age:smoke    0.03672858 0.04872412   0.5682249 0.45096515

Scale is fixed.

Correlation Model:
  Correlation Structure:     exch
  Correlation Link:          identity

  Estimated Correlation Parameters:
        estimate     san.se    wald p
alpha 0.3545883 0.03583136 97.9315 0

Returned Error Value:    0
Number of clusters:   537   Maximum cluster size: 4



## Example for gee() from the R-Help #######################

if(require(MASS)) {
data(OME)
## not fully appropriate link for these data.
(fm.korr <- gee(cbind(Correct, Trials-Correct) ~ Loud + Age 
+ OME, id = ID,
            data = OME, family = binomial(link="corlogit"), 
corstr = "exchangeable"))
summary(fm.korr)
}



From dalmiral at umich.edu  Wed Dec 29 23:54:33 2004
From: dalmiral at umich.edu (Daniel Almirall)
Date: Wed, 29 Dec 2004 17:54:33 -0500 (EST)
Subject: [R] Updating a formula w a portion of another formula
Message-ID: <Pine.SOL.4.58.0412291706310.22330@asteroids.gpcc.itd.umich.edu>


R-list,

Suppose I have

 oldfmla <- y ~ x

I would like to update it to  y ~ x + z  which I know I can get using

 newfmla <- update(oldfmla, ~ . + z)

However, what if I have

fmlatmp <- ~ z

Can I combine oldfmla and fmlatmp to get  y ~ x + z  some how?

Clearly,

 newfmla <- update(oldfmla, ~ . + fmlatmp)

will not work.

Thanks in advance,
Danny



From ripley at stats.ox.ac.uk  Thu Dec 30 00:34:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 29 Dec 2004 23:34:59 +0000 (GMT)
Subject: [R] setting tabstop
In-Reply-To: <000301c4edea$efb3ba80$8b00a8c0@IBM>
References: <000301c4edea$efb3ba80$8b00a8c0@IBM>
Message-ID: <Pine.LNX.4.61.0412292326330.17656@gannet.stats>

On Wed, 29 Dec 2004, [iso-8859-1] S?ren Merser wrote:

> is it possible to change tabstop (default 8) in the GUI ?
>
> regards soren
>
> i'm using
>   winxp sp2
>   R ver. 2.0.1

Yes, it is possible.  Why would you want to do so?

Since you are asking, you presumably haven't explored the source code.
Hint: what do you think TABSIZE might do?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From merser at image.dk  Thu Dec 30 00:52:07 2004
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Thu, 30 Dec 2004 00:52:07 +0100
Subject: [R] setting tabstop
References: <000301c4edea$efb3ba80$8b00a8c0@IBM>
	<Pine.LNX.4.61.0412292326330.17656@gannet.stats>
Message-ID: <004901c4ee01$6cbdc2d0$8b00a8c0@IBM>

you are quite right, i didn't  read the source - only sought through the 
documentation - since i would like to change tabsize from within a function
i'm printing a table aligning entries with tabs
the output is messed up if the maximum number of characters ina matrix of 
strings is to big, i.e. >8
it might not be the wrong trail, so perhaps i have to use formatC somehow
regards soren

----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "S??ren Merser" <merser at image.dk>
Cc: "R - help" <r-help at stat.math.ethz.ch>
Sent: Thursday, December 30, 2004 12:34 AM
Subject: Re: [R] setting tabstop


On Wed, 29 Dec 2004, [iso-8859-1] S??ren Merser wrote:

> is it possible to change tabstop (default 8) in the GUI ?
>
> regards soren
>
> i'm using
>   winxp sp2
>   R ver. 2.0.1

Yes, it is possible.  Why would you want to do so?

Since you are asking, you presumably haven't explored the source code.
Hint: what do you think TABSIZE might do?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gcoppola at ucla.edu  Thu Dec 30 02:40:54 2004
From: gcoppola at ucla.edu (Giovanni Coppola)
Date: Wed, 29 Dec 2004 17:40:54 -0800
Subject: [R] t-test pvalue
Message-ID: <6.1.2.0.2.20041229171555.04796a80@mail.ucla.edu>

Hi all,
I have some t-test values, and I am trying to obtain the associated p-values.
Is 'pt' the right command? I wonder why 1) it returns different values for 
x and -x, and 2) how to obtain a 2-sided p-value.

example [R version 2.0.1, WinXP]:
#if t=2.23 (df=10), the expected p-value is 0.05 for 2-sided and 0.025 for 
1-sided t-test

pt (2.23,10)
[1] 0.9750788

pt (-2.23,10) #or pt (2.23,10,lower.tail=FALSE)
[1] 0.02492124 #as expected

The opposite happens starting from negative t-test values.

Should I convert in negative values my t-test values, and leave 
lower.tail=TRUE?
Thanks and happy new year
Giovanni



From br44114 at yahoo.com  Thu Dec 30 02:44:04 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 29 Dec 2004 17:44:04 -0800 (PST)
Subject: [R] coplot with png: disappearing grid lines
Message-ID: <20041230014405.19937.qmail@web50302.mail.yahoo.com>

Dear useRs,

When I use coplot() and output to png/jpeg/bmp, the grid lines from
the scatter plots disappear. If I output to pdf() the grid lines are
there, however I can't use it - I have many points, and the resulting
PDF file is large and very slow to open and scroll through. (By the
way, if I click File-Save As-png/jpeg/bmp from Rgui.exe, the grid
lines are preserved - but I need to use code.)

With coplot(), is there a way to:
1. Keep the grid lines in the scatter plots when exporting output to
png(), and perhaps change their color?
2. Specify the number of grid lines to be drawn on the x and y axes?

I'm running R 2.0.0 on Win XP.

Thank you,
b.

a <- rnorm(50000)
b <- rnorm(50000)
c <- rnorm(50000)
#pdf("test.pdf",height=9,width=12)
png("test.png",height=900,width=1200)
coplot(a ~ b | c,pch=20,col="navy",
	bar.bg=c(num=gray(0.8),fac=grey(0.95)))
dev.off()



From Charles.Annis at StatisticalEngineering.com  Thu Dec 30 02:53:08 2004
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 29 Dec 2004 20:53:08 -0500
Subject: [R] t-test pvalue
In-Reply-To: <6.1.2.0.2.20041229171555.04796a80@mail.ucla.edu>
Message-ID: <200412300153.iBU1rBHU018465@hypatia.math.ethz.ch>

pt(q, ...) returns the area to the left of q.  The area to the left of 2.23
for your situation is 0.975, while the area to the left of -2.23 (which is
on the left side of zero from 2.23) is 0.025.





Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Giovanni Coppola
Sent: Wednesday, December 29, 2004 8:41 PM
To: r-help at stat.math.ethz.ch
Subject: [R] t-test pvalue

Hi all,
I have some t-test values, and I am trying to obtain the associated
p-values.
Is 'pt' the right command? I wonder why 1) it returns different values for 
x and -x, and 2) how to obtain a 2-sided p-value.

example [R version 2.0.1, WinXP]:
#if t=2.23 (df=10), the expected p-value is 0.05 for 2-sided and 0.025 for 
1-sided t-test

pt (2.23,10)
[1] 0.9750788

pt (-2.23,10) #or pt (2.23,10,lower.tail=FALSE)
[1] 0.02492124 #as expected

The opposite happens starting from negative t-test values.

Should I convert in negative values my t-test values, and leave 
lower.tail=TRUE?
Thanks and happy new year
Giovanni

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Dec 30 03:34:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 30 Dec 2004 02:34:58 +0000 (UTC)
Subject: [R] Updating a formula w a portion of another formula
References: <Pine.SOL.4.58.0412291706310.22330@asteroids.gpcc.itd.umich.edu>
Message-ID: <loom.20041230T032645-688@post.gmane.org>


Daniel Almirall <dalmiral <at> umich.edu> writes:

: 
: R-list,
: 
: Suppose I have
: 
:  oldfmla <- y ~ x
: 
: I would like to update it to  y ~ x + z  which I know I can get using
: 
:  newfmla <- update(oldfmla, ~ . + z)
: 
: However, what if I have
: 
: fmlatmp <- ~ z
: 
: Can I combine oldfmla and fmlatmp to get  y ~ x + z  some how?
: 
: Clearly,
: 
:  newfmla <- update(oldfmla, ~ . + fmlatmp)
: 
: will not work.
: 
: Thanks in advance,
: Danny


Assuming we have:

	old <- y ~ x
	tmp <- ~ z

Then type in this:

	template <- . ~ . + X
	template[[3]][[3]] <- tmp[[2]]
	update(old, template)

In the above we used the fact that formulas are
represented internally as trees whose parts
can be extracted using indexing.  After
running the above, try entering the following to
get a better idea of how formulas are represented:

	as.list(old)
	as.list(tmp)
	as.list(template)
	as.list(template[[3]])



From dataanalytics at rediffmail.com  Thu Dec 30 04:19:47 2004
From: dataanalytics at rediffmail.com (Arin Basu)
Date: 30 Dec 2004 03:19:47 -0000
Subject: [R] R and PHP
Message-ID: <20041230031947.1913.qmail@webmail28.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041230/b9ae9ce3/attachment.pl

From ggrothendieck at myway.com  Thu Dec 30 05:20:11 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 30 Dec 2004 04:20:11 +0000 (UTC)
Subject: [R] Updating a formula w a portion of another formula
References: <Pine.SOL.4.58.0412291706310.22330@asteroids.gpcc.itd.umich.edu>
	<loom.20041230T032645-688@post.gmane.org>
Message-ID: <loom.20041230T051200-752@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Daniel Almirall <dalmiral <at> umich.edu> writes:
: 
: : 
: : R-list,
: : 
: : Suppose I have
: : 
: :  oldfmla <- y ~ x
: : 
: : I would like to update it to  y ~ x + z  which I know I can get using
: : 
: :  newfmla <- update(oldfmla, ~ . + z)
: : 
: : However, what if I have
: : 
: : fmlatmp <- ~ z
: : 
: : Can I combine oldfmla and fmlatmp to get  y ~ x + z  some how?
: : 
: : Clearly,
: : 
: :  newfmla <- update(oldfmla, ~ . + fmlatmp)
: : 
: : will not work.
: : 
: : Thanks in advance,
: : Danny
: 
: Assuming we have:
: 
: 	old <- y ~ x
: 	tmp <- ~ z
: 
: Then type in this:
: 
: 	template <- . ~ . + X
: 	template[[3]][[3]] <- tmp[[2]]
: 	update(old, template)
: 

One can alternately use substitute avoiding determination of
the precise indices of X.  Note that the eval is required
since substitute returns a call object and eval turns it
back into a formula (as per the Note near the bottom of
?substitute):

       tmp2 <- eval(substitute(. ~ . + X, list(X = tmp[[2]])))
       update(old, tmp2)



From gambarimasu at gmail.com  Thu Dec 30 07:34:08 2004
From: gambarimasu at gmail.com (t takahashi)
Date: Wed, 29 Dec 2004 23:34:08 -0700
Subject: [R] simplest possible xyz plot
Message-ID: <4b2fa9b0041229223476e03d84@mail.gmail.com>

New to R, and didn't find the following in the docs.

Main use for R. to get feet wet, is to pipe in x,y,z data, one point
per line, and
plot it in 3d, like this:

{
echo 1,2,3
echo 2,4,6
...
} | R ...

But functions like contour seem to require index vectors that are
sorted.  I am curious why, and how to convert.

Also, I am curious whether any Lisp person has ever written a front
end to R that translates Lisp syntax to R.  e.g. (countour x y f) ->
contour(x,y,f).

Thanks.



From ligges at statistik.uni-dortmund.de  Thu Dec 30 08:30:03 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 30 Dec 2004 08:30:03 +0100
Subject: [R] simplest possible xyz plot
In-Reply-To: <4b2fa9b0041229223476e03d84@mail.gmail.com>
References: <4b2fa9b0041229223476e03d84@mail.gmail.com>
Message-ID: <41D3AE7B.9020500@statistik.uni-dortmund.de>

t takahashi wrote:
> New to R, and didn't find the following in the docs.
> 
> Main use for R. to get feet wet, is to pipe in x,y,z data, one point
> per line, and
> plot it in 3d, like this:
> 
> {
> echo 1,2,3
> echo 2,4,6
> ...
> } | R ...
> 
> But functions like contour 

contour() needs a (equally spaced) matrix to plot contour lines.
For plotting a point cloud see, e.g., packages lattice (function 
cloud()), rgl, and scatterplot3d.


 > seem to require index vectors that are
> sorted.  I am curious why, and how to convert.

Because it makes no sense to reorder rows and columns of the matrix z.

Uwe Ligges

> Also, I am curious whether any Lisp person has ever written a front
> end to R that translates Lisp syntax to R.  e.g. (countour x y f) ->
> contour(x,y,f).
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Dec 30 08:52:41 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 30 Dec 2004 08:52:41 +0100
Subject: [R] coplot with png: disappearing grid lines
In-Reply-To: <20041230014405.19937.qmail@web50302.mail.yahoo.com>
References: <20041230014405.19937.qmail@web50302.mail.yahoo.com>
Message-ID: <41D3B3C9.6090203@statistik.uni-dortmund.de>

bogdan romocea wrote:

> Dear useRs,
> 
> When I use coplot() and output to png/jpeg/bmp, the grid lines from
> the scatter plots disappear. If I output to pdf() the grid lines are
> there, however I can't use it - I have many points, and the resulting
> PDF file is large and very slow to open and scroll through. (By the
> way, if I click File-Save As-png/jpeg/bmp from Rgui.exe, the grid
> lines are preserved - but I need to use code.)
> 
> With coplot(), is there a way to:
> 1. Keep the grid lines in the scatter plots when exporting output to
> png(), and perhaps change their color?
> 2. Specify the number of grid lines to be drawn on the x and y axes?
> 
> I'm running R 2.0.0 on Win XP.
> 
> Thank you,
> b.
> 
> a <- rnorm(50000)
> b <- rnorm(50000)
> c <- rnorm(50000)
> #pdf("test.pdf",height=9,width=12)
> png("test.png",height=900,width=1200)
> coplot(a ~ b | c,pch=20,col="navy",
> 	bar.bg=c(num=gray(0.8),fac=grey(0.95)))
> dev.off()


For me "grid lines" are there using R-2.0.1, Windows NT, with your example.

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Dec 30 09:04:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Dec 2004 08:04:11 +0000 (GMT)
Subject: [R] setting tabstop
In-Reply-To: <004901c4ee01$6cbdc2d0$8b00a8c0@IBM>
References: <000301c4edea$efb3ba80$8b00a8c0@IBM>
	<Pine.LNX.4.61.0412292326330.17656@gannet.stats>
	<004901c4ee01$6cbdc2d0$8b00a8c0@IBM>
Message-ID: <Pine.LNX.4.61.0412300803100.23047@gannet.stats>

On Thu, 30 Dec 2004, [iso-8859-1] S?ren Merser wrote:

> you are quite right, i didn't  read the source - only sought through the 
> documentation - since i would like to change tabsize from within a function
> i'm printing a table aligning entries with tabs
> the output is messed up if the maximum number of characters ina matrix of 
> strings is to big, i.e. >8
> it might not be the wrong trail, so perhaps i have to use formatC somehow

Indeed, setting the tabstop is not a way to format output temporarily: it 
applies to a session at least.

> regards soren
>
> ----- Original Message ----- From: "Prof Brian Ripley" 
> <ripley at stats.ox.ac.uk>
> To: "S?ren Merser" <merser at image.dk>
> Cc: "R - help" <r-help at stat.math.ethz.ch>
> Sent: Thursday, December 30, 2004 12:34 AM
> Subject: Re: [R] setting tabstop
>
>
> On Wed, 29 Dec 2004, [iso-8859-1] S?ren Merser wrote:
>
>> is it possible to change tabstop (default 8) in the GUI ?
>> 
>> regards soren
>> 
>> i'm using
>>   winxp sp2
>>   R ver. 2.0.1
>
> Yes, it is possible.  Why would you want to do so?
>
> Since you are asking, you presumably haven't explored the source code.
> Hint: what do you think TABSIZE might do?
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595 
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Thu Dec 30 09:15:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Dec 2004 08:15:34 +0000 (GMT)
Subject: [R] coplot with png: disappearing grid lines
In-Reply-To: <20041230014405.19937.qmail@web50302.mail.yahoo.com>
References: <20041230014405.19937.qmail@web50302.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0412300810240.23047@gannet.stats>

The lines are there for Uwe and for me: perhaps your PNG viewer is 
defective, possibly your graphics hardware has a buggy driver.

File-Save As-png/jpeg/bmp runs png() internally, so the exact same code is 
used.  As it is when savePlot() is used, which might be a workaround for 
you.

On Wed, 29 Dec 2004, bogdan romocea wrote:

> Dear useRs,
>
> When I use coplot() and output to png/jpeg/bmp, the grid lines from
> the scatter plots disappear. If I output to pdf() the grid lines are
> there, however I can't use it - I have many points, and the resulting
> PDF file is large and very slow to open and scroll through. (By the
> way, if I click File-Save As-png/jpeg/bmp from Rgui.exe, the grid
> lines are preserved - but I need to use code.)
>
> With coplot(), is there a way to:
> 1. Keep the grid lines in the scatter plots when exporting output to
> png(), and perhaps change their color?
> 2. Specify the number of grid lines to be drawn on the x and y axes?
>
> I'm running R 2.0.0 on Win XP.
>
> Thank you,
> b.
>
> a <- rnorm(50000)
> b <- rnorm(50000)
> c <- rnorm(50000)
> #pdf("test.pdf",height=9,width=12)
> png("test.png",height=900,width=1200)
> coplot(a ~ b | c,pch=20,col="navy",
> 	bar.bg=c(num=gray(0.8),fac=grey(0.95)))
> dev.off()
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fred at pik-potsdam.de  Thu Dec 30 10:53:53 2004
From: fred at pik-potsdam.de (Fred Hattermann)
Date: Thu, 30 Dec 2004 10:53:53 +0100
Subject: [R] graphics
Message-ID: <41D3D031.6000408@pik-potsdam.de>


Dear R-user,
I am a R beginner, and therefore my questions are very basic.
I have a simple problem: I would like to plot 100 time series each 
containing 55 steps. The data are stored in a matrix of 100 columns and 
55 rows. The first problem is to load the data from a file: I tried the 
read.table(), the scan() and the matrix(scan()) options, but I have 
problems to allocate the single columns. The list() option could be a 
solution, but it is very unconvenient: list(0,0,0......).
And how do I plot a single time series, let's say the 50s? And how to 
plot all of them?

The last problem is maybe more advanced: I would like to plot all 100 
time series, but with a confidence interval, where the density of data 
is indicated by the density of the colour of the confidence interval. A 
colleague gave me the tip that this is possible in R2.0 and described in 
the newest newsletter ...

Many thanks in advanced,
Fred



From pmccask at cyllene.uwa.edu.au  Thu Dec 30 11:22:59 2004
From: pmccask at cyllene.uwa.edu.au (Pamela McCaskie)
Date: Thu, 30 Dec 2004 18:22:59 +0800 (WST)
Subject: [R] subsetting within a function
Message-ID: <Pine.LNX.4.56.0412301821070.5229@cyllene.uwa.edu.au>

hi
I am trying to write a function around a glm or similar function, in which
I can pass the subsetting constraint as an argument to the function, but I
am having trouble.

the following commands (if I wanted SEX==0 to be my subset) in the global
environment work fine:

subexpr <- expression(SEX==0)
subtest <- with(mydata, eval(subexpr))
test.glm <- glm(y~x1+x2, data=mydata, family=binomial, subset=subtest)

And so my attempt to wrap a simple function around this looks like:
test.fun <- function(formula, mydata, sub=NULL){
  subs <- with(mydata, eval(sub))
  fit.glm <- glm(formula=formula, data=mydata, family=binomial, subset=subs)
  return(fit.glm)
}

But when I tested it out with
test <- test.fun(y~x1+x2, mydata=testdata, sub=expression(SEX==0))

I get:
Error in "[.data.frame"(structure(list(N_ASTHMA = as.integer(c(0, 0, 0,  :
        invalid subscript type
I'm guessing that it's looking in the global environment for
subs, but I'm not sure
how to fix this. Any help would be greatly appreciated.
--
Pamela A McCaskie
BSc(Mathematical Sciences)(Hons)

Western Australian Institute for Medical Research
University of Western Australia
SCGH Campus
Ground Floor, B Block
QE-II Medical Centre
Hospital Avenue, Nedlands
Western Australia  6009
AUSTRALIA
Email:        pmccask at cyllene.uwa.edu.au
Phone:        +61-8-9346 1612
Mob:          0417 926 607



From francoisromain at free.fr  Thu Dec 30 14:01:50 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Thu, 30 Dec 2004 14:01:50 +0100
Subject: [R] splot.screen: multiple plots
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E9027FE24A@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E9027FE24A@exdkba022.novo.dk>
Message-ID: <41D3FC3E.9090104@free.fr>

You can also try ?layout to specify more complicated layout such as
______________
|___1___|__2___|
|                           |
|______  3  ____ |

layout(matrix(c(1,3,2,3),nc=2),widths=c(50,50),heights=c(25,50))

and then use par(mar) to specify the margins of each plot.

Romain.

BXC (Bendix Carstensen) a ??crit :

>You probably want to use something like
>
>par( mar=c(0,0,0,0), oma=c(3,3,2,3) )
>
>which will put all your plots next to each other with no space between
>them
>and with a reasonable margin outside ("oma").
>
>Take a print of the (rather long) help page for "par", and bring it with
>you 
>wherever you go, so youcan study it in waiting lines, traffic jams,
>trains etc.
>It's boring to learn but useful to know all those weird arguments...
>
>best
>Bendix Carstensen
>----------------------
>Bendix Carstensen
>Senior Statistician
>Steno Diabetes Center
>Niels Steensens Vej 2
>DK-2820 Gentofte
>Denmark
>tel: +45 44 43 87 38
>mob: +45 30 75 87 38
>fax: +45 44 43 07 06
>bxc at steno.dk
>www.biostat.ku.dk/~bxc
>----------------------
>
>
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anne G
>>Sent: Monday, December 27, 2004 11:55 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] splot.screen: multiple plots
>>
>>
>>I have multiple conditions that I would like to plot in a
>>grid. To save space, I don't need the ticks, labels etc on
>>the plots which are not on the outside.
>>
>>I tried split.screen, but if it is clearer to use, it seems 
>>pretty rigid since it decides of each screen size apriori 
>>while I need more space for the left column and top or bottom 
>>row screens than for the other plots.
>>
>>axis(1,tick=FALSE) does not seem to do anything, asp=1 does
>>not seem to do anything either. par(pty="s") gave it a 
>>somewhate square aspect, But there is a lot of empty space 
>>which must have to do with margins, but setting the margins 
>>did not seem to change anything.
>>
>>I think I have seen examples like mine in a book, but it
>>might have been a book on Splus. Do you know where I can
>>find an example of what I am looking for.
>>
>>thanks
>>Anne
>>----------------------------------------------------------
>>here is a simplified example with the same data for all
>>plots
>>
>>X<-c(1.00000  3.63168  6.44916 10.17572 20.47440)
>>Y<-c(0.26 0.56 0.80 0.92 0.88)
>>
>>numFreq<-7
>>numdist<-8
>>
>>coefind <- 1
>>quartz(display = "", width =15, height = 10, pointsize = 9,
>>            family = "Helvetica", antialias = TRUE,
>>autorefresh = TRUE)
>>par(pty="s")
>>split.screen(c(numdist,numFreq))
>>
>>for (inddist in 1:numdist)
>>	{
>>	for (indFreq in 1:numFreq)
>>		{
>>		indscreen<- indFreq+(inddist-1)*numFreq
>>
>>			screen(indscreen)
>>#			mar<-c(1,1,1,0)
>>
>>			plot(X,Y,
>>log="x",col="red",xlim=c(1,100),ylim=c(0,1),lab=5, xlab="",
>>ylab="")
>>			axis(1,tick=FALSE)
>>			axis(2,tick=FALSE)
>>
>>			if( inddist==1)
>>				{
>>				title(main=sprintf('Freq =
>>%d',indFreq));
>>				}
>>
>>			if( indFreq==1)
>>				{
>>				axis(2, tick=TRUE)
>>				title(ylab=sprintf('dist =
>>%d',inddist));
>>				}
>> 			if (inddist==8)
>>				{
>>				axis(1,tick=TRUE)
>>				title(xlab="perc face")
>>				}
>>
>>			coefind <- coefind+1
>>		}
>>	}
>>close.screen(all = TRUE)    # exit split-screen mode
>>
>>______________________________________________
>>    
>>

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann??e
Institut de Statistique de l'Universit?? de Paris (ISUP)
Fili??re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From tlumley at u.washington.edu  Thu Dec 30 16:41:01 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 30 Dec 2004 07:41:01 -0800 (PST)
Subject: [R] simplest possible xyz plot
In-Reply-To: <4b2fa9b0041229223476e03d84@mail.gmail.com>
References: <4b2fa9b0041229223476e03d84@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0412300737340.11816@homer11.u.washington.edu>

On Wed, 29 Dec 2004, t takahashi wrote:

> New to R, and didn't find the following in the docs.
>
> Main use for R. to get feet wet, is to pipe in x,y,z data, one point
> per line, and
> plot it in 3d, like this:
>
> {
> echo 1,2,3
> echo 2,4,6
> ...
> } | R ...
>
> But functions like contour seem to require index vectors that are
> sorted.  I am curious why, and how to convert.
>

In addition to Uwe's points, if the elevation data are in a matrix then 
these functions do not require any index vectors.

 	-thomas



From tlumley at u.washington.edu  Thu Dec 30 17:11:00 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 30 Dec 2004 08:11:00 -0800 (PST)
Subject: [R] subsetting within a function
In-Reply-To: <Pine.LNX.4.56.0412301821070.5229@cyllene.uwa.edu.au>
References: <Pine.LNX.4.56.0412301821070.5229@cyllene.uwa.edu.au>
Message-ID: <Pine.A41.4.61b.0412300802280.11816@homer11.u.washington.edu>

On Thu, 30 Dec 2004, Pamela McCaskie wrote:
> And so my attempt to wrap a simple function around this looks like:
> test.fun <- function(formula, mydata, sub=NULL){
>  subs <- with(mydata, eval(sub))
>  fit.glm <- glm(formula=formula, data=mydata, family=binomial, subset=subs)
>  return(fit.glm)
> }
>
> But when I tested it out with
> test <- test.fun(y~x1+x2, mydata=testdata, sub=expression(SEX==0))
>
> I get:
> Error in "[.data.frame"(structure(list(N_ASTHMA = as.integer(c(0, 0, 0,  :
>        invalid subscript type

I get a different error: it may be that you have an object called `subs` 
in the global environment

> I'm guessing that it's looking in the global environment for
> subs,

More precisely, it is looking in environment where `formula` was created, 
which happens to be the global environment.

This is the sort of thing that happens with the fitting functions because 
they go to such lengths to break the basic scoping of R.


You probably have to substitute() the evaluated subset into the glm call.

   fit.glm <- eval(substitute(glm(formula=formula, data=mydata,
      family=binomial, subset=subset),list(subset=subs)))


 	-thomas



From spencer.graves at pdf.com  Thu Dec 30 17:58:03 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 30 Dec 2004 08:58:03 -0800
Subject: [R] t-test pvalue
In-Reply-To: <200412300153.iBU1rBHU018465@hypatia.math.ethz.ch>
References: <200412300153.iBU1rBHU018465@hypatia.math.ethz.ch>
Message-ID: <41D4339B.7040707@pdf.com>

      This may belabor the obvious, but the following packages Mr. 
Annis' answer: 


pt2 <- function(q, df,
             log.p=FALSE){
  2*pt(-abs(q), df, log.p=log.p)
}
 
 > pt2(2.23, 10)
[1] 0.04984247
 > pt2(-2.23, 10)
[1] 0.04984247
 > pt(-2.23, 10)
[1] 0.02492124
 >
      hope this helps.  spencer graves

Charles Annis, P.E. wrote:

>pt(q, ...) returns the area to the left of q.  The area to the left of 2.23
>for your situation is 0.975, while the area to the left of -2.23 (which is
>on the left side of zero from 2.23) is 0.025.
>
>
>
>
>
>Charles Annis, P.E.
> 
>Charles.Annis at StatisticalEngineering.com
>phone: 561-352-9699
>eFax:  614-455-3265
>http://www.StatisticalEngineering.com
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Giovanni Coppola
>Sent: Wednesday, December 29, 2004 8:41 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] t-test pvalue
>
>Hi all,
>I have some t-test values, and I am trying to obtain the associated
>p-values.
>Is 'pt' the right command? I wonder why 1) it returns different values for 
>x and -x, and 2) how to obtain a 2-sided p-value.
>
>example [R version 2.0.1, WinXP]:
>#if t=2.23 (df=10), the expected p-value is 0.05 for 2-sided and 0.025 for 
>1-sided t-test
>
>pt (2.23,10)
>[1] 0.9750788
>
>pt (-2.23,10) #or pt (2.23,10,lower.tail=FALSE)
>[1] 0.02492124 #as expected
>
>The opposite happens starting from negative t-test values.
>
>Should I convert in negative values my t-test values, and leave 
>lower.tail=TRUE?
>Thanks and happy new year
>Giovanni
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From maechler at stat.math.ethz.ch  Thu Dec 30 18:49:37 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 30 Dec 2004 18:49:37 +0100
Subject: [R] gls model and matrix operations
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7404044E31@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7404044E31@dc1ex2.air.org>
Message-ID: <16852.16305.558101.941002@stat.math.ethz.ch>


>>>>> "Doran," == Doran, Harold <HDoran at air.org>
>>>>>     on Wed, 29 Dec 2004 10:02:58 -0500 writes:

    Doran> Dear List:
 
    Doran> (This is a re-post as my original message was sent
    Doran> over 24 hours ao and has not posted)
 
	<.....>

Yes, it ``has posted'' -- for me within 17 minutes after being
received by the stat.math.ethz.ch server!

and so now it has been posted twice !!



From francoisromain at free.fr  Thu Dec 30 19:30:11 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Thu, 30 Dec 2004 19:30:11 +0100
Subject: [R] list(0) to integer
In-Reply-To: <20041230171424.64735.qmail@web51810.mail.yahoo.com>
References: <20041230171424.64735.qmail@web51810.mail.yahoo.com>
Message-ID: <41D44933.3060000@free.fr>

Re-Hello frederic,

Don't worry, i have the same english speaking problem, here is what i 
suggest :

n <- natScan()

natScan <- function(){
  cat("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n")
  cat("Give me a natural (everything after the '.' will be ignored)\n")
  n <- scan("", character(), 1,quiet=T)
  n  <- as.integer(n)
  if(length(n)==0||is.na(n)){
     cat("!! that's not what i want !!\n")
     return(natScan())
  }
  return(n)
}

Hope this helps


Frederic renaud a ??crit :

>Because if the user write something else (a real or a
>character), the program stop. I want in this case (if
>the user don't write a natural) that the program write
>again :" A natural, please!"
>I've some difficult to speak english! Do you
>understand?
>Thanks
>Fred
>
>
>--- Romain Fran??ois <francoisromain at free.fr> wrote:
>
>  
>
>>Hello frederic,
>>
>>why don't you just try something like :
>>
>>n <- scan("", integer(), 1)
>>
>>
>>You may also need to take a look there : 
>>http://zoonek2.free.fr/UNIX/48_R/all.html
>>and maybe there http://addictedtor.free.fr (for the
>>moment there is 
>>noting usefull there but soon ....)
>>
>>
>>Frederic renaud a ??crit :
>>
>>    
>>
>>>Hello
>>>I've another question :-)
>>>I would like to transform a list to a integer.
>>>I must be sure that the number entered by the user
>>>      
>>>
>>is
>>    
>>
>>>an integer! Thus, I've made :
>>>
>>>
>>>repeat{
>>> cat("Effectif des populations (integer):")
>>> n<-scan("",n=1,what=list(0),quiet=TRUE)
>>>
>>> if (is.integer(n[[1]])==TRUE) break
>>> print("L'effectif des population doit etre un
>>>entier")
>>> }
>>>
>>>That doesn't work of course but I've no idea to do
>>>this. How verify that n[[1]] is an integer an
>>>transform them as an integer (as.integer(n) doesn't
>>>work!) Someone can help me? Thanks!
>>>Fred
>>>
>>>_
>>>      
>>>
-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann??e
Institut de Statistique de l'Universit?? de Paris (ISUP)
Fili??re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From Dax42 at web.de  Thu Dec 30 20:01:01 2004
From: Dax42 at web.de (dax42)
Date: Thu, 30 Dec 2004 20:01:01 +0100
Subject: [R] labels and counting
Message-ID: <259A9090-5A95-11D9-83AD-000393883D7E@web.de>

Hello,

I have got the following problem:
given is a large string sequence consisting of the four letters "A" "C" 
"G" and "T" (as before). Additionally, I have got a second string 
sequence of the same length giving a label for each character. The 
labels are "+" and "-".

Now I would like to create an 8x8 matrix which contains the numbers on 
how often we see all possible pairwise combinations, for example "A" 
with the label "+" followed by "C" with the label "+" or "T"->"C" with 
the labels "-"->"+" etc.

Of course I can just use loops to "walk" along the sequence, but as you 
have shown me so much better solutions in response to my last mail, I 
thought you might be able to help and improve my R skills even further 
..

Thanks for your ideas!
Cheers, Winnie



From spencer.graves at pdf.com  Thu Dec 30 21:07:06 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 30 Dec 2004 12:07:06 -0800
Subject: [R] labels and counting
In-Reply-To: <259A9090-5A95-11D9-83AD-000393883D7E@web.de>
References: <259A9090-5A95-11D9-83AD-000393883D7E@web.de>
Message-ID: <41D45FEA.3040004@pdf.com>

      Have you done a search of "www.r-project.org" -> search -> "R site 
search" for "Markov Chain"?  I just got "138 documents matching your 
query".  The fifth one suggested "chapter 5 of Jim Lindsey's online 
document 'The statistical analysis of stochastic processes in Time', at 
his website www.luc.ac.be/~jlindsey".  I found this document mentioned 
under "recent publications".  The book may no longer be downloadable , 
but his examples still are. 

      There are probably other tools of interest to you in that list, 
and perhaps someone else will enlighten both of us on this. 

      There may be an easier way to do what you ask, if I understand 
your question correctly, the following seems to do it for me: 

bases <- c("A","C","G","T")
sgn <- c("+", "-")

signedBases <- as.vector(
     outer(bases, sgn, paste, sep=""))
sBnum <- 1:8
names(sBnum) <- signedBases
set.seed(1)
seqLen <- 100
sBaseSeq <- sample(x=signedBases,
            size=seqLen, replace=TRUE)

nextBase <- aggregate(sBaseSeq[-seqLen],
      list(thisBase=sBaseSeq[-seqLen],
           nextBase=sBaseSeq[-1]), length)
transFreq <- array(0, dim=c(8,8))
dimnames(transFreq) <- list(signedBases,
                            signedBases)
nBnum <- array(
    sBnum[as.matrix(nextBase[1:2])],
               dim=dim(nextBase[1:2]))

transFreq[nBnum]<- nextBase[[3]]

 > transFreq
   A+ C+ G+ T+ A- C- G- T-
A+  1  2  1  2  0  2  0  1
C+  2  3  1  0  0  3  1  1
G+  0  0  2  5  2  1  2  0
T+  1  2  2  1  1  3  8  2
A-  0  0  0  1  1  1  1  1
C-  2  1  1  5  0  2  2  2
G-  3  1  2  4  2  2  1  2
T-  0  2  2  2  0  1  2  1

      hope this helps.  spencer graves

dax42 wrote:

> Hello,
>
> I have got the following problem:
> given is a large string sequence consisting of the four letters "A" 
> "C" "G" and "T" (as before). Additionally, I have got a second string 
> sequence of the same length giving a label for each character. The 
> labels are "+" and "-".
>
> Now I would like to create an 8x8 matrix which contains the numbers on 
> how often we see all possible pairwise combinations, for example "A" 
> with the label "+" followed by "C" with the label "+" or "T"->"C" with 
> the labels "-"->"+" etc.
>
> Of course I can just use loops to "walk" along the sequence, but as 
> you have shown me so much better solutions in response to my last 
> mail, I thought you might be able to help and improve my R skills even 
> further ..
>
> Thanks for your ideas!
> Cheers, Winnie
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From ggrothendieck at myway.com  Thu Dec 30 21:29:33 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 30 Dec 2004 20:29:33 +0000 (UTC)
Subject: [R] labels and counting
References: <259A9090-5A95-11D9-83AD-000393883D7E@web.de>
Message-ID: <loom.20041230T212709-195@post.gmane.org>

dax42 <Dax42 <at> web.de> writes:

: 
: Hello,
: 
: I have got the following problem:
: given is a large string sequence consisting of the four letters "A" "C" 
: "G" and "T" (as before). Additionally, I have got a second string 
: sequence of the same length giving a label for each character. The 
: labels are "+" and "-".
: 
: Now I would like to create an 8x8 matrix which contains the numbers on 
: how often we see all possible pairwise combinations, for example "A" 
: with the label "+" followed by "C" with the label "+" or "T"->"C" with 
: the labels "-"->"+" etc.
: 
: Of course I can just use loops to "walk" along the sequence, but as you 
: have shown me so much better solutions in response to my last mail, I 
: thought you might be able to help and improve my R skills even further 


This is quite similar to your prior question.  Use this as your
factor:

f <- factor( paste(s1, s2, sep = "."), 
	levels = levels(interaction(c("A","C","G","T"), c("-","+")) ) 
)

and process it with the same table expression as last time:

table( f[-length(f)], f[-1] )



From jjvanhoutte at ucdavis.edu  Thu Dec 30 23:31:46 2004
From: jjvanhoutte at ucdavis.edu (Jeroen Van Houtte)
Date: Thu, 30 Dec 2004 14:31:46 -0800 (PST)
Subject: [R] Error in layout(): Too many rows in layout
Message-ID: <200412302231.iBUMVkXH006422@celerio.ucdavis.edu>


To display many small graphics one above the another (like Edward Tufte's 
sparklines), I'd like to use layout in the graphics package with more 
than 15 rows. 

I found that the limit of 15 is set by
#define MAX_LAYOUT_ROWS 15
in 
https://svn.r-project.org/R/trunk/src/include/Graphics.h
but I 'm not familiar enough with compilers to modify this.

Thank you for all help
Jeroen



From Gregor.Gorjanc at bfro.uni-lj.si  Thu Dec 30 23:44:45 2004
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Thu, 30 Dec 2004 23:44:45 +0100
Subject: [R] install.packages() vs. update.packages()
Message-ID: <7FFEE688B57D7346BC6241C55900E7300FD03A@pollux.bfro.uni-lj.si>

Hello!

Is there virtually any difference if one uses install.packages() or
update.packages() for updating/upgrading of R packages?

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From xiaoliu at jhmi.edu  Fri Dec 31 00:25:30 2004
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Thu, 30 Dec 2004 18:25:30 -0500
Subject: [R] Varying  x-axes ticks
Message-ID: <dc69285768a4.41d4481a@jhmimail.jhmi.edu>

 x <- c(0:24, (25:104-24)/(104-24)*25+24)
y <- 0.5*(104-104:0)/104
plot(x,y, xaxt = "n")
 axis(1, at = 0:49, labels = c(0:24, (25:49-24)*(104-24)/25+24))


----- Original Message -----
From: Anthony Gichangi <anthony at stat.sdu.dk>
Date: Tuesday, December 28, 2004 12:57 pm
Subject: [R] Varying  x-axes ticks

> Dear R Users,
> I have the following situations to plot. 
> 
> x-values (times)      :  range from 0 to 104           (in weeks)
> y-values (hazards)  :  range from 0 to  0.5
> 
> Now I want to make a plot of time versus hazards such
> that  in the time axis (i.e x-axis) the values 0-24 occupies
> half of the axis and the values 25-104 ocupies the rest
> of the axis. 
> 
> In other words I want to emphasize on first 24 weeks so that
> the features of the hazard are more pronounced for these 
> values.
> 
> Anybody with ideas how I can accomplish this ?
> 
> 
> Thanks in advance
> 
> 
> With kind regards
> 
> Anthony
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From Dax42 at web.de  Fri Dec 31 00:44:26 2004
From: Dax42 at web.de (dax42)
Date: Fri, 31 Dec 2004 00:44:26 +0100
Subject: [R] number format
Message-ID: <BD46DADE-5ABC-11D9-83AD-000393883D7E@web.de>

This mailing list truly is amazing.

Thank you so much for all the help!

I feel a little strange asking something that "simple" again, but I 
wasn't able to find it in the help...

Is there a way to tell R how to display numbers (double)? How many 
numbers after the decimal point, etc...

Thanks again, folks and have a nice New Years Eve!
Dax



From Ted.Harding at nessie.mcc.ac.uk  Fri Dec 31 00:37:35 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 30 Dec 2004 23:37:35 -0000 (GMT)
Subject: [R] labels and counting
In-Reply-To: <259A9090-5A95-11D9-83AD-000393883D7E@web.de>
Message-ID: <XFMail.041230233735.Ted.Harding@nessie.mcc.ac.uk>

On 30-Dec-04 dax42 wrote:
> Hello,
> 
> I have got the following problem:
> given is a large string sequence consisting of the four letters "A" "C"
> "G" and "T" (as before). Additionally, I have got a second string 
> sequence of the same length giving a label for each character. The 
> labels are "+" and "-".
> 
> Now I would like to create an 8x8 matrix which contains the numbers on 
> how often we see all possible pairwise combinations, for example "A" 
> with the label "+" followed by "C" with the label "+" or "T"->"C" with 
> the labels "-"->"+" etc.
> 
> Of course I can just use loops to "walk" along the sequence, but as you
> have shown me so much better solutions in response to my last mail, I 
> thought you might be able to help and improve my R skills even further 
> ..
> 
> Thanks for your ideas!
> Cheers, Winnie

Well, flattery and all that ...

Anyway, the following is an example of how it can be done.
You can cut&paste all the following.

# Artificial example of pairs, one of "A","C","T","G" paired
#   with one of "-","+"
 S<-sample(c("A","C","G","T"),1000,replace=TRUE)
 T<-sample(c("-","+"),1000,replace=TRUE)
 U<-apply(cbind(S,T),1,paste,collapse="")

 U[1:10]
## [1] "C+" "T-" "G+" "T+" "C+" "T+" "T-" "C+" "C-" "C-"
## Shows the first few of the pairs

# constructs 4-character items, each consisting of a pair
#   (e.g. "C+") pasted to its successor (e.g. "T-")
 V<-apply(cbind(U[1:999],U[2:1000]),1,paste,collapse="")

 V[1:7]
## [1] "C+T-" "T-G+" "G+T+" "T+C+" "C+T+" "T+T-" "T-C+"
## Shows the first few of these. Compare with U above.

## Now this is where the real gurus can show their mettle.
## 
## One way to get the counts is simply

 table(V)

## but this is not a nice layout. Another is the loop:

 for(i in sort(unique(V))){print(paste(i,":",sum(V==i)))}

## and I had hoped to think of a solution that did not
## involve a vulgar loop but would also avoid the unhelpful
## layout of table(V). (This is not your 8x8 matrix, but
## converting the output of the loop to one should not be
## impossible ... )

Pending the elegant solution which someone will come up with,
working through the above and consulting "?" for anything
not understood will reveal a few things about R ...

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 30-Dec-04                                       Time: 23:37:35
------------------------------ XFMail ------------------------------



From tlumley at u.washington.edu  Fri Dec 31 00:57:16 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 30 Dec 2004 15:57:16 -0800 (PST)
Subject: [R] number format
In-Reply-To: <BD46DADE-5ABC-11D9-83AD-000393883D7E@web.de>
References: <BD46DADE-5ABC-11D9-83AD-000393883D7E@web.de>
Message-ID: <Pine.A41.4.61b.0412301551490.219480@homer09.u.washington.edu>

On Fri, 31 Dec 2004, dax42 wrote:

> This mailing list truly is amazing.
>
> Thank you so much for all the help!
>
> I feel a little strange asking something that "simple" again, but I wasn't 
> able to find it in the help...
>
> Is there a way to tell R how to display numbers (double)? How many numbers 
> after the decimal point, etc...
>

Yes. print() has a digits= option, and there are global options that affec 
the number of digits

Eg:
> micropi<-pi*1e-6
> nanopi<-pi*1e-9
> megapi<-pi*1e6
> gigapi<-pi*1e9
> options("digits")
$digits
[1] 7
> options("scipen")
$scipen
[1] 0
> pi
[1] 3.141593
> micropi
[1] 3.141593e-06
> nanopi
[1] 3.141593e-09
> megapi
[1] 3141593
> gigapi
[1] 3141592654
>
> options(digits=4)
> pi
[1] 3.142
> micropi
[1] 3.142e-06
> nanopi
[1] 3.142e-09
> megapi
[1] 3141593
> gigapi
[1] 3.142e+09
> options(scipen=4)
> pi
[1] 3.142
> micropi
[1] 0.000003142
> nanopi
[1] 3.142e-09
> megapi
[1] 3141593
> gigapi
[1] 3141592654


 	-thomas



From giljustino at yahoo.com.br  Fri Dec 31 01:18:41 2004
From: giljustino at yahoo.com.br (Gilvan)
Date: Thu, 30 Dec 2004 22:18:41 -0200
Subject: RES: [R] Developing R classes
In-Reply-To: <16850.47597.746218.332517@gargle.gargle.HOWL>
Message-ID: <200412310019.iBV0JYeF019565@hypatia.math.ethz.ch>

I??m thankful for everyone who answered me. All the suggestions were very
useful to me.

I??m going to start with easy classes both in S3 and S4 models to understand
their differences. I will probably ask you again about new doubts :) or
share my experience with these models. By the way.. I found the package R.oo
which is used to write classes. I believe it is a easy way to create a
class. Someone has something against it ?

And to finish... I am student too Dave, and the books are expensive to me
too. Here in Brazil, I can pay for a good book about 25 USD. I saw those
programming books about 60 Usd!

Thanks again and have a nice 2005 !
Gilvan Justino
giljustino at yahoo.com.br


-----Mensagem original-----
De: David Kane [mailto:dave at kanecap.com] 
Enviada em: quarta-feira, 29 de dezembro de 2004 12:07
Para: Gilvan Justino
Cc: r-help at stat.math.ethz.ch
Assunto: Re: [R] Developing R classes

Gilvan Justino writes:
 > Hi,
 >
 > I??m trying to write some R classes but I din??t find documentation  >
enought to develop them. 

I have faced a similar problem. Here is the summary of what I have been able
to find (including some of the responses that you have already received)
with a focus on S4 classes since some people assert that all new development
should be done with them.

1) I think that the three best free resources for learning about S4 classes
are (in the order that I recommend you read them):

http://www.omegahat.org/RSMethods/Intro.pdf (More or less Section 1.6 from
The Green Book.)

http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/S4Objects.pdf (I think
that this version of the talk is more up-to-date than the one located at the
Bioconductor site at
http://www.bioconductor.org/develPage/guidelines/programming/S4Objects.pdf)

@Article{Rnews:Lumley:2004b,
  author       = {Thomas Lumley},
  title	       = {Programmers' Niche: A Simple Class, in {S3} and {S4}},
  journal      = {R News},
  year	       = 2004,
  volume       = 4,
  number       = 1,
  pages	       = {33--36},
  month	       = {June},
  url	       = {http://CRAN.R-project.org/doc/Rnews/},
}


2) If you want to get serious with S4 classes, you may need to purchase
"Programming with Data" by John Chambers (aka The Green Book) and/or "S
Programming" by Venable and Ripley. I own both, but am interested in finding
material that is similarly comprehensive but free. Many of the people (read:
students) that I urge to use R do not have the sort of budgets for buying
books that I am lucky enough to have.

3) I found all of these references to be worth a read through. 

http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Leisch.pdf
http://www.biostat.jhsph.edu/~rpeng/R-classes-scope.pdf
http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/Methods.pdf
http://www.molgen.mpg.de/~wolski/Robject/Extending.pdf

Hope that helps,

Dave Kane

PS. I can't access the stat.auckland site right now but have been able to in
the past.

--
No virus found in this incoming message.
Checked by AVG Anti-Virus.

 

-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From ggrothendieck at myway.com  Fri Dec 31 08:02:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 31 Dec 2004 07:02:19 +0000 (UTC)
Subject: [R] install.packages() vs. update.packages()
References: <7FFEE688B57D7346BC6241C55900E7300FD03A@pollux.bfro.uni-lj.si>
Message-ID: <loom.20041231T080044-61@post.gmane.org>

Gorjanc Gregor <Gregor.Gorjanc <at> bfro.uni-lj.si> writes:

: 
: Hello!
: 
: Is there virtually any difference if one uses install.packages() or
: update.packages() for updating/upgrading of R packages?

update.packages calls install.packages to do the installation.
Check out the source.



From ligges at statistik.uni-dortmund.de  Fri Dec 31 08:51:06 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 31 Dec 2004 08:51:06 +0100
Subject: [R] Error in layout(): Too many rows in layout
In-Reply-To: <200412302231.iBUMVkXH006422@celerio.ucdavis.edu>
References: <200412302231.iBUMVkXH006422@celerio.ucdavis.edu>
Message-ID: <41D504EA.7070700@statistik.uni-dortmund.de>

Jeroen Van Houtte wrote:

> To display many small graphics one above the another (like Edward Tufte's 
> sparklines), I'd like to use layout in the graphics package with more 
> than 15 rows. 
> 
> I found that the limit of 15 is set by
> #define MAX_LAYOUT_ROWS 15

Well, what about replacing 15 by something like 100?
The point is that you should look where MAX_LAYOUT_ROWS is used for 
calculations and whether there might result any problem after increasing it.

> in 
> https://svn.r-project.org/R/trunk/src/include/Graphics.h
> but I 'm not familiar enough with compilers to modify this.

Looking into your mail header [telling us you are using Mozilla/4.0 
(compatible; MSIE 6.0; Windows NT 5.1; SV1)], I see you are using 
Windows and probably never have compiled R yourself (please read the 
posting guide which tells you to mention relevant information such as 
the OS in use).

Please read the file R/src/gnuwin32/Install.
It tells you how to compile R yourself and which tools are required.

Uwe Ligges




> Thank you for all help
> Jeroen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Dec 31 09:04:24 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 31 Dec 2004 09:04:24 +0100
Subject: [R] graphics
In-Reply-To: <41D3D031.6000408@pik-potsdam.de>
References: <41D3D031.6000408@pik-potsdam.de>
Message-ID: <41D50808.4090508@statistik.uni-dortmund.de>

Fred Hattermann wrote:

> 
> Dear R-user,
> I am a R beginner, and therefore my questions are very basic.
> I have a simple problem: I would like to plot 100 time series each 
> containing 55 steps. The data are stored in a matrix of 100 columns and 
> 55 rows. The first problem is to load the data from a file: I tried the 
> read.table(), the scan() and the matrix(scan()) options, but I have 
> problems to allocate the single columns. The list() option could be a 
> solution, but it is very unconvenient: list(0,0,0......).
> And how do I plot a single time series, let's say the 50s? And how to 
> plot all of them?


These questions are really basic, they are covered in the manuals and 
any good book about R. Please read at least
a) the manual "An Introduction to R"
b) the manual "R Data Import/Export"
c) the posting guide (see below, has been appended at the end of your 
message)

After you have read through and tried to solve your problem again, you 
might want to come up with a specific question again.


> The last problem is maybe more advanced: I would like to plot all 100 
> time series, but with a confidence interval, where the density of data 
> is indicated by the density of the colour of the confidence interval. A 
> colleague gave me the tip that this is possible in R2.0 and described in 
> the newest newsletter ...

This question is also unspecific. Which kind of confidence interval? 
What does "density of data" mean here? Do you mean a global quantity for 
the whole series or really the empirical (estimated) density so that 
color changes from point to point?

Uwe Ligges


> Many thanks in advanced,
> Fred
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From tura at centroin.com.br  Fri Dec 31 10:18:55 2004
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Fri, 31 Dec 2004 07:18:55 -0200
Subject: [R] update.packages
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7404044E31@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7404044E31@dc1ex2.air.org>
Message-ID: <6.1.2.0.2.20041231070455.04b8b230@centroin.com.br>

Hi R-masters!

First of All: Happy new Year !

I have one doubt. I one time for week command in R update.packages() for 
recive new versions of packages in my R, I also subscribe R-pkgs for recive 
notices of new packages.

But I discover other packages in CRAN without notices in R-pkgs. So How to 
I mantain my R- system update with all packages in CRAN (New versions of my 
packages and install New packages)

Thanks in advance

Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil 


-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From paulojus at est.ufpr.br  Fri Dec 31 12:45:51 2004
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Fri, 31 Dec 2004 09:45:51 -0200 (BRST)
Subject: [R] update.packages
In-Reply-To: <6.1.2.0.2.20041231070455.04b8b230@centroin.com.br>
References: <88EAF3512A55DF46B06B1954AEF73F7404044E31@dc1ex2.air.org>
	<6.1.2.0.2.20041231070455.04b8b230@centroin.com.br>
Message-ID: <Pine.LNX.4.58L0.0412310943050.23194@est.ufpr.br>

One possible solution is given by the function packageStatus()
In order to check and install new packages in our system here we use:

x <- packageStatus(repositories="http://cran.br.r-project.org/src/contrib")
st <- x$avai["Status"]
install.packages(rownames(st)[which(st$Status=="not installed")])

best
P.J.

On Fri, 31 Dec 2004, Bernardo Rangel Tura wrote:

> Hi R-masters!
>
> First of All: Happy new Year !
>
> I have one doubt. I one time for week command in R update.packages() for
> recive new versions of packages in my R, I also subscribe R-pkgs for recive
> notices of new packages.
>
> But I discover other packages in CRAN without notices in R-pkgs. So How to
> I mantain my R- system update with all packages in CRAN (New versions of my
> packages and install New packages)
>
> Thanks in advance
>
> Bernardo Rangel Tura, MD, MSc
> National Institute of Cardiology Laranjeiras
> Rio de Janeiro Brazil
>
>
> --
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
LEG (Laborat??rio de Estat??stica e Geoinforma????o)
Departamento de Estat??stica
Universidade Federal do Paran??
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3573
Fax: (+55) 41 361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From mi2kelgrum at yahoo.com  Fri Dec 31 13:05:35 2004
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Fri, 31 Dec 2004 04:05:35 -0800 (PST)
Subject: [R] Supressing empty sections with Sweave
Message-ID: <20041231120535.146.qmail@web60202.mail.yahoo.com>

Dear useRs,

I'm writing regular survey reports using Sweave. Each
report has several sections along the lines of:

\section*{Disease X}

<<MapX,fig=TRUE,echo=FALSE>>=
image(vectorx,vectory,matrixz)

@

Notes with or without Sexpr{a}.

\vfill

\pagebreak

\section*{Disease Y}
<<MapY,fig=TRUE,echo=FALSE>>=
...etc.


Often one or more of the diseases is not observed (all
values in matrixz are 0), in which case I would prefer
not to display the section at all.  Does any one no
whether it is possible automate this with Sweave?

Mikkel



From h.brunschwig at utoronto.ca  Fri Dec 31 17:20:33 2004
From: h.brunschwig at utoronto.ca (h.brunschwig@utoronto.ca)
Date: Fri, 31 Dec 2004 11:20:33 -0500
Subject: [R] lme: Confusion about Variances
Message-ID: <1104510033.41d57c515a5e7@webmail.utoronto.ca>


Dear R users!

I used lme to fit a mixed model with random intercept and spatial Gaussian
correlation i.e. I fitted a model of the following form:

Y = X*beta + error

and

error = U + W(t) + Z

where U is the random intercept (normally distributed), W(t) the stationary
Gaussian process and Z also a normally distributed (the residual) rv. Each of
these three random variables have a variance which I am not sure to which output
in lme they belong to.
VarCorr gives the intercept and residual variance which I assume belong to U and
Z respectively. The output of lme gives another estimate called "range" which I
assume belongs to the parameter estimate needed for the Gaussian correlation.

Are my assumptions correct? And where can I get the variance for the W(t) from?

Thanks for any answers...and happy new year...

Hadassa



From spencer.graves at pdf.com  Fri Dec 31 18:02:54 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 31 Dec 2004 09:02:54 -0800
Subject: [R] lme: Confusion about Variances
In-Reply-To: <1104510033.41d57c515a5e7@webmail.utoronto.ca>
References: <1104510033.41d57c515a5e7@webmail.utoronto.ca>
Message-ID: <41D5863E.6060704@pdf.com>

      I'm unable to parse your question in the time I have right now, 
but have you worked the examples with ?lme in library(nlme), extending 
them with "intervals" and "VarCorr"?  If you do that, I suspect you 
should find the answer to your question.  Also, have you consulted 
Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus 
(Springer)?  This book made the difference between failure and success 
in my attempts to understand and use lme. 

     fm1 <- lme(distance ~ age, data = Orthodont) # random is ~ age
     fm2 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1)
     summary(fm1)
     summary(fm2)
intervals(fm1)
intervals(fm2)
anova(fm1, fm2)
VarCorr(fm1)
VarCorr(fm2)

      hope this helps. 
      spencer graves

h.brunschwig at utoronto.ca wrote:

>Dear R users!
>
>I used lme to fit a mixed model with random intercept and spatial Gaussian
>correlation i.e. I fitted a model of the following form:
>
>Y = X*beta + error
>
>and
>
>error = U + W(t) + Z
>
>where U is the random intercept (normally distributed), W(t) the stationary
>Gaussian process and Z also a normally distributed (the residual) rv. Each of
>these three random variables have a variance which I am not sure to which output
>in lme they belong to.
>VarCorr gives the intercept and residual variance which I assume belong to U and
>Z respectively. The output of lme gives another estimate called "range" which I
>assume belongs to the parameter estimate needed for the Gaussian correlation.
>
>Are my assumptions correct? And where can I get the variance for the W(t) from?
>
>Thanks for any answers...and happy new year...
>
>Hadassa
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From hmaughan at u.arizona.edu  Fri Dec 31 19:20:21 2004
From: hmaughan at u.arizona.edu (Heather Maughan)
Date: Fri, 31 Dec 2004 11:20:21 -0700
Subject: [R] Citation
Message-ID: <BDFAE675.405%hmaughan@u.arizona.edu>

Does anyone know R is to be cited in a publication?  I've looked everywhere
on the website and cannot find this.

Thanks,
Heather
-- 
Heather Maughan
Department of Ecology and Evolutionary Biology
Biosciences West 310
University of Arizona
Tucson, AZ  85701
Phone: 520-626-5108
Fax: 520-621-9190
hmaughan at u.arizona.edu



From jdnewmil at dcn.davis.ca.us  Fri Dec 31 19:30:51 2004
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 31 Dec 2004 10:30:51 -0800 (PST)
Subject: [R] Citation
In-Reply-To: <BDFAE675.405%hmaughan@u.arizona.edu>
Message-ID: <Pine.LNX.4.21.0412311028580.9581-100000@mirimichi.jdn.localnet>

On Fri, 31 Dec 2004, Heather Maughan wrote:

> Does anyone know R is to be cited in a publication?  I've looked everywhere
> on the website and cannot find this.

http://cran.r-project.org/doc/FAQ/R-FAQ.html ... FAQ 2.8

Google is your friend.

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k



From MSchwartz at MedAnalytics.com  Fri Dec 31 19:34:38 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 31 Dec 2004 12:34:38 -0600
Subject: [R] Citation
In-Reply-To: <BDFAE675.405%hmaughan@u.arizona.edu>
References: <BDFAE675.405%hmaughan@u.arizona.edu>
Message-ID: <1104518078.7926.3.camel@horizons.localdomain>

On Fri, 2004-12-31 at 11:20 -0700, Heather Maughan wrote:
> Does anyone know R is to be cited in a publication?  I've looked
> everywhere
> on the website and cannot find this.
> 
> Thanks,
> Heather


When you first open a R session, you will see the following:

...
R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.
...


Also, main R FAQ 2.8 "Citing R"

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Citing-R


KTH,

Marc Schwartz
<Happy New Year in Tucson>



From stephane.dray at umontreal.ca  Fri Dec 31 19:39:29 2004
From: stephane.dray at umontreal.ca (Stephane Dray)
Date: Fri, 31 Dec 2004 13:39:29 -0500
Subject: [R] Citation
Message-ID: <5.2.1.1.0.20041231133922.02117e70@biomserv.univ-lyon1.fr>

When you start R, there is a text... you can read it and find the answer.
The information can also be find the FAQ, and you must read the FAQ before 
sending an email to RHELP as precised in the posting guide.

and I am sure that googling how to cite R will produce results...
not sure that you have looked everywhere

happy new year

At 13:20 31/12/2004, Heather Maughan wrote:
>Does anyone know R is to be cited in a publication?  I've looked everywhere
>on the website and cannot find this.
>
>Thanks,
>Heather
>--
>Heather Maughan
>Department of Ecology and Evolutionary Biology
>Biosciences West 310
>University of Arizona
>Tucson, AZ  85701
>Phone: 520-626-5108
>Fax: 520-621-9190
>hmaughan at u.arizona.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From spencer.graves at pdf.com  Fri Dec 31 19:41:30 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 31 Dec 2004 10:41:30 -0800
Subject: [R] Citation
In-Reply-To: <Pine.LNX.4.21.0412311028580.9581-100000@mirimichi.jdn.localnet>
References: <Pine.LNX.4.21.0412311028580.9581-100000@mirimichi.jdn.localnet>
Message-ID: <41D59D5A.6000706@pdf.com>

         Thanks, Jeff.  In addition, I think it's standard to add the 
"date accessed" when mentioning a web site.  I would also add version 
identification, e.g., R 2.0.1. 

      hope this helps.  spencer graves

Jeff Newmiller wrote:

>On Fri, 31 Dec 2004, Heather Maughan wrote:
>
>  
>
>>Does anyone know R is to be cited in a publication?  I've looked everywhere
>>on the website and cannot find this.
>>    
>>
>
>http://cran.r-project.org/doc/FAQ/R-FAQ.html ... FAQ 2.8
>
>Google is your friend.
>
>---------------------------------------------------------------------------
>Jeff Newmiller                        The     .....       .....  Go Live...
>DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                      Live:   OO#.. Dead: OO#..  Playing
>Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From younko at uiuc.edu  Fri Dec 31 21:35:51 2004
From: younko at uiuc.edu (Ko,Younhee)
Date: Fri, 31 Dec 2004 14:35:51 -0600
Subject: [R] Export the R object
Message-ID: <4a6cba65.b77b8e75.81a0e00@expms3.cites.uiuc.edu>

Hi, 

I just have a quick question.
If I got some result as the result of R, how can I export 
the result object?

I mean, if I want to use the result object in Excel or other 
program in order to more specific investigation, how can I 
export it?

If I just list the result and copy, 
The result is like this..


[586] "BB170029A10B06" "BB170029A20E06"

First column, automatically show the number of result and 
other result also include the "".

If I want to use this result, I have to manipulate the 
result by myself(I mean e.g remove " and remove [586] like 
this way)?????

Or there is any good way to export this result object to 
other program?

Please help me. 

Thanks in advance.
========================
Younhee Ko(younko at uiuc.edu)

http://comedu.korea.ac.kr/~unygo
contact : 217-417-4868
Graduate Student in Dept. of Computer Science
University of Illinois at Urbana-Champaign



From rpeng at jhsph.edu  Fri Dec 31 21:54:11 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 31 Dec 2004 15:54:11 -0500
Subject: [R] Export the R object
In-Reply-To: <4a6cba65.b77b8e75.81a0e00@expms3.cites.uiuc.edu>
References: <4a6cba65.b77b8e75.81a0e00@expms3.cites.uiuc.edu>
Message-ID: <41D5BC73.8070205@jhsph.edu>

If you want to export data frame, you can use write.table() to write out a CSV 
(comma separated value) file, which you can then read into Excel.

-roger

Ko,Younhee wrote:
> Hi, 
> 
> I just have a quick question.
> If I got some result as the result of R, how can I export 
> the result object?
> 
> I mean, if I want to use the result object in Excel or other 
> program in order to more specific investigation, how can I 
> export it?
> 
> If I just list the result and copy, 
> The result is like this..
> 
> 
> [586] "BB170029A10B06" "BB170029A20E06"
> 
> First column, automatically show the number of result and 
> other result also include the "".
> 
> If I want to use this result, I have to manipulate the 
> result by myself(I mean e.g remove " and remove [586] like 
> this way)?????
> 
> Or there is any good way to export this result object to 
> other program?
> 
> Please help me. 
> 
> Thanks in advance.
> ========================
> Younhee Ko(younko at uiuc.edu)
> 
> http://comedu.korea.ac.kr/~unygo
> contact : 217-417-4868
> Graduate Student in Dept. of Computer Science
> University of Illinois at Urbana-Champaign
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Fri Dec 31 21:59:58 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 31 Dec 2004 12:59:58 -0800
Subject: [R] Export the R object
In-Reply-To: <4a6cba65.b77b8e75.81a0e00@expms3.cites.uiuc.edu>
References: <4a6cba65.b77b8e75.81a0e00@expms3.cites.uiuc.edu>
Message-ID: <41D5BDCE.60906@pdf.com>

      I'm sorry, but your question seems to broad for me to provide a 
succinct answer.  You can look at "sink", but if that doesn't answer 
your question, please "read the posting guide! 
http://www.R-project.org/posting-guide.html" on "How to ask good 
questions that prompt useful answers", especially the part about 
providing a small example that someone else can actually run.  If we 
know the kind of "results" you want to export, what you tried, and why 
that was not satisfactory, it will be much easier for someone to help 
you.  (Moreover, you might even find an answer to your own question by 
following the procedure in the posting guide.) 

      sorry I couldn't help more. 
      spencer graves

Ko,Younhee wrote:

>Hi, 
>
>I just have a quick question.
>If I got some result as the result of R, how can I export 
>the result object?
>
>I mean, if I want to use the result object in Excel or other 
>program in order to more specific investigation, how can I 
>export it?
>
>If I just list the result and copy, 
>The result is like this..
>
>
>[586] "BB170029A10B06" "BB170029A20E06"
>
>First column, automatically show the number of result and 
>other result also include the "".
>
>If I want to use this result, I have to manipulate the 
>result by myself(I mean e.g remove " and remove [586] like 
>this way)?????
>
>Or there is any good way to export this result object to 
>other program?
>
>Please help me. 
>
>Thanks in advance.
>========================
>Younhee Ko(younko at uiuc.edu)
>
>http://comedu.korea.ac.kr/~unygo
>contact : 217-417-4868
>Graduate Student in Dept. of Computer Science
>University of Illinois at Urbana-Champaign
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From roebuck at odin.mdacc.tmc.edu  Fri Dec 31 22:01:13 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Fri, 31 Dec 2004 15:01:13 -0600 (CST)
Subject: [R] install.packages() for local source file
Message-ID: <Pine.OSF.4.58.0412271538090.98639@odin.mdacc.tmc.edu>

Wish to install a local source package on Un*x platform from
within R. Same thing as I can accomplish from cmdline as

$ export R_LIBS=~/R/library
$ cd /path/to/pkg
$ R CMD INSTALL -l $R_LIBS <pkgname>


So, how do you go about this anyway?
And isn't this a bug in 'install.packages'?

-------
$ R

R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3

> file.pkg <- "mypkg_0.1.tar.gz"
> path.pkg <- file.path(path.expand("~"), "cvknn", file.pkg)
> file.exists(path.pkg)
[1] TRUE
> uri.pkg <- paste("file://", path.pkg, sep = "")
> install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
Error in file.info(x) : Object "tmpd" not found
> traceback()
4: file.info(x)
3: dirTest(destdir)
2: download.packages(pkgs, destdir = tmpd, available = available,
       contriburl = contriburl, method = method)
1: install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
> version
         _
platform sparc-sun-solaris2.9
arch     sparc
os       solaris2.9
system   sparc, solaris2.9
status
major    1
minor    9.0
year     2004
month    04
day      12
language R


----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From david.whiting at ncl.ac.uk  Fri Dec 31 22:39:55 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 31 Dec 2004 21:39:55 +0000
Subject: [R] Supressing empty sections with Sweave
In-Reply-To: <20041231120535.146.qmail@web60202.mail.yahoo.com>
References: <20041231120535.146.qmail@web60202.mail.yahoo.com>
Message-ID: <m28y7e87v8.fsf@192.168.57.36>

Mikkel Grum <mi2kelgrum at yahoo.com> writes:

Hi Mikkel,

One way would be to use cat("\\section{", diseaseName, "}\n") with if
(sum(matrixz) > 0) within a for loop (or perhaps using one of the
apply family of functions (probably lapply?)).  To some extent this
by-passes the separation of code and documentation that literal
programming is about but I couldn't think of a better way to solve the
problem just now.  Sorry this is not a full worked example, but I'm
off to the pub now for a few pints to welcome 2005.

Dave


> Dear useRs,
> 
> I'm writing regular survey reports using Sweave. Each
> report has several sections along the lines of:
> 
> \section*{Disease X}
> 
> <<MapX,fig=TRUE,echo=FALSE>>=
> image(vectorx,vectory,matrixz)
> 
> @
> 
> Notes with or without Sexpr{a}.
> 
> \vfill
> 
> \pagebreak
> 
> \section*{Disease Y}
> <<MapY,fig=TRUE,echo=FALSE>>=
> ...etc.
> 
> 
> Often one or more of the diseases is not observed (all
> values in matrixz are 0), in which case I would prefer
> not to display the section at all.  Does any one no
> whether it is possible automate this with Sweave?
> 
> Mikkel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
David Whiting
University of Newcastle upon Tyne, UK



From ggrothendieck at myway.com  Fri Dec 31 23:24:53 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 31 Dec 2004 22:24:53 +0000 (UTC)
Subject: [R] Export the R object
References: <4a6cba65.b77b8e75.81a0e00@expms3.cites.uiuc.edu>
Message-ID: <loom.20041231T231533-609@post.gmane.org>

Ko,Younhee <younko <at> uiuc.edu> writes:

: 
: Hi, 
: 
: I just have a quick question.
: If I got some result as the result of R, how can I export 
: the result object?
: 
: I mean, if I want to use the result object in Excel or other 
: program in order to more specific investigation, how can I 
: export it?
: 
: If I just list the result and copy, 
: The result is like this..
: 
: [586] "BB170029A10B06" "BB170029A20E06"
: 
: First column, automatically show the number of result and 
: other result also include the "".
: 
: If I want to use this result, I have to manipulate the 
: result by myself(I mean e.g remove " and remove [586] like 
: this way)?????
: 
: Or there is any good way to export this result object to 
: other program?

There are many different ways to do this in R including
via text files, .csv files, HTML files and 
Microsoft COM objects.   1. Search the mail archives for 
Excel, 2. read the Data Import and Export Manual and 3. read
the FAQ.  Pointers to where to find these are in
the posting guide at the bottom of this and every post
on this list.



