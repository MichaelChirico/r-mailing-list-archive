From g@@@powe|| @end|ng |rom protonm@||@com  Thu May  1 22:00:37 2025
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Thu, 01 May 2025 20:00:37 +0000
Subject: [R] 
 Estimating regression with constraints in model coefficients
In-Reply-To: <CA+dpOJnSXi07Wy2fSP3hwCXh8F3VBYKTj+owYbfzueOhD5zASw@mail.gmail.com>
References: <CA+dpOJ=X2T9xi5TmvWzpFX=ros8NYfmtXsXOm+8wDr19pR6OkQ@mail.gmail.com>
 <Uvig6if21EU5c3HxzIv3etB9zIRRSVy_6QPWO421c2GHYxt5ftqQ-osvq5A4w9186NV6sRZJ-imiIEXjvqkvNWnuhTBq0sMq0RJN4pH5uwc=@protonmail.com>
 <CA+dpOJmCkhXBaZOdjY86eWakRBCSdJtLTx6TmSnGFNjo3xVyJw@mail.gmail.com>
 <DuJ4ewmz_duuQWTRP9JCTx_W4OLnTynyyWUUEDwQFaTCDEXB66XtFAtYKoYHsQpG1Ke9RWwoCL3ERTAequACV_UfWHjPGXxLgIhfhTyH2TQ=@protonmail.com>
 <CA+dpOJ=K9QbYRa8Q+W+gECGf0QMExf9im+-U+nhYaeQkdcDQ2A@mail.gmail.com>
 <g11-DFGbgql8L6nAXJtSknLU8RfDUP1QKxGCLKfU7HEpO8rQ50UnyyMxgTazehrrNtQcNuqw1sFowtJlX64SB4-tw-iYB1v2arxegyQik0Q=@protonmail.com>
 <CA+dpOJkdtjhEfGrXMB=UwYCNCG7662V=aQQg9+jWjK0JZgwAtw@mail.gmail.com>
 <oMVMoZI6PDcvbWetH9qHhWgMJYkMoVNtkUF1wzInX7KvQO1vdnwFBapkiemN5kNdITYMaRlU-XE3wXX8_IQpCn2m9M-OzorsolgwjVUj_fI=@protonmail.com>
 <CA+dpOJnSXi07Wy2fSP3hwCXh8F3VBYKTj+owYbfzueOhD5zASw@mail.gmail.com>
Message-ID: <qmq9vqE8khu_e51Irjt02G-L289bxVqUcMQ1GYgPLwIRTjyzA8EXDcGO6HqKG_EBkbGQazr-grx0ksTzi_spmRyme7uKI0yDa8kXHJB4atQ=@protonmail.com>

Hello Christofer,

That?s good information. Thanks for the precision.
Think I can use this now to work on a script:

Based on what you provided:
1. Two distinct sets of coefficients, ?? and ??, each associated with the logits for:
 ? P(Y?1)P(Y ? 1)P(Y?1)
 ? P(Y?2)P(Y ? 2)P(Y?2)

2. Separate sum constraints:
 ? ??(1)=1.60 \ sum ?^{(1)} = 1.60??(1)=1.60
 ? ??(2)=1.60 \ sum ?^{(2)} = 1.60??(2)=1.60

3. Element-wise bounds applied to each ? set:
 ? lower = c(1, -1, 0)
 ? upper = c(2, 1, 1)

Planning to wrap this into an updated nloptr optimization routine. I?ll work on a .R script as time permits ? if all goes well, you should be able to run it as provided.

r/
Gregg





On Wednesday, April 30th, 2025 at 3:23 AM, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:

> 

> 

> Hi Gregg,
> 

> Below I try to address
> 

> 1) The sum constraint would apply for each set ?? and ?? i.e. sum(??)
> = sum(??) = 1.60
> 2) Just like 1) the lower and upper bounds will be applied for
> individual set i.e. individual elements of ?? are subject to lower =
> c(1, -1, 0) and upper = c(2, 1, 1) and individual elements of ?? are
> subject to lower = c(1, -1, 0) and upper = c(2, 1, 1)
> 

> I hope that I am able to answer your questions. Please let me know if
> further information is required.
> 

> Thanks and regards,
> 

> On Wed, Apr 30, 2025 at 4:22?AM Gregg Powell g.a.powell at protonmail.com wrote:
> 

> > Hello again Christofer,
> > This clarification changes the model structure somewhat significantly -it shifts us from a standard cumulative logit model with proportional odds to a non-parallel cumulative logit model, where each threshold has its own set of ? coefficients. At least, that is now my understanding.
> > So, instead of a single ? vector shared across all class boundaries, you're now specifying:
> > ? One set of coefficients ?(1)?^{(1)}?(1) for the logit of P(Y?1)P(Y ? 1)P(Y?1),
> > ? A second, distinct set ?(2)?^{(2)}?(2) for P(Y?2)P(Y ? 2)P(Y?2),
> > ? And no intercepts, meaning the threshold-specific slope vectors must carry all the signal.
> > 

> > So, we can adjust the log-likelihood accordingly:
> > P(Y=1)=logistic(X?(1))P(Y=2)=logistic(X?(2))?logistic(X?(1))P(Y=3)=1?logistic(X?(2))P(Y = 1) = logistic(X?^{(1)}) P(Y = 2) = logistic(X?^{(2)}) - logistic(X?^{(1)}) P(Y = 3) = 1 - logistic(X?^{(2)}) P(Y=1)=logistic(X?(1))P(Y=2)=logistic(X?(2))?logistic(X?(1))P(Y=3)=1?logistic(X?(2))
> > 

> > Before I attempt a revised script, can you confirm:
> > 1. Should the sum constraint (e.g., sum(?) = 1.60) apply to:
> > ? Only ???
> > ? Only ???
> > ? Or the sum of all 6 coefficients (?? and ?? combined)?
> > 

> > 2. Do you want to apply separate lower/upper bounds to each of the six ? coefficients (and if so, what are they for each)?
> > 

> > Once I understand this last part better, I?ll see about working on a version that fits this updated structure and constraint logic.
> > As always ? no promises.
> > r/
> > Gregg Powell
> > Sierra Vista, AZ
> > 

> > On Tuesday, April 29th, 2025 at 1:51 PM, Christofer Bogaso bogaso.christofer at gmail.com wrote:
> > 

> > > Hi Gregg,
> > 

> > > I am just wondering if you get any time to think about this problem.
> > 

> > > As I understand, typically for this type of problem, we have different
> > > intercepts for different classes, while slope (beta) coefficients
> > > remain the same across classes.
> > 

> > > But in my case, since we do not have any intercept term, the slope
> > > coefficients need to be estimated separately for different classes.
> > > Therefore, since we have 3 classes in the response variable (i.e.
> > > 'apply'), there will be 3 different sets of beta-coefficients for the
> > > independent variables.
> > 

> > > Under this situation, I wonder how I can create the likelihood
> > > function subject to all applicable constraints.
> > 

> > > Any insight would be highly appreciated.
> > 

> > > Thanks and regards,
> > 

> > > On Fri, Apr 25, 2025 at 12:31?AM Gregg Powell g.a.powell at protonmail.com wrote:
> > 

> > > > Christofer,
> > > > That was a detailed follow-up ? you clarified the requirements precisely enough providing a position to proceed from...
> > 

> > > > To implement this, a constrained optimization procedure to estimate an ordinal logistic regression model is needed (cumulative logit), based on:
> > 

> > > > 1. Estimated Cutpoints
> > > > ? No intercept in the linear predictor
> > > > ? Cutpoints (thresholds) will be estimated directly from the data
> > 

> > > > 2. Coefficient Constraints
> > > > ? Box constraints on each coefficient
> > > > ? For now:
> > > > lower = c(1, -1, 0)
> > > > upper = c(2, 1, 1)
> > > > ? These apply to: pared, public, gpa (in that order)
> > 

> > > > 3. Sum Constraint
> > > > ? The sum of coefficients must equal 1.60
> > 

> > > > 4. Data to use...
> > > > ? Use the IDRE ologit.dta dataset from UCLA (for now)
> > 

> > > > Technical Approach
> > 

> > > > ? Attempt to write a custom negative log-likelihood function using the cumulative logit formulation:
> > 

> > > > P(Y?k?X)=11+exp?[?(?k?X?)]P(Y \leq k \mid X) = \frac{1}{1 + \exp[-(\theta_k - X\beta)]}
> > 

> > > > and derive P(Y=k)P(Y = k) from adjacent differences of these.
> > 

> > > > ? Cutpoints ?k\theta_k will be estimated as separate parameters, with constraints to ensure they?re strictly increasing for identifiability.
> > 

> > > > ? The optimization will use nloptr::nloptr(), which supports:
> > > > - Lower/upper bounds (box constraints)
> > > > - Equality constraints (for sum of ?)
> > > > - Nonlinear inequality constraints (to keep cutpoints ordered)
> > 

> > > > I?ll have some time later - in the next day or two to attempt a script with:
> > > > - Custom negative log-likelihood
> > > > - Parameter vector split into ? and cutpoints
> > > > - Constraint functions: sum(?) = 1.60 and increasing cutpoints
> > > > - Optimization via nloptr()
> > 

> > > > Hopefully, you?ll be able to run it locally with only the VGAM, foreign, and nloptr packages.
> > 

> > > > I?ll send the .R file along with the next email. A best attempt, anyway.
> > 

> > > > r/
> > > > Gregg
> > 

> > > > ?Oh, what fun it is!?
> > > > ?Alice, Alice?s Adventures in Wonderland by Lewis Carroll
> > 

> > > > On Thursday, April 24th, 2025 at 1:56 AM, Christofer Bogaso bogaso.christofer at gmail.com wrote:
> > 

> > > > > Hi Gregg,
> > 

> > > > > Many thanks for your detail feedback, those are really super helpful.
> > 

> > > > > I have ordered a copy of Agresti from our local library, however it
> > > > > appears that I would need to wait for a few days.
> > 

> > > > > Regrading my queries, it would be super helpful if we can build a
> > > > > optimization algo based on below criterias:
> > 

> > > > > 1. Whether you want the cutpoints fixed (and to what values), or if
> > > > > you want them estimated separately (with identifiability managed some
> > > > > other way); I would like to have cut-points directly estimated from
> > > > > the data
> > > > > 2. What your bounds on the coefficients are (lower/upper vectors), For
> > > > > this question, I am having different bounds for each of the
> > > > > coefficients. Therefore I would have a vector of lower points and
> > > > > other vector of upper points. However to start with let consider lower
> > > > > and upper bounds as lower = c(1, -1, 0) and upper = c(2, 1, 1) for the
> > > > > variables pared, public, and gpa. In my model, there would not be any
> > > > > Intercept, hence no question on its bounds
> > > > > 3. What value the sum of coefficients should equal (e.g., sum(?) = 1,
> > > > > or something else); Let the sum be 1.60
> > > > > 4. And whether you're working with the IDRE example data, or something
> > > > > else. My original data is actually residing in a computer without any
> > > > > access to the internet (for data security.) However we can start with
> > > > > any suitable data for this experiment, so one such data may be
> > > > > https://stats.idre.ucla.edu/stat/data/ologit.dta. However I am still
> > > > > exploring if there is any possibility to extract a randomized copy of
> > > > > that actual data, if I can - I will share once available
> > 

> > > > > Again, many thanks for your time and insights.
> > 

> > > > > Thanks and regards,
> > 

> > > > > On Wed, Apr 23, 2025 at 9:54?PM Gregg Powell g.a.powell at protonmail.com wrote:
> > 

> > > > > > Hello again Christofer,
> > > > > > Thanks for your thoughtful note ? I?m glad the outline was helpful. Apologies for the long delay getting back to you. Had to do a small bit of research?
> > 

> > > > > > Recommended Text on Ordinal Log-Likelihoods:
> > > > > > You're right that most online sources jump straight to code or canned functions. For a solid theoretical treatment of ordinal models and their likelihoods, consider:
> > > > > > "Categorical Data Analysis" by Alan Agresti
> > > > > > ? Especially Chapters 7 and 8 on ordinal logistic regression.
> > > > > > ? Covers proportional odds models, cumulative logits, adjacent-category logits, and the derivation of likelihood functions.
> > > > > > ? Provides not only equations but also intuition behind the model structure.
> > > > > > It?s a standard reference in the field and explains how to build log-likelihoods from first principles ? including how the cumulative probabilities arise and why the difference of CDFs represents a category-specific probability.
> > > > > > Also helpful:
> > > > > > "An Introduction to Categorical Data Analysis" by Agresti (2nd ed) ? A bit more accessible, and still covers the basics of ordinal models.
> > > > > > ________________________________________
> > 

> > > > > > If You Want to Proceed Practically:
> > > > > > In parallel with theory, if you'd like a working R example coded from scratch ? with:
> > > > > > ? a custom likelihood for an ordinal (cumulative logit) model,
> > > > > > ? fixed thresholds / no intercept,
> > > > > > ? coefficient bounds,
> > > > > > ? and a sum constraint on ?
> > 

> > > > > > I?d be happy to attempt that using nloptr() or constrOptim(). You?d be able to walk through it and tweak it as necessary (no guarantee that I will get it right ?)
> > 

> > > > > > Just let me know:
> > > > > > 1. Whether you want the cutpoints fixed (and to what values), or if you want them estimated separately (with identifiability managed some other way);
> > > > > > 2. What your bounds on the coefficients are (lower/upper vectors),
> > > > > > 3. What value the sum of coefficients should equal (e.g., sum(?) = 1, or something else);
> > > > > > 4. And whether you're working with the IDRE example data, or something else.
> > 

> > > > > > I can use the UCLA ologit.dta dataset as a basis if that's easiest to demo on, or if you have another dataset you?d prefer ? again, let me know.
> > 

> > > > > > All the best!
> > 

> > > > > > gregg
> > 

> > > > > > On Monday, April 21st, 2025 at 11:25 AM, Christofer Bogaso bogaso.christofer at gmail.com wrote:
> > 

> > > > > > > Hi Gregg,
> > 

> > > > > > > I am sincerely thankful for this workout.
> > 

> > > > > > > Could you please suggest any text book on how to create log-likelihood
> > > > > > > for an ordinal model like this? Most of my online search point me
> > > > > > > directly to some R function etc, but a theoretical discussion on this
> > > > > > > subject would be really helpful to construct the same.
> > 

> > > > > > > Thanks and regards,
> > 

> > > > > > > On Mon, Apr 21, 2025 at 9:55?PM Gregg Powell g.a.powell at protonmail.com wrote:
> > 

> > > > > > > > Christofer,
> > 

> > > > > > > > Given the constraints you mentioned?bounded parameters, no intercept, and a sum constraint?you're outside the capabilities of most off-the-shelf ordinal logistic regression functions in R like vglm or polr.
> > 

> > > > > > > > The most flexible recommendation at this point is to implement custom likelihood optimization using constrOptim() or nloptr::nloptr() with a manually coded log-likelihood function for the cumulative logit model.
> > 

> > > > > > > > Since you need:
> > 

> > > > > > > > Coefficient bounds (e.g., lb ? ? ? ub),
> > 

> > > > > > > > No intercept,
> > 

> > > > > > > > And a constraint like sum(?) = C,
> > 

> > > > > > > > ?you'll need to code your own objective function. Here's something of a high-level outline of the approach:
> > 

> > > > > > > > A. Model Setup
> > > > > > > > Let your design matrix X be n x p, and let Y take ordered values 1, 2, ..., K.
> > 

> > > > > > > > B. Parameters
> > > > > > > > Assume the thresholds (?_k) are fixed (or removed entirely), and you?re estimating only the coefficient vector ?. Your constraints are:
> > 

> > > > > > > > Box constraints: lb ? ? ? ub
> > 

> > > > > > > > Equality constraint: sum(?) = C
> > 

> > > > > > > > C. Likelihood
> > > > > > > > The probability of category k is given by:
> > 

> > > > > > > > P(Y = k) = logistic(?_k - X?) - logistic(?_{k-1} - X?)
> > 

> > > > > > > > Without intercepts, this becomes more like:
> > 

> > > > > > > > P(Y ? k) = 1 / (1 + exp(-(c_k - X?)))
> > 

> > > > > > > > ?where c_k are fixed thresholds.
> > 

> > > > > > > > Implementation using nloptr
> > > > > > > > This example shows a rough sketch using nloptr, which allows both equality and bound constraints:
> > 

> > > > > > > > > library(nloptr)
> > 

> > > > > > > > > # Custom negative log-likelihood function
> > > > > > > > > negLL <- function(beta, X, y, K, cutpoints) {
> > > > > > > > > eta <- X %*% beta
> > > > > > > > > loglik <- 0
> > > > > > > > > for (k in 1:K) {
> > > > > > > > > pk <- plogis(cutpoints[k] - eta) - plogis(cutpoints[k - 1] - eta)
> > > > > > > > > loglik <- loglik + sum(log(pk[y == k]))
> > > > > > > > > }
> > > > > > > > > return(-loglik)
> > > > > > > > > }
> > 

> > > > > > > > > # Constraint: sum(beta) = C
> > > > > > > > > sum_constraint <- function(beta, C) {
> > > > > > > > > return(sum(beta) - C)
> > > > > > > > > }
> > 

> > > > > > > > > # Define objective and constraint wrapper
> > > > > > > > > objective <- function(beta) negLL(beta, X, y, K, cutpoints)
> > > > > > > > > eq_constraint <- function(beta) sum_constraint(beta, C = 2) # example >C
> > 

> > > > > > > > > # Run optimization
> > > > > > > > > res <- nloptr(
> > > > > > > > > x0 = rep(0, ncol(X)),
> > > > > > > > > eval_f = objective,
> > > > > > > > > lb = lower_bounds,
> > > > > > > > > ub = upper_bounds,
> > > > > > > > > eval_g_eq = eq_constraint,
> > > > > > > > > opts = list(algorithm = "NLOPT_LD_SLSQP", xtol_rel = 1e-8)
> > > > > > > > > )
> > 

> > > > > > > > The next step would be writing the actual log-likelihood for your specific problem or verifying constraint implementation.
> > 

> > > > > > > > Manually coding the log-likelihood for an ordinal model is nontrivial... so a bit of a challenge :)
> > 

> > > > > > > > All the best,
> > > > > > > > gregg powell
> > > > > > > > Sierra Vista, AZ
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 603 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20250501/cd55a3bd/attachment.sig>

From jc2con|orte @end|ng |rom gm@||@com  Thu May  1 17:27:03 2025
From: jc2con|orte @end|ng |rom gm@||@com (Jorge Conrado Conforte)
Date: Thu, 1 May 2025 12:27:03 -0300
Subject: [R] R plot window in screen of computer
Message-ID: <CAEp62W1p3pS4QCDHaRtcP-u7O+7WJRi=m=afyarmcJ_PMOR+YQ@mail.gmail.com>

Hi,

I'm starting use R. I always use the IDL and Python, to visualize my plot
data and I use these commands:

IDL

window,xsize=500,ysize=500


Python

plt.figure(figsize=(12,9))

I would like to know if R has a similar command to plot my figure on the
computer screen.

Thanks,

Conrado

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  2 09:56:13 2025
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 2 May 2025 08:56:13 +0100
Subject: [R] R plot window in screen of computer
In-Reply-To: <CAEp62W1p3pS4QCDHaRtcP-u7O+7WJRi=m=afyarmcJ_PMOR+YQ@mail.gmail.com>
References: <CAEp62W1p3pS4QCDHaRtcP-u7O+7WJRi=m=afyarmcJ_PMOR+YQ@mail.gmail.com>
Message-ID: <dd77a0ba-6fa8-462a-a00a-f646e2f84e0a@sapo.pt>

?s 16:27 de 01/05/2025, Jorge Conrado Conforte escreveu:
> Hi,
> 
> I'm starting use R. I always use the IDL and Python, to visualize my plot
> data and I use these commands:
> 
> IDL
> 
> window,xsize=500,ysize=500
> 
> 
> Python
> 
> plt.figure(figsize=(12,9))
> 
> I would like to know if R has a similar command to plot my figure on the
> computer screen.
> 
> Thanks,
> 
> Conrado
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Hello,

Yes, R has similar commands (plural). Try


# documentation on graphics devices
help("Devices")
# what your build of R has available
capabilities()


# try two graphics devices
windows(12, 9)
plot(1:10)

X11(width = 12, height = 9)
plot(1:10)


Hope this helps,

Rui Barradas


-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com


From @m@||ep@||on @end|ng |rom proton@me  Sun May  4 17:57:52 2025
From: @m@||ep@||on @end|ng |rom proton@me (smallepsilon)
Date: Sun, 04 May 2025 15:57:52 +0000
Subject: [R] non-reproducible eigen() output with MKL
Message-ID: <yJIVt5GuVv6QIOWNQhDbauOpb5YggyqGD_oq3b7W8CWlZvQwXZb5FH3Hf8Y-XfhUcDK0RuMIYNdU93ompxJq_k8BQOhRjhCdlE8EZsZflEk=@proton.me>

I am using MKL with R 4.5.0 on Linux, and eigen() is producing different results with identical calls. Specifically, when I run the code below, I get "FALSE" from identical(). Just in case it had something to do with a random number generator, I put identical set.seed() calls immediately before each eigen() call, but that did not help. Has anyone seen this behavior before?

(When n is 5, the identical() call almost always returns "TRUE". As n increases, the proportion of FALSE results increases, and it is nearly always FALSE when n is 50.)

Jesse

***

n <- 50
set.seed(20250504)
Sigma <- rWishart(1, df = n, Sigma = diag(n))[,,1]
e1 <- eigen(Sigma)
e2 <- eigen(Sigma)
identical(e1, e2)


From pd@|gd @end|ng |rom gm@||@com  Sun May  4 19:27:41 2025
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Sun, 4 May 2025 19:27:41 +0200
Subject: [R] non-reproducible eigen() output with MKL
In-Reply-To: <yJIVt5GuVv6QIOWNQhDbauOpb5YggyqGD_oq3b7W8CWlZvQwXZb5FH3Hf8Y-XfhUcDK0RuMIYNdU93ompxJq_k8BQOhRjhCdlE8EZsZflEk=@proton.me>
References: <yJIVt5GuVv6QIOWNQhDbauOpb5YggyqGD_oq3b7W8CWlZvQwXZb5FH3Hf8Y-XfhUcDK0RuMIYNdU93ompxJq_k8BQOhRjhCdlE8EZsZflEk=@proton.me>
Message-ID: <171E8522-6EFC-49AC-9074-6D2287472CAC@gmail.com>

Have you looked more closely at the differences? Eigenvectors are only determined up to a sign change, so different platforms often give results that differ by sign. If you use a multitasking numerical library, the same can happen within platform because the exact sequence of computations differs between calls. 

You could check 

a) that e1$values and e2$values are the same
b) that the crossprod(e1$vectors, e2$vectors) is a diagonal matrix with 1 or -1 in the diagonal. (This might fail if you have eigenvalues that are almost identical, though.)

-pd

> On 4 May 2025, at 17.57, smallepsilon via R-help <r-help at r-project.org> wrote:
> 
> I am using MKL with R 4.5.0 on Linux, and eigen() is producing different results with identical calls. Specifically, when I run the code below, I get "FALSE" from identical(). Just in case it had something to do with a random number generator, I put identical set.seed() calls immediately before each eigen() call, but that did not help. Has anyone seen this behavior before?
> 
> (When n is 5, the identical() call almost always returns "TRUE". As n increases, the proportion of FALSE results increases, and it is nearly always FALSE when n is 50.)
> 
> Jesse
> 
> ***
> 
> n <- 50
> set.seed(20250504)
> Sigma <- rWishart(1, df = n, Sigma = diag(n))[,,1]
> e1 <- eigen(Sigma)
> e2 <- eigen(Sigma)
> identical(e1, e2)
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business SchoolSolbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From g@@@powe|| @end|ng |rom protonm@||@com  Sun May  4 22:27:24 2025
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Sun, 04 May 2025 20:27:24 +0000
Subject: [R] =?utf-8?q?Estimating_regression_with_constraints_in_model_co?=
 =?utf-8?q?efficients_-_Follow-up_on_Constrained_Ordinal_Model_=E2=80=94_O?=
 =?utf-8?q?ptimized_via_COBYLA?=
Message-ID: <zVdTUbGCSnS5yTeuSC8UtfaMZ-kvYNaD3w4CRdFeuTCuuNhPZN9rt7w11bDaFX-NAJDqwB2bEkTUWitaS-OkyYnxfA642F4Kp40ZVqryhPc=@protonmail.com>


Hello Christofer,

Just writing with a detailed follow-up. Attached is a script I was able to get running with a bit of work. I did not include the script in the ext of this email. It is only attached.

Optimization Progress
We were initially aiming to solve the dual-slope constrained ordinal model using nloptr's SLSQP algorithm (NLOPT_LD_SLSQP), since it supports:
? Box constraints (per-? lower/upper bounds)
? Equality constraints (sum(?) = 1.60 for each ? set)

However, SLSQP requires explicit gradient definitions for both the objective function and constraint Jacobian. While the documentation suggests that eval_grad_f = NULL should trigger finite-difference approximation, this did not work in my initial script ? I received repeated errors:
?A gradient for the objective function is needed by algorithm NLOPT_LD_SLSQP but was not supplied.?
Even when using wrapper functions with NA values for gradient placeholders, the optimizer failed to recognize them. 


After multiple diagnostic attempts and some research, I shifted to:
NLOPT_LN_COBYLA ? Derivative-Free Optimization
This algorithm supports:
? Equality constraints (sum of ?? and ?? = 1.60)
? Bound constraints (per-element ? limits)

Also - this does not require gradients. This made it the best practical option.

Results
The optimizer successfully converged (status: NLOPT_XTOL_REACHED) after 202 iterations.

Estimated coefficients:
? ?? (for P(Y ? 1)): c(1.0, -0.4, 1.0)
? ?? (for P(Y ? 2)): c(1.0, 0.6, 3.25e-19)

Interpretation:
? Both ? vectors satisfy their respective sum constraint (? 1.60).
? All elements are within their specified bounds.
? The third coefficient for ?? (gpa) hits its lower bound (0), suggesting minimal influence on the second cumulative probability cutoff.
? The model does not estimate cutpoints/intercepts explicitly. Instead, the two separate slope vectors approximate this ordinal structure.
This confirms your structure ? estimating distinct slopes per class boundary without intercepts ? is now functioning with full constraints enforced.

________________________________________
Also, we could take this a step further - Let me know if you'd be interested in:
? A function to plot fitted probabilities per class (based on the estimated ?? and ??),
? A summary write-up of this implementation stage (as a formal hand-off or documentation),
? A switch to alabama::augLag(), which allows both equality and bound constraints and uses augmented Lagrangian methods ? possibly offering improved precision with gradient-based control.

Happy to assist with any of those, as time permits - perhaps next week.

r/
Gregg











On Thursday, May 1st, 2025 at 1:01 PM, Gregg Powell via R-help <r-help at r-project.org> wrote:

> 

> 

> Hello Christofer,
> 

> That?s good information. Thanks for the precision.
> Think I can use this now to work on a script:
> 

> Based on what you provided:
> 1. Two distinct sets of coefficients, ?? and ??, each associated with the logits for:
> ? P(Y?1)P(Y ? 1)P(Y?1)
> ? P(Y?2)P(Y ? 2)P(Y?2)
> 

> 2. Separate sum constraints:
> ? ??(1)=1.60 \ sum ?^{(1)} = 1.60??(1)=1.60
> ? ??(2)=1.60 \ sum ?^{(2)} = 1.60??(2)=1.60
> 

> 3. Element-wise bounds applied to each ? set:
> ? lower = c(1, -1, 0)
> ? upper = c(2, 1, 1)
> 

> Planning to wrap this into an updated nloptr optimization routine. I?ll work on a .R script as time permits ? if all goes well, you should be able to run it as provided.
> 

> r/
> Gregg
> 

> 

> 

> 

> 

> On Wednesday, April 30th, 2025 at 3:23 AM, Christofer Bogaso bogaso.christofer at gmail.com wrote:
> 

> > Hi Gregg,
> 

> > Below I try to address
> 

> > 1) The sum constraint would apply for each set ?? and ?? i.e. sum(??)
> > = sum(??) = 1.60
> > 2) Just like 1) the lower and upper bounds will be applied for
> > individual set i.e. individual elements of ?? are subject to lower =
> > c(1, -1, 0) and upper = c(2, 1, 1) and individual elements of ?? are
> > subject to lower = c(1, -1, 0) and upper = c(2, 1, 1)
> 

> > I hope that I am able to answer your questions. Please let me know if
> > further information is required.
> 

> > Thanks and regards,
> 

> > On Wed, Apr 30, 2025 at 4:22?AM Gregg Powell g.a.powell at protonmail.com wrote:
> 

> > > Hello again Christofer,
> > > This clarification changes the model structure somewhat significantly -it shifts us from a standard cumulative logit model with proportional odds to a non-parallel cumulative logit model, where each threshold has its own set of ? coefficients. At least, that is now my understanding.
> > > So, instead of a single ? vector shared across all class boundaries, you're now specifying:
> > > ? One set of coefficients ?(1)?^{(1)}?(1) for the logit of P(Y?1)P(Y ? 1)P(Y?1),
> > > ? A second, distinct set ?(2)?^{(2)}?(2) for P(Y?2)P(Y ? 2)P(Y?2),
> > > ? And no intercepts, meaning the threshold-specific slope vectors must carry all the signal.
> 

> > > So, we can adjust the log-likelihood accordingly:
> > > P(Y=1)=logistic(X?(1))P(Y=2)=logistic(X?(2))?logistic(X?(1))P(Y=3)=1?logistic(X?(2))P(Y = 1) = logistic(X?^{(1)}) P(Y = 2) = logistic(X?^{(2)}) - logistic(X?^{(1)}) P(Y = 3) = 1 - logistic(X?^{(2)}) P(Y=1)=logistic(X?(1))P(Y=2)=logistic(X?(2))?logistic(X?(1))P(Y=3)=1?logistic(X?(2))
> 

> > > Before I attempt a revised script, can you confirm:
> > > 1. Should the sum constraint (e.g., sum(?) = 1.60) apply to:
> > > ? Only ???
> > > ? Only ???
> > > ? Or the sum of all 6 coefficients (?? and ?? combined)?
> 

> > > 2. Do you want to apply separate lower/upper bounds to each of the six ? coefficients (and if so, what are they for each)?
> 

> > > Once I understand this last part better, I?ll see about working on a version that fits this updated structure and constraint logic.
> > > As always ? no promises.
> > > r/
> > > Gregg Powell
> > > Sierra Vista, AZ
> 

> > > On Tuesday, April 29th, 2025 at 1:51 PM, Christofer Bogaso bogaso.christofer at gmail.com wrote:
> 

> > > > Hi Gregg,
> 

> > > > I am just wondering if you get any time to think about this problem.
> 

> > > > As I understand, typically for this type of problem, we have different
> > > > intercepts for different classes, while slope (beta) coefficients
> > > > remain the same across classes.
> 

> > > > But in my case, since we do not have any intercept term, the slope
> > > > coefficients need to be estimated separately for different classes.
> > > > Therefore, since we have 3 classes in the response variable (i.e.
> > > > 'apply'), there will be 3 different sets of beta-coefficients for the
> > > > independent variables.
> 

> > > > Under this situation, I wonder how I can create the likelihood
> > > > function subject to all applicable constraints.
> 

> > > > Any insight would be highly appreciated.
> 

> > > > Thanks and regards,
> 

> > > > On Fri, Apr 25, 2025 at 12:31?AM Gregg Powell g.a.powell at protonmail.com wrote:
> 

> > > > > Christofer,
> > > > > That was a detailed follow-up ? you clarified the requirements precisely enough providing a position to proceed from...
> 

> > > > > To implement this, a constrained optimization procedure to estimate an ordinal logistic regression model is needed (cumulative logit), based on:
> 

> > > > > 1. Estimated Cutpoints
> > > > > ? No intercept in the linear predictor
> > > > > ? Cutpoints (thresholds) will be estimated directly from the data
> 

> > > > > 2. Coefficient Constraints
> > > > > ? Box constraints on each coefficient
> > > > > ? For now:
> > > > > lower = c(1, -1, 0)
> > > > > upper = c(2, 1, 1)
> > > > > ? These apply to: pared, public, gpa (in that order)
> 

> > > > > 3. Sum Constraint
> > > > > ? The sum of coefficients must equal 1.60
> 

> > > > > 4. Data to use...
> > > > > ? Use the IDRE ologit.dta dataset from UCLA (for now)
> 

> > > > > Technical Approach
> 

> > > > > ? Attempt to write a custom negative log-likelihood function using the cumulative logit formulation:
> 

> > > > > P(Y?k?X)=11+exp?[?(?k?X?)]P(Y \leq k \mid X) = \frac{1}{1 + \exp[-(\theta_k - X\beta)]}
> 

> > > > > and derive P(Y=k)P(Y = k) from adjacent differences of these.
> 

> > > > > ? Cutpoints ?k\theta_k will be estimated as separate parameters, with constraints to ensure they?re strictly increasing for identifiability.
> 

> > > > > ? The optimization will use nloptr::nloptr(), which supports:
> > > > > - Lower/upper bounds (box constraints)
> > > > > - Equality constraints (for sum of ?)
> > > > > - Nonlinear inequality constraints (to keep cutpoints ordered)
> 

> > > > > I?ll have some time later - in the next day or two to attempt a script with:
> > > > > - Custom negative log-likelihood
> > > > > - Parameter vector split into ? and cutpoints
> > > > > - Constraint functions: sum(?) = 1.60 and increasing cutpoints
> > > > > - Optimization via nloptr()
> 

> > > > > Hopefully, you?ll be able to run it locally with only the VGAM, foreign, and nloptr packages.
> 

> > > > > I?ll send the .R file along with the next email. A best attempt, anyway.
> 

> > > > > r/
> > > > > Gregg
> 

> > > > > ?Oh, what fun it is!?
> > > > > ?Alice, Alice?s Adventures in Wonderland by Lewis Carroll
> 

> > > > > On Thursday, April 24th, 2025 at 1:56 AM, Christofer Bogaso bogaso.christofer at gmail.com wrote:
> 

> > > > > > Hi Gregg,
> 

> > > > > > Many thanks for your detail feedback, those are really super helpful.
> 

> > > > > > I have ordered a copy of Agresti from our local library, however it
> > > > > > appears that I would need to wait for a few days.
> 

> > > > > > Regrading my queries, it would be super helpful if we can build a
> > > > > > optimization algo based on below criterias:
> 

> > > > > > 1. Whether you want the cutpoints fixed (and to what values), or if
> > > > > > you want them estimated separately (with identifiability managed some
> > > > > > other way); I would like to have cut-points directly estimated from
> > > > > > the data
> > > > > > 2. What your bounds on the coefficients are (lower/upper vectors), For
> > > > > > this question, I am having different bounds for each of the
> > > > > > coefficients. Therefore I would have a vector of lower points and
> > > > > > other vector of upper points. However to start with let consider lower
> > > > > > and upper bounds as lower = c(1, -1, 0) and upper = c(2, 1, 1) for the
> > > > > > variables pared, public, and gpa. In my model, there would not be any
> > > > > > Intercept, hence no question on its bounds
> > > > > > 3. What value the sum of coefficients should equal (e.g., sum(?) = 1,
> > > > > > or something else); Let the sum be 1.60
> > > > > > 4. And whether you're working with the IDRE example data, or something
> > > > > > else. My original data is actually residing in a computer without any
> > > > > > access to the internet (for data security.) However we can start with
> > > > > > any suitable data for this experiment, so one such data may be
> > > > > > https://stats.idre.ucla.edu/stat/data/ologit.dta. However I am still
> > > > > > exploring if there is any possibility to extract a randomized copy of
> > > > > > that actual data, if I can - I will share once available
> 

> > > > > > Again, many thanks for your time and insights.
> 

> > > > > > Thanks and regards,
> 

> > > > > > On Wed, Apr 23, 2025 at 9:54?PM Gregg Powell g.a.powell at protonmail.com wrote:
> 

> > > > > > > Hello again Christofer,
> > > > > > > Thanks for your thoughtful note ? I?m glad the outline was helpful. Apologies for the long delay getting back to you. Had to do a small bit of research?
> 

> > > > > > > Recommended Text on Ordinal Log-Likelihoods:
> > > > > > > You're right that most online sources jump straight to code or canned functions. For a solid theoretical treatment of ordinal models and their likelihoods, consider:
> > > > > > > "Categorical Data Analysis" by Alan Agresti
> > > > > > > ? Especially Chapters 7 and 8 on ordinal logistic regression.
> > > > > > > ? Covers proportional odds models, cumulative logits, adjacent-category logits, and the derivation of likelihood functions.
> > > > > > > ? Provides not only equations but also intuition behind the model structure.
> > > > > > > It?s a standard reference in the field and explains how to build log-likelihoods from first principles ? including how the cumulative probabilities arise and why the difference of CDFs represents a category-specific probability.
> > > > > > > Also helpful:
> > > > > > > "An Introduction to Categorical Data Analysis" by Agresti (2nd ed) ? A bit more accessible, and still covers the basics of ordinal models.
> > > > > > > ________________________________________
> 

> > > > > > > If You Want to Proceed Practically:
> > > > > > > In parallel with theory, if you'd like a working R example coded from scratch ? with:
> > > > > > > ? a custom likelihood for an ordinal (cumulative logit) model,
> > > > > > > ? fixed thresholds / no intercept,
> > > > > > > ? coefficient bounds,
> > > > > > > ? and a sum constraint on ?
> 

> > > > > > > I?d be happy to attempt that using nloptr() or constrOptim(). You?d be able to walk through it and tweak it as necessary (no guarantee that I will get it right ?)
> 

> > > > > > > Just let me know:
> > > > > > > 1. Whether you want the cutpoints fixed (and to what values), or if you want them estimated separately (with identifiability managed some other way);
> > > > > > > 2. What your bounds on the coefficients are (lower/upper vectors),
> > > > > > > 3. What value the sum of coefficients should equal (e.g., sum(?) = 1, or something else);
> > > > > > > 4. And whether you're working with the IDRE example data, or something else.
> 

> > > > > > > I can use the UCLA ologit.dta dataset as a basis if that's easiest to demo on, or if you have another dataset you?d prefer ? again, let me know.
> 

> > > > > > > All the best!
> 

> > > > > > > gregg
> 

> > > > > > > On Monday, April 21st, 2025 at 11:25 AM, Christofer Bogaso bogaso.christofer at gmail.com wrote:
> 

> > > > > > > > Hi Gregg,
> 

> > > > > > > > I am sincerely thankful for this workout.
> 

> > > > > > > > Could you please suggest any text book on how to create log-likelihood
> > > > > > > > for an ordinal model like this? Most of my online search point me
> > > > > > > > directly to some R function etc, but a theoretical discussion on this
> > > > > > > > subject would be really helpful to construct the same.
> 

> > > > > > > > Thanks and regards,
> 

> > > > > > > > On Mon, Apr 21, 2025 at 9:55?PM Gregg Powell g.a.powell at protonmail.com wrote:
> 

> > > > > > > > > Christofer,
> 

> > > > > > > > > Given the constraints you mentioned?bounded parameters, no intercept, and a sum constraint?you're outside the capabilities of most off-the-shelf ordinal logistic regression functions in R like vglm or polr.
> 

> > > > > > > > > The most flexible recommendation at this point is to implement custom likelihood optimization using constrOptim() or nloptr::nloptr() with a manually coded log-likelihood function for the cumulative logit model.
> 

> > > > > > > > > Since you need:
> 

> > > > > > > > > Coefficient bounds (e.g., lb ? ? ? ub),
> 

> > > > > > > > > No intercept,
> 

> > > > > > > > > And a constraint like sum(?) = C,
> 

> > > > > > > > > ?you'll need to code your own objective function. Here's something of a high-level outline of the approach:
> 

> > > > > > > > > A. Model Setup
> > > > > > > > > Let your design matrix X be n x p, and let Y take ordered values 1, 2, ..., K.
> 

> > > > > > > > > B. Parameters
> > > > > > > > > Assume the thresholds (?_k) are fixed (or removed entirely), and you?re estimating only the coefficient vector ?. Your constraints are:
> 

> > > > > > > > > Box constraints: lb ? ? ? ub
> 

> > > > > > > > > Equality constraint: sum(?) = C
> 

> > > > > > > > > C. Likelihood
> > > > > > > > > The probability of category k is given by:
> 

> > > > > > > > > P(Y = k) = logistic(?_k - X?) - logistic(?_{k-1} - X?)
> 

> > > > > > > > > Without intercepts, this becomes more like:
> 

> > > > > > > > > P(Y ? k) = 1 / (1 + exp(-(c_k - X?)))
> 

> > > > > > > > > ?where c_k are fixed thresholds.
> 

> > > > > > > > > Implementation using nloptr
> > > > > > > > > This example shows a rough sketch using nloptr, which allows both equality and bound constraints:
> 

> > > > > > > > > > library(nloptr)
> 

> > > > > > > > > > # Custom negative log-likelihood function
> > > > > > > > > > negLL <- function(beta, X, y, K, cutpoints) {
> > > > > > > > > > eta <- X %*% beta
> > > > > > > > > > loglik <- 0
> > > > > > > > > > for (k in 1:K) {
> > > > > > > > > > pk <- plogis(cutpoints[k] - eta) - plogis(cutpoints[k - 1] - eta)
> > > > > > > > > > loglik <- loglik + sum(log(pk[y == k]))
> > > > > > > > > > }
> > > > > > > > > > return(-loglik)
> > > > > > > > > > }
> 

> > > > > > > > > > # Constraint: sum(beta) = C
> > > > > > > > > > sum_constraint <- function(beta, C) {
> > > > > > > > > > return(sum(beta) - C)
> > > > > > > > > > }
> 

> > > > > > > > > > # Define objective and constraint wrapper
> > > > > > > > > > objective <- function(beta) negLL(beta, X, y, K, cutpoints)
> > > > > > > > > > eq_constraint <- function(beta) sum_constraint(beta, C = 2) # example >C
> 

> > > > > > > > > > # Run optimization
> > > > > > > > > > res <- nloptr(
> > > > > > > > > > x0 = rep(0, ncol(X)),
> > > > > > > > > > eval_f = objective,
> > > > > > > > > > lb = lower_bounds,
> > > > > > > > > > ub = upper_bounds,
> > > > > > > > > > eval_g_eq = eq_constraint,
> > > > > > > > > > opts = list(algorithm = "NLOPT_LD_SLSQP", xtol_rel = 1e-8)
> > > > > > > > > > )
> 

> > > > > > > > > The next step would be writing the actual log-likelihood for your specific problem or verifying constraint implementation.
> 

> > > > > > > > > Manually coding the log-likelihood for an ordinal model is nontrivial... so a bit of a challenge :)
> 

> > > > > > > > > All the best,
> > > > > > > > > gregg powell
> > > > > > > > > Sierra Vista, AZ______________________________________________
> 

> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 603 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20250504/908ae87c/attachment.sig>

From @m@||ep@||on @end|ng |rom proton@me  Sun May  4 20:11:57 2025
From: @m@||ep@||on @end|ng |rom proton@me (smallepsilon)
Date: Sun, 04 May 2025 18:11:57 +0000
Subject: [R] non-reproducible eigen() output with MKL
In-Reply-To: <171E8522-6EFC-49AC-9074-6D2287472CAC@gmail.com>
References: <yJIVt5GuVv6QIOWNQhDbauOpb5YggyqGD_oq3b7W8CWlZvQwXZb5FH3Hf8Y-XfhUcDK0RuMIYNdU93ompxJq_k8BQOhRjhCdlE8EZsZflEk=@proton.me>
 <171E8522-6EFC-49AC-9074-6D2287472CAC@gmail.com>
Message-ID: <c9SJxWBERVWAB5nRVSubq_j9_xqVkBu19T-ldNed-tQOgeY5mbYM-HL4Jda4HTTSfyu8-XlsAnEoy06qlPK7R0Z1GuirC-kzkrJidaehec8=@proton.me>

Peter,

The eigenvalues are not identical(), but are all.equal(). When n is 20, the crossproduct is (numerically) a diagonal matrix with +-1 on the diagonal. When n is 50, this is not the case, but that could be an issue of nearly identical eigenvalues.

Is there no way within R to require that the sequence of operations be the same for identical calls? The problem arose originally in a package test in which I wanted to verify that two ways of specifying something led to the execution of exactly the same calculations. The best proxy for this seemed to be to use identical() on the outputs, but if the same line of code (that should not involve an RNG) can lead to different results, this approach is doomed, yes? It is not absolutely critical that the outputs be identical(), but it would be much more reassuring than all.equal().

Thanks,
Jesse




On Sunday, May 4th, 2025 at 12:27 PM, peter dalgaard <pdalgd at gmail.com> wrote:

> 
> 
> Have you looked more closely at the differences? Eigenvectors are only determined up to a sign change, so different platforms often give results that differ by sign. If you use a multitasking numerical library, the same can happen within platform because the exact sequence of computations differs between calls.
> 
> You could check
> 
> a) that e1$values and e2$values are the same
> b) that the crossprod(e1$vectors, e2$vectors) is a diagonal matrix with 1 or -1 in the diagonal. (This might fail if you have eigenvalues that are almost identical, though.)
> 
> -pd
> 
> > On 4 May 2025, at 17.57, smallepsilon via R-help r-help at r-project.org wrote:
> > 
> > I am using MKL with R 4.5.0 on Linux, and eigen() is producing different results with identical calls. Specifically, when I run the code below, I get "FALSE" from identical(). Just in case it had something to do with a random number generator, I put identical set.seed() calls immediately before each eigen() call, but that did not help. Has anyone seen this behavior before?
> > 
> > (When n is 5, the identical() call almost always returns "TRUE". As n increases, the proportion of FALSE results increases, and it is nearly always FALSE when n is 50.)
> > 
> > Jesse
> > 
> > ***
> > 
> > n <- 50
> > set.seed(20250504)
> > Sigma <- rWishart(1, df = n, Sigma = diag(n))[,,1]
> > e1 <- eigen(Sigma)
> > e2 <- eigen(Sigma)
> > identical(e1, e2)
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business SchoolSolbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com


From er|cjberger @end|ng |rom gm@||@com  Mon May  5 08:33:49 2025
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Mon, 5 May 2025 09:33:49 +0300
Subject: [R] non-reproducible eigen() output with MKL
In-Reply-To: <171E8522-6EFC-49AC-9074-6D2287472CAC@gmail.com>
References: <yJIVt5GuVv6QIOWNQhDbauOpb5YggyqGD_oq3b7W8CWlZvQwXZb5FH3Hf8Y-XfhUcDK0RuMIYNdU93ompxJq_k8BQOhRjhCdlE8EZsZflEk=@proton.me>
 <171E8522-6EFC-49AC-9074-6D2287472CAC@gmail.com>
Message-ID: <CAGgJW767hq7TTr1qq8K=vDgifj=GGtq__17w5YmZ6Y7X9k4S4g@mail.gmail.com>

Just an FYI that on my system identical(e1,e2) returns TRUE.
Here is the output of sessionInfo() on my system:

> sessionInfo()
R version 4.4.2 (2024-10-31)
Platform: x86_64-pc-linux-gnu
Running under: Ubuntu 22.04.5 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;
 LAPACK version 3.10.0

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

time zone: Asia/Jerusalem
tzcode source: system (glibc)

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base

loaded via a namespace (and not attached):
[1] compiler_4.4.2 tools_4.4.2    bspm_0.5.7

HTH,
Eric

On Sun, May 4, 2025 at 8:28?PM peter dalgaard <pdalgd at gmail.com> wrote:

> Have you looked more closely at the differences? Eigenvectors are only
> determined up to a sign change, so different platforms often give results
> that differ by sign. If you use a multitasking numerical library, the same
> can happen within platform because the exact sequence of computations
> differs between calls.
>
> You could check
>
> a) that e1$values and e2$values are the same
> b) that the crossprod(e1$vectors, e2$vectors) is a diagonal matrix with 1
> or -1 in the diagonal. (This might fail if you have eigenvalues that are
> almost identical, though.)
>
> -pd
>
> > On 4 May 2025, at 17.57, smallepsilon via R-help <r-help at r-project.org>
> wrote:
> >
> > I am using MKL with R 4.5.0 on Linux, and eigen() is producing different
> results with identical calls. Specifically, when I run the code below, I
> get "FALSE" from identical(). Just in case it had something to do with a
> random number generator, I put identical set.seed() calls immediately
> before each eigen() call, but that did not help. Has anyone seen this
> behavior before?
> >
> > (When n is 5, the identical() call almost always returns "TRUE". As n
> increases, the proportion of FALSE results increases, and it is nearly
> always FALSE when n is 50.)
> >
> > Jesse
> >
> > ***
> >
> > n <- 50
> > set.seed(20250504)
> > Sigma <- rWishart(1, df = n, Sigma = diag(n))[,,1]
> > e1 <- eigen(Sigma)
> > e2 <- eigen(Sigma)
> > identical(e1, e2)
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business SchoolSolbjerg Plads 3, 2000
> Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon May  5 10:50:04 2025
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 5 May 2025 10:50:04 +0200
Subject: [R] non-reproducible eigen() output with MKL
In-Reply-To: <c9SJxWBERVWAB5nRVSubq_j9_xqVkBu19T-ldNed-tQOgeY5mbYM-HL4Jda4HTTSfyu8-XlsAnEoy06qlPK7R0Z1GuirC-kzkrJidaehec8=@proton.me>
References: <yJIVt5GuVv6QIOWNQhDbauOpb5YggyqGD_oq3b7W8CWlZvQwXZb5FH3Hf8Y-XfhUcDK0RuMIYNdU93ompxJq_k8BQOhRjhCdlE8EZsZflEk=@proton.me>
 <171E8522-6EFC-49AC-9074-6D2287472CAC@gmail.com>
 <c9SJxWBERVWAB5nRVSubq_j9_xqVkBu19T-ldNed-tQOgeY5mbYM-HL4Jda4HTTSfyu8-XlsAnEoy06qlPK7R0Z1GuirC-kzkrJidaehec8=@proton.me>
Message-ID: <26648.31676.116927.750944@stat.math.ethz.ch>


>>>>> smallepsilon via R-help 
>>>>>     on Sun, 04 May 2025 18:11:57 +0000 writes:

    > Peter, The eigenvalues are not identical(), but are
    > all.equal(). When n is 20, the crossproduct is
    > (numerically) a diagonal matrix with +-1 on the
    > diagonal. When n is 50, this is not the case, but that
    > could be an issue of nearly identical eigenvalues.

    > Is there no way within R to require that the sequence of
    > operations be the same for identical calls? 

As Peter Dalgaard mentioned, you have the problem of underlying
system libraries that try to be fast and hence parallelize
computations, notably in this (and man similar calls), MKL uses
parallelized BLAS and/or LAPACK .. and that's what eigen() in R
(and Julia, python, matlab, ..) all use.

And parallelization is a killer of (strict / bit-level)
reproducibility, as you have just experienced.

If you know how to tell your OS / that you want only one
parallel "thread" / process / ...
you get back to reproducible (and slower) computations.


    > The problem arose originally in a package test in which I wanted to
    > verify that two ways of specifying something led to the
    > execution of exactly the same calculations. The best proxy
    > for this seemed to be to use identical() on the outputs,
    > but if the same line of code (that should not involve an
    > RNG) can lead to different results, this approach is
    > doomed, yes? It is not absolutely critical that the
    > outputs be identical(), but it would be much more
    > reassuring than all.equal().

I understand and agree.

When I first became aware of the irreprodicibility of parallel
computations, only a few years ago, I was quite shocked and deillusionized..

    > Thanks, Jesse


Martin Maechler
ETH Zurich   and  R Core team




    > On Sunday, May 4th, 2025 at 12:27 PM, peter dalgaard
    > <pdalgd at gmail.com> wrote:

    >> 
    >> 
    >> Have you looked more closely at the differences?
    >> Eigenvectors are only determined up to a sign change, so
    >> different platforms often give results that differ by
    >> sign. If you use a multitasking numerical library, the
    >> same can happen within platform because the exact
    >> sequence of computations differs between calls.
    >> 
    >> You could check
    >> 
    >> a) that e1$values and e2$values are the same b) that the
    >> crossprod(e1$vectors, e2$vectors) is a diagonal matrix
    >> with 1 or -1 in the diagonal. (This might fail if you
    >> have eigenvalues that are almost identical, though.)
    >> 
    >> -pd
    >> 
    >> > On 4 May 2025, at 17.57, smallepsilon via R-help
    >> r-help at r-project.org wrote:
    >> > 
    >> > I am using MKL with R 4.5.0 on Linux, and eigen() is
    >> producing different results with identical
    >> calls. Specifically, when I run the code below, I get
    >> "FALSE" from identical(). Just in case it had something
    >> to do with a random number generator, I put identical
    >> set.seed() calls immediately before each eigen() call,
    >> but that did not help. Has anyone seen this behavior
    >> before?
    >> > 
    >> > (When n is 5, the identical() call almost always
    >> returns "TRUE". As n increases, the proportion of FALSE
    >> results increases, and it is nearly always FALSE when n
    >> is 50.)
    >> > 
    >> > Jesse
    >> > 
    >> > ***
    >> > 
    >> > n <- 50 > set.seed(20250504) > Sigma <- rWishart(1, df
    >> = n, Sigma = diag(n))[,,1] > e1 <- eigen(Sigma) > e2 <-
    >> eigen(Sigma) > identical(e1, e2)
    >> > 
    >> > ______________________________________________ >
    >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >> more, see > https://stat.ethz.ch/mailman/listinfo/r-help
    >> > PLEASE do read the posting guide
    >> https://www.R-project.org/posting-guide.html > and
    >> provide commented, minimal, self-contained, reproducible
    >> code.
    >> 
    >> 
    >> --
    >> Peter Dalgaard, Professor, Center for Statistics,
    >> Copenhagen Business SchoolSolbjerg Plads 3, 2000
    >> Frederiksberg, Denmark Phone: (+45)38153501 Office: A
    >> 4.23 Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > https://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Mon May  5 11:25:05 2025
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (NP))
Date: Mon, 5 May 2025 09:25:05 +0000
Subject: [R] non-reproducible eigen() output with MKL
In-Reply-To: <26648.31676.116927.750944@stat.math.ethz.ch>
References: <yJIVt5GuVv6QIOWNQhDbauOpb5YggyqGD_oq3b7W8CWlZvQwXZb5FH3Hf8Y-XfhUcDK0RuMIYNdU93ompxJq_k8BQOhRjhCdlE8EZsZflEk=@proton.me>
 <171E8522-6EFC-49AC-9074-6D2287472CAC@gmail.com>
 <c9SJxWBERVWAB5nRVSubq_j9_xqVkBu19T-ldNed-tQOgeY5mbYM-HL4Jda4HTTSfyu8-XlsAnEoy06qlPK7R0Z1GuirC-kzkrJidaehec8=@proton.me>
 <26648.31676.116927.750944@stat.math.ethz.ch>
Message-ID: <AS8PR08MB9193F68400D42AA488D029588B8E2@AS8PR08MB9193.eurprd08.prod.outlook.com>

A relevant thread from a few years ago where this was discussed:

https://www.stat.math.ethz.ch/pipermail/r-help/2023-August/477904.html

I generally use:

export OPENBLAS_NUM_THREADS=1
export MKL_NUM_THREADS=1

since in my experience the biggest performance gains come from switching to OpenBLAS / MKL in the first place. Using their multithreading capabilities tends to yield diminishing gains in comparison.

For MKL, you could try:

export MKL_THREADING_LAYER=GNU

or

export MKL_THREADING_LAYER=sequential

when using multiple threads to see if this avoids the issue.

But if you are using explicit parallelization (as I often do), then you would want to avoid the multithreading in the math libs anyway.

Best,
Wolfgang

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Martin Maechler
> Sent: Monday, May 5, 2025 10:50
> To: smallepsilon <smallepsilon at proton.me>
> Cc: r-help at r-project.org; peter dalgaard <pdalgd at gmail.com>
> Subject: Re: [R] non-reproducible eigen() output with MKL
>
> >>>>> smallepsilon via R-help
> >>>>>     on Sun, 04 May 2025 18:11:57 +0000 writes:
>
>     > Peter, The eigenvalues are not identical(), but are
>     > all.equal(). When n is 20, the crossproduct is
>     > (numerically) a diagonal matrix with +-1 on the
>     > diagonal. When n is 50, this is not the case, but that
>     > could be an issue of nearly identical eigenvalues.
>
>     > Is there no way within R to require that the sequence of
>     > operations be the same for identical calls?
>
> As Peter Dalgaard mentioned, you have the problem of underlying
> system libraries that try to be fast and hence parallelize
> computations, notably in this (and man similar calls), MKL uses
> parallelized BLAS and/or LAPACK .. and that's what eigen() in R
> (and Julia, python, matlab, ..) all use.
>
> And parallelization is a killer of (strict / bit-level)
> reproducibility, as you have just experienced.
>
> If you know how to tell your OS / that you want only one
> parallel "thread" / process / ...
> you get back to reproducible (and slower) computations.
>
>     > The problem arose originally in a package test in which I wanted to
>     > verify that two ways of specifying something led to the
>     > execution of exactly the same calculations. The best proxy
>     > for this seemed to be to use identical() on the outputs,
>     > but if the same line of code (that should not involve an
>     > RNG) can lead to different results, this approach is
>     > doomed, yes? It is not absolutely critical that the
>     > outputs be identical(), but it would be much more
>     > reassuring than all.equal().
>
> I understand and agree.
>
> When I first became aware of the irreprodicibility of parallel
> computations, only a few years ago, I was quite shocked and deillusionized..
>
>     > Thanks, Jesse
>
> Martin Maechler
> ETH Zurich   and  R Core team
>
>     > On Sunday, May 4th, 2025 at 12:27 PM, peter dalgaard
>     > <pdalgd at gmail.com> wrote:
>     >>
>     >>
>     >> Have you looked more closely at the differences?
>     >> Eigenvectors are only determined up to a sign change, so
>     >> different platforms often give results that differ by
>     >> sign. If you use a multitasking numerical library, the
>     >> same can happen within platform because the exact
>     >> sequence of computations differs between calls.
>     >>
>     >> You could check
>     >>
>     >> a) that e1$values and e2$values are the same b) that the
>     >> crossprod(e1$vectors, e2$vectors) is a diagonal matrix
>     >> with 1 or -1 in the diagonal. (This might fail if you
>     >> have eigenvalues that are almost identical, though.)
>     >>
>     >> -pd
>     >>
>     >> > On 4 May 2025, at 17.57, smallepsilon via R-help
>     >> r-help at r-project.org wrote:
>     >> >
>     >> > I am using MKL with R 4.5.0 on Linux, and eigen() is
>     >> producing different results with identical
>     >> calls. Specifically, when I run the code below, I get
>     >> "FALSE" from identical(). Just in case it had something
>     >> to do with a random number generator, I put identical
>     >> set.seed() calls immediately before each eigen() call,
>     >> but that did not help. Has anyone seen this behavior
>     >> before?
>     >> >
>     >> > (When n is 5, the identical() call almost always
>     >> returns "TRUE". As n increases, the proportion of FALSE
>     >> results increases, and it is nearly always FALSE when n
>     >> is 50.)
>     >> >
>     >> > Jesse
>     >> >
>     >> > ***
>     >> >
>     >> > n <- 50 > set.seed(20250504) > Sigma <- rWishart(1, df
>     >> = n, Sigma = diag(n))[,,1] > e1 <- eigen(Sigma) > e2 <-
>     >> eigen(Sigma) > identical(e1, e2)
>     >>
>     >> --
>     >> Peter Dalgaard, Professor, Center for Statistics,
>     >> Copenhagen Business SchoolSolbjerg Plads 3, 2000
>     >> Frederiksberg, Denmark Phone: (+45)38153501 Office: A
>     >> 4.23 Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com


From kev|n @end|ng |rom zembower@org  Mon May  5 17:17:17 2025
From: kev|n @end|ng |rom zembower@org (=?UTF-8?Q?Kevin_Zembower?=)
Date: Mon, 5 May 2025 15:17:17 +0000
Subject: [R] OT: A philosophical question about statistics
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
Message-ID: <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>

I marked this posting as Off Topic because it doesn?t specifically
apply to R and Statistics, but is rather a general question about
statistics and the teaching of statistics. If this is annoying to you,
I apologize.

As I wrap up my work in my beginning statistics course, I?d like to ask
a philosophical question regarding statistics.

In my course, we?ve learned two different ways to solve statistical
problems: simulations, using bootstraps and randomized distributions,
and theoretical methods, using Normal (z) and t-distributions. We?ve
learned that both systems solve all the questions we?ve asked of them,
and that both give comparable answers. Out of six chapters that we?ve
studied in our textbook, the first four only used simulation methods.
Only the last two used theoretical methods.

My questions are:

1) Why don?t professional statisticians settle on one or the other, and
just apply that system to their problems and work? What advantage does
one system have over the other?

2) As beginning statistics students, why is it important for us to
learn both systems? Do you think that beginning statistics students
will still be learning both systems in the future?

Thank you very much for your time and effort in answering my questions.
I really appreciate the thoughts of the members of this group.

-Kevin




From g@@@powe|| @end|ng |rom protonm@||@com  Mon May  5 18:05:46 2025
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Mon, 05 May 2025 16:05:46 +0000
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
Message-ID: <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>

Hi Kevin,
It might seem like simulation methods (bootstrapping and randomization) and traditional formulas (Normal or t-distributions) are just two ways to do the same job. So why learn both? Each approach has its own strengths, and statisticians use both in practice.

Why do professionals use both?
Each method offers something the other can?t. In practice, both simulation-based and theoretical techniques have unique strengths and weaknesses, and the better choice depends on the problem and its assumptions (check out - biopharmaservices.com). Simulation methods are very flexible. They don?t need strict formulas and still work even if classical conditions (like ?data must be Normal?) aren?t true. Theoretical methods are quicker and widely understood. When their assumptions hold, they give fast, exact results (a simple formula can yield a confidence interval, again, check out - biopharmaservices.com).

Advantages of each approach
? Simulation-based methods: Intuitive and flexible. They require fewer assumptions, so they work well even for odd datasets.
? Theoretical methods: Quick to calculate and convenient. Based on well-known formulas and widely trusted (when standard assumptions hold).

Why learn both?
Knowing both makes you versatile. Simulations give you a feel for what?s happening behind the scenes, while theory provides quick shortcuts and deeper insight. A statistician might use a t-test formula for a simple case but switch to bootstrapping for a complex one. Each method can cross-check the other. Mastering both approaches gives you confidence in your results.

Will future students learn both?
Probably yes. Computers now make simulation methods easy to use, so they?re more common in teaching. Meanwhile, classic Normal and t methods aren?t going away ? they?re fundamental and still useful. Future students will continue to learn both, getting the best of both worlds.

Good luck in your studies!
gregg



On Monday, May 5th, 2025 at 8:17 AM, Kevin Zembower via R-help <r-help at r-project.org> wrote:

>
>
> I marked this posting as Off Topic because it doesn?t specifically
> apply to R and Statistics, but is rather a general question about
> statistics and the teaching of statistics. If this is annoying to you,
> I apologize.
>
> As I wrap up my work in my beginning statistics course, I?d like to ask
> a philosophical question regarding statistics.
>
> In my course, we?ve learned two different ways to solve statistical
> problems: simulations, using bootstraps and randomized distributions,
> and theoretical methods, using Normal (z) and t-distributions. We?ve
> learned that both systems solve all the questions we?ve asked of them,
> and that both give comparable answers. Out of six chapters that we?ve
> studied in our textbook, the first four only used simulation methods.
> Only the last two used theoretical methods.
>
> My questions are:
>
> 1) Why don?t professional statisticians settle on one or the other, and
> just apply that system to their problems and work? What advantage does
> one system have over the other?
>
> 2) As beginning statistics students, why is it important for us to
> learn both systems? Do you think that beginning statistics students
> will still be learning both systems in the future?
>
> Thank you very much for your time and effort in answering my questions.
> I really appreciate the thoughts of the members of this group.
>
> -Kevin
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 603 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20250505/4e570e9e/attachment.sig>

From tebert @end|ng |rom u||@edu  Mon May  5 19:12:39 2025
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 5 May 2025 17:12:39 +0000
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
Message-ID: <CH3PR22MB451404B3410A5CD598D3739CCF8E2@CH3PR22MB4514.namprd22.prod.outlook.com>

(adding slightly to Gregg's answer)
Why do professionals use both? Computer intensive methods (bootstrap, randomization, jackknife) are data hungry. They do not work well if I have a sample size of 4. One could argue that the traditional methods also have trouble, but one could also think of the traditional approach as assuming unobserved values. Assuming that the true distribution is represented by my 4 observations then ... 
   Computer intensive approaches have not been readily available until the invention of widely available faster computers. There is a large body of information and long experience with the traditional methods in all scientific disciplines. If you are unfamiliar with these approaches, then you may not fully understand that key paper published 30 years ago.
   We like to think we have "the answer" but there are times where the answer we get depends on how we ask the question. The different tests ask the same question in different ways. Does the answer for your data change depending on what approach is used? If so, then what assumption or which test is problematic and why? 

Tim


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Gregg Powell via R-help
Sent: Monday, May 5, 2025 12:06 PM
To: Kevin Zembower <kevin at zembower.org>
Cc: R-help email list <r-help at r-project.org>
Subject: [R] OT: A philosophical question about statistics

[External Email]

Hi Kevin,
It might seem like simulation methods (bootstrapping and randomization) and traditional formulas (Normal or t-distributions) are just two ways to do the same job. So why learn both? Each approach has its own strengths, and statisticians use both in practice.

Why do professionals use both?
Each method offers something the other can't. In practice, both simulation-based and theoretical techniques have unique strengths and weaknesses, and the better choice depends on the problem and its assumptions (check out - biopharmaservices.com). Simulation methods are very flexible. They don't need strict formulas and still work even if classical conditions (like "data must be Normal") aren't true. Theoretical methods are quicker and widely understood. When their assumptions hold, they give fast, exact results (a simple formula can yield a confidence interval, again, check out - biopharmaservices.com).

Advantages of each approach
* Simulation-based methods: Intuitive and flexible. They require fewer assumptions, so they work well even for odd datasets.
* Theoretical methods: Quick to calculate and convenient. Based on well-known formulas and widely trusted (when standard assumptions hold).

Why learn both?
Knowing both makes you versatile. Simulations give you a feel for what's happening behind the scenes, while theory provides quick shortcuts and deeper insight. A statistician might use a t-test formula for a simple case but switch to bootstrapping for a complex one. Each method can cross-check the other. Mastering both approaches gives you confidence in your results.

Will future students learn both?
Probably yes. Computers now make simulation methods easy to use, so they're more common in teaching. Meanwhile, classic Normal and t methods aren't going away - they're fundamental and still useful. Future students will continue to learn both, getting the best of both worlds.

Good luck in your studies!
gregg



On Monday, May 5th, 2025 at 8:17 AM, Kevin Zembower via R-help <r-help at r-project.org> wrote:

>
>
> I marked this posting as Off Topic because it doesn't specifically 
> apply to R and Statistics, but is rather a general question about 
> statistics and the teaching of statistics. If this is annoying to you, 
> I apologize.
>
> As I wrap up my work in my beginning statistics course, I'd like to 
> ask a philosophical question regarding statistics.
>
> In my course, we've learned two different ways to solve statistical
> problems: simulations, using bootstraps and randomized distributions, 
> and theoretical methods, using Normal (z) and t-distributions. We've 
> learned that both systems solve all the questions we've asked of them, 
> and that both give comparable answers. Out of six chapters that we've 
> studied in our textbook, the first four only used simulation methods.
> Only the last two used theoretical methods.
>
> My questions are:
>
> 1) Why don't professional statisticians settle on one or the other, 
> and just apply that system to their problems and work? What advantage 
> does one system have over the other?
>
> 2) As beginning statistics students, why is it important for us to 
> learn both systems? Do you think that beginning statistics students 
> will still be learning both systems in the future?
>
> Thank you very much for your time and effort in answering my questions.
> I really appreciate the thoughts of the members of this group.
>
> -Kevin
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
> %7C17e2085007584244e78708dd8beebce9%7C0d4da0f84a314d76ace60a62331e1b84
> %7C0%7C0%7C638820579678440788%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> %3D%3D%7C0%7C%7C%7C&sdata=C26Jn2LVk5CW1IXEglWxFRCuLfjC7LB3p6QBH2KkVCI%
> 3D&reserved=0 PLEASE do read the posting guide 
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.
> r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C1
> 7e2085007584244e78708dd8beebce9%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> %7C0%7C638820579678469839%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> 3D%7C0%7C%7C%7C&sdata=arwwwchCqqRHcCLVTXQSfneEUX2yp6ucFp%2B4IBhrkv8%3D
> &reserved=0 and provide commented, minimal, self-contained, 
> reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Mon May  5 21:09:29 2025
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 5 May 2025 20:09:29 +0100
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <CH3PR22MB451404B3410A5CD598D3739CCF8E2@CH3PR22MB4514.namprd22.prod.outlook.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
 <CH3PR22MB451404B3410A5CD598D3739CCF8E2@CH3PR22MB4514.namprd22.prod.outlook.com>
Message-ID: <CAGxFJbTEKdLbgd8sBBF0Ws-nsac-xxB7MO-FJ3gyMQDDdtdzUQ@mail.gmail.com>

Heh. I suspect you'll get some interesting responses, but I won't try to
answer your questions. Instead, I'll just say:

(All just imo, so caveat emptor)

1. What you have been taught is mostly useless for addressing "real"
statistical issues;

2. Most of my 40 or so years of statistical practice involved trying to
define the questions of interest and determining whether there existed or
how to best obtain relevant data to answer those questions. Once/if that
was done, how to obtain answers from the data was usually straightforward.

Cheers,

Bert
"An educated person is one who can entertain new ideas, entertain others,
and entertain herself."


On Mon, May 5, 2025, 18:12 Ebert,Timothy Aaron <tebert at ufl.edu> wrote:

> (adding slightly to Gregg's answer)
> Why do professionals use both? Computer intensive methods (bootstrap,
> randomization, jackknife) are data hungry. They do not work well if I have
> a sample size of 4. One could argue that the traditional methods also have
> trouble, but one could also think of the traditional approach as assuming
> unobserved values. Assuming that the true distribution is represented by my
> 4 observations then ...
>    Computer intensive approaches have not been readily available until the
> invention of widely available faster computers. There is a large body of
> information and long experience with the traditional methods in all
> scientific disciplines. If you are unfamiliar with these approaches, then
> you may not fully understand that key paper published 30 years ago.
>    We like to think we have "the answer" but there are times where the
> answer we get depends on how we ask the question. The different tests ask
> the same question in different ways. Does the answer for your data change
> depending on what approach is used? If so, then what assumption or which
> test is problematic and why?
>
> Tim
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Gregg Powell via
> R-help
> Sent: Monday, May 5, 2025 12:06 PM
> To: Kevin Zembower <kevin at zembower.org>
> Cc: R-help email list <r-help at r-project.org>
> Subject: [R] OT: A philosophical question about statistics
>
> [External Email]
>
> Hi Kevin,
> It might seem like simulation methods (bootstrapping and randomization)
> and traditional formulas (Normal or t-distributions) are just two ways to
> do the same job. So why learn both? Each approach has its own strengths,
> and statisticians use both in practice.
>
> Why do professionals use both?
> Each method offers something the other can't. In practice, both
> simulation-based and theoretical techniques have unique strengths and
> weaknesses, and the better choice depends on the problem and its
> assumptions (check out - biopharmaservices.com). Simulation methods are
> very flexible. They don't need strict formulas and still work even if
> classical conditions (like "data must be Normal") aren't true. Theoretical
> methods are quicker and widely understood. When their assumptions hold,
> they give fast, exact results (a simple formula can yield a confidence
> interval, again, check out - biopharmaservices.com).
>
> Advantages of each approach
> * Simulation-based methods: Intuitive and flexible. They require fewer
> assumptions, so they work well even for odd datasets.
> * Theoretical methods: Quick to calculate and convenient. Based on
> well-known formulas and widely trusted (when standard assumptions hold).
>
> Why learn both?
> Knowing both makes you versatile. Simulations give you a feel for what's
> happening behind the scenes, while theory provides quick shortcuts and
> deeper insight. A statistician might use a t-test formula for a simple case
> but switch to bootstrapping for a complex one. Each method can cross-check
> the other. Mastering both approaches gives you confidence in your results.
>
> Will future students learn both?
> Probably yes. Computers now make simulation methods easy to use, so
> they're more common in teaching. Meanwhile, classic Normal and t methods
> aren't going away - they're fundamental and still useful. Future students
> will continue to learn both, getting the best of both worlds.
>
> Good luck in your studies!
> gregg
>
>
>
> On Monday, May 5th, 2025 at 8:17 AM, Kevin Zembower via R-help <
> r-help at r-project.org> wrote:
>
> >
> >
> > I marked this posting as Off Topic because it doesn't specifically
> > apply to R and Statistics, but is rather a general question about
> > statistics and the teaching of statistics. If this is annoying to you,
> > I apologize.
> >
> > As I wrap up my work in my beginning statistics course, I'd like to
> > ask a philosophical question regarding statistics.
> >
> > In my course, we've learned two different ways to solve statistical
> > problems: simulations, using bootstraps and randomized distributions,
> > and theoretical methods, using Normal (z) and t-distributions. We've
> > learned that both systems solve all the questions we've asked of them,
> > and that both give comparable answers. Out of six chapters that we've
> > studied in our textbook, the first four only used simulation methods.
> > Only the last two used theoretical methods.
> >
> > My questions are:
> >
> > 1) Why don't professional statisticians settle on one or the other,
> > and just apply that system to their problems and work? What advantage
> > does one system have over the other?
> >
> > 2) As beginning statistics students, why is it important for us to
> > learn both systems? Do you think that beginning statistics students
> > will still be learning both systems in the future?
> >
> > Thank you very much for your time and effort in answering my questions.
> > I really appreciate the thoughts of the members of this group.
> >
> > -Kevin
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
> > %7C17e2085007584244e78708dd8beebce9%7C0d4da0f84a314d76ace60a62331e1b84
> > %7C0%7C0%7C638820579678440788%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> > OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> > %3D%3D%7C0%7C%7C%7C&sdata=C26Jn2LVk5CW1IXEglWxFRCuLfjC7LB3p6QBH2KkVCI%
> > 3D&reserved=0 PLEASE do read the posting guide
> > https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.
> > r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C1
> > 7e2085007584244e78708dd8beebce9%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> > %7C0%7C638820579678469839%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> > dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> > 3D%7C0%7C%7C%7C&sdata=arwwwchCqqRHcCLVTXQSfneEUX2yp6ucFp%2B4IBhrkv8%3D
> > &reserved=0 and provide commented, minimal, self-contained,
> > reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Mon May  5 22:50:25 2025
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Mon, 5 May 2025 16:50:25 -0400
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <CAGxFJbTEKdLbgd8sBBF0Ws-nsac-xxB7MO-FJ3gyMQDDdtdzUQ@mail.gmail.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
 <CH3PR22MB451404B3410A5CD598D3739CCF8E2@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CAGxFJbTEKdLbgd8sBBF0Ws-nsac-xxB7MO-FJ3gyMQDDdtdzUQ@mail.gmail.com>
Message-ID: <00f201dbbdff$53d771a0$fb8654e0$@gmail.com>

A brief answer to this OT question is that many disciplines do the same
thing and teach multiple methods, including some that are historical and are
no longer really used.

But since you say this was an intro course, it would not prepare you well if
later courses and the real world expose you to uses of the other methods
such as being asked to maintain or extend applications already in use from a
while back that use one or another or combinations.

As others have noted, this is not really a case of either/or. It is both.
Would you make US students choose between knowing the metric system and the
one more commonly used now? I see many things labeled with both kinds of
measures, including car speedometers.


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
Sent: Monday, May 5, 2025 3:09 PM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: R-help email list <r-help at r-project.org>; Kevin Zembower
<kevin at zembower.org>
Subject: Re: [R] OT: A philosophical question about statistics

Heh. I suspect you'll get some interesting responses, but I won't try to
answer your questions. Instead, I'll just say:

(All just imo, so caveat emptor)

1. What you have been taught is mostly useless for addressing "real"
statistical issues;

2. Most of my 40 or so years of statistical practice involved trying to
define the questions of interest and determining whether there existed or
how to best obtain relevant data to answer those questions. Once/if that
was done, how to obtain answers from the data was usually straightforward.

Cheers,

Bert
"An educated person is one who can entertain new ideas, entertain others,
and entertain herself."


On Mon, May 5, 2025, 18:12 Ebert,Timothy Aaron <tebert at ufl.edu> wrote:

> (adding slightly to Gregg's answer)
> Why do professionals use both? Computer intensive methods (bootstrap,
> randomization, jackknife) are data hungry. They do not work well if I have
> a sample size of 4. One could argue that the traditional methods also have
> trouble, but one could also think of the traditional approach as assuming
> unobserved values. Assuming that the true distribution is represented by
my
> 4 observations then ...
>    Computer intensive approaches have not been readily available until the
> invention of widely available faster computers. There is a large body of
> information and long experience with the traditional methods in all
> scientific disciplines. If you are unfamiliar with these approaches, then
> you may not fully understand that key paper published 30 years ago.
>    We like to think we have "the answer" but there are times where the
> answer we get depends on how we ask the question. The different tests ask
> the same question in different ways. Does the answer for your data change
> depending on what approach is used? If so, then what assumption or which
> test is problematic and why?
>
> Tim
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Gregg Powell via
> R-help
> Sent: Monday, May 5, 2025 12:06 PM
> To: Kevin Zembower <kevin at zembower.org>
> Cc: R-help email list <r-help at r-project.org>
> Subject: [R] OT: A philosophical question about statistics
>
> [External Email]
>
> Hi Kevin,
> It might seem like simulation methods (bootstrapping and randomization)
> and traditional formulas (Normal or t-distributions) are just two ways to
> do the same job. So why learn both? Each approach has its own strengths,
> and statisticians use both in practice.
>
> Why do professionals use both?
> Each method offers something the other can't. In practice, both
> simulation-based and theoretical techniques have unique strengths and
> weaknesses, and the better choice depends on the problem and its
> assumptions (check out - biopharmaservices.com). Simulation methods are
> very flexible. They don't need strict formulas and still work even if
> classical conditions (like "data must be Normal") aren't true. Theoretical
> methods are quicker and widely understood. When their assumptions hold,
> they give fast, exact results (a simple formula can yield a confidence
> interval, again, check out - biopharmaservices.com).
>
> Advantages of each approach
> * Simulation-based methods: Intuitive and flexible. They require fewer
> assumptions, so they work well even for odd datasets.
> * Theoretical methods: Quick to calculate and convenient. Based on
> well-known formulas and widely trusted (when standard assumptions hold).
>
> Why learn both?
> Knowing both makes you versatile. Simulations give you a feel for what's
> happening behind the scenes, while theory provides quick shortcuts and
> deeper insight. A statistician might use a t-test formula for a simple
case
> but switch to bootstrapping for a complex one. Each method can cross-check
> the other. Mastering both approaches gives you confidence in your results.
>
> Will future students learn both?
> Probably yes. Computers now make simulation methods easy to use, so
> they're more common in teaching. Meanwhile, classic Normal and t methods
> aren't going away - they're fundamental and still useful. Future students
> will continue to learn both, getting the best of both worlds.
>
> Good luck in your studies!
> gregg
>
>
>
> On Monday, May 5th, 2025 at 8:17 AM, Kevin Zembower via R-help <
> r-help at r-project.org> wrote:
>
> >
> >
> > I marked this posting as Off Topic because it doesn't specifically
> > apply to R and Statistics, but is rather a general question about
> > statistics and the teaching of statistics. If this is annoying to you,
> > I apologize.
> >
> > As I wrap up my work in my beginning statistics course, I'd like to
> > ask a philosophical question regarding statistics.
> >
> > In my course, we've learned two different ways to solve statistical
> > problems: simulations, using bootstraps and randomized distributions,
> > and theoretical methods, using Normal (z) and t-distributions. We've
> > learned that both systems solve all the questions we've asked of them,
> > and that both give comparable answers. Out of six chapters that we've
> > studied in our textbook, the first four only used simulation methods.
> > Only the last two used theoretical methods.
> >
> > My questions are:
> >
> > 1) Why don't professional statisticians settle on one or the other,
> > and just apply that system to their problems and work? What advantage
> > does one system have over the other?
> >
> > 2) As beginning statistics students, why is it important for us to
> > learn both systems? Do you think that beginning statistics students
> > will still be learning both systems in the future?
> >
> > Thank you very much for your time and effort in answering my questions.
> > I really appreciate the thoughts of the members of this group.
> >
> > -Kevin
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
> > %7C17e2085007584244e78708dd8beebce9%7C0d4da0f84a314d76ace60a62331e1b84
> > %7C0%7C0%7C638820579678440788%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> > OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> > %3D%3D%7C0%7C%7C%7C&sdata=C26Jn2LVk5CW1IXEglWxFRCuLfjC7LB3p6QBH2KkVCI%
> > 3D&reserved=0 PLEASE do read the posting guide
> > https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.
> > r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C1
> > 7e2085007584244e78708dd8beebce9%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> > %7C0%7C638820579678469839%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> > dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> > 3D%7C0%7C%7C%7C&sdata=arwwwchCqqRHcCLVTXQSfneEUX2yp6ucFp%2B4IBhrkv8%3D
> > &reserved=0 and provide commented, minimal, self-contained,
> > reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From cwr @end|ng |rom @gency@t@t|@t|c@|@com  Mon May  5 23:02:42 2025
From: cwr @end|ng |rom @gency@t@t|@t|c@|@com (Chris Ryan)
Date: Mon, 5 May 2025 17:02:42 -0400
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <00f201dbbdff$53d771a0$fb8654e0$@gmail.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
 <CH3PR22MB451404B3410A5CD598D3739CCF8E2@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CAGxFJbTEKdLbgd8sBBF0Ws-nsac-xxB7MO-FJ3gyMQDDdtdzUQ@mail.gmail.com>
 <00f201dbbdff$53d771a0$fb8654e0$@gmail.com>
Message-ID: <1268a5fe-c27f-5960-3a01-65b0c2368679@agencystatistical.com>

I've often wondered how the field of statistics, and statistical
education, would have evolved if modern-day computers and software and
programming were available in the early years. Would the "traditional"
methods, requiring simplifying assumptions, have been developed at all?

--Chris Ryan

avi.e.gross at gmail.com wrote:
> A brief answer to this OT question is that many disciplines do the same
> thing and teach multiple methods, including some that are historical and are
> no longer really used.
> 
> But since you say this was an intro course, it would not prepare you well if
> later courses and the real world expose you to uses of the other methods
> such as being asked to maintain or extend applications already in use from a
> while back that use one or another or combinations.
> 
> As others have noted, this is not really a case of either/or. It is both.
> Would you make US students choose between knowing the metric system and the
> one more commonly used now? I see many things labeled with both kinds of
> measures, including car speedometers.
> 
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
> Sent: Monday, May 5, 2025 3:09 PM
> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> Cc: R-help email list <r-help at r-project.org>; Kevin Zembower
> <kevin at zembower.org>
> Subject: Re: [R] OT: A philosophical question about statistics
> 
> Heh. I suspect you'll get some interesting responses, but I won't try to
> answer your questions. Instead, I'll just say:
> 
> (All just imo, so caveat emptor)
> 
> 1. What you have been taught is mostly useless for addressing "real"
> statistical issues;
> 
> 2. Most of my 40 or so years of statistical practice involved trying to
> define the questions of interest and determining whether there existed or
> how to best obtain relevant data to answer those questions. Once/if that
> was done, how to obtain answers from the data was usually straightforward.
> 
> Cheers,
> 
> Bert
> "An educated person is one who can entertain new ideas, entertain others,
> and entertain herself."
> 
> 
> On Mon, May 5, 2025, 18:12 Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> 
>> (adding slightly to Gregg's answer)
>> Why do professionals use both? Computer intensive methods (bootstrap,
>> randomization, jackknife) are data hungry. They do not work well if I have
>> a sample size of 4. One could argue that the traditional methods also have
>> trouble, but one could also think of the traditional approach as assuming
>> unobserved values. Assuming that the true distribution is represented by
> my
>> 4 observations then ...
>>    Computer intensive approaches have not been readily available until the
>> invention of widely available faster computers. There is a large body of
>> information and long experience with the traditional methods in all
>> scientific disciplines. If you are unfamiliar with these approaches, then
>> you may not fully understand that key paper published 30 years ago.
>>    We like to think we have "the answer" but there are times where the
>> answer we get depends on how we ask the question. The different tests ask
>> the same question in different ways. Does the answer for your data change
>> depending on what approach is used? If so, then what assumption or which
>> test is problematic and why?
>>
>> Tim
>>
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Gregg Powell via
>> R-help
>> Sent: Monday, May 5, 2025 12:06 PM
>> To: Kevin Zembower <kevin at zembower.org>
>> Cc: R-help email list <r-help at r-project.org>
>> Subject: [R] OT: A philosophical question about statistics
>>
>> [External Email]
>>
>> Hi Kevin,
>> It might seem like simulation methods (bootstrapping and randomization)
>> and traditional formulas (Normal or t-distributions) are just two ways to
>> do the same job. So why learn both? Each approach has its own strengths,
>> and statisticians use both in practice.
>>
>> Why do professionals use both?
>> Each method offers something the other can't. In practice, both
>> simulation-based and theoretical techniques have unique strengths and
>> weaknesses, and the better choice depends on the problem and its
>> assumptions (check out - biopharmaservices.com). Simulation methods are
>> very flexible. They don't need strict formulas and still work even if
>> classical conditions (like "data must be Normal") aren't true. Theoretical
>> methods are quicker and widely understood. When their assumptions hold,
>> they give fast, exact results (a simple formula can yield a confidence
>> interval, again, check out - biopharmaservices.com).
>>
>> Advantages of each approach
>> * Simulation-based methods: Intuitive and flexible. They require fewer
>> assumptions, so they work well even for odd datasets.
>> * Theoretical methods: Quick to calculate and convenient. Based on
>> well-known formulas and widely trusted (when standard assumptions hold).
>>
>> Why learn both?
>> Knowing both makes you versatile. Simulations give you a feel for what's
>> happening behind the scenes, while theory provides quick shortcuts and
>> deeper insight. A statistician might use a t-test formula for a simple
> case
>> but switch to bootstrapping for a complex one. Each method can cross-check
>> the other. Mastering both approaches gives you confidence in your results.
>>
>> Will future students learn both?
>> Probably yes. Computers now make simulation methods easy to use, so
>> they're more common in teaching. Meanwhile, classic Normal and t methods
>> aren't going away - they're fundamental and still useful. Future students
>> will continue to learn both, getting the best of both worlds.
>>
>> Good luck in your studies!
>> gregg
>>
>>
>>
>> On Monday, May 5th, 2025 at 8:17 AM, Kevin Zembower via R-help <
>> r-help at r-project.org> wrote:
>>
>>>
>>>
>>> I marked this posting as Off Topic because it doesn't specifically
>>> apply to R and Statistics, but is rather a general question about
>>> statistics and the teaching of statistics. If this is annoying to you,
>>> I apologize.
>>>
>>> As I wrap up my work in my beginning statistics course, I'd like to
>>> ask a philosophical question regarding statistics.
>>>
>>> In my course, we've learned two different ways to solve statistical
>>> problems: simulations, using bootstraps and randomized distributions,
>>> and theoretical methods, using Normal (z) and t-distributions. We've
>>> learned that both systems solve all the questions we've asked of them,
>>> and that both give comparable answers. Out of six chapters that we've
>>> studied in our textbook, the first four only used simulation methods.
>>> Only the last two used theoretical methods.
>>>
>>> My questions are:
>>>
>>> 1) Why don't professional statisticians settle on one or the other,
>>> and just apply that system to their problems and work? What advantage
>>> does one system have over the other?
>>>
>>> 2) As beginning statistics students, why is it important for us to
>>> learn both systems? Do you think that beginning statistics students
>>> will still be learning both systems in the future?
>>>
>>> Thank you very much for your time and effort in answering my questions.
>>> I really appreciate the thoughts of the members of this group.
>>>
>>> -Kevin
>>>
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
>>> %7C17e2085007584244e78708dd8beebce9%7C0d4da0f84a314d76ace60a62331e1b84
>>> %7C0%7C0%7C638820579678440788%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
>>> OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
>>> %3D%3D%7C0%7C%7C%7C&sdata=C26Jn2LVk5CW1IXEglWxFRCuLfjC7LB3p6QBH2KkVCI%
>>> 3D&reserved=0 PLEASE do read the posting guide
>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.
>>> r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C1
>>> 7e2085007584244e78708dd8beebce9%7C0d4da0f84a314d76ace60a62331e1b84%7C0
>>> %7C0%7C638820579678469839%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
>>> dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
>>> 3D%7C0%7C%7C%7C&sdata=arwwwchCqqRHcCLVTXQSfneEUX2yp6ucFp%2B4IBhrkv8%3D
>>> &reserved=0 and provide commented, minimal, self-contained,
>>> reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> https://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Mon May  5 23:33:23 2025
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Mon, 5 May 2025 21:33:23 +0000
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <1268a5fe-c27f-5960-3a01-65b0c2368679@agencystatistical.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
 <CH3PR22MB451404B3410A5CD598D3739CCF8E2@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CAGxFJbTEKdLbgd8sBBF0Ws-nsac-xxB7MO-FJ3gyMQDDdtdzUQ@mail.gmail.com>
 <00f201dbbdff$53d771a0$fb8654e0$@gmail.com>
 <1268a5fe-c27f-5960-3a01-65b0c2368679@agencystatistical.com>
Message-ID: <DM6PR03MB5049DAC1E04D5B4115FC6777E28E2@DM6PR03MB5049.namprd03.prod.outlook.com>

Chris,

In all likelihood, if computers had been invented, "traditional" statistics would have been invented, but it would be less fully developed than it currently is.

While resampling, simulations, etc. can give answers, they have at least two drawbacks. First, compared to "traditional" methods the new methods can be very time consuming. Second compared to "traditional" methods they can be very resource intensive (i.e. requiring a lot of electrical power). Traditional methods, but taking advantage of a few assumptions, can often obtain an answer to a question faster and with less usage of resources than the newer methods.

A modern statistician would do well to learn both "traditional" and newer methods, and use the best method given the question and resources at hand.

John

John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




________________________________________
From: R-help on behalf of Chris Ryan
Sent: Monday, May 5, 2025 5:02 PM
To: 'R-help email list'
Subject: Re: [R] OT: A philosophical question about statistics


I've often wondered how the field of statistics, and statistical
education, would have evolved if modern-day computers and software and
programming were available in the early years. Would the "traditional"
methods, requiring simplifying assumptions, have been developed at all?

--Chris Ryan

avi.e.gross at gmail.com wrote:
> A brief answer to this OT question is that many disciplines do the same
> thing and teach multiple methods, including some that are historical and are
> no longer really used.
>
> But since you say this was an intro course, it would not prepare you well if
> later courses and the real world expose you to uses of the other methods
> such as being asked to maintain or extend applications already in use from a
> while back that use one or another or combinations.
>
> As others have noted, this is not really a case of either/or. It is both.
> Would you make US students choose between knowing the metric system and the
> one more commonly used now? I see many things labeled with both kinds of
> measures, including car speedometers.
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
> Sent: Monday, May 5, 2025 3:09 PM
> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> Cc: R-help email list <r-help at r-project.org>; Kevin Zembower
> <kevin at zembower.org>
> Subject: Re: [R] OT: A philosophical question about statistics
>
> Heh. I suspect you'll get some interesting responses, but I won't try to
> answer your questions. Instead, I'll just say:
>
> (All just imo, so caveat emptor)
>
> 1. What you have been taught is mostly useless for addressing "real"
> statistical issues;
>
> 2. Most of my 40 or so years of statistical practice involved trying to
> define the questions of interest and determining whether there existed or
> how to best obtain relevant data to answer those questions. Once/if that
> was done, how to obtain answers from the data was usually straightforward.
>
> Cheers,
>
> Bert
> "An educated person is one who can entertain new ideas, entertain others,
> and entertain herself."
>
>
> On Mon, May 5, 2025, 18:12 Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>
>> (adding slightly to Gregg's answer)
>> Why do professionals use both? Computer intensive methods (bootstrap,
>> randomization, jackknife) are data hungry. They do not work well if I have
>> a sample size of 4. One could argue that the traditional methods also have
>> trouble, but one could also think of the traditional approach as assuming
>> unobserved values. Assuming that the true distribution is represented by
> my
>> 4 observations then ...
>>    Computer intensive approaches have not been readily available until the
>> invention of widely available faster computers. There is a large body of
>> information and long experience with the traditional methods in all
>> scientific disciplines. If you are unfamiliar with these approaches, then
>> you may not fully understand that key paper published 30 years ago.
>>    We like to think we have "the answer" but there are times where the
>> answer we get depends on how we ask the question. The different tests ask
>> the same question in different ways. Does the answer for your data change
>> depending on what approach is used? If so, then what assumption or which
>> test is problematic and why?
>>
>> Tim
>>
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Gregg Powell via
>> R-help
>> Sent: Monday, May 5, 2025 12:06 PM
>> To: Kevin Zembower <kevin at zembower.org>
>> Cc: R-help email list <r-help at r-project.org>
>> Subject: [R] OT: A philosophical question about statistics
>>
>> [External Email]
>>
>> Hi Kevin,
>> It might seem like simulation methods (bootstrapping and randomization)
>> and traditional formulas (Normal or t-distributions) are just two ways to
>> do the same job. So why learn both? Each approach has its own strengths,
>> and statisticians use both in practice.
>>
>> Why do professionals use both?
>> Each method offers something the other can't. In practice, both
>> simulation-based and theoretical techniques have unique strengths and
>> weaknesses, and the better choice depends on the problem and its
>> assumptions (check out - biopharmaservices.com). Simulation methods are
>> very flexible. They don't need strict formulas and still work even if
>> classical conditions (like "data must be Normal") aren't true. Theoretical
>> methods are quicker and widely understood. When their assumptions hold,
>> they give fast, exact results (a simple formula can yield a confidence
>> interval, again, check out - biopharmaservices.com).
>>
>> Advantages of each approach
>> * Simulation-based methods: Intuitive and flexible. They require fewer
>> assumptions, so they work well even for odd datasets.
>> * Theoretical methods: Quick to calculate and convenient. Based on
>> well-known formulas and widely trusted (when standard assumptions hold).
>>
>> Why learn both?
>> Knowing both makes you versatile. Simulations give you a feel for what's
>> happening behind the scenes, while theory provides quick shortcuts and
>> deeper insight. A statistician might use a t-test formula for a simple
> case
>> but switch to bootstrapping for a complex one. Each method can cross-check
>> the other. Mastering both approaches gives you confidence in your results.
>>
>> Will future students learn both?
>> Probably yes. Computers now make simulation methods easy to use, so
>> they're more common in teaching. Meanwhile, classic Normal and t methods
>> aren't going away - they're fundamental and still useful. Future students
>> will continue to learn both, getting the best of both worlds.
>>
>> Good luck in your studies!
>> gregg
>>
>>
>>
>> On Monday, May 5th, 2025 at 8:17 AM, Kevin Zembower via R-help <
>> r-help at r-project.org> wrote:
>>
>>>
>>>
>>> I marked this posting as Off Topic because it doesn't specifically
>>> apply to R and Statistics, but is rather a general question about
>>> statistics and the teaching of statistics. If this is annoying to you,
>>> I apologize.
>>>
>>> As I wrap up my work in my beginning statistics course, I'd like to
>>> ask a philosophical question regarding statistics.
>>>
>>> In my course, we've learned two different ways to solve statistical
>>> problems: simulations, using bootstraps and randomized distributions,
>>> and theoretical methods, using Normal (z) and t-distributions. We've
>>> learned that both systems solve all the questions we've asked of them,
>>> and that both give comparable answers. Out of six chapters that we've
>>> studied in our textbook, the first four only used simulation methods.
>>> Only the last two used theoretical methods.
>>>
>>> My questions are:
>>>
>>> 1) Why don't professional statisticians settle on one or the other,
>>> and just apply that system to their problems and work? What advantage
>>> does one system have over the other?
>>>
>>> 2) As beginning statistics students, why is it important for us to
>>> learn both systems? Do you think that beginning statistics students
>>> will still be learning both systems in the future?
>>>
>>> Thank you very much for your time and effort in answering my questions.
>>> I really appreciate the thoughts of the members of this group.
>>>
>>> -Kevin
>>>
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat/
>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
>>> %7C17e2085007584244e78708dd8beebce9%7C0d4da0f84a314d76ace60a62331e1b84
>>> %7C0%7C0%7C638820579678440788%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
>>> OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
>>> %3D%3D%7C0%7C%7C%7C&sdata=C26Jn2LVk5CW1IXEglWxFRCuLfjC7LB3p6QBH2KkVCI%
>>> 3D&reserved=0 PLEASE do read the posting guide
>>> https://www/.
>>> r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C1
>>> 7e2085007584244e78708dd8beebce9%7C0d4da0f84a314d76ace60a62331e1b84%7C0
>>> %7C0%7C638820579678469839%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
>>> dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
>>> 3D%7C0%7C%7C%7C&sdata=arwwwchCqqRHcCLVTXQSfneEUX2yp6ucFp%2B4IBhrkv8%3D
>>> &reserved=0 and provide commented, minimal, self-contained,
>>> reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> https://www.r-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @ggp@@erge| @end|ng |rom gm@||@com  Tue May  6 00:17:09 2025
From: @ggp@@erge| @end|ng |rom gm@||@com (Sergei Ko)
Date: Mon, 5 May 2025 23:17:09 +0100
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
Message-ID: <CAK2fHGcrbejbT_BzsVjXFkXMLPV+=fcZjJX9+syNWdQo0M5Dwg@mail.gmail.com>

From a practitioner perspective. Parametric methods have more power. If
assumptions are here - use formulas. On the other hand my usual
recommendation to colleagues: "If you don't know what to do - use
bootstrap."

Regards,
Sergiy

On Mon, 5 May 2025, 17:06 Gregg Powell via R-help, <r-help at r-project.org>
wrote:

> Hi Kevin,
> It might seem like simulation methods (bootstrapping and randomization)
> and traditional formulas (Normal or t-distributions) are just two ways to
> do the same job. So why learn both? Each approach has its own strengths,
> and statisticians use both in practice.
>
> Why do professionals use both?
> Each method offers something the other can?t. In practice, both
> simulation-based and theoretical techniques have unique strengths and
> weaknesses, and the better choice depends on the problem and its
> assumptions (check out - biopharmaservices.com). Simulation methods are
> very flexible. They don?t need strict formulas and still work even if
> classical conditions (like ?data must be Normal?) aren?t true. Theoretical
> methods are quicker and widely understood. When their assumptions hold,
> they give fast, exact results (a simple formula can yield a confidence
> interval, again, check out - biopharmaservices.com).
>
> Advantages of each approach
> ? Simulation-based methods: Intuitive and flexible. They require fewer
> assumptions, so they work well even for odd datasets.
> ? Theoretical methods: Quick to calculate and convenient. Based on
> well-known formulas and widely trusted (when standard assumptions hold).
>
> Why learn both?
> Knowing both makes you versatile. Simulations give you a feel for what?s
> happening behind the scenes, while theory provides quick shortcuts and
> deeper insight. A statistician might use a t-test formula for a simple case
> but switch to bootstrapping for a complex one. Each method can cross-check
> the other. Mastering both approaches gives you confidence in your results.
>
> Will future students learn both?
> Probably yes. Computers now make simulation methods easy to use, so
> they?re more common in teaching. Meanwhile, classic Normal and t methods
> aren?t going away ? they?re fundamental and still useful. Future students
> will continue to learn both, getting the best of both worlds.
>
> Good luck in your studies!
> gregg
>
>
>
> On Monday, May 5th, 2025 at 8:17 AM, Kevin Zembower via R-help <
> r-help at r-project.org> wrote:
>
> >
> >
> > I marked this posting as Off Topic because it doesn?t specifically
> > apply to R and Statistics, but is rather a general question about
> > statistics and the teaching of statistics. If this is annoying to you,
> > I apologize.
> >
> > As I wrap up my work in my beginning statistics course, I?d like to ask
> > a philosophical question regarding statistics.
> >
> > In my course, we?ve learned two different ways to solve statistical
> > problems: simulations, using bootstraps and randomized distributions,
> > and theoretical methods, using Normal (z) and t-distributions. We?ve
> > learned that both systems solve all the questions we?ve asked of them,
> > and that both give comparable answers. Out of six chapters that we?ve
> > studied in our textbook, the first four only used simulation methods.
> > Only the last two used theoretical methods.
> >
> > My questions are:
> >
> > 1) Why don?t professional statisticians settle on one or the other, and
> > just apply that system to their problems and work? What advantage does
> > one system have over the other?
> >
> > 2) As beginning statistics students, why is it important for us to
> > learn both systems? Do you think that beginning statistics students
> > will still be learning both systems in the future?
> >
> > Thank you very much for your time and effort in answering my questions.
> > I really appreciate the thoughts of the members of this group.
> >
> > -Kevin
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible
> code.______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @m@||ep@||on @end|ng |rom proton@me  Tue May  6 06:01:36 2025
From: @m@||ep@||on @end|ng |rom proton@me (smallepsilon)
Date: Tue, 06 May 2025 04:01:36 +0000
Subject: [R] non-reproducible eigen() output with MKL
In-Reply-To: <AS8PR08MB9193F68400D42AA488D029588B8E2@AS8PR08MB9193.eurprd08.prod.outlook.com>
References: <yJIVt5GuVv6QIOWNQhDbauOpb5YggyqGD_oq3b7W8CWlZvQwXZb5FH3Hf8Y-XfhUcDK0RuMIYNdU93ompxJq_k8BQOhRjhCdlE8EZsZflEk=@proton.me>
 <171E8522-6EFC-49AC-9074-6D2287472CAC@gmail.com>
 <c9SJxWBERVWAB5nRVSubq_j9_xqVkBu19T-ldNed-tQOgeY5mbYM-HL4Jda4HTTSfyu8-XlsAnEoy06qlPK7R0Z1GuirC-kzkrJidaehec8=@proton.me>
 <26648.31676.116927.750944@stat.math.ethz.ch>
 <AS8PR08MB9193F68400D42AA488D029588B8E2@AS8PR08MB9193.eurprd08.prod.outlook.com>
Message-ID: <XwpZ1T7V9UJ4qDtfYbb7UyyMBrtmhg3g5yNeaiTjod1nYy6Wrs1rTFuwU3W_ZnmTDLRtXKmQQmMfYZa2rGOp1dd0wqeA77nsjkYowfuxEGk=@proton.me>

Thanks everyone for your help, all of which was useful. Martin: I am glad that I am not the only one who is (now) disillusioned about parallel computations.

Jesse



On Monday, May 5th, 2025 at 4:25 AM, Viechtbauer, Wolfgang (NP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:

> 
> 
> A relevant thread from a few years ago where this was discussed:
> 
> https://www.stat.math.ethz.ch/pipermail/r-help/2023-August/477904.html
> 
> I generally use:
> 
> export OPENBLAS_NUM_THREADS=1
> export MKL_NUM_THREADS=1
> 
> since in my experience the biggest performance gains come from switching to OpenBLAS / MKL in the first place. Using their multithreading capabilities tends to yield diminishing gains in comparison.
> 
> For MKL, you could try:
> 
> export MKL_THREADING_LAYER=GNU
> 
> or
> 
> export MKL_THREADING_LAYER=sequential
> 
> when using multiple threads to see if this avoids the issue.
> 
> But if you are using explicit parallelization (as I often do), then you would want to avoid the multithreading in the math libs anyway.
> 
> Best,
> Wolfgang
> 
> > -----Original Message-----
> > From: R-help r-help-bounces at r-project.org On Behalf Of Martin Maechler
> > Sent: Monday, May 5, 2025 10:50
> > To: smallepsilon smallepsilon at proton.me
> > Cc: r-help at r-project.org; peter dalgaard pdalgd at gmail.com
> > Subject: Re: [R] non-reproducible eigen() output with MKL
> > 
> > > > > > > smallepsilon via R-help
> > > > > > > on Sun, 04 May 2025 18:11:57 +0000 writes:
> > 
> > > Peter, The eigenvalues are not identical(), but are
> > > all.equal(). When n is 20, the crossproduct is
> > > (numerically) a diagonal matrix with +-1 on the
> > > diagonal. When n is 50, this is not the case, but that
> > > could be an issue of nearly identical eigenvalues.
> > 
> > > Is there no way within R to require that the sequence of
> > > operations be the same for identical calls?
> > 
> > As Peter Dalgaard mentioned, you have the problem of underlying
> > system libraries that try to be fast and hence parallelize
> > computations, notably in this (and man similar calls), MKL uses
> > parallelized BLAS and/or LAPACK .. and that's what eigen() in R
> > (and Julia, python, matlab, ..) all use.
> > 
> > And parallelization is a killer of (strict / bit-level)
> > reproducibility, as you have just experienced.
> > 
> > If you know how to tell your OS / that you want only one
> > parallel "thread" / process / ...
> > you get back to reproducible (and slower) computations.
> > 
> > > The problem arose originally in a package test in which I wanted to
> > > verify that two ways of specifying something led to the
> > > execution of exactly the same calculations. The best proxy
> > > for this seemed to be to use identical() on the outputs,
> > > but if the same line of code (that should not involve an
> > > RNG) can lead to different results, this approach is
> > > doomed, yes? It is not absolutely critical that the
> > > outputs be identical(), but it would be much more
> > > reassuring than all.equal().
> > 
> > I understand and agree.
> > 
> > When I first became aware of the irreprodicibility of parallel
> > computations, only a few years ago, I was quite shocked and deillusionized..
> > 
> > > Thanks, Jesse
> > 
> > Martin Maechler
> > ETH Zurich and R Core team
> > 
> > > On Sunday, May 4th, 2025 at 12:27 PM, peter dalgaard
> > > pdalgd at gmail.com wrote:
> > >>
> > >>
> > >> Have you looked more closely at the differences?
> > >> Eigenvectors are only determined up to a sign change, so
> > >> different platforms often give results that differ by
> > >> sign. If you use a multitasking numerical library, the
> > >> same can happen within platform because the exact
> > >> sequence of computations differs between calls.
> > >>
> > >> You could check
> > >>
> > >> a) that e1$values and e2$values are the same b) that the
> > >> crossprod(e1$vectors, e2$vectors) is a diagonal matrix
> > >> with 1 or -1 in the diagonal. (This might fail if you
> > >> have eigenvalues that are almost identical, though.)
> > >>
> > >> -pd
> > >>
> > >> > On 4 May 2025, at 17.57, smallepsilon via R-help
> > >> r-help at r-project.org wrote:
> > >> >
> > >> > I am using MKL with R 4.5.0 on Linux, and eigen() is
> > >> producing different results with identical
> > >> calls. Specifically, when I run the code below, I get
> > >> "FALSE" from identical(). Just in case it had something
> > >> to do with a random number generator, I put identical
> > >> set.seed() calls immediately before each eigen() call,
> > >> but that did not help. Has anyone seen this behavior
> > >> before?
> > >> >
> > >> > (When n is 5, the identical() call almost always
> > >> returns "TRUE". As n increases, the proportion of FALSE
> > >> results increases, and it is nearly always FALSE when n
> > >> is 50.)
> > >> >
> > >> > Jesse
> > >> >
> > >> > ***
> > >> >
> > >> > n <- 50 > set.seed(20250504) > Sigma <- rWishart(1, df
> > >> = n, Sigma = diag(n))[,,1] > e1 <- eigen(Sigma) > e2 <-
> > >> eigen(Sigma) > identical(e1, e2)
> > >>
> > >> --
> > >> Peter Dalgaard, Professor, Center for Statistics,
> > >> Copenhagen Business SchoolSolbjerg Plads 3, 2000
> > >> Frederiksberg, Denmark Phone: (+45)38153501 Office: A
> > >> 4.23 Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com


From kev|n @end|ng |rom zembower@org  Tue May  6 15:13:13 2025
From: kev|n @end|ng |rom zembower@org (=?UTF-8?Q?Kevin_Zembower?=)
Date: Tue, 6 May 2025 13:13:13 +0000
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <CAGxFJbTEKdLbgd8sBBF0Ws-nsac-xxB7MO-FJ3gyMQDDdtdzUQ@mail.gmail.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org> 
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
 <CH3PR22MB451404B3410A5CD598D3739CCF8E2@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CAGxFJbTEKdLbgd8sBBF0Ws-nsac-xxB7MO-FJ3gyMQDDdtdzUQ@mail.gmail.com> 
 <ed28a2ccd7bf54825cd9b8ce6a54d8d18800caa0.camel@zembower.org>
Message-ID: <01000196a5baa180-b3e94a5a-1447-40fe-b378-88ce723c070e-000000@email.amazonses.com>

Bert, thanks so much for your response. I woke up early this morning
and couldn't go back to sleep, formulating a response. I hope I do it
justice. See in-line below.

On Mon, 2025-05-05 at 20:09 +0100, Bert Gunter wrote:
> 1. What you have been taught is mostly useless for addressing "real"
> statistical issues;

I hope this is not so, but I see your point. If by 'real' statistical
problems, you mean the ones professional statisticians face, I agree. I
kinda assumed that a one-semester course in basic statistics wouldn't
qualify me for a professional position as a statistician.

But, if you mean MY 'real' statistical issues, I hope you're wrong.
Here's two examples:

I have sleep apnea and use a CPAP machine. I have the ability to make
changes to the settings on the machine, and have software to read the
data that it writes to an SD card. The software produces data on the
apnea-hypopnea index (AHI) and length of use each night, both of which
can be used to gauge the effectiveness of its use. When I occasionally
make changes to the settings, I'd like to know if the changes improved
my health.

About three years ago, we installed split-unit AC modules in our house,
replacing individual window air conditioners. Since the split units are
connected to a heat pump, they can also be used to supplement the
output of the natural-gas-fired steam boiler that is the main souce of
heating. We were told that the split units would reduce our home's
energy usage, in both heating and cooling, and ultimately save money.
However, I'm not certain that I've seen that savings reflected in our
bills. I am serviced by a utility that allows me to download my house's
daily energy use, in both kilowatts of electricity and therms of
natural gas.

In both these examples, the results could be analyzed by the knowledge
I've gained by studying statistics, I believe. In addition, the results
have a practical significance to me, and are not self-evident without
the use of some sort of analytical tools. 

Is this nerdy? Of course! I don't imagine most, or even many, CPAP
users conduct a hypothesis test when changing their machine. But, I'm
nerdy, and now I have this new tool in my toolbox to apply to problems
like these.

> 2. Most of my 40 or so years of statistical practice involved trying
> to define the questions of interest and determining whether there
> existed or how to best obtain relevant data to answer those
> questions. Once/if that was done, how to obtain answers from the data
> was usually straightforward.

That's a good insight, and I could certainly see how it's true. Almost
all of the problems I've solved in basic statistics presented the data
in a near-perfect format. We never even had to clean up data with
missing values, for example. Certainly, applying the algorithms
(simulation or theoretical) was the easiest and quickest part.

Thank you again, Bert, for replying. I always enjoy your contributions
to this group.

-Kevin


From kev|n @end|ng |rom zembower@org  Tue May  6 15:13:54 2025
From: kev|n @end|ng |rom zembower@org (=?UTF-8?Q?Kevin_Zembower?=)
Date: Tue, 6 May 2025 13:13:54 +0000
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org> 
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
 <bbb3b3db52f191f3200dabf2919583c0b7c6331f.camel@zembower.org>
Message-ID: <01000196a5bb3fe3-34fa2db9-261a-439e-83d7-39dcf7e4480b-000000@email.amazonses.com>

Gregg, thanks for your reply to my questions. I was looking for exactly
the type of information you included, especially on the strengths and
weaknesses of each approach.

I was very pleased with the intuitive aspects of the simulation
approach in my course. This was the part that was missing from my first
exposure to statistics many years ago. Then, I thought of statistical
formulas as 'black boxes,' where numbers were fed in and results came
out, with unknown processes operating in between. With simulations, I
could count dots on a chart and come up with meaningful results.

I missed the connection to my questions at the website you referred me
to, biopharmaservices.com. This seems to be the home page of a firm
that conducts medical studies, but I couldn't find anything about the
practical use of statistics. Perhaps I didn't search enough.

Thanks, again, for your thoughts and perspective. 

-Kevin

On Mon, 2025-05-05 at 16:05 +0000, Gregg Powell wrote:
> Hi Kevin,
> It might seem like simulation methods (bootstrapping and
> randomization) and traditional formulas (Normal or t-distributions)
> are just two ways to do the same job. So why learn both? Each
> approach has its own strengths, and statisticians use both in
> practice.
> 
> Why do professionals use both?
> Each method offers something the other can?t. In practice, both
> simulation-based and theoretical techniques have unique strengths and
> weaknesses, and the better choice depends on the problem and its
> assumptions (check out - biopharmaservices.com). Simulation methods
> are very flexible. They don?t need strict formulas and still work
> even if classical conditions (like ?data must be Normal?) aren?t
> true. Theoretical methods are quicker and widely understood. When
> their assumptions hold, they give fast, exact results (a simple
> formula can yield a confidence interval, again, check out -
> biopharmaservices.com).
> 
> Advantages of each approach
> ? Simulation-based methods: Intuitive and flexible. They require
> fewer assumptions, so they work well even for odd datasets.
> ? Theoretical methods: Quick to calculate and convenient. Based on
> well-known formulas and widely trusted (when standard assumptions
> hold).
> 
> Why learn both?
> Knowing both makes you versatile. Simulations give you a feel for
> what?s happening behind the scenes, while theory provides quick
> shortcuts and deeper insight. A statistician might use a t-test
> formula for a simple case but switch to bootstrapping for a complex
> one. Each method can cross-check the other. Mastering both approaches
> gives you confidence in your results.
> 
> Will future students learn both?
> Probably yes. Computers now make simulation methods easy to use, so
> they?re more common in teaching. Meanwhile, classic Normal and t
> methods aren?t going away ? they?re fundamental and still useful.
> Future students will continue to learn both, getting the best of both
> worlds.
> 
> Good luck in your studies!
> gregg



From kev|n @end|ng |rom zembower@org  Tue May  6 15:14:28 2025
From: kev|n @end|ng |rom zembower@org (=?UTF-8?Q?Kevin_Zembower?=)
Date: Tue, 6 May 2025 13:14:28 +0000
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <CH3PR22MB451404B3410A5CD598D3739CCF8E2@CH3PR22MB4514.namprd22.prod.outlook.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org> 
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
 <CH3PR22MB451404B3410A5CD598D3739CCF8E2@CH3PR22MB4514.namprd22.prod.outlook.com>
 <7abe69df37f45aa01a5445ddb92734bd868aa016.camel@zembower.org>
Message-ID: <01000196a5bbc3b4-fafaaed2-1099-4ad9-b2b7-e7022b8cbd96-000000@email.amazonses.com>

TIm, thanks for replying to my questions. I really value your insights
in areas of statistics (small sample sizes, agricultural statistics)
that are unique.

The use of one method or technique to check the other was not one I had
thought of. The idea that if one technique, correctly applied, could
yield results different from the other technique, and that this could
lead to insight into the assumptions of a problem that might be in
error, is a powerful idea. Thank you for that.

Thanks, again, for your contribution to my questions.

-Kevin

On Mon, 2025-05-05 at 17:12 +0000, Ebert,Timothy Aaron wrote:
> (adding slightly to Gregg's answer)
> Why do professionals use both? Computer intensive methods (bootstrap,
> randomization, jackknife) are data hungry. They do not work well if I
> have a sample size of 4. One could argue that the traditional methods
> also have trouble, but one could also think of the traditional
> approach as assuming unobserved values. Assuming that the true
> distribution is represented by my 4 observations then ... 
> ?? Computer intensive approaches have not been readily available
> until the invention of widely available faster computers. There is a
> large body of information and long experience with the traditional
> methods in all scientific disciplines. If you are unfamiliar with
> these approaches, then you may not fully understand that key paper
> published 30 years ago.
> ?? We like to think we have "the answer" but there are times where
> the answer we get depends on how we ask the question. The different
> tests ask the same question in different ways. Does the answer for
> your data change depending on what approach is used? If so, then what
> assumption or which test is problematic and why? 
> 
> Tim



From kev|n @end|ng |rom zembower@org  Tue May  6 15:15:19 2025
From: kev|n @end|ng |rom zembower@org (=?UTF-8?Q?Kevin_Zembower?=)
Date: Tue, 6 May 2025 13:15:19 +0000
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org> 
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <7aa6204bb5ac092b4dc90442b8369b5c8b52c149.camel@zembower.org>
Message-ID: <01000196a5bc8d11-a053e1b8-25c4-4cb4-b672-5bf4aaa791d1-000000@email.amazonses.com>

Thank you to everyone who responded. I gained a lot of insight into
statistical methods and the nature of statistical thinking. I replied
to some people privately, to limit the traffic on this OT question.

And thank you for the patience of all who were annoyed by this off-
topic question, and who didn't write to complain. I promise to limit
off-topic questions in the future.

-Kevin

On Mon, 2025-05-05 at 15:17 +0000, Kevin Zembower wrote:
> I marked this posting as Off Topic because it doesn?t specifically
> apply to R and Statistics, but is rather a general question about
> statistics and the teaching of statistics. If this is annoying to
> you,
> I apologize.
> 
> As I wrap up my work in my beginning statistics course, I?d like to
> ask
> a philosophical question regarding statistics.
> 
> In my course, we?ve learned two different ways to solve statistical
> problems: simulations, using bootstraps and randomized distributions,
> and theoretical methods, using Normal (z) and t-distributions. We?ve
> learned that both systems solve all the questions we?ve asked of
> them,
> and that both give comparable answers. Out of six chapters that we?ve
> studied in our textbook, the first four only used simulation methods.
> Only the last two used theoretical methods.
> 
> My questions are:
> 
> 1) Why don?t professional statisticians settle on one or the other,
> and
> just apply that system to their problems and work? What advantage
> does
> one system have over the other?
> 
> 2) As beginning statistics students, why is it important for us to
> learn both systems? Do you think that beginning statistics students
> will still be learning both systems in the future?
> 
> Thank you very much for your time and effort in answering my
> questions.
> I really appreciate the thoughts of the members of this group.
> 
> -Kevin
> 
> 
> 
> 




From g@@@powe|| @end|ng |rom protonm@||@com  Tue May  6 18:34:40 2025
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Tue, 06 May 2025 16:34:40 +0000
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <01000196a5bb3fe3-34fa2db9-261a-439e-83d7-39dcf7e4480b-000000@email.amazonses.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
 <bbb3b3db52f191f3200dabf2919583c0b7c6331f.camel@zembower.org>
 <01000196a5bb3fe3-34fa2db9-261a-439e-83d7-39dcf7e4480b-000000@email.amazonses.com>
Message-ID: <bPCLQzsJser3Nl3IlXEZzOh8lqRBQjGA6vTkXLxkS5B5hZCanjegHNT4eydLcDmfaPAD242YrBT7YuljcpPRViOuCDR62vn1keDUISpC6ew=@protonmail.com>



Sent from Proton Mail Android

Kevin, 
Looks like I sent the wrong URL 
Try this instead. 

https://www.biopharmaservices.com/blog/statistical-methods-the-conventional-approach-vs-the-simulation-based-approach/#:~:text=Both%20simulation,reliability%20of%20their%20statistical%20analyses

Best regards, 
Gregg
-------- Original Message --------
On 5/6/25 06:14, Kevin Zembower <kevin at zembower.org> wrote:

>  Gregg, thanks for your reply to my questions. I was looking for exactly
>  the type of information you included, especially on the strengths and
>  weaknesses of each approach.
>  
>  I was very pleased with the intuitive aspects of the simulation
>  approach in my course. This was the part that was missing from my first
>  exposure to statistics many years ago. Then, I thought of statistical
>  formulas as 'black boxes,' where numbers were fed in and results came
>  out, with unknown processes operating in between. With simulations, I
>  could count dots on a chart and come up with meaningful results.
>  
>  I missed the connection to my questions at the website you referred me
>  to, biopharmaservices.com. This seems to be the home page of a firm
>  that conducts medical studies, but I couldn't find anything about the
>  practical use of statistics. Perhaps I didn't search enough.
>  
>  Thanks, again, for your thoughts and perspective.
>  
>  -Kevin
>  
>  On Mon, 2025-05-05 at 16:05 +0000, Gregg Powell wrote:
>  > Hi Kevin,
>  > It might seem like simulation methods (bootstrapping and
>  > randomization) and traditional formulas (Normal or t-distributions)
>  > are just two ways to do the same job. So why learn both? Each
>  > approach has its own strengths, and statisticians use both in
>  > practice.
>  >
>  > Why do professionals use both?
>  > Each method offers something the other can?t. In practice, both
>  > simulation-based and theoretical techniques have unique strengths and
>  > weaknesses, and the better choice depends on the problem and its
>  > assumptions (check out - biopharmaservices.com). Simulation methods
>  > are very flexible. They don?t need strict formulas and still work
>  > even if classical conditions (like ?data must be Normal?) aren?t
>  > true. Theoretical methods are quicker and widely understood. When
>  > their assumptions hold, they give fast, exact results (a simple
>  > formula can yield a confidence interval, again, check out -
>  > biopharmaservices.com).
>  >
>  > Advantages of each approach
>  > ? Simulation-based methods: Intuitive and flexible. They require
>  > fewer assumptions, so they work well even for odd datasets.
>  > ? Theoretical methods: Quick to calculate and convenient. Based on
>  > well-known formulas and widely trusted (when standard assumptions
>  > hold).
>  >
>  > Why learn both?
>  > Knowing both makes you versatile. Simulations give you a feel for
>  > what?s happening behind the scenes, while theory provides quick
>  > shortcuts and deeper insight. A statistician might use a t-test
>  > formula for a simple case but switch to bootstrapping for a complex
>  > one. Each method can cross-check the other. Mastering both approaches
>  > gives you confidence in your results.
>  >
>  > Will future students learn both?
>  > Probably yes. Computers now make simulation methods easy to use, so
>  > they?re more common in teaching. Meanwhile, classic Normal and t
>  > methods aren?t going away ? they?re fundamental and still useful.
>  > Future students will continue to learn both, getting the best of both
>  > worlds.
>  >
>  > Good luck in your studies!
>  > gregg
>  
>  
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 583 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20250506/6bb8fa92/attachment.sig>

From @vi@e@gross m@iii@g oii gm@ii@com  Tue May  6 21:28:21 2025
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Tue, 6 May 2025 15:28:21 -0400
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <01000196a5baa180-b3e94a5a-1447-40fe-b378-88ce723c070e-000000@email.amazonses.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <67Sb_YNR1qQFBcsrK1ElicTmFMDhtZPHtCLDbmM2mSTt-TY_jDd3nrGEhifJoY3qzB6z2fuWbWPss2iWMYJPZt8yq_9-Xax2olVVSSdc6Lw=@protonmail.com>
 <CH3PR22MB451404B3410A5CD598D3739CCF8E2@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CAGxFJbTEKdLbgd8sBBF0Ws-nsac-xxB7MO-FJ3gyMQDDdtdzUQ@mail.gmail.com>
 <ed28a2ccd7bf54825cd9b8ce6a54d8d18800caa0.camel@zembower.org>
 <01000196a5baa180-b3e94a5a-1447-40fe-b378-88ce723c070e-000000@email.amazonses.com>
Message-ID: <034001dbbebd$07ab8820$17029860$@gmail.com>

Kevin,

What some call Nerdy is actually a meaningful and interesting activity to
others. Your examples are quite a bit like all kinds of things in our lives
that we suddenly decide to measure. I, for example, keep track of books I
read and use R to calculate interesting (to me) statistics about how my
reading rate per month changes or what genres I am reading more of or
compare it to earlier years and make a graph showing the overlay.

And, once you have the education and skills in a language like R, you can
volunteer to help a charitable organization collate the "tax" on various
things it wants you to appreciate and produce statistics such as the average
number of children they are thankful for, or what percent own a house.

Often enough, a problem that starts with a small dataset can grow huge if
you keep collecting data for years. Some methods may turn out to be better
to use. On the other hand, our computers and some software tend to be sped
up.

I wonder how many R functions we have been using for years to do statistics
(or other things) have been enhanced over the years, in good ways or bad, so
they now run faster or slower in some cases? As an example, some get
rewritten partially, or even completely, in some C variant or change some
underlying functions they call. Others add lots of other nice options that
you do not use, and in the process, slow down overall from your perspective.
An optimal solution may not remain optimal.

Avi

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Kevin Zembower via
R-help
Sent: Tuesday, May 6, 2025 9:13 AM
To: Bert Gunter <bgunter.4567 at gmail.com>; R-help email list
<r-help at r-project.org>
Subject: Re: [R] OT: A philosophical question about statistics

Bert, thanks so much for your response. I woke up early this morning
and couldn't go back to sleep, formulating a response. I hope I do it
justice. See in-line below.

On Mon, 2025-05-05 at 20:09 +0100, Bert Gunter wrote:
> 1. What you have been taught is mostly useless for addressing "real"
> statistical issues;

I hope this is not so, but I see your point. If by 'real' statistical
problems, you mean the ones professional statisticians face, I agree. I
kinda assumed that a one-semester course in basic statistics wouldn't
qualify me for a professional position as a statistician.

But, if you mean MY 'real' statistical issues, I hope you're wrong.
Here's two examples:

I have sleep apnea and use a CPAP machine. I have the ability to make
changes to the settings on the machine, and have software to read the
data that it writes to an SD card. The software produces data on the
apnea-hypopnea index (AHI) and length of use each night, both of which
can be used to gauge the effectiveness of its use. When I occasionally
make changes to the settings, I'd like to know if the changes improved
my health.

About three years ago, we installed split-unit AC modules in our house,
replacing individual window air conditioners. Since the split units are
connected to a heat pump, they can also be used to supplement the
output of the natural-gas-fired steam boiler that is the main souce of
heating. We were told that the split units would reduce our home's
energy usage, in both heating and cooling, and ultimately save money.
However, I'm not certain that I've seen that savings reflected in our
bills. I am serviced by a utility that allows me to download my house's
daily energy use, in both kilowatts of electricity and therms of
natural gas.

In both these examples, the results could be analyzed by the knowledge
I've gained by studying statistics, I believe. In addition, the results
have a practical significance to me, and are not self-evident without
the use of some sort of analytical tools. 

Is this nerdy? Of course! I don't imagine most, or even many, CPAP
users conduct a hypothesis test when changing their machine. But, I'm
nerdy, and now I have this new tool in my toolbox to apply to problems
like these.

> 2. Most of my 40 or so years of statistical practice involved trying
> to define the questions of interest and determining whether there
> existed or how to best obtain relevant data to answer those
> questions. Once/if that was done, how to obtain answers from the data
> was usually straightforward.

That's a good insight, and I could certainly see how it's true. Almost
all of the problems I've solved in basic statistics presented the data
in a near-perfect format. We never even had to clean up data with
missing values, for example. Certainly, applying the algorithms
(simulation or theoretical) was the easiest and quickest part.

Thank you again, Bert, for replying. I always enjoy your contributions
to this group.

-Kevin

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Tue May  6 21:53:15 2025
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Tue, 6 May 2025 15:53:15 -0400
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <01000196a5bc8d11-a053e1b8-25c4-4cb4-b672-5bf4aaa791d1-000000@email.amazonses.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <7aa6204bb5ac092b4dc90442b8369b5c8b52c149.camel@zembower.org>
 <01000196a5bc8d11-a053e1b8-25c4-4cb4-b672-5bf4aaa791d1-000000@email.amazonses.com>
Message-ID: <034901dbbec0$81f42120$85dc6360$@gmail.com>

Actually, what I would love to be discussed here as more On Topic is which functions and packages commonly used in R use the various kind of methods you mention. Are newer packages focused some way or another?

What are the drawbacks and advantages. As an example, if a simulation method can return different answers each time it is called, then I see anomalies if you try comparing it to the results obtained another way. I can imagine two methods that each return an answer between .99 and 1.01 where one may be a tad higher 90% of the time but 10% it is lower. Comparing results to each other or to a classical method that always return 1.00 can be misleading as it is not always ...

One final thought. Sometimes a classical approach may still be useful even if a newer twist seems to have advantages. A recent example I noticed was a discussion in the book The Da Vinci Code of the ???? (atbash) substitution cipher. The suggestion was to write out the alephbet/alphabet (22 letters in ancient Hebrew) from one direction to the other and then rewrite in immediately below in the opposite order. For example, the short alphabet ABCDEF would be shown as:

ABCDEF
FEDCBA

And you could encode or decode by finding the letter in one line and substituting the corresponding letter in the other line.

A character suggested a new trick that avoids duplication of just folding one copy of the line to make:

ABC
FED

You now can find a letter needed in either line and simply replace it with the same letter in the other line. This is an elegant solution, albeit with an odd number of letters adjusted a bit.

But which is easier to do in a computer? Obviously both can easily be done and perhaps the first is easier to code even if it occupies a bit more space. Basically, you do some form of search in the top line and get an index of where it is found and reference the second entry in the same index location. Then again, the second method can use a simple trick with indices to get a result in less space. But, some might argue that in a language like Python, an even simpler way is to create a dictionary/hash and skip any linear representation by just asking for atbash['A'] and letting it compute a hash and address in linear time no matter how large the alphabet being used can get.

The example may be contrived but I have seen countless places where people debate which of many methods to use and often the answer turns out to be that there are tradeoffs. In one language, there is a sort algorithm that realizes that sorting one, two and three and maybe four things is trivially done by a few IF statements and only does whatever complex sort is needed if the number of items  is larger. It works really fast for small routine tasks and even something like a merge/sort can be faster as it speeds through the regions where it is down to sorting a few things and can skip some more recursive function calls.

R also has some interesting twists in doing some calculations that may help guide what available statistical functions make sense as you can use various data structures in some but not others. Sometimes you can use a matrix or one of many kinds of data.frame, for example. 

So, I am wondering if besides base R functions, are there fairly detailed packages for statistics that perhaps may be a bit like the tidyverse and some people prefer to use a well-designed and integrated .., ?

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Kevin Zembower via R-help
Sent: Tuesday, May 6, 2025 9:15 AM
To: r-help at r-project.org
Subject: Re: [R] OT: A philosophical question about statistics

Thank you to everyone who responded. I gained a lot of insight into
statistical methods and the nature of statistical thinking. I replied
to some people privately, to limit the traffic on this OT question.

And thank you for the patience of all who were annoyed by this off-
topic question, and who didn't write to complain. I promise to limit
off-topic questions in the future.

-Kevin

On Mon, 2025-05-05 at 15:17 +0000, Kevin Zembower wrote:
> I marked this posting as Off Topic because it doesn?t specifically
> apply to R and Statistics, but is rather a general question about
> statistics and the teaching of statistics. If this is annoying to
> you,
> I apologize.
> 
> As I wrap up my work in my beginning statistics course, I?d like to
> ask
> a philosophical question regarding statistics.
> 
> In my course, we?ve learned two different ways to solve statistical
> problems: simulations, using bootstraps and randomized distributions,
> and theoretical methods, using Normal (z) and t-distributions. We?ve
> learned that both systems solve all the questions we?ve asked of
> them,
> and that both give comparable answers. Out of six chapters that we?ve
> studied in our textbook, the first four only used simulation methods.
> Only the last two used theoretical methods.
> 
> My questions are:
> 
> 1) Why don?t professional statisticians settle on one or the other,
> and
> just apply that system to their problems and work? What advantage
> does
> one system have over the other?
> 
> 2) As beginning statistics students, why is it important for us to
> learn both systems? Do you think that beginning statistics students
> will still be learning both systems in the future?
> 
> Thank you very much for your time and effort in answering my
> questions.
> I really appreciate the thoughts of the members of this group.
> 
> -Kevin
> 
> 
> 
> 



______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Tue May  6 22:58:15 2025
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 6 May 2025 21:58:15 +0100
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <01000196a5bc8d11-a053e1b8-25c4-4cb4-b672-5bf4aaa791d1-000000@email.amazonses.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <7aa6204bb5ac092b4dc90442b8369b5c8b52c149.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <01000196a5bc8d11-a053e1b8-25c4-4cb4-b672-5bf4aaa791d1-000000@email.amazonses.com>
Message-ID: <CAGxFJbQvoT9a2aGC6ceo0xpEeNnY1Aaye8=yLSrXpokS=dcfFQ@mail.gmail.com>

I am out of the country and will reply more fully to you (privately) when I
return. But briefly, and subject to my possible
misunderstanding/misinterpretation of your specification, I would say both
your examples illustrate exactly what I said. In the first, the clea

On Tue, May 6, 2025, 14:23 Kevin Zembower via R-help <r-help at r-project.org>
wrote:

> Thank you to everyone who responded. I gained a lot of insight into
> statistical methods and the nature of statistical thinking. I replied
> to some people privately, to limit the traffic on this OT question.
>
> And thank you for the patience of all who were annoyed by this off-
> topic question, and who didn't write to complain. I promise to limit
> off-topic questions in the future.
>
> -Kevin
>
> On Mon, 2025-05-05 at 15:17 +0000, Kevin Zembower wrote:
> > I marked this posting as Off Topic because it doesn?t specifically
> > apply to R and Statistics, but is rather a general question about
> > statistics and the teaching of statistics. If this is annoying to
> > you,
> > I apologize.
> >
> > As I wrap up my work in my beginning statistics course, I?d like to
> > ask
> > a philosophical question regarding statistics.
> >
> > In my course, we?ve learned two different ways to solve statistical
> > problems: simulations, using bootstraps and randomized distributions,
> > and theoretical methods, using Normal (z) and t-distributions. We?ve
> > learned that both systems solve all the questions we?ve asked of
> > them,
> > and that both give comparable answers. Out of six chapters that we?ve
> > studied in our textbook, the first four only used simulation methods.
> > Only the last two used theoretical methods.
> >
> > My questions are:
> >
> > 1) Why don?t professional statisticians settle on one or the other,
> > and
> > just apply that system to their problems and work? What advantage
> > does
> > one system have over the other?
> >
> > 2) As beginning statistics students, why is it important for us to
> > learn both systems? Do you think that beginning statistics students
> > will still be learning both systems in the future?
> >
> > Thank you very much for your time and effort in answering my
> > questions.
> > I really appreciate the thoughts of the members of this group.
> >
> > -Kevin
> >
> >
> >
> >
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue May  6 23:27:25 2025
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 6 May 2025 22:27:25 +0100
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <CAGxFJbQvoT9a2aGC6ceo0xpEeNnY1Aaye8=yLSrXpokS=dcfFQ@mail.gmail.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <7aa6204bb5ac092b4dc90442b8369b5c8b52c149.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <01000196a5bc8d11-a053e1b8-25c4-4cb4-b672-5bf4aaa791d1-000000@email.amazonses.com>
 <CAGxFJbQvoT9a2aGC6ceo0xpEeNnY1Aaye8=yLSrXpokS=dcfFQ@mail.gmail.com>
Message-ID: <CAGxFJbQXRmFxNTzyBLo-CCMEnXZr07+DUuM6ZqYwH3_79Ng6ow@mail.gmail.com>

I am out of the country and will reply more fully to you (privately) when I
return. But briefly, and subject to my possible
misunderstanding/misinterpretation of your specification, I would say both
example demonstrate my points. In the first, the clear question is how
exactly will you objectively and unbiasedly  measure your health. Note that
your day to day subjective ratings or whatever are subject to a host of
outside influences that you will need to randomize against or somehow
include as covvariates. You will also need to decide exactly how to make
whatever changes you want to make. These are all issues of experimental
design, about which you were taught nothing I expect. Anything you come up
with on the basis of your stats 101 course are likely to be pretty
worthless. Except as a placebo, of course(which actually can be effective).
As for your AC example, clearly how much electricity you use depends on
temperature, humidity, how much you were around, etc. Without this info
over several years before and after the change, there is no way that you
can make a meaningful comparison. In other words, you don't have the data
to answer the question.

Bert

On Tue, May 6, 2025, 21:58 Bert Gunter <bgunter.4567 at gmail.com> wrote:

> I am out of the country and will reply more fully to you (privately) when
> I return. But briefly, and subject to my possible
> misunderstanding/misinterpretation of your specification, I would say both
> your examples illustrate exactly what I said. In the first, the clea
>
> On Tue, May 6, 2025, 14:23 Kevin Zembower via R-help <r-help at r-project.org>
> wrote:
>
>> Thank you to everyone who responded. I gained a lot of insight into
>> statistical methods and the nature of statistical thinking. I replied
>> to some people privately, to limit the traffic on this OT question.
>>
>> And thank you for the patience of all who were annoyed by this off-
>> topic question, and who didn't write to complain. I promise to limit
>> off-topic questions in the future.
>>
>> -Kevin
>>
>> On Mon, 2025-05-05 at 15:17 +0000, Kevin Zembower wrote:
>> > I marked this posting as Off Topic because it doesn?t specifically
>> > apply to R and Statistics, but is rather a general question about
>> > statistics and the teaching of statistics. If this is annoying to
>> > you,
>> > I apologize.
>> >
>> > As I wrap up my work in my beginning statistics course, I?d like to
>> > ask
>> > a philosophical question regarding statistics.
>> >
>> > In my course, we?ve learned two different ways to solve statistical
>> > problems: simulations, using bootstraps and randomized distributions,
>> > and theoretical methods, using Normal (z) and t-distributions. We?ve
>> > learned that both systems solve all the questions we?ve asked of
>> > them,
>> > and that both give comparable answers. Out of six chapters that we?ve
>> > studied in our textbook, the first four only used simulation methods.
>> > Only the last two used theoretical methods.
>> >
>> > My questions are:
>> >
>> > 1) Why don?t professional statisticians settle on one or the other,
>> > and
>> > just apply that system to their problems and work? What advantage
>> > does
>> > one system have over the other?
>> >
>> > 2) As beginning statistics students, why is it important for us to
>> > learn both systems? Do you think that beginning statistics students
>> > will still be learning both systems in the future?
>> >
>> > Thank you very much for your time and effort in answering my
>> > questions.
>> > I really appreciate the thoughts of the members of this group.
>> >
>> > -Kevin
>> >
>> >
>> >
>> >
>>
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> https://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Wed May  7 01:43:32 2025
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Tue, 6 May 2025 19:43:32 -0400
Subject: [R] OT: A philosophical question about statistics
In-Reply-To: <CAGxFJbQXRmFxNTzyBLo-CCMEnXZr07+DUuM6ZqYwH3_79Ng6ow@mail.gmail.com>
References: <1a9bb0dacb2495b27e9bd4d6fe4fe81f38e6d2de.camel@zembower.org>
 <7aa6204bb5ac092b4dc90442b8369b5c8b52c149.camel@zembower.org>
 <01000196a105dbb5-253abca1-2772-400e-9616-ef7a7dd26e19-000000@email.amazonses.com>
 <01000196a5bc8d11-a053e1b8-25c4-4cb4-b672-5bf4aaa791d1-000000@email.amazonses.com>
 <CAGxFJbQvoT9a2aGC6ceo0xpEeNnY1Aaye8=yLSrXpokS=dcfFQ@mail.gmail.com>
 <CAGxFJbQXRmFxNTzyBLo-CCMEnXZr07+DUuM6ZqYwH3_79Ng6ow@mail.gmail.com>
Message-ID: <040101dbbee0$ad93add0$08bb0970$@gmail.com>


Bert,

I noted similar things which indicate the limits a statistician or a programmer in R can run into about what can be measured as meaningful.

But, you can get numbers out of a system that can be helpful even if some confounding factors are not accounted for.

I also have sleep apnea and have access to data on a chip in my CPAP machine and have used software to produce graphs from the data that was supplied as well as done some analysis in R for myself. In one sense, you are generally comparing two versions of yourself if you look at statistics over months when you have your machine settings one way and then months later, another way. It likely would be more useful to switch settings every once in a while during the night as they do in a sleep study accompanied by other forms of monitoring and keep adjusting to find a relatively optimum level. 

But circumstances change. If a person loses a lot of weight, it may well be their form of apnea can even go away, or a lower setting works for them. There may be changes to effectiveness based on the temperature and humidity in the room as you sleep so comparing winter to summer conditions may also not work as well. Sleep is affected by many things including dogs barking or babies crying or by lighting conditions as well as medications, fatigue and so much more.

But Kevin is not doing a scientific study nor claiming his analysis is highly valid. He is just curious and doing some experiments on himself. And, he has a goal of trying to make his own adjustments in hopes of lowering some numbers (or raising) and one question is whether any changes noted as he makes adjustments make a statistically significant difference. Presumably, after some adjusting, he may leave the settings alone and since the measurements happen automatically, perhaps check once in a while to see if his reaction has changed and maybe it is time for a tune-up.

If he were a doctor trying to do this for a patient, then indeed it might be wise to also consider other factors and measures. Examples might include measuring the pulse and oxygen saturation and see how much time is spent in things like REM sleep or in restless leg movements and  even measures to see how alert and with-it they are the next day to see if they seem rested. But, for the use he is considering, it seems like a reasonable start. The reality is most people on CPAP are not at an optimal level, or not for long. 

His other example is even more subjective for reasons you mention and many more. But, it seems reasonable enough to consider the question of whether the system he was sold does really pay for itself. Clearly there can be major factors that are large enough so comparisons are not useful. If I converted a room in my house to house dozens of servers running around the clock and ran my air conditioning to keep them cool while also installing a dozen freezers to store the food for a catering business and even added on a few rooms to the house to rent out, then the electricity used would indeed go up way more than any expected savings. If, on the other hand, I went away for a 6-month sabbatical and disconnected all appliances, ...

In such analyses, the signal can be drowned by the noise. A serious analysis works best if many such variables are controlled or held within a range where the defects are smaller than the expected results. If someone suggests you may see a 1% improvement, the noise may prevail. If they claim your costs will drop in half, and you believe you have not changed your habits substantially, then seeing a 2% improvement suggests you were misled. Seeing an improvement of 40% or 60% might be compatible with enough "confidence" after doing some loose statistical sense. But perhaps only enough to convince yourself, not something you can take to court.

I think a great way to learn is along the lines of what Kevin is doing. Don't just learn. Try to use it and see what happens and get feedback from others.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
Sent: Tuesday, May 6, 2025 5:27 PM
To: Kevin Zembower <kevin at zembower.org>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] OT: A philosophical question about statistics

I am out of the country and will reply more fully to you (privately) when I
return. But briefly, and subject to my possible
misunderstanding/misinterpretation of your specification, I would say both
example demonstrate my points. In the first, the clear question is how
exactly will you objectively and unbiasedly  measure your health. Note that
your day to day subjective ratings or whatever are subject to a host of
outside influences that you will need to randomize against or somehow
include as covvariates. You will also need to decide exactly how to make
whatever changes you want to make. These are all issues of experimental
design, about which you were taught nothing I expect. Anything you come up
with on the basis of your stats 101 course are likely to be pretty
worthless. Except as a placebo, of course(which actually can be effective).
As for your AC example, clearly how much electricity you use depends on
temperature, humidity, how much you were around, etc. Without this info
over several years before and after the change, there is no way that you
can make a meaningful comparison. In other words, you don't have the data
to answer the question.

Bert

On Tue, May 6, 2025, 21:58 Bert Gunter <bgunter.4567 at gmail.com> wrote:

> I am out of the country and will reply more fully to you (privately) when
> I return. But briefly, and subject to my possible
> misunderstanding/misinterpretation of your specification, I would say both
> your examples illustrate exactly what I said. In the first, the clea
>
> On Tue, May 6, 2025, 14:23 Kevin Zembower via R-help <r-help at r-project.org>
> wrote:
>
>> Thank you to everyone who responded. I gained a lot of insight into
>> statistical methods and the nature of statistical thinking. I replied
>> to some people privately, to limit the traffic on this OT question.
>>
>> And thank you for the patience of all who were annoyed by this off-
>> topic question, and who didn't write to complain. I promise to limit
>> off-topic questions in the future.
>>
>> -Kevin
>>
>> On Mon, 2025-05-05 at 15:17 +0000, Kevin Zembower wrote:
>> > I marked this posting as Off Topic because it doesn?t specifically
>> > apply to R and Statistics, but is rather a general question about
>> > statistics and the teaching of statistics. If this is annoying to
>> > you,
>> > I apologize.
>> >
>> > As I wrap up my work in my beginning statistics course, I?d like to
>> > ask
>> > a philosophical question regarding statistics.
>> >
>> > In my course, we?ve learned two different ways to solve statistical
>> > problems: simulations, using bootstraps and randomized distributions,
>> > and theoretical methods, using Normal (z) and t-distributions. We?ve
>> > learned that both systems solve all the questions we?ve asked of
>> > them,
>> > and that both give comparable answers. Out of six chapters that we?ve
>> > studied in our textbook, the first four only used simulation methods.
>> > Only the last two used theoretical methods.
>> >
>> > My questions are:
>> >
>> > 1) Why don?t professional statisticians settle on one or the other,
>> > and
>> > just apply that system to their problems and work? What advantage
>> > does
>> > one system have over the other?
>> >
>> > 2) As beginning statistics students, why is it important for us to
>> > learn both systems? Do you think that beginning statistics students
>> > will still be learning both systems in the future?
>> >
>> > Thank you very much for your time and effort in answering my
>> > questions.
>> > I really appreciate the thoughts of the members of this group.
>> >
>> > -Kevin
>> >
>> >
>> >
>> >
>>
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> https://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From d@n|e|obo9976 @end|ng |rom gm@||@com  Wed May  7 16:36:02 2025
From: d@n|e|obo9976 @end|ng |rom gm@||@com (Daniel Lobo)
Date: Wed, 7 May 2025 20:06:02 +0530
Subject: [R] How to get the same result for GA optimization?
Message-ID: <CADZb7hpexG0WjSNvoeuaYPX0J5f3MVSQ1MYnM+Xt7XUFteDLBw@mail.gmail.com>

I am using *Genetic Algorithm* to maximize some function which use data.

I use GA package in R for this (
https://cran.r-project.org/web/packages/GA/index.html)

Below is my code

library(GA)
set.seed(1)
Dat = data.frame(rnorm(1000), matrix(rnorm(1000 * 30), nc = 30))

Fitness_Fn = function(x) {
    return(cor(Dat[, 1], as.matrix(Dat[, -1]) %*% matrix(x, nc = 1))[1,1])
}

ga(type = 'real-valued', fitness = Fitness_Fn, seed = 1, lower =
rep(0, 30), upper = rep(1, 30), elitism = 10, popSize = 200, maxiter =
1, pcrossover = 0.9, pmutation = 0.9, run = 1)

However now I alter the columns of my data and rerun the GA

Dat = Dat[, c(1, 1 + sample(1:30, 30, replace = F))]
Fitness_Fn = function(x) {
    return(cor(Dat[, 1], as.matrix(Dat[, -1]) %*% matrix(x, nc = 1))[1,1])
}

ga(type = 'real-valued', fitness = Fitness_Fn, seed = 1, lower =
rep(0, 30), upper = rep(1, 30), elitism = 10, popSize = 200, maxiter =
1, pcrossover = 0.9, pmutation = 0.9, run = 1)

Surprisingly, I get different result from above 2 implementations.

In first case, I get

GA | iter = 1 | Mean = 0.01534124 | Best = 0.04351926

In second case,

GA | iter = 1 | Mean = 0.01705027 | Best = 0.04454167

I have fixed the random number generation using seed = 1 in the ga() function,
So I am expecting I would get exactly same result.

Could you please help identify the issue here? I want to get exactly same
result irrespective of the order of the columns of dat for above function.

Thanks for your time.

	[[alternative HTML version deleted]]


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Wed May  7 18:02:44 2025
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Wed, 7 May 2025 17:02:44 +0100
Subject: [R] How to get the same result for GA optimization?
In-Reply-To: <CADZb7hpexG0WjSNvoeuaYPX0J5f3MVSQ1MYnM+Xt7XUFteDLBw@mail.gmail.com>
References: <CADZb7hpexG0WjSNvoeuaYPX0J5f3MVSQ1MYnM+Xt7XUFteDLBw@mail.gmail.com>
Message-ID: <86833566-e041-4bcc-8c27-ed48b0e221d4@dewey.myzen.co.uk>

Dear Daniel

As far as I can see you have re-generated the data before calling ga() 
so the data is not just a permutation of the first set.

Michael

On 07/05/2025 15:36, Daniel Lobo wrote:
> I am using *Genetic Algorithm* to maximize some function which use data.
> 
> I use GA package in R for this (
> https://cran.r-project.org/web/packages/GA/index.html)
> 
> Below is my code
> 
> library(GA)
> set.seed(1)
> Dat = data.frame(rnorm(1000), matrix(rnorm(1000 * 30), nc = 30))
> 
> Fitness_Fn = function(x) {
>      return(cor(Dat[, 1], as.matrix(Dat[, -1]) %*% matrix(x, nc = 1))[1,1])
> }
> 
> ga(type = 'real-valued', fitness = Fitness_Fn, seed = 1, lower =
> rep(0, 30), upper = rep(1, 30), elitism = 10, popSize = 200, maxiter =
> 1, pcrossover = 0.9, pmutation = 0.9, run = 1)
> 
> However now I alter the columns of my data and rerun the GA
> 
> Dat = Dat[, c(1, 1 + sample(1:30, 30, replace = F))]
> Fitness_Fn = function(x) {
>      return(cor(Dat[, 1], as.matrix(Dat[, -1]) %*% matrix(x, nc = 1))[1,1])
> }
> 
> ga(type = 'real-valued', fitness = Fitness_Fn, seed = 1, lower =
> rep(0, 30), upper = rep(1, 30), elitism = 10, popSize = 200, maxiter =
> 1, pcrossover = 0.9, pmutation = 0.9, run = 1)
> 
> Surprisingly, I get different result from above 2 implementations.
> 
> In first case, I get
> 
> GA | iter = 1 | Mean = 0.01534124 | Best = 0.04351926
> 
> In second case,
> 
> GA | iter = 1 | Mean = 0.01705027 | Best = 0.04454167
> 
> I have fixed the random number generation using seed = 1 in the ga() function,
> So I am expecting I would get exactly same result.
> 
> Could you please help identify the issue here? I want to get exactly same
> result irrespective of the order of the columns of dat for above function.
> 
> Thanks for your time.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Michael Dewey


From d@n|e|obo9976 @end|ng |rom gm@||@com  Wed May  7 18:30:59 2025
From: d@n|e|obo9976 @end|ng |rom gm@||@com (Daniel Lobo)
Date: Wed, 7 May 2025 22:00:59 +0530
Subject: [R] How to get the same result for GA optimization?
In-Reply-To: <86833566-e041-4bcc-8c27-ed48b0e221d4@dewey.myzen.co.uk>
References: <CADZb7hpexG0WjSNvoeuaYPX0J5f3MVSQ1MYnM+Xt7XUFteDLBw@mail.gmail.com>
 <86833566-e041-4bcc-8c27-ed48b0e221d4@dewey.myzen.co.uk>
Message-ID: <CADZb7hqYgL5zrm71W7H-rTqg-XQZ7WBttoz8RKbHeVY2VrVJ8A@mail.gmail.com>

Hi,

Before calling the second ga, I am just altering the columns of dat except
the first column

Dat = Dat[, c(1, 1 + sample(1:30, 30, replace = F))]

With that, I expect the positions of the elements of x will be
changed, but objective function should return the same value, same for
ga. Below is an illustration.


Altered_Pos = 1 + sample(1:30, 30, replace = F)
x = rnorm(30)
Fitness_Fn(x) ###  -0.0004543259

Dat = Dat[, c(1, Altered_Pos)]
Fitness_Fn(x[Altered_Pos - 1])  ### -0.0004543259


On Wed, 7 May 2025 at 21:32, Michael Dewey <lists at dewey.myzen.co.uk> wrote:

> Dear Daniel
>
> As far as I can see you have re-generated the data before calling ga()
> so the data is not just a permutation of the first set.
>
> Michael
>
> On 07/05/2025 15:36, Daniel Lobo wrote:
> > I am using *Genetic Algorithm* to maximize some function which use data.
> >
> > I use GA package in R for this (
> > https://cran.r-project.org/web/packages/GA/index.html)
> >
> > Below is my code
> >
> > library(GA)
> > set.seed(1)
> > Dat = data.frame(rnorm(1000), matrix(rnorm(1000 * 30), nc = 30))
> >
> > Fitness_Fn = function(x) {
> >      return(cor(Dat[, 1], as.matrix(Dat[, -1]) %*% matrix(x, nc =
> 1))[1,1])
> > }
> >
> > ga(type = 'real-valued', fitness = Fitness_Fn, seed = 1, lower =
> > rep(0, 30), upper = rep(1, 30), elitism = 10, popSize = 200, maxiter =
> > 1, pcrossover = 0.9, pmutation = 0.9, run = 1)
> >
> > However now I alter the columns of my data and rerun the GA
> >
> > Dat = Dat[, c(1, 1 + sample(1:30, 30, replace = F))]
> > Fitness_Fn = function(x) {
> >      return(cor(Dat[, 1], as.matrix(Dat[, -1]) %*% matrix(x, nc =
> 1))[1,1])
> > }
> >
> > ga(type = 'real-valued', fitness = Fitness_Fn, seed = 1, lower =
> > rep(0, 30), upper = rep(1, 30), elitism = 10, popSize = 200, maxiter =
> > 1, pcrossover = 0.9, pmutation = 0.9, run = 1)
> >
> > Surprisingly, I get different result from above 2 implementations.
> >
> > In first case, I get
> >
> > GA | iter = 1 | Mean = 0.01534124 | Best = 0.04351926
> >
> > In second case,
> >
> > GA | iter = 1 | Mean = 0.01705027 | Best = 0.04454167
> >
> > I have fixed the random number generation using seed = 1 in the ga()
> function,
> > So I am expecting I would get exactly same result.
> >
> > Could you please help identify the issue here? I want to get exactly same
> > result irrespective of the order of the columns of dat for above
> function.
> >
> > Thanks for your time.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Michael Dewey
>
>

	[[alternative HTML version deleted]]


From m@dee|en2 @end|ng |rom @tudent@vu@n|  Wed May  7 08:53:42 2025
From: m@dee|en2 @end|ng |rom @tudent@vu@n| (Deelen, M. (Mirjam))
Date: Wed, 7 May 2025 06:53:42 +0000
Subject: [R] Help merging large datasets in R
Message-ID: <AM7PR09MB37991DBC0591FD97C262BC63E088A@AM7PR09MB3799.eurprd09.prod.outlook.com>

Hi guys,
For my MSc. thesis i am using R studio. The goal is for me to merge a couple (6) of relatively large datasets (min of 200.000 and max of 2mil rows). I have now been able to do so, however I think something might be going wrong in my codes.
For reference, i have a dataset 1 (200.000), dataset 2 (600.000), dataset 3 (2mil) and dataset 4 (2mil) merged into one dataset of 4mil, and dataset 5 (4mil) and dataset 6 (4mil) merged into one dataset of 8mil.
What i have done so far is the following:

  *   Merged dataset 1 and dataset 2 using the following code = merged 1 <- dataset 2[dataset 1, nomatch = NA]. This results in a dataset of 600.000 (looks to be alright).
  *   Merged the dataset merged 1 and datasets 3/4 using the following code = merged 2 <- dataset 3/4[merged 1, nomatch = NA, allow.cartesian = TRUE]. This results in a dataset of 21mil (as expected). To this i have applied an additional criteria (dates in dataset 3/4 should be within 365 days of the dates in merged 1), which reduces merged 2 to around 170.000.
  *   Merged the dataset merged 2 and datasets 5/6 using the following code = merged 3 <- dataset 5/6[merged 2, nomatch = NA, allow.cartesian = TRUE]. Again, this results in a dataset of 8mil (as expected). And again, to this i have applied an additional criteria (dates in dataset 5/6 should be within 365 days of the dates in merged 2), which reduces merged 3 to around 50.000.

What I'm now thinking, is how can the merging + additional criteria lead to such a loss of cases ?? The first merge, of dataset 1 and dataset 2, results in an amount that I think should be the final amount of cases. I understand that by adding an additional criteria the number of possible matches when merging datasets 3/4 and 5/6 is reduced, but I'm not sure this should lead to SUCH a loss. Besides this, the additional criteria was added to reduce the duplication of information that is now happening when merging datasets 3/4 and 5/6.
All cases appear once in dataset 1, but could appear a couple more times in the following datasets (say twice in dataset 2, four times in datasets 3/4 and 8 times in datasets 5/6). Which results in a 1 x 2 x 4 x 8 duplication of information when merging the datasets without additional criteria.
So sum this up, my questions are=

  *   Are there any tips as to not have this duplication ? (so I can drop the additonal criteria and the final amount of cases, probably, increases).
  *   Or are there any tips as to figure out where in these steps cases are lost ?

Thanks!
Mirjam


	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Wed May  7 22:46:47 2025
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Wed, 7 May 2025 20:46:47 +0000
Subject: [R] Help merging large datasets in R
In-Reply-To: <AM7PR09MB37991DBC0591FD97C262BC63E088A@AM7PR09MB3799.eurprd09.prod.outlook.com>
References: <AM7PR09MB37991DBC0591FD97C262BC63E088A@AM7PR09MB3799.eurprd09.prod.outlook.com>
Message-ID: <CH3PR22MB451407D3E25ECA21B18979FDCF88A@CH3PR22MB4514.namprd22.prod.outlook.com>

Some issues:
1) Variable names cannot have spaces. "merged 1" is not valid but "merged_1" is a valid alternative.
2) You need to tell R what to merge by. It looks like you may be using data tables rather than a data frame.
merged <- dataset2[dataset1, on = "id", nomatch = NA]

3) Alternatively: join functions from the dplyr package, cbind(), and rbind() can be used in different ways to combine data.
4) Make sure the process is successful. It looked like 200K + 600K + 2000K + 2000K = 4000K which is obviously wrong. If I merge 200 k and 600k and get back 600k, I am either not telling the whole story or I made an error. Maybe I have 200k observations of 10 variables and 600k observations of 4 variables and I get an object of 600k with 13 variables (one is lost because it was present in both and used to merge the two data sets).

Please try again. Possibly start by extracting the first ten rows from each data set. Try merging that where it is easy to check. Then expand to using all the data. Your approach should work with a little modification. Alternatively make a couple of small fake data sets and play with those. Once you have the code correct there should be no problem expanding to the real data.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Deelen, M. (Mirjam) via R-help
Sent: Wednesday, May 7, 2025 2:54 AM
To: r-help at r-project.org
Subject: [R] Help merging large datasets in R

[External Email]

Hi guys,
For my MSc. thesis i am using R studio. The goal is for me to merge a couple (6) of relatively large datasets (min of 200.000 and max of 2mil rows). I have now been able to do so, however I think something might be going wrong in my codes.
For reference, i have a dataset 1 (200.000), dataset 2 (600.000), dataset 3 (2mil) and dataset 4 (2mil) merged into one dataset of 4mil, and dataset 5 (4mil) and dataset 6 (4mil) merged into one dataset of 8mil.
What i have done so far is the following:

  *   Merged dataset 1 and dataset 2 using the following code = merged 1 <- dataset 2[dataset 1, nomatch = NA]. This results in a dataset of 600.000 (looks to be alright).
  *   Merged the dataset merged 1 and datasets 3/4 using the following code = merged 2 <- dataset 3/4[merged 1, nomatch = NA, allow.cartesian = TRUE]. This results in a dataset of 21mil (as expected). To this i have applied an additional criteria (dates in dataset 3/4 should be within 365 days of the dates in merged 1), which reduces merged 2 to around 170.000.
  *   Merged the dataset merged 2 and datasets 5/6 using the following code = merged 3 <- dataset 5/6[merged 2, nomatch = NA, allow.cartesian = TRUE]. Again, this results in a dataset of 8mil (as expected). And again, to this i have applied an additional criteria (dates in dataset 5/6 should be within 365 days of the dates in merged 2), which reduces merged 3 to around 50.000.

What I'm now thinking, is how can the merging + additional criteria lead to such a loss of cases ?? The first merge, of dataset 1 and dataset 2, results in an amount that I think should be the final amount of cases. I understand that by adding an additional criteria the number of possible matches when merging datasets 3/4 and 5/6 is reduced, but I'm not sure this should lead to SUCH a loss. Besides this, the additional criteria was added to reduce the duplication of information that is now happening when merging datasets 3/4 and 5/6.
All cases appear once in dataset 1, but could appear a couple more times in the following datasets (say twice in dataset 2, four times in datasets 3/4 and 8 times in datasets 5/6). Which results in a 1 x 2 x 4 x 8 duplication of information when merging the datasets without additional criteria.
So sum this up, my questions are=

  *   Are there any tips as to not have this duplication ? (so I can drop the additonal criteria and the final amount of cases, probably, increases).
  *   Or are there any tips as to figure out where in these steps cases are lost ?

Thanks!
Mirjam


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May  8 02:29:01 2025
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 07 May 2025 17:29:01 -0700
Subject: [R] Help merging large datasets in R
In-Reply-To: <CH3PR22MB451407D3E25ECA21B18979FDCF88A@CH3PR22MB4514.namprd22.prod.outlook.com>
References: <AM7PR09MB37991DBC0591FD97C262BC63E088A@AM7PR09MB3799.eurprd09.prod.outlook.com>
 <CH3PR22MB451407D3E25ECA21B18979FDCF88A@CH3PR22MB4514.namprd22.prod.outlook.com>
Message-ID: <1DC72BA8-0250-48BA-91C4-677E2157C429@dcn.davis.ca.us>

> Variable names cannot have spaces

Please soften your words... variables can have all sorts of characters including spaces in them, but it can be inconvenient to quote them all with back-tick quotes like `merged 1` so where possible most people avoid variable names with weird characters.

People also often have to work with column names provided by an external source. It can be harder to explain to people familiar with the original data that you renamed all the columns they see because it was inconvenient not to. If you have that luxury, fine... but R can absolutely avoid munging column names if you use the relevant import parameters for your preferred import function.


On May 7, 2025 1:46:47 PM PDT, "Ebert,Timothy Aaron" <tebert at ufl.edu> wrote:
>Some issues:
>1) Variable names cannot have spaces. "merged 1" is not valid but "merged_1" is a valid alternative.
>2) You need to tell R what to merge by. It looks like you may be using data tables rather than a data frame.
>merged <- dataset2[dataset1, on = "id", nomatch = NA]
>
>3) Alternatively: join functions from the dplyr package, cbind(), and rbind() can be used in different ways to combine data.
>4) Make sure the process is successful. It looked like 200K + 600K + 2000K + 2000K = 4000K which is obviously wrong. If I merge 200 k and 600k and get back 600k, I am either not telling the whole story or I made an error. Maybe I have 200k observations of 10 variables and 600k observations of 4 variables and I get an object of 600k with 13 variables (one is lost because it was present in both and used to merge the two data sets).
>
>Please try again. Possibly start by extracting the first ten rows from each data set. Try merging that where it is easy to check. Then expand to using all the data. Your approach should work with a little modification. Alternatively make a couple of small fake data sets and play with those. Once you have the code correct there should be no problem expanding to the real data.
>
>Tim
>
>-----Original Message-----
>From: R-help <r-help-bounces at r-project.org> On Behalf Of Deelen, M. (Mirjam) via R-help
>Sent: Wednesday, May 7, 2025 2:54 AM
>To: r-help at r-project.org
>Subject: [R] Help merging large datasets in R
>
>[External Email]
>
>Hi guys,
>For my MSc. thesis i am using R studio. The goal is for me to merge a couple (6) of relatively large datasets (min of 200.000 and max of 2mil rows). I have now been able to do so, however I think something might be going wrong in my codes.
>For reference, i have a dataset 1 (200.000), dataset 2 (600.000), dataset 3 (2mil) and dataset 4 (2mil) merged into one dataset of 4mil, and dataset 5 (4mil) and dataset 6 (4mil) merged into one dataset of 8mil.
>What i have done so far is the following:
>
>  *   Merged dataset 1 and dataset 2 using the following code = merged 1 <- dataset 2[dataset 1, nomatch = NA]. This results in a dataset of 600.000 (looks to be alright).
>  *   Merged the dataset merged 1 and datasets 3/4 using the following code = merged 2 <- dataset 3/4[merged 1, nomatch = NA, allow.cartesian = TRUE]. This results in a dataset of 21mil (as expected). To this i have applied an additional criteria (dates in dataset 3/4 should be within 365 days of the dates in merged 1), which reduces merged 2 to around 170.000.
>  *   Merged the dataset merged 2 and datasets 5/6 using the following code = merged 3 <- dataset 5/6[merged 2, nomatch = NA, allow.cartesian = TRUE]. Again, this results in a dataset of 8mil (as expected). And again, to this i have applied an additional criteria (dates in dataset 5/6 should be within 365 days of the dates in merged 2), which reduces merged 3 to around 50.000.
>
>What I'm now thinking, is how can the merging + additional criteria lead to such a loss of cases ?? The first merge, of dataset 1 and dataset 2, results in an amount that I think should be the final amount of cases. I understand that by adding an additional criteria the number of possible matches when merging datasets 3/4 and 5/6 is reduced, but I'm not sure this should lead to SUCH a loss. Besides this, the additional criteria was added to reduce the duplication of information that is now happening when merging datasets 3/4 and 5/6.
>All cases appear once in dataset 1, but could appear a couple more times in the following datasets (say twice in dataset 2, four times in datasets 3/4 and 8 times in datasets 5/6). Which results in a 1 x 2 x 4 x 8 duplication of information when merging the datasets without additional criteria.
>So sum this up, my questions are=
>
>  *   Are there any tips as to not have this duplication ? (so I can drop the additonal criteria and the final amount of cases, probably, increases).
>  *   Or are there any tips as to figure out where in these steps cases are lost ?
>
>Thanks!
>Mirjam
>
>
>        [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May  8 03:03:03 2025
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 07 May 2025 18:03:03 -0700
Subject: [R] Help merging large datasets in R
In-Reply-To: <AM7PR09MB37991DBC0591FD97C262BC63E088A@AM7PR09MB3799.eurprd09.prod.outlook.com>
References: <AM7PR09MB37991DBC0591FD97C262BC63E088A@AM7PR09MB3799.eurprd09.prod.outlook.com>
Message-ID: <A5EEBB16-27B4-4905-9D70-0F5005D66408@dcn.davis.ca.us>

I suggest you study a bit more about types of join (a.k.a. merge) operations. In particular, most of the time you will want to use inner join or left join operations with large data sets. In order to avoid problems with duplication you will need to pay close attention to which set of columns can be used to uniquely identify each and every record. Then you want to confirm that there are no duplicates where you don't want them. Cartesian joins are very specialized and almost certainly should only be used on small data frames... I suspect you should be stacking the data frames rather than building all combinations of rows.

Please do read the Posting Guide... and try to make a minimal reproducible example (Google it, and or study the vignette that comes with the reprex package) that illustrates with a few rows from each table what is going on that you want to fix. The dput function is another big help in constructing this. The help list really works best when we share code snippets that don't work right in our R sessions and comment on them. Make an effort to learn how your email client can be configured to compose your email in plain text so the formatting does not corrupt your code example.

On May 6, 2025 11:53:42 PM PDT, "Deelen, M. (Mirjam) via R-help" <r-help at r-project.org> wrote:
>Hi guys,
>For my MSc. thesis i am using R studio. The goal is for me to merge a couple (6) of relatively large datasets (min of 200.000 and max of 2mil rows). I have now been able to do so, however I think something might be going wrong in my codes.
>For reference, i have a dataset 1 (200.000), dataset 2 (600.000), dataset 3 (2mil) and dataset 4 (2mil) merged into one dataset of 4mil, and dataset 5 (4mil) and dataset 6 (4mil) merged into one dataset of 8mil.
>What i have done so far is the following:
>
>  *   Merged dataset 1 and dataset 2 using the following code = merged 1 <- dataset 2[dataset 1, nomatch = NA]. This results in a dataset of 600.000 (looks to be alright).
>  *   Merged the dataset merged 1 and datasets 3/4 using the following code = merged 2 <- dataset 3/4[merged 1, nomatch = NA, allow.cartesian = TRUE]. This results in a dataset of 21mil (as expected). To this i have applied an additional criteria (dates in dataset 3/4 should be within 365 days of the dates in merged 1), which reduces merged 2 to around 170.000.
>  *   Merged the dataset merged 2 and datasets 5/6 using the following code = merged 3 <- dataset 5/6[merged 2, nomatch = NA, allow.cartesian = TRUE]. Again, this results in a dataset of 8mil (as expected). And again, to this i have applied an additional criteria (dates in dataset 5/6 should be within 365 days of the dates in merged 2), which reduces merged 3 to around 50.000.
>
>What I'm now thinking, is how can the merging + additional criteria lead to such a loss of cases ?? The first merge, of dataset 1 and dataset 2, results in an amount that I think should be the final amount of cases. I understand that by adding an additional criteria the number of possible matches when merging datasets 3/4 and 5/6 is reduced, but I'm not sure this should lead to SUCH a loss. Besides this, the additional criteria was added to reduce the duplication of information that is now happening when merging datasets 3/4 and 5/6.
>All cases appear once in dataset 1, but could appear a couple more times in the following datasets (say twice in dataset 2, four times in datasets 3/4 and 8 times in datasets 5/6). Which results in a 1 x 2 x 4 x 8 duplication of information when merging the datasets without additional criteria.
>So sum this up, my questions are=
>
>  *   Are there any tips as to not have this duplication ? (so I can drop the additonal criteria and the final amount of cases, probably, increases).
>  *   Or are there any tips as to figure out where in these steps cases are lost ?
>
>Thanks!
>Mirjam
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From tebert @end|ng |rom u||@edu  Thu May  8 17:24:56 2025
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 8 May 2025 15:24:56 +0000
Subject: [R] Help merging large datasets in R
In-Reply-To: <1DC72BA8-0250-48BA-91C4-677E2157C429@dcn.davis.ca.us>
References: <AM7PR09MB37991DBC0591FD97C262BC63E088A@AM7PR09MB3799.eurprd09.prod.outlook.com>
 <CH3PR22MB451407D3E25ECA21B18979FDCF88A@CH3PR22MB4514.namprd22.prod.outlook.com>
 <1DC72BA8-0250-48BA-91C4-677E2157C429@dcn.davis.ca.us>
Message-ID: <CH3PR22MB45141495AA1B17EAF71CDF3ACF8BA@CH3PR22MB4514.namprd22.prod.outlook.com>

Fair enough.
Apologies for the oversight.
It is possible to get R to read the variable names from the file and then run an analysis of whatever those names are without specifying variable names in the code. The names can be anything the client wants because I never type in the name. I am just starting to try this approach, looping through all the variable names, and generating a formula from each index value. I want a univariate model for each variable in the dataset (or something like that).

I started in SAS. My default approach is to munge the names in Excel and then at the end un-munge them (demunge them?). Sometimes the client does not care because the changes are simple. 'pound solids per box' becomes pound_solids_per_box and '% infected' becomes Pct_infected where the client has already used "pct" as a short form for percent in some other variable.

I get confused easily. I could have a variable 'Percent E2 > 100' and that gets put into a formula that becomes 'Percent E2 > 100' = 'Number of C < 50' + Time to 1st E2 > 100' and I find the small tick marks look like a bit of dirt on my computer screen and everything goes down hill from there. If I do not start down this road, then I have fewer problems later on. I might make this PrcntE2Gt100 = NumCLt50 + TmFrstE2Gt100, but that is just my preference.

Most programming languages restrict variable names to alphanumeric characters, and I seem to recall that the name should not have a number as the first character. Putting variable names in quotes to allow for other characters is a workaround. Without that the computer will start by looking for a variable Percent if it encounters "Percent E2 > 100 = Number of C < 50 + Time to 1st E2 > 100" and failing to find that variable it returns an error.

ChatGPT gives these rules for variable names in R:
1) Start with a letter (A-Z, a-z) or a dot (.) not followed by a number.
2) Contain only letters, numbers, dots (.), and underscores (_).
3) Not contain spaces.
4) Not be a reserved word (like if, for, TRUE, NULL, etc.).
5) Avoid starting with a digit or special characters like @, $, %, etc.
6) You can avoid these restrictions by putting variables in backticks. In a limited number of cases putting variables into quotes will also work: df[["bad name"]] or assign("bad name", 5). Treating a variable name as a string is very useful in some cases. Say I have a data frame with 1000 variables, and I want to make a plot of all variables where the variable name includes "E2". This is one line of code in R: E2_vars <- df[ , grep("E2", names(df))]

When I was learning programming in the 80's the rules were more restrictive and there was no workaround. I am thankful that variable names can now be longer than 8 characters. I would suggest trying very hard to work within the outlined rules 1 through 5. Using backticks to enable invalid names can make code harder to read and debug. While it makes my brain hurt, if the aesthetic is to make things difficult then:

library(ggplot2)
df <- data.frame(
  check.names = FALSE,
  `mean(x)` = rnorm(100, mean = 5),
  `log(y + 1)` = rnorm(100, mean = 2),
  group = sample(c("A", "B"), 100, replace = TRUE)
)
print(names(df))  # "mean(x)", "log(y + 1)", "group"
ggplot(df, aes(x = `mean(x)`, y = `log(y + 1)`, color = group)) +
  geom_point() +
  labs(title = "Obfuscate Fake Data")

To have more fun, log transform these variables before plotting so that parts of the variable names are also part of the valid code. For me, coding is hard enough without additional complexity.

Tim

-----Original Message-----
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Wednesday, May 7, 2025 8:29 PM
To: r-help at r-project.org; Ebert,Timothy Aaron <tebert at ufl.edu>; Deelen, M. (Mirjam) <m.deelen2 at student.vu.nl>; r-help at r-project.org
Subject: Re: [R] Help merging large datasets in R

[External Email]

> Variable names cannot have spaces

Please soften your words... variables can have all sorts of characters including spaces in them, but it can be inconvenient to quote them all with back-tick quotes like `merged 1` so where possible most people avoid variable names with weird characters.

People also often have to work with column names provided by an external source. It can be harder to explain to people familiar with the original data that you renamed all the columns they see because it was inconvenient not to. If you have that luxury, fine... but R can absolutely avoid munging column names if you use the relevant import parameters for your preferred import function.


On May 7, 2025 1:46:47 PM PDT, "Ebert,Timothy Aaron" <tebert at ufl.edu> wrote:
>Some issues:
>1) Variable names cannot have spaces. "merged 1" is not valid but "merged_1" is a valid alternative.
>2) You need to tell R what to merge by. It looks like you may be using data tables rather than a data frame.
>merged <- dataset2[dataset1, on = "id", nomatch = NA]
>
>3) Alternatively: join functions from the dplyr package, cbind(), and rbind() can be used in different ways to combine data.
>4) Make sure the process is successful. It looked like 200K + 600K + 2000K + 2000K = 4000K which is obviously wrong. If I merge 200 k and 600k and get back 600k, I am either not telling the whole story or I made an error. Maybe I have 200k observations of 10 variables and 600k observations of 4 variables and I get an object of 600k with 13 variables (one is lost because it was present in both and used to merge the two data sets).
>
>Please try again. Possibly start by extracting the first ten rows from each data set. Try merging that where it is easy to check. Then expand to using all the data. Your approach should work with a little modification. Alternatively make a couple of small fake data sets and play with those. Once you have the code correct there should be no problem expanding to the real data.
>
>Tim
>
>-----Original Message-----
>From: R-help <r-help-bounces at r-project.org> On Behalf Of Deelen, M.
>(Mirjam) via R-help
>Sent: Wednesday, May 7, 2025 2:54 AM
>To: r-help at r-project.org
>Subject: [R] Help merging large datasets in R
>
>[External Email]
>
>Hi guys,
>For my MSc. thesis i am using R studio. The goal is for me to merge a couple (6) of relatively large datasets (min of 200.000 and max of 2mil rows). I have now been able to do so, however I think something might be going wrong in my codes.
>For reference, i have a dataset 1 (200.000), dataset 2 (600.000), dataset 3 (2mil) and dataset 4 (2mil) merged into one dataset of 4mil, and dataset 5 (4mil) and dataset 6 (4mil) merged into one dataset of 8mil.
>What i have done so far is the following:
>
>  *   Merged dataset 1 and dataset 2 using the following code = merged 1 <- dataset 2[dataset 1, nomatch = NA]. This results in a dataset of 600.000 (looks to be alright).
>  *   Merged the dataset merged 1 and datasets 3/4 using the following code = merged 2 <- dataset 3/4[merged 1, nomatch = NA, allow.cartesian = TRUE]. This results in a dataset of 21mil (as expected). To this i have applied an additional criteria (dates in dataset 3/4 should be within 365 days of the dates in merged 1), which reduces merged 2 to around 170.000.
>  *   Merged the dataset merged 2 and datasets 5/6 using the following code = merged 3 <- dataset 5/6[merged 2, nomatch = NA, allow.cartesian = TRUE]. Again, this results in a dataset of 8mil (as expected). And again, to this i have applied an additional criteria (dates in dataset 5/6 should be within 365 days of the dates in merged 2), which reduces merged 3 to around 50.000.
>
>What I'm now thinking, is how can the merging + additional criteria lead to such a loss of cases ?? The first merge, of dataset 1 and dataset 2, results in an amount that I think should be the final amount of cases. I understand that by adding an additional criteria the number of possible matches when merging datasets 3/4 and 5/6 is reduced, but I'm not sure this should lead to SUCH a loss. Besides this, the additional criteria was added to reduce the duplication of information that is now happening when merging datasets 3/4 and 5/6.
>All cases appear once in dataset 1, but could appear a couple more times in the following datasets (say twice in dataset 2, four times in datasets 3/4 and 8 times in datasets 5/6). Which results in a 1 x 2 x 4 x 8 duplication of information when merging the datasets without additional criteria.
>So sum this up, my questions are=
>
>  *   Are there any tips as to not have this duplication ? (so I can drop the additonal criteria and the final amount of cases, probably, increases).
>  *   Or are there any tips as to figure out where in these steps cases are lost ?
>
>Thanks!
>Mirjam
>
>
>        [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat/.
>ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu%7
>Cc7324be5d30d4e737f6b08dd8dc75d08%7C0d4da0f84a314d76ace60a62331e1b84%7C
>0%7C0%7C638822609607262385%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
>dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3
>D%7C0%7C%7C%7C&sdata=aFe%2FQZ0sbdUUMVWtmO5IEny%2FGDfumAO99uuQFyKIjZg%3D
>&reserved=0 PLEASE do read the posting guide
>https://www.r/
>-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7Cc73
>24be5d30d4e737f6b08dd8dc75d08%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C
>0%7C638822609607277428%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUs
>IlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C
>0%7C%7C%7C&sdata=iCNTOoN%2FDet7sxrSHYd3FoEOEx5mHGPHO8PR9NOGxjg%3D&reser
>ved=0 and provide commented, minimal, self-contained, reproducible
>code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat/.
>ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu%7
>Cc7324be5d30d4e737f6b08dd8dc75d08%7C0d4da0f84a314d76ace60a62331e1b84%7C
>0%7C0%7C638822609607285873%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
>dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3
>D%7C0%7C%7C%7C&sdata=5gEvnpDP9A%2BXqltIbk6eKfM7HNzWNC%2BswxeyjkfVVNc%3D
>&reserved=0 PLEASE do read the posting guide
>https://www.r/
>-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7Cc73
>24be5d30d4e737f6b08dd8dc75d08%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C
>0%7C638822609607294283%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUs
>IlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C
>0%7C%7C%7C&sdata=pKvzz%2FHe6C%2BuP2Jv9S77AogSSq6xTNhTmf3rt5K7D9c%3D&res
>erved=0 and provide commented, minimal, self-contained, reproducible
>code.

--
Sent from my phone. Please excuse my brevity.


From rv15| @end|ng |rom y@hoo@@e  Thu May  8 21:39:40 2025
From: rv15| @end|ng |rom y@hoo@@e (ravi)
Date: Thu, 8 May 2025 19:39:40 +0000 (UTC)
Subject: [R] customizing library locations for R in ubuntu
References: <553509390.4728839.1746733180049.ref@mail.yahoo.com>
Message-ID: <553509390.4728839.1746733180049@mail.yahoo.com>

Hi,
In Windows, I follow a method with customized library locations that has worked for me when upgrading to new R versions.? I have not been able to follow the same method in Ubuntu. I would like to have help. Let me explain.
In windows, I add the following line:
.libPaths(c(?C:/Rownlib?,?C:/R/R-4.5.0/library?))
? ????in the file C:/R/R-4.5.0/etc/Rprofile.site

I have my own list of packages in the Rownlib folder and the packages that come with the R installation for the latest version in the second folder. When upgrading to a new R version, I just change the 2nd library location to that for the updated R version.
I am not able to follow the same method for ubuntu. I get:
> .libPaths()
[1] "/usr/lib/R/library"????????????"/usr/local/lib/R/site-library" "/usr/lib/R/site-library" 
I don?t understand the reason why the ubuntu R version has the 2nd and 3rd ?site-library? locations. Can somebody explain?
I tried to add a new library location with
.libPaths(new="/home/rvi/Rownlib")
This did not succeed in adding the new library.
I then added the line:
.libPaths(c("/home/rvi/Rownlib","/usr/lib/R/library" ))
in the Rprofile.site file after running:
sudo gedit /usr/lib/R/etc/Rprofile.site
Even this made no difference.
I noticed then that ubuntu has another file??~ etc/Renviron (this seems to be absent on windows). This perhaps has priority over what is in Rprofile.site.
I see the following in Renviron:
R_LIBS_USER=${R_LIBS_USER:-'%U'}
R_LIBS_SITE=${R_LIBS_SITE:-'/usr/local/lib/R/site-library:%S'}
If this is the way to go, I would like to have help in modifying these lines to include the location, "/home/rvi/Rownlib", as the 1st library location.
Apart from a solution, I would also appreciate explanations for why the library system in ubuntu is structured in the way it is.
Thanks, Ravi


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May  9 07:13:46 2025
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 08 May 2025 22:13:46 -0700
Subject: [R] customizing library locations for R in ubuntu
In-Reply-To: <553509390.4728839.1746733180049@mail.yahoo.com>
References: <553509390.4728839.1746733180049.ref@mail.yahoo.com>
 <553509390.4728839.1746733180049@mail.yahoo.com>
Message-ID: <5B900253-0566-4A2B-BCE1-59BF8C4D8396@dcn.davis.ca.us>

You might be better off just using the renv package.

To your question, you should probably be reading the R Installation and Administration manual.

Note that packages can be installed by the system administrator in *nix environments and the alternate libraries allow you to use some set of R packages without installing any in your personal library... if everyone is happy with this arrangement then you can have multiple users on the system using shared read-only package installations (installed by running sudo R for the sole purpose of installing system-wide contributed packages) without duplicating installs of the same package in multiple personal environments. 

However, it is actually more common than not for multiple users to want to run different versions of packages for reproducibility or bleeding edge features, in which case the premise of saving disk space this way is moot.

Given your habits, I can't see any reason for you to worry about keeping /usr/local/lib/R/site-library or /usr/lib/R/site-library intact in your library paths, but whatever local library path you do use, you will want to retain /usr/lib/R/library as an alternate because it contains the packages that are "built-in" to R.

On May 8, 2025 12:39:40 PM PDT, ravi via R-help <r-help at r-project.org> wrote:
>Hi,
>In Windows, I follow a method with customized library locations that has worked for me when upgrading to new R versions.? I have not been able to follow the same method in Ubuntu. I would like to have help. Let me explain.
>In windows, I add the following line:
>.libPaths(c(?C:/Rownlib?,?C:/R/R-4.5.0/library?))
>? ????in the file C:/R/R-4.5.0/etc/Rprofile.site
>
>I have my own list of packages in the Rownlib folder and the packages that come with the R installation for the latest version in the second folder. When upgrading to a new R version, I just change the 2nd library location to that for the updated R version.
>I am not able to follow the same method for ubuntu. I get:
>> .libPaths()
>[1] "/usr/lib/R/library"????????????"/usr/local/lib/R/site-library" "/usr/lib/R/site-library" 
>I don?t understand the reason why the ubuntu R version has the 2nd and 3rd ?site-library? locations. Can somebody explain?
>I tried to add a new library location with
>.libPaths(new="/home/rvi/Rownlib")
>This did not succeed in adding the new library.
>I then added the line:
>.libPaths(c("/home/rvi/Rownlib","/usr/lib/R/library" ))
>in the Rprofile.site file after running:
>sudo gedit /usr/lib/R/etc/Rprofile.site
>Even this made no difference.
>I noticed then that ubuntu has another file??~ etc/Renviron (this seems to be absent on windows). This perhaps has priority over what is in Rprofile.site.
>I see the following in Renviron:
>R_LIBS_USER=${R_LIBS_USER:-'%U'}
>R_LIBS_SITE=${R_LIBS_SITE:-'/usr/local/lib/R/site-library:%S'}
>If this is the way to go, I would like to have help in modifying these lines to include the location, "/home/rvi/Rownlib", as the 1st library location.
>Apart from a solution, I would also appreciate explanations for why the library system in ubuntu is structured in the way it is.
>Thanks, Ravi
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From r_goertz @end|ng |rom web@de  Fri May  9 10:39:28 2025
From: r_goertz @end|ng |rom web@de (Ralf Goertz)
Date: Fri, 9 May 2025 10:39:28 +0200
Subject: [R] empty `Rscript -e "Rcpp:::LdFlags()"` prevents installation
Message-ID: <20250509103928.67083516@delli.fritz.box>

Hi,

sometimes packages (in this case "pbv") can't be installed on my linux
system because in the tarball in src/Makevars there is a line like

PKG_LIBS = `$(R_HOME)/bin/Rscript -e "Rcpp:::LdFlags()"` $(LAPACK_LIBS)?

The command within the backticks doesn't return anything (i.e, no
additional c++ libraries are needed) but because of the backticks an
empty argument is passed to the linker which makes it fail. Using $()
instead of the backticks solves the problem. Should this be reported to
the package author or is it a problem common enough so that developer's
guidelines should address this?


From r_goertz @end|ng |rom web@de  Fri May  9 12:54:23 2025
From: r_goertz @end|ng |rom web@de (Ralf Goertz)
Date: Fri, 9 May 2025 12:54:23 +0200
Subject: [R] empty `Rscript -e "Rcpp:::LdFlags()"` prevents installation
In-Reply-To: <20250509103928.67083516@delli.fritz.box>
References: <20250509103928.67083516@delli.fritz.box>
Message-ID: <20250509125423.5d3329c8@delli.fritz.box>

I wrote

> Hi,
> 
> sometimes packages (in this case "pbv") can't be installed on my linux
> system because in the tarball in src/Makevars there is a line like
> 
> PKG_LIBS = `$(R_HOME)/bin/Rscript -e "Rcpp:::LdFlags()"` $(LAPACK_LIBS)?
> 
> The command within the backticks doesn't return anything (i.e, no
> additional c++ libraries are needed) but because of the backticks an
> empty argument is passed to the linker which makes it fail. Using $()
> instead of the backticks solves the problem. Should this be reported
> to the package author or is it a problem common enough so that
> developer's guidelines should address this?

Sorry for the noise. Backticks were not the problem. For those
interested: I run R using the command

konsole -e R

to have it in its own terminal window. As I sometimes work on different
projects simultaneously I use 

cat("\033]0;R ",getwd(),"\007")

in my ~/.Rprofile to have the konsole window title display the working
directory. This is what the above command puts into the variable making
the linker fail. I now guard the cat command by "if (interactive())" and
the problem is gone. The reason the "$()" variant worked is probably due
to some subtle differences between the two.
<https://mywiki.wooledge.org/BashFAQ/082>


From |kry|ov @end|ng |rom d|@root@org  Fri May  9 15:41:08 2025
From: |kry|ov @end|ng |rom d|@root@org (Ivan Krylov)
Date: Fri, 9 May 2025 16:41:08 +0300
Subject: [R] customizing library locations for R in ubuntu
In-Reply-To: <553509390.4728839.1746733180049@mail.yahoo.com>
References: <553509390.4728839.1746733180049.ref@mail.yahoo.com>
 <553509390.4728839.1746733180049@mail.yahoo.com>
Message-ID: <20250509164108.30b572c2@trisector>

? Thu, 8 May 2025 19:39:40 +0000 (UTC)
ravi via R-help <r-help at r-project.org> ?????:

> In windows, I add the following line:
> .libPaths(c(?C:/Rownlib?,?C:/R/R-4.5.0/library?))
> ? ????in the file C:/R/R-4.5.0/etc/Rprofile.site
> 
> I have my own list of packages in the Rownlib folder and the packages
> that come with the R installation for the latest version in the
> second folder. When upgrading to a new R version, I just change the
> 2nd library location to that for the updated R version.

This is not guaranteed to work, especially for (but not limited to)
packages that compile source code and link to various interfaces inside
R: https://stat.ethz.ch/pipermail/r-help/2025-April/480832.html

This problem is more visible on Ubuntu than on Windows. On GNU/Linux
systems, R packages will link to the third-party libraries you install
from the GNU/Linux repositories. When you update your Ubuntu
installation, the installed packages may stop working.

> I noticed then that ubuntu has another file??~ etc/Renviron (this
> seems to be absent on windows). This perhaps has priority over what
> is in Rprofile.site. I see the following in Renviron:
> R_LIBS_USER=${R_LIBS_USER:-'%U'}

Don't edit the system-wide configuration files to change settings that
are valid for your user only. Instead, set

R_LIBS_USER=/home/rvi/Rownlib

in /home/rvi/.Renviron and make sure that the directory exists. In
addition to "R Installation and Administration" (section 6.2), these
settings are documented in help(Startup) and help(.libPaths).

-- 
Best regards,
Ivan


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri May  9 11:31:29 2025
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 9 May 2025 11:31:29 +0200
Subject: [R] missing value where TRUE/FALSE needed with R ipolygrowth
Message-ID: <CAMk+s2Qp2u_uEAd3ggrgOWoWt_yvALVCh5Xa6nSf94SzgX66Sw@mail.gmail.com>

Dear R-Help,
I am trying to determine the growth rate of bacteria under specific
conditions using
ipolygrowth function `ipg_multisample`. While this worked before, I
got some data that give the error:
```
Error in if (tb.result$peak.growth.time == 0) { :
    missing value where TRUE/FALSE needed
  In addition: Warning message:
    In max(pgr[pgr > 0 & Re(x) >= 0 & Re(x) <= max]) :
    no non-missing arguments to max; returning -Inf
  Error in if (tb.result$peak.growth.time == 0) { :
      missing value where TRUE/FALSE needed
    In addition: Warning message:
      In max(pgr[pgr > 0 & Re(x) >= 0 & Re(x) <= max]) :
      no non-missing arguments to max; returning -Inf
```

I don't have NAs in the data, so I don't understand where the problem
arises, thus I can't generate a working example. The data is too large
to make a dataframe, so I am attaching it ad R object. In the text
below I visualize the data (also attached for ease).
Then I run ipg_multisample.
What would the problem be?
If it were a data fitting problem, I would have gotten a
non-convergence error message; I think this is more about the type of
data passed to the function, but I don't know what the issue is. Any
tip?
Thank you.

```
library(ipolygrowth)
df = readRDS("dfTest.rds")
COL=c("green", "cyan", "red", "blue", "orange", "purple")
plot(OD~Time, df, col="white")
points(OD~Time, df[df$Target==-1,], col=COL[1])
points(OD~Time, df[df$Target==-2,], col=COL[2])
points(OD~Time, df[df$Target==3,], col=COL[3])
points(OD~Time, df[df$Target==7,], col=COL[4])
points(OD~Time, df[df$Target==10,], col=COL[5])
points(OD~Time, df[df$Target==15,], col=COL[6])
legend("topleft", legend=unique(df$Target), pch=16, col=COL)
fit = ipg_multisample(data = df, id = "Target", time.name = "Time",
                      y.name = "OD")
```

-- 
Best regards,
Luigi

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Rplot.png
Type: image/png
Size: 204641 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20250509/3c6c024b/attachment.png>

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May  9 17:51:08 2025
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 09 May 2025 08:51:08 -0700
Subject: [R] missing value where TRUE/FALSE needed with R ipolygrowth
In-Reply-To: <CAMk+s2Qp2u_uEAd3ggrgOWoWt_yvALVCh5Xa6nSf94SzgX66Sw@mail.gmail.com>
References: <CAMk+s2Qp2u_uEAd3ggrgOWoWt_yvALVCh5Xa6nSf94SzgX66Sw@mail.gmail.com>
Message-ID: <6513DDF4-134D-4113-80A8-9F098FD067B4@dcn.davis.ca.us>

Your example is not reproducible. I recommend using the reprex package to get your examples in shape.

You don't have to have NA in your data to encounter a problem... all you need is transformations during your analysis that runs into domain violations to create NA along the way... negative numbers into powers or log or sqrt... kind of things. Read the documentation for your analysis packages carefully.

On May 9, 2025 2:31:29 AM PDT, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>Dear R-Help,
>I am trying to determine the growth rate of bacteria under specific
>conditions using
>ipolygrowth function `ipg_multisample`. While this worked before, I
>got some data that give the error:
>```
>Error in if (tb.result$peak.growth.time == 0) { :
>    missing value where TRUE/FALSE needed
>  In addition: Warning message:
>    In max(pgr[pgr > 0 & Re(x) >= 0 & Re(x) <= max]) :
>    no non-missing arguments to max; returning -Inf
>  Error in if (tb.result$peak.growth.time == 0) { :
>      missing value where TRUE/FALSE needed
>    In addition: Warning message:
>      In max(pgr[pgr > 0 & Re(x) >= 0 & Re(x) <= max]) :
>      no non-missing arguments to max; returning -Inf
>```
>
>I don't have NAs in the data, so I don't understand where the problem
>arises, thus I can't generate a working example. The data is too large
>to make a dataframe, so I am attaching it ad R object. In the text
>below I visualize the data (also attached for ease).
>Then I run ipg_multisample.
>What would the problem be?
>If it were a data fitting problem, I would have gotten a
>non-convergence error message; I think this is more about the type of
>data passed to the function, but I don't know what the issue is. Any
>tip?
>Thank you.
>
>```
>library(ipolygrowth)
>df = readRDS("dfTest.rds")
>COL=c("green", "cyan", "red", "blue", "orange", "purple")
>plot(OD~Time, df, col="white")
>points(OD~Time, df[df$Target==-1,], col=COL[1])
>points(OD~Time, df[df$Target==-2,], col=COL[2])
>points(OD~Time, df[df$Target==3,], col=COL[3])
>points(OD~Time, df[df$Target==7,], col=COL[4])
>points(OD~Time, df[df$Target==10,], col=COL[5])
>points(OD~Time, df[df$Target==15,], col=COL[6])
>legend("topleft", legend=unique(df$Target), pch=16, col=COL)
>fit = ipg_multisample(data = df, id = "Target", time.name = "Time",
>                      y.name = "OD")
>```
>

-- 
Sent from my phone. Please excuse my brevity.


From stms_th m@iii@g oii ybb@@e@jp  Tue May 13 03:37:19 2025
From: stms_th m@iii@g oii ybb@@e@jp (stms_th m@iii@g oii ybb@@e@jp)
Date: Tue, 13 May 2025 10:37:19 +0900 (JST)
Subject: [R] Help merging large datasets in R
In-Reply-To: <CH3PR22MB451407D3E25ECA21B18979FDCF88A@CH3PR22MB4514.namprd22.prod.outlook.com>
References: <AM7PR09MB37991DBC0591FD97C262BC63E088A@AM7PR09MB3799.eurprd09.prod.outlook.com>
 <CH3PR22MB451407D3E25ECA21B18979FDCF88A@CH3PR22MB4514.namprd22.prod.outlook.com>
Message-ID: <1236827621.1154709.1747100239088@mail.yahoo.co.jp>



--
????
????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????     ,,,,,,,,,w ,       ,             ,   0????????????????n,,,,,,


